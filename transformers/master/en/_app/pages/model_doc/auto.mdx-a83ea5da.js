import{S as u4t,i as p4t,s as _4t,e as a,k as l,w as m,t as o,M as b4t,c as s,d as t,m as i,a as n,x as f,h as r,b as d,F as e,g as b,y as g,q as h,o as u,B as p}from"../../chunks/vendor-6b77c823.js";import{T as wxr}from"../../chunks/Tip-39098574.js";import{D as M}from"../../chunks/Docstring-abef54e3.js";import{C as w}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as X}from"../../chunks/IconCopyLink-7a11ce68.js";function v4t(Ii){let J,$e,de,ue,io,me,Me,Vo,ji,$m,ma,Ni,Di,oy,Im,Be,co,qi,qs,ry,Os,Gs,ty,Oi,Xs,ay,Gi,jm,qa;return{c(){J=a("p"),$e=o("If your "),de=a("code"),ue=o("NewModelConfig"),io=o(" is a subclass of "),me=a("code"),Me=o("PretrainedConfig"),Vo=o(`, make sure its
`),ji=a("code"),$m=o("model_type"),ma=o(" attribute is set to the same key you use when registering the config (here "),Ni=a("code"),Di=o('"new-model"'),oy=o(")."),Im=l(),Be=a("p"),co=o("Likewise, if your "),qi=a("code"),qs=o("NewModel"),ry=o(" is a subclass of "),Os=a("a"),Gs=o("PreTrainedModel"),ty=o(`, make sure its
`),Oi=a("code"),Xs=o("config_class"),ay=o(` attribute is set to the same class you use when registering the model (here
`),Gi=a("code"),jm=o("NewModelConfig"),qa=o(")."),this.h()},l(mo){J=s(mo,"P",{});var pe=n(J);$e=r(pe,"If your "),de=s(pe,"CODE",{});var H8=n(de);ue=r(H8,"NewModelConfig"),H8.forEach(t),io=r(pe," is a subclass of "),me=s(pe,"CODE",{});var Xi=n(me);Me=r(Xi,"PretrainedConfig"),Xi.forEach(t),Vo=r(pe,`, make sure its
`),ji=s(pe,"CODE",{});var U8=n(ji);$m=r(U8,"model_type"),U8.forEach(t),ma=r(pe," attribute is set to the same key you use when registering the config (here "),Ni=s(pe,"CODE",{});var J8=n(Ni);Di=r(J8,'"new-model"'),J8.forEach(t),oy=r(pe,")."),pe.forEach(t),Im=i(mo),Be=s(mo,"P",{});var zo=n(Be);co=r(zo,"Likewise, if your "),qi=s(zo,"CODE",{});var Oa=n(qi);qs=r(Oa,"NewModel"),Oa.forEach(t),ry=r(zo," is a subclass of "),Os=s(zo,"A",{href:!0});var Y8=n(Os);Gs=r(Y8,"PreTrainedModel"),Y8.forEach(t),ty=r(zo,`, make sure its
`),Oi=s(zo,"CODE",{});var Nm=n(Oi);Xs=r(Nm,"config_class"),Nm.forEach(t),ay=r(zo,` attribute is set to the same class you use when registering the model (here
`),Gi=s(zo,"CODE",{});var K8=n(Gi);jm=r(K8,"NewModelConfig"),K8.forEach(t),qa=r(zo,")."),zo.forEach(t),this.h()},h(){d(Os,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(mo,pe){b(mo,J,pe),e(J,$e),e(J,de),e(de,ue),e(J,io),e(J,me),e(me,Me),e(J,Vo),e(J,ji),e(ji,$m),e(J,ma),e(J,Ni),e(Ni,Di),e(J,oy),b(mo,Im,pe),b(mo,Be,pe),e(Be,co),e(Be,qi),e(qi,qs),e(Be,ry),e(Be,Os),e(Os,Gs),e(Be,ty),e(Be,Oi),e(Oi,Xs),e(Be,ay),e(Be,Gi),e(Gi,jm),e(Be,qa)},d(mo){mo&&t(J),mo&&t(Im),mo&&t(Be)}}}function T4t(Ii){let J,$e,de,ue,io;return{c(){J=a("p"),$e=o("Passing "),de=a("code"),ue=o("use_auth_token=True"),io=o(" is required when you want to use a private model.")},l(me){J=s(me,"P",{});var Me=n(J);$e=r(Me,"Passing "),de=s(Me,"CODE",{});var Vo=n(de);ue=r(Vo,"use_auth_token=True"),Vo.forEach(t),io=r(Me," is required when you want to use a private model."),Me.forEach(t)},m(me,Me){b(me,J,Me),e(J,$e),e(J,de),e(de,ue),e(J,io)},d(me){me&&t(J)}}}function F4t(Ii){let J,$e,de,ue,io;return{c(){J=a("p"),$e=o("Passing "),de=a("code"),ue=o("use_auth_token=True"),io=o(" is required when you want to use a private model.")},l(me){J=s(me,"P",{});var Me=n(J);$e=r(Me,"Passing "),de=s(Me,"CODE",{});var Vo=n(de);ue=r(Vo,"use_auth_token=True"),Vo.forEach(t),io=r(Me," is required when you want to use a private model."),Me.forEach(t)},m(me,Me){b(me,J,Me),e(J,$e),e(J,de),e(de,ue),e(J,io)},d(me){me&&t(J)}}}function C4t(Ii){let J,$e,de,ue,io,me,Me,Vo,ji,$m,ma,Ni,Di,oy,Im,Be,co,qi,qs,ry,Os,Gs,ty,Oi,Xs,ay,Gi,jm,qa,mo,pe,H8,Xi,U8,J8,zo,Oa,Y8,Nm,K8,KPe,Nxe,Vi,Dm,xQ,sy,ZPe,kQ,e$e,Dxe,Vs,o$e,RQ,r$e,t$e,SQ,a$e,s$e,qxe,ny,Oxe,Z8,n$e,Gxe,qm,Xxe,zi,Om,PQ,ly,l$e,$Q,i$e,Vxe,Wo,iy,d$e,dy,c$e,e9,m$e,f$e,g$e,cy,h$e,IQ,u$e,p$e,_$e,fo,my,b$e,jQ,v$e,T$e,Wi,F$e,NQ,C$e,M$e,DQ,E$e,y$e,w$e,v,Gm,qQ,A$e,L$e,o9,B$e,x$e,k$e,Xm,OQ,R$e,S$e,r9,P$e,$$e,I$e,Vm,GQ,j$e,N$e,t9,D$e,q$e,O$e,zm,XQ,G$e,X$e,a9,V$e,z$e,W$e,Wm,VQ,Q$e,H$e,s9,U$e,J$e,Y$e,Qm,zQ,K$e,Z$e,n9,eIe,oIe,rIe,Hm,WQ,tIe,aIe,l9,sIe,nIe,lIe,Um,QQ,iIe,dIe,i9,cIe,mIe,fIe,Jm,HQ,gIe,hIe,d9,uIe,pIe,_Ie,Ym,UQ,bIe,vIe,c9,TIe,FIe,CIe,Km,JQ,MIe,EIe,m9,yIe,wIe,AIe,Zm,YQ,LIe,BIe,f9,xIe,kIe,RIe,ef,KQ,SIe,PIe,g9,$Ie,IIe,jIe,of,ZQ,NIe,DIe,h9,qIe,OIe,GIe,rf,eH,XIe,VIe,u9,zIe,WIe,QIe,tf,oH,HIe,UIe,p9,JIe,YIe,KIe,af,rH,ZIe,eje,_9,oje,rje,tje,sf,tH,aje,sje,b9,nje,lje,ije,nf,aH,dje,cje,v9,mje,fje,gje,lf,sH,hje,uje,T9,pje,_je,bje,df,nH,vje,Tje,F9,Fje,Cje,Mje,cf,lH,Eje,yje,C9,wje,Aje,Lje,mf,iH,Bje,xje,M9,kje,Rje,Sje,ff,dH,Pje,$je,E9,Ije,jje,Nje,gf,cH,Dje,qje,y9,Oje,Gje,Xje,hf,mH,Vje,zje,w9,Wje,Qje,Hje,uf,fH,Uje,Jje,A9,Yje,Kje,Zje,pf,gH,eNe,oNe,L9,rNe,tNe,aNe,_f,hH,sNe,nNe,B9,lNe,iNe,dNe,bf,uH,cNe,mNe,x9,fNe,gNe,hNe,vf,pH,uNe,pNe,k9,_Ne,bNe,vNe,Tf,_H,TNe,FNe,R9,CNe,MNe,ENe,Ff,bH,yNe,wNe,S9,ANe,LNe,BNe,Cf,vH,xNe,kNe,P9,RNe,SNe,PNe,Mf,TH,$Ne,INe,$9,jNe,NNe,DNe,Ef,FH,qNe,ONe,I9,GNe,XNe,VNe,yf,CH,zNe,WNe,j9,QNe,HNe,UNe,wf,MH,JNe,YNe,N9,KNe,ZNe,eDe,Af,EH,oDe,rDe,D9,tDe,aDe,sDe,Lf,yH,nDe,lDe,q9,iDe,dDe,cDe,Bf,wH,mDe,fDe,O9,gDe,hDe,uDe,xf,AH,pDe,_De,G9,bDe,vDe,TDe,kf,LH,FDe,CDe,X9,MDe,EDe,yDe,Rf,BH,wDe,ADe,V9,LDe,BDe,xDe,Sf,xH,kDe,RDe,z9,SDe,PDe,$De,Pf,kH,IDe,jDe,W9,NDe,DDe,qDe,$f,RH,ODe,GDe,Q9,XDe,VDe,zDe,If,SH,WDe,QDe,H9,HDe,UDe,JDe,jf,PH,YDe,KDe,U9,ZDe,eqe,oqe,Nf,$H,rqe,tqe,J9,aqe,sqe,nqe,Df,IH,lqe,iqe,Y9,dqe,cqe,mqe,qf,jH,fqe,gqe,K9,hqe,uqe,pqe,Of,NH,_qe,bqe,Z9,vqe,Tqe,Fqe,Gf,DH,Cqe,Mqe,eB,Eqe,yqe,wqe,Xf,qH,Aqe,Lqe,oB,Bqe,xqe,kqe,Vf,OH,Rqe,Sqe,rB,Pqe,$qe,Iqe,zf,GH,jqe,Nqe,tB,Dqe,qqe,Oqe,Wf,XH,Gqe,Xqe,aB,Vqe,zqe,Wqe,Qf,VH,Qqe,Hqe,sB,Uqe,Jqe,Yqe,Hf,zH,Kqe,Zqe,nB,eOe,oOe,rOe,Uf,WH,tOe,aOe,lB,sOe,nOe,lOe,Jf,QH,iOe,dOe,iB,cOe,mOe,fOe,Yf,HH,gOe,hOe,dB,uOe,pOe,_Oe,Kf,UH,bOe,vOe,cB,TOe,FOe,COe,Zf,JH,MOe,EOe,mB,yOe,wOe,AOe,eg,YH,LOe,BOe,fB,xOe,kOe,ROe,og,KH,SOe,POe,gB,$Oe,IOe,jOe,rg,ZH,NOe,DOe,hB,qOe,OOe,GOe,tg,eU,XOe,VOe,uB,zOe,WOe,QOe,ag,oU,HOe,UOe,pB,JOe,YOe,KOe,sg,rU,ZOe,eGe,_B,oGe,rGe,tGe,ng,tU,aGe,sGe,bB,nGe,lGe,iGe,lg,aU,dGe,cGe,vB,mGe,fGe,gGe,ig,sU,hGe,uGe,TB,pGe,_Ge,bGe,dg,nU,vGe,TGe,FB,FGe,CGe,MGe,cg,lU,EGe,yGe,CB,wGe,AGe,LGe,mg,iU,BGe,xGe,MB,kGe,RGe,SGe,fg,dU,PGe,$Ge,EB,IGe,jGe,NGe,gg,cU,DGe,qGe,yB,OGe,GGe,XGe,hg,mU,VGe,zGe,wB,WGe,QGe,HGe,ug,fU,UGe,JGe,AB,YGe,KGe,ZGe,pg,gU,eXe,oXe,LB,rXe,tXe,aXe,_g,hU,sXe,nXe,BB,lXe,iXe,dXe,bg,uU,cXe,mXe,xB,fXe,gXe,hXe,vg,pU,uXe,pXe,kB,_Xe,bXe,vXe,Tg,_U,TXe,FXe,RB,CXe,MXe,EXe,Fg,bU,yXe,wXe,SB,AXe,LXe,BXe,Cg,vU,xXe,kXe,PB,RXe,SXe,PXe,Mg,TU,$Xe,IXe,$B,jXe,NXe,DXe,Eg,FU,qXe,OXe,IB,GXe,XXe,VXe,yg,CU,zXe,WXe,jB,QXe,HXe,UXe,wg,MU,JXe,YXe,NB,KXe,ZXe,eVe,Ag,EU,oVe,rVe,DB,tVe,aVe,sVe,Lg,yU,nVe,lVe,qB,iVe,dVe,cVe,Bg,wU,mVe,fVe,OB,gVe,hVe,uVe,xg,AU,pVe,_Ve,GB,bVe,vVe,TVe,LU,FVe,CVe,fy,MVe,kg,gy,EVe,BU,yVe,zxe,Qi,Rg,xU,hy,wVe,kU,AVe,Wxe,Qo,uy,LVe,py,BVe,XB,xVe,kVe,RVe,_y,SVe,RU,PVe,$Ve,IVe,go,by,jVe,SU,NVe,DVe,Ga,qVe,PU,OVe,GVe,$U,XVe,VVe,IU,zVe,WVe,QVe,E,zs,jU,HVe,UVe,VB,JVe,YVe,zB,KVe,ZVe,eze,Ws,NU,oze,rze,WB,tze,aze,QB,sze,nze,lze,Qs,DU,ize,dze,HB,cze,mze,UB,fze,gze,hze,Sg,qU,uze,pze,JB,_ze,bze,vze,Hs,OU,Tze,Fze,YB,Cze,Mze,KB,Eze,yze,wze,Pg,GU,Aze,Lze,ZB,Bze,xze,kze,$g,XU,Rze,Sze,ex,Pze,$ze,Ize,Ig,VU,jze,Nze,ox,Dze,qze,Oze,Us,zU,Gze,Xze,rx,Vze,zze,tx,Wze,Qze,Hze,Js,WU,Uze,Jze,ax,Yze,Kze,sx,Zze,eWe,oWe,Ys,QU,rWe,tWe,nx,aWe,sWe,lx,nWe,lWe,iWe,jg,HU,dWe,cWe,ix,mWe,fWe,gWe,Ng,UU,hWe,uWe,dx,pWe,_We,bWe,Ks,JU,vWe,TWe,cx,FWe,CWe,mx,MWe,EWe,yWe,Dg,YU,wWe,AWe,fx,LWe,BWe,xWe,Zs,KU,kWe,RWe,gx,SWe,PWe,hx,$We,IWe,jWe,en,ZU,NWe,DWe,ux,qWe,OWe,px,GWe,XWe,VWe,on,eJ,zWe,WWe,_x,QWe,HWe,oJ,UWe,JWe,YWe,qg,rJ,KWe,ZWe,bx,eQe,oQe,rQe,rn,tJ,tQe,aQe,vx,sQe,nQe,Tx,lQe,iQe,dQe,Og,aJ,cQe,mQe,Fx,fQe,gQe,hQe,tn,sJ,uQe,pQe,Cx,_Qe,bQe,Mx,vQe,TQe,FQe,an,nJ,CQe,MQe,Ex,EQe,yQe,yx,wQe,AQe,LQe,sn,lJ,BQe,xQe,wx,kQe,RQe,Ax,SQe,PQe,$Qe,Gg,iJ,IQe,jQe,Lx,NQe,DQe,qQe,nn,dJ,OQe,GQe,Bx,XQe,VQe,xx,zQe,WQe,QQe,Xg,cJ,HQe,UQe,kx,JQe,YQe,KQe,ln,mJ,ZQe,eHe,Rx,oHe,rHe,Sx,tHe,aHe,sHe,dn,fJ,nHe,lHe,Px,iHe,dHe,$x,cHe,mHe,fHe,cn,gJ,gHe,hHe,Ix,uHe,pHe,jx,_He,bHe,vHe,mn,hJ,THe,FHe,Nx,CHe,MHe,Dx,EHe,yHe,wHe,Vg,uJ,AHe,LHe,qx,BHe,xHe,kHe,fn,pJ,RHe,SHe,Ox,PHe,$He,Gx,IHe,jHe,NHe,gn,_J,DHe,qHe,Xx,OHe,GHe,Vx,XHe,VHe,zHe,hn,bJ,WHe,QHe,zx,HHe,UHe,Wx,JHe,YHe,KHe,un,vJ,ZHe,eUe,Qx,oUe,rUe,Hx,tUe,aUe,sUe,pn,TJ,nUe,lUe,Ux,iUe,dUe,Jx,cUe,mUe,fUe,_n,FJ,gUe,hUe,Yx,uUe,pUe,Kx,_Ue,bUe,vUe,zg,CJ,TUe,FUe,Zx,CUe,MUe,EUe,bn,MJ,yUe,wUe,ek,AUe,LUe,ok,BUe,xUe,kUe,Wg,EJ,RUe,SUe,rk,PUe,$Ue,IUe,Qg,yJ,jUe,NUe,tk,DUe,qUe,OUe,vn,wJ,GUe,XUe,ak,VUe,zUe,sk,WUe,QUe,HUe,Tn,AJ,UUe,JUe,nk,YUe,KUe,lk,ZUe,eJe,oJe,Hg,LJ,rJe,tJe,ik,aJe,sJe,nJe,Fn,BJ,lJe,iJe,dk,dJe,cJe,ck,mJe,fJe,gJe,Cn,xJ,hJe,uJe,mk,pJe,_Je,fk,bJe,vJe,TJe,Mn,kJ,FJe,CJe,gk,MJe,EJe,hk,yJe,wJe,AJe,En,RJ,LJe,BJe,uk,xJe,kJe,pk,RJe,SJe,PJe,yn,SJ,$Je,IJe,_k,jJe,NJe,bk,DJe,qJe,OJe,Ug,PJ,GJe,XJe,vk,VJe,zJe,WJe,Jg,$J,QJe,HJe,Tk,UJe,JJe,YJe,Yg,IJ,KJe,ZJe,Fk,eYe,oYe,rYe,Kg,jJ,tYe,aYe,Ck,sYe,nYe,lYe,wn,NJ,iYe,dYe,Mk,cYe,mYe,Ek,fYe,gYe,hYe,Zg,DJ,uYe,pYe,yk,_Ye,bYe,vYe,An,qJ,TYe,FYe,wk,CYe,MYe,Ak,EYe,yYe,wYe,Ln,OJ,AYe,LYe,Lk,BYe,xYe,Bk,kYe,RYe,SYe,Bn,GJ,PYe,$Ye,xk,IYe,jYe,kk,NYe,DYe,qYe,xn,XJ,OYe,GYe,Rk,XYe,VYe,Sk,zYe,WYe,QYe,kn,VJ,HYe,UYe,Pk,JYe,YYe,$k,KYe,ZYe,eKe,Rn,zJ,oKe,rKe,Ik,tKe,aKe,jk,sKe,nKe,lKe,eh,WJ,iKe,dKe,Nk,cKe,mKe,fKe,oh,QJ,gKe,hKe,Dk,uKe,pKe,_Ke,Sn,HJ,bKe,vKe,qk,TKe,FKe,Ok,CKe,MKe,EKe,Pn,UJ,yKe,wKe,Gk,AKe,LKe,Xk,BKe,xKe,kKe,$n,JJ,RKe,SKe,Vk,PKe,$Ke,zk,IKe,jKe,NKe,rh,YJ,DKe,qKe,Wk,OKe,GKe,XKe,th,KJ,VKe,zKe,Qk,WKe,QKe,HKe,ah,ZJ,UKe,JKe,Hk,YKe,KKe,ZKe,sh,eY,eZe,oZe,Uk,rZe,tZe,aZe,In,oY,sZe,nZe,Jk,lZe,iZe,Yk,dZe,cZe,mZe,nh,rY,fZe,gZe,Kk,hZe,uZe,pZe,lh,tY,_Ze,bZe,Zk,vZe,TZe,FZe,jn,aY,CZe,MZe,eR,EZe,yZe,oR,wZe,AZe,LZe,Nn,sY,BZe,xZe,rR,kZe,RZe,tR,SZe,PZe,$Ze,nY,IZe,jZe,vy,NZe,ih,Ty,DZe,lY,qZe,Qxe,Hi,dh,iY,Fy,OZe,dY,GZe,Hxe,Ho,Cy,XZe,My,VZe,aR,zZe,WZe,QZe,Ey,HZe,cY,UZe,JZe,YZe,Ie,yy,KZe,mY,ZZe,eeo,Xa,oeo,fY,reo,teo,gY,aeo,seo,hY,neo,leo,ieo,re,ch,uY,deo,ceo,sR,meo,feo,geo,mh,pY,heo,ueo,nR,peo,_eo,beo,fh,_Y,veo,Teo,lR,Feo,Ceo,Meo,gh,bY,Eeo,yeo,iR,weo,Aeo,Leo,hh,vY,Beo,xeo,dR,keo,Reo,Seo,uh,TY,Peo,$eo,cR,Ieo,jeo,Neo,ph,FY,Deo,qeo,mR,Oeo,Geo,Xeo,_h,CY,Veo,zeo,fR,Weo,Qeo,Heo,bh,MY,Ueo,Jeo,gR,Yeo,Keo,Zeo,vh,EY,eoo,ooo,hR,roo,too,aoo,Th,yY,soo,noo,uR,loo,ioo,doo,Fh,wY,coo,moo,pR,foo,goo,hoo,Ch,AY,uoo,poo,_R,_oo,boo,voo,Mh,LY,Too,Foo,bR,Coo,Moo,Eoo,Eh,BY,yoo,woo,vR,Aoo,Loo,Boo,yh,xY,xoo,koo,TR,Roo,Soo,Poo,wh,kY,$oo,Ioo,FR,joo,Noo,Doo,Ah,RY,qoo,Ooo,CR,Goo,Xoo,Voo,Lh,zoo,SY,Woo,Qoo,wy,Hoo,Bh,Ay,Uoo,PY,Joo,Uxe,Ui,xh,$Y,Ly,Yoo,IY,Koo,Jxe,Uo,By,Zoo,xy,ero,MR,oro,rro,tro,ky,aro,jY,sro,nro,lro,je,Ry,iro,NY,dro,cro,Ji,mro,DY,fro,gro,qY,hro,uro,pro,xe,kh,OY,_ro,bro,ER,vro,Tro,Fro,Rh,GY,Cro,Mro,yR,Ero,yro,wro,Sh,XY,Aro,Lro,wR,Bro,xro,kro,Ph,VY,Rro,Sro,AR,Pro,$ro,Iro,$h,zY,jro,Nro,LR,Dro,qro,Oro,Ih,WY,Gro,Xro,BR,Vro,zro,Wro,jh,QY,Qro,Hro,xR,Uro,Jro,Yro,Nh,HY,Kro,Zro,kR,eto,oto,rto,Dh,tto,UY,ato,sto,Sy,nto,qh,Py,lto,JY,ito,Yxe,Yi,Oh,YY,$y,dto,KY,cto,Kxe,Jo,Iy,mto,Ki,fto,ZY,gto,hto,eK,uto,pto,_to,jy,bto,oK,vto,Tto,Fto,Vr,Ny,Cto,rK,Mto,Eto,Zi,yto,tK,wto,Ato,aK,Lto,Bto,xto,sK,kto,Rto,Dy,Sto,Ne,qy,Pto,nK,$to,Ito,Va,jto,lK,Nto,Dto,iK,qto,Oto,dK,Gto,Xto,Vto,F,Gh,cK,zto,Wto,RR,Qto,Hto,Uto,Xh,mK,Jto,Yto,SR,Kto,Zto,eao,Vh,fK,oao,rao,PR,tao,aao,sao,zh,gK,nao,lao,$R,iao,dao,cao,Wh,hK,mao,fao,IR,gao,hao,uao,Qh,uK,pao,_ao,jR,bao,vao,Tao,Hh,pK,Fao,Cao,NR,Mao,Eao,yao,Uh,_K,wao,Aao,DR,Lao,Bao,xao,Jh,bK,kao,Rao,qR,Sao,Pao,$ao,Yh,vK,Iao,jao,OR,Nao,Dao,qao,Kh,TK,Oao,Gao,GR,Xao,Vao,zao,Zh,FK,Wao,Qao,XR,Hao,Uao,Jao,eu,CK,Yao,Kao,VR,Zao,eso,oso,ou,MK,rso,tso,zR,aso,sso,nso,ru,EK,lso,iso,WR,dso,cso,mso,tu,yK,fso,gso,QR,hso,uso,pso,au,wK,_so,bso,HR,vso,Tso,Fso,su,AK,Cso,Mso,UR,Eso,yso,wso,nu,LK,Aso,Lso,JR,Bso,xso,kso,lu,BK,Rso,Sso,YR,Pso,$so,Iso,iu,xK,jso,Nso,KR,Dso,qso,Oso,du,kK,Gso,Xso,ZR,Vso,zso,Wso,cu,RK,Qso,Hso,eS,Uso,Jso,Yso,mu,SK,Kso,Zso,oS,eno,ono,rno,fu,PK,tno,ano,rS,sno,nno,lno,gu,$K,ino,dno,tS,cno,mno,fno,hu,IK,gno,hno,aS,uno,pno,_no,Dn,jK,bno,vno,sS,Tno,Fno,nS,Cno,Mno,Eno,uu,NK,yno,wno,lS,Ano,Lno,Bno,pu,DK,xno,kno,iS,Rno,Sno,Pno,_u,qK,$no,Ino,dS,jno,Nno,Dno,bu,OK,qno,Ono,cS,Gno,Xno,Vno,vu,GK,zno,Wno,mS,Qno,Hno,Uno,Tu,XK,Jno,Yno,fS,Kno,Zno,elo,Fu,VK,olo,rlo,gS,tlo,alo,slo,Cu,zK,nlo,llo,hS,ilo,dlo,clo,Mu,WK,mlo,flo,uS,glo,hlo,ulo,Eu,QK,plo,_lo,pS,blo,vlo,Tlo,yu,HK,Flo,Clo,_S,Mlo,Elo,ylo,wu,UK,wlo,Alo,bS,Llo,Blo,xlo,Au,JK,klo,Rlo,vS,Slo,Plo,$lo,Lu,YK,Ilo,jlo,TS,Nlo,Dlo,qlo,Bu,KK,Olo,Glo,FS,Xlo,Vlo,zlo,xu,ZK,Wlo,Qlo,CS,Hlo,Ulo,Jlo,ku,eZ,Ylo,Klo,MS,Zlo,eio,oio,Ru,oZ,rio,tio,ES,aio,sio,nio,Su,rZ,lio,iio,yS,dio,cio,mio,Pu,tZ,fio,gio,wS,hio,uio,pio,$u,aZ,_io,bio,AS,vio,Tio,Fio,Iu,sZ,Cio,Mio,LS,Eio,yio,wio,ju,nZ,Aio,Lio,BS,Bio,xio,kio,Nu,lZ,Rio,Sio,xS,Pio,$io,Iio,Du,iZ,jio,Nio,kS,Dio,qio,Oio,qu,dZ,Gio,Xio,RS,Vio,zio,Wio,Ou,cZ,Qio,Hio,SS,Uio,Jio,Yio,Gu,mZ,Kio,Zio,PS,edo,odo,rdo,Xu,fZ,tdo,ado,$S,sdo,ndo,ldo,Vu,gZ,ido,ddo,IS,cdo,mdo,fdo,zu,hZ,gdo,hdo,jS,udo,pdo,_do,Wu,uZ,bdo,vdo,NS,Tdo,Fdo,Cdo,Qu,pZ,Mdo,Edo,DS,ydo,wdo,Ado,Hu,_Z,Ldo,Bdo,qS,xdo,kdo,Rdo,Uu,bZ,Sdo,Pdo,OS,$do,Ido,jdo,Ju,vZ,Ndo,Ddo,GS,qdo,Odo,Gdo,Yu,TZ,Xdo,Vdo,XS,zdo,Wdo,Qdo,Ku,FZ,Hdo,Udo,VS,Jdo,Ydo,Kdo,Zu,CZ,Zdo,eco,zS,oco,rco,tco,ep,MZ,aco,sco,WS,nco,lco,ico,op,EZ,dco,cco,QS,mco,fco,gco,rp,yZ,hco,uco,HS,pco,_co,bco,tp,wZ,vco,Tco,US,Fco,Cco,Mco,ap,AZ,Eco,yco,JS,wco,Aco,Lco,sp,LZ,Bco,xco,YS,kco,Rco,Sco,np,BZ,Pco,$co,KS,Ico,jco,Nco,lp,xZ,Dco,qco,ZS,Oco,Gco,Xco,ip,kZ,Vco,zco,eP,Wco,Qco,Hco,dp,RZ,Uco,Jco,oP,Yco,Kco,Zco,cp,SZ,emo,omo,rP,rmo,tmo,amo,mp,PZ,smo,nmo,tP,lmo,imo,dmo,fp,$Z,cmo,mmo,aP,fmo,gmo,hmo,gp,IZ,umo,pmo,sP,_mo,bmo,vmo,hp,jZ,Tmo,Fmo,nP,Cmo,Mmo,Emo,up,NZ,ymo,wmo,lP,Amo,Lmo,Bmo,pp,DZ,xmo,kmo,iP,Rmo,Smo,Pmo,_p,qZ,$mo,Imo,dP,jmo,Nmo,Dmo,bp,OZ,qmo,Omo,cP,Gmo,Xmo,Vmo,vp,GZ,zmo,Wmo,mP,Qmo,Hmo,Umo,Tp,XZ,Jmo,Ymo,fP,Kmo,Zmo,efo,Fp,VZ,ofo,rfo,gP,tfo,afo,sfo,Cp,nfo,zZ,lfo,ifo,WZ,dfo,cfo,QZ,mfo,ffo,Oy,Zxe,ed,Mp,HZ,Gy,gfo,UZ,hfo,eke,Yo,Xy,ufo,od,pfo,JZ,_fo,bfo,YZ,vfo,Tfo,Ffo,Vy,Cfo,KZ,Mfo,Efo,yfo,zr,zy,wfo,ZZ,Afo,Lfo,rd,Bfo,eee,xfo,kfo,oee,Rfo,Sfo,Pfo,ree,$fo,Ifo,Wy,jfo,De,Qy,Nfo,tee,Dfo,qfo,za,Ofo,aee,Gfo,Xfo,see,Vfo,zfo,nee,Wfo,Qfo,Hfo,k,Ep,lee,Ufo,Jfo,hP,Yfo,Kfo,Zfo,yp,iee,ego,ogo,uP,rgo,tgo,ago,wp,dee,sgo,ngo,pP,lgo,igo,dgo,Ap,cee,cgo,mgo,_P,fgo,ggo,hgo,Lp,mee,ugo,pgo,bP,_go,bgo,vgo,Bp,fee,Tgo,Fgo,vP,Cgo,Mgo,Ego,xp,gee,ygo,wgo,TP,Ago,Lgo,Bgo,kp,hee,xgo,kgo,FP,Rgo,Sgo,Pgo,Rp,uee,$go,Igo,CP,jgo,Ngo,Dgo,Sp,pee,qgo,Ogo,MP,Ggo,Xgo,Vgo,Pp,_ee,zgo,Wgo,EP,Qgo,Hgo,Ugo,$p,bee,Jgo,Ygo,yP,Kgo,Zgo,eho,Ip,vee,oho,rho,wP,tho,aho,sho,jp,Tee,nho,lho,AP,iho,dho,cho,Np,Fee,mho,fho,LP,gho,hho,uho,Dp,Cee,pho,_ho,BP,bho,vho,Tho,qp,Mee,Fho,Cho,xP,Mho,Eho,yho,Op,Eee,who,Aho,kP,Lho,Bho,xho,Gp,yee,kho,Rho,RP,Sho,Pho,$ho,Xp,wee,Iho,jho,SP,Nho,Dho,qho,Vp,Aee,Oho,Gho,PP,Xho,Vho,zho,zp,Lee,Who,Qho,$P,Hho,Uho,Jho,Wp,Bee,Yho,Kho,IP,Zho,euo,ouo,Qp,xee,ruo,tuo,jP,auo,suo,nuo,Hp,kee,luo,iuo,NP,duo,cuo,muo,Up,Ree,fuo,guo,DP,huo,uuo,puo,Jp,See,_uo,buo,qP,vuo,Tuo,Fuo,Yp,Pee,Cuo,Muo,OP,Euo,yuo,wuo,Kp,$ee,Auo,Luo,GP,Buo,xuo,kuo,Zp,Iee,Ruo,Suo,XP,Puo,$uo,Iuo,e_,jee,juo,Nuo,VP,Duo,quo,Ouo,o_,Nee,Guo,Xuo,zP,Vuo,zuo,Wuo,r_,Dee,Quo,Huo,WP,Uuo,Juo,Yuo,t_,qee,Kuo,Zuo,QP,epo,opo,rpo,a_,Oee,tpo,apo,HP,spo,npo,lpo,s_,Gee,ipo,dpo,UP,cpo,mpo,fpo,n_,Xee,gpo,hpo,JP,upo,ppo,_po,l_,Vee,bpo,vpo,YP,Tpo,Fpo,Cpo,i_,zee,Mpo,Epo,KP,ypo,wpo,Apo,d_,Lpo,Wee,Bpo,xpo,Qee,kpo,Rpo,Hee,Spo,Ppo,Hy,oke,td,c_,Uee,Uy,$po,Jee,Ipo,rke,Ko,Jy,jpo,ad,Npo,Yee,Dpo,qpo,Kee,Opo,Gpo,Xpo,Yy,Vpo,Zee,zpo,Wpo,Qpo,Wr,Ky,Hpo,eoe,Upo,Jpo,sd,Ypo,ooe,Kpo,Zpo,roe,e_o,o_o,r_o,toe,t_o,a_o,Zy,s_o,qe,ew,n_o,aoe,l_o,i_o,Wa,d_o,soe,c_o,m_o,noe,f_o,g_o,loe,h_o,u_o,p_o,$,m_,ioe,__o,b_o,ZP,v_o,T_o,F_o,f_,doe,C_o,M_o,e$,E_o,y_o,w_o,g_,coe,A_o,L_o,o$,B_o,x_o,k_o,h_,moe,R_o,S_o,r$,P_o,$_o,I_o,u_,foe,j_o,N_o,t$,D_o,q_o,O_o,p_,goe,G_o,X_o,a$,V_o,z_o,W_o,__,hoe,Q_o,H_o,s$,U_o,J_o,Y_o,b_,uoe,K_o,Z_o,n$,ebo,obo,rbo,v_,poe,tbo,abo,l$,sbo,nbo,lbo,T_,_oe,ibo,dbo,i$,cbo,mbo,fbo,F_,boe,gbo,hbo,d$,ubo,pbo,_bo,C_,voe,bbo,vbo,c$,Tbo,Fbo,Cbo,M_,Toe,Mbo,Ebo,m$,ybo,wbo,Abo,E_,Foe,Lbo,Bbo,f$,xbo,kbo,Rbo,y_,Coe,Sbo,Pbo,g$,$bo,Ibo,jbo,w_,Moe,Nbo,Dbo,h$,qbo,Obo,Gbo,A_,Eoe,Xbo,Vbo,u$,zbo,Wbo,Qbo,L_,yoe,Hbo,Ubo,p$,Jbo,Ybo,Kbo,B_,woe,Zbo,e2o,_$,o2o,r2o,t2o,x_,Aoe,a2o,s2o,b$,n2o,l2o,i2o,k_,Loe,d2o,c2o,v$,m2o,f2o,g2o,R_,Boe,h2o,u2o,T$,p2o,_2o,b2o,S_,xoe,v2o,T2o,F$,F2o,C2o,M2o,P_,koe,E2o,y2o,C$,w2o,A2o,L2o,$_,Roe,B2o,x2o,M$,k2o,R2o,S2o,I_,Soe,P2o,$2o,E$,I2o,j2o,N2o,j_,Poe,D2o,q2o,y$,O2o,G2o,X2o,N_,$oe,V2o,z2o,w$,W2o,Q2o,H2o,D_,Ioe,U2o,J2o,A$,Y2o,K2o,Z2o,q_,joe,evo,ovo,L$,rvo,tvo,avo,O_,Noe,svo,nvo,B$,lvo,ivo,dvo,G_,Doe,cvo,mvo,x$,fvo,gvo,hvo,X_,qoe,uvo,pvo,k$,_vo,bvo,vvo,V_,Ooe,Tvo,Fvo,R$,Cvo,Mvo,Evo,z_,Goe,yvo,wvo,S$,Avo,Lvo,Bvo,W_,xvo,Xoe,kvo,Rvo,Voe,Svo,Pvo,zoe,$vo,Ivo,ow,tke,nd,Q_,Woe,rw,jvo,Qoe,Nvo,ake,Zo,tw,Dvo,ld,qvo,Hoe,Ovo,Gvo,Uoe,Xvo,Vvo,zvo,aw,Wvo,Joe,Qvo,Hvo,Uvo,Qr,sw,Jvo,Yoe,Yvo,Kvo,id,Zvo,Koe,eTo,oTo,Zoe,rTo,tTo,aTo,ere,sTo,nTo,nw,lTo,Oe,lw,iTo,ore,dTo,cTo,Qa,mTo,rre,fTo,gTo,tre,hTo,uTo,are,pTo,_To,bTo,I,H_,sre,vTo,TTo,P$,FTo,CTo,MTo,U_,nre,ETo,yTo,$$,wTo,ATo,LTo,J_,lre,BTo,xTo,I$,kTo,RTo,STo,Y_,ire,PTo,$To,j$,ITo,jTo,NTo,K_,dre,DTo,qTo,N$,OTo,GTo,XTo,Z_,cre,VTo,zTo,D$,WTo,QTo,HTo,eb,mre,UTo,JTo,q$,YTo,KTo,ZTo,ob,fre,e1o,o1o,O$,r1o,t1o,a1o,rb,gre,s1o,n1o,G$,l1o,i1o,d1o,tb,hre,c1o,m1o,X$,f1o,g1o,h1o,ab,ure,u1o,p1o,V$,_1o,b1o,v1o,sb,pre,T1o,F1o,z$,C1o,M1o,E1o,nb,_re,y1o,w1o,W$,A1o,L1o,B1o,lb,bre,x1o,k1o,Q$,R1o,S1o,P1o,ib,vre,$1o,I1o,H$,j1o,N1o,D1o,db,Tre,q1o,O1o,U$,G1o,X1o,V1o,cb,Fre,z1o,W1o,J$,Q1o,H1o,U1o,mb,Cre,J1o,Y1o,Y$,K1o,Z1o,eFo,fb,Mre,oFo,rFo,K$,tFo,aFo,sFo,gb,Ere,nFo,lFo,Z$,iFo,dFo,cFo,hb,yre,mFo,fFo,eI,gFo,hFo,uFo,ub,wre,pFo,_Fo,oI,bFo,vFo,TFo,pb,Are,FFo,CFo,rI,MFo,EFo,yFo,_b,Lre,wFo,AFo,tI,LFo,BFo,xFo,bb,Bre,kFo,RFo,aI,SFo,PFo,$Fo,vb,xre,IFo,jFo,sI,NFo,DFo,qFo,Tb,kre,OFo,GFo,nI,XFo,VFo,zFo,Fb,Rre,WFo,QFo,lI,HFo,UFo,JFo,Cb,Sre,YFo,KFo,iI,ZFo,eCo,oCo,Mb,Pre,rCo,tCo,dI,aCo,sCo,nCo,Eb,$re,lCo,iCo,Ire,dCo,cCo,mCo,yb,jre,fCo,gCo,cI,hCo,uCo,pCo,wb,Nre,_Co,bCo,mI,vCo,TCo,FCo,Ab,Dre,CCo,MCo,fI,ECo,yCo,wCo,Lb,qre,ACo,LCo,gI,BCo,xCo,kCo,Bb,RCo,Ore,SCo,PCo,Gre,$Co,ICo,Xre,jCo,NCo,iw,ske,dd,xb,Vre,dw,DCo,zre,qCo,nke,er,cw,OCo,cd,GCo,Wre,XCo,VCo,Qre,zCo,WCo,QCo,mw,HCo,Hre,UCo,JCo,YCo,Hr,fw,KCo,Ure,ZCo,eMo,md,oMo,Jre,rMo,tMo,Yre,aMo,sMo,nMo,Kre,lMo,iMo,gw,dMo,Ge,hw,cMo,Zre,mMo,fMo,Ha,gMo,ete,hMo,uMo,ote,pMo,_Mo,rte,bMo,vMo,TMo,ne,kb,tte,FMo,CMo,hI,MMo,EMo,yMo,Rb,ate,wMo,AMo,uI,LMo,BMo,xMo,Sb,ste,kMo,RMo,pI,SMo,PMo,$Mo,Pb,nte,IMo,jMo,_I,NMo,DMo,qMo,$b,lte,OMo,GMo,bI,XMo,VMo,zMo,Ib,ite,WMo,QMo,vI,HMo,UMo,JMo,jb,dte,YMo,KMo,TI,ZMo,e4o,o4o,Nb,cte,r4o,t4o,FI,a4o,s4o,n4o,Db,mte,l4o,i4o,CI,d4o,c4o,m4o,qb,fte,f4o,g4o,MI,h4o,u4o,p4o,Ob,gte,_4o,b4o,EI,v4o,T4o,F4o,Gb,hte,C4o,M4o,yI,E4o,y4o,w4o,Xb,ute,A4o,L4o,wI,B4o,x4o,k4o,Vb,pte,R4o,S4o,AI,P4o,$4o,I4o,zb,_te,j4o,N4o,LI,D4o,q4o,O4o,Wb,bte,G4o,X4o,BI,V4o,z4o,W4o,Qb,Q4o,vte,H4o,U4o,Tte,J4o,Y4o,Fte,K4o,Z4o,uw,lke,fd,Hb,Cte,pw,eEo,Mte,oEo,ike,or,_w,rEo,gd,tEo,Ete,aEo,sEo,yte,nEo,lEo,iEo,bw,dEo,wte,cEo,mEo,fEo,Ur,vw,gEo,Ate,hEo,uEo,hd,pEo,Lte,_Eo,bEo,Bte,vEo,TEo,FEo,xte,CEo,MEo,Tw,EEo,Xe,Fw,yEo,kte,wEo,AEo,Ua,LEo,Rte,BEo,xEo,Ste,kEo,REo,Pte,SEo,PEo,$Eo,A,Ub,$te,IEo,jEo,xI,NEo,DEo,qEo,Jb,Ite,OEo,GEo,kI,XEo,VEo,zEo,Yb,jte,WEo,QEo,RI,HEo,UEo,JEo,Kb,Nte,YEo,KEo,SI,ZEo,e3o,o3o,Zb,Dte,r3o,t3o,PI,a3o,s3o,n3o,e2,qte,l3o,i3o,$I,d3o,c3o,m3o,o2,Ote,f3o,g3o,II,h3o,u3o,p3o,r2,Gte,_3o,b3o,jI,v3o,T3o,F3o,t2,Xte,C3o,M3o,NI,E3o,y3o,w3o,a2,Vte,A3o,L3o,DI,B3o,x3o,k3o,s2,zte,R3o,S3o,qI,P3o,$3o,I3o,n2,Wte,j3o,N3o,OI,D3o,q3o,O3o,l2,Qte,G3o,X3o,GI,V3o,z3o,W3o,i2,Hte,Q3o,H3o,XI,U3o,J3o,Y3o,d2,Ute,K3o,Z3o,VI,e5o,o5o,r5o,c2,Jte,t5o,a5o,zI,s5o,n5o,l5o,m2,Yte,i5o,d5o,WI,c5o,m5o,f5o,f2,Kte,g5o,h5o,QI,u5o,p5o,_5o,g2,Zte,b5o,v5o,HI,T5o,F5o,C5o,h2,eae,M5o,E5o,UI,y5o,w5o,A5o,u2,oae,L5o,B5o,JI,x5o,k5o,R5o,p2,rae,S5o,P5o,YI,$5o,I5o,j5o,_2,tae,N5o,D5o,KI,q5o,O5o,G5o,b2,aae,X5o,V5o,ZI,z5o,W5o,Q5o,v2,sae,H5o,U5o,ej,J5o,Y5o,K5o,T2,nae,Z5o,eyo,oj,oyo,ryo,tyo,F2,lae,ayo,syo,rj,nyo,lyo,iyo,C2,iae,dyo,cyo,tj,myo,fyo,gyo,M2,dae,hyo,uyo,aj,pyo,_yo,byo,E2,cae,vyo,Tyo,sj,Fyo,Cyo,Myo,y2,mae,Eyo,yyo,nj,wyo,Ayo,Lyo,w2,fae,Byo,xyo,lj,kyo,Ryo,Syo,A2,gae,Pyo,$yo,ij,Iyo,jyo,Nyo,L2,hae,Dyo,qyo,dj,Oyo,Gyo,Xyo,B2,uae,Vyo,zyo,cj,Wyo,Qyo,Hyo,x2,pae,Uyo,Jyo,mj,Yyo,Kyo,Zyo,k2,_ae,ewo,owo,fj,rwo,two,awo,R2,bae,swo,nwo,gj,lwo,iwo,dwo,S2,vae,cwo,mwo,hj,fwo,gwo,hwo,P2,Tae,uwo,pwo,uj,_wo,bwo,vwo,$2,Fae,Two,Fwo,pj,Cwo,Mwo,Ewo,I2,Cae,ywo,wwo,_j,Awo,Lwo,Bwo,j2,Mae,xwo,kwo,bj,Rwo,Swo,Pwo,N2,Eae,$wo,Iwo,vj,jwo,Nwo,Dwo,D2,yae,qwo,Owo,Tj,Gwo,Xwo,Vwo,q2,wae,zwo,Wwo,Fj,Qwo,Hwo,Uwo,O2,Jwo,Aae,Ywo,Kwo,Lae,Zwo,e6o,Bae,o6o,r6o,Cw,dke,ud,G2,xae,Mw,t6o,kae,a6o,cke,rr,Ew,s6o,pd,n6o,Rae,l6o,i6o,Sae,d6o,c6o,m6o,yw,f6o,Pae,g6o,h6o,u6o,Jr,ww,p6o,$ae,_6o,b6o,_d,v6o,Iae,T6o,F6o,jae,C6o,M6o,E6o,Nae,y6o,w6o,Aw,A6o,Ve,Lw,L6o,Dae,B6o,x6o,Ja,k6o,qae,R6o,S6o,Oae,P6o,$6o,Gae,I6o,j6o,N6o,O,X2,Xae,D6o,q6o,Cj,O6o,G6o,X6o,V2,Vae,V6o,z6o,Mj,W6o,Q6o,H6o,z2,zae,U6o,J6o,Ej,Y6o,K6o,Z6o,W2,Wae,eAo,oAo,yj,rAo,tAo,aAo,Q2,Qae,sAo,nAo,wj,lAo,iAo,dAo,H2,Hae,cAo,mAo,Aj,fAo,gAo,hAo,U2,Uae,uAo,pAo,Lj,_Ao,bAo,vAo,J2,Jae,TAo,FAo,Bj,CAo,MAo,EAo,Y2,Yae,yAo,wAo,xj,AAo,LAo,BAo,K2,Kae,xAo,kAo,kj,RAo,SAo,PAo,Z2,Zae,$Ao,IAo,Rj,jAo,NAo,DAo,ev,ese,qAo,OAo,Sj,GAo,XAo,VAo,ov,ose,zAo,WAo,Pj,QAo,HAo,UAo,rv,rse,JAo,YAo,$j,KAo,ZAo,e0o,tv,tse,o0o,r0o,Ij,t0o,a0o,s0o,av,ase,n0o,l0o,jj,i0o,d0o,c0o,sv,sse,m0o,f0o,Nj,g0o,h0o,u0o,nv,nse,p0o,_0o,Dj,b0o,v0o,T0o,lv,lse,F0o,C0o,qj,M0o,E0o,y0o,iv,ise,w0o,A0o,Oj,L0o,B0o,x0o,dv,dse,k0o,R0o,Gj,S0o,P0o,$0o,cv,cse,I0o,j0o,Xj,N0o,D0o,q0o,mv,mse,O0o,G0o,Vj,X0o,V0o,z0o,fv,fse,W0o,Q0o,zj,H0o,U0o,J0o,gv,gse,Y0o,K0o,Wj,Z0o,eLo,oLo,hv,hse,rLo,tLo,Qj,aLo,sLo,nLo,uv,use,lLo,iLo,Hj,dLo,cLo,mLo,pv,pse,fLo,gLo,Uj,hLo,uLo,pLo,_v,_Lo,_se,bLo,vLo,bse,TLo,FLo,vse,CLo,MLo,Bw,mke,bd,bv,Tse,xw,ELo,Fse,yLo,fke,tr,kw,wLo,vd,ALo,Cse,LLo,BLo,Mse,xLo,kLo,RLo,Rw,SLo,Ese,PLo,$Lo,ILo,Yr,Sw,jLo,yse,NLo,DLo,Td,qLo,wse,OLo,GLo,Ase,XLo,VLo,zLo,Lse,WLo,QLo,Pw,HLo,ze,$w,ULo,Bse,JLo,YLo,Ya,KLo,xse,ZLo,e7o,kse,o7o,r7o,Rse,t7o,a7o,s7o,da,vv,Sse,n7o,l7o,Jj,i7o,d7o,c7o,Tv,Pse,m7o,f7o,Yj,g7o,h7o,u7o,Fv,$se,p7o,_7o,Kj,b7o,v7o,T7o,Cv,Ise,F7o,C7o,Zj,M7o,E7o,y7o,Mv,jse,w7o,A7o,eN,L7o,B7o,x7o,Ev,k7o,Nse,R7o,S7o,Dse,P7o,$7o,qse,I7o,j7o,Iw,gke,Fd,yv,Ose,jw,N7o,Gse,D7o,hke,ar,Nw,q7o,Cd,O7o,Xse,G7o,X7o,Vse,V7o,z7o,W7o,Dw,Q7o,zse,H7o,U7o,J7o,Kr,qw,Y7o,Wse,K7o,Z7o,Md,e8o,Qse,o8o,r8o,Hse,t8o,a8o,s8o,Use,n8o,l8o,Ow,i8o,We,Gw,d8o,Jse,c8o,m8o,Ka,f8o,Yse,g8o,h8o,Kse,u8o,p8o,Zse,_8o,b8o,v8o,D,wv,ene,T8o,F8o,oN,C8o,M8o,E8o,Av,one,y8o,w8o,rN,A8o,L8o,B8o,Lv,rne,x8o,k8o,tN,R8o,S8o,P8o,Bv,tne,$8o,I8o,aN,j8o,N8o,D8o,xv,ane,q8o,O8o,sN,G8o,X8o,V8o,kv,sne,z8o,W8o,nN,Q8o,H8o,U8o,Rv,nne,J8o,Y8o,lN,K8o,Z8o,e9o,Sv,lne,o9o,r9o,iN,t9o,a9o,s9o,Pv,ine,n9o,l9o,dN,i9o,d9o,c9o,$v,dne,m9o,f9o,cN,g9o,h9o,u9o,Iv,cne,p9o,_9o,mN,b9o,v9o,T9o,jv,mne,F9o,C9o,fN,M9o,E9o,y9o,Nv,fne,w9o,A9o,gN,L9o,B9o,x9o,Dv,gne,k9o,R9o,hN,S9o,P9o,$9o,qv,hne,I9o,j9o,uN,N9o,D9o,q9o,Ov,une,O9o,G9o,pN,X9o,V9o,z9o,Gv,pne,W9o,Q9o,_N,H9o,U9o,J9o,Xv,_ne,Y9o,K9o,bN,Z9o,eBo,oBo,Vv,bne,rBo,tBo,vN,aBo,sBo,nBo,zv,vne,lBo,iBo,TN,dBo,cBo,mBo,Wv,Tne,fBo,gBo,FN,hBo,uBo,pBo,Qv,Fne,_Bo,bBo,CN,vBo,TBo,FBo,Hv,Cne,CBo,MBo,MN,EBo,yBo,wBo,Uv,Mne,ABo,LBo,EN,BBo,xBo,kBo,Jv,Ene,RBo,SBo,yN,PBo,$Bo,IBo,Yv,yne,jBo,NBo,wN,DBo,qBo,OBo,Kv,wne,GBo,XBo,AN,VBo,zBo,WBo,Zv,Ane,QBo,HBo,LN,UBo,JBo,YBo,eT,Lne,KBo,ZBo,BN,exo,oxo,rxo,oT,Bne,txo,axo,xN,sxo,nxo,lxo,rT,xne,ixo,dxo,kN,cxo,mxo,fxo,tT,kne,gxo,hxo,RN,uxo,pxo,_xo,aT,Rne,bxo,vxo,SN,Txo,Fxo,Cxo,sT,Mxo,Sne,Exo,yxo,Pne,wxo,Axo,$ne,Lxo,Bxo,Xw,uke,Ed,nT,Ine,Vw,xxo,jne,kxo,pke,sr,zw,Rxo,yd,Sxo,Nne,Pxo,$xo,Dne,Ixo,jxo,Nxo,Ww,Dxo,qne,qxo,Oxo,Gxo,Zr,Qw,Xxo,One,Vxo,zxo,wd,Wxo,Gne,Qxo,Hxo,Xne,Uxo,Jxo,Yxo,Vne,Kxo,Zxo,Hw,eko,Qe,Uw,oko,zne,rko,tko,Za,ako,Wne,sko,nko,Qne,lko,iko,Hne,dko,cko,mko,R,lT,Une,fko,gko,PN,hko,uko,pko,iT,Jne,_ko,bko,$N,vko,Tko,Fko,dT,Yne,Cko,Mko,IN,Eko,yko,wko,cT,Kne,Ako,Lko,jN,Bko,xko,kko,mT,Zne,Rko,Sko,NN,Pko,$ko,Iko,fT,ele,jko,Nko,DN,Dko,qko,Oko,gT,ole,Gko,Xko,qN,Vko,zko,Wko,hT,rle,Qko,Hko,ON,Uko,Jko,Yko,uT,tle,Kko,Zko,GN,eRo,oRo,rRo,pT,ale,tRo,aRo,XN,sRo,nRo,lRo,_T,sle,iRo,dRo,VN,cRo,mRo,fRo,bT,nle,gRo,hRo,zN,uRo,pRo,_Ro,vT,lle,bRo,vRo,WN,TRo,FRo,CRo,TT,ile,MRo,ERo,QN,yRo,wRo,ARo,FT,dle,LRo,BRo,HN,xRo,kRo,RRo,CT,cle,SRo,PRo,UN,$Ro,IRo,jRo,MT,mle,NRo,DRo,JN,qRo,ORo,GRo,ET,fle,XRo,VRo,YN,zRo,WRo,QRo,yT,gle,HRo,URo,KN,JRo,YRo,KRo,wT,hle,ZRo,eSo,ZN,oSo,rSo,tSo,AT,ule,aSo,sSo,eD,nSo,lSo,iSo,LT,ple,dSo,cSo,oD,mSo,fSo,gSo,BT,_le,hSo,uSo,rD,pSo,_So,bSo,xT,ble,vSo,TSo,tD,FSo,CSo,MSo,kT,vle,ESo,ySo,aD,wSo,ASo,LSo,RT,Tle,BSo,xSo,sD,kSo,RSo,SSo,ST,Fle,PSo,$So,nD,ISo,jSo,NSo,PT,Cle,DSo,qSo,lD,OSo,GSo,XSo,$T,Mle,VSo,zSo,iD,WSo,QSo,HSo,IT,Ele,USo,JSo,dD,YSo,KSo,ZSo,jT,yle,ePo,oPo,cD,rPo,tPo,aPo,NT,wle,sPo,nPo,mD,lPo,iPo,dPo,DT,Ale,cPo,mPo,fD,fPo,gPo,hPo,qT,Lle,uPo,pPo,gD,_Po,bPo,vPo,OT,Ble,TPo,FPo,hD,CPo,MPo,EPo,GT,xle,yPo,wPo,uD,APo,LPo,BPo,XT,kle,xPo,kPo,pD,RPo,SPo,PPo,VT,Rle,$Po,IPo,_D,jPo,NPo,DPo,zT,Sle,qPo,OPo,bD,GPo,XPo,VPo,WT,zPo,Ple,WPo,QPo,$le,HPo,UPo,Ile,JPo,YPo,Jw,_ke,Ad,QT,jle,Yw,KPo,Nle,ZPo,bke,nr,Kw,e$o,Ld,o$o,Dle,r$o,t$o,qle,a$o,s$o,n$o,Zw,l$o,Ole,i$o,d$o,c$o,et,e6,m$o,Gle,f$o,g$o,Bd,h$o,Xle,u$o,p$o,Vle,_$o,b$o,v$o,zle,T$o,F$o,o6,C$o,He,r6,M$o,Wle,E$o,y$o,es,w$o,Qle,A$o,L$o,Hle,B$o,x$o,Ule,k$o,R$o,S$o,Jle,HT,Yle,P$o,$$o,vD,I$o,j$o,N$o,UT,D$o,Kle,q$o,O$o,Zle,G$o,X$o,eie,V$o,z$o,t6,vke,xd,JT,oie,a6,W$o,rie,Q$o,Tke,lr,s6,H$o,kd,U$o,tie,J$o,Y$o,aie,K$o,Z$o,eIo,n6,oIo,sie,rIo,tIo,aIo,ot,l6,sIo,nie,nIo,lIo,Rd,iIo,lie,dIo,cIo,iie,mIo,fIo,gIo,die,hIo,uIo,i6,pIo,Ue,d6,_Io,cie,bIo,vIo,os,TIo,mie,FIo,CIo,fie,MIo,EIo,gie,yIo,wIo,AIo,fe,YT,hie,LIo,BIo,TD,xIo,kIo,RIo,KT,uie,SIo,PIo,FD,$Io,IIo,jIo,qn,pie,NIo,DIo,CD,qIo,OIo,MD,GIo,XIo,VIo,ZT,_ie,zIo,WIo,ED,QIo,HIo,UIo,fa,bie,JIo,YIo,yD,KIo,ZIo,wD,ejo,ojo,AD,rjo,tjo,ajo,e1,vie,sjo,njo,LD,ljo,ijo,djo,o1,Tie,cjo,mjo,BD,fjo,gjo,hjo,r1,Fie,ujo,pjo,xD,_jo,bjo,vjo,t1,Cie,Tjo,Fjo,kD,Cjo,Mjo,Ejo,a1,Mie,yjo,wjo,RD,Ajo,Ljo,Bjo,s1,Eie,xjo,kjo,SD,Rjo,Sjo,Pjo,n1,$jo,yie,Ijo,jjo,wie,Njo,Djo,Aie,qjo,Ojo,c6,Fke,Sd,l1,Lie,m6,Gjo,Bie,Xjo,Cke,ir,f6,Vjo,Pd,zjo,xie,Wjo,Qjo,kie,Hjo,Ujo,Jjo,g6,Yjo,Rie,Kjo,Zjo,eNo,rt,h6,oNo,Sie,rNo,tNo,$d,aNo,Pie,sNo,nNo,$ie,lNo,iNo,dNo,Iie,cNo,mNo,u6,fNo,Je,p6,gNo,jie,hNo,uNo,rs,pNo,Nie,_No,bNo,Die,vNo,TNo,qie,FNo,CNo,MNo,Oie,i1,Gie,ENo,yNo,PD,wNo,ANo,LNo,d1,BNo,Xie,xNo,kNo,Vie,RNo,SNo,zie,PNo,$No,_6,Mke,Id,c1,Wie,b6,INo,Qie,jNo,Eke,dr,v6,NNo,jd,DNo,Hie,qNo,ONo,Uie,GNo,XNo,VNo,T6,zNo,Jie,WNo,QNo,HNo,tt,F6,UNo,Yie,JNo,YNo,Nd,KNo,Kie,ZNo,eDo,Zie,oDo,rDo,tDo,ede,aDo,sDo,C6,nDo,Ye,M6,lDo,ode,iDo,dDo,ts,cDo,rde,mDo,fDo,tde,gDo,hDo,ade,uDo,pDo,_Do,ke,m1,sde,bDo,vDo,$D,TDo,FDo,CDo,f1,nde,MDo,EDo,ID,yDo,wDo,ADo,g1,lde,LDo,BDo,jD,xDo,kDo,RDo,h1,ide,SDo,PDo,ND,$Do,IDo,jDo,u1,dde,NDo,DDo,DD,qDo,ODo,GDo,p1,cde,XDo,VDo,qD,zDo,WDo,QDo,_1,mde,HDo,UDo,OD,JDo,YDo,KDo,b1,fde,ZDo,eqo,GD,oqo,rqo,tqo,v1,aqo,gde,sqo,nqo,hde,lqo,iqo,ude,dqo,cqo,E6,yke,Dd,T1,pde,y6,mqo,_de,fqo,wke,cr,w6,gqo,qd,hqo,bde,uqo,pqo,vde,_qo,bqo,vqo,A6,Tqo,Tde,Fqo,Cqo,Mqo,at,L6,Eqo,Fde,yqo,wqo,Od,Aqo,Cde,Lqo,Bqo,Mde,xqo,kqo,Rqo,Ede,Sqo,Pqo,B6,$qo,Ke,x6,Iqo,yde,jqo,Nqo,as,Dqo,wde,qqo,Oqo,Ade,Gqo,Xqo,Lde,Vqo,zqo,Wqo,ss,F1,Bde,Qqo,Hqo,XD,Uqo,Jqo,Yqo,C1,xde,Kqo,Zqo,VD,eOo,oOo,rOo,M1,kde,tOo,aOo,zD,sOo,nOo,lOo,E1,Rde,iOo,dOo,WD,cOo,mOo,fOo,y1,gOo,Sde,hOo,uOo,Pde,pOo,_Oo,$de,bOo,vOo,k6,Ake,Gd,w1,Ide,R6,TOo,jde,FOo,Lke,mr,S6,COo,Xd,MOo,Nde,EOo,yOo,Dde,wOo,AOo,LOo,P6,BOo,qde,xOo,kOo,ROo,st,$6,SOo,Ode,POo,$Oo,Vd,IOo,Gde,jOo,NOo,Xde,DOo,qOo,OOo,Vde,GOo,XOo,I6,VOo,Ze,j6,zOo,zde,WOo,QOo,ns,HOo,Wde,UOo,JOo,Qde,YOo,KOo,Hde,ZOo,eGo,oGo,Re,A1,Ude,rGo,tGo,QD,aGo,sGo,nGo,L1,Jde,lGo,iGo,HD,dGo,cGo,mGo,B1,Yde,fGo,gGo,UD,hGo,uGo,pGo,x1,Kde,_Go,bGo,JD,vGo,TGo,FGo,k1,Zde,CGo,MGo,YD,EGo,yGo,wGo,R1,ece,AGo,LGo,KD,BGo,xGo,kGo,S1,oce,RGo,SGo,ZD,PGo,$Go,IGo,P1,rce,jGo,NGo,eq,DGo,qGo,OGo,$1,GGo,tce,XGo,VGo,ace,zGo,WGo,sce,QGo,HGo,N6,Bke,zd,I1,nce,D6,UGo,lce,JGo,xke,fr,q6,YGo,Wd,KGo,ice,ZGo,eXo,dce,oXo,rXo,tXo,O6,aXo,cce,sXo,nXo,lXo,nt,G6,iXo,mce,dXo,cXo,Qd,mXo,fce,fXo,gXo,gce,hXo,uXo,pXo,hce,_Xo,bXo,X6,vXo,eo,V6,TXo,uce,FXo,CXo,ls,MXo,pce,EXo,yXo,_ce,wXo,AXo,bce,LXo,BXo,xXo,z6,j1,vce,kXo,RXo,oq,SXo,PXo,$Xo,N1,Tce,IXo,jXo,rq,NXo,DXo,qXo,D1,OXo,Fce,GXo,XXo,Cce,VXo,zXo,Mce,WXo,QXo,W6,kke,Hd,q1,Ece,Q6,HXo,yce,UXo,Rke,gr,H6,JXo,Ud,YXo,wce,KXo,ZXo,Ace,eVo,oVo,rVo,U6,tVo,Lce,aVo,sVo,nVo,lt,J6,lVo,Bce,iVo,dVo,Jd,cVo,xce,mVo,fVo,kce,gVo,hVo,uVo,Rce,pVo,_Vo,Y6,bVo,oo,K6,vVo,Sce,TVo,FVo,is,CVo,Pce,MVo,EVo,$ce,yVo,wVo,Ice,AVo,LVo,BVo,ds,O1,jce,xVo,kVo,tq,RVo,SVo,PVo,G1,Nce,$Vo,IVo,aq,jVo,NVo,DVo,X1,Dce,qVo,OVo,sq,GVo,XVo,VVo,V1,qce,zVo,WVo,nq,QVo,HVo,UVo,z1,JVo,Oce,YVo,KVo,Gce,ZVo,ezo,Xce,ozo,rzo,Z6,Ske,Yd,W1,Vce,eA,tzo,zce,azo,Pke,hr,oA,szo,Kd,nzo,Wce,lzo,izo,Qce,dzo,czo,mzo,rA,fzo,Hce,gzo,hzo,uzo,it,tA,pzo,Uce,_zo,bzo,Zd,vzo,Jce,Tzo,Fzo,Yce,Czo,Mzo,Ezo,Kce,yzo,wzo,aA,Azo,ro,sA,Lzo,Zce,Bzo,xzo,cs,kzo,eme,Rzo,Szo,ome,Pzo,$zo,rme,Izo,jzo,Nzo,ec,Q1,tme,Dzo,qzo,lq,Ozo,Gzo,Xzo,H1,ame,Vzo,zzo,iq,Wzo,Qzo,Hzo,U1,sme,Uzo,Jzo,dq,Yzo,Kzo,Zzo,J1,eWo,nme,oWo,rWo,lme,tWo,aWo,ime,sWo,nWo,nA,$ke,oc,Y1,dme,lA,lWo,cme,iWo,Ike,ur,iA,dWo,rc,cWo,mme,mWo,fWo,fme,gWo,hWo,uWo,dA,pWo,gme,_Wo,bWo,vWo,dt,cA,TWo,hme,FWo,CWo,tc,MWo,ume,EWo,yWo,pme,wWo,AWo,LWo,_me,BWo,xWo,mA,kWo,to,fA,RWo,bme,SWo,PWo,ms,$Wo,vme,IWo,jWo,Tme,NWo,DWo,Fme,qWo,OWo,GWo,Cme,K1,Mme,XWo,VWo,cq,zWo,WWo,QWo,Z1,HWo,Eme,UWo,JWo,yme,YWo,KWo,wme,ZWo,eQo,gA,jke,ac,eF,Ame,hA,oQo,Lme,rQo,Nke,pr,uA,tQo,sc,aQo,Bme,sQo,nQo,xme,lQo,iQo,dQo,pA,cQo,kme,mQo,fQo,gQo,ct,_A,hQo,Rme,uQo,pQo,nc,_Qo,Sme,bQo,vQo,Pme,TQo,FQo,CQo,$me,MQo,EQo,bA,yQo,ao,vA,wQo,Ime,AQo,LQo,fs,BQo,jme,xQo,kQo,Nme,RQo,SQo,Dme,PQo,$Qo,IQo,qme,oF,Ome,jQo,NQo,mq,DQo,qQo,OQo,rF,GQo,Gme,XQo,VQo,Xme,zQo,WQo,Vme,QQo,HQo,TA,Dke,lc,tF,zme,FA,UQo,Wme,JQo,qke,_r,CA,YQo,ic,KQo,Qme,ZQo,eHo,Hme,oHo,rHo,tHo,MA,aHo,Ume,sHo,nHo,lHo,mt,EA,iHo,Jme,dHo,cHo,dc,mHo,Yme,fHo,gHo,Kme,hHo,uHo,pHo,Zme,_Ho,bHo,yA,vHo,so,wA,THo,efe,FHo,CHo,gs,MHo,ofe,EHo,yHo,rfe,wHo,AHo,tfe,LHo,BHo,xHo,AA,aF,afe,kHo,RHo,fq,SHo,PHo,$Ho,sF,sfe,IHo,jHo,gq,NHo,DHo,qHo,nF,OHo,nfe,GHo,XHo,lfe,VHo,zHo,ife,WHo,QHo,LA,Oke,cc,lF,dfe,BA,HHo,cfe,UHo,Gke,br,xA,JHo,mc,YHo,mfe,KHo,ZHo,ffe,eUo,oUo,rUo,kA,tUo,gfe,aUo,sUo,nUo,ft,RA,lUo,hfe,iUo,dUo,fc,cUo,ufe,mUo,fUo,pfe,gUo,hUo,uUo,_fe,pUo,_Uo,SA,bUo,no,PA,vUo,bfe,TUo,FUo,hs,CUo,vfe,MUo,EUo,Tfe,yUo,wUo,Ffe,AUo,LUo,BUo,Cfe,iF,Mfe,xUo,kUo,hq,RUo,SUo,PUo,dF,$Uo,Efe,IUo,jUo,yfe,NUo,DUo,wfe,qUo,OUo,$A,Xke,gc,cF,Afe,IA,GUo,Lfe,XUo,Vke,vr,jA,VUo,hc,zUo,Bfe,WUo,QUo,xfe,HUo,UUo,JUo,NA,YUo,kfe,KUo,ZUo,eJo,gt,DA,oJo,Rfe,rJo,tJo,uc,aJo,Sfe,sJo,nJo,Pfe,lJo,iJo,dJo,$fe,cJo,mJo,qA,fJo,ho,OA,gJo,Ife,hJo,uJo,us,pJo,jfe,_Jo,bJo,Nfe,vJo,TJo,Dfe,FJo,CJo,MJo,B,mF,qfe,EJo,yJo,uq,wJo,AJo,LJo,fF,Ofe,BJo,xJo,pq,kJo,RJo,SJo,gF,Gfe,PJo,$Jo,_q,IJo,jJo,NJo,hF,Xfe,DJo,qJo,bq,OJo,GJo,XJo,uF,Vfe,VJo,zJo,vq,WJo,QJo,HJo,pF,zfe,UJo,JJo,Tq,YJo,KJo,ZJo,_F,Wfe,eYo,oYo,Fq,rYo,tYo,aYo,bF,Qfe,sYo,nYo,Cq,lYo,iYo,dYo,vF,Hfe,cYo,mYo,Mq,fYo,gYo,hYo,TF,Ufe,uYo,pYo,Eq,_Yo,bYo,vYo,FF,Jfe,TYo,FYo,yq,CYo,MYo,EYo,CF,Yfe,yYo,wYo,wq,AYo,LYo,BYo,MF,Kfe,xYo,kYo,Aq,RYo,SYo,PYo,EF,Zfe,$Yo,IYo,Lq,jYo,NYo,DYo,yF,ege,qYo,OYo,Bq,GYo,XYo,VYo,wF,oge,zYo,WYo,xq,QYo,HYo,UYo,On,rge,JYo,YYo,kq,KYo,ZYo,Rq,eKo,oKo,rKo,AF,tge,tKo,aKo,Sq,sKo,nKo,lKo,LF,age,iKo,dKo,Pq,cKo,mKo,fKo,BF,sge,gKo,hKo,$q,uKo,pKo,_Ko,xF,nge,bKo,vKo,Iq,TKo,FKo,CKo,kF,lge,MKo,EKo,jq,yKo,wKo,AKo,RF,ige,LKo,BKo,Nq,xKo,kKo,RKo,SF,dge,SKo,PKo,Dq,$Ko,IKo,jKo,PF,cge,NKo,DKo,qq,qKo,OKo,GKo,$F,mge,XKo,VKo,Oq,zKo,WKo,QKo,IF,fge,HKo,UKo,Gq,JKo,YKo,KKo,jF,gge,ZKo,eZo,Xq,oZo,rZo,tZo,NF,hge,aZo,sZo,Vq,nZo,lZo,iZo,DF,uge,dZo,cZo,zq,mZo,fZo,gZo,qF,pge,hZo,uZo,Wq,pZo,_Zo,bZo,OF,_ge,vZo,TZo,Qq,FZo,CZo,MZo,GF,bge,EZo,yZo,Hq,wZo,AZo,LZo,XF,vge,BZo,xZo,Uq,kZo,RZo,SZo,VF,Tge,PZo,$Zo,Jq,IZo,jZo,NZo,zF,Fge,DZo,qZo,Yq,OZo,GZo,XZo,WF,Cge,VZo,zZo,Kq,WZo,QZo,HZo,QF,Mge,UZo,JZo,Zq,YZo,KZo,ZZo,HF,Ege,eer,oer,eO,rer,ter,aer,UF,yge,ser,ner,oO,ler,ier,der,JF,wge,cer,mer,rO,fer,ger,her,YF,Age,uer,per,tO,_er,ber,ver,Lge,Ter,Fer,GA,zke,pc,KF,Bge,XA,Cer,xge,Mer,Wke,Tr,VA,Eer,_c,yer,kge,wer,Aer,Rge,Ler,Ber,xer,zA,ker,Sge,Rer,Ser,Per,ht,WA,$er,Pge,Ier,jer,bc,Ner,$ge,Der,qer,Ige,Oer,Ger,Xer,jge,Ver,zer,QA,Wer,uo,HA,Qer,Nge,Her,Uer,ps,Jer,Dge,Yer,Ker,qge,Zer,eor,Oge,oor,ror,tor,H,ZF,Gge,aor,sor,aO,nor,lor,ior,eC,Xge,dor,cor,sO,mor,gor,hor,oC,Vge,uor,por,nO,_or,bor,vor,rC,zge,Tor,For,lO,Cor,Mor,Eor,tC,Wge,yor,wor,iO,Aor,Lor,Bor,aC,Qge,xor,kor,dO,Ror,Sor,Por,sC,Hge,$or,Ior,cO,jor,Nor,Dor,nC,Uge,qor,Oor,mO,Gor,Xor,Vor,lC,Jge,zor,Wor,fO,Qor,Hor,Uor,iC,Yge,Jor,Yor,gO,Kor,Zor,err,dC,Kge,orr,rrr,hO,trr,arr,srr,cC,Zge,nrr,lrr,uO,irr,drr,crr,mC,ehe,mrr,frr,pO,grr,hrr,urr,fC,ohe,prr,_rr,_O,brr,vrr,Trr,gC,rhe,Frr,Crr,bO,Mrr,Err,yrr,hC,the,wrr,Arr,vO,Lrr,Brr,xrr,uC,ahe,krr,Rrr,TO,Srr,Prr,$rr,pC,she,Irr,jrr,FO,Nrr,Drr,qrr,_C,nhe,Orr,Grr,CO,Xrr,Vrr,zrr,bC,lhe,Wrr,Qrr,MO,Hrr,Urr,Jrr,vC,ihe,Yrr,Krr,EO,Zrr,etr,otr,TC,dhe,rtr,ttr,yO,atr,str,ntr,che,ltr,itr,UA,Qke,vc,FC,mhe,JA,dtr,fhe,ctr,Hke,Fr,YA,mtr,Tc,ftr,ghe,gtr,htr,hhe,utr,ptr,_tr,KA,btr,uhe,vtr,Ttr,Ftr,ut,ZA,Ctr,phe,Mtr,Etr,Fc,ytr,_he,wtr,Atr,bhe,Ltr,Btr,xtr,vhe,ktr,Rtr,e0,Str,po,o0,Ptr,The,$tr,Itr,_s,jtr,Fhe,Ntr,Dtr,Che,qtr,Otr,Mhe,Gtr,Xtr,Vtr,ge,CC,Ehe,ztr,Wtr,wO,Qtr,Htr,Utr,MC,yhe,Jtr,Ytr,AO,Ktr,Ztr,ear,EC,whe,oar,rar,LO,tar,aar,sar,yC,Ahe,nar,lar,BO,iar,dar,car,wC,Lhe,mar,far,xO,gar,har,uar,AC,Bhe,par,_ar,kO,bar,Tar,Far,LC,xhe,Car,Mar,RO,Ear,yar,war,BC,khe,Aar,Lar,SO,Bar,xar,kar,xC,Rhe,Rar,Sar,PO,Par,$ar,Iar,kC,She,jar,Nar,$O,Dar,qar,Oar,RC,Phe,Gar,Xar,IO,Var,zar,War,$he,Qar,Har,r0,Uke,Cc,SC,Ihe,t0,Uar,jhe,Jar,Jke,Cr,a0,Yar,Mc,Kar,Nhe,Zar,esr,Dhe,osr,rsr,tsr,s0,asr,qhe,ssr,nsr,lsr,pt,n0,isr,Ohe,dsr,csr,Ec,msr,Ghe,fsr,gsr,Xhe,hsr,usr,psr,Vhe,_sr,bsr,l0,vsr,_o,i0,Tsr,zhe,Fsr,Csr,bs,Msr,Whe,Esr,ysr,Qhe,wsr,Asr,Hhe,Lsr,Bsr,xsr,d0,PC,Uhe,ksr,Rsr,jO,Ssr,Psr,$sr,$C,Jhe,Isr,jsr,NO,Nsr,Dsr,qsr,Yhe,Osr,Gsr,c0,Yke,yc,IC,Khe,m0,Xsr,Zhe,Vsr,Kke,Mr,f0,zsr,wc,Wsr,eue,Qsr,Hsr,oue,Usr,Jsr,Ysr,g0,Ksr,rue,Zsr,enr,onr,_t,h0,rnr,tue,tnr,anr,Ac,snr,aue,nnr,lnr,sue,inr,dnr,cnr,nue,mnr,fnr,u0,gnr,bo,p0,hnr,lue,unr,pnr,vs,_nr,iue,bnr,vnr,due,Tnr,Fnr,cue,Cnr,Mnr,Enr,Y,jC,mue,ynr,wnr,DO,Anr,Lnr,Bnr,NC,fue,xnr,knr,qO,Rnr,Snr,Pnr,DC,gue,$nr,Inr,OO,jnr,Nnr,Dnr,qC,hue,qnr,Onr,GO,Gnr,Xnr,Vnr,OC,uue,znr,Wnr,XO,Qnr,Hnr,Unr,GC,pue,Jnr,Ynr,VO,Knr,Znr,elr,XC,_ue,olr,rlr,zO,tlr,alr,slr,VC,bue,nlr,llr,WO,ilr,dlr,clr,zC,vue,mlr,flr,QO,glr,hlr,ulr,WC,Tue,plr,_lr,HO,blr,vlr,Tlr,QC,Fue,Flr,Clr,UO,Mlr,Elr,ylr,HC,Cue,wlr,Alr,JO,Llr,Blr,xlr,UC,Mue,klr,Rlr,YO,Slr,Plr,$lr,JC,Eue,Ilr,jlr,KO,Nlr,Dlr,qlr,YC,yue,Olr,Glr,ZO,Xlr,Vlr,zlr,KC,wue,Wlr,Qlr,eG,Hlr,Ulr,Jlr,ZC,Aue,Ylr,Klr,oG,Zlr,eir,oir,eM,Lue,rir,tir,rG,air,sir,nir,oM,Bue,lir,iir,tG,dir,cir,mir,rM,xue,fir,gir,aG,hir,uir,pir,kue,_ir,bir,_0,Zke,Lc,tM,Rue,b0,vir,Sue,Tir,eRe,Er,v0,Fir,Bc,Cir,Pue,Mir,Eir,$ue,yir,wir,Air,T0,Lir,Iue,Bir,xir,kir,bt,F0,Rir,jue,Sir,Pir,xc,$ir,Nue,Iir,jir,Due,Nir,Dir,qir,que,Oir,Gir,C0,Xir,vo,M0,Vir,Oue,zir,Wir,Ts,Qir,Gue,Hir,Uir,Xue,Jir,Yir,Vue,Kir,Zir,edr,_e,aM,zue,odr,rdr,sG,tdr,adr,sdr,sM,Wue,ndr,ldr,nG,idr,ddr,cdr,nM,Que,mdr,fdr,lG,gdr,hdr,udr,lM,Hue,pdr,_dr,iG,bdr,vdr,Tdr,iM,Uue,Fdr,Cdr,dG,Mdr,Edr,ydr,dM,Jue,wdr,Adr,cG,Ldr,Bdr,xdr,cM,Yue,kdr,Rdr,mG,Sdr,Pdr,$dr,mM,Kue,Idr,jdr,fG,Ndr,Ddr,qdr,fM,Zue,Odr,Gdr,gG,Xdr,Vdr,zdr,gM,epe,Wdr,Qdr,hG,Hdr,Udr,Jdr,ope,Ydr,Kdr,E0,oRe,kc,hM,rpe,y0,Zdr,tpe,ecr,rRe,yr,w0,ocr,Rc,rcr,ape,tcr,acr,spe,scr,ncr,lcr,A0,icr,npe,dcr,ccr,mcr,vt,L0,fcr,lpe,gcr,hcr,Sc,ucr,ipe,pcr,_cr,dpe,bcr,vcr,Tcr,cpe,Fcr,Ccr,B0,Mcr,To,x0,Ecr,mpe,ycr,wcr,Fs,Acr,fpe,Lcr,Bcr,gpe,xcr,kcr,hpe,Rcr,Scr,Pcr,V,uM,upe,$cr,Icr,uG,jcr,Ncr,Dcr,pM,ppe,qcr,Ocr,pG,Gcr,Xcr,Vcr,_M,_pe,zcr,Wcr,_G,Qcr,Hcr,Ucr,bM,bpe,Jcr,Ycr,bG,Kcr,Zcr,emr,vM,vpe,omr,rmr,vG,tmr,amr,smr,TM,Tpe,nmr,lmr,TG,imr,dmr,cmr,FM,Fpe,mmr,fmr,FG,gmr,hmr,umr,CM,Cpe,pmr,_mr,CG,bmr,vmr,Tmr,MM,Mpe,Fmr,Cmr,MG,Mmr,Emr,ymr,EM,Epe,wmr,Amr,EG,Lmr,Bmr,xmr,yM,ype,kmr,Rmr,yG,Smr,Pmr,$mr,wM,wpe,Imr,jmr,wG,Nmr,Dmr,qmr,AM,Ape,Omr,Gmr,AG,Xmr,Vmr,zmr,LM,Lpe,Wmr,Qmr,LG,Hmr,Umr,Jmr,BM,Bpe,Ymr,Kmr,BG,Zmr,efr,ofr,xM,xpe,rfr,tfr,xG,afr,sfr,nfr,kM,kpe,lfr,ifr,kG,dfr,cfr,mfr,RM,Rpe,ffr,gfr,RG,hfr,ufr,pfr,SM,Spe,_fr,bfr,SG,vfr,Tfr,Ffr,PM,Ppe,Cfr,Mfr,PG,Efr,yfr,wfr,$M,$pe,Afr,Lfr,$G,Bfr,xfr,kfr,IM,Ipe,Rfr,Sfr,IG,Pfr,$fr,Ifr,jM,jpe,jfr,Nfr,jG,Dfr,qfr,Ofr,NM,Npe,Gfr,Xfr,NG,Vfr,zfr,Wfr,DM,Dpe,Qfr,Hfr,DG,Ufr,Jfr,Yfr,qpe,Kfr,Zfr,k0,tRe,Pc,qM,Ope,R0,egr,Gpe,ogr,aRe,wr,S0,rgr,$c,tgr,Xpe,agr,sgr,Vpe,ngr,lgr,igr,P0,dgr,zpe,cgr,mgr,fgr,Tt,$0,ggr,Wpe,hgr,ugr,Ic,pgr,Qpe,_gr,bgr,Hpe,vgr,Tgr,Fgr,Upe,Cgr,Mgr,I0,Egr,Fo,j0,ygr,Jpe,wgr,Agr,Cs,Lgr,Ype,Bgr,xgr,Kpe,kgr,Rgr,Zpe,Sgr,Pgr,$gr,ae,OM,e_e,Igr,jgr,qG,Ngr,Dgr,qgr,GM,o_e,Ogr,Ggr,OG,Xgr,Vgr,zgr,XM,r_e,Wgr,Qgr,GG,Hgr,Ugr,Jgr,VM,t_e,Ygr,Kgr,XG,Zgr,ehr,ohr,zM,a_e,rhr,thr,VG,ahr,shr,nhr,WM,s_e,lhr,ihr,zG,dhr,chr,mhr,QM,n_e,fhr,ghr,WG,hhr,uhr,phr,HM,l_e,_hr,bhr,QG,vhr,Thr,Fhr,UM,i_e,Chr,Mhr,HG,Ehr,yhr,whr,JM,d_e,Ahr,Lhr,UG,Bhr,xhr,khr,YM,c_e,Rhr,Shr,JG,Phr,$hr,Ihr,KM,m_e,jhr,Nhr,YG,Dhr,qhr,Ohr,ZM,f_e,Ghr,Xhr,KG,Vhr,zhr,Whr,e4,g_e,Qhr,Hhr,ZG,Uhr,Jhr,Yhr,o4,h_e,Khr,Zhr,eX,eur,our,rur,r4,u_e,tur,aur,oX,sur,nur,lur,t4,p_e,iur,dur,rX,cur,mur,fur,__e,gur,hur,N0,sRe,jc,a4,b_e,D0,uur,v_e,pur,nRe,Ar,q0,_ur,Nc,bur,T_e,vur,Tur,F_e,Fur,Cur,Mur,O0,Eur,C_e,yur,wur,Aur,Ft,G0,Lur,M_e,Bur,xur,Dc,kur,E_e,Rur,Sur,y_e,Pur,$ur,Iur,w_e,jur,Nur,X0,Dur,Co,V0,qur,A_e,Our,Gur,Ms,Xur,L_e,Vur,zur,B_e,Wur,Qur,x_e,Hur,Uur,Jur,k_e,s4,R_e,Yur,Kur,tX,Zur,epr,opr,S_e,rpr,tpr,z0,lRe,qc,n4,P_e,W0,apr,$_e,spr,iRe,Lr,Q0,npr,Oc,lpr,I_e,ipr,dpr,j_e,cpr,mpr,fpr,H0,gpr,N_e,hpr,upr,ppr,Ct,U0,_pr,D_e,bpr,vpr,Gc,Tpr,q_e,Fpr,Cpr,O_e,Mpr,Epr,ypr,G_e,wpr,Apr,J0,Lpr,Mo,Y0,Bpr,X_e,xpr,kpr,Es,Rpr,V_e,Spr,Ppr,z_e,$pr,Ipr,W_e,jpr,Npr,Dpr,K,l4,Q_e,qpr,Opr,aX,Gpr,Xpr,Vpr,i4,H_e,zpr,Wpr,sX,Qpr,Hpr,Upr,d4,U_e,Jpr,Ypr,nX,Kpr,Zpr,e_r,c4,J_e,o_r,r_r,lX,t_r,a_r,s_r,m4,Y_e,n_r,l_r,iX,i_r,d_r,c_r,f4,K_e,m_r,f_r,dX,g_r,h_r,u_r,g4,Z_e,p_r,__r,cX,b_r,v_r,T_r,h4,ebe,F_r,C_r,mX,M_r,E_r,y_r,u4,obe,w_r,A_r,fX,L_r,B_r,x_r,p4,rbe,k_r,R_r,gX,S_r,P_r,$_r,_4,tbe,I_r,j_r,hX,N_r,D_r,q_r,b4,abe,O_r,G_r,uX,X_r,V_r,z_r,v4,sbe,W_r,Q_r,pX,H_r,U_r,J_r,T4,nbe,Y_r,K_r,_X,Z_r,ebr,obr,F4,lbe,rbr,tbr,bX,abr,sbr,nbr,C4,ibe,lbr,ibr,vX,dbr,cbr,mbr,M4,dbe,fbr,gbr,TX,hbr,ubr,pbr,E4,cbe,_br,bbr,FX,vbr,Tbr,Fbr,y4,mbe,Cbr,Mbr,CX,Ebr,ybr,wbr,w4,fbe,Abr,Lbr,MX,Bbr,xbr,kbr,gbe,Rbr,Sbr,K0,dRe,Xc,A4,hbe,Z0,Pbr,ube,$br,cRe,Br,eL,Ibr,Vc,jbr,pbe,Nbr,Dbr,_be,qbr,Obr,Gbr,oL,Xbr,bbe,Vbr,zbr,Wbr,Mt,rL,Qbr,vbe,Hbr,Ubr,zc,Jbr,Tbe,Ybr,Kbr,Fbe,Zbr,e2r,o2r,Cbe,r2r,t2r,tL,a2r,Eo,aL,s2r,Mbe,n2r,l2r,ys,i2r,Ebe,d2r,c2r,ybe,m2r,f2r,wbe,g2r,h2r,u2r,Z,L4,Abe,p2r,_2r,EX,b2r,v2r,T2r,B4,Lbe,F2r,C2r,yX,M2r,E2r,y2r,x4,Bbe,w2r,A2r,wX,L2r,B2r,x2r,k4,xbe,k2r,R2r,AX,S2r,P2r,$2r,R4,kbe,I2r,j2r,LX,N2r,D2r,q2r,S4,Rbe,O2r,G2r,BX,X2r,V2r,z2r,P4,Sbe,W2r,Q2r,xX,H2r,U2r,J2r,$4,Pbe,Y2r,K2r,kX,Z2r,evr,ovr,I4,$be,rvr,tvr,RX,avr,svr,nvr,j4,Ibe,lvr,ivr,SX,dvr,cvr,mvr,N4,jbe,fvr,gvr,PX,hvr,uvr,pvr,D4,Nbe,_vr,bvr,$X,vvr,Tvr,Fvr,q4,Dbe,Cvr,Mvr,IX,Evr,yvr,wvr,O4,qbe,Avr,Lvr,jX,Bvr,xvr,kvr,G4,Obe,Rvr,Svr,NX,Pvr,$vr,Ivr,X4,Gbe,jvr,Nvr,DX,Dvr,qvr,Ovr,V4,Xbe,Gvr,Xvr,qX,Vvr,zvr,Wvr,z4,Vbe,Qvr,Hvr,OX,Uvr,Jvr,Yvr,W4,zbe,Kvr,Zvr,GX,eTr,oTr,rTr,Wbe,tTr,aTr,sL,mRe,Wc,Q4,Qbe,nL,sTr,Hbe,nTr,fRe,xr,lL,lTr,Qc,iTr,Ube,dTr,cTr,Jbe,mTr,fTr,gTr,iL,hTr,Ybe,uTr,pTr,_Tr,Et,dL,bTr,Kbe,vTr,TTr,Hc,FTr,Zbe,CTr,MTr,e2e,ETr,yTr,wTr,o2e,ATr,LTr,cL,BTr,yo,mL,xTr,r2e,kTr,RTr,ws,STr,t2e,PTr,$Tr,a2e,ITr,jTr,s2e,NTr,DTr,qTr,n2e,H4,l2e,OTr,GTr,XX,XTr,VTr,zTr,i2e,WTr,QTr,fL,gRe,Uc,U4,d2e,gL,HTr,c2e,UTr,hRe,kr,hL,JTr,Jc,YTr,m2e,KTr,ZTr,f2e,e1r,o1r,r1r,uL,t1r,g2e,a1r,s1r,n1r,yt,pL,l1r,h2e,i1r,d1r,Yc,c1r,u2e,m1r,f1r,p2e,g1r,h1r,u1r,_2e,p1r,_1r,_L,b1r,wo,bL,v1r,b2e,T1r,F1r,As,C1r,v2e,M1r,E1r,T2e,y1r,w1r,F2e,A1r,L1r,B1r,C2e,J4,M2e,x1r,k1r,VX,R1r,S1r,P1r,E2e,$1r,I1r,vL,uRe,Kc,Y4,y2e,TL,j1r,w2e,N1r,pRe,Rr,FL,D1r,Zc,q1r,A2e,O1r,G1r,L2e,X1r,V1r,z1r,CL,W1r,B2e,Q1r,H1r,U1r,wt,ML,J1r,x2e,Y1r,K1r,em,Z1r,k2e,eFr,oFr,R2e,rFr,tFr,aFr,S2e,sFr,nFr,EL,lFr,Ao,yL,iFr,P2e,dFr,cFr,Ls,mFr,$2e,fFr,gFr,I2e,hFr,uFr,j2e,pFr,_Fr,bFr,z,K4,N2e,vFr,TFr,zX,FFr,CFr,MFr,Z4,D2e,EFr,yFr,WX,wFr,AFr,LFr,eE,q2e,BFr,xFr,QX,kFr,RFr,SFr,oE,O2e,PFr,$Fr,HX,IFr,jFr,NFr,rE,G2e,DFr,qFr,UX,OFr,GFr,XFr,tE,X2e,VFr,zFr,JX,WFr,QFr,HFr,aE,V2e,UFr,JFr,YX,YFr,KFr,ZFr,sE,z2e,eCr,oCr,KX,rCr,tCr,aCr,nE,W2e,sCr,nCr,ZX,lCr,iCr,dCr,lE,Q2e,cCr,mCr,eV,fCr,gCr,hCr,iE,H2e,uCr,pCr,oV,_Cr,bCr,vCr,dE,U2e,TCr,FCr,rV,CCr,MCr,ECr,cE,J2e,yCr,wCr,tV,ACr,LCr,BCr,mE,Y2e,xCr,kCr,aV,RCr,SCr,PCr,fE,K2e,$Cr,ICr,sV,jCr,NCr,DCr,gE,Z2e,qCr,OCr,nV,GCr,XCr,VCr,hE,eve,zCr,WCr,lV,QCr,HCr,UCr,uE,ove,JCr,YCr,iV,KCr,ZCr,eMr,pE,rve,oMr,rMr,dV,tMr,aMr,sMr,_E,tve,nMr,lMr,cV,iMr,dMr,cMr,bE,ave,mMr,fMr,mV,gMr,hMr,uMr,vE,sve,pMr,_Mr,fV,bMr,vMr,TMr,TE,nve,FMr,CMr,gV,MMr,EMr,yMr,FE,lve,wMr,AMr,hV,LMr,BMr,xMr,CE,ive,kMr,RMr,uV,SMr,PMr,$Mr,dve,IMr,jMr,wL,_Re,om,ME,cve,AL,NMr,mve,DMr,bRe,Sr,LL,qMr,rm,OMr,fve,GMr,XMr,gve,VMr,zMr,WMr,BL,QMr,hve,HMr,UMr,JMr,At,xL,YMr,uve,KMr,ZMr,tm,e4r,pve,o4r,r4r,_ve,t4r,a4r,s4r,bve,n4r,l4r,kL,i4r,Lo,RL,d4r,vve,c4r,m4r,Bs,f4r,Tve,g4r,h4r,Fve,u4r,p4r,Cve,_4r,b4r,v4r,ca,EE,Mve,T4r,F4r,pV,C4r,M4r,E4r,yE,Eve,y4r,w4r,_V,A4r,L4r,B4r,wE,yve,x4r,k4r,bV,R4r,S4r,P4r,AE,wve,$4r,I4r,vV,j4r,N4r,D4r,LE,Ave,q4r,O4r,TV,G4r,X4r,V4r,Lve,z4r,W4r,SL,vRe,am,BE,Bve,PL,Q4r,xve,H4r,TRe,Pr,$L,U4r,sm,J4r,kve,Y4r,K4r,Rve,Z4r,eEr,oEr,IL,rEr,Sve,tEr,aEr,sEr,Lt,jL,nEr,Pve,lEr,iEr,nm,dEr,$ve,cEr,mEr,Ive,fEr,gEr,hEr,jve,uEr,pEr,NL,_Er,Bo,DL,bEr,Nve,vEr,TEr,xs,FEr,Dve,CEr,MEr,qve,EEr,yEr,Ove,wEr,AEr,LEr,ce,xE,Gve,BEr,xEr,FV,kEr,REr,SEr,kE,Xve,PEr,$Er,CV,IEr,jEr,NEr,RE,Vve,DEr,qEr,MV,OEr,GEr,XEr,SE,zve,VEr,zEr,EV,WEr,QEr,HEr,PE,Wve,UEr,JEr,yV,YEr,KEr,ZEr,$E,Qve,e3r,o3r,wV,r3r,t3r,a3r,IE,Hve,s3r,n3r,AV,l3r,i3r,d3r,jE,Uve,c3r,m3r,LV,f3r,g3r,h3r,NE,Jve,u3r,p3r,BV,_3r,b3r,v3r,DE,Yve,T3r,F3r,xV,C3r,M3r,E3r,qE,Kve,y3r,w3r,kV,A3r,L3r,B3r,OE,Zve,x3r,k3r,RV,R3r,S3r,P3r,eTe,$3r,I3r,qL,FRe,lm,GE,oTe,OL,j3r,rTe,N3r,CRe,$r,GL,D3r,im,q3r,tTe,O3r,G3r,aTe,X3r,V3r,z3r,XL,W3r,sTe,Q3r,H3r,U3r,Bt,VL,J3r,nTe,Y3r,K3r,dm,Z3r,lTe,e5r,o5r,iTe,r5r,t5r,a5r,dTe,s5r,n5r,zL,l5r,xo,WL,i5r,cTe,d5r,c5r,ks,m5r,mTe,f5r,g5r,fTe,h5r,u5r,gTe,p5r,_5r,b5r,be,XE,hTe,v5r,T5r,SV,F5r,C5r,M5r,VE,uTe,E5r,y5r,PV,w5r,A5r,L5r,zE,pTe,B5r,x5r,$V,k5r,R5r,S5r,WE,_Te,P5r,$5r,IV,I5r,j5r,N5r,QE,bTe,D5r,q5r,jV,O5r,G5r,X5r,HE,vTe,V5r,z5r,NV,W5r,Q5r,H5r,UE,TTe,U5r,J5r,DV,Y5r,K5r,Z5r,JE,FTe,eyr,oyr,qV,ryr,tyr,ayr,YE,CTe,syr,nyr,OV,lyr,iyr,dyr,KE,MTe,cyr,myr,GV,fyr,gyr,hyr,ETe,uyr,pyr,QL,MRe,cm,ZE,yTe,HL,_yr,wTe,byr,ERe,Ir,UL,vyr,mm,Tyr,ATe,Fyr,Cyr,LTe,Myr,Eyr,yyr,JL,wyr,BTe,Ayr,Lyr,Byr,xt,YL,xyr,xTe,kyr,Ryr,fm,Syr,kTe,Pyr,$yr,RTe,Iyr,jyr,Nyr,STe,Dyr,qyr,KL,Oyr,ko,ZL,Gyr,PTe,Xyr,Vyr,Rs,zyr,$Te,Wyr,Qyr,ITe,Hyr,Uyr,jTe,Jyr,Yyr,Kyr,Ee,e3,NTe,Zyr,ewr,XV,owr,rwr,twr,o3,DTe,awr,swr,VV,nwr,lwr,iwr,r3,qTe,dwr,cwr,zV,mwr,fwr,gwr,t3,OTe,hwr,uwr,WV,pwr,_wr,bwr,a3,GTe,vwr,Twr,QV,Fwr,Cwr,Mwr,s3,XTe,Ewr,ywr,HV,wwr,Awr,Lwr,n3,VTe,Bwr,xwr,UV,kwr,Rwr,Swr,l3,zTe,Pwr,$wr,JV,Iwr,jwr,Nwr,i3,WTe,Dwr,qwr,YV,Owr,Gwr,Xwr,QTe,Vwr,zwr,e7,yRe,gm,d3,HTe,o7,Wwr,UTe,Qwr,wRe,jr,r7,Hwr,hm,Uwr,JTe,Jwr,Ywr,YTe,Kwr,Zwr,e6r,t7,o6r,KTe,r6r,t6r,a6r,kt,a7,s6r,ZTe,n6r,l6r,um,i6r,e1e,d6r,c6r,o1e,m6r,f6r,g6r,r1e,h6r,u6r,s7,p6r,Ro,n7,_6r,t1e,b6r,v6r,Ss,T6r,a1e,F6r,C6r,s1e,M6r,E6r,n1e,y6r,w6r,A6r,ve,c3,l1e,L6r,B6r,KV,x6r,k6r,R6r,m3,i1e,S6r,P6r,ZV,$6r,I6r,j6r,f3,d1e,N6r,D6r,ez,q6r,O6r,G6r,g3,c1e,X6r,V6r,oz,z6r,W6r,Q6r,h3,m1e,H6r,U6r,rz,J6r,Y6r,K6r,u3,f1e,Z6r,eAr,tz,oAr,rAr,tAr,p3,g1e,aAr,sAr,az,nAr,lAr,iAr,_3,h1e,dAr,cAr,sz,mAr,fAr,gAr,b3,u1e,hAr,uAr,nz,pAr,_Ar,bAr,v3,p1e,vAr,TAr,lz,FAr,CAr,MAr,_1e,EAr,yAr,l7,ARe,pm,T3,b1e,i7,wAr,v1e,AAr,LRe,Nr,d7,LAr,_m,BAr,T1e,xAr,kAr,F1e,RAr,SAr,PAr,c7,$Ar,C1e,IAr,jAr,NAr,Rt,m7,DAr,M1e,qAr,OAr,bm,GAr,E1e,XAr,VAr,y1e,zAr,WAr,QAr,w1e,HAr,UAr,f7,JAr,So,g7,YAr,A1e,KAr,ZAr,Ps,e0r,L1e,o0r,r0r,B1e,t0r,a0r,x1e,s0r,n0r,l0r,Te,F3,k1e,i0r,d0r,iz,c0r,m0r,f0r,C3,R1e,g0r,h0r,dz,u0r,p0r,_0r,M3,S1e,b0r,v0r,cz,T0r,F0r,C0r,E3,P1e,M0r,E0r,mz,y0r,w0r,A0r,y3,$1e,L0r,B0r,fz,x0r,k0r,R0r,w3,I1e,S0r,P0r,gz,$0r,I0r,j0r,A3,j1e,N0r,D0r,hz,q0r,O0r,G0r,L3,N1e,X0r,V0r,uz,z0r,W0r,Q0r,B3,D1e,H0r,U0r,pz,J0r,Y0r,K0r,x3,q1e,Z0r,eLr,_z,oLr,rLr,tLr,O1e,aLr,sLr,h7,BRe,vm,k3,G1e,u7,nLr,X1e,lLr,xRe,Dr,p7,iLr,Tm,dLr,V1e,cLr,mLr,z1e,fLr,gLr,hLr,_7,uLr,W1e,pLr,_Lr,bLr,St,b7,vLr,Q1e,TLr,FLr,Fm,CLr,H1e,MLr,ELr,U1e,yLr,wLr,ALr,J1e,LLr,BLr,v7,xLr,Po,T7,kLr,Y1e,RLr,SLr,$s,PLr,K1e,$Lr,ILr,Z1e,jLr,NLr,eFe,DLr,qLr,OLr,Se,R3,oFe,GLr,XLr,bz,VLr,zLr,WLr,S3,rFe,QLr,HLr,vz,ULr,JLr,YLr,P3,tFe,KLr,ZLr,Tz,e7r,o7r,r7r,$3,aFe,t7r,a7r,Fz,s7r,n7r,l7r,I3,sFe,i7r,d7r,Cz,c7r,m7r,f7r,j3,nFe,g7r,h7r,Mz,u7r,p7r,_7r,N3,lFe,b7r,v7r,Ez,T7r,F7r,C7r,D3,iFe,M7r,E7r,yz,y7r,w7r,A7r,dFe,L7r,B7r,F7,kRe,Cm,q3,cFe,C7,x7r,mFe,k7r,RRe,qr,M7,R7r,Mm,S7r,fFe,P7r,$7r,gFe,I7r,j7r,N7r,E7,D7r,hFe,q7r,O7r,G7r,Pt,y7,X7r,uFe,V7r,z7r,Em,W7r,pFe,Q7r,H7r,_Fe,U7r,J7r,Y7r,bFe,K7r,Z7r,w7,e8r,$o,A7,o8r,vFe,r8r,t8r,Is,a8r,TFe,s8r,n8r,FFe,l8r,i8r,CFe,d8r,c8r,m8r,Pe,O3,MFe,f8r,g8r,wz,h8r,u8r,p8r,G3,EFe,_8r,b8r,Az,v8r,T8r,F8r,X3,yFe,C8r,M8r,Lz,E8r,y8r,w8r,V3,wFe,A8r,L8r,Bz,B8r,x8r,k8r,z3,AFe,R8r,S8r,xz,P8r,$8r,I8r,W3,LFe,j8r,N8r,kz,D8r,q8r,O8r,Q3,BFe,G8r,X8r,Rz,V8r,z8r,W8r,H3,xFe,Q8r,H8r,Sz,U8r,J8r,Y8r,kFe,K8r,Z8r,L7,SRe,ym,U3,RFe,B7,e9r,SFe,o9r,PRe,Or,x7,r9r,wm,t9r,PFe,a9r,s9r,$Fe,n9r,l9r,i9r,k7,d9r,IFe,c9r,m9r,f9r,$t,R7,g9r,jFe,h9r,u9r,Am,p9r,NFe,_9r,b9r,DFe,v9r,T9r,F9r,qFe,C9r,M9r,S7,E9r,Io,P7,y9r,OFe,w9r,A9r,js,L9r,GFe,B9r,x9r,XFe,k9r,R9r,VFe,S9r,P9r,$9r,zFe,J3,WFe,I9r,j9r,Pz,N9r,D9r,q9r,QFe,O9r,G9r,$7,$Re,Lm,Y3,HFe,I7,X9r,UFe,V9r,IRe,Gr,j7,z9r,Bm,W9r,JFe,Q9r,H9r,YFe,U9r,J9r,Y9r,N7,K9r,KFe,Z9r,eBr,oBr,It,D7,rBr,ZFe,tBr,aBr,xm,sBr,eCe,nBr,lBr,oCe,iBr,dBr,cBr,rCe,mBr,fBr,q7,gBr,jo,O7,hBr,tCe,uBr,pBr,Ns,_Br,aCe,bBr,vBr,sCe,TBr,FBr,nCe,CBr,MBr,EBr,G7,K3,lCe,yBr,wBr,$z,ABr,LBr,BBr,Z3,iCe,xBr,kBr,Iz,RBr,SBr,PBr,dCe,$Br,IBr,X7,jRe,km,e5,cCe,V7,jBr,mCe,NBr,NRe,Xr,z7,DBr,Rm,qBr,fCe,OBr,GBr,gCe,XBr,VBr,zBr,W7,WBr,hCe,QBr,HBr,UBr,jt,Q7,JBr,uCe,YBr,KBr,Sm,ZBr,pCe,exr,oxr,_Ce,rxr,txr,axr,bCe,sxr,nxr,H7,lxr,No,U7,ixr,vCe,dxr,cxr,Ds,mxr,TCe,fxr,gxr,FCe,hxr,uxr,CCe,pxr,_xr,bxr,MCe,o5,ECe,vxr,Txr,jz,Fxr,Cxr,Mxr,yCe,Exr,yxr,J7,DRe;return me=new X({}),qa=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased")',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),sy=new X({}),ny=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),qm=new wxr({props:{warning:"&lcub;true}",$$slots:{default:[v4t]},$$scope:{ctx:Ii}}}),ly=new X({}),iy=new M({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L533"}}),my=new M({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L556",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),fy=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),gy=new M({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L678",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),hy=new X({}),uy=new M({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L352"}}),by=new M({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L366",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),vy=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),Ty=new M({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L562",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),Fy=new X({}),Cy=new M({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L172"}}),yy=new M({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L186",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Lh=new wxr({props:{$$slots:{default:[T4t]},$$scope:{ctx:Ii}}}),wy=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),Ay=new M({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L313",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),Ly=new X({}),By=new M({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L71"}}),Ry=new M({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Dh=new wxr({props:{$$slots:{default:[F4t]},$$scope:{ctx:Ii}}}),Sy=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),Py=new M({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),$y=new X({}),Iy=new M({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L701"}}),Ny=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/resnet#transformers.ResNetModel">ResNetModel</a> (ResNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/van#transformers.VanConfig">VanConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/van#transformers.VanModel">VanModel</a> (VAN model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Dy=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),qy=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Gy=new X({}),Xy=new M({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L708"}}),zy=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Wy=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),Qy=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Hy=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Uy=new X({}),Jy=new M({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L723"}}),Ky=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Zy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),ew=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ow=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rw=new X({}),tw=new M({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L730"}}),sw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),nw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),lw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dw=new X({}),cw=new M({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L737"}}),fw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),gw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),hw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),pw=new X({}),_w=new M({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L746"}}),vw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Tw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),Fw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Mw=new X({}),Ew=new M({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L780"}}),ww=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Aw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),Lw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Bw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xw=new X({}),kw=new M({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L787"}}),Sw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),Pw=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),$w=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jw=new X({}),Nw=new M({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L773"}}),qw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Ow=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),Gw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Xw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Vw=new X({}),zw=new M({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L755"}}),Qw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Hw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),Uw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Jw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Yw=new X({}),Kw=new M({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L762"}}),e6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),o6=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),r6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),t6=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),a6=new X({}),s6=new M({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L796"}}),l6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/resnet#transformers.ResNetForImageClassification">ResNetForImageClassification</a> (ResNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/van#transformers.VanConfig">VanConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> (VAN model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),i6=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),d6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),c6=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m6=new X({}),f6=new M({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L835"}}),h6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),u6=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),p6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_6=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),b6=new X({}),v6=new M({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L842"}}),F6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),C6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),M6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),E6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),y6=new X({}),w6=new M({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L865"}}),L6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),B6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),x6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),k6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),R6=new X({}),S6=new M({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L849"}}),$6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),I6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),j6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),N6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),D6=new X({}),q6=new M({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L856"}}),G6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),X6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),V6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),W6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Q6=new X({}),H6=new M({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L874"}}),J6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Y6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),K6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Z6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),eA=new X({}),oA=new M({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L881"}}),tA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),aA=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),sA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nA=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lA=new X({}),iA=new M({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L828"}}),cA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),mA=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),fA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gA=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hA=new X({}),uA=new M({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L803"}}),_A=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),vA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),TA=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),FA=new X({}),CA=new M({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L810"}}),EA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),wA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),LA=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),BA=new X({}),xA=new M({props:{name:"class transformers.AutoModelForInstanceSegmentation",anchor:"transformers.AutoModelForInstanceSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L819"}}),RA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>
</ul>`,name:"config"}]}}),SA=new w({props:{code:`from transformers import AutoConfig, AutoModelForInstanceSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForInstanceSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_config(config)`}}),PA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$A=new w({props:{code:`from transformers import AutoConfig, AutoModelForInstanceSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForInstanceSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForInstanceSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForInstanceSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),IA=new X({}),jA=new M({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L374"}}),DA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),qA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),OA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),GA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),XA=new X({}),VA=new M({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L381"}}),WA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),QA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),HA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),UA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),JA=new X({}),YA=new M({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L396"}}),ZA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForCausalLM">TFCamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),e0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),o0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),r0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),t0=new X({}),a0=new M({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L403"}}),n0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),l0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),i0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),c0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m0=new X({}),f0=new M({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L417"}}),h0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),u0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),p0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),b0=new X({}),v0=new M({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L424"}}),F0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),C0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),M0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),E0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),y0=new X({}),w0=new M({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L433"}}),L0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),B0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),x0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),k0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),R0=new X({}),S0=new M({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L469"}}),$0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),I0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),j0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),N0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),D0=new X({}),q0=new M({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L449"}}),G0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),X0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),V0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),z0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),W0=new X({}),Q0=new M({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L460"}}),U0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),J0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Y0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),K0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Z0=new X({}),eL=new M({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L442"}}),rL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),tL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),aL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nL=new X({}),lL=new M({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L410"}}),dL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),cL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),mL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gL=new X({}),hL=new M({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L485"}}),pL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),_L=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),bL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),TL=new X({}),FL=new M({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L237"}}),ML=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaModel">FlaxXLMRobertaModel</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),EL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),yL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),AL=new X({}),LL=new M({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L251"}}),xL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForCausalLM">FlaxBartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),kL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),RL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),SL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),PL=new X({}),$L=new M({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L244"}}),jL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),NL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),DL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),OL=new X({}),GL=new M({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L258"}}),VL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),zL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),WL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),QL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),HL=new X({}),UL=new M({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L265"}}),YL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),KL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),ZL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),e7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),o7=new X({}),r7=new M({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L274"}}),a7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForSequenceClassification">FlaxXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),s7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),n7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),l7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),i7=new X({}),d7=new M({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L283"}}),m7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForQuestionAnswering">FlaxXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),f7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),g7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),u7=new X({}),p7=new M({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L290"}}),b7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForTokenClassification">FlaxXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),v7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),T7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C7=new X({}),M7=new M({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L299"}}),y7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMultipleChoice">FlaxXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),w7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),A7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B7=new X({}),x7=new M({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L306"}}),R7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),S7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),P7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I7=new X({}),j7=new M({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L315"}}),D7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),q7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),O7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),V7=new X({}),z7=new M({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L324"}}),Q7=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),H7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),U7=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),$e=l(),de=a("h1"),ue=a("a"),io=a("span"),m(me.$$.fragment),Me=l(),Vo=a("span"),ji=o("Auto Classes"),$m=l(),ma=a("p"),Ni=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Di=a("code"),oy=o("from_pretrained()"),Im=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Be=l(),co=a("p"),qi=o("Instantiating one of "),qs=a("a"),ry=o("AutoConfig"),Os=o(", "),Gs=a("a"),ty=o("AutoModel"),Oi=o(`, and
`),Xs=a("a"),ay=o("AutoTokenizer"),Gi=o(" will directly create a class of the relevant architecture. For instance"),jm=l(),m(qa.$$.fragment),mo=l(),pe=a("p"),H8=o("will create a model that is an instance of "),Xi=a("a"),U8=o("BertModel"),J8=o("."),zo=l(),Oa=a("p"),Y8=o("There is one class of "),Nm=a("code"),K8=o("AutoModel"),KPe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),Nxe=l(),Vi=a("h2"),Dm=a("a"),xQ=a("span"),m(sy.$$.fragment),ZPe=l(),kQ=a("span"),e$e=o("Extending the Auto Classes"),Dxe=l(),Vs=a("p"),o$e=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),RQ=a("code"),r$e=o("NewModel"),t$e=o(", make sure you have a "),SQ=a("code"),a$e=o("NewModelConfig"),s$e=o(` then you can add those to the auto
classes like this:`),qxe=l(),m(ny.$$.fragment),Oxe=l(),Z8=a("p"),n$e=o("You will then be able to use the auto classes like you would usually do!"),Gxe=l(),m(qm.$$.fragment),Xxe=l(),zi=a("h2"),Om=a("a"),PQ=a("span"),m(ly.$$.fragment),l$e=l(),$Q=a("span"),i$e=o("AutoConfig"),Vxe=l(),Wo=a("div"),m(iy.$$.fragment),d$e=l(),dy=a("p"),c$e=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),e9=a("a"),m$e=o("from_pretrained()"),f$e=o(" class method."),g$e=l(),cy=a("p"),h$e=o("This class cannot be instantiated directly using "),IQ=a("code"),u$e=o("__init__()"),p$e=o(" (throws an error)."),_$e=l(),fo=a("div"),m(my.$$.fragment),b$e=l(),jQ=a("p"),v$e=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),T$e=l(),Wi=a("p"),F$e=o("The configuration class to instantiate is selected based on the "),NQ=a("code"),C$e=o("model_type"),M$e=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),DQ=a("code"),E$e=o("pretrained_model_name_or_path"),y$e=o(":"),w$e=l(),v=a("ul"),Gm=a("li"),qQ=a("strong"),A$e=o("albert"),L$e=o(" \u2014 "),o9=a("a"),B$e=o("AlbertConfig"),x$e=o(" (ALBERT model)"),k$e=l(),Xm=a("li"),OQ=a("strong"),R$e=o("bart"),S$e=o(" \u2014 "),r9=a("a"),P$e=o("BartConfig"),$$e=o(" (BART model)"),I$e=l(),Vm=a("li"),GQ=a("strong"),j$e=o("beit"),N$e=o(" \u2014 "),t9=a("a"),D$e=o("BeitConfig"),q$e=o(" (BEiT model)"),O$e=l(),zm=a("li"),XQ=a("strong"),G$e=o("bert"),X$e=o(" \u2014 "),a9=a("a"),V$e=o("BertConfig"),z$e=o(" (BERT model)"),W$e=l(),Wm=a("li"),VQ=a("strong"),Q$e=o("bert-generation"),H$e=o(" \u2014 "),s9=a("a"),U$e=o("BertGenerationConfig"),J$e=o(" (Bert Generation model)"),Y$e=l(),Qm=a("li"),zQ=a("strong"),K$e=o("big_bird"),Z$e=o(" \u2014 "),n9=a("a"),eIe=o("BigBirdConfig"),oIe=o(" (BigBird model)"),rIe=l(),Hm=a("li"),WQ=a("strong"),tIe=o("bigbird_pegasus"),aIe=o(" \u2014 "),l9=a("a"),sIe=o("BigBirdPegasusConfig"),nIe=o(" (BigBirdPegasus model)"),lIe=l(),Um=a("li"),QQ=a("strong"),iIe=o("blenderbot"),dIe=o(" \u2014 "),i9=a("a"),cIe=o("BlenderbotConfig"),mIe=o(" (Blenderbot model)"),fIe=l(),Jm=a("li"),HQ=a("strong"),gIe=o("blenderbot-small"),hIe=o(" \u2014 "),d9=a("a"),uIe=o("BlenderbotSmallConfig"),pIe=o(" (BlenderbotSmall model)"),_Ie=l(),Ym=a("li"),UQ=a("strong"),bIe=o("camembert"),vIe=o(" \u2014 "),c9=a("a"),TIe=o("CamembertConfig"),FIe=o(" (CamemBERT model)"),CIe=l(),Km=a("li"),JQ=a("strong"),MIe=o("canine"),EIe=o(" \u2014 "),m9=a("a"),yIe=o("CanineConfig"),wIe=o(" (Canine model)"),AIe=l(),Zm=a("li"),YQ=a("strong"),LIe=o("clip"),BIe=o(" \u2014 "),f9=a("a"),xIe=o("CLIPConfig"),kIe=o(" (CLIP model)"),RIe=l(),ef=a("li"),KQ=a("strong"),SIe=o("convbert"),PIe=o(" \u2014 "),g9=a("a"),$Ie=o("ConvBertConfig"),IIe=o(" (ConvBERT model)"),jIe=l(),of=a("li"),ZQ=a("strong"),NIe=o("convnext"),DIe=o(" \u2014 "),h9=a("a"),qIe=o("ConvNextConfig"),OIe=o(" (ConvNext model)"),GIe=l(),rf=a("li"),eH=a("strong"),XIe=o("ctrl"),VIe=o(" \u2014 "),u9=a("a"),zIe=o("CTRLConfig"),WIe=o(" (CTRL model)"),QIe=l(),tf=a("li"),oH=a("strong"),HIe=o("data2vec-audio"),UIe=o(" \u2014 "),p9=a("a"),JIe=o("Data2VecAudioConfig"),YIe=o(" (Data2VecAudio model)"),KIe=l(),af=a("li"),rH=a("strong"),ZIe=o("data2vec-text"),eje=o(" \u2014 "),_9=a("a"),oje=o("Data2VecTextConfig"),rje=o(" (Data2VecText model)"),tje=l(),sf=a("li"),tH=a("strong"),aje=o("deberta"),sje=o(" \u2014 "),b9=a("a"),nje=o("DebertaConfig"),lje=o(" (DeBERTa model)"),ije=l(),nf=a("li"),aH=a("strong"),dje=o("deberta-v2"),cje=o(" \u2014 "),v9=a("a"),mje=o("DebertaV2Config"),fje=o(" (DeBERTa-v2 model)"),gje=l(),lf=a("li"),sH=a("strong"),hje=o("deit"),uje=o(" \u2014 "),T9=a("a"),pje=o("DeiTConfig"),_je=o(" (DeiT model)"),bje=l(),df=a("li"),nH=a("strong"),vje=o("detr"),Tje=o(" \u2014 "),F9=a("a"),Fje=o("DetrConfig"),Cje=o(" (DETR model)"),Mje=l(),cf=a("li"),lH=a("strong"),Eje=o("distilbert"),yje=o(" \u2014 "),C9=a("a"),wje=o("DistilBertConfig"),Aje=o(" (DistilBERT model)"),Lje=l(),mf=a("li"),iH=a("strong"),Bje=o("dpr"),xje=o(" \u2014 "),M9=a("a"),kje=o("DPRConfig"),Rje=o(" (DPR model)"),Sje=l(),ff=a("li"),dH=a("strong"),Pje=o("electra"),$je=o(" \u2014 "),E9=a("a"),Ije=o("ElectraConfig"),jje=o(" (ELECTRA model)"),Nje=l(),gf=a("li"),cH=a("strong"),Dje=o("encoder-decoder"),qje=o(" \u2014 "),y9=a("a"),Oje=o("EncoderDecoderConfig"),Gje=o(" (Encoder decoder model)"),Xje=l(),hf=a("li"),mH=a("strong"),Vje=o("flaubert"),zje=o(" \u2014 "),w9=a("a"),Wje=o("FlaubertConfig"),Qje=o(" (FlauBERT model)"),Hje=l(),uf=a("li"),fH=a("strong"),Uje=o("fnet"),Jje=o(" \u2014 "),A9=a("a"),Yje=o("FNetConfig"),Kje=o(" (FNet model)"),Zje=l(),pf=a("li"),gH=a("strong"),eNe=o("fsmt"),oNe=o(" \u2014 "),L9=a("a"),rNe=o("FSMTConfig"),tNe=o(" (FairSeq Machine-Translation model)"),aNe=l(),_f=a("li"),hH=a("strong"),sNe=o("funnel"),nNe=o(" \u2014 "),B9=a("a"),lNe=o("FunnelConfig"),iNe=o(" (Funnel Transformer model)"),dNe=l(),bf=a("li"),uH=a("strong"),cNe=o("gpt2"),mNe=o(" \u2014 "),x9=a("a"),fNe=o("GPT2Config"),gNe=o(" (OpenAI GPT-2 model)"),hNe=l(),vf=a("li"),pH=a("strong"),uNe=o("gpt_neo"),pNe=o(" \u2014 "),k9=a("a"),_Ne=o("GPTNeoConfig"),bNe=o(" (GPT Neo model)"),vNe=l(),Tf=a("li"),_H=a("strong"),TNe=o("gptj"),FNe=o(" \u2014 "),R9=a("a"),CNe=o("GPTJConfig"),MNe=o(" (GPT-J model)"),ENe=l(),Ff=a("li"),bH=a("strong"),yNe=o("hubert"),wNe=o(" \u2014 "),S9=a("a"),ANe=o("HubertConfig"),LNe=o(" (Hubert model)"),BNe=l(),Cf=a("li"),vH=a("strong"),xNe=o("ibert"),kNe=o(" \u2014 "),P9=a("a"),RNe=o("IBertConfig"),SNe=o(" (I-BERT model)"),PNe=l(),Mf=a("li"),TH=a("strong"),$Ne=o("imagegpt"),INe=o(" \u2014 "),$9=a("a"),jNe=o("ImageGPTConfig"),NNe=o(" (ImageGPT model)"),DNe=l(),Ef=a("li"),FH=a("strong"),qNe=o("layoutlm"),ONe=o(" \u2014 "),I9=a("a"),GNe=o("LayoutLMConfig"),XNe=o(" (LayoutLM model)"),VNe=l(),yf=a("li"),CH=a("strong"),zNe=o("layoutlmv2"),WNe=o(" \u2014 "),j9=a("a"),QNe=o("LayoutLMv2Config"),HNe=o(" (LayoutLMv2 model)"),UNe=l(),wf=a("li"),MH=a("strong"),JNe=o("led"),YNe=o(" \u2014 "),N9=a("a"),KNe=o("LEDConfig"),ZNe=o(" (LED model)"),eDe=l(),Af=a("li"),EH=a("strong"),oDe=o("longformer"),rDe=o(" \u2014 "),D9=a("a"),tDe=o("LongformerConfig"),aDe=o(" (Longformer model)"),sDe=l(),Lf=a("li"),yH=a("strong"),nDe=o("luke"),lDe=o(" \u2014 "),q9=a("a"),iDe=o("LukeConfig"),dDe=o(" (LUKE model)"),cDe=l(),Bf=a("li"),wH=a("strong"),mDe=o("lxmert"),fDe=o(" \u2014 "),O9=a("a"),gDe=o("LxmertConfig"),hDe=o(" (LXMERT model)"),uDe=l(),xf=a("li"),AH=a("strong"),pDe=o("m2m_100"),_De=o(" \u2014 "),G9=a("a"),bDe=o("M2M100Config"),vDe=o(" (M2M100 model)"),TDe=l(),kf=a("li"),LH=a("strong"),FDe=o("marian"),CDe=o(" \u2014 "),X9=a("a"),MDe=o("MarianConfig"),EDe=o(" (Marian model)"),yDe=l(),Rf=a("li"),BH=a("strong"),wDe=o("maskformer"),ADe=o(" \u2014 "),V9=a("a"),LDe=o("MaskFormerConfig"),BDe=o(" (MaskFormer model)"),xDe=l(),Sf=a("li"),xH=a("strong"),kDe=o("mbart"),RDe=o(" \u2014 "),z9=a("a"),SDe=o("MBartConfig"),PDe=o(" (mBART model)"),$De=l(),Pf=a("li"),kH=a("strong"),IDe=o("megatron-bert"),jDe=o(" \u2014 "),W9=a("a"),NDe=o("MegatronBertConfig"),DDe=o(" (MegatronBert model)"),qDe=l(),$f=a("li"),RH=a("strong"),ODe=o("mobilebert"),GDe=o(" \u2014 "),Q9=a("a"),XDe=o("MobileBertConfig"),VDe=o(" (MobileBERT model)"),zDe=l(),If=a("li"),SH=a("strong"),WDe=o("mpnet"),QDe=o(" \u2014 "),H9=a("a"),HDe=o("MPNetConfig"),UDe=o(" (MPNet model)"),JDe=l(),jf=a("li"),PH=a("strong"),YDe=o("mt5"),KDe=o(" \u2014 "),U9=a("a"),ZDe=o("MT5Config"),eqe=o(" (mT5 model)"),oqe=l(),Nf=a("li"),$H=a("strong"),rqe=o("nystromformer"),tqe=o(" \u2014 "),J9=a("a"),aqe=o("NystromformerConfig"),sqe=o(" (Nystromformer model)"),nqe=l(),Df=a("li"),IH=a("strong"),lqe=o("openai-gpt"),iqe=o(" \u2014 "),Y9=a("a"),dqe=o("OpenAIGPTConfig"),cqe=o(" (OpenAI GPT model)"),mqe=l(),qf=a("li"),jH=a("strong"),fqe=o("pegasus"),gqe=o(" \u2014 "),K9=a("a"),hqe=o("PegasusConfig"),uqe=o(" (Pegasus model)"),pqe=l(),Of=a("li"),NH=a("strong"),_qe=o("perceiver"),bqe=o(" \u2014 "),Z9=a("a"),vqe=o("PerceiverConfig"),Tqe=o(" (Perceiver model)"),Fqe=l(),Gf=a("li"),DH=a("strong"),Cqe=o("plbart"),Mqe=o(" \u2014 "),eB=a("a"),Eqe=o("PLBartConfig"),yqe=o(" (PLBart model)"),wqe=l(),Xf=a("li"),qH=a("strong"),Aqe=o("poolformer"),Lqe=o(" \u2014 "),oB=a("a"),Bqe=o("PoolFormerConfig"),xqe=o(" (PoolFormer model)"),kqe=l(),Vf=a("li"),OH=a("strong"),Rqe=o("prophetnet"),Sqe=o(" \u2014 "),rB=a("a"),Pqe=o("ProphetNetConfig"),$qe=o(" (ProphetNet model)"),Iqe=l(),zf=a("li"),GH=a("strong"),jqe=o("qdqbert"),Nqe=o(" \u2014 "),tB=a("a"),Dqe=o("QDQBertConfig"),qqe=o(" (QDQBert model)"),Oqe=l(),Wf=a("li"),XH=a("strong"),Gqe=o("rag"),Xqe=o(" \u2014 "),aB=a("a"),Vqe=o("RagConfig"),zqe=o(" (RAG model)"),Wqe=l(),Qf=a("li"),VH=a("strong"),Qqe=o("realm"),Hqe=o(" \u2014 "),sB=a("a"),Uqe=o("RealmConfig"),Jqe=o(" (Realm model)"),Yqe=l(),Hf=a("li"),zH=a("strong"),Kqe=o("reformer"),Zqe=o(" \u2014 "),nB=a("a"),eOe=o("ReformerConfig"),oOe=o(" (Reformer model)"),rOe=l(),Uf=a("li"),WH=a("strong"),tOe=o("rembert"),aOe=o(" \u2014 "),lB=a("a"),sOe=o("RemBertConfig"),nOe=o(" (RemBERT model)"),lOe=l(),Jf=a("li"),QH=a("strong"),iOe=o("resnet"),dOe=o(" \u2014 "),iB=a("a"),cOe=o("ResNetConfig"),mOe=o(" (ResNet model)"),fOe=l(),Yf=a("li"),HH=a("strong"),gOe=o("retribert"),hOe=o(" \u2014 "),dB=a("a"),uOe=o("RetriBertConfig"),pOe=o(" (RetriBERT model)"),_Oe=l(),Kf=a("li"),UH=a("strong"),bOe=o("roberta"),vOe=o(" \u2014 "),cB=a("a"),TOe=o("RobertaConfig"),FOe=o(" (RoBERTa model)"),COe=l(),Zf=a("li"),JH=a("strong"),MOe=o("roformer"),EOe=o(" \u2014 "),mB=a("a"),yOe=o("RoFormerConfig"),wOe=o(" (RoFormer model)"),AOe=l(),eg=a("li"),YH=a("strong"),LOe=o("segformer"),BOe=o(" \u2014 "),fB=a("a"),xOe=o("SegformerConfig"),kOe=o(" (SegFormer model)"),ROe=l(),og=a("li"),KH=a("strong"),SOe=o("sew"),POe=o(" \u2014 "),gB=a("a"),$Oe=o("SEWConfig"),IOe=o(" (SEW model)"),jOe=l(),rg=a("li"),ZH=a("strong"),NOe=o("sew-d"),DOe=o(" \u2014 "),hB=a("a"),qOe=o("SEWDConfig"),OOe=o(" (SEW-D model)"),GOe=l(),tg=a("li"),eU=a("strong"),XOe=o("speech-encoder-decoder"),VOe=o(" \u2014 "),uB=a("a"),zOe=o("SpeechEncoderDecoderConfig"),WOe=o(" (Speech Encoder decoder model)"),QOe=l(),ag=a("li"),oU=a("strong"),HOe=o("speech_to_text"),UOe=o(" \u2014 "),pB=a("a"),JOe=o("Speech2TextConfig"),YOe=o(" (Speech2Text model)"),KOe=l(),sg=a("li"),rU=a("strong"),ZOe=o("speech_to_text_2"),eGe=o(" \u2014 "),_B=a("a"),oGe=o("Speech2Text2Config"),rGe=o(" (Speech2Text2 model)"),tGe=l(),ng=a("li"),tU=a("strong"),aGe=o("splinter"),sGe=o(" \u2014 "),bB=a("a"),nGe=o("SplinterConfig"),lGe=o(" (Splinter model)"),iGe=l(),lg=a("li"),aU=a("strong"),dGe=o("squeezebert"),cGe=o(" \u2014 "),vB=a("a"),mGe=o("SqueezeBertConfig"),fGe=o(" (SqueezeBERT model)"),gGe=l(),ig=a("li"),sU=a("strong"),hGe=o("swin"),uGe=o(" \u2014 "),TB=a("a"),pGe=o("SwinConfig"),_Ge=o(" (Swin model)"),bGe=l(),dg=a("li"),nU=a("strong"),vGe=o("t5"),TGe=o(" \u2014 "),FB=a("a"),FGe=o("T5Config"),CGe=o(" (T5 model)"),MGe=l(),cg=a("li"),lU=a("strong"),EGe=o("tapas"),yGe=o(" \u2014 "),CB=a("a"),wGe=o("TapasConfig"),AGe=o(" (TAPAS model)"),LGe=l(),mg=a("li"),iU=a("strong"),BGe=o("transfo-xl"),xGe=o(" \u2014 "),MB=a("a"),kGe=o("TransfoXLConfig"),RGe=o(" (Transformer-XL model)"),SGe=l(),fg=a("li"),dU=a("strong"),PGe=o("trocr"),$Ge=o(" \u2014 "),EB=a("a"),IGe=o("TrOCRConfig"),jGe=o(" (TrOCR model)"),NGe=l(),gg=a("li"),cU=a("strong"),DGe=o("unispeech"),qGe=o(" \u2014 "),yB=a("a"),OGe=o("UniSpeechConfig"),GGe=o(" (UniSpeech model)"),XGe=l(),hg=a("li"),mU=a("strong"),VGe=o("unispeech-sat"),zGe=o(" \u2014 "),wB=a("a"),WGe=o("UniSpeechSatConfig"),QGe=o(" (UniSpeechSat model)"),HGe=l(),ug=a("li"),fU=a("strong"),UGe=o("van"),JGe=o(" \u2014 "),AB=a("a"),YGe=o("VanConfig"),KGe=o(" (VAN model)"),ZGe=l(),pg=a("li"),gU=a("strong"),eXe=o("vilt"),oXe=o(" \u2014 "),LB=a("a"),rXe=o("ViltConfig"),tXe=o(" (ViLT model)"),aXe=l(),_g=a("li"),hU=a("strong"),sXe=o("vision-encoder-decoder"),nXe=o(" \u2014 "),BB=a("a"),lXe=o("VisionEncoderDecoderConfig"),iXe=o(" (Vision Encoder decoder model)"),dXe=l(),bg=a("li"),uU=a("strong"),cXe=o("vision-text-dual-encoder"),mXe=o(" \u2014 "),xB=a("a"),fXe=o("VisionTextDualEncoderConfig"),gXe=o(" (VisionTextDualEncoder model)"),hXe=l(),vg=a("li"),pU=a("strong"),uXe=o("visual_bert"),pXe=o(" \u2014 "),kB=a("a"),_Xe=o("VisualBertConfig"),bXe=o(" (VisualBert model)"),vXe=l(),Tg=a("li"),_U=a("strong"),TXe=o("vit"),FXe=o(" \u2014 "),RB=a("a"),CXe=o("ViTConfig"),MXe=o(" (ViT model)"),EXe=l(),Fg=a("li"),bU=a("strong"),yXe=o("vit_mae"),wXe=o(" \u2014 "),SB=a("a"),AXe=o("ViTMAEConfig"),LXe=o(" (ViTMAE model)"),BXe=l(),Cg=a("li"),vU=a("strong"),xXe=o("wav2vec2"),kXe=o(" \u2014 "),PB=a("a"),RXe=o("Wav2Vec2Config"),SXe=o(" (Wav2Vec2 model)"),PXe=l(),Mg=a("li"),TU=a("strong"),$Xe=o("wavlm"),IXe=o(" \u2014 "),$B=a("a"),jXe=o("WavLMConfig"),NXe=o(" (WavLM model)"),DXe=l(),Eg=a("li"),FU=a("strong"),qXe=o("xglm"),OXe=o(" \u2014 "),IB=a("a"),GXe=o("XGLMConfig"),XXe=o(" (XGLM model)"),VXe=l(),yg=a("li"),CU=a("strong"),zXe=o("xlm"),WXe=o(" \u2014 "),jB=a("a"),QXe=o("XLMConfig"),HXe=o(" (XLM model)"),UXe=l(),wg=a("li"),MU=a("strong"),JXe=o("xlm-prophetnet"),YXe=o(" \u2014 "),NB=a("a"),KXe=o("XLMProphetNetConfig"),ZXe=o(" (XLMProphetNet model)"),eVe=l(),Ag=a("li"),EU=a("strong"),oVe=o("xlm-roberta"),rVe=o(" \u2014 "),DB=a("a"),tVe=o("XLMRobertaConfig"),aVe=o(" (XLM-RoBERTa model)"),sVe=l(),Lg=a("li"),yU=a("strong"),nVe=o("xlm-roberta-xl"),lVe=o(" \u2014 "),qB=a("a"),iVe=o("XLMRobertaXLConfig"),dVe=o(" (XLM-RoBERTa-XL model)"),cVe=l(),Bg=a("li"),wU=a("strong"),mVe=o("xlnet"),fVe=o(" \u2014 "),OB=a("a"),gVe=o("XLNetConfig"),hVe=o(" (XLNet model)"),uVe=l(),xg=a("li"),AU=a("strong"),pVe=o("yoso"),_Ve=o(" \u2014 "),GB=a("a"),bVe=o("YosoConfig"),vVe=o(" (YOSO model)"),TVe=l(),LU=a("p"),FVe=o("Examples:"),CVe=l(),m(fy.$$.fragment),MVe=l(),kg=a("div"),m(gy.$$.fragment),EVe=l(),BU=a("p"),yVe=o("Register a new configuration for this class."),zxe=l(),Qi=a("h2"),Rg=a("a"),xU=a("span"),m(hy.$$.fragment),wVe=l(),kU=a("span"),AVe=o("AutoTokenizer"),Wxe=l(),Qo=a("div"),m(uy.$$.fragment),LVe=l(),py=a("p"),BVe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),XB=a("a"),xVe=o("AutoTokenizer.from_pretrained()"),kVe=o(" class method."),RVe=l(),_y=a("p"),SVe=o("This class cannot be instantiated directly using "),RU=a("code"),PVe=o("__init__()"),$Ve=o(" (throws an error)."),IVe=l(),go=a("div"),m(by.$$.fragment),jVe=l(),SU=a("p"),NVe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),DVe=l(),Ga=a("p"),qVe=o("The tokenizer class to instantiate is selected based on the "),PU=a("code"),OVe=o("model_type"),GVe=o(` property of the config object (either
passed as an argument or loaded from `),$U=a("code"),XVe=o("pretrained_model_name_or_path"),VVe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IU=a("code"),zVe=o("pretrained_model_name_or_path"),WVe=o(":"),QVe=l(),E=a("ul"),zs=a("li"),jU=a("strong"),HVe=o("albert"),UVe=o(" \u2014 "),VB=a("a"),JVe=o("AlbertTokenizer"),YVe=o(" or "),zB=a("a"),KVe=o("AlbertTokenizerFast"),ZVe=o(" (ALBERT model)"),eze=l(),Ws=a("li"),NU=a("strong"),oze=o("bart"),rze=o(" \u2014 "),WB=a("a"),tze=o("BartTokenizer"),aze=o(" or "),QB=a("a"),sze=o("BartTokenizerFast"),nze=o(" (BART model)"),lze=l(),Qs=a("li"),DU=a("strong"),ize=o("barthez"),dze=o(" \u2014 "),HB=a("a"),cze=o("BarthezTokenizer"),mze=o(" or "),UB=a("a"),fze=o("BarthezTokenizerFast"),gze=o(" (BARThez model)"),hze=l(),Sg=a("li"),qU=a("strong"),uze=o("bartpho"),pze=o(" \u2014 "),JB=a("a"),_ze=o("BartphoTokenizer"),bze=o(" (BARTpho model)"),vze=l(),Hs=a("li"),OU=a("strong"),Tze=o("bert"),Fze=o(" \u2014 "),YB=a("a"),Cze=o("BertTokenizer"),Mze=o(" or "),KB=a("a"),Eze=o("BertTokenizerFast"),yze=o(" (BERT model)"),wze=l(),Pg=a("li"),GU=a("strong"),Aze=o("bert-generation"),Lze=o(" \u2014 "),ZB=a("a"),Bze=o("BertGenerationTokenizer"),xze=o(" (Bert Generation model)"),kze=l(),$g=a("li"),XU=a("strong"),Rze=o("bert-japanese"),Sze=o(" \u2014 "),ex=a("a"),Pze=o("BertJapaneseTokenizer"),$ze=o(" (BertJapanese model)"),Ize=l(),Ig=a("li"),VU=a("strong"),jze=o("bertweet"),Nze=o(" \u2014 "),ox=a("a"),Dze=o("BertweetTokenizer"),qze=o(" (Bertweet model)"),Oze=l(),Us=a("li"),zU=a("strong"),Gze=o("big_bird"),Xze=o(" \u2014 "),rx=a("a"),Vze=o("BigBirdTokenizer"),zze=o(" or "),tx=a("a"),Wze=o("BigBirdTokenizerFast"),Qze=o(" (BigBird model)"),Hze=l(),Js=a("li"),WU=a("strong"),Uze=o("bigbird_pegasus"),Jze=o(" \u2014 "),ax=a("a"),Yze=o("PegasusTokenizer"),Kze=o(" or "),sx=a("a"),Zze=o("PegasusTokenizerFast"),eWe=o(" (BigBirdPegasus model)"),oWe=l(),Ys=a("li"),QU=a("strong"),rWe=o("blenderbot"),tWe=o(" \u2014 "),nx=a("a"),aWe=o("BlenderbotTokenizer"),sWe=o(" or "),lx=a("a"),nWe=o("BlenderbotTokenizerFast"),lWe=o(" (Blenderbot model)"),iWe=l(),jg=a("li"),HU=a("strong"),dWe=o("blenderbot-small"),cWe=o(" \u2014 "),ix=a("a"),mWe=o("BlenderbotSmallTokenizer"),fWe=o(" (BlenderbotSmall model)"),gWe=l(),Ng=a("li"),UU=a("strong"),hWe=o("byt5"),uWe=o(" \u2014 "),dx=a("a"),pWe=o("ByT5Tokenizer"),_We=o(" (ByT5 model)"),bWe=l(),Ks=a("li"),JU=a("strong"),vWe=o("camembert"),TWe=o(" \u2014 "),cx=a("a"),FWe=o("CamembertTokenizer"),CWe=o(" or "),mx=a("a"),MWe=o("CamembertTokenizerFast"),EWe=o(" (CamemBERT model)"),yWe=l(),Dg=a("li"),YU=a("strong"),wWe=o("canine"),AWe=o(" \u2014 "),fx=a("a"),LWe=o("CanineTokenizer"),BWe=o(" (Canine model)"),xWe=l(),Zs=a("li"),KU=a("strong"),kWe=o("clip"),RWe=o(" \u2014 "),gx=a("a"),SWe=o("CLIPTokenizer"),PWe=o(" or "),hx=a("a"),$We=o("CLIPTokenizerFast"),IWe=o(" (CLIP model)"),jWe=l(),en=a("li"),ZU=a("strong"),NWe=o("convbert"),DWe=o(" \u2014 "),ux=a("a"),qWe=o("ConvBertTokenizer"),OWe=o(" or "),px=a("a"),GWe=o("ConvBertTokenizerFast"),XWe=o(" (ConvBERT model)"),VWe=l(),on=a("li"),eJ=a("strong"),zWe=o("cpm"),WWe=o(" \u2014 "),_x=a("a"),QWe=o("CpmTokenizer"),HWe=o(" or "),oJ=a("code"),UWe=o("CpmTokenizerFast"),JWe=o(" (CPM model)"),YWe=l(),qg=a("li"),rJ=a("strong"),KWe=o("ctrl"),ZWe=o(" \u2014 "),bx=a("a"),eQe=o("CTRLTokenizer"),oQe=o(" (CTRL model)"),rQe=l(),rn=a("li"),tJ=a("strong"),tQe=o("deberta"),aQe=o(" \u2014 "),vx=a("a"),sQe=o("DebertaTokenizer"),nQe=o(" or "),Tx=a("a"),lQe=o("DebertaTokenizerFast"),iQe=o(" (DeBERTa model)"),dQe=l(),Og=a("li"),aJ=a("strong"),cQe=o("deberta-v2"),mQe=o(" \u2014 "),Fx=a("a"),fQe=o("DebertaV2Tokenizer"),gQe=o(" (DeBERTa-v2 model)"),hQe=l(),tn=a("li"),sJ=a("strong"),uQe=o("distilbert"),pQe=o(" \u2014 "),Cx=a("a"),_Qe=o("DistilBertTokenizer"),bQe=o(" or "),Mx=a("a"),vQe=o("DistilBertTokenizerFast"),TQe=o(" (DistilBERT model)"),FQe=l(),an=a("li"),nJ=a("strong"),CQe=o("dpr"),MQe=o(" \u2014 "),Ex=a("a"),EQe=o("DPRQuestionEncoderTokenizer"),yQe=o(" or "),yx=a("a"),wQe=o("DPRQuestionEncoderTokenizerFast"),AQe=o(" (DPR model)"),LQe=l(),sn=a("li"),lJ=a("strong"),BQe=o("electra"),xQe=o(" \u2014 "),wx=a("a"),kQe=o("ElectraTokenizer"),RQe=o(" or "),Ax=a("a"),SQe=o("ElectraTokenizerFast"),PQe=o(" (ELECTRA model)"),$Qe=l(),Gg=a("li"),iJ=a("strong"),IQe=o("flaubert"),jQe=o(" \u2014 "),Lx=a("a"),NQe=o("FlaubertTokenizer"),DQe=o(" (FlauBERT model)"),qQe=l(),nn=a("li"),dJ=a("strong"),OQe=o("fnet"),GQe=o(" \u2014 "),Bx=a("a"),XQe=o("FNetTokenizer"),VQe=o(" or "),xx=a("a"),zQe=o("FNetTokenizerFast"),WQe=o(" (FNet model)"),QQe=l(),Xg=a("li"),cJ=a("strong"),HQe=o("fsmt"),UQe=o(" \u2014 "),kx=a("a"),JQe=o("FSMTTokenizer"),YQe=o(" (FairSeq Machine-Translation model)"),KQe=l(),ln=a("li"),mJ=a("strong"),ZQe=o("funnel"),eHe=o(" \u2014 "),Rx=a("a"),oHe=o("FunnelTokenizer"),rHe=o(" or "),Sx=a("a"),tHe=o("FunnelTokenizerFast"),aHe=o(" (Funnel Transformer model)"),sHe=l(),dn=a("li"),fJ=a("strong"),nHe=o("gpt2"),lHe=o(" \u2014 "),Px=a("a"),iHe=o("GPT2Tokenizer"),dHe=o(" or "),$x=a("a"),cHe=o("GPT2TokenizerFast"),mHe=o(" (OpenAI GPT-2 model)"),fHe=l(),cn=a("li"),gJ=a("strong"),gHe=o("gpt_neo"),hHe=o(" \u2014 "),Ix=a("a"),uHe=o("GPT2Tokenizer"),pHe=o(" or "),jx=a("a"),_He=o("GPT2TokenizerFast"),bHe=o(" (GPT Neo model)"),vHe=l(),mn=a("li"),hJ=a("strong"),THe=o("herbert"),FHe=o(" \u2014 "),Nx=a("a"),CHe=o("HerbertTokenizer"),MHe=o(" or "),Dx=a("a"),EHe=o("HerbertTokenizerFast"),yHe=o(" (HerBERT model)"),wHe=l(),Vg=a("li"),uJ=a("strong"),AHe=o("hubert"),LHe=o(" \u2014 "),qx=a("a"),BHe=o("Wav2Vec2CTCTokenizer"),xHe=o(" (Hubert model)"),kHe=l(),fn=a("li"),pJ=a("strong"),RHe=o("ibert"),SHe=o(" \u2014 "),Ox=a("a"),PHe=o("RobertaTokenizer"),$He=o(" or "),Gx=a("a"),IHe=o("RobertaTokenizerFast"),jHe=o(" (I-BERT model)"),NHe=l(),gn=a("li"),_J=a("strong"),DHe=o("layoutlm"),qHe=o(" \u2014 "),Xx=a("a"),OHe=o("LayoutLMTokenizer"),GHe=o(" or "),Vx=a("a"),XHe=o("LayoutLMTokenizerFast"),VHe=o(" (LayoutLM model)"),zHe=l(),hn=a("li"),bJ=a("strong"),WHe=o("layoutlmv2"),QHe=o(" \u2014 "),zx=a("a"),HHe=o("LayoutLMv2Tokenizer"),UHe=o(" or "),Wx=a("a"),JHe=o("LayoutLMv2TokenizerFast"),YHe=o(" (LayoutLMv2 model)"),KHe=l(),un=a("li"),vJ=a("strong"),ZHe=o("layoutxlm"),eUe=o(" \u2014 "),Qx=a("a"),oUe=o("LayoutXLMTokenizer"),rUe=o(" or "),Hx=a("a"),tUe=o("LayoutXLMTokenizerFast"),aUe=o(" (LayoutXLM model)"),sUe=l(),pn=a("li"),TJ=a("strong"),nUe=o("led"),lUe=o(" \u2014 "),Ux=a("a"),iUe=o("LEDTokenizer"),dUe=o(" or "),Jx=a("a"),cUe=o("LEDTokenizerFast"),mUe=o(" (LED model)"),fUe=l(),_n=a("li"),FJ=a("strong"),gUe=o("longformer"),hUe=o(" \u2014 "),Yx=a("a"),uUe=o("LongformerTokenizer"),pUe=o(" or "),Kx=a("a"),_Ue=o("LongformerTokenizerFast"),bUe=o(" (Longformer model)"),vUe=l(),zg=a("li"),CJ=a("strong"),TUe=o("luke"),FUe=o(" \u2014 "),Zx=a("a"),CUe=o("LukeTokenizer"),MUe=o(" (LUKE model)"),EUe=l(),bn=a("li"),MJ=a("strong"),yUe=o("lxmert"),wUe=o(" \u2014 "),ek=a("a"),AUe=o("LxmertTokenizer"),LUe=o(" or "),ok=a("a"),BUe=o("LxmertTokenizerFast"),xUe=o(" (LXMERT model)"),kUe=l(),Wg=a("li"),EJ=a("strong"),RUe=o("m2m_100"),SUe=o(" \u2014 "),rk=a("a"),PUe=o("M2M100Tokenizer"),$Ue=o(" (M2M100 model)"),IUe=l(),Qg=a("li"),yJ=a("strong"),jUe=o("marian"),NUe=o(" \u2014 "),tk=a("a"),DUe=o("MarianTokenizer"),qUe=o(" (Marian model)"),OUe=l(),vn=a("li"),wJ=a("strong"),GUe=o("mbart"),XUe=o(" \u2014 "),ak=a("a"),VUe=o("MBartTokenizer"),zUe=o(" or "),sk=a("a"),WUe=o("MBartTokenizerFast"),QUe=o(" (mBART model)"),HUe=l(),Tn=a("li"),AJ=a("strong"),UUe=o("mbart50"),JUe=o(" \u2014 "),nk=a("a"),YUe=o("MBart50Tokenizer"),KUe=o(" or "),lk=a("a"),ZUe=o("MBart50TokenizerFast"),eJe=o(" (mBART-50 model)"),oJe=l(),Hg=a("li"),LJ=a("strong"),rJe=o("mluke"),tJe=o(" \u2014 "),ik=a("a"),aJe=o("MLukeTokenizer"),sJe=o(" (mLUKE model)"),nJe=l(),Fn=a("li"),BJ=a("strong"),lJe=o("mobilebert"),iJe=o(" \u2014 "),dk=a("a"),dJe=o("MobileBertTokenizer"),cJe=o(" or "),ck=a("a"),mJe=o("MobileBertTokenizerFast"),fJe=o(" (MobileBERT model)"),gJe=l(),Cn=a("li"),xJ=a("strong"),hJe=o("mpnet"),uJe=o(" \u2014 "),mk=a("a"),pJe=o("MPNetTokenizer"),_Je=o(" or "),fk=a("a"),bJe=o("MPNetTokenizerFast"),vJe=o(" (MPNet model)"),TJe=l(),Mn=a("li"),kJ=a("strong"),FJe=o("mt5"),CJe=o(" \u2014 "),gk=a("a"),MJe=o("MT5Tokenizer"),EJe=o(" or "),hk=a("a"),yJe=o("MT5TokenizerFast"),wJe=o(" (mT5 model)"),AJe=l(),En=a("li"),RJ=a("strong"),LJe=o("openai-gpt"),BJe=o(" \u2014 "),uk=a("a"),xJe=o("OpenAIGPTTokenizer"),kJe=o(" or "),pk=a("a"),RJe=o("OpenAIGPTTokenizerFast"),SJe=o(" (OpenAI GPT model)"),PJe=l(),yn=a("li"),SJ=a("strong"),$Je=o("pegasus"),IJe=o(" \u2014 "),_k=a("a"),jJe=o("PegasusTokenizer"),NJe=o(" or "),bk=a("a"),DJe=o("PegasusTokenizerFast"),qJe=o(" (Pegasus model)"),OJe=l(),Ug=a("li"),PJ=a("strong"),GJe=o("perceiver"),XJe=o(" \u2014 "),vk=a("a"),VJe=o("PerceiverTokenizer"),zJe=o(" (Perceiver model)"),WJe=l(),Jg=a("li"),$J=a("strong"),QJe=o("phobert"),HJe=o(" \u2014 "),Tk=a("a"),UJe=o("PhobertTokenizer"),JJe=o(" (PhoBERT model)"),YJe=l(),Yg=a("li"),IJ=a("strong"),KJe=o("plbart"),ZJe=o(" \u2014 "),Fk=a("a"),eYe=o("PLBartTokenizer"),oYe=o(" (PLBart model)"),rYe=l(),Kg=a("li"),jJ=a("strong"),tYe=o("prophetnet"),aYe=o(" \u2014 "),Ck=a("a"),sYe=o("ProphetNetTokenizer"),nYe=o(" (ProphetNet model)"),lYe=l(),wn=a("li"),NJ=a("strong"),iYe=o("qdqbert"),dYe=o(" \u2014 "),Mk=a("a"),cYe=o("BertTokenizer"),mYe=o(" or "),Ek=a("a"),fYe=o("BertTokenizerFast"),gYe=o(" (QDQBert model)"),hYe=l(),Zg=a("li"),DJ=a("strong"),uYe=o("rag"),pYe=o(" \u2014 "),yk=a("a"),_Ye=o("RagTokenizer"),bYe=o(" (RAG model)"),vYe=l(),An=a("li"),qJ=a("strong"),TYe=o("realm"),FYe=o(" \u2014 "),wk=a("a"),CYe=o("RealmTokenizer"),MYe=o(" or "),Ak=a("a"),EYe=o("RealmTokenizerFast"),yYe=o(" (Realm model)"),wYe=l(),Ln=a("li"),OJ=a("strong"),AYe=o("reformer"),LYe=o(" \u2014 "),Lk=a("a"),BYe=o("ReformerTokenizer"),xYe=o(" or "),Bk=a("a"),kYe=o("ReformerTokenizerFast"),RYe=o(" (Reformer model)"),SYe=l(),Bn=a("li"),GJ=a("strong"),PYe=o("rembert"),$Ye=o(" \u2014 "),xk=a("a"),IYe=o("RemBertTokenizer"),jYe=o(" or "),kk=a("a"),NYe=o("RemBertTokenizerFast"),DYe=o(" (RemBERT model)"),qYe=l(),xn=a("li"),XJ=a("strong"),OYe=o("retribert"),GYe=o(" \u2014 "),Rk=a("a"),XYe=o("RetriBertTokenizer"),VYe=o(" or "),Sk=a("a"),zYe=o("RetriBertTokenizerFast"),WYe=o(" (RetriBERT model)"),QYe=l(),kn=a("li"),VJ=a("strong"),HYe=o("roberta"),UYe=o(" \u2014 "),Pk=a("a"),JYe=o("RobertaTokenizer"),YYe=o(" or "),$k=a("a"),KYe=o("RobertaTokenizerFast"),ZYe=o(" (RoBERTa model)"),eKe=l(),Rn=a("li"),zJ=a("strong"),oKe=o("roformer"),rKe=o(" \u2014 "),Ik=a("a"),tKe=o("RoFormerTokenizer"),aKe=o(" or "),jk=a("a"),sKe=o("RoFormerTokenizerFast"),nKe=o(" (RoFormer model)"),lKe=l(),eh=a("li"),WJ=a("strong"),iKe=o("speech_to_text"),dKe=o(" \u2014 "),Nk=a("a"),cKe=o("Speech2TextTokenizer"),mKe=o(" (Speech2Text model)"),fKe=l(),oh=a("li"),QJ=a("strong"),gKe=o("speech_to_text_2"),hKe=o(" \u2014 "),Dk=a("a"),uKe=o("Speech2Text2Tokenizer"),pKe=o(" (Speech2Text2 model)"),_Ke=l(),Sn=a("li"),HJ=a("strong"),bKe=o("splinter"),vKe=o(" \u2014 "),qk=a("a"),TKe=o("SplinterTokenizer"),FKe=o(" or "),Ok=a("a"),CKe=o("SplinterTokenizerFast"),MKe=o(" (Splinter model)"),EKe=l(),Pn=a("li"),UJ=a("strong"),yKe=o("squeezebert"),wKe=o(" \u2014 "),Gk=a("a"),AKe=o("SqueezeBertTokenizer"),LKe=o(" or "),Xk=a("a"),BKe=o("SqueezeBertTokenizerFast"),xKe=o(" (SqueezeBERT model)"),kKe=l(),$n=a("li"),JJ=a("strong"),RKe=o("t5"),SKe=o(" \u2014 "),Vk=a("a"),PKe=o("T5Tokenizer"),$Ke=o(" or "),zk=a("a"),IKe=o("T5TokenizerFast"),jKe=o(" (T5 model)"),NKe=l(),rh=a("li"),YJ=a("strong"),DKe=o("tapas"),qKe=o(" \u2014 "),Wk=a("a"),OKe=o("TapasTokenizer"),GKe=o(" (TAPAS model)"),XKe=l(),th=a("li"),KJ=a("strong"),VKe=o("transfo-xl"),zKe=o(" \u2014 "),Qk=a("a"),WKe=o("TransfoXLTokenizer"),QKe=o(" (Transformer-XL model)"),HKe=l(),ah=a("li"),ZJ=a("strong"),UKe=o("wav2vec2"),JKe=o(" \u2014 "),Hk=a("a"),YKe=o("Wav2Vec2CTCTokenizer"),KKe=o(" (Wav2Vec2 model)"),ZKe=l(),sh=a("li"),eY=a("strong"),eZe=o("wav2vec2_phoneme"),oZe=o(" \u2014 "),Uk=a("a"),rZe=o("Wav2Vec2PhonemeCTCTokenizer"),tZe=o(" (Wav2Vec2Phoneme model)"),aZe=l(),In=a("li"),oY=a("strong"),sZe=o("xglm"),nZe=o(" \u2014 "),Jk=a("a"),lZe=o("XGLMTokenizer"),iZe=o(" or "),Yk=a("a"),dZe=o("XGLMTokenizerFast"),cZe=o(" (XGLM model)"),mZe=l(),nh=a("li"),rY=a("strong"),fZe=o("xlm"),gZe=o(" \u2014 "),Kk=a("a"),hZe=o("XLMTokenizer"),uZe=o(" (XLM model)"),pZe=l(),lh=a("li"),tY=a("strong"),_Ze=o("xlm-prophetnet"),bZe=o(" \u2014 "),Zk=a("a"),vZe=o("XLMProphetNetTokenizer"),TZe=o(" (XLMProphetNet model)"),FZe=l(),jn=a("li"),aY=a("strong"),CZe=o("xlm-roberta"),MZe=o(" \u2014 "),eR=a("a"),EZe=o("XLMRobertaTokenizer"),yZe=o(" or "),oR=a("a"),wZe=o("XLMRobertaTokenizerFast"),AZe=o(" (XLM-RoBERTa model)"),LZe=l(),Nn=a("li"),sY=a("strong"),BZe=o("xlnet"),xZe=o(" \u2014 "),rR=a("a"),kZe=o("XLNetTokenizer"),RZe=o(" or "),tR=a("a"),SZe=o("XLNetTokenizerFast"),PZe=o(" (XLNet model)"),$Ze=l(),nY=a("p"),IZe=o("Examples:"),jZe=l(),m(vy.$$.fragment),NZe=l(),ih=a("div"),m(Ty.$$.fragment),DZe=l(),lY=a("p"),qZe=o("Register a new tokenizer in this mapping."),Qxe=l(),Hi=a("h2"),dh=a("a"),iY=a("span"),m(Fy.$$.fragment),OZe=l(),dY=a("span"),GZe=o("AutoFeatureExtractor"),Hxe=l(),Ho=a("div"),m(Cy.$$.fragment),XZe=l(),My=a("p"),VZe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),aR=a("a"),zZe=o("AutoFeatureExtractor.from_pretrained()"),WZe=o(" class method."),QZe=l(),Ey=a("p"),HZe=o("This class cannot be instantiated directly using "),cY=a("code"),UZe=o("__init__()"),JZe=o(" (throws an error)."),YZe=l(),Ie=a("div"),m(yy.$$.fragment),KZe=l(),mY=a("p"),ZZe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),eeo=l(),Xa=a("p"),oeo=o("The feature extractor class to instantiate is selected based on the "),fY=a("code"),reo=o("model_type"),teo=o(` property of the config object
(either passed as an argument or loaded from `),gY=a("code"),aeo=o("pretrained_model_name_or_path"),seo=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),hY=a("code"),neo=o("pretrained_model_name_or_path"),leo=o(":"),ieo=l(),re=a("ul"),ch=a("li"),uY=a("strong"),deo=o("beit"),ceo=o(" \u2014 "),sR=a("a"),meo=o("BeitFeatureExtractor"),feo=o(" (BEiT model)"),geo=l(),mh=a("li"),pY=a("strong"),heo=o("clip"),ueo=o(" \u2014 "),nR=a("a"),peo=o("CLIPFeatureExtractor"),_eo=o(" (CLIP model)"),beo=l(),fh=a("li"),_Y=a("strong"),veo=o("convnext"),Teo=o(" \u2014 "),lR=a("a"),Feo=o("ConvNextFeatureExtractor"),Ceo=o(" (ConvNext model)"),Meo=l(),gh=a("li"),bY=a("strong"),Eeo=o("deit"),yeo=o(" \u2014 "),iR=a("a"),weo=o("DeiTFeatureExtractor"),Aeo=o(" (DeiT model)"),Leo=l(),hh=a("li"),vY=a("strong"),Beo=o("detr"),xeo=o(" \u2014 "),dR=a("a"),keo=o("DetrFeatureExtractor"),Reo=o(" (DETR model)"),Seo=l(),uh=a("li"),TY=a("strong"),Peo=o("hubert"),$eo=o(" \u2014 "),cR=a("a"),Ieo=o("Wav2Vec2FeatureExtractor"),jeo=o(" (Hubert model)"),Neo=l(),ph=a("li"),FY=a("strong"),Deo=o("layoutlmv2"),qeo=o(" \u2014 "),mR=a("a"),Oeo=o("LayoutLMv2FeatureExtractor"),Geo=o(" (LayoutLMv2 model)"),Xeo=l(),_h=a("li"),CY=a("strong"),Veo=o("maskformer"),zeo=o(" \u2014 "),fR=a("a"),Weo=o("MaskFormerFeatureExtractor"),Qeo=o(" (MaskFormer model)"),Heo=l(),bh=a("li"),MY=a("strong"),Ueo=o("perceiver"),Jeo=o(" \u2014 "),gR=a("a"),Yeo=o("PerceiverFeatureExtractor"),Keo=o(" (Perceiver model)"),Zeo=l(),vh=a("li"),EY=a("strong"),eoo=o("poolformer"),ooo=o(" \u2014 "),hR=a("a"),roo=o("PoolFormerFeatureExtractor"),too=o(" (PoolFormer model)"),aoo=l(),Th=a("li"),yY=a("strong"),soo=o("resnet"),noo=o(" \u2014 "),uR=a("a"),loo=o("ConvNextFeatureExtractor"),ioo=o(" (ResNet model)"),doo=l(),Fh=a("li"),wY=a("strong"),coo=o("segformer"),moo=o(" \u2014 "),pR=a("a"),foo=o("SegformerFeatureExtractor"),goo=o(" (SegFormer model)"),hoo=l(),Ch=a("li"),AY=a("strong"),uoo=o("speech_to_text"),poo=o(" \u2014 "),_R=a("a"),_oo=o("Speech2TextFeatureExtractor"),boo=o(" (Speech2Text model)"),voo=l(),Mh=a("li"),LY=a("strong"),Too=o("swin"),Foo=o(" \u2014 "),bR=a("a"),Coo=o("ViTFeatureExtractor"),Moo=o(" (Swin model)"),Eoo=l(),Eh=a("li"),BY=a("strong"),yoo=o("van"),woo=o(" \u2014 "),vR=a("a"),Aoo=o("ConvNextFeatureExtractor"),Loo=o(" (VAN model)"),Boo=l(),yh=a("li"),xY=a("strong"),xoo=o("vit"),koo=o(" \u2014 "),TR=a("a"),Roo=o("ViTFeatureExtractor"),Soo=o(" (ViT model)"),Poo=l(),wh=a("li"),kY=a("strong"),$oo=o("vit_mae"),Ioo=o(" \u2014 "),FR=a("a"),joo=o("ViTFeatureExtractor"),Noo=o(" (ViTMAE model)"),Doo=l(),Ah=a("li"),RY=a("strong"),qoo=o("wav2vec2"),Ooo=o(" \u2014 "),CR=a("a"),Goo=o("Wav2Vec2FeatureExtractor"),Xoo=o(" (Wav2Vec2 model)"),Voo=l(),m(Lh.$$.fragment),zoo=l(),SY=a("p"),Woo=o("Examples:"),Qoo=l(),m(wy.$$.fragment),Hoo=l(),Bh=a("div"),m(Ay.$$.fragment),Uoo=l(),PY=a("p"),Joo=o("Register a new feature extractor for this class."),Uxe=l(),Ui=a("h2"),xh=a("a"),$Y=a("span"),m(Ly.$$.fragment),Yoo=l(),IY=a("span"),Koo=o("AutoProcessor"),Jxe=l(),Uo=a("div"),m(By.$$.fragment),Zoo=l(),xy=a("p"),ero=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),MR=a("a"),oro=o("AutoProcessor.from_pretrained()"),rro=o(" class method."),tro=l(),ky=a("p"),aro=o("This class cannot be instantiated directly using "),jY=a("code"),sro=o("__init__()"),nro=o(" (throws an error)."),lro=l(),je=a("div"),m(Ry.$$.fragment),iro=l(),NY=a("p"),dro=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),cro=l(),Ji=a("p"),mro=o("The processor class to instantiate is selected based on the "),DY=a("code"),fro=o("model_type"),gro=o(` property of the config object (either
passed as an argument or loaded from `),qY=a("code"),hro=o("pretrained_model_name_or_path"),uro=o(" if possible):"),pro=l(),xe=a("ul"),kh=a("li"),OY=a("strong"),_ro=o("clip"),bro=o(" \u2014 "),ER=a("a"),vro=o("CLIPProcessor"),Tro=o(" (CLIP model)"),Fro=l(),Rh=a("li"),GY=a("strong"),Cro=o("layoutlmv2"),Mro=o(" \u2014 "),yR=a("a"),Ero=o("LayoutLMv2Processor"),yro=o(" (LayoutLMv2 model)"),wro=l(),Sh=a("li"),XY=a("strong"),Aro=o("layoutxlm"),Lro=o(" \u2014 "),wR=a("a"),Bro=o("LayoutXLMProcessor"),xro=o(" (LayoutXLM model)"),kro=l(),Ph=a("li"),VY=a("strong"),Rro=o("speech_to_text"),Sro=o(" \u2014 "),AR=a("a"),Pro=o("Speech2TextProcessor"),$ro=o(" (Speech2Text model)"),Iro=l(),$h=a("li"),zY=a("strong"),jro=o("speech_to_text_2"),Nro=o(" \u2014 "),LR=a("a"),Dro=o("Speech2Text2Processor"),qro=o(" (Speech2Text2 model)"),Oro=l(),Ih=a("li"),WY=a("strong"),Gro=o("trocr"),Xro=o(" \u2014 "),BR=a("a"),Vro=o("TrOCRProcessor"),zro=o(" (TrOCR model)"),Wro=l(),jh=a("li"),QY=a("strong"),Qro=o("vision-text-dual-encoder"),Hro=o(" \u2014 "),xR=a("a"),Uro=o("VisionTextDualEncoderProcessor"),Jro=o(" (VisionTextDualEncoder model)"),Yro=l(),Nh=a("li"),HY=a("strong"),Kro=o("wav2vec2"),Zro=o(" \u2014 "),kR=a("a"),eto=o("Wav2Vec2Processor"),oto=o(" (Wav2Vec2 model)"),rto=l(),m(Dh.$$.fragment),tto=l(),UY=a("p"),ato=o("Examples:"),sto=l(),m(Sy.$$.fragment),nto=l(),qh=a("div"),m(Py.$$.fragment),lto=l(),JY=a("p"),ito=o("Register a new processor for this class."),Yxe=l(),Yi=a("h2"),Oh=a("a"),YY=a("span"),m($y.$$.fragment),dto=l(),KY=a("span"),cto=o("AutoModel"),Kxe=l(),Jo=a("div"),m(Iy.$$.fragment),mto=l(),Ki=a("p"),fto=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),ZY=a("code"),gto=o("from_pretrained()"),hto=o("class method or the "),eK=a("code"),uto=o("from_config()"),pto=o(`class
method.`),_to=l(),jy=a("p"),bto=o("This class cannot be instantiated directly using "),oK=a("code"),vto=o("__init__()"),Tto=o(" (throws an error)."),Fto=l(),Vr=a("div"),m(Ny.$$.fragment),Cto=l(),rK=a("p"),Mto=o("Instantiates one of the base model classes of the library from a configuration."),Eto=l(),Zi=a("p"),yto=o(`Note:
Loading a model from its configuration file does `),tK=a("strong"),wto=o("not"),Ato=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),aK=a("code"),Lto=o("from_pretrained()"),Bto=o("to load the model weights."),xto=l(),sK=a("p"),kto=o("Examples:"),Rto=l(),m(Dy.$$.fragment),Sto=l(),Ne=a("div"),m(qy.$$.fragment),Pto=l(),nK=a("p"),$to=o("Instantiate one of the base model classes of the library from a pretrained model."),Ito=l(),Va=a("p"),jto=o("The model class to instantiate is selected based on the "),lK=a("code"),Nto=o("model_type"),Dto=o(` property of the config object (either
passed as an argument or loaded from `),iK=a("code"),qto=o("pretrained_model_name_or_path"),Oto=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dK=a("code"),Gto=o("pretrained_model_name_or_path"),Xto=o(":"),Vto=l(),F=a("ul"),Gh=a("li"),cK=a("strong"),zto=o("albert"),Wto=o(" \u2014 "),RR=a("a"),Qto=o("AlbertModel"),Hto=o(" (ALBERT model)"),Uto=l(),Xh=a("li"),mK=a("strong"),Jto=o("bart"),Yto=o(" \u2014 "),SR=a("a"),Kto=o("BartModel"),Zto=o(" (BART model)"),eao=l(),Vh=a("li"),fK=a("strong"),oao=o("beit"),rao=o(" \u2014 "),PR=a("a"),tao=o("BeitModel"),aao=o(" (BEiT model)"),sao=l(),zh=a("li"),gK=a("strong"),nao=o("bert"),lao=o(" \u2014 "),$R=a("a"),iao=o("BertModel"),dao=o(" (BERT model)"),cao=l(),Wh=a("li"),hK=a("strong"),mao=o("bert-generation"),fao=o(" \u2014 "),IR=a("a"),gao=o("BertGenerationEncoder"),hao=o(" (Bert Generation model)"),uao=l(),Qh=a("li"),uK=a("strong"),pao=o("big_bird"),_ao=o(" \u2014 "),jR=a("a"),bao=o("BigBirdModel"),vao=o(" (BigBird model)"),Tao=l(),Hh=a("li"),pK=a("strong"),Fao=o("bigbird_pegasus"),Cao=o(" \u2014 "),NR=a("a"),Mao=o("BigBirdPegasusModel"),Eao=o(" (BigBirdPegasus model)"),yao=l(),Uh=a("li"),_K=a("strong"),wao=o("blenderbot"),Aao=o(" \u2014 "),DR=a("a"),Lao=o("BlenderbotModel"),Bao=o(" (Blenderbot model)"),xao=l(),Jh=a("li"),bK=a("strong"),kao=o("blenderbot-small"),Rao=o(" \u2014 "),qR=a("a"),Sao=o("BlenderbotSmallModel"),Pao=o(" (BlenderbotSmall model)"),$ao=l(),Yh=a("li"),vK=a("strong"),Iao=o("camembert"),jao=o(" \u2014 "),OR=a("a"),Nao=o("CamembertModel"),Dao=o(" (CamemBERT model)"),qao=l(),Kh=a("li"),TK=a("strong"),Oao=o("canine"),Gao=o(" \u2014 "),GR=a("a"),Xao=o("CanineModel"),Vao=o(" (Canine model)"),zao=l(),Zh=a("li"),FK=a("strong"),Wao=o("clip"),Qao=o(" \u2014 "),XR=a("a"),Hao=o("CLIPModel"),Uao=o(" (CLIP model)"),Jao=l(),eu=a("li"),CK=a("strong"),Yao=o("convbert"),Kao=o(" \u2014 "),VR=a("a"),Zao=o("ConvBertModel"),eso=o(" (ConvBERT model)"),oso=l(),ou=a("li"),MK=a("strong"),rso=o("convnext"),tso=o(" \u2014 "),zR=a("a"),aso=o("ConvNextModel"),sso=o(" (ConvNext model)"),nso=l(),ru=a("li"),EK=a("strong"),lso=o("ctrl"),iso=o(" \u2014 "),WR=a("a"),dso=o("CTRLModel"),cso=o(" (CTRL model)"),mso=l(),tu=a("li"),yK=a("strong"),fso=o("data2vec-audio"),gso=o(" \u2014 "),QR=a("a"),hso=o("Data2VecAudioModel"),uso=o(" (Data2VecAudio model)"),pso=l(),au=a("li"),wK=a("strong"),_so=o("data2vec-text"),bso=o(" \u2014 "),HR=a("a"),vso=o("Data2VecTextModel"),Tso=o(" (Data2VecText model)"),Fso=l(),su=a("li"),AK=a("strong"),Cso=o("deberta"),Mso=o(" \u2014 "),UR=a("a"),Eso=o("DebertaModel"),yso=o(" (DeBERTa model)"),wso=l(),nu=a("li"),LK=a("strong"),Aso=o("deberta-v2"),Lso=o(" \u2014 "),JR=a("a"),Bso=o("DebertaV2Model"),xso=o(" (DeBERTa-v2 model)"),kso=l(),lu=a("li"),BK=a("strong"),Rso=o("deit"),Sso=o(" \u2014 "),YR=a("a"),Pso=o("DeiTModel"),$so=o(" (DeiT model)"),Iso=l(),iu=a("li"),xK=a("strong"),jso=o("detr"),Nso=o(" \u2014 "),KR=a("a"),Dso=o("DetrModel"),qso=o(" (DETR model)"),Oso=l(),du=a("li"),kK=a("strong"),Gso=o("distilbert"),Xso=o(" \u2014 "),ZR=a("a"),Vso=o("DistilBertModel"),zso=o(" (DistilBERT model)"),Wso=l(),cu=a("li"),RK=a("strong"),Qso=o("dpr"),Hso=o(" \u2014 "),eS=a("a"),Uso=o("DPRQuestionEncoder"),Jso=o(" (DPR model)"),Yso=l(),mu=a("li"),SK=a("strong"),Kso=o("electra"),Zso=o(" \u2014 "),oS=a("a"),eno=o("ElectraModel"),ono=o(" (ELECTRA model)"),rno=l(),fu=a("li"),PK=a("strong"),tno=o("flaubert"),ano=o(" \u2014 "),rS=a("a"),sno=o("FlaubertModel"),nno=o(" (FlauBERT model)"),lno=l(),gu=a("li"),$K=a("strong"),ino=o("fnet"),dno=o(" \u2014 "),tS=a("a"),cno=o("FNetModel"),mno=o(" (FNet model)"),fno=l(),hu=a("li"),IK=a("strong"),gno=o("fsmt"),hno=o(" \u2014 "),aS=a("a"),uno=o("FSMTModel"),pno=o(" (FairSeq Machine-Translation model)"),_no=l(),Dn=a("li"),jK=a("strong"),bno=o("funnel"),vno=o(" \u2014 "),sS=a("a"),Tno=o("FunnelModel"),Fno=o(" or "),nS=a("a"),Cno=o("FunnelBaseModel"),Mno=o(" (Funnel Transformer model)"),Eno=l(),uu=a("li"),NK=a("strong"),yno=o("gpt2"),wno=o(" \u2014 "),lS=a("a"),Ano=o("GPT2Model"),Lno=o(" (OpenAI GPT-2 model)"),Bno=l(),pu=a("li"),DK=a("strong"),xno=o("gpt_neo"),kno=o(" \u2014 "),iS=a("a"),Rno=o("GPTNeoModel"),Sno=o(" (GPT Neo model)"),Pno=l(),_u=a("li"),qK=a("strong"),$no=o("gptj"),Ino=o(" \u2014 "),dS=a("a"),jno=o("GPTJModel"),Nno=o(" (GPT-J model)"),Dno=l(),bu=a("li"),OK=a("strong"),qno=o("hubert"),Ono=o(" \u2014 "),cS=a("a"),Gno=o("HubertModel"),Xno=o(" (Hubert model)"),Vno=l(),vu=a("li"),GK=a("strong"),zno=o("ibert"),Wno=o(" \u2014 "),mS=a("a"),Qno=o("IBertModel"),Hno=o(" (I-BERT model)"),Uno=l(),Tu=a("li"),XK=a("strong"),Jno=o("imagegpt"),Yno=o(" \u2014 "),fS=a("a"),Kno=o("ImageGPTModel"),Zno=o(" (ImageGPT model)"),elo=l(),Fu=a("li"),VK=a("strong"),olo=o("layoutlm"),rlo=o(" \u2014 "),gS=a("a"),tlo=o("LayoutLMModel"),alo=o(" (LayoutLM model)"),slo=l(),Cu=a("li"),zK=a("strong"),nlo=o("layoutlmv2"),llo=o(" \u2014 "),hS=a("a"),ilo=o("LayoutLMv2Model"),dlo=o(" (LayoutLMv2 model)"),clo=l(),Mu=a("li"),WK=a("strong"),mlo=o("led"),flo=o(" \u2014 "),uS=a("a"),glo=o("LEDModel"),hlo=o(" (LED model)"),ulo=l(),Eu=a("li"),QK=a("strong"),plo=o("longformer"),_lo=o(" \u2014 "),pS=a("a"),blo=o("LongformerModel"),vlo=o(" (Longformer model)"),Tlo=l(),yu=a("li"),HK=a("strong"),Flo=o("luke"),Clo=o(" \u2014 "),_S=a("a"),Mlo=o("LukeModel"),Elo=o(" (LUKE model)"),ylo=l(),wu=a("li"),UK=a("strong"),wlo=o("lxmert"),Alo=o(" \u2014 "),bS=a("a"),Llo=o("LxmertModel"),Blo=o(" (LXMERT model)"),xlo=l(),Au=a("li"),JK=a("strong"),klo=o("m2m_100"),Rlo=o(" \u2014 "),vS=a("a"),Slo=o("M2M100Model"),Plo=o(" (M2M100 model)"),$lo=l(),Lu=a("li"),YK=a("strong"),Ilo=o("marian"),jlo=o(" \u2014 "),TS=a("a"),Nlo=o("MarianModel"),Dlo=o(" (Marian model)"),qlo=l(),Bu=a("li"),KK=a("strong"),Olo=o("maskformer"),Glo=o(" \u2014 "),FS=a("a"),Xlo=o("MaskFormerModel"),Vlo=o(" (MaskFormer model)"),zlo=l(),xu=a("li"),ZK=a("strong"),Wlo=o("mbart"),Qlo=o(" \u2014 "),CS=a("a"),Hlo=o("MBartModel"),Ulo=o(" (mBART model)"),Jlo=l(),ku=a("li"),eZ=a("strong"),Ylo=o("megatron-bert"),Klo=o(" \u2014 "),MS=a("a"),Zlo=o("MegatronBertModel"),eio=o(" (MegatronBert model)"),oio=l(),Ru=a("li"),oZ=a("strong"),rio=o("mobilebert"),tio=o(" \u2014 "),ES=a("a"),aio=o("MobileBertModel"),sio=o(" (MobileBERT model)"),nio=l(),Su=a("li"),rZ=a("strong"),lio=o("mpnet"),iio=o(" \u2014 "),yS=a("a"),dio=o("MPNetModel"),cio=o(" (MPNet model)"),mio=l(),Pu=a("li"),tZ=a("strong"),fio=o("mt5"),gio=o(" \u2014 "),wS=a("a"),hio=o("MT5Model"),uio=o(" (mT5 model)"),pio=l(),$u=a("li"),aZ=a("strong"),_io=o("nystromformer"),bio=o(" \u2014 "),AS=a("a"),vio=o("NystromformerModel"),Tio=o(" (Nystromformer model)"),Fio=l(),Iu=a("li"),sZ=a("strong"),Cio=o("openai-gpt"),Mio=o(" \u2014 "),LS=a("a"),Eio=o("OpenAIGPTModel"),yio=o(" (OpenAI GPT model)"),wio=l(),ju=a("li"),nZ=a("strong"),Aio=o("pegasus"),Lio=o(" \u2014 "),BS=a("a"),Bio=o("PegasusModel"),xio=o(" (Pegasus model)"),kio=l(),Nu=a("li"),lZ=a("strong"),Rio=o("perceiver"),Sio=o(" \u2014 "),xS=a("a"),Pio=o("PerceiverModel"),$io=o(" (Perceiver model)"),Iio=l(),Du=a("li"),iZ=a("strong"),jio=o("plbart"),Nio=o(" \u2014 "),kS=a("a"),Dio=o("PLBartModel"),qio=o(" (PLBart model)"),Oio=l(),qu=a("li"),dZ=a("strong"),Gio=o("poolformer"),Xio=o(" \u2014 "),RS=a("a"),Vio=o("PoolFormerModel"),zio=o(" (PoolFormer model)"),Wio=l(),Ou=a("li"),cZ=a("strong"),Qio=o("prophetnet"),Hio=o(" \u2014 "),SS=a("a"),Uio=o("ProphetNetModel"),Jio=o(" (ProphetNet model)"),Yio=l(),Gu=a("li"),mZ=a("strong"),Kio=o("qdqbert"),Zio=o(" \u2014 "),PS=a("a"),edo=o("QDQBertModel"),odo=o(" (QDQBert model)"),rdo=l(),Xu=a("li"),fZ=a("strong"),tdo=o("reformer"),ado=o(" \u2014 "),$S=a("a"),sdo=o("ReformerModel"),ndo=o(" (Reformer model)"),ldo=l(),Vu=a("li"),gZ=a("strong"),ido=o("rembert"),ddo=o(" \u2014 "),IS=a("a"),cdo=o("RemBertModel"),mdo=o(" (RemBERT model)"),fdo=l(),zu=a("li"),hZ=a("strong"),gdo=o("resnet"),hdo=o(" \u2014 "),jS=a("a"),udo=o("ResNetModel"),pdo=o(" (ResNet model)"),_do=l(),Wu=a("li"),uZ=a("strong"),bdo=o("retribert"),vdo=o(" \u2014 "),NS=a("a"),Tdo=o("RetriBertModel"),Fdo=o(" (RetriBERT model)"),Cdo=l(),Qu=a("li"),pZ=a("strong"),Mdo=o("roberta"),Edo=o(" \u2014 "),DS=a("a"),ydo=o("RobertaModel"),wdo=o(" (RoBERTa model)"),Ado=l(),Hu=a("li"),_Z=a("strong"),Ldo=o("roformer"),Bdo=o(" \u2014 "),qS=a("a"),xdo=o("RoFormerModel"),kdo=o(" (RoFormer model)"),Rdo=l(),Uu=a("li"),bZ=a("strong"),Sdo=o("segformer"),Pdo=o(" \u2014 "),OS=a("a"),$do=o("SegformerModel"),Ido=o(" (SegFormer model)"),jdo=l(),Ju=a("li"),vZ=a("strong"),Ndo=o("sew"),Ddo=o(" \u2014 "),GS=a("a"),qdo=o("SEWModel"),Odo=o(" (SEW model)"),Gdo=l(),Yu=a("li"),TZ=a("strong"),Xdo=o("sew-d"),Vdo=o(" \u2014 "),XS=a("a"),zdo=o("SEWDModel"),Wdo=o(" (SEW-D model)"),Qdo=l(),Ku=a("li"),FZ=a("strong"),Hdo=o("speech_to_text"),Udo=o(" \u2014 "),VS=a("a"),Jdo=o("Speech2TextModel"),Ydo=o(" (Speech2Text model)"),Kdo=l(),Zu=a("li"),CZ=a("strong"),Zdo=o("splinter"),eco=o(" \u2014 "),zS=a("a"),oco=o("SplinterModel"),rco=o(" (Splinter model)"),tco=l(),ep=a("li"),MZ=a("strong"),aco=o("squeezebert"),sco=o(" \u2014 "),WS=a("a"),nco=o("SqueezeBertModel"),lco=o(" (SqueezeBERT model)"),ico=l(),op=a("li"),EZ=a("strong"),dco=o("swin"),cco=o(" \u2014 "),QS=a("a"),mco=o("SwinModel"),fco=o(" (Swin model)"),gco=l(),rp=a("li"),yZ=a("strong"),hco=o("t5"),uco=o(" \u2014 "),HS=a("a"),pco=o("T5Model"),_co=o(" (T5 model)"),bco=l(),tp=a("li"),wZ=a("strong"),vco=o("tapas"),Tco=o(" \u2014 "),US=a("a"),Fco=o("TapasModel"),Cco=o(" (TAPAS model)"),Mco=l(),ap=a("li"),AZ=a("strong"),Eco=o("transfo-xl"),yco=o(" \u2014 "),JS=a("a"),wco=o("TransfoXLModel"),Aco=o(" (Transformer-XL model)"),Lco=l(),sp=a("li"),LZ=a("strong"),Bco=o("unispeech"),xco=o(" \u2014 "),YS=a("a"),kco=o("UniSpeechModel"),Rco=o(" (UniSpeech model)"),Sco=l(),np=a("li"),BZ=a("strong"),Pco=o("unispeech-sat"),$co=o(" \u2014 "),KS=a("a"),Ico=o("UniSpeechSatModel"),jco=o(" (UniSpeechSat model)"),Nco=l(),lp=a("li"),xZ=a("strong"),Dco=o("van"),qco=o(" \u2014 "),ZS=a("a"),Oco=o("VanModel"),Gco=o(" (VAN model)"),Xco=l(),ip=a("li"),kZ=a("strong"),Vco=o("vilt"),zco=o(" \u2014 "),eP=a("a"),Wco=o("ViltModel"),Qco=o(" (ViLT model)"),Hco=l(),dp=a("li"),RZ=a("strong"),Uco=o("vision-text-dual-encoder"),Jco=o(" \u2014 "),oP=a("a"),Yco=o("VisionTextDualEncoderModel"),Kco=o(" (VisionTextDualEncoder model)"),Zco=l(),cp=a("li"),SZ=a("strong"),emo=o("visual_bert"),omo=o(" \u2014 "),rP=a("a"),rmo=o("VisualBertModel"),tmo=o(" (VisualBert model)"),amo=l(),mp=a("li"),PZ=a("strong"),smo=o("vit"),nmo=o(" \u2014 "),tP=a("a"),lmo=o("ViTModel"),imo=o(" (ViT model)"),dmo=l(),fp=a("li"),$Z=a("strong"),cmo=o("vit_mae"),mmo=o(" \u2014 "),aP=a("a"),fmo=o("ViTMAEModel"),gmo=o(" (ViTMAE model)"),hmo=l(),gp=a("li"),IZ=a("strong"),umo=o("wav2vec2"),pmo=o(" \u2014 "),sP=a("a"),_mo=o("Wav2Vec2Model"),bmo=o(" (Wav2Vec2 model)"),vmo=l(),hp=a("li"),jZ=a("strong"),Tmo=o("wavlm"),Fmo=o(" \u2014 "),nP=a("a"),Cmo=o("WavLMModel"),Mmo=o(" (WavLM model)"),Emo=l(),up=a("li"),NZ=a("strong"),ymo=o("xglm"),wmo=o(" \u2014 "),lP=a("a"),Amo=o("XGLMModel"),Lmo=o(" (XGLM model)"),Bmo=l(),pp=a("li"),DZ=a("strong"),xmo=o("xlm"),kmo=o(" \u2014 "),iP=a("a"),Rmo=o("XLMModel"),Smo=o(" (XLM model)"),Pmo=l(),_p=a("li"),qZ=a("strong"),$mo=o("xlm-prophetnet"),Imo=o(" \u2014 "),dP=a("a"),jmo=o("XLMProphetNetModel"),Nmo=o(" (XLMProphetNet model)"),Dmo=l(),bp=a("li"),OZ=a("strong"),qmo=o("xlm-roberta"),Omo=o(" \u2014 "),cP=a("a"),Gmo=o("XLMRobertaModel"),Xmo=o(" (XLM-RoBERTa model)"),Vmo=l(),vp=a("li"),GZ=a("strong"),zmo=o("xlm-roberta-xl"),Wmo=o(" \u2014 "),mP=a("a"),Qmo=o("XLMRobertaXLModel"),Hmo=o(" (XLM-RoBERTa-XL model)"),Umo=l(),Tp=a("li"),XZ=a("strong"),Jmo=o("xlnet"),Ymo=o(" \u2014 "),fP=a("a"),Kmo=o("XLNetModel"),Zmo=o(" (XLNet model)"),efo=l(),Fp=a("li"),VZ=a("strong"),ofo=o("yoso"),rfo=o(" \u2014 "),gP=a("a"),tfo=o("YosoModel"),afo=o(" (YOSO model)"),sfo=l(),Cp=a("p"),nfo=o("The model is set in evaluation mode by default using "),zZ=a("code"),lfo=o("model.eval()"),ifo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),WZ=a("code"),dfo=o("model.train()"),cfo=l(),QZ=a("p"),mfo=o("Examples:"),ffo=l(),m(Oy.$$.fragment),Zxe=l(),ed=a("h2"),Mp=a("a"),HZ=a("span"),m(Gy.$$.fragment),gfo=l(),UZ=a("span"),hfo=o("AutoModelForPreTraining"),eke=l(),Yo=a("div"),m(Xy.$$.fragment),ufo=l(),od=a("p"),pfo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),JZ=a("code"),_fo=o("from_pretrained()"),bfo=o("class method or the "),YZ=a("code"),vfo=o("from_config()"),Tfo=o(`class
method.`),Ffo=l(),Vy=a("p"),Cfo=o("This class cannot be instantiated directly using "),KZ=a("code"),Mfo=o("__init__()"),Efo=o(" (throws an error)."),yfo=l(),zr=a("div"),m(zy.$$.fragment),wfo=l(),ZZ=a("p"),Afo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Lfo=l(),rd=a("p"),Bfo=o(`Note:
Loading a model from its configuration file does `),eee=a("strong"),xfo=o("not"),kfo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),oee=a("code"),Rfo=o("from_pretrained()"),Sfo=o("to load the model weights."),Pfo=l(),ree=a("p"),$fo=o("Examples:"),Ifo=l(),m(Wy.$$.fragment),jfo=l(),De=a("div"),m(Qy.$$.fragment),Nfo=l(),tee=a("p"),Dfo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),qfo=l(),za=a("p"),Ofo=o("The model class to instantiate is selected based on the "),aee=a("code"),Gfo=o("model_type"),Xfo=o(` property of the config object (either
passed as an argument or loaded from `),see=a("code"),Vfo=o("pretrained_model_name_or_path"),zfo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nee=a("code"),Wfo=o("pretrained_model_name_or_path"),Qfo=o(":"),Hfo=l(),k=a("ul"),Ep=a("li"),lee=a("strong"),Ufo=o("albert"),Jfo=o(" \u2014 "),hP=a("a"),Yfo=o("AlbertForPreTraining"),Kfo=o(" (ALBERT model)"),Zfo=l(),yp=a("li"),iee=a("strong"),ego=o("bart"),ogo=o(" \u2014 "),uP=a("a"),rgo=o("BartForConditionalGeneration"),tgo=o(" (BART model)"),ago=l(),wp=a("li"),dee=a("strong"),sgo=o("bert"),ngo=o(" \u2014 "),pP=a("a"),lgo=o("BertForPreTraining"),igo=o(" (BERT model)"),dgo=l(),Ap=a("li"),cee=a("strong"),cgo=o("big_bird"),mgo=o(" \u2014 "),_P=a("a"),fgo=o("BigBirdForPreTraining"),ggo=o(" (BigBird model)"),hgo=l(),Lp=a("li"),mee=a("strong"),ugo=o("camembert"),pgo=o(" \u2014 "),bP=a("a"),_go=o("CamembertForMaskedLM"),bgo=o(" (CamemBERT model)"),vgo=l(),Bp=a("li"),fee=a("strong"),Tgo=o("ctrl"),Fgo=o(" \u2014 "),vP=a("a"),Cgo=o("CTRLLMHeadModel"),Mgo=o(" (CTRL model)"),Ego=l(),xp=a("li"),gee=a("strong"),ygo=o("data2vec-text"),wgo=o(" \u2014 "),TP=a("a"),Ago=o("Data2VecTextForMaskedLM"),Lgo=o(" (Data2VecText model)"),Bgo=l(),kp=a("li"),hee=a("strong"),xgo=o("deberta"),kgo=o(" \u2014 "),FP=a("a"),Rgo=o("DebertaForMaskedLM"),Sgo=o(" (DeBERTa model)"),Pgo=l(),Rp=a("li"),uee=a("strong"),$go=o("deberta-v2"),Igo=o(" \u2014 "),CP=a("a"),jgo=o("DebertaV2ForMaskedLM"),Ngo=o(" (DeBERTa-v2 model)"),Dgo=l(),Sp=a("li"),pee=a("strong"),qgo=o("distilbert"),Ogo=o(" \u2014 "),MP=a("a"),Ggo=o("DistilBertForMaskedLM"),Xgo=o(" (DistilBERT model)"),Vgo=l(),Pp=a("li"),_ee=a("strong"),zgo=o("electra"),Wgo=o(" \u2014 "),EP=a("a"),Qgo=o("ElectraForPreTraining"),Hgo=o(" (ELECTRA model)"),Ugo=l(),$p=a("li"),bee=a("strong"),Jgo=o("flaubert"),Ygo=o(" \u2014 "),yP=a("a"),Kgo=o("FlaubertWithLMHeadModel"),Zgo=o(" (FlauBERT model)"),eho=l(),Ip=a("li"),vee=a("strong"),oho=o("fnet"),rho=o(" \u2014 "),wP=a("a"),tho=o("FNetForPreTraining"),aho=o(" (FNet model)"),sho=l(),jp=a("li"),Tee=a("strong"),nho=o("fsmt"),lho=o(" \u2014 "),AP=a("a"),iho=o("FSMTForConditionalGeneration"),dho=o(" (FairSeq Machine-Translation model)"),cho=l(),Np=a("li"),Fee=a("strong"),mho=o("funnel"),fho=o(" \u2014 "),LP=a("a"),gho=o("FunnelForPreTraining"),hho=o(" (Funnel Transformer model)"),uho=l(),Dp=a("li"),Cee=a("strong"),pho=o("gpt2"),_ho=o(" \u2014 "),BP=a("a"),bho=o("GPT2LMHeadModel"),vho=o(" (OpenAI GPT-2 model)"),Tho=l(),qp=a("li"),Mee=a("strong"),Fho=o("ibert"),Cho=o(" \u2014 "),xP=a("a"),Mho=o("IBertForMaskedLM"),Eho=o(" (I-BERT model)"),yho=l(),Op=a("li"),Eee=a("strong"),who=o("layoutlm"),Aho=o(" \u2014 "),kP=a("a"),Lho=o("LayoutLMForMaskedLM"),Bho=o(" (LayoutLM model)"),xho=l(),Gp=a("li"),yee=a("strong"),kho=o("longformer"),Rho=o(" \u2014 "),RP=a("a"),Sho=o("LongformerForMaskedLM"),Pho=o(" (Longformer model)"),$ho=l(),Xp=a("li"),wee=a("strong"),Iho=o("lxmert"),jho=o(" \u2014 "),SP=a("a"),Nho=o("LxmertForPreTraining"),Dho=o(" (LXMERT model)"),qho=l(),Vp=a("li"),Aee=a("strong"),Oho=o("megatron-bert"),Gho=o(" \u2014 "),PP=a("a"),Xho=o("MegatronBertForPreTraining"),Vho=o(" (MegatronBert model)"),zho=l(),zp=a("li"),Lee=a("strong"),Who=o("mobilebert"),Qho=o(" \u2014 "),$P=a("a"),Hho=o("MobileBertForPreTraining"),Uho=o(" (MobileBERT model)"),Jho=l(),Wp=a("li"),Bee=a("strong"),Yho=o("mpnet"),Kho=o(" \u2014 "),IP=a("a"),Zho=o("MPNetForMaskedLM"),euo=o(" (MPNet model)"),ouo=l(),Qp=a("li"),xee=a("strong"),ruo=o("openai-gpt"),tuo=o(" \u2014 "),jP=a("a"),auo=o("OpenAIGPTLMHeadModel"),suo=o(" (OpenAI GPT model)"),nuo=l(),Hp=a("li"),kee=a("strong"),luo=o("retribert"),iuo=o(" \u2014 "),NP=a("a"),duo=o("RetriBertModel"),cuo=o(" (RetriBERT model)"),muo=l(),Up=a("li"),Ree=a("strong"),fuo=o("roberta"),guo=o(" \u2014 "),DP=a("a"),huo=o("RobertaForMaskedLM"),uuo=o(" (RoBERTa model)"),puo=l(),Jp=a("li"),See=a("strong"),_uo=o("squeezebert"),buo=o(" \u2014 "),qP=a("a"),vuo=o("SqueezeBertForMaskedLM"),Tuo=o(" (SqueezeBERT model)"),Fuo=l(),Yp=a("li"),Pee=a("strong"),Cuo=o("t5"),Muo=o(" \u2014 "),OP=a("a"),Euo=o("T5ForConditionalGeneration"),yuo=o(" (T5 model)"),wuo=l(),Kp=a("li"),$ee=a("strong"),Auo=o("tapas"),Luo=o(" \u2014 "),GP=a("a"),Buo=o("TapasForMaskedLM"),xuo=o(" (TAPAS model)"),kuo=l(),Zp=a("li"),Iee=a("strong"),Ruo=o("transfo-xl"),Suo=o(" \u2014 "),XP=a("a"),Puo=o("TransfoXLLMHeadModel"),$uo=o(" (Transformer-XL model)"),Iuo=l(),e_=a("li"),jee=a("strong"),juo=o("unispeech"),Nuo=o(" \u2014 "),VP=a("a"),Duo=o("UniSpeechForPreTraining"),quo=o(" (UniSpeech model)"),Ouo=l(),o_=a("li"),Nee=a("strong"),Guo=o("unispeech-sat"),Xuo=o(" \u2014 "),zP=a("a"),Vuo=o("UniSpeechSatForPreTraining"),zuo=o(" (UniSpeechSat model)"),Wuo=l(),r_=a("li"),Dee=a("strong"),Quo=o("visual_bert"),Huo=o(" \u2014 "),WP=a("a"),Uuo=o("VisualBertForPreTraining"),Juo=o(" (VisualBert model)"),Yuo=l(),t_=a("li"),qee=a("strong"),Kuo=o("vit_mae"),Zuo=o(" \u2014 "),QP=a("a"),epo=o("ViTMAEForPreTraining"),opo=o(" (ViTMAE model)"),rpo=l(),a_=a("li"),Oee=a("strong"),tpo=o("wav2vec2"),apo=o(" \u2014 "),HP=a("a"),spo=o("Wav2Vec2ForPreTraining"),npo=o(" (Wav2Vec2 model)"),lpo=l(),s_=a("li"),Gee=a("strong"),ipo=o("xlm"),dpo=o(" \u2014 "),UP=a("a"),cpo=o("XLMWithLMHeadModel"),mpo=o(" (XLM model)"),fpo=l(),n_=a("li"),Xee=a("strong"),gpo=o("xlm-roberta"),hpo=o(" \u2014 "),JP=a("a"),upo=o("XLMRobertaForMaskedLM"),ppo=o(" (XLM-RoBERTa model)"),_po=l(),l_=a("li"),Vee=a("strong"),bpo=o("xlm-roberta-xl"),vpo=o(" \u2014 "),YP=a("a"),Tpo=o("XLMRobertaXLForMaskedLM"),Fpo=o(" (XLM-RoBERTa-XL model)"),Cpo=l(),i_=a("li"),zee=a("strong"),Mpo=o("xlnet"),Epo=o(" \u2014 "),KP=a("a"),ypo=o("XLNetLMHeadModel"),wpo=o(" (XLNet model)"),Apo=l(),d_=a("p"),Lpo=o("The model is set in evaluation mode by default using "),Wee=a("code"),Bpo=o("model.eval()"),xpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Qee=a("code"),kpo=o("model.train()"),Rpo=l(),Hee=a("p"),Spo=o("Examples:"),Ppo=l(),m(Hy.$$.fragment),oke=l(),td=a("h2"),c_=a("a"),Uee=a("span"),m(Uy.$$.fragment),$po=l(),Jee=a("span"),Ipo=o("AutoModelForCausalLM"),rke=l(),Ko=a("div"),m(Jy.$$.fragment),jpo=l(),ad=a("p"),Npo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Yee=a("code"),Dpo=o("from_pretrained()"),qpo=o("class method or the "),Kee=a("code"),Opo=o("from_config()"),Gpo=o(`class
method.`),Xpo=l(),Yy=a("p"),Vpo=o("This class cannot be instantiated directly using "),Zee=a("code"),zpo=o("__init__()"),Wpo=o(" (throws an error)."),Qpo=l(),Wr=a("div"),m(Ky.$$.fragment),Hpo=l(),eoe=a("p"),Upo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Jpo=l(),sd=a("p"),Ypo=o(`Note:
Loading a model from its configuration file does `),ooe=a("strong"),Kpo=o("not"),Zpo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),roe=a("code"),e_o=o("from_pretrained()"),o_o=o("to load the model weights."),r_o=l(),toe=a("p"),t_o=o("Examples:"),a_o=l(),m(Zy.$$.fragment),s_o=l(),qe=a("div"),m(ew.$$.fragment),n_o=l(),aoe=a("p"),l_o=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),i_o=l(),Wa=a("p"),d_o=o("The model class to instantiate is selected based on the "),soe=a("code"),c_o=o("model_type"),m_o=o(` property of the config object (either
passed as an argument or loaded from `),noe=a("code"),f_o=o("pretrained_model_name_or_path"),g_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),loe=a("code"),h_o=o("pretrained_model_name_or_path"),u_o=o(":"),p_o=l(),$=a("ul"),m_=a("li"),ioe=a("strong"),__o=o("bart"),b_o=o(" \u2014 "),ZP=a("a"),v_o=o("BartForCausalLM"),T_o=o(" (BART model)"),F_o=l(),f_=a("li"),doe=a("strong"),C_o=o("bert"),M_o=o(" \u2014 "),e$=a("a"),E_o=o("BertLMHeadModel"),y_o=o(" (BERT model)"),w_o=l(),g_=a("li"),coe=a("strong"),A_o=o("bert-generation"),L_o=o(" \u2014 "),o$=a("a"),B_o=o("BertGenerationDecoder"),x_o=o(" (Bert Generation model)"),k_o=l(),h_=a("li"),moe=a("strong"),R_o=o("big_bird"),S_o=o(" \u2014 "),r$=a("a"),P_o=o("BigBirdForCausalLM"),$_o=o(" (BigBird model)"),I_o=l(),u_=a("li"),foe=a("strong"),j_o=o("bigbird_pegasus"),N_o=o(" \u2014 "),t$=a("a"),D_o=o("BigBirdPegasusForCausalLM"),q_o=o(" (BigBirdPegasus model)"),O_o=l(),p_=a("li"),goe=a("strong"),G_o=o("blenderbot"),X_o=o(" \u2014 "),a$=a("a"),V_o=o("BlenderbotForCausalLM"),z_o=o(" (Blenderbot model)"),W_o=l(),__=a("li"),hoe=a("strong"),Q_o=o("blenderbot-small"),H_o=o(" \u2014 "),s$=a("a"),U_o=o("BlenderbotSmallForCausalLM"),J_o=o(" (BlenderbotSmall model)"),Y_o=l(),b_=a("li"),uoe=a("strong"),K_o=o("camembert"),Z_o=o(" \u2014 "),n$=a("a"),ebo=o("CamembertForCausalLM"),obo=o(" (CamemBERT model)"),rbo=l(),v_=a("li"),poe=a("strong"),tbo=o("ctrl"),abo=o(" \u2014 "),l$=a("a"),sbo=o("CTRLLMHeadModel"),nbo=o(" (CTRL model)"),lbo=l(),T_=a("li"),_oe=a("strong"),ibo=o("data2vec-text"),dbo=o(" \u2014 "),i$=a("a"),cbo=o("Data2VecTextForCausalLM"),mbo=o(" (Data2VecText model)"),fbo=l(),F_=a("li"),boe=a("strong"),gbo=o("electra"),hbo=o(" \u2014 "),d$=a("a"),ubo=o("ElectraForCausalLM"),pbo=o(" (ELECTRA model)"),_bo=l(),C_=a("li"),voe=a("strong"),bbo=o("gpt2"),vbo=o(" \u2014 "),c$=a("a"),Tbo=o("GPT2LMHeadModel"),Fbo=o(" (OpenAI GPT-2 model)"),Cbo=l(),M_=a("li"),Toe=a("strong"),Mbo=o("gpt_neo"),Ebo=o(" \u2014 "),m$=a("a"),ybo=o("GPTNeoForCausalLM"),wbo=o(" (GPT Neo model)"),Abo=l(),E_=a("li"),Foe=a("strong"),Lbo=o("gptj"),Bbo=o(" \u2014 "),f$=a("a"),xbo=o("GPTJForCausalLM"),kbo=o(" (GPT-J model)"),Rbo=l(),y_=a("li"),Coe=a("strong"),Sbo=o("marian"),Pbo=o(" \u2014 "),g$=a("a"),$bo=o("MarianForCausalLM"),Ibo=o(" (Marian model)"),jbo=l(),w_=a("li"),Moe=a("strong"),Nbo=o("mbart"),Dbo=o(" \u2014 "),h$=a("a"),qbo=o("MBartForCausalLM"),Obo=o(" (mBART model)"),Gbo=l(),A_=a("li"),Eoe=a("strong"),Xbo=o("megatron-bert"),Vbo=o(" \u2014 "),u$=a("a"),zbo=o("MegatronBertForCausalLM"),Wbo=o(" (MegatronBert model)"),Qbo=l(),L_=a("li"),yoe=a("strong"),Hbo=o("openai-gpt"),Ubo=o(" \u2014 "),p$=a("a"),Jbo=o("OpenAIGPTLMHeadModel"),Ybo=o(" (OpenAI GPT model)"),Kbo=l(),B_=a("li"),woe=a("strong"),Zbo=o("pegasus"),e2o=o(" \u2014 "),_$=a("a"),o2o=o("PegasusForCausalLM"),r2o=o(" (Pegasus model)"),t2o=l(),x_=a("li"),Aoe=a("strong"),a2o=o("plbart"),s2o=o(" \u2014 "),b$=a("a"),n2o=o("PLBartForCausalLM"),l2o=o(" (PLBart model)"),i2o=l(),k_=a("li"),Loe=a("strong"),d2o=o("prophetnet"),c2o=o(" \u2014 "),v$=a("a"),m2o=o("ProphetNetForCausalLM"),f2o=o(" (ProphetNet model)"),g2o=l(),R_=a("li"),Boe=a("strong"),h2o=o("qdqbert"),u2o=o(" \u2014 "),T$=a("a"),p2o=o("QDQBertLMHeadModel"),_2o=o(" (QDQBert model)"),b2o=l(),S_=a("li"),xoe=a("strong"),v2o=o("reformer"),T2o=o(" \u2014 "),F$=a("a"),F2o=o("ReformerModelWithLMHead"),C2o=o(" (Reformer model)"),M2o=l(),P_=a("li"),koe=a("strong"),E2o=o("rembert"),y2o=o(" \u2014 "),C$=a("a"),w2o=o("RemBertForCausalLM"),A2o=o(" (RemBERT model)"),L2o=l(),$_=a("li"),Roe=a("strong"),B2o=o("roberta"),x2o=o(" \u2014 "),M$=a("a"),k2o=o("RobertaForCausalLM"),R2o=o(" (RoBERTa model)"),S2o=l(),I_=a("li"),Soe=a("strong"),P2o=o("roformer"),$2o=o(" \u2014 "),E$=a("a"),I2o=o("RoFormerForCausalLM"),j2o=o(" (RoFormer model)"),N2o=l(),j_=a("li"),Poe=a("strong"),D2o=o("speech_to_text_2"),q2o=o(" \u2014 "),y$=a("a"),O2o=o("Speech2Text2ForCausalLM"),G2o=o(" (Speech2Text2 model)"),X2o=l(),N_=a("li"),$oe=a("strong"),V2o=o("transfo-xl"),z2o=o(" \u2014 "),w$=a("a"),W2o=o("TransfoXLLMHeadModel"),Q2o=o(" (Transformer-XL model)"),H2o=l(),D_=a("li"),Ioe=a("strong"),U2o=o("trocr"),J2o=o(" \u2014 "),A$=a("a"),Y2o=o("TrOCRForCausalLM"),K2o=o(" (TrOCR model)"),Z2o=l(),q_=a("li"),joe=a("strong"),evo=o("xglm"),ovo=o(" \u2014 "),L$=a("a"),rvo=o("XGLMForCausalLM"),tvo=o(" (XGLM model)"),avo=l(),O_=a("li"),Noe=a("strong"),svo=o("xlm"),nvo=o(" \u2014 "),B$=a("a"),lvo=o("XLMWithLMHeadModel"),ivo=o(" (XLM model)"),dvo=l(),G_=a("li"),Doe=a("strong"),cvo=o("xlm-prophetnet"),mvo=o(" \u2014 "),x$=a("a"),fvo=o("XLMProphetNetForCausalLM"),gvo=o(" (XLMProphetNet model)"),hvo=l(),X_=a("li"),qoe=a("strong"),uvo=o("xlm-roberta"),pvo=o(" \u2014 "),k$=a("a"),_vo=o("XLMRobertaForCausalLM"),bvo=o(" (XLM-RoBERTa model)"),vvo=l(),V_=a("li"),Ooe=a("strong"),Tvo=o("xlm-roberta-xl"),Fvo=o(" \u2014 "),R$=a("a"),Cvo=o("XLMRobertaXLForCausalLM"),Mvo=o(" (XLM-RoBERTa-XL model)"),Evo=l(),z_=a("li"),Goe=a("strong"),yvo=o("xlnet"),wvo=o(" \u2014 "),S$=a("a"),Avo=o("XLNetLMHeadModel"),Lvo=o(" (XLNet model)"),Bvo=l(),W_=a("p"),xvo=o("The model is set in evaluation mode by default using "),Xoe=a("code"),kvo=o("model.eval()"),Rvo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Voe=a("code"),Svo=o("model.train()"),Pvo=l(),zoe=a("p"),$vo=o("Examples:"),Ivo=l(),m(ow.$$.fragment),tke=l(),nd=a("h2"),Q_=a("a"),Woe=a("span"),m(rw.$$.fragment),jvo=l(),Qoe=a("span"),Nvo=o("AutoModelForMaskedLM"),ake=l(),Zo=a("div"),m(tw.$$.fragment),Dvo=l(),ld=a("p"),qvo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Hoe=a("code"),Ovo=o("from_pretrained()"),Gvo=o("class method or the "),Uoe=a("code"),Xvo=o("from_config()"),Vvo=o(`class
method.`),zvo=l(),aw=a("p"),Wvo=o("This class cannot be instantiated directly using "),Joe=a("code"),Qvo=o("__init__()"),Hvo=o(" (throws an error)."),Uvo=l(),Qr=a("div"),m(sw.$$.fragment),Jvo=l(),Yoe=a("p"),Yvo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Kvo=l(),id=a("p"),Zvo=o(`Note:
Loading a model from its configuration file does `),Koe=a("strong"),eTo=o("not"),oTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zoe=a("code"),rTo=o("from_pretrained()"),tTo=o("to load the model weights."),aTo=l(),ere=a("p"),sTo=o("Examples:"),nTo=l(),m(nw.$$.fragment),lTo=l(),Oe=a("div"),m(lw.$$.fragment),iTo=l(),ore=a("p"),dTo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),cTo=l(),Qa=a("p"),mTo=o("The model class to instantiate is selected based on the "),rre=a("code"),fTo=o("model_type"),gTo=o(` property of the config object (either
passed as an argument or loaded from `),tre=a("code"),hTo=o("pretrained_model_name_or_path"),uTo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),are=a("code"),pTo=o("pretrained_model_name_or_path"),_To=o(":"),bTo=l(),I=a("ul"),H_=a("li"),sre=a("strong"),vTo=o("albert"),TTo=o(" \u2014 "),P$=a("a"),FTo=o("AlbertForMaskedLM"),CTo=o(" (ALBERT model)"),MTo=l(),U_=a("li"),nre=a("strong"),ETo=o("bart"),yTo=o(" \u2014 "),$$=a("a"),wTo=o("BartForConditionalGeneration"),ATo=o(" (BART model)"),LTo=l(),J_=a("li"),lre=a("strong"),BTo=o("bert"),xTo=o(" \u2014 "),I$=a("a"),kTo=o("BertForMaskedLM"),RTo=o(" (BERT model)"),STo=l(),Y_=a("li"),ire=a("strong"),PTo=o("big_bird"),$To=o(" \u2014 "),j$=a("a"),ITo=o("BigBirdForMaskedLM"),jTo=o(" (BigBird model)"),NTo=l(),K_=a("li"),dre=a("strong"),DTo=o("camembert"),qTo=o(" \u2014 "),N$=a("a"),OTo=o("CamembertForMaskedLM"),GTo=o(" (CamemBERT model)"),XTo=l(),Z_=a("li"),cre=a("strong"),VTo=o("convbert"),zTo=o(" \u2014 "),D$=a("a"),WTo=o("ConvBertForMaskedLM"),QTo=o(" (ConvBERT model)"),HTo=l(),eb=a("li"),mre=a("strong"),UTo=o("data2vec-text"),JTo=o(" \u2014 "),q$=a("a"),YTo=o("Data2VecTextForMaskedLM"),KTo=o(" (Data2VecText model)"),ZTo=l(),ob=a("li"),fre=a("strong"),e1o=o("deberta"),o1o=o(" \u2014 "),O$=a("a"),r1o=o("DebertaForMaskedLM"),t1o=o(" (DeBERTa model)"),a1o=l(),rb=a("li"),gre=a("strong"),s1o=o("deberta-v2"),n1o=o(" \u2014 "),G$=a("a"),l1o=o("DebertaV2ForMaskedLM"),i1o=o(" (DeBERTa-v2 model)"),d1o=l(),tb=a("li"),hre=a("strong"),c1o=o("distilbert"),m1o=o(" \u2014 "),X$=a("a"),f1o=o("DistilBertForMaskedLM"),g1o=o(" (DistilBERT model)"),h1o=l(),ab=a("li"),ure=a("strong"),u1o=o("electra"),p1o=o(" \u2014 "),V$=a("a"),_1o=o("ElectraForMaskedLM"),b1o=o(" (ELECTRA model)"),v1o=l(),sb=a("li"),pre=a("strong"),T1o=o("flaubert"),F1o=o(" \u2014 "),z$=a("a"),C1o=o("FlaubertWithLMHeadModel"),M1o=o(" (FlauBERT model)"),E1o=l(),nb=a("li"),_re=a("strong"),y1o=o("fnet"),w1o=o(" \u2014 "),W$=a("a"),A1o=o("FNetForMaskedLM"),L1o=o(" (FNet model)"),B1o=l(),lb=a("li"),bre=a("strong"),x1o=o("funnel"),k1o=o(" \u2014 "),Q$=a("a"),R1o=o("FunnelForMaskedLM"),S1o=o(" (Funnel Transformer model)"),P1o=l(),ib=a("li"),vre=a("strong"),$1o=o("ibert"),I1o=o(" \u2014 "),H$=a("a"),j1o=o("IBertForMaskedLM"),N1o=o(" (I-BERT model)"),D1o=l(),db=a("li"),Tre=a("strong"),q1o=o("layoutlm"),O1o=o(" \u2014 "),U$=a("a"),G1o=o("LayoutLMForMaskedLM"),X1o=o(" (LayoutLM model)"),V1o=l(),cb=a("li"),Fre=a("strong"),z1o=o("longformer"),W1o=o(" \u2014 "),J$=a("a"),Q1o=o("LongformerForMaskedLM"),H1o=o(" (Longformer model)"),U1o=l(),mb=a("li"),Cre=a("strong"),J1o=o("mbart"),Y1o=o(" \u2014 "),Y$=a("a"),K1o=o("MBartForConditionalGeneration"),Z1o=o(" (mBART model)"),eFo=l(),fb=a("li"),Mre=a("strong"),oFo=o("megatron-bert"),rFo=o(" \u2014 "),K$=a("a"),tFo=o("MegatronBertForMaskedLM"),aFo=o(" (MegatronBert model)"),sFo=l(),gb=a("li"),Ere=a("strong"),nFo=o("mobilebert"),lFo=o(" \u2014 "),Z$=a("a"),iFo=o("MobileBertForMaskedLM"),dFo=o(" (MobileBERT model)"),cFo=l(),hb=a("li"),yre=a("strong"),mFo=o("mpnet"),fFo=o(" \u2014 "),eI=a("a"),gFo=o("MPNetForMaskedLM"),hFo=o(" (MPNet model)"),uFo=l(),ub=a("li"),wre=a("strong"),pFo=o("nystromformer"),_Fo=o(" \u2014 "),oI=a("a"),bFo=o("NystromformerForMaskedLM"),vFo=o(" (Nystromformer model)"),TFo=l(),pb=a("li"),Are=a("strong"),FFo=o("perceiver"),CFo=o(" \u2014 "),rI=a("a"),MFo=o("PerceiverForMaskedLM"),EFo=o(" (Perceiver model)"),yFo=l(),_b=a("li"),Lre=a("strong"),wFo=o("qdqbert"),AFo=o(" \u2014 "),tI=a("a"),LFo=o("QDQBertForMaskedLM"),BFo=o(" (QDQBert model)"),xFo=l(),bb=a("li"),Bre=a("strong"),kFo=o("reformer"),RFo=o(" \u2014 "),aI=a("a"),SFo=o("ReformerForMaskedLM"),PFo=o(" (Reformer model)"),$Fo=l(),vb=a("li"),xre=a("strong"),IFo=o("rembert"),jFo=o(" \u2014 "),sI=a("a"),NFo=o("RemBertForMaskedLM"),DFo=o(" (RemBERT model)"),qFo=l(),Tb=a("li"),kre=a("strong"),OFo=o("roberta"),GFo=o(" \u2014 "),nI=a("a"),XFo=o("RobertaForMaskedLM"),VFo=o(" (RoBERTa model)"),zFo=l(),Fb=a("li"),Rre=a("strong"),WFo=o("roformer"),QFo=o(" \u2014 "),lI=a("a"),HFo=o("RoFormerForMaskedLM"),UFo=o(" (RoFormer model)"),JFo=l(),Cb=a("li"),Sre=a("strong"),YFo=o("squeezebert"),KFo=o(" \u2014 "),iI=a("a"),ZFo=o("SqueezeBertForMaskedLM"),eCo=o(" (SqueezeBERT model)"),oCo=l(),Mb=a("li"),Pre=a("strong"),rCo=o("tapas"),tCo=o(" \u2014 "),dI=a("a"),aCo=o("TapasForMaskedLM"),sCo=o(" (TAPAS model)"),nCo=l(),Eb=a("li"),$re=a("strong"),lCo=o("wav2vec2"),iCo=o(" \u2014 "),Ire=a("code"),dCo=o("Wav2Vec2ForMaskedLM"),cCo=o("(Wav2Vec2 model)"),mCo=l(),yb=a("li"),jre=a("strong"),fCo=o("xlm"),gCo=o(" \u2014 "),cI=a("a"),hCo=o("XLMWithLMHeadModel"),uCo=o(" (XLM model)"),pCo=l(),wb=a("li"),Nre=a("strong"),_Co=o("xlm-roberta"),bCo=o(" \u2014 "),mI=a("a"),vCo=o("XLMRobertaForMaskedLM"),TCo=o(" (XLM-RoBERTa model)"),FCo=l(),Ab=a("li"),Dre=a("strong"),CCo=o("xlm-roberta-xl"),MCo=o(" \u2014 "),fI=a("a"),ECo=o("XLMRobertaXLForMaskedLM"),yCo=o(" (XLM-RoBERTa-XL model)"),wCo=l(),Lb=a("li"),qre=a("strong"),ACo=o("yoso"),LCo=o(" \u2014 "),gI=a("a"),BCo=o("YosoForMaskedLM"),xCo=o(" (YOSO model)"),kCo=l(),Bb=a("p"),RCo=o("The model is set in evaluation mode by default using "),Ore=a("code"),SCo=o("model.eval()"),PCo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gre=a("code"),$Co=o("model.train()"),ICo=l(),Xre=a("p"),jCo=o("Examples:"),NCo=l(),m(iw.$$.fragment),ske=l(),dd=a("h2"),xb=a("a"),Vre=a("span"),m(dw.$$.fragment),DCo=l(),zre=a("span"),qCo=o("AutoModelForSeq2SeqLM"),nke=l(),er=a("div"),m(cw.$$.fragment),OCo=l(),cd=a("p"),GCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wre=a("code"),XCo=o("from_pretrained()"),VCo=o("class method or the "),Qre=a("code"),zCo=o("from_config()"),WCo=o(`class
method.`),QCo=l(),mw=a("p"),HCo=o("This class cannot be instantiated directly using "),Hre=a("code"),UCo=o("__init__()"),JCo=o(" (throws an error)."),YCo=l(),Hr=a("div"),m(fw.$$.fragment),KCo=l(),Ure=a("p"),ZCo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),eMo=l(),md=a("p"),oMo=o(`Note:
Loading a model from its configuration file does `),Jre=a("strong"),rMo=o("not"),tMo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yre=a("code"),aMo=o("from_pretrained()"),sMo=o("to load the model weights."),nMo=l(),Kre=a("p"),lMo=o("Examples:"),iMo=l(),m(gw.$$.fragment),dMo=l(),Ge=a("div"),m(hw.$$.fragment),cMo=l(),Zre=a("p"),mMo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),fMo=l(),Ha=a("p"),gMo=o("The model class to instantiate is selected based on the "),ete=a("code"),hMo=o("model_type"),uMo=o(` property of the config object (either
passed as an argument or loaded from `),ote=a("code"),pMo=o("pretrained_model_name_or_path"),_Mo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rte=a("code"),bMo=o("pretrained_model_name_or_path"),vMo=o(":"),TMo=l(),ne=a("ul"),kb=a("li"),tte=a("strong"),FMo=o("bart"),CMo=o(" \u2014 "),hI=a("a"),MMo=o("BartForConditionalGeneration"),EMo=o(" (BART model)"),yMo=l(),Rb=a("li"),ate=a("strong"),wMo=o("bigbird_pegasus"),AMo=o(" \u2014 "),uI=a("a"),LMo=o("BigBirdPegasusForConditionalGeneration"),BMo=o(" (BigBirdPegasus model)"),xMo=l(),Sb=a("li"),ste=a("strong"),kMo=o("blenderbot"),RMo=o(" \u2014 "),pI=a("a"),SMo=o("BlenderbotForConditionalGeneration"),PMo=o(" (Blenderbot model)"),$Mo=l(),Pb=a("li"),nte=a("strong"),IMo=o("blenderbot-small"),jMo=o(" \u2014 "),_I=a("a"),NMo=o("BlenderbotSmallForConditionalGeneration"),DMo=o(" (BlenderbotSmall model)"),qMo=l(),$b=a("li"),lte=a("strong"),OMo=o("encoder-decoder"),GMo=o(" \u2014 "),bI=a("a"),XMo=o("EncoderDecoderModel"),VMo=o(" (Encoder decoder model)"),zMo=l(),Ib=a("li"),ite=a("strong"),WMo=o("fsmt"),QMo=o(" \u2014 "),vI=a("a"),HMo=o("FSMTForConditionalGeneration"),UMo=o(" (FairSeq Machine-Translation model)"),JMo=l(),jb=a("li"),dte=a("strong"),YMo=o("led"),KMo=o(" \u2014 "),TI=a("a"),ZMo=o("LEDForConditionalGeneration"),e4o=o(" (LED model)"),o4o=l(),Nb=a("li"),cte=a("strong"),r4o=o("m2m_100"),t4o=o(" \u2014 "),FI=a("a"),a4o=o("M2M100ForConditionalGeneration"),s4o=o(" (M2M100 model)"),n4o=l(),Db=a("li"),mte=a("strong"),l4o=o("marian"),i4o=o(" \u2014 "),CI=a("a"),d4o=o("MarianMTModel"),c4o=o(" (Marian model)"),m4o=l(),qb=a("li"),fte=a("strong"),f4o=o("mbart"),g4o=o(" \u2014 "),MI=a("a"),h4o=o("MBartForConditionalGeneration"),u4o=o(" (mBART model)"),p4o=l(),Ob=a("li"),gte=a("strong"),_4o=o("mt5"),b4o=o(" \u2014 "),EI=a("a"),v4o=o("MT5ForConditionalGeneration"),T4o=o(" (mT5 model)"),F4o=l(),Gb=a("li"),hte=a("strong"),C4o=o("pegasus"),M4o=o(" \u2014 "),yI=a("a"),E4o=o("PegasusForConditionalGeneration"),y4o=o(" (Pegasus model)"),w4o=l(),Xb=a("li"),ute=a("strong"),A4o=o("plbart"),L4o=o(" \u2014 "),wI=a("a"),B4o=o("PLBartForConditionalGeneration"),x4o=o(" (PLBart model)"),k4o=l(),Vb=a("li"),pte=a("strong"),R4o=o("prophetnet"),S4o=o(" \u2014 "),AI=a("a"),P4o=o("ProphetNetForConditionalGeneration"),$4o=o(" (ProphetNet model)"),I4o=l(),zb=a("li"),_te=a("strong"),j4o=o("t5"),N4o=o(" \u2014 "),LI=a("a"),D4o=o("T5ForConditionalGeneration"),q4o=o(" (T5 model)"),O4o=l(),Wb=a("li"),bte=a("strong"),G4o=o("xlm-prophetnet"),X4o=o(" \u2014 "),BI=a("a"),V4o=o("XLMProphetNetForConditionalGeneration"),z4o=o(" (XLMProphetNet model)"),W4o=l(),Qb=a("p"),Q4o=o("The model is set in evaluation mode by default using "),vte=a("code"),H4o=o("model.eval()"),U4o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tte=a("code"),J4o=o("model.train()"),Y4o=l(),Fte=a("p"),K4o=o("Examples:"),Z4o=l(),m(uw.$$.fragment),lke=l(),fd=a("h2"),Hb=a("a"),Cte=a("span"),m(pw.$$.fragment),eEo=l(),Mte=a("span"),oEo=o("AutoModelForSequenceClassification"),ike=l(),or=a("div"),m(_w.$$.fragment),rEo=l(),gd=a("p"),tEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Ete=a("code"),aEo=o("from_pretrained()"),sEo=o("class method or the "),yte=a("code"),nEo=o("from_config()"),lEo=o(`class
method.`),iEo=l(),bw=a("p"),dEo=o("This class cannot be instantiated directly using "),wte=a("code"),cEo=o("__init__()"),mEo=o(" (throws an error)."),fEo=l(),Ur=a("div"),m(vw.$$.fragment),gEo=l(),Ate=a("p"),hEo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),uEo=l(),hd=a("p"),pEo=o(`Note:
Loading a model from its configuration file does `),Lte=a("strong"),_Eo=o("not"),bEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bte=a("code"),vEo=o("from_pretrained()"),TEo=o("to load the model weights."),FEo=l(),xte=a("p"),CEo=o("Examples:"),MEo=l(),m(Tw.$$.fragment),EEo=l(),Xe=a("div"),m(Fw.$$.fragment),yEo=l(),kte=a("p"),wEo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),AEo=l(),Ua=a("p"),LEo=o("The model class to instantiate is selected based on the "),Rte=a("code"),BEo=o("model_type"),xEo=o(` property of the config object (either
passed as an argument or loaded from `),Ste=a("code"),kEo=o("pretrained_model_name_or_path"),REo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pte=a("code"),SEo=o("pretrained_model_name_or_path"),PEo=o(":"),$Eo=l(),A=a("ul"),Ub=a("li"),$te=a("strong"),IEo=o("albert"),jEo=o(" \u2014 "),xI=a("a"),NEo=o("AlbertForSequenceClassification"),DEo=o(" (ALBERT model)"),qEo=l(),Jb=a("li"),Ite=a("strong"),OEo=o("bart"),GEo=o(" \u2014 "),kI=a("a"),XEo=o("BartForSequenceClassification"),VEo=o(" (BART model)"),zEo=l(),Yb=a("li"),jte=a("strong"),WEo=o("bert"),QEo=o(" \u2014 "),RI=a("a"),HEo=o("BertForSequenceClassification"),UEo=o(" (BERT model)"),JEo=l(),Kb=a("li"),Nte=a("strong"),YEo=o("big_bird"),KEo=o(" \u2014 "),SI=a("a"),ZEo=o("BigBirdForSequenceClassification"),e3o=o(" (BigBird model)"),o3o=l(),Zb=a("li"),Dte=a("strong"),r3o=o("bigbird_pegasus"),t3o=o(" \u2014 "),PI=a("a"),a3o=o("BigBirdPegasusForSequenceClassification"),s3o=o(" (BigBirdPegasus model)"),n3o=l(),e2=a("li"),qte=a("strong"),l3o=o("camembert"),i3o=o(" \u2014 "),$I=a("a"),d3o=o("CamembertForSequenceClassification"),c3o=o(" (CamemBERT model)"),m3o=l(),o2=a("li"),Ote=a("strong"),f3o=o("canine"),g3o=o(" \u2014 "),II=a("a"),h3o=o("CanineForSequenceClassification"),u3o=o(" (Canine model)"),p3o=l(),r2=a("li"),Gte=a("strong"),_3o=o("convbert"),b3o=o(" \u2014 "),jI=a("a"),v3o=o("ConvBertForSequenceClassification"),T3o=o(" (ConvBERT model)"),F3o=l(),t2=a("li"),Xte=a("strong"),C3o=o("ctrl"),M3o=o(" \u2014 "),NI=a("a"),E3o=o("CTRLForSequenceClassification"),y3o=o(" (CTRL model)"),w3o=l(),a2=a("li"),Vte=a("strong"),A3o=o("data2vec-text"),L3o=o(" \u2014 "),DI=a("a"),B3o=o("Data2VecTextForSequenceClassification"),x3o=o(" (Data2VecText model)"),k3o=l(),s2=a("li"),zte=a("strong"),R3o=o("deberta"),S3o=o(" \u2014 "),qI=a("a"),P3o=o("DebertaForSequenceClassification"),$3o=o(" (DeBERTa model)"),I3o=l(),n2=a("li"),Wte=a("strong"),j3o=o("deberta-v2"),N3o=o(" \u2014 "),OI=a("a"),D3o=o("DebertaV2ForSequenceClassification"),q3o=o(" (DeBERTa-v2 model)"),O3o=l(),l2=a("li"),Qte=a("strong"),G3o=o("distilbert"),X3o=o(" \u2014 "),GI=a("a"),V3o=o("DistilBertForSequenceClassification"),z3o=o(" (DistilBERT model)"),W3o=l(),i2=a("li"),Hte=a("strong"),Q3o=o("electra"),H3o=o(" \u2014 "),XI=a("a"),U3o=o("ElectraForSequenceClassification"),J3o=o(" (ELECTRA model)"),Y3o=l(),d2=a("li"),Ute=a("strong"),K3o=o("flaubert"),Z3o=o(" \u2014 "),VI=a("a"),e5o=o("FlaubertForSequenceClassification"),o5o=o(" (FlauBERT model)"),r5o=l(),c2=a("li"),Jte=a("strong"),t5o=o("fnet"),a5o=o(" \u2014 "),zI=a("a"),s5o=o("FNetForSequenceClassification"),n5o=o(" (FNet model)"),l5o=l(),m2=a("li"),Yte=a("strong"),i5o=o("funnel"),d5o=o(" \u2014 "),WI=a("a"),c5o=o("FunnelForSequenceClassification"),m5o=o(" (Funnel Transformer model)"),f5o=l(),f2=a("li"),Kte=a("strong"),g5o=o("gpt2"),h5o=o(" \u2014 "),QI=a("a"),u5o=o("GPT2ForSequenceClassification"),p5o=o(" (OpenAI GPT-2 model)"),_5o=l(),g2=a("li"),Zte=a("strong"),b5o=o("gpt_neo"),v5o=o(" \u2014 "),HI=a("a"),T5o=o("GPTNeoForSequenceClassification"),F5o=o(" (GPT Neo model)"),C5o=l(),h2=a("li"),eae=a("strong"),M5o=o("gptj"),E5o=o(" \u2014 "),UI=a("a"),y5o=o("GPTJForSequenceClassification"),w5o=o(" (GPT-J model)"),A5o=l(),u2=a("li"),oae=a("strong"),L5o=o("ibert"),B5o=o(" \u2014 "),JI=a("a"),x5o=o("IBertForSequenceClassification"),k5o=o(" (I-BERT model)"),R5o=l(),p2=a("li"),rae=a("strong"),S5o=o("layoutlm"),P5o=o(" \u2014 "),YI=a("a"),$5o=o("LayoutLMForSequenceClassification"),I5o=o(" (LayoutLM model)"),j5o=l(),_2=a("li"),tae=a("strong"),N5o=o("layoutlmv2"),D5o=o(" \u2014 "),KI=a("a"),q5o=o("LayoutLMv2ForSequenceClassification"),O5o=o(" (LayoutLMv2 model)"),G5o=l(),b2=a("li"),aae=a("strong"),X5o=o("led"),V5o=o(" \u2014 "),ZI=a("a"),z5o=o("LEDForSequenceClassification"),W5o=o(" (LED model)"),Q5o=l(),v2=a("li"),sae=a("strong"),H5o=o("longformer"),U5o=o(" \u2014 "),ej=a("a"),J5o=o("LongformerForSequenceClassification"),Y5o=o(" (Longformer model)"),K5o=l(),T2=a("li"),nae=a("strong"),Z5o=o("mbart"),eyo=o(" \u2014 "),oj=a("a"),oyo=o("MBartForSequenceClassification"),ryo=o(" (mBART model)"),tyo=l(),F2=a("li"),lae=a("strong"),ayo=o("megatron-bert"),syo=o(" \u2014 "),rj=a("a"),nyo=o("MegatronBertForSequenceClassification"),lyo=o(" (MegatronBert model)"),iyo=l(),C2=a("li"),iae=a("strong"),dyo=o("mobilebert"),cyo=o(" \u2014 "),tj=a("a"),myo=o("MobileBertForSequenceClassification"),fyo=o(" (MobileBERT model)"),gyo=l(),M2=a("li"),dae=a("strong"),hyo=o("mpnet"),uyo=o(" \u2014 "),aj=a("a"),pyo=o("MPNetForSequenceClassification"),_yo=o(" (MPNet model)"),byo=l(),E2=a("li"),cae=a("strong"),vyo=o("nystromformer"),Tyo=o(" \u2014 "),sj=a("a"),Fyo=o("NystromformerForSequenceClassification"),Cyo=o(" (Nystromformer model)"),Myo=l(),y2=a("li"),mae=a("strong"),Eyo=o("openai-gpt"),yyo=o(" \u2014 "),nj=a("a"),wyo=o("OpenAIGPTForSequenceClassification"),Ayo=o(" (OpenAI GPT model)"),Lyo=l(),w2=a("li"),fae=a("strong"),Byo=o("perceiver"),xyo=o(" \u2014 "),lj=a("a"),kyo=o("PerceiverForSequenceClassification"),Ryo=o(" (Perceiver model)"),Syo=l(),A2=a("li"),gae=a("strong"),Pyo=o("plbart"),$yo=o(" \u2014 "),ij=a("a"),Iyo=o("PLBartForSequenceClassification"),jyo=o(" (PLBart model)"),Nyo=l(),L2=a("li"),hae=a("strong"),Dyo=o("qdqbert"),qyo=o(" \u2014 "),dj=a("a"),Oyo=o("QDQBertForSequenceClassification"),Gyo=o(" (QDQBert model)"),Xyo=l(),B2=a("li"),uae=a("strong"),Vyo=o("reformer"),zyo=o(" \u2014 "),cj=a("a"),Wyo=o("ReformerForSequenceClassification"),Qyo=o(" (Reformer model)"),Hyo=l(),x2=a("li"),pae=a("strong"),Uyo=o("rembert"),Jyo=o(" \u2014 "),mj=a("a"),Yyo=o("RemBertForSequenceClassification"),Kyo=o(" (RemBERT model)"),Zyo=l(),k2=a("li"),_ae=a("strong"),ewo=o("roberta"),owo=o(" \u2014 "),fj=a("a"),rwo=o("RobertaForSequenceClassification"),two=o(" (RoBERTa model)"),awo=l(),R2=a("li"),bae=a("strong"),swo=o("roformer"),nwo=o(" \u2014 "),gj=a("a"),lwo=o("RoFormerForSequenceClassification"),iwo=o(" (RoFormer model)"),dwo=l(),S2=a("li"),vae=a("strong"),cwo=o("squeezebert"),mwo=o(" \u2014 "),hj=a("a"),fwo=o("SqueezeBertForSequenceClassification"),gwo=o(" (SqueezeBERT model)"),hwo=l(),P2=a("li"),Tae=a("strong"),uwo=o("tapas"),pwo=o(" \u2014 "),uj=a("a"),_wo=o("TapasForSequenceClassification"),bwo=o(" (TAPAS model)"),vwo=l(),$2=a("li"),Fae=a("strong"),Two=o("transfo-xl"),Fwo=o(" \u2014 "),pj=a("a"),Cwo=o("TransfoXLForSequenceClassification"),Mwo=o(" (Transformer-XL model)"),Ewo=l(),I2=a("li"),Cae=a("strong"),ywo=o("xlm"),wwo=o(" \u2014 "),_j=a("a"),Awo=o("XLMForSequenceClassification"),Lwo=o(" (XLM model)"),Bwo=l(),j2=a("li"),Mae=a("strong"),xwo=o("xlm-roberta"),kwo=o(" \u2014 "),bj=a("a"),Rwo=o("XLMRobertaForSequenceClassification"),Swo=o(" (XLM-RoBERTa model)"),Pwo=l(),N2=a("li"),Eae=a("strong"),$wo=o("xlm-roberta-xl"),Iwo=o(" \u2014 "),vj=a("a"),jwo=o("XLMRobertaXLForSequenceClassification"),Nwo=o(" (XLM-RoBERTa-XL model)"),Dwo=l(),D2=a("li"),yae=a("strong"),qwo=o("xlnet"),Owo=o(" \u2014 "),Tj=a("a"),Gwo=o("XLNetForSequenceClassification"),Xwo=o(" (XLNet model)"),Vwo=l(),q2=a("li"),wae=a("strong"),zwo=o("yoso"),Wwo=o(" \u2014 "),Fj=a("a"),Qwo=o("YosoForSequenceClassification"),Hwo=o(" (YOSO model)"),Uwo=l(),O2=a("p"),Jwo=o("The model is set in evaluation mode by default using "),Aae=a("code"),Ywo=o("model.eval()"),Kwo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Lae=a("code"),Zwo=o("model.train()"),e6o=l(),Bae=a("p"),o6o=o("Examples:"),r6o=l(),m(Cw.$$.fragment),dke=l(),ud=a("h2"),G2=a("a"),xae=a("span"),m(Mw.$$.fragment),t6o=l(),kae=a("span"),a6o=o("AutoModelForMultipleChoice"),cke=l(),rr=a("div"),m(Ew.$$.fragment),s6o=l(),pd=a("p"),n6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Rae=a("code"),l6o=o("from_pretrained()"),i6o=o("class method or the "),Sae=a("code"),d6o=o("from_config()"),c6o=o(`class
method.`),m6o=l(),yw=a("p"),f6o=o("This class cannot be instantiated directly using "),Pae=a("code"),g6o=o("__init__()"),h6o=o(" (throws an error)."),u6o=l(),Jr=a("div"),m(ww.$$.fragment),p6o=l(),$ae=a("p"),_6o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),b6o=l(),_d=a("p"),v6o=o(`Note:
Loading a model from its configuration file does `),Iae=a("strong"),T6o=o("not"),F6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jae=a("code"),C6o=o("from_pretrained()"),M6o=o("to load the model weights."),E6o=l(),Nae=a("p"),y6o=o("Examples:"),w6o=l(),m(Aw.$$.fragment),A6o=l(),Ve=a("div"),m(Lw.$$.fragment),L6o=l(),Dae=a("p"),B6o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),x6o=l(),Ja=a("p"),k6o=o("The model class to instantiate is selected based on the "),qae=a("code"),R6o=o("model_type"),S6o=o(` property of the config object (either
passed as an argument or loaded from `),Oae=a("code"),P6o=o("pretrained_model_name_or_path"),$6o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gae=a("code"),I6o=o("pretrained_model_name_or_path"),j6o=o(":"),N6o=l(),O=a("ul"),X2=a("li"),Xae=a("strong"),D6o=o("albert"),q6o=o(" \u2014 "),Cj=a("a"),O6o=o("AlbertForMultipleChoice"),G6o=o(" (ALBERT model)"),X6o=l(),V2=a("li"),Vae=a("strong"),V6o=o("bert"),z6o=o(" \u2014 "),Mj=a("a"),W6o=o("BertForMultipleChoice"),Q6o=o(" (BERT model)"),H6o=l(),z2=a("li"),zae=a("strong"),U6o=o("big_bird"),J6o=o(" \u2014 "),Ej=a("a"),Y6o=o("BigBirdForMultipleChoice"),K6o=o(" (BigBird model)"),Z6o=l(),W2=a("li"),Wae=a("strong"),eAo=o("camembert"),oAo=o(" \u2014 "),yj=a("a"),rAo=o("CamembertForMultipleChoice"),tAo=o(" (CamemBERT model)"),aAo=l(),Q2=a("li"),Qae=a("strong"),sAo=o("canine"),nAo=o(" \u2014 "),wj=a("a"),lAo=o("CanineForMultipleChoice"),iAo=o(" (Canine model)"),dAo=l(),H2=a("li"),Hae=a("strong"),cAo=o("convbert"),mAo=o(" \u2014 "),Aj=a("a"),fAo=o("ConvBertForMultipleChoice"),gAo=o(" (ConvBERT model)"),hAo=l(),U2=a("li"),Uae=a("strong"),uAo=o("data2vec-text"),pAo=o(" \u2014 "),Lj=a("a"),_Ao=o("Data2VecTextForMultipleChoice"),bAo=o(" (Data2VecText model)"),vAo=l(),J2=a("li"),Jae=a("strong"),TAo=o("distilbert"),FAo=o(" \u2014 "),Bj=a("a"),CAo=o("DistilBertForMultipleChoice"),MAo=o(" (DistilBERT model)"),EAo=l(),Y2=a("li"),Yae=a("strong"),yAo=o("electra"),wAo=o(" \u2014 "),xj=a("a"),AAo=o("ElectraForMultipleChoice"),LAo=o(" (ELECTRA model)"),BAo=l(),K2=a("li"),Kae=a("strong"),xAo=o("flaubert"),kAo=o(" \u2014 "),kj=a("a"),RAo=o("FlaubertForMultipleChoice"),SAo=o(" (FlauBERT model)"),PAo=l(),Z2=a("li"),Zae=a("strong"),$Ao=o("fnet"),IAo=o(" \u2014 "),Rj=a("a"),jAo=o("FNetForMultipleChoice"),NAo=o(" (FNet model)"),DAo=l(),ev=a("li"),ese=a("strong"),qAo=o("funnel"),OAo=o(" \u2014 "),Sj=a("a"),GAo=o("FunnelForMultipleChoice"),XAo=o(" (Funnel Transformer model)"),VAo=l(),ov=a("li"),ose=a("strong"),zAo=o("ibert"),WAo=o(" \u2014 "),Pj=a("a"),QAo=o("IBertForMultipleChoice"),HAo=o(" (I-BERT model)"),UAo=l(),rv=a("li"),rse=a("strong"),JAo=o("longformer"),YAo=o(" \u2014 "),$j=a("a"),KAo=o("LongformerForMultipleChoice"),ZAo=o(" (Longformer model)"),e0o=l(),tv=a("li"),tse=a("strong"),o0o=o("megatron-bert"),r0o=o(" \u2014 "),Ij=a("a"),t0o=o("MegatronBertForMultipleChoice"),a0o=o(" (MegatronBert model)"),s0o=l(),av=a("li"),ase=a("strong"),n0o=o("mobilebert"),l0o=o(" \u2014 "),jj=a("a"),i0o=o("MobileBertForMultipleChoice"),d0o=o(" (MobileBERT model)"),c0o=l(),sv=a("li"),sse=a("strong"),m0o=o("mpnet"),f0o=o(" \u2014 "),Nj=a("a"),g0o=o("MPNetForMultipleChoice"),h0o=o(" (MPNet model)"),u0o=l(),nv=a("li"),nse=a("strong"),p0o=o("nystromformer"),_0o=o(" \u2014 "),Dj=a("a"),b0o=o("NystromformerForMultipleChoice"),v0o=o(" (Nystromformer model)"),T0o=l(),lv=a("li"),lse=a("strong"),F0o=o("qdqbert"),C0o=o(" \u2014 "),qj=a("a"),M0o=o("QDQBertForMultipleChoice"),E0o=o(" (QDQBert model)"),y0o=l(),iv=a("li"),ise=a("strong"),w0o=o("rembert"),A0o=o(" \u2014 "),Oj=a("a"),L0o=o("RemBertForMultipleChoice"),B0o=o(" (RemBERT model)"),x0o=l(),dv=a("li"),dse=a("strong"),k0o=o("roberta"),R0o=o(" \u2014 "),Gj=a("a"),S0o=o("RobertaForMultipleChoice"),P0o=o(" (RoBERTa model)"),$0o=l(),cv=a("li"),cse=a("strong"),I0o=o("roformer"),j0o=o(" \u2014 "),Xj=a("a"),N0o=o("RoFormerForMultipleChoice"),D0o=o(" (RoFormer model)"),q0o=l(),mv=a("li"),mse=a("strong"),O0o=o("squeezebert"),G0o=o(" \u2014 "),Vj=a("a"),X0o=o("SqueezeBertForMultipleChoice"),V0o=o(" (SqueezeBERT model)"),z0o=l(),fv=a("li"),fse=a("strong"),W0o=o("xlm"),Q0o=o(" \u2014 "),zj=a("a"),H0o=o("XLMForMultipleChoice"),U0o=o(" (XLM model)"),J0o=l(),gv=a("li"),gse=a("strong"),Y0o=o("xlm-roberta"),K0o=o(" \u2014 "),Wj=a("a"),Z0o=o("XLMRobertaForMultipleChoice"),eLo=o(" (XLM-RoBERTa model)"),oLo=l(),hv=a("li"),hse=a("strong"),rLo=o("xlm-roberta-xl"),tLo=o(" \u2014 "),Qj=a("a"),aLo=o("XLMRobertaXLForMultipleChoice"),sLo=o(" (XLM-RoBERTa-XL model)"),nLo=l(),uv=a("li"),use=a("strong"),lLo=o("xlnet"),iLo=o(" \u2014 "),Hj=a("a"),dLo=o("XLNetForMultipleChoice"),cLo=o(" (XLNet model)"),mLo=l(),pv=a("li"),pse=a("strong"),fLo=o("yoso"),gLo=o(" \u2014 "),Uj=a("a"),hLo=o("YosoForMultipleChoice"),uLo=o(" (YOSO model)"),pLo=l(),_v=a("p"),_Lo=o("The model is set in evaluation mode by default using "),_se=a("code"),bLo=o("model.eval()"),vLo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),bse=a("code"),TLo=o("model.train()"),FLo=l(),vse=a("p"),CLo=o("Examples:"),MLo=l(),m(Bw.$$.fragment),mke=l(),bd=a("h2"),bv=a("a"),Tse=a("span"),m(xw.$$.fragment),ELo=l(),Fse=a("span"),yLo=o("AutoModelForNextSentencePrediction"),fke=l(),tr=a("div"),m(kw.$$.fragment),wLo=l(),vd=a("p"),ALo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Cse=a("code"),LLo=o("from_pretrained()"),BLo=o("class method or the "),Mse=a("code"),xLo=o("from_config()"),kLo=o(`class
method.`),RLo=l(),Rw=a("p"),SLo=o("This class cannot be instantiated directly using "),Ese=a("code"),PLo=o("__init__()"),$Lo=o(" (throws an error)."),ILo=l(),Yr=a("div"),m(Sw.$$.fragment),jLo=l(),yse=a("p"),NLo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),DLo=l(),Td=a("p"),qLo=o(`Note:
Loading a model from its configuration file does `),wse=a("strong"),OLo=o("not"),GLo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ase=a("code"),XLo=o("from_pretrained()"),VLo=o("to load the model weights."),zLo=l(),Lse=a("p"),WLo=o("Examples:"),QLo=l(),m(Pw.$$.fragment),HLo=l(),ze=a("div"),m($w.$$.fragment),ULo=l(),Bse=a("p"),JLo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),YLo=l(),Ya=a("p"),KLo=o("The model class to instantiate is selected based on the "),xse=a("code"),ZLo=o("model_type"),e7o=o(` property of the config object (either
passed as an argument or loaded from `),kse=a("code"),o7o=o("pretrained_model_name_or_path"),r7o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rse=a("code"),t7o=o("pretrained_model_name_or_path"),a7o=o(":"),s7o=l(),da=a("ul"),vv=a("li"),Sse=a("strong"),n7o=o("bert"),l7o=o(" \u2014 "),Jj=a("a"),i7o=o("BertForNextSentencePrediction"),d7o=o(" (BERT model)"),c7o=l(),Tv=a("li"),Pse=a("strong"),m7o=o("fnet"),f7o=o(" \u2014 "),Yj=a("a"),g7o=o("FNetForNextSentencePrediction"),h7o=o(" (FNet model)"),u7o=l(),Fv=a("li"),$se=a("strong"),p7o=o("megatron-bert"),_7o=o(" \u2014 "),Kj=a("a"),b7o=o("MegatronBertForNextSentencePrediction"),v7o=o(" (MegatronBert model)"),T7o=l(),Cv=a("li"),Ise=a("strong"),F7o=o("mobilebert"),C7o=o(" \u2014 "),Zj=a("a"),M7o=o("MobileBertForNextSentencePrediction"),E7o=o(" (MobileBERT model)"),y7o=l(),Mv=a("li"),jse=a("strong"),w7o=o("qdqbert"),A7o=o(" \u2014 "),eN=a("a"),L7o=o("QDQBertForNextSentencePrediction"),B7o=o(" (QDQBert model)"),x7o=l(),Ev=a("p"),k7o=o("The model is set in evaluation mode by default using "),Nse=a("code"),R7o=o("model.eval()"),S7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dse=a("code"),P7o=o("model.train()"),$7o=l(),qse=a("p"),I7o=o("Examples:"),j7o=l(),m(Iw.$$.fragment),gke=l(),Fd=a("h2"),yv=a("a"),Ose=a("span"),m(jw.$$.fragment),N7o=l(),Gse=a("span"),D7o=o("AutoModelForTokenClassification"),hke=l(),ar=a("div"),m(Nw.$$.fragment),q7o=l(),Cd=a("p"),O7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Xse=a("code"),G7o=o("from_pretrained()"),X7o=o("class method or the "),Vse=a("code"),V7o=o("from_config()"),z7o=o(`class
method.`),W7o=l(),Dw=a("p"),Q7o=o("This class cannot be instantiated directly using "),zse=a("code"),H7o=o("__init__()"),U7o=o(" (throws an error)."),J7o=l(),Kr=a("div"),m(qw.$$.fragment),Y7o=l(),Wse=a("p"),K7o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Z7o=l(),Md=a("p"),e8o=o(`Note:
Loading a model from its configuration file does `),Qse=a("strong"),o8o=o("not"),r8o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hse=a("code"),t8o=o("from_pretrained()"),a8o=o("to load the model weights."),s8o=l(),Use=a("p"),n8o=o("Examples:"),l8o=l(),m(Ow.$$.fragment),i8o=l(),We=a("div"),m(Gw.$$.fragment),d8o=l(),Jse=a("p"),c8o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),m8o=l(),Ka=a("p"),f8o=o("The model class to instantiate is selected based on the "),Yse=a("code"),g8o=o("model_type"),h8o=o(` property of the config object (either
passed as an argument or loaded from `),Kse=a("code"),u8o=o("pretrained_model_name_or_path"),p8o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zse=a("code"),_8o=o("pretrained_model_name_or_path"),b8o=o(":"),v8o=l(),D=a("ul"),wv=a("li"),ene=a("strong"),T8o=o("albert"),F8o=o(" \u2014 "),oN=a("a"),C8o=o("AlbertForTokenClassification"),M8o=o(" (ALBERT model)"),E8o=l(),Av=a("li"),one=a("strong"),y8o=o("bert"),w8o=o(" \u2014 "),rN=a("a"),A8o=o("BertForTokenClassification"),L8o=o(" (BERT model)"),B8o=l(),Lv=a("li"),rne=a("strong"),x8o=o("big_bird"),k8o=o(" \u2014 "),tN=a("a"),R8o=o("BigBirdForTokenClassification"),S8o=o(" (BigBird model)"),P8o=l(),Bv=a("li"),tne=a("strong"),$8o=o("camembert"),I8o=o(" \u2014 "),aN=a("a"),j8o=o("CamembertForTokenClassification"),N8o=o(" (CamemBERT model)"),D8o=l(),xv=a("li"),ane=a("strong"),q8o=o("canine"),O8o=o(" \u2014 "),sN=a("a"),G8o=o("CanineForTokenClassification"),X8o=o(" (Canine model)"),V8o=l(),kv=a("li"),sne=a("strong"),z8o=o("convbert"),W8o=o(" \u2014 "),nN=a("a"),Q8o=o("ConvBertForTokenClassification"),H8o=o(" (ConvBERT model)"),U8o=l(),Rv=a("li"),nne=a("strong"),J8o=o("data2vec-text"),Y8o=o(" \u2014 "),lN=a("a"),K8o=o("Data2VecTextForTokenClassification"),Z8o=o(" (Data2VecText model)"),e9o=l(),Sv=a("li"),lne=a("strong"),o9o=o("deberta"),r9o=o(" \u2014 "),iN=a("a"),t9o=o("DebertaForTokenClassification"),a9o=o(" (DeBERTa model)"),s9o=l(),Pv=a("li"),ine=a("strong"),n9o=o("deberta-v2"),l9o=o(" \u2014 "),dN=a("a"),i9o=o("DebertaV2ForTokenClassification"),d9o=o(" (DeBERTa-v2 model)"),c9o=l(),$v=a("li"),dne=a("strong"),m9o=o("distilbert"),f9o=o(" \u2014 "),cN=a("a"),g9o=o("DistilBertForTokenClassification"),h9o=o(" (DistilBERT model)"),u9o=l(),Iv=a("li"),cne=a("strong"),p9o=o("electra"),_9o=o(" \u2014 "),mN=a("a"),b9o=o("ElectraForTokenClassification"),v9o=o(" (ELECTRA model)"),T9o=l(),jv=a("li"),mne=a("strong"),F9o=o("flaubert"),C9o=o(" \u2014 "),fN=a("a"),M9o=o("FlaubertForTokenClassification"),E9o=o(" (FlauBERT model)"),y9o=l(),Nv=a("li"),fne=a("strong"),w9o=o("fnet"),A9o=o(" \u2014 "),gN=a("a"),L9o=o("FNetForTokenClassification"),B9o=o(" (FNet model)"),x9o=l(),Dv=a("li"),gne=a("strong"),k9o=o("funnel"),R9o=o(" \u2014 "),hN=a("a"),S9o=o("FunnelForTokenClassification"),P9o=o(" (Funnel Transformer model)"),$9o=l(),qv=a("li"),hne=a("strong"),I9o=o("gpt2"),j9o=o(" \u2014 "),uN=a("a"),N9o=o("GPT2ForTokenClassification"),D9o=o(" (OpenAI GPT-2 model)"),q9o=l(),Ov=a("li"),une=a("strong"),O9o=o("ibert"),G9o=o(" \u2014 "),pN=a("a"),X9o=o("IBertForTokenClassification"),V9o=o(" (I-BERT model)"),z9o=l(),Gv=a("li"),pne=a("strong"),W9o=o("layoutlm"),Q9o=o(" \u2014 "),_N=a("a"),H9o=o("LayoutLMForTokenClassification"),U9o=o(" (LayoutLM model)"),J9o=l(),Xv=a("li"),_ne=a("strong"),Y9o=o("layoutlmv2"),K9o=o(" \u2014 "),bN=a("a"),Z9o=o("LayoutLMv2ForTokenClassification"),eBo=o(" (LayoutLMv2 model)"),oBo=l(),Vv=a("li"),bne=a("strong"),rBo=o("longformer"),tBo=o(" \u2014 "),vN=a("a"),aBo=o("LongformerForTokenClassification"),sBo=o(" (Longformer model)"),nBo=l(),zv=a("li"),vne=a("strong"),lBo=o("megatron-bert"),iBo=o(" \u2014 "),TN=a("a"),dBo=o("MegatronBertForTokenClassification"),cBo=o(" (MegatronBert model)"),mBo=l(),Wv=a("li"),Tne=a("strong"),fBo=o("mobilebert"),gBo=o(" \u2014 "),FN=a("a"),hBo=o("MobileBertForTokenClassification"),uBo=o(" (MobileBERT model)"),pBo=l(),Qv=a("li"),Fne=a("strong"),_Bo=o("mpnet"),bBo=o(" \u2014 "),CN=a("a"),vBo=o("MPNetForTokenClassification"),TBo=o(" (MPNet model)"),FBo=l(),Hv=a("li"),Cne=a("strong"),CBo=o("nystromformer"),MBo=o(" \u2014 "),MN=a("a"),EBo=o("NystromformerForTokenClassification"),yBo=o(" (Nystromformer model)"),wBo=l(),Uv=a("li"),Mne=a("strong"),ABo=o("qdqbert"),LBo=o(" \u2014 "),EN=a("a"),BBo=o("QDQBertForTokenClassification"),xBo=o(" (QDQBert model)"),kBo=l(),Jv=a("li"),Ene=a("strong"),RBo=o("rembert"),SBo=o(" \u2014 "),yN=a("a"),PBo=o("RemBertForTokenClassification"),$Bo=o(" (RemBERT model)"),IBo=l(),Yv=a("li"),yne=a("strong"),jBo=o("roberta"),NBo=o(" \u2014 "),wN=a("a"),DBo=o("RobertaForTokenClassification"),qBo=o(" (RoBERTa model)"),OBo=l(),Kv=a("li"),wne=a("strong"),GBo=o("roformer"),XBo=o(" \u2014 "),AN=a("a"),VBo=o("RoFormerForTokenClassification"),zBo=o(" (RoFormer model)"),WBo=l(),Zv=a("li"),Ane=a("strong"),QBo=o("squeezebert"),HBo=o(" \u2014 "),LN=a("a"),UBo=o("SqueezeBertForTokenClassification"),JBo=o(" (SqueezeBERT model)"),YBo=l(),eT=a("li"),Lne=a("strong"),KBo=o("xlm"),ZBo=o(" \u2014 "),BN=a("a"),exo=o("XLMForTokenClassification"),oxo=o(" (XLM model)"),rxo=l(),oT=a("li"),Bne=a("strong"),txo=o("xlm-roberta"),axo=o(" \u2014 "),xN=a("a"),sxo=o("XLMRobertaForTokenClassification"),nxo=o(" (XLM-RoBERTa model)"),lxo=l(),rT=a("li"),xne=a("strong"),ixo=o("xlm-roberta-xl"),dxo=o(" \u2014 "),kN=a("a"),cxo=o("XLMRobertaXLForTokenClassification"),mxo=o(" (XLM-RoBERTa-XL model)"),fxo=l(),tT=a("li"),kne=a("strong"),gxo=o("xlnet"),hxo=o(" \u2014 "),RN=a("a"),uxo=o("XLNetForTokenClassification"),pxo=o(" (XLNet model)"),_xo=l(),aT=a("li"),Rne=a("strong"),bxo=o("yoso"),vxo=o(" \u2014 "),SN=a("a"),Txo=o("YosoForTokenClassification"),Fxo=o(" (YOSO model)"),Cxo=l(),sT=a("p"),Mxo=o("The model is set in evaluation mode by default using "),Sne=a("code"),Exo=o("model.eval()"),yxo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pne=a("code"),wxo=o("model.train()"),Axo=l(),$ne=a("p"),Lxo=o("Examples:"),Bxo=l(),m(Xw.$$.fragment),uke=l(),Ed=a("h2"),nT=a("a"),Ine=a("span"),m(Vw.$$.fragment),xxo=l(),jne=a("span"),kxo=o("AutoModelForQuestionAnswering"),pke=l(),sr=a("div"),m(zw.$$.fragment),Rxo=l(),yd=a("p"),Sxo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Nne=a("code"),Pxo=o("from_pretrained()"),$xo=o("class method or the "),Dne=a("code"),Ixo=o("from_config()"),jxo=o(`class
method.`),Nxo=l(),Ww=a("p"),Dxo=o("This class cannot be instantiated directly using "),qne=a("code"),qxo=o("__init__()"),Oxo=o(" (throws an error)."),Gxo=l(),Zr=a("div"),m(Qw.$$.fragment),Xxo=l(),One=a("p"),Vxo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),zxo=l(),wd=a("p"),Wxo=o(`Note:
Loading a model from its configuration file does `),Gne=a("strong"),Qxo=o("not"),Hxo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xne=a("code"),Uxo=o("from_pretrained()"),Jxo=o("to load the model weights."),Yxo=l(),Vne=a("p"),Kxo=o("Examples:"),Zxo=l(),m(Hw.$$.fragment),eko=l(),Qe=a("div"),m(Uw.$$.fragment),oko=l(),zne=a("p"),rko=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),tko=l(),Za=a("p"),ako=o("The model class to instantiate is selected based on the "),Wne=a("code"),sko=o("model_type"),nko=o(` property of the config object (either
passed as an argument or loaded from `),Qne=a("code"),lko=o("pretrained_model_name_or_path"),iko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hne=a("code"),dko=o("pretrained_model_name_or_path"),cko=o(":"),mko=l(),R=a("ul"),lT=a("li"),Une=a("strong"),fko=o("albert"),gko=o(" \u2014 "),PN=a("a"),hko=o("AlbertForQuestionAnswering"),uko=o(" (ALBERT model)"),pko=l(),iT=a("li"),Jne=a("strong"),_ko=o("bart"),bko=o(" \u2014 "),$N=a("a"),vko=o("BartForQuestionAnswering"),Tko=o(" (BART model)"),Fko=l(),dT=a("li"),Yne=a("strong"),Cko=o("bert"),Mko=o(" \u2014 "),IN=a("a"),Eko=o("BertForQuestionAnswering"),yko=o(" (BERT model)"),wko=l(),cT=a("li"),Kne=a("strong"),Ako=o("big_bird"),Lko=o(" \u2014 "),jN=a("a"),Bko=o("BigBirdForQuestionAnswering"),xko=o(" (BigBird model)"),kko=l(),mT=a("li"),Zne=a("strong"),Rko=o("bigbird_pegasus"),Sko=o(" \u2014 "),NN=a("a"),Pko=o("BigBirdPegasusForQuestionAnswering"),$ko=o(" (BigBirdPegasus model)"),Iko=l(),fT=a("li"),ele=a("strong"),jko=o("camembert"),Nko=o(" \u2014 "),DN=a("a"),Dko=o("CamembertForQuestionAnswering"),qko=o(" (CamemBERT model)"),Oko=l(),gT=a("li"),ole=a("strong"),Gko=o("canine"),Xko=o(" \u2014 "),qN=a("a"),Vko=o("CanineForQuestionAnswering"),zko=o(" (Canine model)"),Wko=l(),hT=a("li"),rle=a("strong"),Qko=o("convbert"),Hko=o(" \u2014 "),ON=a("a"),Uko=o("ConvBertForQuestionAnswering"),Jko=o(" (ConvBERT model)"),Yko=l(),uT=a("li"),tle=a("strong"),Kko=o("data2vec-text"),Zko=o(" \u2014 "),GN=a("a"),eRo=o("Data2VecTextForQuestionAnswering"),oRo=o(" (Data2VecText model)"),rRo=l(),pT=a("li"),ale=a("strong"),tRo=o("deberta"),aRo=o(" \u2014 "),XN=a("a"),sRo=o("DebertaForQuestionAnswering"),nRo=o(" (DeBERTa model)"),lRo=l(),_T=a("li"),sle=a("strong"),iRo=o("deberta-v2"),dRo=o(" \u2014 "),VN=a("a"),cRo=o("DebertaV2ForQuestionAnswering"),mRo=o(" (DeBERTa-v2 model)"),fRo=l(),bT=a("li"),nle=a("strong"),gRo=o("distilbert"),hRo=o(" \u2014 "),zN=a("a"),uRo=o("DistilBertForQuestionAnswering"),pRo=o(" (DistilBERT model)"),_Ro=l(),vT=a("li"),lle=a("strong"),bRo=o("electra"),vRo=o(" \u2014 "),WN=a("a"),TRo=o("ElectraForQuestionAnswering"),FRo=o(" (ELECTRA model)"),CRo=l(),TT=a("li"),ile=a("strong"),MRo=o("flaubert"),ERo=o(" \u2014 "),QN=a("a"),yRo=o("FlaubertForQuestionAnsweringSimple"),wRo=o(" (FlauBERT model)"),ARo=l(),FT=a("li"),dle=a("strong"),LRo=o("fnet"),BRo=o(" \u2014 "),HN=a("a"),xRo=o("FNetForQuestionAnswering"),kRo=o(" (FNet model)"),RRo=l(),CT=a("li"),cle=a("strong"),SRo=o("funnel"),PRo=o(" \u2014 "),UN=a("a"),$Ro=o("FunnelForQuestionAnswering"),IRo=o(" (Funnel Transformer model)"),jRo=l(),MT=a("li"),mle=a("strong"),NRo=o("gptj"),DRo=o(" \u2014 "),JN=a("a"),qRo=o("GPTJForQuestionAnswering"),ORo=o(" (GPT-J model)"),GRo=l(),ET=a("li"),fle=a("strong"),XRo=o("ibert"),VRo=o(" \u2014 "),YN=a("a"),zRo=o("IBertForQuestionAnswering"),WRo=o(" (I-BERT model)"),QRo=l(),yT=a("li"),gle=a("strong"),HRo=o("layoutlmv2"),URo=o(" \u2014 "),KN=a("a"),JRo=o("LayoutLMv2ForQuestionAnswering"),YRo=o(" (LayoutLMv2 model)"),KRo=l(),wT=a("li"),hle=a("strong"),ZRo=o("led"),eSo=o(" \u2014 "),ZN=a("a"),oSo=o("LEDForQuestionAnswering"),rSo=o(" (LED model)"),tSo=l(),AT=a("li"),ule=a("strong"),aSo=o("longformer"),sSo=o(" \u2014 "),eD=a("a"),nSo=o("LongformerForQuestionAnswering"),lSo=o(" (Longformer model)"),iSo=l(),LT=a("li"),ple=a("strong"),dSo=o("lxmert"),cSo=o(" \u2014 "),oD=a("a"),mSo=o("LxmertForQuestionAnswering"),fSo=o(" (LXMERT model)"),gSo=l(),BT=a("li"),_le=a("strong"),hSo=o("mbart"),uSo=o(" \u2014 "),rD=a("a"),pSo=o("MBartForQuestionAnswering"),_So=o(" (mBART model)"),bSo=l(),xT=a("li"),ble=a("strong"),vSo=o("megatron-bert"),TSo=o(" \u2014 "),tD=a("a"),FSo=o("MegatronBertForQuestionAnswering"),CSo=o(" (MegatronBert model)"),MSo=l(),kT=a("li"),vle=a("strong"),ESo=o("mobilebert"),ySo=o(" \u2014 "),aD=a("a"),wSo=o("MobileBertForQuestionAnswering"),ASo=o(" (MobileBERT model)"),LSo=l(),RT=a("li"),Tle=a("strong"),BSo=o("mpnet"),xSo=o(" \u2014 "),sD=a("a"),kSo=o("MPNetForQuestionAnswering"),RSo=o(" (MPNet model)"),SSo=l(),ST=a("li"),Fle=a("strong"),PSo=o("nystromformer"),$So=o(" \u2014 "),nD=a("a"),ISo=o("NystromformerForQuestionAnswering"),jSo=o(" (Nystromformer model)"),NSo=l(),PT=a("li"),Cle=a("strong"),DSo=o("qdqbert"),qSo=o(" \u2014 "),lD=a("a"),OSo=o("QDQBertForQuestionAnswering"),GSo=o(" (QDQBert model)"),XSo=l(),$T=a("li"),Mle=a("strong"),VSo=o("reformer"),zSo=o(" \u2014 "),iD=a("a"),WSo=o("ReformerForQuestionAnswering"),QSo=o(" (Reformer model)"),HSo=l(),IT=a("li"),Ele=a("strong"),USo=o("rembert"),JSo=o(" \u2014 "),dD=a("a"),YSo=o("RemBertForQuestionAnswering"),KSo=o(" (RemBERT model)"),ZSo=l(),jT=a("li"),yle=a("strong"),ePo=o("roberta"),oPo=o(" \u2014 "),cD=a("a"),rPo=o("RobertaForQuestionAnswering"),tPo=o(" (RoBERTa model)"),aPo=l(),NT=a("li"),wle=a("strong"),sPo=o("roformer"),nPo=o(" \u2014 "),mD=a("a"),lPo=o("RoFormerForQuestionAnswering"),iPo=o(" (RoFormer model)"),dPo=l(),DT=a("li"),Ale=a("strong"),cPo=o("splinter"),mPo=o(" \u2014 "),fD=a("a"),fPo=o("SplinterForQuestionAnswering"),gPo=o(" (Splinter model)"),hPo=l(),qT=a("li"),Lle=a("strong"),uPo=o("squeezebert"),pPo=o(" \u2014 "),gD=a("a"),_Po=o("SqueezeBertForQuestionAnswering"),bPo=o(" (SqueezeBERT model)"),vPo=l(),OT=a("li"),Ble=a("strong"),TPo=o("xlm"),FPo=o(" \u2014 "),hD=a("a"),CPo=o("XLMForQuestionAnsweringSimple"),MPo=o(" (XLM model)"),EPo=l(),GT=a("li"),xle=a("strong"),yPo=o("xlm-roberta"),wPo=o(" \u2014 "),uD=a("a"),APo=o("XLMRobertaForQuestionAnswering"),LPo=o(" (XLM-RoBERTa model)"),BPo=l(),XT=a("li"),kle=a("strong"),xPo=o("xlm-roberta-xl"),kPo=o(" \u2014 "),pD=a("a"),RPo=o("XLMRobertaXLForQuestionAnswering"),SPo=o(" (XLM-RoBERTa-XL model)"),PPo=l(),VT=a("li"),Rle=a("strong"),$Po=o("xlnet"),IPo=o(" \u2014 "),_D=a("a"),jPo=o("XLNetForQuestionAnsweringSimple"),NPo=o(" (XLNet model)"),DPo=l(),zT=a("li"),Sle=a("strong"),qPo=o("yoso"),OPo=o(" \u2014 "),bD=a("a"),GPo=o("YosoForQuestionAnswering"),XPo=o(" (YOSO model)"),VPo=l(),WT=a("p"),zPo=o("The model is set in evaluation mode by default using "),Ple=a("code"),WPo=o("model.eval()"),QPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$le=a("code"),HPo=o("model.train()"),UPo=l(),Ile=a("p"),JPo=o("Examples:"),YPo=l(),m(Jw.$$.fragment),_ke=l(),Ad=a("h2"),QT=a("a"),jle=a("span"),m(Yw.$$.fragment),KPo=l(),Nle=a("span"),ZPo=o("AutoModelForTableQuestionAnswering"),bke=l(),nr=a("div"),m(Kw.$$.fragment),e$o=l(),Ld=a("p"),o$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Dle=a("code"),r$o=o("from_pretrained()"),t$o=o("class method or the "),qle=a("code"),a$o=o("from_config()"),s$o=o(`class
method.`),n$o=l(),Zw=a("p"),l$o=o("This class cannot be instantiated directly using "),Ole=a("code"),i$o=o("__init__()"),d$o=o(" (throws an error)."),c$o=l(),et=a("div"),m(e6.$$.fragment),m$o=l(),Gle=a("p"),f$o=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),g$o=l(),Bd=a("p"),h$o=o(`Note:
Loading a model from its configuration file does `),Xle=a("strong"),u$o=o("not"),p$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vle=a("code"),_$o=o("from_pretrained()"),b$o=o("to load the model weights."),v$o=l(),zle=a("p"),T$o=o("Examples:"),F$o=l(),m(o6.$$.fragment),C$o=l(),He=a("div"),m(r6.$$.fragment),M$o=l(),Wle=a("p"),E$o=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),y$o=l(),es=a("p"),w$o=o("The model class to instantiate is selected based on the "),Qle=a("code"),A$o=o("model_type"),L$o=o(` property of the config object (either
passed as an argument or loaded from `),Hle=a("code"),B$o=o("pretrained_model_name_or_path"),x$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ule=a("code"),k$o=o("pretrained_model_name_or_path"),R$o=o(":"),S$o=l(),Jle=a("ul"),HT=a("li"),Yle=a("strong"),P$o=o("tapas"),$$o=o(" \u2014 "),vD=a("a"),I$o=o("TapasForQuestionAnswering"),j$o=o(" (TAPAS model)"),N$o=l(),UT=a("p"),D$o=o("The model is set in evaluation mode by default using "),Kle=a("code"),q$o=o("model.eval()"),O$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zle=a("code"),G$o=o("model.train()"),X$o=l(),eie=a("p"),V$o=o("Examples:"),z$o=l(),m(t6.$$.fragment),vke=l(),xd=a("h2"),JT=a("a"),oie=a("span"),m(a6.$$.fragment),W$o=l(),rie=a("span"),Q$o=o("AutoModelForImageClassification"),Tke=l(),lr=a("div"),m(s6.$$.fragment),H$o=l(),kd=a("p"),U$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),tie=a("code"),J$o=o("from_pretrained()"),Y$o=o("class method or the "),aie=a("code"),K$o=o("from_config()"),Z$o=o(`class
method.`),eIo=l(),n6=a("p"),oIo=o("This class cannot be instantiated directly using "),sie=a("code"),rIo=o("__init__()"),tIo=o(" (throws an error)."),aIo=l(),ot=a("div"),m(l6.$$.fragment),sIo=l(),nie=a("p"),nIo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),lIo=l(),Rd=a("p"),iIo=o(`Note:
Loading a model from its configuration file does `),lie=a("strong"),dIo=o("not"),cIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),iie=a("code"),mIo=o("from_pretrained()"),fIo=o("to load the model weights."),gIo=l(),die=a("p"),hIo=o("Examples:"),uIo=l(),m(i6.$$.fragment),pIo=l(),Ue=a("div"),m(d6.$$.fragment),_Io=l(),cie=a("p"),bIo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),vIo=l(),os=a("p"),TIo=o("The model class to instantiate is selected based on the "),mie=a("code"),FIo=o("model_type"),CIo=o(` property of the config object (either
passed as an argument or loaded from `),fie=a("code"),MIo=o("pretrained_model_name_or_path"),EIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gie=a("code"),yIo=o("pretrained_model_name_or_path"),wIo=o(":"),AIo=l(),fe=a("ul"),YT=a("li"),hie=a("strong"),LIo=o("beit"),BIo=o(" \u2014 "),TD=a("a"),xIo=o("BeitForImageClassification"),kIo=o(" (BEiT model)"),RIo=l(),KT=a("li"),uie=a("strong"),SIo=o("convnext"),PIo=o(" \u2014 "),FD=a("a"),$Io=o("ConvNextForImageClassification"),IIo=o(" (ConvNext model)"),jIo=l(),qn=a("li"),pie=a("strong"),NIo=o("deit"),DIo=o(" \u2014 "),CD=a("a"),qIo=o("DeiTForImageClassification"),OIo=o(" or "),MD=a("a"),GIo=o("DeiTForImageClassificationWithTeacher"),XIo=o(" (DeiT model)"),VIo=l(),ZT=a("li"),_ie=a("strong"),zIo=o("imagegpt"),WIo=o(" \u2014 "),ED=a("a"),QIo=o("ImageGPTForImageClassification"),HIo=o(" (ImageGPT model)"),UIo=l(),fa=a("li"),bie=a("strong"),JIo=o("perceiver"),YIo=o(" \u2014 "),yD=a("a"),KIo=o("PerceiverForImageClassificationLearned"),ZIo=o(" or "),wD=a("a"),ejo=o("PerceiverForImageClassificationFourier"),ojo=o(" or "),AD=a("a"),rjo=o("PerceiverForImageClassificationConvProcessing"),tjo=o(" (Perceiver model)"),ajo=l(),e1=a("li"),vie=a("strong"),sjo=o("poolformer"),njo=o(" \u2014 "),LD=a("a"),ljo=o("PoolFormerForImageClassification"),ijo=o(" (PoolFormer model)"),djo=l(),o1=a("li"),Tie=a("strong"),cjo=o("resnet"),mjo=o(" \u2014 "),BD=a("a"),fjo=o("ResNetForImageClassification"),gjo=o(" (ResNet model)"),hjo=l(),r1=a("li"),Fie=a("strong"),ujo=o("segformer"),pjo=o(" \u2014 "),xD=a("a"),_jo=o("SegformerForImageClassification"),bjo=o(" (SegFormer model)"),vjo=l(),t1=a("li"),Cie=a("strong"),Tjo=o("swin"),Fjo=o(" \u2014 "),kD=a("a"),Cjo=o("SwinForImageClassification"),Mjo=o(" (Swin model)"),Ejo=l(),a1=a("li"),Mie=a("strong"),yjo=o("van"),wjo=o(" \u2014 "),RD=a("a"),Ajo=o("VanForImageClassification"),Ljo=o(" (VAN model)"),Bjo=l(),s1=a("li"),Eie=a("strong"),xjo=o("vit"),kjo=o(" \u2014 "),SD=a("a"),Rjo=o("ViTForImageClassification"),Sjo=o(" (ViT model)"),Pjo=l(),n1=a("p"),$jo=o("The model is set in evaluation mode by default using "),yie=a("code"),Ijo=o("model.eval()"),jjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wie=a("code"),Njo=o("model.train()"),Djo=l(),Aie=a("p"),qjo=o("Examples:"),Ojo=l(),m(c6.$$.fragment),Fke=l(),Sd=a("h2"),l1=a("a"),Lie=a("span"),m(m6.$$.fragment),Gjo=l(),Bie=a("span"),Xjo=o("AutoModelForVision2Seq"),Cke=l(),ir=a("div"),m(f6.$$.fragment),Vjo=l(),Pd=a("p"),zjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),xie=a("code"),Wjo=o("from_pretrained()"),Qjo=o("class method or the "),kie=a("code"),Hjo=o("from_config()"),Ujo=o(`class
method.`),Jjo=l(),g6=a("p"),Yjo=o("This class cannot be instantiated directly using "),Rie=a("code"),Kjo=o("__init__()"),Zjo=o(" (throws an error)."),eNo=l(),rt=a("div"),m(h6.$$.fragment),oNo=l(),Sie=a("p"),rNo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),tNo=l(),$d=a("p"),aNo=o(`Note:
Loading a model from its configuration file does `),Pie=a("strong"),sNo=o("not"),nNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ie=a("code"),lNo=o("from_pretrained()"),iNo=o("to load the model weights."),dNo=l(),Iie=a("p"),cNo=o("Examples:"),mNo=l(),m(u6.$$.fragment),fNo=l(),Je=a("div"),m(p6.$$.fragment),gNo=l(),jie=a("p"),hNo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),uNo=l(),rs=a("p"),pNo=o("The model class to instantiate is selected based on the "),Nie=a("code"),_No=o("model_type"),bNo=o(` property of the config object (either
passed as an argument or loaded from `),Die=a("code"),vNo=o("pretrained_model_name_or_path"),TNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qie=a("code"),FNo=o("pretrained_model_name_or_path"),CNo=o(":"),MNo=l(),Oie=a("ul"),i1=a("li"),Gie=a("strong"),ENo=o("vision-encoder-decoder"),yNo=o(" \u2014 "),PD=a("a"),wNo=o("VisionEncoderDecoderModel"),ANo=o(" (Vision Encoder decoder model)"),LNo=l(),d1=a("p"),BNo=o("The model is set in evaluation mode by default using "),Xie=a("code"),xNo=o("model.eval()"),kNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Vie=a("code"),RNo=o("model.train()"),SNo=l(),zie=a("p"),PNo=o("Examples:"),$No=l(),m(_6.$$.fragment),Mke=l(),Id=a("h2"),c1=a("a"),Wie=a("span"),m(b6.$$.fragment),INo=l(),Qie=a("span"),jNo=o("AutoModelForAudioClassification"),Eke=l(),dr=a("div"),m(v6.$$.fragment),NNo=l(),jd=a("p"),DNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Hie=a("code"),qNo=o("from_pretrained()"),ONo=o("class method or the "),Uie=a("code"),GNo=o("from_config()"),XNo=o(`class
method.`),VNo=l(),T6=a("p"),zNo=o("This class cannot be instantiated directly using "),Jie=a("code"),WNo=o("__init__()"),QNo=o(" (throws an error)."),HNo=l(),tt=a("div"),m(F6.$$.fragment),UNo=l(),Yie=a("p"),JNo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),YNo=l(),Nd=a("p"),KNo=o(`Note:
Loading a model from its configuration file does `),Kie=a("strong"),ZNo=o("not"),eDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zie=a("code"),oDo=o("from_pretrained()"),rDo=o("to load the model weights."),tDo=l(),ede=a("p"),aDo=o("Examples:"),sDo=l(),m(C6.$$.fragment),nDo=l(),Ye=a("div"),m(M6.$$.fragment),lDo=l(),ode=a("p"),iDo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),dDo=l(),ts=a("p"),cDo=o("The model class to instantiate is selected based on the "),rde=a("code"),mDo=o("model_type"),fDo=o(` property of the config object (either
passed as an argument or loaded from `),tde=a("code"),gDo=o("pretrained_model_name_or_path"),hDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ade=a("code"),uDo=o("pretrained_model_name_or_path"),pDo=o(":"),_Do=l(),ke=a("ul"),m1=a("li"),sde=a("strong"),bDo=o("data2vec-audio"),vDo=o(" \u2014 "),$D=a("a"),TDo=o("Data2VecAudioForSequenceClassification"),FDo=o(" (Data2VecAudio model)"),CDo=l(),f1=a("li"),nde=a("strong"),MDo=o("hubert"),EDo=o(" \u2014 "),ID=a("a"),yDo=o("HubertForSequenceClassification"),wDo=o(" (Hubert model)"),ADo=l(),g1=a("li"),lde=a("strong"),LDo=o("sew"),BDo=o(" \u2014 "),jD=a("a"),xDo=o("SEWForSequenceClassification"),kDo=o(" (SEW model)"),RDo=l(),h1=a("li"),ide=a("strong"),SDo=o("sew-d"),PDo=o(" \u2014 "),ND=a("a"),$Do=o("SEWDForSequenceClassification"),IDo=o(" (SEW-D model)"),jDo=l(),u1=a("li"),dde=a("strong"),NDo=o("unispeech"),DDo=o(" \u2014 "),DD=a("a"),qDo=o("UniSpeechForSequenceClassification"),ODo=o(" (UniSpeech model)"),GDo=l(),p1=a("li"),cde=a("strong"),XDo=o("unispeech-sat"),VDo=o(" \u2014 "),qD=a("a"),zDo=o("UniSpeechSatForSequenceClassification"),WDo=o(" (UniSpeechSat model)"),QDo=l(),_1=a("li"),mde=a("strong"),HDo=o("wav2vec2"),UDo=o(" \u2014 "),OD=a("a"),JDo=o("Wav2Vec2ForSequenceClassification"),YDo=o(" (Wav2Vec2 model)"),KDo=l(),b1=a("li"),fde=a("strong"),ZDo=o("wavlm"),eqo=o(" \u2014 "),GD=a("a"),oqo=o("WavLMForSequenceClassification"),rqo=o(" (WavLM model)"),tqo=l(),v1=a("p"),aqo=o("The model is set in evaluation mode by default using "),gde=a("code"),sqo=o("model.eval()"),nqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hde=a("code"),lqo=o("model.train()"),iqo=l(),ude=a("p"),dqo=o("Examples:"),cqo=l(),m(E6.$$.fragment),yke=l(),Dd=a("h2"),T1=a("a"),pde=a("span"),m(y6.$$.fragment),mqo=l(),_de=a("span"),fqo=o("AutoModelForAudioFrameClassification"),wke=l(),cr=a("div"),m(w6.$$.fragment),gqo=l(),qd=a("p"),hqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),bde=a("code"),uqo=o("from_pretrained()"),pqo=o("class method or the "),vde=a("code"),_qo=o("from_config()"),bqo=o(`class
method.`),vqo=l(),A6=a("p"),Tqo=o("This class cannot be instantiated directly using "),Tde=a("code"),Fqo=o("__init__()"),Cqo=o(" (throws an error)."),Mqo=l(),at=a("div"),m(L6.$$.fragment),Eqo=l(),Fde=a("p"),yqo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),wqo=l(),Od=a("p"),Aqo=o(`Note:
Loading a model from its configuration file does `),Cde=a("strong"),Lqo=o("not"),Bqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mde=a("code"),xqo=o("from_pretrained()"),kqo=o("to load the model weights."),Rqo=l(),Ede=a("p"),Sqo=o("Examples:"),Pqo=l(),m(B6.$$.fragment),$qo=l(),Ke=a("div"),m(x6.$$.fragment),Iqo=l(),yde=a("p"),jqo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),Nqo=l(),as=a("p"),Dqo=o("The model class to instantiate is selected based on the "),wde=a("code"),qqo=o("model_type"),Oqo=o(` property of the config object (either
passed as an argument or loaded from `),Ade=a("code"),Gqo=o("pretrained_model_name_or_path"),Xqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lde=a("code"),Vqo=o("pretrained_model_name_or_path"),zqo=o(":"),Wqo=l(),ss=a("ul"),F1=a("li"),Bde=a("strong"),Qqo=o("data2vec-audio"),Hqo=o(" \u2014 "),XD=a("a"),Uqo=o("Data2VecAudioForAudioFrameClassification"),Jqo=o(" (Data2VecAudio model)"),Yqo=l(),C1=a("li"),xde=a("strong"),Kqo=o("unispeech-sat"),Zqo=o(" \u2014 "),VD=a("a"),eOo=o("UniSpeechSatForAudioFrameClassification"),oOo=o(" (UniSpeechSat model)"),rOo=l(),M1=a("li"),kde=a("strong"),tOo=o("wav2vec2"),aOo=o(" \u2014 "),zD=a("a"),sOo=o("Wav2Vec2ForAudioFrameClassification"),nOo=o(" (Wav2Vec2 model)"),lOo=l(),E1=a("li"),Rde=a("strong"),iOo=o("wavlm"),dOo=o(" \u2014 "),WD=a("a"),cOo=o("WavLMForAudioFrameClassification"),mOo=o(" (WavLM model)"),fOo=l(),y1=a("p"),gOo=o("The model is set in evaluation mode by default using "),Sde=a("code"),hOo=o("model.eval()"),uOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pde=a("code"),pOo=o("model.train()"),_Oo=l(),$de=a("p"),bOo=o("Examples:"),vOo=l(),m(k6.$$.fragment),Ake=l(),Gd=a("h2"),w1=a("a"),Ide=a("span"),m(R6.$$.fragment),TOo=l(),jde=a("span"),FOo=o("AutoModelForCTC"),Lke=l(),mr=a("div"),m(S6.$$.fragment),COo=l(),Xd=a("p"),MOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Nde=a("code"),EOo=o("from_pretrained()"),yOo=o("class method or the "),Dde=a("code"),wOo=o("from_config()"),AOo=o(`class
method.`),LOo=l(),P6=a("p"),BOo=o("This class cannot be instantiated directly using "),qde=a("code"),xOo=o("__init__()"),kOo=o(" (throws an error)."),ROo=l(),st=a("div"),m($6.$$.fragment),SOo=l(),Ode=a("p"),POo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),$Oo=l(),Vd=a("p"),IOo=o(`Note:
Loading a model from its configuration file does `),Gde=a("strong"),jOo=o("not"),NOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xde=a("code"),DOo=o("from_pretrained()"),qOo=o("to load the model weights."),OOo=l(),Vde=a("p"),GOo=o("Examples:"),XOo=l(),m(I6.$$.fragment),VOo=l(),Ze=a("div"),m(j6.$$.fragment),zOo=l(),zde=a("p"),WOo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),QOo=l(),ns=a("p"),HOo=o("The model class to instantiate is selected based on the "),Wde=a("code"),UOo=o("model_type"),JOo=o(` property of the config object (either
passed as an argument or loaded from `),Qde=a("code"),YOo=o("pretrained_model_name_or_path"),KOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hde=a("code"),ZOo=o("pretrained_model_name_or_path"),eGo=o(":"),oGo=l(),Re=a("ul"),A1=a("li"),Ude=a("strong"),rGo=o("data2vec-audio"),tGo=o(" \u2014 "),QD=a("a"),aGo=o("Data2VecAudioForCTC"),sGo=o(" (Data2VecAudio model)"),nGo=l(),L1=a("li"),Jde=a("strong"),lGo=o("hubert"),iGo=o(" \u2014 "),HD=a("a"),dGo=o("HubertForCTC"),cGo=o(" (Hubert model)"),mGo=l(),B1=a("li"),Yde=a("strong"),fGo=o("sew"),gGo=o(" \u2014 "),UD=a("a"),hGo=o("SEWForCTC"),uGo=o(" (SEW model)"),pGo=l(),x1=a("li"),Kde=a("strong"),_Go=o("sew-d"),bGo=o(" \u2014 "),JD=a("a"),vGo=o("SEWDForCTC"),TGo=o(" (SEW-D model)"),FGo=l(),k1=a("li"),Zde=a("strong"),CGo=o("unispeech"),MGo=o(" \u2014 "),YD=a("a"),EGo=o("UniSpeechForCTC"),yGo=o(" (UniSpeech model)"),wGo=l(),R1=a("li"),ece=a("strong"),AGo=o("unispeech-sat"),LGo=o(" \u2014 "),KD=a("a"),BGo=o("UniSpeechSatForCTC"),xGo=o(" (UniSpeechSat model)"),kGo=l(),S1=a("li"),oce=a("strong"),RGo=o("wav2vec2"),SGo=o(" \u2014 "),ZD=a("a"),PGo=o("Wav2Vec2ForCTC"),$Go=o(" (Wav2Vec2 model)"),IGo=l(),P1=a("li"),rce=a("strong"),jGo=o("wavlm"),NGo=o(" \u2014 "),eq=a("a"),DGo=o("WavLMForCTC"),qGo=o(" (WavLM model)"),OGo=l(),$1=a("p"),GGo=o("The model is set in evaluation mode by default using "),tce=a("code"),XGo=o("model.eval()"),VGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ace=a("code"),zGo=o("model.train()"),WGo=l(),sce=a("p"),QGo=o("Examples:"),HGo=l(),m(N6.$$.fragment),Bke=l(),zd=a("h2"),I1=a("a"),nce=a("span"),m(D6.$$.fragment),UGo=l(),lce=a("span"),JGo=o("AutoModelForSpeechSeq2Seq"),xke=l(),fr=a("div"),m(q6.$$.fragment),YGo=l(),Wd=a("p"),KGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),ice=a("code"),ZGo=o("from_pretrained()"),eXo=o("class method or the "),dce=a("code"),oXo=o("from_config()"),rXo=o(`class
method.`),tXo=l(),O6=a("p"),aXo=o("This class cannot be instantiated directly using "),cce=a("code"),sXo=o("__init__()"),nXo=o(" (throws an error)."),lXo=l(),nt=a("div"),m(G6.$$.fragment),iXo=l(),mce=a("p"),dXo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),cXo=l(),Qd=a("p"),mXo=o(`Note:
Loading a model from its configuration file does `),fce=a("strong"),fXo=o("not"),gXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gce=a("code"),hXo=o("from_pretrained()"),uXo=o("to load the model weights."),pXo=l(),hce=a("p"),_Xo=o("Examples:"),bXo=l(),m(X6.$$.fragment),vXo=l(),eo=a("div"),m(V6.$$.fragment),TXo=l(),uce=a("p"),FXo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),CXo=l(),ls=a("p"),MXo=o("The model class to instantiate is selected based on the "),pce=a("code"),EXo=o("model_type"),yXo=o(` property of the config object (either
passed as an argument or loaded from `),_ce=a("code"),wXo=o("pretrained_model_name_or_path"),AXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bce=a("code"),LXo=o("pretrained_model_name_or_path"),BXo=o(":"),xXo=l(),z6=a("ul"),j1=a("li"),vce=a("strong"),kXo=o("speech-encoder-decoder"),RXo=o(" \u2014 "),oq=a("a"),SXo=o("SpeechEncoderDecoderModel"),PXo=o(" (Speech Encoder decoder model)"),$Xo=l(),N1=a("li"),Tce=a("strong"),IXo=o("speech_to_text"),jXo=o(" \u2014 "),rq=a("a"),NXo=o("Speech2TextForConditionalGeneration"),DXo=o(" (Speech2Text model)"),qXo=l(),D1=a("p"),OXo=o("The model is set in evaluation mode by default using "),Fce=a("code"),GXo=o("model.eval()"),XXo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Cce=a("code"),VXo=o("model.train()"),zXo=l(),Mce=a("p"),WXo=o("Examples:"),QXo=l(),m(W6.$$.fragment),kke=l(),Hd=a("h2"),q1=a("a"),Ece=a("span"),m(Q6.$$.fragment),HXo=l(),yce=a("span"),UXo=o("AutoModelForAudioXVector"),Rke=l(),gr=a("div"),m(H6.$$.fragment),JXo=l(),Ud=a("p"),YXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),wce=a("code"),KXo=o("from_pretrained()"),ZXo=o("class method or the "),Ace=a("code"),eVo=o("from_config()"),oVo=o(`class
method.`),rVo=l(),U6=a("p"),tVo=o("This class cannot be instantiated directly using "),Lce=a("code"),aVo=o("__init__()"),sVo=o(" (throws an error)."),nVo=l(),lt=a("div"),m(J6.$$.fragment),lVo=l(),Bce=a("p"),iVo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),dVo=l(),Jd=a("p"),cVo=o(`Note:
Loading a model from its configuration file does `),xce=a("strong"),mVo=o("not"),fVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kce=a("code"),gVo=o("from_pretrained()"),hVo=o("to load the model weights."),uVo=l(),Rce=a("p"),pVo=o("Examples:"),_Vo=l(),m(Y6.$$.fragment),bVo=l(),oo=a("div"),m(K6.$$.fragment),vVo=l(),Sce=a("p"),TVo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),FVo=l(),is=a("p"),CVo=o("The model class to instantiate is selected based on the "),Pce=a("code"),MVo=o("model_type"),EVo=o(` property of the config object (either
passed as an argument or loaded from `),$ce=a("code"),yVo=o("pretrained_model_name_or_path"),wVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ice=a("code"),AVo=o("pretrained_model_name_or_path"),LVo=o(":"),BVo=l(),ds=a("ul"),O1=a("li"),jce=a("strong"),xVo=o("data2vec-audio"),kVo=o(" \u2014 "),tq=a("a"),RVo=o("Data2VecAudioForXVector"),SVo=o(" (Data2VecAudio model)"),PVo=l(),G1=a("li"),Nce=a("strong"),$Vo=o("unispeech-sat"),IVo=o(" \u2014 "),aq=a("a"),jVo=o("UniSpeechSatForXVector"),NVo=o(" (UniSpeechSat model)"),DVo=l(),X1=a("li"),Dce=a("strong"),qVo=o("wav2vec2"),OVo=o(" \u2014 "),sq=a("a"),GVo=o("Wav2Vec2ForXVector"),XVo=o(" (Wav2Vec2 model)"),VVo=l(),V1=a("li"),qce=a("strong"),zVo=o("wavlm"),WVo=o(" \u2014 "),nq=a("a"),QVo=o("WavLMForXVector"),HVo=o(" (WavLM model)"),UVo=l(),z1=a("p"),JVo=o("The model is set in evaluation mode by default using "),Oce=a("code"),YVo=o("model.eval()"),KVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gce=a("code"),ZVo=o("model.train()"),ezo=l(),Xce=a("p"),ozo=o("Examples:"),rzo=l(),m(Z6.$$.fragment),Ske=l(),Yd=a("h2"),W1=a("a"),Vce=a("span"),m(eA.$$.fragment),tzo=l(),zce=a("span"),azo=o("AutoModelForMaskedImageModeling"),Pke=l(),hr=a("div"),m(oA.$$.fragment),szo=l(),Kd=a("p"),nzo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Wce=a("code"),lzo=o("from_pretrained()"),izo=o("class method or the "),Qce=a("code"),dzo=o("from_config()"),czo=o(`class
method.`),mzo=l(),rA=a("p"),fzo=o("This class cannot be instantiated directly using "),Hce=a("code"),gzo=o("__init__()"),hzo=o(" (throws an error)."),uzo=l(),it=a("div"),m(tA.$$.fragment),pzo=l(),Uce=a("p"),_zo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),bzo=l(),Zd=a("p"),vzo=o(`Note:
Loading a model from its configuration file does `),Jce=a("strong"),Tzo=o("not"),Fzo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yce=a("code"),Czo=o("from_pretrained()"),Mzo=o("to load the model weights."),Ezo=l(),Kce=a("p"),yzo=o("Examples:"),wzo=l(),m(aA.$$.fragment),Azo=l(),ro=a("div"),m(sA.$$.fragment),Lzo=l(),Zce=a("p"),Bzo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),xzo=l(),cs=a("p"),kzo=o("The model class to instantiate is selected based on the "),eme=a("code"),Rzo=o("model_type"),Szo=o(` property of the config object (either
passed as an argument or loaded from `),ome=a("code"),Pzo=o("pretrained_model_name_or_path"),$zo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rme=a("code"),Izo=o("pretrained_model_name_or_path"),jzo=o(":"),Nzo=l(),ec=a("ul"),Q1=a("li"),tme=a("strong"),Dzo=o("deit"),qzo=o(" \u2014 "),lq=a("a"),Ozo=o("DeiTForMaskedImageModeling"),Gzo=o(" (DeiT model)"),Xzo=l(),H1=a("li"),ame=a("strong"),Vzo=o("swin"),zzo=o(" \u2014 "),iq=a("a"),Wzo=o("SwinForMaskedImageModeling"),Qzo=o(" (Swin model)"),Hzo=l(),U1=a("li"),sme=a("strong"),Uzo=o("vit"),Jzo=o(" \u2014 "),dq=a("a"),Yzo=o("ViTForMaskedImageModeling"),Kzo=o(" (ViT model)"),Zzo=l(),J1=a("p"),eWo=o("The model is set in evaluation mode by default using "),nme=a("code"),oWo=o("model.eval()"),rWo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lme=a("code"),tWo=o("model.train()"),aWo=l(),ime=a("p"),sWo=o("Examples:"),nWo=l(),m(nA.$$.fragment),$ke=l(),oc=a("h2"),Y1=a("a"),dme=a("span"),m(lA.$$.fragment),lWo=l(),cme=a("span"),iWo=o("AutoModelForObjectDetection"),Ike=l(),ur=a("div"),m(iA.$$.fragment),dWo=l(),rc=a("p"),cWo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),mme=a("code"),mWo=o("from_pretrained()"),fWo=o("class method or the "),fme=a("code"),gWo=o("from_config()"),hWo=o(`class
method.`),uWo=l(),dA=a("p"),pWo=o("This class cannot be instantiated directly using "),gme=a("code"),_Wo=o("__init__()"),bWo=o(" (throws an error)."),vWo=l(),dt=a("div"),m(cA.$$.fragment),TWo=l(),hme=a("p"),FWo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),CWo=l(),tc=a("p"),MWo=o(`Note:
Loading a model from its configuration file does `),ume=a("strong"),EWo=o("not"),yWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pme=a("code"),wWo=o("from_pretrained()"),AWo=o("to load the model weights."),LWo=l(),_me=a("p"),BWo=o("Examples:"),xWo=l(),m(mA.$$.fragment),kWo=l(),to=a("div"),m(fA.$$.fragment),RWo=l(),bme=a("p"),SWo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),PWo=l(),ms=a("p"),$Wo=o("The model class to instantiate is selected based on the "),vme=a("code"),IWo=o("model_type"),jWo=o(` property of the config object (either
passed as an argument or loaded from `),Tme=a("code"),NWo=o("pretrained_model_name_or_path"),DWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fme=a("code"),qWo=o("pretrained_model_name_or_path"),OWo=o(":"),GWo=l(),Cme=a("ul"),K1=a("li"),Mme=a("strong"),XWo=o("detr"),VWo=o(" \u2014 "),cq=a("a"),zWo=o("DetrForObjectDetection"),WWo=o(" (DETR model)"),QWo=l(),Z1=a("p"),HWo=o("The model is set in evaluation mode by default using "),Eme=a("code"),UWo=o("model.eval()"),JWo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yme=a("code"),YWo=o("model.train()"),KWo=l(),wme=a("p"),ZWo=o("Examples:"),eQo=l(),m(gA.$$.fragment),jke=l(),ac=a("h2"),eF=a("a"),Ame=a("span"),m(hA.$$.fragment),oQo=l(),Lme=a("span"),rQo=o("AutoModelForImageSegmentation"),Nke=l(),pr=a("div"),m(uA.$$.fragment),tQo=l(),sc=a("p"),aQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Bme=a("code"),sQo=o("from_pretrained()"),nQo=o("class method or the "),xme=a("code"),lQo=o("from_config()"),iQo=o(`class
method.`),dQo=l(),pA=a("p"),cQo=o("This class cannot be instantiated directly using "),kme=a("code"),mQo=o("__init__()"),fQo=o(" (throws an error)."),gQo=l(),ct=a("div"),m(_A.$$.fragment),hQo=l(),Rme=a("p"),uQo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),pQo=l(),nc=a("p"),_Qo=o(`Note:
Loading a model from its configuration file does `),Sme=a("strong"),bQo=o("not"),vQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pme=a("code"),TQo=o("from_pretrained()"),FQo=o("to load the model weights."),CQo=l(),$me=a("p"),MQo=o("Examples:"),EQo=l(),m(bA.$$.fragment),yQo=l(),ao=a("div"),m(vA.$$.fragment),wQo=l(),Ime=a("p"),AQo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),LQo=l(),fs=a("p"),BQo=o("The model class to instantiate is selected based on the "),jme=a("code"),xQo=o("model_type"),kQo=o(` property of the config object (either
passed as an argument or loaded from `),Nme=a("code"),RQo=o("pretrained_model_name_or_path"),SQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dme=a("code"),PQo=o("pretrained_model_name_or_path"),$Qo=o(":"),IQo=l(),qme=a("ul"),oF=a("li"),Ome=a("strong"),jQo=o("detr"),NQo=o(" \u2014 "),mq=a("a"),DQo=o("DetrForSegmentation"),qQo=o(" (DETR model)"),OQo=l(),rF=a("p"),GQo=o("The model is set in evaluation mode by default using "),Gme=a("code"),XQo=o("model.eval()"),VQo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xme=a("code"),zQo=o("model.train()"),WQo=l(),Vme=a("p"),QQo=o("Examples:"),HQo=l(),m(TA.$$.fragment),Dke=l(),lc=a("h2"),tF=a("a"),zme=a("span"),m(FA.$$.fragment),UQo=l(),Wme=a("span"),JQo=o("AutoModelForSemanticSegmentation"),qke=l(),_r=a("div"),m(CA.$$.fragment),YQo=l(),ic=a("p"),KQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Qme=a("code"),ZQo=o("from_pretrained()"),eHo=o("class method or the "),Hme=a("code"),oHo=o("from_config()"),rHo=o(`class
method.`),tHo=l(),MA=a("p"),aHo=o("This class cannot be instantiated directly using "),Ume=a("code"),sHo=o("__init__()"),nHo=o(" (throws an error)."),lHo=l(),mt=a("div"),m(EA.$$.fragment),iHo=l(),Jme=a("p"),dHo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),cHo=l(),dc=a("p"),mHo=o(`Note:
Loading a model from its configuration file does `),Yme=a("strong"),fHo=o("not"),gHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kme=a("code"),hHo=o("from_pretrained()"),uHo=o("to load the model weights."),pHo=l(),Zme=a("p"),_Ho=o("Examples:"),bHo=l(),m(yA.$$.fragment),vHo=l(),so=a("div"),m(wA.$$.fragment),THo=l(),efe=a("p"),FHo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),CHo=l(),gs=a("p"),MHo=o("The model class to instantiate is selected based on the "),ofe=a("code"),EHo=o("model_type"),yHo=o(` property of the config object (either
passed as an argument or loaded from `),rfe=a("code"),wHo=o("pretrained_model_name_or_path"),AHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tfe=a("code"),LHo=o("pretrained_model_name_or_path"),BHo=o(":"),xHo=l(),AA=a("ul"),aF=a("li"),afe=a("strong"),kHo=o("beit"),RHo=o(" \u2014 "),fq=a("a"),SHo=o("BeitForSemanticSegmentation"),PHo=o(" (BEiT model)"),$Ho=l(),sF=a("li"),sfe=a("strong"),IHo=o("segformer"),jHo=o(" \u2014 "),gq=a("a"),NHo=o("SegformerForSemanticSegmentation"),DHo=o(" (SegFormer model)"),qHo=l(),nF=a("p"),OHo=o("The model is set in evaluation mode by default using "),nfe=a("code"),GHo=o("model.eval()"),XHo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lfe=a("code"),VHo=o("model.train()"),zHo=l(),ife=a("p"),WHo=o("Examples:"),QHo=l(),m(LA.$$.fragment),Oke=l(),cc=a("h2"),lF=a("a"),dfe=a("span"),m(BA.$$.fragment),HHo=l(),cfe=a("span"),UHo=o("AutoModelForInstanceSegmentation"),Gke=l(),br=a("div"),m(xA.$$.fragment),JHo=l(),mc=a("p"),YHo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the `),mfe=a("code"),KHo=o("from_pretrained()"),ZHo=o("class method or the "),ffe=a("code"),eUo=o("from_config()"),oUo=o(`class
method.`),rUo=l(),kA=a("p"),tUo=o("This class cannot be instantiated directly using "),gfe=a("code"),aUo=o("__init__()"),sUo=o(" (throws an error)."),nUo=l(),ft=a("div"),m(RA.$$.fragment),lUo=l(),hfe=a("p"),iUo=o("Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration."),dUo=l(),fc=a("p"),cUo=o(`Note:
Loading a model from its configuration file does `),ufe=a("strong"),mUo=o("not"),fUo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pfe=a("code"),gUo=o("from_pretrained()"),hUo=o("to load the model weights."),uUo=l(),_fe=a("p"),pUo=o("Examples:"),_Uo=l(),m(SA.$$.fragment),bUo=l(),no=a("div"),m(PA.$$.fragment),vUo=l(),bfe=a("p"),TUo=o("Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model."),FUo=l(),hs=a("p"),CUo=o("The model class to instantiate is selected based on the "),vfe=a("code"),MUo=o("model_type"),EUo=o(` property of the config object (either
passed as an argument or loaded from `),Tfe=a("code"),yUo=o("pretrained_model_name_or_path"),wUo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ffe=a("code"),AUo=o("pretrained_model_name_or_path"),LUo=o(":"),BUo=l(),Cfe=a("ul"),iF=a("li"),Mfe=a("strong"),xUo=o("maskformer"),kUo=o(" \u2014 "),hq=a("a"),RUo=o("MaskFormerForInstanceSegmentation"),SUo=o(" (MaskFormer model)"),PUo=l(),dF=a("p"),$Uo=o("The model is set in evaluation mode by default using "),Efe=a("code"),IUo=o("model.eval()"),jUo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yfe=a("code"),NUo=o("model.train()"),DUo=l(),wfe=a("p"),qUo=o("Examples:"),OUo=l(),m($A.$$.fragment),Xke=l(),gc=a("h2"),cF=a("a"),Afe=a("span"),m(IA.$$.fragment),GUo=l(),Lfe=a("span"),XUo=o("TFAutoModel"),Vke=l(),vr=a("div"),m(jA.$$.fragment),VUo=l(),hc=a("p"),zUo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Bfe=a("code"),WUo=o("from_pretrained()"),QUo=o("class method or the "),xfe=a("code"),HUo=o("from_config()"),UUo=o(`class
method.`),JUo=l(),NA=a("p"),YUo=o("This class cannot be instantiated directly using "),kfe=a("code"),KUo=o("__init__()"),ZUo=o(" (throws an error)."),eJo=l(),gt=a("div"),m(DA.$$.fragment),oJo=l(),Rfe=a("p"),rJo=o("Instantiates one of the base model classes of the library from a configuration."),tJo=l(),uc=a("p"),aJo=o(`Note:
Loading a model from its configuration file does `),Sfe=a("strong"),sJo=o("not"),nJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pfe=a("code"),lJo=o("from_pretrained()"),iJo=o("to load the model weights."),dJo=l(),$fe=a("p"),cJo=o("Examples:"),mJo=l(),m(qA.$$.fragment),fJo=l(),ho=a("div"),m(OA.$$.fragment),gJo=l(),Ife=a("p"),hJo=o("Instantiate one of the base model classes of the library from a pretrained model."),uJo=l(),us=a("p"),pJo=o("The model class to instantiate is selected based on the "),jfe=a("code"),_Jo=o("model_type"),bJo=o(` property of the config object (either
passed as an argument or loaded from `),Nfe=a("code"),vJo=o("pretrained_model_name_or_path"),TJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dfe=a("code"),FJo=o("pretrained_model_name_or_path"),CJo=o(":"),MJo=l(),B=a("ul"),mF=a("li"),qfe=a("strong"),EJo=o("albert"),yJo=o(" \u2014 "),uq=a("a"),wJo=o("TFAlbertModel"),AJo=o(" (ALBERT model)"),LJo=l(),fF=a("li"),Ofe=a("strong"),BJo=o("bart"),xJo=o(" \u2014 "),pq=a("a"),kJo=o("TFBartModel"),RJo=o(" (BART model)"),SJo=l(),gF=a("li"),Gfe=a("strong"),PJo=o("bert"),$Jo=o(" \u2014 "),_q=a("a"),IJo=o("TFBertModel"),jJo=o(" (BERT model)"),NJo=l(),hF=a("li"),Xfe=a("strong"),DJo=o("blenderbot"),qJo=o(" \u2014 "),bq=a("a"),OJo=o("TFBlenderbotModel"),GJo=o(" (Blenderbot model)"),XJo=l(),uF=a("li"),Vfe=a("strong"),VJo=o("blenderbot-small"),zJo=o(" \u2014 "),vq=a("a"),WJo=o("TFBlenderbotSmallModel"),QJo=o(" (BlenderbotSmall model)"),HJo=l(),pF=a("li"),zfe=a("strong"),UJo=o("camembert"),JJo=o(" \u2014 "),Tq=a("a"),YJo=o("TFCamembertModel"),KJo=o(" (CamemBERT model)"),ZJo=l(),_F=a("li"),Wfe=a("strong"),eYo=o("clip"),oYo=o(" \u2014 "),Fq=a("a"),rYo=o("TFCLIPModel"),tYo=o(" (CLIP model)"),aYo=l(),bF=a("li"),Qfe=a("strong"),sYo=o("convbert"),nYo=o(" \u2014 "),Cq=a("a"),lYo=o("TFConvBertModel"),iYo=o(" (ConvBERT model)"),dYo=l(),vF=a("li"),Hfe=a("strong"),cYo=o("convnext"),mYo=o(" \u2014 "),Mq=a("a"),fYo=o("TFConvNextModel"),gYo=o(" (ConvNext model)"),hYo=l(),TF=a("li"),Ufe=a("strong"),uYo=o("ctrl"),pYo=o(" \u2014 "),Eq=a("a"),_Yo=o("TFCTRLModel"),bYo=o(" (CTRL model)"),vYo=l(),FF=a("li"),Jfe=a("strong"),TYo=o("deberta"),FYo=o(" \u2014 "),yq=a("a"),CYo=o("TFDebertaModel"),MYo=o(" (DeBERTa model)"),EYo=l(),CF=a("li"),Yfe=a("strong"),yYo=o("deberta-v2"),wYo=o(" \u2014 "),wq=a("a"),AYo=o("TFDebertaV2Model"),LYo=o(" (DeBERTa-v2 model)"),BYo=l(),MF=a("li"),Kfe=a("strong"),xYo=o("distilbert"),kYo=o(" \u2014 "),Aq=a("a"),RYo=o("TFDistilBertModel"),SYo=o(" (DistilBERT model)"),PYo=l(),EF=a("li"),Zfe=a("strong"),$Yo=o("dpr"),IYo=o(" \u2014 "),Lq=a("a"),jYo=o("TFDPRQuestionEncoder"),NYo=o(" (DPR model)"),DYo=l(),yF=a("li"),ege=a("strong"),qYo=o("electra"),OYo=o(" \u2014 "),Bq=a("a"),GYo=o("TFElectraModel"),XYo=o(" (ELECTRA model)"),VYo=l(),wF=a("li"),oge=a("strong"),zYo=o("flaubert"),WYo=o(" \u2014 "),xq=a("a"),QYo=o("TFFlaubertModel"),HYo=o(" (FlauBERT model)"),UYo=l(),On=a("li"),rge=a("strong"),JYo=o("funnel"),YYo=o(" \u2014 "),kq=a("a"),KYo=o("TFFunnelModel"),ZYo=o(" or "),Rq=a("a"),eKo=o("TFFunnelBaseModel"),oKo=o(" (Funnel Transformer model)"),rKo=l(),AF=a("li"),tge=a("strong"),tKo=o("gpt2"),aKo=o(" \u2014 "),Sq=a("a"),sKo=o("TFGPT2Model"),nKo=o(" (OpenAI GPT-2 model)"),lKo=l(),LF=a("li"),age=a("strong"),iKo=o("hubert"),dKo=o(" \u2014 "),Pq=a("a"),cKo=o("TFHubertModel"),mKo=o(" (Hubert model)"),fKo=l(),BF=a("li"),sge=a("strong"),gKo=o("layoutlm"),hKo=o(" \u2014 "),$q=a("a"),uKo=o("TFLayoutLMModel"),pKo=o(" (LayoutLM model)"),_Ko=l(),xF=a("li"),nge=a("strong"),bKo=o("led"),vKo=o(" \u2014 "),Iq=a("a"),TKo=o("TFLEDModel"),FKo=o(" (LED model)"),CKo=l(),kF=a("li"),lge=a("strong"),MKo=o("longformer"),EKo=o(" \u2014 "),jq=a("a"),yKo=o("TFLongformerModel"),wKo=o(" (Longformer model)"),AKo=l(),RF=a("li"),ige=a("strong"),LKo=o("lxmert"),BKo=o(" \u2014 "),Nq=a("a"),xKo=o("TFLxmertModel"),kKo=o(" (LXMERT model)"),RKo=l(),SF=a("li"),dge=a("strong"),SKo=o("marian"),PKo=o(" \u2014 "),Dq=a("a"),$Ko=o("TFMarianModel"),IKo=o(" (Marian model)"),jKo=l(),PF=a("li"),cge=a("strong"),NKo=o("mbart"),DKo=o(" \u2014 "),qq=a("a"),qKo=o("TFMBartModel"),OKo=o(" (mBART model)"),GKo=l(),$F=a("li"),mge=a("strong"),XKo=o("mobilebert"),VKo=o(" \u2014 "),Oq=a("a"),zKo=o("TFMobileBertModel"),WKo=o(" (MobileBERT model)"),QKo=l(),IF=a("li"),fge=a("strong"),HKo=o("mpnet"),UKo=o(" \u2014 "),Gq=a("a"),JKo=o("TFMPNetModel"),YKo=o(" (MPNet model)"),KKo=l(),jF=a("li"),gge=a("strong"),ZKo=o("mt5"),eZo=o(" \u2014 "),Xq=a("a"),oZo=o("TFMT5Model"),rZo=o(" (mT5 model)"),tZo=l(),NF=a("li"),hge=a("strong"),aZo=o("openai-gpt"),sZo=o(" \u2014 "),Vq=a("a"),nZo=o("TFOpenAIGPTModel"),lZo=o(" (OpenAI GPT model)"),iZo=l(),DF=a("li"),uge=a("strong"),dZo=o("pegasus"),cZo=o(" \u2014 "),zq=a("a"),mZo=o("TFPegasusModel"),fZo=o(" (Pegasus model)"),gZo=l(),qF=a("li"),pge=a("strong"),hZo=o("rembert"),uZo=o(" \u2014 "),Wq=a("a"),pZo=o("TFRemBertModel"),_Zo=o(" (RemBERT model)"),bZo=l(),OF=a("li"),_ge=a("strong"),vZo=o("roberta"),TZo=o(" \u2014 "),Qq=a("a"),FZo=o("TFRobertaModel"),CZo=o(" (RoBERTa model)"),MZo=l(),GF=a("li"),bge=a("strong"),EZo=o("roformer"),yZo=o(" \u2014 "),Hq=a("a"),wZo=o("TFRoFormerModel"),AZo=o(" (RoFormer model)"),LZo=l(),XF=a("li"),vge=a("strong"),BZo=o("speech_to_text"),xZo=o(" \u2014 "),Uq=a("a"),kZo=o("TFSpeech2TextModel"),RZo=o(" (Speech2Text model)"),SZo=l(),VF=a("li"),Tge=a("strong"),PZo=o("t5"),$Zo=o(" \u2014 "),Jq=a("a"),IZo=o("TFT5Model"),jZo=o(" (T5 model)"),NZo=l(),zF=a("li"),Fge=a("strong"),DZo=o("tapas"),qZo=o(" \u2014 "),Yq=a("a"),OZo=o("TFTapasModel"),GZo=o(" (TAPAS model)"),XZo=l(),WF=a("li"),Cge=a("strong"),VZo=o("transfo-xl"),zZo=o(" \u2014 "),Kq=a("a"),WZo=o("TFTransfoXLModel"),QZo=o(" (Transformer-XL model)"),HZo=l(),QF=a("li"),Mge=a("strong"),UZo=o("vit"),JZo=o(" \u2014 "),Zq=a("a"),YZo=o("TFViTModel"),KZo=o(" (ViT model)"),ZZo=l(),HF=a("li"),Ege=a("strong"),eer=o("wav2vec2"),oer=o(" \u2014 "),eO=a("a"),rer=o("TFWav2Vec2Model"),ter=o(" (Wav2Vec2 model)"),aer=l(),UF=a("li"),yge=a("strong"),ser=o("xlm"),ner=o(" \u2014 "),oO=a("a"),ler=o("TFXLMModel"),ier=o(" (XLM model)"),der=l(),JF=a("li"),wge=a("strong"),cer=o("xlm-roberta"),mer=o(" \u2014 "),rO=a("a"),fer=o("TFXLMRobertaModel"),ger=o(" (XLM-RoBERTa model)"),her=l(),YF=a("li"),Age=a("strong"),uer=o("xlnet"),per=o(" \u2014 "),tO=a("a"),_er=o("TFXLNetModel"),ber=o(" (XLNet model)"),ver=l(),Lge=a("p"),Ter=o("Examples:"),Fer=l(),m(GA.$$.fragment),zke=l(),pc=a("h2"),KF=a("a"),Bge=a("span"),m(XA.$$.fragment),Cer=l(),xge=a("span"),Mer=o("TFAutoModelForPreTraining"),Wke=l(),Tr=a("div"),m(VA.$$.fragment),Eer=l(),_c=a("p"),yer=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),kge=a("code"),wer=o("from_pretrained()"),Aer=o("class method or the "),Rge=a("code"),Ler=o("from_config()"),Ber=o(`class
method.`),xer=l(),zA=a("p"),ker=o("This class cannot be instantiated directly using "),Sge=a("code"),Rer=o("__init__()"),Ser=o(" (throws an error)."),Per=l(),ht=a("div"),m(WA.$$.fragment),$er=l(),Pge=a("p"),Ier=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),jer=l(),bc=a("p"),Ner=o(`Note:
Loading a model from its configuration file does `),$ge=a("strong"),Der=o("not"),qer=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ige=a("code"),Oer=o("from_pretrained()"),Ger=o("to load the model weights."),Xer=l(),jge=a("p"),Ver=o("Examples:"),zer=l(),m(QA.$$.fragment),Wer=l(),uo=a("div"),m(HA.$$.fragment),Qer=l(),Nge=a("p"),Her=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Uer=l(),ps=a("p"),Jer=o("The model class to instantiate is selected based on the "),Dge=a("code"),Yer=o("model_type"),Ker=o(` property of the config object (either
passed as an argument or loaded from `),qge=a("code"),Zer=o("pretrained_model_name_or_path"),eor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Oge=a("code"),oor=o("pretrained_model_name_or_path"),ror=o(":"),tor=l(),H=a("ul"),ZF=a("li"),Gge=a("strong"),aor=o("albert"),sor=o(" \u2014 "),aO=a("a"),nor=o("TFAlbertForPreTraining"),lor=o(" (ALBERT model)"),ior=l(),eC=a("li"),Xge=a("strong"),dor=o("bart"),cor=o(" \u2014 "),sO=a("a"),mor=o("TFBartForConditionalGeneration"),gor=o(" (BART model)"),hor=l(),oC=a("li"),Vge=a("strong"),uor=o("bert"),por=o(" \u2014 "),nO=a("a"),_or=o("TFBertForPreTraining"),bor=o(" (BERT model)"),vor=l(),rC=a("li"),zge=a("strong"),Tor=o("camembert"),For=o(" \u2014 "),lO=a("a"),Cor=o("TFCamembertForMaskedLM"),Mor=o(" (CamemBERT model)"),Eor=l(),tC=a("li"),Wge=a("strong"),yor=o("ctrl"),wor=o(" \u2014 "),iO=a("a"),Aor=o("TFCTRLLMHeadModel"),Lor=o(" (CTRL model)"),Bor=l(),aC=a("li"),Qge=a("strong"),xor=o("distilbert"),kor=o(" \u2014 "),dO=a("a"),Ror=o("TFDistilBertForMaskedLM"),Sor=o(" (DistilBERT model)"),Por=l(),sC=a("li"),Hge=a("strong"),$or=o("electra"),Ior=o(" \u2014 "),cO=a("a"),jor=o("TFElectraForPreTraining"),Nor=o(" (ELECTRA model)"),Dor=l(),nC=a("li"),Uge=a("strong"),qor=o("flaubert"),Oor=o(" \u2014 "),mO=a("a"),Gor=o("TFFlaubertWithLMHeadModel"),Xor=o(" (FlauBERT model)"),Vor=l(),lC=a("li"),Jge=a("strong"),zor=o("funnel"),Wor=o(" \u2014 "),fO=a("a"),Qor=o("TFFunnelForPreTraining"),Hor=o(" (Funnel Transformer model)"),Uor=l(),iC=a("li"),Yge=a("strong"),Jor=o("gpt2"),Yor=o(" \u2014 "),gO=a("a"),Kor=o("TFGPT2LMHeadModel"),Zor=o(" (OpenAI GPT-2 model)"),err=l(),dC=a("li"),Kge=a("strong"),orr=o("layoutlm"),rrr=o(" \u2014 "),hO=a("a"),trr=o("TFLayoutLMForMaskedLM"),arr=o(" (LayoutLM model)"),srr=l(),cC=a("li"),Zge=a("strong"),nrr=o("lxmert"),lrr=o(" \u2014 "),uO=a("a"),irr=o("TFLxmertForPreTraining"),drr=o(" (LXMERT model)"),crr=l(),mC=a("li"),ehe=a("strong"),mrr=o("mobilebert"),frr=o(" \u2014 "),pO=a("a"),grr=o("TFMobileBertForPreTraining"),hrr=o(" (MobileBERT model)"),urr=l(),fC=a("li"),ohe=a("strong"),prr=o("mpnet"),_rr=o(" \u2014 "),_O=a("a"),brr=o("TFMPNetForMaskedLM"),vrr=o(" (MPNet model)"),Trr=l(),gC=a("li"),rhe=a("strong"),Frr=o("openai-gpt"),Crr=o(" \u2014 "),bO=a("a"),Mrr=o("TFOpenAIGPTLMHeadModel"),Err=o(" (OpenAI GPT model)"),yrr=l(),hC=a("li"),the=a("strong"),wrr=o("roberta"),Arr=o(" \u2014 "),vO=a("a"),Lrr=o("TFRobertaForMaskedLM"),Brr=o(" (RoBERTa model)"),xrr=l(),uC=a("li"),ahe=a("strong"),krr=o("t5"),Rrr=o(" \u2014 "),TO=a("a"),Srr=o("TFT5ForConditionalGeneration"),Prr=o(" (T5 model)"),$rr=l(),pC=a("li"),she=a("strong"),Irr=o("tapas"),jrr=o(" \u2014 "),FO=a("a"),Nrr=o("TFTapasForMaskedLM"),Drr=o(" (TAPAS model)"),qrr=l(),_C=a("li"),nhe=a("strong"),Orr=o("transfo-xl"),Grr=o(" \u2014 "),CO=a("a"),Xrr=o("TFTransfoXLLMHeadModel"),Vrr=o(" (Transformer-XL model)"),zrr=l(),bC=a("li"),lhe=a("strong"),Wrr=o("xlm"),Qrr=o(" \u2014 "),MO=a("a"),Hrr=o("TFXLMWithLMHeadModel"),Urr=o(" (XLM model)"),Jrr=l(),vC=a("li"),ihe=a("strong"),Yrr=o("xlm-roberta"),Krr=o(" \u2014 "),EO=a("a"),Zrr=o("TFXLMRobertaForMaskedLM"),etr=o(" (XLM-RoBERTa model)"),otr=l(),TC=a("li"),dhe=a("strong"),rtr=o("xlnet"),ttr=o(" \u2014 "),yO=a("a"),atr=o("TFXLNetLMHeadModel"),str=o(" (XLNet model)"),ntr=l(),che=a("p"),ltr=o("Examples:"),itr=l(),m(UA.$$.fragment),Qke=l(),vc=a("h2"),FC=a("a"),mhe=a("span"),m(JA.$$.fragment),dtr=l(),fhe=a("span"),ctr=o("TFAutoModelForCausalLM"),Hke=l(),Fr=a("div"),m(YA.$$.fragment),mtr=l(),Tc=a("p"),ftr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),ghe=a("code"),gtr=o("from_pretrained()"),htr=o("class method or the "),hhe=a("code"),utr=o("from_config()"),ptr=o(`class
method.`),_tr=l(),KA=a("p"),btr=o("This class cannot be instantiated directly using "),uhe=a("code"),vtr=o("__init__()"),Ttr=o(" (throws an error)."),Ftr=l(),ut=a("div"),m(ZA.$$.fragment),Ctr=l(),phe=a("p"),Mtr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Etr=l(),Fc=a("p"),ytr=o(`Note:
Loading a model from its configuration file does `),_he=a("strong"),wtr=o("not"),Atr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=a("code"),Ltr=o("from_pretrained()"),Btr=o("to load the model weights."),xtr=l(),vhe=a("p"),ktr=o("Examples:"),Rtr=l(),m(e0.$$.fragment),Str=l(),po=a("div"),m(o0.$$.fragment),Ptr=l(),The=a("p"),$tr=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Itr=l(),_s=a("p"),jtr=o("The model class to instantiate is selected based on the "),Fhe=a("code"),Ntr=o("model_type"),Dtr=o(` property of the config object (either
passed as an argument or loaded from `),Che=a("code"),qtr=o("pretrained_model_name_or_path"),Otr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=a("code"),Gtr=o("pretrained_model_name_or_path"),Xtr=o(":"),Vtr=l(),ge=a("ul"),CC=a("li"),Ehe=a("strong"),ztr=o("bert"),Wtr=o(" \u2014 "),wO=a("a"),Qtr=o("TFBertLMHeadModel"),Htr=o(" (BERT model)"),Utr=l(),MC=a("li"),yhe=a("strong"),Jtr=o("camembert"),Ytr=o(" \u2014 "),AO=a("a"),Ktr=o("TFCamembertForCausalLM"),Ztr=o(" (CamemBERT model)"),ear=l(),EC=a("li"),whe=a("strong"),oar=o("ctrl"),rar=o(" \u2014 "),LO=a("a"),tar=o("TFCTRLLMHeadModel"),aar=o(" (CTRL model)"),sar=l(),yC=a("li"),Ahe=a("strong"),nar=o("gpt2"),lar=o(" \u2014 "),BO=a("a"),iar=o("TFGPT2LMHeadModel"),dar=o(" (OpenAI GPT-2 model)"),car=l(),wC=a("li"),Lhe=a("strong"),mar=o("openai-gpt"),far=o(" \u2014 "),xO=a("a"),gar=o("TFOpenAIGPTLMHeadModel"),har=o(" (OpenAI GPT model)"),uar=l(),AC=a("li"),Bhe=a("strong"),par=o("rembert"),_ar=o(" \u2014 "),kO=a("a"),bar=o("TFRemBertForCausalLM"),Tar=o(" (RemBERT model)"),Far=l(),LC=a("li"),xhe=a("strong"),Car=o("roberta"),Mar=o(" \u2014 "),RO=a("a"),Ear=o("TFRobertaForCausalLM"),yar=o(" (RoBERTa model)"),war=l(),BC=a("li"),khe=a("strong"),Aar=o("roformer"),Lar=o(" \u2014 "),SO=a("a"),Bar=o("TFRoFormerForCausalLM"),xar=o(" (RoFormer model)"),kar=l(),xC=a("li"),Rhe=a("strong"),Rar=o("transfo-xl"),Sar=o(" \u2014 "),PO=a("a"),Par=o("TFTransfoXLLMHeadModel"),$ar=o(" (Transformer-XL model)"),Iar=l(),kC=a("li"),She=a("strong"),jar=o("xlm"),Nar=o(" \u2014 "),$O=a("a"),Dar=o("TFXLMWithLMHeadModel"),qar=o(" (XLM model)"),Oar=l(),RC=a("li"),Phe=a("strong"),Gar=o("xlnet"),Xar=o(" \u2014 "),IO=a("a"),Var=o("TFXLNetLMHeadModel"),zar=o(" (XLNet model)"),War=l(),$he=a("p"),Qar=o("Examples:"),Har=l(),m(r0.$$.fragment),Uke=l(),Cc=a("h2"),SC=a("a"),Ihe=a("span"),m(t0.$$.fragment),Uar=l(),jhe=a("span"),Jar=o("TFAutoModelForImageClassification"),Jke=l(),Cr=a("div"),m(a0.$$.fragment),Yar=l(),Mc=a("p"),Kar=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Nhe=a("code"),Zar=o("from_pretrained()"),esr=o("class method or the "),Dhe=a("code"),osr=o("from_config()"),rsr=o(`class
method.`),tsr=l(),s0=a("p"),asr=o("This class cannot be instantiated directly using "),qhe=a("code"),ssr=o("__init__()"),nsr=o(" (throws an error)."),lsr=l(),pt=a("div"),m(n0.$$.fragment),isr=l(),Ohe=a("p"),dsr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),csr=l(),Ec=a("p"),msr=o(`Note:
Loading a model from its configuration file does `),Ghe=a("strong"),fsr=o("not"),gsr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xhe=a("code"),hsr=o("from_pretrained()"),usr=o("to load the model weights."),psr=l(),Vhe=a("p"),_sr=o("Examples:"),bsr=l(),m(l0.$$.fragment),vsr=l(),_o=a("div"),m(i0.$$.fragment),Tsr=l(),zhe=a("p"),Fsr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Csr=l(),bs=a("p"),Msr=o("The model class to instantiate is selected based on the "),Whe=a("code"),Esr=o("model_type"),ysr=o(` property of the config object (either
passed as an argument or loaded from `),Qhe=a("code"),wsr=o("pretrained_model_name_or_path"),Asr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hhe=a("code"),Lsr=o("pretrained_model_name_or_path"),Bsr=o(":"),xsr=l(),d0=a("ul"),PC=a("li"),Uhe=a("strong"),ksr=o("convnext"),Rsr=o(" \u2014 "),jO=a("a"),Ssr=o("TFConvNextForImageClassification"),Psr=o(" (ConvNext model)"),$sr=l(),$C=a("li"),Jhe=a("strong"),Isr=o("vit"),jsr=o(" \u2014 "),NO=a("a"),Nsr=o("TFViTForImageClassification"),Dsr=o(" (ViT model)"),qsr=l(),Yhe=a("p"),Osr=o("Examples:"),Gsr=l(),m(c0.$$.fragment),Yke=l(),yc=a("h2"),IC=a("a"),Khe=a("span"),m(m0.$$.fragment),Xsr=l(),Zhe=a("span"),Vsr=o("TFAutoModelForMaskedLM"),Kke=l(),Mr=a("div"),m(f0.$$.fragment),zsr=l(),wc=a("p"),Wsr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),eue=a("code"),Qsr=o("from_pretrained()"),Hsr=o("class method or the "),oue=a("code"),Usr=o("from_config()"),Jsr=o(`class
method.`),Ysr=l(),g0=a("p"),Ksr=o("This class cannot be instantiated directly using "),rue=a("code"),Zsr=o("__init__()"),enr=o(" (throws an error)."),onr=l(),_t=a("div"),m(h0.$$.fragment),rnr=l(),tue=a("p"),tnr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),anr=l(),Ac=a("p"),snr=o(`Note:
Loading a model from its configuration file does `),aue=a("strong"),nnr=o("not"),lnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sue=a("code"),inr=o("from_pretrained()"),dnr=o("to load the model weights."),cnr=l(),nue=a("p"),mnr=o("Examples:"),fnr=l(),m(u0.$$.fragment),gnr=l(),bo=a("div"),m(p0.$$.fragment),hnr=l(),lue=a("p"),unr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),pnr=l(),vs=a("p"),_nr=o("The model class to instantiate is selected based on the "),iue=a("code"),bnr=o("model_type"),vnr=o(` property of the config object (either
passed as an argument or loaded from `),due=a("code"),Tnr=o("pretrained_model_name_or_path"),Fnr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cue=a("code"),Cnr=o("pretrained_model_name_or_path"),Mnr=o(":"),Enr=l(),Y=a("ul"),jC=a("li"),mue=a("strong"),ynr=o("albert"),wnr=o(" \u2014 "),DO=a("a"),Anr=o("TFAlbertForMaskedLM"),Lnr=o(" (ALBERT model)"),Bnr=l(),NC=a("li"),fue=a("strong"),xnr=o("bert"),knr=o(" \u2014 "),qO=a("a"),Rnr=o("TFBertForMaskedLM"),Snr=o(" (BERT model)"),Pnr=l(),DC=a("li"),gue=a("strong"),$nr=o("camembert"),Inr=o(" \u2014 "),OO=a("a"),jnr=o("TFCamembertForMaskedLM"),Nnr=o(" (CamemBERT model)"),Dnr=l(),qC=a("li"),hue=a("strong"),qnr=o("convbert"),Onr=o(" \u2014 "),GO=a("a"),Gnr=o("TFConvBertForMaskedLM"),Xnr=o(" (ConvBERT model)"),Vnr=l(),OC=a("li"),uue=a("strong"),znr=o("deberta"),Wnr=o(" \u2014 "),XO=a("a"),Qnr=o("TFDebertaForMaskedLM"),Hnr=o(" (DeBERTa model)"),Unr=l(),GC=a("li"),pue=a("strong"),Jnr=o("deberta-v2"),Ynr=o(" \u2014 "),VO=a("a"),Knr=o("TFDebertaV2ForMaskedLM"),Znr=o(" (DeBERTa-v2 model)"),elr=l(),XC=a("li"),_ue=a("strong"),olr=o("distilbert"),rlr=o(" \u2014 "),zO=a("a"),tlr=o("TFDistilBertForMaskedLM"),alr=o(" (DistilBERT model)"),slr=l(),VC=a("li"),bue=a("strong"),nlr=o("electra"),llr=o(" \u2014 "),WO=a("a"),ilr=o("TFElectraForMaskedLM"),dlr=o(" (ELECTRA model)"),clr=l(),zC=a("li"),vue=a("strong"),mlr=o("flaubert"),flr=o(" \u2014 "),QO=a("a"),glr=o("TFFlaubertWithLMHeadModel"),hlr=o(" (FlauBERT model)"),ulr=l(),WC=a("li"),Tue=a("strong"),plr=o("funnel"),_lr=o(" \u2014 "),HO=a("a"),blr=o("TFFunnelForMaskedLM"),vlr=o(" (Funnel Transformer model)"),Tlr=l(),QC=a("li"),Fue=a("strong"),Flr=o("layoutlm"),Clr=o(" \u2014 "),UO=a("a"),Mlr=o("TFLayoutLMForMaskedLM"),Elr=o(" (LayoutLM model)"),ylr=l(),HC=a("li"),Cue=a("strong"),wlr=o("longformer"),Alr=o(" \u2014 "),JO=a("a"),Llr=o("TFLongformerForMaskedLM"),Blr=o(" (Longformer model)"),xlr=l(),UC=a("li"),Mue=a("strong"),klr=o("mobilebert"),Rlr=o(" \u2014 "),YO=a("a"),Slr=o("TFMobileBertForMaskedLM"),Plr=o(" (MobileBERT model)"),$lr=l(),JC=a("li"),Eue=a("strong"),Ilr=o("mpnet"),jlr=o(" \u2014 "),KO=a("a"),Nlr=o("TFMPNetForMaskedLM"),Dlr=o(" (MPNet model)"),qlr=l(),YC=a("li"),yue=a("strong"),Olr=o("rembert"),Glr=o(" \u2014 "),ZO=a("a"),Xlr=o("TFRemBertForMaskedLM"),Vlr=o(" (RemBERT model)"),zlr=l(),KC=a("li"),wue=a("strong"),Wlr=o("roberta"),Qlr=o(" \u2014 "),eG=a("a"),Hlr=o("TFRobertaForMaskedLM"),Ulr=o(" (RoBERTa model)"),Jlr=l(),ZC=a("li"),Aue=a("strong"),Ylr=o("roformer"),Klr=o(" \u2014 "),oG=a("a"),Zlr=o("TFRoFormerForMaskedLM"),eir=o(" (RoFormer model)"),oir=l(),eM=a("li"),Lue=a("strong"),rir=o("tapas"),tir=o(" \u2014 "),rG=a("a"),air=o("TFTapasForMaskedLM"),sir=o(" (TAPAS model)"),nir=l(),oM=a("li"),Bue=a("strong"),lir=o("xlm"),iir=o(" \u2014 "),tG=a("a"),dir=o("TFXLMWithLMHeadModel"),cir=o(" (XLM model)"),mir=l(),rM=a("li"),xue=a("strong"),fir=o("xlm-roberta"),gir=o(" \u2014 "),aG=a("a"),hir=o("TFXLMRobertaForMaskedLM"),uir=o(" (XLM-RoBERTa model)"),pir=l(),kue=a("p"),_ir=o("Examples:"),bir=l(),m(_0.$$.fragment),Zke=l(),Lc=a("h2"),tM=a("a"),Rue=a("span"),m(b0.$$.fragment),vir=l(),Sue=a("span"),Tir=o("TFAutoModelForSeq2SeqLM"),eRe=l(),Er=a("div"),m(v0.$$.fragment),Fir=l(),Bc=a("p"),Cir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pue=a("code"),Mir=o("from_pretrained()"),Eir=o("class method or the "),$ue=a("code"),yir=o("from_config()"),wir=o(`class
method.`),Air=l(),T0=a("p"),Lir=o("This class cannot be instantiated directly using "),Iue=a("code"),Bir=o("__init__()"),xir=o(" (throws an error)."),kir=l(),bt=a("div"),m(F0.$$.fragment),Rir=l(),jue=a("p"),Sir=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Pir=l(),xc=a("p"),$ir=o(`Note:
Loading a model from its configuration file does `),Nue=a("strong"),Iir=o("not"),jir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Due=a("code"),Nir=o("from_pretrained()"),Dir=o("to load the model weights."),qir=l(),que=a("p"),Oir=o("Examples:"),Gir=l(),m(C0.$$.fragment),Xir=l(),vo=a("div"),m(M0.$$.fragment),Vir=l(),Oue=a("p"),zir=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Wir=l(),Ts=a("p"),Qir=o("The model class to instantiate is selected based on the "),Gue=a("code"),Hir=o("model_type"),Uir=o(` property of the config object (either
passed as an argument or loaded from `),Xue=a("code"),Jir=o("pretrained_model_name_or_path"),Yir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vue=a("code"),Kir=o("pretrained_model_name_or_path"),Zir=o(":"),edr=l(),_e=a("ul"),aM=a("li"),zue=a("strong"),odr=o("bart"),rdr=o(" \u2014 "),sG=a("a"),tdr=o("TFBartForConditionalGeneration"),adr=o(" (BART model)"),sdr=l(),sM=a("li"),Wue=a("strong"),ndr=o("blenderbot"),ldr=o(" \u2014 "),nG=a("a"),idr=o("TFBlenderbotForConditionalGeneration"),ddr=o(" (Blenderbot model)"),cdr=l(),nM=a("li"),Que=a("strong"),mdr=o("blenderbot-small"),fdr=o(" \u2014 "),lG=a("a"),gdr=o("TFBlenderbotSmallForConditionalGeneration"),hdr=o(" (BlenderbotSmall model)"),udr=l(),lM=a("li"),Hue=a("strong"),pdr=o("encoder-decoder"),_dr=o(" \u2014 "),iG=a("a"),bdr=o("TFEncoderDecoderModel"),vdr=o(" (Encoder decoder model)"),Tdr=l(),iM=a("li"),Uue=a("strong"),Fdr=o("led"),Cdr=o(" \u2014 "),dG=a("a"),Mdr=o("TFLEDForConditionalGeneration"),Edr=o(" (LED model)"),ydr=l(),dM=a("li"),Jue=a("strong"),wdr=o("marian"),Adr=o(" \u2014 "),cG=a("a"),Ldr=o("TFMarianMTModel"),Bdr=o(" (Marian model)"),xdr=l(),cM=a("li"),Yue=a("strong"),kdr=o("mbart"),Rdr=o(" \u2014 "),mG=a("a"),Sdr=o("TFMBartForConditionalGeneration"),Pdr=o(" (mBART model)"),$dr=l(),mM=a("li"),Kue=a("strong"),Idr=o("mt5"),jdr=o(" \u2014 "),fG=a("a"),Ndr=o("TFMT5ForConditionalGeneration"),Ddr=o(" (mT5 model)"),qdr=l(),fM=a("li"),Zue=a("strong"),Odr=o("pegasus"),Gdr=o(" \u2014 "),gG=a("a"),Xdr=o("TFPegasusForConditionalGeneration"),Vdr=o(" (Pegasus model)"),zdr=l(),gM=a("li"),epe=a("strong"),Wdr=o("t5"),Qdr=o(" \u2014 "),hG=a("a"),Hdr=o("TFT5ForConditionalGeneration"),Udr=o(" (T5 model)"),Jdr=l(),ope=a("p"),Ydr=o("Examples:"),Kdr=l(),m(E0.$$.fragment),oRe=l(),kc=a("h2"),hM=a("a"),rpe=a("span"),m(y0.$$.fragment),Zdr=l(),tpe=a("span"),ecr=o("TFAutoModelForSequenceClassification"),rRe=l(),yr=a("div"),m(w0.$$.fragment),ocr=l(),Rc=a("p"),rcr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ape=a("code"),tcr=o("from_pretrained()"),acr=o("class method or the "),spe=a("code"),scr=o("from_config()"),ncr=o(`class
method.`),lcr=l(),A0=a("p"),icr=o("This class cannot be instantiated directly using "),npe=a("code"),dcr=o("__init__()"),ccr=o(" (throws an error)."),mcr=l(),vt=a("div"),m(L0.$$.fragment),fcr=l(),lpe=a("p"),gcr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),hcr=l(),Sc=a("p"),ucr=o(`Note:
Loading a model from its configuration file does `),ipe=a("strong"),pcr=o("not"),_cr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dpe=a("code"),bcr=o("from_pretrained()"),vcr=o("to load the model weights."),Tcr=l(),cpe=a("p"),Fcr=o("Examples:"),Ccr=l(),m(B0.$$.fragment),Mcr=l(),To=a("div"),m(x0.$$.fragment),Ecr=l(),mpe=a("p"),ycr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),wcr=l(),Fs=a("p"),Acr=o("The model class to instantiate is selected based on the "),fpe=a("code"),Lcr=o("model_type"),Bcr=o(` property of the config object (either
passed as an argument or loaded from `),gpe=a("code"),xcr=o("pretrained_model_name_or_path"),kcr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hpe=a("code"),Rcr=o("pretrained_model_name_or_path"),Scr=o(":"),Pcr=l(),V=a("ul"),uM=a("li"),upe=a("strong"),$cr=o("albert"),Icr=o(" \u2014 "),uG=a("a"),jcr=o("TFAlbertForSequenceClassification"),Ncr=o(" (ALBERT model)"),Dcr=l(),pM=a("li"),ppe=a("strong"),qcr=o("bert"),Ocr=o(" \u2014 "),pG=a("a"),Gcr=o("TFBertForSequenceClassification"),Xcr=o(" (BERT model)"),Vcr=l(),_M=a("li"),_pe=a("strong"),zcr=o("camembert"),Wcr=o(" \u2014 "),_G=a("a"),Qcr=o("TFCamembertForSequenceClassification"),Hcr=o(" (CamemBERT model)"),Ucr=l(),bM=a("li"),bpe=a("strong"),Jcr=o("convbert"),Ycr=o(" \u2014 "),bG=a("a"),Kcr=o("TFConvBertForSequenceClassification"),Zcr=o(" (ConvBERT model)"),emr=l(),vM=a("li"),vpe=a("strong"),omr=o("ctrl"),rmr=o(" \u2014 "),vG=a("a"),tmr=o("TFCTRLForSequenceClassification"),amr=o(" (CTRL model)"),smr=l(),TM=a("li"),Tpe=a("strong"),nmr=o("deberta"),lmr=o(" \u2014 "),TG=a("a"),imr=o("TFDebertaForSequenceClassification"),dmr=o(" (DeBERTa model)"),cmr=l(),FM=a("li"),Fpe=a("strong"),mmr=o("deberta-v2"),fmr=o(" \u2014 "),FG=a("a"),gmr=o("TFDebertaV2ForSequenceClassification"),hmr=o(" (DeBERTa-v2 model)"),umr=l(),CM=a("li"),Cpe=a("strong"),pmr=o("distilbert"),_mr=o(" \u2014 "),CG=a("a"),bmr=o("TFDistilBertForSequenceClassification"),vmr=o(" (DistilBERT model)"),Tmr=l(),MM=a("li"),Mpe=a("strong"),Fmr=o("electra"),Cmr=o(" \u2014 "),MG=a("a"),Mmr=o("TFElectraForSequenceClassification"),Emr=o(" (ELECTRA model)"),ymr=l(),EM=a("li"),Epe=a("strong"),wmr=o("flaubert"),Amr=o(" \u2014 "),EG=a("a"),Lmr=o("TFFlaubertForSequenceClassification"),Bmr=o(" (FlauBERT model)"),xmr=l(),yM=a("li"),ype=a("strong"),kmr=o("funnel"),Rmr=o(" \u2014 "),yG=a("a"),Smr=o("TFFunnelForSequenceClassification"),Pmr=o(" (Funnel Transformer model)"),$mr=l(),wM=a("li"),wpe=a("strong"),Imr=o("gpt2"),jmr=o(" \u2014 "),wG=a("a"),Nmr=o("TFGPT2ForSequenceClassification"),Dmr=o(" (OpenAI GPT-2 model)"),qmr=l(),AM=a("li"),Ape=a("strong"),Omr=o("layoutlm"),Gmr=o(" \u2014 "),AG=a("a"),Xmr=o("TFLayoutLMForSequenceClassification"),Vmr=o(" (LayoutLM model)"),zmr=l(),LM=a("li"),Lpe=a("strong"),Wmr=o("longformer"),Qmr=o(" \u2014 "),LG=a("a"),Hmr=o("TFLongformerForSequenceClassification"),Umr=o(" (Longformer model)"),Jmr=l(),BM=a("li"),Bpe=a("strong"),Ymr=o("mobilebert"),Kmr=o(" \u2014 "),BG=a("a"),Zmr=o("TFMobileBertForSequenceClassification"),efr=o(" (MobileBERT model)"),ofr=l(),xM=a("li"),xpe=a("strong"),rfr=o("mpnet"),tfr=o(" \u2014 "),xG=a("a"),afr=o("TFMPNetForSequenceClassification"),sfr=o(" (MPNet model)"),nfr=l(),kM=a("li"),kpe=a("strong"),lfr=o("openai-gpt"),ifr=o(" \u2014 "),kG=a("a"),dfr=o("TFOpenAIGPTForSequenceClassification"),cfr=o(" (OpenAI GPT model)"),mfr=l(),RM=a("li"),Rpe=a("strong"),ffr=o("rembert"),gfr=o(" \u2014 "),RG=a("a"),hfr=o("TFRemBertForSequenceClassification"),ufr=o(" (RemBERT model)"),pfr=l(),SM=a("li"),Spe=a("strong"),_fr=o("roberta"),bfr=o(" \u2014 "),SG=a("a"),vfr=o("TFRobertaForSequenceClassification"),Tfr=o(" (RoBERTa model)"),Ffr=l(),PM=a("li"),Ppe=a("strong"),Cfr=o("roformer"),Mfr=o(" \u2014 "),PG=a("a"),Efr=o("TFRoFormerForSequenceClassification"),yfr=o(" (RoFormer model)"),wfr=l(),$M=a("li"),$pe=a("strong"),Afr=o("tapas"),Lfr=o(" \u2014 "),$G=a("a"),Bfr=o("TFTapasForSequenceClassification"),xfr=o(" (TAPAS model)"),kfr=l(),IM=a("li"),Ipe=a("strong"),Rfr=o("transfo-xl"),Sfr=o(" \u2014 "),IG=a("a"),Pfr=o("TFTransfoXLForSequenceClassification"),$fr=o(" (Transformer-XL model)"),Ifr=l(),jM=a("li"),jpe=a("strong"),jfr=o("xlm"),Nfr=o(" \u2014 "),jG=a("a"),Dfr=o("TFXLMForSequenceClassification"),qfr=o(" (XLM model)"),Ofr=l(),NM=a("li"),Npe=a("strong"),Gfr=o("xlm-roberta"),Xfr=o(" \u2014 "),NG=a("a"),Vfr=o("TFXLMRobertaForSequenceClassification"),zfr=o(" (XLM-RoBERTa model)"),Wfr=l(),DM=a("li"),Dpe=a("strong"),Qfr=o("xlnet"),Hfr=o(" \u2014 "),DG=a("a"),Ufr=o("TFXLNetForSequenceClassification"),Jfr=o(" (XLNet model)"),Yfr=l(),qpe=a("p"),Kfr=o("Examples:"),Zfr=l(),m(k0.$$.fragment),tRe=l(),Pc=a("h2"),qM=a("a"),Ope=a("span"),m(R0.$$.fragment),egr=l(),Gpe=a("span"),ogr=o("TFAutoModelForMultipleChoice"),aRe=l(),wr=a("div"),m(S0.$$.fragment),rgr=l(),$c=a("p"),tgr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Xpe=a("code"),agr=o("from_pretrained()"),sgr=o("class method or the "),Vpe=a("code"),ngr=o("from_config()"),lgr=o(`class
method.`),igr=l(),P0=a("p"),dgr=o("This class cannot be instantiated directly using "),zpe=a("code"),cgr=o("__init__()"),mgr=o(" (throws an error)."),fgr=l(),Tt=a("div"),m($0.$$.fragment),ggr=l(),Wpe=a("p"),hgr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),ugr=l(),Ic=a("p"),pgr=o(`Note:
Loading a model from its configuration file does `),Qpe=a("strong"),_gr=o("not"),bgr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hpe=a("code"),vgr=o("from_pretrained()"),Tgr=o("to load the model weights."),Fgr=l(),Upe=a("p"),Cgr=o("Examples:"),Mgr=l(),m(I0.$$.fragment),Egr=l(),Fo=a("div"),m(j0.$$.fragment),ygr=l(),Jpe=a("p"),wgr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Agr=l(),Cs=a("p"),Lgr=o("The model class to instantiate is selected based on the "),Ype=a("code"),Bgr=o("model_type"),xgr=o(` property of the config object (either
passed as an argument or loaded from `),Kpe=a("code"),kgr=o("pretrained_model_name_or_path"),Rgr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zpe=a("code"),Sgr=o("pretrained_model_name_or_path"),Pgr=o(":"),$gr=l(),ae=a("ul"),OM=a("li"),e_e=a("strong"),Igr=o("albert"),jgr=o(" \u2014 "),qG=a("a"),Ngr=o("TFAlbertForMultipleChoice"),Dgr=o(" (ALBERT model)"),qgr=l(),GM=a("li"),o_e=a("strong"),Ogr=o("bert"),Ggr=o(" \u2014 "),OG=a("a"),Xgr=o("TFBertForMultipleChoice"),Vgr=o(" (BERT model)"),zgr=l(),XM=a("li"),r_e=a("strong"),Wgr=o("camembert"),Qgr=o(" \u2014 "),GG=a("a"),Hgr=o("TFCamembertForMultipleChoice"),Ugr=o(" (CamemBERT model)"),Jgr=l(),VM=a("li"),t_e=a("strong"),Ygr=o("convbert"),Kgr=o(" \u2014 "),XG=a("a"),Zgr=o("TFConvBertForMultipleChoice"),ehr=o(" (ConvBERT model)"),ohr=l(),zM=a("li"),a_e=a("strong"),rhr=o("distilbert"),thr=o(" \u2014 "),VG=a("a"),ahr=o("TFDistilBertForMultipleChoice"),shr=o(" (DistilBERT model)"),nhr=l(),WM=a("li"),s_e=a("strong"),lhr=o("electra"),ihr=o(" \u2014 "),zG=a("a"),dhr=o("TFElectraForMultipleChoice"),chr=o(" (ELECTRA model)"),mhr=l(),QM=a("li"),n_e=a("strong"),fhr=o("flaubert"),ghr=o(" \u2014 "),WG=a("a"),hhr=o("TFFlaubertForMultipleChoice"),uhr=o(" (FlauBERT model)"),phr=l(),HM=a("li"),l_e=a("strong"),_hr=o("funnel"),bhr=o(" \u2014 "),QG=a("a"),vhr=o("TFFunnelForMultipleChoice"),Thr=o(" (Funnel Transformer model)"),Fhr=l(),UM=a("li"),i_e=a("strong"),Chr=o("longformer"),Mhr=o(" \u2014 "),HG=a("a"),Ehr=o("TFLongformerForMultipleChoice"),yhr=o(" (Longformer model)"),whr=l(),JM=a("li"),d_e=a("strong"),Ahr=o("mobilebert"),Lhr=o(" \u2014 "),UG=a("a"),Bhr=o("TFMobileBertForMultipleChoice"),xhr=o(" (MobileBERT model)"),khr=l(),YM=a("li"),c_e=a("strong"),Rhr=o("mpnet"),Shr=o(" \u2014 "),JG=a("a"),Phr=o("TFMPNetForMultipleChoice"),$hr=o(" (MPNet model)"),Ihr=l(),KM=a("li"),m_e=a("strong"),jhr=o("rembert"),Nhr=o(" \u2014 "),YG=a("a"),Dhr=o("TFRemBertForMultipleChoice"),qhr=o(" (RemBERT model)"),Ohr=l(),ZM=a("li"),f_e=a("strong"),Ghr=o("roberta"),Xhr=o(" \u2014 "),KG=a("a"),Vhr=o("TFRobertaForMultipleChoice"),zhr=o(" (RoBERTa model)"),Whr=l(),e4=a("li"),g_e=a("strong"),Qhr=o("roformer"),Hhr=o(" \u2014 "),ZG=a("a"),Uhr=o("TFRoFormerForMultipleChoice"),Jhr=o(" (RoFormer model)"),Yhr=l(),o4=a("li"),h_e=a("strong"),Khr=o("xlm"),Zhr=o(" \u2014 "),eX=a("a"),eur=o("TFXLMForMultipleChoice"),our=o(" (XLM model)"),rur=l(),r4=a("li"),u_e=a("strong"),tur=o("xlm-roberta"),aur=o(" \u2014 "),oX=a("a"),sur=o("TFXLMRobertaForMultipleChoice"),nur=o(" (XLM-RoBERTa model)"),lur=l(),t4=a("li"),p_e=a("strong"),iur=o("xlnet"),dur=o(" \u2014 "),rX=a("a"),cur=o("TFXLNetForMultipleChoice"),mur=o(" (XLNet model)"),fur=l(),__e=a("p"),gur=o("Examples:"),hur=l(),m(N0.$$.fragment),sRe=l(),jc=a("h2"),a4=a("a"),b_e=a("span"),m(D0.$$.fragment),uur=l(),v_e=a("span"),pur=o("TFAutoModelForTableQuestionAnswering"),nRe=l(),Ar=a("div"),m(q0.$$.fragment),_ur=l(),Nc=a("p"),bur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),T_e=a("code"),vur=o("from_pretrained()"),Tur=o("class method or the "),F_e=a("code"),Fur=o("from_config()"),Cur=o(`class
method.`),Mur=l(),O0=a("p"),Eur=o("This class cannot be instantiated directly using "),C_e=a("code"),yur=o("__init__()"),wur=o(" (throws an error)."),Aur=l(),Ft=a("div"),m(G0.$$.fragment),Lur=l(),M_e=a("p"),Bur=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),xur=l(),Dc=a("p"),kur=o(`Note:
Loading a model from its configuration file does `),E_e=a("strong"),Rur=o("not"),Sur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),y_e=a("code"),Pur=o("from_pretrained()"),$ur=o("to load the model weights."),Iur=l(),w_e=a("p"),jur=o("Examples:"),Nur=l(),m(X0.$$.fragment),Dur=l(),Co=a("div"),m(V0.$$.fragment),qur=l(),A_e=a("p"),Our=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Gur=l(),Ms=a("p"),Xur=o("The model class to instantiate is selected based on the "),L_e=a("code"),Vur=o("model_type"),zur=o(` property of the config object (either
passed as an argument or loaded from `),B_e=a("code"),Wur=o("pretrained_model_name_or_path"),Qur=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x_e=a("code"),Hur=o("pretrained_model_name_or_path"),Uur=o(":"),Jur=l(),k_e=a("ul"),s4=a("li"),R_e=a("strong"),Yur=o("tapas"),Kur=o(" \u2014 "),tX=a("a"),Zur=o("TFTapasForQuestionAnswering"),epr=o(" (TAPAS model)"),opr=l(),S_e=a("p"),rpr=o("Examples:"),tpr=l(),m(z0.$$.fragment),lRe=l(),qc=a("h2"),n4=a("a"),P_e=a("span"),m(W0.$$.fragment),apr=l(),$_e=a("span"),spr=o("TFAutoModelForTokenClassification"),iRe=l(),Lr=a("div"),m(Q0.$$.fragment),npr=l(),Oc=a("p"),lpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),I_e=a("code"),ipr=o("from_pretrained()"),dpr=o("class method or the "),j_e=a("code"),cpr=o("from_config()"),mpr=o(`class
method.`),fpr=l(),H0=a("p"),gpr=o("This class cannot be instantiated directly using "),N_e=a("code"),hpr=o("__init__()"),upr=o(" (throws an error)."),ppr=l(),Ct=a("div"),m(U0.$$.fragment),_pr=l(),D_e=a("p"),bpr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),vpr=l(),Gc=a("p"),Tpr=o(`Note:
Loading a model from its configuration file does `),q_e=a("strong"),Fpr=o("not"),Cpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),O_e=a("code"),Mpr=o("from_pretrained()"),Epr=o("to load the model weights."),ypr=l(),G_e=a("p"),wpr=o("Examples:"),Apr=l(),m(J0.$$.fragment),Lpr=l(),Mo=a("div"),m(Y0.$$.fragment),Bpr=l(),X_e=a("p"),xpr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),kpr=l(),Es=a("p"),Rpr=o("The model class to instantiate is selected based on the "),V_e=a("code"),Spr=o("model_type"),Ppr=o(` property of the config object (either
passed as an argument or loaded from `),z_e=a("code"),$pr=o("pretrained_model_name_or_path"),Ipr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),W_e=a("code"),jpr=o("pretrained_model_name_or_path"),Npr=o(":"),Dpr=l(),K=a("ul"),l4=a("li"),Q_e=a("strong"),qpr=o("albert"),Opr=o(" \u2014 "),aX=a("a"),Gpr=o("TFAlbertForTokenClassification"),Xpr=o(" (ALBERT model)"),Vpr=l(),i4=a("li"),H_e=a("strong"),zpr=o("bert"),Wpr=o(" \u2014 "),sX=a("a"),Qpr=o("TFBertForTokenClassification"),Hpr=o(" (BERT model)"),Upr=l(),d4=a("li"),U_e=a("strong"),Jpr=o("camembert"),Ypr=o(" \u2014 "),nX=a("a"),Kpr=o("TFCamembertForTokenClassification"),Zpr=o(" (CamemBERT model)"),e_r=l(),c4=a("li"),J_e=a("strong"),o_r=o("convbert"),r_r=o(" \u2014 "),lX=a("a"),t_r=o("TFConvBertForTokenClassification"),a_r=o(" (ConvBERT model)"),s_r=l(),m4=a("li"),Y_e=a("strong"),n_r=o("deberta"),l_r=o(" \u2014 "),iX=a("a"),i_r=o("TFDebertaForTokenClassification"),d_r=o(" (DeBERTa model)"),c_r=l(),f4=a("li"),K_e=a("strong"),m_r=o("deberta-v2"),f_r=o(" \u2014 "),dX=a("a"),g_r=o("TFDebertaV2ForTokenClassification"),h_r=o(" (DeBERTa-v2 model)"),u_r=l(),g4=a("li"),Z_e=a("strong"),p_r=o("distilbert"),__r=o(" \u2014 "),cX=a("a"),b_r=o("TFDistilBertForTokenClassification"),v_r=o(" (DistilBERT model)"),T_r=l(),h4=a("li"),ebe=a("strong"),F_r=o("electra"),C_r=o(" \u2014 "),mX=a("a"),M_r=o("TFElectraForTokenClassification"),E_r=o(" (ELECTRA model)"),y_r=l(),u4=a("li"),obe=a("strong"),w_r=o("flaubert"),A_r=o(" \u2014 "),fX=a("a"),L_r=o("TFFlaubertForTokenClassification"),B_r=o(" (FlauBERT model)"),x_r=l(),p4=a("li"),rbe=a("strong"),k_r=o("funnel"),R_r=o(" \u2014 "),gX=a("a"),S_r=o("TFFunnelForTokenClassification"),P_r=o(" (Funnel Transformer model)"),$_r=l(),_4=a("li"),tbe=a("strong"),I_r=o("layoutlm"),j_r=o(" \u2014 "),hX=a("a"),N_r=o("TFLayoutLMForTokenClassification"),D_r=o(" (LayoutLM model)"),q_r=l(),b4=a("li"),abe=a("strong"),O_r=o("longformer"),G_r=o(" \u2014 "),uX=a("a"),X_r=o("TFLongformerForTokenClassification"),V_r=o(" (Longformer model)"),z_r=l(),v4=a("li"),sbe=a("strong"),W_r=o("mobilebert"),Q_r=o(" \u2014 "),pX=a("a"),H_r=o("TFMobileBertForTokenClassification"),U_r=o(" (MobileBERT model)"),J_r=l(),T4=a("li"),nbe=a("strong"),Y_r=o("mpnet"),K_r=o(" \u2014 "),_X=a("a"),Z_r=o("TFMPNetForTokenClassification"),ebr=o(" (MPNet model)"),obr=l(),F4=a("li"),lbe=a("strong"),rbr=o("rembert"),tbr=o(" \u2014 "),bX=a("a"),abr=o("TFRemBertForTokenClassification"),sbr=o(" (RemBERT model)"),nbr=l(),C4=a("li"),ibe=a("strong"),lbr=o("roberta"),ibr=o(" \u2014 "),vX=a("a"),dbr=o("TFRobertaForTokenClassification"),cbr=o(" (RoBERTa model)"),mbr=l(),M4=a("li"),dbe=a("strong"),fbr=o("roformer"),gbr=o(" \u2014 "),TX=a("a"),hbr=o("TFRoFormerForTokenClassification"),ubr=o(" (RoFormer model)"),pbr=l(),E4=a("li"),cbe=a("strong"),_br=o("xlm"),bbr=o(" \u2014 "),FX=a("a"),vbr=o("TFXLMForTokenClassification"),Tbr=o(" (XLM model)"),Fbr=l(),y4=a("li"),mbe=a("strong"),Cbr=o("xlm-roberta"),Mbr=o(" \u2014 "),CX=a("a"),Ebr=o("TFXLMRobertaForTokenClassification"),ybr=o(" (XLM-RoBERTa model)"),wbr=l(),w4=a("li"),fbe=a("strong"),Abr=o("xlnet"),Lbr=o(" \u2014 "),MX=a("a"),Bbr=o("TFXLNetForTokenClassification"),xbr=o(" (XLNet model)"),kbr=l(),gbe=a("p"),Rbr=o("Examples:"),Sbr=l(),m(K0.$$.fragment),dRe=l(),Xc=a("h2"),A4=a("a"),hbe=a("span"),m(Z0.$$.fragment),Pbr=l(),ube=a("span"),$br=o("TFAutoModelForQuestionAnswering"),cRe=l(),Br=a("div"),m(eL.$$.fragment),Ibr=l(),Vc=a("p"),jbr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),pbe=a("code"),Nbr=o("from_pretrained()"),Dbr=o("class method or the "),_be=a("code"),qbr=o("from_config()"),Obr=o(`class
method.`),Gbr=l(),oL=a("p"),Xbr=o("This class cannot be instantiated directly using "),bbe=a("code"),Vbr=o("__init__()"),zbr=o(" (throws an error)."),Wbr=l(),Mt=a("div"),m(rL.$$.fragment),Qbr=l(),vbe=a("p"),Hbr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Ubr=l(),zc=a("p"),Jbr=o(`Note:
Loading a model from its configuration file does `),Tbe=a("strong"),Ybr=o("not"),Kbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fbe=a("code"),Zbr=o("from_pretrained()"),e2r=o("to load the model weights."),o2r=l(),Cbe=a("p"),r2r=o("Examples:"),t2r=l(),m(tL.$$.fragment),a2r=l(),Eo=a("div"),m(aL.$$.fragment),s2r=l(),Mbe=a("p"),n2r=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),l2r=l(),ys=a("p"),i2r=o("The model class to instantiate is selected based on the "),Ebe=a("code"),d2r=o("model_type"),c2r=o(` property of the config object (either
passed as an argument or loaded from `),ybe=a("code"),m2r=o("pretrained_model_name_or_path"),f2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wbe=a("code"),g2r=o("pretrained_model_name_or_path"),h2r=o(":"),u2r=l(),Z=a("ul"),L4=a("li"),Abe=a("strong"),p2r=o("albert"),_2r=o(" \u2014 "),EX=a("a"),b2r=o("TFAlbertForQuestionAnswering"),v2r=o(" (ALBERT model)"),T2r=l(),B4=a("li"),Lbe=a("strong"),F2r=o("bert"),C2r=o(" \u2014 "),yX=a("a"),M2r=o("TFBertForQuestionAnswering"),E2r=o(" (BERT model)"),y2r=l(),x4=a("li"),Bbe=a("strong"),w2r=o("camembert"),A2r=o(" \u2014 "),wX=a("a"),L2r=o("TFCamembertForQuestionAnswering"),B2r=o(" (CamemBERT model)"),x2r=l(),k4=a("li"),xbe=a("strong"),k2r=o("convbert"),R2r=o(" \u2014 "),AX=a("a"),S2r=o("TFConvBertForQuestionAnswering"),P2r=o(" (ConvBERT model)"),$2r=l(),R4=a("li"),kbe=a("strong"),I2r=o("deberta"),j2r=o(" \u2014 "),LX=a("a"),N2r=o("TFDebertaForQuestionAnswering"),D2r=o(" (DeBERTa model)"),q2r=l(),S4=a("li"),Rbe=a("strong"),O2r=o("deberta-v2"),G2r=o(" \u2014 "),BX=a("a"),X2r=o("TFDebertaV2ForQuestionAnswering"),V2r=o(" (DeBERTa-v2 model)"),z2r=l(),P4=a("li"),Sbe=a("strong"),W2r=o("distilbert"),Q2r=o(" \u2014 "),xX=a("a"),H2r=o("TFDistilBertForQuestionAnswering"),U2r=o(" (DistilBERT model)"),J2r=l(),$4=a("li"),Pbe=a("strong"),Y2r=o("electra"),K2r=o(" \u2014 "),kX=a("a"),Z2r=o("TFElectraForQuestionAnswering"),evr=o(" (ELECTRA model)"),ovr=l(),I4=a("li"),$be=a("strong"),rvr=o("flaubert"),tvr=o(" \u2014 "),RX=a("a"),avr=o("TFFlaubertForQuestionAnsweringSimple"),svr=o(" (FlauBERT model)"),nvr=l(),j4=a("li"),Ibe=a("strong"),lvr=o("funnel"),ivr=o(" \u2014 "),SX=a("a"),dvr=o("TFFunnelForQuestionAnswering"),cvr=o(" (Funnel Transformer model)"),mvr=l(),N4=a("li"),jbe=a("strong"),fvr=o("longformer"),gvr=o(" \u2014 "),PX=a("a"),hvr=o("TFLongformerForQuestionAnswering"),uvr=o(" (Longformer model)"),pvr=l(),D4=a("li"),Nbe=a("strong"),_vr=o("mobilebert"),bvr=o(" \u2014 "),$X=a("a"),vvr=o("TFMobileBertForQuestionAnswering"),Tvr=o(" (MobileBERT model)"),Fvr=l(),q4=a("li"),Dbe=a("strong"),Cvr=o("mpnet"),Mvr=o(" \u2014 "),IX=a("a"),Evr=o("TFMPNetForQuestionAnswering"),yvr=o(" (MPNet model)"),wvr=l(),O4=a("li"),qbe=a("strong"),Avr=o("rembert"),Lvr=o(" \u2014 "),jX=a("a"),Bvr=o("TFRemBertForQuestionAnswering"),xvr=o(" (RemBERT model)"),kvr=l(),G4=a("li"),Obe=a("strong"),Rvr=o("roberta"),Svr=o(" \u2014 "),NX=a("a"),Pvr=o("TFRobertaForQuestionAnswering"),$vr=o(" (RoBERTa model)"),Ivr=l(),X4=a("li"),Gbe=a("strong"),jvr=o("roformer"),Nvr=o(" \u2014 "),DX=a("a"),Dvr=o("TFRoFormerForQuestionAnswering"),qvr=o(" (RoFormer model)"),Ovr=l(),V4=a("li"),Xbe=a("strong"),Gvr=o("xlm"),Xvr=o(" \u2014 "),qX=a("a"),Vvr=o("TFXLMForQuestionAnsweringSimple"),zvr=o(" (XLM model)"),Wvr=l(),z4=a("li"),Vbe=a("strong"),Qvr=o("xlm-roberta"),Hvr=o(" \u2014 "),OX=a("a"),Uvr=o("TFXLMRobertaForQuestionAnswering"),Jvr=o(" (XLM-RoBERTa model)"),Yvr=l(),W4=a("li"),zbe=a("strong"),Kvr=o("xlnet"),Zvr=o(" \u2014 "),GX=a("a"),eTr=o("TFXLNetForQuestionAnsweringSimple"),oTr=o(" (XLNet model)"),rTr=l(),Wbe=a("p"),tTr=o("Examples:"),aTr=l(),m(sL.$$.fragment),mRe=l(),Wc=a("h2"),Q4=a("a"),Qbe=a("span"),m(nL.$$.fragment),sTr=l(),Hbe=a("span"),nTr=o("TFAutoModelForVision2Seq"),fRe=l(),xr=a("div"),m(lL.$$.fragment),lTr=l(),Qc=a("p"),iTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ube=a("code"),dTr=o("from_pretrained()"),cTr=o("class method or the "),Jbe=a("code"),mTr=o("from_config()"),fTr=o(`class
method.`),gTr=l(),iL=a("p"),hTr=o("This class cannot be instantiated directly using "),Ybe=a("code"),uTr=o("__init__()"),pTr=o(" (throws an error)."),_Tr=l(),Et=a("div"),m(dL.$$.fragment),bTr=l(),Kbe=a("p"),vTr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),TTr=l(),Hc=a("p"),FTr=o(`Note:
Loading a model from its configuration file does `),Zbe=a("strong"),CTr=o("not"),MTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),e2e=a("code"),ETr=o("from_pretrained()"),yTr=o("to load the model weights."),wTr=l(),o2e=a("p"),ATr=o("Examples:"),LTr=l(),m(cL.$$.fragment),BTr=l(),yo=a("div"),m(mL.$$.fragment),xTr=l(),r2e=a("p"),kTr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),RTr=l(),ws=a("p"),STr=o("The model class to instantiate is selected based on the "),t2e=a("code"),PTr=o("model_type"),$Tr=o(` property of the config object (either
passed as an argument or loaded from `),a2e=a("code"),ITr=o("pretrained_model_name_or_path"),jTr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s2e=a("code"),NTr=o("pretrained_model_name_or_path"),DTr=o(":"),qTr=l(),n2e=a("ul"),H4=a("li"),l2e=a("strong"),OTr=o("vision-encoder-decoder"),GTr=o(" \u2014 "),XX=a("a"),XTr=o("TFVisionEncoderDecoderModel"),VTr=o(" (Vision Encoder decoder model)"),zTr=l(),i2e=a("p"),WTr=o("Examples:"),QTr=l(),m(fL.$$.fragment),gRe=l(),Uc=a("h2"),U4=a("a"),d2e=a("span"),m(gL.$$.fragment),HTr=l(),c2e=a("span"),UTr=o("TFAutoModelForSpeechSeq2Seq"),hRe=l(),kr=a("div"),m(hL.$$.fragment),JTr=l(),Jc=a("p"),YTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),m2e=a("code"),KTr=o("from_pretrained()"),ZTr=o("class method or the "),f2e=a("code"),e1r=o("from_config()"),o1r=o(`class
method.`),r1r=l(),uL=a("p"),t1r=o("This class cannot be instantiated directly using "),g2e=a("code"),a1r=o("__init__()"),s1r=o(" (throws an error)."),n1r=l(),yt=a("div"),m(pL.$$.fragment),l1r=l(),h2e=a("p"),i1r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),d1r=l(),Yc=a("p"),c1r=o(`Note:
Loading a model from its configuration file does `),u2e=a("strong"),m1r=o("not"),f1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),p2e=a("code"),g1r=o("from_pretrained()"),h1r=o("to load the model weights."),u1r=l(),_2e=a("p"),p1r=o("Examples:"),_1r=l(),m(_L.$$.fragment),b1r=l(),wo=a("div"),m(bL.$$.fragment),v1r=l(),b2e=a("p"),T1r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),F1r=l(),As=a("p"),C1r=o("The model class to instantiate is selected based on the "),v2e=a("code"),M1r=o("model_type"),E1r=o(` property of the config object (either
passed as an argument or loaded from `),T2e=a("code"),y1r=o("pretrained_model_name_or_path"),w1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),F2e=a("code"),A1r=o("pretrained_model_name_or_path"),L1r=o(":"),B1r=l(),C2e=a("ul"),J4=a("li"),M2e=a("strong"),x1r=o("speech_to_text"),k1r=o(" \u2014 "),VX=a("a"),R1r=o("TFSpeech2TextForConditionalGeneration"),S1r=o(" (Speech2Text model)"),P1r=l(),E2e=a("p"),$1r=o("Examples:"),I1r=l(),m(vL.$$.fragment),uRe=l(),Kc=a("h2"),Y4=a("a"),y2e=a("span"),m(TL.$$.fragment),j1r=l(),w2e=a("span"),N1r=o("FlaxAutoModel"),pRe=l(),Rr=a("div"),m(FL.$$.fragment),D1r=l(),Zc=a("p"),q1r=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),A2e=a("code"),O1r=o("from_pretrained()"),G1r=o("class method or the "),L2e=a("code"),X1r=o("from_config()"),V1r=o(`class
method.`),z1r=l(),CL=a("p"),W1r=o("This class cannot be instantiated directly using "),B2e=a("code"),Q1r=o("__init__()"),H1r=o(" (throws an error)."),U1r=l(),wt=a("div"),m(ML.$$.fragment),J1r=l(),x2e=a("p"),Y1r=o("Instantiates one of the base model classes of the library from a configuration."),K1r=l(),em=a("p"),Z1r=o(`Note:
Loading a model from its configuration file does `),k2e=a("strong"),eFr=o("not"),oFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),R2e=a("code"),rFr=o("from_pretrained()"),tFr=o("to load the model weights."),aFr=l(),S2e=a("p"),sFr=o("Examples:"),nFr=l(),m(EL.$$.fragment),lFr=l(),Ao=a("div"),m(yL.$$.fragment),iFr=l(),P2e=a("p"),dFr=o("Instantiate one of the base model classes of the library from a pretrained model."),cFr=l(),Ls=a("p"),mFr=o("The model class to instantiate is selected based on the "),$2e=a("code"),fFr=o("model_type"),gFr=o(` property of the config object (either
passed as an argument or loaded from `),I2e=a("code"),hFr=o("pretrained_model_name_or_path"),uFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j2e=a("code"),pFr=o("pretrained_model_name_or_path"),_Fr=o(":"),bFr=l(),z=a("ul"),K4=a("li"),N2e=a("strong"),vFr=o("albert"),TFr=o(" \u2014 "),zX=a("a"),FFr=o("FlaxAlbertModel"),CFr=o(" (ALBERT model)"),MFr=l(),Z4=a("li"),D2e=a("strong"),EFr=o("bart"),yFr=o(" \u2014 "),WX=a("a"),wFr=o("FlaxBartModel"),AFr=o(" (BART model)"),LFr=l(),eE=a("li"),q2e=a("strong"),BFr=o("beit"),xFr=o(" \u2014 "),QX=a("a"),kFr=o("FlaxBeitModel"),RFr=o(" (BEiT model)"),SFr=l(),oE=a("li"),O2e=a("strong"),PFr=o("bert"),$Fr=o(" \u2014 "),HX=a("a"),IFr=o("FlaxBertModel"),jFr=o(" (BERT model)"),NFr=l(),rE=a("li"),G2e=a("strong"),DFr=o("big_bird"),qFr=o(" \u2014 "),UX=a("a"),OFr=o("FlaxBigBirdModel"),GFr=o(" (BigBird model)"),XFr=l(),tE=a("li"),X2e=a("strong"),VFr=o("blenderbot"),zFr=o(" \u2014 "),JX=a("a"),WFr=o("FlaxBlenderbotModel"),QFr=o(" (Blenderbot model)"),HFr=l(),aE=a("li"),V2e=a("strong"),UFr=o("blenderbot-small"),JFr=o(" \u2014 "),YX=a("a"),YFr=o("FlaxBlenderbotSmallModel"),KFr=o(" (BlenderbotSmall model)"),ZFr=l(),sE=a("li"),z2e=a("strong"),eCr=o("clip"),oCr=o(" \u2014 "),KX=a("a"),rCr=o("FlaxCLIPModel"),tCr=o(" (CLIP model)"),aCr=l(),nE=a("li"),W2e=a("strong"),sCr=o("distilbert"),nCr=o(" \u2014 "),ZX=a("a"),lCr=o("FlaxDistilBertModel"),iCr=o(" (DistilBERT model)"),dCr=l(),lE=a("li"),Q2e=a("strong"),cCr=o("electra"),mCr=o(" \u2014 "),eV=a("a"),fCr=o("FlaxElectraModel"),gCr=o(" (ELECTRA model)"),hCr=l(),iE=a("li"),H2e=a("strong"),uCr=o("gpt2"),pCr=o(" \u2014 "),oV=a("a"),_Cr=o("FlaxGPT2Model"),bCr=o(" (OpenAI GPT-2 model)"),vCr=l(),dE=a("li"),U2e=a("strong"),TCr=o("gpt_neo"),FCr=o(" \u2014 "),rV=a("a"),CCr=o("FlaxGPTNeoModel"),MCr=o(" (GPT Neo model)"),ECr=l(),cE=a("li"),J2e=a("strong"),yCr=o("gptj"),wCr=o(" \u2014 "),tV=a("a"),ACr=o("FlaxGPTJModel"),LCr=o(" (GPT-J model)"),BCr=l(),mE=a("li"),Y2e=a("strong"),xCr=o("marian"),kCr=o(" \u2014 "),aV=a("a"),RCr=o("FlaxMarianModel"),SCr=o(" (Marian model)"),PCr=l(),fE=a("li"),K2e=a("strong"),$Cr=o("mbart"),ICr=o(" \u2014 "),sV=a("a"),jCr=o("FlaxMBartModel"),NCr=o(" (mBART model)"),DCr=l(),gE=a("li"),Z2e=a("strong"),qCr=o("mt5"),OCr=o(" \u2014 "),nV=a("a"),GCr=o("FlaxMT5Model"),XCr=o(" (mT5 model)"),VCr=l(),hE=a("li"),eve=a("strong"),zCr=o("pegasus"),WCr=o(" \u2014 "),lV=a("a"),QCr=o("FlaxPegasusModel"),HCr=o(" (Pegasus model)"),UCr=l(),uE=a("li"),ove=a("strong"),JCr=o("roberta"),YCr=o(" \u2014 "),iV=a("a"),KCr=o("FlaxRobertaModel"),ZCr=o(" (RoBERTa model)"),eMr=l(),pE=a("li"),rve=a("strong"),oMr=o("roformer"),rMr=o(" \u2014 "),dV=a("a"),tMr=o("FlaxRoFormerModel"),aMr=o(" (RoFormer model)"),sMr=l(),_E=a("li"),tve=a("strong"),nMr=o("t5"),lMr=o(" \u2014 "),cV=a("a"),iMr=o("FlaxT5Model"),dMr=o(" (T5 model)"),cMr=l(),bE=a("li"),ave=a("strong"),mMr=o("vision-text-dual-encoder"),fMr=o(" \u2014 "),mV=a("a"),gMr=o("FlaxVisionTextDualEncoderModel"),hMr=o(" (VisionTextDualEncoder model)"),uMr=l(),vE=a("li"),sve=a("strong"),pMr=o("vit"),_Mr=o(" \u2014 "),fV=a("a"),bMr=o("FlaxViTModel"),vMr=o(" (ViT model)"),TMr=l(),TE=a("li"),nve=a("strong"),FMr=o("wav2vec2"),CMr=o(" \u2014 "),gV=a("a"),MMr=o("FlaxWav2Vec2Model"),EMr=o(" (Wav2Vec2 model)"),yMr=l(),FE=a("li"),lve=a("strong"),wMr=o("xglm"),AMr=o(" \u2014 "),hV=a("a"),LMr=o("FlaxXGLMModel"),BMr=o(" (XGLM model)"),xMr=l(),CE=a("li"),ive=a("strong"),kMr=o("xlm-roberta"),RMr=o(" \u2014 "),uV=a("a"),SMr=o("FlaxXLMRobertaModel"),PMr=o(" (XLM-RoBERTa model)"),$Mr=l(),dve=a("p"),IMr=o("Examples:"),jMr=l(),m(wL.$$.fragment),_Re=l(),om=a("h2"),ME=a("a"),cve=a("span"),m(AL.$$.fragment),NMr=l(),mve=a("span"),DMr=o("FlaxAutoModelForCausalLM"),bRe=l(),Sr=a("div"),m(LL.$$.fragment),qMr=l(),rm=a("p"),OMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),fve=a("code"),GMr=o("from_pretrained()"),XMr=o("class method or the "),gve=a("code"),VMr=o("from_config()"),zMr=o(`class
method.`),WMr=l(),BL=a("p"),QMr=o("This class cannot be instantiated directly using "),hve=a("code"),HMr=o("__init__()"),UMr=o(" (throws an error)."),JMr=l(),At=a("div"),m(xL.$$.fragment),YMr=l(),uve=a("p"),KMr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ZMr=l(),tm=a("p"),e4r=o(`Note:
Loading a model from its configuration file does `),pve=a("strong"),o4r=o("not"),r4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ve=a("code"),t4r=o("from_pretrained()"),a4r=o("to load the model weights."),s4r=l(),bve=a("p"),n4r=o("Examples:"),l4r=l(),m(kL.$$.fragment),i4r=l(),Lo=a("div"),m(RL.$$.fragment),d4r=l(),vve=a("p"),c4r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),m4r=l(),Bs=a("p"),f4r=o("The model class to instantiate is selected based on the "),Tve=a("code"),g4r=o("model_type"),h4r=o(` property of the config object (either
passed as an argument or loaded from `),Fve=a("code"),u4r=o("pretrained_model_name_or_path"),p4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cve=a("code"),_4r=o("pretrained_model_name_or_path"),b4r=o(":"),v4r=l(),ca=a("ul"),EE=a("li"),Mve=a("strong"),T4r=o("bart"),F4r=o(" \u2014 "),pV=a("a"),C4r=o("FlaxBartForCausalLM"),M4r=o(" (BART model)"),E4r=l(),yE=a("li"),Eve=a("strong"),y4r=o("gpt2"),w4r=o(" \u2014 "),_V=a("a"),A4r=o("FlaxGPT2LMHeadModel"),L4r=o(" (OpenAI GPT-2 model)"),B4r=l(),wE=a("li"),yve=a("strong"),x4r=o("gpt_neo"),k4r=o(" \u2014 "),bV=a("a"),R4r=o("FlaxGPTNeoForCausalLM"),S4r=o(" (GPT Neo model)"),P4r=l(),AE=a("li"),wve=a("strong"),$4r=o("gptj"),I4r=o(" \u2014 "),vV=a("a"),j4r=o("FlaxGPTJForCausalLM"),N4r=o(" (GPT-J model)"),D4r=l(),LE=a("li"),Ave=a("strong"),q4r=o("xglm"),O4r=o(" \u2014 "),TV=a("a"),G4r=o("FlaxXGLMForCausalLM"),X4r=o(" (XGLM model)"),V4r=l(),Lve=a("p"),z4r=o("Examples:"),W4r=l(),m(SL.$$.fragment),vRe=l(),am=a("h2"),BE=a("a"),Bve=a("span"),m(PL.$$.fragment),Q4r=l(),xve=a("span"),H4r=o("FlaxAutoModelForPreTraining"),TRe=l(),Pr=a("div"),m($L.$$.fragment),U4r=l(),sm=a("p"),J4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),kve=a("code"),Y4r=o("from_pretrained()"),K4r=o("class method or the "),Rve=a("code"),Z4r=o("from_config()"),eEr=o(`class
method.`),oEr=l(),IL=a("p"),rEr=o("This class cannot be instantiated directly using "),Sve=a("code"),tEr=o("__init__()"),aEr=o(" (throws an error)."),sEr=l(),Lt=a("div"),m(jL.$$.fragment),nEr=l(),Pve=a("p"),lEr=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),iEr=l(),nm=a("p"),dEr=o(`Note:
Loading a model from its configuration file does `),$ve=a("strong"),cEr=o("not"),mEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ive=a("code"),fEr=o("from_pretrained()"),gEr=o("to load the model weights."),hEr=l(),jve=a("p"),uEr=o("Examples:"),pEr=l(),m(NL.$$.fragment),_Er=l(),Bo=a("div"),m(DL.$$.fragment),bEr=l(),Nve=a("p"),vEr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),TEr=l(),xs=a("p"),FEr=o("The model class to instantiate is selected based on the "),Dve=a("code"),CEr=o("model_type"),MEr=o(` property of the config object (either
passed as an argument or loaded from `),qve=a("code"),EEr=o("pretrained_model_name_or_path"),yEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ove=a("code"),wEr=o("pretrained_model_name_or_path"),AEr=o(":"),LEr=l(),ce=a("ul"),xE=a("li"),Gve=a("strong"),BEr=o("albert"),xEr=o(" \u2014 "),FV=a("a"),kEr=o("FlaxAlbertForPreTraining"),REr=o(" (ALBERT model)"),SEr=l(),kE=a("li"),Xve=a("strong"),PEr=o("bart"),$Er=o(" \u2014 "),CV=a("a"),IEr=o("FlaxBartForConditionalGeneration"),jEr=o(" (BART model)"),NEr=l(),RE=a("li"),Vve=a("strong"),DEr=o("bert"),qEr=o(" \u2014 "),MV=a("a"),OEr=o("FlaxBertForPreTraining"),GEr=o(" (BERT model)"),XEr=l(),SE=a("li"),zve=a("strong"),VEr=o("big_bird"),zEr=o(" \u2014 "),EV=a("a"),WEr=o("FlaxBigBirdForPreTraining"),QEr=o(" (BigBird model)"),HEr=l(),PE=a("li"),Wve=a("strong"),UEr=o("electra"),JEr=o(" \u2014 "),yV=a("a"),YEr=o("FlaxElectraForPreTraining"),KEr=o(" (ELECTRA model)"),ZEr=l(),$E=a("li"),Qve=a("strong"),e3r=o("mbart"),o3r=o(" \u2014 "),wV=a("a"),r3r=o("FlaxMBartForConditionalGeneration"),t3r=o(" (mBART model)"),a3r=l(),IE=a("li"),Hve=a("strong"),s3r=o("mt5"),n3r=o(" \u2014 "),AV=a("a"),l3r=o("FlaxMT5ForConditionalGeneration"),i3r=o(" (mT5 model)"),d3r=l(),jE=a("li"),Uve=a("strong"),c3r=o("roberta"),m3r=o(" \u2014 "),LV=a("a"),f3r=o("FlaxRobertaForMaskedLM"),g3r=o(" (RoBERTa model)"),h3r=l(),NE=a("li"),Jve=a("strong"),u3r=o("roformer"),p3r=o(" \u2014 "),BV=a("a"),_3r=o("FlaxRoFormerForMaskedLM"),b3r=o(" (RoFormer model)"),v3r=l(),DE=a("li"),Yve=a("strong"),T3r=o("t5"),F3r=o(" \u2014 "),xV=a("a"),C3r=o("FlaxT5ForConditionalGeneration"),M3r=o(" (T5 model)"),E3r=l(),qE=a("li"),Kve=a("strong"),y3r=o("wav2vec2"),w3r=o(" \u2014 "),kV=a("a"),A3r=o("FlaxWav2Vec2ForPreTraining"),L3r=o(" (Wav2Vec2 model)"),B3r=l(),OE=a("li"),Zve=a("strong"),x3r=o("xlm-roberta"),k3r=o(" \u2014 "),RV=a("a"),R3r=o("FlaxXLMRobertaForMaskedLM"),S3r=o(" (XLM-RoBERTa model)"),P3r=l(),eTe=a("p"),$3r=o("Examples:"),I3r=l(),m(qL.$$.fragment),FRe=l(),lm=a("h2"),GE=a("a"),oTe=a("span"),m(OL.$$.fragment),j3r=l(),rTe=a("span"),N3r=o("FlaxAutoModelForMaskedLM"),CRe=l(),$r=a("div"),m(GL.$$.fragment),D3r=l(),im=a("p"),q3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),tTe=a("code"),O3r=o("from_pretrained()"),G3r=o("class method or the "),aTe=a("code"),X3r=o("from_config()"),V3r=o(`class
method.`),z3r=l(),XL=a("p"),W3r=o("This class cannot be instantiated directly using "),sTe=a("code"),Q3r=o("__init__()"),H3r=o(" (throws an error)."),U3r=l(),Bt=a("div"),m(VL.$$.fragment),J3r=l(),nTe=a("p"),Y3r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),K3r=l(),dm=a("p"),Z3r=o(`Note:
Loading a model from its configuration file does `),lTe=a("strong"),e5r=o("not"),o5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),iTe=a("code"),r5r=o("from_pretrained()"),t5r=o("to load the model weights."),a5r=l(),dTe=a("p"),s5r=o("Examples:"),n5r=l(),m(zL.$$.fragment),l5r=l(),xo=a("div"),m(WL.$$.fragment),i5r=l(),cTe=a("p"),d5r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),c5r=l(),ks=a("p"),m5r=o("The model class to instantiate is selected based on the "),mTe=a("code"),f5r=o("model_type"),g5r=o(` property of the config object (either
passed as an argument or loaded from `),fTe=a("code"),h5r=o("pretrained_model_name_or_path"),u5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gTe=a("code"),p5r=o("pretrained_model_name_or_path"),_5r=o(":"),b5r=l(),be=a("ul"),XE=a("li"),hTe=a("strong"),v5r=o("albert"),T5r=o(" \u2014 "),SV=a("a"),F5r=o("FlaxAlbertForMaskedLM"),C5r=o(" (ALBERT model)"),M5r=l(),VE=a("li"),uTe=a("strong"),E5r=o("bart"),y5r=o(" \u2014 "),PV=a("a"),w5r=o("FlaxBartForConditionalGeneration"),A5r=o(" (BART model)"),L5r=l(),zE=a("li"),pTe=a("strong"),B5r=o("bert"),x5r=o(" \u2014 "),$V=a("a"),k5r=o("FlaxBertForMaskedLM"),R5r=o(" (BERT model)"),S5r=l(),WE=a("li"),_Te=a("strong"),P5r=o("big_bird"),$5r=o(" \u2014 "),IV=a("a"),I5r=o("FlaxBigBirdForMaskedLM"),j5r=o(" (BigBird model)"),N5r=l(),QE=a("li"),bTe=a("strong"),D5r=o("distilbert"),q5r=o(" \u2014 "),jV=a("a"),O5r=o("FlaxDistilBertForMaskedLM"),G5r=o(" (DistilBERT model)"),X5r=l(),HE=a("li"),vTe=a("strong"),V5r=o("electra"),z5r=o(" \u2014 "),NV=a("a"),W5r=o("FlaxElectraForMaskedLM"),Q5r=o(" (ELECTRA model)"),H5r=l(),UE=a("li"),TTe=a("strong"),U5r=o("mbart"),J5r=o(" \u2014 "),DV=a("a"),Y5r=o("FlaxMBartForConditionalGeneration"),K5r=o(" (mBART model)"),Z5r=l(),JE=a("li"),FTe=a("strong"),eyr=o("roberta"),oyr=o(" \u2014 "),qV=a("a"),ryr=o("FlaxRobertaForMaskedLM"),tyr=o(" (RoBERTa model)"),ayr=l(),YE=a("li"),CTe=a("strong"),syr=o("roformer"),nyr=o(" \u2014 "),OV=a("a"),lyr=o("FlaxRoFormerForMaskedLM"),iyr=o(" (RoFormer model)"),dyr=l(),KE=a("li"),MTe=a("strong"),cyr=o("xlm-roberta"),myr=o(" \u2014 "),GV=a("a"),fyr=o("FlaxXLMRobertaForMaskedLM"),gyr=o(" (XLM-RoBERTa model)"),hyr=l(),ETe=a("p"),uyr=o("Examples:"),pyr=l(),m(QL.$$.fragment),MRe=l(),cm=a("h2"),ZE=a("a"),yTe=a("span"),m(HL.$$.fragment),_yr=l(),wTe=a("span"),byr=o("FlaxAutoModelForSeq2SeqLM"),ERe=l(),Ir=a("div"),m(UL.$$.fragment),vyr=l(),mm=a("p"),Tyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ATe=a("code"),Fyr=o("from_pretrained()"),Cyr=o("class method or the "),LTe=a("code"),Myr=o("from_config()"),Eyr=o(`class
method.`),yyr=l(),JL=a("p"),wyr=o("This class cannot be instantiated directly using "),BTe=a("code"),Ayr=o("__init__()"),Lyr=o(" (throws an error)."),Byr=l(),xt=a("div"),m(YL.$$.fragment),xyr=l(),xTe=a("p"),kyr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Ryr=l(),fm=a("p"),Syr=o(`Note:
Loading a model from its configuration file does `),kTe=a("strong"),Pyr=o("not"),$yr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),RTe=a("code"),Iyr=o("from_pretrained()"),jyr=o("to load the model weights."),Nyr=l(),STe=a("p"),Dyr=o("Examples:"),qyr=l(),m(KL.$$.fragment),Oyr=l(),ko=a("div"),m(ZL.$$.fragment),Gyr=l(),PTe=a("p"),Xyr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Vyr=l(),Rs=a("p"),zyr=o("The model class to instantiate is selected based on the "),$Te=a("code"),Wyr=o("model_type"),Qyr=o(` property of the config object (either
passed as an argument or loaded from `),ITe=a("code"),Hyr=o("pretrained_model_name_or_path"),Uyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jTe=a("code"),Jyr=o("pretrained_model_name_or_path"),Yyr=o(":"),Kyr=l(),Ee=a("ul"),e3=a("li"),NTe=a("strong"),Zyr=o("bart"),ewr=o(" \u2014 "),XV=a("a"),owr=o("FlaxBartForConditionalGeneration"),rwr=o(" (BART model)"),twr=l(),o3=a("li"),DTe=a("strong"),awr=o("blenderbot"),swr=o(" \u2014 "),VV=a("a"),nwr=o("FlaxBlenderbotForConditionalGeneration"),lwr=o(" (Blenderbot model)"),iwr=l(),r3=a("li"),qTe=a("strong"),dwr=o("blenderbot-small"),cwr=o(" \u2014 "),zV=a("a"),mwr=o("FlaxBlenderbotSmallForConditionalGeneration"),fwr=o(" (BlenderbotSmall model)"),gwr=l(),t3=a("li"),OTe=a("strong"),hwr=o("encoder-decoder"),uwr=o(" \u2014 "),WV=a("a"),pwr=o("FlaxEncoderDecoderModel"),_wr=o(" (Encoder decoder model)"),bwr=l(),a3=a("li"),GTe=a("strong"),vwr=o("marian"),Twr=o(" \u2014 "),QV=a("a"),Fwr=o("FlaxMarianMTModel"),Cwr=o(" (Marian model)"),Mwr=l(),s3=a("li"),XTe=a("strong"),Ewr=o("mbart"),ywr=o(" \u2014 "),HV=a("a"),wwr=o("FlaxMBartForConditionalGeneration"),Awr=o(" (mBART model)"),Lwr=l(),n3=a("li"),VTe=a("strong"),Bwr=o("mt5"),xwr=o(" \u2014 "),UV=a("a"),kwr=o("FlaxMT5ForConditionalGeneration"),Rwr=o(" (mT5 model)"),Swr=l(),l3=a("li"),zTe=a("strong"),Pwr=o("pegasus"),$wr=o(" \u2014 "),JV=a("a"),Iwr=o("FlaxPegasusForConditionalGeneration"),jwr=o(" (Pegasus model)"),Nwr=l(),i3=a("li"),WTe=a("strong"),Dwr=o("t5"),qwr=o(" \u2014 "),YV=a("a"),Owr=o("FlaxT5ForConditionalGeneration"),Gwr=o(" (T5 model)"),Xwr=l(),QTe=a("p"),Vwr=o("Examples:"),zwr=l(),m(e7.$$.fragment),yRe=l(),gm=a("h2"),d3=a("a"),HTe=a("span"),m(o7.$$.fragment),Wwr=l(),UTe=a("span"),Qwr=o("FlaxAutoModelForSequenceClassification"),wRe=l(),jr=a("div"),m(r7.$$.fragment),Hwr=l(),hm=a("p"),Uwr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),JTe=a("code"),Jwr=o("from_pretrained()"),Ywr=o("class method or the "),YTe=a("code"),Kwr=o("from_config()"),Zwr=o(`class
method.`),e6r=l(),t7=a("p"),o6r=o("This class cannot be instantiated directly using "),KTe=a("code"),r6r=o("__init__()"),t6r=o(" (throws an error)."),a6r=l(),kt=a("div"),m(a7.$$.fragment),s6r=l(),ZTe=a("p"),n6r=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),l6r=l(),um=a("p"),i6r=o(`Note:
Loading a model from its configuration file does `),e1e=a("strong"),d6r=o("not"),c6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),o1e=a("code"),m6r=o("from_pretrained()"),f6r=o("to load the model weights."),g6r=l(),r1e=a("p"),h6r=o("Examples:"),u6r=l(),m(s7.$$.fragment),p6r=l(),Ro=a("div"),m(n7.$$.fragment),_6r=l(),t1e=a("p"),b6r=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),v6r=l(),Ss=a("p"),T6r=o("The model class to instantiate is selected based on the "),a1e=a("code"),F6r=o("model_type"),C6r=o(` property of the config object (either
passed as an argument or loaded from `),s1e=a("code"),M6r=o("pretrained_model_name_or_path"),E6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),n1e=a("code"),y6r=o("pretrained_model_name_or_path"),w6r=o(":"),A6r=l(),ve=a("ul"),c3=a("li"),l1e=a("strong"),L6r=o("albert"),B6r=o(" \u2014 "),KV=a("a"),x6r=o("FlaxAlbertForSequenceClassification"),k6r=o(" (ALBERT model)"),R6r=l(),m3=a("li"),i1e=a("strong"),S6r=o("bart"),P6r=o(" \u2014 "),ZV=a("a"),$6r=o("FlaxBartForSequenceClassification"),I6r=o(" (BART model)"),j6r=l(),f3=a("li"),d1e=a("strong"),N6r=o("bert"),D6r=o(" \u2014 "),ez=a("a"),q6r=o("FlaxBertForSequenceClassification"),O6r=o(" (BERT model)"),G6r=l(),g3=a("li"),c1e=a("strong"),X6r=o("big_bird"),V6r=o(" \u2014 "),oz=a("a"),z6r=o("FlaxBigBirdForSequenceClassification"),W6r=o(" (BigBird model)"),Q6r=l(),h3=a("li"),m1e=a("strong"),H6r=o("distilbert"),U6r=o(" \u2014 "),rz=a("a"),J6r=o("FlaxDistilBertForSequenceClassification"),Y6r=o(" (DistilBERT model)"),K6r=l(),u3=a("li"),f1e=a("strong"),Z6r=o("electra"),eAr=o(" \u2014 "),tz=a("a"),oAr=o("FlaxElectraForSequenceClassification"),rAr=o(" (ELECTRA model)"),tAr=l(),p3=a("li"),g1e=a("strong"),aAr=o("mbart"),sAr=o(" \u2014 "),az=a("a"),nAr=o("FlaxMBartForSequenceClassification"),lAr=o(" (mBART model)"),iAr=l(),_3=a("li"),h1e=a("strong"),dAr=o("roberta"),cAr=o(" \u2014 "),sz=a("a"),mAr=o("FlaxRobertaForSequenceClassification"),fAr=o(" (RoBERTa model)"),gAr=l(),b3=a("li"),u1e=a("strong"),hAr=o("roformer"),uAr=o(" \u2014 "),nz=a("a"),pAr=o("FlaxRoFormerForSequenceClassification"),_Ar=o(" (RoFormer model)"),bAr=l(),v3=a("li"),p1e=a("strong"),vAr=o("xlm-roberta"),TAr=o(" \u2014 "),lz=a("a"),FAr=o("FlaxXLMRobertaForSequenceClassification"),CAr=o(" (XLM-RoBERTa model)"),MAr=l(),_1e=a("p"),EAr=o("Examples:"),yAr=l(),m(l7.$$.fragment),ARe=l(),pm=a("h2"),T3=a("a"),b1e=a("span"),m(i7.$$.fragment),wAr=l(),v1e=a("span"),AAr=o("FlaxAutoModelForQuestionAnswering"),LRe=l(),Nr=a("div"),m(d7.$$.fragment),LAr=l(),_m=a("p"),BAr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),T1e=a("code"),xAr=o("from_pretrained()"),kAr=o("class method or the "),F1e=a("code"),RAr=o("from_config()"),SAr=o(`class
method.`),PAr=l(),c7=a("p"),$Ar=o("This class cannot be instantiated directly using "),C1e=a("code"),IAr=o("__init__()"),jAr=o(" (throws an error)."),NAr=l(),Rt=a("div"),m(m7.$$.fragment),DAr=l(),M1e=a("p"),qAr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),OAr=l(),bm=a("p"),GAr=o(`Note:
Loading a model from its configuration file does `),E1e=a("strong"),XAr=o("not"),VAr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),y1e=a("code"),zAr=o("from_pretrained()"),WAr=o("to load the model weights."),QAr=l(),w1e=a("p"),HAr=o("Examples:"),UAr=l(),m(f7.$$.fragment),JAr=l(),So=a("div"),m(g7.$$.fragment),YAr=l(),A1e=a("p"),KAr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),ZAr=l(),Ps=a("p"),e0r=o("The model class to instantiate is selected based on the "),L1e=a("code"),o0r=o("model_type"),r0r=o(` property of the config object (either
passed as an argument or loaded from `),B1e=a("code"),t0r=o("pretrained_model_name_or_path"),a0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x1e=a("code"),s0r=o("pretrained_model_name_or_path"),n0r=o(":"),l0r=l(),Te=a("ul"),F3=a("li"),k1e=a("strong"),i0r=o("albert"),d0r=o(" \u2014 "),iz=a("a"),c0r=o("FlaxAlbertForQuestionAnswering"),m0r=o(" (ALBERT model)"),f0r=l(),C3=a("li"),R1e=a("strong"),g0r=o("bart"),h0r=o(" \u2014 "),dz=a("a"),u0r=o("FlaxBartForQuestionAnswering"),p0r=o(" (BART model)"),_0r=l(),M3=a("li"),S1e=a("strong"),b0r=o("bert"),v0r=o(" \u2014 "),cz=a("a"),T0r=o("FlaxBertForQuestionAnswering"),F0r=o(" (BERT model)"),C0r=l(),E3=a("li"),P1e=a("strong"),M0r=o("big_bird"),E0r=o(" \u2014 "),mz=a("a"),y0r=o("FlaxBigBirdForQuestionAnswering"),w0r=o(" (BigBird model)"),A0r=l(),y3=a("li"),$1e=a("strong"),L0r=o("distilbert"),B0r=o(" \u2014 "),fz=a("a"),x0r=o("FlaxDistilBertForQuestionAnswering"),k0r=o(" (DistilBERT model)"),R0r=l(),w3=a("li"),I1e=a("strong"),S0r=o("electra"),P0r=o(" \u2014 "),gz=a("a"),$0r=o("FlaxElectraForQuestionAnswering"),I0r=o(" (ELECTRA model)"),j0r=l(),A3=a("li"),j1e=a("strong"),N0r=o("mbart"),D0r=o(" \u2014 "),hz=a("a"),q0r=o("FlaxMBartForQuestionAnswering"),O0r=o(" (mBART model)"),G0r=l(),L3=a("li"),N1e=a("strong"),X0r=o("roberta"),V0r=o(" \u2014 "),uz=a("a"),z0r=o("FlaxRobertaForQuestionAnswering"),W0r=o(" (RoBERTa model)"),Q0r=l(),B3=a("li"),D1e=a("strong"),H0r=o("roformer"),U0r=o(" \u2014 "),pz=a("a"),J0r=o("FlaxRoFormerForQuestionAnswering"),Y0r=o(" (RoFormer model)"),K0r=l(),x3=a("li"),q1e=a("strong"),Z0r=o("xlm-roberta"),eLr=o(" \u2014 "),_z=a("a"),oLr=o("FlaxXLMRobertaForQuestionAnswering"),rLr=o(" (XLM-RoBERTa model)"),tLr=l(),O1e=a("p"),aLr=o("Examples:"),sLr=l(),m(h7.$$.fragment),BRe=l(),vm=a("h2"),k3=a("a"),G1e=a("span"),m(u7.$$.fragment),nLr=l(),X1e=a("span"),lLr=o("FlaxAutoModelForTokenClassification"),xRe=l(),Dr=a("div"),m(p7.$$.fragment),iLr=l(),Tm=a("p"),dLr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),V1e=a("code"),cLr=o("from_pretrained()"),mLr=o("class method or the "),z1e=a("code"),fLr=o("from_config()"),gLr=o(`class
method.`),hLr=l(),_7=a("p"),uLr=o("This class cannot be instantiated directly using "),W1e=a("code"),pLr=o("__init__()"),_Lr=o(" (throws an error)."),bLr=l(),St=a("div"),m(b7.$$.fragment),vLr=l(),Q1e=a("p"),TLr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),FLr=l(),Fm=a("p"),CLr=o(`Note:
Loading a model from its configuration file does `),H1e=a("strong"),MLr=o("not"),ELr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),U1e=a("code"),yLr=o("from_pretrained()"),wLr=o("to load the model weights."),ALr=l(),J1e=a("p"),LLr=o("Examples:"),BLr=l(),m(v7.$$.fragment),xLr=l(),Po=a("div"),m(T7.$$.fragment),kLr=l(),Y1e=a("p"),RLr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),SLr=l(),$s=a("p"),PLr=o("The model class to instantiate is selected based on the "),K1e=a("code"),$Lr=o("model_type"),ILr=o(` property of the config object (either
passed as an argument or loaded from `),Z1e=a("code"),jLr=o("pretrained_model_name_or_path"),NLr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eFe=a("code"),DLr=o("pretrained_model_name_or_path"),qLr=o(":"),OLr=l(),Se=a("ul"),R3=a("li"),oFe=a("strong"),GLr=o("albert"),XLr=o(" \u2014 "),bz=a("a"),VLr=o("FlaxAlbertForTokenClassification"),zLr=o(" (ALBERT model)"),WLr=l(),S3=a("li"),rFe=a("strong"),QLr=o("bert"),HLr=o(" \u2014 "),vz=a("a"),ULr=o("FlaxBertForTokenClassification"),JLr=o(" (BERT model)"),YLr=l(),P3=a("li"),tFe=a("strong"),KLr=o("big_bird"),ZLr=o(" \u2014 "),Tz=a("a"),e7r=o("FlaxBigBirdForTokenClassification"),o7r=o(" (BigBird model)"),r7r=l(),$3=a("li"),aFe=a("strong"),t7r=o("distilbert"),a7r=o(" \u2014 "),Fz=a("a"),s7r=o("FlaxDistilBertForTokenClassification"),n7r=o(" (DistilBERT model)"),l7r=l(),I3=a("li"),sFe=a("strong"),i7r=o("electra"),d7r=o(" \u2014 "),Cz=a("a"),c7r=o("FlaxElectraForTokenClassification"),m7r=o(" (ELECTRA model)"),f7r=l(),j3=a("li"),nFe=a("strong"),g7r=o("roberta"),h7r=o(" \u2014 "),Mz=a("a"),u7r=o("FlaxRobertaForTokenClassification"),p7r=o(" (RoBERTa model)"),_7r=l(),N3=a("li"),lFe=a("strong"),b7r=o("roformer"),v7r=o(" \u2014 "),Ez=a("a"),T7r=o("FlaxRoFormerForTokenClassification"),F7r=o(" (RoFormer model)"),C7r=l(),D3=a("li"),iFe=a("strong"),M7r=o("xlm-roberta"),E7r=o(" \u2014 "),yz=a("a"),y7r=o("FlaxXLMRobertaForTokenClassification"),w7r=o(" (XLM-RoBERTa model)"),A7r=l(),dFe=a("p"),L7r=o("Examples:"),B7r=l(),m(F7.$$.fragment),kRe=l(),Cm=a("h2"),q3=a("a"),cFe=a("span"),m(C7.$$.fragment),x7r=l(),mFe=a("span"),k7r=o("FlaxAutoModelForMultipleChoice"),RRe=l(),qr=a("div"),m(M7.$$.fragment),R7r=l(),Mm=a("p"),S7r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),fFe=a("code"),P7r=o("from_pretrained()"),$7r=o("class method or the "),gFe=a("code"),I7r=o("from_config()"),j7r=o(`class
method.`),N7r=l(),E7=a("p"),D7r=o("This class cannot be instantiated directly using "),hFe=a("code"),q7r=o("__init__()"),O7r=o(" (throws an error)."),G7r=l(),Pt=a("div"),m(y7.$$.fragment),X7r=l(),uFe=a("p"),V7r=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),z7r=l(),Em=a("p"),W7r=o(`Note:
Loading a model from its configuration file does `),pFe=a("strong"),Q7r=o("not"),H7r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_Fe=a("code"),U7r=o("from_pretrained()"),J7r=o("to load the model weights."),Y7r=l(),bFe=a("p"),K7r=o("Examples:"),Z7r=l(),m(w7.$$.fragment),e8r=l(),$o=a("div"),m(A7.$$.fragment),o8r=l(),vFe=a("p"),r8r=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),t8r=l(),Is=a("p"),a8r=o("The model class to instantiate is selected based on the "),TFe=a("code"),s8r=o("model_type"),n8r=o(` property of the config object (either
passed as an argument or loaded from `),FFe=a("code"),l8r=o("pretrained_model_name_or_path"),i8r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),CFe=a("code"),d8r=o("pretrained_model_name_or_path"),c8r=o(":"),m8r=l(),Pe=a("ul"),O3=a("li"),MFe=a("strong"),f8r=o("albert"),g8r=o(" \u2014 "),wz=a("a"),h8r=o("FlaxAlbertForMultipleChoice"),u8r=o(" (ALBERT model)"),p8r=l(),G3=a("li"),EFe=a("strong"),_8r=o("bert"),b8r=o(" \u2014 "),Az=a("a"),v8r=o("FlaxBertForMultipleChoice"),T8r=o(" (BERT model)"),F8r=l(),X3=a("li"),yFe=a("strong"),C8r=o("big_bird"),M8r=o(" \u2014 "),Lz=a("a"),E8r=o("FlaxBigBirdForMultipleChoice"),y8r=o(" (BigBird model)"),w8r=l(),V3=a("li"),wFe=a("strong"),A8r=o("distilbert"),L8r=o(" \u2014 "),Bz=a("a"),B8r=o("FlaxDistilBertForMultipleChoice"),x8r=o(" (DistilBERT model)"),k8r=l(),z3=a("li"),AFe=a("strong"),R8r=o("electra"),S8r=o(" \u2014 "),xz=a("a"),P8r=o("FlaxElectraForMultipleChoice"),$8r=o(" (ELECTRA model)"),I8r=l(),W3=a("li"),LFe=a("strong"),j8r=o("roberta"),N8r=o(" \u2014 "),kz=a("a"),D8r=o("FlaxRobertaForMultipleChoice"),q8r=o(" (RoBERTa model)"),O8r=l(),Q3=a("li"),BFe=a("strong"),G8r=o("roformer"),X8r=o(" \u2014 "),Rz=a("a"),V8r=o("FlaxRoFormerForMultipleChoice"),z8r=o(" (RoFormer model)"),W8r=l(),H3=a("li"),xFe=a("strong"),Q8r=o("xlm-roberta"),H8r=o(" \u2014 "),Sz=a("a"),U8r=o("FlaxXLMRobertaForMultipleChoice"),J8r=o(" (XLM-RoBERTa model)"),Y8r=l(),kFe=a("p"),K8r=o("Examples:"),Z8r=l(),m(L7.$$.fragment),SRe=l(),ym=a("h2"),U3=a("a"),RFe=a("span"),m(B7.$$.fragment),e9r=l(),SFe=a("span"),o9r=o("FlaxAutoModelForNextSentencePrediction"),PRe=l(),Or=a("div"),m(x7.$$.fragment),r9r=l(),wm=a("p"),t9r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),PFe=a("code"),a9r=o("from_pretrained()"),s9r=o("class method or the "),$Fe=a("code"),n9r=o("from_config()"),l9r=o(`class
method.`),i9r=l(),k7=a("p"),d9r=o("This class cannot be instantiated directly using "),IFe=a("code"),c9r=o("__init__()"),m9r=o(" (throws an error)."),f9r=l(),$t=a("div"),m(R7.$$.fragment),g9r=l(),jFe=a("p"),h9r=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),u9r=l(),Am=a("p"),p9r=o(`Note:
Loading a model from its configuration file does `),NFe=a("strong"),_9r=o("not"),b9r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),DFe=a("code"),v9r=o("from_pretrained()"),T9r=o("to load the model weights."),F9r=l(),qFe=a("p"),C9r=o("Examples:"),M9r=l(),m(S7.$$.fragment),E9r=l(),Io=a("div"),m(P7.$$.fragment),y9r=l(),OFe=a("p"),w9r=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),A9r=l(),js=a("p"),L9r=o("The model class to instantiate is selected based on the "),GFe=a("code"),B9r=o("model_type"),x9r=o(` property of the config object (either
passed as an argument or loaded from `),XFe=a("code"),k9r=o("pretrained_model_name_or_path"),R9r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),VFe=a("code"),S9r=o("pretrained_model_name_or_path"),P9r=o(":"),$9r=l(),zFe=a("ul"),J3=a("li"),WFe=a("strong"),I9r=o("bert"),j9r=o(" \u2014 "),Pz=a("a"),N9r=o("FlaxBertForNextSentencePrediction"),D9r=o(" (BERT model)"),q9r=l(),QFe=a("p"),O9r=o("Examples:"),G9r=l(),m($7.$$.fragment),$Re=l(),Lm=a("h2"),Y3=a("a"),HFe=a("span"),m(I7.$$.fragment),X9r=l(),UFe=a("span"),V9r=o("FlaxAutoModelForImageClassification"),IRe=l(),Gr=a("div"),m(j7.$$.fragment),z9r=l(),Bm=a("p"),W9r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),JFe=a("code"),Q9r=o("from_pretrained()"),H9r=o("class method or the "),YFe=a("code"),U9r=o("from_config()"),J9r=o(`class
method.`),Y9r=l(),N7=a("p"),K9r=o("This class cannot be instantiated directly using "),KFe=a("code"),Z9r=o("__init__()"),eBr=o(" (throws an error)."),oBr=l(),It=a("div"),m(D7.$$.fragment),rBr=l(),ZFe=a("p"),tBr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),aBr=l(),xm=a("p"),sBr=o(`Note:
Loading a model from its configuration file does `),eCe=a("strong"),nBr=o("not"),lBr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),oCe=a("code"),iBr=o("from_pretrained()"),dBr=o("to load the model weights."),cBr=l(),rCe=a("p"),mBr=o("Examples:"),fBr=l(),m(q7.$$.fragment),gBr=l(),jo=a("div"),m(O7.$$.fragment),hBr=l(),tCe=a("p"),uBr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),pBr=l(),Ns=a("p"),_Br=o("The model class to instantiate is selected based on the "),aCe=a("code"),bBr=o("model_type"),vBr=o(` property of the config object (either
passed as an argument or loaded from `),sCe=a("code"),TBr=o("pretrained_model_name_or_path"),FBr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nCe=a("code"),CBr=o("pretrained_model_name_or_path"),MBr=o(":"),EBr=l(),G7=a("ul"),K3=a("li"),lCe=a("strong"),yBr=o("beit"),wBr=o(" \u2014 "),$z=a("a"),ABr=o("FlaxBeitForImageClassification"),LBr=o(" (BEiT model)"),BBr=l(),Z3=a("li"),iCe=a("strong"),xBr=o("vit"),kBr=o(" \u2014 "),Iz=a("a"),RBr=o("FlaxViTForImageClassification"),SBr=o(" (ViT model)"),PBr=l(),dCe=a("p"),$Br=o("Examples:"),IBr=l(),m(X7.$$.fragment),jRe=l(),km=a("h2"),e5=a("a"),cCe=a("span"),m(V7.$$.fragment),jBr=l(),mCe=a("span"),NBr=o("FlaxAutoModelForVision2Seq"),NRe=l(),Xr=a("div"),m(z7.$$.fragment),DBr=l(),Rm=a("p"),qBr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),fCe=a("code"),OBr=o("from_pretrained()"),GBr=o("class method or the "),gCe=a("code"),XBr=o("from_config()"),VBr=o(`class
method.`),zBr=l(),W7=a("p"),WBr=o("This class cannot be instantiated directly using "),hCe=a("code"),QBr=o("__init__()"),HBr=o(" (throws an error)."),UBr=l(),jt=a("div"),m(Q7.$$.fragment),JBr=l(),uCe=a("p"),YBr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),KBr=l(),Sm=a("p"),ZBr=o(`Note:
Loading a model from its configuration file does `),pCe=a("strong"),exr=o("not"),oxr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_Ce=a("code"),rxr=o("from_pretrained()"),txr=o("to load the model weights."),axr=l(),bCe=a("p"),sxr=o("Examples:"),nxr=l(),m(H7.$$.fragment),lxr=l(),No=a("div"),m(U7.$$.fragment),ixr=l(),vCe=a("p"),dxr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),cxr=l(),Ds=a("p"),mxr=o("The model class to instantiate is selected based on the "),TCe=a("code"),fxr=o("model_type"),gxr=o(` property of the config object (either
passed as an argument or loaded from `),FCe=a("code"),hxr=o("pretrained_model_name_or_path"),uxr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),CCe=a("code"),pxr=o("pretrained_model_name_or_path"),_xr=o(":"),bxr=l(),MCe=a("ul"),o5=a("li"),ECe=a("strong"),vxr=o("vision-encoder-decoder"),Txr=o(" \u2014 "),jz=a("a"),Fxr=o("FlaxVisionEncoderDecoderModel"),Cxr=o(" (Vision Encoder decoder model)"),Mxr=l(),yCe=a("p"),Exr=o("Examples:"),yxr=l(),m(J7.$$.fragment),this.h()},l(c){const _=b4t('[data-svelte="svelte-1phssyn"]',document.head);J=s(_,"META",{name:!0,content:!0}),_.forEach(t),$e=i(c),de=s(c,"H1",{class:!0});var Y7=n(de);ue=s(Y7,"A",{id:!0,class:!0,href:!0});var wCe=n(ue);io=s(wCe,"SPAN",{});var ACe=n(io);f(me.$$.fragment,ACe),ACe.forEach(t),wCe.forEach(t),Me=i(Y7),Vo=s(Y7,"SPAN",{});var Axr=n(Vo);ji=r(Axr,"Auto Classes"),Axr.forEach(t),Y7.forEach(t),$m=i(c),ma=s(c,"P",{});var qRe=n(ma);Ni=r(qRe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Di=s(qRe,"CODE",{});var Lxr=n(Di);oy=r(Lxr,"from_pretrained()"),Lxr.forEach(t),Im=r(qRe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),qRe.forEach(t),Be=i(c),co=s(c,"P",{});var r5=n(co);qi=r(r5,"Instantiating one of "),qs=s(r5,"A",{href:!0});var Bxr=n(qs);ry=r(Bxr,"AutoConfig"),Bxr.forEach(t),Os=r(r5,", "),Gs=s(r5,"A",{href:!0});var xxr=n(Gs);ty=r(xxr,"AutoModel"),xxr.forEach(t),Oi=r(r5,`, and
`),Xs=s(r5,"A",{href:!0});var kxr=n(Xs);ay=r(kxr,"AutoTokenizer"),kxr.forEach(t),Gi=r(r5," will directly create a class of the relevant architecture. For instance"),r5.forEach(t),jm=i(c),f(qa.$$.fragment,c),mo=i(c),pe=s(c,"P",{});var ORe=n(pe);H8=r(ORe,"will create a model that is an instance of "),Xi=s(ORe,"A",{href:!0});var Rxr=n(Xi);U8=r(Rxr,"BertModel"),Rxr.forEach(t),J8=r(ORe,"."),ORe.forEach(t),zo=i(c),Oa=s(c,"P",{});var GRe=n(Oa);Y8=r(GRe,"There is one class of "),Nm=s(GRe,"CODE",{});var Sxr=n(Nm);K8=r(Sxr,"AutoModel"),Sxr.forEach(t),KPe=r(GRe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),GRe.forEach(t),Nxe=i(c),Vi=s(c,"H2",{class:!0});var XRe=n(Vi);Dm=s(XRe,"A",{id:!0,class:!0,href:!0});var Pxr=n(Dm);xQ=s(Pxr,"SPAN",{});var $xr=n(xQ);f(sy.$$.fragment,$xr),$xr.forEach(t),Pxr.forEach(t),ZPe=i(XRe),kQ=s(XRe,"SPAN",{});var Ixr=n(kQ);e$e=r(Ixr,"Extending the Auto Classes"),Ixr.forEach(t),XRe.forEach(t),Dxe=i(c),Vs=s(c,"P",{});var Nz=n(Vs);o$e=r(Nz,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),RQ=s(Nz,"CODE",{});var jxr=n(RQ);r$e=r(jxr,"NewModel"),jxr.forEach(t),t$e=r(Nz,", make sure you have a "),SQ=s(Nz,"CODE",{});var Nxr=n(SQ);a$e=r(Nxr,"NewModelConfig"),Nxr.forEach(t),s$e=r(Nz,` then you can add those to the auto
classes like this:`),Nz.forEach(t),qxe=i(c),f(ny.$$.fragment,c),Oxe=i(c),Z8=s(c,"P",{});var Dxr=n(Z8);n$e=r(Dxr,"You will then be able to use the auto classes like you would usually do!"),Dxr.forEach(t),Gxe=i(c),f(qm.$$.fragment,c),Xxe=i(c),zi=s(c,"H2",{class:!0});var VRe=n(zi);Om=s(VRe,"A",{id:!0,class:!0,href:!0});var qxr=n(Om);PQ=s(qxr,"SPAN",{});var Oxr=n(PQ);f(ly.$$.fragment,Oxr),Oxr.forEach(t),qxr.forEach(t),l$e=i(VRe),$Q=s(VRe,"SPAN",{});var Gxr=n($Q);i$e=r(Gxr,"AutoConfig"),Gxr.forEach(t),VRe.forEach(t),Vxe=i(c),Wo=s(c,"DIV",{class:!0});var Gn=n(Wo);f(iy.$$.fragment,Gn),d$e=i(Gn),dy=s(Gn,"P",{});var zRe=n(dy);c$e=r(zRe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),e9=s(zRe,"A",{href:!0});var Xxr=n(e9);m$e=r(Xxr,"from_pretrained()"),Xxr.forEach(t),f$e=r(zRe," class method."),zRe.forEach(t),g$e=i(Gn),cy=s(Gn,"P",{});var WRe=n(cy);h$e=r(WRe,"This class cannot be instantiated directly using "),IQ=s(WRe,"CODE",{});var Vxr=n(IQ);u$e=r(Vxr,"__init__()"),Vxr.forEach(t),p$e=r(WRe," (throws an error)."),WRe.forEach(t),_$e=i(Gn),fo=s(Gn,"DIV",{class:!0});var ga=n(fo);f(my.$$.fragment,ga),b$e=i(ga),jQ=s(ga,"P",{});var zxr=n(jQ);v$e=r(zxr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),zxr.forEach(t),T$e=i(ga),Wi=s(ga,"P",{});var Dz=n(Wi);F$e=r(Dz,"The configuration class to instantiate is selected based on the "),NQ=s(Dz,"CODE",{});var Wxr=n(NQ);C$e=r(Wxr,"model_type"),Wxr.forEach(t),M$e=r(Dz,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),DQ=s(Dz,"CODE",{});var Qxr=n(DQ);E$e=r(Qxr,"pretrained_model_name_or_path"),Qxr.forEach(t),y$e=r(Dz,":"),Dz.forEach(t),w$e=i(ga),v=s(ga,"UL",{});var T=n(v);Gm=s(T,"LI",{});var LCe=n(Gm);qQ=s(LCe,"STRONG",{});var Hxr=n(qQ);A$e=r(Hxr,"albert"),Hxr.forEach(t),L$e=r(LCe," \u2014 "),o9=s(LCe,"A",{href:!0});var Uxr=n(o9);B$e=r(Uxr,"AlbertConfig"),Uxr.forEach(t),x$e=r(LCe," (ALBERT model)"),LCe.forEach(t),k$e=i(T),Xm=s(T,"LI",{});var BCe=n(Xm);OQ=s(BCe,"STRONG",{});var Jxr=n(OQ);R$e=r(Jxr,"bart"),Jxr.forEach(t),S$e=r(BCe," \u2014 "),r9=s(BCe,"A",{href:!0});var Yxr=n(r9);P$e=r(Yxr,"BartConfig"),Yxr.forEach(t),$$e=r(BCe," (BART model)"),BCe.forEach(t),I$e=i(T),Vm=s(T,"LI",{});var xCe=n(Vm);GQ=s(xCe,"STRONG",{});var Kxr=n(GQ);j$e=r(Kxr,"beit"),Kxr.forEach(t),N$e=r(xCe," \u2014 "),t9=s(xCe,"A",{href:!0});var Zxr=n(t9);D$e=r(Zxr,"BeitConfig"),Zxr.forEach(t),q$e=r(xCe," (BEiT model)"),xCe.forEach(t),O$e=i(T),zm=s(T,"LI",{});var kCe=n(zm);XQ=s(kCe,"STRONG",{});var ekr=n(XQ);G$e=r(ekr,"bert"),ekr.forEach(t),X$e=r(kCe," \u2014 "),a9=s(kCe,"A",{href:!0});var okr=n(a9);V$e=r(okr,"BertConfig"),okr.forEach(t),z$e=r(kCe," (BERT model)"),kCe.forEach(t),W$e=i(T),Wm=s(T,"LI",{});var RCe=n(Wm);VQ=s(RCe,"STRONG",{});var rkr=n(VQ);Q$e=r(rkr,"bert-generation"),rkr.forEach(t),H$e=r(RCe," \u2014 "),s9=s(RCe,"A",{href:!0});var tkr=n(s9);U$e=r(tkr,"BertGenerationConfig"),tkr.forEach(t),J$e=r(RCe," (Bert Generation model)"),RCe.forEach(t),Y$e=i(T),Qm=s(T,"LI",{});var SCe=n(Qm);zQ=s(SCe,"STRONG",{});var akr=n(zQ);K$e=r(akr,"big_bird"),akr.forEach(t),Z$e=r(SCe," \u2014 "),n9=s(SCe,"A",{href:!0});var skr=n(n9);eIe=r(skr,"BigBirdConfig"),skr.forEach(t),oIe=r(SCe," (BigBird model)"),SCe.forEach(t),rIe=i(T),Hm=s(T,"LI",{});var PCe=n(Hm);WQ=s(PCe,"STRONG",{});var nkr=n(WQ);tIe=r(nkr,"bigbird_pegasus"),nkr.forEach(t),aIe=r(PCe," \u2014 "),l9=s(PCe,"A",{href:!0});var lkr=n(l9);sIe=r(lkr,"BigBirdPegasusConfig"),lkr.forEach(t),nIe=r(PCe," (BigBirdPegasus model)"),PCe.forEach(t),lIe=i(T),Um=s(T,"LI",{});var $Ce=n(Um);QQ=s($Ce,"STRONG",{});var ikr=n(QQ);iIe=r(ikr,"blenderbot"),ikr.forEach(t),dIe=r($Ce," \u2014 "),i9=s($Ce,"A",{href:!0});var dkr=n(i9);cIe=r(dkr,"BlenderbotConfig"),dkr.forEach(t),mIe=r($Ce," (Blenderbot model)"),$Ce.forEach(t),fIe=i(T),Jm=s(T,"LI",{});var ICe=n(Jm);HQ=s(ICe,"STRONG",{});var ckr=n(HQ);gIe=r(ckr,"blenderbot-small"),ckr.forEach(t),hIe=r(ICe," \u2014 "),d9=s(ICe,"A",{href:!0});var mkr=n(d9);uIe=r(mkr,"BlenderbotSmallConfig"),mkr.forEach(t),pIe=r(ICe," (BlenderbotSmall model)"),ICe.forEach(t),_Ie=i(T),Ym=s(T,"LI",{});var jCe=n(Ym);UQ=s(jCe,"STRONG",{});var fkr=n(UQ);bIe=r(fkr,"camembert"),fkr.forEach(t),vIe=r(jCe," \u2014 "),c9=s(jCe,"A",{href:!0});var gkr=n(c9);TIe=r(gkr,"CamembertConfig"),gkr.forEach(t),FIe=r(jCe," (CamemBERT model)"),jCe.forEach(t),CIe=i(T),Km=s(T,"LI",{});var NCe=n(Km);JQ=s(NCe,"STRONG",{});var hkr=n(JQ);MIe=r(hkr,"canine"),hkr.forEach(t),EIe=r(NCe," \u2014 "),m9=s(NCe,"A",{href:!0});var ukr=n(m9);yIe=r(ukr,"CanineConfig"),ukr.forEach(t),wIe=r(NCe," (Canine model)"),NCe.forEach(t),AIe=i(T),Zm=s(T,"LI",{});var DCe=n(Zm);YQ=s(DCe,"STRONG",{});var pkr=n(YQ);LIe=r(pkr,"clip"),pkr.forEach(t),BIe=r(DCe," \u2014 "),f9=s(DCe,"A",{href:!0});var _kr=n(f9);xIe=r(_kr,"CLIPConfig"),_kr.forEach(t),kIe=r(DCe," (CLIP model)"),DCe.forEach(t),RIe=i(T),ef=s(T,"LI",{});var qCe=n(ef);KQ=s(qCe,"STRONG",{});var bkr=n(KQ);SIe=r(bkr,"convbert"),bkr.forEach(t),PIe=r(qCe," \u2014 "),g9=s(qCe,"A",{href:!0});var vkr=n(g9);$Ie=r(vkr,"ConvBertConfig"),vkr.forEach(t),IIe=r(qCe," (ConvBERT model)"),qCe.forEach(t),jIe=i(T),of=s(T,"LI",{});var OCe=n(of);ZQ=s(OCe,"STRONG",{});var Tkr=n(ZQ);NIe=r(Tkr,"convnext"),Tkr.forEach(t),DIe=r(OCe," \u2014 "),h9=s(OCe,"A",{href:!0});var Fkr=n(h9);qIe=r(Fkr,"ConvNextConfig"),Fkr.forEach(t),OIe=r(OCe," (ConvNext model)"),OCe.forEach(t),GIe=i(T),rf=s(T,"LI",{});var GCe=n(rf);eH=s(GCe,"STRONG",{});var Ckr=n(eH);XIe=r(Ckr,"ctrl"),Ckr.forEach(t),VIe=r(GCe," \u2014 "),u9=s(GCe,"A",{href:!0});var Mkr=n(u9);zIe=r(Mkr,"CTRLConfig"),Mkr.forEach(t),WIe=r(GCe," (CTRL model)"),GCe.forEach(t),QIe=i(T),tf=s(T,"LI",{});var XCe=n(tf);oH=s(XCe,"STRONG",{});var Ekr=n(oH);HIe=r(Ekr,"data2vec-audio"),Ekr.forEach(t),UIe=r(XCe," \u2014 "),p9=s(XCe,"A",{href:!0});var ykr=n(p9);JIe=r(ykr,"Data2VecAudioConfig"),ykr.forEach(t),YIe=r(XCe," (Data2VecAudio model)"),XCe.forEach(t),KIe=i(T),af=s(T,"LI",{});var VCe=n(af);rH=s(VCe,"STRONG",{});var wkr=n(rH);ZIe=r(wkr,"data2vec-text"),wkr.forEach(t),eje=r(VCe," \u2014 "),_9=s(VCe,"A",{href:!0});var Akr=n(_9);oje=r(Akr,"Data2VecTextConfig"),Akr.forEach(t),rje=r(VCe," (Data2VecText model)"),VCe.forEach(t),tje=i(T),sf=s(T,"LI",{});var zCe=n(sf);tH=s(zCe,"STRONG",{});var Lkr=n(tH);aje=r(Lkr,"deberta"),Lkr.forEach(t),sje=r(zCe," \u2014 "),b9=s(zCe,"A",{href:!0});var Bkr=n(b9);nje=r(Bkr,"DebertaConfig"),Bkr.forEach(t),lje=r(zCe," (DeBERTa model)"),zCe.forEach(t),ije=i(T),nf=s(T,"LI",{});var WCe=n(nf);aH=s(WCe,"STRONG",{});var xkr=n(aH);dje=r(xkr,"deberta-v2"),xkr.forEach(t),cje=r(WCe," \u2014 "),v9=s(WCe,"A",{href:!0});var kkr=n(v9);mje=r(kkr,"DebertaV2Config"),kkr.forEach(t),fje=r(WCe," (DeBERTa-v2 model)"),WCe.forEach(t),gje=i(T),lf=s(T,"LI",{});var QCe=n(lf);sH=s(QCe,"STRONG",{});var Rkr=n(sH);hje=r(Rkr,"deit"),Rkr.forEach(t),uje=r(QCe," \u2014 "),T9=s(QCe,"A",{href:!0});var Skr=n(T9);pje=r(Skr,"DeiTConfig"),Skr.forEach(t),_je=r(QCe," (DeiT model)"),QCe.forEach(t),bje=i(T),df=s(T,"LI",{});var HCe=n(df);nH=s(HCe,"STRONG",{});var Pkr=n(nH);vje=r(Pkr,"detr"),Pkr.forEach(t),Tje=r(HCe," \u2014 "),F9=s(HCe,"A",{href:!0});var $kr=n(F9);Fje=r($kr,"DetrConfig"),$kr.forEach(t),Cje=r(HCe," (DETR model)"),HCe.forEach(t),Mje=i(T),cf=s(T,"LI",{});var UCe=n(cf);lH=s(UCe,"STRONG",{});var Ikr=n(lH);Eje=r(Ikr,"distilbert"),Ikr.forEach(t),yje=r(UCe," \u2014 "),C9=s(UCe,"A",{href:!0});var jkr=n(C9);wje=r(jkr,"DistilBertConfig"),jkr.forEach(t),Aje=r(UCe," (DistilBERT model)"),UCe.forEach(t),Lje=i(T),mf=s(T,"LI",{});var JCe=n(mf);iH=s(JCe,"STRONG",{});var Nkr=n(iH);Bje=r(Nkr,"dpr"),Nkr.forEach(t),xje=r(JCe," \u2014 "),M9=s(JCe,"A",{href:!0});var Dkr=n(M9);kje=r(Dkr,"DPRConfig"),Dkr.forEach(t),Rje=r(JCe," (DPR model)"),JCe.forEach(t),Sje=i(T),ff=s(T,"LI",{});var YCe=n(ff);dH=s(YCe,"STRONG",{});var qkr=n(dH);Pje=r(qkr,"electra"),qkr.forEach(t),$je=r(YCe," \u2014 "),E9=s(YCe,"A",{href:!0});var Okr=n(E9);Ije=r(Okr,"ElectraConfig"),Okr.forEach(t),jje=r(YCe," (ELECTRA model)"),YCe.forEach(t),Nje=i(T),gf=s(T,"LI",{});var KCe=n(gf);cH=s(KCe,"STRONG",{});var Gkr=n(cH);Dje=r(Gkr,"encoder-decoder"),Gkr.forEach(t),qje=r(KCe," \u2014 "),y9=s(KCe,"A",{href:!0});var Xkr=n(y9);Oje=r(Xkr,"EncoderDecoderConfig"),Xkr.forEach(t),Gje=r(KCe," (Encoder decoder model)"),KCe.forEach(t),Xje=i(T),hf=s(T,"LI",{});var ZCe=n(hf);mH=s(ZCe,"STRONG",{});var Vkr=n(mH);Vje=r(Vkr,"flaubert"),Vkr.forEach(t),zje=r(ZCe," \u2014 "),w9=s(ZCe,"A",{href:!0});var zkr=n(w9);Wje=r(zkr,"FlaubertConfig"),zkr.forEach(t),Qje=r(ZCe," (FlauBERT model)"),ZCe.forEach(t),Hje=i(T),uf=s(T,"LI",{});var eMe=n(uf);fH=s(eMe,"STRONG",{});var Wkr=n(fH);Uje=r(Wkr,"fnet"),Wkr.forEach(t),Jje=r(eMe," \u2014 "),A9=s(eMe,"A",{href:!0});var Qkr=n(A9);Yje=r(Qkr,"FNetConfig"),Qkr.forEach(t),Kje=r(eMe," (FNet model)"),eMe.forEach(t),Zje=i(T),pf=s(T,"LI",{});var oMe=n(pf);gH=s(oMe,"STRONG",{});var Hkr=n(gH);eNe=r(Hkr,"fsmt"),Hkr.forEach(t),oNe=r(oMe," \u2014 "),L9=s(oMe,"A",{href:!0});var Ukr=n(L9);rNe=r(Ukr,"FSMTConfig"),Ukr.forEach(t),tNe=r(oMe," (FairSeq Machine-Translation model)"),oMe.forEach(t),aNe=i(T),_f=s(T,"LI",{});var rMe=n(_f);hH=s(rMe,"STRONG",{});var Jkr=n(hH);sNe=r(Jkr,"funnel"),Jkr.forEach(t),nNe=r(rMe," \u2014 "),B9=s(rMe,"A",{href:!0});var Ykr=n(B9);lNe=r(Ykr,"FunnelConfig"),Ykr.forEach(t),iNe=r(rMe," (Funnel Transformer model)"),rMe.forEach(t),dNe=i(T),bf=s(T,"LI",{});var tMe=n(bf);uH=s(tMe,"STRONG",{});var Kkr=n(uH);cNe=r(Kkr,"gpt2"),Kkr.forEach(t),mNe=r(tMe," \u2014 "),x9=s(tMe,"A",{href:!0});var Zkr=n(x9);fNe=r(Zkr,"GPT2Config"),Zkr.forEach(t),gNe=r(tMe," (OpenAI GPT-2 model)"),tMe.forEach(t),hNe=i(T),vf=s(T,"LI",{});var aMe=n(vf);pH=s(aMe,"STRONG",{});var eRr=n(pH);uNe=r(eRr,"gpt_neo"),eRr.forEach(t),pNe=r(aMe," \u2014 "),k9=s(aMe,"A",{href:!0});var oRr=n(k9);_Ne=r(oRr,"GPTNeoConfig"),oRr.forEach(t),bNe=r(aMe," (GPT Neo model)"),aMe.forEach(t),vNe=i(T),Tf=s(T,"LI",{});var sMe=n(Tf);_H=s(sMe,"STRONG",{});var rRr=n(_H);TNe=r(rRr,"gptj"),rRr.forEach(t),FNe=r(sMe," \u2014 "),R9=s(sMe,"A",{href:!0});var tRr=n(R9);CNe=r(tRr,"GPTJConfig"),tRr.forEach(t),MNe=r(sMe," (GPT-J model)"),sMe.forEach(t),ENe=i(T),Ff=s(T,"LI",{});var nMe=n(Ff);bH=s(nMe,"STRONG",{});var aRr=n(bH);yNe=r(aRr,"hubert"),aRr.forEach(t),wNe=r(nMe," \u2014 "),S9=s(nMe,"A",{href:!0});var sRr=n(S9);ANe=r(sRr,"HubertConfig"),sRr.forEach(t),LNe=r(nMe," (Hubert model)"),nMe.forEach(t),BNe=i(T),Cf=s(T,"LI",{});var lMe=n(Cf);vH=s(lMe,"STRONG",{});var nRr=n(vH);xNe=r(nRr,"ibert"),nRr.forEach(t),kNe=r(lMe," \u2014 "),P9=s(lMe,"A",{href:!0});var lRr=n(P9);RNe=r(lRr,"IBertConfig"),lRr.forEach(t),SNe=r(lMe," (I-BERT model)"),lMe.forEach(t),PNe=i(T),Mf=s(T,"LI",{});var iMe=n(Mf);TH=s(iMe,"STRONG",{});var iRr=n(TH);$Ne=r(iRr,"imagegpt"),iRr.forEach(t),INe=r(iMe," \u2014 "),$9=s(iMe,"A",{href:!0});var dRr=n($9);jNe=r(dRr,"ImageGPTConfig"),dRr.forEach(t),NNe=r(iMe," (ImageGPT model)"),iMe.forEach(t),DNe=i(T),Ef=s(T,"LI",{});var dMe=n(Ef);FH=s(dMe,"STRONG",{});var cRr=n(FH);qNe=r(cRr,"layoutlm"),cRr.forEach(t),ONe=r(dMe," \u2014 "),I9=s(dMe,"A",{href:!0});var mRr=n(I9);GNe=r(mRr,"LayoutLMConfig"),mRr.forEach(t),XNe=r(dMe," (LayoutLM model)"),dMe.forEach(t),VNe=i(T),yf=s(T,"LI",{});var cMe=n(yf);CH=s(cMe,"STRONG",{});var fRr=n(CH);zNe=r(fRr,"layoutlmv2"),fRr.forEach(t),WNe=r(cMe," \u2014 "),j9=s(cMe,"A",{href:!0});var gRr=n(j9);QNe=r(gRr,"LayoutLMv2Config"),gRr.forEach(t),HNe=r(cMe," (LayoutLMv2 model)"),cMe.forEach(t),UNe=i(T),wf=s(T,"LI",{});var mMe=n(wf);MH=s(mMe,"STRONG",{});var hRr=n(MH);JNe=r(hRr,"led"),hRr.forEach(t),YNe=r(mMe," \u2014 "),N9=s(mMe,"A",{href:!0});var uRr=n(N9);KNe=r(uRr,"LEDConfig"),uRr.forEach(t),ZNe=r(mMe," (LED model)"),mMe.forEach(t),eDe=i(T),Af=s(T,"LI",{});var fMe=n(Af);EH=s(fMe,"STRONG",{});var pRr=n(EH);oDe=r(pRr,"longformer"),pRr.forEach(t),rDe=r(fMe," \u2014 "),D9=s(fMe,"A",{href:!0});var _Rr=n(D9);tDe=r(_Rr,"LongformerConfig"),_Rr.forEach(t),aDe=r(fMe," (Longformer model)"),fMe.forEach(t),sDe=i(T),Lf=s(T,"LI",{});var gMe=n(Lf);yH=s(gMe,"STRONG",{});var bRr=n(yH);nDe=r(bRr,"luke"),bRr.forEach(t),lDe=r(gMe," \u2014 "),q9=s(gMe,"A",{href:!0});var vRr=n(q9);iDe=r(vRr,"LukeConfig"),vRr.forEach(t),dDe=r(gMe," (LUKE model)"),gMe.forEach(t),cDe=i(T),Bf=s(T,"LI",{});var hMe=n(Bf);wH=s(hMe,"STRONG",{});var TRr=n(wH);mDe=r(TRr,"lxmert"),TRr.forEach(t),fDe=r(hMe," \u2014 "),O9=s(hMe,"A",{href:!0});var FRr=n(O9);gDe=r(FRr,"LxmertConfig"),FRr.forEach(t),hDe=r(hMe," (LXMERT model)"),hMe.forEach(t),uDe=i(T),xf=s(T,"LI",{});var uMe=n(xf);AH=s(uMe,"STRONG",{});var CRr=n(AH);pDe=r(CRr,"m2m_100"),CRr.forEach(t),_De=r(uMe," \u2014 "),G9=s(uMe,"A",{href:!0});var MRr=n(G9);bDe=r(MRr,"M2M100Config"),MRr.forEach(t),vDe=r(uMe," (M2M100 model)"),uMe.forEach(t),TDe=i(T),kf=s(T,"LI",{});var pMe=n(kf);LH=s(pMe,"STRONG",{});var ERr=n(LH);FDe=r(ERr,"marian"),ERr.forEach(t),CDe=r(pMe," \u2014 "),X9=s(pMe,"A",{href:!0});var yRr=n(X9);MDe=r(yRr,"MarianConfig"),yRr.forEach(t),EDe=r(pMe," (Marian model)"),pMe.forEach(t),yDe=i(T),Rf=s(T,"LI",{});var _Me=n(Rf);BH=s(_Me,"STRONG",{});var wRr=n(BH);wDe=r(wRr,"maskformer"),wRr.forEach(t),ADe=r(_Me," \u2014 "),V9=s(_Me,"A",{href:!0});var ARr=n(V9);LDe=r(ARr,"MaskFormerConfig"),ARr.forEach(t),BDe=r(_Me," (MaskFormer model)"),_Me.forEach(t),xDe=i(T),Sf=s(T,"LI",{});var bMe=n(Sf);xH=s(bMe,"STRONG",{});var LRr=n(xH);kDe=r(LRr,"mbart"),LRr.forEach(t),RDe=r(bMe," \u2014 "),z9=s(bMe,"A",{href:!0});var BRr=n(z9);SDe=r(BRr,"MBartConfig"),BRr.forEach(t),PDe=r(bMe," (mBART model)"),bMe.forEach(t),$De=i(T),Pf=s(T,"LI",{});var vMe=n(Pf);kH=s(vMe,"STRONG",{});var xRr=n(kH);IDe=r(xRr,"megatron-bert"),xRr.forEach(t),jDe=r(vMe," \u2014 "),W9=s(vMe,"A",{href:!0});var kRr=n(W9);NDe=r(kRr,"MegatronBertConfig"),kRr.forEach(t),DDe=r(vMe," (MegatronBert model)"),vMe.forEach(t),qDe=i(T),$f=s(T,"LI",{});var TMe=n($f);RH=s(TMe,"STRONG",{});var RRr=n(RH);ODe=r(RRr,"mobilebert"),RRr.forEach(t),GDe=r(TMe," \u2014 "),Q9=s(TMe,"A",{href:!0});var SRr=n(Q9);XDe=r(SRr,"MobileBertConfig"),SRr.forEach(t),VDe=r(TMe," (MobileBERT model)"),TMe.forEach(t),zDe=i(T),If=s(T,"LI",{});var FMe=n(If);SH=s(FMe,"STRONG",{});var PRr=n(SH);WDe=r(PRr,"mpnet"),PRr.forEach(t),QDe=r(FMe," \u2014 "),H9=s(FMe,"A",{href:!0});var $Rr=n(H9);HDe=r($Rr,"MPNetConfig"),$Rr.forEach(t),UDe=r(FMe," (MPNet model)"),FMe.forEach(t),JDe=i(T),jf=s(T,"LI",{});var CMe=n(jf);PH=s(CMe,"STRONG",{});var IRr=n(PH);YDe=r(IRr,"mt5"),IRr.forEach(t),KDe=r(CMe," \u2014 "),U9=s(CMe,"A",{href:!0});var jRr=n(U9);ZDe=r(jRr,"MT5Config"),jRr.forEach(t),eqe=r(CMe," (mT5 model)"),CMe.forEach(t),oqe=i(T),Nf=s(T,"LI",{});var MMe=n(Nf);$H=s(MMe,"STRONG",{});var NRr=n($H);rqe=r(NRr,"nystromformer"),NRr.forEach(t),tqe=r(MMe," \u2014 "),J9=s(MMe,"A",{href:!0});var DRr=n(J9);aqe=r(DRr,"NystromformerConfig"),DRr.forEach(t),sqe=r(MMe," (Nystromformer model)"),MMe.forEach(t),nqe=i(T),Df=s(T,"LI",{});var EMe=n(Df);IH=s(EMe,"STRONG",{});var qRr=n(IH);lqe=r(qRr,"openai-gpt"),qRr.forEach(t),iqe=r(EMe," \u2014 "),Y9=s(EMe,"A",{href:!0});var ORr=n(Y9);dqe=r(ORr,"OpenAIGPTConfig"),ORr.forEach(t),cqe=r(EMe," (OpenAI GPT model)"),EMe.forEach(t),mqe=i(T),qf=s(T,"LI",{});var yMe=n(qf);jH=s(yMe,"STRONG",{});var GRr=n(jH);fqe=r(GRr,"pegasus"),GRr.forEach(t),gqe=r(yMe," \u2014 "),K9=s(yMe,"A",{href:!0});var XRr=n(K9);hqe=r(XRr,"PegasusConfig"),XRr.forEach(t),uqe=r(yMe," (Pegasus model)"),yMe.forEach(t),pqe=i(T),Of=s(T,"LI",{});var wMe=n(Of);NH=s(wMe,"STRONG",{});var VRr=n(NH);_qe=r(VRr,"perceiver"),VRr.forEach(t),bqe=r(wMe," \u2014 "),Z9=s(wMe,"A",{href:!0});var zRr=n(Z9);vqe=r(zRr,"PerceiverConfig"),zRr.forEach(t),Tqe=r(wMe," (Perceiver model)"),wMe.forEach(t),Fqe=i(T),Gf=s(T,"LI",{});var AMe=n(Gf);DH=s(AMe,"STRONG",{});var WRr=n(DH);Cqe=r(WRr,"plbart"),WRr.forEach(t),Mqe=r(AMe," \u2014 "),eB=s(AMe,"A",{href:!0});var QRr=n(eB);Eqe=r(QRr,"PLBartConfig"),QRr.forEach(t),yqe=r(AMe," (PLBart model)"),AMe.forEach(t),wqe=i(T),Xf=s(T,"LI",{});var LMe=n(Xf);qH=s(LMe,"STRONG",{});var HRr=n(qH);Aqe=r(HRr,"poolformer"),HRr.forEach(t),Lqe=r(LMe," \u2014 "),oB=s(LMe,"A",{href:!0});var URr=n(oB);Bqe=r(URr,"PoolFormerConfig"),URr.forEach(t),xqe=r(LMe," (PoolFormer model)"),LMe.forEach(t),kqe=i(T),Vf=s(T,"LI",{});var BMe=n(Vf);OH=s(BMe,"STRONG",{});var JRr=n(OH);Rqe=r(JRr,"prophetnet"),JRr.forEach(t),Sqe=r(BMe," \u2014 "),rB=s(BMe,"A",{href:!0});var YRr=n(rB);Pqe=r(YRr,"ProphetNetConfig"),YRr.forEach(t),$qe=r(BMe," (ProphetNet model)"),BMe.forEach(t),Iqe=i(T),zf=s(T,"LI",{});var xMe=n(zf);GH=s(xMe,"STRONG",{});var KRr=n(GH);jqe=r(KRr,"qdqbert"),KRr.forEach(t),Nqe=r(xMe," \u2014 "),tB=s(xMe,"A",{href:!0});var ZRr=n(tB);Dqe=r(ZRr,"QDQBertConfig"),ZRr.forEach(t),qqe=r(xMe," (QDQBert model)"),xMe.forEach(t),Oqe=i(T),Wf=s(T,"LI",{});var kMe=n(Wf);XH=s(kMe,"STRONG",{});var eSr=n(XH);Gqe=r(eSr,"rag"),eSr.forEach(t),Xqe=r(kMe," \u2014 "),aB=s(kMe,"A",{href:!0});var oSr=n(aB);Vqe=r(oSr,"RagConfig"),oSr.forEach(t),zqe=r(kMe," (RAG model)"),kMe.forEach(t),Wqe=i(T),Qf=s(T,"LI",{});var RMe=n(Qf);VH=s(RMe,"STRONG",{});var rSr=n(VH);Qqe=r(rSr,"realm"),rSr.forEach(t),Hqe=r(RMe," \u2014 "),sB=s(RMe,"A",{href:!0});var tSr=n(sB);Uqe=r(tSr,"RealmConfig"),tSr.forEach(t),Jqe=r(RMe," (Realm model)"),RMe.forEach(t),Yqe=i(T),Hf=s(T,"LI",{});var SMe=n(Hf);zH=s(SMe,"STRONG",{});var aSr=n(zH);Kqe=r(aSr,"reformer"),aSr.forEach(t),Zqe=r(SMe," \u2014 "),nB=s(SMe,"A",{href:!0});var sSr=n(nB);eOe=r(sSr,"ReformerConfig"),sSr.forEach(t),oOe=r(SMe," (Reformer model)"),SMe.forEach(t),rOe=i(T),Uf=s(T,"LI",{});var PMe=n(Uf);WH=s(PMe,"STRONG",{});var nSr=n(WH);tOe=r(nSr,"rembert"),nSr.forEach(t),aOe=r(PMe," \u2014 "),lB=s(PMe,"A",{href:!0});var lSr=n(lB);sOe=r(lSr,"RemBertConfig"),lSr.forEach(t),nOe=r(PMe," (RemBERT model)"),PMe.forEach(t),lOe=i(T),Jf=s(T,"LI",{});var $Me=n(Jf);QH=s($Me,"STRONG",{});var iSr=n(QH);iOe=r(iSr,"resnet"),iSr.forEach(t),dOe=r($Me," \u2014 "),iB=s($Me,"A",{href:!0});var dSr=n(iB);cOe=r(dSr,"ResNetConfig"),dSr.forEach(t),mOe=r($Me," (ResNet model)"),$Me.forEach(t),fOe=i(T),Yf=s(T,"LI",{});var IMe=n(Yf);HH=s(IMe,"STRONG",{});var cSr=n(HH);gOe=r(cSr,"retribert"),cSr.forEach(t),hOe=r(IMe," \u2014 "),dB=s(IMe,"A",{href:!0});var mSr=n(dB);uOe=r(mSr,"RetriBertConfig"),mSr.forEach(t),pOe=r(IMe," (RetriBERT model)"),IMe.forEach(t),_Oe=i(T),Kf=s(T,"LI",{});var jMe=n(Kf);UH=s(jMe,"STRONG",{});var fSr=n(UH);bOe=r(fSr,"roberta"),fSr.forEach(t),vOe=r(jMe," \u2014 "),cB=s(jMe,"A",{href:!0});var gSr=n(cB);TOe=r(gSr,"RobertaConfig"),gSr.forEach(t),FOe=r(jMe," (RoBERTa model)"),jMe.forEach(t),COe=i(T),Zf=s(T,"LI",{});var NMe=n(Zf);JH=s(NMe,"STRONG",{});var hSr=n(JH);MOe=r(hSr,"roformer"),hSr.forEach(t),EOe=r(NMe," \u2014 "),mB=s(NMe,"A",{href:!0});var uSr=n(mB);yOe=r(uSr,"RoFormerConfig"),uSr.forEach(t),wOe=r(NMe," (RoFormer model)"),NMe.forEach(t),AOe=i(T),eg=s(T,"LI",{});var DMe=n(eg);YH=s(DMe,"STRONG",{});var pSr=n(YH);LOe=r(pSr,"segformer"),pSr.forEach(t),BOe=r(DMe," \u2014 "),fB=s(DMe,"A",{href:!0});var _Sr=n(fB);xOe=r(_Sr,"SegformerConfig"),_Sr.forEach(t),kOe=r(DMe," (SegFormer model)"),DMe.forEach(t),ROe=i(T),og=s(T,"LI",{});var qMe=n(og);KH=s(qMe,"STRONG",{});var bSr=n(KH);SOe=r(bSr,"sew"),bSr.forEach(t),POe=r(qMe," \u2014 "),gB=s(qMe,"A",{href:!0});var vSr=n(gB);$Oe=r(vSr,"SEWConfig"),vSr.forEach(t),IOe=r(qMe," (SEW model)"),qMe.forEach(t),jOe=i(T),rg=s(T,"LI",{});var OMe=n(rg);ZH=s(OMe,"STRONG",{});var TSr=n(ZH);NOe=r(TSr,"sew-d"),TSr.forEach(t),DOe=r(OMe," \u2014 "),hB=s(OMe,"A",{href:!0});var FSr=n(hB);qOe=r(FSr,"SEWDConfig"),FSr.forEach(t),OOe=r(OMe," (SEW-D model)"),OMe.forEach(t),GOe=i(T),tg=s(T,"LI",{});var GMe=n(tg);eU=s(GMe,"STRONG",{});var CSr=n(eU);XOe=r(CSr,"speech-encoder-decoder"),CSr.forEach(t),VOe=r(GMe," \u2014 "),uB=s(GMe,"A",{href:!0});var MSr=n(uB);zOe=r(MSr,"SpeechEncoderDecoderConfig"),MSr.forEach(t),WOe=r(GMe," (Speech Encoder decoder model)"),GMe.forEach(t),QOe=i(T),ag=s(T,"LI",{});var XMe=n(ag);oU=s(XMe,"STRONG",{});var ESr=n(oU);HOe=r(ESr,"speech_to_text"),ESr.forEach(t),UOe=r(XMe," \u2014 "),pB=s(XMe,"A",{href:!0});var ySr=n(pB);JOe=r(ySr,"Speech2TextConfig"),ySr.forEach(t),YOe=r(XMe," (Speech2Text model)"),XMe.forEach(t),KOe=i(T),sg=s(T,"LI",{});var VMe=n(sg);rU=s(VMe,"STRONG",{});var wSr=n(rU);ZOe=r(wSr,"speech_to_text_2"),wSr.forEach(t),eGe=r(VMe," \u2014 "),_B=s(VMe,"A",{href:!0});var ASr=n(_B);oGe=r(ASr,"Speech2Text2Config"),ASr.forEach(t),rGe=r(VMe," (Speech2Text2 model)"),VMe.forEach(t),tGe=i(T),ng=s(T,"LI",{});var zMe=n(ng);tU=s(zMe,"STRONG",{});var LSr=n(tU);aGe=r(LSr,"splinter"),LSr.forEach(t),sGe=r(zMe," \u2014 "),bB=s(zMe,"A",{href:!0});var BSr=n(bB);nGe=r(BSr,"SplinterConfig"),BSr.forEach(t),lGe=r(zMe," (Splinter model)"),zMe.forEach(t),iGe=i(T),lg=s(T,"LI",{});var WMe=n(lg);aU=s(WMe,"STRONG",{});var xSr=n(aU);dGe=r(xSr,"squeezebert"),xSr.forEach(t),cGe=r(WMe," \u2014 "),vB=s(WMe,"A",{href:!0});var kSr=n(vB);mGe=r(kSr,"SqueezeBertConfig"),kSr.forEach(t),fGe=r(WMe," (SqueezeBERT model)"),WMe.forEach(t),gGe=i(T),ig=s(T,"LI",{});var QMe=n(ig);sU=s(QMe,"STRONG",{});var RSr=n(sU);hGe=r(RSr,"swin"),RSr.forEach(t),uGe=r(QMe," \u2014 "),TB=s(QMe,"A",{href:!0});var SSr=n(TB);pGe=r(SSr,"SwinConfig"),SSr.forEach(t),_Ge=r(QMe," (Swin model)"),QMe.forEach(t),bGe=i(T),dg=s(T,"LI",{});var HMe=n(dg);nU=s(HMe,"STRONG",{});var PSr=n(nU);vGe=r(PSr,"t5"),PSr.forEach(t),TGe=r(HMe," \u2014 "),FB=s(HMe,"A",{href:!0});var $Sr=n(FB);FGe=r($Sr,"T5Config"),$Sr.forEach(t),CGe=r(HMe," (T5 model)"),HMe.forEach(t),MGe=i(T),cg=s(T,"LI",{});var UMe=n(cg);lU=s(UMe,"STRONG",{});var ISr=n(lU);EGe=r(ISr,"tapas"),ISr.forEach(t),yGe=r(UMe," \u2014 "),CB=s(UMe,"A",{href:!0});var jSr=n(CB);wGe=r(jSr,"TapasConfig"),jSr.forEach(t),AGe=r(UMe," (TAPAS model)"),UMe.forEach(t),LGe=i(T),mg=s(T,"LI",{});var JMe=n(mg);iU=s(JMe,"STRONG",{});var NSr=n(iU);BGe=r(NSr,"transfo-xl"),NSr.forEach(t),xGe=r(JMe," \u2014 "),MB=s(JMe,"A",{href:!0});var DSr=n(MB);kGe=r(DSr,"TransfoXLConfig"),DSr.forEach(t),RGe=r(JMe," (Transformer-XL model)"),JMe.forEach(t),SGe=i(T),fg=s(T,"LI",{});var YMe=n(fg);dU=s(YMe,"STRONG",{});var qSr=n(dU);PGe=r(qSr,"trocr"),qSr.forEach(t),$Ge=r(YMe," \u2014 "),EB=s(YMe,"A",{href:!0});var OSr=n(EB);IGe=r(OSr,"TrOCRConfig"),OSr.forEach(t),jGe=r(YMe," (TrOCR model)"),YMe.forEach(t),NGe=i(T),gg=s(T,"LI",{});var KMe=n(gg);cU=s(KMe,"STRONG",{});var GSr=n(cU);DGe=r(GSr,"unispeech"),GSr.forEach(t),qGe=r(KMe," \u2014 "),yB=s(KMe,"A",{href:!0});var XSr=n(yB);OGe=r(XSr,"UniSpeechConfig"),XSr.forEach(t),GGe=r(KMe," (UniSpeech model)"),KMe.forEach(t),XGe=i(T),hg=s(T,"LI",{});var ZMe=n(hg);mU=s(ZMe,"STRONG",{});var VSr=n(mU);VGe=r(VSr,"unispeech-sat"),VSr.forEach(t),zGe=r(ZMe," \u2014 "),wB=s(ZMe,"A",{href:!0});var zSr=n(wB);WGe=r(zSr,"UniSpeechSatConfig"),zSr.forEach(t),QGe=r(ZMe," (UniSpeechSat model)"),ZMe.forEach(t),HGe=i(T),ug=s(T,"LI",{});var e4e=n(ug);fU=s(e4e,"STRONG",{});var WSr=n(fU);UGe=r(WSr,"van"),WSr.forEach(t),JGe=r(e4e," \u2014 "),AB=s(e4e,"A",{href:!0});var QSr=n(AB);YGe=r(QSr,"VanConfig"),QSr.forEach(t),KGe=r(e4e," (VAN model)"),e4e.forEach(t),ZGe=i(T),pg=s(T,"LI",{});var o4e=n(pg);gU=s(o4e,"STRONG",{});var HSr=n(gU);eXe=r(HSr,"vilt"),HSr.forEach(t),oXe=r(o4e," \u2014 "),LB=s(o4e,"A",{href:!0});var USr=n(LB);rXe=r(USr,"ViltConfig"),USr.forEach(t),tXe=r(o4e," (ViLT model)"),o4e.forEach(t),aXe=i(T),_g=s(T,"LI",{});var r4e=n(_g);hU=s(r4e,"STRONG",{});var JSr=n(hU);sXe=r(JSr,"vision-encoder-decoder"),JSr.forEach(t),nXe=r(r4e," \u2014 "),BB=s(r4e,"A",{href:!0});var YSr=n(BB);lXe=r(YSr,"VisionEncoderDecoderConfig"),YSr.forEach(t),iXe=r(r4e," (Vision Encoder decoder model)"),r4e.forEach(t),dXe=i(T),bg=s(T,"LI",{});var t4e=n(bg);uU=s(t4e,"STRONG",{});var KSr=n(uU);cXe=r(KSr,"vision-text-dual-encoder"),KSr.forEach(t),mXe=r(t4e," \u2014 "),xB=s(t4e,"A",{href:!0});var ZSr=n(xB);fXe=r(ZSr,"VisionTextDualEncoderConfig"),ZSr.forEach(t),gXe=r(t4e," (VisionTextDualEncoder model)"),t4e.forEach(t),hXe=i(T),vg=s(T,"LI",{});var a4e=n(vg);pU=s(a4e,"STRONG",{});var ePr=n(pU);uXe=r(ePr,"visual_bert"),ePr.forEach(t),pXe=r(a4e," \u2014 "),kB=s(a4e,"A",{href:!0});var oPr=n(kB);_Xe=r(oPr,"VisualBertConfig"),oPr.forEach(t),bXe=r(a4e," (VisualBert model)"),a4e.forEach(t),vXe=i(T),Tg=s(T,"LI",{});var s4e=n(Tg);_U=s(s4e,"STRONG",{});var rPr=n(_U);TXe=r(rPr,"vit"),rPr.forEach(t),FXe=r(s4e," \u2014 "),RB=s(s4e,"A",{href:!0});var tPr=n(RB);CXe=r(tPr,"ViTConfig"),tPr.forEach(t),MXe=r(s4e," (ViT model)"),s4e.forEach(t),EXe=i(T),Fg=s(T,"LI",{});var n4e=n(Fg);bU=s(n4e,"STRONG",{});var aPr=n(bU);yXe=r(aPr,"vit_mae"),aPr.forEach(t),wXe=r(n4e," \u2014 "),SB=s(n4e,"A",{href:!0});var sPr=n(SB);AXe=r(sPr,"ViTMAEConfig"),sPr.forEach(t),LXe=r(n4e," (ViTMAE model)"),n4e.forEach(t),BXe=i(T),Cg=s(T,"LI",{});var l4e=n(Cg);vU=s(l4e,"STRONG",{});var nPr=n(vU);xXe=r(nPr,"wav2vec2"),nPr.forEach(t),kXe=r(l4e," \u2014 "),PB=s(l4e,"A",{href:!0});var lPr=n(PB);RXe=r(lPr,"Wav2Vec2Config"),lPr.forEach(t),SXe=r(l4e," (Wav2Vec2 model)"),l4e.forEach(t),PXe=i(T),Mg=s(T,"LI",{});var i4e=n(Mg);TU=s(i4e,"STRONG",{});var iPr=n(TU);$Xe=r(iPr,"wavlm"),iPr.forEach(t),IXe=r(i4e," \u2014 "),$B=s(i4e,"A",{href:!0});var dPr=n($B);jXe=r(dPr,"WavLMConfig"),dPr.forEach(t),NXe=r(i4e," (WavLM model)"),i4e.forEach(t),DXe=i(T),Eg=s(T,"LI",{});var d4e=n(Eg);FU=s(d4e,"STRONG",{});var cPr=n(FU);qXe=r(cPr,"xglm"),cPr.forEach(t),OXe=r(d4e," \u2014 "),IB=s(d4e,"A",{href:!0});var mPr=n(IB);GXe=r(mPr,"XGLMConfig"),mPr.forEach(t),XXe=r(d4e," (XGLM model)"),d4e.forEach(t),VXe=i(T),yg=s(T,"LI",{});var c4e=n(yg);CU=s(c4e,"STRONG",{});var fPr=n(CU);zXe=r(fPr,"xlm"),fPr.forEach(t),WXe=r(c4e," \u2014 "),jB=s(c4e,"A",{href:!0});var gPr=n(jB);QXe=r(gPr,"XLMConfig"),gPr.forEach(t),HXe=r(c4e," (XLM model)"),c4e.forEach(t),UXe=i(T),wg=s(T,"LI",{});var m4e=n(wg);MU=s(m4e,"STRONG",{});var hPr=n(MU);JXe=r(hPr,"xlm-prophetnet"),hPr.forEach(t),YXe=r(m4e," \u2014 "),NB=s(m4e,"A",{href:!0});var uPr=n(NB);KXe=r(uPr,"XLMProphetNetConfig"),uPr.forEach(t),ZXe=r(m4e," (XLMProphetNet model)"),m4e.forEach(t),eVe=i(T),Ag=s(T,"LI",{});var f4e=n(Ag);EU=s(f4e,"STRONG",{});var pPr=n(EU);oVe=r(pPr,"xlm-roberta"),pPr.forEach(t),rVe=r(f4e," \u2014 "),DB=s(f4e,"A",{href:!0});var _Pr=n(DB);tVe=r(_Pr,"XLMRobertaConfig"),_Pr.forEach(t),aVe=r(f4e," (XLM-RoBERTa model)"),f4e.forEach(t),sVe=i(T),Lg=s(T,"LI",{});var g4e=n(Lg);yU=s(g4e,"STRONG",{});var bPr=n(yU);nVe=r(bPr,"xlm-roberta-xl"),bPr.forEach(t),lVe=r(g4e," \u2014 "),qB=s(g4e,"A",{href:!0});var vPr=n(qB);iVe=r(vPr,"XLMRobertaXLConfig"),vPr.forEach(t),dVe=r(g4e," (XLM-RoBERTa-XL model)"),g4e.forEach(t),cVe=i(T),Bg=s(T,"LI",{});var h4e=n(Bg);wU=s(h4e,"STRONG",{});var TPr=n(wU);mVe=r(TPr,"xlnet"),TPr.forEach(t),fVe=r(h4e," \u2014 "),OB=s(h4e,"A",{href:!0});var FPr=n(OB);gVe=r(FPr,"XLNetConfig"),FPr.forEach(t),hVe=r(h4e," (XLNet model)"),h4e.forEach(t),uVe=i(T),xg=s(T,"LI",{});var u4e=n(xg);AU=s(u4e,"STRONG",{});var CPr=n(AU);pVe=r(CPr,"yoso"),CPr.forEach(t),_Ve=r(u4e," \u2014 "),GB=s(u4e,"A",{href:!0});var MPr=n(GB);bVe=r(MPr,"YosoConfig"),MPr.forEach(t),vVe=r(u4e," (YOSO model)"),u4e.forEach(t),T.forEach(t),TVe=i(ga),LU=s(ga,"P",{});var EPr=n(LU);FVe=r(EPr,"Examples:"),EPr.forEach(t),CVe=i(ga),f(fy.$$.fragment,ga),ga.forEach(t),MVe=i(Gn),kg=s(Gn,"DIV",{class:!0});var QRe=n(kg);f(gy.$$.fragment,QRe),EVe=i(QRe),BU=s(QRe,"P",{});var yPr=n(BU);yVe=r(yPr,"Register a new configuration for this class."),yPr.forEach(t),QRe.forEach(t),Gn.forEach(t),zxe=i(c),Qi=s(c,"H2",{class:!0});var HRe=n(Qi);Rg=s(HRe,"A",{id:!0,class:!0,href:!0});var wPr=n(Rg);xU=s(wPr,"SPAN",{});var APr=n(xU);f(hy.$$.fragment,APr),APr.forEach(t),wPr.forEach(t),wVe=i(HRe),kU=s(HRe,"SPAN",{});var LPr=n(kU);AVe=r(LPr,"AutoTokenizer"),LPr.forEach(t),HRe.forEach(t),Wxe=i(c),Qo=s(c,"DIV",{class:!0});var Xn=n(Qo);f(uy.$$.fragment,Xn),LVe=i(Xn),py=s(Xn,"P",{});var URe=n(py);BVe=r(URe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),XB=s(URe,"A",{href:!0});var BPr=n(XB);xVe=r(BPr,"AutoTokenizer.from_pretrained()"),BPr.forEach(t),kVe=r(URe," class method."),URe.forEach(t),RVe=i(Xn),_y=s(Xn,"P",{});var JRe=n(_y);SVe=r(JRe,"This class cannot be instantiated directly using "),RU=s(JRe,"CODE",{});var xPr=n(RU);PVe=r(xPr,"__init__()"),xPr.forEach(t),$Ve=r(JRe," (throws an error)."),JRe.forEach(t),IVe=i(Xn),go=s(Xn,"DIV",{class:!0});var ha=n(go);f(by.$$.fragment,ha),jVe=i(ha),SU=s(ha,"P",{});var kPr=n(SU);NVe=r(kPr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),kPr.forEach(t),DVe=i(ha),Ga=s(ha,"P",{});var t5=n(Ga);qVe=r(t5,"The tokenizer class to instantiate is selected based on the "),PU=s(t5,"CODE",{});var RPr=n(PU);OVe=r(RPr,"model_type"),RPr.forEach(t),GVe=r(t5,` property of the config object (either
passed as an argument or loaded from `),$U=s(t5,"CODE",{});var SPr=n($U);XVe=r(SPr,"pretrained_model_name_or_path"),SPr.forEach(t),VVe=r(t5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IU=s(t5,"CODE",{});var PPr=n(IU);zVe=r(PPr,"pretrained_model_name_or_path"),PPr.forEach(t),WVe=r(t5,":"),t5.forEach(t),QVe=i(ha),E=s(ha,"UL",{});var y=n(E);zs=s(y,"LI",{});var K7=n(zs);jU=s(K7,"STRONG",{});var $Pr=n(jU);HVe=r($Pr,"albert"),$Pr.forEach(t),UVe=r(K7," \u2014 "),VB=s(K7,"A",{href:!0});var IPr=n(VB);JVe=r(IPr,"AlbertTokenizer"),IPr.forEach(t),YVe=r(K7," or "),zB=s(K7,"A",{href:!0});var jPr=n(zB);KVe=r(jPr,"AlbertTokenizerFast"),jPr.forEach(t),ZVe=r(K7," (ALBERT model)"),K7.forEach(t),eze=i(y),Ws=s(y,"LI",{});var Z7=n(Ws);NU=s(Z7,"STRONG",{});var NPr=n(NU);oze=r(NPr,"bart"),NPr.forEach(t),rze=r(Z7," \u2014 "),WB=s(Z7,"A",{href:!0});var DPr=n(WB);tze=r(DPr,"BartTokenizer"),DPr.forEach(t),aze=r(Z7," or "),QB=s(Z7,"A",{href:!0});var qPr=n(QB);sze=r(qPr,"BartTokenizerFast"),qPr.forEach(t),nze=r(Z7," (BART model)"),Z7.forEach(t),lze=i(y),Qs=s(y,"LI",{});var e8=n(Qs);DU=s(e8,"STRONG",{});var OPr=n(DU);ize=r(OPr,"barthez"),OPr.forEach(t),dze=r(e8," \u2014 "),HB=s(e8,"A",{href:!0});var GPr=n(HB);cze=r(GPr,"BarthezTokenizer"),GPr.forEach(t),mze=r(e8," or "),UB=s(e8,"A",{href:!0});var XPr=n(UB);fze=r(XPr,"BarthezTokenizerFast"),XPr.forEach(t),gze=r(e8," (BARThez model)"),e8.forEach(t),hze=i(y),Sg=s(y,"LI",{});var p4e=n(Sg);qU=s(p4e,"STRONG",{});var VPr=n(qU);uze=r(VPr,"bartpho"),VPr.forEach(t),pze=r(p4e," \u2014 "),JB=s(p4e,"A",{href:!0});var zPr=n(JB);_ze=r(zPr,"BartphoTokenizer"),zPr.forEach(t),bze=r(p4e," (BARTpho model)"),p4e.forEach(t),vze=i(y),Hs=s(y,"LI",{});var o8=n(Hs);OU=s(o8,"STRONG",{});var WPr=n(OU);Tze=r(WPr,"bert"),WPr.forEach(t),Fze=r(o8," \u2014 "),YB=s(o8,"A",{href:!0});var QPr=n(YB);Cze=r(QPr,"BertTokenizer"),QPr.forEach(t),Mze=r(o8," or "),KB=s(o8,"A",{href:!0});var HPr=n(KB);Eze=r(HPr,"BertTokenizerFast"),HPr.forEach(t),yze=r(o8," (BERT model)"),o8.forEach(t),wze=i(y),Pg=s(y,"LI",{});var _4e=n(Pg);GU=s(_4e,"STRONG",{});var UPr=n(GU);Aze=r(UPr,"bert-generation"),UPr.forEach(t),Lze=r(_4e," \u2014 "),ZB=s(_4e,"A",{href:!0});var JPr=n(ZB);Bze=r(JPr,"BertGenerationTokenizer"),JPr.forEach(t),xze=r(_4e," (Bert Generation model)"),_4e.forEach(t),kze=i(y),$g=s(y,"LI",{});var b4e=n($g);XU=s(b4e,"STRONG",{});var YPr=n(XU);Rze=r(YPr,"bert-japanese"),YPr.forEach(t),Sze=r(b4e," \u2014 "),ex=s(b4e,"A",{href:!0});var KPr=n(ex);Pze=r(KPr,"BertJapaneseTokenizer"),KPr.forEach(t),$ze=r(b4e," (BertJapanese model)"),b4e.forEach(t),Ize=i(y),Ig=s(y,"LI",{});var v4e=n(Ig);VU=s(v4e,"STRONG",{});var ZPr=n(VU);jze=r(ZPr,"bertweet"),ZPr.forEach(t),Nze=r(v4e," \u2014 "),ox=s(v4e,"A",{href:!0});var e$r=n(ox);Dze=r(e$r,"BertweetTokenizer"),e$r.forEach(t),qze=r(v4e," (Bertweet model)"),v4e.forEach(t),Oze=i(y),Us=s(y,"LI",{});var r8=n(Us);zU=s(r8,"STRONG",{});var o$r=n(zU);Gze=r(o$r,"big_bird"),o$r.forEach(t),Xze=r(r8," \u2014 "),rx=s(r8,"A",{href:!0});var r$r=n(rx);Vze=r(r$r,"BigBirdTokenizer"),r$r.forEach(t),zze=r(r8," or "),tx=s(r8,"A",{href:!0});var t$r=n(tx);Wze=r(t$r,"BigBirdTokenizerFast"),t$r.forEach(t),Qze=r(r8," (BigBird model)"),r8.forEach(t),Hze=i(y),Js=s(y,"LI",{});var t8=n(Js);WU=s(t8,"STRONG",{});var a$r=n(WU);Uze=r(a$r,"bigbird_pegasus"),a$r.forEach(t),Jze=r(t8," \u2014 "),ax=s(t8,"A",{href:!0});var s$r=n(ax);Yze=r(s$r,"PegasusTokenizer"),s$r.forEach(t),Kze=r(t8," or "),sx=s(t8,"A",{href:!0});var n$r=n(sx);Zze=r(n$r,"PegasusTokenizerFast"),n$r.forEach(t),eWe=r(t8," (BigBirdPegasus model)"),t8.forEach(t),oWe=i(y),Ys=s(y,"LI",{});var a8=n(Ys);QU=s(a8,"STRONG",{});var l$r=n(QU);rWe=r(l$r,"blenderbot"),l$r.forEach(t),tWe=r(a8," \u2014 "),nx=s(a8,"A",{href:!0});var i$r=n(nx);aWe=r(i$r,"BlenderbotTokenizer"),i$r.forEach(t),sWe=r(a8," or "),lx=s(a8,"A",{href:!0});var d$r=n(lx);nWe=r(d$r,"BlenderbotTokenizerFast"),d$r.forEach(t),lWe=r(a8," (Blenderbot model)"),a8.forEach(t),iWe=i(y),jg=s(y,"LI",{});var T4e=n(jg);HU=s(T4e,"STRONG",{});var c$r=n(HU);dWe=r(c$r,"blenderbot-small"),c$r.forEach(t),cWe=r(T4e," \u2014 "),ix=s(T4e,"A",{href:!0});var m$r=n(ix);mWe=r(m$r,"BlenderbotSmallTokenizer"),m$r.forEach(t),fWe=r(T4e," (BlenderbotSmall model)"),T4e.forEach(t),gWe=i(y),Ng=s(y,"LI",{});var F4e=n(Ng);UU=s(F4e,"STRONG",{});var f$r=n(UU);hWe=r(f$r,"byt5"),f$r.forEach(t),uWe=r(F4e," \u2014 "),dx=s(F4e,"A",{href:!0});var g$r=n(dx);pWe=r(g$r,"ByT5Tokenizer"),g$r.forEach(t),_We=r(F4e," (ByT5 model)"),F4e.forEach(t),bWe=i(y),Ks=s(y,"LI",{});var s8=n(Ks);JU=s(s8,"STRONG",{});var h$r=n(JU);vWe=r(h$r,"camembert"),h$r.forEach(t),TWe=r(s8," \u2014 "),cx=s(s8,"A",{href:!0});var u$r=n(cx);FWe=r(u$r,"CamembertTokenizer"),u$r.forEach(t),CWe=r(s8," or "),mx=s(s8,"A",{href:!0});var p$r=n(mx);MWe=r(p$r,"CamembertTokenizerFast"),p$r.forEach(t),EWe=r(s8," (CamemBERT model)"),s8.forEach(t),yWe=i(y),Dg=s(y,"LI",{});var C4e=n(Dg);YU=s(C4e,"STRONG",{});var _$r=n(YU);wWe=r(_$r,"canine"),_$r.forEach(t),AWe=r(C4e," \u2014 "),fx=s(C4e,"A",{href:!0});var b$r=n(fx);LWe=r(b$r,"CanineTokenizer"),b$r.forEach(t),BWe=r(C4e," (Canine model)"),C4e.forEach(t),xWe=i(y),Zs=s(y,"LI",{});var n8=n(Zs);KU=s(n8,"STRONG",{});var v$r=n(KU);kWe=r(v$r,"clip"),v$r.forEach(t),RWe=r(n8," \u2014 "),gx=s(n8,"A",{href:!0});var T$r=n(gx);SWe=r(T$r,"CLIPTokenizer"),T$r.forEach(t),PWe=r(n8," or "),hx=s(n8,"A",{href:!0});var F$r=n(hx);$We=r(F$r,"CLIPTokenizerFast"),F$r.forEach(t),IWe=r(n8," (CLIP model)"),n8.forEach(t),jWe=i(y),en=s(y,"LI",{});var l8=n(en);ZU=s(l8,"STRONG",{});var C$r=n(ZU);NWe=r(C$r,"convbert"),C$r.forEach(t),DWe=r(l8," \u2014 "),ux=s(l8,"A",{href:!0});var M$r=n(ux);qWe=r(M$r,"ConvBertTokenizer"),M$r.forEach(t),OWe=r(l8," or "),px=s(l8,"A",{href:!0});var E$r=n(px);GWe=r(E$r,"ConvBertTokenizerFast"),E$r.forEach(t),XWe=r(l8," (ConvBERT model)"),l8.forEach(t),VWe=i(y),on=s(y,"LI",{});var i8=n(on);eJ=s(i8,"STRONG",{});var y$r=n(eJ);zWe=r(y$r,"cpm"),y$r.forEach(t),WWe=r(i8," \u2014 "),_x=s(i8,"A",{href:!0});var w$r=n(_x);QWe=r(w$r,"CpmTokenizer"),w$r.forEach(t),HWe=r(i8," or "),oJ=s(i8,"CODE",{});var A$r=n(oJ);UWe=r(A$r,"CpmTokenizerFast"),A$r.forEach(t),JWe=r(i8," (CPM model)"),i8.forEach(t),YWe=i(y),qg=s(y,"LI",{});var M4e=n(qg);rJ=s(M4e,"STRONG",{});var L$r=n(rJ);KWe=r(L$r,"ctrl"),L$r.forEach(t),ZWe=r(M4e," \u2014 "),bx=s(M4e,"A",{href:!0});var B$r=n(bx);eQe=r(B$r,"CTRLTokenizer"),B$r.forEach(t),oQe=r(M4e," (CTRL model)"),M4e.forEach(t),rQe=i(y),rn=s(y,"LI",{});var d8=n(rn);tJ=s(d8,"STRONG",{});var x$r=n(tJ);tQe=r(x$r,"deberta"),x$r.forEach(t),aQe=r(d8," \u2014 "),vx=s(d8,"A",{href:!0});var k$r=n(vx);sQe=r(k$r,"DebertaTokenizer"),k$r.forEach(t),nQe=r(d8," or "),Tx=s(d8,"A",{href:!0});var R$r=n(Tx);lQe=r(R$r,"DebertaTokenizerFast"),R$r.forEach(t),iQe=r(d8," (DeBERTa model)"),d8.forEach(t),dQe=i(y),Og=s(y,"LI",{});var E4e=n(Og);aJ=s(E4e,"STRONG",{});var S$r=n(aJ);cQe=r(S$r,"deberta-v2"),S$r.forEach(t),mQe=r(E4e," \u2014 "),Fx=s(E4e,"A",{href:!0});var P$r=n(Fx);fQe=r(P$r,"DebertaV2Tokenizer"),P$r.forEach(t),gQe=r(E4e," (DeBERTa-v2 model)"),E4e.forEach(t),hQe=i(y),tn=s(y,"LI",{});var c8=n(tn);sJ=s(c8,"STRONG",{});var $$r=n(sJ);uQe=r($$r,"distilbert"),$$r.forEach(t),pQe=r(c8," \u2014 "),Cx=s(c8,"A",{href:!0});var I$r=n(Cx);_Qe=r(I$r,"DistilBertTokenizer"),I$r.forEach(t),bQe=r(c8," or "),Mx=s(c8,"A",{href:!0});var j$r=n(Mx);vQe=r(j$r,"DistilBertTokenizerFast"),j$r.forEach(t),TQe=r(c8," (DistilBERT model)"),c8.forEach(t),FQe=i(y),an=s(y,"LI",{});var m8=n(an);nJ=s(m8,"STRONG",{});var N$r=n(nJ);CQe=r(N$r,"dpr"),N$r.forEach(t),MQe=r(m8," \u2014 "),Ex=s(m8,"A",{href:!0});var D$r=n(Ex);EQe=r(D$r,"DPRQuestionEncoderTokenizer"),D$r.forEach(t),yQe=r(m8," or "),yx=s(m8,"A",{href:!0});var q$r=n(yx);wQe=r(q$r,"DPRQuestionEncoderTokenizerFast"),q$r.forEach(t),AQe=r(m8," (DPR model)"),m8.forEach(t),LQe=i(y),sn=s(y,"LI",{});var f8=n(sn);lJ=s(f8,"STRONG",{});var O$r=n(lJ);BQe=r(O$r,"electra"),O$r.forEach(t),xQe=r(f8," \u2014 "),wx=s(f8,"A",{href:!0});var G$r=n(wx);kQe=r(G$r,"ElectraTokenizer"),G$r.forEach(t),RQe=r(f8," or "),Ax=s(f8,"A",{href:!0});var X$r=n(Ax);SQe=r(X$r,"ElectraTokenizerFast"),X$r.forEach(t),PQe=r(f8," (ELECTRA model)"),f8.forEach(t),$Qe=i(y),Gg=s(y,"LI",{});var y4e=n(Gg);iJ=s(y4e,"STRONG",{});var V$r=n(iJ);IQe=r(V$r,"flaubert"),V$r.forEach(t),jQe=r(y4e," \u2014 "),Lx=s(y4e,"A",{href:!0});var z$r=n(Lx);NQe=r(z$r,"FlaubertTokenizer"),z$r.forEach(t),DQe=r(y4e," (FlauBERT model)"),y4e.forEach(t),qQe=i(y),nn=s(y,"LI",{});var g8=n(nn);dJ=s(g8,"STRONG",{});var W$r=n(dJ);OQe=r(W$r,"fnet"),W$r.forEach(t),GQe=r(g8," \u2014 "),Bx=s(g8,"A",{href:!0});var Q$r=n(Bx);XQe=r(Q$r,"FNetTokenizer"),Q$r.forEach(t),VQe=r(g8," or "),xx=s(g8,"A",{href:!0});var H$r=n(xx);zQe=r(H$r,"FNetTokenizerFast"),H$r.forEach(t),WQe=r(g8," (FNet model)"),g8.forEach(t),QQe=i(y),Xg=s(y,"LI",{});var w4e=n(Xg);cJ=s(w4e,"STRONG",{});var U$r=n(cJ);HQe=r(U$r,"fsmt"),U$r.forEach(t),UQe=r(w4e," \u2014 "),kx=s(w4e,"A",{href:!0});var J$r=n(kx);JQe=r(J$r,"FSMTTokenizer"),J$r.forEach(t),YQe=r(w4e," (FairSeq Machine-Translation model)"),w4e.forEach(t),KQe=i(y),ln=s(y,"LI",{});var h8=n(ln);mJ=s(h8,"STRONG",{});var Y$r=n(mJ);ZQe=r(Y$r,"funnel"),Y$r.forEach(t),eHe=r(h8," \u2014 "),Rx=s(h8,"A",{href:!0});var K$r=n(Rx);oHe=r(K$r,"FunnelTokenizer"),K$r.forEach(t),rHe=r(h8," or "),Sx=s(h8,"A",{href:!0});var Z$r=n(Sx);tHe=r(Z$r,"FunnelTokenizerFast"),Z$r.forEach(t),aHe=r(h8," (Funnel Transformer model)"),h8.forEach(t),sHe=i(y),dn=s(y,"LI",{});var u8=n(dn);fJ=s(u8,"STRONG",{});var eIr=n(fJ);nHe=r(eIr,"gpt2"),eIr.forEach(t),lHe=r(u8," \u2014 "),Px=s(u8,"A",{href:!0});var oIr=n(Px);iHe=r(oIr,"GPT2Tokenizer"),oIr.forEach(t),dHe=r(u8," or "),$x=s(u8,"A",{href:!0});var rIr=n($x);cHe=r(rIr,"GPT2TokenizerFast"),rIr.forEach(t),mHe=r(u8," (OpenAI GPT-2 model)"),u8.forEach(t),fHe=i(y),cn=s(y,"LI",{});var p8=n(cn);gJ=s(p8,"STRONG",{});var tIr=n(gJ);gHe=r(tIr,"gpt_neo"),tIr.forEach(t),hHe=r(p8," \u2014 "),Ix=s(p8,"A",{href:!0});var aIr=n(Ix);uHe=r(aIr,"GPT2Tokenizer"),aIr.forEach(t),pHe=r(p8," or "),jx=s(p8,"A",{href:!0});var sIr=n(jx);_He=r(sIr,"GPT2TokenizerFast"),sIr.forEach(t),bHe=r(p8," (GPT Neo model)"),p8.forEach(t),vHe=i(y),mn=s(y,"LI",{});var _8=n(mn);hJ=s(_8,"STRONG",{});var nIr=n(hJ);THe=r(nIr,"herbert"),nIr.forEach(t),FHe=r(_8," \u2014 "),Nx=s(_8,"A",{href:!0});var lIr=n(Nx);CHe=r(lIr,"HerbertTokenizer"),lIr.forEach(t),MHe=r(_8," or "),Dx=s(_8,"A",{href:!0});var iIr=n(Dx);EHe=r(iIr,"HerbertTokenizerFast"),iIr.forEach(t),yHe=r(_8," (HerBERT model)"),_8.forEach(t),wHe=i(y),Vg=s(y,"LI",{});var A4e=n(Vg);uJ=s(A4e,"STRONG",{});var dIr=n(uJ);AHe=r(dIr,"hubert"),dIr.forEach(t),LHe=r(A4e," \u2014 "),qx=s(A4e,"A",{href:!0});var cIr=n(qx);BHe=r(cIr,"Wav2Vec2CTCTokenizer"),cIr.forEach(t),xHe=r(A4e," (Hubert model)"),A4e.forEach(t),kHe=i(y),fn=s(y,"LI",{});var b8=n(fn);pJ=s(b8,"STRONG",{});var mIr=n(pJ);RHe=r(mIr,"ibert"),mIr.forEach(t),SHe=r(b8," \u2014 "),Ox=s(b8,"A",{href:!0});var fIr=n(Ox);PHe=r(fIr,"RobertaTokenizer"),fIr.forEach(t),$He=r(b8," or "),Gx=s(b8,"A",{href:!0});var gIr=n(Gx);IHe=r(gIr,"RobertaTokenizerFast"),gIr.forEach(t),jHe=r(b8," (I-BERT model)"),b8.forEach(t),NHe=i(y),gn=s(y,"LI",{});var v8=n(gn);_J=s(v8,"STRONG",{});var hIr=n(_J);DHe=r(hIr,"layoutlm"),hIr.forEach(t),qHe=r(v8," \u2014 "),Xx=s(v8,"A",{href:!0});var uIr=n(Xx);OHe=r(uIr,"LayoutLMTokenizer"),uIr.forEach(t),GHe=r(v8," or "),Vx=s(v8,"A",{href:!0});var pIr=n(Vx);XHe=r(pIr,"LayoutLMTokenizerFast"),pIr.forEach(t),VHe=r(v8," (LayoutLM model)"),v8.forEach(t),zHe=i(y),hn=s(y,"LI",{});var T8=n(hn);bJ=s(T8,"STRONG",{});var _Ir=n(bJ);WHe=r(_Ir,"layoutlmv2"),_Ir.forEach(t),QHe=r(T8," \u2014 "),zx=s(T8,"A",{href:!0});var bIr=n(zx);HHe=r(bIr,"LayoutLMv2Tokenizer"),bIr.forEach(t),UHe=r(T8," or "),Wx=s(T8,"A",{href:!0});var vIr=n(Wx);JHe=r(vIr,"LayoutLMv2TokenizerFast"),vIr.forEach(t),YHe=r(T8," (LayoutLMv2 model)"),T8.forEach(t),KHe=i(y),un=s(y,"LI",{});var F8=n(un);vJ=s(F8,"STRONG",{});var TIr=n(vJ);ZHe=r(TIr,"layoutxlm"),TIr.forEach(t),eUe=r(F8," \u2014 "),Qx=s(F8,"A",{href:!0});var FIr=n(Qx);oUe=r(FIr,"LayoutXLMTokenizer"),FIr.forEach(t),rUe=r(F8," or "),Hx=s(F8,"A",{href:!0});var CIr=n(Hx);tUe=r(CIr,"LayoutXLMTokenizerFast"),CIr.forEach(t),aUe=r(F8," (LayoutXLM model)"),F8.forEach(t),sUe=i(y),pn=s(y,"LI",{});var C8=n(pn);TJ=s(C8,"STRONG",{});var MIr=n(TJ);nUe=r(MIr,"led"),MIr.forEach(t),lUe=r(C8," \u2014 "),Ux=s(C8,"A",{href:!0});var EIr=n(Ux);iUe=r(EIr,"LEDTokenizer"),EIr.forEach(t),dUe=r(C8," or "),Jx=s(C8,"A",{href:!0});var yIr=n(Jx);cUe=r(yIr,"LEDTokenizerFast"),yIr.forEach(t),mUe=r(C8," (LED model)"),C8.forEach(t),fUe=i(y),_n=s(y,"LI",{});var M8=n(_n);FJ=s(M8,"STRONG",{});var wIr=n(FJ);gUe=r(wIr,"longformer"),wIr.forEach(t),hUe=r(M8," \u2014 "),Yx=s(M8,"A",{href:!0});var AIr=n(Yx);uUe=r(AIr,"LongformerTokenizer"),AIr.forEach(t),pUe=r(M8," or "),Kx=s(M8,"A",{href:!0});var LIr=n(Kx);_Ue=r(LIr,"LongformerTokenizerFast"),LIr.forEach(t),bUe=r(M8," (Longformer model)"),M8.forEach(t),vUe=i(y),zg=s(y,"LI",{});var L4e=n(zg);CJ=s(L4e,"STRONG",{});var BIr=n(CJ);TUe=r(BIr,"luke"),BIr.forEach(t),FUe=r(L4e," \u2014 "),Zx=s(L4e,"A",{href:!0});var xIr=n(Zx);CUe=r(xIr,"LukeTokenizer"),xIr.forEach(t),MUe=r(L4e," (LUKE model)"),L4e.forEach(t),EUe=i(y),bn=s(y,"LI",{});var E8=n(bn);MJ=s(E8,"STRONG",{});var kIr=n(MJ);yUe=r(kIr,"lxmert"),kIr.forEach(t),wUe=r(E8," \u2014 "),ek=s(E8,"A",{href:!0});var RIr=n(ek);AUe=r(RIr,"LxmertTokenizer"),RIr.forEach(t),LUe=r(E8," or "),ok=s(E8,"A",{href:!0});var SIr=n(ok);BUe=r(SIr,"LxmertTokenizerFast"),SIr.forEach(t),xUe=r(E8," (LXMERT model)"),E8.forEach(t),kUe=i(y),Wg=s(y,"LI",{});var B4e=n(Wg);EJ=s(B4e,"STRONG",{});var PIr=n(EJ);RUe=r(PIr,"m2m_100"),PIr.forEach(t),SUe=r(B4e," \u2014 "),rk=s(B4e,"A",{href:!0});var $Ir=n(rk);PUe=r($Ir,"M2M100Tokenizer"),$Ir.forEach(t),$Ue=r(B4e," (M2M100 model)"),B4e.forEach(t),IUe=i(y),Qg=s(y,"LI",{});var x4e=n(Qg);yJ=s(x4e,"STRONG",{});var IIr=n(yJ);jUe=r(IIr,"marian"),IIr.forEach(t),NUe=r(x4e," \u2014 "),tk=s(x4e,"A",{href:!0});var jIr=n(tk);DUe=r(jIr,"MarianTokenizer"),jIr.forEach(t),qUe=r(x4e," (Marian model)"),x4e.forEach(t),OUe=i(y),vn=s(y,"LI",{});var y8=n(vn);wJ=s(y8,"STRONG",{});var NIr=n(wJ);GUe=r(NIr,"mbart"),NIr.forEach(t),XUe=r(y8," \u2014 "),ak=s(y8,"A",{href:!0});var DIr=n(ak);VUe=r(DIr,"MBartTokenizer"),DIr.forEach(t),zUe=r(y8," or "),sk=s(y8,"A",{href:!0});var qIr=n(sk);WUe=r(qIr,"MBartTokenizerFast"),qIr.forEach(t),QUe=r(y8," (mBART model)"),y8.forEach(t),HUe=i(y),Tn=s(y,"LI",{});var w8=n(Tn);AJ=s(w8,"STRONG",{});var OIr=n(AJ);UUe=r(OIr,"mbart50"),OIr.forEach(t),JUe=r(w8," \u2014 "),nk=s(w8,"A",{href:!0});var GIr=n(nk);YUe=r(GIr,"MBart50Tokenizer"),GIr.forEach(t),KUe=r(w8," or "),lk=s(w8,"A",{href:!0});var XIr=n(lk);ZUe=r(XIr,"MBart50TokenizerFast"),XIr.forEach(t),eJe=r(w8," (mBART-50 model)"),w8.forEach(t),oJe=i(y),Hg=s(y,"LI",{});var k4e=n(Hg);LJ=s(k4e,"STRONG",{});var VIr=n(LJ);rJe=r(VIr,"mluke"),VIr.forEach(t),tJe=r(k4e," \u2014 "),ik=s(k4e,"A",{href:!0});var zIr=n(ik);aJe=r(zIr,"MLukeTokenizer"),zIr.forEach(t),sJe=r(k4e," (mLUKE model)"),k4e.forEach(t),nJe=i(y),Fn=s(y,"LI",{});var A8=n(Fn);BJ=s(A8,"STRONG",{});var WIr=n(BJ);lJe=r(WIr,"mobilebert"),WIr.forEach(t),iJe=r(A8," \u2014 "),dk=s(A8,"A",{href:!0});var QIr=n(dk);dJe=r(QIr,"MobileBertTokenizer"),QIr.forEach(t),cJe=r(A8," or "),ck=s(A8,"A",{href:!0});var HIr=n(ck);mJe=r(HIr,"MobileBertTokenizerFast"),HIr.forEach(t),fJe=r(A8," (MobileBERT model)"),A8.forEach(t),gJe=i(y),Cn=s(y,"LI",{});var L8=n(Cn);xJ=s(L8,"STRONG",{});var UIr=n(xJ);hJe=r(UIr,"mpnet"),UIr.forEach(t),uJe=r(L8," \u2014 "),mk=s(L8,"A",{href:!0});var JIr=n(mk);pJe=r(JIr,"MPNetTokenizer"),JIr.forEach(t),_Je=r(L8," or "),fk=s(L8,"A",{href:!0});var YIr=n(fk);bJe=r(YIr,"MPNetTokenizerFast"),YIr.forEach(t),vJe=r(L8," (MPNet model)"),L8.forEach(t),TJe=i(y),Mn=s(y,"LI",{});var B8=n(Mn);kJ=s(B8,"STRONG",{});var KIr=n(kJ);FJe=r(KIr,"mt5"),KIr.forEach(t),CJe=r(B8," \u2014 "),gk=s(B8,"A",{href:!0});var ZIr=n(gk);MJe=r(ZIr,"MT5Tokenizer"),ZIr.forEach(t),EJe=r(B8," or "),hk=s(B8,"A",{href:!0});var ejr=n(hk);yJe=r(ejr,"MT5TokenizerFast"),ejr.forEach(t),wJe=r(B8," (mT5 model)"),B8.forEach(t),AJe=i(y),En=s(y,"LI",{});var x8=n(En);RJ=s(x8,"STRONG",{});var ojr=n(RJ);LJe=r(ojr,"openai-gpt"),ojr.forEach(t),BJe=r(x8," \u2014 "),uk=s(x8,"A",{href:!0});var rjr=n(uk);xJe=r(rjr,"OpenAIGPTTokenizer"),rjr.forEach(t),kJe=r(x8," or "),pk=s(x8,"A",{href:!0});var tjr=n(pk);RJe=r(tjr,"OpenAIGPTTokenizerFast"),tjr.forEach(t),SJe=r(x8," (OpenAI GPT model)"),x8.forEach(t),PJe=i(y),yn=s(y,"LI",{});var k8=n(yn);SJ=s(k8,"STRONG",{});var ajr=n(SJ);$Je=r(ajr,"pegasus"),ajr.forEach(t),IJe=r(k8," \u2014 "),_k=s(k8,"A",{href:!0});var sjr=n(_k);jJe=r(sjr,"PegasusTokenizer"),sjr.forEach(t),NJe=r(k8," or "),bk=s(k8,"A",{href:!0});var njr=n(bk);DJe=r(njr,"PegasusTokenizerFast"),njr.forEach(t),qJe=r(k8," (Pegasus model)"),k8.forEach(t),OJe=i(y),Ug=s(y,"LI",{});var R4e=n(Ug);PJ=s(R4e,"STRONG",{});var ljr=n(PJ);GJe=r(ljr,"perceiver"),ljr.forEach(t),XJe=r(R4e," \u2014 "),vk=s(R4e,"A",{href:!0});var ijr=n(vk);VJe=r(ijr,"PerceiverTokenizer"),ijr.forEach(t),zJe=r(R4e," (Perceiver model)"),R4e.forEach(t),WJe=i(y),Jg=s(y,"LI",{});var S4e=n(Jg);$J=s(S4e,"STRONG",{});var djr=n($J);QJe=r(djr,"phobert"),djr.forEach(t),HJe=r(S4e," \u2014 "),Tk=s(S4e,"A",{href:!0});var cjr=n(Tk);UJe=r(cjr,"PhobertTokenizer"),cjr.forEach(t),JJe=r(S4e," (PhoBERT model)"),S4e.forEach(t),YJe=i(y),Yg=s(y,"LI",{});var P4e=n(Yg);IJ=s(P4e,"STRONG",{});var mjr=n(IJ);KJe=r(mjr,"plbart"),mjr.forEach(t),ZJe=r(P4e," \u2014 "),Fk=s(P4e,"A",{href:!0});var fjr=n(Fk);eYe=r(fjr,"PLBartTokenizer"),fjr.forEach(t),oYe=r(P4e," (PLBart model)"),P4e.forEach(t),rYe=i(y),Kg=s(y,"LI",{});var $4e=n(Kg);jJ=s($4e,"STRONG",{});var gjr=n(jJ);tYe=r(gjr,"prophetnet"),gjr.forEach(t),aYe=r($4e," \u2014 "),Ck=s($4e,"A",{href:!0});var hjr=n(Ck);sYe=r(hjr,"ProphetNetTokenizer"),hjr.forEach(t),nYe=r($4e," (ProphetNet model)"),$4e.forEach(t),lYe=i(y),wn=s(y,"LI",{});var R8=n(wn);NJ=s(R8,"STRONG",{});var ujr=n(NJ);iYe=r(ujr,"qdqbert"),ujr.forEach(t),dYe=r(R8," \u2014 "),Mk=s(R8,"A",{href:!0});var pjr=n(Mk);cYe=r(pjr,"BertTokenizer"),pjr.forEach(t),mYe=r(R8," or "),Ek=s(R8,"A",{href:!0});var _jr=n(Ek);fYe=r(_jr,"BertTokenizerFast"),_jr.forEach(t),gYe=r(R8," (QDQBert model)"),R8.forEach(t),hYe=i(y),Zg=s(y,"LI",{});var I4e=n(Zg);DJ=s(I4e,"STRONG",{});var bjr=n(DJ);uYe=r(bjr,"rag"),bjr.forEach(t),pYe=r(I4e," \u2014 "),yk=s(I4e,"A",{href:!0});var vjr=n(yk);_Ye=r(vjr,"RagTokenizer"),vjr.forEach(t),bYe=r(I4e," (RAG model)"),I4e.forEach(t),vYe=i(y),An=s(y,"LI",{});var S8=n(An);qJ=s(S8,"STRONG",{});var Tjr=n(qJ);TYe=r(Tjr,"realm"),Tjr.forEach(t),FYe=r(S8," \u2014 "),wk=s(S8,"A",{href:!0});var Fjr=n(wk);CYe=r(Fjr,"RealmTokenizer"),Fjr.forEach(t),MYe=r(S8," or "),Ak=s(S8,"A",{href:!0});var Cjr=n(Ak);EYe=r(Cjr,"RealmTokenizerFast"),Cjr.forEach(t),yYe=r(S8," (Realm model)"),S8.forEach(t),wYe=i(y),Ln=s(y,"LI",{});var P8=n(Ln);OJ=s(P8,"STRONG",{});var Mjr=n(OJ);AYe=r(Mjr,"reformer"),Mjr.forEach(t),LYe=r(P8," \u2014 "),Lk=s(P8,"A",{href:!0});var Ejr=n(Lk);BYe=r(Ejr,"ReformerTokenizer"),Ejr.forEach(t),xYe=r(P8," or "),Bk=s(P8,"A",{href:!0});var yjr=n(Bk);kYe=r(yjr,"ReformerTokenizerFast"),yjr.forEach(t),RYe=r(P8," (Reformer model)"),P8.forEach(t),SYe=i(y),Bn=s(y,"LI",{});var $8=n(Bn);GJ=s($8,"STRONG",{});var wjr=n(GJ);PYe=r(wjr,"rembert"),wjr.forEach(t),$Ye=r($8," \u2014 "),xk=s($8,"A",{href:!0});var Ajr=n(xk);IYe=r(Ajr,"RemBertTokenizer"),Ajr.forEach(t),jYe=r($8," or "),kk=s($8,"A",{href:!0});var Ljr=n(kk);NYe=r(Ljr,"RemBertTokenizerFast"),Ljr.forEach(t),DYe=r($8," (RemBERT model)"),$8.forEach(t),qYe=i(y),xn=s(y,"LI",{});var I8=n(xn);XJ=s(I8,"STRONG",{});var Bjr=n(XJ);OYe=r(Bjr,"retribert"),Bjr.forEach(t),GYe=r(I8," \u2014 "),Rk=s(I8,"A",{href:!0});var xjr=n(Rk);XYe=r(xjr,"RetriBertTokenizer"),xjr.forEach(t),VYe=r(I8," or "),Sk=s(I8,"A",{href:!0});var kjr=n(Sk);zYe=r(kjr,"RetriBertTokenizerFast"),kjr.forEach(t),WYe=r(I8," (RetriBERT model)"),I8.forEach(t),QYe=i(y),kn=s(y,"LI",{});var j8=n(kn);VJ=s(j8,"STRONG",{});var Rjr=n(VJ);HYe=r(Rjr,"roberta"),Rjr.forEach(t),UYe=r(j8," \u2014 "),Pk=s(j8,"A",{href:!0});var Sjr=n(Pk);JYe=r(Sjr,"RobertaTokenizer"),Sjr.forEach(t),YYe=r(j8," or "),$k=s(j8,"A",{href:!0});var Pjr=n($k);KYe=r(Pjr,"RobertaTokenizerFast"),Pjr.forEach(t),ZYe=r(j8," (RoBERTa model)"),j8.forEach(t),eKe=i(y),Rn=s(y,"LI",{});var N8=n(Rn);zJ=s(N8,"STRONG",{});var $jr=n(zJ);oKe=r($jr,"roformer"),$jr.forEach(t),rKe=r(N8," \u2014 "),Ik=s(N8,"A",{href:!0});var Ijr=n(Ik);tKe=r(Ijr,"RoFormerTokenizer"),Ijr.forEach(t),aKe=r(N8," or "),jk=s(N8,"A",{href:!0});var jjr=n(jk);sKe=r(jjr,"RoFormerTokenizerFast"),jjr.forEach(t),nKe=r(N8," (RoFormer model)"),N8.forEach(t),lKe=i(y),eh=s(y,"LI",{});var j4e=n(eh);WJ=s(j4e,"STRONG",{});var Njr=n(WJ);iKe=r(Njr,"speech_to_text"),Njr.forEach(t),dKe=r(j4e," \u2014 "),Nk=s(j4e,"A",{href:!0});var Djr=n(Nk);cKe=r(Djr,"Speech2TextTokenizer"),Djr.forEach(t),mKe=r(j4e," (Speech2Text model)"),j4e.forEach(t),fKe=i(y),oh=s(y,"LI",{});var N4e=n(oh);QJ=s(N4e,"STRONG",{});var qjr=n(QJ);gKe=r(qjr,"speech_to_text_2"),qjr.forEach(t),hKe=r(N4e," \u2014 "),Dk=s(N4e,"A",{href:!0});var Ojr=n(Dk);uKe=r(Ojr,"Speech2Text2Tokenizer"),Ojr.forEach(t),pKe=r(N4e," (Speech2Text2 model)"),N4e.forEach(t),_Ke=i(y),Sn=s(y,"LI",{});var D8=n(Sn);HJ=s(D8,"STRONG",{});var Gjr=n(HJ);bKe=r(Gjr,"splinter"),Gjr.forEach(t),vKe=r(D8," \u2014 "),qk=s(D8,"A",{href:!0});var Xjr=n(qk);TKe=r(Xjr,"SplinterTokenizer"),Xjr.forEach(t),FKe=r(D8," or "),Ok=s(D8,"A",{href:!0});var Vjr=n(Ok);CKe=r(Vjr,"SplinterTokenizerFast"),Vjr.forEach(t),MKe=r(D8," (Splinter model)"),D8.forEach(t),EKe=i(y),Pn=s(y,"LI",{});var q8=n(Pn);UJ=s(q8,"STRONG",{});var zjr=n(UJ);yKe=r(zjr,"squeezebert"),zjr.forEach(t),wKe=r(q8," \u2014 "),Gk=s(q8,"A",{href:!0});var Wjr=n(Gk);AKe=r(Wjr,"SqueezeBertTokenizer"),Wjr.forEach(t),LKe=r(q8," or "),Xk=s(q8,"A",{href:!0});var Qjr=n(Xk);BKe=r(Qjr,"SqueezeBertTokenizerFast"),Qjr.forEach(t),xKe=r(q8," (SqueezeBERT model)"),q8.forEach(t),kKe=i(y),$n=s(y,"LI",{});var O8=n($n);JJ=s(O8,"STRONG",{});var Hjr=n(JJ);RKe=r(Hjr,"t5"),Hjr.forEach(t),SKe=r(O8," \u2014 "),Vk=s(O8,"A",{href:!0});var Ujr=n(Vk);PKe=r(Ujr,"T5Tokenizer"),Ujr.forEach(t),$Ke=r(O8," or "),zk=s(O8,"A",{href:!0});var Jjr=n(zk);IKe=r(Jjr,"T5TokenizerFast"),Jjr.forEach(t),jKe=r(O8," (T5 model)"),O8.forEach(t),NKe=i(y),rh=s(y,"LI",{});var D4e=n(rh);YJ=s(D4e,"STRONG",{});var Yjr=n(YJ);DKe=r(Yjr,"tapas"),Yjr.forEach(t),qKe=r(D4e," \u2014 "),Wk=s(D4e,"A",{href:!0});var Kjr=n(Wk);OKe=r(Kjr,"TapasTokenizer"),Kjr.forEach(t),GKe=r(D4e," (TAPAS model)"),D4e.forEach(t),XKe=i(y),th=s(y,"LI",{});var q4e=n(th);KJ=s(q4e,"STRONG",{});var Zjr=n(KJ);VKe=r(Zjr,"transfo-xl"),Zjr.forEach(t),zKe=r(q4e," \u2014 "),Qk=s(q4e,"A",{href:!0});var eNr=n(Qk);WKe=r(eNr,"TransfoXLTokenizer"),eNr.forEach(t),QKe=r(q4e," (Transformer-XL model)"),q4e.forEach(t),HKe=i(y),ah=s(y,"LI",{});var O4e=n(ah);ZJ=s(O4e,"STRONG",{});var oNr=n(ZJ);UKe=r(oNr,"wav2vec2"),oNr.forEach(t),JKe=r(O4e," \u2014 "),Hk=s(O4e,"A",{href:!0});var rNr=n(Hk);YKe=r(rNr,"Wav2Vec2CTCTokenizer"),rNr.forEach(t),KKe=r(O4e," (Wav2Vec2 model)"),O4e.forEach(t),ZKe=i(y),sh=s(y,"LI",{});var G4e=n(sh);eY=s(G4e,"STRONG",{});var tNr=n(eY);eZe=r(tNr,"wav2vec2_phoneme"),tNr.forEach(t),oZe=r(G4e," \u2014 "),Uk=s(G4e,"A",{href:!0});var aNr=n(Uk);rZe=r(aNr,"Wav2Vec2PhonemeCTCTokenizer"),aNr.forEach(t),tZe=r(G4e," (Wav2Vec2Phoneme model)"),G4e.forEach(t),aZe=i(y),In=s(y,"LI",{});var G8=n(In);oY=s(G8,"STRONG",{});var sNr=n(oY);sZe=r(sNr,"xglm"),sNr.forEach(t),nZe=r(G8," \u2014 "),Jk=s(G8,"A",{href:!0});var nNr=n(Jk);lZe=r(nNr,"XGLMTokenizer"),nNr.forEach(t),iZe=r(G8," or "),Yk=s(G8,"A",{href:!0});var lNr=n(Yk);dZe=r(lNr,"XGLMTokenizerFast"),lNr.forEach(t),cZe=r(G8," (XGLM model)"),G8.forEach(t),mZe=i(y),nh=s(y,"LI",{});var X4e=n(nh);rY=s(X4e,"STRONG",{});var iNr=n(rY);fZe=r(iNr,"xlm"),iNr.forEach(t),gZe=r(X4e," \u2014 "),Kk=s(X4e,"A",{href:!0});var dNr=n(Kk);hZe=r(dNr,"XLMTokenizer"),dNr.forEach(t),uZe=r(X4e," (XLM model)"),X4e.forEach(t),pZe=i(y),lh=s(y,"LI",{});var V4e=n(lh);tY=s(V4e,"STRONG",{});var cNr=n(tY);_Ze=r(cNr,"xlm-prophetnet"),cNr.forEach(t),bZe=r(V4e," \u2014 "),Zk=s(V4e,"A",{href:!0});var mNr=n(Zk);vZe=r(mNr,"XLMProphetNetTokenizer"),mNr.forEach(t),TZe=r(V4e," (XLMProphetNet model)"),V4e.forEach(t),FZe=i(y),jn=s(y,"LI",{});var X8=n(jn);aY=s(X8,"STRONG",{});var fNr=n(aY);CZe=r(fNr,"xlm-roberta"),fNr.forEach(t),MZe=r(X8," \u2014 "),eR=s(X8,"A",{href:!0});var gNr=n(eR);EZe=r(gNr,"XLMRobertaTokenizer"),gNr.forEach(t),yZe=r(X8," or "),oR=s(X8,"A",{href:!0});var hNr=n(oR);wZe=r(hNr,"XLMRobertaTokenizerFast"),hNr.forEach(t),AZe=r(X8," (XLM-RoBERTa model)"),X8.forEach(t),LZe=i(y),Nn=s(y,"LI",{});var V8=n(Nn);sY=s(V8,"STRONG",{});var uNr=n(sY);BZe=r(uNr,"xlnet"),uNr.forEach(t),xZe=r(V8," \u2014 "),rR=s(V8,"A",{href:!0});var pNr=n(rR);kZe=r(pNr,"XLNetTokenizer"),pNr.forEach(t),RZe=r(V8," or "),tR=s(V8,"A",{href:!0});var _Nr=n(tR);SZe=r(_Nr,"XLNetTokenizerFast"),_Nr.forEach(t),PZe=r(V8," (XLNet model)"),V8.forEach(t),y.forEach(t),$Ze=i(ha),nY=s(ha,"P",{});var bNr=n(nY);IZe=r(bNr,"Examples:"),bNr.forEach(t),jZe=i(ha),f(vy.$$.fragment,ha),ha.forEach(t),NZe=i(Xn),ih=s(Xn,"DIV",{class:!0});var YRe=n(ih);f(Ty.$$.fragment,YRe),DZe=i(YRe),lY=s(YRe,"P",{});var vNr=n(lY);qZe=r(vNr,"Register a new tokenizer in this mapping."),vNr.forEach(t),YRe.forEach(t),Xn.forEach(t),Qxe=i(c),Hi=s(c,"H2",{class:!0});var KRe=n(Hi);dh=s(KRe,"A",{id:!0,class:!0,href:!0});var TNr=n(dh);iY=s(TNr,"SPAN",{});var FNr=n(iY);f(Fy.$$.fragment,FNr),FNr.forEach(t),TNr.forEach(t),OZe=i(KRe),dY=s(KRe,"SPAN",{});var CNr=n(dY);GZe=r(CNr,"AutoFeatureExtractor"),CNr.forEach(t),KRe.forEach(t),Hxe=i(c),Ho=s(c,"DIV",{class:!0});var Vn=n(Ho);f(Cy.$$.fragment,Vn),XZe=i(Vn),My=s(Vn,"P",{});var ZRe=n(My);VZe=r(ZRe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),aR=s(ZRe,"A",{href:!0});var MNr=n(aR);zZe=r(MNr,"AutoFeatureExtractor.from_pretrained()"),MNr.forEach(t),WZe=r(ZRe," class method."),ZRe.forEach(t),QZe=i(Vn),Ey=s(Vn,"P",{});var eSe=n(Ey);HZe=r(eSe,"This class cannot be instantiated directly using "),cY=s(eSe,"CODE",{});var ENr=n(cY);UZe=r(ENr,"__init__()"),ENr.forEach(t),JZe=r(eSe," (throws an error)."),eSe.forEach(t),YZe=i(Vn),Ie=s(Vn,"DIV",{class:!0});var Nt=n(Ie);f(yy.$$.fragment,Nt),KZe=i(Nt),mY=s(Nt,"P",{});var yNr=n(mY);ZZe=r(yNr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),yNr.forEach(t),eeo=i(Nt),Xa=s(Nt,"P",{});var a5=n(Xa);oeo=r(a5,"The feature extractor class to instantiate is selected based on the "),fY=s(a5,"CODE",{});var wNr=n(fY);reo=r(wNr,"model_type"),wNr.forEach(t),teo=r(a5,` property of the config object
(either passed as an argument or loaded from `),gY=s(a5,"CODE",{});var ANr=n(gY);aeo=r(ANr,"pretrained_model_name_or_path"),ANr.forEach(t),seo=r(a5,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),hY=s(a5,"CODE",{});var LNr=n(hY);neo=r(LNr,"pretrained_model_name_or_path"),LNr.forEach(t),leo=r(a5,":"),a5.forEach(t),ieo=i(Nt),re=s(Nt,"UL",{});var se=n(re);ch=s(se,"LI",{});var z4e=n(ch);uY=s(z4e,"STRONG",{});var BNr=n(uY);deo=r(BNr,"beit"),BNr.forEach(t),ceo=r(z4e," \u2014 "),sR=s(z4e,"A",{href:!0});var xNr=n(sR);meo=r(xNr,"BeitFeatureExtractor"),xNr.forEach(t),feo=r(z4e," (BEiT model)"),z4e.forEach(t),geo=i(se),mh=s(se,"LI",{});var W4e=n(mh);pY=s(W4e,"STRONG",{});var kNr=n(pY);heo=r(kNr,"clip"),kNr.forEach(t),ueo=r(W4e," \u2014 "),nR=s(W4e,"A",{href:!0});var RNr=n(nR);peo=r(RNr,"CLIPFeatureExtractor"),RNr.forEach(t),_eo=r(W4e," (CLIP model)"),W4e.forEach(t),beo=i(se),fh=s(se,"LI",{});var Q4e=n(fh);_Y=s(Q4e,"STRONG",{});var SNr=n(_Y);veo=r(SNr,"convnext"),SNr.forEach(t),Teo=r(Q4e," \u2014 "),lR=s(Q4e,"A",{href:!0});var PNr=n(lR);Feo=r(PNr,"ConvNextFeatureExtractor"),PNr.forEach(t),Ceo=r(Q4e," (ConvNext model)"),Q4e.forEach(t),Meo=i(se),gh=s(se,"LI",{});var H4e=n(gh);bY=s(H4e,"STRONG",{});var $Nr=n(bY);Eeo=r($Nr,"deit"),$Nr.forEach(t),yeo=r(H4e," \u2014 "),iR=s(H4e,"A",{href:!0});var INr=n(iR);weo=r(INr,"DeiTFeatureExtractor"),INr.forEach(t),Aeo=r(H4e," (DeiT model)"),H4e.forEach(t),Leo=i(se),hh=s(se,"LI",{});var U4e=n(hh);vY=s(U4e,"STRONG",{});var jNr=n(vY);Beo=r(jNr,"detr"),jNr.forEach(t),xeo=r(U4e," \u2014 "),dR=s(U4e,"A",{href:!0});var NNr=n(dR);keo=r(NNr,"DetrFeatureExtractor"),NNr.forEach(t),Reo=r(U4e," (DETR model)"),U4e.forEach(t),Seo=i(se),uh=s(se,"LI",{});var J4e=n(uh);TY=s(J4e,"STRONG",{});var DNr=n(TY);Peo=r(DNr,"hubert"),DNr.forEach(t),$eo=r(J4e," \u2014 "),cR=s(J4e,"A",{href:!0});var qNr=n(cR);Ieo=r(qNr,"Wav2Vec2FeatureExtractor"),qNr.forEach(t),jeo=r(J4e," (Hubert model)"),J4e.forEach(t),Neo=i(se),ph=s(se,"LI",{});var Y4e=n(ph);FY=s(Y4e,"STRONG",{});var ONr=n(FY);Deo=r(ONr,"layoutlmv2"),ONr.forEach(t),qeo=r(Y4e," \u2014 "),mR=s(Y4e,"A",{href:!0});var GNr=n(mR);Oeo=r(GNr,"LayoutLMv2FeatureExtractor"),GNr.forEach(t),Geo=r(Y4e," (LayoutLMv2 model)"),Y4e.forEach(t),Xeo=i(se),_h=s(se,"LI",{});var K4e=n(_h);CY=s(K4e,"STRONG",{});var XNr=n(CY);Veo=r(XNr,"maskformer"),XNr.forEach(t),zeo=r(K4e," \u2014 "),fR=s(K4e,"A",{href:!0});var VNr=n(fR);Weo=r(VNr,"MaskFormerFeatureExtractor"),VNr.forEach(t),Qeo=r(K4e," (MaskFormer model)"),K4e.forEach(t),Heo=i(se),bh=s(se,"LI",{});var Z4e=n(bh);MY=s(Z4e,"STRONG",{});var zNr=n(MY);Ueo=r(zNr,"perceiver"),zNr.forEach(t),Jeo=r(Z4e," \u2014 "),gR=s(Z4e,"A",{href:!0});var WNr=n(gR);Yeo=r(WNr,"PerceiverFeatureExtractor"),WNr.forEach(t),Keo=r(Z4e," (Perceiver model)"),Z4e.forEach(t),Zeo=i(se),vh=s(se,"LI",{});var eEe=n(vh);EY=s(eEe,"STRONG",{});var QNr=n(EY);eoo=r(QNr,"poolformer"),QNr.forEach(t),ooo=r(eEe," \u2014 "),hR=s(eEe,"A",{href:!0});var HNr=n(hR);roo=r(HNr,"PoolFormerFeatureExtractor"),HNr.forEach(t),too=r(eEe," (PoolFormer model)"),eEe.forEach(t),aoo=i(se),Th=s(se,"LI",{});var oEe=n(Th);yY=s(oEe,"STRONG",{});var UNr=n(yY);soo=r(UNr,"resnet"),UNr.forEach(t),noo=r(oEe," \u2014 "),uR=s(oEe,"A",{href:!0});var JNr=n(uR);loo=r(JNr,"ConvNextFeatureExtractor"),JNr.forEach(t),ioo=r(oEe," (ResNet model)"),oEe.forEach(t),doo=i(se),Fh=s(se,"LI",{});var rEe=n(Fh);wY=s(rEe,"STRONG",{});var YNr=n(wY);coo=r(YNr,"segformer"),YNr.forEach(t),moo=r(rEe," \u2014 "),pR=s(rEe,"A",{href:!0});var KNr=n(pR);foo=r(KNr,"SegformerFeatureExtractor"),KNr.forEach(t),goo=r(rEe," (SegFormer model)"),rEe.forEach(t),hoo=i(se),Ch=s(se,"LI",{});var tEe=n(Ch);AY=s(tEe,"STRONG",{});var ZNr=n(AY);uoo=r(ZNr,"speech_to_text"),ZNr.forEach(t),poo=r(tEe," \u2014 "),_R=s(tEe,"A",{href:!0});var eDr=n(_R);_oo=r(eDr,"Speech2TextFeatureExtractor"),eDr.forEach(t),boo=r(tEe," (Speech2Text model)"),tEe.forEach(t),voo=i(se),Mh=s(se,"LI",{});var aEe=n(Mh);LY=s(aEe,"STRONG",{});var oDr=n(LY);Too=r(oDr,"swin"),oDr.forEach(t),Foo=r(aEe," \u2014 "),bR=s(aEe,"A",{href:!0});var rDr=n(bR);Coo=r(rDr,"ViTFeatureExtractor"),rDr.forEach(t),Moo=r(aEe," (Swin model)"),aEe.forEach(t),Eoo=i(se),Eh=s(se,"LI",{});var sEe=n(Eh);BY=s(sEe,"STRONG",{});var tDr=n(BY);yoo=r(tDr,"van"),tDr.forEach(t),woo=r(sEe," \u2014 "),vR=s(sEe,"A",{href:!0});var aDr=n(vR);Aoo=r(aDr,"ConvNextFeatureExtractor"),aDr.forEach(t),Loo=r(sEe," (VAN model)"),sEe.forEach(t),Boo=i(se),yh=s(se,"LI",{});var nEe=n(yh);xY=s(nEe,"STRONG",{});var sDr=n(xY);xoo=r(sDr,"vit"),sDr.forEach(t),koo=r(nEe," \u2014 "),TR=s(nEe,"A",{href:!0});var nDr=n(TR);Roo=r(nDr,"ViTFeatureExtractor"),nDr.forEach(t),Soo=r(nEe," (ViT model)"),nEe.forEach(t),Poo=i(se),wh=s(se,"LI",{});var lEe=n(wh);kY=s(lEe,"STRONG",{});var lDr=n(kY);$oo=r(lDr,"vit_mae"),lDr.forEach(t),Ioo=r(lEe," \u2014 "),FR=s(lEe,"A",{href:!0});var iDr=n(FR);joo=r(iDr,"ViTFeatureExtractor"),iDr.forEach(t),Noo=r(lEe," (ViTMAE model)"),lEe.forEach(t),Doo=i(se),Ah=s(se,"LI",{});var iEe=n(Ah);RY=s(iEe,"STRONG",{});var dDr=n(RY);qoo=r(dDr,"wav2vec2"),dDr.forEach(t),Ooo=r(iEe," \u2014 "),CR=s(iEe,"A",{href:!0});var cDr=n(CR);Goo=r(cDr,"Wav2Vec2FeatureExtractor"),cDr.forEach(t),Xoo=r(iEe," (Wav2Vec2 model)"),iEe.forEach(t),se.forEach(t),Voo=i(Nt),f(Lh.$$.fragment,Nt),zoo=i(Nt),SY=s(Nt,"P",{});var mDr=n(SY);Woo=r(mDr,"Examples:"),mDr.forEach(t),Qoo=i(Nt),f(wy.$$.fragment,Nt),Nt.forEach(t),Hoo=i(Vn),Bh=s(Vn,"DIV",{class:!0});var oSe=n(Bh);f(Ay.$$.fragment,oSe),Uoo=i(oSe),PY=s(oSe,"P",{});var fDr=n(PY);Joo=r(fDr,"Register a new feature extractor for this class."),fDr.forEach(t),oSe.forEach(t),Vn.forEach(t),Uxe=i(c),Ui=s(c,"H2",{class:!0});var rSe=n(Ui);xh=s(rSe,"A",{id:!0,class:!0,href:!0});var gDr=n(xh);$Y=s(gDr,"SPAN",{});var hDr=n($Y);f(Ly.$$.fragment,hDr),hDr.forEach(t),gDr.forEach(t),Yoo=i(rSe),IY=s(rSe,"SPAN",{});var uDr=n(IY);Koo=r(uDr,"AutoProcessor"),uDr.forEach(t),rSe.forEach(t),Jxe=i(c),Uo=s(c,"DIV",{class:!0});var zn=n(Uo);f(By.$$.fragment,zn),Zoo=i(zn),xy=s(zn,"P",{});var tSe=n(xy);ero=r(tSe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),MR=s(tSe,"A",{href:!0});var pDr=n(MR);oro=r(pDr,"AutoProcessor.from_pretrained()"),pDr.forEach(t),rro=r(tSe," class method."),tSe.forEach(t),tro=i(zn),ky=s(zn,"P",{});var aSe=n(ky);aro=r(aSe,"This class cannot be instantiated directly using "),jY=s(aSe,"CODE",{});var _Dr=n(jY);sro=r(_Dr,"__init__()"),_Dr.forEach(t),nro=r(aSe," (throws an error)."),aSe.forEach(t),lro=i(zn),je=s(zn,"DIV",{class:!0});var Dt=n(je);f(Ry.$$.fragment,Dt),iro=i(Dt),NY=s(Dt,"P",{});var bDr=n(NY);dro=r(bDr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),bDr.forEach(t),cro=i(Dt),Ji=s(Dt,"P",{});var qz=n(Ji);mro=r(qz,"The processor class to instantiate is selected based on the "),DY=s(qz,"CODE",{});var vDr=n(DY);fro=r(vDr,"model_type"),vDr.forEach(t),gro=r(qz,` property of the config object (either
passed as an argument or loaded from `),qY=s(qz,"CODE",{});var TDr=n(qY);hro=r(TDr,"pretrained_model_name_or_path"),TDr.forEach(t),uro=r(qz," if possible):"),qz.forEach(t),pro=i(Dt),xe=s(Dt,"UL",{});var Do=n(xe);kh=s(Do,"LI",{});var dEe=n(kh);OY=s(dEe,"STRONG",{});var FDr=n(OY);_ro=r(FDr,"clip"),FDr.forEach(t),bro=r(dEe," \u2014 "),ER=s(dEe,"A",{href:!0});var CDr=n(ER);vro=r(CDr,"CLIPProcessor"),CDr.forEach(t),Tro=r(dEe," (CLIP model)"),dEe.forEach(t),Fro=i(Do),Rh=s(Do,"LI",{});var cEe=n(Rh);GY=s(cEe,"STRONG",{});var MDr=n(GY);Cro=r(MDr,"layoutlmv2"),MDr.forEach(t),Mro=r(cEe," \u2014 "),yR=s(cEe,"A",{href:!0});var EDr=n(yR);Ero=r(EDr,"LayoutLMv2Processor"),EDr.forEach(t),yro=r(cEe," (LayoutLMv2 model)"),cEe.forEach(t),wro=i(Do),Sh=s(Do,"LI",{});var mEe=n(Sh);XY=s(mEe,"STRONG",{});var yDr=n(XY);Aro=r(yDr,"layoutxlm"),yDr.forEach(t),Lro=r(mEe," \u2014 "),wR=s(mEe,"A",{href:!0});var wDr=n(wR);Bro=r(wDr,"LayoutXLMProcessor"),wDr.forEach(t),xro=r(mEe," (LayoutXLM model)"),mEe.forEach(t),kro=i(Do),Ph=s(Do,"LI",{});var fEe=n(Ph);VY=s(fEe,"STRONG",{});var ADr=n(VY);Rro=r(ADr,"speech_to_text"),ADr.forEach(t),Sro=r(fEe," \u2014 "),AR=s(fEe,"A",{href:!0});var LDr=n(AR);Pro=r(LDr,"Speech2TextProcessor"),LDr.forEach(t),$ro=r(fEe," (Speech2Text model)"),fEe.forEach(t),Iro=i(Do),$h=s(Do,"LI",{});var gEe=n($h);zY=s(gEe,"STRONG",{});var BDr=n(zY);jro=r(BDr,"speech_to_text_2"),BDr.forEach(t),Nro=r(gEe," \u2014 "),LR=s(gEe,"A",{href:!0});var xDr=n(LR);Dro=r(xDr,"Speech2Text2Processor"),xDr.forEach(t),qro=r(gEe," (Speech2Text2 model)"),gEe.forEach(t),Oro=i(Do),Ih=s(Do,"LI",{});var hEe=n(Ih);WY=s(hEe,"STRONG",{});var kDr=n(WY);Gro=r(kDr,"trocr"),kDr.forEach(t),Xro=r(hEe," \u2014 "),BR=s(hEe,"A",{href:!0});var RDr=n(BR);Vro=r(RDr,"TrOCRProcessor"),RDr.forEach(t),zro=r(hEe," (TrOCR model)"),hEe.forEach(t),Wro=i(Do),jh=s(Do,"LI",{});var uEe=n(jh);QY=s(uEe,"STRONG",{});var SDr=n(QY);Qro=r(SDr,"vision-text-dual-encoder"),SDr.forEach(t),Hro=r(uEe," \u2014 "),xR=s(uEe,"A",{href:!0});var PDr=n(xR);Uro=r(PDr,"VisionTextDualEncoderProcessor"),PDr.forEach(t),Jro=r(uEe," (VisionTextDualEncoder model)"),uEe.forEach(t),Yro=i(Do),Nh=s(Do,"LI",{});var pEe=n(Nh);HY=s(pEe,"STRONG",{});var $Dr=n(HY);Kro=r($Dr,"wav2vec2"),$Dr.forEach(t),Zro=r(pEe," \u2014 "),kR=s(pEe,"A",{href:!0});var IDr=n(kR);eto=r(IDr,"Wav2Vec2Processor"),IDr.forEach(t),oto=r(pEe," (Wav2Vec2 model)"),pEe.forEach(t),Do.forEach(t),rto=i(Dt),f(Dh.$$.fragment,Dt),tto=i(Dt),UY=s(Dt,"P",{});var jDr=n(UY);ato=r(jDr,"Examples:"),jDr.forEach(t),sto=i(Dt),f(Sy.$$.fragment,Dt),Dt.forEach(t),nto=i(zn),qh=s(zn,"DIV",{class:!0});var sSe=n(qh);f(Py.$$.fragment,sSe),lto=i(sSe),JY=s(sSe,"P",{});var NDr=n(JY);ito=r(NDr,"Register a new processor for this class."),NDr.forEach(t),sSe.forEach(t),zn.forEach(t),Yxe=i(c),Yi=s(c,"H2",{class:!0});var nSe=n(Yi);Oh=s(nSe,"A",{id:!0,class:!0,href:!0});var DDr=n(Oh);YY=s(DDr,"SPAN",{});var qDr=n(YY);f($y.$$.fragment,qDr),qDr.forEach(t),DDr.forEach(t),dto=i(nSe),KY=s(nSe,"SPAN",{});var ODr=n(KY);cto=r(ODr,"AutoModel"),ODr.forEach(t),nSe.forEach(t),Kxe=i(c),Jo=s(c,"DIV",{class:!0});var Wn=n(Jo);f(Iy.$$.fragment,Wn),mto=i(Wn),Ki=s(Wn,"P",{});var Oz=n(Ki);fto=r(Oz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),ZY=s(Oz,"CODE",{});var GDr=n(ZY);gto=r(GDr,"from_pretrained()"),GDr.forEach(t),hto=r(Oz,"class method or the "),eK=s(Oz,"CODE",{});var XDr=n(eK);uto=r(XDr,"from_config()"),XDr.forEach(t),pto=r(Oz,`class
method.`),Oz.forEach(t),_to=i(Wn),jy=s(Wn,"P",{});var lSe=n(jy);bto=r(lSe,"This class cannot be instantiated directly using "),oK=s(lSe,"CODE",{});var VDr=n(oK);vto=r(VDr,"__init__()"),VDr.forEach(t),Tto=r(lSe," (throws an error)."),lSe.forEach(t),Fto=i(Wn),Vr=s(Wn,"DIV",{class:!0});var Qn=n(Vr);f(Ny.$$.fragment,Qn),Cto=i(Qn),rK=s(Qn,"P",{});var zDr=n(rK);Mto=r(zDr,"Instantiates one of the base model classes of the library from a configuration."),zDr.forEach(t),Eto=i(Qn),Zi=s(Qn,"P",{});var Gz=n(Zi);yto=r(Gz,`Note:
Loading a model from its configuration file does `),tK=s(Gz,"STRONG",{});var WDr=n(tK);wto=r(WDr,"not"),WDr.forEach(t),Ato=r(Gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),aK=s(Gz,"CODE",{});var QDr=n(aK);Lto=r(QDr,"from_pretrained()"),QDr.forEach(t),Bto=r(Gz,"to load the model weights."),Gz.forEach(t),xto=i(Qn),sK=s(Qn,"P",{});var HDr=n(sK);kto=r(HDr,"Examples:"),HDr.forEach(t),Rto=i(Qn),f(Dy.$$.fragment,Qn),Qn.forEach(t),Sto=i(Wn),Ne=s(Wn,"DIV",{class:!0});var qt=n(Ne);f(qy.$$.fragment,qt),Pto=i(qt),nK=s(qt,"P",{});var UDr=n(nK);$to=r(UDr,"Instantiate one of the base model classes of the library from a pretrained model."),UDr.forEach(t),Ito=i(qt),Va=s(qt,"P",{});var s5=n(Va);jto=r(s5,"The model class to instantiate is selected based on the "),lK=s(s5,"CODE",{});var JDr=n(lK);Nto=r(JDr,"model_type"),JDr.forEach(t),Dto=r(s5,` property of the config object (either
passed as an argument or loaded from `),iK=s(s5,"CODE",{});var YDr=n(iK);qto=r(YDr,"pretrained_model_name_or_path"),YDr.forEach(t),Oto=r(s5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dK=s(s5,"CODE",{});var KDr=n(dK);Gto=r(KDr,"pretrained_model_name_or_path"),KDr.forEach(t),Xto=r(s5,":"),s5.forEach(t),Vto=i(qt),F=s(qt,"UL",{});var C=n(F);Gh=s(C,"LI",{});var _Ee=n(Gh);cK=s(_Ee,"STRONG",{});var ZDr=n(cK);zto=r(ZDr,"albert"),ZDr.forEach(t),Wto=r(_Ee," \u2014 "),RR=s(_Ee,"A",{href:!0});var eqr=n(RR);Qto=r(eqr,"AlbertModel"),eqr.forEach(t),Hto=r(_Ee," (ALBERT model)"),_Ee.forEach(t),Uto=i(C),Xh=s(C,"LI",{});var bEe=n(Xh);mK=s(bEe,"STRONG",{});var oqr=n(mK);Jto=r(oqr,"bart"),oqr.forEach(t),Yto=r(bEe," \u2014 "),SR=s(bEe,"A",{href:!0});var rqr=n(SR);Kto=r(rqr,"BartModel"),rqr.forEach(t),Zto=r(bEe," (BART model)"),bEe.forEach(t),eao=i(C),Vh=s(C,"LI",{});var vEe=n(Vh);fK=s(vEe,"STRONG",{});var tqr=n(fK);oao=r(tqr,"beit"),tqr.forEach(t),rao=r(vEe," \u2014 "),PR=s(vEe,"A",{href:!0});var aqr=n(PR);tao=r(aqr,"BeitModel"),aqr.forEach(t),aao=r(vEe," (BEiT model)"),vEe.forEach(t),sao=i(C),zh=s(C,"LI",{});var TEe=n(zh);gK=s(TEe,"STRONG",{});var sqr=n(gK);nao=r(sqr,"bert"),sqr.forEach(t),lao=r(TEe," \u2014 "),$R=s(TEe,"A",{href:!0});var nqr=n($R);iao=r(nqr,"BertModel"),nqr.forEach(t),dao=r(TEe," (BERT model)"),TEe.forEach(t),cao=i(C),Wh=s(C,"LI",{});var FEe=n(Wh);hK=s(FEe,"STRONG",{});var lqr=n(hK);mao=r(lqr,"bert-generation"),lqr.forEach(t),fao=r(FEe," \u2014 "),IR=s(FEe,"A",{href:!0});var iqr=n(IR);gao=r(iqr,"BertGenerationEncoder"),iqr.forEach(t),hao=r(FEe," (Bert Generation model)"),FEe.forEach(t),uao=i(C),Qh=s(C,"LI",{});var CEe=n(Qh);uK=s(CEe,"STRONG",{});var dqr=n(uK);pao=r(dqr,"big_bird"),dqr.forEach(t),_ao=r(CEe," \u2014 "),jR=s(CEe,"A",{href:!0});var cqr=n(jR);bao=r(cqr,"BigBirdModel"),cqr.forEach(t),vao=r(CEe," (BigBird model)"),CEe.forEach(t),Tao=i(C),Hh=s(C,"LI",{});var MEe=n(Hh);pK=s(MEe,"STRONG",{});var mqr=n(pK);Fao=r(mqr,"bigbird_pegasus"),mqr.forEach(t),Cao=r(MEe," \u2014 "),NR=s(MEe,"A",{href:!0});var fqr=n(NR);Mao=r(fqr,"BigBirdPegasusModel"),fqr.forEach(t),Eao=r(MEe," (BigBirdPegasus model)"),MEe.forEach(t),yao=i(C),Uh=s(C,"LI",{});var EEe=n(Uh);_K=s(EEe,"STRONG",{});var gqr=n(_K);wao=r(gqr,"blenderbot"),gqr.forEach(t),Aao=r(EEe," \u2014 "),DR=s(EEe,"A",{href:!0});var hqr=n(DR);Lao=r(hqr,"BlenderbotModel"),hqr.forEach(t),Bao=r(EEe," (Blenderbot model)"),EEe.forEach(t),xao=i(C),Jh=s(C,"LI",{});var yEe=n(Jh);bK=s(yEe,"STRONG",{});var uqr=n(bK);kao=r(uqr,"blenderbot-small"),uqr.forEach(t),Rao=r(yEe," \u2014 "),qR=s(yEe,"A",{href:!0});var pqr=n(qR);Sao=r(pqr,"BlenderbotSmallModel"),pqr.forEach(t),Pao=r(yEe," (BlenderbotSmall model)"),yEe.forEach(t),$ao=i(C),Yh=s(C,"LI",{});var wEe=n(Yh);vK=s(wEe,"STRONG",{});var _qr=n(vK);Iao=r(_qr,"camembert"),_qr.forEach(t),jao=r(wEe," \u2014 "),OR=s(wEe,"A",{href:!0});var bqr=n(OR);Nao=r(bqr,"CamembertModel"),bqr.forEach(t),Dao=r(wEe," (CamemBERT model)"),wEe.forEach(t),qao=i(C),Kh=s(C,"LI",{});var AEe=n(Kh);TK=s(AEe,"STRONG",{});var vqr=n(TK);Oao=r(vqr,"canine"),vqr.forEach(t),Gao=r(AEe," \u2014 "),GR=s(AEe,"A",{href:!0});var Tqr=n(GR);Xao=r(Tqr,"CanineModel"),Tqr.forEach(t),Vao=r(AEe," (Canine model)"),AEe.forEach(t),zao=i(C),Zh=s(C,"LI",{});var LEe=n(Zh);FK=s(LEe,"STRONG",{});var Fqr=n(FK);Wao=r(Fqr,"clip"),Fqr.forEach(t),Qao=r(LEe," \u2014 "),XR=s(LEe,"A",{href:!0});var Cqr=n(XR);Hao=r(Cqr,"CLIPModel"),Cqr.forEach(t),Uao=r(LEe," (CLIP model)"),LEe.forEach(t),Jao=i(C),eu=s(C,"LI",{});var BEe=n(eu);CK=s(BEe,"STRONG",{});var Mqr=n(CK);Yao=r(Mqr,"convbert"),Mqr.forEach(t),Kao=r(BEe," \u2014 "),VR=s(BEe,"A",{href:!0});var Eqr=n(VR);Zao=r(Eqr,"ConvBertModel"),Eqr.forEach(t),eso=r(BEe," (ConvBERT model)"),BEe.forEach(t),oso=i(C),ou=s(C,"LI",{});var xEe=n(ou);MK=s(xEe,"STRONG",{});var yqr=n(MK);rso=r(yqr,"convnext"),yqr.forEach(t),tso=r(xEe," \u2014 "),zR=s(xEe,"A",{href:!0});var wqr=n(zR);aso=r(wqr,"ConvNextModel"),wqr.forEach(t),sso=r(xEe," (ConvNext model)"),xEe.forEach(t),nso=i(C),ru=s(C,"LI",{});var kEe=n(ru);EK=s(kEe,"STRONG",{});var Aqr=n(EK);lso=r(Aqr,"ctrl"),Aqr.forEach(t),iso=r(kEe," \u2014 "),WR=s(kEe,"A",{href:!0});var Lqr=n(WR);dso=r(Lqr,"CTRLModel"),Lqr.forEach(t),cso=r(kEe," (CTRL model)"),kEe.forEach(t),mso=i(C),tu=s(C,"LI",{});var REe=n(tu);yK=s(REe,"STRONG",{});var Bqr=n(yK);fso=r(Bqr,"data2vec-audio"),Bqr.forEach(t),gso=r(REe," \u2014 "),QR=s(REe,"A",{href:!0});var xqr=n(QR);hso=r(xqr,"Data2VecAudioModel"),xqr.forEach(t),uso=r(REe," (Data2VecAudio model)"),REe.forEach(t),pso=i(C),au=s(C,"LI",{});var SEe=n(au);wK=s(SEe,"STRONG",{});var kqr=n(wK);_so=r(kqr,"data2vec-text"),kqr.forEach(t),bso=r(SEe," \u2014 "),HR=s(SEe,"A",{href:!0});var Rqr=n(HR);vso=r(Rqr,"Data2VecTextModel"),Rqr.forEach(t),Tso=r(SEe," (Data2VecText model)"),SEe.forEach(t),Fso=i(C),su=s(C,"LI",{});var PEe=n(su);AK=s(PEe,"STRONG",{});var Sqr=n(AK);Cso=r(Sqr,"deberta"),Sqr.forEach(t),Mso=r(PEe," \u2014 "),UR=s(PEe,"A",{href:!0});var Pqr=n(UR);Eso=r(Pqr,"DebertaModel"),Pqr.forEach(t),yso=r(PEe," (DeBERTa model)"),PEe.forEach(t),wso=i(C),nu=s(C,"LI",{});var $Ee=n(nu);LK=s($Ee,"STRONG",{});var $qr=n(LK);Aso=r($qr,"deberta-v2"),$qr.forEach(t),Lso=r($Ee," \u2014 "),JR=s($Ee,"A",{href:!0});var Iqr=n(JR);Bso=r(Iqr,"DebertaV2Model"),Iqr.forEach(t),xso=r($Ee," (DeBERTa-v2 model)"),$Ee.forEach(t),kso=i(C),lu=s(C,"LI",{});var IEe=n(lu);BK=s(IEe,"STRONG",{});var jqr=n(BK);Rso=r(jqr,"deit"),jqr.forEach(t),Sso=r(IEe," \u2014 "),YR=s(IEe,"A",{href:!0});var Nqr=n(YR);Pso=r(Nqr,"DeiTModel"),Nqr.forEach(t),$so=r(IEe," (DeiT model)"),IEe.forEach(t),Iso=i(C),iu=s(C,"LI",{});var jEe=n(iu);xK=s(jEe,"STRONG",{});var Dqr=n(xK);jso=r(Dqr,"detr"),Dqr.forEach(t),Nso=r(jEe," \u2014 "),KR=s(jEe,"A",{href:!0});var qqr=n(KR);Dso=r(qqr,"DetrModel"),qqr.forEach(t),qso=r(jEe," (DETR model)"),jEe.forEach(t),Oso=i(C),du=s(C,"LI",{});var NEe=n(du);kK=s(NEe,"STRONG",{});var Oqr=n(kK);Gso=r(Oqr,"distilbert"),Oqr.forEach(t),Xso=r(NEe," \u2014 "),ZR=s(NEe,"A",{href:!0});var Gqr=n(ZR);Vso=r(Gqr,"DistilBertModel"),Gqr.forEach(t),zso=r(NEe," (DistilBERT model)"),NEe.forEach(t),Wso=i(C),cu=s(C,"LI",{});var DEe=n(cu);RK=s(DEe,"STRONG",{});var Xqr=n(RK);Qso=r(Xqr,"dpr"),Xqr.forEach(t),Hso=r(DEe," \u2014 "),eS=s(DEe,"A",{href:!0});var Vqr=n(eS);Uso=r(Vqr,"DPRQuestionEncoder"),Vqr.forEach(t),Jso=r(DEe," (DPR model)"),DEe.forEach(t),Yso=i(C),mu=s(C,"LI",{});var qEe=n(mu);SK=s(qEe,"STRONG",{});var zqr=n(SK);Kso=r(zqr,"electra"),zqr.forEach(t),Zso=r(qEe," \u2014 "),oS=s(qEe,"A",{href:!0});var Wqr=n(oS);eno=r(Wqr,"ElectraModel"),Wqr.forEach(t),ono=r(qEe," (ELECTRA model)"),qEe.forEach(t),rno=i(C),fu=s(C,"LI",{});var OEe=n(fu);PK=s(OEe,"STRONG",{});var Qqr=n(PK);tno=r(Qqr,"flaubert"),Qqr.forEach(t),ano=r(OEe," \u2014 "),rS=s(OEe,"A",{href:!0});var Hqr=n(rS);sno=r(Hqr,"FlaubertModel"),Hqr.forEach(t),nno=r(OEe," (FlauBERT model)"),OEe.forEach(t),lno=i(C),gu=s(C,"LI",{});var GEe=n(gu);$K=s(GEe,"STRONG",{});var Uqr=n($K);ino=r(Uqr,"fnet"),Uqr.forEach(t),dno=r(GEe," \u2014 "),tS=s(GEe,"A",{href:!0});var Jqr=n(tS);cno=r(Jqr,"FNetModel"),Jqr.forEach(t),mno=r(GEe," (FNet model)"),GEe.forEach(t),fno=i(C),hu=s(C,"LI",{});var XEe=n(hu);IK=s(XEe,"STRONG",{});var Yqr=n(IK);gno=r(Yqr,"fsmt"),Yqr.forEach(t),hno=r(XEe," \u2014 "),aS=s(XEe,"A",{href:!0});var Kqr=n(aS);uno=r(Kqr,"FSMTModel"),Kqr.forEach(t),pno=r(XEe," (FairSeq Machine-Translation model)"),XEe.forEach(t),_no=i(C),Dn=s(C,"LI",{});var z8=n(Dn);jK=s(z8,"STRONG",{});var Zqr=n(jK);bno=r(Zqr,"funnel"),Zqr.forEach(t),vno=r(z8," \u2014 "),sS=s(z8,"A",{href:!0});var eOr=n(sS);Tno=r(eOr,"FunnelModel"),eOr.forEach(t),Fno=r(z8," or "),nS=s(z8,"A",{href:!0});var oOr=n(nS);Cno=r(oOr,"FunnelBaseModel"),oOr.forEach(t),Mno=r(z8," (Funnel Transformer model)"),z8.forEach(t),Eno=i(C),uu=s(C,"LI",{});var VEe=n(uu);NK=s(VEe,"STRONG",{});var rOr=n(NK);yno=r(rOr,"gpt2"),rOr.forEach(t),wno=r(VEe," \u2014 "),lS=s(VEe,"A",{href:!0});var tOr=n(lS);Ano=r(tOr,"GPT2Model"),tOr.forEach(t),Lno=r(VEe," (OpenAI GPT-2 model)"),VEe.forEach(t),Bno=i(C),pu=s(C,"LI",{});var zEe=n(pu);DK=s(zEe,"STRONG",{});var aOr=n(DK);xno=r(aOr,"gpt_neo"),aOr.forEach(t),kno=r(zEe," \u2014 "),iS=s(zEe,"A",{href:!0});var sOr=n(iS);Rno=r(sOr,"GPTNeoModel"),sOr.forEach(t),Sno=r(zEe," (GPT Neo model)"),zEe.forEach(t),Pno=i(C),_u=s(C,"LI",{});var WEe=n(_u);qK=s(WEe,"STRONG",{});var nOr=n(qK);$no=r(nOr,"gptj"),nOr.forEach(t),Ino=r(WEe," \u2014 "),dS=s(WEe,"A",{href:!0});var lOr=n(dS);jno=r(lOr,"GPTJModel"),lOr.forEach(t),Nno=r(WEe," (GPT-J model)"),WEe.forEach(t),Dno=i(C),bu=s(C,"LI",{});var QEe=n(bu);OK=s(QEe,"STRONG",{});var iOr=n(OK);qno=r(iOr,"hubert"),iOr.forEach(t),Ono=r(QEe," \u2014 "),cS=s(QEe,"A",{href:!0});var dOr=n(cS);Gno=r(dOr,"HubertModel"),dOr.forEach(t),Xno=r(QEe," (Hubert model)"),QEe.forEach(t),Vno=i(C),vu=s(C,"LI",{});var HEe=n(vu);GK=s(HEe,"STRONG",{});var cOr=n(GK);zno=r(cOr,"ibert"),cOr.forEach(t),Wno=r(HEe," \u2014 "),mS=s(HEe,"A",{href:!0});var mOr=n(mS);Qno=r(mOr,"IBertModel"),mOr.forEach(t),Hno=r(HEe," (I-BERT model)"),HEe.forEach(t),Uno=i(C),Tu=s(C,"LI",{});var UEe=n(Tu);XK=s(UEe,"STRONG",{});var fOr=n(XK);Jno=r(fOr,"imagegpt"),fOr.forEach(t),Yno=r(UEe," \u2014 "),fS=s(UEe,"A",{href:!0});var gOr=n(fS);Kno=r(gOr,"ImageGPTModel"),gOr.forEach(t),Zno=r(UEe," (ImageGPT model)"),UEe.forEach(t),elo=i(C),Fu=s(C,"LI",{});var JEe=n(Fu);VK=s(JEe,"STRONG",{});var hOr=n(VK);olo=r(hOr,"layoutlm"),hOr.forEach(t),rlo=r(JEe," \u2014 "),gS=s(JEe,"A",{href:!0});var uOr=n(gS);tlo=r(uOr,"LayoutLMModel"),uOr.forEach(t),alo=r(JEe," (LayoutLM model)"),JEe.forEach(t),slo=i(C),Cu=s(C,"LI",{});var YEe=n(Cu);zK=s(YEe,"STRONG",{});var pOr=n(zK);nlo=r(pOr,"layoutlmv2"),pOr.forEach(t),llo=r(YEe," \u2014 "),hS=s(YEe,"A",{href:!0});var _Or=n(hS);ilo=r(_Or,"LayoutLMv2Model"),_Or.forEach(t),dlo=r(YEe," (LayoutLMv2 model)"),YEe.forEach(t),clo=i(C),Mu=s(C,"LI",{});var KEe=n(Mu);WK=s(KEe,"STRONG",{});var bOr=n(WK);mlo=r(bOr,"led"),bOr.forEach(t),flo=r(KEe," \u2014 "),uS=s(KEe,"A",{href:!0});var vOr=n(uS);glo=r(vOr,"LEDModel"),vOr.forEach(t),hlo=r(KEe," (LED model)"),KEe.forEach(t),ulo=i(C),Eu=s(C,"LI",{});var ZEe=n(Eu);QK=s(ZEe,"STRONG",{});var TOr=n(QK);plo=r(TOr,"longformer"),TOr.forEach(t),_lo=r(ZEe," \u2014 "),pS=s(ZEe,"A",{href:!0});var FOr=n(pS);blo=r(FOr,"LongformerModel"),FOr.forEach(t),vlo=r(ZEe," (Longformer model)"),ZEe.forEach(t),Tlo=i(C),yu=s(C,"LI",{});var e3e=n(yu);HK=s(e3e,"STRONG",{});var COr=n(HK);Flo=r(COr,"luke"),COr.forEach(t),Clo=r(e3e," \u2014 "),_S=s(e3e,"A",{href:!0});var MOr=n(_S);Mlo=r(MOr,"LukeModel"),MOr.forEach(t),Elo=r(e3e," (LUKE model)"),e3e.forEach(t),ylo=i(C),wu=s(C,"LI",{});var o3e=n(wu);UK=s(o3e,"STRONG",{});var EOr=n(UK);wlo=r(EOr,"lxmert"),EOr.forEach(t),Alo=r(o3e," \u2014 "),bS=s(o3e,"A",{href:!0});var yOr=n(bS);Llo=r(yOr,"LxmertModel"),yOr.forEach(t),Blo=r(o3e," (LXMERT model)"),o3e.forEach(t),xlo=i(C),Au=s(C,"LI",{});var r3e=n(Au);JK=s(r3e,"STRONG",{});var wOr=n(JK);klo=r(wOr,"m2m_100"),wOr.forEach(t),Rlo=r(r3e," \u2014 "),vS=s(r3e,"A",{href:!0});var AOr=n(vS);Slo=r(AOr,"M2M100Model"),AOr.forEach(t),Plo=r(r3e," (M2M100 model)"),r3e.forEach(t),$lo=i(C),Lu=s(C,"LI",{});var t3e=n(Lu);YK=s(t3e,"STRONG",{});var LOr=n(YK);Ilo=r(LOr,"marian"),LOr.forEach(t),jlo=r(t3e," \u2014 "),TS=s(t3e,"A",{href:!0});var BOr=n(TS);Nlo=r(BOr,"MarianModel"),BOr.forEach(t),Dlo=r(t3e," (Marian model)"),t3e.forEach(t),qlo=i(C),Bu=s(C,"LI",{});var a3e=n(Bu);KK=s(a3e,"STRONG",{});var xOr=n(KK);Olo=r(xOr,"maskformer"),xOr.forEach(t),Glo=r(a3e," \u2014 "),FS=s(a3e,"A",{href:!0});var kOr=n(FS);Xlo=r(kOr,"MaskFormerModel"),kOr.forEach(t),Vlo=r(a3e," (MaskFormer model)"),a3e.forEach(t),zlo=i(C),xu=s(C,"LI",{});var s3e=n(xu);ZK=s(s3e,"STRONG",{});var ROr=n(ZK);Wlo=r(ROr,"mbart"),ROr.forEach(t),Qlo=r(s3e," \u2014 "),CS=s(s3e,"A",{href:!0});var SOr=n(CS);Hlo=r(SOr,"MBartModel"),SOr.forEach(t),Ulo=r(s3e," (mBART model)"),s3e.forEach(t),Jlo=i(C),ku=s(C,"LI",{});var n3e=n(ku);eZ=s(n3e,"STRONG",{});var POr=n(eZ);Ylo=r(POr,"megatron-bert"),POr.forEach(t),Klo=r(n3e," \u2014 "),MS=s(n3e,"A",{href:!0});var $Or=n(MS);Zlo=r($Or,"MegatronBertModel"),$Or.forEach(t),eio=r(n3e," (MegatronBert model)"),n3e.forEach(t),oio=i(C),Ru=s(C,"LI",{});var l3e=n(Ru);oZ=s(l3e,"STRONG",{});var IOr=n(oZ);rio=r(IOr,"mobilebert"),IOr.forEach(t),tio=r(l3e," \u2014 "),ES=s(l3e,"A",{href:!0});var jOr=n(ES);aio=r(jOr,"MobileBertModel"),jOr.forEach(t),sio=r(l3e," (MobileBERT model)"),l3e.forEach(t),nio=i(C),Su=s(C,"LI",{});var i3e=n(Su);rZ=s(i3e,"STRONG",{});var NOr=n(rZ);lio=r(NOr,"mpnet"),NOr.forEach(t),iio=r(i3e," \u2014 "),yS=s(i3e,"A",{href:!0});var DOr=n(yS);dio=r(DOr,"MPNetModel"),DOr.forEach(t),cio=r(i3e," (MPNet model)"),i3e.forEach(t),mio=i(C),Pu=s(C,"LI",{});var d3e=n(Pu);tZ=s(d3e,"STRONG",{});var qOr=n(tZ);fio=r(qOr,"mt5"),qOr.forEach(t),gio=r(d3e," \u2014 "),wS=s(d3e,"A",{href:!0});var OOr=n(wS);hio=r(OOr,"MT5Model"),OOr.forEach(t),uio=r(d3e," (mT5 model)"),d3e.forEach(t),pio=i(C),$u=s(C,"LI",{});var c3e=n($u);aZ=s(c3e,"STRONG",{});var GOr=n(aZ);_io=r(GOr,"nystromformer"),GOr.forEach(t),bio=r(c3e," \u2014 "),AS=s(c3e,"A",{href:!0});var XOr=n(AS);vio=r(XOr,"NystromformerModel"),XOr.forEach(t),Tio=r(c3e," (Nystromformer model)"),c3e.forEach(t),Fio=i(C),Iu=s(C,"LI",{});var m3e=n(Iu);sZ=s(m3e,"STRONG",{});var VOr=n(sZ);Cio=r(VOr,"openai-gpt"),VOr.forEach(t),Mio=r(m3e," \u2014 "),LS=s(m3e,"A",{href:!0});var zOr=n(LS);Eio=r(zOr,"OpenAIGPTModel"),zOr.forEach(t),yio=r(m3e," (OpenAI GPT model)"),m3e.forEach(t),wio=i(C),ju=s(C,"LI",{});var f3e=n(ju);nZ=s(f3e,"STRONG",{});var WOr=n(nZ);Aio=r(WOr,"pegasus"),WOr.forEach(t),Lio=r(f3e," \u2014 "),BS=s(f3e,"A",{href:!0});var QOr=n(BS);Bio=r(QOr,"PegasusModel"),QOr.forEach(t),xio=r(f3e," (Pegasus model)"),f3e.forEach(t),kio=i(C),Nu=s(C,"LI",{});var g3e=n(Nu);lZ=s(g3e,"STRONG",{});var HOr=n(lZ);Rio=r(HOr,"perceiver"),HOr.forEach(t),Sio=r(g3e," \u2014 "),xS=s(g3e,"A",{href:!0});var UOr=n(xS);Pio=r(UOr,"PerceiverModel"),UOr.forEach(t),$io=r(g3e," (Perceiver model)"),g3e.forEach(t),Iio=i(C),Du=s(C,"LI",{});var h3e=n(Du);iZ=s(h3e,"STRONG",{});var JOr=n(iZ);jio=r(JOr,"plbart"),JOr.forEach(t),Nio=r(h3e," \u2014 "),kS=s(h3e,"A",{href:!0});var YOr=n(kS);Dio=r(YOr,"PLBartModel"),YOr.forEach(t),qio=r(h3e," (PLBart model)"),h3e.forEach(t),Oio=i(C),qu=s(C,"LI",{});var u3e=n(qu);dZ=s(u3e,"STRONG",{});var KOr=n(dZ);Gio=r(KOr,"poolformer"),KOr.forEach(t),Xio=r(u3e," \u2014 "),RS=s(u3e,"A",{href:!0});var ZOr=n(RS);Vio=r(ZOr,"PoolFormerModel"),ZOr.forEach(t),zio=r(u3e," (PoolFormer model)"),u3e.forEach(t),Wio=i(C),Ou=s(C,"LI",{});var p3e=n(Ou);cZ=s(p3e,"STRONG",{});var eGr=n(cZ);Qio=r(eGr,"prophetnet"),eGr.forEach(t),Hio=r(p3e," \u2014 "),SS=s(p3e,"A",{href:!0});var oGr=n(SS);Uio=r(oGr,"ProphetNetModel"),oGr.forEach(t),Jio=r(p3e," (ProphetNet model)"),p3e.forEach(t),Yio=i(C),Gu=s(C,"LI",{});var _3e=n(Gu);mZ=s(_3e,"STRONG",{});var rGr=n(mZ);Kio=r(rGr,"qdqbert"),rGr.forEach(t),Zio=r(_3e," \u2014 "),PS=s(_3e,"A",{href:!0});var tGr=n(PS);edo=r(tGr,"QDQBertModel"),tGr.forEach(t),odo=r(_3e," (QDQBert model)"),_3e.forEach(t),rdo=i(C),Xu=s(C,"LI",{});var b3e=n(Xu);fZ=s(b3e,"STRONG",{});var aGr=n(fZ);tdo=r(aGr,"reformer"),aGr.forEach(t),ado=r(b3e," \u2014 "),$S=s(b3e,"A",{href:!0});var sGr=n($S);sdo=r(sGr,"ReformerModel"),sGr.forEach(t),ndo=r(b3e," (Reformer model)"),b3e.forEach(t),ldo=i(C),Vu=s(C,"LI",{});var v3e=n(Vu);gZ=s(v3e,"STRONG",{});var nGr=n(gZ);ido=r(nGr,"rembert"),nGr.forEach(t),ddo=r(v3e," \u2014 "),IS=s(v3e,"A",{href:!0});var lGr=n(IS);cdo=r(lGr,"RemBertModel"),lGr.forEach(t),mdo=r(v3e," (RemBERT model)"),v3e.forEach(t),fdo=i(C),zu=s(C,"LI",{});var T3e=n(zu);hZ=s(T3e,"STRONG",{});var iGr=n(hZ);gdo=r(iGr,"resnet"),iGr.forEach(t),hdo=r(T3e," \u2014 "),jS=s(T3e,"A",{href:!0});var dGr=n(jS);udo=r(dGr,"ResNetModel"),dGr.forEach(t),pdo=r(T3e," (ResNet model)"),T3e.forEach(t),_do=i(C),Wu=s(C,"LI",{});var F3e=n(Wu);uZ=s(F3e,"STRONG",{});var cGr=n(uZ);bdo=r(cGr,"retribert"),cGr.forEach(t),vdo=r(F3e," \u2014 "),NS=s(F3e,"A",{href:!0});var mGr=n(NS);Tdo=r(mGr,"RetriBertModel"),mGr.forEach(t),Fdo=r(F3e," (RetriBERT model)"),F3e.forEach(t),Cdo=i(C),Qu=s(C,"LI",{});var C3e=n(Qu);pZ=s(C3e,"STRONG",{});var fGr=n(pZ);Mdo=r(fGr,"roberta"),fGr.forEach(t),Edo=r(C3e," \u2014 "),DS=s(C3e,"A",{href:!0});var gGr=n(DS);ydo=r(gGr,"RobertaModel"),gGr.forEach(t),wdo=r(C3e," (RoBERTa model)"),C3e.forEach(t),Ado=i(C),Hu=s(C,"LI",{});var M3e=n(Hu);_Z=s(M3e,"STRONG",{});var hGr=n(_Z);Ldo=r(hGr,"roformer"),hGr.forEach(t),Bdo=r(M3e," \u2014 "),qS=s(M3e,"A",{href:!0});var uGr=n(qS);xdo=r(uGr,"RoFormerModel"),uGr.forEach(t),kdo=r(M3e," (RoFormer model)"),M3e.forEach(t),Rdo=i(C),Uu=s(C,"LI",{});var E3e=n(Uu);bZ=s(E3e,"STRONG",{});var pGr=n(bZ);Sdo=r(pGr,"segformer"),pGr.forEach(t),Pdo=r(E3e," \u2014 "),OS=s(E3e,"A",{href:!0});var _Gr=n(OS);$do=r(_Gr,"SegformerModel"),_Gr.forEach(t),Ido=r(E3e," (SegFormer model)"),E3e.forEach(t),jdo=i(C),Ju=s(C,"LI",{});var y3e=n(Ju);vZ=s(y3e,"STRONG",{});var bGr=n(vZ);Ndo=r(bGr,"sew"),bGr.forEach(t),Ddo=r(y3e," \u2014 "),GS=s(y3e,"A",{href:!0});var vGr=n(GS);qdo=r(vGr,"SEWModel"),vGr.forEach(t),Odo=r(y3e," (SEW model)"),y3e.forEach(t),Gdo=i(C),Yu=s(C,"LI",{});var w3e=n(Yu);TZ=s(w3e,"STRONG",{});var TGr=n(TZ);Xdo=r(TGr,"sew-d"),TGr.forEach(t),Vdo=r(w3e," \u2014 "),XS=s(w3e,"A",{href:!0});var FGr=n(XS);zdo=r(FGr,"SEWDModel"),FGr.forEach(t),Wdo=r(w3e," (SEW-D model)"),w3e.forEach(t),Qdo=i(C),Ku=s(C,"LI",{});var A3e=n(Ku);FZ=s(A3e,"STRONG",{});var CGr=n(FZ);Hdo=r(CGr,"speech_to_text"),CGr.forEach(t),Udo=r(A3e," \u2014 "),VS=s(A3e,"A",{href:!0});var MGr=n(VS);Jdo=r(MGr,"Speech2TextModel"),MGr.forEach(t),Ydo=r(A3e," (Speech2Text model)"),A3e.forEach(t),Kdo=i(C),Zu=s(C,"LI",{});var L3e=n(Zu);CZ=s(L3e,"STRONG",{});var EGr=n(CZ);Zdo=r(EGr,"splinter"),EGr.forEach(t),eco=r(L3e," \u2014 "),zS=s(L3e,"A",{href:!0});var yGr=n(zS);oco=r(yGr,"SplinterModel"),yGr.forEach(t),rco=r(L3e," (Splinter model)"),L3e.forEach(t),tco=i(C),ep=s(C,"LI",{});var B3e=n(ep);MZ=s(B3e,"STRONG",{});var wGr=n(MZ);aco=r(wGr,"squeezebert"),wGr.forEach(t),sco=r(B3e," \u2014 "),WS=s(B3e,"A",{href:!0});var AGr=n(WS);nco=r(AGr,"SqueezeBertModel"),AGr.forEach(t),lco=r(B3e," (SqueezeBERT model)"),B3e.forEach(t),ico=i(C),op=s(C,"LI",{});var x3e=n(op);EZ=s(x3e,"STRONG",{});var LGr=n(EZ);dco=r(LGr,"swin"),LGr.forEach(t),cco=r(x3e," \u2014 "),QS=s(x3e,"A",{href:!0});var BGr=n(QS);mco=r(BGr,"SwinModel"),BGr.forEach(t),fco=r(x3e," (Swin model)"),x3e.forEach(t),gco=i(C),rp=s(C,"LI",{});var k3e=n(rp);yZ=s(k3e,"STRONG",{});var xGr=n(yZ);hco=r(xGr,"t5"),xGr.forEach(t),uco=r(k3e," \u2014 "),HS=s(k3e,"A",{href:!0});var kGr=n(HS);pco=r(kGr,"T5Model"),kGr.forEach(t),_co=r(k3e," (T5 model)"),k3e.forEach(t),bco=i(C),tp=s(C,"LI",{});var R3e=n(tp);wZ=s(R3e,"STRONG",{});var RGr=n(wZ);vco=r(RGr,"tapas"),RGr.forEach(t),Tco=r(R3e," \u2014 "),US=s(R3e,"A",{href:!0});var SGr=n(US);Fco=r(SGr,"TapasModel"),SGr.forEach(t),Cco=r(R3e," (TAPAS model)"),R3e.forEach(t),Mco=i(C),ap=s(C,"LI",{});var S3e=n(ap);AZ=s(S3e,"STRONG",{});var PGr=n(AZ);Eco=r(PGr,"transfo-xl"),PGr.forEach(t),yco=r(S3e," \u2014 "),JS=s(S3e,"A",{href:!0});var $Gr=n(JS);wco=r($Gr,"TransfoXLModel"),$Gr.forEach(t),Aco=r(S3e," (Transformer-XL model)"),S3e.forEach(t),Lco=i(C),sp=s(C,"LI",{});var P3e=n(sp);LZ=s(P3e,"STRONG",{});var IGr=n(LZ);Bco=r(IGr,"unispeech"),IGr.forEach(t),xco=r(P3e," \u2014 "),YS=s(P3e,"A",{href:!0});var jGr=n(YS);kco=r(jGr,"UniSpeechModel"),jGr.forEach(t),Rco=r(P3e," (UniSpeech model)"),P3e.forEach(t),Sco=i(C),np=s(C,"LI",{});var $3e=n(np);BZ=s($3e,"STRONG",{});var NGr=n(BZ);Pco=r(NGr,"unispeech-sat"),NGr.forEach(t),$co=r($3e," \u2014 "),KS=s($3e,"A",{href:!0});var DGr=n(KS);Ico=r(DGr,"UniSpeechSatModel"),DGr.forEach(t),jco=r($3e," (UniSpeechSat model)"),$3e.forEach(t),Nco=i(C),lp=s(C,"LI",{});var I3e=n(lp);xZ=s(I3e,"STRONG",{});var qGr=n(xZ);Dco=r(qGr,"van"),qGr.forEach(t),qco=r(I3e," \u2014 "),ZS=s(I3e,"A",{href:!0});var OGr=n(ZS);Oco=r(OGr,"VanModel"),OGr.forEach(t),Gco=r(I3e," (VAN model)"),I3e.forEach(t),Xco=i(C),ip=s(C,"LI",{});var j3e=n(ip);kZ=s(j3e,"STRONG",{});var GGr=n(kZ);Vco=r(GGr,"vilt"),GGr.forEach(t),zco=r(j3e," \u2014 "),eP=s(j3e,"A",{href:!0});var XGr=n(eP);Wco=r(XGr,"ViltModel"),XGr.forEach(t),Qco=r(j3e," (ViLT model)"),j3e.forEach(t),Hco=i(C),dp=s(C,"LI",{});var N3e=n(dp);RZ=s(N3e,"STRONG",{});var VGr=n(RZ);Uco=r(VGr,"vision-text-dual-encoder"),VGr.forEach(t),Jco=r(N3e," \u2014 "),oP=s(N3e,"A",{href:!0});var zGr=n(oP);Yco=r(zGr,"VisionTextDualEncoderModel"),zGr.forEach(t),Kco=r(N3e," (VisionTextDualEncoder model)"),N3e.forEach(t),Zco=i(C),cp=s(C,"LI",{});var D3e=n(cp);SZ=s(D3e,"STRONG",{});var WGr=n(SZ);emo=r(WGr,"visual_bert"),WGr.forEach(t),omo=r(D3e," \u2014 "),rP=s(D3e,"A",{href:!0});var QGr=n(rP);rmo=r(QGr,"VisualBertModel"),QGr.forEach(t),tmo=r(D3e," (VisualBert model)"),D3e.forEach(t),amo=i(C),mp=s(C,"LI",{});var q3e=n(mp);PZ=s(q3e,"STRONG",{});var HGr=n(PZ);smo=r(HGr,"vit"),HGr.forEach(t),nmo=r(q3e," \u2014 "),tP=s(q3e,"A",{href:!0});var UGr=n(tP);lmo=r(UGr,"ViTModel"),UGr.forEach(t),imo=r(q3e," (ViT model)"),q3e.forEach(t),dmo=i(C),fp=s(C,"LI",{});var O3e=n(fp);$Z=s(O3e,"STRONG",{});var JGr=n($Z);cmo=r(JGr,"vit_mae"),JGr.forEach(t),mmo=r(O3e," \u2014 "),aP=s(O3e,"A",{href:!0});var YGr=n(aP);fmo=r(YGr,"ViTMAEModel"),YGr.forEach(t),gmo=r(O3e," (ViTMAE model)"),O3e.forEach(t),hmo=i(C),gp=s(C,"LI",{});var G3e=n(gp);IZ=s(G3e,"STRONG",{});var KGr=n(IZ);umo=r(KGr,"wav2vec2"),KGr.forEach(t),pmo=r(G3e," \u2014 "),sP=s(G3e,"A",{href:!0});var ZGr=n(sP);_mo=r(ZGr,"Wav2Vec2Model"),ZGr.forEach(t),bmo=r(G3e," (Wav2Vec2 model)"),G3e.forEach(t),vmo=i(C),hp=s(C,"LI",{});var X3e=n(hp);jZ=s(X3e,"STRONG",{});var eXr=n(jZ);Tmo=r(eXr,"wavlm"),eXr.forEach(t),Fmo=r(X3e," \u2014 "),nP=s(X3e,"A",{href:!0});var oXr=n(nP);Cmo=r(oXr,"WavLMModel"),oXr.forEach(t),Mmo=r(X3e," (WavLM model)"),X3e.forEach(t),Emo=i(C),up=s(C,"LI",{});var V3e=n(up);NZ=s(V3e,"STRONG",{});var rXr=n(NZ);ymo=r(rXr,"xglm"),rXr.forEach(t),wmo=r(V3e," \u2014 "),lP=s(V3e,"A",{href:!0});var tXr=n(lP);Amo=r(tXr,"XGLMModel"),tXr.forEach(t),Lmo=r(V3e," (XGLM model)"),V3e.forEach(t),Bmo=i(C),pp=s(C,"LI",{});var z3e=n(pp);DZ=s(z3e,"STRONG",{});var aXr=n(DZ);xmo=r(aXr,"xlm"),aXr.forEach(t),kmo=r(z3e," \u2014 "),iP=s(z3e,"A",{href:!0});var sXr=n(iP);Rmo=r(sXr,"XLMModel"),sXr.forEach(t),Smo=r(z3e," (XLM model)"),z3e.forEach(t),Pmo=i(C),_p=s(C,"LI",{});var W3e=n(_p);qZ=s(W3e,"STRONG",{});var nXr=n(qZ);$mo=r(nXr,"xlm-prophetnet"),nXr.forEach(t),Imo=r(W3e," \u2014 "),dP=s(W3e,"A",{href:!0});var lXr=n(dP);jmo=r(lXr,"XLMProphetNetModel"),lXr.forEach(t),Nmo=r(W3e," (XLMProphetNet model)"),W3e.forEach(t),Dmo=i(C),bp=s(C,"LI",{});var Q3e=n(bp);OZ=s(Q3e,"STRONG",{});var iXr=n(OZ);qmo=r(iXr,"xlm-roberta"),iXr.forEach(t),Omo=r(Q3e," \u2014 "),cP=s(Q3e,"A",{href:!0});var dXr=n(cP);Gmo=r(dXr,"XLMRobertaModel"),dXr.forEach(t),Xmo=r(Q3e," (XLM-RoBERTa model)"),Q3e.forEach(t),Vmo=i(C),vp=s(C,"LI",{});var H3e=n(vp);GZ=s(H3e,"STRONG",{});var cXr=n(GZ);zmo=r(cXr,"xlm-roberta-xl"),cXr.forEach(t),Wmo=r(H3e," \u2014 "),mP=s(H3e,"A",{href:!0});var mXr=n(mP);Qmo=r(mXr,"XLMRobertaXLModel"),mXr.forEach(t),Hmo=r(H3e," (XLM-RoBERTa-XL model)"),H3e.forEach(t),Umo=i(C),Tp=s(C,"LI",{});var U3e=n(Tp);XZ=s(U3e,"STRONG",{});var fXr=n(XZ);Jmo=r(fXr,"xlnet"),fXr.forEach(t),Ymo=r(U3e," \u2014 "),fP=s(U3e,"A",{href:!0});var gXr=n(fP);Kmo=r(gXr,"XLNetModel"),gXr.forEach(t),Zmo=r(U3e," (XLNet model)"),U3e.forEach(t),efo=i(C),Fp=s(C,"LI",{});var J3e=n(Fp);VZ=s(J3e,"STRONG",{});var hXr=n(VZ);ofo=r(hXr,"yoso"),hXr.forEach(t),rfo=r(J3e," \u2014 "),gP=s(J3e,"A",{href:!0});var uXr=n(gP);tfo=r(uXr,"YosoModel"),uXr.forEach(t),afo=r(J3e," (YOSO model)"),J3e.forEach(t),C.forEach(t),sfo=i(qt),Cp=s(qt,"P",{});var Y3e=n(Cp);nfo=r(Y3e,"The model is set in evaluation mode by default using "),zZ=s(Y3e,"CODE",{});var pXr=n(zZ);lfo=r(pXr,"model.eval()"),pXr.forEach(t),ifo=r(Y3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),WZ=s(Y3e,"CODE",{});var _Xr=n(WZ);dfo=r(_Xr,"model.train()"),_Xr.forEach(t),Y3e.forEach(t),cfo=i(qt),QZ=s(qt,"P",{});var bXr=n(QZ);mfo=r(bXr,"Examples:"),bXr.forEach(t),ffo=i(qt),f(Oy.$$.fragment,qt),qt.forEach(t),Wn.forEach(t),Zxe=i(c),ed=s(c,"H2",{class:!0});var iSe=n(ed);Mp=s(iSe,"A",{id:!0,class:!0,href:!0});var vXr=n(Mp);HZ=s(vXr,"SPAN",{});var TXr=n(HZ);f(Gy.$$.fragment,TXr),TXr.forEach(t),vXr.forEach(t),gfo=i(iSe),UZ=s(iSe,"SPAN",{});var FXr=n(UZ);hfo=r(FXr,"AutoModelForPreTraining"),FXr.forEach(t),iSe.forEach(t),eke=i(c),Yo=s(c,"DIV",{class:!0});var Hn=n(Yo);f(Xy.$$.fragment,Hn),ufo=i(Hn),od=s(Hn,"P",{});var Xz=n(od);pfo=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),JZ=s(Xz,"CODE",{});var CXr=n(JZ);_fo=r(CXr,"from_pretrained()"),CXr.forEach(t),bfo=r(Xz,"class method or the "),YZ=s(Xz,"CODE",{});var MXr=n(YZ);vfo=r(MXr,"from_config()"),MXr.forEach(t),Tfo=r(Xz,`class
method.`),Xz.forEach(t),Ffo=i(Hn),Vy=s(Hn,"P",{});var dSe=n(Vy);Cfo=r(dSe,"This class cannot be instantiated directly using "),KZ=s(dSe,"CODE",{});var EXr=n(KZ);Mfo=r(EXr,"__init__()"),EXr.forEach(t),Efo=r(dSe," (throws an error)."),dSe.forEach(t),yfo=i(Hn),zr=s(Hn,"DIV",{class:!0});var Un=n(zr);f(zy.$$.fragment,Un),wfo=i(Un),ZZ=s(Un,"P",{});var yXr=n(ZZ);Afo=r(yXr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),yXr.forEach(t),Lfo=i(Un),rd=s(Un,"P",{});var Vz=n(rd);Bfo=r(Vz,`Note:
Loading a model from its configuration file does `),eee=s(Vz,"STRONG",{});var wXr=n(eee);xfo=r(wXr,"not"),wXr.forEach(t),kfo=r(Vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),oee=s(Vz,"CODE",{});var AXr=n(oee);Rfo=r(AXr,"from_pretrained()"),AXr.forEach(t),Sfo=r(Vz,"to load the model weights."),Vz.forEach(t),Pfo=i(Un),ree=s(Un,"P",{});var LXr=n(ree);$fo=r(LXr,"Examples:"),LXr.forEach(t),Ifo=i(Un),f(Wy.$$.fragment,Un),Un.forEach(t),jfo=i(Hn),De=s(Hn,"DIV",{class:!0});var Ot=n(De);f(Qy.$$.fragment,Ot),Nfo=i(Ot),tee=s(Ot,"P",{});var BXr=n(tee);Dfo=r(BXr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),BXr.forEach(t),qfo=i(Ot),za=s(Ot,"P",{});var n5=n(za);Ofo=r(n5,"The model class to instantiate is selected based on the "),aee=s(n5,"CODE",{});var xXr=n(aee);Gfo=r(xXr,"model_type"),xXr.forEach(t),Xfo=r(n5,` property of the config object (either
passed as an argument or loaded from `),see=s(n5,"CODE",{});var kXr=n(see);Vfo=r(kXr,"pretrained_model_name_or_path"),kXr.forEach(t),zfo=r(n5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nee=s(n5,"CODE",{});var RXr=n(nee);Wfo=r(RXr,"pretrained_model_name_or_path"),RXr.forEach(t),Qfo=r(n5,":"),n5.forEach(t),Hfo=i(Ot),k=s(Ot,"UL",{});var S=n(k);Ep=s(S,"LI",{});var K3e=n(Ep);lee=s(K3e,"STRONG",{});var SXr=n(lee);Ufo=r(SXr,"albert"),SXr.forEach(t),Jfo=r(K3e," \u2014 "),hP=s(K3e,"A",{href:!0});var PXr=n(hP);Yfo=r(PXr,"AlbertForPreTraining"),PXr.forEach(t),Kfo=r(K3e," (ALBERT model)"),K3e.forEach(t),Zfo=i(S),yp=s(S,"LI",{});var Z3e=n(yp);iee=s(Z3e,"STRONG",{});var $Xr=n(iee);ego=r($Xr,"bart"),$Xr.forEach(t),ogo=r(Z3e," \u2014 "),uP=s(Z3e,"A",{href:!0});var IXr=n(uP);rgo=r(IXr,"BartForConditionalGeneration"),IXr.forEach(t),tgo=r(Z3e," (BART model)"),Z3e.forEach(t),ago=i(S),wp=s(S,"LI",{});var e5e=n(wp);dee=s(e5e,"STRONG",{});var jXr=n(dee);sgo=r(jXr,"bert"),jXr.forEach(t),ngo=r(e5e," \u2014 "),pP=s(e5e,"A",{href:!0});var NXr=n(pP);lgo=r(NXr,"BertForPreTraining"),NXr.forEach(t),igo=r(e5e," (BERT model)"),e5e.forEach(t),dgo=i(S),Ap=s(S,"LI",{});var o5e=n(Ap);cee=s(o5e,"STRONG",{});var DXr=n(cee);cgo=r(DXr,"big_bird"),DXr.forEach(t),mgo=r(o5e," \u2014 "),_P=s(o5e,"A",{href:!0});var qXr=n(_P);fgo=r(qXr,"BigBirdForPreTraining"),qXr.forEach(t),ggo=r(o5e," (BigBird model)"),o5e.forEach(t),hgo=i(S),Lp=s(S,"LI",{});var r5e=n(Lp);mee=s(r5e,"STRONG",{});var OXr=n(mee);ugo=r(OXr,"camembert"),OXr.forEach(t),pgo=r(r5e," \u2014 "),bP=s(r5e,"A",{href:!0});var GXr=n(bP);_go=r(GXr,"CamembertForMaskedLM"),GXr.forEach(t),bgo=r(r5e," (CamemBERT model)"),r5e.forEach(t),vgo=i(S),Bp=s(S,"LI",{});var t5e=n(Bp);fee=s(t5e,"STRONG",{});var XXr=n(fee);Tgo=r(XXr,"ctrl"),XXr.forEach(t),Fgo=r(t5e," \u2014 "),vP=s(t5e,"A",{href:!0});var VXr=n(vP);Cgo=r(VXr,"CTRLLMHeadModel"),VXr.forEach(t),Mgo=r(t5e," (CTRL model)"),t5e.forEach(t),Ego=i(S),xp=s(S,"LI",{});var a5e=n(xp);gee=s(a5e,"STRONG",{});var zXr=n(gee);ygo=r(zXr,"data2vec-text"),zXr.forEach(t),wgo=r(a5e," \u2014 "),TP=s(a5e,"A",{href:!0});var WXr=n(TP);Ago=r(WXr,"Data2VecTextForMaskedLM"),WXr.forEach(t),Lgo=r(a5e," (Data2VecText model)"),a5e.forEach(t),Bgo=i(S),kp=s(S,"LI",{});var s5e=n(kp);hee=s(s5e,"STRONG",{});var QXr=n(hee);xgo=r(QXr,"deberta"),QXr.forEach(t),kgo=r(s5e," \u2014 "),FP=s(s5e,"A",{href:!0});var HXr=n(FP);Rgo=r(HXr,"DebertaForMaskedLM"),HXr.forEach(t),Sgo=r(s5e," (DeBERTa model)"),s5e.forEach(t),Pgo=i(S),Rp=s(S,"LI",{});var n5e=n(Rp);uee=s(n5e,"STRONG",{});var UXr=n(uee);$go=r(UXr,"deberta-v2"),UXr.forEach(t),Igo=r(n5e," \u2014 "),CP=s(n5e,"A",{href:!0});var JXr=n(CP);jgo=r(JXr,"DebertaV2ForMaskedLM"),JXr.forEach(t),Ngo=r(n5e," (DeBERTa-v2 model)"),n5e.forEach(t),Dgo=i(S),Sp=s(S,"LI",{});var l5e=n(Sp);pee=s(l5e,"STRONG",{});var YXr=n(pee);qgo=r(YXr,"distilbert"),YXr.forEach(t),Ogo=r(l5e," \u2014 "),MP=s(l5e,"A",{href:!0});var KXr=n(MP);Ggo=r(KXr,"DistilBertForMaskedLM"),KXr.forEach(t),Xgo=r(l5e," (DistilBERT model)"),l5e.forEach(t),Vgo=i(S),Pp=s(S,"LI",{});var i5e=n(Pp);_ee=s(i5e,"STRONG",{});var ZXr=n(_ee);zgo=r(ZXr,"electra"),ZXr.forEach(t),Wgo=r(i5e," \u2014 "),EP=s(i5e,"A",{href:!0});var eVr=n(EP);Qgo=r(eVr,"ElectraForPreTraining"),eVr.forEach(t),Hgo=r(i5e," (ELECTRA model)"),i5e.forEach(t),Ugo=i(S),$p=s(S,"LI",{});var d5e=n($p);bee=s(d5e,"STRONG",{});var oVr=n(bee);Jgo=r(oVr,"flaubert"),oVr.forEach(t),Ygo=r(d5e," \u2014 "),yP=s(d5e,"A",{href:!0});var rVr=n(yP);Kgo=r(rVr,"FlaubertWithLMHeadModel"),rVr.forEach(t),Zgo=r(d5e," (FlauBERT model)"),d5e.forEach(t),eho=i(S),Ip=s(S,"LI",{});var c5e=n(Ip);vee=s(c5e,"STRONG",{});var tVr=n(vee);oho=r(tVr,"fnet"),tVr.forEach(t),rho=r(c5e," \u2014 "),wP=s(c5e,"A",{href:!0});var aVr=n(wP);tho=r(aVr,"FNetForPreTraining"),aVr.forEach(t),aho=r(c5e," (FNet model)"),c5e.forEach(t),sho=i(S),jp=s(S,"LI",{});var m5e=n(jp);Tee=s(m5e,"STRONG",{});var sVr=n(Tee);nho=r(sVr,"fsmt"),sVr.forEach(t),lho=r(m5e," \u2014 "),AP=s(m5e,"A",{href:!0});var nVr=n(AP);iho=r(nVr,"FSMTForConditionalGeneration"),nVr.forEach(t),dho=r(m5e," (FairSeq Machine-Translation model)"),m5e.forEach(t),cho=i(S),Np=s(S,"LI",{});var f5e=n(Np);Fee=s(f5e,"STRONG",{});var lVr=n(Fee);mho=r(lVr,"funnel"),lVr.forEach(t),fho=r(f5e," \u2014 "),LP=s(f5e,"A",{href:!0});var iVr=n(LP);gho=r(iVr,"FunnelForPreTraining"),iVr.forEach(t),hho=r(f5e," (Funnel Transformer model)"),f5e.forEach(t),uho=i(S),Dp=s(S,"LI",{});var g5e=n(Dp);Cee=s(g5e,"STRONG",{});var dVr=n(Cee);pho=r(dVr,"gpt2"),dVr.forEach(t),_ho=r(g5e," \u2014 "),BP=s(g5e,"A",{href:!0});var cVr=n(BP);bho=r(cVr,"GPT2LMHeadModel"),cVr.forEach(t),vho=r(g5e," (OpenAI GPT-2 model)"),g5e.forEach(t),Tho=i(S),qp=s(S,"LI",{});var h5e=n(qp);Mee=s(h5e,"STRONG",{});var mVr=n(Mee);Fho=r(mVr,"ibert"),mVr.forEach(t),Cho=r(h5e," \u2014 "),xP=s(h5e,"A",{href:!0});var fVr=n(xP);Mho=r(fVr,"IBertForMaskedLM"),fVr.forEach(t),Eho=r(h5e," (I-BERT model)"),h5e.forEach(t),yho=i(S),Op=s(S,"LI",{});var u5e=n(Op);Eee=s(u5e,"STRONG",{});var gVr=n(Eee);who=r(gVr,"layoutlm"),gVr.forEach(t),Aho=r(u5e," \u2014 "),kP=s(u5e,"A",{href:!0});var hVr=n(kP);Lho=r(hVr,"LayoutLMForMaskedLM"),hVr.forEach(t),Bho=r(u5e," (LayoutLM model)"),u5e.forEach(t),xho=i(S),Gp=s(S,"LI",{});var p5e=n(Gp);yee=s(p5e,"STRONG",{});var uVr=n(yee);kho=r(uVr,"longformer"),uVr.forEach(t),Rho=r(p5e," \u2014 "),RP=s(p5e,"A",{href:!0});var pVr=n(RP);Sho=r(pVr,"LongformerForMaskedLM"),pVr.forEach(t),Pho=r(p5e," (Longformer model)"),p5e.forEach(t),$ho=i(S),Xp=s(S,"LI",{});var _5e=n(Xp);wee=s(_5e,"STRONG",{});var _Vr=n(wee);Iho=r(_Vr,"lxmert"),_Vr.forEach(t),jho=r(_5e," \u2014 "),SP=s(_5e,"A",{href:!0});var bVr=n(SP);Nho=r(bVr,"LxmertForPreTraining"),bVr.forEach(t),Dho=r(_5e," (LXMERT model)"),_5e.forEach(t),qho=i(S),Vp=s(S,"LI",{});var b5e=n(Vp);Aee=s(b5e,"STRONG",{});var vVr=n(Aee);Oho=r(vVr,"megatron-bert"),vVr.forEach(t),Gho=r(b5e," \u2014 "),PP=s(b5e,"A",{href:!0});var TVr=n(PP);Xho=r(TVr,"MegatronBertForPreTraining"),TVr.forEach(t),Vho=r(b5e," (MegatronBert model)"),b5e.forEach(t),zho=i(S),zp=s(S,"LI",{});var v5e=n(zp);Lee=s(v5e,"STRONG",{});var FVr=n(Lee);Who=r(FVr,"mobilebert"),FVr.forEach(t),Qho=r(v5e," \u2014 "),$P=s(v5e,"A",{href:!0});var CVr=n($P);Hho=r(CVr,"MobileBertForPreTraining"),CVr.forEach(t),Uho=r(v5e," (MobileBERT model)"),v5e.forEach(t),Jho=i(S),Wp=s(S,"LI",{});var T5e=n(Wp);Bee=s(T5e,"STRONG",{});var MVr=n(Bee);Yho=r(MVr,"mpnet"),MVr.forEach(t),Kho=r(T5e," \u2014 "),IP=s(T5e,"A",{href:!0});var EVr=n(IP);Zho=r(EVr,"MPNetForMaskedLM"),EVr.forEach(t),euo=r(T5e," (MPNet model)"),T5e.forEach(t),ouo=i(S),Qp=s(S,"LI",{});var F5e=n(Qp);xee=s(F5e,"STRONG",{});var yVr=n(xee);ruo=r(yVr,"openai-gpt"),yVr.forEach(t),tuo=r(F5e," \u2014 "),jP=s(F5e,"A",{href:!0});var wVr=n(jP);auo=r(wVr,"OpenAIGPTLMHeadModel"),wVr.forEach(t),suo=r(F5e," (OpenAI GPT model)"),F5e.forEach(t),nuo=i(S),Hp=s(S,"LI",{});var C5e=n(Hp);kee=s(C5e,"STRONG",{});var AVr=n(kee);luo=r(AVr,"retribert"),AVr.forEach(t),iuo=r(C5e," \u2014 "),NP=s(C5e,"A",{href:!0});var LVr=n(NP);duo=r(LVr,"RetriBertModel"),LVr.forEach(t),cuo=r(C5e," (RetriBERT model)"),C5e.forEach(t),muo=i(S),Up=s(S,"LI",{});var M5e=n(Up);Ree=s(M5e,"STRONG",{});var BVr=n(Ree);fuo=r(BVr,"roberta"),BVr.forEach(t),guo=r(M5e," \u2014 "),DP=s(M5e,"A",{href:!0});var xVr=n(DP);huo=r(xVr,"RobertaForMaskedLM"),xVr.forEach(t),uuo=r(M5e," (RoBERTa model)"),M5e.forEach(t),puo=i(S),Jp=s(S,"LI",{});var E5e=n(Jp);See=s(E5e,"STRONG",{});var kVr=n(See);_uo=r(kVr,"squeezebert"),kVr.forEach(t),buo=r(E5e," \u2014 "),qP=s(E5e,"A",{href:!0});var RVr=n(qP);vuo=r(RVr,"SqueezeBertForMaskedLM"),RVr.forEach(t),Tuo=r(E5e," (SqueezeBERT model)"),E5e.forEach(t),Fuo=i(S),Yp=s(S,"LI",{});var y5e=n(Yp);Pee=s(y5e,"STRONG",{});var SVr=n(Pee);Cuo=r(SVr,"t5"),SVr.forEach(t),Muo=r(y5e," \u2014 "),OP=s(y5e,"A",{href:!0});var PVr=n(OP);Euo=r(PVr,"T5ForConditionalGeneration"),PVr.forEach(t),yuo=r(y5e," (T5 model)"),y5e.forEach(t),wuo=i(S),Kp=s(S,"LI",{});var w5e=n(Kp);$ee=s(w5e,"STRONG",{});var $Vr=n($ee);Auo=r($Vr,"tapas"),$Vr.forEach(t),Luo=r(w5e," \u2014 "),GP=s(w5e,"A",{href:!0});var IVr=n(GP);Buo=r(IVr,"TapasForMaskedLM"),IVr.forEach(t),xuo=r(w5e," (TAPAS model)"),w5e.forEach(t),kuo=i(S),Zp=s(S,"LI",{});var A5e=n(Zp);Iee=s(A5e,"STRONG",{});var jVr=n(Iee);Ruo=r(jVr,"transfo-xl"),jVr.forEach(t),Suo=r(A5e," \u2014 "),XP=s(A5e,"A",{href:!0});var NVr=n(XP);Puo=r(NVr,"TransfoXLLMHeadModel"),NVr.forEach(t),$uo=r(A5e," (Transformer-XL model)"),A5e.forEach(t),Iuo=i(S),e_=s(S,"LI",{});var L5e=n(e_);jee=s(L5e,"STRONG",{});var DVr=n(jee);juo=r(DVr,"unispeech"),DVr.forEach(t),Nuo=r(L5e," \u2014 "),VP=s(L5e,"A",{href:!0});var qVr=n(VP);Duo=r(qVr,"UniSpeechForPreTraining"),qVr.forEach(t),quo=r(L5e," (UniSpeech model)"),L5e.forEach(t),Ouo=i(S),o_=s(S,"LI",{});var B5e=n(o_);Nee=s(B5e,"STRONG",{});var OVr=n(Nee);Guo=r(OVr,"unispeech-sat"),OVr.forEach(t),Xuo=r(B5e," \u2014 "),zP=s(B5e,"A",{href:!0});var GVr=n(zP);Vuo=r(GVr,"UniSpeechSatForPreTraining"),GVr.forEach(t),zuo=r(B5e," (UniSpeechSat model)"),B5e.forEach(t),Wuo=i(S),r_=s(S,"LI",{});var x5e=n(r_);Dee=s(x5e,"STRONG",{});var XVr=n(Dee);Quo=r(XVr,"visual_bert"),XVr.forEach(t),Huo=r(x5e," \u2014 "),WP=s(x5e,"A",{href:!0});var VVr=n(WP);Uuo=r(VVr,"VisualBertForPreTraining"),VVr.forEach(t),Juo=r(x5e," (VisualBert model)"),x5e.forEach(t),Yuo=i(S),t_=s(S,"LI",{});var k5e=n(t_);qee=s(k5e,"STRONG",{});var zVr=n(qee);Kuo=r(zVr,"vit_mae"),zVr.forEach(t),Zuo=r(k5e," \u2014 "),QP=s(k5e,"A",{href:!0});var WVr=n(QP);epo=r(WVr,"ViTMAEForPreTraining"),WVr.forEach(t),opo=r(k5e," (ViTMAE model)"),k5e.forEach(t),rpo=i(S),a_=s(S,"LI",{});var R5e=n(a_);Oee=s(R5e,"STRONG",{});var QVr=n(Oee);tpo=r(QVr,"wav2vec2"),QVr.forEach(t),apo=r(R5e," \u2014 "),HP=s(R5e,"A",{href:!0});var HVr=n(HP);spo=r(HVr,"Wav2Vec2ForPreTraining"),HVr.forEach(t),npo=r(R5e," (Wav2Vec2 model)"),R5e.forEach(t),lpo=i(S),s_=s(S,"LI",{});var S5e=n(s_);Gee=s(S5e,"STRONG",{});var UVr=n(Gee);ipo=r(UVr,"xlm"),UVr.forEach(t),dpo=r(S5e," \u2014 "),UP=s(S5e,"A",{href:!0});var JVr=n(UP);cpo=r(JVr,"XLMWithLMHeadModel"),JVr.forEach(t),mpo=r(S5e," (XLM model)"),S5e.forEach(t),fpo=i(S),n_=s(S,"LI",{});var P5e=n(n_);Xee=s(P5e,"STRONG",{});var YVr=n(Xee);gpo=r(YVr,"xlm-roberta"),YVr.forEach(t),hpo=r(P5e," \u2014 "),JP=s(P5e,"A",{href:!0});var KVr=n(JP);upo=r(KVr,"XLMRobertaForMaskedLM"),KVr.forEach(t),ppo=r(P5e," (XLM-RoBERTa model)"),P5e.forEach(t),_po=i(S),l_=s(S,"LI",{});var $5e=n(l_);Vee=s($5e,"STRONG",{});var ZVr=n(Vee);bpo=r(ZVr,"xlm-roberta-xl"),ZVr.forEach(t),vpo=r($5e," \u2014 "),YP=s($5e,"A",{href:!0});var ezr=n(YP);Tpo=r(ezr,"XLMRobertaXLForMaskedLM"),ezr.forEach(t),Fpo=r($5e," (XLM-RoBERTa-XL model)"),$5e.forEach(t),Cpo=i(S),i_=s(S,"LI",{});var I5e=n(i_);zee=s(I5e,"STRONG",{});var ozr=n(zee);Mpo=r(ozr,"xlnet"),ozr.forEach(t),Epo=r(I5e," \u2014 "),KP=s(I5e,"A",{href:!0});var rzr=n(KP);ypo=r(rzr,"XLNetLMHeadModel"),rzr.forEach(t),wpo=r(I5e," (XLNet model)"),I5e.forEach(t),S.forEach(t),Apo=i(Ot),d_=s(Ot,"P",{});var j5e=n(d_);Lpo=r(j5e,"The model is set in evaluation mode by default using "),Wee=s(j5e,"CODE",{});var tzr=n(Wee);Bpo=r(tzr,"model.eval()"),tzr.forEach(t),xpo=r(j5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Qee=s(j5e,"CODE",{});var azr=n(Qee);kpo=r(azr,"model.train()"),azr.forEach(t),j5e.forEach(t),Rpo=i(Ot),Hee=s(Ot,"P",{});var szr=n(Hee);Spo=r(szr,"Examples:"),szr.forEach(t),Ppo=i(Ot),f(Hy.$$.fragment,Ot),Ot.forEach(t),Hn.forEach(t),oke=i(c),td=s(c,"H2",{class:!0});var cSe=n(td);c_=s(cSe,"A",{id:!0,class:!0,href:!0});var nzr=n(c_);Uee=s(nzr,"SPAN",{});var lzr=n(Uee);f(Uy.$$.fragment,lzr),lzr.forEach(t),nzr.forEach(t),$po=i(cSe),Jee=s(cSe,"SPAN",{});var izr=n(Jee);Ipo=r(izr,"AutoModelForCausalLM"),izr.forEach(t),cSe.forEach(t),rke=i(c),Ko=s(c,"DIV",{class:!0});var Jn=n(Ko);f(Jy.$$.fragment,Jn),jpo=i(Jn),ad=s(Jn,"P",{});var zz=n(ad);Npo=r(zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Yee=s(zz,"CODE",{});var dzr=n(Yee);Dpo=r(dzr,"from_pretrained()"),dzr.forEach(t),qpo=r(zz,"class method or the "),Kee=s(zz,"CODE",{});var czr=n(Kee);Opo=r(czr,"from_config()"),czr.forEach(t),Gpo=r(zz,`class
method.`),zz.forEach(t),Xpo=i(Jn),Yy=s(Jn,"P",{});var mSe=n(Yy);Vpo=r(mSe,"This class cannot be instantiated directly using "),Zee=s(mSe,"CODE",{});var mzr=n(Zee);zpo=r(mzr,"__init__()"),mzr.forEach(t),Wpo=r(mSe," (throws an error)."),mSe.forEach(t),Qpo=i(Jn),Wr=s(Jn,"DIV",{class:!0});var Yn=n(Wr);f(Ky.$$.fragment,Yn),Hpo=i(Yn),eoe=s(Yn,"P",{});var fzr=n(eoe);Upo=r(fzr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),fzr.forEach(t),Jpo=i(Yn),sd=s(Yn,"P",{});var Wz=n(sd);Ypo=r(Wz,`Note:
Loading a model from its configuration file does `),ooe=s(Wz,"STRONG",{});var gzr=n(ooe);Kpo=r(gzr,"not"),gzr.forEach(t),Zpo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),roe=s(Wz,"CODE",{});var hzr=n(roe);e_o=r(hzr,"from_pretrained()"),hzr.forEach(t),o_o=r(Wz,"to load the model weights."),Wz.forEach(t),r_o=i(Yn),toe=s(Yn,"P",{});var uzr=n(toe);t_o=r(uzr,"Examples:"),uzr.forEach(t),a_o=i(Yn),f(Zy.$$.fragment,Yn),Yn.forEach(t),s_o=i(Jn),qe=s(Jn,"DIV",{class:!0});var Gt=n(qe);f(ew.$$.fragment,Gt),n_o=i(Gt),aoe=s(Gt,"P",{});var pzr=n(aoe);l_o=r(pzr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),pzr.forEach(t),i_o=i(Gt),Wa=s(Gt,"P",{});var l5=n(Wa);d_o=r(l5,"The model class to instantiate is selected based on the "),soe=s(l5,"CODE",{});var _zr=n(soe);c_o=r(_zr,"model_type"),_zr.forEach(t),m_o=r(l5,` property of the config object (either
passed as an argument or loaded from `),noe=s(l5,"CODE",{});var bzr=n(noe);f_o=r(bzr,"pretrained_model_name_or_path"),bzr.forEach(t),g_o=r(l5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),loe=s(l5,"CODE",{});var vzr=n(loe);h_o=r(vzr,"pretrained_model_name_or_path"),vzr.forEach(t),u_o=r(l5,":"),l5.forEach(t),p_o=i(Gt),$=s(Gt,"UL",{});var j=n($);m_=s(j,"LI",{});var N5e=n(m_);ioe=s(N5e,"STRONG",{});var Tzr=n(ioe);__o=r(Tzr,"bart"),Tzr.forEach(t),b_o=r(N5e," \u2014 "),ZP=s(N5e,"A",{href:!0});var Fzr=n(ZP);v_o=r(Fzr,"BartForCausalLM"),Fzr.forEach(t),T_o=r(N5e," (BART model)"),N5e.forEach(t),F_o=i(j),f_=s(j,"LI",{});var D5e=n(f_);doe=s(D5e,"STRONG",{});var Czr=n(doe);C_o=r(Czr,"bert"),Czr.forEach(t),M_o=r(D5e," \u2014 "),e$=s(D5e,"A",{href:!0});var Mzr=n(e$);E_o=r(Mzr,"BertLMHeadModel"),Mzr.forEach(t),y_o=r(D5e," (BERT model)"),D5e.forEach(t),w_o=i(j),g_=s(j,"LI",{});var q5e=n(g_);coe=s(q5e,"STRONG",{});var Ezr=n(coe);A_o=r(Ezr,"bert-generation"),Ezr.forEach(t),L_o=r(q5e," \u2014 "),o$=s(q5e,"A",{href:!0});var yzr=n(o$);B_o=r(yzr,"BertGenerationDecoder"),yzr.forEach(t),x_o=r(q5e," (Bert Generation model)"),q5e.forEach(t),k_o=i(j),h_=s(j,"LI",{});var O5e=n(h_);moe=s(O5e,"STRONG",{});var wzr=n(moe);R_o=r(wzr,"big_bird"),wzr.forEach(t),S_o=r(O5e," \u2014 "),r$=s(O5e,"A",{href:!0});var Azr=n(r$);P_o=r(Azr,"BigBirdForCausalLM"),Azr.forEach(t),$_o=r(O5e," (BigBird model)"),O5e.forEach(t),I_o=i(j),u_=s(j,"LI",{});var G5e=n(u_);foe=s(G5e,"STRONG",{});var Lzr=n(foe);j_o=r(Lzr,"bigbird_pegasus"),Lzr.forEach(t),N_o=r(G5e," \u2014 "),t$=s(G5e,"A",{href:!0});var Bzr=n(t$);D_o=r(Bzr,"BigBirdPegasusForCausalLM"),Bzr.forEach(t),q_o=r(G5e," (BigBirdPegasus model)"),G5e.forEach(t),O_o=i(j),p_=s(j,"LI",{});var X5e=n(p_);goe=s(X5e,"STRONG",{});var xzr=n(goe);G_o=r(xzr,"blenderbot"),xzr.forEach(t),X_o=r(X5e," \u2014 "),a$=s(X5e,"A",{href:!0});var kzr=n(a$);V_o=r(kzr,"BlenderbotForCausalLM"),kzr.forEach(t),z_o=r(X5e," (Blenderbot model)"),X5e.forEach(t),W_o=i(j),__=s(j,"LI",{});var V5e=n(__);hoe=s(V5e,"STRONG",{});var Rzr=n(hoe);Q_o=r(Rzr,"blenderbot-small"),Rzr.forEach(t),H_o=r(V5e," \u2014 "),s$=s(V5e,"A",{href:!0});var Szr=n(s$);U_o=r(Szr,"BlenderbotSmallForCausalLM"),Szr.forEach(t),J_o=r(V5e," (BlenderbotSmall model)"),V5e.forEach(t),Y_o=i(j),b_=s(j,"LI",{});var z5e=n(b_);uoe=s(z5e,"STRONG",{});var Pzr=n(uoe);K_o=r(Pzr,"camembert"),Pzr.forEach(t),Z_o=r(z5e," \u2014 "),n$=s(z5e,"A",{href:!0});var $zr=n(n$);ebo=r($zr,"CamembertForCausalLM"),$zr.forEach(t),obo=r(z5e," (CamemBERT model)"),z5e.forEach(t),rbo=i(j),v_=s(j,"LI",{});var W5e=n(v_);poe=s(W5e,"STRONG",{});var Izr=n(poe);tbo=r(Izr,"ctrl"),Izr.forEach(t),abo=r(W5e," \u2014 "),l$=s(W5e,"A",{href:!0});var jzr=n(l$);sbo=r(jzr,"CTRLLMHeadModel"),jzr.forEach(t),nbo=r(W5e," (CTRL model)"),W5e.forEach(t),lbo=i(j),T_=s(j,"LI",{});var Q5e=n(T_);_oe=s(Q5e,"STRONG",{});var Nzr=n(_oe);ibo=r(Nzr,"data2vec-text"),Nzr.forEach(t),dbo=r(Q5e," \u2014 "),i$=s(Q5e,"A",{href:!0});var Dzr=n(i$);cbo=r(Dzr,"Data2VecTextForCausalLM"),Dzr.forEach(t),mbo=r(Q5e," (Data2VecText model)"),Q5e.forEach(t),fbo=i(j),F_=s(j,"LI",{});var H5e=n(F_);boe=s(H5e,"STRONG",{});var qzr=n(boe);gbo=r(qzr,"electra"),qzr.forEach(t),hbo=r(H5e," \u2014 "),d$=s(H5e,"A",{href:!0});var Ozr=n(d$);ubo=r(Ozr,"ElectraForCausalLM"),Ozr.forEach(t),pbo=r(H5e," (ELECTRA model)"),H5e.forEach(t),_bo=i(j),C_=s(j,"LI",{});var U5e=n(C_);voe=s(U5e,"STRONG",{});var Gzr=n(voe);bbo=r(Gzr,"gpt2"),Gzr.forEach(t),vbo=r(U5e," \u2014 "),c$=s(U5e,"A",{href:!0});var Xzr=n(c$);Tbo=r(Xzr,"GPT2LMHeadModel"),Xzr.forEach(t),Fbo=r(U5e," (OpenAI GPT-2 model)"),U5e.forEach(t),Cbo=i(j),M_=s(j,"LI",{});var J5e=n(M_);Toe=s(J5e,"STRONG",{});var Vzr=n(Toe);Mbo=r(Vzr,"gpt_neo"),Vzr.forEach(t),Ebo=r(J5e," \u2014 "),m$=s(J5e,"A",{href:!0});var zzr=n(m$);ybo=r(zzr,"GPTNeoForCausalLM"),zzr.forEach(t),wbo=r(J5e," (GPT Neo model)"),J5e.forEach(t),Abo=i(j),E_=s(j,"LI",{});var Y5e=n(E_);Foe=s(Y5e,"STRONG",{});var Wzr=n(Foe);Lbo=r(Wzr,"gptj"),Wzr.forEach(t),Bbo=r(Y5e," \u2014 "),f$=s(Y5e,"A",{href:!0});var Qzr=n(f$);xbo=r(Qzr,"GPTJForCausalLM"),Qzr.forEach(t),kbo=r(Y5e," (GPT-J model)"),Y5e.forEach(t),Rbo=i(j),y_=s(j,"LI",{});var K5e=n(y_);Coe=s(K5e,"STRONG",{});var Hzr=n(Coe);Sbo=r(Hzr,"marian"),Hzr.forEach(t),Pbo=r(K5e," \u2014 "),g$=s(K5e,"A",{href:!0});var Uzr=n(g$);$bo=r(Uzr,"MarianForCausalLM"),Uzr.forEach(t),Ibo=r(K5e," (Marian model)"),K5e.forEach(t),jbo=i(j),w_=s(j,"LI",{});var Z5e=n(w_);Moe=s(Z5e,"STRONG",{});var Jzr=n(Moe);Nbo=r(Jzr,"mbart"),Jzr.forEach(t),Dbo=r(Z5e," \u2014 "),h$=s(Z5e,"A",{href:!0});var Yzr=n(h$);qbo=r(Yzr,"MBartForCausalLM"),Yzr.forEach(t),Obo=r(Z5e," (mBART model)"),Z5e.forEach(t),Gbo=i(j),A_=s(j,"LI",{});var eye=n(A_);Eoe=s(eye,"STRONG",{});var Kzr=n(Eoe);Xbo=r(Kzr,"megatron-bert"),Kzr.forEach(t),Vbo=r(eye," \u2014 "),u$=s(eye,"A",{href:!0});var Zzr=n(u$);zbo=r(Zzr,"MegatronBertForCausalLM"),Zzr.forEach(t),Wbo=r(eye," (MegatronBert model)"),eye.forEach(t),Qbo=i(j),L_=s(j,"LI",{});var oye=n(L_);yoe=s(oye,"STRONG",{});var eWr=n(yoe);Hbo=r(eWr,"openai-gpt"),eWr.forEach(t),Ubo=r(oye," \u2014 "),p$=s(oye,"A",{href:!0});var oWr=n(p$);Jbo=r(oWr,"OpenAIGPTLMHeadModel"),oWr.forEach(t),Ybo=r(oye," (OpenAI GPT model)"),oye.forEach(t),Kbo=i(j),B_=s(j,"LI",{});var rye=n(B_);woe=s(rye,"STRONG",{});var rWr=n(woe);Zbo=r(rWr,"pegasus"),rWr.forEach(t),e2o=r(rye," \u2014 "),_$=s(rye,"A",{href:!0});var tWr=n(_$);o2o=r(tWr,"PegasusForCausalLM"),tWr.forEach(t),r2o=r(rye," (Pegasus model)"),rye.forEach(t),t2o=i(j),x_=s(j,"LI",{});var tye=n(x_);Aoe=s(tye,"STRONG",{});var aWr=n(Aoe);a2o=r(aWr,"plbart"),aWr.forEach(t),s2o=r(tye," \u2014 "),b$=s(tye,"A",{href:!0});var sWr=n(b$);n2o=r(sWr,"PLBartForCausalLM"),sWr.forEach(t),l2o=r(tye," (PLBart model)"),tye.forEach(t),i2o=i(j),k_=s(j,"LI",{});var aye=n(k_);Loe=s(aye,"STRONG",{});var nWr=n(Loe);d2o=r(nWr,"prophetnet"),nWr.forEach(t),c2o=r(aye," \u2014 "),v$=s(aye,"A",{href:!0});var lWr=n(v$);m2o=r(lWr,"ProphetNetForCausalLM"),lWr.forEach(t),f2o=r(aye," (ProphetNet model)"),aye.forEach(t),g2o=i(j),R_=s(j,"LI",{});var sye=n(R_);Boe=s(sye,"STRONG",{});var iWr=n(Boe);h2o=r(iWr,"qdqbert"),iWr.forEach(t),u2o=r(sye," \u2014 "),T$=s(sye,"A",{href:!0});var dWr=n(T$);p2o=r(dWr,"QDQBertLMHeadModel"),dWr.forEach(t),_2o=r(sye," (QDQBert model)"),sye.forEach(t),b2o=i(j),S_=s(j,"LI",{});var nye=n(S_);xoe=s(nye,"STRONG",{});var cWr=n(xoe);v2o=r(cWr,"reformer"),cWr.forEach(t),T2o=r(nye," \u2014 "),F$=s(nye,"A",{href:!0});var mWr=n(F$);F2o=r(mWr,"ReformerModelWithLMHead"),mWr.forEach(t),C2o=r(nye," (Reformer model)"),nye.forEach(t),M2o=i(j),P_=s(j,"LI",{});var lye=n(P_);koe=s(lye,"STRONG",{});var fWr=n(koe);E2o=r(fWr,"rembert"),fWr.forEach(t),y2o=r(lye," \u2014 "),C$=s(lye,"A",{href:!0});var gWr=n(C$);w2o=r(gWr,"RemBertForCausalLM"),gWr.forEach(t),A2o=r(lye," (RemBERT model)"),lye.forEach(t),L2o=i(j),$_=s(j,"LI",{});var iye=n($_);Roe=s(iye,"STRONG",{});var hWr=n(Roe);B2o=r(hWr,"roberta"),hWr.forEach(t),x2o=r(iye," \u2014 "),M$=s(iye,"A",{href:!0});var uWr=n(M$);k2o=r(uWr,"RobertaForCausalLM"),uWr.forEach(t),R2o=r(iye," (RoBERTa model)"),iye.forEach(t),S2o=i(j),I_=s(j,"LI",{});var dye=n(I_);Soe=s(dye,"STRONG",{});var pWr=n(Soe);P2o=r(pWr,"roformer"),pWr.forEach(t),$2o=r(dye," \u2014 "),E$=s(dye,"A",{href:!0});var _Wr=n(E$);I2o=r(_Wr,"RoFormerForCausalLM"),_Wr.forEach(t),j2o=r(dye," (RoFormer model)"),dye.forEach(t),N2o=i(j),j_=s(j,"LI",{});var cye=n(j_);Poe=s(cye,"STRONG",{});var bWr=n(Poe);D2o=r(bWr,"speech_to_text_2"),bWr.forEach(t),q2o=r(cye," \u2014 "),y$=s(cye,"A",{href:!0});var vWr=n(y$);O2o=r(vWr,"Speech2Text2ForCausalLM"),vWr.forEach(t),G2o=r(cye," (Speech2Text2 model)"),cye.forEach(t),X2o=i(j),N_=s(j,"LI",{});var mye=n(N_);$oe=s(mye,"STRONG",{});var TWr=n($oe);V2o=r(TWr,"transfo-xl"),TWr.forEach(t),z2o=r(mye," \u2014 "),w$=s(mye,"A",{href:!0});var FWr=n(w$);W2o=r(FWr,"TransfoXLLMHeadModel"),FWr.forEach(t),Q2o=r(mye," (Transformer-XL model)"),mye.forEach(t),H2o=i(j),D_=s(j,"LI",{});var fye=n(D_);Ioe=s(fye,"STRONG",{});var CWr=n(Ioe);U2o=r(CWr,"trocr"),CWr.forEach(t),J2o=r(fye," \u2014 "),A$=s(fye,"A",{href:!0});var MWr=n(A$);Y2o=r(MWr,"TrOCRForCausalLM"),MWr.forEach(t),K2o=r(fye," (TrOCR model)"),fye.forEach(t),Z2o=i(j),q_=s(j,"LI",{});var gye=n(q_);joe=s(gye,"STRONG",{});var EWr=n(joe);evo=r(EWr,"xglm"),EWr.forEach(t),ovo=r(gye," \u2014 "),L$=s(gye,"A",{href:!0});var yWr=n(L$);rvo=r(yWr,"XGLMForCausalLM"),yWr.forEach(t),tvo=r(gye," (XGLM model)"),gye.forEach(t),avo=i(j),O_=s(j,"LI",{});var hye=n(O_);Noe=s(hye,"STRONG",{});var wWr=n(Noe);svo=r(wWr,"xlm"),wWr.forEach(t),nvo=r(hye," \u2014 "),B$=s(hye,"A",{href:!0});var AWr=n(B$);lvo=r(AWr,"XLMWithLMHeadModel"),AWr.forEach(t),ivo=r(hye," (XLM model)"),hye.forEach(t),dvo=i(j),G_=s(j,"LI",{});var uye=n(G_);Doe=s(uye,"STRONG",{});var LWr=n(Doe);cvo=r(LWr,"xlm-prophetnet"),LWr.forEach(t),mvo=r(uye," \u2014 "),x$=s(uye,"A",{href:!0});var BWr=n(x$);fvo=r(BWr,"XLMProphetNetForCausalLM"),BWr.forEach(t),gvo=r(uye," (XLMProphetNet model)"),uye.forEach(t),hvo=i(j),X_=s(j,"LI",{});var pye=n(X_);qoe=s(pye,"STRONG",{});var xWr=n(qoe);uvo=r(xWr,"xlm-roberta"),xWr.forEach(t),pvo=r(pye," \u2014 "),k$=s(pye,"A",{href:!0});var kWr=n(k$);_vo=r(kWr,"XLMRobertaForCausalLM"),kWr.forEach(t),bvo=r(pye," (XLM-RoBERTa model)"),pye.forEach(t),vvo=i(j),V_=s(j,"LI",{});var _ye=n(V_);Ooe=s(_ye,"STRONG",{});var RWr=n(Ooe);Tvo=r(RWr,"xlm-roberta-xl"),RWr.forEach(t),Fvo=r(_ye," \u2014 "),R$=s(_ye,"A",{href:!0});var SWr=n(R$);Cvo=r(SWr,"XLMRobertaXLForCausalLM"),SWr.forEach(t),Mvo=r(_ye," (XLM-RoBERTa-XL model)"),_ye.forEach(t),Evo=i(j),z_=s(j,"LI",{});var bye=n(z_);Goe=s(bye,"STRONG",{});var PWr=n(Goe);yvo=r(PWr,"xlnet"),PWr.forEach(t),wvo=r(bye," \u2014 "),S$=s(bye,"A",{href:!0});var $Wr=n(S$);Avo=r($Wr,"XLNetLMHeadModel"),$Wr.forEach(t),Lvo=r(bye," (XLNet model)"),bye.forEach(t),j.forEach(t),Bvo=i(Gt),W_=s(Gt,"P",{});var vye=n(W_);xvo=r(vye,"The model is set in evaluation mode by default using "),Xoe=s(vye,"CODE",{});var IWr=n(Xoe);kvo=r(IWr,"model.eval()"),IWr.forEach(t),Rvo=r(vye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Voe=s(vye,"CODE",{});var jWr=n(Voe);Svo=r(jWr,"model.train()"),jWr.forEach(t),vye.forEach(t),Pvo=i(Gt),zoe=s(Gt,"P",{});var NWr=n(zoe);$vo=r(NWr,"Examples:"),NWr.forEach(t),Ivo=i(Gt),f(ow.$$.fragment,Gt),Gt.forEach(t),Jn.forEach(t),tke=i(c),nd=s(c,"H2",{class:!0});var fSe=n(nd);Q_=s(fSe,"A",{id:!0,class:!0,href:!0});var DWr=n(Q_);Woe=s(DWr,"SPAN",{});var qWr=n(Woe);f(rw.$$.fragment,qWr),qWr.forEach(t),DWr.forEach(t),jvo=i(fSe),Qoe=s(fSe,"SPAN",{});var OWr=n(Qoe);Nvo=r(OWr,"AutoModelForMaskedLM"),OWr.forEach(t),fSe.forEach(t),ake=i(c),Zo=s(c,"DIV",{class:!0});var Kn=n(Zo);f(tw.$$.fragment,Kn),Dvo=i(Kn),ld=s(Kn,"P",{});var Qz=n(ld);qvo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Hoe=s(Qz,"CODE",{});var GWr=n(Hoe);Ovo=r(GWr,"from_pretrained()"),GWr.forEach(t),Gvo=r(Qz,"class method or the "),Uoe=s(Qz,"CODE",{});var XWr=n(Uoe);Xvo=r(XWr,"from_config()"),XWr.forEach(t),Vvo=r(Qz,`class
method.`),Qz.forEach(t),zvo=i(Kn),aw=s(Kn,"P",{});var gSe=n(aw);Wvo=r(gSe,"This class cannot be instantiated directly using "),Joe=s(gSe,"CODE",{});var VWr=n(Joe);Qvo=r(VWr,"__init__()"),VWr.forEach(t),Hvo=r(gSe," (throws an error)."),gSe.forEach(t),Uvo=i(Kn),Qr=s(Kn,"DIV",{class:!0});var Zn=n(Qr);f(sw.$$.fragment,Zn),Jvo=i(Zn),Yoe=s(Zn,"P",{});var zWr=n(Yoe);Yvo=r(zWr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),zWr.forEach(t),Kvo=i(Zn),id=s(Zn,"P",{});var Hz=n(id);Zvo=r(Hz,`Note:
Loading a model from its configuration file does `),Koe=s(Hz,"STRONG",{});var WWr=n(Koe);eTo=r(WWr,"not"),WWr.forEach(t),oTo=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zoe=s(Hz,"CODE",{});var QWr=n(Zoe);rTo=r(QWr,"from_pretrained()"),QWr.forEach(t),tTo=r(Hz,"to load the model weights."),Hz.forEach(t),aTo=i(Zn),ere=s(Zn,"P",{});var HWr=n(ere);sTo=r(HWr,"Examples:"),HWr.forEach(t),nTo=i(Zn),f(nw.$$.fragment,Zn),Zn.forEach(t),lTo=i(Kn),Oe=s(Kn,"DIV",{class:!0});var Xt=n(Oe);f(lw.$$.fragment,Xt),iTo=i(Xt),ore=s(Xt,"P",{});var UWr=n(ore);dTo=r(UWr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),UWr.forEach(t),cTo=i(Xt),Qa=s(Xt,"P",{});var i5=n(Qa);mTo=r(i5,"The model class to instantiate is selected based on the "),rre=s(i5,"CODE",{});var JWr=n(rre);fTo=r(JWr,"model_type"),JWr.forEach(t),gTo=r(i5,` property of the config object (either
passed as an argument or loaded from `),tre=s(i5,"CODE",{});var YWr=n(tre);hTo=r(YWr,"pretrained_model_name_or_path"),YWr.forEach(t),uTo=r(i5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),are=s(i5,"CODE",{});var KWr=n(are);pTo=r(KWr,"pretrained_model_name_or_path"),KWr.forEach(t),_To=r(i5,":"),i5.forEach(t),bTo=i(Xt),I=s(Xt,"UL",{});var N=n(I);H_=s(N,"LI",{});var Tye=n(H_);sre=s(Tye,"STRONG",{});var ZWr=n(sre);vTo=r(ZWr,"albert"),ZWr.forEach(t),TTo=r(Tye," \u2014 "),P$=s(Tye,"A",{href:!0});var eQr=n(P$);FTo=r(eQr,"AlbertForMaskedLM"),eQr.forEach(t),CTo=r(Tye," (ALBERT model)"),Tye.forEach(t),MTo=i(N),U_=s(N,"LI",{});var Fye=n(U_);nre=s(Fye,"STRONG",{});var oQr=n(nre);ETo=r(oQr,"bart"),oQr.forEach(t),yTo=r(Fye," \u2014 "),$$=s(Fye,"A",{href:!0});var rQr=n($$);wTo=r(rQr,"BartForConditionalGeneration"),rQr.forEach(t),ATo=r(Fye," (BART model)"),Fye.forEach(t),LTo=i(N),J_=s(N,"LI",{});var Cye=n(J_);lre=s(Cye,"STRONG",{});var tQr=n(lre);BTo=r(tQr,"bert"),tQr.forEach(t),xTo=r(Cye," \u2014 "),I$=s(Cye,"A",{href:!0});var aQr=n(I$);kTo=r(aQr,"BertForMaskedLM"),aQr.forEach(t),RTo=r(Cye," (BERT model)"),Cye.forEach(t),STo=i(N),Y_=s(N,"LI",{});var Mye=n(Y_);ire=s(Mye,"STRONG",{});var sQr=n(ire);PTo=r(sQr,"big_bird"),sQr.forEach(t),$To=r(Mye," \u2014 "),j$=s(Mye,"A",{href:!0});var nQr=n(j$);ITo=r(nQr,"BigBirdForMaskedLM"),nQr.forEach(t),jTo=r(Mye," (BigBird model)"),Mye.forEach(t),NTo=i(N),K_=s(N,"LI",{});var Eye=n(K_);dre=s(Eye,"STRONG",{});var lQr=n(dre);DTo=r(lQr,"camembert"),lQr.forEach(t),qTo=r(Eye," \u2014 "),N$=s(Eye,"A",{href:!0});var iQr=n(N$);OTo=r(iQr,"CamembertForMaskedLM"),iQr.forEach(t),GTo=r(Eye," (CamemBERT model)"),Eye.forEach(t),XTo=i(N),Z_=s(N,"LI",{});var yye=n(Z_);cre=s(yye,"STRONG",{});var dQr=n(cre);VTo=r(dQr,"convbert"),dQr.forEach(t),zTo=r(yye," \u2014 "),D$=s(yye,"A",{href:!0});var cQr=n(D$);WTo=r(cQr,"ConvBertForMaskedLM"),cQr.forEach(t),QTo=r(yye," (ConvBERT model)"),yye.forEach(t),HTo=i(N),eb=s(N,"LI",{});var wye=n(eb);mre=s(wye,"STRONG",{});var mQr=n(mre);UTo=r(mQr,"data2vec-text"),mQr.forEach(t),JTo=r(wye," \u2014 "),q$=s(wye,"A",{href:!0});var fQr=n(q$);YTo=r(fQr,"Data2VecTextForMaskedLM"),fQr.forEach(t),KTo=r(wye," (Data2VecText model)"),wye.forEach(t),ZTo=i(N),ob=s(N,"LI",{});var Aye=n(ob);fre=s(Aye,"STRONG",{});var gQr=n(fre);e1o=r(gQr,"deberta"),gQr.forEach(t),o1o=r(Aye," \u2014 "),O$=s(Aye,"A",{href:!0});var hQr=n(O$);r1o=r(hQr,"DebertaForMaskedLM"),hQr.forEach(t),t1o=r(Aye," (DeBERTa model)"),Aye.forEach(t),a1o=i(N),rb=s(N,"LI",{});var Lye=n(rb);gre=s(Lye,"STRONG",{});var uQr=n(gre);s1o=r(uQr,"deberta-v2"),uQr.forEach(t),n1o=r(Lye," \u2014 "),G$=s(Lye,"A",{href:!0});var pQr=n(G$);l1o=r(pQr,"DebertaV2ForMaskedLM"),pQr.forEach(t),i1o=r(Lye," (DeBERTa-v2 model)"),Lye.forEach(t),d1o=i(N),tb=s(N,"LI",{});var Bye=n(tb);hre=s(Bye,"STRONG",{});var _Qr=n(hre);c1o=r(_Qr,"distilbert"),_Qr.forEach(t),m1o=r(Bye," \u2014 "),X$=s(Bye,"A",{href:!0});var bQr=n(X$);f1o=r(bQr,"DistilBertForMaskedLM"),bQr.forEach(t),g1o=r(Bye," (DistilBERT model)"),Bye.forEach(t),h1o=i(N),ab=s(N,"LI",{});var xye=n(ab);ure=s(xye,"STRONG",{});var vQr=n(ure);u1o=r(vQr,"electra"),vQr.forEach(t),p1o=r(xye," \u2014 "),V$=s(xye,"A",{href:!0});var TQr=n(V$);_1o=r(TQr,"ElectraForMaskedLM"),TQr.forEach(t),b1o=r(xye," (ELECTRA model)"),xye.forEach(t),v1o=i(N),sb=s(N,"LI",{});var kye=n(sb);pre=s(kye,"STRONG",{});var FQr=n(pre);T1o=r(FQr,"flaubert"),FQr.forEach(t),F1o=r(kye," \u2014 "),z$=s(kye,"A",{href:!0});var CQr=n(z$);C1o=r(CQr,"FlaubertWithLMHeadModel"),CQr.forEach(t),M1o=r(kye," (FlauBERT model)"),kye.forEach(t),E1o=i(N),nb=s(N,"LI",{});var Rye=n(nb);_re=s(Rye,"STRONG",{});var MQr=n(_re);y1o=r(MQr,"fnet"),MQr.forEach(t),w1o=r(Rye," \u2014 "),W$=s(Rye,"A",{href:!0});var EQr=n(W$);A1o=r(EQr,"FNetForMaskedLM"),EQr.forEach(t),L1o=r(Rye," (FNet model)"),Rye.forEach(t),B1o=i(N),lb=s(N,"LI",{});var Sye=n(lb);bre=s(Sye,"STRONG",{});var yQr=n(bre);x1o=r(yQr,"funnel"),yQr.forEach(t),k1o=r(Sye," \u2014 "),Q$=s(Sye,"A",{href:!0});var wQr=n(Q$);R1o=r(wQr,"FunnelForMaskedLM"),wQr.forEach(t),S1o=r(Sye," (Funnel Transformer model)"),Sye.forEach(t),P1o=i(N),ib=s(N,"LI",{});var Pye=n(ib);vre=s(Pye,"STRONG",{});var AQr=n(vre);$1o=r(AQr,"ibert"),AQr.forEach(t),I1o=r(Pye," \u2014 "),H$=s(Pye,"A",{href:!0});var LQr=n(H$);j1o=r(LQr,"IBertForMaskedLM"),LQr.forEach(t),N1o=r(Pye," (I-BERT model)"),Pye.forEach(t),D1o=i(N),db=s(N,"LI",{});var $ye=n(db);Tre=s($ye,"STRONG",{});var BQr=n(Tre);q1o=r(BQr,"layoutlm"),BQr.forEach(t),O1o=r($ye," \u2014 "),U$=s($ye,"A",{href:!0});var xQr=n(U$);G1o=r(xQr,"LayoutLMForMaskedLM"),xQr.forEach(t),X1o=r($ye," (LayoutLM model)"),$ye.forEach(t),V1o=i(N),cb=s(N,"LI",{});var Iye=n(cb);Fre=s(Iye,"STRONG",{});var kQr=n(Fre);z1o=r(kQr,"longformer"),kQr.forEach(t),W1o=r(Iye," \u2014 "),J$=s(Iye,"A",{href:!0});var RQr=n(J$);Q1o=r(RQr,"LongformerForMaskedLM"),RQr.forEach(t),H1o=r(Iye," (Longformer model)"),Iye.forEach(t),U1o=i(N),mb=s(N,"LI",{});var jye=n(mb);Cre=s(jye,"STRONG",{});var SQr=n(Cre);J1o=r(SQr,"mbart"),SQr.forEach(t),Y1o=r(jye," \u2014 "),Y$=s(jye,"A",{href:!0});var PQr=n(Y$);K1o=r(PQr,"MBartForConditionalGeneration"),PQr.forEach(t),Z1o=r(jye," (mBART model)"),jye.forEach(t),eFo=i(N),fb=s(N,"LI",{});var Nye=n(fb);Mre=s(Nye,"STRONG",{});var $Qr=n(Mre);oFo=r($Qr,"megatron-bert"),$Qr.forEach(t),rFo=r(Nye," \u2014 "),K$=s(Nye,"A",{href:!0});var IQr=n(K$);tFo=r(IQr,"MegatronBertForMaskedLM"),IQr.forEach(t),aFo=r(Nye," (MegatronBert model)"),Nye.forEach(t),sFo=i(N),gb=s(N,"LI",{});var Dye=n(gb);Ere=s(Dye,"STRONG",{});var jQr=n(Ere);nFo=r(jQr,"mobilebert"),jQr.forEach(t),lFo=r(Dye," \u2014 "),Z$=s(Dye,"A",{href:!0});var NQr=n(Z$);iFo=r(NQr,"MobileBertForMaskedLM"),NQr.forEach(t),dFo=r(Dye," (MobileBERT model)"),Dye.forEach(t),cFo=i(N),hb=s(N,"LI",{});var qye=n(hb);yre=s(qye,"STRONG",{});var DQr=n(yre);mFo=r(DQr,"mpnet"),DQr.forEach(t),fFo=r(qye," \u2014 "),eI=s(qye,"A",{href:!0});var qQr=n(eI);gFo=r(qQr,"MPNetForMaskedLM"),qQr.forEach(t),hFo=r(qye," (MPNet model)"),qye.forEach(t),uFo=i(N),ub=s(N,"LI",{});var Oye=n(ub);wre=s(Oye,"STRONG",{});var OQr=n(wre);pFo=r(OQr,"nystromformer"),OQr.forEach(t),_Fo=r(Oye," \u2014 "),oI=s(Oye,"A",{href:!0});var GQr=n(oI);bFo=r(GQr,"NystromformerForMaskedLM"),GQr.forEach(t),vFo=r(Oye," (Nystromformer model)"),Oye.forEach(t),TFo=i(N),pb=s(N,"LI",{});var Gye=n(pb);Are=s(Gye,"STRONG",{});var XQr=n(Are);FFo=r(XQr,"perceiver"),XQr.forEach(t),CFo=r(Gye," \u2014 "),rI=s(Gye,"A",{href:!0});var VQr=n(rI);MFo=r(VQr,"PerceiverForMaskedLM"),VQr.forEach(t),EFo=r(Gye," (Perceiver model)"),Gye.forEach(t),yFo=i(N),_b=s(N,"LI",{});var Xye=n(_b);Lre=s(Xye,"STRONG",{});var zQr=n(Lre);wFo=r(zQr,"qdqbert"),zQr.forEach(t),AFo=r(Xye," \u2014 "),tI=s(Xye,"A",{href:!0});var WQr=n(tI);LFo=r(WQr,"QDQBertForMaskedLM"),WQr.forEach(t),BFo=r(Xye," (QDQBert model)"),Xye.forEach(t),xFo=i(N),bb=s(N,"LI",{});var Vye=n(bb);Bre=s(Vye,"STRONG",{});var QQr=n(Bre);kFo=r(QQr,"reformer"),QQr.forEach(t),RFo=r(Vye," \u2014 "),aI=s(Vye,"A",{href:!0});var HQr=n(aI);SFo=r(HQr,"ReformerForMaskedLM"),HQr.forEach(t),PFo=r(Vye," (Reformer model)"),Vye.forEach(t),$Fo=i(N),vb=s(N,"LI",{});var zye=n(vb);xre=s(zye,"STRONG",{});var UQr=n(xre);IFo=r(UQr,"rembert"),UQr.forEach(t),jFo=r(zye," \u2014 "),sI=s(zye,"A",{href:!0});var JQr=n(sI);NFo=r(JQr,"RemBertForMaskedLM"),JQr.forEach(t),DFo=r(zye," (RemBERT model)"),zye.forEach(t),qFo=i(N),Tb=s(N,"LI",{});var Wye=n(Tb);kre=s(Wye,"STRONG",{});var YQr=n(kre);OFo=r(YQr,"roberta"),YQr.forEach(t),GFo=r(Wye," \u2014 "),nI=s(Wye,"A",{href:!0});var KQr=n(nI);XFo=r(KQr,"RobertaForMaskedLM"),KQr.forEach(t),VFo=r(Wye," (RoBERTa model)"),Wye.forEach(t),zFo=i(N),Fb=s(N,"LI",{});var Qye=n(Fb);Rre=s(Qye,"STRONG",{});var ZQr=n(Rre);WFo=r(ZQr,"roformer"),ZQr.forEach(t),QFo=r(Qye," \u2014 "),lI=s(Qye,"A",{href:!0});var eHr=n(lI);HFo=r(eHr,"RoFormerForMaskedLM"),eHr.forEach(t),UFo=r(Qye," (RoFormer model)"),Qye.forEach(t),JFo=i(N),Cb=s(N,"LI",{});var Hye=n(Cb);Sre=s(Hye,"STRONG",{});var oHr=n(Sre);YFo=r(oHr,"squeezebert"),oHr.forEach(t),KFo=r(Hye," \u2014 "),iI=s(Hye,"A",{href:!0});var rHr=n(iI);ZFo=r(rHr,"SqueezeBertForMaskedLM"),rHr.forEach(t),eCo=r(Hye," (SqueezeBERT model)"),Hye.forEach(t),oCo=i(N),Mb=s(N,"LI",{});var Uye=n(Mb);Pre=s(Uye,"STRONG",{});var tHr=n(Pre);rCo=r(tHr,"tapas"),tHr.forEach(t),tCo=r(Uye," \u2014 "),dI=s(Uye,"A",{href:!0});var aHr=n(dI);aCo=r(aHr,"TapasForMaskedLM"),aHr.forEach(t),sCo=r(Uye," (TAPAS model)"),Uye.forEach(t),nCo=i(N),Eb=s(N,"LI",{});var Jye=n(Eb);$re=s(Jye,"STRONG",{});var sHr=n($re);lCo=r(sHr,"wav2vec2"),sHr.forEach(t),iCo=r(Jye," \u2014 "),Ire=s(Jye,"CODE",{});var nHr=n(Ire);dCo=r(nHr,"Wav2Vec2ForMaskedLM"),nHr.forEach(t),cCo=r(Jye,"(Wav2Vec2 model)"),Jye.forEach(t),mCo=i(N),yb=s(N,"LI",{});var Yye=n(yb);jre=s(Yye,"STRONG",{});var lHr=n(jre);fCo=r(lHr,"xlm"),lHr.forEach(t),gCo=r(Yye," \u2014 "),cI=s(Yye,"A",{href:!0});var iHr=n(cI);hCo=r(iHr,"XLMWithLMHeadModel"),iHr.forEach(t),uCo=r(Yye," (XLM model)"),Yye.forEach(t),pCo=i(N),wb=s(N,"LI",{});var Kye=n(wb);Nre=s(Kye,"STRONG",{});var dHr=n(Nre);_Co=r(dHr,"xlm-roberta"),dHr.forEach(t),bCo=r(Kye," \u2014 "),mI=s(Kye,"A",{href:!0});var cHr=n(mI);vCo=r(cHr,"XLMRobertaForMaskedLM"),cHr.forEach(t),TCo=r(Kye," (XLM-RoBERTa model)"),Kye.forEach(t),FCo=i(N),Ab=s(N,"LI",{});var Zye=n(Ab);Dre=s(Zye,"STRONG",{});var mHr=n(Dre);CCo=r(mHr,"xlm-roberta-xl"),mHr.forEach(t),MCo=r(Zye," \u2014 "),fI=s(Zye,"A",{href:!0});var fHr=n(fI);ECo=r(fHr,"XLMRobertaXLForMaskedLM"),fHr.forEach(t),yCo=r(Zye," (XLM-RoBERTa-XL model)"),Zye.forEach(t),wCo=i(N),Lb=s(N,"LI",{});var ewe=n(Lb);qre=s(ewe,"STRONG",{});var gHr=n(qre);ACo=r(gHr,"yoso"),gHr.forEach(t),LCo=r(ewe," \u2014 "),gI=s(ewe,"A",{href:!0});var hHr=n(gI);BCo=r(hHr,"YosoForMaskedLM"),hHr.forEach(t),xCo=r(ewe," (YOSO model)"),ewe.forEach(t),N.forEach(t),kCo=i(Xt),Bb=s(Xt,"P",{});var owe=n(Bb);RCo=r(owe,"The model is set in evaluation mode by default using "),Ore=s(owe,"CODE",{});var uHr=n(Ore);SCo=r(uHr,"model.eval()"),uHr.forEach(t),PCo=r(owe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gre=s(owe,"CODE",{});var pHr=n(Gre);$Co=r(pHr,"model.train()"),pHr.forEach(t),owe.forEach(t),ICo=i(Xt),Xre=s(Xt,"P",{});var _Hr=n(Xre);jCo=r(_Hr,"Examples:"),_Hr.forEach(t),NCo=i(Xt),f(iw.$$.fragment,Xt),Xt.forEach(t),Kn.forEach(t),ske=i(c),dd=s(c,"H2",{class:!0});var hSe=n(dd);xb=s(hSe,"A",{id:!0,class:!0,href:!0});var bHr=n(xb);Vre=s(bHr,"SPAN",{});var vHr=n(Vre);f(dw.$$.fragment,vHr),vHr.forEach(t),bHr.forEach(t),DCo=i(hSe),zre=s(hSe,"SPAN",{});var THr=n(zre);qCo=r(THr,"AutoModelForSeq2SeqLM"),THr.forEach(t),hSe.forEach(t),nke=i(c),er=s(c,"DIV",{class:!0});var el=n(er);f(cw.$$.fragment,el),OCo=i(el),cd=s(el,"P",{});var Uz=n(cd);GCo=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wre=s(Uz,"CODE",{});var FHr=n(Wre);XCo=r(FHr,"from_pretrained()"),FHr.forEach(t),VCo=r(Uz,"class method or the "),Qre=s(Uz,"CODE",{});var CHr=n(Qre);zCo=r(CHr,"from_config()"),CHr.forEach(t),WCo=r(Uz,`class
method.`),Uz.forEach(t),QCo=i(el),mw=s(el,"P",{});var uSe=n(mw);HCo=r(uSe,"This class cannot be instantiated directly using "),Hre=s(uSe,"CODE",{});var MHr=n(Hre);UCo=r(MHr,"__init__()"),MHr.forEach(t),JCo=r(uSe," (throws an error)."),uSe.forEach(t),YCo=i(el),Hr=s(el,"DIV",{class:!0});var ol=n(Hr);f(fw.$$.fragment,ol),KCo=i(ol),Ure=s(ol,"P",{});var EHr=n(Ure);ZCo=r(EHr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),EHr.forEach(t),eMo=i(ol),md=s(ol,"P",{});var Jz=n(md);oMo=r(Jz,`Note:
Loading a model from its configuration file does `),Jre=s(Jz,"STRONG",{});var yHr=n(Jre);rMo=r(yHr,"not"),yHr.forEach(t),tMo=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yre=s(Jz,"CODE",{});var wHr=n(Yre);aMo=r(wHr,"from_pretrained()"),wHr.forEach(t),sMo=r(Jz,"to load the model weights."),Jz.forEach(t),nMo=i(ol),Kre=s(ol,"P",{});var AHr=n(Kre);lMo=r(AHr,"Examples:"),AHr.forEach(t),iMo=i(ol),f(gw.$$.fragment,ol),ol.forEach(t),dMo=i(el),Ge=s(el,"DIV",{class:!0});var Vt=n(Ge);f(hw.$$.fragment,Vt),cMo=i(Vt),Zre=s(Vt,"P",{});var LHr=n(Zre);mMo=r(LHr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),LHr.forEach(t),fMo=i(Vt),Ha=s(Vt,"P",{});var d5=n(Ha);gMo=r(d5,"The model class to instantiate is selected based on the "),ete=s(d5,"CODE",{});var BHr=n(ete);hMo=r(BHr,"model_type"),BHr.forEach(t),uMo=r(d5,` property of the config object (either
passed as an argument or loaded from `),ote=s(d5,"CODE",{});var xHr=n(ote);pMo=r(xHr,"pretrained_model_name_or_path"),xHr.forEach(t),_Mo=r(d5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rte=s(d5,"CODE",{});var kHr=n(rte);bMo=r(kHr,"pretrained_model_name_or_path"),kHr.forEach(t),vMo=r(d5,":"),d5.forEach(t),TMo=i(Vt),ne=s(Vt,"UL",{});var ie=n(ne);kb=s(ie,"LI",{});var rwe=n(kb);tte=s(rwe,"STRONG",{});var RHr=n(tte);FMo=r(RHr,"bart"),RHr.forEach(t),CMo=r(rwe," \u2014 "),hI=s(rwe,"A",{href:!0});var SHr=n(hI);MMo=r(SHr,"BartForConditionalGeneration"),SHr.forEach(t),EMo=r(rwe," (BART model)"),rwe.forEach(t),yMo=i(ie),Rb=s(ie,"LI",{});var twe=n(Rb);ate=s(twe,"STRONG",{});var PHr=n(ate);wMo=r(PHr,"bigbird_pegasus"),PHr.forEach(t),AMo=r(twe," \u2014 "),uI=s(twe,"A",{href:!0});var $Hr=n(uI);LMo=r($Hr,"BigBirdPegasusForConditionalGeneration"),$Hr.forEach(t),BMo=r(twe," (BigBirdPegasus model)"),twe.forEach(t),xMo=i(ie),Sb=s(ie,"LI",{});var awe=n(Sb);ste=s(awe,"STRONG",{});var IHr=n(ste);kMo=r(IHr,"blenderbot"),IHr.forEach(t),RMo=r(awe," \u2014 "),pI=s(awe,"A",{href:!0});var jHr=n(pI);SMo=r(jHr,"BlenderbotForConditionalGeneration"),jHr.forEach(t),PMo=r(awe," (Blenderbot model)"),awe.forEach(t),$Mo=i(ie),Pb=s(ie,"LI",{});var swe=n(Pb);nte=s(swe,"STRONG",{});var NHr=n(nte);IMo=r(NHr,"blenderbot-small"),NHr.forEach(t),jMo=r(swe," \u2014 "),_I=s(swe,"A",{href:!0});var DHr=n(_I);NMo=r(DHr,"BlenderbotSmallForConditionalGeneration"),DHr.forEach(t),DMo=r(swe," (BlenderbotSmall model)"),swe.forEach(t),qMo=i(ie),$b=s(ie,"LI",{});var nwe=n($b);lte=s(nwe,"STRONG",{});var qHr=n(lte);OMo=r(qHr,"encoder-decoder"),qHr.forEach(t),GMo=r(nwe," \u2014 "),bI=s(nwe,"A",{href:!0});var OHr=n(bI);XMo=r(OHr,"EncoderDecoderModel"),OHr.forEach(t),VMo=r(nwe," (Encoder decoder model)"),nwe.forEach(t),zMo=i(ie),Ib=s(ie,"LI",{});var lwe=n(Ib);ite=s(lwe,"STRONG",{});var GHr=n(ite);WMo=r(GHr,"fsmt"),GHr.forEach(t),QMo=r(lwe," \u2014 "),vI=s(lwe,"A",{href:!0});var XHr=n(vI);HMo=r(XHr,"FSMTForConditionalGeneration"),XHr.forEach(t),UMo=r(lwe," (FairSeq Machine-Translation model)"),lwe.forEach(t),JMo=i(ie),jb=s(ie,"LI",{});var iwe=n(jb);dte=s(iwe,"STRONG",{});var VHr=n(dte);YMo=r(VHr,"led"),VHr.forEach(t),KMo=r(iwe," \u2014 "),TI=s(iwe,"A",{href:!0});var zHr=n(TI);ZMo=r(zHr,"LEDForConditionalGeneration"),zHr.forEach(t),e4o=r(iwe," (LED model)"),iwe.forEach(t),o4o=i(ie),Nb=s(ie,"LI",{});var dwe=n(Nb);cte=s(dwe,"STRONG",{});var WHr=n(cte);r4o=r(WHr,"m2m_100"),WHr.forEach(t),t4o=r(dwe," \u2014 "),FI=s(dwe,"A",{href:!0});var QHr=n(FI);a4o=r(QHr,"M2M100ForConditionalGeneration"),QHr.forEach(t),s4o=r(dwe," (M2M100 model)"),dwe.forEach(t),n4o=i(ie),Db=s(ie,"LI",{});var cwe=n(Db);mte=s(cwe,"STRONG",{});var HHr=n(mte);l4o=r(HHr,"marian"),HHr.forEach(t),i4o=r(cwe," \u2014 "),CI=s(cwe,"A",{href:!0});var UHr=n(CI);d4o=r(UHr,"MarianMTModel"),UHr.forEach(t),c4o=r(cwe," (Marian model)"),cwe.forEach(t),m4o=i(ie),qb=s(ie,"LI",{});var mwe=n(qb);fte=s(mwe,"STRONG",{});var JHr=n(fte);f4o=r(JHr,"mbart"),JHr.forEach(t),g4o=r(mwe," \u2014 "),MI=s(mwe,"A",{href:!0});var YHr=n(MI);h4o=r(YHr,"MBartForConditionalGeneration"),YHr.forEach(t),u4o=r(mwe," (mBART model)"),mwe.forEach(t),p4o=i(ie),Ob=s(ie,"LI",{});var fwe=n(Ob);gte=s(fwe,"STRONG",{});var KHr=n(gte);_4o=r(KHr,"mt5"),KHr.forEach(t),b4o=r(fwe," \u2014 "),EI=s(fwe,"A",{href:!0});var ZHr=n(EI);v4o=r(ZHr,"MT5ForConditionalGeneration"),ZHr.forEach(t),T4o=r(fwe," (mT5 model)"),fwe.forEach(t),F4o=i(ie),Gb=s(ie,"LI",{});var gwe=n(Gb);hte=s(gwe,"STRONG",{});var eUr=n(hte);C4o=r(eUr,"pegasus"),eUr.forEach(t),M4o=r(gwe," \u2014 "),yI=s(gwe,"A",{href:!0});var oUr=n(yI);E4o=r(oUr,"PegasusForConditionalGeneration"),oUr.forEach(t),y4o=r(gwe," (Pegasus model)"),gwe.forEach(t),w4o=i(ie),Xb=s(ie,"LI",{});var hwe=n(Xb);ute=s(hwe,"STRONG",{});var rUr=n(ute);A4o=r(rUr,"plbart"),rUr.forEach(t),L4o=r(hwe," \u2014 "),wI=s(hwe,"A",{href:!0});var tUr=n(wI);B4o=r(tUr,"PLBartForConditionalGeneration"),tUr.forEach(t),x4o=r(hwe," (PLBart model)"),hwe.forEach(t),k4o=i(ie),Vb=s(ie,"LI",{});var uwe=n(Vb);pte=s(uwe,"STRONG",{});var aUr=n(pte);R4o=r(aUr,"prophetnet"),aUr.forEach(t),S4o=r(uwe," \u2014 "),AI=s(uwe,"A",{href:!0});var sUr=n(AI);P4o=r(sUr,"ProphetNetForConditionalGeneration"),sUr.forEach(t),$4o=r(uwe," (ProphetNet model)"),uwe.forEach(t),I4o=i(ie),zb=s(ie,"LI",{});var pwe=n(zb);_te=s(pwe,"STRONG",{});var nUr=n(_te);j4o=r(nUr,"t5"),nUr.forEach(t),N4o=r(pwe," \u2014 "),LI=s(pwe,"A",{href:!0});var lUr=n(LI);D4o=r(lUr,"T5ForConditionalGeneration"),lUr.forEach(t),q4o=r(pwe," (T5 model)"),pwe.forEach(t),O4o=i(ie),Wb=s(ie,"LI",{});var _we=n(Wb);bte=s(_we,"STRONG",{});var iUr=n(bte);G4o=r(iUr,"xlm-prophetnet"),iUr.forEach(t),X4o=r(_we," \u2014 "),BI=s(_we,"A",{href:!0});var dUr=n(BI);V4o=r(dUr,"XLMProphetNetForConditionalGeneration"),dUr.forEach(t),z4o=r(_we," (XLMProphetNet model)"),_we.forEach(t),ie.forEach(t),W4o=i(Vt),Qb=s(Vt,"P",{});var bwe=n(Qb);Q4o=r(bwe,"The model is set in evaluation mode by default using "),vte=s(bwe,"CODE",{});var cUr=n(vte);H4o=r(cUr,"model.eval()"),cUr.forEach(t),U4o=r(bwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tte=s(bwe,"CODE",{});var mUr=n(Tte);J4o=r(mUr,"model.train()"),mUr.forEach(t),bwe.forEach(t),Y4o=i(Vt),Fte=s(Vt,"P",{});var fUr=n(Fte);K4o=r(fUr,"Examples:"),fUr.forEach(t),Z4o=i(Vt),f(uw.$$.fragment,Vt),Vt.forEach(t),el.forEach(t),lke=i(c),fd=s(c,"H2",{class:!0});var pSe=n(fd);Hb=s(pSe,"A",{id:!0,class:!0,href:!0});var gUr=n(Hb);Cte=s(gUr,"SPAN",{});var hUr=n(Cte);f(pw.$$.fragment,hUr),hUr.forEach(t),gUr.forEach(t),eEo=i(pSe),Mte=s(pSe,"SPAN",{});var uUr=n(Mte);oEo=r(uUr,"AutoModelForSequenceClassification"),uUr.forEach(t),pSe.forEach(t),ike=i(c),or=s(c,"DIV",{class:!0});var rl=n(or);f(_w.$$.fragment,rl),rEo=i(rl),gd=s(rl,"P",{});var Yz=n(gd);tEo=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Ete=s(Yz,"CODE",{});var pUr=n(Ete);aEo=r(pUr,"from_pretrained()"),pUr.forEach(t),sEo=r(Yz,"class method or the "),yte=s(Yz,"CODE",{});var _Ur=n(yte);nEo=r(_Ur,"from_config()"),_Ur.forEach(t),lEo=r(Yz,`class
method.`),Yz.forEach(t),iEo=i(rl),bw=s(rl,"P",{});var _Se=n(bw);dEo=r(_Se,"This class cannot be instantiated directly using "),wte=s(_Se,"CODE",{});var bUr=n(wte);cEo=r(bUr,"__init__()"),bUr.forEach(t),mEo=r(_Se," (throws an error)."),_Se.forEach(t),fEo=i(rl),Ur=s(rl,"DIV",{class:!0});var tl=n(Ur);f(vw.$$.fragment,tl),gEo=i(tl),Ate=s(tl,"P",{});var vUr=n(Ate);hEo=r(vUr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),vUr.forEach(t),uEo=i(tl),hd=s(tl,"P",{});var Kz=n(hd);pEo=r(Kz,`Note:
Loading a model from its configuration file does `),Lte=s(Kz,"STRONG",{});var TUr=n(Lte);_Eo=r(TUr,"not"),TUr.forEach(t),bEo=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bte=s(Kz,"CODE",{});var FUr=n(Bte);vEo=r(FUr,"from_pretrained()"),FUr.forEach(t),TEo=r(Kz,"to load the model weights."),Kz.forEach(t),FEo=i(tl),xte=s(tl,"P",{});var CUr=n(xte);CEo=r(CUr,"Examples:"),CUr.forEach(t),MEo=i(tl),f(Tw.$$.fragment,tl),tl.forEach(t),EEo=i(rl),Xe=s(rl,"DIV",{class:!0});var zt=n(Xe);f(Fw.$$.fragment,zt),yEo=i(zt),kte=s(zt,"P",{});var MUr=n(kte);wEo=r(MUr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),MUr.forEach(t),AEo=i(zt),Ua=s(zt,"P",{});var c5=n(Ua);LEo=r(c5,"The model class to instantiate is selected based on the "),Rte=s(c5,"CODE",{});var EUr=n(Rte);BEo=r(EUr,"model_type"),EUr.forEach(t),xEo=r(c5,` property of the config object (either
passed as an argument or loaded from `),Ste=s(c5,"CODE",{});var yUr=n(Ste);kEo=r(yUr,"pretrained_model_name_or_path"),yUr.forEach(t),REo=r(c5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pte=s(c5,"CODE",{});var wUr=n(Pte);SEo=r(wUr,"pretrained_model_name_or_path"),wUr.forEach(t),PEo=r(c5,":"),c5.forEach(t),$Eo=i(zt),A=s(zt,"UL",{});var L=n(A);Ub=s(L,"LI",{});var vwe=n(Ub);$te=s(vwe,"STRONG",{});var AUr=n($te);IEo=r(AUr,"albert"),AUr.forEach(t),jEo=r(vwe," \u2014 "),xI=s(vwe,"A",{href:!0});var LUr=n(xI);NEo=r(LUr,"AlbertForSequenceClassification"),LUr.forEach(t),DEo=r(vwe," (ALBERT model)"),vwe.forEach(t),qEo=i(L),Jb=s(L,"LI",{});var Twe=n(Jb);Ite=s(Twe,"STRONG",{});var BUr=n(Ite);OEo=r(BUr,"bart"),BUr.forEach(t),GEo=r(Twe," \u2014 "),kI=s(Twe,"A",{href:!0});var xUr=n(kI);XEo=r(xUr,"BartForSequenceClassification"),xUr.forEach(t),VEo=r(Twe," (BART model)"),Twe.forEach(t),zEo=i(L),Yb=s(L,"LI",{});var Fwe=n(Yb);jte=s(Fwe,"STRONG",{});var kUr=n(jte);WEo=r(kUr,"bert"),kUr.forEach(t),QEo=r(Fwe," \u2014 "),RI=s(Fwe,"A",{href:!0});var RUr=n(RI);HEo=r(RUr,"BertForSequenceClassification"),RUr.forEach(t),UEo=r(Fwe," (BERT model)"),Fwe.forEach(t),JEo=i(L),Kb=s(L,"LI",{});var Cwe=n(Kb);Nte=s(Cwe,"STRONG",{});var SUr=n(Nte);YEo=r(SUr,"big_bird"),SUr.forEach(t),KEo=r(Cwe," \u2014 "),SI=s(Cwe,"A",{href:!0});var PUr=n(SI);ZEo=r(PUr,"BigBirdForSequenceClassification"),PUr.forEach(t),e3o=r(Cwe," (BigBird model)"),Cwe.forEach(t),o3o=i(L),Zb=s(L,"LI",{});var Mwe=n(Zb);Dte=s(Mwe,"STRONG",{});var $Ur=n(Dte);r3o=r($Ur,"bigbird_pegasus"),$Ur.forEach(t),t3o=r(Mwe," \u2014 "),PI=s(Mwe,"A",{href:!0});var IUr=n(PI);a3o=r(IUr,"BigBirdPegasusForSequenceClassification"),IUr.forEach(t),s3o=r(Mwe," (BigBirdPegasus model)"),Mwe.forEach(t),n3o=i(L),e2=s(L,"LI",{});var Ewe=n(e2);qte=s(Ewe,"STRONG",{});var jUr=n(qte);l3o=r(jUr,"camembert"),jUr.forEach(t),i3o=r(Ewe," \u2014 "),$I=s(Ewe,"A",{href:!0});var NUr=n($I);d3o=r(NUr,"CamembertForSequenceClassification"),NUr.forEach(t),c3o=r(Ewe," (CamemBERT model)"),Ewe.forEach(t),m3o=i(L),o2=s(L,"LI",{});var ywe=n(o2);Ote=s(ywe,"STRONG",{});var DUr=n(Ote);f3o=r(DUr,"canine"),DUr.forEach(t),g3o=r(ywe," \u2014 "),II=s(ywe,"A",{href:!0});var qUr=n(II);h3o=r(qUr,"CanineForSequenceClassification"),qUr.forEach(t),u3o=r(ywe," (Canine model)"),ywe.forEach(t),p3o=i(L),r2=s(L,"LI",{});var wwe=n(r2);Gte=s(wwe,"STRONG",{});var OUr=n(Gte);_3o=r(OUr,"convbert"),OUr.forEach(t),b3o=r(wwe," \u2014 "),jI=s(wwe,"A",{href:!0});var GUr=n(jI);v3o=r(GUr,"ConvBertForSequenceClassification"),GUr.forEach(t),T3o=r(wwe," (ConvBERT model)"),wwe.forEach(t),F3o=i(L),t2=s(L,"LI",{});var Awe=n(t2);Xte=s(Awe,"STRONG",{});var XUr=n(Xte);C3o=r(XUr,"ctrl"),XUr.forEach(t),M3o=r(Awe," \u2014 "),NI=s(Awe,"A",{href:!0});var VUr=n(NI);E3o=r(VUr,"CTRLForSequenceClassification"),VUr.forEach(t),y3o=r(Awe," (CTRL model)"),Awe.forEach(t),w3o=i(L),a2=s(L,"LI",{});var Lwe=n(a2);Vte=s(Lwe,"STRONG",{});var zUr=n(Vte);A3o=r(zUr,"data2vec-text"),zUr.forEach(t),L3o=r(Lwe," \u2014 "),DI=s(Lwe,"A",{href:!0});var WUr=n(DI);B3o=r(WUr,"Data2VecTextForSequenceClassification"),WUr.forEach(t),x3o=r(Lwe," (Data2VecText model)"),Lwe.forEach(t),k3o=i(L),s2=s(L,"LI",{});var Bwe=n(s2);zte=s(Bwe,"STRONG",{});var QUr=n(zte);R3o=r(QUr,"deberta"),QUr.forEach(t),S3o=r(Bwe," \u2014 "),qI=s(Bwe,"A",{href:!0});var HUr=n(qI);P3o=r(HUr,"DebertaForSequenceClassification"),HUr.forEach(t),$3o=r(Bwe," (DeBERTa model)"),Bwe.forEach(t),I3o=i(L),n2=s(L,"LI",{});var xwe=n(n2);Wte=s(xwe,"STRONG",{});var UUr=n(Wte);j3o=r(UUr,"deberta-v2"),UUr.forEach(t),N3o=r(xwe," \u2014 "),OI=s(xwe,"A",{href:!0});var JUr=n(OI);D3o=r(JUr,"DebertaV2ForSequenceClassification"),JUr.forEach(t),q3o=r(xwe," (DeBERTa-v2 model)"),xwe.forEach(t),O3o=i(L),l2=s(L,"LI",{});var kwe=n(l2);Qte=s(kwe,"STRONG",{});var YUr=n(Qte);G3o=r(YUr,"distilbert"),YUr.forEach(t),X3o=r(kwe," \u2014 "),GI=s(kwe,"A",{href:!0});var KUr=n(GI);V3o=r(KUr,"DistilBertForSequenceClassification"),KUr.forEach(t),z3o=r(kwe," (DistilBERT model)"),kwe.forEach(t),W3o=i(L),i2=s(L,"LI",{});var Rwe=n(i2);Hte=s(Rwe,"STRONG",{});var ZUr=n(Hte);Q3o=r(ZUr,"electra"),ZUr.forEach(t),H3o=r(Rwe," \u2014 "),XI=s(Rwe,"A",{href:!0});var eJr=n(XI);U3o=r(eJr,"ElectraForSequenceClassification"),eJr.forEach(t),J3o=r(Rwe," (ELECTRA model)"),Rwe.forEach(t),Y3o=i(L),d2=s(L,"LI",{});var Swe=n(d2);Ute=s(Swe,"STRONG",{});var oJr=n(Ute);K3o=r(oJr,"flaubert"),oJr.forEach(t),Z3o=r(Swe," \u2014 "),VI=s(Swe,"A",{href:!0});var rJr=n(VI);e5o=r(rJr,"FlaubertForSequenceClassification"),rJr.forEach(t),o5o=r(Swe," (FlauBERT model)"),Swe.forEach(t),r5o=i(L),c2=s(L,"LI",{});var Pwe=n(c2);Jte=s(Pwe,"STRONG",{});var tJr=n(Jte);t5o=r(tJr,"fnet"),tJr.forEach(t),a5o=r(Pwe," \u2014 "),zI=s(Pwe,"A",{href:!0});var aJr=n(zI);s5o=r(aJr,"FNetForSequenceClassification"),aJr.forEach(t),n5o=r(Pwe," (FNet model)"),Pwe.forEach(t),l5o=i(L),m2=s(L,"LI",{});var $we=n(m2);Yte=s($we,"STRONG",{});var sJr=n(Yte);i5o=r(sJr,"funnel"),sJr.forEach(t),d5o=r($we," \u2014 "),WI=s($we,"A",{href:!0});var nJr=n(WI);c5o=r(nJr,"FunnelForSequenceClassification"),nJr.forEach(t),m5o=r($we," (Funnel Transformer model)"),$we.forEach(t),f5o=i(L),f2=s(L,"LI",{});var Iwe=n(f2);Kte=s(Iwe,"STRONG",{});var lJr=n(Kte);g5o=r(lJr,"gpt2"),lJr.forEach(t),h5o=r(Iwe," \u2014 "),QI=s(Iwe,"A",{href:!0});var iJr=n(QI);u5o=r(iJr,"GPT2ForSequenceClassification"),iJr.forEach(t),p5o=r(Iwe," (OpenAI GPT-2 model)"),Iwe.forEach(t),_5o=i(L),g2=s(L,"LI",{});var jwe=n(g2);Zte=s(jwe,"STRONG",{});var dJr=n(Zte);b5o=r(dJr,"gpt_neo"),dJr.forEach(t),v5o=r(jwe," \u2014 "),HI=s(jwe,"A",{href:!0});var cJr=n(HI);T5o=r(cJr,"GPTNeoForSequenceClassification"),cJr.forEach(t),F5o=r(jwe," (GPT Neo model)"),jwe.forEach(t),C5o=i(L),h2=s(L,"LI",{});var Nwe=n(h2);eae=s(Nwe,"STRONG",{});var mJr=n(eae);M5o=r(mJr,"gptj"),mJr.forEach(t),E5o=r(Nwe," \u2014 "),UI=s(Nwe,"A",{href:!0});var fJr=n(UI);y5o=r(fJr,"GPTJForSequenceClassification"),fJr.forEach(t),w5o=r(Nwe," (GPT-J model)"),Nwe.forEach(t),A5o=i(L),u2=s(L,"LI",{});var Dwe=n(u2);oae=s(Dwe,"STRONG",{});var gJr=n(oae);L5o=r(gJr,"ibert"),gJr.forEach(t),B5o=r(Dwe," \u2014 "),JI=s(Dwe,"A",{href:!0});var hJr=n(JI);x5o=r(hJr,"IBertForSequenceClassification"),hJr.forEach(t),k5o=r(Dwe," (I-BERT model)"),Dwe.forEach(t),R5o=i(L),p2=s(L,"LI",{});var qwe=n(p2);rae=s(qwe,"STRONG",{});var uJr=n(rae);S5o=r(uJr,"layoutlm"),uJr.forEach(t),P5o=r(qwe," \u2014 "),YI=s(qwe,"A",{href:!0});var pJr=n(YI);$5o=r(pJr,"LayoutLMForSequenceClassification"),pJr.forEach(t),I5o=r(qwe," (LayoutLM model)"),qwe.forEach(t),j5o=i(L),_2=s(L,"LI",{});var Owe=n(_2);tae=s(Owe,"STRONG",{});var _Jr=n(tae);N5o=r(_Jr,"layoutlmv2"),_Jr.forEach(t),D5o=r(Owe," \u2014 "),KI=s(Owe,"A",{href:!0});var bJr=n(KI);q5o=r(bJr,"LayoutLMv2ForSequenceClassification"),bJr.forEach(t),O5o=r(Owe," (LayoutLMv2 model)"),Owe.forEach(t),G5o=i(L),b2=s(L,"LI",{});var Gwe=n(b2);aae=s(Gwe,"STRONG",{});var vJr=n(aae);X5o=r(vJr,"led"),vJr.forEach(t),V5o=r(Gwe," \u2014 "),ZI=s(Gwe,"A",{href:!0});var TJr=n(ZI);z5o=r(TJr,"LEDForSequenceClassification"),TJr.forEach(t),W5o=r(Gwe," (LED model)"),Gwe.forEach(t),Q5o=i(L),v2=s(L,"LI",{});var Xwe=n(v2);sae=s(Xwe,"STRONG",{});var FJr=n(sae);H5o=r(FJr,"longformer"),FJr.forEach(t),U5o=r(Xwe," \u2014 "),ej=s(Xwe,"A",{href:!0});var CJr=n(ej);J5o=r(CJr,"LongformerForSequenceClassification"),CJr.forEach(t),Y5o=r(Xwe," (Longformer model)"),Xwe.forEach(t),K5o=i(L),T2=s(L,"LI",{});var Vwe=n(T2);nae=s(Vwe,"STRONG",{});var MJr=n(nae);Z5o=r(MJr,"mbart"),MJr.forEach(t),eyo=r(Vwe," \u2014 "),oj=s(Vwe,"A",{href:!0});var EJr=n(oj);oyo=r(EJr,"MBartForSequenceClassification"),EJr.forEach(t),ryo=r(Vwe," (mBART model)"),Vwe.forEach(t),tyo=i(L),F2=s(L,"LI",{});var zwe=n(F2);lae=s(zwe,"STRONG",{});var yJr=n(lae);ayo=r(yJr,"megatron-bert"),yJr.forEach(t),syo=r(zwe," \u2014 "),rj=s(zwe,"A",{href:!0});var wJr=n(rj);nyo=r(wJr,"MegatronBertForSequenceClassification"),wJr.forEach(t),lyo=r(zwe," (MegatronBert model)"),zwe.forEach(t),iyo=i(L),C2=s(L,"LI",{});var Wwe=n(C2);iae=s(Wwe,"STRONG",{});var AJr=n(iae);dyo=r(AJr,"mobilebert"),AJr.forEach(t),cyo=r(Wwe," \u2014 "),tj=s(Wwe,"A",{href:!0});var LJr=n(tj);myo=r(LJr,"MobileBertForSequenceClassification"),LJr.forEach(t),fyo=r(Wwe," (MobileBERT model)"),Wwe.forEach(t),gyo=i(L),M2=s(L,"LI",{});var Qwe=n(M2);dae=s(Qwe,"STRONG",{});var BJr=n(dae);hyo=r(BJr,"mpnet"),BJr.forEach(t),uyo=r(Qwe," \u2014 "),aj=s(Qwe,"A",{href:!0});var xJr=n(aj);pyo=r(xJr,"MPNetForSequenceClassification"),xJr.forEach(t),_yo=r(Qwe," (MPNet model)"),Qwe.forEach(t),byo=i(L),E2=s(L,"LI",{});var Hwe=n(E2);cae=s(Hwe,"STRONG",{});var kJr=n(cae);vyo=r(kJr,"nystromformer"),kJr.forEach(t),Tyo=r(Hwe," \u2014 "),sj=s(Hwe,"A",{href:!0});var RJr=n(sj);Fyo=r(RJr,"NystromformerForSequenceClassification"),RJr.forEach(t),Cyo=r(Hwe," (Nystromformer model)"),Hwe.forEach(t),Myo=i(L),y2=s(L,"LI",{});var Uwe=n(y2);mae=s(Uwe,"STRONG",{});var SJr=n(mae);Eyo=r(SJr,"openai-gpt"),SJr.forEach(t),yyo=r(Uwe," \u2014 "),nj=s(Uwe,"A",{href:!0});var PJr=n(nj);wyo=r(PJr,"OpenAIGPTForSequenceClassification"),PJr.forEach(t),Ayo=r(Uwe," (OpenAI GPT model)"),Uwe.forEach(t),Lyo=i(L),w2=s(L,"LI",{});var Jwe=n(w2);fae=s(Jwe,"STRONG",{});var $Jr=n(fae);Byo=r($Jr,"perceiver"),$Jr.forEach(t),xyo=r(Jwe," \u2014 "),lj=s(Jwe,"A",{href:!0});var IJr=n(lj);kyo=r(IJr,"PerceiverForSequenceClassification"),IJr.forEach(t),Ryo=r(Jwe," (Perceiver model)"),Jwe.forEach(t),Syo=i(L),A2=s(L,"LI",{});var Ywe=n(A2);gae=s(Ywe,"STRONG",{});var jJr=n(gae);Pyo=r(jJr,"plbart"),jJr.forEach(t),$yo=r(Ywe," \u2014 "),ij=s(Ywe,"A",{href:!0});var NJr=n(ij);Iyo=r(NJr,"PLBartForSequenceClassification"),NJr.forEach(t),jyo=r(Ywe," (PLBart model)"),Ywe.forEach(t),Nyo=i(L),L2=s(L,"LI",{});var Kwe=n(L2);hae=s(Kwe,"STRONG",{});var DJr=n(hae);Dyo=r(DJr,"qdqbert"),DJr.forEach(t),qyo=r(Kwe," \u2014 "),dj=s(Kwe,"A",{href:!0});var qJr=n(dj);Oyo=r(qJr,"QDQBertForSequenceClassification"),qJr.forEach(t),Gyo=r(Kwe," (QDQBert model)"),Kwe.forEach(t),Xyo=i(L),B2=s(L,"LI",{});var Zwe=n(B2);uae=s(Zwe,"STRONG",{});var OJr=n(uae);Vyo=r(OJr,"reformer"),OJr.forEach(t),zyo=r(Zwe," \u2014 "),cj=s(Zwe,"A",{href:!0});var GJr=n(cj);Wyo=r(GJr,"ReformerForSequenceClassification"),GJr.forEach(t),Qyo=r(Zwe," (Reformer model)"),Zwe.forEach(t),Hyo=i(L),x2=s(L,"LI",{});var e6e=n(x2);pae=s(e6e,"STRONG",{});var XJr=n(pae);Uyo=r(XJr,"rembert"),XJr.forEach(t),Jyo=r(e6e," \u2014 "),mj=s(e6e,"A",{href:!0});var VJr=n(mj);Yyo=r(VJr,"RemBertForSequenceClassification"),VJr.forEach(t),Kyo=r(e6e," (RemBERT model)"),e6e.forEach(t),Zyo=i(L),k2=s(L,"LI",{});var o6e=n(k2);_ae=s(o6e,"STRONG",{});var zJr=n(_ae);ewo=r(zJr,"roberta"),zJr.forEach(t),owo=r(o6e," \u2014 "),fj=s(o6e,"A",{href:!0});var WJr=n(fj);rwo=r(WJr,"RobertaForSequenceClassification"),WJr.forEach(t),two=r(o6e," (RoBERTa model)"),o6e.forEach(t),awo=i(L),R2=s(L,"LI",{});var r6e=n(R2);bae=s(r6e,"STRONG",{});var QJr=n(bae);swo=r(QJr,"roformer"),QJr.forEach(t),nwo=r(r6e," \u2014 "),gj=s(r6e,"A",{href:!0});var HJr=n(gj);lwo=r(HJr,"RoFormerForSequenceClassification"),HJr.forEach(t),iwo=r(r6e," (RoFormer model)"),r6e.forEach(t),dwo=i(L),S2=s(L,"LI",{});var t6e=n(S2);vae=s(t6e,"STRONG",{});var UJr=n(vae);cwo=r(UJr,"squeezebert"),UJr.forEach(t),mwo=r(t6e," \u2014 "),hj=s(t6e,"A",{href:!0});var JJr=n(hj);fwo=r(JJr,"SqueezeBertForSequenceClassification"),JJr.forEach(t),gwo=r(t6e," (SqueezeBERT model)"),t6e.forEach(t),hwo=i(L),P2=s(L,"LI",{});var a6e=n(P2);Tae=s(a6e,"STRONG",{});var YJr=n(Tae);uwo=r(YJr,"tapas"),YJr.forEach(t),pwo=r(a6e," \u2014 "),uj=s(a6e,"A",{href:!0});var KJr=n(uj);_wo=r(KJr,"TapasForSequenceClassification"),KJr.forEach(t),bwo=r(a6e," (TAPAS model)"),a6e.forEach(t),vwo=i(L),$2=s(L,"LI",{});var s6e=n($2);Fae=s(s6e,"STRONG",{});var ZJr=n(Fae);Two=r(ZJr,"transfo-xl"),ZJr.forEach(t),Fwo=r(s6e," \u2014 "),pj=s(s6e,"A",{href:!0});var eYr=n(pj);Cwo=r(eYr,"TransfoXLForSequenceClassification"),eYr.forEach(t),Mwo=r(s6e," (Transformer-XL model)"),s6e.forEach(t),Ewo=i(L),I2=s(L,"LI",{});var n6e=n(I2);Cae=s(n6e,"STRONG",{});var oYr=n(Cae);ywo=r(oYr,"xlm"),oYr.forEach(t),wwo=r(n6e," \u2014 "),_j=s(n6e,"A",{href:!0});var rYr=n(_j);Awo=r(rYr,"XLMForSequenceClassification"),rYr.forEach(t),Lwo=r(n6e," (XLM model)"),n6e.forEach(t),Bwo=i(L),j2=s(L,"LI",{});var l6e=n(j2);Mae=s(l6e,"STRONG",{});var tYr=n(Mae);xwo=r(tYr,"xlm-roberta"),tYr.forEach(t),kwo=r(l6e," \u2014 "),bj=s(l6e,"A",{href:!0});var aYr=n(bj);Rwo=r(aYr,"XLMRobertaForSequenceClassification"),aYr.forEach(t),Swo=r(l6e," (XLM-RoBERTa model)"),l6e.forEach(t),Pwo=i(L),N2=s(L,"LI",{});var i6e=n(N2);Eae=s(i6e,"STRONG",{});var sYr=n(Eae);$wo=r(sYr,"xlm-roberta-xl"),sYr.forEach(t),Iwo=r(i6e," \u2014 "),vj=s(i6e,"A",{href:!0});var nYr=n(vj);jwo=r(nYr,"XLMRobertaXLForSequenceClassification"),nYr.forEach(t),Nwo=r(i6e," (XLM-RoBERTa-XL model)"),i6e.forEach(t),Dwo=i(L),D2=s(L,"LI",{});var d6e=n(D2);yae=s(d6e,"STRONG",{});var lYr=n(yae);qwo=r(lYr,"xlnet"),lYr.forEach(t),Owo=r(d6e," \u2014 "),Tj=s(d6e,"A",{href:!0});var iYr=n(Tj);Gwo=r(iYr,"XLNetForSequenceClassification"),iYr.forEach(t),Xwo=r(d6e," (XLNet model)"),d6e.forEach(t),Vwo=i(L),q2=s(L,"LI",{});var c6e=n(q2);wae=s(c6e,"STRONG",{});var dYr=n(wae);zwo=r(dYr,"yoso"),dYr.forEach(t),Wwo=r(c6e," \u2014 "),Fj=s(c6e,"A",{href:!0});var cYr=n(Fj);Qwo=r(cYr,"YosoForSequenceClassification"),cYr.forEach(t),Hwo=r(c6e," (YOSO model)"),c6e.forEach(t),L.forEach(t),Uwo=i(zt),O2=s(zt,"P",{});var m6e=n(O2);Jwo=r(m6e,"The model is set in evaluation mode by default using "),Aae=s(m6e,"CODE",{});var mYr=n(Aae);Ywo=r(mYr,"model.eval()"),mYr.forEach(t),Kwo=r(m6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Lae=s(m6e,"CODE",{});var fYr=n(Lae);Zwo=r(fYr,"model.train()"),fYr.forEach(t),m6e.forEach(t),e6o=i(zt),Bae=s(zt,"P",{});var gYr=n(Bae);o6o=r(gYr,"Examples:"),gYr.forEach(t),r6o=i(zt),f(Cw.$$.fragment,zt),zt.forEach(t),rl.forEach(t),dke=i(c),ud=s(c,"H2",{class:!0});var bSe=n(ud);G2=s(bSe,"A",{id:!0,class:!0,href:!0});var hYr=n(G2);xae=s(hYr,"SPAN",{});var uYr=n(xae);f(Mw.$$.fragment,uYr),uYr.forEach(t),hYr.forEach(t),t6o=i(bSe),kae=s(bSe,"SPAN",{});var pYr=n(kae);a6o=r(pYr,"AutoModelForMultipleChoice"),pYr.forEach(t),bSe.forEach(t),cke=i(c),rr=s(c,"DIV",{class:!0});var al=n(rr);f(Ew.$$.fragment,al),s6o=i(al),pd=s(al,"P",{});var Zz=n(pd);n6o=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Rae=s(Zz,"CODE",{});var _Yr=n(Rae);l6o=r(_Yr,"from_pretrained()"),_Yr.forEach(t),i6o=r(Zz,"class method or the "),Sae=s(Zz,"CODE",{});var bYr=n(Sae);d6o=r(bYr,"from_config()"),bYr.forEach(t),c6o=r(Zz,`class
method.`),Zz.forEach(t),m6o=i(al),yw=s(al,"P",{});var vSe=n(yw);f6o=r(vSe,"This class cannot be instantiated directly using "),Pae=s(vSe,"CODE",{});var vYr=n(Pae);g6o=r(vYr,"__init__()"),vYr.forEach(t),h6o=r(vSe," (throws an error)."),vSe.forEach(t),u6o=i(al),Jr=s(al,"DIV",{class:!0});var sl=n(Jr);f(ww.$$.fragment,sl),p6o=i(sl),$ae=s(sl,"P",{});var TYr=n($ae);_6o=r(TYr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),TYr.forEach(t),b6o=i(sl),_d=s(sl,"P",{});var eW=n(_d);v6o=r(eW,`Note:
Loading a model from its configuration file does `),Iae=s(eW,"STRONG",{});var FYr=n(Iae);T6o=r(FYr,"not"),FYr.forEach(t),F6o=r(eW,` load the model weights. It only affects the
model\u2019s configuration. Use `),jae=s(eW,"CODE",{});var CYr=n(jae);C6o=r(CYr,"from_pretrained()"),CYr.forEach(t),M6o=r(eW,"to load the model weights."),eW.forEach(t),E6o=i(sl),Nae=s(sl,"P",{});var MYr=n(Nae);y6o=r(MYr,"Examples:"),MYr.forEach(t),w6o=i(sl),f(Aw.$$.fragment,sl),sl.forEach(t),A6o=i(al),Ve=s(al,"DIV",{class:!0});var Wt=n(Ve);f(Lw.$$.fragment,Wt),L6o=i(Wt),Dae=s(Wt,"P",{});var EYr=n(Dae);B6o=r(EYr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),EYr.forEach(t),x6o=i(Wt),Ja=s(Wt,"P",{});var m5=n(Ja);k6o=r(m5,"The model class to instantiate is selected based on the "),qae=s(m5,"CODE",{});var yYr=n(qae);R6o=r(yYr,"model_type"),yYr.forEach(t),S6o=r(m5,` property of the config object (either
passed as an argument or loaded from `),Oae=s(m5,"CODE",{});var wYr=n(Oae);P6o=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),$6o=r(m5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gae=s(m5,"CODE",{});var AYr=n(Gae);I6o=r(AYr,"pretrained_model_name_or_path"),AYr.forEach(t),j6o=r(m5,":"),m5.forEach(t),N6o=i(Wt),O=s(Wt,"UL",{});var G=n(O);X2=s(G,"LI",{});var f6e=n(X2);Xae=s(f6e,"STRONG",{});var LYr=n(Xae);D6o=r(LYr,"albert"),LYr.forEach(t),q6o=r(f6e," \u2014 "),Cj=s(f6e,"A",{href:!0});var BYr=n(Cj);O6o=r(BYr,"AlbertForMultipleChoice"),BYr.forEach(t),G6o=r(f6e," (ALBERT model)"),f6e.forEach(t),X6o=i(G),V2=s(G,"LI",{});var g6e=n(V2);Vae=s(g6e,"STRONG",{});var xYr=n(Vae);V6o=r(xYr,"bert"),xYr.forEach(t),z6o=r(g6e," \u2014 "),Mj=s(g6e,"A",{href:!0});var kYr=n(Mj);W6o=r(kYr,"BertForMultipleChoice"),kYr.forEach(t),Q6o=r(g6e," (BERT model)"),g6e.forEach(t),H6o=i(G),z2=s(G,"LI",{});var h6e=n(z2);zae=s(h6e,"STRONG",{});var RYr=n(zae);U6o=r(RYr,"big_bird"),RYr.forEach(t),J6o=r(h6e," \u2014 "),Ej=s(h6e,"A",{href:!0});var SYr=n(Ej);Y6o=r(SYr,"BigBirdForMultipleChoice"),SYr.forEach(t),K6o=r(h6e," (BigBird model)"),h6e.forEach(t),Z6o=i(G),W2=s(G,"LI",{});var u6e=n(W2);Wae=s(u6e,"STRONG",{});var PYr=n(Wae);eAo=r(PYr,"camembert"),PYr.forEach(t),oAo=r(u6e," \u2014 "),yj=s(u6e,"A",{href:!0});var $Yr=n(yj);rAo=r($Yr,"CamembertForMultipleChoice"),$Yr.forEach(t),tAo=r(u6e," (CamemBERT model)"),u6e.forEach(t),aAo=i(G),Q2=s(G,"LI",{});var p6e=n(Q2);Qae=s(p6e,"STRONG",{});var IYr=n(Qae);sAo=r(IYr,"canine"),IYr.forEach(t),nAo=r(p6e," \u2014 "),wj=s(p6e,"A",{href:!0});var jYr=n(wj);lAo=r(jYr,"CanineForMultipleChoice"),jYr.forEach(t),iAo=r(p6e," (Canine model)"),p6e.forEach(t),dAo=i(G),H2=s(G,"LI",{});var _6e=n(H2);Hae=s(_6e,"STRONG",{});var NYr=n(Hae);cAo=r(NYr,"convbert"),NYr.forEach(t),mAo=r(_6e," \u2014 "),Aj=s(_6e,"A",{href:!0});var DYr=n(Aj);fAo=r(DYr,"ConvBertForMultipleChoice"),DYr.forEach(t),gAo=r(_6e," (ConvBERT model)"),_6e.forEach(t),hAo=i(G),U2=s(G,"LI",{});var b6e=n(U2);Uae=s(b6e,"STRONG",{});var qYr=n(Uae);uAo=r(qYr,"data2vec-text"),qYr.forEach(t),pAo=r(b6e," \u2014 "),Lj=s(b6e,"A",{href:!0});var OYr=n(Lj);_Ao=r(OYr,"Data2VecTextForMultipleChoice"),OYr.forEach(t),bAo=r(b6e," (Data2VecText model)"),b6e.forEach(t),vAo=i(G),J2=s(G,"LI",{});var v6e=n(J2);Jae=s(v6e,"STRONG",{});var GYr=n(Jae);TAo=r(GYr,"distilbert"),GYr.forEach(t),FAo=r(v6e," \u2014 "),Bj=s(v6e,"A",{href:!0});var XYr=n(Bj);CAo=r(XYr,"DistilBertForMultipleChoice"),XYr.forEach(t),MAo=r(v6e," (DistilBERT model)"),v6e.forEach(t),EAo=i(G),Y2=s(G,"LI",{});var T6e=n(Y2);Yae=s(T6e,"STRONG",{});var VYr=n(Yae);yAo=r(VYr,"electra"),VYr.forEach(t),wAo=r(T6e," \u2014 "),xj=s(T6e,"A",{href:!0});var zYr=n(xj);AAo=r(zYr,"ElectraForMultipleChoice"),zYr.forEach(t),LAo=r(T6e," (ELECTRA model)"),T6e.forEach(t),BAo=i(G),K2=s(G,"LI",{});var F6e=n(K2);Kae=s(F6e,"STRONG",{});var WYr=n(Kae);xAo=r(WYr,"flaubert"),WYr.forEach(t),kAo=r(F6e," \u2014 "),kj=s(F6e,"A",{href:!0});var QYr=n(kj);RAo=r(QYr,"FlaubertForMultipleChoice"),QYr.forEach(t),SAo=r(F6e," (FlauBERT model)"),F6e.forEach(t),PAo=i(G),Z2=s(G,"LI",{});var C6e=n(Z2);Zae=s(C6e,"STRONG",{});var HYr=n(Zae);$Ao=r(HYr,"fnet"),HYr.forEach(t),IAo=r(C6e," \u2014 "),Rj=s(C6e,"A",{href:!0});var UYr=n(Rj);jAo=r(UYr,"FNetForMultipleChoice"),UYr.forEach(t),NAo=r(C6e," (FNet model)"),C6e.forEach(t),DAo=i(G),ev=s(G,"LI",{});var M6e=n(ev);ese=s(M6e,"STRONG",{});var JYr=n(ese);qAo=r(JYr,"funnel"),JYr.forEach(t),OAo=r(M6e," \u2014 "),Sj=s(M6e,"A",{href:!0});var YYr=n(Sj);GAo=r(YYr,"FunnelForMultipleChoice"),YYr.forEach(t),XAo=r(M6e," (Funnel Transformer model)"),M6e.forEach(t),VAo=i(G),ov=s(G,"LI",{});var E6e=n(ov);ose=s(E6e,"STRONG",{});var KYr=n(ose);zAo=r(KYr,"ibert"),KYr.forEach(t),WAo=r(E6e," \u2014 "),Pj=s(E6e,"A",{href:!0});var ZYr=n(Pj);QAo=r(ZYr,"IBertForMultipleChoice"),ZYr.forEach(t),HAo=r(E6e," (I-BERT model)"),E6e.forEach(t),UAo=i(G),rv=s(G,"LI",{});var y6e=n(rv);rse=s(y6e,"STRONG",{});var eKr=n(rse);JAo=r(eKr,"longformer"),eKr.forEach(t),YAo=r(y6e," \u2014 "),$j=s(y6e,"A",{href:!0});var oKr=n($j);KAo=r(oKr,"LongformerForMultipleChoice"),oKr.forEach(t),ZAo=r(y6e," (Longformer model)"),y6e.forEach(t),e0o=i(G),tv=s(G,"LI",{});var w6e=n(tv);tse=s(w6e,"STRONG",{});var rKr=n(tse);o0o=r(rKr,"megatron-bert"),rKr.forEach(t),r0o=r(w6e," \u2014 "),Ij=s(w6e,"A",{href:!0});var tKr=n(Ij);t0o=r(tKr,"MegatronBertForMultipleChoice"),tKr.forEach(t),a0o=r(w6e," (MegatronBert model)"),w6e.forEach(t),s0o=i(G),av=s(G,"LI",{});var A6e=n(av);ase=s(A6e,"STRONG",{});var aKr=n(ase);n0o=r(aKr,"mobilebert"),aKr.forEach(t),l0o=r(A6e," \u2014 "),jj=s(A6e,"A",{href:!0});var sKr=n(jj);i0o=r(sKr,"MobileBertForMultipleChoice"),sKr.forEach(t),d0o=r(A6e," (MobileBERT model)"),A6e.forEach(t),c0o=i(G),sv=s(G,"LI",{});var L6e=n(sv);sse=s(L6e,"STRONG",{});var nKr=n(sse);m0o=r(nKr,"mpnet"),nKr.forEach(t),f0o=r(L6e," \u2014 "),Nj=s(L6e,"A",{href:!0});var lKr=n(Nj);g0o=r(lKr,"MPNetForMultipleChoice"),lKr.forEach(t),h0o=r(L6e," (MPNet model)"),L6e.forEach(t),u0o=i(G),nv=s(G,"LI",{});var B6e=n(nv);nse=s(B6e,"STRONG",{});var iKr=n(nse);p0o=r(iKr,"nystromformer"),iKr.forEach(t),_0o=r(B6e," \u2014 "),Dj=s(B6e,"A",{href:!0});var dKr=n(Dj);b0o=r(dKr,"NystromformerForMultipleChoice"),dKr.forEach(t),v0o=r(B6e," (Nystromformer model)"),B6e.forEach(t),T0o=i(G),lv=s(G,"LI",{});var x6e=n(lv);lse=s(x6e,"STRONG",{});var cKr=n(lse);F0o=r(cKr,"qdqbert"),cKr.forEach(t),C0o=r(x6e," \u2014 "),qj=s(x6e,"A",{href:!0});var mKr=n(qj);M0o=r(mKr,"QDQBertForMultipleChoice"),mKr.forEach(t),E0o=r(x6e," (QDQBert model)"),x6e.forEach(t),y0o=i(G),iv=s(G,"LI",{});var k6e=n(iv);ise=s(k6e,"STRONG",{});var fKr=n(ise);w0o=r(fKr,"rembert"),fKr.forEach(t),A0o=r(k6e," \u2014 "),Oj=s(k6e,"A",{href:!0});var gKr=n(Oj);L0o=r(gKr,"RemBertForMultipleChoice"),gKr.forEach(t),B0o=r(k6e," (RemBERT model)"),k6e.forEach(t),x0o=i(G),dv=s(G,"LI",{});var R6e=n(dv);dse=s(R6e,"STRONG",{});var hKr=n(dse);k0o=r(hKr,"roberta"),hKr.forEach(t),R0o=r(R6e," \u2014 "),Gj=s(R6e,"A",{href:!0});var uKr=n(Gj);S0o=r(uKr,"RobertaForMultipleChoice"),uKr.forEach(t),P0o=r(R6e," (RoBERTa model)"),R6e.forEach(t),$0o=i(G),cv=s(G,"LI",{});var S6e=n(cv);cse=s(S6e,"STRONG",{});var pKr=n(cse);I0o=r(pKr,"roformer"),pKr.forEach(t),j0o=r(S6e," \u2014 "),Xj=s(S6e,"A",{href:!0});var _Kr=n(Xj);N0o=r(_Kr,"RoFormerForMultipleChoice"),_Kr.forEach(t),D0o=r(S6e," (RoFormer model)"),S6e.forEach(t),q0o=i(G),mv=s(G,"LI",{});var P6e=n(mv);mse=s(P6e,"STRONG",{});var bKr=n(mse);O0o=r(bKr,"squeezebert"),bKr.forEach(t),G0o=r(P6e," \u2014 "),Vj=s(P6e,"A",{href:!0});var vKr=n(Vj);X0o=r(vKr,"SqueezeBertForMultipleChoice"),vKr.forEach(t),V0o=r(P6e," (SqueezeBERT model)"),P6e.forEach(t),z0o=i(G),fv=s(G,"LI",{});var $6e=n(fv);fse=s($6e,"STRONG",{});var TKr=n(fse);W0o=r(TKr,"xlm"),TKr.forEach(t),Q0o=r($6e," \u2014 "),zj=s($6e,"A",{href:!0});var FKr=n(zj);H0o=r(FKr,"XLMForMultipleChoice"),FKr.forEach(t),U0o=r($6e," (XLM model)"),$6e.forEach(t),J0o=i(G),gv=s(G,"LI",{});var I6e=n(gv);gse=s(I6e,"STRONG",{});var CKr=n(gse);Y0o=r(CKr,"xlm-roberta"),CKr.forEach(t),K0o=r(I6e," \u2014 "),Wj=s(I6e,"A",{href:!0});var MKr=n(Wj);Z0o=r(MKr,"XLMRobertaForMultipleChoice"),MKr.forEach(t),eLo=r(I6e," (XLM-RoBERTa model)"),I6e.forEach(t),oLo=i(G),hv=s(G,"LI",{});var j6e=n(hv);hse=s(j6e,"STRONG",{});var EKr=n(hse);rLo=r(EKr,"xlm-roberta-xl"),EKr.forEach(t),tLo=r(j6e," \u2014 "),Qj=s(j6e,"A",{href:!0});var yKr=n(Qj);aLo=r(yKr,"XLMRobertaXLForMultipleChoice"),yKr.forEach(t),sLo=r(j6e," (XLM-RoBERTa-XL model)"),j6e.forEach(t),nLo=i(G),uv=s(G,"LI",{});var N6e=n(uv);use=s(N6e,"STRONG",{});var wKr=n(use);lLo=r(wKr,"xlnet"),wKr.forEach(t),iLo=r(N6e," \u2014 "),Hj=s(N6e,"A",{href:!0});var AKr=n(Hj);dLo=r(AKr,"XLNetForMultipleChoice"),AKr.forEach(t),cLo=r(N6e," (XLNet model)"),N6e.forEach(t),mLo=i(G),pv=s(G,"LI",{});var D6e=n(pv);pse=s(D6e,"STRONG",{});var LKr=n(pse);fLo=r(LKr,"yoso"),LKr.forEach(t),gLo=r(D6e," \u2014 "),Uj=s(D6e,"A",{href:!0});var BKr=n(Uj);hLo=r(BKr,"YosoForMultipleChoice"),BKr.forEach(t),uLo=r(D6e," (YOSO model)"),D6e.forEach(t),G.forEach(t),pLo=i(Wt),_v=s(Wt,"P",{});var q6e=n(_v);_Lo=r(q6e,"The model is set in evaluation mode by default using "),_se=s(q6e,"CODE",{});var xKr=n(_se);bLo=r(xKr,"model.eval()"),xKr.forEach(t),vLo=r(q6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),bse=s(q6e,"CODE",{});var kKr=n(bse);TLo=r(kKr,"model.train()"),kKr.forEach(t),q6e.forEach(t),FLo=i(Wt),vse=s(Wt,"P",{});var RKr=n(vse);CLo=r(RKr,"Examples:"),RKr.forEach(t),MLo=i(Wt),f(Bw.$$.fragment,Wt),Wt.forEach(t),al.forEach(t),mke=i(c),bd=s(c,"H2",{class:!0});var TSe=n(bd);bv=s(TSe,"A",{id:!0,class:!0,href:!0});var SKr=n(bv);Tse=s(SKr,"SPAN",{});var PKr=n(Tse);f(xw.$$.fragment,PKr),PKr.forEach(t),SKr.forEach(t),ELo=i(TSe),Fse=s(TSe,"SPAN",{});var $Kr=n(Fse);yLo=r($Kr,"AutoModelForNextSentencePrediction"),$Kr.forEach(t),TSe.forEach(t),fke=i(c),tr=s(c,"DIV",{class:!0});var nl=n(tr);f(kw.$$.fragment,nl),wLo=i(nl),vd=s(nl,"P",{});var oW=n(vd);ALo=r(oW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Cse=s(oW,"CODE",{});var IKr=n(Cse);LLo=r(IKr,"from_pretrained()"),IKr.forEach(t),BLo=r(oW,"class method or the "),Mse=s(oW,"CODE",{});var jKr=n(Mse);xLo=r(jKr,"from_config()"),jKr.forEach(t),kLo=r(oW,`class
method.`),oW.forEach(t),RLo=i(nl),Rw=s(nl,"P",{});var FSe=n(Rw);SLo=r(FSe,"This class cannot be instantiated directly using "),Ese=s(FSe,"CODE",{});var NKr=n(Ese);PLo=r(NKr,"__init__()"),NKr.forEach(t),$Lo=r(FSe," (throws an error)."),FSe.forEach(t),ILo=i(nl),Yr=s(nl,"DIV",{class:!0});var ll=n(Yr);f(Sw.$$.fragment,ll),jLo=i(ll),yse=s(ll,"P",{});var DKr=n(yse);NLo=r(DKr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),DKr.forEach(t),DLo=i(ll),Td=s(ll,"P",{});var rW=n(Td);qLo=r(rW,`Note:
Loading a model from its configuration file does `),wse=s(rW,"STRONG",{});var qKr=n(wse);OLo=r(qKr,"not"),qKr.forEach(t),GLo=r(rW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ase=s(rW,"CODE",{});var OKr=n(Ase);XLo=r(OKr,"from_pretrained()"),OKr.forEach(t),VLo=r(rW,"to load the model weights."),rW.forEach(t),zLo=i(ll),Lse=s(ll,"P",{});var GKr=n(Lse);WLo=r(GKr,"Examples:"),GKr.forEach(t),QLo=i(ll),f(Pw.$$.fragment,ll),ll.forEach(t),HLo=i(nl),ze=s(nl,"DIV",{class:!0});var Qt=n(ze);f($w.$$.fragment,Qt),ULo=i(Qt),Bse=s(Qt,"P",{});var XKr=n(Bse);JLo=r(XKr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),XKr.forEach(t),YLo=i(Qt),Ya=s(Qt,"P",{});var f5=n(Ya);KLo=r(f5,"The model class to instantiate is selected based on the "),xse=s(f5,"CODE",{});var VKr=n(xse);ZLo=r(VKr,"model_type"),VKr.forEach(t),e7o=r(f5,` property of the config object (either
passed as an argument or loaded from `),kse=s(f5,"CODE",{});var zKr=n(kse);o7o=r(zKr,"pretrained_model_name_or_path"),zKr.forEach(t),r7o=r(f5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rse=s(f5,"CODE",{});var WKr=n(Rse);t7o=r(WKr,"pretrained_model_name_or_path"),WKr.forEach(t),a7o=r(f5,":"),f5.forEach(t),s7o=i(Qt),da=s(Qt,"UL",{});var il=n(da);vv=s(il,"LI",{});var O6e=n(vv);Sse=s(O6e,"STRONG",{});var QKr=n(Sse);n7o=r(QKr,"bert"),QKr.forEach(t),l7o=r(O6e," \u2014 "),Jj=s(O6e,"A",{href:!0});var HKr=n(Jj);i7o=r(HKr,"BertForNextSentencePrediction"),HKr.forEach(t),d7o=r(O6e," (BERT model)"),O6e.forEach(t),c7o=i(il),Tv=s(il,"LI",{});var G6e=n(Tv);Pse=s(G6e,"STRONG",{});var UKr=n(Pse);m7o=r(UKr,"fnet"),UKr.forEach(t),f7o=r(G6e," \u2014 "),Yj=s(G6e,"A",{href:!0});var JKr=n(Yj);g7o=r(JKr,"FNetForNextSentencePrediction"),JKr.forEach(t),h7o=r(G6e," (FNet model)"),G6e.forEach(t),u7o=i(il),Fv=s(il,"LI",{});var X6e=n(Fv);$se=s(X6e,"STRONG",{});var YKr=n($se);p7o=r(YKr,"megatron-bert"),YKr.forEach(t),_7o=r(X6e," \u2014 "),Kj=s(X6e,"A",{href:!0});var KKr=n(Kj);b7o=r(KKr,"MegatronBertForNextSentencePrediction"),KKr.forEach(t),v7o=r(X6e," (MegatronBert model)"),X6e.forEach(t),T7o=i(il),Cv=s(il,"LI",{});var V6e=n(Cv);Ise=s(V6e,"STRONG",{});var ZKr=n(Ise);F7o=r(ZKr,"mobilebert"),ZKr.forEach(t),C7o=r(V6e," \u2014 "),Zj=s(V6e,"A",{href:!0});var eZr=n(Zj);M7o=r(eZr,"MobileBertForNextSentencePrediction"),eZr.forEach(t),E7o=r(V6e," (MobileBERT model)"),V6e.forEach(t),y7o=i(il),Mv=s(il,"LI",{});var z6e=n(Mv);jse=s(z6e,"STRONG",{});var oZr=n(jse);w7o=r(oZr,"qdqbert"),oZr.forEach(t),A7o=r(z6e," \u2014 "),eN=s(z6e,"A",{href:!0});var rZr=n(eN);L7o=r(rZr,"QDQBertForNextSentencePrediction"),rZr.forEach(t),B7o=r(z6e," (QDQBert model)"),z6e.forEach(t),il.forEach(t),x7o=i(Qt),Ev=s(Qt,"P",{});var W6e=n(Ev);k7o=r(W6e,"The model is set in evaluation mode by default using "),Nse=s(W6e,"CODE",{});var tZr=n(Nse);R7o=r(tZr,"model.eval()"),tZr.forEach(t),S7o=r(W6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dse=s(W6e,"CODE",{});var aZr=n(Dse);P7o=r(aZr,"model.train()"),aZr.forEach(t),W6e.forEach(t),$7o=i(Qt),qse=s(Qt,"P",{});var sZr=n(qse);I7o=r(sZr,"Examples:"),sZr.forEach(t),j7o=i(Qt),f(Iw.$$.fragment,Qt),Qt.forEach(t),nl.forEach(t),gke=i(c),Fd=s(c,"H2",{class:!0});var CSe=n(Fd);yv=s(CSe,"A",{id:!0,class:!0,href:!0});var nZr=n(yv);Ose=s(nZr,"SPAN",{});var lZr=n(Ose);f(jw.$$.fragment,lZr),lZr.forEach(t),nZr.forEach(t),N7o=i(CSe),Gse=s(CSe,"SPAN",{});var iZr=n(Gse);D7o=r(iZr,"AutoModelForTokenClassification"),iZr.forEach(t),CSe.forEach(t),hke=i(c),ar=s(c,"DIV",{class:!0});var dl=n(ar);f(Nw.$$.fragment,dl),q7o=i(dl),Cd=s(dl,"P",{});var tW=n(Cd);O7o=r(tW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Xse=s(tW,"CODE",{});var dZr=n(Xse);G7o=r(dZr,"from_pretrained()"),dZr.forEach(t),X7o=r(tW,"class method or the "),Vse=s(tW,"CODE",{});var cZr=n(Vse);V7o=r(cZr,"from_config()"),cZr.forEach(t),z7o=r(tW,`class
method.`),tW.forEach(t),W7o=i(dl),Dw=s(dl,"P",{});var MSe=n(Dw);Q7o=r(MSe,"This class cannot be instantiated directly using "),zse=s(MSe,"CODE",{});var mZr=n(zse);H7o=r(mZr,"__init__()"),mZr.forEach(t),U7o=r(MSe," (throws an error)."),MSe.forEach(t),J7o=i(dl),Kr=s(dl,"DIV",{class:!0});var cl=n(Kr);f(qw.$$.fragment,cl),Y7o=i(cl),Wse=s(cl,"P",{});var fZr=n(Wse);K7o=r(fZr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),fZr.forEach(t),Z7o=i(cl),Md=s(cl,"P",{});var aW=n(Md);e8o=r(aW,`Note:
Loading a model from its configuration file does `),Qse=s(aW,"STRONG",{});var gZr=n(Qse);o8o=r(gZr,"not"),gZr.forEach(t),r8o=r(aW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hse=s(aW,"CODE",{});var hZr=n(Hse);t8o=r(hZr,"from_pretrained()"),hZr.forEach(t),a8o=r(aW,"to load the model weights."),aW.forEach(t),s8o=i(cl),Use=s(cl,"P",{});var uZr=n(Use);n8o=r(uZr,"Examples:"),uZr.forEach(t),l8o=i(cl),f(Ow.$$.fragment,cl),cl.forEach(t),i8o=i(dl),We=s(dl,"DIV",{class:!0});var Ht=n(We);f(Gw.$$.fragment,Ht),d8o=i(Ht),Jse=s(Ht,"P",{});var pZr=n(Jse);c8o=r(pZr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),pZr.forEach(t),m8o=i(Ht),Ka=s(Ht,"P",{});var g5=n(Ka);f8o=r(g5,"The model class to instantiate is selected based on the "),Yse=s(g5,"CODE",{});var _Zr=n(Yse);g8o=r(_Zr,"model_type"),_Zr.forEach(t),h8o=r(g5,` property of the config object (either
passed as an argument or loaded from `),Kse=s(g5,"CODE",{});var bZr=n(Kse);u8o=r(bZr,"pretrained_model_name_or_path"),bZr.forEach(t),p8o=r(g5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zse=s(g5,"CODE",{});var vZr=n(Zse);_8o=r(vZr,"pretrained_model_name_or_path"),vZr.forEach(t),b8o=r(g5,":"),g5.forEach(t),v8o=i(Ht),D=s(Ht,"UL",{});var q=n(D);wv=s(q,"LI",{});var Q6e=n(wv);ene=s(Q6e,"STRONG",{});var TZr=n(ene);T8o=r(TZr,"albert"),TZr.forEach(t),F8o=r(Q6e," \u2014 "),oN=s(Q6e,"A",{href:!0});var FZr=n(oN);C8o=r(FZr,"AlbertForTokenClassification"),FZr.forEach(t),M8o=r(Q6e," (ALBERT model)"),Q6e.forEach(t),E8o=i(q),Av=s(q,"LI",{});var H6e=n(Av);one=s(H6e,"STRONG",{});var CZr=n(one);y8o=r(CZr,"bert"),CZr.forEach(t),w8o=r(H6e," \u2014 "),rN=s(H6e,"A",{href:!0});var MZr=n(rN);A8o=r(MZr,"BertForTokenClassification"),MZr.forEach(t),L8o=r(H6e," (BERT model)"),H6e.forEach(t),B8o=i(q),Lv=s(q,"LI",{});var U6e=n(Lv);rne=s(U6e,"STRONG",{});var EZr=n(rne);x8o=r(EZr,"big_bird"),EZr.forEach(t),k8o=r(U6e," \u2014 "),tN=s(U6e,"A",{href:!0});var yZr=n(tN);R8o=r(yZr,"BigBirdForTokenClassification"),yZr.forEach(t),S8o=r(U6e," (BigBird model)"),U6e.forEach(t),P8o=i(q),Bv=s(q,"LI",{});var J6e=n(Bv);tne=s(J6e,"STRONG",{});var wZr=n(tne);$8o=r(wZr,"camembert"),wZr.forEach(t),I8o=r(J6e," \u2014 "),aN=s(J6e,"A",{href:!0});var AZr=n(aN);j8o=r(AZr,"CamembertForTokenClassification"),AZr.forEach(t),N8o=r(J6e," (CamemBERT model)"),J6e.forEach(t),D8o=i(q),xv=s(q,"LI",{});var Y6e=n(xv);ane=s(Y6e,"STRONG",{});var LZr=n(ane);q8o=r(LZr,"canine"),LZr.forEach(t),O8o=r(Y6e," \u2014 "),sN=s(Y6e,"A",{href:!0});var BZr=n(sN);G8o=r(BZr,"CanineForTokenClassification"),BZr.forEach(t),X8o=r(Y6e," (Canine model)"),Y6e.forEach(t),V8o=i(q),kv=s(q,"LI",{});var K6e=n(kv);sne=s(K6e,"STRONG",{});var xZr=n(sne);z8o=r(xZr,"convbert"),xZr.forEach(t),W8o=r(K6e," \u2014 "),nN=s(K6e,"A",{href:!0});var kZr=n(nN);Q8o=r(kZr,"ConvBertForTokenClassification"),kZr.forEach(t),H8o=r(K6e," (ConvBERT model)"),K6e.forEach(t),U8o=i(q),Rv=s(q,"LI",{});var Z6e=n(Rv);nne=s(Z6e,"STRONG",{});var RZr=n(nne);J8o=r(RZr,"data2vec-text"),RZr.forEach(t),Y8o=r(Z6e," \u2014 "),lN=s(Z6e,"A",{href:!0});var SZr=n(lN);K8o=r(SZr,"Data2VecTextForTokenClassification"),SZr.forEach(t),Z8o=r(Z6e," (Data2VecText model)"),Z6e.forEach(t),e9o=i(q),Sv=s(q,"LI",{});var eAe=n(Sv);lne=s(eAe,"STRONG",{});var PZr=n(lne);o9o=r(PZr,"deberta"),PZr.forEach(t),r9o=r(eAe," \u2014 "),iN=s(eAe,"A",{href:!0});var $Zr=n(iN);t9o=r($Zr,"DebertaForTokenClassification"),$Zr.forEach(t),a9o=r(eAe," (DeBERTa model)"),eAe.forEach(t),s9o=i(q),Pv=s(q,"LI",{});var oAe=n(Pv);ine=s(oAe,"STRONG",{});var IZr=n(ine);n9o=r(IZr,"deberta-v2"),IZr.forEach(t),l9o=r(oAe," \u2014 "),dN=s(oAe,"A",{href:!0});var jZr=n(dN);i9o=r(jZr,"DebertaV2ForTokenClassification"),jZr.forEach(t),d9o=r(oAe," (DeBERTa-v2 model)"),oAe.forEach(t),c9o=i(q),$v=s(q,"LI",{});var rAe=n($v);dne=s(rAe,"STRONG",{});var NZr=n(dne);m9o=r(NZr,"distilbert"),NZr.forEach(t),f9o=r(rAe," \u2014 "),cN=s(rAe,"A",{href:!0});var DZr=n(cN);g9o=r(DZr,"DistilBertForTokenClassification"),DZr.forEach(t),h9o=r(rAe," (DistilBERT model)"),rAe.forEach(t),u9o=i(q),Iv=s(q,"LI",{});var tAe=n(Iv);cne=s(tAe,"STRONG",{});var qZr=n(cne);p9o=r(qZr,"electra"),qZr.forEach(t),_9o=r(tAe," \u2014 "),mN=s(tAe,"A",{href:!0});var OZr=n(mN);b9o=r(OZr,"ElectraForTokenClassification"),OZr.forEach(t),v9o=r(tAe," (ELECTRA model)"),tAe.forEach(t),T9o=i(q),jv=s(q,"LI",{});var aAe=n(jv);mne=s(aAe,"STRONG",{});var GZr=n(mne);F9o=r(GZr,"flaubert"),GZr.forEach(t),C9o=r(aAe," \u2014 "),fN=s(aAe,"A",{href:!0});var XZr=n(fN);M9o=r(XZr,"FlaubertForTokenClassification"),XZr.forEach(t),E9o=r(aAe," (FlauBERT model)"),aAe.forEach(t),y9o=i(q),Nv=s(q,"LI",{});var sAe=n(Nv);fne=s(sAe,"STRONG",{});var VZr=n(fne);w9o=r(VZr,"fnet"),VZr.forEach(t),A9o=r(sAe," \u2014 "),gN=s(sAe,"A",{href:!0});var zZr=n(gN);L9o=r(zZr,"FNetForTokenClassification"),zZr.forEach(t),B9o=r(sAe," (FNet model)"),sAe.forEach(t),x9o=i(q),Dv=s(q,"LI",{});var nAe=n(Dv);gne=s(nAe,"STRONG",{});var WZr=n(gne);k9o=r(WZr,"funnel"),WZr.forEach(t),R9o=r(nAe," \u2014 "),hN=s(nAe,"A",{href:!0});var QZr=n(hN);S9o=r(QZr,"FunnelForTokenClassification"),QZr.forEach(t),P9o=r(nAe," (Funnel Transformer model)"),nAe.forEach(t),$9o=i(q),qv=s(q,"LI",{});var lAe=n(qv);hne=s(lAe,"STRONG",{});var HZr=n(hne);I9o=r(HZr,"gpt2"),HZr.forEach(t),j9o=r(lAe," \u2014 "),uN=s(lAe,"A",{href:!0});var UZr=n(uN);N9o=r(UZr,"GPT2ForTokenClassification"),UZr.forEach(t),D9o=r(lAe," (OpenAI GPT-2 model)"),lAe.forEach(t),q9o=i(q),Ov=s(q,"LI",{});var iAe=n(Ov);une=s(iAe,"STRONG",{});var JZr=n(une);O9o=r(JZr,"ibert"),JZr.forEach(t),G9o=r(iAe," \u2014 "),pN=s(iAe,"A",{href:!0});var YZr=n(pN);X9o=r(YZr,"IBertForTokenClassification"),YZr.forEach(t),V9o=r(iAe," (I-BERT model)"),iAe.forEach(t),z9o=i(q),Gv=s(q,"LI",{});var dAe=n(Gv);pne=s(dAe,"STRONG",{});var KZr=n(pne);W9o=r(KZr,"layoutlm"),KZr.forEach(t),Q9o=r(dAe," \u2014 "),_N=s(dAe,"A",{href:!0});var ZZr=n(_N);H9o=r(ZZr,"LayoutLMForTokenClassification"),ZZr.forEach(t),U9o=r(dAe," (LayoutLM model)"),dAe.forEach(t),J9o=i(q),Xv=s(q,"LI",{});var cAe=n(Xv);_ne=s(cAe,"STRONG",{});var eet=n(_ne);Y9o=r(eet,"layoutlmv2"),eet.forEach(t),K9o=r(cAe," \u2014 "),bN=s(cAe,"A",{href:!0});var oet=n(bN);Z9o=r(oet,"LayoutLMv2ForTokenClassification"),oet.forEach(t),eBo=r(cAe," (LayoutLMv2 model)"),cAe.forEach(t),oBo=i(q),Vv=s(q,"LI",{});var mAe=n(Vv);bne=s(mAe,"STRONG",{});var ret=n(bne);rBo=r(ret,"longformer"),ret.forEach(t),tBo=r(mAe," \u2014 "),vN=s(mAe,"A",{href:!0});var tet=n(vN);aBo=r(tet,"LongformerForTokenClassification"),tet.forEach(t),sBo=r(mAe," (Longformer model)"),mAe.forEach(t),nBo=i(q),zv=s(q,"LI",{});var fAe=n(zv);vne=s(fAe,"STRONG",{});var aet=n(vne);lBo=r(aet,"megatron-bert"),aet.forEach(t),iBo=r(fAe," \u2014 "),TN=s(fAe,"A",{href:!0});var set=n(TN);dBo=r(set,"MegatronBertForTokenClassification"),set.forEach(t),cBo=r(fAe," (MegatronBert model)"),fAe.forEach(t),mBo=i(q),Wv=s(q,"LI",{});var gAe=n(Wv);Tne=s(gAe,"STRONG",{});var net=n(Tne);fBo=r(net,"mobilebert"),net.forEach(t),gBo=r(gAe," \u2014 "),FN=s(gAe,"A",{href:!0});var iet=n(FN);hBo=r(iet,"MobileBertForTokenClassification"),iet.forEach(t),uBo=r(gAe," (MobileBERT model)"),gAe.forEach(t),pBo=i(q),Qv=s(q,"LI",{});var hAe=n(Qv);Fne=s(hAe,"STRONG",{});var det=n(Fne);_Bo=r(det,"mpnet"),det.forEach(t),bBo=r(hAe," \u2014 "),CN=s(hAe,"A",{href:!0});var cet=n(CN);vBo=r(cet,"MPNetForTokenClassification"),cet.forEach(t),TBo=r(hAe," (MPNet model)"),hAe.forEach(t),FBo=i(q),Hv=s(q,"LI",{});var uAe=n(Hv);Cne=s(uAe,"STRONG",{});var met=n(Cne);CBo=r(met,"nystromformer"),met.forEach(t),MBo=r(uAe," \u2014 "),MN=s(uAe,"A",{href:!0});var fet=n(MN);EBo=r(fet,"NystromformerForTokenClassification"),fet.forEach(t),yBo=r(uAe," (Nystromformer model)"),uAe.forEach(t),wBo=i(q),Uv=s(q,"LI",{});var pAe=n(Uv);Mne=s(pAe,"STRONG",{});var get=n(Mne);ABo=r(get,"qdqbert"),get.forEach(t),LBo=r(pAe," \u2014 "),EN=s(pAe,"A",{href:!0});var het=n(EN);BBo=r(het,"QDQBertForTokenClassification"),het.forEach(t),xBo=r(pAe," (QDQBert model)"),pAe.forEach(t),kBo=i(q),Jv=s(q,"LI",{});var _Ae=n(Jv);Ene=s(_Ae,"STRONG",{});var uet=n(Ene);RBo=r(uet,"rembert"),uet.forEach(t),SBo=r(_Ae," \u2014 "),yN=s(_Ae,"A",{href:!0});var pet=n(yN);PBo=r(pet,"RemBertForTokenClassification"),pet.forEach(t),$Bo=r(_Ae," (RemBERT model)"),_Ae.forEach(t),IBo=i(q),Yv=s(q,"LI",{});var bAe=n(Yv);yne=s(bAe,"STRONG",{});var _et=n(yne);jBo=r(_et,"roberta"),_et.forEach(t),NBo=r(bAe," \u2014 "),wN=s(bAe,"A",{href:!0});var bet=n(wN);DBo=r(bet,"RobertaForTokenClassification"),bet.forEach(t),qBo=r(bAe," (RoBERTa model)"),bAe.forEach(t),OBo=i(q),Kv=s(q,"LI",{});var vAe=n(Kv);wne=s(vAe,"STRONG",{});var vet=n(wne);GBo=r(vet,"roformer"),vet.forEach(t),XBo=r(vAe," \u2014 "),AN=s(vAe,"A",{href:!0});var Tet=n(AN);VBo=r(Tet,"RoFormerForTokenClassification"),Tet.forEach(t),zBo=r(vAe," (RoFormer model)"),vAe.forEach(t),WBo=i(q),Zv=s(q,"LI",{});var TAe=n(Zv);Ane=s(TAe,"STRONG",{});var Fet=n(Ane);QBo=r(Fet,"squeezebert"),Fet.forEach(t),HBo=r(TAe," \u2014 "),LN=s(TAe,"A",{href:!0});var Cet=n(LN);UBo=r(Cet,"SqueezeBertForTokenClassification"),Cet.forEach(t),JBo=r(TAe," (SqueezeBERT model)"),TAe.forEach(t),YBo=i(q),eT=s(q,"LI",{});var FAe=n(eT);Lne=s(FAe,"STRONG",{});var Met=n(Lne);KBo=r(Met,"xlm"),Met.forEach(t),ZBo=r(FAe," \u2014 "),BN=s(FAe,"A",{href:!0});var Eet=n(BN);exo=r(Eet,"XLMForTokenClassification"),Eet.forEach(t),oxo=r(FAe," (XLM model)"),FAe.forEach(t),rxo=i(q),oT=s(q,"LI",{});var CAe=n(oT);Bne=s(CAe,"STRONG",{});var yet=n(Bne);txo=r(yet,"xlm-roberta"),yet.forEach(t),axo=r(CAe," \u2014 "),xN=s(CAe,"A",{href:!0});var wet=n(xN);sxo=r(wet,"XLMRobertaForTokenClassification"),wet.forEach(t),nxo=r(CAe," (XLM-RoBERTa model)"),CAe.forEach(t),lxo=i(q),rT=s(q,"LI",{});var MAe=n(rT);xne=s(MAe,"STRONG",{});var Aet=n(xne);ixo=r(Aet,"xlm-roberta-xl"),Aet.forEach(t),dxo=r(MAe," \u2014 "),kN=s(MAe,"A",{href:!0});var Let=n(kN);cxo=r(Let,"XLMRobertaXLForTokenClassification"),Let.forEach(t),mxo=r(MAe," (XLM-RoBERTa-XL model)"),MAe.forEach(t),fxo=i(q),tT=s(q,"LI",{});var EAe=n(tT);kne=s(EAe,"STRONG",{});var Bet=n(kne);gxo=r(Bet,"xlnet"),Bet.forEach(t),hxo=r(EAe," \u2014 "),RN=s(EAe,"A",{href:!0});var xet=n(RN);uxo=r(xet,"XLNetForTokenClassification"),xet.forEach(t),pxo=r(EAe," (XLNet model)"),EAe.forEach(t),_xo=i(q),aT=s(q,"LI",{});var yAe=n(aT);Rne=s(yAe,"STRONG",{});var ket=n(Rne);bxo=r(ket,"yoso"),ket.forEach(t),vxo=r(yAe," \u2014 "),SN=s(yAe,"A",{href:!0});var Ret=n(SN);Txo=r(Ret,"YosoForTokenClassification"),Ret.forEach(t),Fxo=r(yAe," (YOSO model)"),yAe.forEach(t),q.forEach(t),Cxo=i(Ht),sT=s(Ht,"P",{});var wAe=n(sT);Mxo=r(wAe,"The model is set in evaluation mode by default using "),Sne=s(wAe,"CODE",{});var Set=n(Sne);Exo=r(Set,"model.eval()"),Set.forEach(t),yxo=r(wAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pne=s(wAe,"CODE",{});var Pet=n(Pne);wxo=r(Pet,"model.train()"),Pet.forEach(t),wAe.forEach(t),Axo=i(Ht),$ne=s(Ht,"P",{});var $et=n($ne);Lxo=r($et,"Examples:"),$et.forEach(t),Bxo=i(Ht),f(Xw.$$.fragment,Ht),Ht.forEach(t),dl.forEach(t),uke=i(c),Ed=s(c,"H2",{class:!0});var ESe=n(Ed);nT=s(ESe,"A",{id:!0,class:!0,href:!0});var Iet=n(nT);Ine=s(Iet,"SPAN",{});var jet=n(Ine);f(Vw.$$.fragment,jet),jet.forEach(t),Iet.forEach(t),xxo=i(ESe),jne=s(ESe,"SPAN",{});var Net=n(jne);kxo=r(Net,"AutoModelForQuestionAnswering"),Net.forEach(t),ESe.forEach(t),pke=i(c),sr=s(c,"DIV",{class:!0});var ml=n(sr);f(zw.$$.fragment,ml),Rxo=i(ml),yd=s(ml,"P",{});var sW=n(yd);Sxo=r(sW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Nne=s(sW,"CODE",{});var Det=n(Nne);Pxo=r(Det,"from_pretrained()"),Det.forEach(t),$xo=r(sW,"class method or the "),Dne=s(sW,"CODE",{});var qet=n(Dne);Ixo=r(qet,"from_config()"),qet.forEach(t),jxo=r(sW,`class
method.`),sW.forEach(t),Nxo=i(ml),Ww=s(ml,"P",{});var ySe=n(Ww);Dxo=r(ySe,"This class cannot be instantiated directly using "),qne=s(ySe,"CODE",{});var Oet=n(qne);qxo=r(Oet,"__init__()"),Oet.forEach(t),Oxo=r(ySe," (throws an error)."),ySe.forEach(t),Gxo=i(ml),Zr=s(ml,"DIV",{class:!0});var fl=n(Zr);f(Qw.$$.fragment,fl),Xxo=i(fl),One=s(fl,"P",{});var Get=n(One);Vxo=r(Get,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Get.forEach(t),zxo=i(fl),wd=s(fl,"P",{});var nW=n(wd);Wxo=r(nW,`Note:
Loading a model from its configuration file does `),Gne=s(nW,"STRONG",{});var Xet=n(Gne);Qxo=r(Xet,"not"),Xet.forEach(t),Hxo=r(nW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xne=s(nW,"CODE",{});var Vet=n(Xne);Uxo=r(Vet,"from_pretrained()"),Vet.forEach(t),Jxo=r(nW,"to load the model weights."),nW.forEach(t),Yxo=i(fl),Vne=s(fl,"P",{});var zet=n(Vne);Kxo=r(zet,"Examples:"),zet.forEach(t),Zxo=i(fl),f(Hw.$$.fragment,fl),fl.forEach(t),eko=i(ml),Qe=s(ml,"DIV",{class:!0});var Ut=n(Qe);f(Uw.$$.fragment,Ut),oko=i(Ut),zne=s(Ut,"P",{});var Wet=n(zne);rko=r(Wet,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Wet.forEach(t),tko=i(Ut),Za=s(Ut,"P",{});var h5=n(Za);ako=r(h5,"The model class to instantiate is selected based on the "),Wne=s(h5,"CODE",{});var Qet=n(Wne);sko=r(Qet,"model_type"),Qet.forEach(t),nko=r(h5,` property of the config object (either
passed as an argument or loaded from `),Qne=s(h5,"CODE",{});var Het=n(Qne);lko=r(Het,"pretrained_model_name_or_path"),Het.forEach(t),iko=r(h5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hne=s(h5,"CODE",{});var Uet=n(Hne);dko=r(Uet,"pretrained_model_name_or_path"),Uet.forEach(t),cko=r(h5,":"),h5.forEach(t),mko=i(Ut),R=s(Ut,"UL",{});var P=n(R);lT=s(P,"LI",{});var AAe=n(lT);Une=s(AAe,"STRONG",{});var Jet=n(Une);fko=r(Jet,"albert"),Jet.forEach(t),gko=r(AAe," \u2014 "),PN=s(AAe,"A",{href:!0});var Yet=n(PN);hko=r(Yet,"AlbertForQuestionAnswering"),Yet.forEach(t),uko=r(AAe," (ALBERT model)"),AAe.forEach(t),pko=i(P),iT=s(P,"LI",{});var LAe=n(iT);Jne=s(LAe,"STRONG",{});var Ket=n(Jne);_ko=r(Ket,"bart"),Ket.forEach(t),bko=r(LAe," \u2014 "),$N=s(LAe,"A",{href:!0});var Zet=n($N);vko=r(Zet,"BartForQuestionAnswering"),Zet.forEach(t),Tko=r(LAe," (BART model)"),LAe.forEach(t),Fko=i(P),dT=s(P,"LI",{});var BAe=n(dT);Yne=s(BAe,"STRONG",{});var eot=n(Yne);Cko=r(eot,"bert"),eot.forEach(t),Mko=r(BAe," \u2014 "),IN=s(BAe,"A",{href:!0});var oot=n(IN);Eko=r(oot,"BertForQuestionAnswering"),oot.forEach(t),yko=r(BAe," (BERT model)"),BAe.forEach(t),wko=i(P),cT=s(P,"LI",{});var xAe=n(cT);Kne=s(xAe,"STRONG",{});var rot=n(Kne);Ako=r(rot,"big_bird"),rot.forEach(t),Lko=r(xAe," \u2014 "),jN=s(xAe,"A",{href:!0});var tot=n(jN);Bko=r(tot,"BigBirdForQuestionAnswering"),tot.forEach(t),xko=r(xAe," (BigBird model)"),xAe.forEach(t),kko=i(P),mT=s(P,"LI",{});var kAe=n(mT);Zne=s(kAe,"STRONG",{});var aot=n(Zne);Rko=r(aot,"bigbird_pegasus"),aot.forEach(t),Sko=r(kAe," \u2014 "),NN=s(kAe,"A",{href:!0});var sot=n(NN);Pko=r(sot,"BigBirdPegasusForQuestionAnswering"),sot.forEach(t),$ko=r(kAe," (BigBirdPegasus model)"),kAe.forEach(t),Iko=i(P),fT=s(P,"LI",{});var RAe=n(fT);ele=s(RAe,"STRONG",{});var not=n(ele);jko=r(not,"camembert"),not.forEach(t),Nko=r(RAe," \u2014 "),DN=s(RAe,"A",{href:!0});var lot=n(DN);Dko=r(lot,"CamembertForQuestionAnswering"),lot.forEach(t),qko=r(RAe," (CamemBERT model)"),RAe.forEach(t),Oko=i(P),gT=s(P,"LI",{});var SAe=n(gT);ole=s(SAe,"STRONG",{});var iot=n(ole);Gko=r(iot,"canine"),iot.forEach(t),Xko=r(SAe," \u2014 "),qN=s(SAe,"A",{href:!0});var dot=n(qN);Vko=r(dot,"CanineForQuestionAnswering"),dot.forEach(t),zko=r(SAe," (Canine model)"),SAe.forEach(t),Wko=i(P),hT=s(P,"LI",{});var PAe=n(hT);rle=s(PAe,"STRONG",{});var cot=n(rle);Qko=r(cot,"convbert"),cot.forEach(t),Hko=r(PAe," \u2014 "),ON=s(PAe,"A",{href:!0});var mot=n(ON);Uko=r(mot,"ConvBertForQuestionAnswering"),mot.forEach(t),Jko=r(PAe," (ConvBERT model)"),PAe.forEach(t),Yko=i(P),uT=s(P,"LI",{});var $Ae=n(uT);tle=s($Ae,"STRONG",{});var fot=n(tle);Kko=r(fot,"data2vec-text"),fot.forEach(t),Zko=r($Ae," \u2014 "),GN=s($Ae,"A",{href:!0});var got=n(GN);eRo=r(got,"Data2VecTextForQuestionAnswering"),got.forEach(t),oRo=r($Ae," (Data2VecText model)"),$Ae.forEach(t),rRo=i(P),pT=s(P,"LI",{});var IAe=n(pT);ale=s(IAe,"STRONG",{});var hot=n(ale);tRo=r(hot,"deberta"),hot.forEach(t),aRo=r(IAe," \u2014 "),XN=s(IAe,"A",{href:!0});var uot=n(XN);sRo=r(uot,"DebertaForQuestionAnswering"),uot.forEach(t),nRo=r(IAe," (DeBERTa model)"),IAe.forEach(t),lRo=i(P),_T=s(P,"LI",{});var jAe=n(_T);sle=s(jAe,"STRONG",{});var pot=n(sle);iRo=r(pot,"deberta-v2"),pot.forEach(t),dRo=r(jAe," \u2014 "),VN=s(jAe,"A",{href:!0});var _ot=n(VN);cRo=r(_ot,"DebertaV2ForQuestionAnswering"),_ot.forEach(t),mRo=r(jAe," (DeBERTa-v2 model)"),jAe.forEach(t),fRo=i(P),bT=s(P,"LI",{});var NAe=n(bT);nle=s(NAe,"STRONG",{});var bot=n(nle);gRo=r(bot,"distilbert"),bot.forEach(t),hRo=r(NAe," \u2014 "),zN=s(NAe,"A",{href:!0});var vot=n(zN);uRo=r(vot,"DistilBertForQuestionAnswering"),vot.forEach(t),pRo=r(NAe," (DistilBERT model)"),NAe.forEach(t),_Ro=i(P),vT=s(P,"LI",{});var DAe=n(vT);lle=s(DAe,"STRONG",{});var Tot=n(lle);bRo=r(Tot,"electra"),Tot.forEach(t),vRo=r(DAe," \u2014 "),WN=s(DAe,"A",{href:!0});var Fot=n(WN);TRo=r(Fot,"ElectraForQuestionAnswering"),Fot.forEach(t),FRo=r(DAe," (ELECTRA model)"),DAe.forEach(t),CRo=i(P),TT=s(P,"LI",{});var qAe=n(TT);ile=s(qAe,"STRONG",{});var Cot=n(ile);MRo=r(Cot,"flaubert"),Cot.forEach(t),ERo=r(qAe," \u2014 "),QN=s(qAe,"A",{href:!0});var Mot=n(QN);yRo=r(Mot,"FlaubertForQuestionAnsweringSimple"),Mot.forEach(t),wRo=r(qAe," (FlauBERT model)"),qAe.forEach(t),ARo=i(P),FT=s(P,"LI",{});var OAe=n(FT);dle=s(OAe,"STRONG",{});var Eot=n(dle);LRo=r(Eot,"fnet"),Eot.forEach(t),BRo=r(OAe," \u2014 "),HN=s(OAe,"A",{href:!0});var yot=n(HN);xRo=r(yot,"FNetForQuestionAnswering"),yot.forEach(t),kRo=r(OAe," (FNet model)"),OAe.forEach(t),RRo=i(P),CT=s(P,"LI",{});var GAe=n(CT);cle=s(GAe,"STRONG",{});var wot=n(cle);SRo=r(wot,"funnel"),wot.forEach(t),PRo=r(GAe," \u2014 "),UN=s(GAe,"A",{href:!0});var Aot=n(UN);$Ro=r(Aot,"FunnelForQuestionAnswering"),Aot.forEach(t),IRo=r(GAe," (Funnel Transformer model)"),GAe.forEach(t),jRo=i(P),MT=s(P,"LI",{});var XAe=n(MT);mle=s(XAe,"STRONG",{});var Lot=n(mle);NRo=r(Lot,"gptj"),Lot.forEach(t),DRo=r(XAe," \u2014 "),JN=s(XAe,"A",{href:!0});var Bot=n(JN);qRo=r(Bot,"GPTJForQuestionAnswering"),Bot.forEach(t),ORo=r(XAe," (GPT-J model)"),XAe.forEach(t),GRo=i(P),ET=s(P,"LI",{});var VAe=n(ET);fle=s(VAe,"STRONG",{});var xot=n(fle);XRo=r(xot,"ibert"),xot.forEach(t),VRo=r(VAe," \u2014 "),YN=s(VAe,"A",{href:!0});var kot=n(YN);zRo=r(kot,"IBertForQuestionAnswering"),kot.forEach(t),WRo=r(VAe," (I-BERT model)"),VAe.forEach(t),QRo=i(P),yT=s(P,"LI",{});var zAe=n(yT);gle=s(zAe,"STRONG",{});var Rot=n(gle);HRo=r(Rot,"layoutlmv2"),Rot.forEach(t),URo=r(zAe," \u2014 "),KN=s(zAe,"A",{href:!0});var Sot=n(KN);JRo=r(Sot,"LayoutLMv2ForQuestionAnswering"),Sot.forEach(t),YRo=r(zAe," (LayoutLMv2 model)"),zAe.forEach(t),KRo=i(P),wT=s(P,"LI",{});var WAe=n(wT);hle=s(WAe,"STRONG",{});var Pot=n(hle);ZRo=r(Pot,"led"),Pot.forEach(t),eSo=r(WAe," \u2014 "),ZN=s(WAe,"A",{href:!0});var $ot=n(ZN);oSo=r($ot,"LEDForQuestionAnswering"),$ot.forEach(t),rSo=r(WAe," (LED model)"),WAe.forEach(t),tSo=i(P),AT=s(P,"LI",{});var QAe=n(AT);ule=s(QAe,"STRONG",{});var Iot=n(ule);aSo=r(Iot,"longformer"),Iot.forEach(t),sSo=r(QAe," \u2014 "),eD=s(QAe,"A",{href:!0});var jot=n(eD);nSo=r(jot,"LongformerForQuestionAnswering"),jot.forEach(t),lSo=r(QAe," (Longformer model)"),QAe.forEach(t),iSo=i(P),LT=s(P,"LI",{});var HAe=n(LT);ple=s(HAe,"STRONG",{});var Not=n(ple);dSo=r(Not,"lxmert"),Not.forEach(t),cSo=r(HAe," \u2014 "),oD=s(HAe,"A",{href:!0});var Dot=n(oD);mSo=r(Dot,"LxmertForQuestionAnswering"),Dot.forEach(t),fSo=r(HAe," (LXMERT model)"),HAe.forEach(t),gSo=i(P),BT=s(P,"LI",{});var UAe=n(BT);_le=s(UAe,"STRONG",{});var qot=n(_le);hSo=r(qot,"mbart"),qot.forEach(t),uSo=r(UAe," \u2014 "),rD=s(UAe,"A",{href:!0});var Oot=n(rD);pSo=r(Oot,"MBartForQuestionAnswering"),Oot.forEach(t),_So=r(UAe," (mBART model)"),UAe.forEach(t),bSo=i(P),xT=s(P,"LI",{});var JAe=n(xT);ble=s(JAe,"STRONG",{});var Got=n(ble);vSo=r(Got,"megatron-bert"),Got.forEach(t),TSo=r(JAe," \u2014 "),tD=s(JAe,"A",{href:!0});var Xot=n(tD);FSo=r(Xot,"MegatronBertForQuestionAnswering"),Xot.forEach(t),CSo=r(JAe," (MegatronBert model)"),JAe.forEach(t),MSo=i(P),kT=s(P,"LI",{});var YAe=n(kT);vle=s(YAe,"STRONG",{});var Vot=n(vle);ESo=r(Vot,"mobilebert"),Vot.forEach(t),ySo=r(YAe," \u2014 "),aD=s(YAe,"A",{href:!0});var zot=n(aD);wSo=r(zot,"MobileBertForQuestionAnswering"),zot.forEach(t),ASo=r(YAe," (MobileBERT model)"),YAe.forEach(t),LSo=i(P),RT=s(P,"LI",{});var KAe=n(RT);Tle=s(KAe,"STRONG",{});var Wot=n(Tle);BSo=r(Wot,"mpnet"),Wot.forEach(t),xSo=r(KAe," \u2014 "),sD=s(KAe,"A",{href:!0});var Qot=n(sD);kSo=r(Qot,"MPNetForQuestionAnswering"),Qot.forEach(t),RSo=r(KAe," (MPNet model)"),KAe.forEach(t),SSo=i(P),ST=s(P,"LI",{});var ZAe=n(ST);Fle=s(ZAe,"STRONG",{});var Hot=n(Fle);PSo=r(Hot,"nystromformer"),Hot.forEach(t),$So=r(ZAe," \u2014 "),nD=s(ZAe,"A",{href:!0});var Uot=n(nD);ISo=r(Uot,"NystromformerForQuestionAnswering"),Uot.forEach(t),jSo=r(ZAe," (Nystromformer model)"),ZAe.forEach(t),NSo=i(P),PT=s(P,"LI",{});var e0e=n(PT);Cle=s(e0e,"STRONG",{});var Jot=n(Cle);DSo=r(Jot,"qdqbert"),Jot.forEach(t),qSo=r(e0e," \u2014 "),lD=s(e0e,"A",{href:!0});var Yot=n(lD);OSo=r(Yot,"QDQBertForQuestionAnswering"),Yot.forEach(t),GSo=r(e0e," (QDQBert model)"),e0e.forEach(t),XSo=i(P),$T=s(P,"LI",{});var o0e=n($T);Mle=s(o0e,"STRONG",{});var Kot=n(Mle);VSo=r(Kot,"reformer"),Kot.forEach(t),zSo=r(o0e," \u2014 "),iD=s(o0e,"A",{href:!0});var Zot=n(iD);WSo=r(Zot,"ReformerForQuestionAnswering"),Zot.forEach(t),QSo=r(o0e," (Reformer model)"),o0e.forEach(t),HSo=i(P),IT=s(P,"LI",{});var r0e=n(IT);Ele=s(r0e,"STRONG",{});var ert=n(Ele);USo=r(ert,"rembert"),ert.forEach(t),JSo=r(r0e," \u2014 "),dD=s(r0e,"A",{href:!0});var ort=n(dD);YSo=r(ort,"RemBertForQuestionAnswering"),ort.forEach(t),KSo=r(r0e," (RemBERT model)"),r0e.forEach(t),ZSo=i(P),jT=s(P,"LI",{});var t0e=n(jT);yle=s(t0e,"STRONG",{});var rrt=n(yle);ePo=r(rrt,"roberta"),rrt.forEach(t),oPo=r(t0e," \u2014 "),cD=s(t0e,"A",{href:!0});var trt=n(cD);rPo=r(trt,"RobertaForQuestionAnswering"),trt.forEach(t),tPo=r(t0e," (RoBERTa model)"),t0e.forEach(t),aPo=i(P),NT=s(P,"LI",{});var a0e=n(NT);wle=s(a0e,"STRONG",{});var art=n(wle);sPo=r(art,"roformer"),art.forEach(t),nPo=r(a0e," \u2014 "),mD=s(a0e,"A",{href:!0});var srt=n(mD);lPo=r(srt,"RoFormerForQuestionAnswering"),srt.forEach(t),iPo=r(a0e," (RoFormer model)"),a0e.forEach(t),dPo=i(P),DT=s(P,"LI",{});var s0e=n(DT);Ale=s(s0e,"STRONG",{});var nrt=n(Ale);cPo=r(nrt,"splinter"),nrt.forEach(t),mPo=r(s0e," \u2014 "),fD=s(s0e,"A",{href:!0});var lrt=n(fD);fPo=r(lrt,"SplinterForQuestionAnswering"),lrt.forEach(t),gPo=r(s0e," (Splinter model)"),s0e.forEach(t),hPo=i(P),qT=s(P,"LI",{});var n0e=n(qT);Lle=s(n0e,"STRONG",{});var irt=n(Lle);uPo=r(irt,"squeezebert"),irt.forEach(t),pPo=r(n0e," \u2014 "),gD=s(n0e,"A",{href:!0});var drt=n(gD);_Po=r(drt,"SqueezeBertForQuestionAnswering"),drt.forEach(t),bPo=r(n0e," (SqueezeBERT model)"),n0e.forEach(t),vPo=i(P),OT=s(P,"LI",{});var l0e=n(OT);Ble=s(l0e,"STRONG",{});var crt=n(Ble);TPo=r(crt,"xlm"),crt.forEach(t),FPo=r(l0e," \u2014 "),hD=s(l0e,"A",{href:!0});var mrt=n(hD);CPo=r(mrt,"XLMForQuestionAnsweringSimple"),mrt.forEach(t),MPo=r(l0e," (XLM model)"),l0e.forEach(t),EPo=i(P),GT=s(P,"LI",{});var i0e=n(GT);xle=s(i0e,"STRONG",{});var frt=n(xle);yPo=r(frt,"xlm-roberta"),frt.forEach(t),wPo=r(i0e," \u2014 "),uD=s(i0e,"A",{href:!0});var grt=n(uD);APo=r(grt,"XLMRobertaForQuestionAnswering"),grt.forEach(t),LPo=r(i0e," (XLM-RoBERTa model)"),i0e.forEach(t),BPo=i(P),XT=s(P,"LI",{});var d0e=n(XT);kle=s(d0e,"STRONG",{});var hrt=n(kle);xPo=r(hrt,"xlm-roberta-xl"),hrt.forEach(t),kPo=r(d0e," \u2014 "),pD=s(d0e,"A",{href:!0});var urt=n(pD);RPo=r(urt,"XLMRobertaXLForQuestionAnswering"),urt.forEach(t),SPo=r(d0e," (XLM-RoBERTa-XL model)"),d0e.forEach(t),PPo=i(P),VT=s(P,"LI",{});var c0e=n(VT);Rle=s(c0e,"STRONG",{});var prt=n(Rle);$Po=r(prt,"xlnet"),prt.forEach(t),IPo=r(c0e," \u2014 "),_D=s(c0e,"A",{href:!0});var _rt=n(_D);jPo=r(_rt,"XLNetForQuestionAnsweringSimple"),_rt.forEach(t),NPo=r(c0e," (XLNet model)"),c0e.forEach(t),DPo=i(P),zT=s(P,"LI",{});var m0e=n(zT);Sle=s(m0e,"STRONG",{});var brt=n(Sle);qPo=r(brt,"yoso"),brt.forEach(t),OPo=r(m0e," \u2014 "),bD=s(m0e,"A",{href:!0});var vrt=n(bD);GPo=r(vrt,"YosoForQuestionAnswering"),vrt.forEach(t),XPo=r(m0e," (YOSO model)"),m0e.forEach(t),P.forEach(t),VPo=i(Ut),WT=s(Ut,"P",{});var f0e=n(WT);zPo=r(f0e,"The model is set in evaluation mode by default using "),Ple=s(f0e,"CODE",{});var Trt=n(Ple);WPo=r(Trt,"model.eval()"),Trt.forEach(t),QPo=r(f0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$le=s(f0e,"CODE",{});var Frt=n($le);HPo=r(Frt,"model.train()"),Frt.forEach(t),f0e.forEach(t),UPo=i(Ut),Ile=s(Ut,"P",{});var Crt=n(Ile);JPo=r(Crt,"Examples:"),Crt.forEach(t),YPo=i(Ut),f(Jw.$$.fragment,Ut),Ut.forEach(t),ml.forEach(t),_ke=i(c),Ad=s(c,"H2",{class:!0});var wSe=n(Ad);QT=s(wSe,"A",{id:!0,class:!0,href:!0});var Mrt=n(QT);jle=s(Mrt,"SPAN",{});var Ert=n(jle);f(Yw.$$.fragment,Ert),Ert.forEach(t),Mrt.forEach(t),KPo=i(wSe),Nle=s(wSe,"SPAN",{});var yrt=n(Nle);ZPo=r(yrt,"AutoModelForTableQuestionAnswering"),yrt.forEach(t),wSe.forEach(t),bke=i(c),nr=s(c,"DIV",{class:!0});var gl=n(nr);f(Kw.$$.fragment,gl),e$o=i(gl),Ld=s(gl,"P",{});var lW=n(Ld);o$o=r(lW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Dle=s(lW,"CODE",{});var wrt=n(Dle);r$o=r(wrt,"from_pretrained()"),wrt.forEach(t),t$o=r(lW,"class method or the "),qle=s(lW,"CODE",{});var Art=n(qle);a$o=r(Art,"from_config()"),Art.forEach(t),s$o=r(lW,`class
method.`),lW.forEach(t),n$o=i(gl),Zw=s(gl,"P",{});var ASe=n(Zw);l$o=r(ASe,"This class cannot be instantiated directly using "),Ole=s(ASe,"CODE",{});var Lrt=n(Ole);i$o=r(Lrt,"__init__()"),Lrt.forEach(t),d$o=r(ASe," (throws an error)."),ASe.forEach(t),c$o=i(gl),et=s(gl,"DIV",{class:!0});var hl=n(et);f(e6.$$.fragment,hl),m$o=i(hl),Gle=s(hl,"P",{});var Brt=n(Gle);f$o=r(Brt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Brt.forEach(t),g$o=i(hl),Bd=s(hl,"P",{});var iW=n(Bd);h$o=r(iW,`Note:
Loading a model from its configuration file does `),Xle=s(iW,"STRONG",{});var xrt=n(Xle);u$o=r(xrt,"not"),xrt.forEach(t),p$o=r(iW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vle=s(iW,"CODE",{});var krt=n(Vle);_$o=r(krt,"from_pretrained()"),krt.forEach(t),b$o=r(iW,"to load the model weights."),iW.forEach(t),v$o=i(hl),zle=s(hl,"P",{});var Rrt=n(zle);T$o=r(Rrt,"Examples:"),Rrt.forEach(t),F$o=i(hl),f(o6.$$.fragment,hl),hl.forEach(t),C$o=i(gl),He=s(gl,"DIV",{class:!0});var Jt=n(He);f(r6.$$.fragment,Jt),M$o=i(Jt),Wle=s(Jt,"P",{});var Srt=n(Wle);E$o=r(Srt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Srt.forEach(t),y$o=i(Jt),es=s(Jt,"P",{});var u5=n(es);w$o=r(u5,"The model class to instantiate is selected based on the "),Qle=s(u5,"CODE",{});var Prt=n(Qle);A$o=r(Prt,"model_type"),Prt.forEach(t),L$o=r(u5,` property of the config object (either
passed as an argument or loaded from `),Hle=s(u5,"CODE",{});var $rt=n(Hle);B$o=r($rt,"pretrained_model_name_or_path"),$rt.forEach(t),x$o=r(u5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ule=s(u5,"CODE",{});var Irt=n(Ule);k$o=r(Irt,"pretrained_model_name_or_path"),Irt.forEach(t),R$o=r(u5,":"),u5.forEach(t),S$o=i(Jt),Jle=s(Jt,"UL",{});var jrt=n(Jle);HT=s(jrt,"LI",{});var g0e=n(HT);Yle=s(g0e,"STRONG",{});var Nrt=n(Yle);P$o=r(Nrt,"tapas"),Nrt.forEach(t),$$o=r(g0e," \u2014 "),vD=s(g0e,"A",{href:!0});var Drt=n(vD);I$o=r(Drt,"TapasForQuestionAnswering"),Drt.forEach(t),j$o=r(g0e," (TAPAS model)"),g0e.forEach(t),jrt.forEach(t),N$o=i(Jt),UT=s(Jt,"P",{});var h0e=n(UT);D$o=r(h0e,"The model is set in evaluation mode by default using "),Kle=s(h0e,"CODE",{});var qrt=n(Kle);q$o=r(qrt,"model.eval()"),qrt.forEach(t),O$o=r(h0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zle=s(h0e,"CODE",{});var Ort=n(Zle);G$o=r(Ort,"model.train()"),Ort.forEach(t),h0e.forEach(t),X$o=i(Jt),eie=s(Jt,"P",{});var Grt=n(eie);V$o=r(Grt,"Examples:"),Grt.forEach(t),z$o=i(Jt),f(t6.$$.fragment,Jt),Jt.forEach(t),gl.forEach(t),vke=i(c),xd=s(c,"H2",{class:!0});var LSe=n(xd);JT=s(LSe,"A",{id:!0,class:!0,href:!0});var Xrt=n(JT);oie=s(Xrt,"SPAN",{});var Vrt=n(oie);f(a6.$$.fragment,Vrt),Vrt.forEach(t),Xrt.forEach(t),W$o=i(LSe),rie=s(LSe,"SPAN",{});var zrt=n(rie);Q$o=r(zrt,"AutoModelForImageClassification"),zrt.forEach(t),LSe.forEach(t),Tke=i(c),lr=s(c,"DIV",{class:!0});var ul=n(lr);f(s6.$$.fragment,ul),H$o=i(ul),kd=s(ul,"P",{});var dW=n(kd);U$o=r(dW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),tie=s(dW,"CODE",{});var Wrt=n(tie);J$o=r(Wrt,"from_pretrained()"),Wrt.forEach(t),Y$o=r(dW,"class method or the "),aie=s(dW,"CODE",{});var Qrt=n(aie);K$o=r(Qrt,"from_config()"),Qrt.forEach(t),Z$o=r(dW,`class
method.`),dW.forEach(t),eIo=i(ul),n6=s(ul,"P",{});var BSe=n(n6);oIo=r(BSe,"This class cannot be instantiated directly using "),sie=s(BSe,"CODE",{});var Hrt=n(sie);rIo=r(Hrt,"__init__()"),Hrt.forEach(t),tIo=r(BSe," (throws an error)."),BSe.forEach(t),aIo=i(ul),ot=s(ul,"DIV",{class:!0});var pl=n(ot);f(l6.$$.fragment,pl),sIo=i(pl),nie=s(pl,"P",{});var Urt=n(nie);nIo=r(Urt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Urt.forEach(t),lIo=i(pl),Rd=s(pl,"P",{});var cW=n(Rd);iIo=r(cW,`Note:
Loading a model from its configuration file does `),lie=s(cW,"STRONG",{});var Jrt=n(lie);dIo=r(Jrt,"not"),Jrt.forEach(t),cIo=r(cW,` load the model weights. It only affects the
model\u2019s configuration. Use `),iie=s(cW,"CODE",{});var Yrt=n(iie);mIo=r(Yrt,"from_pretrained()"),Yrt.forEach(t),fIo=r(cW,"to load the model weights."),cW.forEach(t),gIo=i(pl),die=s(pl,"P",{});var Krt=n(die);hIo=r(Krt,"Examples:"),Krt.forEach(t),uIo=i(pl),f(i6.$$.fragment,pl),pl.forEach(t),pIo=i(ul),Ue=s(ul,"DIV",{class:!0});var Yt=n(Ue);f(d6.$$.fragment,Yt),_Io=i(Yt),cie=s(Yt,"P",{});var Zrt=n(cie);bIo=r(Zrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Zrt.forEach(t),vIo=i(Yt),os=s(Yt,"P",{});var p5=n(os);TIo=r(p5,"The model class to instantiate is selected based on the "),mie=s(p5,"CODE",{});var ett=n(mie);FIo=r(ett,"model_type"),ett.forEach(t),CIo=r(p5,` property of the config object (either
passed as an argument or loaded from `),fie=s(p5,"CODE",{});var ott=n(fie);MIo=r(ott,"pretrained_model_name_or_path"),ott.forEach(t),EIo=r(p5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gie=s(p5,"CODE",{});var rtt=n(gie);yIo=r(rtt,"pretrained_model_name_or_path"),rtt.forEach(t),wIo=r(p5,":"),p5.forEach(t),AIo=i(Yt),fe=s(Yt,"UL",{});var Fe=n(fe);YT=s(Fe,"LI",{});var u0e=n(YT);hie=s(u0e,"STRONG",{});var ttt=n(hie);LIo=r(ttt,"beit"),ttt.forEach(t),BIo=r(u0e," \u2014 "),TD=s(u0e,"A",{href:!0});var att=n(TD);xIo=r(att,"BeitForImageClassification"),att.forEach(t),kIo=r(u0e," (BEiT model)"),u0e.forEach(t),RIo=i(Fe),KT=s(Fe,"LI",{});var p0e=n(KT);uie=s(p0e,"STRONG",{});var stt=n(uie);SIo=r(stt,"convnext"),stt.forEach(t),PIo=r(p0e," \u2014 "),FD=s(p0e,"A",{href:!0});var ntt=n(FD);$Io=r(ntt,"ConvNextForImageClassification"),ntt.forEach(t),IIo=r(p0e," (ConvNext model)"),p0e.forEach(t),jIo=i(Fe),qn=s(Fe,"LI",{});var W8=n(qn);pie=s(W8,"STRONG",{});var ltt=n(pie);NIo=r(ltt,"deit"),ltt.forEach(t),DIo=r(W8," \u2014 "),CD=s(W8,"A",{href:!0});var itt=n(CD);qIo=r(itt,"DeiTForImageClassification"),itt.forEach(t),OIo=r(W8," or "),MD=s(W8,"A",{href:!0});var dtt=n(MD);GIo=r(dtt,"DeiTForImageClassificationWithTeacher"),dtt.forEach(t),XIo=r(W8," (DeiT model)"),W8.forEach(t),VIo=i(Fe),ZT=s(Fe,"LI",{});var _0e=n(ZT);_ie=s(_0e,"STRONG",{});var ctt=n(_ie);zIo=r(ctt,"imagegpt"),ctt.forEach(t),WIo=r(_0e," \u2014 "),ED=s(_0e,"A",{href:!0});var mtt=n(ED);QIo=r(mtt,"ImageGPTForImageClassification"),mtt.forEach(t),HIo=r(_0e," (ImageGPT model)"),_0e.forEach(t),UIo=i(Fe),fa=s(Fe,"LI",{});var Pm=n(fa);bie=s(Pm,"STRONG",{});var ftt=n(bie);JIo=r(ftt,"perceiver"),ftt.forEach(t),YIo=r(Pm," \u2014 "),yD=s(Pm,"A",{href:!0});var gtt=n(yD);KIo=r(gtt,"PerceiverForImageClassificationLearned"),gtt.forEach(t),ZIo=r(Pm," or "),wD=s(Pm,"A",{href:!0});var htt=n(wD);ejo=r(htt,"PerceiverForImageClassificationFourier"),htt.forEach(t),ojo=r(Pm," or "),AD=s(Pm,"A",{href:!0});var utt=n(AD);rjo=r(utt,"PerceiverForImageClassificationConvProcessing"),utt.forEach(t),tjo=r(Pm," (Perceiver model)"),Pm.forEach(t),ajo=i(Fe),e1=s(Fe,"LI",{});var b0e=n(e1);vie=s(b0e,"STRONG",{});var ptt=n(vie);sjo=r(ptt,"poolformer"),ptt.forEach(t),njo=r(b0e," \u2014 "),LD=s(b0e,"A",{href:!0});var _tt=n(LD);ljo=r(_tt,"PoolFormerForImageClassification"),_tt.forEach(t),ijo=r(b0e," (PoolFormer model)"),b0e.forEach(t),djo=i(Fe),o1=s(Fe,"LI",{});var v0e=n(o1);Tie=s(v0e,"STRONG",{});var btt=n(Tie);cjo=r(btt,"resnet"),btt.forEach(t),mjo=r(v0e," \u2014 "),BD=s(v0e,"A",{href:!0});var vtt=n(BD);fjo=r(vtt,"ResNetForImageClassification"),vtt.forEach(t),gjo=r(v0e," (ResNet model)"),v0e.forEach(t),hjo=i(Fe),r1=s(Fe,"LI",{});var T0e=n(r1);Fie=s(T0e,"STRONG",{});var Ttt=n(Fie);ujo=r(Ttt,"segformer"),Ttt.forEach(t),pjo=r(T0e," \u2014 "),xD=s(T0e,"A",{href:!0});var Ftt=n(xD);_jo=r(Ftt,"SegformerForImageClassification"),Ftt.forEach(t),bjo=r(T0e," (SegFormer model)"),T0e.forEach(t),vjo=i(Fe),t1=s(Fe,"LI",{});var F0e=n(t1);Cie=s(F0e,"STRONG",{});var Ctt=n(Cie);Tjo=r(Ctt,"swin"),Ctt.forEach(t),Fjo=r(F0e," \u2014 "),kD=s(F0e,"A",{href:!0});var Mtt=n(kD);Cjo=r(Mtt,"SwinForImageClassification"),Mtt.forEach(t),Mjo=r(F0e," (Swin model)"),F0e.forEach(t),Ejo=i(Fe),a1=s(Fe,"LI",{});var C0e=n(a1);Mie=s(C0e,"STRONG",{});var Ett=n(Mie);yjo=r(Ett,"van"),Ett.forEach(t),wjo=r(C0e," \u2014 "),RD=s(C0e,"A",{href:!0});var ytt=n(RD);Ajo=r(ytt,"VanForImageClassification"),ytt.forEach(t),Ljo=r(C0e," (VAN model)"),C0e.forEach(t),Bjo=i(Fe),s1=s(Fe,"LI",{});var M0e=n(s1);Eie=s(M0e,"STRONG",{});var wtt=n(Eie);xjo=r(wtt,"vit"),wtt.forEach(t),kjo=r(M0e," \u2014 "),SD=s(M0e,"A",{href:!0});var Att=n(SD);Rjo=r(Att,"ViTForImageClassification"),Att.forEach(t),Sjo=r(M0e," (ViT model)"),M0e.forEach(t),Fe.forEach(t),Pjo=i(Yt),n1=s(Yt,"P",{});var E0e=n(n1);$jo=r(E0e,"The model is set in evaluation mode by default using "),yie=s(E0e,"CODE",{});var Ltt=n(yie);Ijo=r(Ltt,"model.eval()"),Ltt.forEach(t),jjo=r(E0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wie=s(E0e,"CODE",{});var Btt=n(wie);Njo=r(Btt,"model.train()"),Btt.forEach(t),E0e.forEach(t),Djo=i(Yt),Aie=s(Yt,"P",{});var xtt=n(Aie);qjo=r(xtt,"Examples:"),xtt.forEach(t),Ojo=i(Yt),f(c6.$$.fragment,Yt),Yt.forEach(t),ul.forEach(t),Fke=i(c),Sd=s(c,"H2",{class:!0});var xSe=n(Sd);l1=s(xSe,"A",{id:!0,class:!0,href:!0});var ktt=n(l1);Lie=s(ktt,"SPAN",{});var Rtt=n(Lie);f(m6.$$.fragment,Rtt),Rtt.forEach(t),ktt.forEach(t),Gjo=i(xSe),Bie=s(xSe,"SPAN",{});var Stt=n(Bie);Xjo=r(Stt,"AutoModelForVision2Seq"),Stt.forEach(t),xSe.forEach(t),Cke=i(c),ir=s(c,"DIV",{class:!0});var _l=n(ir);f(f6.$$.fragment,_l),Vjo=i(_l),Pd=s(_l,"P",{});var mW=n(Pd);zjo=r(mW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),xie=s(mW,"CODE",{});var Ptt=n(xie);Wjo=r(Ptt,"from_pretrained()"),Ptt.forEach(t),Qjo=r(mW,"class method or the "),kie=s(mW,"CODE",{});var $tt=n(kie);Hjo=r($tt,"from_config()"),$tt.forEach(t),Ujo=r(mW,`class
method.`),mW.forEach(t),Jjo=i(_l),g6=s(_l,"P",{});var kSe=n(g6);Yjo=r(kSe,"This class cannot be instantiated directly using "),Rie=s(kSe,"CODE",{});var Itt=n(Rie);Kjo=r(Itt,"__init__()"),Itt.forEach(t),Zjo=r(kSe," (throws an error)."),kSe.forEach(t),eNo=i(_l),rt=s(_l,"DIV",{class:!0});var bl=n(rt);f(h6.$$.fragment,bl),oNo=i(bl),Sie=s(bl,"P",{});var jtt=n(Sie);rNo=r(jtt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),jtt.forEach(t),tNo=i(bl),$d=s(bl,"P",{});var fW=n($d);aNo=r(fW,`Note:
Loading a model from its configuration file does `),Pie=s(fW,"STRONG",{});var Ntt=n(Pie);sNo=r(Ntt,"not"),Ntt.forEach(t),nNo=r(fW,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ie=s(fW,"CODE",{});var Dtt=n($ie);lNo=r(Dtt,"from_pretrained()"),Dtt.forEach(t),iNo=r(fW,"to load the model weights."),fW.forEach(t),dNo=i(bl),Iie=s(bl,"P",{});var qtt=n(Iie);cNo=r(qtt,"Examples:"),qtt.forEach(t),mNo=i(bl),f(u6.$$.fragment,bl),bl.forEach(t),fNo=i(_l),Je=s(_l,"DIV",{class:!0});var Kt=n(Je);f(p6.$$.fragment,Kt),gNo=i(Kt),jie=s(Kt,"P",{});var Ott=n(jie);hNo=r(Ott,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Ott.forEach(t),uNo=i(Kt),rs=s(Kt,"P",{});var _5=n(rs);pNo=r(_5,"The model class to instantiate is selected based on the "),Nie=s(_5,"CODE",{});var Gtt=n(Nie);_No=r(Gtt,"model_type"),Gtt.forEach(t),bNo=r(_5,` property of the config object (either
passed as an argument or loaded from `),Die=s(_5,"CODE",{});var Xtt=n(Die);vNo=r(Xtt,"pretrained_model_name_or_path"),Xtt.forEach(t),TNo=r(_5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qie=s(_5,"CODE",{});var Vtt=n(qie);FNo=r(Vtt,"pretrained_model_name_or_path"),Vtt.forEach(t),CNo=r(_5,":"),_5.forEach(t),MNo=i(Kt),Oie=s(Kt,"UL",{});var ztt=n(Oie);i1=s(ztt,"LI",{});var y0e=n(i1);Gie=s(y0e,"STRONG",{});var Wtt=n(Gie);ENo=r(Wtt,"vision-encoder-decoder"),Wtt.forEach(t),yNo=r(y0e," \u2014 "),PD=s(y0e,"A",{href:!0});var Qtt=n(PD);wNo=r(Qtt,"VisionEncoderDecoderModel"),Qtt.forEach(t),ANo=r(y0e," (Vision Encoder decoder model)"),y0e.forEach(t),ztt.forEach(t),LNo=i(Kt),d1=s(Kt,"P",{});var w0e=n(d1);BNo=r(w0e,"The model is set in evaluation mode by default using "),Xie=s(w0e,"CODE",{});var Htt=n(Xie);xNo=r(Htt,"model.eval()"),Htt.forEach(t),kNo=r(w0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Vie=s(w0e,"CODE",{});var Utt=n(Vie);RNo=r(Utt,"model.train()"),Utt.forEach(t),w0e.forEach(t),SNo=i(Kt),zie=s(Kt,"P",{});var Jtt=n(zie);PNo=r(Jtt,"Examples:"),Jtt.forEach(t),$No=i(Kt),f(_6.$$.fragment,Kt),Kt.forEach(t),_l.forEach(t),Mke=i(c),Id=s(c,"H2",{class:!0});var RSe=n(Id);c1=s(RSe,"A",{id:!0,class:!0,href:!0});var Ytt=n(c1);Wie=s(Ytt,"SPAN",{});var Ktt=n(Wie);f(b6.$$.fragment,Ktt),Ktt.forEach(t),Ytt.forEach(t),INo=i(RSe),Qie=s(RSe,"SPAN",{});var Ztt=n(Qie);jNo=r(Ztt,"AutoModelForAudioClassification"),Ztt.forEach(t),RSe.forEach(t),Eke=i(c),dr=s(c,"DIV",{class:!0});var vl=n(dr);f(v6.$$.fragment,vl),NNo=i(vl),jd=s(vl,"P",{});var gW=n(jd);DNo=r(gW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Hie=s(gW,"CODE",{});var eat=n(Hie);qNo=r(eat,"from_pretrained()"),eat.forEach(t),ONo=r(gW,"class method or the "),Uie=s(gW,"CODE",{});var oat=n(Uie);GNo=r(oat,"from_config()"),oat.forEach(t),XNo=r(gW,`class
method.`),gW.forEach(t),VNo=i(vl),T6=s(vl,"P",{});var SSe=n(T6);zNo=r(SSe,"This class cannot be instantiated directly using "),Jie=s(SSe,"CODE",{});var rat=n(Jie);WNo=r(rat,"__init__()"),rat.forEach(t),QNo=r(SSe," (throws an error)."),SSe.forEach(t),HNo=i(vl),tt=s(vl,"DIV",{class:!0});var Tl=n(tt);f(F6.$$.fragment,Tl),UNo=i(Tl),Yie=s(Tl,"P",{});var tat=n(Yie);JNo=r(tat,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),tat.forEach(t),YNo=i(Tl),Nd=s(Tl,"P",{});var hW=n(Nd);KNo=r(hW,`Note:
Loading a model from its configuration file does `),Kie=s(hW,"STRONG",{});var aat=n(Kie);ZNo=r(aat,"not"),aat.forEach(t),eDo=r(hW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zie=s(hW,"CODE",{});var sat=n(Zie);oDo=r(sat,"from_pretrained()"),sat.forEach(t),rDo=r(hW,"to load the model weights."),hW.forEach(t),tDo=i(Tl),ede=s(Tl,"P",{});var nat=n(ede);aDo=r(nat,"Examples:"),nat.forEach(t),sDo=i(Tl),f(C6.$$.fragment,Tl),Tl.forEach(t),nDo=i(vl),Ye=s(vl,"DIV",{class:!0});var Zt=n(Ye);f(M6.$$.fragment,Zt),lDo=i(Zt),ode=s(Zt,"P",{});var lat=n(ode);iDo=r(lat,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),lat.forEach(t),dDo=i(Zt),ts=s(Zt,"P",{});var b5=n(ts);cDo=r(b5,"The model class to instantiate is selected based on the "),rde=s(b5,"CODE",{});var iat=n(rde);mDo=r(iat,"model_type"),iat.forEach(t),fDo=r(b5,` property of the config object (either
passed as an argument or loaded from `),tde=s(b5,"CODE",{});var dat=n(tde);gDo=r(dat,"pretrained_model_name_or_path"),dat.forEach(t),hDo=r(b5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ade=s(b5,"CODE",{});var cat=n(ade);uDo=r(cat,"pretrained_model_name_or_path"),cat.forEach(t),pDo=r(b5,":"),b5.forEach(t),_Do=i(Zt),ke=s(Zt,"UL",{});var qo=n(ke);m1=s(qo,"LI",{});var A0e=n(m1);sde=s(A0e,"STRONG",{});var mat=n(sde);bDo=r(mat,"data2vec-audio"),mat.forEach(t),vDo=r(A0e," \u2014 "),$D=s(A0e,"A",{href:!0});var fat=n($D);TDo=r(fat,"Data2VecAudioForSequenceClassification"),fat.forEach(t),FDo=r(A0e," (Data2VecAudio model)"),A0e.forEach(t),CDo=i(qo),f1=s(qo,"LI",{});var L0e=n(f1);nde=s(L0e,"STRONG",{});var gat=n(nde);MDo=r(gat,"hubert"),gat.forEach(t),EDo=r(L0e," \u2014 "),ID=s(L0e,"A",{href:!0});var hat=n(ID);yDo=r(hat,"HubertForSequenceClassification"),hat.forEach(t),wDo=r(L0e," (Hubert model)"),L0e.forEach(t),ADo=i(qo),g1=s(qo,"LI",{});var B0e=n(g1);lde=s(B0e,"STRONG",{});var uat=n(lde);LDo=r(uat,"sew"),uat.forEach(t),BDo=r(B0e," \u2014 "),jD=s(B0e,"A",{href:!0});var pat=n(jD);xDo=r(pat,"SEWForSequenceClassification"),pat.forEach(t),kDo=r(B0e," (SEW model)"),B0e.forEach(t),RDo=i(qo),h1=s(qo,"LI",{});var x0e=n(h1);ide=s(x0e,"STRONG",{});var _at=n(ide);SDo=r(_at,"sew-d"),_at.forEach(t),PDo=r(x0e," \u2014 "),ND=s(x0e,"A",{href:!0});var bat=n(ND);$Do=r(bat,"SEWDForSequenceClassification"),bat.forEach(t),IDo=r(x0e," (SEW-D model)"),x0e.forEach(t),jDo=i(qo),u1=s(qo,"LI",{});var k0e=n(u1);dde=s(k0e,"STRONG",{});var vat=n(dde);NDo=r(vat,"unispeech"),vat.forEach(t),DDo=r(k0e," \u2014 "),DD=s(k0e,"A",{href:!0});var Tat=n(DD);qDo=r(Tat,"UniSpeechForSequenceClassification"),Tat.forEach(t),ODo=r(k0e," (UniSpeech model)"),k0e.forEach(t),GDo=i(qo),p1=s(qo,"LI",{});var R0e=n(p1);cde=s(R0e,"STRONG",{});var Fat=n(cde);XDo=r(Fat,"unispeech-sat"),Fat.forEach(t),VDo=r(R0e," \u2014 "),qD=s(R0e,"A",{href:!0});var Cat=n(qD);zDo=r(Cat,"UniSpeechSatForSequenceClassification"),Cat.forEach(t),WDo=r(R0e," (UniSpeechSat model)"),R0e.forEach(t),QDo=i(qo),_1=s(qo,"LI",{});var S0e=n(_1);mde=s(S0e,"STRONG",{});var Mat=n(mde);HDo=r(Mat,"wav2vec2"),Mat.forEach(t),UDo=r(S0e," \u2014 "),OD=s(S0e,"A",{href:!0});var Eat=n(OD);JDo=r(Eat,"Wav2Vec2ForSequenceClassification"),Eat.forEach(t),YDo=r(S0e," (Wav2Vec2 model)"),S0e.forEach(t),KDo=i(qo),b1=s(qo,"LI",{});var P0e=n(b1);fde=s(P0e,"STRONG",{});var yat=n(fde);ZDo=r(yat,"wavlm"),yat.forEach(t),eqo=r(P0e," \u2014 "),GD=s(P0e,"A",{href:!0});var wat=n(GD);oqo=r(wat,"WavLMForSequenceClassification"),wat.forEach(t),rqo=r(P0e," (WavLM model)"),P0e.forEach(t),qo.forEach(t),tqo=i(Zt),v1=s(Zt,"P",{});var $0e=n(v1);aqo=r($0e,"The model is set in evaluation mode by default using "),gde=s($0e,"CODE",{});var Aat=n(gde);sqo=r(Aat,"model.eval()"),Aat.forEach(t),nqo=r($0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hde=s($0e,"CODE",{});var Lat=n(hde);lqo=r(Lat,"model.train()"),Lat.forEach(t),$0e.forEach(t),iqo=i(Zt),ude=s(Zt,"P",{});var Bat=n(ude);dqo=r(Bat,"Examples:"),Bat.forEach(t),cqo=i(Zt),f(E6.$$.fragment,Zt),Zt.forEach(t),vl.forEach(t),yke=i(c),Dd=s(c,"H2",{class:!0});var PSe=n(Dd);T1=s(PSe,"A",{id:!0,class:!0,href:!0});var xat=n(T1);pde=s(xat,"SPAN",{});var kat=n(pde);f(y6.$$.fragment,kat),kat.forEach(t),xat.forEach(t),mqo=i(PSe),_de=s(PSe,"SPAN",{});var Rat=n(_de);fqo=r(Rat,"AutoModelForAudioFrameClassification"),Rat.forEach(t),PSe.forEach(t),wke=i(c),cr=s(c,"DIV",{class:!0});var Fl=n(cr);f(w6.$$.fragment,Fl),gqo=i(Fl),qd=s(Fl,"P",{});var uW=n(qd);hqo=r(uW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),bde=s(uW,"CODE",{});var Sat=n(bde);uqo=r(Sat,"from_pretrained()"),Sat.forEach(t),pqo=r(uW,"class method or the "),vde=s(uW,"CODE",{});var Pat=n(vde);_qo=r(Pat,"from_config()"),Pat.forEach(t),bqo=r(uW,`class
method.`),uW.forEach(t),vqo=i(Fl),A6=s(Fl,"P",{});var $Se=n(A6);Tqo=r($Se,"This class cannot be instantiated directly using "),Tde=s($Se,"CODE",{});var $at=n(Tde);Fqo=r($at,"__init__()"),$at.forEach(t),Cqo=r($Se," (throws an error)."),$Se.forEach(t),Mqo=i(Fl),at=s(Fl,"DIV",{class:!0});var Cl=n(at);f(L6.$$.fragment,Cl),Eqo=i(Cl),Fde=s(Cl,"P",{});var Iat=n(Fde);yqo=r(Iat,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),Iat.forEach(t),wqo=i(Cl),Od=s(Cl,"P",{});var pW=n(Od);Aqo=r(pW,`Note:
Loading a model from its configuration file does `),Cde=s(pW,"STRONG",{});var jat=n(Cde);Lqo=r(jat,"not"),jat.forEach(t),Bqo=r(pW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mde=s(pW,"CODE",{});var Nat=n(Mde);xqo=r(Nat,"from_pretrained()"),Nat.forEach(t),kqo=r(pW,"to load the model weights."),pW.forEach(t),Rqo=i(Cl),Ede=s(Cl,"P",{});var Dat=n(Ede);Sqo=r(Dat,"Examples:"),Dat.forEach(t),Pqo=i(Cl),f(B6.$$.fragment,Cl),Cl.forEach(t),$qo=i(Fl),Ke=s(Fl,"DIV",{class:!0});var ea=n(Ke);f(x6.$$.fragment,ea),Iqo=i(ea),yde=s(ea,"P",{});var qat=n(yde);jqo=r(qat,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),qat.forEach(t),Nqo=i(ea),as=s(ea,"P",{});var v5=n(as);Dqo=r(v5,"The model class to instantiate is selected based on the "),wde=s(v5,"CODE",{});var Oat=n(wde);qqo=r(Oat,"model_type"),Oat.forEach(t),Oqo=r(v5,` property of the config object (either
passed as an argument or loaded from `),Ade=s(v5,"CODE",{});var Gat=n(Ade);Gqo=r(Gat,"pretrained_model_name_or_path"),Gat.forEach(t),Xqo=r(v5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lde=s(v5,"CODE",{});var Xat=n(Lde);Vqo=r(Xat,"pretrained_model_name_or_path"),Xat.forEach(t),zqo=r(v5,":"),v5.forEach(t),Wqo=i(ea),ss=s(ea,"UL",{});var T5=n(ss);F1=s(T5,"LI",{});var I0e=n(F1);Bde=s(I0e,"STRONG",{});var Vat=n(Bde);Qqo=r(Vat,"data2vec-audio"),Vat.forEach(t),Hqo=r(I0e," \u2014 "),XD=s(I0e,"A",{href:!0});var zat=n(XD);Uqo=r(zat,"Data2VecAudioForAudioFrameClassification"),zat.forEach(t),Jqo=r(I0e," (Data2VecAudio model)"),I0e.forEach(t),Yqo=i(T5),C1=s(T5,"LI",{});var j0e=n(C1);xde=s(j0e,"STRONG",{});var Wat=n(xde);Kqo=r(Wat,"unispeech-sat"),Wat.forEach(t),Zqo=r(j0e," \u2014 "),VD=s(j0e,"A",{href:!0});var Qat=n(VD);eOo=r(Qat,"UniSpeechSatForAudioFrameClassification"),Qat.forEach(t),oOo=r(j0e," (UniSpeechSat model)"),j0e.forEach(t),rOo=i(T5),M1=s(T5,"LI",{});var N0e=n(M1);kde=s(N0e,"STRONG",{});var Hat=n(kde);tOo=r(Hat,"wav2vec2"),Hat.forEach(t),aOo=r(N0e," \u2014 "),zD=s(N0e,"A",{href:!0});var Uat=n(zD);sOo=r(Uat,"Wav2Vec2ForAudioFrameClassification"),Uat.forEach(t),nOo=r(N0e," (Wav2Vec2 model)"),N0e.forEach(t),lOo=i(T5),E1=s(T5,"LI",{});var D0e=n(E1);Rde=s(D0e,"STRONG",{});var Jat=n(Rde);iOo=r(Jat,"wavlm"),Jat.forEach(t),dOo=r(D0e," \u2014 "),WD=s(D0e,"A",{href:!0});var Yat=n(WD);cOo=r(Yat,"WavLMForAudioFrameClassification"),Yat.forEach(t),mOo=r(D0e," (WavLM model)"),D0e.forEach(t),T5.forEach(t),fOo=i(ea),y1=s(ea,"P",{});var q0e=n(y1);gOo=r(q0e,"The model is set in evaluation mode by default using "),Sde=s(q0e,"CODE",{});var Kat=n(Sde);hOo=r(Kat,"model.eval()"),Kat.forEach(t),uOo=r(q0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pde=s(q0e,"CODE",{});var Zat=n(Pde);pOo=r(Zat,"model.train()"),Zat.forEach(t),q0e.forEach(t),_Oo=i(ea),$de=s(ea,"P",{});var est=n($de);bOo=r(est,"Examples:"),est.forEach(t),vOo=i(ea),f(k6.$$.fragment,ea),ea.forEach(t),Fl.forEach(t),Ake=i(c),Gd=s(c,"H2",{class:!0});var ISe=n(Gd);w1=s(ISe,"A",{id:!0,class:!0,href:!0});var ost=n(w1);Ide=s(ost,"SPAN",{});var rst=n(Ide);f(R6.$$.fragment,rst),rst.forEach(t),ost.forEach(t),TOo=i(ISe),jde=s(ISe,"SPAN",{});var tst=n(jde);FOo=r(tst,"AutoModelForCTC"),tst.forEach(t),ISe.forEach(t),Lke=i(c),mr=s(c,"DIV",{class:!0});var Ml=n(mr);f(S6.$$.fragment,Ml),COo=i(Ml),Xd=s(Ml,"P",{});var _W=n(Xd);MOo=r(_W,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Nde=s(_W,"CODE",{});var ast=n(Nde);EOo=r(ast,"from_pretrained()"),ast.forEach(t),yOo=r(_W,"class method or the "),Dde=s(_W,"CODE",{});var sst=n(Dde);wOo=r(sst,"from_config()"),sst.forEach(t),AOo=r(_W,`class
method.`),_W.forEach(t),LOo=i(Ml),P6=s(Ml,"P",{});var jSe=n(P6);BOo=r(jSe,"This class cannot be instantiated directly using "),qde=s(jSe,"CODE",{});var nst=n(qde);xOo=r(nst,"__init__()"),nst.forEach(t),kOo=r(jSe," (throws an error)."),jSe.forEach(t),ROo=i(Ml),st=s(Ml,"DIV",{class:!0});var El=n(st);f($6.$$.fragment,El),SOo=i(El),Ode=s(El,"P",{});var lst=n(Ode);POo=r(lst,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),lst.forEach(t),$Oo=i(El),Vd=s(El,"P",{});var bW=n(Vd);IOo=r(bW,`Note:
Loading a model from its configuration file does `),Gde=s(bW,"STRONG",{});var ist=n(Gde);jOo=r(ist,"not"),ist.forEach(t),NOo=r(bW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xde=s(bW,"CODE",{});var dst=n(Xde);DOo=r(dst,"from_pretrained()"),dst.forEach(t),qOo=r(bW,"to load the model weights."),bW.forEach(t),OOo=i(El),Vde=s(El,"P",{});var cst=n(Vde);GOo=r(cst,"Examples:"),cst.forEach(t),XOo=i(El),f(I6.$$.fragment,El),El.forEach(t),VOo=i(Ml),Ze=s(Ml,"DIV",{class:!0});var oa=n(Ze);f(j6.$$.fragment,oa),zOo=i(oa),zde=s(oa,"P",{});var mst=n(zde);WOo=r(mst,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),mst.forEach(t),QOo=i(oa),ns=s(oa,"P",{});var F5=n(ns);HOo=r(F5,"The model class to instantiate is selected based on the "),Wde=s(F5,"CODE",{});var fst=n(Wde);UOo=r(fst,"model_type"),fst.forEach(t),JOo=r(F5,` property of the config object (either
passed as an argument or loaded from `),Qde=s(F5,"CODE",{});var gst=n(Qde);YOo=r(gst,"pretrained_model_name_or_path"),gst.forEach(t),KOo=r(F5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hde=s(F5,"CODE",{});var hst=n(Hde);ZOo=r(hst,"pretrained_model_name_or_path"),hst.forEach(t),eGo=r(F5,":"),F5.forEach(t),oGo=i(oa),Re=s(oa,"UL",{});var Oo=n(Re);A1=s(Oo,"LI",{});var O0e=n(A1);Ude=s(O0e,"STRONG",{});var ust=n(Ude);rGo=r(ust,"data2vec-audio"),ust.forEach(t),tGo=r(O0e," \u2014 "),QD=s(O0e,"A",{href:!0});var pst=n(QD);aGo=r(pst,"Data2VecAudioForCTC"),pst.forEach(t),sGo=r(O0e," (Data2VecAudio model)"),O0e.forEach(t),nGo=i(Oo),L1=s(Oo,"LI",{});var G0e=n(L1);Jde=s(G0e,"STRONG",{});var _st=n(Jde);lGo=r(_st,"hubert"),_st.forEach(t),iGo=r(G0e," \u2014 "),HD=s(G0e,"A",{href:!0});var bst=n(HD);dGo=r(bst,"HubertForCTC"),bst.forEach(t),cGo=r(G0e," (Hubert model)"),G0e.forEach(t),mGo=i(Oo),B1=s(Oo,"LI",{});var X0e=n(B1);Yde=s(X0e,"STRONG",{});var vst=n(Yde);fGo=r(vst,"sew"),vst.forEach(t),gGo=r(X0e," \u2014 "),UD=s(X0e,"A",{href:!0});var Tst=n(UD);hGo=r(Tst,"SEWForCTC"),Tst.forEach(t),uGo=r(X0e," (SEW model)"),X0e.forEach(t),pGo=i(Oo),x1=s(Oo,"LI",{});var V0e=n(x1);Kde=s(V0e,"STRONG",{});var Fst=n(Kde);_Go=r(Fst,"sew-d"),Fst.forEach(t),bGo=r(V0e," \u2014 "),JD=s(V0e,"A",{href:!0});var Cst=n(JD);vGo=r(Cst,"SEWDForCTC"),Cst.forEach(t),TGo=r(V0e," (SEW-D model)"),V0e.forEach(t),FGo=i(Oo),k1=s(Oo,"LI",{});var z0e=n(k1);Zde=s(z0e,"STRONG",{});var Mst=n(Zde);CGo=r(Mst,"unispeech"),Mst.forEach(t),MGo=r(z0e," \u2014 "),YD=s(z0e,"A",{href:!0});var Est=n(YD);EGo=r(Est,"UniSpeechForCTC"),Est.forEach(t),yGo=r(z0e," (UniSpeech model)"),z0e.forEach(t),wGo=i(Oo),R1=s(Oo,"LI",{});var W0e=n(R1);ece=s(W0e,"STRONG",{});var yst=n(ece);AGo=r(yst,"unispeech-sat"),yst.forEach(t),LGo=r(W0e," \u2014 "),KD=s(W0e,"A",{href:!0});var wst=n(KD);BGo=r(wst,"UniSpeechSatForCTC"),wst.forEach(t),xGo=r(W0e," (UniSpeechSat model)"),W0e.forEach(t),kGo=i(Oo),S1=s(Oo,"LI",{});var Q0e=n(S1);oce=s(Q0e,"STRONG",{});var Ast=n(oce);RGo=r(Ast,"wav2vec2"),Ast.forEach(t),SGo=r(Q0e," \u2014 "),ZD=s(Q0e,"A",{href:!0});var Lst=n(ZD);PGo=r(Lst,"Wav2Vec2ForCTC"),Lst.forEach(t),$Go=r(Q0e," (Wav2Vec2 model)"),Q0e.forEach(t),IGo=i(Oo),P1=s(Oo,"LI",{});var H0e=n(P1);rce=s(H0e,"STRONG",{});var Bst=n(rce);jGo=r(Bst,"wavlm"),Bst.forEach(t),NGo=r(H0e," \u2014 "),eq=s(H0e,"A",{href:!0});var xst=n(eq);DGo=r(xst,"WavLMForCTC"),xst.forEach(t),qGo=r(H0e," (WavLM model)"),H0e.forEach(t),Oo.forEach(t),OGo=i(oa),$1=s(oa,"P",{});var U0e=n($1);GGo=r(U0e,"The model is set in evaluation mode by default using "),tce=s(U0e,"CODE",{});var kst=n(tce);XGo=r(kst,"model.eval()"),kst.forEach(t),VGo=r(U0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ace=s(U0e,"CODE",{});var Rst=n(ace);zGo=r(Rst,"model.train()"),Rst.forEach(t),U0e.forEach(t),WGo=i(oa),sce=s(oa,"P",{});var Sst=n(sce);QGo=r(Sst,"Examples:"),Sst.forEach(t),HGo=i(oa),f(N6.$$.fragment,oa),oa.forEach(t),Ml.forEach(t),Bke=i(c),zd=s(c,"H2",{class:!0});var NSe=n(zd);I1=s(NSe,"A",{id:!0,class:!0,href:!0});var Pst=n(I1);nce=s(Pst,"SPAN",{});var $st=n(nce);f(D6.$$.fragment,$st),$st.forEach(t),Pst.forEach(t),UGo=i(NSe),lce=s(NSe,"SPAN",{});var Ist=n(lce);JGo=r(Ist,"AutoModelForSpeechSeq2Seq"),Ist.forEach(t),NSe.forEach(t),xke=i(c),fr=s(c,"DIV",{class:!0});var yl=n(fr);f(q6.$$.fragment,yl),YGo=i(yl),Wd=s(yl,"P",{});var vW=n(Wd);KGo=r(vW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),ice=s(vW,"CODE",{});var jst=n(ice);ZGo=r(jst,"from_pretrained()"),jst.forEach(t),eXo=r(vW,"class method or the "),dce=s(vW,"CODE",{});var Nst=n(dce);oXo=r(Nst,"from_config()"),Nst.forEach(t),rXo=r(vW,`class
method.`),vW.forEach(t),tXo=i(yl),O6=s(yl,"P",{});var DSe=n(O6);aXo=r(DSe,"This class cannot be instantiated directly using "),cce=s(DSe,"CODE",{});var Dst=n(cce);sXo=r(Dst,"__init__()"),Dst.forEach(t),nXo=r(DSe," (throws an error)."),DSe.forEach(t),lXo=i(yl),nt=s(yl,"DIV",{class:!0});var wl=n(nt);f(G6.$$.fragment,wl),iXo=i(wl),mce=s(wl,"P",{});var qst=n(mce);dXo=r(qst,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),qst.forEach(t),cXo=i(wl),Qd=s(wl,"P",{});var TW=n(Qd);mXo=r(TW,`Note:
Loading a model from its configuration file does `),fce=s(TW,"STRONG",{});var Ost=n(fce);fXo=r(Ost,"not"),Ost.forEach(t),gXo=r(TW,` load the model weights. It only affects the
model\u2019s configuration. Use `),gce=s(TW,"CODE",{});var Gst=n(gce);hXo=r(Gst,"from_pretrained()"),Gst.forEach(t),uXo=r(TW,"to load the model weights."),TW.forEach(t),pXo=i(wl),hce=s(wl,"P",{});var Xst=n(hce);_Xo=r(Xst,"Examples:"),Xst.forEach(t),bXo=i(wl),f(X6.$$.fragment,wl),wl.forEach(t),vXo=i(yl),eo=s(yl,"DIV",{class:!0});var ra=n(eo);f(V6.$$.fragment,ra),TXo=i(ra),uce=s(ra,"P",{});var Vst=n(uce);FXo=r(Vst,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Vst.forEach(t),CXo=i(ra),ls=s(ra,"P",{});var C5=n(ls);MXo=r(C5,"The model class to instantiate is selected based on the "),pce=s(C5,"CODE",{});var zst=n(pce);EXo=r(zst,"model_type"),zst.forEach(t),yXo=r(C5,` property of the config object (either
passed as an argument or loaded from `),_ce=s(C5,"CODE",{});var Wst=n(_ce);wXo=r(Wst,"pretrained_model_name_or_path"),Wst.forEach(t),AXo=r(C5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bce=s(C5,"CODE",{});var Qst=n(bce);LXo=r(Qst,"pretrained_model_name_or_path"),Qst.forEach(t),BXo=r(C5,":"),C5.forEach(t),xXo=i(ra),z6=s(ra,"UL",{});var qSe=n(z6);j1=s(qSe,"LI",{});var J0e=n(j1);vce=s(J0e,"STRONG",{});var Hst=n(vce);kXo=r(Hst,"speech-encoder-decoder"),Hst.forEach(t),RXo=r(J0e," \u2014 "),oq=s(J0e,"A",{href:!0});var Ust=n(oq);SXo=r(Ust,"SpeechEncoderDecoderModel"),Ust.forEach(t),PXo=r(J0e," (Speech Encoder decoder model)"),J0e.forEach(t),$Xo=i(qSe),N1=s(qSe,"LI",{});var Y0e=n(N1);Tce=s(Y0e,"STRONG",{});var Jst=n(Tce);IXo=r(Jst,"speech_to_text"),Jst.forEach(t),jXo=r(Y0e," \u2014 "),rq=s(Y0e,"A",{href:!0});var Yst=n(rq);NXo=r(Yst,"Speech2TextForConditionalGeneration"),Yst.forEach(t),DXo=r(Y0e," (Speech2Text model)"),Y0e.forEach(t),qSe.forEach(t),qXo=i(ra),D1=s(ra,"P",{});var K0e=n(D1);OXo=r(K0e,"The model is set in evaluation mode by default using "),Fce=s(K0e,"CODE",{});var Kst=n(Fce);GXo=r(Kst,"model.eval()"),Kst.forEach(t),XXo=r(K0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Cce=s(K0e,"CODE",{});var Zst=n(Cce);VXo=r(Zst,"model.train()"),Zst.forEach(t),K0e.forEach(t),zXo=i(ra),Mce=s(ra,"P",{});var ent=n(Mce);WXo=r(ent,"Examples:"),ent.forEach(t),QXo=i(ra),f(W6.$$.fragment,ra),ra.forEach(t),yl.forEach(t),kke=i(c),Hd=s(c,"H2",{class:!0});var OSe=n(Hd);q1=s(OSe,"A",{id:!0,class:!0,href:!0});var ont=n(q1);Ece=s(ont,"SPAN",{});var rnt=n(Ece);f(Q6.$$.fragment,rnt),rnt.forEach(t),ont.forEach(t),HXo=i(OSe),yce=s(OSe,"SPAN",{});var tnt=n(yce);UXo=r(tnt,"AutoModelForAudioXVector"),tnt.forEach(t),OSe.forEach(t),Rke=i(c),gr=s(c,"DIV",{class:!0});var Al=n(gr);f(H6.$$.fragment,Al),JXo=i(Al),Ud=s(Al,"P",{});var FW=n(Ud);YXo=r(FW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),wce=s(FW,"CODE",{});var ant=n(wce);KXo=r(ant,"from_pretrained()"),ant.forEach(t),ZXo=r(FW,"class method or the "),Ace=s(FW,"CODE",{});var snt=n(Ace);eVo=r(snt,"from_config()"),snt.forEach(t),oVo=r(FW,`class
method.`),FW.forEach(t),rVo=i(Al),U6=s(Al,"P",{});var GSe=n(U6);tVo=r(GSe,"This class cannot be instantiated directly using "),Lce=s(GSe,"CODE",{});var nnt=n(Lce);aVo=r(nnt,"__init__()"),nnt.forEach(t),sVo=r(GSe," (throws an error)."),GSe.forEach(t),nVo=i(Al),lt=s(Al,"DIV",{class:!0});var Ll=n(lt);f(J6.$$.fragment,Ll),lVo=i(Ll),Bce=s(Ll,"P",{});var lnt=n(Bce);iVo=r(lnt,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),lnt.forEach(t),dVo=i(Ll),Jd=s(Ll,"P",{});var CW=n(Jd);cVo=r(CW,`Note:
Loading a model from its configuration file does `),xce=s(CW,"STRONG",{});var int=n(xce);mVo=r(int,"not"),int.forEach(t),fVo=r(CW,` load the model weights. It only affects the
model\u2019s configuration. Use `),kce=s(CW,"CODE",{});var dnt=n(kce);gVo=r(dnt,"from_pretrained()"),dnt.forEach(t),hVo=r(CW,"to load the model weights."),CW.forEach(t),uVo=i(Ll),Rce=s(Ll,"P",{});var cnt=n(Rce);pVo=r(cnt,"Examples:"),cnt.forEach(t),_Vo=i(Ll),f(Y6.$$.fragment,Ll),Ll.forEach(t),bVo=i(Al),oo=s(Al,"DIV",{class:!0});var ta=n(oo);f(K6.$$.fragment,ta),vVo=i(ta),Sce=s(ta,"P",{});var mnt=n(Sce);TVo=r(mnt,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),mnt.forEach(t),FVo=i(ta),is=s(ta,"P",{});var M5=n(is);CVo=r(M5,"The model class to instantiate is selected based on the "),Pce=s(M5,"CODE",{});var fnt=n(Pce);MVo=r(fnt,"model_type"),fnt.forEach(t),EVo=r(M5,` property of the config object (either
passed as an argument or loaded from `),$ce=s(M5,"CODE",{});var gnt=n($ce);yVo=r(gnt,"pretrained_model_name_or_path"),gnt.forEach(t),wVo=r(M5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ice=s(M5,"CODE",{});var hnt=n(Ice);AVo=r(hnt,"pretrained_model_name_or_path"),hnt.forEach(t),LVo=r(M5,":"),M5.forEach(t),BVo=i(ta),ds=s(ta,"UL",{});var E5=n(ds);O1=s(E5,"LI",{});var Z0e=n(O1);jce=s(Z0e,"STRONG",{});var unt=n(jce);xVo=r(unt,"data2vec-audio"),unt.forEach(t),kVo=r(Z0e," \u2014 "),tq=s(Z0e,"A",{href:!0});var pnt=n(tq);RVo=r(pnt,"Data2VecAudioForXVector"),pnt.forEach(t),SVo=r(Z0e," (Data2VecAudio model)"),Z0e.forEach(t),PVo=i(E5),G1=s(E5,"LI",{});var eLe=n(G1);Nce=s(eLe,"STRONG",{});var _nt=n(Nce);$Vo=r(_nt,"unispeech-sat"),_nt.forEach(t),IVo=r(eLe," \u2014 "),aq=s(eLe,"A",{href:!0});var bnt=n(aq);jVo=r(bnt,"UniSpeechSatForXVector"),bnt.forEach(t),NVo=r(eLe," (UniSpeechSat model)"),eLe.forEach(t),DVo=i(E5),X1=s(E5,"LI",{});var oLe=n(X1);Dce=s(oLe,"STRONG",{});var vnt=n(Dce);qVo=r(vnt,"wav2vec2"),vnt.forEach(t),OVo=r(oLe," \u2014 "),sq=s(oLe,"A",{href:!0});var Tnt=n(sq);GVo=r(Tnt,"Wav2Vec2ForXVector"),Tnt.forEach(t),XVo=r(oLe," (Wav2Vec2 model)"),oLe.forEach(t),VVo=i(E5),V1=s(E5,"LI",{});var rLe=n(V1);qce=s(rLe,"STRONG",{});var Fnt=n(qce);zVo=r(Fnt,"wavlm"),Fnt.forEach(t),WVo=r(rLe," \u2014 "),nq=s(rLe,"A",{href:!0});var Cnt=n(nq);QVo=r(Cnt,"WavLMForXVector"),Cnt.forEach(t),HVo=r(rLe," (WavLM model)"),rLe.forEach(t),E5.forEach(t),UVo=i(ta),z1=s(ta,"P",{});var tLe=n(z1);JVo=r(tLe,"The model is set in evaluation mode by default using "),Oce=s(tLe,"CODE",{});var Mnt=n(Oce);YVo=r(Mnt,"model.eval()"),Mnt.forEach(t),KVo=r(tLe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gce=s(tLe,"CODE",{});var Ent=n(Gce);ZVo=r(Ent,"model.train()"),Ent.forEach(t),tLe.forEach(t),ezo=i(ta),Xce=s(ta,"P",{});var ynt=n(Xce);ozo=r(ynt,"Examples:"),ynt.forEach(t),rzo=i(ta),f(Z6.$$.fragment,ta),ta.forEach(t),Al.forEach(t),Ske=i(c),Yd=s(c,"H2",{class:!0});var XSe=n(Yd);W1=s(XSe,"A",{id:!0,class:!0,href:!0});var wnt=n(W1);Vce=s(wnt,"SPAN",{});var Ant=n(Vce);f(eA.$$.fragment,Ant),Ant.forEach(t),wnt.forEach(t),tzo=i(XSe),zce=s(XSe,"SPAN",{});var Lnt=n(zce);azo=r(Lnt,"AutoModelForMaskedImageModeling"),Lnt.forEach(t),XSe.forEach(t),Pke=i(c),hr=s(c,"DIV",{class:!0});var Bl=n(hr);f(oA.$$.fragment,Bl),szo=i(Bl),Kd=s(Bl,"P",{});var MW=n(Kd);nzo=r(MW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Wce=s(MW,"CODE",{});var Bnt=n(Wce);lzo=r(Bnt,"from_pretrained()"),Bnt.forEach(t),izo=r(MW,"class method or the "),Qce=s(MW,"CODE",{});var xnt=n(Qce);dzo=r(xnt,"from_config()"),xnt.forEach(t),czo=r(MW,`class
method.`),MW.forEach(t),mzo=i(Bl),rA=s(Bl,"P",{});var VSe=n(rA);fzo=r(VSe,"This class cannot be instantiated directly using "),Hce=s(VSe,"CODE",{});var knt=n(Hce);gzo=r(knt,"__init__()"),knt.forEach(t),hzo=r(VSe," (throws an error)."),VSe.forEach(t),uzo=i(Bl),it=s(Bl,"DIV",{class:!0});var xl=n(it);f(tA.$$.fragment,xl),pzo=i(xl),Uce=s(xl,"P",{});var Rnt=n(Uce);_zo=r(Rnt,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),Rnt.forEach(t),bzo=i(xl),Zd=s(xl,"P",{});var EW=n(Zd);vzo=r(EW,`Note:
Loading a model from its configuration file does `),Jce=s(EW,"STRONG",{});var Snt=n(Jce);Tzo=r(Snt,"not"),Snt.forEach(t),Fzo=r(EW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yce=s(EW,"CODE",{});var Pnt=n(Yce);Czo=r(Pnt,"from_pretrained()"),Pnt.forEach(t),Mzo=r(EW,"to load the model weights."),EW.forEach(t),Ezo=i(xl),Kce=s(xl,"P",{});var $nt=n(Kce);yzo=r($nt,"Examples:"),$nt.forEach(t),wzo=i(xl),f(aA.$$.fragment,xl),xl.forEach(t),Azo=i(Bl),ro=s(Bl,"DIV",{class:!0});var aa=n(ro);f(sA.$$.fragment,aa),Lzo=i(aa),Zce=s(aa,"P",{});var Int=n(Zce);Bzo=r(Int,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),Int.forEach(t),xzo=i(aa),cs=s(aa,"P",{});var y5=n(cs);kzo=r(y5,"The model class to instantiate is selected based on the "),eme=s(y5,"CODE",{});var jnt=n(eme);Rzo=r(jnt,"model_type"),jnt.forEach(t),Szo=r(y5,` property of the config object (either
passed as an argument or loaded from `),ome=s(y5,"CODE",{});var Nnt=n(ome);Pzo=r(Nnt,"pretrained_model_name_or_path"),Nnt.forEach(t),$zo=r(y5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rme=s(y5,"CODE",{});var Dnt=n(rme);Izo=r(Dnt,"pretrained_model_name_or_path"),Dnt.forEach(t),jzo=r(y5,":"),y5.forEach(t),Nzo=i(aa),ec=s(aa,"UL",{});var yW=n(ec);Q1=s(yW,"LI",{});var aLe=n(Q1);tme=s(aLe,"STRONG",{});var qnt=n(tme);Dzo=r(qnt,"deit"),qnt.forEach(t),qzo=r(aLe," \u2014 "),lq=s(aLe,"A",{href:!0});var Ont=n(lq);Ozo=r(Ont,"DeiTForMaskedImageModeling"),Ont.forEach(t),Gzo=r(aLe," (DeiT model)"),aLe.forEach(t),Xzo=i(yW),H1=s(yW,"LI",{});var sLe=n(H1);ame=s(sLe,"STRONG",{});var Gnt=n(ame);Vzo=r(Gnt,"swin"),Gnt.forEach(t),zzo=r(sLe," \u2014 "),iq=s(sLe,"A",{href:!0});var Xnt=n(iq);Wzo=r(Xnt,"SwinForMaskedImageModeling"),Xnt.forEach(t),Qzo=r(sLe," (Swin model)"),sLe.forEach(t),Hzo=i(yW),U1=s(yW,"LI",{});var nLe=n(U1);sme=s(nLe,"STRONG",{});var Vnt=n(sme);Uzo=r(Vnt,"vit"),Vnt.forEach(t),Jzo=r(nLe," \u2014 "),dq=s(nLe,"A",{href:!0});var znt=n(dq);Yzo=r(znt,"ViTForMaskedImageModeling"),znt.forEach(t),Kzo=r(nLe," (ViT model)"),nLe.forEach(t),yW.forEach(t),Zzo=i(aa),J1=s(aa,"P",{});var lLe=n(J1);eWo=r(lLe,"The model is set in evaluation mode by default using "),nme=s(lLe,"CODE",{});var Wnt=n(nme);oWo=r(Wnt,"model.eval()"),Wnt.forEach(t),rWo=r(lLe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lme=s(lLe,"CODE",{});var Qnt=n(lme);tWo=r(Qnt,"model.train()"),Qnt.forEach(t),lLe.forEach(t),aWo=i(aa),ime=s(aa,"P",{});var Hnt=n(ime);sWo=r(Hnt,"Examples:"),Hnt.forEach(t),nWo=i(aa),f(nA.$$.fragment,aa),aa.forEach(t),Bl.forEach(t),$ke=i(c),oc=s(c,"H2",{class:!0});var zSe=n(oc);Y1=s(zSe,"A",{id:!0,class:!0,href:!0});var Unt=n(Y1);dme=s(Unt,"SPAN",{});var Jnt=n(dme);f(lA.$$.fragment,Jnt),Jnt.forEach(t),Unt.forEach(t),lWo=i(zSe),cme=s(zSe,"SPAN",{});var Ynt=n(cme);iWo=r(Ynt,"AutoModelForObjectDetection"),Ynt.forEach(t),zSe.forEach(t),Ike=i(c),ur=s(c,"DIV",{class:!0});var kl=n(ur);f(iA.$$.fragment,kl),dWo=i(kl),rc=s(kl,"P",{});var wW=n(rc);cWo=r(wW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),mme=s(wW,"CODE",{});var Knt=n(mme);mWo=r(Knt,"from_pretrained()"),Knt.forEach(t),fWo=r(wW,"class method or the "),fme=s(wW,"CODE",{});var Znt=n(fme);gWo=r(Znt,"from_config()"),Znt.forEach(t),hWo=r(wW,`class
method.`),wW.forEach(t),uWo=i(kl),dA=s(kl,"P",{});var WSe=n(dA);pWo=r(WSe,"This class cannot be instantiated directly using "),gme=s(WSe,"CODE",{});var elt=n(gme);_Wo=r(elt,"__init__()"),elt.forEach(t),bWo=r(WSe," (throws an error)."),WSe.forEach(t),vWo=i(kl),dt=s(kl,"DIV",{class:!0});var Rl=n(dt);f(cA.$$.fragment,Rl),TWo=i(Rl),hme=s(Rl,"P",{});var olt=n(hme);FWo=r(olt,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),olt.forEach(t),CWo=i(Rl),tc=s(Rl,"P",{});var AW=n(tc);MWo=r(AW,`Note:
Loading a model from its configuration file does `),ume=s(AW,"STRONG",{});var rlt=n(ume);EWo=r(rlt,"not"),rlt.forEach(t),yWo=r(AW,` load the model weights. It only affects the
model\u2019s configuration. Use `),pme=s(AW,"CODE",{});var tlt=n(pme);wWo=r(tlt,"from_pretrained()"),tlt.forEach(t),AWo=r(AW,"to load the model weights."),AW.forEach(t),LWo=i(Rl),_me=s(Rl,"P",{});var alt=n(_me);BWo=r(alt,"Examples:"),alt.forEach(t),xWo=i(Rl),f(mA.$$.fragment,Rl),Rl.forEach(t),kWo=i(kl),to=s(kl,"DIV",{class:!0});var sa=n(to);f(fA.$$.fragment,sa),RWo=i(sa),bme=s(sa,"P",{});var slt=n(bme);SWo=r(slt,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),slt.forEach(t),PWo=i(sa),ms=s(sa,"P",{});var w5=n(ms);$Wo=r(w5,"The model class to instantiate is selected based on the "),vme=s(w5,"CODE",{});var nlt=n(vme);IWo=r(nlt,"model_type"),nlt.forEach(t),jWo=r(w5,` property of the config object (either
passed as an argument or loaded from `),Tme=s(w5,"CODE",{});var llt=n(Tme);NWo=r(llt,"pretrained_model_name_or_path"),llt.forEach(t),DWo=r(w5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fme=s(w5,"CODE",{});var ilt=n(Fme);qWo=r(ilt,"pretrained_model_name_or_path"),ilt.forEach(t),OWo=r(w5,":"),w5.forEach(t),GWo=i(sa),Cme=s(sa,"UL",{});var dlt=n(Cme);K1=s(dlt,"LI",{});var iLe=n(K1);Mme=s(iLe,"STRONG",{});var clt=n(Mme);XWo=r(clt,"detr"),clt.forEach(t),VWo=r(iLe," \u2014 "),cq=s(iLe,"A",{href:!0});var mlt=n(cq);zWo=r(mlt,"DetrForObjectDetection"),mlt.forEach(t),WWo=r(iLe," (DETR model)"),iLe.forEach(t),dlt.forEach(t),QWo=i(sa),Z1=s(sa,"P",{});var dLe=n(Z1);HWo=r(dLe,"The model is set in evaluation mode by default using "),Eme=s(dLe,"CODE",{});var flt=n(Eme);UWo=r(flt,"model.eval()"),flt.forEach(t),JWo=r(dLe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yme=s(dLe,"CODE",{});var glt=n(yme);YWo=r(glt,"model.train()"),glt.forEach(t),dLe.forEach(t),KWo=i(sa),wme=s(sa,"P",{});var hlt=n(wme);ZWo=r(hlt,"Examples:"),hlt.forEach(t),eQo=i(sa),f(gA.$$.fragment,sa),sa.forEach(t),kl.forEach(t),jke=i(c),ac=s(c,"H2",{class:!0});var QSe=n(ac);eF=s(QSe,"A",{id:!0,class:!0,href:!0});var ult=n(eF);Ame=s(ult,"SPAN",{});var plt=n(Ame);f(hA.$$.fragment,plt),plt.forEach(t),ult.forEach(t),oQo=i(QSe),Lme=s(QSe,"SPAN",{});var _lt=n(Lme);rQo=r(_lt,"AutoModelForImageSegmentation"),_lt.forEach(t),QSe.forEach(t),Nke=i(c),pr=s(c,"DIV",{class:!0});var Sl=n(pr);f(uA.$$.fragment,Sl),tQo=i(Sl),sc=s(Sl,"P",{});var LW=n(sc);aQo=r(LW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Bme=s(LW,"CODE",{});var blt=n(Bme);sQo=r(blt,"from_pretrained()"),blt.forEach(t),nQo=r(LW,"class method or the "),xme=s(LW,"CODE",{});var vlt=n(xme);lQo=r(vlt,"from_config()"),vlt.forEach(t),iQo=r(LW,`class
method.`),LW.forEach(t),dQo=i(Sl),pA=s(Sl,"P",{});var HSe=n(pA);cQo=r(HSe,"This class cannot be instantiated directly using "),kme=s(HSe,"CODE",{});var Tlt=n(kme);mQo=r(Tlt,"__init__()"),Tlt.forEach(t),fQo=r(HSe," (throws an error)."),HSe.forEach(t),gQo=i(Sl),ct=s(Sl,"DIV",{class:!0});var Pl=n(ct);f(_A.$$.fragment,Pl),hQo=i(Pl),Rme=s(Pl,"P",{});var Flt=n(Rme);uQo=r(Flt,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),Flt.forEach(t),pQo=i(Pl),nc=s(Pl,"P",{});var BW=n(nc);_Qo=r(BW,`Note:
Loading a model from its configuration file does `),Sme=s(BW,"STRONG",{});var Clt=n(Sme);bQo=r(Clt,"not"),Clt.forEach(t),vQo=r(BW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pme=s(BW,"CODE",{});var Mlt=n(Pme);TQo=r(Mlt,"from_pretrained()"),Mlt.forEach(t),FQo=r(BW,"to load the model weights."),BW.forEach(t),CQo=i(Pl),$me=s(Pl,"P",{});var Elt=n($me);MQo=r(Elt,"Examples:"),Elt.forEach(t),EQo=i(Pl),f(bA.$$.fragment,Pl),Pl.forEach(t),yQo=i(Sl),ao=s(Sl,"DIV",{class:!0});var na=n(ao);f(vA.$$.fragment,na),wQo=i(na),Ime=s(na,"P",{});var ylt=n(Ime);AQo=r(ylt,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),ylt.forEach(t),LQo=i(na),fs=s(na,"P",{});var A5=n(fs);BQo=r(A5,"The model class to instantiate is selected based on the "),jme=s(A5,"CODE",{});var wlt=n(jme);xQo=r(wlt,"model_type"),wlt.forEach(t),kQo=r(A5,` property of the config object (either
passed as an argument or loaded from `),Nme=s(A5,"CODE",{});var Alt=n(Nme);RQo=r(Alt,"pretrained_model_name_or_path"),Alt.forEach(t),SQo=r(A5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dme=s(A5,"CODE",{});var Llt=n(Dme);PQo=r(Llt,"pretrained_model_name_or_path"),Llt.forEach(t),$Qo=r(A5,":"),A5.forEach(t),IQo=i(na),qme=s(na,"UL",{});var Blt=n(qme);oF=s(Blt,"LI",{});var cLe=n(oF);Ome=s(cLe,"STRONG",{});var xlt=n(Ome);jQo=r(xlt,"detr"),xlt.forEach(t),NQo=r(cLe," \u2014 "),mq=s(cLe,"A",{href:!0});var klt=n(mq);DQo=r(klt,"DetrForSegmentation"),klt.forEach(t),qQo=r(cLe," (DETR model)"),cLe.forEach(t),Blt.forEach(t),OQo=i(na),rF=s(na,"P",{});var mLe=n(rF);GQo=r(mLe,"The model is set in evaluation mode by default using "),Gme=s(mLe,"CODE",{});var Rlt=n(Gme);XQo=r(Rlt,"model.eval()"),Rlt.forEach(t),VQo=r(mLe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xme=s(mLe,"CODE",{});var Slt=n(Xme);zQo=r(Slt,"model.train()"),Slt.forEach(t),mLe.forEach(t),WQo=i(na),Vme=s(na,"P",{});var Plt=n(Vme);QQo=r(Plt,"Examples:"),Plt.forEach(t),HQo=i(na),f(TA.$$.fragment,na),na.forEach(t),Sl.forEach(t),Dke=i(c),lc=s(c,"H2",{class:!0});var USe=n(lc);tF=s(USe,"A",{id:!0,class:!0,href:!0});var $lt=n(tF);zme=s($lt,"SPAN",{});var Ilt=n(zme);f(FA.$$.fragment,Ilt),Ilt.forEach(t),$lt.forEach(t),UQo=i(USe),Wme=s(USe,"SPAN",{});var jlt=n(Wme);JQo=r(jlt,"AutoModelForSemanticSegmentation"),jlt.forEach(t),USe.forEach(t),qke=i(c),_r=s(c,"DIV",{class:!0});var $l=n(_r);f(CA.$$.fragment,$l),YQo=i($l),ic=s($l,"P",{});var xW=n(ic);KQo=r(xW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Qme=s(xW,"CODE",{});var Nlt=n(Qme);ZQo=r(Nlt,"from_pretrained()"),Nlt.forEach(t),eHo=r(xW,"class method or the "),Hme=s(xW,"CODE",{});var Dlt=n(Hme);oHo=r(Dlt,"from_config()"),Dlt.forEach(t),rHo=r(xW,`class
method.`),xW.forEach(t),tHo=i($l),MA=s($l,"P",{});var JSe=n(MA);aHo=r(JSe,"This class cannot be instantiated directly using "),Ume=s(JSe,"CODE",{});var qlt=n(Ume);sHo=r(qlt,"__init__()"),qlt.forEach(t),nHo=r(JSe," (throws an error)."),JSe.forEach(t),lHo=i($l),mt=s($l,"DIV",{class:!0});var Il=n(mt);f(EA.$$.fragment,Il),iHo=i(Il),Jme=s(Il,"P",{});var Olt=n(Jme);dHo=r(Olt,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Olt.forEach(t),cHo=i(Il),dc=s(Il,"P",{});var kW=n(dc);mHo=r(kW,`Note:
Loading a model from its configuration file does `),Yme=s(kW,"STRONG",{});var Glt=n(Yme);fHo=r(Glt,"not"),Glt.forEach(t),gHo=r(kW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kme=s(kW,"CODE",{});var Xlt=n(Kme);hHo=r(Xlt,"from_pretrained()"),Xlt.forEach(t),uHo=r(kW,"to load the model weights."),kW.forEach(t),pHo=i(Il),Zme=s(Il,"P",{});var Vlt=n(Zme);_Ho=r(Vlt,"Examples:"),Vlt.forEach(t),bHo=i(Il),f(yA.$$.fragment,Il),Il.forEach(t),vHo=i($l),so=s($l,"DIV",{class:!0});var la=n(so);f(wA.$$.fragment,la),THo=i(la),efe=s(la,"P",{});var zlt=n(efe);FHo=r(zlt,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),zlt.forEach(t),CHo=i(la),gs=s(la,"P",{});var L5=n(gs);MHo=r(L5,"The model class to instantiate is selected based on the "),ofe=s(L5,"CODE",{});var Wlt=n(ofe);EHo=r(Wlt,"model_type"),Wlt.forEach(t),yHo=r(L5,` property of the config object (either
passed as an argument or loaded from `),rfe=s(L5,"CODE",{});var Qlt=n(rfe);wHo=r(Qlt,"pretrained_model_name_or_path"),Qlt.forEach(t),AHo=r(L5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tfe=s(L5,"CODE",{});var Hlt=n(tfe);LHo=r(Hlt,"pretrained_model_name_or_path"),Hlt.forEach(t),BHo=r(L5,":"),L5.forEach(t),xHo=i(la),AA=s(la,"UL",{});var YSe=n(AA);aF=s(YSe,"LI",{});var fLe=n(aF);afe=s(fLe,"STRONG",{});var Ult=n(afe);kHo=r(Ult,"beit"),Ult.forEach(t),RHo=r(fLe," \u2014 "),fq=s(fLe,"A",{href:!0});var Jlt=n(fq);SHo=r(Jlt,"BeitForSemanticSegmentation"),Jlt.forEach(t),PHo=r(fLe," (BEiT model)"),fLe.forEach(t),$Ho=i(YSe),sF=s(YSe,"LI",{});var gLe=n(sF);sfe=s(gLe,"STRONG",{});var Ylt=n(sfe);IHo=r(Ylt,"segformer"),Ylt.forEach(t),jHo=r(gLe," \u2014 "),gq=s(gLe,"A",{href:!0});var Klt=n(gq);NHo=r(Klt,"SegformerForSemanticSegmentation"),Klt.forEach(t),DHo=r(gLe," (SegFormer model)"),gLe.forEach(t),YSe.forEach(t),qHo=i(la),nF=s(la,"P",{});var hLe=n(nF);OHo=r(hLe,"The model is set in evaluation mode by default using "),nfe=s(hLe,"CODE",{});var Zlt=n(nfe);GHo=r(Zlt,"model.eval()"),Zlt.forEach(t),XHo=r(hLe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lfe=s(hLe,"CODE",{});var eit=n(lfe);VHo=r(eit,"model.train()"),eit.forEach(t),hLe.forEach(t),zHo=i(la),ife=s(la,"P",{});var oit=n(ife);WHo=r(oit,"Examples:"),oit.forEach(t),QHo=i(la),f(LA.$$.fragment,la),la.forEach(t),$l.forEach(t),Oke=i(c),cc=s(c,"H2",{class:!0});var KSe=n(cc);lF=s(KSe,"A",{id:!0,class:!0,href:!0});var rit=n(lF);dfe=s(rit,"SPAN",{});var tit=n(dfe);f(BA.$$.fragment,tit),tit.forEach(t),rit.forEach(t),HHo=i(KSe),cfe=s(KSe,"SPAN",{});var ait=n(cfe);UHo=r(ait,"AutoModelForInstanceSegmentation"),ait.forEach(t),KSe.forEach(t),Gke=i(c),br=s(c,"DIV",{class:!0});var jl=n(br);f(xA.$$.fragment,jl),JHo=i(jl),mc=s(jl,"P",{});var RW=n(mc);YHo=r(RW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the `),mfe=s(RW,"CODE",{});var sit=n(mfe);KHo=r(sit,"from_pretrained()"),sit.forEach(t),ZHo=r(RW,"class method or the "),ffe=s(RW,"CODE",{});var nit=n(ffe);eUo=r(nit,"from_config()"),nit.forEach(t),oUo=r(RW,`class
method.`),RW.forEach(t),rUo=i(jl),kA=s(jl,"P",{});var ZSe=n(kA);tUo=r(ZSe,"This class cannot be instantiated directly using "),gfe=s(ZSe,"CODE",{});var lit=n(gfe);aUo=r(lit,"__init__()"),lit.forEach(t),sUo=r(ZSe," (throws an error)."),ZSe.forEach(t),nUo=i(jl),ft=s(jl,"DIV",{class:!0});var Nl=n(ft);f(RA.$$.fragment,Nl),lUo=i(Nl),hfe=s(Nl,"P",{});var iit=n(hfe);iUo=r(iit,"Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration."),iit.forEach(t),dUo=i(Nl),fc=s(Nl,"P",{});var SW=n(fc);cUo=r(SW,`Note:
Loading a model from its configuration file does `),ufe=s(SW,"STRONG",{});var dit=n(ufe);mUo=r(dit,"not"),dit.forEach(t),fUo=r(SW,` load the model weights. It only affects the
model\u2019s configuration. Use `),pfe=s(SW,"CODE",{});var cit=n(pfe);gUo=r(cit,"from_pretrained()"),cit.forEach(t),hUo=r(SW,"to load the model weights."),SW.forEach(t),uUo=i(Nl),_fe=s(Nl,"P",{});var mit=n(_fe);pUo=r(mit,"Examples:"),mit.forEach(t),_Uo=i(Nl),f(SA.$$.fragment,Nl),Nl.forEach(t),bUo=i(jl),no=s(jl,"DIV",{class:!0});var ia=n(no);f(PA.$$.fragment,ia),vUo=i(ia),bfe=s(ia,"P",{});var fit=n(bfe);TUo=r(fit,"Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model."),fit.forEach(t),FUo=i(ia),hs=s(ia,"P",{});var B5=n(hs);CUo=r(B5,"The model class to instantiate is selected based on the "),vfe=s(B5,"CODE",{});var git=n(vfe);MUo=r(git,"model_type"),git.forEach(t),EUo=r(B5,` property of the config object (either
passed as an argument or loaded from `),Tfe=s(B5,"CODE",{});var hit=n(Tfe);yUo=r(hit,"pretrained_model_name_or_path"),hit.forEach(t),wUo=r(B5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ffe=s(B5,"CODE",{});var uit=n(Ffe);AUo=r(uit,"pretrained_model_name_or_path"),uit.forEach(t),LUo=r(B5,":"),B5.forEach(t),BUo=i(ia),Cfe=s(ia,"UL",{});var pit=n(Cfe);iF=s(pit,"LI",{});var uLe=n(iF);Mfe=s(uLe,"STRONG",{});var _it=n(Mfe);xUo=r(_it,"maskformer"),_it.forEach(t),kUo=r(uLe," \u2014 "),hq=s(uLe,"A",{href:!0});var bit=n(hq);RUo=r(bit,"MaskFormerForInstanceSegmentation"),bit.forEach(t),SUo=r(uLe," (MaskFormer model)"),uLe.forEach(t),pit.forEach(t),PUo=i(ia),dF=s(ia,"P",{});var pLe=n(dF);$Uo=r(pLe,"The model is set in evaluation mode by default using "),Efe=s(pLe,"CODE",{});var vit=n(Efe);IUo=r(vit,"model.eval()"),vit.forEach(t),jUo=r(pLe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yfe=s(pLe,"CODE",{});var Tit=n(yfe);NUo=r(Tit,"model.train()"),Tit.forEach(t),pLe.forEach(t),DUo=i(ia),wfe=s(ia,"P",{});var Fit=n(wfe);qUo=r(Fit,"Examples:"),Fit.forEach(t),OUo=i(ia),f($A.$$.fragment,ia),ia.forEach(t),jl.forEach(t),Xke=i(c),gc=s(c,"H2",{class:!0});var ePe=n(gc);cF=s(ePe,"A",{id:!0,class:!0,href:!0});var Cit=n(cF);Afe=s(Cit,"SPAN",{});var Mit=n(Afe);f(IA.$$.fragment,Mit),Mit.forEach(t),Cit.forEach(t),GUo=i(ePe),Lfe=s(ePe,"SPAN",{});var Eit=n(Lfe);XUo=r(Eit,"TFAutoModel"),Eit.forEach(t),ePe.forEach(t),Vke=i(c),vr=s(c,"DIV",{class:!0});var Dl=n(vr);f(jA.$$.fragment,Dl),VUo=i(Dl),hc=s(Dl,"P",{});var PW=n(hc);zUo=r(PW,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Bfe=s(PW,"CODE",{});var yit=n(Bfe);WUo=r(yit,"from_pretrained()"),yit.forEach(t),QUo=r(PW,"class method or the "),xfe=s(PW,"CODE",{});var wit=n(xfe);HUo=r(wit,"from_config()"),wit.forEach(t),UUo=r(PW,`class
method.`),PW.forEach(t),JUo=i(Dl),NA=s(Dl,"P",{});var oPe=n(NA);YUo=r(oPe,"This class cannot be instantiated directly using "),kfe=s(oPe,"CODE",{});var Ait=n(kfe);KUo=r(Ait,"__init__()"),Ait.forEach(t),ZUo=r(oPe," (throws an error)."),oPe.forEach(t),eJo=i(Dl),gt=s(Dl,"DIV",{class:!0});var ql=n(gt);f(DA.$$.fragment,ql),oJo=i(ql),Rfe=s(ql,"P",{});var Lit=n(Rfe);rJo=r(Lit,"Instantiates one of the base model classes of the library from a configuration."),Lit.forEach(t),tJo=i(ql),uc=s(ql,"P",{});var $W=n(uc);aJo=r($W,`Note:
Loading a model from its configuration file does `),Sfe=s($W,"STRONG",{});var Bit=n(Sfe);sJo=r(Bit,"not"),Bit.forEach(t),nJo=r($W,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pfe=s($W,"CODE",{});var xit=n(Pfe);lJo=r(xit,"from_pretrained()"),xit.forEach(t),iJo=r($W,"to load the model weights."),$W.forEach(t),dJo=i(ql),$fe=s(ql,"P",{});var kit=n($fe);cJo=r(kit,"Examples:"),kit.forEach(t),mJo=i(ql),f(qA.$$.fragment,ql),ql.forEach(t),fJo=i(Dl),ho=s(Dl,"DIV",{class:!0});var ua=n(ho);f(OA.$$.fragment,ua),gJo=i(ua),Ife=s(ua,"P",{});var Rit=n(Ife);hJo=r(Rit,"Instantiate one of the base model classes of the library from a pretrained model."),Rit.forEach(t),uJo=i(ua),us=s(ua,"P",{});var x5=n(us);pJo=r(x5,"The model class to instantiate is selected based on the "),jfe=s(x5,"CODE",{});var Sit=n(jfe);_Jo=r(Sit,"model_type"),Sit.forEach(t),bJo=r(x5,` property of the config object (either
passed as an argument or loaded from `),Nfe=s(x5,"CODE",{});var Pit=n(Nfe);vJo=r(Pit,"pretrained_model_name_or_path"),Pit.forEach(t),TJo=r(x5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dfe=s(x5,"CODE",{});var $it=n(Dfe);FJo=r($it,"pretrained_model_name_or_path"),$it.forEach(t),CJo=r(x5,":"),x5.forEach(t),MJo=i(ua),B=s(ua,"UL",{});var x=n(B);mF=s(x,"LI",{});var _Le=n(mF);qfe=s(_Le,"STRONG",{});var Iit=n(qfe);EJo=r(Iit,"albert"),Iit.forEach(t),yJo=r(_Le," \u2014 "),uq=s(_Le,"A",{href:!0});var jit=n(uq);wJo=r(jit,"TFAlbertModel"),jit.forEach(t),AJo=r(_Le," (ALBERT model)"),_Le.forEach(t),LJo=i(x),fF=s(x,"LI",{});var bLe=n(fF);Ofe=s(bLe,"STRONG",{});var Nit=n(Ofe);BJo=r(Nit,"bart"),Nit.forEach(t),xJo=r(bLe," \u2014 "),pq=s(bLe,"A",{href:!0});var Dit=n(pq);kJo=r(Dit,"TFBartModel"),Dit.forEach(t),RJo=r(bLe," (BART model)"),bLe.forEach(t),SJo=i(x),gF=s(x,"LI",{});var vLe=n(gF);Gfe=s(vLe,"STRONG",{});var qit=n(Gfe);PJo=r(qit,"bert"),qit.forEach(t),$Jo=r(vLe," \u2014 "),_q=s(vLe,"A",{href:!0});var Oit=n(_q);IJo=r(Oit,"TFBertModel"),Oit.forEach(t),jJo=r(vLe," (BERT model)"),vLe.forEach(t),NJo=i(x),hF=s(x,"LI",{});var TLe=n(hF);Xfe=s(TLe,"STRONG",{});var Git=n(Xfe);DJo=r(Git,"blenderbot"),Git.forEach(t),qJo=r(TLe," \u2014 "),bq=s(TLe,"A",{href:!0});var Xit=n(bq);OJo=r(Xit,"TFBlenderbotModel"),Xit.forEach(t),GJo=r(TLe," (Blenderbot model)"),TLe.forEach(t),XJo=i(x),uF=s(x,"LI",{});var FLe=n(uF);Vfe=s(FLe,"STRONG",{});var Vit=n(Vfe);VJo=r(Vit,"blenderbot-small"),Vit.forEach(t),zJo=r(FLe," \u2014 "),vq=s(FLe,"A",{href:!0});var zit=n(vq);WJo=r(zit,"TFBlenderbotSmallModel"),zit.forEach(t),QJo=r(FLe," (BlenderbotSmall model)"),FLe.forEach(t),HJo=i(x),pF=s(x,"LI",{});var CLe=n(pF);zfe=s(CLe,"STRONG",{});var Wit=n(zfe);UJo=r(Wit,"camembert"),Wit.forEach(t),JJo=r(CLe," \u2014 "),Tq=s(CLe,"A",{href:!0});var Qit=n(Tq);YJo=r(Qit,"TFCamembertModel"),Qit.forEach(t),KJo=r(CLe," (CamemBERT model)"),CLe.forEach(t),ZJo=i(x),_F=s(x,"LI",{});var MLe=n(_F);Wfe=s(MLe,"STRONG",{});var Hit=n(Wfe);eYo=r(Hit,"clip"),Hit.forEach(t),oYo=r(MLe," \u2014 "),Fq=s(MLe,"A",{href:!0});var Uit=n(Fq);rYo=r(Uit,"TFCLIPModel"),Uit.forEach(t),tYo=r(MLe," (CLIP model)"),MLe.forEach(t),aYo=i(x),bF=s(x,"LI",{});var ELe=n(bF);Qfe=s(ELe,"STRONG",{});var Jit=n(Qfe);sYo=r(Jit,"convbert"),Jit.forEach(t),nYo=r(ELe," \u2014 "),Cq=s(ELe,"A",{href:!0});var Yit=n(Cq);lYo=r(Yit,"TFConvBertModel"),Yit.forEach(t),iYo=r(ELe," (ConvBERT model)"),ELe.forEach(t),dYo=i(x),vF=s(x,"LI",{});var yLe=n(vF);Hfe=s(yLe,"STRONG",{});var Kit=n(Hfe);cYo=r(Kit,"convnext"),Kit.forEach(t),mYo=r(yLe," \u2014 "),Mq=s(yLe,"A",{href:!0});var Zit=n(Mq);fYo=r(Zit,"TFConvNextModel"),Zit.forEach(t),gYo=r(yLe," (ConvNext model)"),yLe.forEach(t),hYo=i(x),TF=s(x,"LI",{});var wLe=n(TF);Ufe=s(wLe,"STRONG",{});var edt=n(Ufe);uYo=r(edt,"ctrl"),edt.forEach(t),pYo=r(wLe," \u2014 "),Eq=s(wLe,"A",{href:!0});var odt=n(Eq);_Yo=r(odt,"TFCTRLModel"),odt.forEach(t),bYo=r(wLe," (CTRL model)"),wLe.forEach(t),vYo=i(x),FF=s(x,"LI",{});var ALe=n(FF);Jfe=s(ALe,"STRONG",{});var rdt=n(Jfe);TYo=r(rdt,"deberta"),rdt.forEach(t),FYo=r(ALe," \u2014 "),yq=s(ALe,"A",{href:!0});var tdt=n(yq);CYo=r(tdt,"TFDebertaModel"),tdt.forEach(t),MYo=r(ALe," (DeBERTa model)"),ALe.forEach(t),EYo=i(x),CF=s(x,"LI",{});var LLe=n(CF);Yfe=s(LLe,"STRONG",{});var adt=n(Yfe);yYo=r(adt,"deberta-v2"),adt.forEach(t),wYo=r(LLe," \u2014 "),wq=s(LLe,"A",{href:!0});var sdt=n(wq);AYo=r(sdt,"TFDebertaV2Model"),sdt.forEach(t),LYo=r(LLe," (DeBERTa-v2 model)"),LLe.forEach(t),BYo=i(x),MF=s(x,"LI",{});var BLe=n(MF);Kfe=s(BLe,"STRONG",{});var ndt=n(Kfe);xYo=r(ndt,"distilbert"),ndt.forEach(t),kYo=r(BLe," \u2014 "),Aq=s(BLe,"A",{href:!0});var ldt=n(Aq);RYo=r(ldt,"TFDistilBertModel"),ldt.forEach(t),SYo=r(BLe," (DistilBERT model)"),BLe.forEach(t),PYo=i(x),EF=s(x,"LI",{});var xLe=n(EF);Zfe=s(xLe,"STRONG",{});var idt=n(Zfe);$Yo=r(idt,"dpr"),idt.forEach(t),IYo=r(xLe," \u2014 "),Lq=s(xLe,"A",{href:!0});var ddt=n(Lq);jYo=r(ddt,"TFDPRQuestionEncoder"),ddt.forEach(t),NYo=r(xLe," (DPR model)"),xLe.forEach(t),DYo=i(x),yF=s(x,"LI",{});var kLe=n(yF);ege=s(kLe,"STRONG",{});var cdt=n(ege);qYo=r(cdt,"electra"),cdt.forEach(t),OYo=r(kLe," \u2014 "),Bq=s(kLe,"A",{href:!0});var mdt=n(Bq);GYo=r(mdt,"TFElectraModel"),mdt.forEach(t),XYo=r(kLe," (ELECTRA model)"),kLe.forEach(t),VYo=i(x),wF=s(x,"LI",{});var RLe=n(wF);oge=s(RLe,"STRONG",{});var fdt=n(oge);zYo=r(fdt,"flaubert"),fdt.forEach(t),WYo=r(RLe," \u2014 "),xq=s(RLe,"A",{href:!0});var gdt=n(xq);QYo=r(gdt,"TFFlaubertModel"),gdt.forEach(t),HYo=r(RLe," (FlauBERT model)"),RLe.forEach(t),UYo=i(x),On=s(x,"LI",{});var Q8=n(On);rge=s(Q8,"STRONG",{});var hdt=n(rge);JYo=r(hdt,"funnel"),hdt.forEach(t),YYo=r(Q8," \u2014 "),kq=s(Q8,"A",{href:!0});var udt=n(kq);KYo=r(udt,"TFFunnelModel"),udt.forEach(t),ZYo=r(Q8," or "),Rq=s(Q8,"A",{href:!0});var pdt=n(Rq);eKo=r(pdt,"TFFunnelBaseModel"),pdt.forEach(t),oKo=r(Q8," (Funnel Transformer model)"),Q8.forEach(t),rKo=i(x),AF=s(x,"LI",{});var SLe=n(AF);tge=s(SLe,"STRONG",{});var _dt=n(tge);tKo=r(_dt,"gpt2"),_dt.forEach(t),aKo=r(SLe," \u2014 "),Sq=s(SLe,"A",{href:!0});var bdt=n(Sq);sKo=r(bdt,"TFGPT2Model"),bdt.forEach(t),nKo=r(SLe," (OpenAI GPT-2 model)"),SLe.forEach(t),lKo=i(x),LF=s(x,"LI",{});var PLe=n(LF);age=s(PLe,"STRONG",{});var vdt=n(age);iKo=r(vdt,"hubert"),vdt.forEach(t),dKo=r(PLe," \u2014 "),Pq=s(PLe,"A",{href:!0});var Tdt=n(Pq);cKo=r(Tdt,"TFHubertModel"),Tdt.forEach(t),mKo=r(PLe," (Hubert model)"),PLe.forEach(t),fKo=i(x),BF=s(x,"LI",{});var $Le=n(BF);sge=s($Le,"STRONG",{});var Fdt=n(sge);gKo=r(Fdt,"layoutlm"),Fdt.forEach(t),hKo=r($Le," \u2014 "),$q=s($Le,"A",{href:!0});var Cdt=n($q);uKo=r(Cdt,"TFLayoutLMModel"),Cdt.forEach(t),pKo=r($Le," (LayoutLM model)"),$Le.forEach(t),_Ko=i(x),xF=s(x,"LI",{});var ILe=n(xF);nge=s(ILe,"STRONG",{});var Mdt=n(nge);bKo=r(Mdt,"led"),Mdt.forEach(t),vKo=r(ILe," \u2014 "),Iq=s(ILe,"A",{href:!0});var Edt=n(Iq);TKo=r(Edt,"TFLEDModel"),Edt.forEach(t),FKo=r(ILe," (LED model)"),ILe.forEach(t),CKo=i(x),kF=s(x,"LI",{});var jLe=n(kF);lge=s(jLe,"STRONG",{});var ydt=n(lge);MKo=r(ydt,"longformer"),ydt.forEach(t),EKo=r(jLe," \u2014 "),jq=s(jLe,"A",{href:!0});var wdt=n(jq);yKo=r(wdt,"TFLongformerModel"),wdt.forEach(t),wKo=r(jLe," (Longformer model)"),jLe.forEach(t),AKo=i(x),RF=s(x,"LI",{});var NLe=n(RF);ige=s(NLe,"STRONG",{});var Adt=n(ige);LKo=r(Adt,"lxmert"),Adt.forEach(t),BKo=r(NLe," \u2014 "),Nq=s(NLe,"A",{href:!0});var Ldt=n(Nq);xKo=r(Ldt,"TFLxmertModel"),Ldt.forEach(t),kKo=r(NLe," (LXMERT model)"),NLe.forEach(t),RKo=i(x),SF=s(x,"LI",{});var DLe=n(SF);dge=s(DLe,"STRONG",{});var Bdt=n(dge);SKo=r(Bdt,"marian"),Bdt.forEach(t),PKo=r(DLe," \u2014 "),Dq=s(DLe,"A",{href:!0});var xdt=n(Dq);$Ko=r(xdt,"TFMarianModel"),xdt.forEach(t),IKo=r(DLe," (Marian model)"),DLe.forEach(t),jKo=i(x),PF=s(x,"LI",{});var qLe=n(PF);cge=s(qLe,"STRONG",{});var kdt=n(cge);NKo=r(kdt,"mbart"),kdt.forEach(t),DKo=r(qLe," \u2014 "),qq=s(qLe,"A",{href:!0});var Rdt=n(qq);qKo=r(Rdt,"TFMBartModel"),Rdt.forEach(t),OKo=r(qLe," (mBART model)"),qLe.forEach(t),GKo=i(x),$F=s(x,"LI",{});var OLe=n($F);mge=s(OLe,"STRONG",{});var Sdt=n(mge);XKo=r(Sdt,"mobilebert"),Sdt.forEach(t),VKo=r(OLe," \u2014 "),Oq=s(OLe,"A",{href:!0});var Pdt=n(Oq);zKo=r(Pdt,"TFMobileBertModel"),Pdt.forEach(t),WKo=r(OLe," (MobileBERT model)"),OLe.forEach(t),QKo=i(x),IF=s(x,"LI",{});var GLe=n(IF);fge=s(GLe,"STRONG",{});var $dt=n(fge);HKo=r($dt,"mpnet"),$dt.forEach(t),UKo=r(GLe," \u2014 "),Gq=s(GLe,"A",{href:!0});var Idt=n(Gq);JKo=r(Idt,"TFMPNetModel"),Idt.forEach(t),YKo=r(GLe," (MPNet model)"),GLe.forEach(t),KKo=i(x),jF=s(x,"LI",{});var XLe=n(jF);gge=s(XLe,"STRONG",{});var jdt=n(gge);ZKo=r(jdt,"mt5"),jdt.forEach(t),eZo=r(XLe," \u2014 "),Xq=s(XLe,"A",{href:!0});var Ndt=n(Xq);oZo=r(Ndt,"TFMT5Model"),Ndt.forEach(t),rZo=r(XLe," (mT5 model)"),XLe.forEach(t),tZo=i(x),NF=s(x,"LI",{});var VLe=n(NF);hge=s(VLe,"STRONG",{});var Ddt=n(hge);aZo=r(Ddt,"openai-gpt"),Ddt.forEach(t),sZo=r(VLe," \u2014 "),Vq=s(VLe,"A",{href:!0});var qdt=n(Vq);nZo=r(qdt,"TFOpenAIGPTModel"),qdt.forEach(t),lZo=r(VLe," (OpenAI GPT model)"),VLe.forEach(t),iZo=i(x),DF=s(x,"LI",{});var zLe=n(DF);uge=s(zLe,"STRONG",{});var Odt=n(uge);dZo=r(Odt,"pegasus"),Odt.forEach(t),cZo=r(zLe," \u2014 "),zq=s(zLe,"A",{href:!0});var Gdt=n(zq);mZo=r(Gdt,"TFPegasusModel"),Gdt.forEach(t),fZo=r(zLe," (Pegasus model)"),zLe.forEach(t),gZo=i(x),qF=s(x,"LI",{});var WLe=n(qF);pge=s(WLe,"STRONG",{});var Xdt=n(pge);hZo=r(Xdt,"rembert"),Xdt.forEach(t),uZo=r(WLe," \u2014 "),Wq=s(WLe,"A",{href:!0});var Vdt=n(Wq);pZo=r(Vdt,"TFRemBertModel"),Vdt.forEach(t),_Zo=r(WLe," (RemBERT model)"),WLe.forEach(t),bZo=i(x),OF=s(x,"LI",{});var QLe=n(OF);_ge=s(QLe,"STRONG",{});var zdt=n(_ge);vZo=r(zdt,"roberta"),zdt.forEach(t),TZo=r(QLe," \u2014 "),Qq=s(QLe,"A",{href:!0});var Wdt=n(Qq);FZo=r(Wdt,"TFRobertaModel"),Wdt.forEach(t),CZo=r(QLe," (RoBERTa model)"),QLe.forEach(t),MZo=i(x),GF=s(x,"LI",{});var HLe=n(GF);bge=s(HLe,"STRONG",{});var Qdt=n(bge);EZo=r(Qdt,"roformer"),Qdt.forEach(t),yZo=r(HLe," \u2014 "),Hq=s(HLe,"A",{href:!0});var Hdt=n(Hq);wZo=r(Hdt,"TFRoFormerModel"),Hdt.forEach(t),AZo=r(HLe," (RoFormer model)"),HLe.forEach(t),LZo=i(x),XF=s(x,"LI",{});var ULe=n(XF);vge=s(ULe,"STRONG",{});var Udt=n(vge);BZo=r(Udt,"speech_to_text"),Udt.forEach(t),xZo=r(ULe," \u2014 "),Uq=s(ULe,"A",{href:!0});var Jdt=n(Uq);kZo=r(Jdt,"TFSpeech2TextModel"),Jdt.forEach(t),RZo=r(ULe," (Speech2Text model)"),ULe.forEach(t),SZo=i(x),VF=s(x,"LI",{});var JLe=n(VF);Tge=s(JLe,"STRONG",{});var Ydt=n(Tge);PZo=r(Ydt,"t5"),Ydt.forEach(t),$Zo=r(JLe," \u2014 "),Jq=s(JLe,"A",{href:!0});var Kdt=n(Jq);IZo=r(Kdt,"TFT5Model"),Kdt.forEach(t),jZo=r(JLe," (T5 model)"),JLe.forEach(t),NZo=i(x),zF=s(x,"LI",{});var YLe=n(zF);Fge=s(YLe,"STRONG",{});var Zdt=n(Fge);DZo=r(Zdt,"tapas"),Zdt.forEach(t),qZo=r(YLe," \u2014 "),Yq=s(YLe,"A",{href:!0});var ect=n(Yq);OZo=r(ect,"TFTapasModel"),ect.forEach(t),GZo=r(YLe," (TAPAS model)"),YLe.forEach(t),XZo=i(x),WF=s(x,"LI",{});var KLe=n(WF);Cge=s(KLe,"STRONG",{});var oct=n(Cge);VZo=r(oct,"transfo-xl"),oct.forEach(t),zZo=r(KLe," \u2014 "),Kq=s(KLe,"A",{href:!0});var rct=n(Kq);WZo=r(rct,"TFTransfoXLModel"),rct.forEach(t),QZo=r(KLe," (Transformer-XL model)"),KLe.forEach(t),HZo=i(x),QF=s(x,"LI",{});var ZLe=n(QF);Mge=s(ZLe,"STRONG",{});var tct=n(Mge);UZo=r(tct,"vit"),tct.forEach(t),JZo=r(ZLe," \u2014 "),Zq=s(ZLe,"A",{href:!0});var act=n(Zq);YZo=r(act,"TFViTModel"),act.forEach(t),KZo=r(ZLe," (ViT model)"),ZLe.forEach(t),ZZo=i(x),HF=s(x,"LI",{});var e7e=n(HF);Ege=s(e7e,"STRONG",{});var sct=n(Ege);eer=r(sct,"wav2vec2"),sct.forEach(t),oer=r(e7e," \u2014 "),eO=s(e7e,"A",{href:!0});var nct=n(eO);rer=r(nct,"TFWav2Vec2Model"),nct.forEach(t),ter=r(e7e," (Wav2Vec2 model)"),e7e.forEach(t),aer=i(x),UF=s(x,"LI",{});var o7e=n(UF);yge=s(o7e,"STRONG",{});var lct=n(yge);ser=r(lct,"xlm"),lct.forEach(t),ner=r(o7e," \u2014 "),oO=s(o7e,"A",{href:!0});var ict=n(oO);ler=r(ict,"TFXLMModel"),ict.forEach(t),ier=r(o7e," (XLM model)"),o7e.forEach(t),der=i(x),JF=s(x,"LI",{});var r7e=n(JF);wge=s(r7e,"STRONG",{});var dct=n(wge);cer=r(dct,"xlm-roberta"),dct.forEach(t),mer=r(r7e," \u2014 "),rO=s(r7e,"A",{href:!0});var cct=n(rO);fer=r(cct,"TFXLMRobertaModel"),cct.forEach(t),ger=r(r7e," (XLM-RoBERTa model)"),r7e.forEach(t),her=i(x),YF=s(x,"LI",{});var t7e=n(YF);Age=s(t7e,"STRONG",{});var mct=n(Age);uer=r(mct,"xlnet"),mct.forEach(t),per=r(t7e," \u2014 "),tO=s(t7e,"A",{href:!0});var fct=n(tO);_er=r(fct,"TFXLNetModel"),fct.forEach(t),ber=r(t7e," (XLNet model)"),t7e.forEach(t),x.forEach(t),ver=i(ua),Lge=s(ua,"P",{});var gct=n(Lge);Ter=r(gct,"Examples:"),gct.forEach(t),Fer=i(ua),f(GA.$$.fragment,ua),ua.forEach(t),Dl.forEach(t),zke=i(c),pc=s(c,"H2",{class:!0});var rPe=n(pc);KF=s(rPe,"A",{id:!0,class:!0,href:!0});var hct=n(KF);Bge=s(hct,"SPAN",{});var uct=n(Bge);f(XA.$$.fragment,uct),uct.forEach(t),hct.forEach(t),Cer=i(rPe),xge=s(rPe,"SPAN",{});var pct=n(xge);Mer=r(pct,"TFAutoModelForPreTraining"),pct.forEach(t),rPe.forEach(t),Wke=i(c),Tr=s(c,"DIV",{class:!0});var Ol=n(Tr);f(VA.$$.fragment,Ol),Eer=i(Ol),_c=s(Ol,"P",{});var IW=n(_c);yer=r(IW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),kge=s(IW,"CODE",{});var _ct=n(kge);wer=r(_ct,"from_pretrained()"),_ct.forEach(t),Aer=r(IW,"class method or the "),Rge=s(IW,"CODE",{});var bct=n(Rge);Ler=r(bct,"from_config()"),bct.forEach(t),Ber=r(IW,`class
method.`),IW.forEach(t),xer=i(Ol),zA=s(Ol,"P",{});var tPe=n(zA);ker=r(tPe,"This class cannot be instantiated directly using "),Sge=s(tPe,"CODE",{});var vct=n(Sge);Rer=r(vct,"__init__()"),vct.forEach(t),Ser=r(tPe," (throws an error)."),tPe.forEach(t),Per=i(Ol),ht=s(Ol,"DIV",{class:!0});var Gl=n(ht);f(WA.$$.fragment,Gl),$er=i(Gl),Pge=s(Gl,"P",{});var Tct=n(Pge);Ier=r(Tct,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Tct.forEach(t),jer=i(Gl),bc=s(Gl,"P",{});var jW=n(bc);Ner=r(jW,`Note:
Loading a model from its configuration file does `),$ge=s(jW,"STRONG",{});var Fct=n($ge);Der=r(Fct,"not"),Fct.forEach(t),qer=r(jW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ige=s(jW,"CODE",{});var Cct=n(Ige);Oer=r(Cct,"from_pretrained()"),Cct.forEach(t),Ger=r(jW,"to load the model weights."),jW.forEach(t),Xer=i(Gl),jge=s(Gl,"P",{});var Mct=n(jge);Ver=r(Mct,"Examples:"),Mct.forEach(t),zer=i(Gl),f(QA.$$.fragment,Gl),Gl.forEach(t),Wer=i(Ol),uo=s(Ol,"DIV",{class:!0});var pa=n(uo);f(HA.$$.fragment,pa),Qer=i(pa),Nge=s(pa,"P",{});var Ect=n(Nge);Her=r(Ect,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ect.forEach(t),Uer=i(pa),ps=s(pa,"P",{});var k5=n(ps);Jer=r(k5,"The model class to instantiate is selected based on the "),Dge=s(k5,"CODE",{});var yct=n(Dge);Yer=r(yct,"model_type"),yct.forEach(t),Ker=r(k5,` property of the config object (either
passed as an argument or loaded from `),qge=s(k5,"CODE",{});var wct=n(qge);Zer=r(wct,"pretrained_model_name_or_path"),wct.forEach(t),eor=r(k5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Oge=s(k5,"CODE",{});var Act=n(Oge);oor=r(Act,"pretrained_model_name_or_path"),Act.forEach(t),ror=r(k5,":"),k5.forEach(t),tor=i(pa),H=s(pa,"UL",{});var U=n(H);ZF=s(U,"LI",{});var a7e=n(ZF);Gge=s(a7e,"STRONG",{});var Lct=n(Gge);aor=r(Lct,"albert"),Lct.forEach(t),sor=r(a7e," \u2014 "),aO=s(a7e,"A",{href:!0});var Bct=n(aO);nor=r(Bct,"TFAlbertForPreTraining"),Bct.forEach(t),lor=r(a7e," (ALBERT model)"),a7e.forEach(t),ior=i(U),eC=s(U,"LI",{});var s7e=n(eC);Xge=s(s7e,"STRONG",{});var xct=n(Xge);dor=r(xct,"bart"),xct.forEach(t),cor=r(s7e," \u2014 "),sO=s(s7e,"A",{href:!0});var kct=n(sO);mor=r(kct,"TFBartForConditionalGeneration"),kct.forEach(t),gor=r(s7e," (BART model)"),s7e.forEach(t),hor=i(U),oC=s(U,"LI",{});var n7e=n(oC);Vge=s(n7e,"STRONG",{});var Rct=n(Vge);uor=r(Rct,"bert"),Rct.forEach(t),por=r(n7e," \u2014 "),nO=s(n7e,"A",{href:!0});var Sct=n(nO);_or=r(Sct,"TFBertForPreTraining"),Sct.forEach(t),bor=r(n7e," (BERT model)"),n7e.forEach(t),vor=i(U),rC=s(U,"LI",{});var l7e=n(rC);zge=s(l7e,"STRONG",{});var Pct=n(zge);Tor=r(Pct,"camembert"),Pct.forEach(t),For=r(l7e," \u2014 "),lO=s(l7e,"A",{href:!0});var $ct=n(lO);Cor=r($ct,"TFCamembertForMaskedLM"),$ct.forEach(t),Mor=r(l7e," (CamemBERT model)"),l7e.forEach(t),Eor=i(U),tC=s(U,"LI",{});var i7e=n(tC);Wge=s(i7e,"STRONG",{});var Ict=n(Wge);yor=r(Ict,"ctrl"),Ict.forEach(t),wor=r(i7e," \u2014 "),iO=s(i7e,"A",{href:!0});var jct=n(iO);Aor=r(jct,"TFCTRLLMHeadModel"),jct.forEach(t),Lor=r(i7e," (CTRL model)"),i7e.forEach(t),Bor=i(U),aC=s(U,"LI",{});var d7e=n(aC);Qge=s(d7e,"STRONG",{});var Nct=n(Qge);xor=r(Nct,"distilbert"),Nct.forEach(t),kor=r(d7e," \u2014 "),dO=s(d7e,"A",{href:!0});var Dct=n(dO);Ror=r(Dct,"TFDistilBertForMaskedLM"),Dct.forEach(t),Sor=r(d7e," (DistilBERT model)"),d7e.forEach(t),Por=i(U),sC=s(U,"LI",{});var c7e=n(sC);Hge=s(c7e,"STRONG",{});var qct=n(Hge);$or=r(qct,"electra"),qct.forEach(t),Ior=r(c7e," \u2014 "),cO=s(c7e,"A",{href:!0});var Oct=n(cO);jor=r(Oct,"TFElectraForPreTraining"),Oct.forEach(t),Nor=r(c7e," (ELECTRA model)"),c7e.forEach(t),Dor=i(U),nC=s(U,"LI",{});var m7e=n(nC);Uge=s(m7e,"STRONG",{});var Gct=n(Uge);qor=r(Gct,"flaubert"),Gct.forEach(t),Oor=r(m7e," \u2014 "),mO=s(m7e,"A",{href:!0});var Xct=n(mO);Gor=r(Xct,"TFFlaubertWithLMHeadModel"),Xct.forEach(t),Xor=r(m7e," (FlauBERT model)"),m7e.forEach(t),Vor=i(U),lC=s(U,"LI",{});var f7e=n(lC);Jge=s(f7e,"STRONG",{});var Vct=n(Jge);zor=r(Vct,"funnel"),Vct.forEach(t),Wor=r(f7e," \u2014 "),fO=s(f7e,"A",{href:!0});var zct=n(fO);Qor=r(zct,"TFFunnelForPreTraining"),zct.forEach(t),Hor=r(f7e," (Funnel Transformer model)"),f7e.forEach(t),Uor=i(U),iC=s(U,"LI",{});var g7e=n(iC);Yge=s(g7e,"STRONG",{});var Wct=n(Yge);Jor=r(Wct,"gpt2"),Wct.forEach(t),Yor=r(g7e," \u2014 "),gO=s(g7e,"A",{href:!0});var Qct=n(gO);Kor=r(Qct,"TFGPT2LMHeadModel"),Qct.forEach(t),Zor=r(g7e," (OpenAI GPT-2 model)"),g7e.forEach(t),err=i(U),dC=s(U,"LI",{});var h7e=n(dC);Kge=s(h7e,"STRONG",{});var Hct=n(Kge);orr=r(Hct,"layoutlm"),Hct.forEach(t),rrr=r(h7e," \u2014 "),hO=s(h7e,"A",{href:!0});var Uct=n(hO);trr=r(Uct,"TFLayoutLMForMaskedLM"),Uct.forEach(t),arr=r(h7e," (LayoutLM model)"),h7e.forEach(t),srr=i(U),cC=s(U,"LI",{});var u7e=n(cC);Zge=s(u7e,"STRONG",{});var Jct=n(Zge);nrr=r(Jct,"lxmert"),Jct.forEach(t),lrr=r(u7e," \u2014 "),uO=s(u7e,"A",{href:!0});var Yct=n(uO);irr=r(Yct,"TFLxmertForPreTraining"),Yct.forEach(t),drr=r(u7e," (LXMERT model)"),u7e.forEach(t),crr=i(U),mC=s(U,"LI",{});var p7e=n(mC);ehe=s(p7e,"STRONG",{});var Kct=n(ehe);mrr=r(Kct,"mobilebert"),Kct.forEach(t),frr=r(p7e," \u2014 "),pO=s(p7e,"A",{href:!0});var Zct=n(pO);grr=r(Zct,"TFMobileBertForPreTraining"),Zct.forEach(t),hrr=r(p7e," (MobileBERT model)"),p7e.forEach(t),urr=i(U),fC=s(U,"LI",{});var _7e=n(fC);ohe=s(_7e,"STRONG",{});var emt=n(ohe);prr=r(emt,"mpnet"),emt.forEach(t),_rr=r(_7e," \u2014 "),_O=s(_7e,"A",{href:!0});var omt=n(_O);brr=r(omt,"TFMPNetForMaskedLM"),omt.forEach(t),vrr=r(_7e," (MPNet model)"),_7e.forEach(t),Trr=i(U),gC=s(U,"LI",{});var b7e=n(gC);rhe=s(b7e,"STRONG",{});var rmt=n(rhe);Frr=r(rmt,"openai-gpt"),rmt.forEach(t),Crr=r(b7e," \u2014 "),bO=s(b7e,"A",{href:!0});var tmt=n(bO);Mrr=r(tmt,"TFOpenAIGPTLMHeadModel"),tmt.forEach(t),Err=r(b7e," (OpenAI GPT model)"),b7e.forEach(t),yrr=i(U),hC=s(U,"LI",{});var v7e=n(hC);the=s(v7e,"STRONG",{});var amt=n(the);wrr=r(amt,"roberta"),amt.forEach(t),Arr=r(v7e," \u2014 "),vO=s(v7e,"A",{href:!0});var smt=n(vO);Lrr=r(smt,"TFRobertaForMaskedLM"),smt.forEach(t),Brr=r(v7e," (RoBERTa model)"),v7e.forEach(t),xrr=i(U),uC=s(U,"LI",{});var T7e=n(uC);ahe=s(T7e,"STRONG",{});var nmt=n(ahe);krr=r(nmt,"t5"),nmt.forEach(t),Rrr=r(T7e," \u2014 "),TO=s(T7e,"A",{href:!0});var lmt=n(TO);Srr=r(lmt,"TFT5ForConditionalGeneration"),lmt.forEach(t),Prr=r(T7e," (T5 model)"),T7e.forEach(t),$rr=i(U),pC=s(U,"LI",{});var F7e=n(pC);she=s(F7e,"STRONG",{});var imt=n(she);Irr=r(imt,"tapas"),imt.forEach(t),jrr=r(F7e," \u2014 "),FO=s(F7e,"A",{href:!0});var dmt=n(FO);Nrr=r(dmt,"TFTapasForMaskedLM"),dmt.forEach(t),Drr=r(F7e," (TAPAS model)"),F7e.forEach(t),qrr=i(U),_C=s(U,"LI",{});var C7e=n(_C);nhe=s(C7e,"STRONG",{});var cmt=n(nhe);Orr=r(cmt,"transfo-xl"),cmt.forEach(t),Grr=r(C7e," \u2014 "),CO=s(C7e,"A",{href:!0});var mmt=n(CO);Xrr=r(mmt,"TFTransfoXLLMHeadModel"),mmt.forEach(t),Vrr=r(C7e," (Transformer-XL model)"),C7e.forEach(t),zrr=i(U),bC=s(U,"LI",{});var M7e=n(bC);lhe=s(M7e,"STRONG",{});var fmt=n(lhe);Wrr=r(fmt,"xlm"),fmt.forEach(t),Qrr=r(M7e," \u2014 "),MO=s(M7e,"A",{href:!0});var gmt=n(MO);Hrr=r(gmt,"TFXLMWithLMHeadModel"),gmt.forEach(t),Urr=r(M7e," (XLM model)"),M7e.forEach(t),Jrr=i(U),vC=s(U,"LI",{});var E7e=n(vC);ihe=s(E7e,"STRONG",{});var hmt=n(ihe);Yrr=r(hmt,"xlm-roberta"),hmt.forEach(t),Krr=r(E7e," \u2014 "),EO=s(E7e,"A",{href:!0});var umt=n(EO);Zrr=r(umt,"TFXLMRobertaForMaskedLM"),umt.forEach(t),etr=r(E7e," (XLM-RoBERTa model)"),E7e.forEach(t),otr=i(U),TC=s(U,"LI",{});var y7e=n(TC);dhe=s(y7e,"STRONG",{});var pmt=n(dhe);rtr=r(pmt,"xlnet"),pmt.forEach(t),ttr=r(y7e," \u2014 "),yO=s(y7e,"A",{href:!0});var _mt=n(yO);atr=r(_mt,"TFXLNetLMHeadModel"),_mt.forEach(t),str=r(y7e," (XLNet model)"),y7e.forEach(t),U.forEach(t),ntr=i(pa),che=s(pa,"P",{});var bmt=n(che);ltr=r(bmt,"Examples:"),bmt.forEach(t),itr=i(pa),f(UA.$$.fragment,pa),pa.forEach(t),Ol.forEach(t),Qke=i(c),vc=s(c,"H2",{class:!0});var aPe=n(vc);FC=s(aPe,"A",{id:!0,class:!0,href:!0});var vmt=n(FC);mhe=s(vmt,"SPAN",{});var Tmt=n(mhe);f(JA.$$.fragment,Tmt),Tmt.forEach(t),vmt.forEach(t),dtr=i(aPe),fhe=s(aPe,"SPAN",{});var Fmt=n(fhe);ctr=r(Fmt,"TFAutoModelForCausalLM"),Fmt.forEach(t),aPe.forEach(t),Hke=i(c),Fr=s(c,"DIV",{class:!0});var Xl=n(Fr);f(YA.$$.fragment,Xl),mtr=i(Xl),Tc=s(Xl,"P",{});var NW=n(Tc);ftr=r(NW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),ghe=s(NW,"CODE",{});var Cmt=n(ghe);gtr=r(Cmt,"from_pretrained()"),Cmt.forEach(t),htr=r(NW,"class method or the "),hhe=s(NW,"CODE",{});var Mmt=n(hhe);utr=r(Mmt,"from_config()"),Mmt.forEach(t),ptr=r(NW,`class
method.`),NW.forEach(t),_tr=i(Xl),KA=s(Xl,"P",{});var sPe=n(KA);btr=r(sPe,"This class cannot be instantiated directly using "),uhe=s(sPe,"CODE",{});var Emt=n(uhe);vtr=r(Emt,"__init__()"),Emt.forEach(t),Ttr=r(sPe," (throws an error)."),sPe.forEach(t),Ftr=i(Xl),ut=s(Xl,"DIV",{class:!0});var Vl=n(ut);f(ZA.$$.fragment,Vl),Ctr=i(Vl),phe=s(Vl,"P",{});var ymt=n(phe);Mtr=r(ymt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ymt.forEach(t),Etr=i(Vl),Fc=s(Vl,"P",{});var DW=n(Fc);ytr=r(DW,`Note:
Loading a model from its configuration file does `),_he=s(DW,"STRONG",{});var wmt=n(_he);wtr=r(wmt,"not"),wmt.forEach(t),Atr=r(DW,` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=s(DW,"CODE",{});var Amt=n(bhe);Ltr=r(Amt,"from_pretrained()"),Amt.forEach(t),Btr=r(DW,"to load the model weights."),DW.forEach(t),xtr=i(Vl),vhe=s(Vl,"P",{});var Lmt=n(vhe);ktr=r(Lmt,"Examples:"),Lmt.forEach(t),Rtr=i(Vl),f(e0.$$.fragment,Vl),Vl.forEach(t),Str=i(Xl),po=s(Xl,"DIV",{class:!0});var _a=n(po);f(o0.$$.fragment,_a),Ptr=i(_a),The=s(_a,"P",{});var Bmt=n(The);$tr=r(Bmt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Bmt.forEach(t),Itr=i(_a),_s=s(_a,"P",{});var R5=n(_s);jtr=r(R5,"The model class to instantiate is selected based on the "),Fhe=s(R5,"CODE",{});var xmt=n(Fhe);Ntr=r(xmt,"model_type"),xmt.forEach(t),Dtr=r(R5,` property of the config object (either
passed as an argument or loaded from `),Che=s(R5,"CODE",{});var kmt=n(Che);qtr=r(kmt,"pretrained_model_name_or_path"),kmt.forEach(t),Otr=r(R5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=s(R5,"CODE",{});var Rmt=n(Mhe);Gtr=r(Rmt,"pretrained_model_name_or_path"),Rmt.forEach(t),Xtr=r(R5,":"),R5.forEach(t),Vtr=i(_a),ge=s(_a,"UL",{});var Ce=n(ge);CC=s(Ce,"LI",{});var w7e=n(CC);Ehe=s(w7e,"STRONG",{});var Smt=n(Ehe);ztr=r(Smt,"bert"),Smt.forEach(t),Wtr=r(w7e," \u2014 "),wO=s(w7e,"A",{href:!0});var Pmt=n(wO);Qtr=r(Pmt,"TFBertLMHeadModel"),Pmt.forEach(t),Htr=r(w7e," (BERT model)"),w7e.forEach(t),Utr=i(Ce),MC=s(Ce,"LI",{});var A7e=n(MC);yhe=s(A7e,"STRONG",{});var $mt=n(yhe);Jtr=r($mt,"camembert"),$mt.forEach(t),Ytr=r(A7e," \u2014 "),AO=s(A7e,"A",{href:!0});var Imt=n(AO);Ktr=r(Imt,"TFCamembertForCausalLM"),Imt.forEach(t),Ztr=r(A7e," (CamemBERT model)"),A7e.forEach(t),ear=i(Ce),EC=s(Ce,"LI",{});var L7e=n(EC);whe=s(L7e,"STRONG",{});var jmt=n(whe);oar=r(jmt,"ctrl"),jmt.forEach(t),rar=r(L7e," \u2014 "),LO=s(L7e,"A",{href:!0});var Nmt=n(LO);tar=r(Nmt,"TFCTRLLMHeadModel"),Nmt.forEach(t),aar=r(L7e," (CTRL model)"),L7e.forEach(t),sar=i(Ce),yC=s(Ce,"LI",{});var B7e=n(yC);Ahe=s(B7e,"STRONG",{});var Dmt=n(Ahe);nar=r(Dmt,"gpt2"),Dmt.forEach(t),lar=r(B7e," \u2014 "),BO=s(B7e,"A",{href:!0});var qmt=n(BO);iar=r(qmt,"TFGPT2LMHeadModel"),qmt.forEach(t),dar=r(B7e," (OpenAI GPT-2 model)"),B7e.forEach(t),car=i(Ce),wC=s(Ce,"LI",{});var x7e=n(wC);Lhe=s(x7e,"STRONG",{});var Omt=n(Lhe);mar=r(Omt,"openai-gpt"),Omt.forEach(t),far=r(x7e," \u2014 "),xO=s(x7e,"A",{href:!0});var Gmt=n(xO);gar=r(Gmt,"TFOpenAIGPTLMHeadModel"),Gmt.forEach(t),har=r(x7e," (OpenAI GPT model)"),x7e.forEach(t),uar=i(Ce),AC=s(Ce,"LI",{});var k7e=n(AC);Bhe=s(k7e,"STRONG",{});var Xmt=n(Bhe);par=r(Xmt,"rembert"),Xmt.forEach(t),_ar=r(k7e," \u2014 "),kO=s(k7e,"A",{href:!0});var Vmt=n(kO);bar=r(Vmt,"TFRemBertForCausalLM"),Vmt.forEach(t),Tar=r(k7e," (RemBERT model)"),k7e.forEach(t),Far=i(Ce),LC=s(Ce,"LI",{});var R7e=n(LC);xhe=s(R7e,"STRONG",{});var zmt=n(xhe);Car=r(zmt,"roberta"),zmt.forEach(t),Mar=r(R7e," \u2014 "),RO=s(R7e,"A",{href:!0});var Wmt=n(RO);Ear=r(Wmt,"TFRobertaForCausalLM"),Wmt.forEach(t),yar=r(R7e," (RoBERTa model)"),R7e.forEach(t),war=i(Ce),BC=s(Ce,"LI",{});var S7e=n(BC);khe=s(S7e,"STRONG",{});var Qmt=n(khe);Aar=r(Qmt,"roformer"),Qmt.forEach(t),Lar=r(S7e," \u2014 "),SO=s(S7e,"A",{href:!0});var Hmt=n(SO);Bar=r(Hmt,"TFRoFormerForCausalLM"),Hmt.forEach(t),xar=r(S7e," (RoFormer model)"),S7e.forEach(t),kar=i(Ce),xC=s(Ce,"LI",{});var P7e=n(xC);Rhe=s(P7e,"STRONG",{});var Umt=n(Rhe);Rar=r(Umt,"transfo-xl"),Umt.forEach(t),Sar=r(P7e," \u2014 "),PO=s(P7e,"A",{href:!0});var Jmt=n(PO);Par=r(Jmt,"TFTransfoXLLMHeadModel"),Jmt.forEach(t),$ar=r(P7e," (Transformer-XL model)"),P7e.forEach(t),Iar=i(Ce),kC=s(Ce,"LI",{});var $7e=n(kC);She=s($7e,"STRONG",{});var Ymt=n(She);jar=r(Ymt,"xlm"),Ymt.forEach(t),Nar=r($7e," \u2014 "),$O=s($7e,"A",{href:!0});var Kmt=n($O);Dar=r(Kmt,"TFXLMWithLMHeadModel"),Kmt.forEach(t),qar=r($7e," (XLM model)"),$7e.forEach(t),Oar=i(Ce),RC=s(Ce,"LI",{});var I7e=n(RC);Phe=s(I7e,"STRONG",{});var Zmt=n(Phe);Gar=r(Zmt,"xlnet"),Zmt.forEach(t),Xar=r(I7e," \u2014 "),IO=s(I7e,"A",{href:!0});var eft=n(IO);Var=r(eft,"TFXLNetLMHeadModel"),eft.forEach(t),zar=r(I7e," (XLNet model)"),I7e.forEach(t),Ce.forEach(t),War=i(_a),$he=s(_a,"P",{});var oft=n($he);Qar=r(oft,"Examples:"),oft.forEach(t),Har=i(_a),f(r0.$$.fragment,_a),_a.forEach(t),Xl.forEach(t),Uke=i(c),Cc=s(c,"H2",{class:!0});var nPe=n(Cc);SC=s(nPe,"A",{id:!0,class:!0,href:!0});var rft=n(SC);Ihe=s(rft,"SPAN",{});var tft=n(Ihe);f(t0.$$.fragment,tft),tft.forEach(t),rft.forEach(t),Uar=i(nPe),jhe=s(nPe,"SPAN",{});var aft=n(jhe);Jar=r(aft,"TFAutoModelForImageClassification"),aft.forEach(t),nPe.forEach(t),Jke=i(c),Cr=s(c,"DIV",{class:!0});var zl=n(Cr);f(a0.$$.fragment,zl),Yar=i(zl),Mc=s(zl,"P",{});var qW=n(Mc);Kar=r(qW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Nhe=s(qW,"CODE",{});var sft=n(Nhe);Zar=r(sft,"from_pretrained()"),sft.forEach(t),esr=r(qW,"class method or the "),Dhe=s(qW,"CODE",{});var nft=n(Dhe);osr=r(nft,"from_config()"),nft.forEach(t),rsr=r(qW,`class
method.`),qW.forEach(t),tsr=i(zl),s0=s(zl,"P",{});var lPe=n(s0);asr=r(lPe,"This class cannot be instantiated directly using "),qhe=s(lPe,"CODE",{});var lft=n(qhe);ssr=r(lft,"__init__()"),lft.forEach(t),nsr=r(lPe," (throws an error)."),lPe.forEach(t),lsr=i(zl),pt=s(zl,"DIV",{class:!0});var Wl=n(pt);f(n0.$$.fragment,Wl),isr=i(Wl),Ohe=s(Wl,"P",{});var ift=n(Ohe);dsr=r(ift,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),ift.forEach(t),csr=i(Wl),Ec=s(Wl,"P",{});var OW=n(Ec);msr=r(OW,`Note:
Loading a model from its configuration file does `),Ghe=s(OW,"STRONG",{});var dft=n(Ghe);fsr=r(dft,"not"),dft.forEach(t),gsr=r(OW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xhe=s(OW,"CODE",{});var cft=n(Xhe);hsr=r(cft,"from_pretrained()"),cft.forEach(t),usr=r(OW,"to load the model weights."),OW.forEach(t),psr=i(Wl),Vhe=s(Wl,"P",{});var mft=n(Vhe);_sr=r(mft,"Examples:"),mft.forEach(t),bsr=i(Wl),f(l0.$$.fragment,Wl),Wl.forEach(t),vsr=i(zl),_o=s(zl,"DIV",{class:!0});var ba=n(_o);f(i0.$$.fragment,ba),Tsr=i(ba),zhe=s(ba,"P",{});var fft=n(zhe);Fsr=r(fft,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),fft.forEach(t),Csr=i(ba),bs=s(ba,"P",{});var S5=n(bs);Msr=r(S5,"The model class to instantiate is selected based on the "),Whe=s(S5,"CODE",{});var gft=n(Whe);Esr=r(gft,"model_type"),gft.forEach(t),ysr=r(S5,` property of the config object (either
passed as an argument or loaded from `),Qhe=s(S5,"CODE",{});var hft=n(Qhe);wsr=r(hft,"pretrained_model_name_or_path"),hft.forEach(t),Asr=r(S5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hhe=s(S5,"CODE",{});var uft=n(Hhe);Lsr=r(uft,"pretrained_model_name_or_path"),uft.forEach(t),Bsr=r(S5,":"),S5.forEach(t),xsr=i(ba),d0=s(ba,"UL",{});var iPe=n(d0);PC=s(iPe,"LI",{});var j7e=n(PC);Uhe=s(j7e,"STRONG",{});var pft=n(Uhe);ksr=r(pft,"convnext"),pft.forEach(t),Rsr=r(j7e," \u2014 "),jO=s(j7e,"A",{href:!0});var _ft=n(jO);Ssr=r(_ft,"TFConvNextForImageClassification"),_ft.forEach(t),Psr=r(j7e," (ConvNext model)"),j7e.forEach(t),$sr=i(iPe),$C=s(iPe,"LI",{});var N7e=n($C);Jhe=s(N7e,"STRONG",{});var bft=n(Jhe);Isr=r(bft,"vit"),bft.forEach(t),jsr=r(N7e," \u2014 "),NO=s(N7e,"A",{href:!0});var vft=n(NO);Nsr=r(vft,"TFViTForImageClassification"),vft.forEach(t),Dsr=r(N7e," (ViT model)"),N7e.forEach(t),iPe.forEach(t),qsr=i(ba),Yhe=s(ba,"P",{});var Tft=n(Yhe);Osr=r(Tft,"Examples:"),Tft.forEach(t),Gsr=i(ba),f(c0.$$.fragment,ba),ba.forEach(t),zl.forEach(t),Yke=i(c),yc=s(c,"H2",{class:!0});var dPe=n(yc);IC=s(dPe,"A",{id:!0,class:!0,href:!0});var Fft=n(IC);Khe=s(Fft,"SPAN",{});var Cft=n(Khe);f(m0.$$.fragment,Cft),Cft.forEach(t),Fft.forEach(t),Xsr=i(dPe),Zhe=s(dPe,"SPAN",{});var Mft=n(Zhe);Vsr=r(Mft,"TFAutoModelForMaskedLM"),Mft.forEach(t),dPe.forEach(t),Kke=i(c),Mr=s(c,"DIV",{class:!0});var Ql=n(Mr);f(f0.$$.fragment,Ql),zsr=i(Ql),wc=s(Ql,"P",{});var GW=n(wc);Wsr=r(GW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),eue=s(GW,"CODE",{});var Eft=n(eue);Qsr=r(Eft,"from_pretrained()"),Eft.forEach(t),Hsr=r(GW,"class method or the "),oue=s(GW,"CODE",{});var yft=n(oue);Usr=r(yft,"from_config()"),yft.forEach(t),Jsr=r(GW,`class
method.`),GW.forEach(t),Ysr=i(Ql),g0=s(Ql,"P",{});var cPe=n(g0);Ksr=r(cPe,"This class cannot be instantiated directly using "),rue=s(cPe,"CODE",{});var wft=n(rue);Zsr=r(wft,"__init__()"),wft.forEach(t),enr=r(cPe," (throws an error)."),cPe.forEach(t),onr=i(Ql),_t=s(Ql,"DIV",{class:!0});var Hl=n(_t);f(h0.$$.fragment,Hl),rnr=i(Hl),tue=s(Hl,"P",{});var Aft=n(tue);tnr=r(Aft,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Aft.forEach(t),anr=i(Hl),Ac=s(Hl,"P",{});var XW=n(Ac);snr=r(XW,`Note:
Loading a model from its configuration file does `),aue=s(XW,"STRONG",{});var Lft=n(aue);nnr=r(Lft,"not"),Lft.forEach(t),lnr=r(XW,` load the model weights. It only affects the
model\u2019s configuration. Use `),sue=s(XW,"CODE",{});var Bft=n(sue);inr=r(Bft,"from_pretrained()"),Bft.forEach(t),dnr=r(XW,"to load the model weights."),XW.forEach(t),cnr=i(Hl),nue=s(Hl,"P",{});var xft=n(nue);mnr=r(xft,"Examples:"),xft.forEach(t),fnr=i(Hl),f(u0.$$.fragment,Hl),Hl.forEach(t),gnr=i(Ql),bo=s(Ql,"DIV",{class:!0});var va=n(bo);f(p0.$$.fragment,va),hnr=i(va),lue=s(va,"P",{});var kft=n(lue);unr=r(kft,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),kft.forEach(t),pnr=i(va),vs=s(va,"P",{});var P5=n(vs);_nr=r(P5,"The model class to instantiate is selected based on the "),iue=s(P5,"CODE",{});var Rft=n(iue);bnr=r(Rft,"model_type"),Rft.forEach(t),vnr=r(P5,` property of the config object (either
passed as an argument or loaded from `),due=s(P5,"CODE",{});var Sft=n(due);Tnr=r(Sft,"pretrained_model_name_or_path"),Sft.forEach(t),Fnr=r(P5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cue=s(P5,"CODE",{});var Pft=n(cue);Cnr=r(Pft,"pretrained_model_name_or_path"),Pft.forEach(t),Mnr=r(P5,":"),P5.forEach(t),Enr=i(va),Y=s(va,"UL",{});var ee=n(Y);jC=s(ee,"LI",{});var D7e=n(jC);mue=s(D7e,"STRONG",{});var $ft=n(mue);ynr=r($ft,"albert"),$ft.forEach(t),wnr=r(D7e," \u2014 "),DO=s(D7e,"A",{href:!0});var Ift=n(DO);Anr=r(Ift,"TFAlbertForMaskedLM"),Ift.forEach(t),Lnr=r(D7e," (ALBERT model)"),D7e.forEach(t),Bnr=i(ee),NC=s(ee,"LI",{});var q7e=n(NC);fue=s(q7e,"STRONG",{});var jft=n(fue);xnr=r(jft,"bert"),jft.forEach(t),knr=r(q7e," \u2014 "),qO=s(q7e,"A",{href:!0});var Nft=n(qO);Rnr=r(Nft,"TFBertForMaskedLM"),Nft.forEach(t),Snr=r(q7e," (BERT model)"),q7e.forEach(t),Pnr=i(ee),DC=s(ee,"LI",{});var O7e=n(DC);gue=s(O7e,"STRONG",{});var Dft=n(gue);$nr=r(Dft,"camembert"),Dft.forEach(t),Inr=r(O7e," \u2014 "),OO=s(O7e,"A",{href:!0});var qft=n(OO);jnr=r(qft,"TFCamembertForMaskedLM"),qft.forEach(t),Nnr=r(O7e," (CamemBERT model)"),O7e.forEach(t),Dnr=i(ee),qC=s(ee,"LI",{});var G7e=n(qC);hue=s(G7e,"STRONG",{});var Oft=n(hue);qnr=r(Oft,"convbert"),Oft.forEach(t),Onr=r(G7e," \u2014 "),GO=s(G7e,"A",{href:!0});var Gft=n(GO);Gnr=r(Gft,"TFConvBertForMaskedLM"),Gft.forEach(t),Xnr=r(G7e," (ConvBERT model)"),G7e.forEach(t),Vnr=i(ee),OC=s(ee,"LI",{});var X7e=n(OC);uue=s(X7e,"STRONG",{});var Xft=n(uue);znr=r(Xft,"deberta"),Xft.forEach(t),Wnr=r(X7e," \u2014 "),XO=s(X7e,"A",{href:!0});var Vft=n(XO);Qnr=r(Vft,"TFDebertaForMaskedLM"),Vft.forEach(t),Hnr=r(X7e," (DeBERTa model)"),X7e.forEach(t),Unr=i(ee),GC=s(ee,"LI",{});var V7e=n(GC);pue=s(V7e,"STRONG",{});var zft=n(pue);Jnr=r(zft,"deberta-v2"),zft.forEach(t),Ynr=r(V7e," \u2014 "),VO=s(V7e,"A",{href:!0});var Wft=n(VO);Knr=r(Wft,"TFDebertaV2ForMaskedLM"),Wft.forEach(t),Znr=r(V7e," (DeBERTa-v2 model)"),V7e.forEach(t),elr=i(ee),XC=s(ee,"LI",{});var z7e=n(XC);_ue=s(z7e,"STRONG",{});var Qft=n(_ue);olr=r(Qft,"distilbert"),Qft.forEach(t),rlr=r(z7e," \u2014 "),zO=s(z7e,"A",{href:!0});var Hft=n(zO);tlr=r(Hft,"TFDistilBertForMaskedLM"),Hft.forEach(t),alr=r(z7e," (DistilBERT model)"),z7e.forEach(t),slr=i(ee),VC=s(ee,"LI",{});var W7e=n(VC);bue=s(W7e,"STRONG",{});var Uft=n(bue);nlr=r(Uft,"electra"),Uft.forEach(t),llr=r(W7e," \u2014 "),WO=s(W7e,"A",{href:!0});var Jft=n(WO);ilr=r(Jft,"TFElectraForMaskedLM"),Jft.forEach(t),dlr=r(W7e," (ELECTRA model)"),W7e.forEach(t),clr=i(ee),zC=s(ee,"LI",{});var Q7e=n(zC);vue=s(Q7e,"STRONG",{});var Yft=n(vue);mlr=r(Yft,"flaubert"),Yft.forEach(t),flr=r(Q7e," \u2014 "),QO=s(Q7e,"A",{href:!0});var Kft=n(QO);glr=r(Kft,"TFFlaubertWithLMHeadModel"),Kft.forEach(t),hlr=r(Q7e," (FlauBERT model)"),Q7e.forEach(t),ulr=i(ee),WC=s(ee,"LI",{});var H7e=n(WC);Tue=s(H7e,"STRONG",{});var Zft=n(Tue);plr=r(Zft,"funnel"),Zft.forEach(t),_lr=r(H7e," \u2014 "),HO=s(H7e,"A",{href:!0});var egt=n(HO);blr=r(egt,"TFFunnelForMaskedLM"),egt.forEach(t),vlr=r(H7e," (Funnel Transformer model)"),H7e.forEach(t),Tlr=i(ee),QC=s(ee,"LI",{});var U7e=n(QC);Fue=s(U7e,"STRONG",{});var ogt=n(Fue);Flr=r(ogt,"layoutlm"),ogt.forEach(t),Clr=r(U7e," \u2014 "),UO=s(U7e,"A",{href:!0});var rgt=n(UO);Mlr=r(rgt,"TFLayoutLMForMaskedLM"),rgt.forEach(t),Elr=r(U7e," (LayoutLM model)"),U7e.forEach(t),ylr=i(ee),HC=s(ee,"LI",{});var J7e=n(HC);Cue=s(J7e,"STRONG",{});var tgt=n(Cue);wlr=r(tgt,"longformer"),tgt.forEach(t),Alr=r(J7e," \u2014 "),JO=s(J7e,"A",{href:!0});var agt=n(JO);Llr=r(agt,"TFLongformerForMaskedLM"),agt.forEach(t),Blr=r(J7e," (Longformer model)"),J7e.forEach(t),xlr=i(ee),UC=s(ee,"LI",{});var Y7e=n(UC);Mue=s(Y7e,"STRONG",{});var sgt=n(Mue);klr=r(sgt,"mobilebert"),sgt.forEach(t),Rlr=r(Y7e," \u2014 "),YO=s(Y7e,"A",{href:!0});var ngt=n(YO);Slr=r(ngt,"TFMobileBertForMaskedLM"),ngt.forEach(t),Plr=r(Y7e," (MobileBERT model)"),Y7e.forEach(t),$lr=i(ee),JC=s(ee,"LI",{});var K7e=n(JC);Eue=s(K7e,"STRONG",{});var lgt=n(Eue);Ilr=r(lgt,"mpnet"),lgt.forEach(t),jlr=r(K7e," \u2014 "),KO=s(K7e,"A",{href:!0});var igt=n(KO);Nlr=r(igt,"TFMPNetForMaskedLM"),igt.forEach(t),Dlr=r(K7e," (MPNet model)"),K7e.forEach(t),qlr=i(ee),YC=s(ee,"LI",{});var Z7e=n(YC);yue=s(Z7e,"STRONG",{});var dgt=n(yue);Olr=r(dgt,"rembert"),dgt.forEach(t),Glr=r(Z7e," \u2014 "),ZO=s(Z7e,"A",{href:!0});var cgt=n(ZO);Xlr=r(cgt,"TFRemBertForMaskedLM"),cgt.forEach(t),Vlr=r(Z7e," (RemBERT model)"),Z7e.forEach(t),zlr=i(ee),KC=s(ee,"LI",{});var e8e=n(KC);wue=s(e8e,"STRONG",{});var mgt=n(wue);Wlr=r(mgt,"roberta"),mgt.forEach(t),Qlr=r(e8e," \u2014 "),eG=s(e8e,"A",{href:!0});var fgt=n(eG);Hlr=r(fgt,"TFRobertaForMaskedLM"),fgt.forEach(t),Ulr=r(e8e," (RoBERTa model)"),e8e.forEach(t),Jlr=i(ee),ZC=s(ee,"LI",{});var o8e=n(ZC);Aue=s(o8e,"STRONG",{});var ggt=n(Aue);Ylr=r(ggt,"roformer"),ggt.forEach(t),Klr=r(o8e," \u2014 "),oG=s(o8e,"A",{href:!0});var hgt=n(oG);Zlr=r(hgt,"TFRoFormerForMaskedLM"),hgt.forEach(t),eir=r(o8e," (RoFormer model)"),o8e.forEach(t),oir=i(ee),eM=s(ee,"LI",{});var r8e=n(eM);Lue=s(r8e,"STRONG",{});var ugt=n(Lue);rir=r(ugt,"tapas"),ugt.forEach(t),tir=r(r8e," \u2014 "),rG=s(r8e,"A",{href:!0});var pgt=n(rG);air=r(pgt,"TFTapasForMaskedLM"),pgt.forEach(t),sir=r(r8e," (TAPAS model)"),r8e.forEach(t),nir=i(ee),oM=s(ee,"LI",{});var t8e=n(oM);Bue=s(t8e,"STRONG",{});var _gt=n(Bue);lir=r(_gt,"xlm"),_gt.forEach(t),iir=r(t8e," \u2014 "),tG=s(t8e,"A",{href:!0});var bgt=n(tG);dir=r(bgt,"TFXLMWithLMHeadModel"),bgt.forEach(t),cir=r(t8e," (XLM model)"),t8e.forEach(t),mir=i(ee),rM=s(ee,"LI",{});var a8e=n(rM);xue=s(a8e,"STRONG",{});var vgt=n(xue);fir=r(vgt,"xlm-roberta"),vgt.forEach(t),gir=r(a8e," \u2014 "),aG=s(a8e,"A",{href:!0});var Tgt=n(aG);hir=r(Tgt,"TFXLMRobertaForMaskedLM"),Tgt.forEach(t),uir=r(a8e," (XLM-RoBERTa model)"),a8e.forEach(t),ee.forEach(t),pir=i(va),kue=s(va,"P",{});var Fgt=n(kue);_ir=r(Fgt,"Examples:"),Fgt.forEach(t),bir=i(va),f(_0.$$.fragment,va),va.forEach(t),Ql.forEach(t),Zke=i(c),Lc=s(c,"H2",{class:!0});var mPe=n(Lc);tM=s(mPe,"A",{id:!0,class:!0,href:!0});var Cgt=n(tM);Rue=s(Cgt,"SPAN",{});var Mgt=n(Rue);f(b0.$$.fragment,Mgt),Mgt.forEach(t),Cgt.forEach(t),vir=i(mPe),Sue=s(mPe,"SPAN",{});var Egt=n(Sue);Tir=r(Egt,"TFAutoModelForSeq2SeqLM"),Egt.forEach(t),mPe.forEach(t),eRe=i(c),Er=s(c,"DIV",{class:!0});var Ul=n(Er);f(v0.$$.fragment,Ul),Fir=i(Ul),Bc=s(Ul,"P",{});var VW=n(Bc);Cir=r(VW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pue=s(VW,"CODE",{});var ygt=n(Pue);Mir=r(ygt,"from_pretrained()"),ygt.forEach(t),Eir=r(VW,"class method or the "),$ue=s(VW,"CODE",{});var wgt=n($ue);yir=r(wgt,"from_config()"),wgt.forEach(t),wir=r(VW,`class
method.`),VW.forEach(t),Air=i(Ul),T0=s(Ul,"P",{});var fPe=n(T0);Lir=r(fPe,"This class cannot be instantiated directly using "),Iue=s(fPe,"CODE",{});var Agt=n(Iue);Bir=r(Agt,"__init__()"),Agt.forEach(t),xir=r(fPe," (throws an error)."),fPe.forEach(t),kir=i(Ul),bt=s(Ul,"DIV",{class:!0});var Jl=n(bt);f(F0.$$.fragment,Jl),Rir=i(Jl),jue=s(Jl,"P",{});var Lgt=n(jue);Sir=r(Lgt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Lgt.forEach(t),Pir=i(Jl),xc=s(Jl,"P",{});var zW=n(xc);$ir=r(zW,`Note:
Loading a model from its configuration file does `),Nue=s(zW,"STRONG",{});var Bgt=n(Nue);Iir=r(Bgt,"not"),Bgt.forEach(t),jir=r(zW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Due=s(zW,"CODE",{});var xgt=n(Due);Nir=r(xgt,"from_pretrained()"),xgt.forEach(t),Dir=r(zW,"to load the model weights."),zW.forEach(t),qir=i(Jl),que=s(Jl,"P",{});var kgt=n(que);Oir=r(kgt,"Examples:"),kgt.forEach(t),Gir=i(Jl),f(C0.$$.fragment,Jl),Jl.forEach(t),Xir=i(Ul),vo=s(Ul,"DIV",{class:!0});var Ta=n(vo);f(M0.$$.fragment,Ta),Vir=i(Ta),Oue=s(Ta,"P",{});var Rgt=n(Oue);zir=r(Rgt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Rgt.forEach(t),Wir=i(Ta),Ts=s(Ta,"P",{});var $5=n(Ts);Qir=r($5,"The model class to instantiate is selected based on the "),Gue=s($5,"CODE",{});var Sgt=n(Gue);Hir=r(Sgt,"model_type"),Sgt.forEach(t),Uir=r($5,` property of the config object (either
passed as an argument or loaded from `),Xue=s($5,"CODE",{});var Pgt=n(Xue);Jir=r(Pgt,"pretrained_model_name_or_path"),Pgt.forEach(t),Yir=r($5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vue=s($5,"CODE",{});var $gt=n(Vue);Kir=r($gt,"pretrained_model_name_or_path"),$gt.forEach(t),Zir=r($5,":"),$5.forEach(t),edr=i(Ta),_e=s(Ta,"UL",{});var ye=n(_e);aM=s(ye,"LI",{});var s8e=n(aM);zue=s(s8e,"STRONG",{});var Igt=n(zue);odr=r(Igt,"bart"),Igt.forEach(t),rdr=r(s8e," \u2014 "),sG=s(s8e,"A",{href:!0});var jgt=n(sG);tdr=r(jgt,"TFBartForConditionalGeneration"),jgt.forEach(t),adr=r(s8e," (BART model)"),s8e.forEach(t),sdr=i(ye),sM=s(ye,"LI",{});var n8e=n(sM);Wue=s(n8e,"STRONG",{});var Ngt=n(Wue);ndr=r(Ngt,"blenderbot"),Ngt.forEach(t),ldr=r(n8e," \u2014 "),nG=s(n8e,"A",{href:!0});var Dgt=n(nG);idr=r(Dgt,"TFBlenderbotForConditionalGeneration"),Dgt.forEach(t),ddr=r(n8e," (Blenderbot model)"),n8e.forEach(t),cdr=i(ye),nM=s(ye,"LI",{});var l8e=n(nM);Que=s(l8e,"STRONG",{});var qgt=n(Que);mdr=r(qgt,"blenderbot-small"),qgt.forEach(t),fdr=r(l8e," \u2014 "),lG=s(l8e,"A",{href:!0});var Ogt=n(lG);gdr=r(Ogt,"TFBlenderbotSmallForConditionalGeneration"),Ogt.forEach(t),hdr=r(l8e," (BlenderbotSmall model)"),l8e.forEach(t),udr=i(ye),lM=s(ye,"LI",{});var i8e=n(lM);Hue=s(i8e,"STRONG",{});var Ggt=n(Hue);pdr=r(Ggt,"encoder-decoder"),Ggt.forEach(t),_dr=r(i8e," \u2014 "),iG=s(i8e,"A",{href:!0});var Xgt=n(iG);bdr=r(Xgt,"TFEncoderDecoderModel"),Xgt.forEach(t),vdr=r(i8e," (Encoder decoder model)"),i8e.forEach(t),Tdr=i(ye),iM=s(ye,"LI",{});var d8e=n(iM);Uue=s(d8e,"STRONG",{});var Vgt=n(Uue);Fdr=r(Vgt,"led"),Vgt.forEach(t),Cdr=r(d8e," \u2014 "),dG=s(d8e,"A",{href:!0});var zgt=n(dG);Mdr=r(zgt,"TFLEDForConditionalGeneration"),zgt.forEach(t),Edr=r(d8e," (LED model)"),d8e.forEach(t),ydr=i(ye),dM=s(ye,"LI",{});var c8e=n(dM);Jue=s(c8e,"STRONG",{});var Wgt=n(Jue);wdr=r(Wgt,"marian"),Wgt.forEach(t),Adr=r(c8e," \u2014 "),cG=s(c8e,"A",{href:!0});var Qgt=n(cG);Ldr=r(Qgt,"TFMarianMTModel"),Qgt.forEach(t),Bdr=r(c8e," (Marian model)"),c8e.forEach(t),xdr=i(ye),cM=s(ye,"LI",{});var m8e=n(cM);Yue=s(m8e,"STRONG",{});var Hgt=n(Yue);kdr=r(Hgt,"mbart"),Hgt.forEach(t),Rdr=r(m8e," \u2014 "),mG=s(m8e,"A",{href:!0});var Ugt=n(mG);Sdr=r(Ugt,"TFMBartForConditionalGeneration"),Ugt.forEach(t),Pdr=r(m8e," (mBART model)"),m8e.forEach(t),$dr=i(ye),mM=s(ye,"LI",{});var f8e=n(mM);Kue=s(f8e,"STRONG",{});var Jgt=n(Kue);Idr=r(Jgt,"mt5"),Jgt.forEach(t),jdr=r(f8e," \u2014 "),fG=s(f8e,"A",{href:!0});var Ygt=n(fG);Ndr=r(Ygt,"TFMT5ForConditionalGeneration"),Ygt.forEach(t),Ddr=r(f8e," (mT5 model)"),f8e.forEach(t),qdr=i(ye),fM=s(ye,"LI",{});var g8e=n(fM);Zue=s(g8e,"STRONG",{});var Kgt=n(Zue);Odr=r(Kgt,"pegasus"),Kgt.forEach(t),Gdr=r(g8e," \u2014 "),gG=s(g8e,"A",{href:!0});var Zgt=n(gG);Xdr=r(Zgt,"TFPegasusForConditionalGeneration"),Zgt.forEach(t),Vdr=r(g8e," (Pegasus model)"),g8e.forEach(t),zdr=i(ye),gM=s(ye,"LI",{});var h8e=n(gM);epe=s(h8e,"STRONG",{});var eht=n(epe);Wdr=r(eht,"t5"),eht.forEach(t),Qdr=r(h8e," \u2014 "),hG=s(h8e,"A",{href:!0});var oht=n(hG);Hdr=r(oht,"TFT5ForConditionalGeneration"),oht.forEach(t),Udr=r(h8e," (T5 model)"),h8e.forEach(t),ye.forEach(t),Jdr=i(Ta),ope=s(Ta,"P",{});var rht=n(ope);Ydr=r(rht,"Examples:"),rht.forEach(t),Kdr=i(Ta),f(E0.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),oRe=i(c),kc=s(c,"H2",{class:!0});var gPe=n(kc);hM=s(gPe,"A",{id:!0,class:!0,href:!0});var tht=n(hM);rpe=s(tht,"SPAN",{});var aht=n(rpe);f(y0.$$.fragment,aht),aht.forEach(t),tht.forEach(t),Zdr=i(gPe),tpe=s(gPe,"SPAN",{});var sht=n(tpe);ecr=r(sht,"TFAutoModelForSequenceClassification"),sht.forEach(t),gPe.forEach(t),rRe=i(c),yr=s(c,"DIV",{class:!0});var Yl=n(yr);f(w0.$$.fragment,Yl),ocr=i(Yl),Rc=s(Yl,"P",{});var WW=n(Rc);rcr=r(WW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ape=s(WW,"CODE",{});var nht=n(ape);tcr=r(nht,"from_pretrained()"),nht.forEach(t),acr=r(WW,"class method or the "),spe=s(WW,"CODE",{});var lht=n(spe);scr=r(lht,"from_config()"),lht.forEach(t),ncr=r(WW,`class
method.`),WW.forEach(t),lcr=i(Yl),A0=s(Yl,"P",{});var hPe=n(A0);icr=r(hPe,"This class cannot be instantiated directly using "),npe=s(hPe,"CODE",{});var iht=n(npe);dcr=r(iht,"__init__()"),iht.forEach(t),ccr=r(hPe," (throws an error)."),hPe.forEach(t),mcr=i(Yl),vt=s(Yl,"DIV",{class:!0});var Kl=n(vt);f(L0.$$.fragment,Kl),fcr=i(Kl),lpe=s(Kl,"P",{});var dht=n(lpe);gcr=r(dht,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),dht.forEach(t),hcr=i(Kl),Sc=s(Kl,"P",{});var QW=n(Sc);ucr=r(QW,`Note:
Loading a model from its configuration file does `),ipe=s(QW,"STRONG",{});var cht=n(ipe);pcr=r(cht,"not"),cht.forEach(t),_cr=r(QW,` load the model weights. It only affects the
model\u2019s configuration. Use `),dpe=s(QW,"CODE",{});var mht=n(dpe);bcr=r(mht,"from_pretrained()"),mht.forEach(t),vcr=r(QW,"to load the model weights."),QW.forEach(t),Tcr=i(Kl),cpe=s(Kl,"P",{});var fht=n(cpe);Fcr=r(fht,"Examples:"),fht.forEach(t),Ccr=i(Kl),f(B0.$$.fragment,Kl),Kl.forEach(t),Mcr=i(Yl),To=s(Yl,"DIV",{class:!0});var Fa=n(To);f(x0.$$.fragment,Fa),Ecr=i(Fa),mpe=s(Fa,"P",{});var ght=n(mpe);ycr=r(ght,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),ght.forEach(t),wcr=i(Fa),Fs=s(Fa,"P",{});var I5=n(Fs);Acr=r(I5,"The model class to instantiate is selected based on the "),fpe=s(I5,"CODE",{});var hht=n(fpe);Lcr=r(hht,"model_type"),hht.forEach(t),Bcr=r(I5,` property of the config object (either
passed as an argument or loaded from `),gpe=s(I5,"CODE",{});var uht=n(gpe);xcr=r(uht,"pretrained_model_name_or_path"),uht.forEach(t),kcr=r(I5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hpe=s(I5,"CODE",{});var pht=n(hpe);Rcr=r(pht,"pretrained_model_name_or_path"),pht.forEach(t),Scr=r(I5,":"),I5.forEach(t),Pcr=i(Fa),V=s(Fa,"UL",{});var W=n(V);uM=s(W,"LI",{});var u8e=n(uM);upe=s(u8e,"STRONG",{});var _ht=n(upe);$cr=r(_ht,"albert"),_ht.forEach(t),Icr=r(u8e," \u2014 "),uG=s(u8e,"A",{href:!0});var bht=n(uG);jcr=r(bht,"TFAlbertForSequenceClassification"),bht.forEach(t),Ncr=r(u8e," (ALBERT model)"),u8e.forEach(t),Dcr=i(W),pM=s(W,"LI",{});var p8e=n(pM);ppe=s(p8e,"STRONG",{});var vht=n(ppe);qcr=r(vht,"bert"),vht.forEach(t),Ocr=r(p8e," \u2014 "),pG=s(p8e,"A",{href:!0});var Tht=n(pG);Gcr=r(Tht,"TFBertForSequenceClassification"),Tht.forEach(t),Xcr=r(p8e," (BERT model)"),p8e.forEach(t),Vcr=i(W),_M=s(W,"LI",{});var _8e=n(_M);_pe=s(_8e,"STRONG",{});var Fht=n(_pe);zcr=r(Fht,"camembert"),Fht.forEach(t),Wcr=r(_8e," \u2014 "),_G=s(_8e,"A",{href:!0});var Cht=n(_G);Qcr=r(Cht,"TFCamembertForSequenceClassification"),Cht.forEach(t),Hcr=r(_8e," (CamemBERT model)"),_8e.forEach(t),Ucr=i(W),bM=s(W,"LI",{});var b8e=n(bM);bpe=s(b8e,"STRONG",{});var Mht=n(bpe);Jcr=r(Mht,"convbert"),Mht.forEach(t),Ycr=r(b8e," \u2014 "),bG=s(b8e,"A",{href:!0});var Eht=n(bG);Kcr=r(Eht,"TFConvBertForSequenceClassification"),Eht.forEach(t),Zcr=r(b8e," (ConvBERT model)"),b8e.forEach(t),emr=i(W),vM=s(W,"LI",{});var v8e=n(vM);vpe=s(v8e,"STRONG",{});var yht=n(vpe);omr=r(yht,"ctrl"),yht.forEach(t),rmr=r(v8e," \u2014 "),vG=s(v8e,"A",{href:!0});var wht=n(vG);tmr=r(wht,"TFCTRLForSequenceClassification"),wht.forEach(t),amr=r(v8e," (CTRL model)"),v8e.forEach(t),smr=i(W),TM=s(W,"LI",{});var T8e=n(TM);Tpe=s(T8e,"STRONG",{});var Aht=n(Tpe);nmr=r(Aht,"deberta"),Aht.forEach(t),lmr=r(T8e," \u2014 "),TG=s(T8e,"A",{href:!0});var Lht=n(TG);imr=r(Lht,"TFDebertaForSequenceClassification"),Lht.forEach(t),dmr=r(T8e," (DeBERTa model)"),T8e.forEach(t),cmr=i(W),FM=s(W,"LI",{});var F8e=n(FM);Fpe=s(F8e,"STRONG",{});var Bht=n(Fpe);mmr=r(Bht,"deberta-v2"),Bht.forEach(t),fmr=r(F8e," \u2014 "),FG=s(F8e,"A",{href:!0});var xht=n(FG);gmr=r(xht,"TFDebertaV2ForSequenceClassification"),xht.forEach(t),hmr=r(F8e," (DeBERTa-v2 model)"),F8e.forEach(t),umr=i(W),CM=s(W,"LI",{});var C8e=n(CM);Cpe=s(C8e,"STRONG",{});var kht=n(Cpe);pmr=r(kht,"distilbert"),kht.forEach(t),_mr=r(C8e," \u2014 "),CG=s(C8e,"A",{href:!0});var Rht=n(CG);bmr=r(Rht,"TFDistilBertForSequenceClassification"),Rht.forEach(t),vmr=r(C8e," (DistilBERT model)"),C8e.forEach(t),Tmr=i(W),MM=s(W,"LI",{});var M8e=n(MM);Mpe=s(M8e,"STRONG",{});var Sht=n(Mpe);Fmr=r(Sht,"electra"),Sht.forEach(t),Cmr=r(M8e," \u2014 "),MG=s(M8e,"A",{href:!0});var Pht=n(MG);Mmr=r(Pht,"TFElectraForSequenceClassification"),Pht.forEach(t),Emr=r(M8e," (ELECTRA model)"),M8e.forEach(t),ymr=i(W),EM=s(W,"LI",{});var E8e=n(EM);Epe=s(E8e,"STRONG",{});var $ht=n(Epe);wmr=r($ht,"flaubert"),$ht.forEach(t),Amr=r(E8e," \u2014 "),EG=s(E8e,"A",{href:!0});var Iht=n(EG);Lmr=r(Iht,"TFFlaubertForSequenceClassification"),Iht.forEach(t),Bmr=r(E8e," (FlauBERT model)"),E8e.forEach(t),xmr=i(W),yM=s(W,"LI",{});var y8e=n(yM);ype=s(y8e,"STRONG",{});var jht=n(ype);kmr=r(jht,"funnel"),jht.forEach(t),Rmr=r(y8e," \u2014 "),yG=s(y8e,"A",{href:!0});var Nht=n(yG);Smr=r(Nht,"TFFunnelForSequenceClassification"),Nht.forEach(t),Pmr=r(y8e," (Funnel Transformer model)"),y8e.forEach(t),$mr=i(W),wM=s(W,"LI",{});var w8e=n(wM);wpe=s(w8e,"STRONG",{});var Dht=n(wpe);Imr=r(Dht,"gpt2"),Dht.forEach(t),jmr=r(w8e," \u2014 "),wG=s(w8e,"A",{href:!0});var qht=n(wG);Nmr=r(qht,"TFGPT2ForSequenceClassification"),qht.forEach(t),Dmr=r(w8e," (OpenAI GPT-2 model)"),w8e.forEach(t),qmr=i(W),AM=s(W,"LI",{});var A8e=n(AM);Ape=s(A8e,"STRONG",{});var Oht=n(Ape);Omr=r(Oht,"layoutlm"),Oht.forEach(t),Gmr=r(A8e," \u2014 "),AG=s(A8e,"A",{href:!0});var Ght=n(AG);Xmr=r(Ght,"TFLayoutLMForSequenceClassification"),Ght.forEach(t),Vmr=r(A8e," (LayoutLM model)"),A8e.forEach(t),zmr=i(W),LM=s(W,"LI",{});var L8e=n(LM);Lpe=s(L8e,"STRONG",{});var Xht=n(Lpe);Wmr=r(Xht,"longformer"),Xht.forEach(t),Qmr=r(L8e," \u2014 "),LG=s(L8e,"A",{href:!0});var Vht=n(LG);Hmr=r(Vht,"TFLongformerForSequenceClassification"),Vht.forEach(t),Umr=r(L8e," (Longformer model)"),L8e.forEach(t),Jmr=i(W),BM=s(W,"LI",{});var B8e=n(BM);Bpe=s(B8e,"STRONG",{});var zht=n(Bpe);Ymr=r(zht,"mobilebert"),zht.forEach(t),Kmr=r(B8e," \u2014 "),BG=s(B8e,"A",{href:!0});var Wht=n(BG);Zmr=r(Wht,"TFMobileBertForSequenceClassification"),Wht.forEach(t),efr=r(B8e," (MobileBERT model)"),B8e.forEach(t),ofr=i(W),xM=s(W,"LI",{});var x8e=n(xM);xpe=s(x8e,"STRONG",{});var Qht=n(xpe);rfr=r(Qht,"mpnet"),Qht.forEach(t),tfr=r(x8e," \u2014 "),xG=s(x8e,"A",{href:!0});var Hht=n(xG);afr=r(Hht,"TFMPNetForSequenceClassification"),Hht.forEach(t),sfr=r(x8e," (MPNet model)"),x8e.forEach(t),nfr=i(W),kM=s(W,"LI",{});var k8e=n(kM);kpe=s(k8e,"STRONG",{});var Uht=n(kpe);lfr=r(Uht,"openai-gpt"),Uht.forEach(t),ifr=r(k8e," \u2014 "),kG=s(k8e,"A",{href:!0});var Jht=n(kG);dfr=r(Jht,"TFOpenAIGPTForSequenceClassification"),Jht.forEach(t),cfr=r(k8e," (OpenAI GPT model)"),k8e.forEach(t),mfr=i(W),RM=s(W,"LI",{});var R8e=n(RM);Rpe=s(R8e,"STRONG",{});var Yht=n(Rpe);ffr=r(Yht,"rembert"),Yht.forEach(t),gfr=r(R8e," \u2014 "),RG=s(R8e,"A",{href:!0});var Kht=n(RG);hfr=r(Kht,"TFRemBertForSequenceClassification"),Kht.forEach(t),ufr=r(R8e," (RemBERT model)"),R8e.forEach(t),pfr=i(W),SM=s(W,"LI",{});var S8e=n(SM);Spe=s(S8e,"STRONG",{});var Zht=n(Spe);_fr=r(Zht,"roberta"),Zht.forEach(t),bfr=r(S8e," \u2014 "),SG=s(S8e,"A",{href:!0});var eut=n(SG);vfr=r(eut,"TFRobertaForSequenceClassification"),eut.forEach(t),Tfr=r(S8e," (RoBERTa model)"),S8e.forEach(t),Ffr=i(W),PM=s(W,"LI",{});var P8e=n(PM);Ppe=s(P8e,"STRONG",{});var out=n(Ppe);Cfr=r(out,"roformer"),out.forEach(t),Mfr=r(P8e," \u2014 "),PG=s(P8e,"A",{href:!0});var rut=n(PG);Efr=r(rut,"TFRoFormerForSequenceClassification"),rut.forEach(t),yfr=r(P8e," (RoFormer model)"),P8e.forEach(t),wfr=i(W),$M=s(W,"LI",{});var $8e=n($M);$pe=s($8e,"STRONG",{});var tut=n($pe);Afr=r(tut,"tapas"),tut.forEach(t),Lfr=r($8e," \u2014 "),$G=s($8e,"A",{href:!0});var aut=n($G);Bfr=r(aut,"TFTapasForSequenceClassification"),aut.forEach(t),xfr=r($8e," (TAPAS model)"),$8e.forEach(t),kfr=i(W),IM=s(W,"LI",{});var I8e=n(IM);Ipe=s(I8e,"STRONG",{});var sut=n(Ipe);Rfr=r(sut,"transfo-xl"),sut.forEach(t),Sfr=r(I8e," \u2014 "),IG=s(I8e,"A",{href:!0});var nut=n(IG);Pfr=r(nut,"TFTransfoXLForSequenceClassification"),nut.forEach(t),$fr=r(I8e," (Transformer-XL model)"),I8e.forEach(t),Ifr=i(W),jM=s(W,"LI",{});var j8e=n(jM);jpe=s(j8e,"STRONG",{});var lut=n(jpe);jfr=r(lut,"xlm"),lut.forEach(t),Nfr=r(j8e," \u2014 "),jG=s(j8e,"A",{href:!0});var iut=n(jG);Dfr=r(iut,"TFXLMForSequenceClassification"),iut.forEach(t),qfr=r(j8e," (XLM model)"),j8e.forEach(t),Ofr=i(W),NM=s(W,"LI",{});var N8e=n(NM);Npe=s(N8e,"STRONG",{});var dut=n(Npe);Gfr=r(dut,"xlm-roberta"),dut.forEach(t),Xfr=r(N8e," \u2014 "),NG=s(N8e,"A",{href:!0});var cut=n(NG);Vfr=r(cut,"TFXLMRobertaForSequenceClassification"),cut.forEach(t),zfr=r(N8e," (XLM-RoBERTa model)"),N8e.forEach(t),Wfr=i(W),DM=s(W,"LI",{});var D8e=n(DM);Dpe=s(D8e,"STRONG",{});var mut=n(Dpe);Qfr=r(mut,"xlnet"),mut.forEach(t),Hfr=r(D8e," \u2014 "),DG=s(D8e,"A",{href:!0});var fut=n(DG);Ufr=r(fut,"TFXLNetForSequenceClassification"),fut.forEach(t),Jfr=r(D8e," (XLNet model)"),D8e.forEach(t),W.forEach(t),Yfr=i(Fa),qpe=s(Fa,"P",{});var gut=n(qpe);Kfr=r(gut,"Examples:"),gut.forEach(t),Zfr=i(Fa),f(k0.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),tRe=i(c),Pc=s(c,"H2",{class:!0});var uPe=n(Pc);qM=s(uPe,"A",{id:!0,class:!0,href:!0});var hut=n(qM);Ope=s(hut,"SPAN",{});var uut=n(Ope);f(R0.$$.fragment,uut),uut.forEach(t),hut.forEach(t),egr=i(uPe),Gpe=s(uPe,"SPAN",{});var put=n(Gpe);ogr=r(put,"TFAutoModelForMultipleChoice"),put.forEach(t),uPe.forEach(t),aRe=i(c),wr=s(c,"DIV",{class:!0});var Zl=n(wr);f(S0.$$.fragment,Zl),rgr=i(Zl),$c=s(Zl,"P",{});var HW=n($c);tgr=r(HW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Xpe=s(HW,"CODE",{});var _ut=n(Xpe);agr=r(_ut,"from_pretrained()"),_ut.forEach(t),sgr=r(HW,"class method or the "),Vpe=s(HW,"CODE",{});var but=n(Vpe);ngr=r(but,"from_config()"),but.forEach(t),lgr=r(HW,`class
method.`),HW.forEach(t),igr=i(Zl),P0=s(Zl,"P",{});var pPe=n(P0);dgr=r(pPe,"This class cannot be instantiated directly using "),zpe=s(pPe,"CODE",{});var vut=n(zpe);cgr=r(vut,"__init__()"),vut.forEach(t),mgr=r(pPe," (throws an error)."),pPe.forEach(t),fgr=i(Zl),Tt=s(Zl,"DIV",{class:!0});var ei=n(Tt);f($0.$$.fragment,ei),ggr=i(ei),Wpe=s(ei,"P",{});var Tut=n(Wpe);hgr=r(Tut,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Tut.forEach(t),ugr=i(ei),Ic=s(ei,"P",{});var UW=n(Ic);pgr=r(UW,`Note:
Loading a model from its configuration file does `),Qpe=s(UW,"STRONG",{});var Fut=n(Qpe);_gr=r(Fut,"not"),Fut.forEach(t),bgr=r(UW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hpe=s(UW,"CODE",{});var Cut=n(Hpe);vgr=r(Cut,"from_pretrained()"),Cut.forEach(t),Tgr=r(UW,"to load the model weights."),UW.forEach(t),Fgr=i(ei),Upe=s(ei,"P",{});var Mut=n(Upe);Cgr=r(Mut,"Examples:"),Mut.forEach(t),Mgr=i(ei),f(I0.$$.fragment,ei),ei.forEach(t),Egr=i(Zl),Fo=s(Zl,"DIV",{class:!0});var Ca=n(Fo);f(j0.$$.fragment,Ca),ygr=i(Ca),Jpe=s(Ca,"P",{});var Eut=n(Jpe);wgr=r(Eut,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Eut.forEach(t),Agr=i(Ca),Cs=s(Ca,"P",{});var j5=n(Cs);Lgr=r(j5,"The model class to instantiate is selected based on the "),Ype=s(j5,"CODE",{});var yut=n(Ype);Bgr=r(yut,"model_type"),yut.forEach(t),xgr=r(j5,` property of the config object (either
passed as an argument or loaded from `),Kpe=s(j5,"CODE",{});var wut=n(Kpe);kgr=r(wut,"pretrained_model_name_or_path"),wut.forEach(t),Rgr=r(j5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zpe=s(j5,"CODE",{});var Aut=n(Zpe);Sgr=r(Aut,"pretrained_model_name_or_path"),Aut.forEach(t),Pgr=r(j5,":"),j5.forEach(t),$gr=i(Ca),ae=s(Ca,"UL",{});var le=n(ae);OM=s(le,"LI",{});var q8e=n(OM);e_e=s(q8e,"STRONG",{});var Lut=n(e_e);Igr=r(Lut,"albert"),Lut.forEach(t),jgr=r(q8e," \u2014 "),qG=s(q8e,"A",{href:!0});var But=n(qG);Ngr=r(But,"TFAlbertForMultipleChoice"),But.forEach(t),Dgr=r(q8e," (ALBERT model)"),q8e.forEach(t),qgr=i(le),GM=s(le,"LI",{});var O8e=n(GM);o_e=s(O8e,"STRONG",{});var xut=n(o_e);Ogr=r(xut,"bert"),xut.forEach(t),Ggr=r(O8e," \u2014 "),OG=s(O8e,"A",{href:!0});var kut=n(OG);Xgr=r(kut,"TFBertForMultipleChoice"),kut.forEach(t),Vgr=r(O8e," (BERT model)"),O8e.forEach(t),zgr=i(le),XM=s(le,"LI",{});var G8e=n(XM);r_e=s(G8e,"STRONG",{});var Rut=n(r_e);Wgr=r(Rut,"camembert"),Rut.forEach(t),Qgr=r(G8e," \u2014 "),GG=s(G8e,"A",{href:!0});var Sut=n(GG);Hgr=r(Sut,"TFCamembertForMultipleChoice"),Sut.forEach(t),Ugr=r(G8e," (CamemBERT model)"),G8e.forEach(t),Jgr=i(le),VM=s(le,"LI",{});var X8e=n(VM);t_e=s(X8e,"STRONG",{});var Put=n(t_e);Ygr=r(Put,"convbert"),Put.forEach(t),Kgr=r(X8e," \u2014 "),XG=s(X8e,"A",{href:!0});var $ut=n(XG);Zgr=r($ut,"TFConvBertForMultipleChoice"),$ut.forEach(t),ehr=r(X8e," (ConvBERT model)"),X8e.forEach(t),ohr=i(le),zM=s(le,"LI",{});var V8e=n(zM);a_e=s(V8e,"STRONG",{});var Iut=n(a_e);rhr=r(Iut,"distilbert"),Iut.forEach(t),thr=r(V8e," \u2014 "),VG=s(V8e,"A",{href:!0});var jut=n(VG);ahr=r(jut,"TFDistilBertForMultipleChoice"),jut.forEach(t),shr=r(V8e," (DistilBERT model)"),V8e.forEach(t),nhr=i(le),WM=s(le,"LI",{});var z8e=n(WM);s_e=s(z8e,"STRONG",{});var Nut=n(s_e);lhr=r(Nut,"electra"),Nut.forEach(t),ihr=r(z8e," \u2014 "),zG=s(z8e,"A",{href:!0});var Dut=n(zG);dhr=r(Dut,"TFElectraForMultipleChoice"),Dut.forEach(t),chr=r(z8e," (ELECTRA model)"),z8e.forEach(t),mhr=i(le),QM=s(le,"LI",{});var W8e=n(QM);n_e=s(W8e,"STRONG",{});var qut=n(n_e);fhr=r(qut,"flaubert"),qut.forEach(t),ghr=r(W8e," \u2014 "),WG=s(W8e,"A",{href:!0});var Out=n(WG);hhr=r(Out,"TFFlaubertForMultipleChoice"),Out.forEach(t),uhr=r(W8e," (FlauBERT model)"),W8e.forEach(t),phr=i(le),HM=s(le,"LI",{});var Q8e=n(HM);l_e=s(Q8e,"STRONG",{});var Gut=n(l_e);_hr=r(Gut,"funnel"),Gut.forEach(t),bhr=r(Q8e," \u2014 "),QG=s(Q8e,"A",{href:!0});var Xut=n(QG);vhr=r(Xut,"TFFunnelForMultipleChoice"),Xut.forEach(t),Thr=r(Q8e," (Funnel Transformer model)"),Q8e.forEach(t),Fhr=i(le),UM=s(le,"LI",{});var H8e=n(UM);i_e=s(H8e,"STRONG",{});var Vut=n(i_e);Chr=r(Vut,"longformer"),Vut.forEach(t),Mhr=r(H8e," \u2014 "),HG=s(H8e,"A",{href:!0});var zut=n(HG);Ehr=r(zut,"TFLongformerForMultipleChoice"),zut.forEach(t),yhr=r(H8e," (Longformer model)"),H8e.forEach(t),whr=i(le),JM=s(le,"LI",{});var U8e=n(JM);d_e=s(U8e,"STRONG",{});var Wut=n(d_e);Ahr=r(Wut,"mobilebert"),Wut.forEach(t),Lhr=r(U8e," \u2014 "),UG=s(U8e,"A",{href:!0});var Qut=n(UG);Bhr=r(Qut,"TFMobileBertForMultipleChoice"),Qut.forEach(t),xhr=r(U8e," (MobileBERT model)"),U8e.forEach(t),khr=i(le),YM=s(le,"LI",{});var J8e=n(YM);c_e=s(J8e,"STRONG",{});var Hut=n(c_e);Rhr=r(Hut,"mpnet"),Hut.forEach(t),Shr=r(J8e," \u2014 "),JG=s(J8e,"A",{href:!0});var Uut=n(JG);Phr=r(Uut,"TFMPNetForMultipleChoice"),Uut.forEach(t),$hr=r(J8e," (MPNet model)"),J8e.forEach(t),Ihr=i(le),KM=s(le,"LI",{});var Y8e=n(KM);m_e=s(Y8e,"STRONG",{});var Jut=n(m_e);jhr=r(Jut,"rembert"),Jut.forEach(t),Nhr=r(Y8e," \u2014 "),YG=s(Y8e,"A",{href:!0});var Yut=n(YG);Dhr=r(Yut,"TFRemBertForMultipleChoice"),Yut.forEach(t),qhr=r(Y8e," (RemBERT model)"),Y8e.forEach(t),Ohr=i(le),ZM=s(le,"LI",{});var K8e=n(ZM);f_e=s(K8e,"STRONG",{});var Kut=n(f_e);Ghr=r(Kut,"roberta"),Kut.forEach(t),Xhr=r(K8e," \u2014 "),KG=s(K8e,"A",{href:!0});var Zut=n(KG);Vhr=r(Zut,"TFRobertaForMultipleChoice"),Zut.forEach(t),zhr=r(K8e," (RoBERTa model)"),K8e.forEach(t),Whr=i(le),e4=s(le,"LI",{});var Z8e=n(e4);g_e=s(Z8e,"STRONG",{});var ept=n(g_e);Qhr=r(ept,"roformer"),ept.forEach(t),Hhr=r(Z8e," \u2014 "),ZG=s(Z8e,"A",{href:!0});var opt=n(ZG);Uhr=r(opt,"TFRoFormerForMultipleChoice"),opt.forEach(t),Jhr=r(Z8e," (RoFormer model)"),Z8e.forEach(t),Yhr=i(le),o4=s(le,"LI",{});var e9e=n(o4);h_e=s(e9e,"STRONG",{});var rpt=n(h_e);Khr=r(rpt,"xlm"),rpt.forEach(t),Zhr=r(e9e," \u2014 "),eX=s(e9e,"A",{href:!0});var tpt=n(eX);eur=r(tpt,"TFXLMForMultipleChoice"),tpt.forEach(t),our=r(e9e," (XLM model)"),e9e.forEach(t),rur=i(le),r4=s(le,"LI",{});var o9e=n(r4);u_e=s(o9e,"STRONG",{});var apt=n(u_e);tur=r(apt,"xlm-roberta"),apt.forEach(t),aur=r(o9e," \u2014 "),oX=s(o9e,"A",{href:!0});var spt=n(oX);sur=r(spt,"TFXLMRobertaForMultipleChoice"),spt.forEach(t),nur=r(o9e," (XLM-RoBERTa model)"),o9e.forEach(t),lur=i(le),t4=s(le,"LI",{});var r9e=n(t4);p_e=s(r9e,"STRONG",{});var npt=n(p_e);iur=r(npt,"xlnet"),npt.forEach(t),dur=r(r9e," \u2014 "),rX=s(r9e,"A",{href:!0});var lpt=n(rX);cur=r(lpt,"TFXLNetForMultipleChoice"),lpt.forEach(t),mur=r(r9e," (XLNet model)"),r9e.forEach(t),le.forEach(t),fur=i(Ca),__e=s(Ca,"P",{});var ipt=n(__e);gur=r(ipt,"Examples:"),ipt.forEach(t),hur=i(Ca),f(N0.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),sRe=i(c),jc=s(c,"H2",{class:!0});var _Pe=n(jc);a4=s(_Pe,"A",{id:!0,class:!0,href:!0});var dpt=n(a4);b_e=s(dpt,"SPAN",{});var cpt=n(b_e);f(D0.$$.fragment,cpt),cpt.forEach(t),dpt.forEach(t),uur=i(_Pe),v_e=s(_Pe,"SPAN",{});var mpt=n(v_e);pur=r(mpt,"TFAutoModelForTableQuestionAnswering"),mpt.forEach(t),_Pe.forEach(t),nRe=i(c),Ar=s(c,"DIV",{class:!0});var oi=n(Ar);f(q0.$$.fragment,oi),_ur=i(oi),Nc=s(oi,"P",{});var JW=n(Nc);bur=r(JW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),T_e=s(JW,"CODE",{});var fpt=n(T_e);vur=r(fpt,"from_pretrained()"),fpt.forEach(t),Tur=r(JW,"class method or the "),F_e=s(JW,"CODE",{});var gpt=n(F_e);Fur=r(gpt,"from_config()"),gpt.forEach(t),Cur=r(JW,`class
method.`),JW.forEach(t),Mur=i(oi),O0=s(oi,"P",{});var bPe=n(O0);Eur=r(bPe,"This class cannot be instantiated directly using "),C_e=s(bPe,"CODE",{});var hpt=n(C_e);yur=r(hpt,"__init__()"),hpt.forEach(t),wur=r(bPe," (throws an error)."),bPe.forEach(t),Aur=i(oi),Ft=s(oi,"DIV",{class:!0});var ri=n(Ft);f(G0.$$.fragment,ri),Lur=i(ri),M_e=s(ri,"P",{});var upt=n(M_e);Bur=r(upt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),upt.forEach(t),xur=i(ri),Dc=s(ri,"P",{});var YW=n(Dc);kur=r(YW,`Note:
Loading a model from its configuration file does `),E_e=s(YW,"STRONG",{});var ppt=n(E_e);Rur=r(ppt,"not"),ppt.forEach(t),Sur=r(YW,` load the model weights. It only affects the
model\u2019s configuration. Use `),y_e=s(YW,"CODE",{});var _pt=n(y_e);Pur=r(_pt,"from_pretrained()"),_pt.forEach(t),$ur=r(YW,"to load the model weights."),YW.forEach(t),Iur=i(ri),w_e=s(ri,"P",{});var bpt=n(w_e);jur=r(bpt,"Examples:"),bpt.forEach(t),Nur=i(ri),f(X0.$$.fragment,ri),ri.forEach(t),Dur=i(oi),Co=s(oi,"DIV",{class:!0});var Ma=n(Co);f(V0.$$.fragment,Ma),qur=i(Ma),A_e=s(Ma,"P",{});var vpt=n(A_e);Our=r(vpt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),vpt.forEach(t),Gur=i(Ma),Ms=s(Ma,"P",{});var N5=n(Ms);Xur=r(N5,"The model class to instantiate is selected based on the "),L_e=s(N5,"CODE",{});var Tpt=n(L_e);Vur=r(Tpt,"model_type"),Tpt.forEach(t),zur=r(N5,` property of the config object (either
passed as an argument or loaded from `),B_e=s(N5,"CODE",{});var Fpt=n(B_e);Wur=r(Fpt,"pretrained_model_name_or_path"),Fpt.forEach(t),Qur=r(N5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x_e=s(N5,"CODE",{});var Cpt=n(x_e);Hur=r(Cpt,"pretrained_model_name_or_path"),Cpt.forEach(t),Uur=r(N5,":"),N5.forEach(t),Jur=i(Ma),k_e=s(Ma,"UL",{});var Mpt=n(k_e);s4=s(Mpt,"LI",{});var t9e=n(s4);R_e=s(t9e,"STRONG",{});var Ept=n(R_e);Yur=r(Ept,"tapas"),Ept.forEach(t),Kur=r(t9e," \u2014 "),tX=s(t9e,"A",{href:!0});var ypt=n(tX);Zur=r(ypt,"TFTapasForQuestionAnswering"),ypt.forEach(t),epr=r(t9e," (TAPAS model)"),t9e.forEach(t),Mpt.forEach(t),opr=i(Ma),S_e=s(Ma,"P",{});var wpt=n(S_e);rpr=r(wpt,"Examples:"),wpt.forEach(t),tpr=i(Ma),f(z0.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),lRe=i(c),qc=s(c,"H2",{class:!0});var vPe=n(qc);n4=s(vPe,"A",{id:!0,class:!0,href:!0});var Apt=n(n4);P_e=s(Apt,"SPAN",{});var Lpt=n(P_e);f(W0.$$.fragment,Lpt),Lpt.forEach(t),Apt.forEach(t),apr=i(vPe),$_e=s(vPe,"SPAN",{});var Bpt=n($_e);spr=r(Bpt,"TFAutoModelForTokenClassification"),Bpt.forEach(t),vPe.forEach(t),iRe=i(c),Lr=s(c,"DIV",{class:!0});var ti=n(Lr);f(Q0.$$.fragment,ti),npr=i(ti),Oc=s(ti,"P",{});var KW=n(Oc);lpr=r(KW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),I_e=s(KW,"CODE",{});var xpt=n(I_e);ipr=r(xpt,"from_pretrained()"),xpt.forEach(t),dpr=r(KW,"class method or the "),j_e=s(KW,"CODE",{});var kpt=n(j_e);cpr=r(kpt,"from_config()"),kpt.forEach(t),mpr=r(KW,`class
method.`),KW.forEach(t),fpr=i(ti),H0=s(ti,"P",{});var TPe=n(H0);gpr=r(TPe,"This class cannot be instantiated directly using "),N_e=s(TPe,"CODE",{});var Rpt=n(N_e);hpr=r(Rpt,"__init__()"),Rpt.forEach(t),upr=r(TPe," (throws an error)."),TPe.forEach(t),ppr=i(ti),Ct=s(ti,"DIV",{class:!0});var ai=n(Ct);f(U0.$$.fragment,ai),_pr=i(ai),D_e=s(ai,"P",{});var Spt=n(D_e);bpr=r(Spt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Spt.forEach(t),vpr=i(ai),Gc=s(ai,"P",{});var ZW=n(Gc);Tpr=r(ZW,`Note:
Loading a model from its configuration file does `),q_e=s(ZW,"STRONG",{});var Ppt=n(q_e);Fpr=r(Ppt,"not"),Ppt.forEach(t),Cpr=r(ZW,` load the model weights. It only affects the
model\u2019s configuration. Use `),O_e=s(ZW,"CODE",{});var $pt=n(O_e);Mpr=r($pt,"from_pretrained()"),$pt.forEach(t),Epr=r(ZW,"to load the model weights."),ZW.forEach(t),ypr=i(ai),G_e=s(ai,"P",{});var Ipt=n(G_e);wpr=r(Ipt,"Examples:"),Ipt.forEach(t),Apr=i(ai),f(J0.$$.fragment,ai),ai.forEach(t),Lpr=i(ti),Mo=s(ti,"DIV",{class:!0});var Ea=n(Mo);f(Y0.$$.fragment,Ea),Bpr=i(Ea),X_e=s(Ea,"P",{});var jpt=n(X_e);xpr=r(jpt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),jpt.forEach(t),kpr=i(Ea),Es=s(Ea,"P",{});var D5=n(Es);Rpr=r(D5,"The model class to instantiate is selected based on the "),V_e=s(D5,"CODE",{});var Npt=n(V_e);Spr=r(Npt,"model_type"),Npt.forEach(t),Ppr=r(D5,` property of the config object (either
passed as an argument or loaded from `),z_e=s(D5,"CODE",{});var Dpt=n(z_e);$pr=r(Dpt,"pretrained_model_name_or_path"),Dpt.forEach(t),Ipr=r(D5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),W_e=s(D5,"CODE",{});var qpt=n(W_e);jpr=r(qpt,"pretrained_model_name_or_path"),qpt.forEach(t),Npr=r(D5,":"),D5.forEach(t),Dpr=i(Ea),K=s(Ea,"UL",{});var oe=n(K);l4=s(oe,"LI",{});var a9e=n(l4);Q_e=s(a9e,"STRONG",{});var Opt=n(Q_e);qpr=r(Opt,"albert"),Opt.forEach(t),Opr=r(a9e," \u2014 "),aX=s(a9e,"A",{href:!0});var Gpt=n(aX);Gpr=r(Gpt,"TFAlbertForTokenClassification"),Gpt.forEach(t),Xpr=r(a9e," (ALBERT model)"),a9e.forEach(t),Vpr=i(oe),i4=s(oe,"LI",{});var s9e=n(i4);H_e=s(s9e,"STRONG",{});var Xpt=n(H_e);zpr=r(Xpt,"bert"),Xpt.forEach(t),Wpr=r(s9e," \u2014 "),sX=s(s9e,"A",{href:!0});var Vpt=n(sX);Qpr=r(Vpt,"TFBertForTokenClassification"),Vpt.forEach(t),Hpr=r(s9e," (BERT model)"),s9e.forEach(t),Upr=i(oe),d4=s(oe,"LI",{});var n9e=n(d4);U_e=s(n9e,"STRONG",{});var zpt=n(U_e);Jpr=r(zpt,"camembert"),zpt.forEach(t),Ypr=r(n9e," \u2014 "),nX=s(n9e,"A",{href:!0});var Wpt=n(nX);Kpr=r(Wpt,"TFCamembertForTokenClassification"),Wpt.forEach(t),Zpr=r(n9e," (CamemBERT model)"),n9e.forEach(t),e_r=i(oe),c4=s(oe,"LI",{});var l9e=n(c4);J_e=s(l9e,"STRONG",{});var Qpt=n(J_e);o_r=r(Qpt,"convbert"),Qpt.forEach(t),r_r=r(l9e," \u2014 "),lX=s(l9e,"A",{href:!0});var Hpt=n(lX);t_r=r(Hpt,"TFConvBertForTokenClassification"),Hpt.forEach(t),a_r=r(l9e," (ConvBERT model)"),l9e.forEach(t),s_r=i(oe),m4=s(oe,"LI",{});var i9e=n(m4);Y_e=s(i9e,"STRONG",{});var Upt=n(Y_e);n_r=r(Upt,"deberta"),Upt.forEach(t),l_r=r(i9e," \u2014 "),iX=s(i9e,"A",{href:!0});var Jpt=n(iX);i_r=r(Jpt,"TFDebertaForTokenClassification"),Jpt.forEach(t),d_r=r(i9e," (DeBERTa model)"),i9e.forEach(t),c_r=i(oe),f4=s(oe,"LI",{});var d9e=n(f4);K_e=s(d9e,"STRONG",{});var Ypt=n(K_e);m_r=r(Ypt,"deberta-v2"),Ypt.forEach(t),f_r=r(d9e," \u2014 "),dX=s(d9e,"A",{href:!0});var Kpt=n(dX);g_r=r(Kpt,"TFDebertaV2ForTokenClassification"),Kpt.forEach(t),h_r=r(d9e," (DeBERTa-v2 model)"),d9e.forEach(t),u_r=i(oe),g4=s(oe,"LI",{});var c9e=n(g4);Z_e=s(c9e,"STRONG",{});var Zpt=n(Z_e);p_r=r(Zpt,"distilbert"),Zpt.forEach(t),__r=r(c9e," \u2014 "),cX=s(c9e,"A",{href:!0});var e_t=n(cX);b_r=r(e_t,"TFDistilBertForTokenClassification"),e_t.forEach(t),v_r=r(c9e," (DistilBERT model)"),c9e.forEach(t),T_r=i(oe),h4=s(oe,"LI",{});var m9e=n(h4);ebe=s(m9e,"STRONG",{});var o_t=n(ebe);F_r=r(o_t,"electra"),o_t.forEach(t),C_r=r(m9e," \u2014 "),mX=s(m9e,"A",{href:!0});var r_t=n(mX);M_r=r(r_t,"TFElectraForTokenClassification"),r_t.forEach(t),E_r=r(m9e," (ELECTRA model)"),m9e.forEach(t),y_r=i(oe),u4=s(oe,"LI",{});var f9e=n(u4);obe=s(f9e,"STRONG",{});var t_t=n(obe);w_r=r(t_t,"flaubert"),t_t.forEach(t),A_r=r(f9e," \u2014 "),fX=s(f9e,"A",{href:!0});var a_t=n(fX);L_r=r(a_t,"TFFlaubertForTokenClassification"),a_t.forEach(t),B_r=r(f9e," (FlauBERT model)"),f9e.forEach(t),x_r=i(oe),p4=s(oe,"LI",{});var g9e=n(p4);rbe=s(g9e,"STRONG",{});var s_t=n(rbe);k_r=r(s_t,"funnel"),s_t.forEach(t),R_r=r(g9e," \u2014 "),gX=s(g9e,"A",{href:!0});var n_t=n(gX);S_r=r(n_t,"TFFunnelForTokenClassification"),n_t.forEach(t),P_r=r(g9e," (Funnel Transformer model)"),g9e.forEach(t),$_r=i(oe),_4=s(oe,"LI",{});var h9e=n(_4);tbe=s(h9e,"STRONG",{});var l_t=n(tbe);I_r=r(l_t,"layoutlm"),l_t.forEach(t),j_r=r(h9e," \u2014 "),hX=s(h9e,"A",{href:!0});var i_t=n(hX);N_r=r(i_t,"TFLayoutLMForTokenClassification"),i_t.forEach(t),D_r=r(h9e," (LayoutLM model)"),h9e.forEach(t),q_r=i(oe),b4=s(oe,"LI",{});var u9e=n(b4);abe=s(u9e,"STRONG",{});var d_t=n(abe);O_r=r(d_t,"longformer"),d_t.forEach(t),G_r=r(u9e," \u2014 "),uX=s(u9e,"A",{href:!0});var c_t=n(uX);X_r=r(c_t,"TFLongformerForTokenClassification"),c_t.forEach(t),V_r=r(u9e," (Longformer model)"),u9e.forEach(t),z_r=i(oe),v4=s(oe,"LI",{});var p9e=n(v4);sbe=s(p9e,"STRONG",{});var m_t=n(sbe);W_r=r(m_t,"mobilebert"),m_t.forEach(t),Q_r=r(p9e," \u2014 "),pX=s(p9e,"A",{href:!0});var f_t=n(pX);H_r=r(f_t,"TFMobileBertForTokenClassification"),f_t.forEach(t),U_r=r(p9e," (MobileBERT model)"),p9e.forEach(t),J_r=i(oe),T4=s(oe,"LI",{});var _9e=n(T4);nbe=s(_9e,"STRONG",{});var g_t=n(nbe);Y_r=r(g_t,"mpnet"),g_t.forEach(t),K_r=r(_9e," \u2014 "),_X=s(_9e,"A",{href:!0});var h_t=n(_X);Z_r=r(h_t,"TFMPNetForTokenClassification"),h_t.forEach(t),ebr=r(_9e," (MPNet model)"),_9e.forEach(t),obr=i(oe),F4=s(oe,"LI",{});var b9e=n(F4);lbe=s(b9e,"STRONG",{});var u_t=n(lbe);rbr=r(u_t,"rembert"),u_t.forEach(t),tbr=r(b9e," \u2014 "),bX=s(b9e,"A",{href:!0});var p_t=n(bX);abr=r(p_t,"TFRemBertForTokenClassification"),p_t.forEach(t),sbr=r(b9e," (RemBERT model)"),b9e.forEach(t),nbr=i(oe),C4=s(oe,"LI",{});var v9e=n(C4);ibe=s(v9e,"STRONG",{});var __t=n(ibe);lbr=r(__t,"roberta"),__t.forEach(t),ibr=r(v9e," \u2014 "),vX=s(v9e,"A",{href:!0});var b_t=n(vX);dbr=r(b_t,"TFRobertaForTokenClassification"),b_t.forEach(t),cbr=r(v9e," (RoBERTa model)"),v9e.forEach(t),mbr=i(oe),M4=s(oe,"LI",{});var T9e=n(M4);dbe=s(T9e,"STRONG",{});var v_t=n(dbe);fbr=r(v_t,"roformer"),v_t.forEach(t),gbr=r(T9e," \u2014 "),TX=s(T9e,"A",{href:!0});var T_t=n(TX);hbr=r(T_t,"TFRoFormerForTokenClassification"),T_t.forEach(t),ubr=r(T9e," (RoFormer model)"),T9e.forEach(t),pbr=i(oe),E4=s(oe,"LI",{});var F9e=n(E4);cbe=s(F9e,"STRONG",{});var F_t=n(cbe);_br=r(F_t,"xlm"),F_t.forEach(t),bbr=r(F9e," \u2014 "),FX=s(F9e,"A",{href:!0});var C_t=n(FX);vbr=r(C_t,"TFXLMForTokenClassification"),C_t.forEach(t),Tbr=r(F9e," (XLM model)"),F9e.forEach(t),Fbr=i(oe),y4=s(oe,"LI",{});var C9e=n(y4);mbe=s(C9e,"STRONG",{});var M_t=n(mbe);Cbr=r(M_t,"xlm-roberta"),M_t.forEach(t),Mbr=r(C9e," \u2014 "),CX=s(C9e,"A",{href:!0});var E_t=n(CX);Ebr=r(E_t,"TFXLMRobertaForTokenClassification"),E_t.forEach(t),ybr=r(C9e," (XLM-RoBERTa model)"),C9e.forEach(t),wbr=i(oe),w4=s(oe,"LI",{});var M9e=n(w4);fbe=s(M9e,"STRONG",{});var y_t=n(fbe);Abr=r(y_t,"xlnet"),y_t.forEach(t),Lbr=r(M9e," \u2014 "),MX=s(M9e,"A",{href:!0});var w_t=n(MX);Bbr=r(w_t,"TFXLNetForTokenClassification"),w_t.forEach(t),xbr=r(M9e," (XLNet model)"),M9e.forEach(t),oe.forEach(t),kbr=i(Ea),gbe=s(Ea,"P",{});var A_t=n(gbe);Rbr=r(A_t,"Examples:"),A_t.forEach(t),Sbr=i(Ea),f(K0.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),dRe=i(c),Xc=s(c,"H2",{class:!0});var FPe=n(Xc);A4=s(FPe,"A",{id:!0,class:!0,href:!0});var L_t=n(A4);hbe=s(L_t,"SPAN",{});var B_t=n(hbe);f(Z0.$$.fragment,B_t),B_t.forEach(t),L_t.forEach(t),Pbr=i(FPe),ube=s(FPe,"SPAN",{});var x_t=n(ube);$br=r(x_t,"TFAutoModelForQuestionAnswering"),x_t.forEach(t),FPe.forEach(t),cRe=i(c),Br=s(c,"DIV",{class:!0});var si=n(Br);f(eL.$$.fragment,si),Ibr=i(si),Vc=s(si,"P",{});var eQ=n(Vc);jbr=r(eQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),pbe=s(eQ,"CODE",{});var k_t=n(pbe);Nbr=r(k_t,"from_pretrained()"),k_t.forEach(t),Dbr=r(eQ,"class method or the "),_be=s(eQ,"CODE",{});var R_t=n(_be);qbr=r(R_t,"from_config()"),R_t.forEach(t),Obr=r(eQ,`class
method.`),eQ.forEach(t),Gbr=i(si),oL=s(si,"P",{});var CPe=n(oL);Xbr=r(CPe,"This class cannot be instantiated directly using "),bbe=s(CPe,"CODE",{});var S_t=n(bbe);Vbr=r(S_t,"__init__()"),S_t.forEach(t),zbr=r(CPe," (throws an error)."),CPe.forEach(t),Wbr=i(si),Mt=s(si,"DIV",{class:!0});var ni=n(Mt);f(rL.$$.fragment,ni),Qbr=i(ni),vbe=s(ni,"P",{});var P_t=n(vbe);Hbr=r(P_t,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),P_t.forEach(t),Ubr=i(ni),zc=s(ni,"P",{});var oQ=n(zc);Jbr=r(oQ,`Note:
Loading a model from its configuration file does `),Tbe=s(oQ,"STRONG",{});var $_t=n(Tbe);Ybr=r($_t,"not"),$_t.forEach(t),Kbr=r(oQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fbe=s(oQ,"CODE",{});var I_t=n(Fbe);Zbr=r(I_t,"from_pretrained()"),I_t.forEach(t),e2r=r(oQ,"to load the model weights."),oQ.forEach(t),o2r=i(ni),Cbe=s(ni,"P",{});var j_t=n(Cbe);r2r=r(j_t,"Examples:"),j_t.forEach(t),t2r=i(ni),f(tL.$$.fragment,ni),ni.forEach(t),a2r=i(si),Eo=s(si,"DIV",{class:!0});var ya=n(Eo);f(aL.$$.fragment,ya),s2r=i(ya),Mbe=s(ya,"P",{});var N_t=n(Mbe);n2r=r(N_t,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),N_t.forEach(t),l2r=i(ya),ys=s(ya,"P",{});var q5=n(ys);i2r=r(q5,"The model class to instantiate is selected based on the "),Ebe=s(q5,"CODE",{});var D_t=n(Ebe);d2r=r(D_t,"model_type"),D_t.forEach(t),c2r=r(q5,` property of the config object (either
passed as an argument or loaded from `),ybe=s(q5,"CODE",{});var q_t=n(ybe);m2r=r(q_t,"pretrained_model_name_or_path"),q_t.forEach(t),f2r=r(q5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wbe=s(q5,"CODE",{});var O_t=n(wbe);g2r=r(O_t,"pretrained_model_name_or_path"),O_t.forEach(t),h2r=r(q5,":"),q5.forEach(t),u2r=i(ya),Z=s(ya,"UL",{});var te=n(Z);L4=s(te,"LI",{});var E9e=n(L4);Abe=s(E9e,"STRONG",{});var G_t=n(Abe);p2r=r(G_t,"albert"),G_t.forEach(t),_2r=r(E9e," \u2014 "),EX=s(E9e,"A",{href:!0});var X_t=n(EX);b2r=r(X_t,"TFAlbertForQuestionAnswering"),X_t.forEach(t),v2r=r(E9e," (ALBERT model)"),E9e.forEach(t),T2r=i(te),B4=s(te,"LI",{});var y9e=n(B4);Lbe=s(y9e,"STRONG",{});var V_t=n(Lbe);F2r=r(V_t,"bert"),V_t.forEach(t),C2r=r(y9e," \u2014 "),yX=s(y9e,"A",{href:!0});var z_t=n(yX);M2r=r(z_t,"TFBertForQuestionAnswering"),z_t.forEach(t),E2r=r(y9e," (BERT model)"),y9e.forEach(t),y2r=i(te),x4=s(te,"LI",{});var w9e=n(x4);Bbe=s(w9e,"STRONG",{});var W_t=n(Bbe);w2r=r(W_t,"camembert"),W_t.forEach(t),A2r=r(w9e," \u2014 "),wX=s(w9e,"A",{href:!0});var Q_t=n(wX);L2r=r(Q_t,"TFCamembertForQuestionAnswering"),Q_t.forEach(t),B2r=r(w9e," (CamemBERT model)"),w9e.forEach(t),x2r=i(te),k4=s(te,"LI",{});var A9e=n(k4);xbe=s(A9e,"STRONG",{});var H_t=n(xbe);k2r=r(H_t,"convbert"),H_t.forEach(t),R2r=r(A9e," \u2014 "),AX=s(A9e,"A",{href:!0});var U_t=n(AX);S2r=r(U_t,"TFConvBertForQuestionAnswering"),U_t.forEach(t),P2r=r(A9e," (ConvBERT model)"),A9e.forEach(t),$2r=i(te),R4=s(te,"LI",{});var L9e=n(R4);kbe=s(L9e,"STRONG",{});var J_t=n(kbe);I2r=r(J_t,"deberta"),J_t.forEach(t),j2r=r(L9e," \u2014 "),LX=s(L9e,"A",{href:!0});var Y_t=n(LX);N2r=r(Y_t,"TFDebertaForQuestionAnswering"),Y_t.forEach(t),D2r=r(L9e," (DeBERTa model)"),L9e.forEach(t),q2r=i(te),S4=s(te,"LI",{});var B9e=n(S4);Rbe=s(B9e,"STRONG",{});var K_t=n(Rbe);O2r=r(K_t,"deberta-v2"),K_t.forEach(t),G2r=r(B9e," \u2014 "),BX=s(B9e,"A",{href:!0});var Z_t=n(BX);X2r=r(Z_t,"TFDebertaV2ForQuestionAnswering"),Z_t.forEach(t),V2r=r(B9e," (DeBERTa-v2 model)"),B9e.forEach(t),z2r=i(te),P4=s(te,"LI",{});var x9e=n(P4);Sbe=s(x9e,"STRONG",{});var ebt=n(Sbe);W2r=r(ebt,"distilbert"),ebt.forEach(t),Q2r=r(x9e," \u2014 "),xX=s(x9e,"A",{href:!0});var obt=n(xX);H2r=r(obt,"TFDistilBertForQuestionAnswering"),obt.forEach(t),U2r=r(x9e," (DistilBERT model)"),x9e.forEach(t),J2r=i(te),$4=s(te,"LI",{});var k9e=n($4);Pbe=s(k9e,"STRONG",{});var rbt=n(Pbe);Y2r=r(rbt,"electra"),rbt.forEach(t),K2r=r(k9e," \u2014 "),kX=s(k9e,"A",{href:!0});var tbt=n(kX);Z2r=r(tbt,"TFElectraForQuestionAnswering"),tbt.forEach(t),evr=r(k9e," (ELECTRA model)"),k9e.forEach(t),ovr=i(te),I4=s(te,"LI",{});var R9e=n(I4);$be=s(R9e,"STRONG",{});var abt=n($be);rvr=r(abt,"flaubert"),abt.forEach(t),tvr=r(R9e," \u2014 "),RX=s(R9e,"A",{href:!0});var sbt=n(RX);avr=r(sbt,"TFFlaubertForQuestionAnsweringSimple"),sbt.forEach(t),svr=r(R9e," (FlauBERT model)"),R9e.forEach(t),nvr=i(te),j4=s(te,"LI",{});var S9e=n(j4);Ibe=s(S9e,"STRONG",{});var nbt=n(Ibe);lvr=r(nbt,"funnel"),nbt.forEach(t),ivr=r(S9e," \u2014 "),SX=s(S9e,"A",{href:!0});var lbt=n(SX);dvr=r(lbt,"TFFunnelForQuestionAnswering"),lbt.forEach(t),cvr=r(S9e," (Funnel Transformer model)"),S9e.forEach(t),mvr=i(te),N4=s(te,"LI",{});var P9e=n(N4);jbe=s(P9e,"STRONG",{});var ibt=n(jbe);fvr=r(ibt,"longformer"),ibt.forEach(t),gvr=r(P9e," \u2014 "),PX=s(P9e,"A",{href:!0});var dbt=n(PX);hvr=r(dbt,"TFLongformerForQuestionAnswering"),dbt.forEach(t),uvr=r(P9e," (Longformer model)"),P9e.forEach(t),pvr=i(te),D4=s(te,"LI",{});var $9e=n(D4);Nbe=s($9e,"STRONG",{});var cbt=n(Nbe);_vr=r(cbt,"mobilebert"),cbt.forEach(t),bvr=r($9e," \u2014 "),$X=s($9e,"A",{href:!0});var mbt=n($X);vvr=r(mbt,"TFMobileBertForQuestionAnswering"),mbt.forEach(t),Tvr=r($9e," (MobileBERT model)"),$9e.forEach(t),Fvr=i(te),q4=s(te,"LI",{});var I9e=n(q4);Dbe=s(I9e,"STRONG",{});var fbt=n(Dbe);Cvr=r(fbt,"mpnet"),fbt.forEach(t),Mvr=r(I9e," \u2014 "),IX=s(I9e,"A",{href:!0});var gbt=n(IX);Evr=r(gbt,"TFMPNetForQuestionAnswering"),gbt.forEach(t),yvr=r(I9e," (MPNet model)"),I9e.forEach(t),wvr=i(te),O4=s(te,"LI",{});var j9e=n(O4);qbe=s(j9e,"STRONG",{});var hbt=n(qbe);Avr=r(hbt,"rembert"),hbt.forEach(t),Lvr=r(j9e," \u2014 "),jX=s(j9e,"A",{href:!0});var ubt=n(jX);Bvr=r(ubt,"TFRemBertForQuestionAnswering"),ubt.forEach(t),xvr=r(j9e," (RemBERT model)"),j9e.forEach(t),kvr=i(te),G4=s(te,"LI",{});var N9e=n(G4);Obe=s(N9e,"STRONG",{});var pbt=n(Obe);Rvr=r(pbt,"roberta"),pbt.forEach(t),Svr=r(N9e," \u2014 "),NX=s(N9e,"A",{href:!0});var _bt=n(NX);Pvr=r(_bt,"TFRobertaForQuestionAnswering"),_bt.forEach(t),$vr=r(N9e," (RoBERTa model)"),N9e.forEach(t),Ivr=i(te),X4=s(te,"LI",{});var D9e=n(X4);Gbe=s(D9e,"STRONG",{});var bbt=n(Gbe);jvr=r(bbt,"roformer"),bbt.forEach(t),Nvr=r(D9e," \u2014 "),DX=s(D9e,"A",{href:!0});var vbt=n(DX);Dvr=r(vbt,"TFRoFormerForQuestionAnswering"),vbt.forEach(t),qvr=r(D9e," (RoFormer model)"),D9e.forEach(t),Ovr=i(te),V4=s(te,"LI",{});var q9e=n(V4);Xbe=s(q9e,"STRONG",{});var Tbt=n(Xbe);Gvr=r(Tbt,"xlm"),Tbt.forEach(t),Xvr=r(q9e," \u2014 "),qX=s(q9e,"A",{href:!0});var Fbt=n(qX);Vvr=r(Fbt,"TFXLMForQuestionAnsweringSimple"),Fbt.forEach(t),zvr=r(q9e," (XLM model)"),q9e.forEach(t),Wvr=i(te),z4=s(te,"LI",{});var O9e=n(z4);Vbe=s(O9e,"STRONG",{});var Cbt=n(Vbe);Qvr=r(Cbt,"xlm-roberta"),Cbt.forEach(t),Hvr=r(O9e," \u2014 "),OX=s(O9e,"A",{href:!0});var Mbt=n(OX);Uvr=r(Mbt,"TFXLMRobertaForQuestionAnswering"),Mbt.forEach(t),Jvr=r(O9e," (XLM-RoBERTa model)"),O9e.forEach(t),Yvr=i(te),W4=s(te,"LI",{});var G9e=n(W4);zbe=s(G9e,"STRONG",{});var Ebt=n(zbe);Kvr=r(Ebt,"xlnet"),Ebt.forEach(t),Zvr=r(G9e," \u2014 "),GX=s(G9e,"A",{href:!0});var ybt=n(GX);eTr=r(ybt,"TFXLNetForQuestionAnsweringSimple"),ybt.forEach(t),oTr=r(G9e," (XLNet model)"),G9e.forEach(t),te.forEach(t),rTr=i(ya),Wbe=s(ya,"P",{});var wbt=n(Wbe);tTr=r(wbt,"Examples:"),wbt.forEach(t),aTr=i(ya),f(sL.$$.fragment,ya),ya.forEach(t),si.forEach(t),mRe=i(c),Wc=s(c,"H2",{class:!0});var MPe=n(Wc);Q4=s(MPe,"A",{id:!0,class:!0,href:!0});var Abt=n(Q4);Qbe=s(Abt,"SPAN",{});var Lbt=n(Qbe);f(nL.$$.fragment,Lbt),Lbt.forEach(t),Abt.forEach(t),sTr=i(MPe),Hbe=s(MPe,"SPAN",{});var Bbt=n(Hbe);nTr=r(Bbt,"TFAutoModelForVision2Seq"),Bbt.forEach(t),MPe.forEach(t),fRe=i(c),xr=s(c,"DIV",{class:!0});var li=n(xr);f(lL.$$.fragment,li),lTr=i(li),Qc=s(li,"P",{});var rQ=n(Qc);iTr=r(rQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Ube=s(rQ,"CODE",{});var xbt=n(Ube);dTr=r(xbt,"from_pretrained()"),xbt.forEach(t),cTr=r(rQ,"class method or the "),Jbe=s(rQ,"CODE",{});var kbt=n(Jbe);mTr=r(kbt,"from_config()"),kbt.forEach(t),fTr=r(rQ,`class
method.`),rQ.forEach(t),gTr=i(li),iL=s(li,"P",{});var EPe=n(iL);hTr=r(EPe,"This class cannot be instantiated directly using "),Ybe=s(EPe,"CODE",{});var Rbt=n(Ybe);uTr=r(Rbt,"__init__()"),Rbt.forEach(t),pTr=r(EPe," (throws an error)."),EPe.forEach(t),_Tr=i(li),Et=s(li,"DIV",{class:!0});var ii=n(Et);f(dL.$$.fragment,ii),bTr=i(ii),Kbe=s(ii,"P",{});var Sbt=n(Kbe);vTr=r(Sbt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Sbt.forEach(t),TTr=i(ii),Hc=s(ii,"P",{});var tQ=n(Hc);FTr=r(tQ,`Note:
Loading a model from its configuration file does `),Zbe=s(tQ,"STRONG",{});var Pbt=n(Zbe);CTr=r(Pbt,"not"),Pbt.forEach(t),MTr=r(tQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),e2e=s(tQ,"CODE",{});var $bt=n(e2e);ETr=r($bt,"from_pretrained()"),$bt.forEach(t),yTr=r(tQ,"to load the model weights."),tQ.forEach(t),wTr=i(ii),o2e=s(ii,"P",{});var Ibt=n(o2e);ATr=r(Ibt,"Examples:"),Ibt.forEach(t),LTr=i(ii),f(cL.$$.fragment,ii),ii.forEach(t),BTr=i(li),yo=s(li,"DIV",{class:!0});var wa=n(yo);f(mL.$$.fragment,wa),xTr=i(wa),r2e=s(wa,"P",{});var jbt=n(r2e);kTr=r(jbt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),jbt.forEach(t),RTr=i(wa),ws=s(wa,"P",{});var O5=n(ws);STr=r(O5,"The model class to instantiate is selected based on the "),t2e=s(O5,"CODE",{});var Nbt=n(t2e);PTr=r(Nbt,"model_type"),Nbt.forEach(t),$Tr=r(O5,` property of the config object (either
passed as an argument or loaded from `),a2e=s(O5,"CODE",{});var Dbt=n(a2e);ITr=r(Dbt,"pretrained_model_name_or_path"),Dbt.forEach(t),jTr=r(O5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s2e=s(O5,"CODE",{});var qbt=n(s2e);NTr=r(qbt,"pretrained_model_name_or_path"),qbt.forEach(t),DTr=r(O5,":"),O5.forEach(t),qTr=i(wa),n2e=s(wa,"UL",{});var Obt=n(n2e);H4=s(Obt,"LI",{});var X9e=n(H4);l2e=s(X9e,"STRONG",{});var Gbt=n(l2e);OTr=r(Gbt,"vision-encoder-decoder"),Gbt.forEach(t),GTr=r(X9e," \u2014 "),XX=s(X9e,"A",{href:!0});var Xbt=n(XX);XTr=r(Xbt,"TFVisionEncoderDecoderModel"),Xbt.forEach(t),VTr=r(X9e," (Vision Encoder decoder model)"),X9e.forEach(t),Obt.forEach(t),zTr=i(wa),i2e=s(wa,"P",{});var Vbt=n(i2e);WTr=r(Vbt,"Examples:"),Vbt.forEach(t),QTr=i(wa),f(fL.$$.fragment,wa),wa.forEach(t),li.forEach(t),gRe=i(c),Uc=s(c,"H2",{class:!0});var yPe=n(Uc);U4=s(yPe,"A",{id:!0,class:!0,href:!0});var zbt=n(U4);d2e=s(zbt,"SPAN",{});var Wbt=n(d2e);f(gL.$$.fragment,Wbt),Wbt.forEach(t),zbt.forEach(t),HTr=i(yPe),c2e=s(yPe,"SPAN",{});var Qbt=n(c2e);UTr=r(Qbt,"TFAutoModelForSpeechSeq2Seq"),Qbt.forEach(t),yPe.forEach(t),hRe=i(c),kr=s(c,"DIV",{class:!0});var di=n(kr);f(hL.$$.fragment,di),JTr=i(di),Jc=s(di,"P",{});var aQ=n(Jc);YTr=r(aQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),m2e=s(aQ,"CODE",{});var Hbt=n(m2e);KTr=r(Hbt,"from_pretrained()"),Hbt.forEach(t),ZTr=r(aQ,"class method or the "),f2e=s(aQ,"CODE",{});var Ubt=n(f2e);e1r=r(Ubt,"from_config()"),Ubt.forEach(t),o1r=r(aQ,`class
method.`),aQ.forEach(t),r1r=i(di),uL=s(di,"P",{});var wPe=n(uL);t1r=r(wPe,"This class cannot be instantiated directly using "),g2e=s(wPe,"CODE",{});var Jbt=n(g2e);a1r=r(Jbt,"__init__()"),Jbt.forEach(t),s1r=r(wPe," (throws an error)."),wPe.forEach(t),n1r=i(di),yt=s(di,"DIV",{class:!0});var ci=n(yt);f(pL.$$.fragment,ci),l1r=i(ci),h2e=s(ci,"P",{});var Ybt=n(h2e);i1r=r(Ybt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Ybt.forEach(t),d1r=i(ci),Yc=s(ci,"P",{});var sQ=n(Yc);c1r=r(sQ,`Note:
Loading a model from its configuration file does `),u2e=s(sQ,"STRONG",{});var Kbt=n(u2e);m1r=r(Kbt,"not"),Kbt.forEach(t),f1r=r(sQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),p2e=s(sQ,"CODE",{});var Zbt=n(p2e);g1r=r(Zbt,"from_pretrained()"),Zbt.forEach(t),h1r=r(sQ,"to load the model weights."),sQ.forEach(t),u1r=i(ci),_2e=s(ci,"P",{});var e2t=n(_2e);p1r=r(e2t,"Examples:"),e2t.forEach(t),_1r=i(ci),f(_L.$$.fragment,ci),ci.forEach(t),b1r=i(di),wo=s(di,"DIV",{class:!0});var Aa=n(wo);f(bL.$$.fragment,Aa),v1r=i(Aa),b2e=s(Aa,"P",{});var o2t=n(b2e);T1r=r(o2t,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),o2t.forEach(t),F1r=i(Aa),As=s(Aa,"P",{});var G5=n(As);C1r=r(G5,"The model class to instantiate is selected based on the "),v2e=s(G5,"CODE",{});var r2t=n(v2e);M1r=r(r2t,"model_type"),r2t.forEach(t),E1r=r(G5,` property of the config object (either
passed as an argument or loaded from `),T2e=s(G5,"CODE",{});var t2t=n(T2e);y1r=r(t2t,"pretrained_model_name_or_path"),t2t.forEach(t),w1r=r(G5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),F2e=s(G5,"CODE",{});var a2t=n(F2e);A1r=r(a2t,"pretrained_model_name_or_path"),a2t.forEach(t),L1r=r(G5,":"),G5.forEach(t),B1r=i(Aa),C2e=s(Aa,"UL",{});var s2t=n(C2e);J4=s(s2t,"LI",{});var V9e=n(J4);M2e=s(V9e,"STRONG",{});var n2t=n(M2e);x1r=r(n2t,"speech_to_text"),n2t.forEach(t),k1r=r(V9e," \u2014 "),VX=s(V9e,"A",{href:!0});var l2t=n(VX);R1r=r(l2t,"TFSpeech2TextForConditionalGeneration"),l2t.forEach(t),S1r=r(V9e," (Speech2Text model)"),V9e.forEach(t),s2t.forEach(t),P1r=i(Aa),E2e=s(Aa,"P",{});var i2t=n(E2e);$1r=r(i2t,"Examples:"),i2t.forEach(t),I1r=i(Aa),f(vL.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),uRe=i(c),Kc=s(c,"H2",{class:!0});var APe=n(Kc);Y4=s(APe,"A",{id:!0,class:!0,href:!0});var d2t=n(Y4);y2e=s(d2t,"SPAN",{});var c2t=n(y2e);f(TL.$$.fragment,c2t),c2t.forEach(t),d2t.forEach(t),j1r=i(APe),w2e=s(APe,"SPAN",{});var m2t=n(w2e);N1r=r(m2t,"FlaxAutoModel"),m2t.forEach(t),APe.forEach(t),pRe=i(c),Rr=s(c,"DIV",{class:!0});var mi=n(Rr);f(FL.$$.fragment,mi),D1r=i(mi),Zc=s(mi,"P",{});var nQ=n(Zc);q1r=r(nQ,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),A2e=s(nQ,"CODE",{});var f2t=n(A2e);O1r=r(f2t,"from_pretrained()"),f2t.forEach(t),G1r=r(nQ,"class method or the "),L2e=s(nQ,"CODE",{});var g2t=n(L2e);X1r=r(g2t,"from_config()"),g2t.forEach(t),V1r=r(nQ,`class
method.`),nQ.forEach(t),z1r=i(mi),CL=s(mi,"P",{});var LPe=n(CL);W1r=r(LPe,"This class cannot be instantiated directly using "),B2e=s(LPe,"CODE",{});var h2t=n(B2e);Q1r=r(h2t,"__init__()"),h2t.forEach(t),H1r=r(LPe," (throws an error)."),LPe.forEach(t),U1r=i(mi),wt=s(mi,"DIV",{class:!0});var fi=n(wt);f(ML.$$.fragment,fi),J1r=i(fi),x2e=s(fi,"P",{});var u2t=n(x2e);Y1r=r(u2t,"Instantiates one of the base model classes of the library from a configuration."),u2t.forEach(t),K1r=i(fi),em=s(fi,"P",{});var lQ=n(em);Z1r=r(lQ,`Note:
Loading a model from its configuration file does `),k2e=s(lQ,"STRONG",{});var p2t=n(k2e);eFr=r(p2t,"not"),p2t.forEach(t),oFr=r(lQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),R2e=s(lQ,"CODE",{});var _2t=n(R2e);rFr=r(_2t,"from_pretrained()"),_2t.forEach(t),tFr=r(lQ,"to load the model weights."),lQ.forEach(t),aFr=i(fi),S2e=s(fi,"P",{});var b2t=n(S2e);sFr=r(b2t,"Examples:"),b2t.forEach(t),nFr=i(fi),f(EL.$$.fragment,fi),fi.forEach(t),lFr=i(mi),Ao=s(mi,"DIV",{class:!0});var La=n(Ao);f(yL.$$.fragment,La),iFr=i(La),P2e=s(La,"P",{});var v2t=n(P2e);dFr=r(v2t,"Instantiate one of the base model classes of the library from a pretrained model."),v2t.forEach(t),cFr=i(La),Ls=s(La,"P",{});var X5=n(Ls);mFr=r(X5,"The model class to instantiate is selected based on the "),$2e=s(X5,"CODE",{});var T2t=n($2e);fFr=r(T2t,"model_type"),T2t.forEach(t),gFr=r(X5,` property of the config object (either
passed as an argument or loaded from `),I2e=s(X5,"CODE",{});var F2t=n(I2e);hFr=r(F2t,"pretrained_model_name_or_path"),F2t.forEach(t),uFr=r(X5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j2e=s(X5,"CODE",{});var C2t=n(j2e);pFr=r(C2t,"pretrained_model_name_or_path"),C2t.forEach(t),_Fr=r(X5,":"),X5.forEach(t),bFr=i(La),z=s(La,"UL",{});var Q=n(z);K4=s(Q,"LI",{});var z9e=n(K4);N2e=s(z9e,"STRONG",{});var M2t=n(N2e);vFr=r(M2t,"albert"),M2t.forEach(t),TFr=r(z9e," \u2014 "),zX=s(z9e,"A",{href:!0});var E2t=n(zX);FFr=r(E2t,"FlaxAlbertModel"),E2t.forEach(t),CFr=r(z9e," (ALBERT model)"),z9e.forEach(t),MFr=i(Q),Z4=s(Q,"LI",{});var W9e=n(Z4);D2e=s(W9e,"STRONG",{});var y2t=n(D2e);EFr=r(y2t,"bart"),y2t.forEach(t),yFr=r(W9e," \u2014 "),WX=s(W9e,"A",{href:!0});var w2t=n(WX);wFr=r(w2t,"FlaxBartModel"),w2t.forEach(t),AFr=r(W9e," (BART model)"),W9e.forEach(t),LFr=i(Q),eE=s(Q,"LI",{});var Q9e=n(eE);q2e=s(Q9e,"STRONG",{});var A2t=n(q2e);BFr=r(A2t,"beit"),A2t.forEach(t),xFr=r(Q9e," \u2014 "),QX=s(Q9e,"A",{href:!0});var L2t=n(QX);kFr=r(L2t,"FlaxBeitModel"),L2t.forEach(t),RFr=r(Q9e," (BEiT model)"),Q9e.forEach(t),SFr=i(Q),oE=s(Q,"LI",{});var H9e=n(oE);O2e=s(H9e,"STRONG",{});var B2t=n(O2e);PFr=r(B2t,"bert"),B2t.forEach(t),$Fr=r(H9e," \u2014 "),HX=s(H9e,"A",{href:!0});var x2t=n(HX);IFr=r(x2t,"FlaxBertModel"),x2t.forEach(t),jFr=r(H9e," (BERT model)"),H9e.forEach(t),NFr=i(Q),rE=s(Q,"LI",{});var U9e=n(rE);G2e=s(U9e,"STRONG",{});var k2t=n(G2e);DFr=r(k2t,"big_bird"),k2t.forEach(t),qFr=r(U9e," \u2014 "),UX=s(U9e,"A",{href:!0});var R2t=n(UX);OFr=r(R2t,"FlaxBigBirdModel"),R2t.forEach(t),GFr=r(U9e," (BigBird model)"),U9e.forEach(t),XFr=i(Q),tE=s(Q,"LI",{});var J9e=n(tE);X2e=s(J9e,"STRONG",{});var S2t=n(X2e);VFr=r(S2t,"blenderbot"),S2t.forEach(t),zFr=r(J9e," \u2014 "),JX=s(J9e,"A",{href:!0});var P2t=n(JX);WFr=r(P2t,"FlaxBlenderbotModel"),P2t.forEach(t),QFr=r(J9e," (Blenderbot model)"),J9e.forEach(t),HFr=i(Q),aE=s(Q,"LI",{});var Y9e=n(aE);V2e=s(Y9e,"STRONG",{});var $2t=n(V2e);UFr=r($2t,"blenderbot-small"),$2t.forEach(t),JFr=r(Y9e," \u2014 "),YX=s(Y9e,"A",{href:!0});var I2t=n(YX);YFr=r(I2t,"FlaxBlenderbotSmallModel"),I2t.forEach(t),KFr=r(Y9e," (BlenderbotSmall model)"),Y9e.forEach(t),ZFr=i(Q),sE=s(Q,"LI",{});var K9e=n(sE);z2e=s(K9e,"STRONG",{});var j2t=n(z2e);eCr=r(j2t,"clip"),j2t.forEach(t),oCr=r(K9e," \u2014 "),KX=s(K9e,"A",{href:!0});var N2t=n(KX);rCr=r(N2t,"FlaxCLIPModel"),N2t.forEach(t),tCr=r(K9e," (CLIP model)"),K9e.forEach(t),aCr=i(Q),nE=s(Q,"LI",{});var Z9e=n(nE);W2e=s(Z9e,"STRONG",{});var D2t=n(W2e);sCr=r(D2t,"distilbert"),D2t.forEach(t),nCr=r(Z9e," \u2014 "),ZX=s(Z9e,"A",{href:!0});var q2t=n(ZX);lCr=r(q2t,"FlaxDistilBertModel"),q2t.forEach(t),iCr=r(Z9e," (DistilBERT model)"),Z9e.forEach(t),dCr=i(Q),lE=s(Q,"LI",{});var eBe=n(lE);Q2e=s(eBe,"STRONG",{});var O2t=n(Q2e);cCr=r(O2t,"electra"),O2t.forEach(t),mCr=r(eBe," \u2014 "),eV=s(eBe,"A",{href:!0});var G2t=n(eV);fCr=r(G2t,"FlaxElectraModel"),G2t.forEach(t),gCr=r(eBe," (ELECTRA model)"),eBe.forEach(t),hCr=i(Q),iE=s(Q,"LI",{});var oBe=n(iE);H2e=s(oBe,"STRONG",{});var X2t=n(H2e);uCr=r(X2t,"gpt2"),X2t.forEach(t),pCr=r(oBe," \u2014 "),oV=s(oBe,"A",{href:!0});var V2t=n(oV);_Cr=r(V2t,"FlaxGPT2Model"),V2t.forEach(t),bCr=r(oBe," (OpenAI GPT-2 model)"),oBe.forEach(t),vCr=i(Q),dE=s(Q,"LI",{});var rBe=n(dE);U2e=s(rBe,"STRONG",{});var z2t=n(U2e);TCr=r(z2t,"gpt_neo"),z2t.forEach(t),FCr=r(rBe," \u2014 "),rV=s(rBe,"A",{href:!0});var W2t=n(rV);CCr=r(W2t,"FlaxGPTNeoModel"),W2t.forEach(t),MCr=r(rBe," (GPT Neo model)"),rBe.forEach(t),ECr=i(Q),cE=s(Q,"LI",{});var tBe=n(cE);J2e=s(tBe,"STRONG",{});var Q2t=n(J2e);yCr=r(Q2t,"gptj"),Q2t.forEach(t),wCr=r(tBe," \u2014 "),tV=s(tBe,"A",{href:!0});var H2t=n(tV);ACr=r(H2t,"FlaxGPTJModel"),H2t.forEach(t),LCr=r(tBe," (GPT-J model)"),tBe.forEach(t),BCr=i(Q),mE=s(Q,"LI",{});var aBe=n(mE);Y2e=s(aBe,"STRONG",{});var U2t=n(Y2e);xCr=r(U2t,"marian"),U2t.forEach(t),kCr=r(aBe," \u2014 "),aV=s(aBe,"A",{href:!0});var J2t=n(aV);RCr=r(J2t,"FlaxMarianModel"),J2t.forEach(t),SCr=r(aBe," (Marian model)"),aBe.forEach(t),PCr=i(Q),fE=s(Q,"LI",{});var sBe=n(fE);K2e=s(sBe,"STRONG",{});var Y2t=n(K2e);$Cr=r(Y2t,"mbart"),Y2t.forEach(t),ICr=r(sBe," \u2014 "),sV=s(sBe,"A",{href:!0});var K2t=n(sV);jCr=r(K2t,"FlaxMBartModel"),K2t.forEach(t),NCr=r(sBe," (mBART model)"),sBe.forEach(t),DCr=i(Q),gE=s(Q,"LI",{});var nBe=n(gE);Z2e=s(nBe,"STRONG",{});var Z2t=n(Z2e);qCr=r(Z2t,"mt5"),Z2t.forEach(t),OCr=r(nBe," \u2014 "),nV=s(nBe,"A",{href:!0});var evt=n(nV);GCr=r(evt,"FlaxMT5Model"),evt.forEach(t),XCr=r(nBe," (mT5 model)"),nBe.forEach(t),VCr=i(Q),hE=s(Q,"LI",{});var lBe=n(hE);eve=s(lBe,"STRONG",{});var ovt=n(eve);zCr=r(ovt,"pegasus"),ovt.forEach(t),WCr=r(lBe," \u2014 "),lV=s(lBe,"A",{href:!0});var rvt=n(lV);QCr=r(rvt,"FlaxPegasusModel"),rvt.forEach(t),HCr=r(lBe," (Pegasus model)"),lBe.forEach(t),UCr=i(Q),uE=s(Q,"LI",{});var iBe=n(uE);ove=s(iBe,"STRONG",{});var tvt=n(ove);JCr=r(tvt,"roberta"),tvt.forEach(t),YCr=r(iBe," \u2014 "),iV=s(iBe,"A",{href:!0});var avt=n(iV);KCr=r(avt,"FlaxRobertaModel"),avt.forEach(t),ZCr=r(iBe," (RoBERTa model)"),iBe.forEach(t),eMr=i(Q),pE=s(Q,"LI",{});var dBe=n(pE);rve=s(dBe,"STRONG",{});var svt=n(rve);oMr=r(svt,"roformer"),svt.forEach(t),rMr=r(dBe," \u2014 "),dV=s(dBe,"A",{href:!0});var nvt=n(dV);tMr=r(nvt,"FlaxRoFormerModel"),nvt.forEach(t),aMr=r(dBe," (RoFormer model)"),dBe.forEach(t),sMr=i(Q),_E=s(Q,"LI",{});var cBe=n(_E);tve=s(cBe,"STRONG",{});var lvt=n(tve);nMr=r(lvt,"t5"),lvt.forEach(t),lMr=r(cBe," \u2014 "),cV=s(cBe,"A",{href:!0});var ivt=n(cV);iMr=r(ivt,"FlaxT5Model"),ivt.forEach(t),dMr=r(cBe," (T5 model)"),cBe.forEach(t),cMr=i(Q),bE=s(Q,"LI",{});var mBe=n(bE);ave=s(mBe,"STRONG",{});var dvt=n(ave);mMr=r(dvt,"vision-text-dual-encoder"),dvt.forEach(t),fMr=r(mBe," \u2014 "),mV=s(mBe,"A",{href:!0});var cvt=n(mV);gMr=r(cvt,"FlaxVisionTextDualEncoderModel"),cvt.forEach(t),hMr=r(mBe," (VisionTextDualEncoder model)"),mBe.forEach(t),uMr=i(Q),vE=s(Q,"LI",{});var fBe=n(vE);sve=s(fBe,"STRONG",{});var mvt=n(sve);pMr=r(mvt,"vit"),mvt.forEach(t),_Mr=r(fBe," \u2014 "),fV=s(fBe,"A",{href:!0});var fvt=n(fV);bMr=r(fvt,"FlaxViTModel"),fvt.forEach(t),vMr=r(fBe," (ViT model)"),fBe.forEach(t),TMr=i(Q),TE=s(Q,"LI",{});var gBe=n(TE);nve=s(gBe,"STRONG",{});var gvt=n(nve);FMr=r(gvt,"wav2vec2"),gvt.forEach(t),CMr=r(gBe," \u2014 "),gV=s(gBe,"A",{href:!0});var hvt=n(gV);MMr=r(hvt,"FlaxWav2Vec2Model"),hvt.forEach(t),EMr=r(gBe," (Wav2Vec2 model)"),gBe.forEach(t),yMr=i(Q),FE=s(Q,"LI",{});var hBe=n(FE);lve=s(hBe,"STRONG",{});var uvt=n(lve);wMr=r(uvt,"xglm"),uvt.forEach(t),AMr=r(hBe," \u2014 "),hV=s(hBe,"A",{href:!0});var pvt=n(hV);LMr=r(pvt,"FlaxXGLMModel"),pvt.forEach(t),BMr=r(hBe," (XGLM model)"),hBe.forEach(t),xMr=i(Q),CE=s(Q,"LI",{});var uBe=n(CE);ive=s(uBe,"STRONG",{});var _vt=n(ive);kMr=r(_vt,"xlm-roberta"),_vt.forEach(t),RMr=r(uBe," \u2014 "),uV=s(uBe,"A",{href:!0});var bvt=n(uV);SMr=r(bvt,"FlaxXLMRobertaModel"),bvt.forEach(t),PMr=r(uBe," (XLM-RoBERTa model)"),uBe.forEach(t),Q.forEach(t),$Mr=i(La),dve=s(La,"P",{});var vvt=n(dve);IMr=r(vvt,"Examples:"),vvt.forEach(t),jMr=i(La),f(wL.$$.fragment,La),La.forEach(t),mi.forEach(t),_Re=i(c),om=s(c,"H2",{class:!0});var BPe=n(om);ME=s(BPe,"A",{id:!0,class:!0,href:!0});var Tvt=n(ME);cve=s(Tvt,"SPAN",{});var Fvt=n(cve);f(AL.$$.fragment,Fvt),Fvt.forEach(t),Tvt.forEach(t),NMr=i(BPe),mve=s(BPe,"SPAN",{});var Cvt=n(mve);DMr=r(Cvt,"FlaxAutoModelForCausalLM"),Cvt.forEach(t),BPe.forEach(t),bRe=i(c),Sr=s(c,"DIV",{class:!0});var gi=n(Sr);f(LL.$$.fragment,gi),qMr=i(gi),rm=s(gi,"P",{});var iQ=n(rm);OMr=r(iQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),fve=s(iQ,"CODE",{});var Mvt=n(fve);GMr=r(Mvt,"from_pretrained()"),Mvt.forEach(t),XMr=r(iQ,"class method or the "),gve=s(iQ,"CODE",{});var Evt=n(gve);VMr=r(Evt,"from_config()"),Evt.forEach(t),zMr=r(iQ,`class
method.`),iQ.forEach(t),WMr=i(gi),BL=s(gi,"P",{});var xPe=n(BL);QMr=r(xPe,"This class cannot be instantiated directly using "),hve=s(xPe,"CODE",{});var yvt=n(hve);HMr=r(yvt,"__init__()"),yvt.forEach(t),UMr=r(xPe," (throws an error)."),xPe.forEach(t),JMr=i(gi),At=s(gi,"DIV",{class:!0});var hi=n(At);f(xL.$$.fragment,hi),YMr=i(hi),uve=s(hi,"P",{});var wvt=n(uve);KMr=r(wvt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),wvt.forEach(t),ZMr=i(hi),tm=s(hi,"P",{});var dQ=n(tm);e4r=r(dQ,`Note:
Loading a model from its configuration file does `),pve=s(dQ,"STRONG",{});var Avt=n(pve);o4r=r(Avt,"not"),Avt.forEach(t),r4r=r(dQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ve=s(dQ,"CODE",{});var Lvt=n(_ve);t4r=r(Lvt,"from_pretrained()"),Lvt.forEach(t),a4r=r(dQ,"to load the model weights."),dQ.forEach(t),s4r=i(hi),bve=s(hi,"P",{});var Bvt=n(bve);n4r=r(Bvt,"Examples:"),Bvt.forEach(t),l4r=i(hi),f(kL.$$.fragment,hi),hi.forEach(t),i4r=i(gi),Lo=s(gi,"DIV",{class:!0});var Ba=n(Lo);f(RL.$$.fragment,Ba),d4r=i(Ba),vve=s(Ba,"P",{});var xvt=n(vve);c4r=r(xvt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),xvt.forEach(t),m4r=i(Ba),Bs=s(Ba,"P",{});var V5=n(Bs);f4r=r(V5,"The model class to instantiate is selected based on the "),Tve=s(V5,"CODE",{});var kvt=n(Tve);g4r=r(kvt,"model_type"),kvt.forEach(t),h4r=r(V5,` property of the config object (either
passed as an argument or loaded from `),Fve=s(V5,"CODE",{});var Rvt=n(Fve);u4r=r(Rvt,"pretrained_model_name_or_path"),Rvt.forEach(t),p4r=r(V5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cve=s(V5,"CODE",{});var Svt=n(Cve);_4r=r(Svt,"pretrained_model_name_or_path"),Svt.forEach(t),b4r=r(V5,":"),V5.forEach(t),v4r=i(Ba),ca=s(Ba,"UL",{});var ui=n(ca);EE=s(ui,"LI",{});var pBe=n(EE);Mve=s(pBe,"STRONG",{});var Pvt=n(Mve);T4r=r(Pvt,"bart"),Pvt.forEach(t),F4r=r(pBe," \u2014 "),pV=s(pBe,"A",{href:!0});var $vt=n(pV);C4r=r($vt,"FlaxBartForCausalLM"),$vt.forEach(t),M4r=r(pBe," (BART model)"),pBe.forEach(t),E4r=i(ui),yE=s(ui,"LI",{});var _Be=n(yE);Eve=s(_Be,"STRONG",{});var Ivt=n(Eve);y4r=r(Ivt,"gpt2"),Ivt.forEach(t),w4r=r(_Be," \u2014 "),_V=s(_Be,"A",{href:!0});var jvt=n(_V);A4r=r(jvt,"FlaxGPT2LMHeadModel"),jvt.forEach(t),L4r=r(_Be," (OpenAI GPT-2 model)"),_Be.forEach(t),B4r=i(ui),wE=s(ui,"LI",{});var bBe=n(wE);yve=s(bBe,"STRONG",{});var Nvt=n(yve);x4r=r(Nvt,"gpt_neo"),Nvt.forEach(t),k4r=r(bBe," \u2014 "),bV=s(bBe,"A",{href:!0});var Dvt=n(bV);R4r=r(Dvt,"FlaxGPTNeoForCausalLM"),Dvt.forEach(t),S4r=r(bBe," (GPT Neo model)"),bBe.forEach(t),P4r=i(ui),AE=s(ui,"LI",{});var vBe=n(AE);wve=s(vBe,"STRONG",{});var qvt=n(wve);$4r=r(qvt,"gptj"),qvt.forEach(t),I4r=r(vBe," \u2014 "),vV=s(vBe,"A",{href:!0});var Ovt=n(vV);j4r=r(Ovt,"FlaxGPTJForCausalLM"),Ovt.forEach(t),N4r=r(vBe," (GPT-J model)"),vBe.forEach(t),D4r=i(ui),LE=s(ui,"LI",{});var TBe=n(LE);Ave=s(TBe,"STRONG",{});var Gvt=n(Ave);q4r=r(Gvt,"xglm"),Gvt.forEach(t),O4r=r(TBe," \u2014 "),TV=s(TBe,"A",{href:!0});var Xvt=n(TV);G4r=r(Xvt,"FlaxXGLMForCausalLM"),Xvt.forEach(t),X4r=r(TBe," (XGLM model)"),TBe.forEach(t),ui.forEach(t),V4r=i(Ba),Lve=s(Ba,"P",{});var Vvt=n(Lve);z4r=r(Vvt,"Examples:"),Vvt.forEach(t),W4r=i(Ba),f(SL.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),vRe=i(c),am=s(c,"H2",{class:!0});var kPe=n(am);BE=s(kPe,"A",{id:!0,class:!0,href:!0});var zvt=n(BE);Bve=s(zvt,"SPAN",{});var Wvt=n(Bve);f(PL.$$.fragment,Wvt),Wvt.forEach(t),zvt.forEach(t),Q4r=i(kPe),xve=s(kPe,"SPAN",{});var Qvt=n(xve);H4r=r(Qvt,"FlaxAutoModelForPreTraining"),Qvt.forEach(t),kPe.forEach(t),TRe=i(c),Pr=s(c,"DIV",{class:!0});var pi=n(Pr);f($L.$$.fragment,pi),U4r=i(pi),sm=s(pi,"P",{});var cQ=n(sm);J4r=r(cQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),kve=s(cQ,"CODE",{});var Hvt=n(kve);Y4r=r(Hvt,"from_pretrained()"),Hvt.forEach(t),K4r=r(cQ,"class method or the "),Rve=s(cQ,"CODE",{});var Uvt=n(Rve);Z4r=r(Uvt,"from_config()"),Uvt.forEach(t),eEr=r(cQ,`class
method.`),cQ.forEach(t),oEr=i(pi),IL=s(pi,"P",{});var RPe=n(IL);rEr=r(RPe,"This class cannot be instantiated directly using "),Sve=s(RPe,"CODE",{});var Jvt=n(Sve);tEr=r(Jvt,"__init__()"),Jvt.forEach(t),aEr=r(RPe," (throws an error)."),RPe.forEach(t),sEr=i(pi),Lt=s(pi,"DIV",{class:!0});var _i=n(Lt);f(jL.$$.fragment,_i),nEr=i(_i),Pve=s(_i,"P",{});var Yvt=n(Pve);lEr=r(Yvt,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Yvt.forEach(t),iEr=i(_i),nm=s(_i,"P",{});var mQ=n(nm);dEr=r(mQ,`Note:
Loading a model from its configuration file does `),$ve=s(mQ,"STRONG",{});var Kvt=n($ve);cEr=r(Kvt,"not"),Kvt.forEach(t),mEr=r(mQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ive=s(mQ,"CODE",{});var Zvt=n(Ive);fEr=r(Zvt,"from_pretrained()"),Zvt.forEach(t),gEr=r(mQ,"to load the model weights."),mQ.forEach(t),hEr=i(_i),jve=s(_i,"P",{});var eTt=n(jve);uEr=r(eTt,"Examples:"),eTt.forEach(t),pEr=i(_i),f(NL.$$.fragment,_i),_i.forEach(t),_Er=i(pi),Bo=s(pi,"DIV",{class:!0});var xa=n(Bo);f(DL.$$.fragment,xa),bEr=i(xa),Nve=s(xa,"P",{});var oTt=n(Nve);vEr=r(oTt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),oTt.forEach(t),TEr=i(xa),xs=s(xa,"P",{});var z5=n(xs);FEr=r(z5,"The model class to instantiate is selected based on the "),Dve=s(z5,"CODE",{});var rTt=n(Dve);CEr=r(rTt,"model_type"),rTt.forEach(t),MEr=r(z5,` property of the config object (either
passed as an argument or loaded from `),qve=s(z5,"CODE",{});var tTt=n(qve);EEr=r(tTt,"pretrained_model_name_or_path"),tTt.forEach(t),yEr=r(z5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ove=s(z5,"CODE",{});var aTt=n(Ove);wEr=r(aTt,"pretrained_model_name_or_path"),aTt.forEach(t),AEr=r(z5,":"),z5.forEach(t),LEr=i(xa),ce=s(xa,"UL",{});var he=n(ce);xE=s(he,"LI",{});var FBe=n(xE);Gve=s(FBe,"STRONG",{});var sTt=n(Gve);BEr=r(sTt,"albert"),sTt.forEach(t),xEr=r(FBe," \u2014 "),FV=s(FBe,"A",{href:!0});var nTt=n(FV);kEr=r(nTt,"FlaxAlbertForPreTraining"),nTt.forEach(t),REr=r(FBe," (ALBERT model)"),FBe.forEach(t),SEr=i(he),kE=s(he,"LI",{});var CBe=n(kE);Xve=s(CBe,"STRONG",{});var lTt=n(Xve);PEr=r(lTt,"bart"),lTt.forEach(t),$Er=r(CBe," \u2014 "),CV=s(CBe,"A",{href:!0});var iTt=n(CV);IEr=r(iTt,"FlaxBartForConditionalGeneration"),iTt.forEach(t),jEr=r(CBe," (BART model)"),CBe.forEach(t),NEr=i(he),RE=s(he,"LI",{});var MBe=n(RE);Vve=s(MBe,"STRONG",{});var dTt=n(Vve);DEr=r(dTt,"bert"),dTt.forEach(t),qEr=r(MBe," \u2014 "),MV=s(MBe,"A",{href:!0});var cTt=n(MV);OEr=r(cTt,"FlaxBertForPreTraining"),cTt.forEach(t),GEr=r(MBe," (BERT model)"),MBe.forEach(t),XEr=i(he),SE=s(he,"LI",{});var EBe=n(SE);zve=s(EBe,"STRONG",{});var mTt=n(zve);VEr=r(mTt,"big_bird"),mTt.forEach(t),zEr=r(EBe," \u2014 "),EV=s(EBe,"A",{href:!0});var fTt=n(EV);WEr=r(fTt,"FlaxBigBirdForPreTraining"),fTt.forEach(t),QEr=r(EBe," (BigBird model)"),EBe.forEach(t),HEr=i(he),PE=s(he,"LI",{});var yBe=n(PE);Wve=s(yBe,"STRONG",{});var gTt=n(Wve);UEr=r(gTt,"electra"),gTt.forEach(t),JEr=r(yBe," \u2014 "),yV=s(yBe,"A",{href:!0});var hTt=n(yV);YEr=r(hTt,"FlaxElectraForPreTraining"),hTt.forEach(t),KEr=r(yBe," (ELECTRA model)"),yBe.forEach(t),ZEr=i(he),$E=s(he,"LI",{});var wBe=n($E);Qve=s(wBe,"STRONG",{});var uTt=n(Qve);e3r=r(uTt,"mbart"),uTt.forEach(t),o3r=r(wBe," \u2014 "),wV=s(wBe,"A",{href:!0});var pTt=n(wV);r3r=r(pTt,"FlaxMBartForConditionalGeneration"),pTt.forEach(t),t3r=r(wBe," (mBART model)"),wBe.forEach(t),a3r=i(he),IE=s(he,"LI",{});var ABe=n(IE);Hve=s(ABe,"STRONG",{});var _Tt=n(Hve);s3r=r(_Tt,"mt5"),_Tt.forEach(t),n3r=r(ABe," \u2014 "),AV=s(ABe,"A",{href:!0});var bTt=n(AV);l3r=r(bTt,"FlaxMT5ForConditionalGeneration"),bTt.forEach(t),i3r=r(ABe," (mT5 model)"),ABe.forEach(t),d3r=i(he),jE=s(he,"LI",{});var LBe=n(jE);Uve=s(LBe,"STRONG",{});var vTt=n(Uve);c3r=r(vTt,"roberta"),vTt.forEach(t),m3r=r(LBe," \u2014 "),LV=s(LBe,"A",{href:!0});var TTt=n(LV);f3r=r(TTt,"FlaxRobertaForMaskedLM"),TTt.forEach(t),g3r=r(LBe," (RoBERTa model)"),LBe.forEach(t),h3r=i(he),NE=s(he,"LI",{});var BBe=n(NE);Jve=s(BBe,"STRONG",{});var FTt=n(Jve);u3r=r(FTt,"roformer"),FTt.forEach(t),p3r=r(BBe," \u2014 "),BV=s(BBe,"A",{href:!0});var CTt=n(BV);_3r=r(CTt,"FlaxRoFormerForMaskedLM"),CTt.forEach(t),b3r=r(BBe," (RoFormer model)"),BBe.forEach(t),v3r=i(he),DE=s(he,"LI",{});var xBe=n(DE);Yve=s(xBe,"STRONG",{});var MTt=n(Yve);T3r=r(MTt,"t5"),MTt.forEach(t),F3r=r(xBe," \u2014 "),xV=s(xBe,"A",{href:!0});var ETt=n(xV);C3r=r(ETt,"FlaxT5ForConditionalGeneration"),ETt.forEach(t),M3r=r(xBe," (T5 model)"),xBe.forEach(t),E3r=i(he),qE=s(he,"LI",{});var kBe=n(qE);Kve=s(kBe,"STRONG",{});var yTt=n(Kve);y3r=r(yTt,"wav2vec2"),yTt.forEach(t),w3r=r(kBe," \u2014 "),kV=s(kBe,"A",{href:!0});var wTt=n(kV);A3r=r(wTt,"FlaxWav2Vec2ForPreTraining"),wTt.forEach(t),L3r=r(kBe," (Wav2Vec2 model)"),kBe.forEach(t),B3r=i(he),OE=s(he,"LI",{});var RBe=n(OE);Zve=s(RBe,"STRONG",{});var ATt=n(Zve);x3r=r(ATt,"xlm-roberta"),ATt.forEach(t),k3r=r(RBe," \u2014 "),RV=s(RBe,"A",{href:!0});var LTt=n(RV);R3r=r(LTt,"FlaxXLMRobertaForMaskedLM"),LTt.forEach(t),S3r=r(RBe," (XLM-RoBERTa model)"),RBe.forEach(t),he.forEach(t),P3r=i(xa),eTe=s(xa,"P",{});var BTt=n(eTe);$3r=r(BTt,"Examples:"),BTt.forEach(t),I3r=i(xa),f(qL.$$.fragment,xa),xa.forEach(t),pi.forEach(t),FRe=i(c),lm=s(c,"H2",{class:!0});var SPe=n(lm);GE=s(SPe,"A",{id:!0,class:!0,href:!0});var xTt=n(GE);oTe=s(xTt,"SPAN",{});var kTt=n(oTe);f(OL.$$.fragment,kTt),kTt.forEach(t),xTt.forEach(t),j3r=i(SPe),rTe=s(SPe,"SPAN",{});var RTt=n(rTe);N3r=r(RTt,"FlaxAutoModelForMaskedLM"),RTt.forEach(t),SPe.forEach(t),CRe=i(c),$r=s(c,"DIV",{class:!0});var bi=n($r);f(GL.$$.fragment,bi),D3r=i(bi),im=s(bi,"P",{});var fQ=n(im);q3r=r(fQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),tTe=s(fQ,"CODE",{});var STt=n(tTe);O3r=r(STt,"from_pretrained()"),STt.forEach(t),G3r=r(fQ,"class method or the "),aTe=s(fQ,"CODE",{});var PTt=n(aTe);X3r=r(PTt,"from_config()"),PTt.forEach(t),V3r=r(fQ,`class
method.`),fQ.forEach(t),z3r=i(bi),XL=s(bi,"P",{});var PPe=n(XL);W3r=r(PPe,"This class cannot be instantiated directly using "),sTe=s(PPe,"CODE",{});var $Tt=n(sTe);Q3r=r($Tt,"__init__()"),$Tt.forEach(t),H3r=r(PPe," (throws an error)."),PPe.forEach(t),U3r=i(bi),Bt=s(bi,"DIV",{class:!0});var vi=n(Bt);f(VL.$$.fragment,vi),J3r=i(vi),nTe=s(vi,"P",{});var ITt=n(nTe);Y3r=r(ITt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),ITt.forEach(t),K3r=i(vi),dm=s(vi,"P",{});var gQ=n(dm);Z3r=r(gQ,`Note:
Loading a model from its configuration file does `),lTe=s(gQ,"STRONG",{});var jTt=n(lTe);e5r=r(jTt,"not"),jTt.forEach(t),o5r=r(gQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),iTe=s(gQ,"CODE",{});var NTt=n(iTe);r5r=r(NTt,"from_pretrained()"),NTt.forEach(t),t5r=r(gQ,"to load the model weights."),gQ.forEach(t),a5r=i(vi),dTe=s(vi,"P",{});var DTt=n(dTe);s5r=r(DTt,"Examples:"),DTt.forEach(t),n5r=i(vi),f(zL.$$.fragment,vi),vi.forEach(t),l5r=i(bi),xo=s(bi,"DIV",{class:!0});var ka=n(xo);f(WL.$$.fragment,ka),i5r=i(ka),cTe=s(ka,"P",{});var qTt=n(cTe);d5r=r(qTt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),qTt.forEach(t),c5r=i(ka),ks=s(ka,"P",{});var W5=n(ks);m5r=r(W5,"The model class to instantiate is selected based on the "),mTe=s(W5,"CODE",{});var OTt=n(mTe);f5r=r(OTt,"model_type"),OTt.forEach(t),g5r=r(W5,` property of the config object (either
passed as an argument or loaded from `),fTe=s(W5,"CODE",{});var GTt=n(fTe);h5r=r(GTt,"pretrained_model_name_or_path"),GTt.forEach(t),u5r=r(W5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gTe=s(W5,"CODE",{});var XTt=n(gTe);p5r=r(XTt,"pretrained_model_name_or_path"),XTt.forEach(t),_5r=r(W5,":"),W5.forEach(t),b5r=i(ka),be=s(ka,"UL",{});var we=n(be);XE=s(we,"LI",{});var SBe=n(XE);hTe=s(SBe,"STRONG",{});var VTt=n(hTe);v5r=r(VTt,"albert"),VTt.forEach(t),T5r=r(SBe," \u2014 "),SV=s(SBe,"A",{href:!0});var zTt=n(SV);F5r=r(zTt,"FlaxAlbertForMaskedLM"),zTt.forEach(t),C5r=r(SBe," (ALBERT model)"),SBe.forEach(t),M5r=i(we),VE=s(we,"LI",{});var PBe=n(VE);uTe=s(PBe,"STRONG",{});var WTt=n(uTe);E5r=r(WTt,"bart"),WTt.forEach(t),y5r=r(PBe," \u2014 "),PV=s(PBe,"A",{href:!0});var QTt=n(PV);w5r=r(QTt,"FlaxBartForConditionalGeneration"),QTt.forEach(t),A5r=r(PBe," (BART model)"),PBe.forEach(t),L5r=i(we),zE=s(we,"LI",{});var $Be=n(zE);pTe=s($Be,"STRONG",{});var HTt=n(pTe);B5r=r(HTt,"bert"),HTt.forEach(t),x5r=r($Be," \u2014 "),$V=s($Be,"A",{href:!0});var UTt=n($V);k5r=r(UTt,"FlaxBertForMaskedLM"),UTt.forEach(t),R5r=r($Be," (BERT model)"),$Be.forEach(t),S5r=i(we),WE=s(we,"LI",{});var IBe=n(WE);_Te=s(IBe,"STRONG",{});var JTt=n(_Te);P5r=r(JTt,"big_bird"),JTt.forEach(t),$5r=r(IBe," \u2014 "),IV=s(IBe,"A",{href:!0});var YTt=n(IV);I5r=r(YTt,"FlaxBigBirdForMaskedLM"),YTt.forEach(t),j5r=r(IBe," (BigBird model)"),IBe.forEach(t),N5r=i(we),QE=s(we,"LI",{});var jBe=n(QE);bTe=s(jBe,"STRONG",{});var KTt=n(bTe);D5r=r(KTt,"distilbert"),KTt.forEach(t),q5r=r(jBe," \u2014 "),jV=s(jBe,"A",{href:!0});var ZTt=n(jV);O5r=r(ZTt,"FlaxDistilBertForMaskedLM"),ZTt.forEach(t),G5r=r(jBe," (DistilBERT model)"),jBe.forEach(t),X5r=i(we),HE=s(we,"LI",{});var NBe=n(HE);vTe=s(NBe,"STRONG",{});var e1t=n(vTe);V5r=r(e1t,"electra"),e1t.forEach(t),z5r=r(NBe," \u2014 "),NV=s(NBe,"A",{href:!0});var o1t=n(NV);W5r=r(o1t,"FlaxElectraForMaskedLM"),o1t.forEach(t),Q5r=r(NBe," (ELECTRA model)"),NBe.forEach(t),H5r=i(we),UE=s(we,"LI",{});var DBe=n(UE);TTe=s(DBe,"STRONG",{});var r1t=n(TTe);U5r=r(r1t,"mbart"),r1t.forEach(t),J5r=r(DBe," \u2014 "),DV=s(DBe,"A",{href:!0});var t1t=n(DV);Y5r=r(t1t,"FlaxMBartForConditionalGeneration"),t1t.forEach(t),K5r=r(DBe," (mBART model)"),DBe.forEach(t),Z5r=i(we),JE=s(we,"LI",{});var qBe=n(JE);FTe=s(qBe,"STRONG",{});var a1t=n(FTe);eyr=r(a1t,"roberta"),a1t.forEach(t),oyr=r(qBe," \u2014 "),qV=s(qBe,"A",{href:!0});var s1t=n(qV);ryr=r(s1t,"FlaxRobertaForMaskedLM"),s1t.forEach(t),tyr=r(qBe," (RoBERTa model)"),qBe.forEach(t),ayr=i(we),YE=s(we,"LI",{});var OBe=n(YE);CTe=s(OBe,"STRONG",{});var n1t=n(CTe);syr=r(n1t,"roformer"),n1t.forEach(t),nyr=r(OBe," \u2014 "),OV=s(OBe,"A",{href:!0});var l1t=n(OV);lyr=r(l1t,"FlaxRoFormerForMaskedLM"),l1t.forEach(t),iyr=r(OBe," (RoFormer model)"),OBe.forEach(t),dyr=i(we),KE=s(we,"LI",{});var GBe=n(KE);MTe=s(GBe,"STRONG",{});var i1t=n(MTe);cyr=r(i1t,"xlm-roberta"),i1t.forEach(t),myr=r(GBe," \u2014 "),GV=s(GBe,"A",{href:!0});var d1t=n(GV);fyr=r(d1t,"FlaxXLMRobertaForMaskedLM"),d1t.forEach(t),gyr=r(GBe," (XLM-RoBERTa model)"),GBe.forEach(t),we.forEach(t),hyr=i(ka),ETe=s(ka,"P",{});var c1t=n(ETe);uyr=r(c1t,"Examples:"),c1t.forEach(t),pyr=i(ka),f(QL.$$.fragment,ka),ka.forEach(t),bi.forEach(t),MRe=i(c),cm=s(c,"H2",{class:!0});var $Pe=n(cm);ZE=s($Pe,"A",{id:!0,class:!0,href:!0});var m1t=n(ZE);yTe=s(m1t,"SPAN",{});var f1t=n(yTe);f(HL.$$.fragment,f1t),f1t.forEach(t),m1t.forEach(t),_yr=i($Pe),wTe=s($Pe,"SPAN",{});var g1t=n(wTe);byr=r(g1t,"FlaxAutoModelForSeq2SeqLM"),g1t.forEach(t),$Pe.forEach(t),ERe=i(c),Ir=s(c,"DIV",{class:!0});var Ti=n(Ir);f(UL.$$.fragment,Ti),vyr=i(Ti),mm=s(Ti,"P",{});var hQ=n(mm);Tyr=r(hQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ATe=s(hQ,"CODE",{});var h1t=n(ATe);Fyr=r(h1t,"from_pretrained()"),h1t.forEach(t),Cyr=r(hQ,"class method or the "),LTe=s(hQ,"CODE",{});var u1t=n(LTe);Myr=r(u1t,"from_config()"),u1t.forEach(t),Eyr=r(hQ,`class
method.`),hQ.forEach(t),yyr=i(Ti),JL=s(Ti,"P",{});var IPe=n(JL);wyr=r(IPe,"This class cannot be instantiated directly using "),BTe=s(IPe,"CODE",{});var p1t=n(BTe);Ayr=r(p1t,"__init__()"),p1t.forEach(t),Lyr=r(IPe," (throws an error)."),IPe.forEach(t),Byr=i(Ti),xt=s(Ti,"DIV",{class:!0});var Fi=n(xt);f(YL.$$.fragment,Fi),xyr=i(Fi),xTe=s(Fi,"P",{});var _1t=n(xTe);kyr=r(_1t,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),_1t.forEach(t),Ryr=i(Fi),fm=s(Fi,"P",{});var uQ=n(fm);Syr=r(uQ,`Note:
Loading a model from its configuration file does `),kTe=s(uQ,"STRONG",{});var b1t=n(kTe);Pyr=r(b1t,"not"),b1t.forEach(t),$yr=r(uQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),RTe=s(uQ,"CODE",{});var v1t=n(RTe);Iyr=r(v1t,"from_pretrained()"),v1t.forEach(t),jyr=r(uQ,"to load the model weights."),uQ.forEach(t),Nyr=i(Fi),STe=s(Fi,"P",{});var T1t=n(STe);Dyr=r(T1t,"Examples:"),T1t.forEach(t),qyr=i(Fi),f(KL.$$.fragment,Fi),Fi.forEach(t),Oyr=i(Ti),ko=s(Ti,"DIV",{class:!0});var Ra=n(ko);f(ZL.$$.fragment,Ra),Gyr=i(Ra),PTe=s(Ra,"P",{});var F1t=n(PTe);Xyr=r(F1t,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),F1t.forEach(t),Vyr=i(Ra),Rs=s(Ra,"P",{});var Q5=n(Rs);zyr=r(Q5,"The model class to instantiate is selected based on the "),$Te=s(Q5,"CODE",{});var C1t=n($Te);Wyr=r(C1t,"model_type"),C1t.forEach(t),Qyr=r(Q5,` property of the config object (either
passed as an argument or loaded from `),ITe=s(Q5,"CODE",{});var M1t=n(ITe);Hyr=r(M1t,"pretrained_model_name_or_path"),M1t.forEach(t),Uyr=r(Q5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),jTe=s(Q5,"CODE",{});var E1t=n(jTe);Jyr=r(E1t,"pretrained_model_name_or_path"),E1t.forEach(t),Yyr=r(Q5,":"),Q5.forEach(t),Kyr=i(Ra),Ee=s(Ra,"UL",{});var lo=n(Ee);e3=s(lo,"LI",{});var XBe=n(e3);NTe=s(XBe,"STRONG",{});var y1t=n(NTe);Zyr=r(y1t,"bart"),y1t.forEach(t),ewr=r(XBe," \u2014 "),XV=s(XBe,"A",{href:!0});var w1t=n(XV);owr=r(w1t,"FlaxBartForConditionalGeneration"),w1t.forEach(t),rwr=r(XBe," (BART model)"),XBe.forEach(t),twr=i(lo),o3=s(lo,"LI",{});var VBe=n(o3);DTe=s(VBe,"STRONG",{});var A1t=n(DTe);awr=r(A1t,"blenderbot"),A1t.forEach(t),swr=r(VBe," \u2014 "),VV=s(VBe,"A",{href:!0});var L1t=n(VV);nwr=r(L1t,"FlaxBlenderbotForConditionalGeneration"),L1t.forEach(t),lwr=r(VBe," (Blenderbot model)"),VBe.forEach(t),iwr=i(lo),r3=s(lo,"LI",{});var zBe=n(r3);qTe=s(zBe,"STRONG",{});var B1t=n(qTe);dwr=r(B1t,"blenderbot-small"),B1t.forEach(t),cwr=r(zBe," \u2014 "),zV=s(zBe,"A",{href:!0});var x1t=n(zV);mwr=r(x1t,"FlaxBlenderbotSmallForConditionalGeneration"),x1t.forEach(t),fwr=r(zBe," (BlenderbotSmall model)"),zBe.forEach(t),gwr=i(lo),t3=s(lo,"LI",{});var WBe=n(t3);OTe=s(WBe,"STRONG",{});var k1t=n(OTe);hwr=r(k1t,"encoder-decoder"),k1t.forEach(t),uwr=r(WBe," \u2014 "),WV=s(WBe,"A",{href:!0});var R1t=n(WV);pwr=r(R1t,"FlaxEncoderDecoderModel"),R1t.forEach(t),_wr=r(WBe," (Encoder decoder model)"),WBe.forEach(t),bwr=i(lo),a3=s(lo,"LI",{});var QBe=n(a3);GTe=s(QBe,"STRONG",{});var S1t=n(GTe);vwr=r(S1t,"marian"),S1t.forEach(t),Twr=r(QBe," \u2014 "),QV=s(QBe,"A",{href:!0});var P1t=n(QV);Fwr=r(P1t,"FlaxMarianMTModel"),P1t.forEach(t),Cwr=r(QBe," (Marian model)"),QBe.forEach(t),Mwr=i(lo),s3=s(lo,"LI",{});var HBe=n(s3);XTe=s(HBe,"STRONG",{});var $1t=n(XTe);Ewr=r($1t,"mbart"),$1t.forEach(t),ywr=r(HBe," \u2014 "),HV=s(HBe,"A",{href:!0});var I1t=n(HV);wwr=r(I1t,"FlaxMBartForConditionalGeneration"),I1t.forEach(t),Awr=r(HBe," (mBART model)"),HBe.forEach(t),Lwr=i(lo),n3=s(lo,"LI",{});var UBe=n(n3);VTe=s(UBe,"STRONG",{});var j1t=n(VTe);Bwr=r(j1t,"mt5"),j1t.forEach(t),xwr=r(UBe," \u2014 "),UV=s(UBe,"A",{href:!0});var N1t=n(UV);kwr=r(N1t,"FlaxMT5ForConditionalGeneration"),N1t.forEach(t),Rwr=r(UBe," (mT5 model)"),UBe.forEach(t),Swr=i(lo),l3=s(lo,"LI",{});var JBe=n(l3);zTe=s(JBe,"STRONG",{});var D1t=n(zTe);Pwr=r(D1t,"pegasus"),D1t.forEach(t),$wr=r(JBe," \u2014 "),JV=s(JBe,"A",{href:!0});var q1t=n(JV);Iwr=r(q1t,"FlaxPegasusForConditionalGeneration"),q1t.forEach(t),jwr=r(JBe," (Pegasus model)"),JBe.forEach(t),Nwr=i(lo),i3=s(lo,"LI",{});var YBe=n(i3);WTe=s(YBe,"STRONG",{});var O1t=n(WTe);Dwr=r(O1t,"t5"),O1t.forEach(t),qwr=r(YBe," \u2014 "),YV=s(YBe,"A",{href:!0});var G1t=n(YV);Owr=r(G1t,"FlaxT5ForConditionalGeneration"),G1t.forEach(t),Gwr=r(YBe," (T5 model)"),YBe.forEach(t),lo.forEach(t),Xwr=i(Ra),QTe=s(Ra,"P",{});var X1t=n(QTe);Vwr=r(X1t,"Examples:"),X1t.forEach(t),zwr=i(Ra),f(e7.$$.fragment,Ra),Ra.forEach(t),Ti.forEach(t),yRe=i(c),gm=s(c,"H2",{class:!0});var jPe=n(gm);d3=s(jPe,"A",{id:!0,class:!0,href:!0});var V1t=n(d3);HTe=s(V1t,"SPAN",{});var z1t=n(HTe);f(o7.$$.fragment,z1t),z1t.forEach(t),V1t.forEach(t),Wwr=i(jPe),UTe=s(jPe,"SPAN",{});var W1t=n(UTe);Qwr=r(W1t,"FlaxAutoModelForSequenceClassification"),W1t.forEach(t),jPe.forEach(t),wRe=i(c),jr=s(c,"DIV",{class:!0});var Ci=n(jr);f(r7.$$.fragment,Ci),Hwr=i(Ci),hm=s(Ci,"P",{});var pQ=n(hm);Uwr=r(pQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),JTe=s(pQ,"CODE",{});var Q1t=n(JTe);Jwr=r(Q1t,"from_pretrained()"),Q1t.forEach(t),Ywr=r(pQ,"class method or the "),YTe=s(pQ,"CODE",{});var H1t=n(YTe);Kwr=r(H1t,"from_config()"),H1t.forEach(t),Zwr=r(pQ,`class
method.`),pQ.forEach(t),e6r=i(Ci),t7=s(Ci,"P",{});var NPe=n(t7);o6r=r(NPe,"This class cannot be instantiated directly using "),KTe=s(NPe,"CODE",{});var U1t=n(KTe);r6r=r(U1t,"__init__()"),U1t.forEach(t),t6r=r(NPe," (throws an error)."),NPe.forEach(t),a6r=i(Ci),kt=s(Ci,"DIV",{class:!0});var Mi=n(kt);f(a7.$$.fragment,Mi),s6r=i(Mi),ZTe=s(Mi,"P",{});var J1t=n(ZTe);n6r=r(J1t,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),J1t.forEach(t),l6r=i(Mi),um=s(Mi,"P",{});var _Q=n(um);i6r=r(_Q,`Note:
Loading a model from its configuration file does `),e1e=s(_Q,"STRONG",{});var Y1t=n(e1e);d6r=r(Y1t,"not"),Y1t.forEach(t),c6r=r(_Q,` load the model weights. It only affects the
model\u2019s configuration. Use `),o1e=s(_Q,"CODE",{});var K1t=n(o1e);m6r=r(K1t,"from_pretrained()"),K1t.forEach(t),f6r=r(_Q,"to load the model weights."),_Q.forEach(t),g6r=i(Mi),r1e=s(Mi,"P",{});var Z1t=n(r1e);h6r=r(Z1t,"Examples:"),Z1t.forEach(t),u6r=i(Mi),f(s7.$$.fragment,Mi),Mi.forEach(t),p6r=i(Ci),Ro=s(Ci,"DIV",{class:!0});var Sa=n(Ro);f(n7.$$.fragment,Sa),_6r=i(Sa),t1e=s(Sa,"P",{});var eFt=n(t1e);b6r=r(eFt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),eFt.forEach(t),v6r=i(Sa),Ss=s(Sa,"P",{});var H5=n(Ss);T6r=r(H5,"The model class to instantiate is selected based on the "),a1e=s(H5,"CODE",{});var oFt=n(a1e);F6r=r(oFt,"model_type"),oFt.forEach(t),C6r=r(H5,` property of the config object (either
passed as an argument or loaded from `),s1e=s(H5,"CODE",{});var rFt=n(s1e);M6r=r(rFt,"pretrained_model_name_or_path"),rFt.forEach(t),E6r=r(H5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),n1e=s(H5,"CODE",{});var tFt=n(n1e);y6r=r(tFt,"pretrained_model_name_or_path"),tFt.forEach(t),w6r=r(H5,":"),H5.forEach(t),A6r=i(Sa),ve=s(Sa,"UL",{});var Ae=n(ve);c3=s(Ae,"LI",{});var KBe=n(c3);l1e=s(KBe,"STRONG",{});var aFt=n(l1e);L6r=r(aFt,"albert"),aFt.forEach(t),B6r=r(KBe," \u2014 "),KV=s(KBe,"A",{href:!0});var sFt=n(KV);x6r=r(sFt,"FlaxAlbertForSequenceClassification"),sFt.forEach(t),k6r=r(KBe," (ALBERT model)"),KBe.forEach(t),R6r=i(Ae),m3=s(Ae,"LI",{});var ZBe=n(m3);i1e=s(ZBe,"STRONG",{});var nFt=n(i1e);S6r=r(nFt,"bart"),nFt.forEach(t),P6r=r(ZBe," \u2014 "),ZV=s(ZBe,"A",{href:!0});var lFt=n(ZV);$6r=r(lFt,"FlaxBartForSequenceClassification"),lFt.forEach(t),I6r=r(ZBe," (BART model)"),ZBe.forEach(t),j6r=i(Ae),f3=s(Ae,"LI",{});var exe=n(f3);d1e=s(exe,"STRONG",{});var iFt=n(d1e);N6r=r(iFt,"bert"),iFt.forEach(t),D6r=r(exe," \u2014 "),ez=s(exe,"A",{href:!0});var dFt=n(ez);q6r=r(dFt,"FlaxBertForSequenceClassification"),dFt.forEach(t),O6r=r(exe," (BERT model)"),exe.forEach(t),G6r=i(Ae),g3=s(Ae,"LI",{});var oxe=n(g3);c1e=s(oxe,"STRONG",{});var cFt=n(c1e);X6r=r(cFt,"big_bird"),cFt.forEach(t),V6r=r(oxe," \u2014 "),oz=s(oxe,"A",{href:!0});var mFt=n(oz);z6r=r(mFt,"FlaxBigBirdForSequenceClassification"),mFt.forEach(t),W6r=r(oxe," (BigBird model)"),oxe.forEach(t),Q6r=i(Ae),h3=s(Ae,"LI",{});var rxe=n(h3);m1e=s(rxe,"STRONG",{});var fFt=n(m1e);H6r=r(fFt,"distilbert"),fFt.forEach(t),U6r=r(rxe," \u2014 "),rz=s(rxe,"A",{href:!0});var gFt=n(rz);J6r=r(gFt,"FlaxDistilBertForSequenceClassification"),gFt.forEach(t),Y6r=r(rxe," (DistilBERT model)"),rxe.forEach(t),K6r=i(Ae),u3=s(Ae,"LI",{});var txe=n(u3);f1e=s(txe,"STRONG",{});var hFt=n(f1e);Z6r=r(hFt,"electra"),hFt.forEach(t),eAr=r(txe," \u2014 "),tz=s(txe,"A",{href:!0});var uFt=n(tz);oAr=r(uFt,"FlaxElectraForSequenceClassification"),uFt.forEach(t),rAr=r(txe," (ELECTRA model)"),txe.forEach(t),tAr=i(Ae),p3=s(Ae,"LI",{});var axe=n(p3);g1e=s(axe,"STRONG",{});var pFt=n(g1e);aAr=r(pFt,"mbart"),pFt.forEach(t),sAr=r(axe," \u2014 "),az=s(axe,"A",{href:!0});var _Ft=n(az);nAr=r(_Ft,"FlaxMBartForSequenceClassification"),_Ft.forEach(t),lAr=r(axe," (mBART model)"),axe.forEach(t),iAr=i(Ae),_3=s(Ae,"LI",{});var sxe=n(_3);h1e=s(sxe,"STRONG",{});var bFt=n(h1e);dAr=r(bFt,"roberta"),bFt.forEach(t),cAr=r(sxe," \u2014 "),sz=s(sxe,"A",{href:!0});var vFt=n(sz);mAr=r(vFt,"FlaxRobertaForSequenceClassification"),vFt.forEach(t),fAr=r(sxe," (RoBERTa model)"),sxe.forEach(t),gAr=i(Ae),b3=s(Ae,"LI",{});var nxe=n(b3);u1e=s(nxe,"STRONG",{});var TFt=n(u1e);hAr=r(TFt,"roformer"),TFt.forEach(t),uAr=r(nxe," \u2014 "),nz=s(nxe,"A",{href:!0});var FFt=n(nz);pAr=r(FFt,"FlaxRoFormerForSequenceClassification"),FFt.forEach(t),_Ar=r(nxe," (RoFormer model)"),nxe.forEach(t),bAr=i(Ae),v3=s(Ae,"LI",{});var lxe=n(v3);p1e=s(lxe,"STRONG",{});var CFt=n(p1e);vAr=r(CFt,"xlm-roberta"),CFt.forEach(t),TAr=r(lxe," \u2014 "),lz=s(lxe,"A",{href:!0});var MFt=n(lz);FAr=r(MFt,"FlaxXLMRobertaForSequenceClassification"),MFt.forEach(t),CAr=r(lxe," (XLM-RoBERTa model)"),lxe.forEach(t),Ae.forEach(t),MAr=i(Sa),_1e=s(Sa,"P",{});var EFt=n(_1e);EAr=r(EFt,"Examples:"),EFt.forEach(t),yAr=i(Sa),f(l7.$$.fragment,Sa),Sa.forEach(t),Ci.forEach(t),ARe=i(c),pm=s(c,"H2",{class:!0});var DPe=n(pm);T3=s(DPe,"A",{id:!0,class:!0,href:!0});var yFt=n(T3);b1e=s(yFt,"SPAN",{});var wFt=n(b1e);f(i7.$$.fragment,wFt),wFt.forEach(t),yFt.forEach(t),wAr=i(DPe),v1e=s(DPe,"SPAN",{});var AFt=n(v1e);AAr=r(AFt,"FlaxAutoModelForQuestionAnswering"),AFt.forEach(t),DPe.forEach(t),LRe=i(c),Nr=s(c,"DIV",{class:!0});var Ei=n(Nr);f(d7.$$.fragment,Ei),LAr=i(Ei),_m=s(Ei,"P",{});var bQ=n(_m);BAr=r(bQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),T1e=s(bQ,"CODE",{});var LFt=n(T1e);xAr=r(LFt,"from_pretrained()"),LFt.forEach(t),kAr=r(bQ,"class method or the "),F1e=s(bQ,"CODE",{});var BFt=n(F1e);RAr=r(BFt,"from_config()"),BFt.forEach(t),SAr=r(bQ,`class
method.`),bQ.forEach(t),PAr=i(Ei),c7=s(Ei,"P",{});var qPe=n(c7);$Ar=r(qPe,"This class cannot be instantiated directly using "),C1e=s(qPe,"CODE",{});var xFt=n(C1e);IAr=r(xFt,"__init__()"),xFt.forEach(t),jAr=r(qPe," (throws an error)."),qPe.forEach(t),NAr=i(Ei),Rt=s(Ei,"DIV",{class:!0});var yi=n(Rt);f(m7.$$.fragment,yi),DAr=i(yi),M1e=s(yi,"P",{});var kFt=n(M1e);qAr=r(kFt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),kFt.forEach(t),OAr=i(yi),bm=s(yi,"P",{});var vQ=n(bm);GAr=r(vQ,`Note:
Loading a model from its configuration file does `),E1e=s(vQ,"STRONG",{});var RFt=n(E1e);XAr=r(RFt,"not"),RFt.forEach(t),VAr=r(vQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),y1e=s(vQ,"CODE",{});var SFt=n(y1e);zAr=r(SFt,"from_pretrained()"),SFt.forEach(t),WAr=r(vQ,"to load the model weights."),vQ.forEach(t),QAr=i(yi),w1e=s(yi,"P",{});var PFt=n(w1e);HAr=r(PFt,"Examples:"),PFt.forEach(t),UAr=i(yi),f(f7.$$.fragment,yi),yi.forEach(t),JAr=i(Ei),So=s(Ei,"DIV",{class:!0});var Pa=n(So);f(g7.$$.fragment,Pa),YAr=i(Pa),A1e=s(Pa,"P",{});var $Ft=n(A1e);KAr=r($Ft,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),$Ft.forEach(t),ZAr=i(Pa),Ps=s(Pa,"P",{});var U5=n(Ps);e0r=r(U5,"The model class to instantiate is selected based on the "),L1e=s(U5,"CODE",{});var IFt=n(L1e);o0r=r(IFt,"model_type"),IFt.forEach(t),r0r=r(U5,` property of the config object (either
passed as an argument or loaded from `),B1e=s(U5,"CODE",{});var jFt=n(B1e);t0r=r(jFt,"pretrained_model_name_or_path"),jFt.forEach(t),a0r=r(U5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x1e=s(U5,"CODE",{});var NFt=n(x1e);s0r=r(NFt,"pretrained_model_name_or_path"),NFt.forEach(t),n0r=r(U5,":"),U5.forEach(t),l0r=i(Pa),Te=s(Pa,"UL",{});var Le=n(Te);F3=s(Le,"LI",{});var ixe=n(F3);k1e=s(ixe,"STRONG",{});var DFt=n(k1e);i0r=r(DFt,"albert"),DFt.forEach(t),d0r=r(ixe," \u2014 "),iz=s(ixe,"A",{href:!0});var qFt=n(iz);c0r=r(qFt,"FlaxAlbertForQuestionAnswering"),qFt.forEach(t),m0r=r(ixe," (ALBERT model)"),ixe.forEach(t),f0r=i(Le),C3=s(Le,"LI",{});var dxe=n(C3);R1e=s(dxe,"STRONG",{});var OFt=n(R1e);g0r=r(OFt,"bart"),OFt.forEach(t),h0r=r(dxe," \u2014 "),dz=s(dxe,"A",{href:!0});var GFt=n(dz);u0r=r(GFt,"FlaxBartForQuestionAnswering"),GFt.forEach(t),p0r=r(dxe," (BART model)"),dxe.forEach(t),_0r=i(Le),M3=s(Le,"LI",{});var cxe=n(M3);S1e=s(cxe,"STRONG",{});var XFt=n(S1e);b0r=r(XFt,"bert"),XFt.forEach(t),v0r=r(cxe," \u2014 "),cz=s(cxe,"A",{href:!0});var VFt=n(cz);T0r=r(VFt,"FlaxBertForQuestionAnswering"),VFt.forEach(t),F0r=r(cxe," (BERT model)"),cxe.forEach(t),C0r=i(Le),E3=s(Le,"LI",{});var mxe=n(E3);P1e=s(mxe,"STRONG",{});var zFt=n(P1e);M0r=r(zFt,"big_bird"),zFt.forEach(t),E0r=r(mxe," \u2014 "),mz=s(mxe,"A",{href:!0});var WFt=n(mz);y0r=r(WFt,"FlaxBigBirdForQuestionAnswering"),WFt.forEach(t),w0r=r(mxe," (BigBird model)"),mxe.forEach(t),A0r=i(Le),y3=s(Le,"LI",{});var fxe=n(y3);$1e=s(fxe,"STRONG",{});var QFt=n($1e);L0r=r(QFt,"distilbert"),QFt.forEach(t),B0r=r(fxe," \u2014 "),fz=s(fxe,"A",{href:!0});var HFt=n(fz);x0r=r(HFt,"FlaxDistilBertForQuestionAnswering"),HFt.forEach(t),k0r=r(fxe," (DistilBERT model)"),fxe.forEach(t),R0r=i(Le),w3=s(Le,"LI",{});var gxe=n(w3);I1e=s(gxe,"STRONG",{});var UFt=n(I1e);S0r=r(UFt,"electra"),UFt.forEach(t),P0r=r(gxe," \u2014 "),gz=s(gxe,"A",{href:!0});var JFt=n(gz);$0r=r(JFt,"FlaxElectraForQuestionAnswering"),JFt.forEach(t),I0r=r(gxe," (ELECTRA model)"),gxe.forEach(t),j0r=i(Le),A3=s(Le,"LI",{});var hxe=n(A3);j1e=s(hxe,"STRONG",{});var YFt=n(j1e);N0r=r(YFt,"mbart"),YFt.forEach(t),D0r=r(hxe," \u2014 "),hz=s(hxe,"A",{href:!0});var KFt=n(hz);q0r=r(KFt,"FlaxMBartForQuestionAnswering"),KFt.forEach(t),O0r=r(hxe," (mBART model)"),hxe.forEach(t),G0r=i(Le),L3=s(Le,"LI",{});var uxe=n(L3);N1e=s(uxe,"STRONG",{});var ZFt=n(N1e);X0r=r(ZFt,"roberta"),ZFt.forEach(t),V0r=r(uxe," \u2014 "),uz=s(uxe,"A",{href:!0});var eCt=n(uz);z0r=r(eCt,"FlaxRobertaForQuestionAnswering"),eCt.forEach(t),W0r=r(uxe," (RoBERTa model)"),uxe.forEach(t),Q0r=i(Le),B3=s(Le,"LI",{});var pxe=n(B3);D1e=s(pxe,"STRONG",{});var oCt=n(D1e);H0r=r(oCt,"roformer"),oCt.forEach(t),U0r=r(pxe," \u2014 "),pz=s(pxe,"A",{href:!0});var rCt=n(pz);J0r=r(rCt,"FlaxRoFormerForQuestionAnswering"),rCt.forEach(t),Y0r=r(pxe," (RoFormer model)"),pxe.forEach(t),K0r=i(Le),x3=s(Le,"LI",{});var _xe=n(x3);q1e=s(_xe,"STRONG",{});var tCt=n(q1e);Z0r=r(tCt,"xlm-roberta"),tCt.forEach(t),eLr=r(_xe," \u2014 "),_z=s(_xe,"A",{href:!0});var aCt=n(_z);oLr=r(aCt,"FlaxXLMRobertaForQuestionAnswering"),aCt.forEach(t),rLr=r(_xe," (XLM-RoBERTa model)"),_xe.forEach(t),Le.forEach(t),tLr=i(Pa),O1e=s(Pa,"P",{});var sCt=n(O1e);aLr=r(sCt,"Examples:"),sCt.forEach(t),sLr=i(Pa),f(h7.$$.fragment,Pa),Pa.forEach(t),Ei.forEach(t),BRe=i(c),vm=s(c,"H2",{class:!0});var OPe=n(vm);k3=s(OPe,"A",{id:!0,class:!0,href:!0});var nCt=n(k3);G1e=s(nCt,"SPAN",{});var lCt=n(G1e);f(u7.$$.fragment,lCt),lCt.forEach(t),nCt.forEach(t),nLr=i(OPe),X1e=s(OPe,"SPAN",{});var iCt=n(X1e);lLr=r(iCt,"FlaxAutoModelForTokenClassification"),iCt.forEach(t),OPe.forEach(t),xRe=i(c),Dr=s(c,"DIV",{class:!0});var wi=n(Dr);f(p7.$$.fragment,wi),iLr=i(wi),Tm=s(wi,"P",{});var TQ=n(Tm);dLr=r(TQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),V1e=s(TQ,"CODE",{});var dCt=n(V1e);cLr=r(dCt,"from_pretrained()"),dCt.forEach(t),mLr=r(TQ,"class method or the "),z1e=s(TQ,"CODE",{});var cCt=n(z1e);fLr=r(cCt,"from_config()"),cCt.forEach(t),gLr=r(TQ,`class
method.`),TQ.forEach(t),hLr=i(wi),_7=s(wi,"P",{});var GPe=n(_7);uLr=r(GPe,"This class cannot be instantiated directly using "),W1e=s(GPe,"CODE",{});var mCt=n(W1e);pLr=r(mCt,"__init__()"),mCt.forEach(t),_Lr=r(GPe," (throws an error)."),GPe.forEach(t),bLr=i(wi),St=s(wi,"DIV",{class:!0});var Ai=n(St);f(b7.$$.fragment,Ai),vLr=i(Ai),Q1e=s(Ai,"P",{});var fCt=n(Q1e);TLr=r(fCt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),fCt.forEach(t),FLr=i(Ai),Fm=s(Ai,"P",{});var FQ=n(Fm);CLr=r(FQ,`Note:
Loading a model from its configuration file does `),H1e=s(FQ,"STRONG",{});var gCt=n(H1e);MLr=r(gCt,"not"),gCt.forEach(t),ELr=r(FQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),U1e=s(FQ,"CODE",{});var hCt=n(U1e);yLr=r(hCt,"from_pretrained()"),hCt.forEach(t),wLr=r(FQ,"to load the model weights."),FQ.forEach(t),ALr=i(Ai),J1e=s(Ai,"P",{});var uCt=n(J1e);LLr=r(uCt,"Examples:"),uCt.forEach(t),BLr=i(Ai),f(v7.$$.fragment,Ai),Ai.forEach(t),xLr=i(wi),Po=s(wi,"DIV",{class:!0});var $a=n(Po);f(T7.$$.fragment,$a),kLr=i($a),Y1e=s($a,"P",{});var pCt=n(Y1e);RLr=r(pCt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),pCt.forEach(t),SLr=i($a),$s=s($a,"P",{});var J5=n($s);PLr=r(J5,"The model class to instantiate is selected based on the "),K1e=s(J5,"CODE",{});var _Ct=n(K1e);$Lr=r(_Ct,"model_type"),_Ct.forEach(t),ILr=r(J5,` property of the config object (either
passed as an argument or loaded from `),Z1e=s(J5,"CODE",{});var bCt=n(Z1e);jLr=r(bCt,"pretrained_model_name_or_path"),bCt.forEach(t),NLr=r(J5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eFe=s(J5,"CODE",{});var vCt=n(eFe);DLr=r(vCt,"pretrained_model_name_or_path"),vCt.forEach(t),qLr=r(J5,":"),J5.forEach(t),OLr=i($a),Se=s($a,"UL",{});var Go=n(Se);R3=s(Go,"LI",{});var bxe=n(R3);oFe=s(bxe,"STRONG",{});var TCt=n(oFe);GLr=r(TCt,"albert"),TCt.forEach(t),XLr=r(bxe," \u2014 "),bz=s(bxe,"A",{href:!0});var FCt=n(bz);VLr=r(FCt,"FlaxAlbertForTokenClassification"),FCt.forEach(t),zLr=r(bxe," (ALBERT model)"),bxe.forEach(t),WLr=i(Go),S3=s(Go,"LI",{});var vxe=n(S3);rFe=s(vxe,"STRONG",{});var CCt=n(rFe);QLr=r(CCt,"bert"),CCt.forEach(t),HLr=r(vxe," \u2014 "),vz=s(vxe,"A",{href:!0});var MCt=n(vz);ULr=r(MCt,"FlaxBertForTokenClassification"),MCt.forEach(t),JLr=r(vxe," (BERT model)"),vxe.forEach(t),YLr=i(Go),P3=s(Go,"LI",{});var Txe=n(P3);tFe=s(Txe,"STRONG",{});var ECt=n(tFe);KLr=r(ECt,"big_bird"),ECt.forEach(t),ZLr=r(Txe," \u2014 "),Tz=s(Txe,"A",{href:!0});var yCt=n(Tz);e7r=r(yCt,"FlaxBigBirdForTokenClassification"),yCt.forEach(t),o7r=r(Txe," (BigBird model)"),Txe.forEach(t),r7r=i(Go),$3=s(Go,"LI",{});var Fxe=n($3);aFe=s(Fxe,"STRONG",{});var wCt=n(aFe);t7r=r(wCt,"distilbert"),wCt.forEach(t),a7r=r(Fxe," \u2014 "),Fz=s(Fxe,"A",{href:!0});var ACt=n(Fz);s7r=r(ACt,"FlaxDistilBertForTokenClassification"),ACt.forEach(t),n7r=r(Fxe," (DistilBERT model)"),Fxe.forEach(t),l7r=i(Go),I3=s(Go,"LI",{});var Cxe=n(I3);sFe=s(Cxe,"STRONG",{});var LCt=n(sFe);i7r=r(LCt,"electra"),LCt.forEach(t),d7r=r(Cxe," \u2014 "),Cz=s(Cxe,"A",{href:!0});var BCt=n(Cz);c7r=r(BCt,"FlaxElectraForTokenClassification"),BCt.forEach(t),m7r=r(Cxe," (ELECTRA model)"),Cxe.forEach(t),f7r=i(Go),j3=s(Go,"LI",{});var Mxe=n(j3);nFe=s(Mxe,"STRONG",{});var xCt=n(nFe);g7r=r(xCt,"roberta"),xCt.forEach(t),h7r=r(Mxe," \u2014 "),Mz=s(Mxe,"A",{href:!0});var kCt=n(Mz);u7r=r(kCt,"FlaxRobertaForTokenClassification"),kCt.forEach(t),p7r=r(Mxe," (RoBERTa model)"),Mxe.forEach(t),_7r=i(Go),N3=s(Go,"LI",{});var Exe=n(N3);lFe=s(Exe,"STRONG",{});var RCt=n(lFe);b7r=r(RCt,"roformer"),RCt.forEach(t),v7r=r(Exe," \u2014 "),Ez=s(Exe,"A",{href:!0});var SCt=n(Ez);T7r=r(SCt,"FlaxRoFormerForTokenClassification"),SCt.forEach(t),F7r=r(Exe," (RoFormer model)"),Exe.forEach(t),C7r=i(Go),D3=s(Go,"LI",{});var yxe=n(D3);iFe=s(yxe,"STRONG",{});var PCt=n(iFe);M7r=r(PCt,"xlm-roberta"),PCt.forEach(t),E7r=r(yxe," \u2014 "),yz=s(yxe,"A",{href:!0});var $Ct=n(yz);y7r=r($Ct,"FlaxXLMRobertaForTokenClassification"),$Ct.forEach(t),w7r=r(yxe," (XLM-RoBERTa model)"),yxe.forEach(t),Go.forEach(t),A7r=i($a),dFe=s($a,"P",{});var ICt=n(dFe);L7r=r(ICt,"Examples:"),ICt.forEach(t),B7r=i($a),f(F7.$$.fragment,$a),$a.forEach(t),wi.forEach(t),kRe=i(c),Cm=s(c,"H2",{class:!0});var XPe=n(Cm);q3=s(XPe,"A",{id:!0,class:!0,href:!0});var jCt=n(q3);cFe=s(jCt,"SPAN",{});var NCt=n(cFe);f(C7.$$.fragment,NCt),NCt.forEach(t),jCt.forEach(t),x7r=i(XPe),mFe=s(XPe,"SPAN",{});var DCt=n(mFe);k7r=r(DCt,"FlaxAutoModelForMultipleChoice"),DCt.forEach(t),XPe.forEach(t),RRe=i(c),qr=s(c,"DIV",{class:!0});var Li=n(qr);f(M7.$$.fragment,Li),R7r=i(Li),Mm=s(Li,"P",{});var CQ=n(Mm);S7r=r(CQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),fFe=s(CQ,"CODE",{});var qCt=n(fFe);P7r=r(qCt,"from_pretrained()"),qCt.forEach(t),$7r=r(CQ,"class method or the "),gFe=s(CQ,"CODE",{});var OCt=n(gFe);I7r=r(OCt,"from_config()"),OCt.forEach(t),j7r=r(CQ,`class
method.`),CQ.forEach(t),N7r=i(Li),E7=s(Li,"P",{});var VPe=n(E7);D7r=r(VPe,"This class cannot be instantiated directly using "),hFe=s(VPe,"CODE",{});var GCt=n(hFe);q7r=r(GCt,"__init__()"),GCt.forEach(t),O7r=r(VPe," (throws an error)."),VPe.forEach(t),G7r=i(Li),Pt=s(Li,"DIV",{class:!0});var Bi=n(Pt);f(y7.$$.fragment,Bi),X7r=i(Bi),uFe=s(Bi,"P",{});var XCt=n(uFe);V7r=r(XCt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),XCt.forEach(t),z7r=i(Bi),Em=s(Bi,"P",{});var MQ=n(Em);W7r=r(MQ,`Note:
Loading a model from its configuration file does `),pFe=s(MQ,"STRONG",{});var VCt=n(pFe);Q7r=r(VCt,"not"),VCt.forEach(t),H7r=r(MQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),_Fe=s(MQ,"CODE",{});var zCt=n(_Fe);U7r=r(zCt,"from_pretrained()"),zCt.forEach(t),J7r=r(MQ,"to load the model weights."),MQ.forEach(t),Y7r=i(Bi),bFe=s(Bi,"P",{});var WCt=n(bFe);K7r=r(WCt,"Examples:"),WCt.forEach(t),Z7r=i(Bi),f(w7.$$.fragment,Bi),Bi.forEach(t),e8r=i(Li),$o=s(Li,"DIV",{class:!0});var Ia=n($o);f(A7.$$.fragment,Ia),o8r=i(Ia),vFe=s(Ia,"P",{});var QCt=n(vFe);r8r=r(QCt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),QCt.forEach(t),t8r=i(Ia),Is=s(Ia,"P",{});var Y5=n(Is);a8r=r(Y5,"The model class to instantiate is selected based on the "),TFe=s(Y5,"CODE",{});var HCt=n(TFe);s8r=r(HCt,"model_type"),HCt.forEach(t),n8r=r(Y5,` property of the config object (either
passed as an argument or loaded from `),FFe=s(Y5,"CODE",{});var UCt=n(FFe);l8r=r(UCt,"pretrained_model_name_or_path"),UCt.forEach(t),i8r=r(Y5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),CFe=s(Y5,"CODE",{});var JCt=n(CFe);d8r=r(JCt,"pretrained_model_name_or_path"),JCt.forEach(t),c8r=r(Y5,":"),Y5.forEach(t),m8r=i(Ia),Pe=s(Ia,"UL",{});var Xo=n(Pe);O3=s(Xo,"LI",{});var wxe=n(O3);MFe=s(wxe,"STRONG",{});var YCt=n(MFe);f8r=r(YCt,"albert"),YCt.forEach(t),g8r=r(wxe," \u2014 "),wz=s(wxe,"A",{href:!0});var KCt=n(wz);h8r=r(KCt,"FlaxAlbertForMultipleChoice"),KCt.forEach(t),u8r=r(wxe," (ALBERT model)"),wxe.forEach(t),p8r=i(Xo),G3=s(Xo,"LI",{});var Axe=n(G3);EFe=s(Axe,"STRONG",{});var ZCt=n(EFe);_8r=r(ZCt,"bert"),ZCt.forEach(t),b8r=r(Axe," \u2014 "),Az=s(Axe,"A",{href:!0});var eMt=n(Az);v8r=r(eMt,"FlaxBertForMultipleChoice"),eMt.forEach(t),T8r=r(Axe," (BERT model)"),Axe.forEach(t),F8r=i(Xo),X3=s(Xo,"LI",{});var Lxe=n(X3);yFe=s(Lxe,"STRONG",{});var oMt=n(yFe);C8r=r(oMt,"big_bird"),oMt.forEach(t),M8r=r(Lxe," \u2014 "),Lz=s(Lxe,"A",{href:!0});var rMt=n(Lz);E8r=r(rMt,"FlaxBigBirdForMultipleChoice"),rMt.forEach(t),y8r=r(Lxe," (BigBird model)"),Lxe.forEach(t),w8r=i(Xo),V3=s(Xo,"LI",{});var Bxe=n(V3);wFe=s(Bxe,"STRONG",{});var tMt=n(wFe);A8r=r(tMt,"distilbert"),tMt.forEach(t),L8r=r(Bxe," \u2014 "),Bz=s(Bxe,"A",{href:!0});var aMt=n(Bz);B8r=r(aMt,"FlaxDistilBertForMultipleChoice"),aMt.forEach(t),x8r=r(Bxe," (DistilBERT model)"),Bxe.forEach(t),k8r=i(Xo),z3=s(Xo,"LI",{});var xxe=n(z3);AFe=s(xxe,"STRONG",{});var sMt=n(AFe);R8r=r(sMt,"electra"),sMt.forEach(t),S8r=r(xxe," \u2014 "),xz=s(xxe,"A",{href:!0});var nMt=n(xz);P8r=r(nMt,"FlaxElectraForMultipleChoice"),nMt.forEach(t),$8r=r(xxe," (ELECTRA model)"),xxe.forEach(t),I8r=i(Xo),W3=s(Xo,"LI",{});var kxe=n(W3);LFe=s(kxe,"STRONG",{});var lMt=n(LFe);j8r=r(lMt,"roberta"),lMt.forEach(t),N8r=r(kxe," \u2014 "),kz=s(kxe,"A",{href:!0});var iMt=n(kz);D8r=r(iMt,"FlaxRobertaForMultipleChoice"),iMt.forEach(t),q8r=r(kxe," (RoBERTa model)"),kxe.forEach(t),O8r=i(Xo),Q3=s(Xo,"LI",{});var Rxe=n(Q3);BFe=s(Rxe,"STRONG",{});var dMt=n(BFe);G8r=r(dMt,"roformer"),dMt.forEach(t),X8r=r(Rxe," \u2014 "),Rz=s(Rxe,"A",{href:!0});var cMt=n(Rz);V8r=r(cMt,"FlaxRoFormerForMultipleChoice"),cMt.forEach(t),z8r=r(Rxe," (RoFormer model)"),Rxe.forEach(t),W8r=i(Xo),H3=s(Xo,"LI",{});var Sxe=n(H3);xFe=s(Sxe,"STRONG",{});var mMt=n(xFe);Q8r=r(mMt,"xlm-roberta"),mMt.forEach(t),H8r=r(Sxe," \u2014 "),Sz=s(Sxe,"A",{href:!0});var fMt=n(Sz);U8r=r(fMt,"FlaxXLMRobertaForMultipleChoice"),fMt.forEach(t),J8r=r(Sxe," (XLM-RoBERTa model)"),Sxe.forEach(t),Xo.forEach(t),Y8r=i(Ia),kFe=s(Ia,"P",{});var gMt=n(kFe);K8r=r(gMt,"Examples:"),gMt.forEach(t),Z8r=i(Ia),f(L7.$$.fragment,Ia),Ia.forEach(t),Li.forEach(t),SRe=i(c),ym=s(c,"H2",{class:!0});var zPe=n(ym);U3=s(zPe,"A",{id:!0,class:!0,href:!0});var hMt=n(U3);RFe=s(hMt,"SPAN",{});var uMt=n(RFe);f(B7.$$.fragment,uMt),uMt.forEach(t),hMt.forEach(t),e9r=i(zPe),SFe=s(zPe,"SPAN",{});var pMt=n(SFe);o9r=r(pMt,"FlaxAutoModelForNextSentencePrediction"),pMt.forEach(t),zPe.forEach(t),PRe=i(c),Or=s(c,"DIV",{class:!0});var xi=n(Or);f(x7.$$.fragment,xi),r9r=i(xi),wm=s(xi,"P",{});var EQ=n(wm);t9r=r(EQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),PFe=s(EQ,"CODE",{});var _Mt=n(PFe);a9r=r(_Mt,"from_pretrained()"),_Mt.forEach(t),s9r=r(EQ,"class method or the "),$Fe=s(EQ,"CODE",{});var bMt=n($Fe);n9r=r(bMt,"from_config()"),bMt.forEach(t),l9r=r(EQ,`class
method.`),EQ.forEach(t),i9r=i(xi),k7=s(xi,"P",{});var WPe=n(k7);d9r=r(WPe,"This class cannot be instantiated directly using "),IFe=s(WPe,"CODE",{});var vMt=n(IFe);c9r=r(vMt,"__init__()"),vMt.forEach(t),m9r=r(WPe," (throws an error)."),WPe.forEach(t),f9r=i(xi),$t=s(xi,"DIV",{class:!0});var ki=n($t);f(R7.$$.fragment,ki),g9r=i(ki),jFe=s(ki,"P",{});var TMt=n(jFe);h9r=r(TMt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),TMt.forEach(t),u9r=i(ki),Am=s(ki,"P",{});var yQ=n(Am);p9r=r(yQ,`Note:
Loading a model from its configuration file does `),NFe=s(yQ,"STRONG",{});var FMt=n(NFe);_9r=r(FMt,"not"),FMt.forEach(t),b9r=r(yQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),DFe=s(yQ,"CODE",{});var CMt=n(DFe);v9r=r(CMt,"from_pretrained()"),CMt.forEach(t),T9r=r(yQ,"to load the model weights."),yQ.forEach(t),F9r=i(ki),qFe=s(ki,"P",{});var MMt=n(qFe);C9r=r(MMt,"Examples:"),MMt.forEach(t),M9r=i(ki),f(S7.$$.fragment,ki),ki.forEach(t),E9r=i(xi),Io=s(xi,"DIV",{class:!0});var ja=n(Io);f(P7.$$.fragment,ja),y9r=i(ja),OFe=s(ja,"P",{});var EMt=n(OFe);w9r=r(EMt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),EMt.forEach(t),A9r=i(ja),js=s(ja,"P",{});var K5=n(js);L9r=r(K5,"The model class to instantiate is selected based on the "),GFe=s(K5,"CODE",{});var yMt=n(GFe);B9r=r(yMt,"model_type"),yMt.forEach(t),x9r=r(K5,` property of the config object (either
passed as an argument or loaded from `),XFe=s(K5,"CODE",{});var wMt=n(XFe);k9r=r(wMt,"pretrained_model_name_or_path"),wMt.forEach(t),R9r=r(K5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),VFe=s(K5,"CODE",{});var AMt=n(VFe);S9r=r(AMt,"pretrained_model_name_or_path"),AMt.forEach(t),P9r=r(K5,":"),K5.forEach(t),$9r=i(ja),zFe=s(ja,"UL",{});var LMt=n(zFe);J3=s(LMt,"LI",{});var Pxe=n(J3);WFe=s(Pxe,"STRONG",{});var BMt=n(WFe);I9r=r(BMt,"bert"),BMt.forEach(t),j9r=r(Pxe," \u2014 "),Pz=s(Pxe,"A",{href:!0});var xMt=n(Pz);N9r=r(xMt,"FlaxBertForNextSentencePrediction"),xMt.forEach(t),D9r=r(Pxe," (BERT model)"),Pxe.forEach(t),LMt.forEach(t),q9r=i(ja),QFe=s(ja,"P",{});var kMt=n(QFe);O9r=r(kMt,"Examples:"),kMt.forEach(t),G9r=i(ja),f($7.$$.fragment,ja),ja.forEach(t),xi.forEach(t),$Re=i(c),Lm=s(c,"H2",{class:!0});var QPe=n(Lm);Y3=s(QPe,"A",{id:!0,class:!0,href:!0});var RMt=n(Y3);HFe=s(RMt,"SPAN",{});var SMt=n(HFe);f(I7.$$.fragment,SMt),SMt.forEach(t),RMt.forEach(t),X9r=i(QPe),UFe=s(QPe,"SPAN",{});var PMt=n(UFe);V9r=r(PMt,"FlaxAutoModelForImageClassification"),PMt.forEach(t),QPe.forEach(t),IRe=i(c),Gr=s(c,"DIV",{class:!0});var Ri=n(Gr);f(j7.$$.fragment,Ri),z9r=i(Ri),Bm=s(Ri,"P",{});var wQ=n(Bm);W9r=r(wQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),JFe=s(wQ,"CODE",{});var $Mt=n(JFe);Q9r=r($Mt,"from_pretrained()"),$Mt.forEach(t),H9r=r(wQ,"class method or the "),YFe=s(wQ,"CODE",{});var IMt=n(YFe);U9r=r(IMt,"from_config()"),IMt.forEach(t),J9r=r(wQ,`class
method.`),wQ.forEach(t),Y9r=i(Ri),N7=s(Ri,"P",{});var HPe=n(N7);K9r=r(HPe,"This class cannot be instantiated directly using "),KFe=s(HPe,"CODE",{});var jMt=n(KFe);Z9r=r(jMt,"__init__()"),jMt.forEach(t),eBr=r(HPe," (throws an error)."),HPe.forEach(t),oBr=i(Ri),It=s(Ri,"DIV",{class:!0});var Si=n(It);f(D7.$$.fragment,Si),rBr=i(Si),ZFe=s(Si,"P",{});var NMt=n(ZFe);tBr=r(NMt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),NMt.forEach(t),aBr=i(Si),xm=s(Si,"P",{});var AQ=n(xm);sBr=r(AQ,`Note:
Loading a model from its configuration file does `),eCe=s(AQ,"STRONG",{});var DMt=n(eCe);nBr=r(DMt,"not"),DMt.forEach(t),lBr=r(AQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),oCe=s(AQ,"CODE",{});var qMt=n(oCe);iBr=r(qMt,"from_pretrained()"),qMt.forEach(t),dBr=r(AQ,"to load the model weights."),AQ.forEach(t),cBr=i(Si),rCe=s(Si,"P",{});var OMt=n(rCe);mBr=r(OMt,"Examples:"),OMt.forEach(t),fBr=i(Si),f(q7.$$.fragment,Si),Si.forEach(t),gBr=i(Ri),jo=s(Ri,"DIV",{class:!0});var Na=n(jo);f(O7.$$.fragment,Na),hBr=i(Na),tCe=s(Na,"P",{});var GMt=n(tCe);uBr=r(GMt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),GMt.forEach(t),pBr=i(Na),Ns=s(Na,"P",{});var Z5=n(Ns);_Br=r(Z5,"The model class to instantiate is selected based on the "),aCe=s(Z5,"CODE",{});var XMt=n(aCe);bBr=r(XMt,"model_type"),XMt.forEach(t),vBr=r(Z5,` property of the config object (either
passed as an argument or loaded from `),sCe=s(Z5,"CODE",{});var VMt=n(sCe);TBr=r(VMt,"pretrained_model_name_or_path"),VMt.forEach(t),FBr=r(Z5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nCe=s(Z5,"CODE",{});var zMt=n(nCe);CBr=r(zMt,"pretrained_model_name_or_path"),zMt.forEach(t),MBr=r(Z5,":"),Z5.forEach(t),EBr=i(Na),G7=s(Na,"UL",{});var UPe=n(G7);K3=s(UPe,"LI",{});var $xe=n(K3);lCe=s($xe,"STRONG",{});var WMt=n(lCe);yBr=r(WMt,"beit"),WMt.forEach(t),wBr=r($xe," \u2014 "),$z=s($xe,"A",{href:!0});var QMt=n($z);ABr=r(QMt,"FlaxBeitForImageClassification"),QMt.forEach(t),LBr=r($xe," (BEiT model)"),$xe.forEach(t),BBr=i(UPe),Z3=s(UPe,"LI",{});var Ixe=n(Z3);iCe=s(Ixe,"STRONG",{});var HMt=n(iCe);xBr=r(HMt,"vit"),HMt.forEach(t),kBr=r(Ixe," \u2014 "),Iz=s(Ixe,"A",{href:!0});var UMt=n(Iz);RBr=r(UMt,"FlaxViTForImageClassification"),UMt.forEach(t),SBr=r(Ixe," (ViT model)"),Ixe.forEach(t),UPe.forEach(t),PBr=i(Na),dCe=s(Na,"P",{});var JMt=n(dCe);$Br=r(JMt,"Examples:"),JMt.forEach(t),IBr=i(Na),f(X7.$$.fragment,Na),Na.forEach(t),Ri.forEach(t),jRe=i(c),km=s(c,"H2",{class:!0});var JPe=n(km);e5=s(JPe,"A",{id:!0,class:!0,href:!0});var YMt=n(e5);cCe=s(YMt,"SPAN",{});var KMt=n(cCe);f(V7.$$.fragment,KMt),KMt.forEach(t),YMt.forEach(t),jBr=i(JPe),mCe=s(JPe,"SPAN",{});var ZMt=n(mCe);NBr=r(ZMt,"FlaxAutoModelForVision2Seq"),ZMt.forEach(t),JPe.forEach(t),NRe=i(c),Xr=s(c,"DIV",{class:!0});var Pi=n(Xr);f(z7.$$.fragment,Pi),DBr=i(Pi),Rm=s(Pi,"P",{});var LQ=n(Rm);qBr=r(LQ,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),fCe=s(LQ,"CODE",{});var e4t=n(fCe);OBr=r(e4t,"from_pretrained()"),e4t.forEach(t),GBr=r(LQ,"class method or the "),gCe=s(LQ,"CODE",{});var o4t=n(gCe);XBr=r(o4t,"from_config()"),o4t.forEach(t),VBr=r(LQ,`class
method.`),LQ.forEach(t),zBr=i(Pi),W7=s(Pi,"P",{});var YPe=n(W7);WBr=r(YPe,"This class cannot be instantiated directly using "),hCe=s(YPe,"CODE",{});var r4t=n(hCe);QBr=r(r4t,"__init__()"),r4t.forEach(t),HBr=r(YPe," (throws an error)."),YPe.forEach(t),UBr=i(Pi),jt=s(Pi,"DIV",{class:!0});var $i=n(jt);f(Q7.$$.fragment,$i),JBr=i($i),uCe=s($i,"P",{});var t4t=n(uCe);YBr=r(t4t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),t4t.forEach(t),KBr=i($i),Sm=s($i,"P",{});var BQ=n(Sm);ZBr=r(BQ,`Note:
Loading a model from its configuration file does `),pCe=s(BQ,"STRONG",{});var a4t=n(pCe);exr=r(a4t,"not"),a4t.forEach(t),oxr=r(BQ,` load the model weights. It only affects the
model\u2019s configuration. Use `),_Ce=s(BQ,"CODE",{});var s4t=n(_Ce);rxr=r(s4t,"from_pretrained()"),s4t.forEach(t),txr=r(BQ,"to load the model weights."),BQ.forEach(t),axr=i($i),bCe=s($i,"P",{});var n4t=n(bCe);sxr=r(n4t,"Examples:"),n4t.forEach(t),nxr=i($i),f(H7.$$.fragment,$i),$i.forEach(t),lxr=i(Pi),No=s(Pi,"DIV",{class:!0});var Da=n(No);f(U7.$$.fragment,Da),ixr=i(Da),vCe=s(Da,"P",{});var l4t=n(vCe);dxr=r(l4t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),l4t.forEach(t),cxr=i(Da),Ds=s(Da,"P",{});var ey=n(Ds);mxr=r(ey,"The model class to instantiate is selected based on the "),TCe=s(ey,"CODE",{});var i4t=n(TCe);fxr=r(i4t,"model_type"),i4t.forEach(t),gxr=r(ey,` property of the config object (either
passed as an argument or loaded from `),FCe=s(ey,"CODE",{});var d4t=n(FCe);hxr=r(d4t,"pretrained_model_name_or_path"),d4t.forEach(t),uxr=r(ey,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),CCe=s(ey,"CODE",{});var c4t=n(CCe);pxr=r(c4t,"pretrained_model_name_or_path"),c4t.forEach(t),_xr=r(ey,":"),ey.forEach(t),bxr=i(Da),MCe=s(Da,"UL",{});var m4t=n(MCe);o5=s(m4t,"LI",{});var jxe=n(o5);ECe=s(jxe,"STRONG",{});var f4t=n(ECe);vxr=r(f4t,"vision-encoder-decoder"),f4t.forEach(t),Txr=r(jxe," \u2014 "),jz=s(jxe,"A",{href:!0});var g4t=n(jz);Fxr=r(g4t,"FlaxVisionEncoderDecoderModel"),g4t.forEach(t),Cxr=r(jxe," (Vision Encoder decoder model)"),jxe.forEach(t),m4t.forEach(t),Mxr=i(Da),yCe=s(Da,"P",{});var h4t=n(yCe);Exr=r(h4t,"Examples:"),h4t.forEach(t),yxr=i(Da),f(J7.$$.fragment,Da),Da.forEach(t),Pi.forEach(t),this.h()},h(){d(J,"name","hf:doc:metadata"),d(J,"content",JSON.stringify(M4t)),d(ue,"id","auto-classes"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#auto-classes"),d(de,"class","relative group"),d(qs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),d(Gs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),d(Xs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),d(Xi,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),d(Dm,"id","extending-the-auto-classes"),d(Dm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Dm,"href","#extending-the-auto-classes"),d(Vi,"class","relative group"),d(Om,"id","transformers.AutoConfig"),d(Om,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Om,"href","#transformers.AutoConfig"),d(zi,"class","relative group"),d(e9,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),d(o9,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),d(r9,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),d(t9,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),d(a9,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),d(s9,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig"),d(n9,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig"),d(l9,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),d(i9,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),d(d9,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),d(c9,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),d(m9,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),d(f9,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),d(g9,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),d(h9,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig"),d(u9,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),d(p9,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig"),d(_9,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig"),d(b9,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),d(v9,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config"),d(T9,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),d(F9,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),d(C9,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),d(M9,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),d(E9,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),d(y9,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),d(w9,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),d(A9,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),d(L9,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),d(B9,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),d(x9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),d(k9,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),d(R9,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),d(S9,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),d(P9,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),d($9,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),d(I9,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),d(j9,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),d(N9,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),d(D9,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),d(q9,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),d(O9,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),d(G9,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),d(X9,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),d(V9,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig"),d(z9,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),d(W9,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),d(Q9,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),d(H9,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),d(U9,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),d(J9,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig"),d(Y9,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),d(K9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),d(Z9,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),d(eB,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig"),d(oB,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig"),d(rB,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),d(tB,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig"),d(aB,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),d(sB,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmConfig"),d(nB,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),d(lB,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),d(iB,"href","/docs/transformers/master/en/model_doc/resnet#transformers.ResNetConfig"),d(dB,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),d(cB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),d(mB,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),d(fB,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),d(gB,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),d(hB,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig"),d(uB,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),d(pB,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),d(_B,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),d(bB,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),d(vB,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),d(TB,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig"),d(FB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),d(CB,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),d(MB,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),d(EB,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),d(yB,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),d(wB,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),d(AB,"href","/docs/transformers/master/en/model_doc/van#transformers.VanConfig"),d(LB,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig"),d(BB,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),d(xB,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),d(kB,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),d(RB,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),d(SB,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig"),d(PB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),d($B,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig"),d(IB,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig"),d(jB,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),d(NB,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),d(DB,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),d(qB,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),d(OB,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),d(GB,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig"),d(fo,"class","docstring"),d(kg,"class","docstring"),d(Wo,"class","docstring"),d(Rg,"id","transformers.AutoTokenizer"),d(Rg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Rg,"href","#transformers.AutoTokenizer"),d(Qi,"class","relative group"),d(XB,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(VB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),d(zB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),d(WB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),d(QB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),d(HB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),d(UB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),d(JB,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),d(YB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),d(KB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),d(ZB,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),d(ex,"href","/docs/transformers/master/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),d(ox,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),d(rx,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizer"),d(tx,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),d(ax,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(sx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(nx,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),d(lx,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),d(ix,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),d(dx,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),d(cx,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),d(mx,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),d(fx,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),d(gx,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),d(hx,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(ux,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),d(px,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),d(_x,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),d(bx,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),d(vx,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),d(Tx,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),d(Fx,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),d(Cx,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),d(Mx,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),d(Ex,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(yx,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(wx,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),d(Ax,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),d(Lx,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),d(Bx,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),d(xx,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),d(kx,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),d(Rx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),d(Sx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),d(Px,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d($x,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(Ix,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(jx,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(Nx,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizer"),d(Dx,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizerFast"),d(qx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(Ox,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),d(Gx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(Xx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),d(Vx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),d(zx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),d(Wx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),d(Qx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),d(Hx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),d(Ux,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),d(Jx,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),d(Yx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),d(Kx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),d(Zx,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),d(ek,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),d(ok,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),d(rk,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),d(tk,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),d(ak,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),d(sk,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),d(nk,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),d(lk,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),d(ik,"href","/docs/transformers/master/en/model_doc/mluke#transformers.MLukeTokenizer"),d(dk,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),d(ck,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),d(mk,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),d(fk,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),d(gk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),d(hk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),d(uk,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),d(pk,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),d(_k,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(bk,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(vk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverTokenizer"),d(Tk,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),d(Fk,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartTokenizer"),d(Ck,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),d(Mk,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),d(Ek,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),d(yk,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),d(wk,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmTokenizer"),d(Ak,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmTokenizerFast"),d(Lk,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),d(Bk,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),d(xk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),d(kk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),d(Rk,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),d(Sk,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),d(Pk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),d($k,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(Ik,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),d(jk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),d(Nk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),d(Dk,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),d(qk,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),d(Ok,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),d(Gk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),d(Xk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),d(Vk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),d(zk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),d(Wk,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),d(Qk,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),d(Hk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(Uk,"href","/docs/transformers/master/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),d(Jk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizer"),d(Yk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizerFast"),d(Kk,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),d(Zk,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),d(eR,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),d(oR,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),d(rR,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),d(tR,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),d(go,"class","docstring"),d(ih,"class","docstring"),d(Qo,"class","docstring"),d(dh,"id","transformers.AutoFeatureExtractor"),d(dh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(dh,"href","#transformers.AutoFeatureExtractor"),d(Hi,"class","relative group"),d(aR,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(sR,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),d(nR,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),d(lR,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(iR,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),d(dR,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),d(cR,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(mR,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),d(fR,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor"),d(gR,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),d(hR,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),d(uR,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(pR,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),d(_R,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),d(bR,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(vR,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(TR,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(FR,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(CR,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(Ie,"class","docstring"),d(Bh,"class","docstring"),d(Ho,"class","docstring"),d(xh,"id","transformers.AutoProcessor"),d(xh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xh,"href","#transformers.AutoProcessor"),d(Ui,"class","relative group"),d(MR,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(ER,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),d(yR,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),d(wR,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),d(AR,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),d(LR,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),d(BR,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),d(xR,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),d(kR,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),d(je,"class","docstring"),d(qh,"class","docstring"),d(Uo,"class","docstring"),d(Oh,"id","transformers.AutoModel"),d(Oh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oh,"href","#transformers.AutoModel"),d(Yi,"class","relative group"),d(Vr,"class","docstring"),d(RR,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),d(SR,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),d(PR,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),d($R,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),d(IR,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),d(jR,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel"),d(NR,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),d(DR,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),d(qR,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),d(OR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),d(GR,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),d(XR,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),d(VR,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),d(zR,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel"),d(WR,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),d(QR,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel"),d(HR,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel"),d(UR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),d(JR,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model"),d(YR,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),d(KR,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),d(ZR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),d(eS,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(oS,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),d(rS,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),d(tS,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),d(aS,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),d(sS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),d(nS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),d(lS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),d(iS,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),d(dS,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),d(cS,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),d(mS,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),d(fS,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),d(gS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),d(hS,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),d(uS,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),d(pS,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),d(_S,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),d(bS,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),d(vS,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),d(TS,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),d(FS,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerModel"),d(CS,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),d(MS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel"),d(ES,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),d(yS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),d(wS,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),d(AS,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel"),d(LS,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),d(BS,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),d(xS,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),d(kS,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel"),d(RS,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel"),d(SS,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),d(PS,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel"),d($S,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),d(IS,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),d(jS,"href","/docs/transformers/master/en/model_doc/resnet#transformers.ResNetModel"),d(NS,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),d(DS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),d(qS,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),d(OS,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),d(GS,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),d(XS,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel"),d(VS,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),d(zS,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),d(WS,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),d(QS,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinModel"),d(HS,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),d(US,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),d(JS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel"),d(YS,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),d(KS,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),d(ZS,"href","/docs/transformers/master/en/model_doc/van#transformers.VanModel"),d(eP,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel"),d(oP,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),d(rP,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),d(tP,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),d(aP,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel"),d(sP,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),d(nP,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel"),d(lP,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel"),d(iP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),d(dP,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),d(cP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),d(mP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),d(fP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),d(gP,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel"),d(Ne,"class","docstring"),d(Jo,"class","docstring"),d(Mp,"id","transformers.AutoModelForPreTraining"),d(Mp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Mp,"href","#transformers.AutoModelForPreTraining"),d(ed,"class","relative group"),d(zr,"class","docstring"),d(hP,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),d(uP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(pP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),d(_P,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),d(bP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(vP,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(TP,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(FP,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(CP,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(MP,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(EP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),d(yP,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(wP,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),d(AP,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(LP,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),d(BP,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(xP,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(kP,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(RP,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(SP,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),d(PP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),d($P,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),d(IP,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(jP,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(NP,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),d(DP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(qP,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(OP,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(GP,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(XP,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(VP,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),d(zP,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),d(WP,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),d(QP,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),d(HP,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),d(UP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(JP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(YP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(KP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(De,"class","docstring"),d(Yo,"class","docstring"),d(c_,"id","transformers.AutoModelForCausalLM"),d(c_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(c_,"href","#transformers.AutoModelForCausalLM"),d(td,"class","relative group"),d(Wr,"class","docstring"),d(ZP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),d(e$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),d(o$,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),d(r$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),d(t$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),d(a$,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),d(s$,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),d(n$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),d(l$,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(i$,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM"),d(d$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM"),d(c$,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(m$,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),d(f$,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),d(g$,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),d(h$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),d(u$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),d(p$,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(_$,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),d(b$,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM"),d(v$,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),d(T$,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),d(F$,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),d(C$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),d(M$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),d(E$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),d(y$,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),d(w$,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(A$,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),d(L$,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM"),d(B$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(x$,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),d(k$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),d(R$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),d(S$,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(qe,"class","docstring"),d(Ko,"class","docstring"),d(Q_,"id","transformers.AutoModelForMaskedLM"),d(Q_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Q_,"href","#transformers.AutoModelForMaskedLM"),d(nd,"class","relative group"),d(Qr,"class","docstring"),d(P$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),d($$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(I$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),d(j$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),d(N$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(D$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),d(q$,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(O$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(G$,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(X$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(V$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),d(z$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(W$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),d(Q$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),d(H$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(U$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(J$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(Y$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(K$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),d(Z$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),d(eI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(oI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),d(rI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(tI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),d(aI,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),d(sI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),d(nI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(lI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),d(iI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(dI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(cI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(mI,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(fI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(gI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM"),d(Oe,"class","docstring"),d(Zo,"class","docstring"),d(xb,"id","transformers.AutoModelForSeq2SeqLM"),d(xb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xb,"href","#transformers.AutoModelForSeq2SeqLM"),d(dd,"class","relative group"),d(Hr,"class","docstring"),d(hI,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(uI,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),d(pI,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),d(_I,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),d(bI,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),d(vI,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(TI,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),d(FI,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),d(CI,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),d(MI,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(EI,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),d(yI,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),d(wI,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),d(AI,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),d(LI,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(BI,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),d(Ge,"class","docstring"),d(er,"class","docstring"),d(Hb,"id","transformers.AutoModelForSequenceClassification"),d(Hb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Hb,"href","#transformers.AutoModelForSequenceClassification"),d(fd,"class","relative group"),d(Ur,"class","docstring"),d(xI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),d(kI,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),d(RI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),d(SI,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),d(PI,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),d($I,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),d(II,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),d(jI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),d(NI,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),d(DI,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification"),d(qI,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),d(OI,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),d(GI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),d(XI,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),d(VI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),d(zI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),d(WI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),d(QI,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),d(HI,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),d(UI,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),d(JI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),d(YI,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),d(KI,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),d(ZI,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),d(ej,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),d(oj,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),d(rj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),d(tj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),d(aj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),d(sj,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),d(nj,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),d(lj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(ij,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),d(dj,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),d(cj,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),d(mj,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),d(fj,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),d(gj,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),d(hj,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),d(uj,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),d(pj,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),d(_j,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),d(bj,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),d(vj,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),d(Tj,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),d(Fj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification"),d(Xe,"class","docstring"),d(or,"class","docstring"),d(G2,"id","transformers.AutoModelForMultipleChoice"),d(G2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G2,"href","#transformers.AutoModelForMultipleChoice"),d(ud,"class","relative group"),d(Jr,"class","docstring"),d(Cj,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),d(Mj,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),d(Ej,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),d(yj,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),d(wj,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),d(Aj,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),d(Lj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice"),d(Bj,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),d(xj,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),d(kj,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),d(Rj,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),d(Sj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),d(Pj,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),d($j,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),d(Ij,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),d(jj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),d(Nj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),d(Dj,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),d(qj,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),d(Oj,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),d(Gj,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),d(Xj,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),d(Vj,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),d(zj,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),d(Wj,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),d(Qj,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),d(Hj,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),d(Uj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice"),d(Ve,"class","docstring"),d(rr,"class","docstring"),d(bv,"id","transformers.AutoModelForNextSentencePrediction"),d(bv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bv,"href","#transformers.AutoModelForNextSentencePrediction"),d(bd,"class","relative group"),d(Yr,"class","docstring"),d(Jj,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),d(Yj,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),d(Kj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),d(Zj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),d(eN,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),d(ze,"class","docstring"),d(tr,"class","docstring"),d(yv,"id","transformers.AutoModelForTokenClassification"),d(yv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yv,"href","#transformers.AutoModelForTokenClassification"),d(Fd,"class","relative group"),d(Kr,"class","docstring"),d(oN,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),d(rN,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),d(tN,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),d(aN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),d(sN,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),d(nN,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),d(lN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification"),d(iN,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),d(dN,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),d(cN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),d(mN,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),d(fN,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),d(gN,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),d(hN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),d(uN,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),d(pN,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),d(_N,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),d(bN,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),d(vN,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),d(TN,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),d(FN,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),d(CN,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),d(MN,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),d(EN,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),d(yN,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),d(wN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),d(AN,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),d(LN,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),d(BN,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),d(xN,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),d(kN,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),d(RN,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),d(SN,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification"),d(We,"class","docstring"),d(ar,"class","docstring"),d(nT,"id","transformers.AutoModelForQuestionAnswering"),d(nT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(nT,"href","#transformers.AutoModelForQuestionAnswering"),d(Ed,"class","relative group"),d(Zr,"class","docstring"),d(PN,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),d($N,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),d(IN,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),d(jN,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),d(NN,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),d(DN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),d(qN,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),d(ON,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),d(GN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering"),d(XN,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),d(VN,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),d(zN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),d(WN,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),d(QN,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),d(HN,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),d(UN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),d(JN,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),d(YN,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),d(KN,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),d(ZN,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),d(eD,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),d(oD,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),d(rD,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),d(tD,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),d(aD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),d(sD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),d(nD,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),d(lD,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),d(iD,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),d(dD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),d(cD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),d(mD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),d(fD,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),d(gD,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),d(hD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),d(uD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),d(pD,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),d(_D,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),d(bD,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),d(Qe,"class","docstring"),d(sr,"class","docstring"),d(QT,"id","transformers.AutoModelForTableQuestionAnswering"),d(QT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(QT,"href","#transformers.AutoModelForTableQuestionAnswering"),d(Ad,"class","relative group"),d(et,"class","docstring"),d(vD,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),d(He,"class","docstring"),d(nr,"class","docstring"),d(JT,"id","transformers.AutoModelForImageClassification"),d(JT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(JT,"href","#transformers.AutoModelForImageClassification"),d(xd,"class","relative group"),d(ot,"class","docstring"),d(TD,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),d(FD,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification"),d(CD,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),d(MD,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),d(ED,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),d(yD,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(wD,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d(AD,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(LD,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),d(BD,"href","/docs/transformers/master/en/model_doc/resnet#transformers.ResNetForImageClassification"),d(xD,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),d(kD,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification"),d(RD,"href","/docs/transformers/master/en/model_doc/van#transformers.VanForImageClassification"),d(SD,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),d(Ue,"class","docstring"),d(lr,"class","docstring"),d(l1,"id","transformers.AutoModelForVision2Seq"),d(l1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(l1,"href","#transformers.AutoModelForVision2Seq"),d(Sd,"class","relative group"),d(rt,"class","docstring"),d(PD,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),d(Je,"class","docstring"),d(ir,"class","docstring"),d(c1,"id","transformers.AutoModelForAudioClassification"),d(c1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(c1,"href","#transformers.AutoModelForAudioClassification"),d(Id,"class","relative group"),d(tt,"class","docstring"),d($D,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification"),d(ID,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),d(jD,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),d(ND,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),d(DD,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),d(qD,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),d(OD,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),d(GD,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),d(Ye,"class","docstring"),d(dr,"class","docstring"),d(T1,"id","transformers.AutoModelForAudioFrameClassification"),d(T1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(T1,"href","#transformers.AutoModelForAudioFrameClassification"),d(Dd,"class","relative group"),d(at,"class","docstring"),d(XD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification"),d(VD,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),d(zD,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),d(WD,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),d(Ke,"class","docstring"),d(cr,"class","docstring"),d(w1,"id","transformers.AutoModelForCTC"),d(w1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w1,"href","#transformers.AutoModelForCTC"),d(Gd,"class","relative group"),d(st,"class","docstring"),d(QD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC"),d(HD,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),d(UD,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),d(JD,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC"),d(YD,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),d(KD,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),d(ZD,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),d(eq,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC"),d(Ze,"class","docstring"),d(mr,"class","docstring"),d(I1,"id","transformers.AutoModelForSpeechSeq2Seq"),d(I1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(I1,"href","#transformers.AutoModelForSpeechSeq2Seq"),d(zd,"class","relative group"),d(nt,"class","docstring"),d(oq,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),d(rq,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),d(eo,"class","docstring"),d(fr,"class","docstring"),d(q1,"id","transformers.AutoModelForAudioXVector"),d(q1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(q1,"href","#transformers.AutoModelForAudioXVector"),d(Hd,"class","relative group"),d(lt,"class","docstring"),d(tq,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector"),d(aq,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),d(sq,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),d(nq,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector"),d(oo,"class","docstring"),d(gr,"class","docstring"),d(W1,"id","transformers.AutoModelForMaskedImageModeling"),d(W1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(W1,"href","#transformers.AutoModelForMaskedImageModeling"),d(Yd,"class","relative group"),d(it,"class","docstring"),d(lq,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),d(iq,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),d(dq,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),d(ro,"class","docstring"),d(hr,"class","docstring"),d(Y1,"id","transformers.AutoModelForObjectDetection"),d(Y1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y1,"href","#transformers.AutoModelForObjectDetection"),d(oc,"class","relative group"),d(dt,"class","docstring"),d(cq,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),d(to,"class","docstring"),d(ur,"class","docstring"),d(eF,"id","transformers.AutoModelForImageSegmentation"),d(eF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(eF,"href","#transformers.AutoModelForImageSegmentation"),d(ac,"class","relative group"),d(ct,"class","docstring"),d(mq,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),d(ao,"class","docstring"),d(pr,"class","docstring"),d(tF,"id","transformers.AutoModelForSemanticSegmentation"),d(tF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tF,"href","#transformers.AutoModelForSemanticSegmentation"),d(lc,"class","relative group"),d(mt,"class","docstring"),d(fq,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),d(gq,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),d(so,"class","docstring"),d(_r,"class","docstring"),d(lF,"id","transformers.AutoModelForInstanceSegmentation"),d(lF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lF,"href","#transformers.AutoModelForInstanceSegmentation"),d(cc,"class","relative group"),d(ft,"class","docstring"),d(hq,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation"),d(no,"class","docstring"),d(br,"class","docstring"),d(cF,"id","transformers.TFAutoModel"),d(cF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(cF,"href","#transformers.TFAutoModel"),d(gc,"class","relative group"),d(gt,"class","docstring"),d(uq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),d(pq,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),d(_q,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),d(bq,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),d(vq,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),d(Tq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),d(Fq,"href","/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel"),d(Cq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),d(Mq,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel"),d(Eq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),d(yq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),d(wq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),d(Aq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),d(Lq,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(Bq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),d(xq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),d(kq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),d(Rq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),d(Sq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),d(Pq,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),d($q,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),d(Iq,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),d(jq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),d(Nq,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),d(Dq,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),d(qq,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),d(Oq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),d(Gq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),d(Xq,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),d(Vq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),d(zq,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),d(Wq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),d(Qq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),d(Hq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),d(Uq,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),d(Jq,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),d(Yq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),d(Kq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),d(Zq,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),d(eO,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),d(oO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),d(rO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),d(tO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),d(ho,"class","docstring"),d(vr,"class","docstring"),d(KF,"id","transformers.TFAutoModelForPreTraining"),d(KF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(KF,"href","#transformers.TFAutoModelForPreTraining"),d(pc,"class","relative group"),d(ht,"class","docstring"),d(aO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),d(sO,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(nO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),d(lO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(iO,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(dO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(cO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),d(mO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(fO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),d(gO,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(hO,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(uO,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),d(pO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),d(_O,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(bO,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(vO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(TO,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(FO,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(CO,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(MO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(EO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(yO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(uo,"class","docstring"),d(Tr,"class","docstring"),d(FC,"id","transformers.TFAutoModelForCausalLM"),d(FC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(FC,"href","#transformers.TFAutoModelForCausalLM"),d(vc,"class","relative group"),d(ut,"class","docstring"),d(wO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),d(AO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForCausalLM"),d(LO,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(BO,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(xO,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(kO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),d(RO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),d(SO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),d(PO,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d($O,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(IO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(po,"class","docstring"),d(Fr,"class","docstring"),d(SC,"id","transformers.TFAutoModelForImageClassification"),d(SC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(SC,"href","#transformers.TFAutoModelForImageClassification"),d(Cc,"class","relative group"),d(pt,"class","docstring"),d(jO,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),d(NO,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),d(_o,"class","docstring"),d(Cr,"class","docstring"),d(IC,"id","transformers.TFAutoModelForMaskedLM"),d(IC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(IC,"href","#transformers.TFAutoModelForMaskedLM"),d(yc,"class","relative group"),d(_t,"class","docstring"),d(DO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),d(qO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),d(OO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(GO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),d(XO,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),d(VO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),d(zO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(WO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),d(QO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(HO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),d(UO,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(JO,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),d(YO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),d(KO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(ZO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),d(eG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(oG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),d(rG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(tG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(aG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(bo,"class","docstring"),d(Mr,"class","docstring"),d(tM,"id","transformers.TFAutoModelForSeq2SeqLM"),d(tM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tM,"href","#transformers.TFAutoModelForSeq2SeqLM"),d(Lc,"class","relative group"),d(bt,"class","docstring"),d(sG,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(nG,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),d(lG,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),d(iG,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),d(dG,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),d(cG,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),d(mG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),d(fG,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),d(gG,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),d(hG,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(vo,"class","docstring"),d(Er,"class","docstring"),d(hM,"id","transformers.TFAutoModelForSequenceClassification"),d(hM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hM,"href","#transformers.TFAutoModelForSequenceClassification"),d(kc,"class","relative group"),d(vt,"class","docstring"),d(uG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),d(pG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),d(_G,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),d(bG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),d(vG,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),d(TG,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),d(FG,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),d(CG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),d(MG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),d(EG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),d(yG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),d(wG,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),d(AG,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),d(LG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),d(BG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),d(xG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),d(kG,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),d(RG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),d(SG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),d(PG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),d($G,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),d(IG,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),d(jG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),d(NG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),d(DG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),d(To,"class","docstring"),d(yr,"class","docstring"),d(qM,"id","transformers.TFAutoModelForMultipleChoice"),d(qM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qM,"href","#transformers.TFAutoModelForMultipleChoice"),d(Pc,"class","relative group"),d(Tt,"class","docstring"),d(qG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),d(OG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),d(GG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),d(XG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),d(VG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),d(zG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),d(WG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),d(QG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),d(HG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),d(UG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),d(JG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),d(YG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),d(KG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),d(ZG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),d(eX,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),d(oX,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),d(rX,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),d(Fo,"class","docstring"),d(wr,"class","docstring"),d(a4,"id","transformers.TFAutoModelForTableQuestionAnswering"),d(a4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(a4,"href","#transformers.TFAutoModelForTableQuestionAnswering"),d(jc,"class","relative group"),d(Ft,"class","docstring"),d(tX,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),d(Co,"class","docstring"),d(Ar,"class","docstring"),d(n4,"id","transformers.TFAutoModelForTokenClassification"),d(n4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(n4,"href","#transformers.TFAutoModelForTokenClassification"),d(qc,"class","relative group"),d(Ct,"class","docstring"),d(aX,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),d(sX,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),d(nX,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),d(lX,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),d(iX,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),d(dX,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),d(cX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),d(mX,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),d(fX,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),d(gX,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),d(hX,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),d(uX,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),d(pX,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),d(_X,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),d(bX,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),d(vX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),d(TX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),d(FX,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),d(CX,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),d(MX,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),d(Mo,"class","docstring"),d(Lr,"class","docstring"),d(A4,"id","transformers.TFAutoModelForQuestionAnswering"),d(A4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(A4,"href","#transformers.TFAutoModelForQuestionAnswering"),d(Xc,"class","relative group"),d(Mt,"class","docstring"),d(EX,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),d(yX,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),d(wX,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),d(AX,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),d(LX,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),d(BX,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),d(xX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),d(kX,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),d(RX,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),d(SX,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),d(PX,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),d($X,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),d(IX,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),d(jX,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),d(NX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),d(DX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),d(qX,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),d(OX,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),d(GX,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),d(Eo,"class","docstring"),d(Br,"class","docstring"),d(Q4,"id","transformers.TFAutoModelForVision2Seq"),d(Q4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Q4,"href","#transformers.TFAutoModelForVision2Seq"),d(Wc,"class","relative group"),d(Et,"class","docstring"),d(XX,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),d(yo,"class","docstring"),d(xr,"class","docstring"),d(U4,"id","transformers.TFAutoModelForSpeechSeq2Seq"),d(U4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(U4,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),d(Uc,"class","relative group"),d(yt,"class","docstring"),d(VX,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),d(wo,"class","docstring"),d(kr,"class","docstring"),d(Y4,"id","transformers.FlaxAutoModel"),d(Y4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y4,"href","#transformers.FlaxAutoModel"),d(Kc,"class","relative group"),d(wt,"class","docstring"),d(zX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),d(WX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),d(QX,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),d(HX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),d(UX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),d(JX,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),d(YX,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),d(KX,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),d(ZX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),d(eV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),d(oV,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),d(rV,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),d(tV,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),d(aV,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),d(sV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),d(nV,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),d(lV,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),d(iV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),d(dV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel"),d(cV,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),d(mV,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),d(fV,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),d(gV,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),d(hV,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel"),d(uV,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaModel"),d(Ao,"class","docstring"),d(Rr,"class","docstring"),d(ME,"id","transformers.FlaxAutoModelForCausalLM"),d(ME,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ME,"href","#transformers.FlaxAutoModelForCausalLM"),d(om,"class","relative group"),d(At,"class","docstring"),d(pV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForCausalLM"),d(_V,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),d(bV,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),d(vV,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),d(TV,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),d(Lo,"class","docstring"),d(Sr,"class","docstring"),d(BE,"id","transformers.FlaxAutoModelForPreTraining"),d(BE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(BE,"href","#transformers.FlaxAutoModelForPreTraining"),d(am,"class","relative group"),d(Lt,"class","docstring"),d(FV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),d(CV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(MV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),d(EV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),d(yV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),d(wV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(AV,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(LV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(BV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(xV,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(kV,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),d(RV,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM"),d(Bo,"class","docstring"),d(Pr,"class","docstring"),d(GE,"id","transformers.FlaxAutoModelForMaskedLM"),d(GE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(GE,"href","#transformers.FlaxAutoModelForMaskedLM"),d(lm,"class","relative group"),d(Bt,"class","docstring"),d(SV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),d(PV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d($V,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),d(IV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),d(jV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),d(NV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),d(DV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(qV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(OV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(GV,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM"),d(xo,"class","docstring"),d($r,"class","docstring"),d(ZE,"id","transformers.FlaxAutoModelForSeq2SeqLM"),d(ZE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ZE,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),d(cm,"class","relative group"),d(xt,"class","docstring"),d(XV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(VV,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),d(zV,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),d(WV,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),d(QV,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),d(HV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(UV,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(JV,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),d(YV,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(ko,"class","docstring"),d(Ir,"class","docstring"),d(d3,"id","transformers.FlaxAutoModelForSequenceClassification"),d(d3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(d3,"href","#transformers.FlaxAutoModelForSequenceClassification"),d(gm,"class","relative group"),d(kt,"class","docstring"),d(KV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),d(ZV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),d(ez,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),d(oz,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),d(rz,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),d(tz,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),d(az,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),d(sz,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),d(nz,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),d(lz,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForSequenceClassification"),d(Ro,"class","docstring"),d(jr,"class","docstring"),d(T3,"id","transformers.FlaxAutoModelForQuestionAnswering"),d(T3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(T3,"href","#transformers.FlaxAutoModelForQuestionAnswering"),d(pm,"class","relative group"),d(Rt,"class","docstring"),d(iz,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),d(dz,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),d(cz,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),d(mz,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),d(fz,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),d(gz,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),d(hz,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),d(uz,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),d(pz,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),d(_z,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForQuestionAnswering"),d(So,"class","docstring"),d(Nr,"class","docstring"),d(k3,"id","transformers.FlaxAutoModelForTokenClassification"),d(k3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(k3,"href","#transformers.FlaxAutoModelForTokenClassification"),d(vm,"class","relative group"),d(St,"class","docstring"),d(bz,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),d(vz,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),d(Tz,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),d(Fz,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),d(Cz,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),d(Mz,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),d(Ez,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),d(yz,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForTokenClassification"),d(Po,"class","docstring"),d(Dr,"class","docstring"),d(q3,"id","transformers.FlaxAutoModelForMultipleChoice"),d(q3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(q3,"href","#transformers.FlaxAutoModelForMultipleChoice"),d(Cm,"class","relative group"),d(Pt,"class","docstring"),d(wz,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),d(Az,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),d(Lz,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),d(Bz,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),d(xz,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),d(kz,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),d(Rz,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),d(Sz,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMultipleChoice"),d($o,"class","docstring"),d(qr,"class","docstring"),d(U3,"id","transformers.FlaxAutoModelForNextSentencePrediction"),d(U3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(U3,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),d(ym,"class","relative group"),d($t,"class","docstring"),d(Pz,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),d(Io,"class","docstring"),d(Or,"class","docstring"),d(Y3,"id","transformers.FlaxAutoModelForImageClassification"),d(Y3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y3,"href","#transformers.FlaxAutoModelForImageClassification"),d(Lm,"class","relative group"),d(It,"class","docstring"),d($z,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),d(Iz,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),d(jo,"class","docstring"),d(Gr,"class","docstring"),d(e5,"id","transformers.FlaxAutoModelForVision2Seq"),d(e5,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(e5,"href","#transformers.FlaxAutoModelForVision2Seq"),d(km,"class","relative group"),d(jt,"class","docstring"),d(jz,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),d(No,"class","docstring"),d(Xr,"class","docstring")},m(c,_){e(document.head,J),b(c,$e,_),b(c,de,_),e(de,ue),e(ue,io),g(me,io,null),e(de,Me),e(de,Vo),e(Vo,ji),b(c,$m,_),b(c,ma,_),e(ma,Ni),e(ma,Di),e(Di,oy),e(ma,Im),b(c,Be,_),b(c,co,_),e(co,qi),e(co,qs),e(qs,ry),e(co,Os),e(co,Gs),e(Gs,ty),e(co,Oi),e(co,Xs),e(Xs,ay),e(co,Gi),b(c,jm,_),g(qa,c,_),b(c,mo,_),b(c,pe,_),e(pe,H8),e(pe,Xi),e(Xi,U8),e(pe,J8),b(c,zo,_),b(c,Oa,_),e(Oa,Y8),e(Oa,Nm),e(Nm,K8),e(Oa,KPe),b(c,Nxe,_),b(c,Vi,_),e(Vi,Dm),e(Dm,xQ),g(sy,xQ,null),e(Vi,ZPe),e(Vi,kQ),e(kQ,e$e),b(c,Dxe,_),b(c,Vs,_),e(Vs,o$e),e(Vs,RQ),e(RQ,r$e),e(Vs,t$e),e(Vs,SQ),e(SQ,a$e),e(Vs,s$e),b(c,qxe,_),g(ny,c,_),b(c,Oxe,_),b(c,Z8,_),e(Z8,n$e),b(c,Gxe,_),g(qm,c,_),b(c,Xxe,_),b(c,zi,_),e(zi,Om),e(Om,PQ),g(ly,PQ,null),e(zi,l$e),e(zi,$Q),e($Q,i$e),b(c,Vxe,_),b(c,Wo,_),g(iy,Wo,null),e(Wo,d$e),e(Wo,dy),e(dy,c$e),e(dy,e9),e(e9,m$e),e(dy,f$e),e(Wo,g$e),e(Wo,cy),e(cy,h$e),e(cy,IQ),e(IQ,u$e),e(cy,p$e),e(Wo,_$e),e(Wo,fo),g(my,fo,null),e(fo,b$e),e(fo,jQ),e(jQ,v$e),e(fo,T$e),e(fo,Wi),e(Wi,F$e),e(Wi,NQ),e(NQ,C$e),e(Wi,M$e),e(Wi,DQ),e(DQ,E$e),e(Wi,y$e),e(fo,w$e),e(fo,v),e(v,Gm),e(Gm,qQ),e(qQ,A$e),e(Gm,L$e),e(Gm,o9),e(o9,B$e),e(Gm,x$e),e(v,k$e),e(v,Xm),e(Xm,OQ),e(OQ,R$e),e(Xm,S$e),e(Xm,r9),e(r9,P$e),e(Xm,$$e),e(v,I$e),e(v,Vm),e(Vm,GQ),e(GQ,j$e),e(Vm,N$e),e(Vm,t9),e(t9,D$e),e(Vm,q$e),e(v,O$e),e(v,zm),e(zm,XQ),e(XQ,G$e),e(zm,X$e),e(zm,a9),e(a9,V$e),e(zm,z$e),e(v,W$e),e(v,Wm),e(Wm,VQ),e(VQ,Q$e),e(Wm,H$e),e(Wm,s9),e(s9,U$e),e(Wm,J$e),e(v,Y$e),e(v,Qm),e(Qm,zQ),e(zQ,K$e),e(Qm,Z$e),e(Qm,n9),e(n9,eIe),e(Qm,oIe),e(v,rIe),e(v,Hm),e(Hm,WQ),e(WQ,tIe),e(Hm,aIe),e(Hm,l9),e(l9,sIe),e(Hm,nIe),e(v,lIe),e(v,Um),e(Um,QQ),e(QQ,iIe),e(Um,dIe),e(Um,i9),e(i9,cIe),e(Um,mIe),e(v,fIe),e(v,Jm),e(Jm,HQ),e(HQ,gIe),e(Jm,hIe),e(Jm,d9),e(d9,uIe),e(Jm,pIe),e(v,_Ie),e(v,Ym),e(Ym,UQ),e(UQ,bIe),e(Ym,vIe),e(Ym,c9),e(c9,TIe),e(Ym,FIe),e(v,CIe),e(v,Km),e(Km,JQ),e(JQ,MIe),e(Km,EIe),e(Km,m9),e(m9,yIe),e(Km,wIe),e(v,AIe),e(v,Zm),e(Zm,YQ),e(YQ,LIe),e(Zm,BIe),e(Zm,f9),e(f9,xIe),e(Zm,kIe),e(v,RIe),e(v,ef),e(ef,KQ),e(KQ,SIe),e(ef,PIe),e(ef,g9),e(g9,$Ie),e(ef,IIe),e(v,jIe),e(v,of),e(of,ZQ),e(ZQ,NIe),e(of,DIe),e(of,h9),e(h9,qIe),e(of,OIe),e(v,GIe),e(v,rf),e(rf,eH),e(eH,XIe),e(rf,VIe),e(rf,u9),e(u9,zIe),e(rf,WIe),e(v,QIe),e(v,tf),e(tf,oH),e(oH,HIe),e(tf,UIe),e(tf,p9),e(p9,JIe),e(tf,YIe),e(v,KIe),e(v,af),e(af,rH),e(rH,ZIe),e(af,eje),e(af,_9),e(_9,oje),e(af,rje),e(v,tje),e(v,sf),e(sf,tH),e(tH,aje),e(sf,sje),e(sf,b9),e(b9,nje),e(sf,lje),e(v,ije),e(v,nf),e(nf,aH),e(aH,dje),e(nf,cje),e(nf,v9),e(v9,mje),e(nf,fje),e(v,gje),e(v,lf),e(lf,sH),e(sH,hje),e(lf,uje),e(lf,T9),e(T9,pje),e(lf,_je),e(v,bje),e(v,df),e(df,nH),e(nH,vje),e(df,Tje),e(df,F9),e(F9,Fje),e(df,Cje),e(v,Mje),e(v,cf),e(cf,lH),e(lH,Eje),e(cf,yje),e(cf,C9),e(C9,wje),e(cf,Aje),e(v,Lje),e(v,mf),e(mf,iH),e(iH,Bje),e(mf,xje),e(mf,M9),e(M9,kje),e(mf,Rje),e(v,Sje),e(v,ff),e(ff,dH),e(dH,Pje),e(ff,$je),e(ff,E9),e(E9,Ije),e(ff,jje),e(v,Nje),e(v,gf),e(gf,cH),e(cH,Dje),e(gf,qje),e(gf,y9),e(y9,Oje),e(gf,Gje),e(v,Xje),e(v,hf),e(hf,mH),e(mH,Vje),e(hf,zje),e(hf,w9),e(w9,Wje),e(hf,Qje),e(v,Hje),e(v,uf),e(uf,fH),e(fH,Uje),e(uf,Jje),e(uf,A9),e(A9,Yje),e(uf,Kje),e(v,Zje),e(v,pf),e(pf,gH),e(gH,eNe),e(pf,oNe),e(pf,L9),e(L9,rNe),e(pf,tNe),e(v,aNe),e(v,_f),e(_f,hH),e(hH,sNe),e(_f,nNe),e(_f,B9),e(B9,lNe),e(_f,iNe),e(v,dNe),e(v,bf),e(bf,uH),e(uH,cNe),e(bf,mNe),e(bf,x9),e(x9,fNe),e(bf,gNe),e(v,hNe),e(v,vf),e(vf,pH),e(pH,uNe),e(vf,pNe),e(vf,k9),e(k9,_Ne),e(vf,bNe),e(v,vNe),e(v,Tf),e(Tf,_H),e(_H,TNe),e(Tf,FNe),e(Tf,R9),e(R9,CNe),e(Tf,MNe),e(v,ENe),e(v,Ff),e(Ff,bH),e(bH,yNe),e(Ff,wNe),e(Ff,S9),e(S9,ANe),e(Ff,LNe),e(v,BNe),e(v,Cf),e(Cf,vH),e(vH,xNe),e(Cf,kNe),e(Cf,P9),e(P9,RNe),e(Cf,SNe),e(v,PNe),e(v,Mf),e(Mf,TH),e(TH,$Ne),e(Mf,INe),e(Mf,$9),e($9,jNe),e(Mf,NNe),e(v,DNe),e(v,Ef),e(Ef,FH),e(FH,qNe),e(Ef,ONe),e(Ef,I9),e(I9,GNe),e(Ef,XNe),e(v,VNe),e(v,yf),e(yf,CH),e(CH,zNe),e(yf,WNe),e(yf,j9),e(j9,QNe),e(yf,HNe),e(v,UNe),e(v,wf),e(wf,MH),e(MH,JNe),e(wf,YNe),e(wf,N9),e(N9,KNe),e(wf,ZNe),e(v,eDe),e(v,Af),e(Af,EH),e(EH,oDe),e(Af,rDe),e(Af,D9),e(D9,tDe),e(Af,aDe),e(v,sDe),e(v,Lf),e(Lf,yH),e(yH,nDe),e(Lf,lDe),e(Lf,q9),e(q9,iDe),e(Lf,dDe),e(v,cDe),e(v,Bf),e(Bf,wH),e(wH,mDe),e(Bf,fDe),e(Bf,O9),e(O9,gDe),e(Bf,hDe),e(v,uDe),e(v,xf),e(xf,AH),e(AH,pDe),e(xf,_De),e(xf,G9),e(G9,bDe),e(xf,vDe),e(v,TDe),e(v,kf),e(kf,LH),e(LH,FDe),e(kf,CDe),e(kf,X9),e(X9,MDe),e(kf,EDe),e(v,yDe),e(v,Rf),e(Rf,BH),e(BH,wDe),e(Rf,ADe),e(Rf,V9),e(V9,LDe),e(Rf,BDe),e(v,xDe),e(v,Sf),e(Sf,xH),e(xH,kDe),e(Sf,RDe),e(Sf,z9),e(z9,SDe),e(Sf,PDe),e(v,$De),e(v,Pf),e(Pf,kH),e(kH,IDe),e(Pf,jDe),e(Pf,W9),e(W9,NDe),e(Pf,DDe),e(v,qDe),e(v,$f),e($f,RH),e(RH,ODe),e($f,GDe),e($f,Q9),e(Q9,XDe),e($f,VDe),e(v,zDe),e(v,If),e(If,SH),e(SH,WDe),e(If,QDe),e(If,H9),e(H9,HDe),e(If,UDe),e(v,JDe),e(v,jf),e(jf,PH),e(PH,YDe),e(jf,KDe),e(jf,U9),e(U9,ZDe),e(jf,eqe),e(v,oqe),e(v,Nf),e(Nf,$H),e($H,rqe),e(Nf,tqe),e(Nf,J9),e(J9,aqe),e(Nf,sqe),e(v,nqe),e(v,Df),e(Df,IH),e(IH,lqe),e(Df,iqe),e(Df,Y9),e(Y9,dqe),e(Df,cqe),e(v,mqe),e(v,qf),e(qf,jH),e(jH,fqe),e(qf,gqe),e(qf,K9),e(K9,hqe),e(qf,uqe),e(v,pqe),e(v,Of),e(Of,NH),e(NH,_qe),e(Of,bqe),e(Of,Z9),e(Z9,vqe),e(Of,Tqe),e(v,Fqe),e(v,Gf),e(Gf,DH),e(DH,Cqe),e(Gf,Mqe),e(Gf,eB),e(eB,Eqe),e(Gf,yqe),e(v,wqe),e(v,Xf),e(Xf,qH),e(qH,Aqe),e(Xf,Lqe),e(Xf,oB),e(oB,Bqe),e(Xf,xqe),e(v,kqe),e(v,Vf),e(Vf,OH),e(OH,Rqe),e(Vf,Sqe),e(Vf,rB),e(rB,Pqe),e(Vf,$qe),e(v,Iqe),e(v,zf),e(zf,GH),e(GH,jqe),e(zf,Nqe),e(zf,tB),e(tB,Dqe),e(zf,qqe),e(v,Oqe),e(v,Wf),e(Wf,XH),e(XH,Gqe),e(Wf,Xqe),e(Wf,aB),e(aB,Vqe),e(Wf,zqe),e(v,Wqe),e(v,Qf),e(Qf,VH),e(VH,Qqe),e(Qf,Hqe),e(Qf,sB),e(sB,Uqe),e(Qf,Jqe),e(v,Yqe),e(v,Hf),e(Hf,zH),e(zH,Kqe),e(Hf,Zqe),e(Hf,nB),e(nB,eOe),e(Hf,oOe),e(v,rOe),e(v,Uf),e(Uf,WH),e(WH,tOe),e(Uf,aOe),e(Uf,lB),e(lB,sOe),e(Uf,nOe),e(v,lOe),e(v,Jf),e(Jf,QH),e(QH,iOe),e(Jf,dOe),e(Jf,iB),e(iB,cOe),e(Jf,mOe),e(v,fOe),e(v,Yf),e(Yf,HH),e(HH,gOe),e(Yf,hOe),e(Yf,dB),e(dB,uOe),e(Yf,pOe),e(v,_Oe),e(v,Kf),e(Kf,UH),e(UH,bOe),e(Kf,vOe),e(Kf,cB),e(cB,TOe),e(Kf,FOe),e(v,COe),e(v,Zf),e(Zf,JH),e(JH,MOe),e(Zf,EOe),e(Zf,mB),e(mB,yOe),e(Zf,wOe),e(v,AOe),e(v,eg),e(eg,YH),e(YH,LOe),e(eg,BOe),e(eg,fB),e(fB,xOe),e(eg,kOe),e(v,ROe),e(v,og),e(og,KH),e(KH,SOe),e(og,POe),e(og,gB),e(gB,$Oe),e(og,IOe),e(v,jOe),e(v,rg),e(rg,ZH),e(ZH,NOe),e(rg,DOe),e(rg,hB),e(hB,qOe),e(rg,OOe),e(v,GOe),e(v,tg),e(tg,eU),e(eU,XOe),e(tg,VOe),e(tg,uB),e(uB,zOe),e(tg,WOe),e(v,QOe),e(v,ag),e(ag,oU),e(oU,HOe),e(ag,UOe),e(ag,pB),e(pB,JOe),e(ag,YOe),e(v,KOe),e(v,sg),e(sg,rU),e(rU,ZOe),e(sg,eGe),e(sg,_B),e(_B,oGe),e(sg,rGe),e(v,tGe),e(v,ng),e(ng,tU),e(tU,aGe),e(ng,sGe),e(ng,bB),e(bB,nGe),e(ng,lGe),e(v,iGe),e(v,lg),e(lg,aU),e(aU,dGe),e(lg,cGe),e(lg,vB),e(vB,mGe),e(lg,fGe),e(v,gGe),e(v,ig),e(ig,sU),e(sU,hGe),e(ig,uGe),e(ig,TB),e(TB,pGe),e(ig,_Ge),e(v,bGe),e(v,dg),e(dg,nU),e(nU,vGe),e(dg,TGe),e(dg,FB),e(FB,FGe),e(dg,CGe),e(v,MGe),e(v,cg),e(cg,lU),e(lU,EGe),e(cg,yGe),e(cg,CB),e(CB,wGe),e(cg,AGe),e(v,LGe),e(v,mg),e(mg,iU),e(iU,BGe),e(mg,xGe),e(mg,MB),e(MB,kGe),e(mg,RGe),e(v,SGe),e(v,fg),e(fg,dU),e(dU,PGe),e(fg,$Ge),e(fg,EB),e(EB,IGe),e(fg,jGe),e(v,NGe),e(v,gg),e(gg,cU),e(cU,DGe),e(gg,qGe),e(gg,yB),e(yB,OGe),e(gg,GGe),e(v,XGe),e(v,hg),e(hg,mU),e(mU,VGe),e(hg,zGe),e(hg,wB),e(wB,WGe),e(hg,QGe),e(v,HGe),e(v,ug),e(ug,fU),e(fU,UGe),e(ug,JGe),e(ug,AB),e(AB,YGe),e(ug,KGe),e(v,ZGe),e(v,pg),e(pg,gU),e(gU,eXe),e(pg,oXe),e(pg,LB),e(LB,rXe),e(pg,tXe),e(v,aXe),e(v,_g),e(_g,hU),e(hU,sXe),e(_g,nXe),e(_g,BB),e(BB,lXe),e(_g,iXe),e(v,dXe),e(v,bg),e(bg,uU),e(uU,cXe),e(bg,mXe),e(bg,xB),e(xB,fXe),e(bg,gXe),e(v,hXe),e(v,vg),e(vg,pU),e(pU,uXe),e(vg,pXe),e(vg,kB),e(kB,_Xe),e(vg,bXe),e(v,vXe),e(v,Tg),e(Tg,_U),e(_U,TXe),e(Tg,FXe),e(Tg,RB),e(RB,CXe),e(Tg,MXe),e(v,EXe),e(v,Fg),e(Fg,bU),e(bU,yXe),e(Fg,wXe),e(Fg,SB),e(SB,AXe),e(Fg,LXe),e(v,BXe),e(v,Cg),e(Cg,vU),e(vU,xXe),e(Cg,kXe),e(Cg,PB),e(PB,RXe),e(Cg,SXe),e(v,PXe),e(v,Mg),e(Mg,TU),e(TU,$Xe),e(Mg,IXe),e(Mg,$B),e($B,jXe),e(Mg,NXe),e(v,DXe),e(v,Eg),e(Eg,FU),e(FU,qXe),e(Eg,OXe),e(Eg,IB),e(IB,GXe),e(Eg,XXe),e(v,VXe),e(v,yg),e(yg,CU),e(CU,zXe),e(yg,WXe),e(yg,jB),e(jB,QXe),e(yg,HXe),e(v,UXe),e(v,wg),e(wg,MU),e(MU,JXe),e(wg,YXe),e(wg,NB),e(NB,KXe),e(wg,ZXe),e(v,eVe),e(v,Ag),e(Ag,EU),e(EU,oVe),e(Ag,rVe),e(Ag,DB),e(DB,tVe),e(Ag,aVe),e(v,sVe),e(v,Lg),e(Lg,yU),e(yU,nVe),e(Lg,lVe),e(Lg,qB),e(qB,iVe),e(Lg,dVe),e(v,cVe),e(v,Bg),e(Bg,wU),e(wU,mVe),e(Bg,fVe),e(Bg,OB),e(OB,gVe),e(Bg,hVe),e(v,uVe),e(v,xg),e(xg,AU),e(AU,pVe),e(xg,_Ve),e(xg,GB),e(GB,bVe),e(xg,vVe),e(fo,TVe),e(fo,LU),e(LU,FVe),e(fo,CVe),g(fy,fo,null),e(Wo,MVe),e(Wo,kg),g(gy,kg,null),e(kg,EVe),e(kg,BU),e(BU,yVe),b(c,zxe,_),b(c,Qi,_),e(Qi,Rg),e(Rg,xU),g(hy,xU,null),e(Qi,wVe),e(Qi,kU),e(kU,AVe),b(c,Wxe,_),b(c,Qo,_),g(uy,Qo,null),e(Qo,LVe),e(Qo,py),e(py,BVe),e(py,XB),e(XB,xVe),e(py,kVe),e(Qo,RVe),e(Qo,_y),e(_y,SVe),e(_y,RU),e(RU,PVe),e(_y,$Ve),e(Qo,IVe),e(Qo,go),g(by,go,null),e(go,jVe),e(go,SU),e(SU,NVe),e(go,DVe),e(go,Ga),e(Ga,qVe),e(Ga,PU),e(PU,OVe),e(Ga,GVe),e(Ga,$U),e($U,XVe),e(Ga,VVe),e(Ga,IU),e(IU,zVe),e(Ga,WVe),e(go,QVe),e(go,E),e(E,zs),e(zs,jU),e(jU,HVe),e(zs,UVe),e(zs,VB),e(VB,JVe),e(zs,YVe),e(zs,zB),e(zB,KVe),e(zs,ZVe),e(E,eze),e(E,Ws),e(Ws,NU),e(NU,oze),e(Ws,rze),e(Ws,WB),e(WB,tze),e(Ws,aze),e(Ws,QB),e(QB,sze),e(Ws,nze),e(E,lze),e(E,Qs),e(Qs,DU),e(DU,ize),e(Qs,dze),e(Qs,HB),e(HB,cze),e(Qs,mze),e(Qs,UB),e(UB,fze),e(Qs,gze),e(E,hze),e(E,Sg),e(Sg,qU),e(qU,uze),e(Sg,pze),e(Sg,JB),e(JB,_ze),e(Sg,bze),e(E,vze),e(E,Hs),e(Hs,OU),e(OU,Tze),e(Hs,Fze),e(Hs,YB),e(YB,Cze),e(Hs,Mze),e(Hs,KB),e(KB,Eze),e(Hs,yze),e(E,wze),e(E,Pg),e(Pg,GU),e(GU,Aze),e(Pg,Lze),e(Pg,ZB),e(ZB,Bze),e(Pg,xze),e(E,kze),e(E,$g),e($g,XU),e(XU,Rze),e($g,Sze),e($g,ex),e(ex,Pze),e($g,$ze),e(E,Ize),e(E,Ig),e(Ig,VU),e(VU,jze),e(Ig,Nze),e(Ig,ox),e(ox,Dze),e(Ig,qze),e(E,Oze),e(E,Us),e(Us,zU),e(zU,Gze),e(Us,Xze),e(Us,rx),e(rx,Vze),e(Us,zze),e(Us,tx),e(tx,Wze),e(Us,Qze),e(E,Hze),e(E,Js),e(Js,WU),e(WU,Uze),e(Js,Jze),e(Js,ax),e(ax,Yze),e(Js,Kze),e(Js,sx),e(sx,Zze),e(Js,eWe),e(E,oWe),e(E,Ys),e(Ys,QU),e(QU,rWe),e(Ys,tWe),e(Ys,nx),e(nx,aWe),e(Ys,sWe),e(Ys,lx),e(lx,nWe),e(Ys,lWe),e(E,iWe),e(E,jg),e(jg,HU),e(HU,dWe),e(jg,cWe),e(jg,ix),e(ix,mWe),e(jg,fWe),e(E,gWe),e(E,Ng),e(Ng,UU),e(UU,hWe),e(Ng,uWe),e(Ng,dx),e(dx,pWe),e(Ng,_We),e(E,bWe),e(E,Ks),e(Ks,JU),e(JU,vWe),e(Ks,TWe),e(Ks,cx),e(cx,FWe),e(Ks,CWe),e(Ks,mx),e(mx,MWe),e(Ks,EWe),e(E,yWe),e(E,Dg),e(Dg,YU),e(YU,wWe),e(Dg,AWe),e(Dg,fx),e(fx,LWe),e(Dg,BWe),e(E,xWe),e(E,Zs),e(Zs,KU),e(KU,kWe),e(Zs,RWe),e(Zs,gx),e(gx,SWe),e(Zs,PWe),e(Zs,hx),e(hx,$We),e(Zs,IWe),e(E,jWe),e(E,en),e(en,ZU),e(ZU,NWe),e(en,DWe),e(en,ux),e(ux,qWe),e(en,OWe),e(en,px),e(px,GWe),e(en,XWe),e(E,VWe),e(E,on),e(on,eJ),e(eJ,zWe),e(on,WWe),e(on,_x),e(_x,QWe),e(on,HWe),e(on,oJ),e(oJ,UWe),e(on,JWe),e(E,YWe),e(E,qg),e(qg,rJ),e(rJ,KWe),e(qg,ZWe),e(qg,bx),e(bx,eQe),e(qg,oQe),e(E,rQe),e(E,rn),e(rn,tJ),e(tJ,tQe),e(rn,aQe),e(rn,vx),e(vx,sQe),e(rn,nQe),e(rn,Tx),e(Tx,lQe),e(rn,iQe),e(E,dQe),e(E,Og),e(Og,aJ),e(aJ,cQe),e(Og,mQe),e(Og,Fx),e(Fx,fQe),e(Og,gQe),e(E,hQe),e(E,tn),e(tn,sJ),e(sJ,uQe),e(tn,pQe),e(tn,Cx),e(Cx,_Qe),e(tn,bQe),e(tn,Mx),e(Mx,vQe),e(tn,TQe),e(E,FQe),e(E,an),e(an,nJ),e(nJ,CQe),e(an,MQe),e(an,Ex),e(Ex,EQe),e(an,yQe),e(an,yx),e(yx,wQe),e(an,AQe),e(E,LQe),e(E,sn),e(sn,lJ),e(lJ,BQe),e(sn,xQe),e(sn,wx),e(wx,kQe),e(sn,RQe),e(sn,Ax),e(Ax,SQe),e(sn,PQe),e(E,$Qe),e(E,Gg),e(Gg,iJ),e(iJ,IQe),e(Gg,jQe),e(Gg,Lx),e(Lx,NQe),e(Gg,DQe),e(E,qQe),e(E,nn),e(nn,dJ),e(dJ,OQe),e(nn,GQe),e(nn,Bx),e(Bx,XQe),e(nn,VQe),e(nn,xx),e(xx,zQe),e(nn,WQe),e(E,QQe),e(E,Xg),e(Xg,cJ),e(cJ,HQe),e(Xg,UQe),e(Xg,kx),e(kx,JQe),e(Xg,YQe),e(E,KQe),e(E,ln),e(ln,mJ),e(mJ,ZQe),e(ln,eHe),e(ln,Rx),e(Rx,oHe),e(ln,rHe),e(ln,Sx),e(Sx,tHe),e(ln,aHe),e(E,sHe),e(E,dn),e(dn,fJ),e(fJ,nHe),e(dn,lHe),e(dn,Px),e(Px,iHe),e(dn,dHe),e(dn,$x),e($x,cHe),e(dn,mHe),e(E,fHe),e(E,cn),e(cn,gJ),e(gJ,gHe),e(cn,hHe),e(cn,Ix),e(Ix,uHe),e(cn,pHe),e(cn,jx),e(jx,_He),e(cn,bHe),e(E,vHe),e(E,mn),e(mn,hJ),e(hJ,THe),e(mn,FHe),e(mn,Nx),e(Nx,CHe),e(mn,MHe),e(mn,Dx),e(Dx,EHe),e(mn,yHe),e(E,wHe),e(E,Vg),e(Vg,uJ),e(uJ,AHe),e(Vg,LHe),e(Vg,qx),e(qx,BHe),e(Vg,xHe),e(E,kHe),e(E,fn),e(fn,pJ),e(pJ,RHe),e(fn,SHe),e(fn,Ox),e(Ox,PHe),e(fn,$He),e(fn,Gx),e(Gx,IHe),e(fn,jHe),e(E,NHe),e(E,gn),e(gn,_J),e(_J,DHe),e(gn,qHe),e(gn,Xx),e(Xx,OHe),e(gn,GHe),e(gn,Vx),e(Vx,XHe),e(gn,VHe),e(E,zHe),e(E,hn),e(hn,bJ),e(bJ,WHe),e(hn,QHe),e(hn,zx),e(zx,HHe),e(hn,UHe),e(hn,Wx),e(Wx,JHe),e(hn,YHe),e(E,KHe),e(E,un),e(un,vJ),e(vJ,ZHe),e(un,eUe),e(un,Qx),e(Qx,oUe),e(un,rUe),e(un,Hx),e(Hx,tUe),e(un,aUe),e(E,sUe),e(E,pn),e(pn,TJ),e(TJ,nUe),e(pn,lUe),e(pn,Ux),e(Ux,iUe),e(pn,dUe),e(pn,Jx),e(Jx,cUe),e(pn,mUe),e(E,fUe),e(E,_n),e(_n,FJ),e(FJ,gUe),e(_n,hUe),e(_n,Yx),e(Yx,uUe),e(_n,pUe),e(_n,Kx),e(Kx,_Ue),e(_n,bUe),e(E,vUe),e(E,zg),e(zg,CJ),e(CJ,TUe),e(zg,FUe),e(zg,Zx),e(Zx,CUe),e(zg,MUe),e(E,EUe),e(E,bn),e(bn,MJ),e(MJ,yUe),e(bn,wUe),e(bn,ek),e(ek,AUe),e(bn,LUe),e(bn,ok),e(ok,BUe),e(bn,xUe),e(E,kUe),e(E,Wg),e(Wg,EJ),e(EJ,RUe),e(Wg,SUe),e(Wg,rk),e(rk,PUe),e(Wg,$Ue),e(E,IUe),e(E,Qg),e(Qg,yJ),e(yJ,jUe),e(Qg,NUe),e(Qg,tk),e(tk,DUe),e(Qg,qUe),e(E,OUe),e(E,vn),e(vn,wJ),e(wJ,GUe),e(vn,XUe),e(vn,ak),e(ak,VUe),e(vn,zUe),e(vn,sk),e(sk,WUe),e(vn,QUe),e(E,HUe),e(E,Tn),e(Tn,AJ),e(AJ,UUe),e(Tn,JUe),e(Tn,nk),e(nk,YUe),e(Tn,KUe),e(Tn,lk),e(lk,ZUe),e(Tn,eJe),e(E,oJe),e(E,Hg),e(Hg,LJ),e(LJ,rJe),e(Hg,tJe),e(Hg,ik),e(ik,aJe),e(Hg,sJe),e(E,nJe),e(E,Fn),e(Fn,BJ),e(BJ,lJe),e(Fn,iJe),e(Fn,dk),e(dk,dJe),e(Fn,cJe),e(Fn,ck),e(ck,mJe),e(Fn,fJe),e(E,gJe),e(E,Cn),e(Cn,xJ),e(xJ,hJe),e(Cn,uJe),e(Cn,mk),e(mk,pJe),e(Cn,_Je),e(Cn,fk),e(fk,bJe),e(Cn,vJe),e(E,TJe),e(E,Mn),e(Mn,kJ),e(kJ,FJe),e(Mn,CJe),e(Mn,gk),e(gk,MJe),e(Mn,EJe),e(Mn,hk),e(hk,yJe),e(Mn,wJe),e(E,AJe),e(E,En),e(En,RJ),e(RJ,LJe),e(En,BJe),e(En,uk),e(uk,xJe),e(En,kJe),e(En,pk),e(pk,RJe),e(En,SJe),e(E,PJe),e(E,yn),e(yn,SJ),e(SJ,$Je),e(yn,IJe),e(yn,_k),e(_k,jJe),e(yn,NJe),e(yn,bk),e(bk,DJe),e(yn,qJe),e(E,OJe),e(E,Ug),e(Ug,PJ),e(PJ,GJe),e(Ug,XJe),e(Ug,vk),e(vk,VJe),e(Ug,zJe),e(E,WJe),e(E,Jg),e(Jg,$J),e($J,QJe),e(Jg,HJe),e(Jg,Tk),e(Tk,UJe),e(Jg,JJe),e(E,YJe),e(E,Yg),e(Yg,IJ),e(IJ,KJe),e(Yg,ZJe),e(Yg,Fk),e(Fk,eYe),e(Yg,oYe),e(E,rYe),e(E,Kg),e(Kg,jJ),e(jJ,tYe),e(Kg,aYe),e(Kg,Ck),e(Ck,sYe),e(Kg,nYe),e(E,lYe),e(E,wn),e(wn,NJ),e(NJ,iYe),e(wn,dYe),e(wn,Mk),e(Mk,cYe),e(wn,mYe),e(wn,Ek),e(Ek,fYe),e(wn,gYe),e(E,hYe),e(E,Zg),e(Zg,DJ),e(DJ,uYe),e(Zg,pYe),e(Zg,yk),e(yk,_Ye),e(Zg,bYe),e(E,vYe),e(E,An),e(An,qJ),e(qJ,TYe),e(An,FYe),e(An,wk),e(wk,CYe),e(An,MYe),e(An,Ak),e(Ak,EYe),e(An,yYe),e(E,wYe),e(E,Ln),e(Ln,OJ),e(OJ,AYe),e(Ln,LYe),e(Ln,Lk),e(Lk,BYe),e(Ln,xYe),e(Ln,Bk),e(Bk,kYe),e(Ln,RYe),e(E,SYe),e(E,Bn),e(Bn,GJ),e(GJ,PYe),e(Bn,$Ye),e(Bn,xk),e(xk,IYe),e(Bn,jYe),e(Bn,kk),e(kk,NYe),e(Bn,DYe),e(E,qYe),e(E,xn),e(xn,XJ),e(XJ,OYe),e(xn,GYe),e(xn,Rk),e(Rk,XYe),e(xn,VYe),e(xn,Sk),e(Sk,zYe),e(xn,WYe),e(E,QYe),e(E,kn),e(kn,VJ),e(VJ,HYe),e(kn,UYe),e(kn,Pk),e(Pk,JYe),e(kn,YYe),e(kn,$k),e($k,KYe),e(kn,ZYe),e(E,eKe),e(E,Rn),e(Rn,zJ),e(zJ,oKe),e(Rn,rKe),e(Rn,Ik),e(Ik,tKe),e(Rn,aKe),e(Rn,jk),e(jk,sKe),e(Rn,nKe),e(E,lKe),e(E,eh),e(eh,WJ),e(WJ,iKe),e(eh,dKe),e(eh,Nk),e(Nk,cKe),e(eh,mKe),e(E,fKe),e(E,oh),e(oh,QJ),e(QJ,gKe),e(oh,hKe),e(oh,Dk),e(Dk,uKe),e(oh,pKe),e(E,_Ke),e(E,Sn),e(Sn,HJ),e(HJ,bKe),e(Sn,vKe),e(Sn,qk),e(qk,TKe),e(Sn,FKe),e(Sn,Ok),e(Ok,CKe),e(Sn,MKe),e(E,EKe),e(E,Pn),e(Pn,UJ),e(UJ,yKe),e(Pn,wKe),e(Pn,Gk),e(Gk,AKe),e(Pn,LKe),e(Pn,Xk),e(Xk,BKe),e(Pn,xKe),e(E,kKe),e(E,$n),e($n,JJ),e(JJ,RKe),e($n,SKe),e($n,Vk),e(Vk,PKe),e($n,$Ke),e($n,zk),e(zk,IKe),e($n,jKe),e(E,NKe),e(E,rh),e(rh,YJ),e(YJ,DKe),e(rh,qKe),e(rh,Wk),e(Wk,OKe),e(rh,GKe),e(E,XKe),e(E,th),e(th,KJ),e(KJ,VKe),e(th,zKe),e(th,Qk),e(Qk,WKe),e(th,QKe),e(E,HKe),e(E,ah),e(ah,ZJ),e(ZJ,UKe),e(ah,JKe),e(ah,Hk),e(Hk,YKe),e(ah,KKe),e(E,ZKe),e(E,sh),e(sh,eY),e(eY,eZe),e(sh,oZe),e(sh,Uk),e(Uk,rZe),e(sh,tZe),e(E,aZe),e(E,In),e(In,oY),e(oY,sZe),e(In,nZe),e(In,Jk),e(Jk,lZe),e(In,iZe),e(In,Yk),e(Yk,dZe),e(In,cZe),e(E,mZe),e(E,nh),e(nh,rY),e(rY,fZe),e(nh,gZe),e(nh,Kk),e(Kk,hZe),e(nh,uZe),e(E,pZe),e(E,lh),e(lh,tY),e(tY,_Ze),e(lh,bZe),e(lh,Zk),e(Zk,vZe),e(lh,TZe),e(E,FZe),e(E,jn),e(jn,aY),e(aY,CZe),e(jn,MZe),e(jn,eR),e(eR,EZe),e(jn,yZe),e(jn,oR),e(oR,wZe),e(jn,AZe),e(E,LZe),e(E,Nn),e(Nn,sY),e(sY,BZe),e(Nn,xZe),e(Nn,rR),e(rR,kZe),e(Nn,RZe),e(Nn,tR),e(tR,SZe),e(Nn,PZe),e(go,$Ze),e(go,nY),e(nY,IZe),e(go,jZe),g(vy,go,null),e(Qo,NZe),e(Qo,ih),g(Ty,ih,null),e(ih,DZe),e(ih,lY),e(lY,qZe),b(c,Qxe,_),b(c,Hi,_),e(Hi,dh),e(dh,iY),g(Fy,iY,null),e(Hi,OZe),e(Hi,dY),e(dY,GZe),b(c,Hxe,_),b(c,Ho,_),g(Cy,Ho,null),e(Ho,XZe),e(Ho,My),e(My,VZe),e(My,aR),e(aR,zZe),e(My,WZe),e(Ho,QZe),e(Ho,Ey),e(Ey,HZe),e(Ey,cY),e(cY,UZe),e(Ey,JZe),e(Ho,YZe),e(Ho,Ie),g(yy,Ie,null),e(Ie,KZe),e(Ie,mY),e(mY,ZZe),e(Ie,eeo),e(Ie,Xa),e(Xa,oeo),e(Xa,fY),e(fY,reo),e(Xa,teo),e(Xa,gY),e(gY,aeo),e(Xa,seo),e(Xa,hY),e(hY,neo),e(Xa,leo),e(Ie,ieo),e(Ie,re),e(re,ch),e(ch,uY),e(uY,deo),e(ch,ceo),e(ch,sR),e(sR,meo),e(ch,feo),e(re,geo),e(re,mh),e(mh,pY),e(pY,heo),e(mh,ueo),e(mh,nR),e(nR,peo),e(mh,_eo),e(re,beo),e(re,fh),e(fh,_Y),e(_Y,veo),e(fh,Teo),e(fh,lR),e(lR,Feo),e(fh,Ceo),e(re,Meo),e(re,gh),e(gh,bY),e(bY,Eeo),e(gh,yeo),e(gh,iR),e(iR,weo),e(gh,Aeo),e(re,Leo),e(re,hh),e(hh,vY),e(vY,Beo),e(hh,xeo),e(hh,dR),e(dR,keo),e(hh,Reo),e(re,Seo),e(re,uh),e(uh,TY),e(TY,Peo),e(uh,$eo),e(uh,cR),e(cR,Ieo),e(uh,jeo),e(re,Neo),e(re,ph),e(ph,FY),e(FY,Deo),e(ph,qeo),e(ph,mR),e(mR,Oeo),e(ph,Geo),e(re,Xeo),e(re,_h),e(_h,CY),e(CY,Veo),e(_h,zeo),e(_h,fR),e(fR,Weo),e(_h,Qeo),e(re,Heo),e(re,bh),e(bh,MY),e(MY,Ueo),e(bh,Jeo),e(bh,gR),e(gR,Yeo),e(bh,Keo),e(re,Zeo),e(re,vh),e(vh,EY),e(EY,eoo),e(vh,ooo),e(vh,hR),e(hR,roo),e(vh,too),e(re,aoo),e(re,Th),e(Th,yY),e(yY,soo),e(Th,noo),e(Th,uR),e(uR,loo),e(Th,ioo),e(re,doo),e(re,Fh),e(Fh,wY),e(wY,coo),e(Fh,moo),e(Fh,pR),e(pR,foo),e(Fh,goo),e(re,hoo),e(re,Ch),e(Ch,AY),e(AY,uoo),e(Ch,poo),e(Ch,_R),e(_R,_oo),e(Ch,boo),e(re,voo),e(re,Mh),e(Mh,LY),e(LY,Too),e(Mh,Foo),e(Mh,bR),e(bR,Coo),e(Mh,Moo),e(re,Eoo),e(re,Eh),e(Eh,BY),e(BY,yoo),e(Eh,woo),e(Eh,vR),e(vR,Aoo),e(Eh,Loo),e(re,Boo),e(re,yh),e(yh,xY),e(xY,xoo),e(yh,koo),e(yh,TR),e(TR,Roo),e(yh,Soo),e(re,Poo),e(re,wh),e(wh,kY),e(kY,$oo),e(wh,Ioo),e(wh,FR),e(FR,joo),e(wh,Noo),e(re,Doo),e(re,Ah),e(Ah,RY),e(RY,qoo),e(Ah,Ooo),e(Ah,CR),e(CR,Goo),e(Ah,Xoo),e(Ie,Voo),g(Lh,Ie,null),e(Ie,zoo),e(Ie,SY),e(SY,Woo),e(Ie,Qoo),g(wy,Ie,null),e(Ho,Hoo),e(Ho,Bh),g(Ay,Bh,null),e(Bh,Uoo),e(Bh,PY),e(PY,Joo),b(c,Uxe,_),b(c,Ui,_),e(Ui,xh),e(xh,$Y),g(Ly,$Y,null),e(Ui,Yoo),e(Ui,IY),e(IY,Koo),b(c,Jxe,_),b(c,Uo,_),g(By,Uo,null),e(Uo,Zoo),e(Uo,xy),e(xy,ero),e(xy,MR),e(MR,oro),e(xy,rro),e(Uo,tro),e(Uo,ky),e(ky,aro),e(ky,jY),e(jY,sro),e(ky,nro),e(Uo,lro),e(Uo,je),g(Ry,je,null),e(je,iro),e(je,NY),e(NY,dro),e(je,cro),e(je,Ji),e(Ji,mro),e(Ji,DY),e(DY,fro),e(Ji,gro),e(Ji,qY),e(qY,hro),e(Ji,uro),e(je,pro),e(je,xe),e(xe,kh),e(kh,OY),e(OY,_ro),e(kh,bro),e(kh,ER),e(ER,vro),e(kh,Tro),e(xe,Fro),e(xe,Rh),e(Rh,GY),e(GY,Cro),e(Rh,Mro),e(Rh,yR),e(yR,Ero),e(Rh,yro),e(xe,wro),e(xe,Sh),e(Sh,XY),e(XY,Aro),e(Sh,Lro),e(Sh,wR),e(wR,Bro),e(Sh,xro),e(xe,kro),e(xe,Ph),e(Ph,VY),e(VY,Rro),e(Ph,Sro),e(Ph,AR),e(AR,Pro),e(Ph,$ro),e(xe,Iro),e(xe,$h),e($h,zY),e(zY,jro),e($h,Nro),e($h,LR),e(LR,Dro),e($h,qro),e(xe,Oro),e(xe,Ih),e(Ih,WY),e(WY,Gro),e(Ih,Xro),e(Ih,BR),e(BR,Vro),e(Ih,zro),e(xe,Wro),e(xe,jh),e(jh,QY),e(QY,Qro),e(jh,Hro),e(jh,xR),e(xR,Uro),e(jh,Jro),e(xe,Yro),e(xe,Nh),e(Nh,HY),e(HY,Kro),e(Nh,Zro),e(Nh,kR),e(kR,eto),e(Nh,oto),e(je,rto),g(Dh,je,null),e(je,tto),e(je,UY),e(UY,ato),e(je,sto),g(Sy,je,null),e(Uo,nto),e(Uo,qh),g(Py,qh,null),e(qh,lto),e(qh,JY),e(JY,ito),b(c,Yxe,_),b(c,Yi,_),e(Yi,Oh),e(Oh,YY),g($y,YY,null),e(Yi,dto),e(Yi,KY),e(KY,cto),b(c,Kxe,_),b(c,Jo,_),g(Iy,Jo,null),e(Jo,mto),e(Jo,Ki),e(Ki,fto),e(Ki,ZY),e(ZY,gto),e(Ki,hto),e(Ki,eK),e(eK,uto),e(Ki,pto),e(Jo,_to),e(Jo,jy),e(jy,bto),e(jy,oK),e(oK,vto),e(jy,Tto),e(Jo,Fto),e(Jo,Vr),g(Ny,Vr,null),e(Vr,Cto),e(Vr,rK),e(rK,Mto),e(Vr,Eto),e(Vr,Zi),e(Zi,yto),e(Zi,tK),e(tK,wto),e(Zi,Ato),e(Zi,aK),e(aK,Lto),e(Zi,Bto),e(Vr,xto),e(Vr,sK),e(sK,kto),e(Vr,Rto),g(Dy,Vr,null),e(Jo,Sto),e(Jo,Ne),g(qy,Ne,null),e(Ne,Pto),e(Ne,nK),e(nK,$to),e(Ne,Ito),e(Ne,Va),e(Va,jto),e(Va,lK),e(lK,Nto),e(Va,Dto),e(Va,iK),e(iK,qto),e(Va,Oto),e(Va,dK),e(dK,Gto),e(Va,Xto),e(Ne,Vto),e(Ne,F),e(F,Gh),e(Gh,cK),e(cK,zto),e(Gh,Wto),e(Gh,RR),e(RR,Qto),e(Gh,Hto),e(F,Uto),e(F,Xh),e(Xh,mK),e(mK,Jto),e(Xh,Yto),e(Xh,SR),e(SR,Kto),e(Xh,Zto),e(F,eao),e(F,Vh),e(Vh,fK),e(fK,oao),e(Vh,rao),e(Vh,PR),e(PR,tao),e(Vh,aao),e(F,sao),e(F,zh),e(zh,gK),e(gK,nao),e(zh,lao),e(zh,$R),e($R,iao),e(zh,dao),e(F,cao),e(F,Wh),e(Wh,hK),e(hK,mao),e(Wh,fao),e(Wh,IR),e(IR,gao),e(Wh,hao),e(F,uao),e(F,Qh),e(Qh,uK),e(uK,pao),e(Qh,_ao),e(Qh,jR),e(jR,bao),e(Qh,vao),e(F,Tao),e(F,Hh),e(Hh,pK),e(pK,Fao),e(Hh,Cao),e(Hh,NR),e(NR,Mao),e(Hh,Eao),e(F,yao),e(F,Uh),e(Uh,_K),e(_K,wao),e(Uh,Aao),e(Uh,DR),e(DR,Lao),e(Uh,Bao),e(F,xao),e(F,Jh),e(Jh,bK),e(bK,kao),e(Jh,Rao),e(Jh,qR),e(qR,Sao),e(Jh,Pao),e(F,$ao),e(F,Yh),e(Yh,vK),e(vK,Iao),e(Yh,jao),e(Yh,OR),e(OR,Nao),e(Yh,Dao),e(F,qao),e(F,Kh),e(Kh,TK),e(TK,Oao),e(Kh,Gao),e(Kh,GR),e(GR,Xao),e(Kh,Vao),e(F,zao),e(F,Zh),e(Zh,FK),e(FK,Wao),e(Zh,Qao),e(Zh,XR),e(XR,Hao),e(Zh,Uao),e(F,Jao),e(F,eu),e(eu,CK),e(CK,Yao),e(eu,Kao),e(eu,VR),e(VR,Zao),e(eu,eso),e(F,oso),e(F,ou),e(ou,MK),e(MK,rso),e(ou,tso),e(ou,zR),e(zR,aso),e(ou,sso),e(F,nso),e(F,ru),e(ru,EK),e(EK,lso),e(ru,iso),e(ru,WR),e(WR,dso),e(ru,cso),e(F,mso),e(F,tu),e(tu,yK),e(yK,fso),e(tu,gso),e(tu,QR),e(QR,hso),e(tu,uso),e(F,pso),e(F,au),e(au,wK),e(wK,_so),e(au,bso),e(au,HR),e(HR,vso),e(au,Tso),e(F,Fso),e(F,su),e(su,AK),e(AK,Cso),e(su,Mso),e(su,UR),e(UR,Eso),e(su,yso),e(F,wso),e(F,nu),e(nu,LK),e(LK,Aso),e(nu,Lso),e(nu,JR),e(JR,Bso),e(nu,xso),e(F,kso),e(F,lu),e(lu,BK),e(BK,Rso),e(lu,Sso),e(lu,YR),e(YR,Pso),e(lu,$so),e(F,Iso),e(F,iu),e(iu,xK),e(xK,jso),e(iu,Nso),e(iu,KR),e(KR,Dso),e(iu,qso),e(F,Oso),e(F,du),e(du,kK),e(kK,Gso),e(du,Xso),e(du,ZR),e(ZR,Vso),e(du,zso),e(F,Wso),e(F,cu),e(cu,RK),e(RK,Qso),e(cu,Hso),e(cu,eS),e(eS,Uso),e(cu,Jso),e(F,Yso),e(F,mu),e(mu,SK),e(SK,Kso),e(mu,Zso),e(mu,oS),e(oS,eno),e(mu,ono),e(F,rno),e(F,fu),e(fu,PK),e(PK,tno),e(fu,ano),e(fu,rS),e(rS,sno),e(fu,nno),e(F,lno),e(F,gu),e(gu,$K),e($K,ino),e(gu,dno),e(gu,tS),e(tS,cno),e(gu,mno),e(F,fno),e(F,hu),e(hu,IK),e(IK,gno),e(hu,hno),e(hu,aS),e(aS,uno),e(hu,pno),e(F,_no),e(F,Dn),e(Dn,jK),e(jK,bno),e(Dn,vno),e(Dn,sS),e(sS,Tno),e(Dn,Fno),e(Dn,nS),e(nS,Cno),e(Dn,Mno),e(F,Eno),e(F,uu),e(uu,NK),e(NK,yno),e(uu,wno),e(uu,lS),e(lS,Ano),e(uu,Lno),e(F,Bno),e(F,pu),e(pu,DK),e(DK,xno),e(pu,kno),e(pu,iS),e(iS,Rno),e(pu,Sno),e(F,Pno),e(F,_u),e(_u,qK),e(qK,$no),e(_u,Ino),e(_u,dS),e(dS,jno),e(_u,Nno),e(F,Dno),e(F,bu),e(bu,OK),e(OK,qno),e(bu,Ono),e(bu,cS),e(cS,Gno),e(bu,Xno),e(F,Vno),e(F,vu),e(vu,GK),e(GK,zno),e(vu,Wno),e(vu,mS),e(mS,Qno),e(vu,Hno),e(F,Uno),e(F,Tu),e(Tu,XK),e(XK,Jno),e(Tu,Yno),e(Tu,fS),e(fS,Kno),e(Tu,Zno),e(F,elo),e(F,Fu),e(Fu,VK),e(VK,olo),e(Fu,rlo),e(Fu,gS),e(gS,tlo),e(Fu,alo),e(F,slo),e(F,Cu),e(Cu,zK),e(zK,nlo),e(Cu,llo),e(Cu,hS),e(hS,ilo),e(Cu,dlo),e(F,clo),e(F,Mu),e(Mu,WK),e(WK,mlo),e(Mu,flo),e(Mu,uS),e(uS,glo),e(Mu,hlo),e(F,ulo),e(F,Eu),e(Eu,QK),e(QK,plo),e(Eu,_lo),e(Eu,pS),e(pS,blo),e(Eu,vlo),e(F,Tlo),e(F,yu),e(yu,HK),e(HK,Flo),e(yu,Clo),e(yu,_S),e(_S,Mlo),e(yu,Elo),e(F,ylo),e(F,wu),e(wu,UK),e(UK,wlo),e(wu,Alo),e(wu,bS),e(bS,Llo),e(wu,Blo),e(F,xlo),e(F,Au),e(Au,JK),e(JK,klo),e(Au,Rlo),e(Au,vS),e(vS,Slo),e(Au,Plo),e(F,$lo),e(F,Lu),e(Lu,YK),e(YK,Ilo),e(Lu,jlo),e(Lu,TS),e(TS,Nlo),e(Lu,Dlo),e(F,qlo),e(F,Bu),e(Bu,KK),e(KK,Olo),e(Bu,Glo),e(Bu,FS),e(FS,Xlo),e(Bu,Vlo),e(F,zlo),e(F,xu),e(xu,ZK),e(ZK,Wlo),e(xu,Qlo),e(xu,CS),e(CS,Hlo),e(xu,Ulo),e(F,Jlo),e(F,ku),e(ku,eZ),e(eZ,Ylo),e(ku,Klo),e(ku,MS),e(MS,Zlo),e(ku,eio),e(F,oio),e(F,Ru),e(Ru,oZ),e(oZ,rio),e(Ru,tio),e(Ru,ES),e(ES,aio),e(Ru,sio),e(F,nio),e(F,Su),e(Su,rZ),e(rZ,lio),e(Su,iio),e(Su,yS),e(yS,dio),e(Su,cio),e(F,mio),e(F,Pu),e(Pu,tZ),e(tZ,fio),e(Pu,gio),e(Pu,wS),e(wS,hio),e(Pu,uio),e(F,pio),e(F,$u),e($u,aZ),e(aZ,_io),e($u,bio),e($u,AS),e(AS,vio),e($u,Tio),e(F,Fio),e(F,Iu),e(Iu,sZ),e(sZ,Cio),e(Iu,Mio),e(Iu,LS),e(LS,Eio),e(Iu,yio),e(F,wio),e(F,ju),e(ju,nZ),e(nZ,Aio),e(ju,Lio),e(ju,BS),e(BS,Bio),e(ju,xio),e(F,kio),e(F,Nu),e(Nu,lZ),e(lZ,Rio),e(Nu,Sio),e(Nu,xS),e(xS,Pio),e(Nu,$io),e(F,Iio),e(F,Du),e(Du,iZ),e(iZ,jio),e(Du,Nio),e(Du,kS),e(kS,Dio),e(Du,qio),e(F,Oio),e(F,qu),e(qu,dZ),e(dZ,Gio),e(qu,Xio),e(qu,RS),e(RS,Vio),e(qu,zio),e(F,Wio),e(F,Ou),e(Ou,cZ),e(cZ,Qio),e(Ou,Hio),e(Ou,SS),e(SS,Uio),e(Ou,Jio),e(F,Yio),e(F,Gu),e(Gu,mZ),e(mZ,Kio),e(Gu,Zio),e(Gu,PS),e(PS,edo),e(Gu,odo),e(F,rdo),e(F,Xu),e(Xu,fZ),e(fZ,tdo),e(Xu,ado),e(Xu,$S),e($S,sdo),e(Xu,ndo),e(F,ldo),e(F,Vu),e(Vu,gZ),e(gZ,ido),e(Vu,ddo),e(Vu,IS),e(IS,cdo),e(Vu,mdo),e(F,fdo),e(F,zu),e(zu,hZ),e(hZ,gdo),e(zu,hdo),e(zu,jS),e(jS,udo),e(zu,pdo),e(F,_do),e(F,Wu),e(Wu,uZ),e(uZ,bdo),e(Wu,vdo),e(Wu,NS),e(NS,Tdo),e(Wu,Fdo),e(F,Cdo),e(F,Qu),e(Qu,pZ),e(pZ,Mdo),e(Qu,Edo),e(Qu,DS),e(DS,ydo),e(Qu,wdo),e(F,Ado),e(F,Hu),e(Hu,_Z),e(_Z,Ldo),e(Hu,Bdo),e(Hu,qS),e(qS,xdo),e(Hu,kdo),e(F,Rdo),e(F,Uu),e(Uu,bZ),e(bZ,Sdo),e(Uu,Pdo),e(Uu,OS),e(OS,$do),e(Uu,Ido),e(F,jdo),e(F,Ju),e(Ju,vZ),e(vZ,Ndo),e(Ju,Ddo),e(Ju,GS),e(GS,qdo),e(Ju,Odo),e(F,Gdo),e(F,Yu),e(Yu,TZ),e(TZ,Xdo),e(Yu,Vdo),e(Yu,XS),e(XS,zdo),e(Yu,Wdo),e(F,Qdo),e(F,Ku),e(Ku,FZ),e(FZ,Hdo),e(Ku,Udo),e(Ku,VS),e(VS,Jdo),e(Ku,Ydo),e(F,Kdo),e(F,Zu),e(Zu,CZ),e(CZ,Zdo),e(Zu,eco),e(Zu,zS),e(zS,oco),e(Zu,rco),e(F,tco),e(F,ep),e(ep,MZ),e(MZ,aco),e(ep,sco),e(ep,WS),e(WS,nco),e(ep,lco),e(F,ico),e(F,op),e(op,EZ),e(EZ,dco),e(op,cco),e(op,QS),e(QS,mco),e(op,fco),e(F,gco),e(F,rp),e(rp,yZ),e(yZ,hco),e(rp,uco),e(rp,HS),e(HS,pco),e(rp,_co),e(F,bco),e(F,tp),e(tp,wZ),e(wZ,vco),e(tp,Tco),e(tp,US),e(US,Fco),e(tp,Cco),e(F,Mco),e(F,ap),e(ap,AZ),e(AZ,Eco),e(ap,yco),e(ap,JS),e(JS,wco),e(ap,Aco),e(F,Lco),e(F,sp),e(sp,LZ),e(LZ,Bco),e(sp,xco),e(sp,YS),e(YS,kco),e(sp,Rco),e(F,Sco),e(F,np),e(np,BZ),e(BZ,Pco),e(np,$co),e(np,KS),e(KS,Ico),e(np,jco),e(F,Nco),e(F,lp),e(lp,xZ),e(xZ,Dco),e(lp,qco),e(lp,ZS),e(ZS,Oco),e(lp,Gco),e(F,Xco),e(F,ip),e(ip,kZ),e(kZ,Vco),e(ip,zco),e(ip,eP),e(eP,Wco),e(ip,Qco),e(F,Hco),e(F,dp),e(dp,RZ),e(RZ,Uco),e(dp,Jco),e(dp,oP),e(oP,Yco),e(dp,Kco),e(F,Zco),e(F,cp),e(cp,SZ),e(SZ,emo),e(cp,omo),e(cp,rP),e(rP,rmo),e(cp,tmo),e(F,amo),e(F,mp),e(mp,PZ),e(PZ,smo),e(mp,nmo),e(mp,tP),e(tP,lmo),e(mp,imo),e(F,dmo),e(F,fp),e(fp,$Z),e($Z,cmo),e(fp,mmo),e(fp,aP),e(aP,fmo),e(fp,gmo),e(F,hmo),e(F,gp),e(gp,IZ),e(IZ,umo),e(gp,pmo),e(gp,sP),e(sP,_mo),e(gp,bmo),e(F,vmo),e(F,hp),e(hp,jZ),e(jZ,Tmo),e(hp,Fmo),e(hp,nP),e(nP,Cmo),e(hp,Mmo),e(F,Emo),e(F,up),e(up,NZ),e(NZ,ymo),e(up,wmo),e(up,lP),e(lP,Amo),e(up,Lmo),e(F,Bmo),e(F,pp),e(pp,DZ),e(DZ,xmo),e(pp,kmo),e(pp,iP),e(iP,Rmo),e(pp,Smo),e(F,Pmo),e(F,_p),e(_p,qZ),e(qZ,$mo),e(_p,Imo),e(_p,dP),e(dP,jmo),e(_p,Nmo),e(F,Dmo),e(F,bp),e(bp,OZ),e(OZ,qmo),e(bp,Omo),e(bp,cP),e(cP,Gmo),e(bp,Xmo),e(F,Vmo),e(F,vp),e(vp,GZ),e(GZ,zmo),e(vp,Wmo),e(vp,mP),e(mP,Qmo),e(vp,Hmo),e(F,Umo),e(F,Tp),e(Tp,XZ),e(XZ,Jmo),e(Tp,Ymo),e(Tp,fP),e(fP,Kmo),e(Tp,Zmo),e(F,efo),e(F,Fp),e(Fp,VZ),e(VZ,ofo),e(Fp,rfo),e(Fp,gP),e(gP,tfo),e(Fp,afo),e(Ne,sfo),e(Ne,Cp),e(Cp,nfo),e(Cp,zZ),e(zZ,lfo),e(Cp,ifo),e(Cp,WZ),e(WZ,dfo),e(Ne,cfo),e(Ne,QZ),e(QZ,mfo),e(Ne,ffo),g(Oy,Ne,null),b(c,Zxe,_),b(c,ed,_),e(ed,Mp),e(Mp,HZ),g(Gy,HZ,null),e(ed,gfo),e(ed,UZ),e(UZ,hfo),b(c,eke,_),b(c,Yo,_),g(Xy,Yo,null),e(Yo,ufo),e(Yo,od),e(od,pfo),e(od,JZ),e(JZ,_fo),e(od,bfo),e(od,YZ),e(YZ,vfo),e(od,Tfo),e(Yo,Ffo),e(Yo,Vy),e(Vy,Cfo),e(Vy,KZ),e(KZ,Mfo),e(Vy,Efo),e(Yo,yfo),e(Yo,zr),g(zy,zr,null),e(zr,wfo),e(zr,ZZ),e(ZZ,Afo),e(zr,Lfo),e(zr,rd),e(rd,Bfo),e(rd,eee),e(eee,xfo),e(rd,kfo),e(rd,oee),e(oee,Rfo),e(rd,Sfo),e(zr,Pfo),e(zr,ree),e(ree,$fo),e(zr,Ifo),g(Wy,zr,null),e(Yo,jfo),e(Yo,De),g(Qy,De,null),e(De,Nfo),e(De,tee),e(tee,Dfo),e(De,qfo),e(De,za),e(za,Ofo),e(za,aee),e(aee,Gfo),e(za,Xfo),e(za,see),e(see,Vfo),e(za,zfo),e(za,nee),e(nee,Wfo),e(za,Qfo),e(De,Hfo),e(De,k),e(k,Ep),e(Ep,lee),e(lee,Ufo),e(Ep,Jfo),e(Ep,hP),e(hP,Yfo),e(Ep,Kfo),e(k,Zfo),e(k,yp),e(yp,iee),e(iee,ego),e(yp,ogo),e(yp,uP),e(uP,rgo),e(yp,tgo),e(k,ago),e(k,wp),e(wp,dee),e(dee,sgo),e(wp,ngo),e(wp,pP),e(pP,lgo),e(wp,igo),e(k,dgo),e(k,Ap),e(Ap,cee),e(cee,cgo),e(Ap,mgo),e(Ap,_P),e(_P,fgo),e(Ap,ggo),e(k,hgo),e(k,Lp),e(Lp,mee),e(mee,ugo),e(Lp,pgo),e(Lp,bP),e(bP,_go),e(Lp,bgo),e(k,vgo),e(k,Bp),e(Bp,fee),e(fee,Tgo),e(Bp,Fgo),e(Bp,vP),e(vP,Cgo),e(Bp,Mgo),e(k,Ego),e(k,xp),e(xp,gee),e(gee,ygo),e(xp,wgo),e(xp,TP),e(TP,Ago),e(xp,Lgo),e(k,Bgo),e(k,kp),e(kp,hee),e(hee,xgo),e(kp,kgo),e(kp,FP),e(FP,Rgo),e(kp,Sgo),e(k,Pgo),e(k,Rp),e(Rp,uee),e(uee,$go),e(Rp,Igo),e(Rp,CP),e(CP,jgo),e(Rp,Ngo),e(k,Dgo),e(k,Sp),e(Sp,pee),e(pee,qgo),e(Sp,Ogo),e(Sp,MP),e(MP,Ggo),e(Sp,Xgo),e(k,Vgo),e(k,Pp),e(Pp,_ee),e(_ee,zgo),e(Pp,Wgo),e(Pp,EP),e(EP,Qgo),e(Pp,Hgo),e(k,Ugo),e(k,$p),e($p,bee),e(bee,Jgo),e($p,Ygo),e($p,yP),e(yP,Kgo),e($p,Zgo),e(k,eho),e(k,Ip),e(Ip,vee),e(vee,oho),e(Ip,rho),e(Ip,wP),e(wP,tho),e(Ip,aho),e(k,sho),e(k,jp),e(jp,Tee),e(Tee,nho),e(jp,lho),e(jp,AP),e(AP,iho),e(jp,dho),e(k,cho),e(k,Np),e(Np,Fee),e(Fee,mho),e(Np,fho),e(Np,LP),e(LP,gho),e(Np,hho),e(k,uho),e(k,Dp),e(Dp,Cee),e(Cee,pho),e(Dp,_ho),e(Dp,BP),e(BP,bho),e(Dp,vho),e(k,Tho),e(k,qp),e(qp,Mee),e(Mee,Fho),e(qp,Cho),e(qp,xP),e(xP,Mho),e(qp,Eho),e(k,yho),e(k,Op),e(Op,Eee),e(Eee,who),e(Op,Aho),e(Op,kP),e(kP,Lho),e(Op,Bho),e(k,xho),e(k,Gp),e(Gp,yee),e(yee,kho),e(Gp,Rho),e(Gp,RP),e(RP,Sho),e(Gp,Pho),e(k,$ho),e(k,Xp),e(Xp,wee),e(wee,Iho),e(Xp,jho),e(Xp,SP),e(SP,Nho),e(Xp,Dho),e(k,qho),e(k,Vp),e(Vp,Aee),e(Aee,Oho),e(Vp,Gho),e(Vp,PP),e(PP,Xho),e(Vp,Vho),e(k,zho),e(k,zp),e(zp,Lee),e(Lee,Who),e(zp,Qho),e(zp,$P),e($P,Hho),e(zp,Uho),e(k,Jho),e(k,Wp),e(Wp,Bee),e(Bee,Yho),e(Wp,Kho),e(Wp,IP),e(IP,Zho),e(Wp,euo),e(k,ouo),e(k,Qp),e(Qp,xee),e(xee,ruo),e(Qp,tuo),e(Qp,jP),e(jP,auo),e(Qp,suo),e(k,nuo),e(k,Hp),e(Hp,kee),e(kee,luo),e(Hp,iuo),e(Hp,NP),e(NP,duo),e(Hp,cuo),e(k,muo),e(k,Up),e(Up,Ree),e(Ree,fuo),e(Up,guo),e(Up,DP),e(DP,huo),e(Up,uuo),e(k,puo),e(k,Jp),e(Jp,See),e(See,_uo),e(Jp,buo),e(Jp,qP),e(qP,vuo),e(Jp,Tuo),e(k,Fuo),e(k,Yp),e(Yp,Pee),e(Pee,Cuo),e(Yp,Muo),e(Yp,OP),e(OP,Euo),e(Yp,yuo),e(k,wuo),e(k,Kp),e(Kp,$ee),e($ee,Auo),e(Kp,Luo),e(Kp,GP),e(GP,Buo),e(Kp,xuo),e(k,kuo),e(k,Zp),e(Zp,Iee),e(Iee,Ruo),e(Zp,Suo),e(Zp,XP),e(XP,Puo),e(Zp,$uo),e(k,Iuo),e(k,e_),e(e_,jee),e(jee,juo),e(e_,Nuo),e(e_,VP),e(VP,Duo),e(e_,quo),e(k,Ouo),e(k,o_),e(o_,Nee),e(Nee,Guo),e(o_,Xuo),e(o_,zP),e(zP,Vuo),e(o_,zuo),e(k,Wuo),e(k,r_),e(r_,Dee),e(Dee,Quo),e(r_,Huo),e(r_,WP),e(WP,Uuo),e(r_,Juo),e(k,Yuo),e(k,t_),e(t_,qee),e(qee,Kuo),e(t_,Zuo),e(t_,QP),e(QP,epo),e(t_,opo),e(k,rpo),e(k,a_),e(a_,Oee),e(Oee,tpo),e(a_,apo),e(a_,HP),e(HP,spo),e(a_,npo),e(k,lpo),e(k,s_),e(s_,Gee),e(Gee,ipo),e(s_,dpo),e(s_,UP),e(UP,cpo),e(s_,mpo),e(k,fpo),e(k,n_),e(n_,Xee),e(Xee,gpo),e(n_,hpo),e(n_,JP),e(JP,upo),e(n_,ppo),e(k,_po),e(k,l_),e(l_,Vee),e(Vee,bpo),e(l_,vpo),e(l_,YP),e(YP,Tpo),e(l_,Fpo),e(k,Cpo),e(k,i_),e(i_,zee),e(zee,Mpo),e(i_,Epo),e(i_,KP),e(KP,ypo),e(i_,wpo),e(De,Apo),e(De,d_),e(d_,Lpo),e(d_,Wee),e(Wee,Bpo),e(d_,xpo),e(d_,Qee),e(Qee,kpo),e(De,Rpo),e(De,Hee),e(Hee,Spo),e(De,Ppo),g(Hy,De,null),b(c,oke,_),b(c,td,_),e(td,c_),e(c_,Uee),g(Uy,Uee,null),e(td,$po),e(td,Jee),e(Jee,Ipo),b(c,rke,_),b(c,Ko,_),g(Jy,Ko,null),e(Ko,jpo),e(Ko,ad),e(ad,Npo),e(ad,Yee),e(Yee,Dpo),e(ad,qpo),e(ad,Kee),e(Kee,Opo),e(ad,Gpo),e(Ko,Xpo),e(Ko,Yy),e(Yy,Vpo),e(Yy,Zee),e(Zee,zpo),e(Yy,Wpo),e(Ko,Qpo),e(Ko,Wr),g(Ky,Wr,null),e(Wr,Hpo),e(Wr,eoe),e(eoe,Upo),e(Wr,Jpo),e(Wr,sd),e(sd,Ypo),e(sd,ooe),e(ooe,Kpo),e(sd,Zpo),e(sd,roe),e(roe,e_o),e(sd,o_o),e(Wr,r_o),e(Wr,toe),e(toe,t_o),e(Wr,a_o),g(Zy,Wr,null),e(Ko,s_o),e(Ko,qe),g(ew,qe,null),e(qe,n_o),e(qe,aoe),e(aoe,l_o),e(qe,i_o),e(qe,Wa),e(Wa,d_o),e(Wa,soe),e(soe,c_o),e(Wa,m_o),e(Wa,noe),e(noe,f_o),e(Wa,g_o),e(Wa,loe),e(loe,h_o),e(Wa,u_o),e(qe,p_o),e(qe,$),e($,m_),e(m_,ioe),e(ioe,__o),e(m_,b_o),e(m_,ZP),e(ZP,v_o),e(m_,T_o),e($,F_o),e($,f_),e(f_,doe),e(doe,C_o),e(f_,M_o),e(f_,e$),e(e$,E_o),e(f_,y_o),e($,w_o),e($,g_),e(g_,coe),e(coe,A_o),e(g_,L_o),e(g_,o$),e(o$,B_o),e(g_,x_o),e($,k_o),e($,h_),e(h_,moe),e(moe,R_o),e(h_,S_o),e(h_,r$),e(r$,P_o),e(h_,$_o),e($,I_o),e($,u_),e(u_,foe),e(foe,j_o),e(u_,N_o),e(u_,t$),e(t$,D_o),e(u_,q_o),e($,O_o),e($,p_),e(p_,goe),e(goe,G_o),e(p_,X_o),e(p_,a$),e(a$,V_o),e(p_,z_o),e($,W_o),e($,__),e(__,hoe),e(hoe,Q_o),e(__,H_o),e(__,s$),e(s$,U_o),e(__,J_o),e($,Y_o),e($,b_),e(b_,uoe),e(uoe,K_o),e(b_,Z_o),e(b_,n$),e(n$,ebo),e(b_,obo),e($,rbo),e($,v_),e(v_,poe),e(poe,tbo),e(v_,abo),e(v_,l$),e(l$,sbo),e(v_,nbo),e($,lbo),e($,T_),e(T_,_oe),e(_oe,ibo),e(T_,dbo),e(T_,i$),e(i$,cbo),e(T_,mbo),e($,fbo),e($,F_),e(F_,boe),e(boe,gbo),e(F_,hbo),e(F_,d$),e(d$,ubo),e(F_,pbo),e($,_bo),e($,C_),e(C_,voe),e(voe,bbo),e(C_,vbo),e(C_,c$),e(c$,Tbo),e(C_,Fbo),e($,Cbo),e($,M_),e(M_,Toe),e(Toe,Mbo),e(M_,Ebo),e(M_,m$),e(m$,ybo),e(M_,wbo),e($,Abo),e($,E_),e(E_,Foe),e(Foe,Lbo),e(E_,Bbo),e(E_,f$),e(f$,xbo),e(E_,kbo),e($,Rbo),e($,y_),e(y_,Coe),e(Coe,Sbo),e(y_,Pbo),e(y_,g$),e(g$,$bo),e(y_,Ibo),e($,jbo),e($,w_),e(w_,Moe),e(Moe,Nbo),e(w_,Dbo),e(w_,h$),e(h$,qbo),e(w_,Obo),e($,Gbo),e($,A_),e(A_,Eoe),e(Eoe,Xbo),e(A_,Vbo),e(A_,u$),e(u$,zbo),e(A_,Wbo),e($,Qbo),e($,L_),e(L_,yoe),e(yoe,Hbo),e(L_,Ubo),e(L_,p$),e(p$,Jbo),e(L_,Ybo),e($,Kbo),e($,B_),e(B_,woe),e(woe,Zbo),e(B_,e2o),e(B_,_$),e(_$,o2o),e(B_,r2o),e($,t2o),e($,x_),e(x_,Aoe),e(Aoe,a2o),e(x_,s2o),e(x_,b$),e(b$,n2o),e(x_,l2o),e($,i2o),e($,k_),e(k_,Loe),e(Loe,d2o),e(k_,c2o),e(k_,v$),e(v$,m2o),e(k_,f2o),e($,g2o),e($,R_),e(R_,Boe),e(Boe,h2o),e(R_,u2o),e(R_,T$),e(T$,p2o),e(R_,_2o),e($,b2o),e($,S_),e(S_,xoe),e(xoe,v2o),e(S_,T2o),e(S_,F$),e(F$,F2o),e(S_,C2o),e($,M2o),e($,P_),e(P_,koe),e(koe,E2o),e(P_,y2o),e(P_,C$),e(C$,w2o),e(P_,A2o),e($,L2o),e($,$_),e($_,Roe),e(Roe,B2o),e($_,x2o),e($_,M$),e(M$,k2o),e($_,R2o),e($,S2o),e($,I_),e(I_,Soe),e(Soe,P2o),e(I_,$2o),e(I_,E$),e(E$,I2o),e(I_,j2o),e($,N2o),e($,j_),e(j_,Poe),e(Poe,D2o),e(j_,q2o),e(j_,y$),e(y$,O2o),e(j_,G2o),e($,X2o),e($,N_),e(N_,$oe),e($oe,V2o),e(N_,z2o),e(N_,w$),e(w$,W2o),e(N_,Q2o),e($,H2o),e($,D_),e(D_,Ioe),e(Ioe,U2o),e(D_,J2o),e(D_,A$),e(A$,Y2o),e(D_,K2o),e($,Z2o),e($,q_),e(q_,joe),e(joe,evo),e(q_,ovo),e(q_,L$),e(L$,rvo),e(q_,tvo),e($,avo),e($,O_),e(O_,Noe),e(Noe,svo),e(O_,nvo),e(O_,B$),e(B$,lvo),e(O_,ivo),e($,dvo),e($,G_),e(G_,Doe),e(Doe,cvo),e(G_,mvo),e(G_,x$),e(x$,fvo),e(G_,gvo),e($,hvo),e($,X_),e(X_,qoe),e(qoe,uvo),e(X_,pvo),e(X_,k$),e(k$,_vo),e(X_,bvo),e($,vvo),e($,V_),e(V_,Ooe),e(Ooe,Tvo),e(V_,Fvo),e(V_,R$),e(R$,Cvo),e(V_,Mvo),e($,Evo),e($,z_),e(z_,Goe),e(Goe,yvo),e(z_,wvo),e(z_,S$),e(S$,Avo),e(z_,Lvo),e(qe,Bvo),e(qe,W_),e(W_,xvo),e(W_,Xoe),e(Xoe,kvo),e(W_,Rvo),e(W_,Voe),e(Voe,Svo),e(qe,Pvo),e(qe,zoe),e(zoe,$vo),e(qe,Ivo),g(ow,qe,null),b(c,tke,_),b(c,nd,_),e(nd,Q_),e(Q_,Woe),g(rw,Woe,null),e(nd,jvo),e(nd,Qoe),e(Qoe,Nvo),b(c,ake,_),b(c,Zo,_),g(tw,Zo,null),e(Zo,Dvo),e(Zo,ld),e(ld,qvo),e(ld,Hoe),e(Hoe,Ovo),e(ld,Gvo),e(ld,Uoe),e(Uoe,Xvo),e(ld,Vvo),e(Zo,zvo),e(Zo,aw),e(aw,Wvo),e(aw,Joe),e(Joe,Qvo),e(aw,Hvo),e(Zo,Uvo),e(Zo,Qr),g(sw,Qr,null),e(Qr,Jvo),e(Qr,Yoe),e(Yoe,Yvo),e(Qr,Kvo),e(Qr,id),e(id,Zvo),e(id,Koe),e(Koe,eTo),e(id,oTo),e(id,Zoe),e(Zoe,rTo),e(id,tTo),e(Qr,aTo),e(Qr,ere),e(ere,sTo),e(Qr,nTo),g(nw,Qr,null),e(Zo,lTo),e(Zo,Oe),g(lw,Oe,null),e(Oe,iTo),e(Oe,ore),e(ore,dTo),e(Oe,cTo),e(Oe,Qa),e(Qa,mTo),e(Qa,rre),e(rre,fTo),e(Qa,gTo),e(Qa,tre),e(tre,hTo),e(Qa,uTo),e(Qa,are),e(are,pTo),e(Qa,_To),e(Oe,bTo),e(Oe,I),e(I,H_),e(H_,sre),e(sre,vTo),e(H_,TTo),e(H_,P$),e(P$,FTo),e(H_,CTo),e(I,MTo),e(I,U_),e(U_,nre),e(nre,ETo),e(U_,yTo),e(U_,$$),e($$,wTo),e(U_,ATo),e(I,LTo),e(I,J_),e(J_,lre),e(lre,BTo),e(J_,xTo),e(J_,I$),e(I$,kTo),e(J_,RTo),e(I,STo),e(I,Y_),e(Y_,ire),e(ire,PTo),e(Y_,$To),e(Y_,j$),e(j$,ITo),e(Y_,jTo),e(I,NTo),e(I,K_),e(K_,dre),e(dre,DTo),e(K_,qTo),e(K_,N$),e(N$,OTo),e(K_,GTo),e(I,XTo),e(I,Z_),e(Z_,cre),e(cre,VTo),e(Z_,zTo),e(Z_,D$),e(D$,WTo),e(Z_,QTo),e(I,HTo),e(I,eb),e(eb,mre),e(mre,UTo),e(eb,JTo),e(eb,q$),e(q$,YTo),e(eb,KTo),e(I,ZTo),e(I,ob),e(ob,fre),e(fre,e1o),e(ob,o1o),e(ob,O$),e(O$,r1o),e(ob,t1o),e(I,a1o),e(I,rb),e(rb,gre),e(gre,s1o),e(rb,n1o),e(rb,G$),e(G$,l1o),e(rb,i1o),e(I,d1o),e(I,tb),e(tb,hre),e(hre,c1o),e(tb,m1o),e(tb,X$),e(X$,f1o),e(tb,g1o),e(I,h1o),e(I,ab),e(ab,ure),e(ure,u1o),e(ab,p1o),e(ab,V$),e(V$,_1o),e(ab,b1o),e(I,v1o),e(I,sb),e(sb,pre),e(pre,T1o),e(sb,F1o),e(sb,z$),e(z$,C1o),e(sb,M1o),e(I,E1o),e(I,nb),e(nb,_re),e(_re,y1o),e(nb,w1o),e(nb,W$),e(W$,A1o),e(nb,L1o),e(I,B1o),e(I,lb),e(lb,bre),e(bre,x1o),e(lb,k1o),e(lb,Q$),e(Q$,R1o),e(lb,S1o),e(I,P1o),e(I,ib),e(ib,vre),e(vre,$1o),e(ib,I1o),e(ib,H$),e(H$,j1o),e(ib,N1o),e(I,D1o),e(I,db),e(db,Tre),e(Tre,q1o),e(db,O1o),e(db,U$),e(U$,G1o),e(db,X1o),e(I,V1o),e(I,cb),e(cb,Fre),e(Fre,z1o),e(cb,W1o),e(cb,J$),e(J$,Q1o),e(cb,H1o),e(I,U1o),e(I,mb),e(mb,Cre),e(Cre,J1o),e(mb,Y1o),e(mb,Y$),e(Y$,K1o),e(mb,Z1o),e(I,eFo),e(I,fb),e(fb,Mre),e(Mre,oFo),e(fb,rFo),e(fb,K$),e(K$,tFo),e(fb,aFo),e(I,sFo),e(I,gb),e(gb,Ere),e(Ere,nFo),e(gb,lFo),e(gb,Z$),e(Z$,iFo),e(gb,dFo),e(I,cFo),e(I,hb),e(hb,yre),e(yre,mFo),e(hb,fFo),e(hb,eI),e(eI,gFo),e(hb,hFo),e(I,uFo),e(I,ub),e(ub,wre),e(wre,pFo),e(ub,_Fo),e(ub,oI),e(oI,bFo),e(ub,vFo),e(I,TFo),e(I,pb),e(pb,Are),e(Are,FFo),e(pb,CFo),e(pb,rI),e(rI,MFo),e(pb,EFo),e(I,yFo),e(I,_b),e(_b,Lre),e(Lre,wFo),e(_b,AFo),e(_b,tI),e(tI,LFo),e(_b,BFo),e(I,xFo),e(I,bb),e(bb,Bre),e(Bre,kFo),e(bb,RFo),e(bb,aI),e(aI,SFo),e(bb,PFo),e(I,$Fo),e(I,vb),e(vb,xre),e(xre,IFo),e(vb,jFo),e(vb,sI),e(sI,NFo),e(vb,DFo),e(I,qFo),e(I,Tb),e(Tb,kre),e(kre,OFo),e(Tb,GFo),e(Tb,nI),e(nI,XFo),e(Tb,VFo),e(I,zFo),e(I,Fb),e(Fb,Rre),e(Rre,WFo),e(Fb,QFo),e(Fb,lI),e(lI,HFo),e(Fb,UFo),e(I,JFo),e(I,Cb),e(Cb,Sre),e(Sre,YFo),e(Cb,KFo),e(Cb,iI),e(iI,ZFo),e(Cb,eCo),e(I,oCo),e(I,Mb),e(Mb,Pre),e(Pre,rCo),e(Mb,tCo),e(Mb,dI),e(dI,aCo),e(Mb,sCo),e(I,nCo),e(I,Eb),e(Eb,$re),e($re,lCo),e(Eb,iCo),e(Eb,Ire),e(Ire,dCo),e(Eb,cCo),e(I,mCo),e(I,yb),e(yb,jre),e(jre,fCo),e(yb,gCo),e(yb,cI),e(cI,hCo),e(yb,uCo),e(I,pCo),e(I,wb),e(wb,Nre),e(Nre,_Co),e(wb,bCo),e(wb,mI),e(mI,vCo),e(wb,TCo),e(I,FCo),e(I,Ab),e(Ab,Dre),e(Dre,CCo),e(Ab,MCo),e(Ab,fI),e(fI,ECo),e(Ab,yCo),e(I,wCo),e(I,Lb),e(Lb,qre),e(qre,ACo),e(Lb,LCo),e(Lb,gI),e(gI,BCo),e(Lb,xCo),e(Oe,kCo),e(Oe,Bb),e(Bb,RCo),e(Bb,Ore),e(Ore,SCo),e(Bb,PCo),e(Bb,Gre),e(Gre,$Co),e(Oe,ICo),e(Oe,Xre),e(Xre,jCo),e(Oe,NCo),g(iw,Oe,null),b(c,ske,_),b(c,dd,_),e(dd,xb),e(xb,Vre),g(dw,Vre,null),e(dd,DCo),e(dd,zre),e(zre,qCo),b(c,nke,_),b(c,er,_),g(cw,er,null),e(er,OCo),e(er,cd),e(cd,GCo),e(cd,Wre),e(Wre,XCo),e(cd,VCo),e(cd,Qre),e(Qre,zCo),e(cd,WCo),e(er,QCo),e(er,mw),e(mw,HCo),e(mw,Hre),e(Hre,UCo),e(mw,JCo),e(er,YCo),e(er,Hr),g(fw,Hr,null),e(Hr,KCo),e(Hr,Ure),e(Ure,ZCo),e(Hr,eMo),e(Hr,md),e(md,oMo),e(md,Jre),e(Jre,rMo),e(md,tMo),e(md,Yre),e(Yre,aMo),e(md,sMo),e(Hr,nMo),e(Hr,Kre),e(Kre,lMo),e(Hr,iMo),g(gw,Hr,null),e(er,dMo),e(er,Ge),g(hw,Ge,null),e(Ge,cMo),e(Ge,Zre),e(Zre,mMo),e(Ge,fMo),e(Ge,Ha),e(Ha,gMo),e(Ha,ete),e(ete,hMo),e(Ha,uMo),e(Ha,ote),e(ote,pMo),e(Ha,_Mo),e(Ha,rte),e(rte,bMo),e(Ha,vMo),e(Ge,TMo),e(Ge,ne),e(ne,kb),e(kb,tte),e(tte,FMo),e(kb,CMo),e(kb,hI),e(hI,MMo),e(kb,EMo),e(ne,yMo),e(ne,Rb),e(Rb,ate),e(ate,wMo),e(Rb,AMo),e(Rb,uI),e(uI,LMo),e(Rb,BMo),e(ne,xMo),e(ne,Sb),e(Sb,ste),e(ste,kMo),e(Sb,RMo),e(Sb,pI),e(pI,SMo),e(Sb,PMo),e(ne,$Mo),e(ne,Pb),e(Pb,nte),e(nte,IMo),e(Pb,jMo),e(Pb,_I),e(_I,NMo),e(Pb,DMo),e(ne,qMo),e(ne,$b),e($b,lte),e(lte,OMo),e($b,GMo),e($b,bI),e(bI,XMo),e($b,VMo),e(ne,zMo),e(ne,Ib),e(Ib,ite),e(ite,WMo),e(Ib,QMo),e(Ib,vI),e(vI,HMo),e(Ib,UMo),e(ne,JMo),e(ne,jb),e(jb,dte),e(dte,YMo),e(jb,KMo),e(jb,TI),e(TI,ZMo),e(jb,e4o),e(ne,o4o),e(ne,Nb),e(Nb,cte),e(cte,r4o),e(Nb,t4o),e(Nb,FI),e(FI,a4o),e(Nb,s4o),e(ne,n4o),e(ne,Db),e(Db,mte),e(mte,l4o),e(Db,i4o),e(Db,CI),e(CI,d4o),e(Db,c4o),e(ne,m4o),e(ne,qb),e(qb,fte),e(fte,f4o),e(qb,g4o),e(qb,MI),e(MI,h4o),e(qb,u4o),e(ne,p4o),e(ne,Ob),e(Ob,gte),e(gte,_4o),e(Ob,b4o),e(Ob,EI),e(EI,v4o),e(Ob,T4o),e(ne,F4o),e(ne,Gb),e(Gb,hte),e(hte,C4o),e(Gb,M4o),e(Gb,yI),e(yI,E4o),e(Gb,y4o),e(ne,w4o),e(ne,Xb),e(Xb,ute),e(ute,A4o),e(Xb,L4o),e(Xb,wI),e(wI,B4o),e(Xb,x4o),e(ne,k4o),e(ne,Vb),e(Vb,pte),e(pte,R4o),e(Vb,S4o),e(Vb,AI),e(AI,P4o),e(Vb,$4o),e(ne,I4o),e(ne,zb),e(zb,_te),e(_te,j4o),e(zb,N4o),e(zb,LI),e(LI,D4o),e(zb,q4o),e(ne,O4o),e(ne,Wb),e(Wb,bte),e(bte,G4o),e(Wb,X4o),e(Wb,BI),e(BI,V4o),e(Wb,z4o),e(Ge,W4o),e(Ge,Qb),e(Qb,Q4o),e(Qb,vte),e(vte,H4o),e(Qb,U4o),e(Qb,Tte),e(Tte,J4o),e(Ge,Y4o),e(Ge,Fte),e(Fte,K4o),e(Ge,Z4o),g(uw,Ge,null),b(c,lke,_),b(c,fd,_),e(fd,Hb),e(Hb,Cte),g(pw,Cte,null),e(fd,eEo),e(fd,Mte),e(Mte,oEo),b(c,ike,_),b(c,or,_),g(_w,or,null),e(or,rEo),e(or,gd),e(gd,tEo),e(gd,Ete),e(Ete,aEo),e(gd,sEo),e(gd,yte),e(yte,nEo),e(gd,lEo),e(or,iEo),e(or,bw),e(bw,dEo),e(bw,wte),e(wte,cEo),e(bw,mEo),e(or,fEo),e(or,Ur),g(vw,Ur,null),e(Ur,gEo),e(Ur,Ate),e(Ate,hEo),e(Ur,uEo),e(Ur,hd),e(hd,pEo),e(hd,Lte),e(Lte,_Eo),e(hd,bEo),e(hd,Bte),e(Bte,vEo),e(hd,TEo),e(Ur,FEo),e(Ur,xte),e(xte,CEo),e(Ur,MEo),g(Tw,Ur,null),e(or,EEo),e(or,Xe),g(Fw,Xe,null),e(Xe,yEo),e(Xe,kte),e(kte,wEo),e(Xe,AEo),e(Xe,Ua),e(Ua,LEo),e(Ua,Rte),e(Rte,BEo),e(Ua,xEo),e(Ua,Ste),e(Ste,kEo),e(Ua,REo),e(Ua,Pte),e(Pte,SEo),e(Ua,PEo),e(Xe,$Eo),e(Xe,A),e(A,Ub),e(Ub,$te),e($te,IEo),e(Ub,jEo),e(Ub,xI),e(xI,NEo),e(Ub,DEo),e(A,qEo),e(A,Jb),e(Jb,Ite),e(Ite,OEo),e(Jb,GEo),e(Jb,kI),e(kI,XEo),e(Jb,VEo),e(A,zEo),e(A,Yb),e(Yb,jte),e(jte,WEo),e(Yb,QEo),e(Yb,RI),e(RI,HEo),e(Yb,UEo),e(A,JEo),e(A,Kb),e(Kb,Nte),e(Nte,YEo),e(Kb,KEo),e(Kb,SI),e(SI,ZEo),e(Kb,e3o),e(A,o3o),e(A,Zb),e(Zb,Dte),e(Dte,r3o),e(Zb,t3o),e(Zb,PI),e(PI,a3o),e(Zb,s3o),e(A,n3o),e(A,e2),e(e2,qte),e(qte,l3o),e(e2,i3o),e(e2,$I),e($I,d3o),e(e2,c3o),e(A,m3o),e(A,o2),e(o2,Ote),e(Ote,f3o),e(o2,g3o),e(o2,II),e(II,h3o),e(o2,u3o),e(A,p3o),e(A,r2),e(r2,Gte),e(Gte,_3o),e(r2,b3o),e(r2,jI),e(jI,v3o),e(r2,T3o),e(A,F3o),e(A,t2),e(t2,Xte),e(Xte,C3o),e(t2,M3o),e(t2,NI),e(NI,E3o),e(t2,y3o),e(A,w3o),e(A,a2),e(a2,Vte),e(Vte,A3o),e(a2,L3o),e(a2,DI),e(DI,B3o),e(a2,x3o),e(A,k3o),e(A,s2),e(s2,zte),e(zte,R3o),e(s2,S3o),e(s2,qI),e(qI,P3o),e(s2,$3o),e(A,I3o),e(A,n2),e(n2,Wte),e(Wte,j3o),e(n2,N3o),e(n2,OI),e(OI,D3o),e(n2,q3o),e(A,O3o),e(A,l2),e(l2,Qte),e(Qte,G3o),e(l2,X3o),e(l2,GI),e(GI,V3o),e(l2,z3o),e(A,W3o),e(A,i2),e(i2,Hte),e(Hte,Q3o),e(i2,H3o),e(i2,XI),e(XI,U3o),e(i2,J3o),e(A,Y3o),e(A,d2),e(d2,Ute),e(Ute,K3o),e(d2,Z3o),e(d2,VI),e(VI,e5o),e(d2,o5o),e(A,r5o),e(A,c2),e(c2,Jte),e(Jte,t5o),e(c2,a5o),e(c2,zI),e(zI,s5o),e(c2,n5o),e(A,l5o),e(A,m2),e(m2,Yte),e(Yte,i5o),e(m2,d5o),e(m2,WI),e(WI,c5o),e(m2,m5o),e(A,f5o),e(A,f2),e(f2,Kte),e(Kte,g5o),e(f2,h5o),e(f2,QI),e(QI,u5o),e(f2,p5o),e(A,_5o),e(A,g2),e(g2,Zte),e(Zte,b5o),e(g2,v5o),e(g2,HI),e(HI,T5o),e(g2,F5o),e(A,C5o),e(A,h2),e(h2,eae),e(eae,M5o),e(h2,E5o),e(h2,UI),e(UI,y5o),e(h2,w5o),e(A,A5o),e(A,u2),e(u2,oae),e(oae,L5o),e(u2,B5o),e(u2,JI),e(JI,x5o),e(u2,k5o),e(A,R5o),e(A,p2),e(p2,rae),e(rae,S5o),e(p2,P5o),e(p2,YI),e(YI,$5o),e(p2,I5o),e(A,j5o),e(A,_2),e(_2,tae),e(tae,N5o),e(_2,D5o),e(_2,KI),e(KI,q5o),e(_2,O5o),e(A,G5o),e(A,b2),e(b2,aae),e(aae,X5o),e(b2,V5o),e(b2,ZI),e(ZI,z5o),e(b2,W5o),e(A,Q5o),e(A,v2),e(v2,sae),e(sae,H5o),e(v2,U5o),e(v2,ej),e(ej,J5o),e(v2,Y5o),e(A,K5o),e(A,T2),e(T2,nae),e(nae,Z5o),e(T2,eyo),e(T2,oj),e(oj,oyo),e(T2,ryo),e(A,tyo),e(A,F2),e(F2,lae),e(lae,ayo),e(F2,syo),e(F2,rj),e(rj,nyo),e(F2,lyo),e(A,iyo),e(A,C2),e(C2,iae),e(iae,dyo),e(C2,cyo),e(C2,tj),e(tj,myo),e(C2,fyo),e(A,gyo),e(A,M2),e(M2,dae),e(dae,hyo),e(M2,uyo),e(M2,aj),e(aj,pyo),e(M2,_yo),e(A,byo),e(A,E2),e(E2,cae),e(cae,vyo),e(E2,Tyo),e(E2,sj),e(sj,Fyo),e(E2,Cyo),e(A,Myo),e(A,y2),e(y2,mae),e(mae,Eyo),e(y2,yyo),e(y2,nj),e(nj,wyo),e(y2,Ayo),e(A,Lyo),e(A,w2),e(w2,fae),e(fae,Byo),e(w2,xyo),e(w2,lj),e(lj,kyo),e(w2,Ryo),e(A,Syo),e(A,A2),e(A2,gae),e(gae,Pyo),e(A2,$yo),e(A2,ij),e(ij,Iyo),e(A2,jyo),e(A,Nyo),e(A,L2),e(L2,hae),e(hae,Dyo),e(L2,qyo),e(L2,dj),e(dj,Oyo),e(L2,Gyo),e(A,Xyo),e(A,B2),e(B2,uae),e(uae,Vyo),e(B2,zyo),e(B2,cj),e(cj,Wyo),e(B2,Qyo),e(A,Hyo),e(A,x2),e(x2,pae),e(pae,Uyo),e(x2,Jyo),e(x2,mj),e(mj,Yyo),e(x2,Kyo),e(A,Zyo),e(A,k2),e(k2,_ae),e(_ae,ewo),e(k2,owo),e(k2,fj),e(fj,rwo),e(k2,two),e(A,awo),e(A,R2),e(R2,bae),e(bae,swo),e(R2,nwo),e(R2,gj),e(gj,lwo),e(R2,iwo),e(A,dwo),e(A,S2),e(S2,vae),e(vae,cwo),e(S2,mwo),e(S2,hj),e(hj,fwo),e(S2,gwo),e(A,hwo),e(A,P2),e(P2,Tae),e(Tae,uwo),e(P2,pwo),e(P2,uj),e(uj,_wo),e(P2,bwo),e(A,vwo),e(A,$2),e($2,Fae),e(Fae,Two),e($2,Fwo),e($2,pj),e(pj,Cwo),e($2,Mwo),e(A,Ewo),e(A,I2),e(I2,Cae),e(Cae,ywo),e(I2,wwo),e(I2,_j),e(_j,Awo),e(I2,Lwo),e(A,Bwo),e(A,j2),e(j2,Mae),e(Mae,xwo),e(j2,kwo),e(j2,bj),e(bj,Rwo),e(j2,Swo),e(A,Pwo),e(A,N2),e(N2,Eae),e(Eae,$wo),e(N2,Iwo),e(N2,vj),e(vj,jwo),e(N2,Nwo),e(A,Dwo),e(A,D2),e(D2,yae),e(yae,qwo),e(D2,Owo),e(D2,Tj),e(Tj,Gwo),e(D2,Xwo),e(A,Vwo),e(A,q2),e(q2,wae),e(wae,zwo),e(q2,Wwo),e(q2,Fj),e(Fj,Qwo),e(q2,Hwo),e(Xe,Uwo),e(Xe,O2),e(O2,Jwo),e(O2,Aae),e(Aae,Ywo),e(O2,Kwo),e(O2,Lae),e(Lae,Zwo),e(Xe,e6o),e(Xe,Bae),e(Bae,o6o),e(Xe,r6o),g(Cw,Xe,null),b(c,dke,_),b(c,ud,_),e(ud,G2),e(G2,xae),g(Mw,xae,null),e(ud,t6o),e(ud,kae),e(kae,a6o),b(c,cke,_),b(c,rr,_),g(Ew,rr,null),e(rr,s6o),e(rr,pd),e(pd,n6o),e(pd,Rae),e(Rae,l6o),e(pd,i6o),e(pd,Sae),e(Sae,d6o),e(pd,c6o),e(rr,m6o),e(rr,yw),e(yw,f6o),e(yw,Pae),e(Pae,g6o),e(yw,h6o),e(rr,u6o),e(rr,Jr),g(ww,Jr,null),e(Jr,p6o),e(Jr,$ae),e($ae,_6o),e(Jr,b6o),e(Jr,_d),e(_d,v6o),e(_d,Iae),e(Iae,T6o),e(_d,F6o),e(_d,jae),e(jae,C6o),e(_d,M6o),e(Jr,E6o),e(Jr,Nae),e(Nae,y6o),e(Jr,w6o),g(Aw,Jr,null),e(rr,A6o),e(rr,Ve),g(Lw,Ve,null),e(Ve,L6o),e(Ve,Dae),e(Dae,B6o),e(Ve,x6o),e(Ve,Ja),e(Ja,k6o),e(Ja,qae),e(qae,R6o),e(Ja,S6o),e(Ja,Oae),e(Oae,P6o),e(Ja,$6o),e(Ja,Gae),e(Gae,I6o),e(Ja,j6o),e(Ve,N6o),e(Ve,O),e(O,X2),e(X2,Xae),e(Xae,D6o),e(X2,q6o),e(X2,Cj),e(Cj,O6o),e(X2,G6o),e(O,X6o),e(O,V2),e(V2,Vae),e(Vae,V6o),e(V2,z6o),e(V2,Mj),e(Mj,W6o),e(V2,Q6o),e(O,H6o),e(O,z2),e(z2,zae),e(zae,U6o),e(z2,J6o),e(z2,Ej),e(Ej,Y6o),e(z2,K6o),e(O,Z6o),e(O,W2),e(W2,Wae),e(Wae,eAo),e(W2,oAo),e(W2,yj),e(yj,rAo),e(W2,tAo),e(O,aAo),e(O,Q2),e(Q2,Qae),e(Qae,sAo),e(Q2,nAo),e(Q2,wj),e(wj,lAo),e(Q2,iAo),e(O,dAo),e(O,H2),e(H2,Hae),e(Hae,cAo),e(H2,mAo),e(H2,Aj),e(Aj,fAo),e(H2,gAo),e(O,hAo),e(O,U2),e(U2,Uae),e(Uae,uAo),e(U2,pAo),e(U2,Lj),e(Lj,_Ao),e(U2,bAo),e(O,vAo),e(O,J2),e(J2,Jae),e(Jae,TAo),e(J2,FAo),e(J2,Bj),e(Bj,CAo),e(J2,MAo),e(O,EAo),e(O,Y2),e(Y2,Yae),e(Yae,yAo),e(Y2,wAo),e(Y2,xj),e(xj,AAo),e(Y2,LAo),e(O,BAo),e(O,K2),e(K2,Kae),e(Kae,xAo),e(K2,kAo),e(K2,kj),e(kj,RAo),e(K2,SAo),e(O,PAo),e(O,Z2),e(Z2,Zae),e(Zae,$Ao),e(Z2,IAo),e(Z2,Rj),e(Rj,jAo),e(Z2,NAo),e(O,DAo),e(O,ev),e(ev,ese),e(ese,qAo),e(ev,OAo),e(ev,Sj),e(Sj,GAo),e(ev,XAo),e(O,VAo),e(O,ov),e(ov,ose),e(ose,zAo),e(ov,WAo),e(ov,Pj),e(Pj,QAo),e(ov,HAo),e(O,UAo),e(O,rv),e(rv,rse),e(rse,JAo),e(rv,YAo),e(rv,$j),e($j,KAo),e(rv,ZAo),e(O,e0o),e(O,tv),e(tv,tse),e(tse,o0o),e(tv,r0o),e(tv,Ij),e(Ij,t0o),e(tv,a0o),e(O,s0o),e(O,av),e(av,ase),e(ase,n0o),e(av,l0o),e(av,jj),e(jj,i0o),e(av,d0o),e(O,c0o),e(O,sv),e(sv,sse),e(sse,m0o),e(sv,f0o),e(sv,Nj),e(Nj,g0o),e(sv,h0o),e(O,u0o),e(O,nv),e(nv,nse),e(nse,p0o),e(nv,_0o),e(nv,Dj),e(Dj,b0o),e(nv,v0o),e(O,T0o),e(O,lv),e(lv,lse),e(lse,F0o),e(lv,C0o),e(lv,qj),e(qj,M0o),e(lv,E0o),e(O,y0o),e(O,iv),e(iv,ise),e(ise,w0o),e(iv,A0o),e(iv,Oj),e(Oj,L0o),e(iv,B0o),e(O,x0o),e(O,dv),e(dv,dse),e(dse,k0o),e(dv,R0o),e(dv,Gj),e(Gj,S0o),e(dv,P0o),e(O,$0o),e(O,cv),e(cv,cse),e(cse,I0o),e(cv,j0o),e(cv,Xj),e(Xj,N0o),e(cv,D0o),e(O,q0o),e(O,mv),e(mv,mse),e(mse,O0o),e(mv,G0o),e(mv,Vj),e(Vj,X0o),e(mv,V0o),e(O,z0o),e(O,fv),e(fv,fse),e(fse,W0o),e(fv,Q0o),e(fv,zj),e(zj,H0o),e(fv,U0o),e(O,J0o),e(O,gv),e(gv,gse),e(gse,Y0o),e(gv,K0o),e(gv,Wj),e(Wj,Z0o),e(gv,eLo),e(O,oLo),e(O,hv),e(hv,hse),e(hse,rLo),e(hv,tLo),e(hv,Qj),e(Qj,aLo),e(hv,sLo),e(O,nLo),e(O,uv),e(uv,use),e(use,lLo),e(uv,iLo),e(uv,Hj),e(Hj,dLo),e(uv,cLo),e(O,mLo),e(O,pv),e(pv,pse),e(pse,fLo),e(pv,gLo),e(pv,Uj),e(Uj,hLo),e(pv,uLo),e(Ve,pLo),e(Ve,_v),e(_v,_Lo),e(_v,_se),e(_se,bLo),e(_v,vLo),e(_v,bse),e(bse,TLo),e(Ve,FLo),e(Ve,vse),e(vse,CLo),e(Ve,MLo),g(Bw,Ve,null),b(c,mke,_),b(c,bd,_),e(bd,bv),e(bv,Tse),g(xw,Tse,null),e(bd,ELo),e(bd,Fse),e(Fse,yLo),b(c,fke,_),b(c,tr,_),g(kw,tr,null),e(tr,wLo),e(tr,vd),e(vd,ALo),e(vd,Cse),e(Cse,LLo),e(vd,BLo),e(vd,Mse),e(Mse,xLo),e(vd,kLo),e(tr,RLo),e(tr,Rw),e(Rw,SLo),e(Rw,Ese),e(Ese,PLo),e(Rw,$Lo),e(tr,ILo),e(tr,Yr),g(Sw,Yr,null),e(Yr,jLo),e(Yr,yse),e(yse,NLo),e(Yr,DLo),e(Yr,Td),e(Td,qLo),e(Td,wse),e(wse,OLo),e(Td,GLo),e(Td,Ase),e(Ase,XLo),e(Td,VLo),e(Yr,zLo),e(Yr,Lse),e(Lse,WLo),e(Yr,QLo),g(Pw,Yr,null),e(tr,HLo),e(tr,ze),g($w,ze,null),e(ze,ULo),e(ze,Bse),e(Bse,JLo),e(ze,YLo),e(ze,Ya),e(Ya,KLo),e(Ya,xse),e(xse,ZLo),e(Ya,e7o),e(Ya,kse),e(kse,o7o),e(Ya,r7o),e(Ya,Rse),e(Rse,t7o),e(Ya,a7o),e(ze,s7o),e(ze,da),e(da,vv),e(vv,Sse),e(Sse,n7o),e(vv,l7o),e(vv,Jj),e(Jj,i7o),e(vv,d7o),e(da,c7o),e(da,Tv),e(Tv,Pse),e(Pse,m7o),e(Tv,f7o),e(Tv,Yj),e(Yj,g7o),e(Tv,h7o),e(da,u7o),e(da,Fv),e(Fv,$se),e($se,p7o),e(Fv,_7o),e(Fv,Kj),e(Kj,b7o),e(Fv,v7o),e(da,T7o),e(da,Cv),e(Cv,Ise),e(Ise,F7o),e(Cv,C7o),e(Cv,Zj),e(Zj,M7o),e(Cv,E7o),e(da,y7o),e(da,Mv),e(Mv,jse),e(jse,w7o),e(Mv,A7o),e(Mv,eN),e(eN,L7o),e(Mv,B7o),e(ze,x7o),e(ze,Ev),e(Ev,k7o),e(Ev,Nse),e(Nse,R7o),e(Ev,S7o),e(Ev,Dse),e(Dse,P7o),e(ze,$7o),e(ze,qse),e(qse,I7o),e(ze,j7o),g(Iw,ze,null),b(c,gke,_),b(c,Fd,_),e(Fd,yv),e(yv,Ose),g(jw,Ose,null),e(Fd,N7o),e(Fd,Gse),e(Gse,D7o),b(c,hke,_),b(c,ar,_),g(Nw,ar,null),e(ar,q7o),e(ar,Cd),e(Cd,O7o),e(Cd,Xse),e(Xse,G7o),e(Cd,X7o),e(Cd,Vse),e(Vse,V7o),e(Cd,z7o),e(ar,W7o),e(ar,Dw),e(Dw,Q7o),e(Dw,zse),e(zse,H7o),e(Dw,U7o),e(ar,J7o),e(ar,Kr),g(qw,Kr,null),e(Kr,Y7o),e(Kr,Wse),e(Wse,K7o),e(Kr,Z7o),e(Kr,Md),e(Md,e8o),e(Md,Qse),e(Qse,o8o),e(Md,r8o),e(Md,Hse),e(Hse,t8o),e(Md,a8o),e(Kr,s8o),e(Kr,Use),e(Use,n8o),e(Kr,l8o),g(Ow,Kr,null),e(ar,i8o),e(ar,We),g(Gw,We,null),e(We,d8o),e(We,Jse),e(Jse,c8o),e(We,m8o),e(We,Ka),e(Ka,f8o),e(Ka,Yse),e(Yse,g8o),e(Ka,h8o),e(Ka,Kse),e(Kse,u8o),e(Ka,p8o),e(Ka,Zse),e(Zse,_8o),e(Ka,b8o),e(We,v8o),e(We,D),e(D,wv),e(wv,ene),e(ene,T8o),e(wv,F8o),e(wv,oN),e(oN,C8o),e(wv,M8o),e(D,E8o),e(D,Av),e(Av,one),e(one,y8o),e(Av,w8o),e(Av,rN),e(rN,A8o),e(Av,L8o),e(D,B8o),e(D,Lv),e(Lv,rne),e(rne,x8o),e(Lv,k8o),e(Lv,tN),e(tN,R8o),e(Lv,S8o),e(D,P8o),e(D,Bv),e(Bv,tne),e(tne,$8o),e(Bv,I8o),e(Bv,aN),e(aN,j8o),e(Bv,N8o),e(D,D8o),e(D,xv),e(xv,ane),e(ane,q8o),e(xv,O8o),e(xv,sN),e(sN,G8o),e(xv,X8o),e(D,V8o),e(D,kv),e(kv,sne),e(sne,z8o),e(kv,W8o),e(kv,nN),e(nN,Q8o),e(kv,H8o),e(D,U8o),e(D,Rv),e(Rv,nne),e(nne,J8o),e(Rv,Y8o),e(Rv,lN),e(lN,K8o),e(Rv,Z8o),e(D,e9o),e(D,Sv),e(Sv,lne),e(lne,o9o),e(Sv,r9o),e(Sv,iN),e(iN,t9o),e(Sv,a9o),e(D,s9o),e(D,Pv),e(Pv,ine),e(ine,n9o),e(Pv,l9o),e(Pv,dN),e(dN,i9o),e(Pv,d9o),e(D,c9o),e(D,$v),e($v,dne),e(dne,m9o),e($v,f9o),e($v,cN),e(cN,g9o),e($v,h9o),e(D,u9o),e(D,Iv),e(Iv,cne),e(cne,p9o),e(Iv,_9o),e(Iv,mN),e(mN,b9o),e(Iv,v9o),e(D,T9o),e(D,jv),e(jv,mne),e(mne,F9o),e(jv,C9o),e(jv,fN),e(fN,M9o),e(jv,E9o),e(D,y9o),e(D,Nv),e(Nv,fne),e(fne,w9o),e(Nv,A9o),e(Nv,gN),e(gN,L9o),e(Nv,B9o),e(D,x9o),e(D,Dv),e(Dv,gne),e(gne,k9o),e(Dv,R9o),e(Dv,hN),e(hN,S9o),e(Dv,P9o),e(D,$9o),e(D,qv),e(qv,hne),e(hne,I9o),e(qv,j9o),e(qv,uN),e(uN,N9o),e(qv,D9o),e(D,q9o),e(D,Ov),e(Ov,une),e(une,O9o),e(Ov,G9o),e(Ov,pN),e(pN,X9o),e(Ov,V9o),e(D,z9o),e(D,Gv),e(Gv,pne),e(pne,W9o),e(Gv,Q9o),e(Gv,_N),e(_N,H9o),e(Gv,U9o),e(D,J9o),e(D,Xv),e(Xv,_ne),e(_ne,Y9o),e(Xv,K9o),e(Xv,bN),e(bN,Z9o),e(Xv,eBo),e(D,oBo),e(D,Vv),e(Vv,bne),e(bne,rBo),e(Vv,tBo),e(Vv,vN),e(vN,aBo),e(Vv,sBo),e(D,nBo),e(D,zv),e(zv,vne),e(vne,lBo),e(zv,iBo),e(zv,TN),e(TN,dBo),e(zv,cBo),e(D,mBo),e(D,Wv),e(Wv,Tne),e(Tne,fBo),e(Wv,gBo),e(Wv,FN),e(FN,hBo),e(Wv,uBo),e(D,pBo),e(D,Qv),e(Qv,Fne),e(Fne,_Bo),e(Qv,bBo),e(Qv,CN),e(CN,vBo),e(Qv,TBo),e(D,FBo),e(D,Hv),e(Hv,Cne),e(Cne,CBo),e(Hv,MBo),e(Hv,MN),e(MN,EBo),e(Hv,yBo),e(D,wBo),e(D,Uv),e(Uv,Mne),e(Mne,ABo),e(Uv,LBo),e(Uv,EN),e(EN,BBo),e(Uv,xBo),e(D,kBo),e(D,Jv),e(Jv,Ene),e(Ene,RBo),e(Jv,SBo),e(Jv,yN),e(yN,PBo),e(Jv,$Bo),e(D,IBo),e(D,Yv),e(Yv,yne),e(yne,jBo),e(Yv,NBo),e(Yv,wN),e(wN,DBo),e(Yv,qBo),e(D,OBo),e(D,Kv),e(Kv,wne),e(wne,GBo),e(Kv,XBo),e(Kv,AN),e(AN,VBo),e(Kv,zBo),e(D,WBo),e(D,Zv),e(Zv,Ane),e(Ane,QBo),e(Zv,HBo),e(Zv,LN),e(LN,UBo),e(Zv,JBo),e(D,YBo),e(D,eT),e(eT,Lne),e(Lne,KBo),e(eT,ZBo),e(eT,BN),e(BN,exo),e(eT,oxo),e(D,rxo),e(D,oT),e(oT,Bne),e(Bne,txo),e(oT,axo),e(oT,xN),e(xN,sxo),e(oT,nxo),e(D,lxo),e(D,rT),e(rT,xne),e(xne,ixo),e(rT,dxo),e(rT,kN),e(kN,cxo),e(rT,mxo),e(D,fxo),e(D,tT),e(tT,kne),e(kne,gxo),e(tT,hxo),e(tT,RN),e(RN,uxo),e(tT,pxo),e(D,_xo),e(D,aT),e(aT,Rne),e(Rne,bxo),e(aT,vxo),e(aT,SN),e(SN,Txo),e(aT,Fxo),e(We,Cxo),e(We,sT),e(sT,Mxo),e(sT,Sne),e(Sne,Exo),e(sT,yxo),e(sT,Pne),e(Pne,wxo),e(We,Axo),e(We,$ne),e($ne,Lxo),e(We,Bxo),g(Xw,We,null),b(c,uke,_),b(c,Ed,_),e(Ed,nT),e(nT,Ine),g(Vw,Ine,null),e(Ed,xxo),e(Ed,jne),e(jne,kxo),b(c,pke,_),b(c,sr,_),g(zw,sr,null),e(sr,Rxo),e(sr,yd),e(yd,Sxo),e(yd,Nne),e(Nne,Pxo),e(yd,$xo),e(yd,Dne),e(Dne,Ixo),e(yd,jxo),e(sr,Nxo),e(sr,Ww),e(Ww,Dxo),e(Ww,qne),e(qne,qxo),e(Ww,Oxo),e(sr,Gxo),e(sr,Zr),g(Qw,Zr,null),e(Zr,Xxo),e(Zr,One),e(One,Vxo),e(Zr,zxo),e(Zr,wd),e(wd,Wxo),e(wd,Gne),e(Gne,Qxo),e(wd,Hxo),e(wd,Xne),e(Xne,Uxo),e(wd,Jxo),e(Zr,Yxo),e(Zr,Vne),e(Vne,Kxo),e(Zr,Zxo),g(Hw,Zr,null),e(sr,eko),e(sr,Qe),g(Uw,Qe,null),e(Qe,oko),e(Qe,zne),e(zne,rko),e(Qe,tko),e(Qe,Za),e(Za,ako),e(Za,Wne),e(Wne,sko),e(Za,nko),e(Za,Qne),e(Qne,lko),e(Za,iko),e(Za,Hne),e(Hne,dko),e(Za,cko),e(Qe,mko),e(Qe,R),e(R,lT),e(lT,Une),e(Une,fko),e(lT,gko),e(lT,PN),e(PN,hko),e(lT,uko),e(R,pko),e(R,iT),e(iT,Jne),e(Jne,_ko),e(iT,bko),e(iT,$N),e($N,vko),e(iT,Tko),e(R,Fko),e(R,dT),e(dT,Yne),e(Yne,Cko),e(dT,Mko),e(dT,IN),e(IN,Eko),e(dT,yko),e(R,wko),e(R,cT),e(cT,Kne),e(Kne,Ako),e(cT,Lko),e(cT,jN),e(jN,Bko),e(cT,xko),e(R,kko),e(R,mT),e(mT,Zne),e(Zne,Rko),e(mT,Sko),e(mT,NN),e(NN,Pko),e(mT,$ko),e(R,Iko),e(R,fT),e(fT,ele),e(ele,jko),e(fT,Nko),e(fT,DN),e(DN,Dko),e(fT,qko),e(R,Oko),e(R,gT),e(gT,ole),e(ole,Gko),e(gT,Xko),e(gT,qN),e(qN,Vko),e(gT,zko),e(R,Wko),e(R,hT),e(hT,rle),e(rle,Qko),e(hT,Hko),e(hT,ON),e(ON,Uko),e(hT,Jko),e(R,Yko),e(R,uT),e(uT,tle),e(tle,Kko),e(uT,Zko),e(uT,GN),e(GN,eRo),e(uT,oRo),e(R,rRo),e(R,pT),e(pT,ale),e(ale,tRo),e(pT,aRo),e(pT,XN),e(XN,sRo),e(pT,nRo),e(R,lRo),e(R,_T),e(_T,sle),e(sle,iRo),e(_T,dRo),e(_T,VN),e(VN,cRo),e(_T,mRo),e(R,fRo),e(R,bT),e(bT,nle),e(nle,gRo),e(bT,hRo),e(bT,zN),e(zN,uRo),e(bT,pRo),e(R,_Ro),e(R,vT),e(vT,lle),e(lle,bRo),e(vT,vRo),e(vT,WN),e(WN,TRo),e(vT,FRo),e(R,CRo),e(R,TT),e(TT,ile),e(ile,MRo),e(TT,ERo),e(TT,QN),e(QN,yRo),e(TT,wRo),e(R,ARo),e(R,FT),e(FT,dle),e(dle,LRo),e(FT,BRo),e(FT,HN),e(HN,xRo),e(FT,kRo),e(R,RRo),e(R,CT),e(CT,cle),e(cle,SRo),e(CT,PRo),e(CT,UN),e(UN,$Ro),e(CT,IRo),e(R,jRo),e(R,MT),e(MT,mle),e(mle,NRo),e(MT,DRo),e(MT,JN),e(JN,qRo),e(MT,ORo),e(R,GRo),e(R,ET),e(ET,fle),e(fle,XRo),e(ET,VRo),e(ET,YN),e(YN,zRo),e(ET,WRo),e(R,QRo),e(R,yT),e(yT,gle),e(gle,HRo),e(yT,URo),e(yT,KN),e(KN,JRo),e(yT,YRo),e(R,KRo),e(R,wT),e(wT,hle),e(hle,ZRo),e(wT,eSo),e(wT,ZN),e(ZN,oSo),e(wT,rSo),e(R,tSo),e(R,AT),e(AT,ule),e(ule,aSo),e(AT,sSo),e(AT,eD),e(eD,nSo),e(AT,lSo),e(R,iSo),e(R,LT),e(LT,ple),e(ple,dSo),e(LT,cSo),e(LT,oD),e(oD,mSo),e(LT,fSo),e(R,gSo),e(R,BT),e(BT,_le),e(_le,hSo),e(BT,uSo),e(BT,rD),e(rD,pSo),e(BT,_So),e(R,bSo),e(R,xT),e(xT,ble),e(ble,vSo),e(xT,TSo),e(xT,tD),e(tD,FSo),e(xT,CSo),e(R,MSo),e(R,kT),e(kT,vle),e(vle,ESo),e(kT,ySo),e(kT,aD),e(aD,wSo),e(kT,ASo),e(R,LSo),e(R,RT),e(RT,Tle),e(Tle,BSo),e(RT,xSo),e(RT,sD),e(sD,kSo),e(RT,RSo),e(R,SSo),e(R,ST),e(ST,Fle),e(Fle,PSo),e(ST,$So),e(ST,nD),e(nD,ISo),e(ST,jSo),e(R,NSo),e(R,PT),e(PT,Cle),e(Cle,DSo),e(PT,qSo),e(PT,lD),e(lD,OSo),e(PT,GSo),e(R,XSo),e(R,$T),e($T,Mle),e(Mle,VSo),e($T,zSo),e($T,iD),e(iD,WSo),e($T,QSo),e(R,HSo),e(R,IT),e(IT,Ele),e(Ele,USo),e(IT,JSo),e(IT,dD),e(dD,YSo),e(IT,KSo),e(R,ZSo),e(R,jT),e(jT,yle),e(yle,ePo),e(jT,oPo),e(jT,cD),e(cD,rPo),e(jT,tPo),e(R,aPo),e(R,NT),e(NT,wle),e(wle,sPo),e(NT,nPo),e(NT,mD),e(mD,lPo),e(NT,iPo),e(R,dPo),e(R,DT),e(DT,Ale),e(Ale,cPo),e(DT,mPo),e(DT,fD),e(fD,fPo),e(DT,gPo),e(R,hPo),e(R,qT),e(qT,Lle),e(Lle,uPo),e(qT,pPo),e(qT,gD),e(gD,_Po),e(qT,bPo),e(R,vPo),e(R,OT),e(OT,Ble),e(Ble,TPo),e(OT,FPo),e(OT,hD),e(hD,CPo),e(OT,MPo),e(R,EPo),e(R,GT),e(GT,xle),e(xle,yPo),e(GT,wPo),e(GT,uD),e(uD,APo),e(GT,LPo),e(R,BPo),e(R,XT),e(XT,kle),e(kle,xPo),e(XT,kPo),e(XT,pD),e(pD,RPo),e(XT,SPo),e(R,PPo),e(R,VT),e(VT,Rle),e(Rle,$Po),e(VT,IPo),e(VT,_D),e(_D,jPo),e(VT,NPo),e(R,DPo),e(R,zT),e(zT,Sle),e(Sle,qPo),e(zT,OPo),e(zT,bD),e(bD,GPo),e(zT,XPo),e(Qe,VPo),e(Qe,WT),e(WT,zPo),e(WT,Ple),e(Ple,WPo),e(WT,QPo),e(WT,$le),e($le,HPo),e(Qe,UPo),e(Qe,Ile),e(Ile,JPo),e(Qe,YPo),g(Jw,Qe,null),b(c,_ke,_),b(c,Ad,_),e(Ad,QT),e(QT,jle),g(Yw,jle,null),e(Ad,KPo),e(Ad,Nle),e(Nle,ZPo),b(c,bke,_),b(c,nr,_),g(Kw,nr,null),e(nr,e$o),e(nr,Ld),e(Ld,o$o),e(Ld,Dle),e(Dle,r$o),e(Ld,t$o),e(Ld,qle),e(qle,a$o),e(Ld,s$o),e(nr,n$o),e(nr,Zw),e(Zw,l$o),e(Zw,Ole),e(Ole,i$o),e(Zw,d$o),e(nr,c$o),e(nr,et),g(e6,et,null),e(et,m$o),e(et,Gle),e(Gle,f$o),e(et,g$o),e(et,Bd),e(Bd,h$o),e(Bd,Xle),e(Xle,u$o),e(Bd,p$o),e(Bd,Vle),e(Vle,_$o),e(Bd,b$o),e(et,v$o),e(et,zle),e(zle,T$o),e(et,F$o),g(o6,et,null),e(nr,C$o),e(nr,He),g(r6,He,null),e(He,M$o),e(He,Wle),e(Wle,E$o),e(He,y$o),e(He,es),e(es,w$o),e(es,Qle),e(Qle,A$o),e(es,L$o),e(es,Hle),e(Hle,B$o),e(es,x$o),e(es,Ule),e(Ule,k$o),e(es,R$o),e(He,S$o),e(He,Jle),e(Jle,HT),e(HT,Yle),e(Yle,P$o),e(HT,$$o),e(HT,vD),e(vD,I$o),e(HT,j$o),e(He,N$o),e(He,UT),e(UT,D$o),e(UT,Kle),e(Kle,q$o),e(UT,O$o),e(UT,Zle),e(Zle,G$o),e(He,X$o),e(He,eie),e(eie,V$o),e(He,z$o),g(t6,He,null),b(c,vke,_),b(c,xd,_),e(xd,JT),e(JT,oie),g(a6,oie,null),e(xd,W$o),e(xd,rie),e(rie,Q$o),b(c,Tke,_),b(c,lr,_),g(s6,lr,null),e(lr,H$o),e(lr,kd),e(kd,U$o),e(kd,tie),e(tie,J$o),e(kd,Y$o),e(kd,aie),e(aie,K$o),e(kd,Z$o),e(lr,eIo),e(lr,n6),e(n6,oIo),e(n6,sie),e(sie,rIo),e(n6,tIo),e(lr,aIo),e(lr,ot),g(l6,ot,null),e(ot,sIo),e(ot,nie),e(nie,nIo),e(ot,lIo),e(ot,Rd),e(Rd,iIo),e(Rd,lie),e(lie,dIo),e(Rd,cIo),e(Rd,iie),e(iie,mIo),e(Rd,fIo),e(ot,gIo),e(ot,die),e(die,hIo),e(ot,uIo),g(i6,ot,null),e(lr,pIo),e(lr,Ue),g(d6,Ue,null),e(Ue,_Io),e(Ue,cie),e(cie,bIo),e(Ue,vIo),e(Ue,os),e(os,TIo),e(os,mie),e(mie,FIo),e(os,CIo),e(os,fie),e(fie,MIo),e(os,EIo),e(os,gie),e(gie,yIo),e(os,wIo),e(Ue,AIo),e(Ue,fe),e(fe,YT),e(YT,hie),e(hie,LIo),e(YT,BIo),e(YT,TD),e(TD,xIo),e(YT,kIo),e(fe,RIo),e(fe,KT),e(KT,uie),e(uie,SIo),e(KT,PIo),e(KT,FD),e(FD,$Io),e(KT,IIo),e(fe,jIo),e(fe,qn),e(qn,pie),e(pie,NIo),e(qn,DIo),e(qn,CD),e(CD,qIo),e(qn,OIo),e(qn,MD),e(MD,GIo),e(qn,XIo),e(fe,VIo),e(fe,ZT),e(ZT,_ie),e(_ie,zIo),e(ZT,WIo),e(ZT,ED),e(ED,QIo),e(ZT,HIo),e(fe,UIo),e(fe,fa),e(fa,bie),e(bie,JIo),e(fa,YIo),e(fa,yD),e(yD,KIo),e(fa,ZIo),e(fa,wD),e(wD,ejo),e(fa,ojo),e(fa,AD),e(AD,rjo),e(fa,tjo),e(fe,ajo),e(fe,e1),e(e1,vie),e(vie,sjo),e(e1,njo),e(e1,LD),e(LD,ljo),e(e1,ijo),e(fe,djo),e(fe,o1),e(o1,Tie),e(Tie,cjo),e(o1,mjo),e(o1,BD),e(BD,fjo),e(o1,gjo),e(fe,hjo),e(fe,r1),e(r1,Fie),e(Fie,ujo),e(r1,pjo),e(r1,xD),e(xD,_jo),e(r1,bjo),e(fe,vjo),e(fe,t1),e(t1,Cie),e(Cie,Tjo),e(t1,Fjo),e(t1,kD),e(kD,Cjo),e(t1,Mjo),e(fe,Ejo),e(fe,a1),e(a1,Mie),e(Mie,yjo),e(a1,wjo),e(a1,RD),e(RD,Ajo),e(a1,Ljo),e(fe,Bjo),e(fe,s1),e(s1,Eie),e(Eie,xjo),e(s1,kjo),e(s1,SD),e(SD,Rjo),e(s1,Sjo),e(Ue,Pjo),e(Ue,n1),e(n1,$jo),e(n1,yie),e(yie,Ijo),e(n1,jjo),e(n1,wie),e(wie,Njo),e(Ue,Djo),e(Ue,Aie),e(Aie,qjo),e(Ue,Ojo),g(c6,Ue,null),b(c,Fke,_),b(c,Sd,_),e(Sd,l1),e(l1,Lie),g(m6,Lie,null),e(Sd,Gjo),e(Sd,Bie),e(Bie,Xjo),b(c,Cke,_),b(c,ir,_),g(f6,ir,null),e(ir,Vjo),e(ir,Pd),e(Pd,zjo),e(Pd,xie),e(xie,Wjo),e(Pd,Qjo),e(Pd,kie),e(kie,Hjo),e(Pd,Ujo),e(ir,Jjo),e(ir,g6),e(g6,Yjo),e(g6,Rie),e(Rie,Kjo),e(g6,Zjo),e(ir,eNo),e(ir,rt),g(h6,rt,null),e(rt,oNo),e(rt,Sie),e(Sie,rNo),e(rt,tNo),e(rt,$d),e($d,aNo),e($d,Pie),e(Pie,sNo),e($d,nNo),e($d,$ie),e($ie,lNo),e($d,iNo),e(rt,dNo),e(rt,Iie),e(Iie,cNo),e(rt,mNo),g(u6,rt,null),e(ir,fNo),e(ir,Je),g(p6,Je,null),e(Je,gNo),e(Je,jie),e(jie,hNo),e(Je,uNo),e(Je,rs),e(rs,pNo),e(rs,Nie),e(Nie,_No),e(rs,bNo),e(rs,Die),e(Die,vNo),e(rs,TNo),e(rs,qie),e(qie,FNo),e(rs,CNo),e(Je,MNo),e(Je,Oie),e(Oie,i1),e(i1,Gie),e(Gie,ENo),e(i1,yNo),e(i1,PD),e(PD,wNo),e(i1,ANo),e(Je,LNo),e(Je,d1),e(d1,BNo),e(d1,Xie),e(Xie,xNo),e(d1,kNo),e(d1,Vie),e(Vie,RNo),e(Je,SNo),e(Je,zie),e(zie,PNo),e(Je,$No),g(_6,Je,null),b(c,Mke,_),b(c,Id,_),e(Id,c1),e(c1,Wie),g(b6,Wie,null),e(Id,INo),e(Id,Qie),e(Qie,jNo),b(c,Eke,_),b(c,dr,_),g(v6,dr,null),e(dr,NNo),e(dr,jd),e(jd,DNo),e(jd,Hie),e(Hie,qNo),e(jd,ONo),e(jd,Uie),e(Uie,GNo),e(jd,XNo),e(dr,VNo),e(dr,T6),e(T6,zNo),e(T6,Jie),e(Jie,WNo),e(T6,QNo),e(dr,HNo),e(dr,tt),g(F6,tt,null),e(tt,UNo),e(tt,Yie),e(Yie,JNo),e(tt,YNo),e(tt,Nd),e(Nd,KNo),e(Nd,Kie),e(Kie,ZNo),e(Nd,eDo),e(Nd,Zie),e(Zie,oDo),e(Nd,rDo),e(tt,tDo),e(tt,ede),e(ede,aDo),e(tt,sDo),g(C6,tt,null),e(dr,nDo),e(dr,Ye),g(M6,Ye,null),e(Ye,lDo),e(Ye,ode),e(ode,iDo),e(Ye,dDo),e(Ye,ts),e(ts,cDo),e(ts,rde),e(rde,mDo),e(ts,fDo),e(ts,tde),e(tde,gDo),e(ts,hDo),e(ts,ade),e(ade,uDo),e(ts,pDo),e(Ye,_Do),e(Ye,ke),e(ke,m1),e(m1,sde),e(sde,bDo),e(m1,vDo),e(m1,$D),e($D,TDo),e(m1,FDo),e(ke,CDo),e(ke,f1),e(f1,nde),e(nde,MDo),e(f1,EDo),e(f1,ID),e(ID,yDo),e(f1,wDo),e(ke,ADo),e(ke,g1),e(g1,lde),e(lde,LDo),e(g1,BDo),e(g1,jD),e(jD,xDo),e(g1,kDo),e(ke,RDo),e(ke,h1),e(h1,ide),e(ide,SDo),e(h1,PDo),e(h1,ND),e(ND,$Do),e(h1,IDo),e(ke,jDo),e(ke,u1),e(u1,dde),e(dde,NDo),e(u1,DDo),e(u1,DD),e(DD,qDo),e(u1,ODo),e(ke,GDo),e(ke,p1),e(p1,cde),e(cde,XDo),e(p1,VDo),e(p1,qD),e(qD,zDo),e(p1,WDo),e(ke,QDo),e(ke,_1),e(_1,mde),e(mde,HDo),e(_1,UDo),e(_1,OD),e(OD,JDo),e(_1,YDo),e(ke,KDo),e(ke,b1),e(b1,fde),e(fde,ZDo),e(b1,eqo),e(b1,GD),e(GD,oqo),e(b1,rqo),e(Ye,tqo),e(Ye,v1),e(v1,aqo),e(v1,gde),e(gde,sqo),e(v1,nqo),e(v1,hde),e(hde,lqo),e(Ye,iqo),e(Ye,ude),e(ude,dqo),e(Ye,cqo),g(E6,Ye,null),b(c,yke,_),b(c,Dd,_),e(Dd,T1),e(T1,pde),g(y6,pde,null),e(Dd,mqo),e(Dd,_de),e(_de,fqo),b(c,wke,_),b(c,cr,_),g(w6,cr,null),e(cr,gqo),e(cr,qd),e(qd,hqo),e(qd,bde),e(bde,uqo),e(qd,pqo),e(qd,vde),e(vde,_qo),e(qd,bqo),e(cr,vqo),e(cr,A6),e(A6,Tqo),e(A6,Tde),e(Tde,Fqo),e(A6,Cqo),e(cr,Mqo),e(cr,at),g(L6,at,null),e(at,Eqo),e(at,Fde),e(Fde,yqo),e(at,wqo),e(at,Od),e(Od,Aqo),e(Od,Cde),e(Cde,Lqo),e(Od,Bqo),e(Od,Mde),e(Mde,xqo),e(Od,kqo),e(at,Rqo),e(at,Ede),e(Ede,Sqo),e(at,Pqo),g(B6,at,null),e(cr,$qo),e(cr,Ke),g(x6,Ke,null),e(Ke,Iqo),e(Ke,yde),e(yde,jqo),e(Ke,Nqo),e(Ke,as),e(as,Dqo),e(as,wde),e(wde,qqo),e(as,Oqo),e(as,Ade),e(Ade,Gqo),e(as,Xqo),e(as,Lde),e(Lde,Vqo),e(as,zqo),e(Ke,Wqo),e(Ke,ss),e(ss,F1),e(F1,Bde),e(Bde,Qqo),e(F1,Hqo),e(F1,XD),e(XD,Uqo),e(F1,Jqo),e(ss,Yqo),e(ss,C1),e(C1,xde),e(xde,Kqo),e(C1,Zqo),e(C1,VD),e(VD,eOo),e(C1,oOo),e(ss,rOo),e(ss,M1),e(M1,kde),e(kde,tOo),e(M1,aOo),e(M1,zD),e(zD,sOo),e(M1,nOo),e(ss,lOo),e(ss,E1),e(E1,Rde),e(Rde,iOo),e(E1,dOo),e(E1,WD),e(WD,cOo),e(E1,mOo),e(Ke,fOo),e(Ke,y1),e(y1,gOo),e(y1,Sde),e(Sde,hOo),e(y1,uOo),e(y1,Pde),e(Pde,pOo),e(Ke,_Oo),e(Ke,$de),e($de,bOo),e(Ke,vOo),g(k6,Ke,null),b(c,Ake,_),b(c,Gd,_),e(Gd,w1),e(w1,Ide),g(R6,Ide,null),e(Gd,TOo),e(Gd,jde),e(jde,FOo),b(c,Lke,_),b(c,mr,_),g(S6,mr,null),e(mr,COo),e(mr,Xd),e(Xd,MOo),e(Xd,Nde),e(Nde,EOo),e(Xd,yOo),e(Xd,Dde),e(Dde,wOo),e(Xd,AOo),e(mr,LOo),e(mr,P6),e(P6,BOo),e(P6,qde),e(qde,xOo),e(P6,kOo),e(mr,ROo),e(mr,st),g($6,st,null),e(st,SOo),e(st,Ode),e(Ode,POo),e(st,$Oo),e(st,Vd),e(Vd,IOo),e(Vd,Gde),e(Gde,jOo),e(Vd,NOo),e(Vd,Xde),e(Xde,DOo),e(Vd,qOo),e(st,OOo),e(st,Vde),e(Vde,GOo),e(st,XOo),g(I6,st,null),e(mr,VOo),e(mr,Ze),g(j6,Ze,null),e(Ze,zOo),e(Ze,zde),e(zde,WOo),e(Ze,QOo),e(Ze,ns),e(ns,HOo),e(ns,Wde),e(Wde,UOo),e(ns,JOo),e(ns,Qde),e(Qde,YOo),e(ns,KOo),e(ns,Hde),e(Hde,ZOo),e(ns,eGo),e(Ze,oGo),e(Ze,Re),e(Re,A1),e(A1,Ude),e(Ude,rGo),e(A1,tGo),e(A1,QD),e(QD,aGo),e(A1,sGo),e(Re,nGo),e(Re,L1),e(L1,Jde),e(Jde,lGo),e(L1,iGo),e(L1,HD),e(HD,dGo),e(L1,cGo),e(Re,mGo),e(Re,B1),e(B1,Yde),e(Yde,fGo),e(B1,gGo),e(B1,UD),e(UD,hGo),e(B1,uGo),e(Re,pGo),e(Re,x1),e(x1,Kde),e(Kde,_Go),e(x1,bGo),e(x1,JD),e(JD,vGo),e(x1,TGo),e(Re,FGo),e(Re,k1),e(k1,Zde),e(Zde,CGo),e(k1,MGo),e(k1,YD),e(YD,EGo),e(k1,yGo),e(Re,wGo),e(Re,R1),e(R1,ece),e(ece,AGo),e(R1,LGo),e(R1,KD),e(KD,BGo),e(R1,xGo),e(Re,kGo),e(Re,S1),e(S1,oce),e(oce,RGo),e(S1,SGo),e(S1,ZD),e(ZD,PGo),e(S1,$Go),e(Re,IGo),e(Re,P1),e(P1,rce),e(rce,jGo),e(P1,NGo),e(P1,eq),e(eq,DGo),e(P1,qGo),e(Ze,OGo),e(Ze,$1),e($1,GGo),e($1,tce),e(tce,XGo),e($1,VGo),e($1,ace),e(ace,zGo),e(Ze,WGo),e(Ze,sce),e(sce,QGo),e(Ze,HGo),g(N6,Ze,null),b(c,Bke,_),b(c,zd,_),e(zd,I1),e(I1,nce),g(D6,nce,null),e(zd,UGo),e(zd,lce),e(lce,JGo),b(c,xke,_),b(c,fr,_),g(q6,fr,null),e(fr,YGo),e(fr,Wd),e(Wd,KGo),e(Wd,ice),e(ice,ZGo),e(Wd,eXo),e(Wd,dce),e(dce,oXo),e(Wd,rXo),e(fr,tXo),e(fr,O6),e(O6,aXo),e(O6,cce),e(cce,sXo),e(O6,nXo),e(fr,lXo),e(fr,nt),g(G6,nt,null),e(nt,iXo),e(nt,mce),e(mce,dXo),e(nt,cXo),e(nt,Qd),e(Qd,mXo),e(Qd,fce),e(fce,fXo),e(Qd,gXo),e(Qd,gce),e(gce,hXo),e(Qd,uXo),e(nt,pXo),e(nt,hce),e(hce,_Xo),e(nt,bXo),g(X6,nt,null),e(fr,vXo),e(fr,eo),g(V6,eo,null),e(eo,TXo),e(eo,uce),e(uce,FXo),e(eo,CXo),e(eo,ls),e(ls,MXo),e(ls,pce),e(pce,EXo),e(ls,yXo),e(ls,_ce),e(_ce,wXo),e(ls,AXo),e(ls,bce),e(bce,LXo),e(ls,BXo),e(eo,xXo),e(eo,z6),e(z6,j1),e(j1,vce),e(vce,kXo),e(j1,RXo),e(j1,oq),e(oq,SXo),e(j1,PXo),e(z6,$Xo),e(z6,N1),e(N1,Tce),e(Tce,IXo),e(N1,jXo),e(N1,rq),e(rq,NXo),e(N1,DXo),e(eo,qXo),e(eo,D1),e(D1,OXo),e(D1,Fce),e(Fce,GXo),e(D1,XXo),e(D1,Cce),e(Cce,VXo),e(eo,zXo),e(eo,Mce),e(Mce,WXo),e(eo,QXo),g(W6,eo,null),b(c,kke,_),b(c,Hd,_),e(Hd,q1),e(q1,Ece),g(Q6,Ece,null),e(Hd,HXo),e(Hd,yce),e(yce,UXo),b(c,Rke,_),b(c,gr,_),g(H6,gr,null),e(gr,JXo),e(gr,Ud),e(Ud,YXo),e(Ud,wce),e(wce,KXo),e(Ud,ZXo),e(Ud,Ace),e(Ace,eVo),e(Ud,oVo),e(gr,rVo),e(gr,U6),e(U6,tVo),e(U6,Lce),e(Lce,aVo),e(U6,sVo),e(gr,nVo),e(gr,lt),g(J6,lt,null),e(lt,lVo),e(lt,Bce),e(Bce,iVo),e(lt,dVo),e(lt,Jd),e(Jd,cVo),e(Jd,xce),e(xce,mVo),e(Jd,fVo),e(Jd,kce),e(kce,gVo),e(Jd,hVo),e(lt,uVo),e(lt,Rce),e(Rce,pVo),e(lt,_Vo),g(Y6,lt,null),e(gr,bVo),e(gr,oo),g(K6,oo,null),e(oo,vVo),e(oo,Sce),e(Sce,TVo),e(oo,FVo),e(oo,is),e(is,CVo),e(is,Pce),e(Pce,MVo),e(is,EVo),e(is,$ce),e($ce,yVo),e(is,wVo),e(is,Ice),e(Ice,AVo),e(is,LVo),e(oo,BVo),e(oo,ds),e(ds,O1),e(O1,jce),e(jce,xVo),e(O1,kVo),e(O1,tq),e(tq,RVo),e(O1,SVo),e(ds,PVo),e(ds,G1),e(G1,Nce),e(Nce,$Vo),e(G1,IVo),e(G1,aq),e(aq,jVo),e(G1,NVo),e(ds,DVo),e(ds,X1),e(X1,Dce),e(Dce,qVo),e(X1,OVo),e(X1,sq),e(sq,GVo),e(X1,XVo),e(ds,VVo),e(ds,V1),e(V1,qce),e(qce,zVo),e(V1,WVo),e(V1,nq),e(nq,QVo),e(V1,HVo),e(oo,UVo),e(oo,z1),e(z1,JVo),e(z1,Oce),e(Oce,YVo),e(z1,KVo),e(z1,Gce),e(Gce,ZVo),e(oo,ezo),e(oo,Xce),e(Xce,ozo),e(oo,rzo),g(Z6,oo,null),b(c,Ske,_),b(c,Yd,_),e(Yd,W1),e(W1,Vce),g(eA,Vce,null),e(Yd,tzo),e(Yd,zce),e(zce,azo),b(c,Pke,_),b(c,hr,_),g(oA,hr,null),e(hr,szo),e(hr,Kd),e(Kd,nzo),e(Kd,Wce),e(Wce,lzo),e(Kd,izo),e(Kd,Qce),e(Qce,dzo),e(Kd,czo),e(hr,mzo),e(hr,rA),e(rA,fzo),e(rA,Hce),e(Hce,gzo),e(rA,hzo),e(hr,uzo),e(hr,it),g(tA,it,null),e(it,pzo),e(it,Uce),e(Uce,_zo),e(it,bzo),e(it,Zd),e(Zd,vzo),e(Zd,Jce),e(Jce,Tzo),e(Zd,Fzo),e(Zd,Yce),e(Yce,Czo),e(Zd,Mzo),e(it,Ezo),e(it,Kce),e(Kce,yzo),e(it,wzo),g(aA,it,null),e(hr,Azo),e(hr,ro),g(sA,ro,null),e(ro,Lzo),e(ro,Zce),e(Zce,Bzo),e(ro,xzo),e(ro,cs),e(cs,kzo),e(cs,eme),e(eme,Rzo),e(cs,Szo),e(cs,ome),e(ome,Pzo),e(cs,$zo),e(cs,rme),e(rme,Izo),e(cs,jzo),e(ro,Nzo),e(ro,ec),e(ec,Q1),e(Q1,tme),e(tme,Dzo),e(Q1,qzo),e(Q1,lq),e(lq,Ozo),e(Q1,Gzo),e(ec,Xzo),e(ec,H1),e(H1,ame),e(ame,Vzo),e(H1,zzo),e(H1,iq),e(iq,Wzo),e(H1,Qzo),e(ec,Hzo),e(ec,U1),e(U1,sme),e(sme,Uzo),e(U1,Jzo),e(U1,dq),e(dq,Yzo),e(U1,Kzo),e(ro,Zzo),e(ro,J1),e(J1,eWo),e(J1,nme),e(nme,oWo),e(J1,rWo),e(J1,lme),e(lme,tWo),e(ro,aWo),e(ro,ime),e(ime,sWo),e(ro,nWo),g(nA,ro,null),b(c,$ke,_),b(c,oc,_),e(oc,Y1),e(Y1,dme),g(lA,dme,null),e(oc,lWo),e(oc,cme),e(cme,iWo),b(c,Ike,_),b(c,ur,_),g(iA,ur,null),e(ur,dWo),e(ur,rc),e(rc,cWo),e(rc,mme),e(mme,mWo),e(rc,fWo),e(rc,fme),e(fme,gWo),e(rc,hWo),e(ur,uWo),e(ur,dA),e(dA,pWo),e(dA,gme),e(gme,_Wo),e(dA,bWo),e(ur,vWo),e(ur,dt),g(cA,dt,null),e(dt,TWo),e(dt,hme),e(hme,FWo),e(dt,CWo),e(dt,tc),e(tc,MWo),e(tc,ume),e(ume,EWo),e(tc,yWo),e(tc,pme),e(pme,wWo),e(tc,AWo),e(dt,LWo),e(dt,_me),e(_me,BWo),e(dt,xWo),g(mA,dt,null),e(ur,kWo),e(ur,to),g(fA,to,null),e(to,RWo),e(to,bme),e(bme,SWo),e(to,PWo),e(to,ms),e(ms,$Wo),e(ms,vme),e(vme,IWo),e(ms,jWo),e(ms,Tme),e(Tme,NWo),e(ms,DWo),e(ms,Fme),e(Fme,qWo),e(ms,OWo),e(to,GWo),e(to,Cme),e(Cme,K1),e(K1,Mme),e(Mme,XWo),e(K1,VWo),e(K1,cq),e(cq,zWo),e(K1,WWo),e(to,QWo),e(to,Z1),e(Z1,HWo),e(Z1,Eme),e(Eme,UWo),e(Z1,JWo),e(Z1,yme),e(yme,YWo),e(to,KWo),e(to,wme),e(wme,ZWo),e(to,eQo),g(gA,to,null),b(c,jke,_),b(c,ac,_),e(ac,eF),e(eF,Ame),g(hA,Ame,null),e(ac,oQo),e(ac,Lme),e(Lme,rQo),b(c,Nke,_),b(c,pr,_),g(uA,pr,null),e(pr,tQo),e(pr,sc),e(sc,aQo),e(sc,Bme),e(Bme,sQo),e(sc,nQo),e(sc,xme),e(xme,lQo),e(sc,iQo),e(pr,dQo),e(pr,pA),e(pA,cQo),e(pA,kme),e(kme,mQo),e(pA,fQo),e(pr,gQo),e(pr,ct),g(_A,ct,null),e(ct,hQo),e(ct,Rme),e(Rme,uQo),e(ct,pQo),e(ct,nc),e(nc,_Qo),e(nc,Sme),e(Sme,bQo),e(nc,vQo),e(nc,Pme),e(Pme,TQo),e(nc,FQo),e(ct,CQo),e(ct,$me),e($me,MQo),e(ct,EQo),g(bA,ct,null),e(pr,yQo),e(pr,ao),g(vA,ao,null),e(ao,wQo),e(ao,Ime),e(Ime,AQo),e(ao,LQo),e(ao,fs),e(fs,BQo),e(fs,jme),e(jme,xQo),e(fs,kQo),e(fs,Nme),e(Nme,RQo),e(fs,SQo),e(fs,Dme),e(Dme,PQo),e(fs,$Qo),e(ao,IQo),e(ao,qme),e(qme,oF),e(oF,Ome),e(Ome,jQo),e(oF,NQo),e(oF,mq),e(mq,DQo),e(oF,qQo),e(ao,OQo),e(ao,rF),e(rF,GQo),e(rF,Gme),e(Gme,XQo),e(rF,VQo),e(rF,Xme),e(Xme,zQo),e(ao,WQo),e(ao,Vme),e(Vme,QQo),e(ao,HQo),g(TA,ao,null),b(c,Dke,_),b(c,lc,_),e(lc,tF),e(tF,zme),g(FA,zme,null),e(lc,UQo),e(lc,Wme),e(Wme,JQo),b(c,qke,_),b(c,_r,_),g(CA,_r,null),e(_r,YQo),e(_r,ic),e(ic,KQo),e(ic,Qme),e(Qme,ZQo),e(ic,eHo),e(ic,Hme),e(Hme,oHo),e(ic,rHo),e(_r,tHo),e(_r,MA),e(MA,aHo),e(MA,Ume),e(Ume,sHo),e(MA,nHo),e(_r,lHo),e(_r,mt),g(EA,mt,null),e(mt,iHo),e(mt,Jme),e(Jme,dHo),e(mt,cHo),e(mt,dc),e(dc,mHo),e(dc,Yme),e(Yme,fHo),e(dc,gHo),e(dc,Kme),e(Kme,hHo),e(dc,uHo),e(mt,pHo),e(mt,Zme),e(Zme,_Ho),e(mt,bHo),g(yA,mt,null),e(_r,vHo),e(_r,so),g(wA,so,null),e(so,THo),e(so,efe),e(efe,FHo),e(so,CHo),e(so,gs),e(gs,MHo),e(gs,ofe),e(ofe,EHo),e(gs,yHo),e(gs,rfe),e(rfe,wHo),e(gs,AHo),e(gs,tfe),e(tfe,LHo),e(gs,BHo),e(so,xHo),e(so,AA),e(AA,aF),e(aF,afe),e(afe,kHo),e(aF,RHo),e(aF,fq),e(fq,SHo),e(aF,PHo),e(AA,$Ho),e(AA,sF),e(sF,sfe),e(sfe,IHo),e(sF,jHo),e(sF,gq),e(gq,NHo),e(sF,DHo),e(so,qHo),e(so,nF),e(nF,OHo),e(nF,nfe),e(nfe,GHo),e(nF,XHo),e(nF,lfe),e(lfe,VHo),e(so,zHo),e(so,ife),e(ife,WHo),e(so,QHo),g(LA,so,null),b(c,Oke,_),b(c,cc,_),e(cc,lF),e(lF,dfe),g(BA,dfe,null),e(cc,HHo),e(cc,cfe),e(cfe,UHo),b(c,Gke,_),b(c,br,_),g(xA,br,null),e(br,JHo),e(br,mc),e(mc,YHo),e(mc,mfe),e(mfe,KHo),e(mc,ZHo),e(mc,ffe),e(ffe,eUo),e(mc,oUo),e(br,rUo),e(br,kA),e(kA,tUo),e(kA,gfe),e(gfe,aUo),e(kA,sUo),e(br,nUo),e(br,ft),g(RA,ft,null),e(ft,lUo),e(ft,hfe),e(hfe,iUo),e(ft,dUo),e(ft,fc),e(fc,cUo),e(fc,ufe),e(ufe,mUo),e(fc,fUo),e(fc,pfe),e(pfe,gUo),e(fc,hUo),e(ft,uUo),e(ft,_fe),e(_fe,pUo),e(ft,_Uo),g(SA,ft,null),e(br,bUo),e(br,no),g(PA,no,null),e(no,vUo),e(no,bfe),e(bfe,TUo),e(no,FUo),e(no,hs),e(hs,CUo),e(hs,vfe),e(vfe,MUo),e(hs,EUo),e(hs,Tfe),e(Tfe,yUo),e(hs,wUo),e(hs,Ffe),e(Ffe,AUo),e(hs,LUo),e(no,BUo),e(no,Cfe),e(Cfe,iF),e(iF,Mfe),e(Mfe,xUo),e(iF,kUo),e(iF,hq),e(hq,RUo),e(iF,SUo),e(no,PUo),e(no,dF),e(dF,$Uo),e(dF,Efe),e(Efe,IUo),e(dF,jUo),e(dF,yfe),e(yfe,NUo),e(no,DUo),e(no,wfe),e(wfe,qUo),e(no,OUo),g($A,no,null),b(c,Xke,_),b(c,gc,_),e(gc,cF),e(cF,Afe),g(IA,Afe,null),e(gc,GUo),e(gc,Lfe),e(Lfe,XUo),b(c,Vke,_),b(c,vr,_),g(jA,vr,null),e(vr,VUo),e(vr,hc),e(hc,zUo),e(hc,Bfe),e(Bfe,WUo),e(hc,QUo),e(hc,xfe),e(xfe,HUo),e(hc,UUo),e(vr,JUo),e(vr,NA),e(NA,YUo),e(NA,kfe),e(kfe,KUo),e(NA,ZUo),e(vr,eJo),e(vr,gt),g(DA,gt,null),e(gt,oJo),e(gt,Rfe),e(Rfe,rJo),e(gt,tJo),e(gt,uc),e(uc,aJo),e(uc,Sfe),e(Sfe,sJo),e(uc,nJo),e(uc,Pfe),e(Pfe,lJo),e(uc,iJo),e(gt,dJo),e(gt,$fe),e($fe,cJo),e(gt,mJo),g(qA,gt,null),e(vr,fJo),e(vr,ho),g(OA,ho,null),e(ho,gJo),e(ho,Ife),e(Ife,hJo),e(ho,uJo),e(ho,us),e(us,pJo),e(us,jfe),e(jfe,_Jo),e(us,bJo),e(us,Nfe),e(Nfe,vJo),e(us,TJo),e(us,Dfe),e(Dfe,FJo),e(us,CJo),e(ho,MJo),e(ho,B),e(B,mF),e(mF,qfe),e(qfe,EJo),e(mF,yJo),e(mF,uq),e(uq,wJo),e(mF,AJo),e(B,LJo),e(B,fF),e(fF,Ofe),e(Ofe,BJo),e(fF,xJo),e(fF,pq),e(pq,kJo),e(fF,RJo),e(B,SJo),e(B,gF),e(gF,Gfe),e(Gfe,PJo),e(gF,$Jo),e(gF,_q),e(_q,IJo),e(gF,jJo),e(B,NJo),e(B,hF),e(hF,Xfe),e(Xfe,DJo),e(hF,qJo),e(hF,bq),e(bq,OJo),e(hF,GJo),e(B,XJo),e(B,uF),e(uF,Vfe),e(Vfe,VJo),e(uF,zJo),e(uF,vq),e(vq,WJo),e(uF,QJo),e(B,HJo),e(B,pF),e(pF,zfe),e(zfe,UJo),e(pF,JJo),e(pF,Tq),e(Tq,YJo),e(pF,KJo),e(B,ZJo),e(B,_F),e(_F,Wfe),e(Wfe,eYo),e(_F,oYo),e(_F,Fq),e(Fq,rYo),e(_F,tYo),e(B,aYo),e(B,bF),e(bF,Qfe),e(Qfe,sYo),e(bF,nYo),e(bF,Cq),e(Cq,lYo),e(bF,iYo),e(B,dYo),e(B,vF),e(vF,Hfe),e(Hfe,cYo),e(vF,mYo),e(vF,Mq),e(Mq,fYo),e(vF,gYo),e(B,hYo),e(B,TF),e(TF,Ufe),e(Ufe,uYo),e(TF,pYo),e(TF,Eq),e(Eq,_Yo),e(TF,bYo),e(B,vYo),e(B,FF),e(FF,Jfe),e(Jfe,TYo),e(FF,FYo),e(FF,yq),e(yq,CYo),e(FF,MYo),e(B,EYo),e(B,CF),e(CF,Yfe),e(Yfe,yYo),e(CF,wYo),e(CF,wq),e(wq,AYo),e(CF,LYo),e(B,BYo),e(B,MF),e(MF,Kfe),e(Kfe,xYo),e(MF,kYo),e(MF,Aq),e(Aq,RYo),e(MF,SYo),e(B,PYo),e(B,EF),e(EF,Zfe),e(Zfe,$Yo),e(EF,IYo),e(EF,Lq),e(Lq,jYo),e(EF,NYo),e(B,DYo),e(B,yF),e(yF,ege),e(ege,qYo),e(yF,OYo),e(yF,Bq),e(Bq,GYo),e(yF,XYo),e(B,VYo),e(B,wF),e(wF,oge),e(oge,zYo),e(wF,WYo),e(wF,xq),e(xq,QYo),e(wF,HYo),e(B,UYo),e(B,On),e(On,rge),e(rge,JYo),e(On,YYo),e(On,kq),e(kq,KYo),e(On,ZYo),e(On,Rq),e(Rq,eKo),e(On,oKo),e(B,rKo),e(B,AF),e(AF,tge),e(tge,tKo),e(AF,aKo),e(AF,Sq),e(Sq,sKo),e(AF,nKo),e(B,lKo),e(B,LF),e(LF,age),e(age,iKo),e(LF,dKo),e(LF,Pq),e(Pq,cKo),e(LF,mKo),e(B,fKo),e(B,BF),e(BF,sge),e(sge,gKo),e(BF,hKo),e(BF,$q),e($q,uKo),e(BF,pKo),e(B,_Ko),e(B,xF),e(xF,nge),e(nge,bKo),e(xF,vKo),e(xF,Iq),e(Iq,TKo),e(xF,FKo),e(B,CKo),e(B,kF),e(kF,lge),e(lge,MKo),e(kF,EKo),e(kF,jq),e(jq,yKo),e(kF,wKo),e(B,AKo),e(B,RF),e(RF,ige),e(ige,LKo),e(RF,BKo),e(RF,Nq),e(Nq,xKo),e(RF,kKo),e(B,RKo),e(B,SF),e(SF,dge),e(dge,SKo),e(SF,PKo),e(SF,Dq),e(Dq,$Ko),e(SF,IKo),e(B,jKo),e(B,PF),e(PF,cge),e(cge,NKo),e(PF,DKo),e(PF,qq),e(qq,qKo),e(PF,OKo),e(B,GKo),e(B,$F),e($F,mge),e(mge,XKo),e($F,VKo),e($F,Oq),e(Oq,zKo),e($F,WKo),e(B,QKo),e(B,IF),e(IF,fge),e(fge,HKo),e(IF,UKo),e(IF,Gq),e(Gq,JKo),e(IF,YKo),e(B,KKo),e(B,jF),e(jF,gge),e(gge,ZKo),e(jF,eZo),e(jF,Xq),e(Xq,oZo),e(jF,rZo),e(B,tZo),e(B,NF),e(NF,hge),e(hge,aZo),e(NF,sZo),e(NF,Vq),e(Vq,nZo),e(NF,lZo),e(B,iZo),e(B,DF),e(DF,uge),e(uge,dZo),e(DF,cZo),e(DF,zq),e(zq,mZo),e(DF,fZo),e(B,gZo),e(B,qF),e(qF,pge),e(pge,hZo),e(qF,uZo),e(qF,Wq),e(Wq,pZo),e(qF,_Zo),e(B,bZo),e(B,OF),e(OF,_ge),e(_ge,vZo),e(OF,TZo),e(OF,Qq),e(Qq,FZo),e(OF,CZo),e(B,MZo),e(B,GF),e(GF,bge),e(bge,EZo),e(GF,yZo),e(GF,Hq),e(Hq,wZo),e(GF,AZo),e(B,LZo),e(B,XF),e(XF,vge),e(vge,BZo),e(XF,xZo),e(XF,Uq),e(Uq,kZo),e(XF,RZo),e(B,SZo),e(B,VF),e(VF,Tge),e(Tge,PZo),e(VF,$Zo),e(VF,Jq),e(Jq,IZo),e(VF,jZo),e(B,NZo),e(B,zF),e(zF,Fge),e(Fge,DZo),e(zF,qZo),e(zF,Yq),e(Yq,OZo),e(zF,GZo),e(B,XZo),e(B,WF),e(WF,Cge),e(Cge,VZo),e(WF,zZo),e(WF,Kq),e(Kq,WZo),e(WF,QZo),e(B,HZo),e(B,QF),e(QF,Mge),e(Mge,UZo),e(QF,JZo),e(QF,Zq),e(Zq,YZo),e(QF,KZo),e(B,ZZo),e(B,HF),e(HF,Ege),e(Ege,eer),e(HF,oer),e(HF,eO),e(eO,rer),e(HF,ter),e(B,aer),e(B,UF),e(UF,yge),e(yge,ser),e(UF,ner),e(UF,oO),e(oO,ler),e(UF,ier),e(B,der),e(B,JF),e(JF,wge),e(wge,cer),e(JF,mer),e(JF,rO),e(rO,fer),e(JF,ger),e(B,her),e(B,YF),e(YF,Age),e(Age,uer),e(YF,per),e(YF,tO),e(tO,_er),e(YF,ber),e(ho,ver),e(ho,Lge),e(Lge,Ter),e(ho,Fer),g(GA,ho,null),b(c,zke,_),b(c,pc,_),e(pc,KF),e(KF,Bge),g(XA,Bge,null),e(pc,Cer),e(pc,xge),e(xge,Mer),b(c,Wke,_),b(c,Tr,_),g(VA,Tr,null),e(Tr,Eer),e(Tr,_c),e(_c,yer),e(_c,kge),e(kge,wer),e(_c,Aer),e(_c,Rge),e(Rge,Ler),e(_c,Ber),e(Tr,xer),e(Tr,zA),e(zA,ker),e(zA,Sge),e(Sge,Rer),e(zA,Ser),e(Tr,Per),e(Tr,ht),g(WA,ht,null),e(ht,$er),e(ht,Pge),e(Pge,Ier),e(ht,jer),e(ht,bc),e(bc,Ner),e(bc,$ge),e($ge,Der),e(bc,qer),e(bc,Ige),e(Ige,Oer),e(bc,Ger),e(ht,Xer),e(ht,jge),e(jge,Ver),e(ht,zer),g(QA,ht,null),e(Tr,Wer),e(Tr,uo),g(HA,uo,null),e(uo,Qer),e(uo,Nge),e(Nge,Her),e(uo,Uer),e(uo,ps),e(ps,Jer),e(ps,Dge),e(Dge,Yer),e(ps,Ker),e(ps,qge),e(qge,Zer),e(ps,eor),e(ps,Oge),e(Oge,oor),e(ps,ror),e(uo,tor),e(uo,H),e(H,ZF),e(ZF,Gge),e(Gge,aor),e(ZF,sor),e(ZF,aO),e(aO,nor),e(ZF,lor),e(H,ior),e(H,eC),e(eC,Xge),e(Xge,dor),e(eC,cor),e(eC,sO),e(sO,mor),e(eC,gor),e(H,hor),e(H,oC),e(oC,Vge),e(Vge,uor),e(oC,por),e(oC,nO),e(nO,_or),e(oC,bor),e(H,vor),e(H,rC),e(rC,zge),e(zge,Tor),e(rC,For),e(rC,lO),e(lO,Cor),e(rC,Mor),e(H,Eor),e(H,tC),e(tC,Wge),e(Wge,yor),e(tC,wor),e(tC,iO),e(iO,Aor),e(tC,Lor),e(H,Bor),e(H,aC),e(aC,Qge),e(Qge,xor),e(aC,kor),e(aC,dO),e(dO,Ror),e(aC,Sor),e(H,Por),e(H,sC),e(sC,Hge),e(Hge,$or),e(sC,Ior),e(sC,cO),e(cO,jor),e(sC,Nor),e(H,Dor),e(H,nC),e(nC,Uge),e(Uge,qor),e(nC,Oor),e(nC,mO),e(mO,Gor),e(nC,Xor),e(H,Vor),e(H,lC),e(lC,Jge),e(Jge,zor),e(lC,Wor),e(lC,fO),e(fO,Qor),e(lC,Hor),e(H,Uor),e(H,iC),e(iC,Yge),e(Yge,Jor),e(iC,Yor),e(iC,gO),e(gO,Kor),e(iC,Zor),e(H,err),e(H,dC),e(dC,Kge),e(Kge,orr),e(dC,rrr),e(dC,hO),e(hO,trr),e(dC,arr),e(H,srr),e(H,cC),e(cC,Zge),e(Zge,nrr),e(cC,lrr),e(cC,uO),e(uO,irr),e(cC,drr),e(H,crr),e(H,mC),e(mC,ehe),e(ehe,mrr),e(mC,frr),e(mC,pO),e(pO,grr),e(mC,hrr),e(H,urr),e(H,fC),e(fC,ohe),e(ohe,prr),e(fC,_rr),e(fC,_O),e(_O,brr),e(fC,vrr),e(H,Trr),e(H,gC),e(gC,rhe),e(rhe,Frr),e(gC,Crr),e(gC,bO),e(bO,Mrr),e(gC,Err),e(H,yrr),e(H,hC),e(hC,the),e(the,wrr),e(hC,Arr),e(hC,vO),e(vO,Lrr),e(hC,Brr),e(H,xrr),e(H,uC),e(uC,ahe),e(ahe,krr),e(uC,Rrr),e(uC,TO),e(TO,Srr),e(uC,Prr),e(H,$rr),e(H,pC),e(pC,she),e(she,Irr),e(pC,jrr),e(pC,FO),e(FO,Nrr),e(pC,Drr),e(H,qrr),e(H,_C),e(_C,nhe),e(nhe,Orr),e(_C,Grr),e(_C,CO),e(CO,Xrr),e(_C,Vrr),e(H,zrr),e(H,bC),e(bC,lhe),e(lhe,Wrr),e(bC,Qrr),e(bC,MO),e(MO,Hrr),e(bC,Urr),e(H,Jrr),e(H,vC),e(vC,ihe),e(ihe,Yrr),e(vC,Krr),e(vC,EO),e(EO,Zrr),e(vC,etr),e(H,otr),e(H,TC),e(TC,dhe),e(dhe,rtr),e(TC,ttr),e(TC,yO),e(yO,atr),e(TC,str),e(uo,ntr),e(uo,che),e(che,ltr),e(uo,itr),g(UA,uo,null),b(c,Qke,_),b(c,vc,_),e(vc,FC),e(FC,mhe),g(JA,mhe,null),e(vc,dtr),e(vc,fhe),e(fhe,ctr),b(c,Hke,_),b(c,Fr,_),g(YA,Fr,null),e(Fr,mtr),e(Fr,Tc),e(Tc,ftr),e(Tc,ghe),e(ghe,gtr),e(Tc,htr),e(Tc,hhe),e(hhe,utr),e(Tc,ptr),e(Fr,_tr),e(Fr,KA),e(KA,btr),e(KA,uhe),e(uhe,vtr),e(KA,Ttr),e(Fr,Ftr),e(Fr,ut),g(ZA,ut,null),e(ut,Ctr),e(ut,phe),e(phe,Mtr),e(ut,Etr),e(ut,Fc),e(Fc,ytr),e(Fc,_he),e(_he,wtr),e(Fc,Atr),e(Fc,bhe),e(bhe,Ltr),e(Fc,Btr),e(ut,xtr),e(ut,vhe),e(vhe,ktr),e(ut,Rtr),g(e0,ut,null),e(Fr,Str),e(Fr,po),g(o0,po,null),e(po,Ptr),e(po,The),e(The,$tr),e(po,Itr),e(po,_s),e(_s,jtr),e(_s,Fhe),e(Fhe,Ntr),e(_s,Dtr),e(_s,Che),e(Che,qtr),e(_s,Otr),e(_s,Mhe),e(Mhe,Gtr),e(_s,Xtr),e(po,Vtr),e(po,ge),e(ge,CC),e(CC,Ehe),e(Ehe,ztr),e(CC,Wtr),e(CC,wO),e(wO,Qtr),e(CC,Htr),e(ge,Utr),e(ge,MC),e(MC,yhe),e(yhe,Jtr),e(MC,Ytr),e(MC,AO),e(AO,Ktr),e(MC,Ztr),e(ge,ear),e(ge,EC),e(EC,whe),e(whe,oar),e(EC,rar),e(EC,LO),e(LO,tar),e(EC,aar),e(ge,sar),e(ge,yC),e(yC,Ahe),e(Ahe,nar),e(yC,lar),e(yC,BO),e(BO,iar),e(yC,dar),e(ge,car),e(ge,wC),e(wC,Lhe),e(Lhe,mar),e(wC,far),e(wC,xO),e(xO,gar),e(wC,har),e(ge,uar),e(ge,AC),e(AC,Bhe),e(Bhe,par),e(AC,_ar),e(AC,kO),e(kO,bar),e(AC,Tar),e(ge,Far),e(ge,LC),e(LC,xhe),e(xhe,Car),e(LC,Mar),e(LC,RO),e(RO,Ear),e(LC,yar),e(ge,war),e(ge,BC),e(BC,khe),e(khe,Aar),e(BC,Lar),e(BC,SO),e(SO,Bar),e(BC,xar),e(ge,kar),e(ge,xC),e(xC,Rhe),e(Rhe,Rar),e(xC,Sar),e(xC,PO),e(PO,Par),e(xC,$ar),e(ge,Iar),e(ge,kC),e(kC,She),e(She,jar),e(kC,Nar),e(kC,$O),e($O,Dar),e(kC,qar),e(ge,Oar),e(ge,RC),e(RC,Phe),e(Phe,Gar),e(RC,Xar),e(RC,IO),e(IO,Var),e(RC,zar),e(po,War),e(po,$he),e($he,Qar),e(po,Har),g(r0,po,null),b(c,Uke,_),b(c,Cc,_),e(Cc,SC),e(SC,Ihe),g(t0,Ihe,null),e(Cc,Uar),e(Cc,jhe),e(jhe,Jar),b(c,Jke,_),b(c,Cr,_),g(a0,Cr,null),e(Cr,Yar),e(Cr,Mc),e(Mc,Kar),e(Mc,Nhe),e(Nhe,Zar),e(Mc,esr),e(Mc,Dhe),e(Dhe,osr),e(Mc,rsr),e(Cr,tsr),e(Cr,s0),e(s0,asr),e(s0,qhe),e(qhe,ssr),e(s0,nsr),e(Cr,lsr),e(Cr,pt),g(n0,pt,null),e(pt,isr),e(pt,Ohe),e(Ohe,dsr),e(pt,csr),e(pt,Ec),e(Ec,msr),e(Ec,Ghe),e(Ghe,fsr),e(Ec,gsr),e(Ec,Xhe),e(Xhe,hsr),e(Ec,usr),e(pt,psr),e(pt,Vhe),e(Vhe,_sr),e(pt,bsr),g(l0,pt,null),e(Cr,vsr),e(Cr,_o),g(i0,_o,null),e(_o,Tsr),e(_o,zhe),e(zhe,Fsr),e(_o,Csr),e(_o,bs),e(bs,Msr),e(bs,Whe),e(Whe,Esr),e(bs,ysr),e(bs,Qhe),e(Qhe,wsr),e(bs,Asr),e(bs,Hhe),e(Hhe,Lsr),e(bs,Bsr),e(_o,xsr),e(_o,d0),e(d0,PC),e(PC,Uhe),e(Uhe,ksr),e(PC,Rsr),e(PC,jO),e(jO,Ssr),e(PC,Psr),e(d0,$sr),e(d0,$C),e($C,Jhe),e(Jhe,Isr),e($C,jsr),e($C,NO),e(NO,Nsr),e($C,Dsr),e(_o,qsr),e(_o,Yhe),e(Yhe,Osr),e(_o,Gsr),g(c0,_o,null),b(c,Yke,_),b(c,yc,_),e(yc,IC),e(IC,Khe),g(m0,Khe,null),e(yc,Xsr),e(yc,Zhe),e(Zhe,Vsr),b(c,Kke,_),b(c,Mr,_),g(f0,Mr,null),e(Mr,zsr),e(Mr,wc),e(wc,Wsr),e(wc,eue),e(eue,Qsr),e(wc,Hsr),e(wc,oue),e(oue,Usr),e(wc,Jsr),e(Mr,Ysr),e(Mr,g0),e(g0,Ksr),e(g0,rue),e(rue,Zsr),e(g0,enr),e(Mr,onr),e(Mr,_t),g(h0,_t,null),e(_t,rnr),e(_t,tue),e(tue,tnr),e(_t,anr),e(_t,Ac),e(Ac,snr),e(Ac,aue),e(aue,nnr),e(Ac,lnr),e(Ac,sue),e(sue,inr),e(Ac,dnr),e(_t,cnr),e(_t,nue),e(nue,mnr),e(_t,fnr),g(u0,_t,null),e(Mr,gnr),e(Mr,bo),g(p0,bo,null),e(bo,hnr),e(bo,lue),e(lue,unr),e(bo,pnr),e(bo,vs),e(vs,_nr),e(vs,iue),e(iue,bnr),e(vs,vnr),e(vs,due),e(due,Tnr),e(vs,Fnr),e(vs,cue),e(cue,Cnr),e(vs,Mnr),e(bo,Enr),e(bo,Y),e(Y,jC),e(jC,mue),e(mue,ynr),e(jC,wnr),e(jC,DO),e(DO,Anr),e(jC,Lnr),e(Y,Bnr),e(Y,NC),e(NC,fue),e(fue,xnr),e(NC,knr),e(NC,qO),e(qO,Rnr),e(NC,Snr),e(Y,Pnr),e(Y,DC),e(DC,gue),e(gue,$nr),e(DC,Inr),e(DC,OO),e(OO,jnr),e(DC,Nnr),e(Y,Dnr),e(Y,qC),e(qC,hue),e(hue,qnr),e(qC,Onr),e(qC,GO),e(GO,Gnr),e(qC,Xnr),e(Y,Vnr),e(Y,OC),e(OC,uue),e(uue,znr),e(OC,Wnr),e(OC,XO),e(XO,Qnr),e(OC,Hnr),e(Y,Unr),e(Y,GC),e(GC,pue),e(pue,Jnr),e(GC,Ynr),e(GC,VO),e(VO,Knr),e(GC,Znr),e(Y,elr),e(Y,XC),e(XC,_ue),e(_ue,olr),e(XC,rlr),e(XC,zO),e(zO,tlr),e(XC,alr),e(Y,slr),e(Y,VC),e(VC,bue),e(bue,nlr),e(VC,llr),e(VC,WO),e(WO,ilr),e(VC,dlr),e(Y,clr),e(Y,zC),e(zC,vue),e(vue,mlr),e(zC,flr),e(zC,QO),e(QO,glr),e(zC,hlr),e(Y,ulr),e(Y,WC),e(WC,Tue),e(Tue,plr),e(WC,_lr),e(WC,HO),e(HO,blr),e(WC,vlr),e(Y,Tlr),e(Y,QC),e(QC,Fue),e(Fue,Flr),e(QC,Clr),e(QC,UO),e(UO,Mlr),e(QC,Elr),e(Y,ylr),e(Y,HC),e(HC,Cue),e(Cue,wlr),e(HC,Alr),e(HC,JO),e(JO,Llr),e(HC,Blr),e(Y,xlr),e(Y,UC),e(UC,Mue),e(Mue,klr),e(UC,Rlr),e(UC,YO),e(YO,Slr),e(UC,Plr),e(Y,$lr),e(Y,JC),e(JC,Eue),e(Eue,Ilr),e(JC,jlr),e(JC,KO),e(KO,Nlr),e(JC,Dlr),e(Y,qlr),e(Y,YC),e(YC,yue),e(yue,Olr),e(YC,Glr),e(YC,ZO),e(ZO,Xlr),e(YC,Vlr),e(Y,zlr),e(Y,KC),e(KC,wue),e(wue,Wlr),e(KC,Qlr),e(KC,eG),e(eG,Hlr),e(KC,Ulr),e(Y,Jlr),e(Y,ZC),e(ZC,Aue),e(Aue,Ylr),e(ZC,Klr),e(ZC,oG),e(oG,Zlr),e(ZC,eir),e(Y,oir),e(Y,eM),e(eM,Lue),e(Lue,rir),e(eM,tir),e(eM,rG),e(rG,air),e(eM,sir),e(Y,nir),e(Y,oM),e(oM,Bue),e(Bue,lir),e(oM,iir),e(oM,tG),e(tG,dir),e(oM,cir),e(Y,mir),e(Y,rM),e(rM,xue),e(xue,fir),e(rM,gir),e(rM,aG),e(aG,hir),e(rM,uir),e(bo,pir),e(bo,kue),e(kue,_ir),e(bo,bir),g(_0,bo,null),b(c,Zke,_),b(c,Lc,_),e(Lc,tM),e(tM,Rue),g(b0,Rue,null),e(Lc,vir),e(Lc,Sue),e(Sue,Tir),b(c,eRe,_),b(c,Er,_),g(v0,Er,null),e(Er,Fir),e(Er,Bc),e(Bc,Cir),e(Bc,Pue),e(Pue,Mir),e(Bc,Eir),e(Bc,$ue),e($ue,yir),e(Bc,wir),e(Er,Air),e(Er,T0),e(T0,Lir),e(T0,Iue),e(Iue,Bir),e(T0,xir),e(Er,kir),e(Er,bt),g(F0,bt,null),e(bt,Rir),e(bt,jue),e(jue,Sir),e(bt,Pir),e(bt,xc),e(xc,$ir),e(xc,Nue),e(Nue,Iir),e(xc,jir),e(xc,Due),e(Due,Nir),e(xc,Dir),e(bt,qir),e(bt,que),e(que,Oir),e(bt,Gir),g(C0,bt,null),e(Er,Xir),e(Er,vo),g(M0,vo,null),e(vo,Vir),e(vo,Oue),e(Oue,zir),e(vo,Wir),e(vo,Ts),e(Ts,Qir),e(Ts,Gue),e(Gue,Hir),e(Ts,Uir),e(Ts,Xue),e(Xue,Jir),e(Ts,Yir),e(Ts,Vue),e(Vue,Kir),e(Ts,Zir),e(vo,edr),e(vo,_e),e(_e,aM),e(aM,zue),e(zue,odr),e(aM,rdr),e(aM,sG),e(sG,tdr),e(aM,adr),e(_e,sdr),e(_e,sM),e(sM,Wue),e(Wue,ndr),e(sM,ldr),e(sM,nG),e(nG,idr),e(sM,ddr),e(_e,cdr),e(_e,nM),e(nM,Que),e(Que,mdr),e(nM,fdr),e(nM,lG),e(lG,gdr),e(nM,hdr),e(_e,udr),e(_e,lM),e(lM,Hue),e(Hue,pdr),e(lM,_dr),e(lM,iG),e(iG,bdr),e(lM,vdr),e(_e,Tdr),e(_e,iM),e(iM,Uue),e(Uue,Fdr),e(iM,Cdr),e(iM,dG),e(dG,Mdr),e(iM,Edr),e(_e,ydr),e(_e,dM),e(dM,Jue),e(Jue,wdr),e(dM,Adr),e(dM,cG),e(cG,Ldr),e(dM,Bdr),e(_e,xdr),e(_e,cM),e(cM,Yue),e(Yue,kdr),e(cM,Rdr),e(cM,mG),e(mG,Sdr),e(cM,Pdr),e(_e,$dr),e(_e,mM),e(mM,Kue),e(Kue,Idr),e(mM,jdr),e(mM,fG),e(fG,Ndr),e(mM,Ddr),e(_e,qdr),e(_e,fM),e(fM,Zue),e(Zue,Odr),e(fM,Gdr),e(fM,gG),e(gG,Xdr),e(fM,Vdr),e(_e,zdr),e(_e,gM),e(gM,epe),e(epe,Wdr),e(gM,Qdr),e(gM,hG),e(hG,Hdr),e(gM,Udr),e(vo,Jdr),e(vo,ope),e(ope,Ydr),e(vo,Kdr),g(E0,vo,null),b(c,oRe,_),b(c,kc,_),e(kc,hM),e(hM,rpe),g(y0,rpe,null),e(kc,Zdr),e(kc,tpe),e(tpe,ecr),b(c,rRe,_),b(c,yr,_),g(w0,yr,null),e(yr,ocr),e(yr,Rc),e(Rc,rcr),e(Rc,ape),e(ape,tcr),e(Rc,acr),e(Rc,spe),e(spe,scr),e(Rc,ncr),e(yr,lcr),e(yr,A0),e(A0,icr),e(A0,npe),e(npe,dcr),e(A0,ccr),e(yr,mcr),e(yr,vt),g(L0,vt,null),e(vt,fcr),e(vt,lpe),e(lpe,gcr),e(vt,hcr),e(vt,Sc),e(Sc,ucr),e(Sc,ipe),e(ipe,pcr),e(Sc,_cr),e(Sc,dpe),e(dpe,bcr),e(Sc,vcr),e(vt,Tcr),e(vt,cpe),e(cpe,Fcr),e(vt,Ccr),g(B0,vt,null),e(yr,Mcr),e(yr,To),g(x0,To,null),e(To,Ecr),e(To,mpe),e(mpe,ycr),e(To,wcr),e(To,Fs),e(Fs,Acr),e(Fs,fpe),e(fpe,Lcr),e(Fs,Bcr),e(Fs,gpe),e(gpe,xcr),e(Fs,kcr),e(Fs,hpe),e(hpe,Rcr),e(Fs,Scr),e(To,Pcr),e(To,V),e(V,uM),e(uM,upe),e(upe,$cr),e(uM,Icr),e(uM,uG),e(uG,jcr),e(uM,Ncr),e(V,Dcr),e(V,pM),e(pM,ppe),e(ppe,qcr),e(pM,Ocr),e(pM,pG),e(pG,Gcr),e(pM,Xcr),e(V,Vcr),e(V,_M),e(_M,_pe),e(_pe,zcr),e(_M,Wcr),e(_M,_G),e(_G,Qcr),e(_M,Hcr),e(V,Ucr),e(V,bM),e(bM,bpe),e(bpe,Jcr),e(bM,Ycr),e(bM,bG),e(bG,Kcr),e(bM,Zcr),e(V,emr),e(V,vM),e(vM,vpe),e(vpe,omr),e(vM,rmr),e(vM,vG),e(vG,tmr),e(vM,amr),e(V,smr),e(V,TM),e(TM,Tpe),e(Tpe,nmr),e(TM,lmr),e(TM,TG),e(TG,imr),e(TM,dmr),e(V,cmr),e(V,FM),e(FM,Fpe),e(Fpe,mmr),e(FM,fmr),e(FM,FG),e(FG,gmr),e(FM,hmr),e(V,umr),e(V,CM),e(CM,Cpe),e(Cpe,pmr),e(CM,_mr),e(CM,CG),e(CG,bmr),e(CM,vmr),e(V,Tmr),e(V,MM),e(MM,Mpe),e(Mpe,Fmr),e(MM,Cmr),e(MM,MG),e(MG,Mmr),e(MM,Emr),e(V,ymr),e(V,EM),e(EM,Epe),e(Epe,wmr),e(EM,Amr),e(EM,EG),e(EG,Lmr),e(EM,Bmr),e(V,xmr),e(V,yM),e(yM,ype),e(ype,kmr),e(yM,Rmr),e(yM,yG),e(yG,Smr),e(yM,Pmr),e(V,$mr),e(V,wM),e(wM,wpe),e(wpe,Imr),e(wM,jmr),e(wM,wG),e(wG,Nmr),e(wM,Dmr),e(V,qmr),e(V,AM),e(AM,Ape),e(Ape,Omr),e(AM,Gmr),e(AM,AG),e(AG,Xmr),e(AM,Vmr),e(V,zmr),e(V,LM),e(LM,Lpe),e(Lpe,Wmr),e(LM,Qmr),e(LM,LG),e(LG,Hmr),e(LM,Umr),e(V,Jmr),e(V,BM),e(BM,Bpe),e(Bpe,Ymr),e(BM,Kmr),e(BM,BG),e(BG,Zmr),e(BM,efr),e(V,ofr),e(V,xM),e(xM,xpe),e(xpe,rfr),e(xM,tfr),e(xM,xG),e(xG,afr),e(xM,sfr),e(V,nfr),e(V,kM),e(kM,kpe),e(kpe,lfr),e(kM,ifr),e(kM,kG),e(kG,dfr),e(kM,cfr),e(V,mfr),e(V,RM),e(RM,Rpe),e(Rpe,ffr),e(RM,gfr),e(RM,RG),e(RG,hfr),e(RM,ufr),e(V,pfr),e(V,SM),e(SM,Spe),e(Spe,_fr),e(SM,bfr),e(SM,SG),e(SG,vfr),e(SM,Tfr),e(V,Ffr),e(V,PM),e(PM,Ppe),e(Ppe,Cfr),e(PM,Mfr),e(PM,PG),e(PG,Efr),e(PM,yfr),e(V,wfr),e(V,$M),e($M,$pe),e($pe,Afr),e($M,Lfr),e($M,$G),e($G,Bfr),e($M,xfr),e(V,kfr),e(V,IM),e(IM,Ipe),e(Ipe,Rfr),e(IM,Sfr),e(IM,IG),e(IG,Pfr),e(IM,$fr),e(V,Ifr),e(V,jM),e(jM,jpe),e(jpe,jfr),e(jM,Nfr),e(jM,jG),e(jG,Dfr),e(jM,qfr),e(V,Ofr),e(V,NM),e(NM,Npe),e(Npe,Gfr),e(NM,Xfr),e(NM,NG),e(NG,Vfr),e(NM,zfr),e(V,Wfr),e(V,DM),e(DM,Dpe),e(Dpe,Qfr),e(DM,Hfr),e(DM,DG),e(DG,Ufr),e(DM,Jfr),e(To,Yfr),e(To,qpe),e(qpe,Kfr),e(To,Zfr),g(k0,To,null),b(c,tRe,_),b(c,Pc,_),e(Pc,qM),e(qM,Ope),g(R0,Ope,null),e(Pc,egr),e(Pc,Gpe),e(Gpe,ogr),b(c,aRe,_),b(c,wr,_),g(S0,wr,null),e(wr,rgr),e(wr,$c),e($c,tgr),e($c,Xpe),e(Xpe,agr),e($c,sgr),e($c,Vpe),e(Vpe,ngr),e($c,lgr),e(wr,igr),e(wr,P0),e(P0,dgr),e(P0,zpe),e(zpe,cgr),e(P0,mgr),e(wr,fgr),e(wr,Tt),g($0,Tt,null),e(Tt,ggr),e(Tt,Wpe),e(Wpe,hgr),e(Tt,ugr),e(Tt,Ic),e(Ic,pgr),e(Ic,Qpe),e(Qpe,_gr),e(Ic,bgr),e(Ic,Hpe),e(Hpe,vgr),e(Ic,Tgr),e(Tt,Fgr),e(Tt,Upe),e(Upe,Cgr),e(Tt,Mgr),g(I0,Tt,null),e(wr,Egr),e(wr,Fo),g(j0,Fo,null),e(Fo,ygr),e(Fo,Jpe),e(Jpe,wgr),e(Fo,Agr),e(Fo,Cs),e(Cs,Lgr),e(Cs,Ype),e(Ype,Bgr),e(Cs,xgr),e(Cs,Kpe),e(Kpe,kgr),e(Cs,Rgr),e(Cs,Zpe),e(Zpe,Sgr),e(Cs,Pgr),e(Fo,$gr),e(Fo,ae),e(ae,OM),e(OM,e_e),e(e_e,Igr),e(OM,jgr),e(OM,qG),e(qG,Ngr),e(OM,Dgr),e(ae,qgr),e(ae,GM),e(GM,o_e),e(o_e,Ogr),e(GM,Ggr),e(GM,OG),e(OG,Xgr),e(GM,Vgr),e(ae,zgr),e(ae,XM),e(XM,r_e),e(r_e,Wgr),e(XM,Qgr),e(XM,GG),e(GG,Hgr),e(XM,Ugr),e(ae,Jgr),e(ae,VM),e(VM,t_e),e(t_e,Ygr),e(VM,Kgr),e(VM,XG),e(XG,Zgr),e(VM,ehr),e(ae,ohr),e(ae,zM),e(zM,a_e),e(a_e,rhr),e(zM,thr),e(zM,VG),e(VG,ahr),e(zM,shr),e(ae,nhr),e(ae,WM),e(WM,s_e),e(s_e,lhr),e(WM,ihr),e(WM,zG),e(zG,dhr),e(WM,chr),e(ae,mhr),e(ae,QM),e(QM,n_e),e(n_e,fhr),e(QM,ghr),e(QM,WG),e(WG,hhr),e(QM,uhr),e(ae,phr),e(ae,HM),e(HM,l_e),e(l_e,_hr),e(HM,bhr),e(HM,QG),e(QG,vhr),e(HM,Thr),e(ae,Fhr),e(ae,UM),e(UM,i_e),e(i_e,Chr),e(UM,Mhr),e(UM,HG),e(HG,Ehr),e(UM,yhr),e(ae,whr),e(ae,JM),e(JM,d_e),e(d_e,Ahr),e(JM,Lhr),e(JM,UG),e(UG,Bhr),e(JM,xhr),e(ae,khr),e(ae,YM),e(YM,c_e),e(c_e,Rhr),e(YM,Shr),e(YM,JG),e(JG,Phr),e(YM,$hr),e(ae,Ihr),e(ae,KM),e(KM,m_e),e(m_e,jhr),e(KM,Nhr),e(KM,YG),e(YG,Dhr),e(KM,qhr),e(ae,Ohr),e(ae,ZM),e(ZM,f_e),e(f_e,Ghr),e(ZM,Xhr),e(ZM,KG),e(KG,Vhr),e(ZM,zhr),e(ae,Whr),e(ae,e4),e(e4,g_e),e(g_e,Qhr),e(e4,Hhr),e(e4,ZG),e(ZG,Uhr),e(e4,Jhr),e(ae,Yhr),e(ae,o4),e(o4,h_e),e(h_e,Khr),e(o4,Zhr),e(o4,eX),e(eX,eur),e(o4,our),e(ae,rur),e(ae,r4),e(r4,u_e),e(u_e,tur),e(r4,aur),e(r4,oX),e(oX,sur),e(r4,nur),e(ae,lur),e(ae,t4),e(t4,p_e),e(p_e,iur),e(t4,dur),e(t4,rX),e(rX,cur),e(t4,mur),e(Fo,fur),e(Fo,__e),e(__e,gur),e(Fo,hur),g(N0,Fo,null),b(c,sRe,_),b(c,jc,_),e(jc,a4),e(a4,b_e),g(D0,b_e,null),e(jc,uur),e(jc,v_e),e(v_e,pur),b(c,nRe,_),b(c,Ar,_),g(q0,Ar,null),e(Ar,_ur),e(Ar,Nc),e(Nc,bur),e(Nc,T_e),e(T_e,vur),e(Nc,Tur),e(Nc,F_e),e(F_e,Fur),e(Nc,Cur),e(Ar,Mur),e(Ar,O0),e(O0,Eur),e(O0,C_e),e(C_e,yur),e(O0,wur),e(Ar,Aur),e(Ar,Ft),g(G0,Ft,null),e(Ft,Lur),e(Ft,M_e),e(M_e,Bur),e(Ft,xur),e(Ft,Dc),e(Dc,kur),e(Dc,E_e),e(E_e,Rur),e(Dc,Sur),e(Dc,y_e),e(y_e,Pur),e(Dc,$ur),e(Ft,Iur),e(Ft,w_e),e(w_e,jur),e(Ft,Nur),g(X0,Ft,null),e(Ar,Dur),e(Ar,Co),g(V0,Co,null),e(Co,qur),e(Co,A_e),e(A_e,Our),e(Co,Gur),e(Co,Ms),e(Ms,Xur),e(Ms,L_e),e(L_e,Vur),e(Ms,zur),e(Ms,B_e),e(B_e,Wur),e(Ms,Qur),e(Ms,x_e),e(x_e,Hur),e(Ms,Uur),e(Co,Jur),e(Co,k_e),e(k_e,s4),e(s4,R_e),e(R_e,Yur),e(s4,Kur),e(s4,tX),e(tX,Zur),e(s4,epr),e(Co,opr),e(Co,S_e),e(S_e,rpr),e(Co,tpr),g(z0,Co,null),b(c,lRe,_),b(c,qc,_),e(qc,n4),e(n4,P_e),g(W0,P_e,null),e(qc,apr),e(qc,$_e),e($_e,spr),b(c,iRe,_),b(c,Lr,_),g(Q0,Lr,null),e(Lr,npr),e(Lr,Oc),e(Oc,lpr),e(Oc,I_e),e(I_e,ipr),e(Oc,dpr),e(Oc,j_e),e(j_e,cpr),e(Oc,mpr),e(Lr,fpr),e(Lr,H0),e(H0,gpr),e(H0,N_e),e(N_e,hpr),e(H0,upr),e(Lr,ppr),e(Lr,Ct),g(U0,Ct,null),e(Ct,_pr),e(Ct,D_e),e(D_e,bpr),e(Ct,vpr),e(Ct,Gc),e(Gc,Tpr),e(Gc,q_e),e(q_e,Fpr),e(Gc,Cpr),e(Gc,O_e),e(O_e,Mpr),e(Gc,Epr),e(Ct,ypr),e(Ct,G_e),e(G_e,wpr),e(Ct,Apr),g(J0,Ct,null),e(Lr,Lpr),e(Lr,Mo),g(Y0,Mo,null),e(Mo,Bpr),e(Mo,X_e),e(X_e,xpr),e(Mo,kpr),e(Mo,Es),e(Es,Rpr),e(Es,V_e),e(V_e,Spr),e(Es,Ppr),e(Es,z_e),e(z_e,$pr),e(Es,Ipr),e(Es,W_e),e(W_e,jpr),e(Es,Npr),e(Mo,Dpr),e(Mo,K),e(K,l4),e(l4,Q_e),e(Q_e,qpr),e(l4,Opr),e(l4,aX),e(aX,Gpr),e(l4,Xpr),e(K,Vpr),e(K,i4),e(i4,H_e),e(H_e,zpr),e(i4,Wpr),e(i4,sX),e(sX,Qpr),e(i4,Hpr),e(K,Upr),e(K,d4),e(d4,U_e),e(U_e,Jpr),e(d4,Ypr),e(d4,nX),e(nX,Kpr),e(d4,Zpr),e(K,e_r),e(K,c4),e(c4,J_e),e(J_e,o_r),e(c4,r_r),e(c4,lX),e(lX,t_r),e(c4,a_r),e(K,s_r),e(K,m4),e(m4,Y_e),e(Y_e,n_r),e(m4,l_r),e(m4,iX),e(iX,i_r),e(m4,d_r),e(K,c_r),e(K,f4),e(f4,K_e),e(K_e,m_r),e(f4,f_r),e(f4,dX),e(dX,g_r),e(f4,h_r),e(K,u_r),e(K,g4),e(g4,Z_e),e(Z_e,p_r),e(g4,__r),e(g4,cX),e(cX,b_r),e(g4,v_r),e(K,T_r),e(K,h4),e(h4,ebe),e(ebe,F_r),e(h4,C_r),e(h4,mX),e(mX,M_r),e(h4,E_r),e(K,y_r),e(K,u4),e(u4,obe),e(obe,w_r),e(u4,A_r),e(u4,fX),e(fX,L_r),e(u4,B_r),e(K,x_r),e(K,p4),e(p4,rbe),e(rbe,k_r),e(p4,R_r),e(p4,gX),e(gX,S_r),e(p4,P_r),e(K,$_r),e(K,_4),e(_4,tbe),e(tbe,I_r),e(_4,j_r),e(_4,hX),e(hX,N_r),e(_4,D_r),e(K,q_r),e(K,b4),e(b4,abe),e(abe,O_r),e(b4,G_r),e(b4,uX),e(uX,X_r),e(b4,V_r),e(K,z_r),e(K,v4),e(v4,sbe),e(sbe,W_r),e(v4,Q_r),e(v4,pX),e(pX,H_r),e(v4,U_r),e(K,J_r),e(K,T4),e(T4,nbe),e(nbe,Y_r),e(T4,K_r),e(T4,_X),e(_X,Z_r),e(T4,ebr),e(K,obr),e(K,F4),e(F4,lbe),e(lbe,rbr),e(F4,tbr),e(F4,bX),e(bX,abr),e(F4,sbr),e(K,nbr),e(K,C4),e(C4,ibe),e(ibe,lbr),e(C4,ibr),e(C4,vX),e(vX,dbr),e(C4,cbr),e(K,mbr),e(K,M4),e(M4,dbe),e(dbe,fbr),e(M4,gbr),e(M4,TX),e(TX,hbr),e(M4,ubr),e(K,pbr),e(K,E4),e(E4,cbe),e(cbe,_br),e(E4,bbr),e(E4,FX),e(FX,vbr),e(E4,Tbr),e(K,Fbr),e(K,y4),e(y4,mbe),e(mbe,Cbr),e(y4,Mbr),e(y4,CX),e(CX,Ebr),e(y4,ybr),e(K,wbr),e(K,w4),e(w4,fbe),e(fbe,Abr),e(w4,Lbr),e(w4,MX),e(MX,Bbr),e(w4,xbr),e(Mo,kbr),e(Mo,gbe),e(gbe,Rbr),e(Mo,Sbr),g(K0,Mo,null),b(c,dRe,_),b(c,Xc,_),e(Xc,A4),e(A4,hbe),g(Z0,hbe,null),e(Xc,Pbr),e(Xc,ube),e(ube,$br),b(c,cRe,_),b(c,Br,_),g(eL,Br,null),e(Br,Ibr),e(Br,Vc),e(Vc,jbr),e(Vc,pbe),e(pbe,Nbr),e(Vc,Dbr),e(Vc,_be),e(_be,qbr),e(Vc,Obr),e(Br,Gbr),e(Br,oL),e(oL,Xbr),e(oL,bbe),e(bbe,Vbr),e(oL,zbr),e(Br,Wbr),e(Br,Mt),g(rL,Mt,null),e(Mt,Qbr),e(Mt,vbe),e(vbe,Hbr),e(Mt,Ubr),e(Mt,zc),e(zc,Jbr),e(zc,Tbe),e(Tbe,Ybr),e(zc,Kbr),e(zc,Fbe),e(Fbe,Zbr),e(zc,e2r),e(Mt,o2r),e(Mt,Cbe),e(Cbe,r2r),e(Mt,t2r),g(tL,Mt,null),e(Br,a2r),e(Br,Eo),g(aL,Eo,null),e(Eo,s2r),e(Eo,Mbe),e(Mbe,n2r),e(Eo,l2r),e(Eo,ys),e(ys,i2r),e(ys,Ebe),e(Ebe,d2r),e(ys,c2r),e(ys,ybe),e(ybe,m2r),e(ys,f2r),e(ys,wbe),e(wbe,g2r),e(ys,h2r),e(Eo,u2r),e(Eo,Z),e(Z,L4),e(L4,Abe),e(Abe,p2r),e(L4,_2r),e(L4,EX),e(EX,b2r),e(L4,v2r),e(Z,T2r),e(Z,B4),e(B4,Lbe),e(Lbe,F2r),e(B4,C2r),e(B4,yX),e(yX,M2r),e(B4,E2r),e(Z,y2r),e(Z,x4),e(x4,Bbe),e(Bbe,w2r),e(x4,A2r),e(x4,wX),e(wX,L2r),e(x4,B2r),e(Z,x2r),e(Z,k4),e(k4,xbe),e(xbe,k2r),e(k4,R2r),e(k4,AX),e(AX,S2r),e(k4,P2r),e(Z,$2r),e(Z,R4),e(R4,kbe),e(kbe,I2r),e(R4,j2r),e(R4,LX),e(LX,N2r),e(R4,D2r),e(Z,q2r),e(Z,S4),e(S4,Rbe),e(Rbe,O2r),e(S4,G2r),e(S4,BX),e(BX,X2r),e(S4,V2r),e(Z,z2r),e(Z,P4),e(P4,Sbe),e(Sbe,W2r),e(P4,Q2r),e(P4,xX),e(xX,H2r),e(P4,U2r),e(Z,J2r),e(Z,$4),e($4,Pbe),e(Pbe,Y2r),e($4,K2r),e($4,kX),e(kX,Z2r),e($4,evr),e(Z,ovr),e(Z,I4),e(I4,$be),e($be,rvr),e(I4,tvr),e(I4,RX),e(RX,avr),e(I4,svr),e(Z,nvr),e(Z,j4),e(j4,Ibe),e(Ibe,lvr),e(j4,ivr),e(j4,SX),e(SX,dvr),e(j4,cvr),e(Z,mvr),e(Z,N4),e(N4,jbe),e(jbe,fvr),e(N4,gvr),e(N4,PX),e(PX,hvr),e(N4,uvr),e(Z,pvr),e(Z,D4),e(D4,Nbe),e(Nbe,_vr),e(D4,bvr),e(D4,$X),e($X,vvr),e(D4,Tvr),e(Z,Fvr),e(Z,q4),e(q4,Dbe),e(Dbe,Cvr),e(q4,Mvr),e(q4,IX),e(IX,Evr),e(q4,yvr),e(Z,wvr),e(Z,O4),e(O4,qbe),e(qbe,Avr),e(O4,Lvr),e(O4,jX),e(jX,Bvr),e(O4,xvr),e(Z,kvr),e(Z,G4),e(G4,Obe),e(Obe,Rvr),e(G4,Svr),e(G4,NX),e(NX,Pvr),e(G4,$vr),e(Z,Ivr),e(Z,X4),e(X4,Gbe),e(Gbe,jvr),e(X4,Nvr),e(X4,DX),e(DX,Dvr),e(X4,qvr),e(Z,Ovr),e(Z,V4),e(V4,Xbe),e(Xbe,Gvr),e(V4,Xvr),e(V4,qX),e(qX,Vvr),e(V4,zvr),e(Z,Wvr),e(Z,z4),e(z4,Vbe),e(Vbe,Qvr),e(z4,Hvr),e(z4,OX),e(OX,Uvr),e(z4,Jvr),e(Z,Yvr),e(Z,W4),e(W4,zbe),e(zbe,Kvr),e(W4,Zvr),e(W4,GX),e(GX,eTr),e(W4,oTr),e(Eo,rTr),e(Eo,Wbe),e(Wbe,tTr),e(Eo,aTr),g(sL,Eo,null),b(c,mRe,_),b(c,Wc,_),e(Wc,Q4),e(Q4,Qbe),g(nL,Qbe,null),e(Wc,sTr),e(Wc,Hbe),e(Hbe,nTr),b(c,fRe,_),b(c,xr,_),g(lL,xr,null),e(xr,lTr),e(xr,Qc),e(Qc,iTr),e(Qc,Ube),e(Ube,dTr),e(Qc,cTr),e(Qc,Jbe),e(Jbe,mTr),e(Qc,fTr),e(xr,gTr),e(xr,iL),e(iL,hTr),e(iL,Ybe),e(Ybe,uTr),e(iL,pTr),e(xr,_Tr),e(xr,Et),g(dL,Et,null),e(Et,bTr),e(Et,Kbe),e(Kbe,vTr),e(Et,TTr),e(Et,Hc),e(Hc,FTr),e(Hc,Zbe),e(Zbe,CTr),e(Hc,MTr),e(Hc,e2e),e(e2e,ETr),e(Hc,yTr),e(Et,wTr),e(Et,o2e),e(o2e,ATr),e(Et,LTr),g(cL,Et,null),e(xr,BTr),e(xr,yo),g(mL,yo,null),e(yo,xTr),e(yo,r2e),e(r2e,kTr),e(yo,RTr),e(yo,ws),e(ws,STr),e(ws,t2e),e(t2e,PTr),e(ws,$Tr),e(ws,a2e),e(a2e,ITr),e(ws,jTr),e(ws,s2e),e(s2e,NTr),e(ws,DTr),e(yo,qTr),e(yo,n2e),e(n2e,H4),e(H4,l2e),e(l2e,OTr),e(H4,GTr),e(H4,XX),e(XX,XTr),e(H4,VTr),e(yo,zTr),e(yo,i2e),e(i2e,WTr),e(yo,QTr),g(fL,yo,null),b(c,gRe,_),b(c,Uc,_),e(Uc,U4),e(U4,d2e),g(gL,d2e,null),e(Uc,HTr),e(Uc,c2e),e(c2e,UTr),b(c,hRe,_),b(c,kr,_),g(hL,kr,null),e(kr,JTr),e(kr,Jc),e(Jc,YTr),e(Jc,m2e),e(m2e,KTr),e(Jc,ZTr),e(Jc,f2e),e(f2e,e1r),e(Jc,o1r),e(kr,r1r),e(kr,uL),e(uL,t1r),e(uL,g2e),e(g2e,a1r),e(uL,s1r),e(kr,n1r),e(kr,yt),g(pL,yt,null),e(yt,l1r),e(yt,h2e),e(h2e,i1r),e(yt,d1r),e(yt,Yc),e(Yc,c1r),e(Yc,u2e),e(u2e,m1r),e(Yc,f1r),e(Yc,p2e),e(p2e,g1r),e(Yc,h1r),e(yt,u1r),e(yt,_2e),e(_2e,p1r),e(yt,_1r),g(_L,yt,null),e(kr,b1r),e(kr,wo),g(bL,wo,null),e(wo,v1r),e(wo,b2e),e(b2e,T1r),e(wo,F1r),e(wo,As),e(As,C1r),e(As,v2e),e(v2e,M1r),e(As,E1r),e(As,T2e),e(T2e,y1r),e(As,w1r),e(As,F2e),e(F2e,A1r),e(As,L1r),e(wo,B1r),e(wo,C2e),e(C2e,J4),e(J4,M2e),e(M2e,x1r),e(J4,k1r),e(J4,VX),e(VX,R1r),e(J4,S1r),e(wo,P1r),e(wo,E2e),e(E2e,$1r),e(wo,I1r),g(vL,wo,null),b(c,uRe,_),b(c,Kc,_),e(Kc,Y4),e(Y4,y2e),g(TL,y2e,null),e(Kc,j1r),e(Kc,w2e),e(w2e,N1r),b(c,pRe,_),b(c,Rr,_),g(FL,Rr,null),e(Rr,D1r),e(Rr,Zc),e(Zc,q1r),e(Zc,A2e),e(A2e,O1r),e(Zc,G1r),e(Zc,L2e),e(L2e,X1r),e(Zc,V1r),e(Rr,z1r),e(Rr,CL),e(CL,W1r),e(CL,B2e),e(B2e,Q1r),e(CL,H1r),e(Rr,U1r),e(Rr,wt),g(ML,wt,null),e(wt,J1r),e(wt,x2e),e(x2e,Y1r),e(wt,K1r),e(wt,em),e(em,Z1r),e(em,k2e),e(k2e,eFr),e(em,oFr),e(em,R2e),e(R2e,rFr),e(em,tFr),e(wt,aFr),e(wt,S2e),e(S2e,sFr),e(wt,nFr),g(EL,wt,null),e(Rr,lFr),e(Rr,Ao),g(yL,Ao,null),e(Ao,iFr),e(Ao,P2e),e(P2e,dFr),e(Ao,cFr),e(Ao,Ls),e(Ls,mFr),e(Ls,$2e),e($2e,fFr),e(Ls,gFr),e(Ls,I2e),e(I2e,hFr),e(Ls,uFr),e(Ls,j2e),e(j2e,pFr),e(Ls,_Fr),e(Ao,bFr),e(Ao,z),e(z,K4),e(K4,N2e),e(N2e,vFr),e(K4,TFr),e(K4,zX),e(zX,FFr),e(K4,CFr),e(z,MFr),e(z,Z4),e(Z4,D2e),e(D2e,EFr),e(Z4,yFr),e(Z4,WX),e(WX,wFr),e(Z4,AFr),e(z,LFr),e(z,eE),e(eE,q2e),e(q2e,BFr),e(eE,xFr),e(eE,QX),e(QX,kFr),e(eE,RFr),e(z,SFr),e(z,oE),e(oE,O2e),e(O2e,PFr),e(oE,$Fr),e(oE,HX),e(HX,IFr),e(oE,jFr),e(z,NFr),e(z,rE),e(rE,G2e),e(G2e,DFr),e(rE,qFr),e(rE,UX),e(UX,OFr),e(rE,GFr),e(z,XFr),e(z,tE),e(tE,X2e),e(X2e,VFr),e(tE,zFr),e(tE,JX),e(JX,WFr),e(tE,QFr),e(z,HFr),e(z,aE),e(aE,V2e),e(V2e,UFr),e(aE,JFr),e(aE,YX),e(YX,YFr),e(aE,KFr),e(z,ZFr),e(z,sE),e(sE,z2e),e(z2e,eCr),e(sE,oCr),e(sE,KX),e(KX,rCr),e(sE,tCr),e(z,aCr),e(z,nE),e(nE,W2e),e(W2e,sCr),e(nE,nCr),e(nE,ZX),e(ZX,lCr),e(nE,iCr),e(z,dCr),e(z,lE),e(lE,Q2e),e(Q2e,cCr),e(lE,mCr),e(lE,eV),e(eV,fCr),e(lE,gCr),e(z,hCr),e(z,iE),e(iE,H2e),e(H2e,uCr),e(iE,pCr),e(iE,oV),e(oV,_Cr),e(iE,bCr),e(z,vCr),e(z,dE),e(dE,U2e),e(U2e,TCr),e(dE,FCr),e(dE,rV),e(rV,CCr),e(dE,MCr),e(z,ECr),e(z,cE),e(cE,J2e),e(J2e,yCr),e(cE,wCr),e(cE,tV),e(tV,ACr),e(cE,LCr),e(z,BCr),e(z,mE),e(mE,Y2e),e(Y2e,xCr),e(mE,kCr),e(mE,aV),e(aV,RCr),e(mE,SCr),e(z,PCr),e(z,fE),e(fE,K2e),e(K2e,$Cr),e(fE,ICr),e(fE,sV),e(sV,jCr),e(fE,NCr),e(z,DCr),e(z,gE),e(gE,Z2e),e(Z2e,qCr),e(gE,OCr),e(gE,nV),e(nV,GCr),e(gE,XCr),e(z,VCr),e(z,hE),e(hE,eve),e(eve,zCr),e(hE,WCr),e(hE,lV),e(lV,QCr),e(hE,HCr),e(z,UCr),e(z,uE),e(uE,ove),e(ove,JCr),e(uE,YCr),e(uE,iV),e(iV,KCr),e(uE,ZCr),e(z,eMr),e(z,pE),e(pE,rve),e(rve,oMr),e(pE,rMr),e(pE,dV),e(dV,tMr),e(pE,aMr),e(z,sMr),e(z,_E),e(_E,tve),e(tve,nMr),e(_E,lMr),e(_E,cV),e(cV,iMr),e(_E,dMr),e(z,cMr),e(z,bE),e(bE,ave),e(ave,mMr),e(bE,fMr),e(bE,mV),e(mV,gMr),e(bE,hMr),e(z,uMr),e(z,vE),e(vE,sve),e(sve,pMr),e(vE,_Mr),e(vE,fV),e(fV,bMr),e(vE,vMr),e(z,TMr),e(z,TE),e(TE,nve),e(nve,FMr),e(TE,CMr),e(TE,gV),e(gV,MMr),e(TE,EMr),e(z,yMr),e(z,FE),e(FE,lve),e(lve,wMr),e(FE,AMr),e(FE,hV),e(hV,LMr),e(FE,BMr),e(z,xMr),e(z,CE),e(CE,ive),e(ive,kMr),e(CE,RMr),e(CE,uV),e(uV,SMr),e(CE,PMr),e(Ao,$Mr),e(Ao,dve),e(dve,IMr),e(Ao,jMr),g(wL,Ao,null),b(c,_Re,_),b(c,om,_),e(om,ME),e(ME,cve),g(AL,cve,null),e(om,NMr),e(om,mve),e(mve,DMr),b(c,bRe,_),b(c,Sr,_),g(LL,Sr,null),e(Sr,qMr),e(Sr,rm),e(rm,OMr),e(rm,fve),e(fve,GMr),e(rm,XMr),e(rm,gve),e(gve,VMr),e(rm,zMr),e(Sr,WMr),e(Sr,BL),e(BL,QMr),e(BL,hve),e(hve,HMr),e(BL,UMr),e(Sr,JMr),e(Sr,At),g(xL,At,null),e(At,YMr),e(At,uve),e(uve,KMr),e(At,ZMr),e(At,tm),e(tm,e4r),e(tm,pve),e(pve,o4r),e(tm,r4r),e(tm,_ve),e(_ve,t4r),e(tm,a4r),e(At,s4r),e(At,bve),e(bve,n4r),e(At,l4r),g(kL,At,null),e(Sr,i4r),e(Sr,Lo),g(RL,Lo,null),e(Lo,d4r),e(Lo,vve),e(vve,c4r),e(Lo,m4r),e(Lo,Bs),e(Bs,f4r),e(Bs,Tve),e(Tve,g4r),e(Bs,h4r),e(Bs,Fve),e(Fve,u4r),e(Bs,p4r),e(Bs,Cve),e(Cve,_4r),e(Bs,b4r),e(Lo,v4r),e(Lo,ca),e(ca,EE),e(EE,Mve),e(Mve,T4r),e(EE,F4r),e(EE,pV),e(pV,C4r),e(EE,M4r),e(ca,E4r),e(ca,yE),e(yE,Eve),e(Eve,y4r),e(yE,w4r),e(yE,_V),e(_V,A4r),e(yE,L4r),e(ca,B4r),e(ca,wE),e(wE,yve),e(yve,x4r),e(wE,k4r),e(wE,bV),e(bV,R4r),e(wE,S4r),e(ca,P4r),e(ca,AE),e(AE,wve),e(wve,$4r),e(AE,I4r),e(AE,vV),e(vV,j4r),e(AE,N4r),e(ca,D4r),e(ca,LE),e(LE,Ave),e(Ave,q4r),e(LE,O4r),e(LE,TV),e(TV,G4r),e(LE,X4r),e(Lo,V4r),e(Lo,Lve),e(Lve,z4r),e(Lo,W4r),g(SL,Lo,null),b(c,vRe,_),b(c,am,_),e(am,BE),e(BE,Bve),g(PL,Bve,null),e(am,Q4r),e(am,xve),e(xve,H4r),b(c,TRe,_),b(c,Pr,_),g($L,Pr,null),e(Pr,U4r),e(Pr,sm),e(sm,J4r),e(sm,kve),e(kve,Y4r),e(sm,K4r),e(sm,Rve),e(Rve,Z4r),e(sm,eEr),e(Pr,oEr),e(Pr,IL),e(IL,rEr),e(IL,Sve),e(Sve,tEr),e(IL,aEr),e(Pr,sEr),e(Pr,Lt),g(jL,Lt,null),e(Lt,nEr),e(Lt,Pve),e(Pve,lEr),e(Lt,iEr),e(Lt,nm),e(nm,dEr),e(nm,$ve),e($ve,cEr),e(nm,mEr),e(nm,Ive),e(Ive,fEr),e(nm,gEr),e(Lt,hEr),e(Lt,jve),e(jve,uEr),e(Lt,pEr),g(NL,Lt,null),e(Pr,_Er),e(Pr,Bo),g(DL,Bo,null),e(Bo,bEr),e(Bo,Nve),e(Nve,vEr),e(Bo,TEr),e(Bo,xs),e(xs,FEr),e(xs,Dve),e(Dve,CEr),e(xs,MEr),e(xs,qve),e(qve,EEr),e(xs,yEr),e(xs,Ove),e(Ove,wEr),e(xs,AEr),e(Bo,LEr),e(Bo,ce),e(ce,xE),e(xE,Gve),e(Gve,BEr),e(xE,xEr),e(xE,FV),e(FV,kEr),e(xE,REr),e(ce,SEr),e(ce,kE),e(kE,Xve),e(Xve,PEr),e(kE,$Er),e(kE,CV),e(CV,IEr),e(kE,jEr),e(ce,NEr),e(ce,RE),e(RE,Vve),e(Vve,DEr),e(RE,qEr),e(RE,MV),e(MV,OEr),e(RE,GEr),e(ce,XEr),e(ce,SE),e(SE,zve),e(zve,VEr),e(SE,zEr),e(SE,EV),e(EV,WEr),e(SE,QEr),e(ce,HEr),e(ce,PE),e(PE,Wve),e(Wve,UEr),e(PE,JEr),e(PE,yV),e(yV,YEr),e(PE,KEr),e(ce,ZEr),e(ce,$E),e($E,Qve),e(Qve,e3r),e($E,o3r),e($E,wV),e(wV,r3r),e($E,t3r),e(ce,a3r),e(ce,IE),e(IE,Hve),e(Hve,s3r),e(IE,n3r),e(IE,AV),e(AV,l3r),e(IE,i3r),e(ce,d3r),e(ce,jE),e(jE,Uve),e(Uve,c3r),e(jE,m3r),e(jE,LV),e(LV,f3r),e(jE,g3r),e(ce,h3r),e(ce,NE),e(NE,Jve),e(Jve,u3r),e(NE,p3r),e(NE,BV),e(BV,_3r),e(NE,b3r),e(ce,v3r),e(ce,DE),e(DE,Yve),e(Yve,T3r),e(DE,F3r),e(DE,xV),e(xV,C3r),e(DE,M3r),e(ce,E3r),e(ce,qE),e(qE,Kve),e(Kve,y3r),e(qE,w3r),e(qE,kV),e(kV,A3r),e(qE,L3r),e(ce,B3r),e(ce,OE),e(OE,Zve),e(Zve,x3r),e(OE,k3r),e(OE,RV),e(RV,R3r),e(OE,S3r),e(Bo,P3r),e(Bo,eTe),e(eTe,$3r),e(Bo,I3r),g(qL,Bo,null),b(c,FRe,_),b(c,lm,_),e(lm,GE),e(GE,oTe),g(OL,oTe,null),e(lm,j3r),e(lm,rTe),e(rTe,N3r),b(c,CRe,_),b(c,$r,_),g(GL,$r,null),e($r,D3r),e($r,im),e(im,q3r),e(im,tTe),e(tTe,O3r),e(im,G3r),e(im,aTe),e(aTe,X3r),e(im,V3r),e($r,z3r),e($r,XL),e(XL,W3r),e(XL,sTe),e(sTe,Q3r),e(XL,H3r),e($r,U3r),e($r,Bt),g(VL,Bt,null),e(Bt,J3r),e(Bt,nTe),e(nTe,Y3r),e(Bt,K3r),e(Bt,dm),e(dm,Z3r),e(dm,lTe),e(lTe,e5r),e(dm,o5r),e(dm,iTe),e(iTe,r5r),e(dm,t5r),e(Bt,a5r),e(Bt,dTe),e(dTe,s5r),e(Bt,n5r),g(zL,Bt,null),e($r,l5r),e($r,xo),g(WL,xo,null),e(xo,i5r),e(xo,cTe),e(cTe,d5r),e(xo,c5r),e(xo,ks),e(ks,m5r),e(ks,mTe),e(mTe,f5r),e(ks,g5r),e(ks,fTe),e(fTe,h5r),e(ks,u5r),e(ks,gTe),e(gTe,p5r),e(ks,_5r),e(xo,b5r),e(xo,be),e(be,XE),e(XE,hTe),e(hTe,v5r),e(XE,T5r),e(XE,SV),e(SV,F5r),e(XE,C5r),e(be,M5r),e(be,VE),e(VE,uTe),e(uTe,E5r),e(VE,y5r),e(VE,PV),e(PV,w5r),e(VE,A5r),e(be,L5r),e(be,zE),e(zE,pTe),e(pTe,B5r),e(zE,x5r),e(zE,$V),e($V,k5r),e(zE,R5r),e(be,S5r),e(be,WE),e(WE,_Te),e(_Te,P5r),e(WE,$5r),e(WE,IV),e(IV,I5r),e(WE,j5r),e(be,N5r),e(be,QE),e(QE,bTe),e(bTe,D5r),e(QE,q5r),e(QE,jV),e(jV,O5r),e(QE,G5r),e(be,X5r),e(be,HE),e(HE,vTe),e(vTe,V5r),e(HE,z5r),e(HE,NV),e(NV,W5r),e(HE,Q5r),e(be,H5r),e(be,UE),e(UE,TTe),e(TTe,U5r),e(UE,J5r),e(UE,DV),e(DV,Y5r),e(UE,K5r),e(be,Z5r),e(be,JE),e(JE,FTe),e(FTe,eyr),e(JE,oyr),e(JE,qV),e(qV,ryr),e(JE,tyr),e(be,ayr),e(be,YE),e(YE,CTe),e(CTe,syr),e(YE,nyr),e(YE,OV),e(OV,lyr),e(YE,iyr),e(be,dyr),e(be,KE),e(KE,MTe),e(MTe,cyr),e(KE,myr),e(KE,GV),e(GV,fyr),e(KE,gyr),e(xo,hyr),e(xo,ETe),e(ETe,uyr),e(xo,pyr),g(QL,xo,null),b(c,MRe,_),b(c,cm,_),e(cm,ZE),e(ZE,yTe),g(HL,yTe,null),e(cm,_yr),e(cm,wTe),e(wTe,byr),b(c,ERe,_),b(c,Ir,_),g(UL,Ir,null),e(Ir,vyr),e(Ir,mm),e(mm,Tyr),e(mm,ATe),e(ATe,Fyr),e(mm,Cyr),e(mm,LTe),e(LTe,Myr),e(mm,Eyr),e(Ir,yyr),e(Ir,JL),e(JL,wyr),e(JL,BTe),e(BTe,Ayr),e(JL,Lyr),e(Ir,Byr),e(Ir,xt),g(YL,xt,null),e(xt,xyr),e(xt,xTe),e(xTe,kyr),e(xt,Ryr),e(xt,fm),e(fm,Syr),e(fm,kTe),e(kTe,Pyr),e(fm,$yr),e(fm,RTe),e(RTe,Iyr),e(fm,jyr),e(xt,Nyr),e(xt,STe),e(STe,Dyr),e(xt,qyr),g(KL,xt,null),e(Ir,Oyr),e(Ir,ko),g(ZL,ko,null),e(ko,Gyr),e(ko,PTe),e(PTe,Xyr),e(ko,Vyr),e(ko,Rs),e(Rs,zyr),e(Rs,$Te),e($Te,Wyr),e(Rs,Qyr),e(Rs,ITe),e(ITe,Hyr),e(Rs,Uyr),e(Rs,jTe),e(jTe,Jyr),e(Rs,Yyr),e(ko,Kyr),e(ko,Ee),e(Ee,e3),e(e3,NTe),e(NTe,Zyr),e(e3,ewr),e(e3,XV),e(XV,owr),e(e3,rwr),e(Ee,twr),e(Ee,o3),e(o3,DTe),e(DTe,awr),e(o3,swr),e(o3,VV),e(VV,nwr),e(o3,lwr),e(Ee,iwr),e(Ee,r3),e(r3,qTe),e(qTe,dwr),e(r3,cwr),e(r3,zV),e(zV,mwr),e(r3,fwr),e(Ee,gwr),e(Ee,t3),e(t3,OTe),e(OTe,hwr),e(t3,uwr),e(t3,WV),e(WV,pwr),e(t3,_wr),e(Ee,bwr),e(Ee,a3),e(a3,GTe),e(GTe,vwr),e(a3,Twr),e(a3,QV),e(QV,Fwr),e(a3,Cwr),e(Ee,Mwr),e(Ee,s3),e(s3,XTe),e(XTe,Ewr),e(s3,ywr),e(s3,HV),e(HV,wwr),e(s3,Awr),e(Ee,Lwr),e(Ee,n3),e(n3,VTe),e(VTe,Bwr),e(n3,xwr),e(n3,UV),e(UV,kwr),e(n3,Rwr),e(Ee,Swr),e(Ee,l3),e(l3,zTe),e(zTe,Pwr),e(l3,$wr),e(l3,JV),e(JV,Iwr),e(l3,jwr),e(Ee,Nwr),e(Ee,i3),e(i3,WTe),e(WTe,Dwr),e(i3,qwr),e(i3,YV),e(YV,Owr),e(i3,Gwr),e(ko,Xwr),e(ko,QTe),e(QTe,Vwr),e(ko,zwr),g(e7,ko,null),b(c,yRe,_),b(c,gm,_),e(gm,d3),e(d3,HTe),g(o7,HTe,null),e(gm,Wwr),e(gm,UTe),e(UTe,Qwr),b(c,wRe,_),b(c,jr,_),g(r7,jr,null),e(jr,Hwr),e(jr,hm),e(hm,Uwr),e(hm,JTe),e(JTe,Jwr),e(hm,Ywr),e(hm,YTe),e(YTe,Kwr),e(hm,Zwr),e(jr,e6r),e(jr,t7),e(t7,o6r),e(t7,KTe),e(KTe,r6r),e(t7,t6r),e(jr,a6r),e(jr,kt),g(a7,kt,null),e(kt,s6r),e(kt,ZTe),e(ZTe,n6r),e(kt,l6r),e(kt,um),e(um,i6r),e(um,e1e),e(e1e,d6r),e(um,c6r),e(um,o1e),e(o1e,m6r),e(um,f6r),e(kt,g6r),e(kt,r1e),e(r1e,h6r),e(kt,u6r),g(s7,kt,null),e(jr,p6r),e(jr,Ro),g(n7,Ro,null),e(Ro,_6r),e(Ro,t1e),e(t1e,b6r),e(Ro,v6r),e(Ro,Ss),e(Ss,T6r),e(Ss,a1e),e(a1e,F6r),e(Ss,C6r),e(Ss,s1e),e(s1e,M6r),e(Ss,E6r),e(Ss,n1e),e(n1e,y6r),e(Ss,w6r),e(Ro,A6r),e(Ro,ve),e(ve,c3),e(c3,l1e),e(l1e,L6r),e(c3,B6r),e(c3,KV),e(KV,x6r),e(c3,k6r),e(ve,R6r),e(ve,m3),e(m3,i1e),e(i1e,S6r),e(m3,P6r),e(m3,ZV),e(ZV,$6r),e(m3,I6r),e(ve,j6r),e(ve,f3),e(f3,d1e),e(d1e,N6r),e(f3,D6r),e(f3,ez),e(ez,q6r),e(f3,O6r),e(ve,G6r),e(ve,g3),e(g3,c1e),e(c1e,X6r),e(g3,V6r),e(g3,oz),e(oz,z6r),e(g3,W6r),e(ve,Q6r),e(ve,h3),e(h3,m1e),e(m1e,H6r),e(h3,U6r),e(h3,rz),e(rz,J6r),e(h3,Y6r),e(ve,K6r),e(ve,u3),e(u3,f1e),e(f1e,Z6r),e(u3,eAr),e(u3,tz),e(tz,oAr),e(u3,rAr),e(ve,tAr),e(ve,p3),e(p3,g1e),e(g1e,aAr),e(p3,sAr),e(p3,az),e(az,nAr),e(p3,lAr),e(ve,iAr),e(ve,_3),e(_3,h1e),e(h1e,dAr),e(_3,cAr),e(_3,sz),e(sz,mAr),e(_3,fAr),e(ve,gAr),e(ve,b3),e(b3,u1e),e(u1e,hAr),e(b3,uAr),e(b3,nz),e(nz,pAr),e(b3,_Ar),e(ve,bAr),e(ve,v3),e(v3,p1e),e(p1e,vAr),e(v3,TAr),e(v3,lz),e(lz,FAr),e(v3,CAr),e(Ro,MAr),e(Ro,_1e),e(_1e,EAr),e(Ro,yAr),g(l7,Ro,null),b(c,ARe,_),b(c,pm,_),e(pm,T3),e(T3,b1e),g(i7,b1e,null),e(pm,wAr),e(pm,v1e),e(v1e,AAr),b(c,LRe,_),b(c,Nr,_),g(d7,Nr,null),e(Nr,LAr),e(Nr,_m),e(_m,BAr),e(_m,T1e),e(T1e,xAr),e(_m,kAr),e(_m,F1e),e(F1e,RAr),e(_m,SAr),e(Nr,PAr),e(Nr,c7),e(c7,$Ar),e(c7,C1e),e(C1e,IAr),e(c7,jAr),e(Nr,NAr),e(Nr,Rt),g(m7,Rt,null),e(Rt,DAr),e(Rt,M1e),e(M1e,qAr),e(Rt,OAr),e(Rt,bm),e(bm,GAr),e(bm,E1e),e(E1e,XAr),e(bm,VAr),e(bm,y1e),e(y1e,zAr),e(bm,WAr),e(Rt,QAr),e(Rt,w1e),e(w1e,HAr),e(Rt,UAr),g(f7,Rt,null),e(Nr,JAr),e(Nr,So),g(g7,So,null),e(So,YAr),e(So,A1e),e(A1e,KAr),e(So,ZAr),e(So,Ps),e(Ps,e0r),e(Ps,L1e),e(L1e,o0r),e(Ps,r0r),e(Ps,B1e),e(B1e,t0r),e(Ps,a0r),e(Ps,x1e),e(x1e,s0r),e(Ps,n0r),e(So,l0r),e(So,Te),e(Te,F3),e(F3,k1e),e(k1e,i0r),e(F3,d0r),e(F3,iz),e(iz,c0r),e(F3,m0r),e(Te,f0r),e(Te,C3),e(C3,R1e),e(R1e,g0r),e(C3,h0r),e(C3,dz),e(dz,u0r),e(C3,p0r),e(Te,_0r),e(Te,M3),e(M3,S1e),e(S1e,b0r),e(M3,v0r),e(M3,cz),e(cz,T0r),e(M3,F0r),e(Te,C0r),e(Te,E3),e(E3,P1e),e(P1e,M0r),e(E3,E0r),e(E3,mz),e(mz,y0r),e(E3,w0r),e(Te,A0r),e(Te,y3),e(y3,$1e),e($1e,L0r),e(y3,B0r),e(y3,fz),e(fz,x0r),e(y3,k0r),e(Te,R0r),e(Te,w3),e(w3,I1e),e(I1e,S0r),e(w3,P0r),e(w3,gz),e(gz,$0r),e(w3,I0r),e(Te,j0r),e(Te,A3),e(A3,j1e),e(j1e,N0r),e(A3,D0r),e(A3,hz),e(hz,q0r),e(A3,O0r),e(Te,G0r),e(Te,L3),e(L3,N1e),e(N1e,X0r),e(L3,V0r),e(L3,uz),e(uz,z0r),e(L3,W0r),e(Te,Q0r),e(Te,B3),e(B3,D1e),e(D1e,H0r),e(B3,U0r),e(B3,pz),e(pz,J0r),e(B3,Y0r),e(Te,K0r),e(Te,x3),e(x3,q1e),e(q1e,Z0r),e(x3,eLr),e(x3,_z),e(_z,oLr),e(x3,rLr),e(So,tLr),e(So,O1e),e(O1e,aLr),e(So,sLr),g(h7,So,null),b(c,BRe,_),b(c,vm,_),e(vm,k3),e(k3,G1e),g(u7,G1e,null),e(vm,nLr),e(vm,X1e),e(X1e,lLr),b(c,xRe,_),b(c,Dr,_),g(p7,Dr,null),e(Dr,iLr),e(Dr,Tm),e(Tm,dLr),e(Tm,V1e),e(V1e,cLr),e(Tm,mLr),e(Tm,z1e),e(z1e,fLr),e(Tm,gLr),e(Dr,hLr),e(Dr,_7),e(_7,uLr),e(_7,W1e),e(W1e,pLr),e(_7,_Lr),e(Dr,bLr),e(Dr,St),g(b7,St,null),e(St,vLr),e(St,Q1e),e(Q1e,TLr),e(St,FLr),e(St,Fm),e(Fm,CLr),e(Fm,H1e),e(H1e,MLr),e(Fm,ELr),e(Fm,U1e),e(U1e,yLr),e(Fm,wLr),e(St,ALr),e(St,J1e),e(J1e,LLr),e(St,BLr),g(v7,St,null),e(Dr,xLr),e(Dr,Po),g(T7,Po,null),e(Po,kLr),e(Po,Y1e),e(Y1e,RLr),e(Po,SLr),e(Po,$s),e($s,PLr),e($s,K1e),e(K1e,$Lr),e($s,ILr),e($s,Z1e),e(Z1e,jLr),e($s,NLr),e($s,eFe),e(eFe,DLr),e($s,qLr),e(Po,OLr),e(Po,Se),e(Se,R3),e(R3,oFe),e(oFe,GLr),e(R3,XLr),e(R3,bz),e(bz,VLr),e(R3,zLr),e(Se,WLr),e(Se,S3),e(S3,rFe),e(rFe,QLr),e(S3,HLr),e(S3,vz),e(vz,ULr),e(S3,JLr),e(Se,YLr),e(Se,P3),e(P3,tFe),e(tFe,KLr),e(P3,ZLr),e(P3,Tz),e(Tz,e7r),e(P3,o7r),e(Se,r7r),e(Se,$3),e($3,aFe),e(aFe,t7r),e($3,a7r),e($3,Fz),e(Fz,s7r),e($3,n7r),e(Se,l7r),e(Se,I3),e(I3,sFe),e(sFe,i7r),e(I3,d7r),e(I3,Cz),e(Cz,c7r),e(I3,m7r),e(Se,f7r),e(Se,j3),e(j3,nFe),e(nFe,g7r),e(j3,h7r),e(j3,Mz),e(Mz,u7r),e(j3,p7r),e(Se,_7r),e(Se,N3),e(N3,lFe),e(lFe,b7r),e(N3,v7r),e(N3,Ez),e(Ez,T7r),e(N3,F7r),e(Se,C7r),e(Se,D3),e(D3,iFe),e(iFe,M7r),e(D3,E7r),e(D3,yz),e(yz,y7r),e(D3,w7r),e(Po,A7r),e(Po,dFe),e(dFe,L7r),e(Po,B7r),g(F7,Po,null),b(c,kRe,_),b(c,Cm,_),e(Cm,q3),e(q3,cFe),g(C7,cFe,null),e(Cm,x7r),e(Cm,mFe),e(mFe,k7r),b(c,RRe,_),b(c,qr,_),g(M7,qr,null),e(qr,R7r),e(qr,Mm),e(Mm,S7r),e(Mm,fFe),e(fFe,P7r),e(Mm,$7r),e(Mm,gFe),e(gFe,I7r),e(Mm,j7r),e(qr,N7r),e(qr,E7),e(E7,D7r),e(E7,hFe),e(hFe,q7r),e(E7,O7r),e(qr,G7r),e(qr,Pt),g(y7,Pt,null),e(Pt,X7r),e(Pt,uFe),e(uFe,V7r),e(Pt,z7r),e(Pt,Em),e(Em,W7r),e(Em,pFe),e(pFe,Q7r),e(Em,H7r),e(Em,_Fe),e(_Fe,U7r),e(Em,J7r),e(Pt,Y7r),e(Pt,bFe),e(bFe,K7r),e(Pt,Z7r),g(w7,Pt,null),e(qr,e8r),e(qr,$o),g(A7,$o,null),e($o,o8r),e($o,vFe),e(vFe,r8r),e($o,t8r),e($o,Is),e(Is,a8r),e(Is,TFe),e(TFe,s8r),e(Is,n8r),e(Is,FFe),e(FFe,l8r),e(Is,i8r),e(Is,CFe),e(CFe,d8r),e(Is,c8r),e($o,m8r),e($o,Pe),e(Pe,O3),e(O3,MFe),e(MFe,f8r),e(O3,g8r),e(O3,wz),e(wz,h8r),e(O3,u8r),e(Pe,p8r),e(Pe,G3),e(G3,EFe),e(EFe,_8r),e(G3,b8r),e(G3,Az),e(Az,v8r),e(G3,T8r),e(Pe,F8r),e(Pe,X3),e(X3,yFe),e(yFe,C8r),e(X3,M8r),e(X3,Lz),e(Lz,E8r),e(X3,y8r),e(Pe,w8r),e(Pe,V3),e(V3,wFe),e(wFe,A8r),e(V3,L8r),e(V3,Bz),e(Bz,B8r),e(V3,x8r),e(Pe,k8r),e(Pe,z3),e(z3,AFe),e(AFe,R8r),e(z3,S8r),e(z3,xz),e(xz,P8r),e(z3,$8r),e(Pe,I8r),e(Pe,W3),e(W3,LFe),e(LFe,j8r),e(W3,N8r),e(W3,kz),e(kz,D8r),e(W3,q8r),e(Pe,O8r),e(Pe,Q3),e(Q3,BFe),e(BFe,G8r),e(Q3,X8r),e(Q3,Rz),e(Rz,V8r),e(Q3,z8r),e(Pe,W8r),e(Pe,H3),e(H3,xFe),e(xFe,Q8r),e(H3,H8r),e(H3,Sz),e(Sz,U8r),e(H3,J8r),e($o,Y8r),e($o,kFe),e(kFe,K8r),e($o,Z8r),g(L7,$o,null),b(c,SRe,_),b(c,ym,_),e(ym,U3),e(U3,RFe),g(B7,RFe,null),e(ym,e9r),e(ym,SFe),e(SFe,o9r),b(c,PRe,_),b(c,Or,_),g(x7,Or,null),e(Or,r9r),e(Or,wm),e(wm,t9r),e(wm,PFe),e(PFe,a9r),e(wm,s9r),e(wm,$Fe),e($Fe,n9r),e(wm,l9r),e(Or,i9r),e(Or,k7),e(k7,d9r),e(k7,IFe),e(IFe,c9r),e(k7,m9r),e(Or,f9r),e(Or,$t),g(R7,$t,null),e($t,g9r),e($t,jFe),e(jFe,h9r),e($t,u9r),e($t,Am),e(Am,p9r),e(Am,NFe),e(NFe,_9r),e(Am,b9r),e(Am,DFe),e(DFe,v9r),e(Am,T9r),e($t,F9r),e($t,qFe),e(qFe,C9r),e($t,M9r),g(S7,$t,null),e(Or,E9r),e(Or,Io),g(P7,Io,null),e(Io,y9r),e(Io,OFe),e(OFe,w9r),e(Io,A9r),e(Io,js),e(js,L9r),e(js,GFe),e(GFe,B9r),e(js,x9r),e(js,XFe),e(XFe,k9r),e(js,R9r),e(js,VFe),e(VFe,S9r),e(js,P9r),e(Io,$9r),e(Io,zFe),e(zFe,J3),e(J3,WFe),e(WFe,I9r),e(J3,j9r),e(J3,Pz),e(Pz,N9r),e(J3,D9r),e(Io,q9r),e(Io,QFe),e(QFe,O9r),e(Io,G9r),g($7,Io,null),b(c,$Re,_),b(c,Lm,_),e(Lm,Y3),e(Y3,HFe),g(I7,HFe,null),e(Lm,X9r),e(Lm,UFe),e(UFe,V9r),b(c,IRe,_),b(c,Gr,_),g(j7,Gr,null),e(Gr,z9r),e(Gr,Bm),e(Bm,W9r),e(Bm,JFe),e(JFe,Q9r),e(Bm,H9r),e(Bm,YFe),e(YFe,U9r),e(Bm,J9r),e(Gr,Y9r),e(Gr,N7),e(N7,K9r),e(N7,KFe),e(KFe,Z9r),e(N7,eBr),e(Gr,oBr),e(Gr,It),g(D7,It,null),e(It,rBr),e(It,ZFe),e(ZFe,tBr),e(It,aBr),e(It,xm),e(xm,sBr),e(xm,eCe),e(eCe,nBr),e(xm,lBr),e(xm,oCe),e(oCe,iBr),e(xm,dBr),e(It,cBr),e(It,rCe),e(rCe,mBr),e(It,fBr),g(q7,It,null),e(Gr,gBr),e(Gr,jo),g(O7,jo,null),e(jo,hBr),e(jo,tCe),e(tCe,uBr),e(jo,pBr),e(jo,Ns),e(Ns,_Br),e(Ns,aCe),e(aCe,bBr),e(Ns,vBr),e(Ns,sCe),e(sCe,TBr),e(Ns,FBr),e(Ns,nCe),e(nCe,CBr),e(Ns,MBr),e(jo,EBr),e(jo,G7),e(G7,K3),e(K3,lCe),e(lCe,yBr),e(K3,wBr),e(K3,$z),e($z,ABr),e(K3,LBr),e(G7,BBr),e(G7,Z3),e(Z3,iCe),e(iCe,xBr),e(Z3,kBr),e(Z3,Iz),e(Iz,RBr),e(Z3,SBr),e(jo,PBr),e(jo,dCe),e(dCe,$Br),e(jo,IBr),g(X7,jo,null),b(c,jRe,_),b(c,km,_),e(km,e5),e(e5,cCe),g(V7,cCe,null),e(km,jBr),e(km,mCe),e(mCe,NBr),b(c,NRe,_),b(c,Xr,_),g(z7,Xr,null),e(Xr,DBr),e(Xr,Rm),e(Rm,qBr),e(Rm,fCe),e(fCe,OBr),e(Rm,GBr),e(Rm,gCe),e(gCe,XBr),e(Rm,VBr),e(Xr,zBr),e(Xr,W7),e(W7,WBr),e(W7,hCe),e(hCe,QBr),e(W7,HBr),e(Xr,UBr),e(Xr,jt),g(Q7,jt,null),e(jt,JBr),e(jt,uCe),e(uCe,YBr),e(jt,KBr),e(jt,Sm),e(Sm,ZBr),e(Sm,pCe),e(pCe,exr),e(Sm,oxr),e(Sm,_Ce),e(_Ce,rxr),e(Sm,txr),e(jt,axr),e(jt,bCe),e(bCe,sxr),e(jt,nxr),g(H7,jt,null),e(Xr,lxr),e(Xr,No),g(U7,No,null),e(No,ixr),e(No,vCe),e(vCe,dxr),e(No,cxr),e(No,Ds),e(Ds,mxr),e(Ds,TCe),e(TCe,fxr),e(Ds,gxr),e(Ds,FCe),e(FCe,hxr),e(Ds,uxr),e(Ds,CCe),e(CCe,pxr),e(Ds,_xr),e(No,bxr),e(No,MCe),e(MCe,o5),e(o5,ECe),e(ECe,vxr),e(o5,Txr),e(o5,jz),e(jz,Fxr),e(o5,Cxr),e(No,Mxr),e(No,yCe),e(yCe,Exr),e(No,yxr),g(J7,No,null),DRe=!0},p(c,[_]){const Y7={};_&2&&(Y7.$$scope={dirty:_,ctx:c}),qm.$set(Y7);const wCe={};_&2&&(wCe.$$scope={dirty:_,ctx:c}),Lh.$set(wCe);const ACe={};_&2&&(ACe.$$scope={dirty:_,ctx:c}),Dh.$set(ACe)},i(c){DRe||(h(me.$$.fragment,c),h(qa.$$.fragment,c),h(sy.$$.fragment,c),h(ny.$$.fragment,c),h(qm.$$.fragment,c),h(ly.$$.fragment,c),h(iy.$$.fragment,c),h(my.$$.fragment,c),h(fy.$$.fragment,c),h(gy.$$.fragment,c),h(hy.$$.fragment,c),h(uy.$$.fragment,c),h(by.$$.fragment,c),h(vy.$$.fragment,c),h(Ty.$$.fragment,c),h(Fy.$$.fragment,c),h(Cy.$$.fragment,c),h(yy.$$.fragment,c),h(Lh.$$.fragment,c),h(wy.$$.fragment,c),h(Ay.$$.fragment,c),h(Ly.$$.fragment,c),h(By.$$.fragment,c),h(Ry.$$.fragment,c),h(Dh.$$.fragment,c),h(Sy.$$.fragment,c),h(Py.$$.fragment,c),h($y.$$.fragment,c),h(Iy.$$.fragment,c),h(Ny.$$.fragment,c),h(Dy.$$.fragment,c),h(qy.$$.fragment,c),h(Oy.$$.fragment,c),h(Gy.$$.fragment,c),h(Xy.$$.fragment,c),h(zy.$$.fragment,c),h(Wy.$$.fragment,c),h(Qy.$$.fragment,c),h(Hy.$$.fragment,c),h(Uy.$$.fragment,c),h(Jy.$$.fragment,c),h(Ky.$$.fragment,c),h(Zy.$$.fragment,c),h(ew.$$.fragment,c),h(ow.$$.fragment,c),h(rw.$$.fragment,c),h(tw.$$.fragment,c),h(sw.$$.fragment,c),h(nw.$$.fragment,c),h(lw.$$.fragment,c),h(iw.$$.fragment,c),h(dw.$$.fragment,c),h(cw.$$.fragment,c),h(fw.$$.fragment,c),h(gw.$$.fragment,c),h(hw.$$.fragment,c),h(uw.$$.fragment,c),h(pw.$$.fragment,c),h(_w.$$.fragment,c),h(vw.$$.fragment,c),h(Tw.$$.fragment,c),h(Fw.$$.fragment,c),h(Cw.$$.fragment,c),h(Mw.$$.fragment,c),h(Ew.$$.fragment,c),h(ww.$$.fragment,c),h(Aw.$$.fragment,c),h(Lw.$$.fragment,c),h(Bw.$$.fragment,c),h(xw.$$.fragment,c),h(kw.$$.fragment,c),h(Sw.$$.fragment,c),h(Pw.$$.fragment,c),h($w.$$.fragment,c),h(Iw.$$.fragment,c),h(jw.$$.fragment,c),h(Nw.$$.fragment,c),h(qw.$$.fragment,c),h(Ow.$$.fragment,c),h(Gw.$$.fragment,c),h(Xw.$$.fragment,c),h(Vw.$$.fragment,c),h(zw.$$.fragment,c),h(Qw.$$.fragment,c),h(Hw.$$.fragment,c),h(Uw.$$.fragment,c),h(Jw.$$.fragment,c),h(Yw.$$.fragment,c),h(Kw.$$.fragment,c),h(e6.$$.fragment,c),h(o6.$$.fragment,c),h(r6.$$.fragment,c),h(t6.$$.fragment,c),h(a6.$$.fragment,c),h(s6.$$.fragment,c),h(l6.$$.fragment,c),h(i6.$$.fragment,c),h(d6.$$.fragment,c),h(c6.$$.fragment,c),h(m6.$$.fragment,c),h(f6.$$.fragment,c),h(h6.$$.fragment,c),h(u6.$$.fragment,c),h(p6.$$.fragment,c),h(_6.$$.fragment,c),h(b6.$$.fragment,c),h(v6.$$.fragment,c),h(F6.$$.fragment,c),h(C6.$$.fragment,c),h(M6.$$.fragment,c),h(E6.$$.fragment,c),h(y6.$$.fragment,c),h(w6.$$.fragment,c),h(L6.$$.fragment,c),h(B6.$$.fragment,c),h(x6.$$.fragment,c),h(k6.$$.fragment,c),h(R6.$$.fragment,c),h(S6.$$.fragment,c),h($6.$$.fragment,c),h(I6.$$.fragment,c),h(j6.$$.fragment,c),h(N6.$$.fragment,c),h(D6.$$.fragment,c),h(q6.$$.fragment,c),h(G6.$$.fragment,c),h(X6.$$.fragment,c),h(V6.$$.fragment,c),h(W6.$$.fragment,c),h(Q6.$$.fragment,c),h(H6.$$.fragment,c),h(J6.$$.fragment,c),h(Y6.$$.fragment,c),h(K6.$$.fragment,c),h(Z6.$$.fragment,c),h(eA.$$.fragment,c),h(oA.$$.fragment,c),h(tA.$$.fragment,c),h(aA.$$.fragment,c),h(sA.$$.fragment,c),h(nA.$$.fragment,c),h(lA.$$.fragment,c),h(iA.$$.fragment,c),h(cA.$$.fragment,c),h(mA.$$.fragment,c),h(fA.$$.fragment,c),h(gA.$$.fragment,c),h(hA.$$.fragment,c),h(uA.$$.fragment,c),h(_A.$$.fragment,c),h(bA.$$.fragment,c),h(vA.$$.fragment,c),h(TA.$$.fragment,c),h(FA.$$.fragment,c),h(CA.$$.fragment,c),h(EA.$$.fragment,c),h(yA.$$.fragment,c),h(wA.$$.fragment,c),h(LA.$$.fragment,c),h(BA.$$.fragment,c),h(xA.$$.fragment,c),h(RA.$$.fragment,c),h(SA.$$.fragment,c),h(PA.$$.fragment,c),h($A.$$.fragment,c),h(IA.$$.fragment,c),h(jA.$$.fragment,c),h(DA.$$.fragment,c),h(qA.$$.fragment,c),h(OA.$$.fragment,c),h(GA.$$.fragment,c),h(XA.$$.fragment,c),h(VA.$$.fragment,c),h(WA.$$.fragment,c),h(QA.$$.fragment,c),h(HA.$$.fragment,c),h(UA.$$.fragment,c),h(JA.$$.fragment,c),h(YA.$$.fragment,c),h(ZA.$$.fragment,c),h(e0.$$.fragment,c),h(o0.$$.fragment,c),h(r0.$$.fragment,c),h(t0.$$.fragment,c),h(a0.$$.fragment,c),h(n0.$$.fragment,c),h(l0.$$.fragment,c),h(i0.$$.fragment,c),h(c0.$$.fragment,c),h(m0.$$.fragment,c),h(f0.$$.fragment,c),h(h0.$$.fragment,c),h(u0.$$.fragment,c),h(p0.$$.fragment,c),h(_0.$$.fragment,c),h(b0.$$.fragment,c),h(v0.$$.fragment,c),h(F0.$$.fragment,c),h(C0.$$.fragment,c),h(M0.$$.fragment,c),h(E0.$$.fragment,c),h(y0.$$.fragment,c),h(w0.$$.fragment,c),h(L0.$$.fragment,c),h(B0.$$.fragment,c),h(x0.$$.fragment,c),h(k0.$$.fragment,c),h(R0.$$.fragment,c),h(S0.$$.fragment,c),h($0.$$.fragment,c),h(I0.$$.fragment,c),h(j0.$$.fragment,c),h(N0.$$.fragment,c),h(D0.$$.fragment,c),h(q0.$$.fragment,c),h(G0.$$.fragment,c),h(X0.$$.fragment,c),h(V0.$$.fragment,c),h(z0.$$.fragment,c),h(W0.$$.fragment,c),h(Q0.$$.fragment,c),h(U0.$$.fragment,c),h(J0.$$.fragment,c),h(Y0.$$.fragment,c),h(K0.$$.fragment,c),h(Z0.$$.fragment,c),h(eL.$$.fragment,c),h(rL.$$.fragment,c),h(tL.$$.fragment,c),h(aL.$$.fragment,c),h(sL.$$.fragment,c),h(nL.$$.fragment,c),h(lL.$$.fragment,c),h(dL.$$.fragment,c),h(cL.$$.fragment,c),h(mL.$$.fragment,c),h(fL.$$.fragment,c),h(gL.$$.fragment,c),h(hL.$$.fragment,c),h(pL.$$.fragment,c),h(_L.$$.fragment,c),h(bL.$$.fragment,c),h(vL.$$.fragment,c),h(TL.$$.fragment,c),h(FL.$$.fragment,c),h(ML.$$.fragment,c),h(EL.$$.fragment,c),h(yL.$$.fragment,c),h(wL.$$.fragment,c),h(AL.$$.fragment,c),h(LL.$$.fragment,c),h(xL.$$.fragment,c),h(kL.$$.fragment,c),h(RL.$$.fragment,c),h(SL.$$.fragment,c),h(PL.$$.fragment,c),h($L.$$.fragment,c),h(jL.$$.fragment,c),h(NL.$$.fragment,c),h(DL.$$.fragment,c),h(qL.$$.fragment,c),h(OL.$$.fragment,c),h(GL.$$.fragment,c),h(VL.$$.fragment,c),h(zL.$$.fragment,c),h(WL.$$.fragment,c),h(QL.$$.fragment,c),h(HL.$$.fragment,c),h(UL.$$.fragment,c),h(YL.$$.fragment,c),h(KL.$$.fragment,c),h(ZL.$$.fragment,c),h(e7.$$.fragment,c),h(o7.$$.fragment,c),h(r7.$$.fragment,c),h(a7.$$.fragment,c),h(s7.$$.fragment,c),h(n7.$$.fragment,c),h(l7.$$.fragment,c),h(i7.$$.fragment,c),h(d7.$$.fragment,c),h(m7.$$.fragment,c),h(f7.$$.fragment,c),h(g7.$$.fragment,c),h(h7.$$.fragment,c),h(u7.$$.fragment,c),h(p7.$$.fragment,c),h(b7.$$.fragment,c),h(v7.$$.fragment,c),h(T7.$$.fragment,c),h(F7.$$.fragment,c),h(C7.$$.fragment,c),h(M7.$$.fragment,c),h(y7.$$.fragment,c),h(w7.$$.fragment,c),h(A7.$$.fragment,c),h(L7.$$.fragment,c),h(B7.$$.fragment,c),h(x7.$$.fragment,c),h(R7.$$.fragment,c),h(S7.$$.fragment,c),h(P7.$$.fragment,c),h($7.$$.fragment,c),h(I7.$$.fragment,c),h(j7.$$.fragment,c),h(D7.$$.fragment,c),h(q7.$$.fragment,c),h(O7.$$.fragment,c),h(X7.$$.fragment,c),h(V7.$$.fragment,c),h(z7.$$.fragment,c),h(Q7.$$.fragment,c),h(H7.$$.fragment,c),h(U7.$$.fragment,c),h(J7.$$.fragment,c),DRe=!0)},o(c){u(me.$$.fragment,c),u(qa.$$.fragment,c),u(sy.$$.fragment,c),u(ny.$$.fragment,c),u(qm.$$.fragment,c),u(ly.$$.fragment,c),u(iy.$$.fragment,c),u(my.$$.fragment,c),u(fy.$$.fragment,c),u(gy.$$.fragment,c),u(hy.$$.fragment,c),u(uy.$$.fragment,c),u(by.$$.fragment,c),u(vy.$$.fragment,c),u(Ty.$$.fragment,c),u(Fy.$$.fragment,c),u(Cy.$$.fragment,c),u(yy.$$.fragment,c),u(Lh.$$.fragment,c),u(wy.$$.fragment,c),u(Ay.$$.fragment,c),u(Ly.$$.fragment,c),u(By.$$.fragment,c),u(Ry.$$.fragment,c),u(Dh.$$.fragment,c),u(Sy.$$.fragment,c),u(Py.$$.fragment,c),u($y.$$.fragment,c),u(Iy.$$.fragment,c),u(Ny.$$.fragment,c),u(Dy.$$.fragment,c),u(qy.$$.fragment,c),u(Oy.$$.fragment,c),u(Gy.$$.fragment,c),u(Xy.$$.fragment,c),u(zy.$$.fragment,c),u(Wy.$$.fragment,c),u(Qy.$$.fragment,c),u(Hy.$$.fragment,c),u(Uy.$$.fragment,c),u(Jy.$$.fragment,c),u(Ky.$$.fragment,c),u(Zy.$$.fragment,c),u(ew.$$.fragment,c),u(ow.$$.fragment,c),u(rw.$$.fragment,c),u(tw.$$.fragment,c),u(sw.$$.fragment,c),u(nw.$$.fragment,c),u(lw.$$.fragment,c),u(iw.$$.fragment,c),u(dw.$$.fragment,c),u(cw.$$.fragment,c),u(fw.$$.fragment,c),u(gw.$$.fragment,c),u(hw.$$.fragment,c),u(uw.$$.fragment,c),u(pw.$$.fragment,c),u(_w.$$.fragment,c),u(vw.$$.fragment,c),u(Tw.$$.fragment,c),u(Fw.$$.fragment,c),u(Cw.$$.fragment,c),u(Mw.$$.fragment,c),u(Ew.$$.fragment,c),u(ww.$$.fragment,c),u(Aw.$$.fragment,c),u(Lw.$$.fragment,c),u(Bw.$$.fragment,c),u(xw.$$.fragment,c),u(kw.$$.fragment,c),u(Sw.$$.fragment,c),u(Pw.$$.fragment,c),u($w.$$.fragment,c),u(Iw.$$.fragment,c),u(jw.$$.fragment,c),u(Nw.$$.fragment,c),u(qw.$$.fragment,c),u(Ow.$$.fragment,c),u(Gw.$$.fragment,c),u(Xw.$$.fragment,c),u(Vw.$$.fragment,c),u(zw.$$.fragment,c),u(Qw.$$.fragment,c),u(Hw.$$.fragment,c),u(Uw.$$.fragment,c),u(Jw.$$.fragment,c),u(Yw.$$.fragment,c),u(Kw.$$.fragment,c),u(e6.$$.fragment,c),u(o6.$$.fragment,c),u(r6.$$.fragment,c),u(t6.$$.fragment,c),u(a6.$$.fragment,c),u(s6.$$.fragment,c),u(l6.$$.fragment,c),u(i6.$$.fragment,c),u(d6.$$.fragment,c),u(c6.$$.fragment,c),u(m6.$$.fragment,c),u(f6.$$.fragment,c),u(h6.$$.fragment,c),u(u6.$$.fragment,c),u(p6.$$.fragment,c),u(_6.$$.fragment,c),u(b6.$$.fragment,c),u(v6.$$.fragment,c),u(F6.$$.fragment,c),u(C6.$$.fragment,c),u(M6.$$.fragment,c),u(E6.$$.fragment,c),u(y6.$$.fragment,c),u(w6.$$.fragment,c),u(L6.$$.fragment,c),u(B6.$$.fragment,c),u(x6.$$.fragment,c),u(k6.$$.fragment,c),u(R6.$$.fragment,c),u(S6.$$.fragment,c),u($6.$$.fragment,c),u(I6.$$.fragment,c),u(j6.$$.fragment,c),u(N6.$$.fragment,c),u(D6.$$.fragment,c),u(q6.$$.fragment,c),u(G6.$$.fragment,c),u(X6.$$.fragment,c),u(V6.$$.fragment,c),u(W6.$$.fragment,c),u(Q6.$$.fragment,c),u(H6.$$.fragment,c),u(J6.$$.fragment,c),u(Y6.$$.fragment,c),u(K6.$$.fragment,c),u(Z6.$$.fragment,c),u(eA.$$.fragment,c),u(oA.$$.fragment,c),u(tA.$$.fragment,c),u(aA.$$.fragment,c),u(sA.$$.fragment,c),u(nA.$$.fragment,c),u(lA.$$.fragment,c),u(iA.$$.fragment,c),u(cA.$$.fragment,c),u(mA.$$.fragment,c),u(fA.$$.fragment,c),u(gA.$$.fragment,c),u(hA.$$.fragment,c),u(uA.$$.fragment,c),u(_A.$$.fragment,c),u(bA.$$.fragment,c),u(vA.$$.fragment,c),u(TA.$$.fragment,c),u(FA.$$.fragment,c),u(CA.$$.fragment,c),u(EA.$$.fragment,c),u(yA.$$.fragment,c),u(wA.$$.fragment,c),u(LA.$$.fragment,c),u(BA.$$.fragment,c),u(xA.$$.fragment,c),u(RA.$$.fragment,c),u(SA.$$.fragment,c),u(PA.$$.fragment,c),u($A.$$.fragment,c),u(IA.$$.fragment,c),u(jA.$$.fragment,c),u(DA.$$.fragment,c),u(qA.$$.fragment,c),u(OA.$$.fragment,c),u(GA.$$.fragment,c),u(XA.$$.fragment,c),u(VA.$$.fragment,c),u(WA.$$.fragment,c),u(QA.$$.fragment,c),u(HA.$$.fragment,c),u(UA.$$.fragment,c),u(JA.$$.fragment,c),u(YA.$$.fragment,c),u(ZA.$$.fragment,c),u(e0.$$.fragment,c),u(o0.$$.fragment,c),u(r0.$$.fragment,c),u(t0.$$.fragment,c),u(a0.$$.fragment,c),u(n0.$$.fragment,c),u(l0.$$.fragment,c),u(i0.$$.fragment,c),u(c0.$$.fragment,c),u(m0.$$.fragment,c),u(f0.$$.fragment,c),u(h0.$$.fragment,c),u(u0.$$.fragment,c),u(p0.$$.fragment,c),u(_0.$$.fragment,c),u(b0.$$.fragment,c),u(v0.$$.fragment,c),u(F0.$$.fragment,c),u(C0.$$.fragment,c),u(M0.$$.fragment,c),u(E0.$$.fragment,c),u(y0.$$.fragment,c),u(w0.$$.fragment,c),u(L0.$$.fragment,c),u(B0.$$.fragment,c),u(x0.$$.fragment,c),u(k0.$$.fragment,c),u(R0.$$.fragment,c),u(S0.$$.fragment,c),u($0.$$.fragment,c),u(I0.$$.fragment,c),u(j0.$$.fragment,c),u(N0.$$.fragment,c),u(D0.$$.fragment,c),u(q0.$$.fragment,c),u(G0.$$.fragment,c),u(X0.$$.fragment,c),u(V0.$$.fragment,c),u(z0.$$.fragment,c),u(W0.$$.fragment,c),u(Q0.$$.fragment,c),u(U0.$$.fragment,c),u(J0.$$.fragment,c),u(Y0.$$.fragment,c),u(K0.$$.fragment,c),u(Z0.$$.fragment,c),u(eL.$$.fragment,c),u(rL.$$.fragment,c),u(tL.$$.fragment,c),u(aL.$$.fragment,c),u(sL.$$.fragment,c),u(nL.$$.fragment,c),u(lL.$$.fragment,c),u(dL.$$.fragment,c),u(cL.$$.fragment,c),u(mL.$$.fragment,c),u(fL.$$.fragment,c),u(gL.$$.fragment,c),u(hL.$$.fragment,c),u(pL.$$.fragment,c),u(_L.$$.fragment,c),u(bL.$$.fragment,c),u(vL.$$.fragment,c),u(TL.$$.fragment,c),u(FL.$$.fragment,c),u(ML.$$.fragment,c),u(EL.$$.fragment,c),u(yL.$$.fragment,c),u(wL.$$.fragment,c),u(AL.$$.fragment,c),u(LL.$$.fragment,c),u(xL.$$.fragment,c),u(kL.$$.fragment,c),u(RL.$$.fragment,c),u(SL.$$.fragment,c),u(PL.$$.fragment,c),u($L.$$.fragment,c),u(jL.$$.fragment,c),u(NL.$$.fragment,c),u(DL.$$.fragment,c),u(qL.$$.fragment,c),u(OL.$$.fragment,c),u(GL.$$.fragment,c),u(VL.$$.fragment,c),u(zL.$$.fragment,c),u(WL.$$.fragment,c),u(QL.$$.fragment,c),u(HL.$$.fragment,c),u(UL.$$.fragment,c),u(YL.$$.fragment,c),u(KL.$$.fragment,c),u(ZL.$$.fragment,c),u(e7.$$.fragment,c),u(o7.$$.fragment,c),u(r7.$$.fragment,c),u(a7.$$.fragment,c),u(s7.$$.fragment,c),u(n7.$$.fragment,c),u(l7.$$.fragment,c),u(i7.$$.fragment,c),u(d7.$$.fragment,c),u(m7.$$.fragment,c),u(f7.$$.fragment,c),u(g7.$$.fragment,c),u(h7.$$.fragment,c),u(u7.$$.fragment,c),u(p7.$$.fragment,c),u(b7.$$.fragment,c),u(v7.$$.fragment,c),u(T7.$$.fragment,c),u(F7.$$.fragment,c),u(C7.$$.fragment,c),u(M7.$$.fragment,c),u(y7.$$.fragment,c),u(w7.$$.fragment,c),u(A7.$$.fragment,c),u(L7.$$.fragment,c),u(B7.$$.fragment,c),u(x7.$$.fragment,c),u(R7.$$.fragment,c),u(S7.$$.fragment,c),u(P7.$$.fragment,c),u($7.$$.fragment,c),u(I7.$$.fragment,c),u(j7.$$.fragment,c),u(D7.$$.fragment,c),u(q7.$$.fragment,c),u(O7.$$.fragment,c),u(X7.$$.fragment,c),u(V7.$$.fragment,c),u(z7.$$.fragment,c),u(Q7.$$.fragment,c),u(H7.$$.fragment,c),u(U7.$$.fragment,c),u(J7.$$.fragment,c),DRe=!1},d(c){t(J),c&&t($e),c&&t(de),p(me),c&&t($m),c&&t(ma),c&&t(Be),c&&t(co),c&&t(jm),p(qa,c),c&&t(mo),c&&t(pe),c&&t(zo),c&&t(Oa),c&&t(Nxe),c&&t(Vi),p(sy),c&&t(Dxe),c&&t(Vs),c&&t(qxe),p(ny,c),c&&t(Oxe),c&&t(Z8),c&&t(Gxe),p(qm,c),c&&t(Xxe),c&&t(zi),p(ly),c&&t(Vxe),c&&t(Wo),p(iy),p(my),p(fy),p(gy),c&&t(zxe),c&&t(Qi),p(hy),c&&t(Wxe),c&&t(Qo),p(uy),p(by),p(vy),p(Ty),c&&t(Qxe),c&&t(Hi),p(Fy),c&&t(Hxe),c&&t(Ho),p(Cy),p(yy),p(Lh),p(wy),p(Ay),c&&t(Uxe),c&&t(Ui),p(Ly),c&&t(Jxe),c&&t(Uo),p(By),p(Ry),p(Dh),p(Sy),p(Py),c&&t(Yxe),c&&t(Yi),p($y),c&&t(Kxe),c&&t(Jo),p(Iy),p(Ny),p(Dy),p(qy),p(Oy),c&&t(Zxe),c&&t(ed),p(Gy),c&&t(eke),c&&t(Yo),p(Xy),p(zy),p(Wy),p(Qy),p(Hy),c&&t(oke),c&&t(td),p(Uy),c&&t(rke),c&&t(Ko),p(Jy),p(Ky),p(Zy),p(ew),p(ow),c&&t(tke),c&&t(nd),p(rw),c&&t(ake),c&&t(Zo),p(tw),p(sw),p(nw),p(lw),p(iw),c&&t(ske),c&&t(dd),p(dw),c&&t(nke),c&&t(er),p(cw),p(fw),p(gw),p(hw),p(uw),c&&t(lke),c&&t(fd),p(pw),c&&t(ike),c&&t(or),p(_w),p(vw),p(Tw),p(Fw),p(Cw),c&&t(dke),c&&t(ud),p(Mw),c&&t(cke),c&&t(rr),p(Ew),p(ww),p(Aw),p(Lw),p(Bw),c&&t(mke),c&&t(bd),p(xw),c&&t(fke),c&&t(tr),p(kw),p(Sw),p(Pw),p($w),p(Iw),c&&t(gke),c&&t(Fd),p(jw),c&&t(hke),c&&t(ar),p(Nw),p(qw),p(Ow),p(Gw),p(Xw),c&&t(uke),c&&t(Ed),p(Vw),c&&t(pke),c&&t(sr),p(zw),p(Qw),p(Hw),p(Uw),p(Jw),c&&t(_ke),c&&t(Ad),p(Yw),c&&t(bke),c&&t(nr),p(Kw),p(e6),p(o6),p(r6),p(t6),c&&t(vke),c&&t(xd),p(a6),c&&t(Tke),c&&t(lr),p(s6),p(l6),p(i6),p(d6),p(c6),c&&t(Fke),c&&t(Sd),p(m6),c&&t(Cke),c&&t(ir),p(f6),p(h6),p(u6),p(p6),p(_6),c&&t(Mke),c&&t(Id),p(b6),c&&t(Eke),c&&t(dr),p(v6),p(F6),p(C6),p(M6),p(E6),c&&t(yke),c&&t(Dd),p(y6),c&&t(wke),c&&t(cr),p(w6),p(L6),p(B6),p(x6),p(k6),c&&t(Ake),c&&t(Gd),p(R6),c&&t(Lke),c&&t(mr),p(S6),p($6),p(I6),p(j6),p(N6),c&&t(Bke),c&&t(zd),p(D6),c&&t(xke),c&&t(fr),p(q6),p(G6),p(X6),p(V6),p(W6),c&&t(kke),c&&t(Hd),p(Q6),c&&t(Rke),c&&t(gr),p(H6),p(J6),p(Y6),p(K6),p(Z6),c&&t(Ske),c&&t(Yd),p(eA),c&&t(Pke),c&&t(hr),p(oA),p(tA),p(aA),p(sA),p(nA),c&&t($ke),c&&t(oc),p(lA),c&&t(Ike),c&&t(ur),p(iA),p(cA),p(mA),p(fA),p(gA),c&&t(jke),c&&t(ac),p(hA),c&&t(Nke),c&&t(pr),p(uA),p(_A),p(bA),p(vA),p(TA),c&&t(Dke),c&&t(lc),p(FA),c&&t(qke),c&&t(_r),p(CA),p(EA),p(yA),p(wA),p(LA),c&&t(Oke),c&&t(cc),p(BA),c&&t(Gke),c&&t(br),p(xA),p(RA),p(SA),p(PA),p($A),c&&t(Xke),c&&t(gc),p(IA),c&&t(Vke),c&&t(vr),p(jA),p(DA),p(qA),p(OA),p(GA),c&&t(zke),c&&t(pc),p(XA),c&&t(Wke),c&&t(Tr),p(VA),p(WA),p(QA),p(HA),p(UA),c&&t(Qke),c&&t(vc),p(JA),c&&t(Hke),c&&t(Fr),p(YA),p(ZA),p(e0),p(o0),p(r0),c&&t(Uke),c&&t(Cc),p(t0),c&&t(Jke),c&&t(Cr),p(a0),p(n0),p(l0),p(i0),p(c0),c&&t(Yke),c&&t(yc),p(m0),c&&t(Kke),c&&t(Mr),p(f0),p(h0),p(u0),p(p0),p(_0),c&&t(Zke),c&&t(Lc),p(b0),c&&t(eRe),c&&t(Er),p(v0),p(F0),p(C0),p(M0),p(E0),c&&t(oRe),c&&t(kc),p(y0),c&&t(rRe),c&&t(yr),p(w0),p(L0),p(B0),p(x0),p(k0),c&&t(tRe),c&&t(Pc),p(R0),c&&t(aRe),c&&t(wr),p(S0),p($0),p(I0),p(j0),p(N0),c&&t(sRe),c&&t(jc),p(D0),c&&t(nRe),c&&t(Ar),p(q0),p(G0),p(X0),p(V0),p(z0),c&&t(lRe),c&&t(qc),p(W0),c&&t(iRe),c&&t(Lr),p(Q0),p(U0),p(J0),p(Y0),p(K0),c&&t(dRe),c&&t(Xc),p(Z0),c&&t(cRe),c&&t(Br),p(eL),p(rL),p(tL),p(aL),p(sL),c&&t(mRe),c&&t(Wc),p(nL),c&&t(fRe),c&&t(xr),p(lL),p(dL),p(cL),p(mL),p(fL),c&&t(gRe),c&&t(Uc),p(gL),c&&t(hRe),c&&t(kr),p(hL),p(pL),p(_L),p(bL),p(vL),c&&t(uRe),c&&t(Kc),p(TL),c&&t(pRe),c&&t(Rr),p(FL),p(ML),p(EL),p(yL),p(wL),c&&t(_Re),c&&t(om),p(AL),c&&t(bRe),c&&t(Sr),p(LL),p(xL),p(kL),p(RL),p(SL),c&&t(vRe),c&&t(am),p(PL),c&&t(TRe),c&&t(Pr),p($L),p(jL),p(NL),p(DL),p(qL),c&&t(FRe),c&&t(lm),p(OL),c&&t(CRe),c&&t($r),p(GL),p(VL),p(zL),p(WL),p(QL),c&&t(MRe),c&&t(cm),p(HL),c&&t(ERe),c&&t(Ir),p(UL),p(YL),p(KL),p(ZL),p(e7),c&&t(yRe),c&&t(gm),p(o7),c&&t(wRe),c&&t(jr),p(r7),p(a7),p(s7),p(n7),p(l7),c&&t(ARe),c&&t(pm),p(i7),c&&t(LRe),c&&t(Nr),p(d7),p(m7),p(f7),p(g7),p(h7),c&&t(BRe),c&&t(vm),p(u7),c&&t(xRe),c&&t(Dr),p(p7),p(b7),p(v7),p(T7),p(F7),c&&t(kRe),c&&t(Cm),p(C7),c&&t(RRe),c&&t(qr),p(M7),p(y7),p(w7),p(A7),p(L7),c&&t(SRe),c&&t(ym),p(B7),c&&t(PRe),c&&t(Or),p(x7),p(R7),p(S7),p(P7),p($7),c&&t($Re),c&&t(Lm),p(I7),c&&t(IRe),c&&t(Gr),p(j7),p(D7),p(q7),p(O7),p(X7),c&&t(jRe),c&&t(km),p(V7),c&&t(NRe),c&&t(Xr),p(z7),p(Q7),p(H7),p(U7),p(J7)}}}const M4t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.AutoModelForInstanceSegmentation",title:"AutoModelForInstanceSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function E4t(Ii,J,$e){let{fw:de}=J;return Ii.$$set=ue=>{"fw"in ue&&$e(0,de=ue.fw)},[de]}class x4t extends u4t{constructor(J){super();p4t(this,J,E4t,C4t,_4t,{fw:0})}}export{x4t as default,M4t as metadata};
