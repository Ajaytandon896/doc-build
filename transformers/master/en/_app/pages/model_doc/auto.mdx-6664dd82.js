import{S as V1r,i as z1r,s as W1r,e as a,k as l,w as m,t as o,M as Q1r,c as s,d as r,m as i,a as n,x as f,h as t,b as c,F as e,g as b,y as g,q as h,o as u,B as p}from"../../chunks/vendor-4833417e.js";import{T as P7t}from"../../chunks/Tip-fffd6df1.js";import{D as M}from"../../chunks/Docstring-7b52c3d4.js";import{C as w}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as X}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function H1r($i){let J,Be,de,fe,no,ce,_e,Go,Ii,Pm,ca,ji,Di,D5,$m,ye,co,Ni,qs,N5,Os,Gs,q5,qi,Xs,O5,Oi,Im,Na;return{c(){J=a("p"),Be=o("If your "),de=a("code"),fe=o("NewModelConfig"),no=o(" is a subclass of "),ce=a("code"),_e=o("PretrainedConfig"),Go=o(`, make sure its
`),Ii=a("code"),Pm=o("model_type"),ca=o(" attribute is set to the same key you use when registering the config (here "),ji=a("code"),Di=o('"new-model"'),D5=o(")."),$m=l(),ye=a("p"),co=o("Likewise, if your "),Ni=a("code"),qs=o("NewModel"),N5=o(" is a subclass of "),Os=a("a"),Gs=o("PreTrainedModel"),q5=o(`, make sure its
`),qi=a("code"),Xs=o("config_class"),O5=o(` attribute is set to the same class you use when registering the model (here
`),Oi=a("code"),Im=o("NewModelConfig"),Na=o(")."),this.h()},l(mo){J=s(mo,"P",{});var ge=n(J);Be=t(ge,"If your "),de=s(ge,"CODE",{});var k7=n(de);fe=t(k7,"NewModelConfig"),k7.forEach(r),no=t(ge," is a subclass of "),ce=s(ge,"CODE",{});var Gi=n(ce);_e=t(Gi,"PretrainedConfig"),Gi.forEach(r),Go=t(ge,`, make sure its
`),Ii=s(ge,"CODE",{});var R7=n(Ii);Pm=t(R7,"model_type"),R7.forEach(r),ca=t(ge," attribute is set to the same key you use when registering the config (here "),ji=s(ge,"CODE",{});var S7=n(ji);Di=t(S7,'"new-model"'),S7.forEach(r),D5=t(ge,")."),ge.forEach(r),$m=i(mo),ye=s(mo,"P",{});var Xo=n(ye);co=t(Xo,"Likewise, if your "),Ni=s(Xo,"CODE",{});var qa=n(Ni);qs=t(qa,"NewModel"),qa.forEach(r),N5=t(Xo," is a subclass of "),Os=s(Xo,"A",{href:!0});var P7=n(Os);Gs=t(P7,"PreTrainedModel"),P7.forEach(r),q5=t(Xo,`, make sure its
`),qi=s(Xo,"CODE",{});var jm=n(qi);Xs=t(jm,"config_class"),jm.forEach(r),O5=t(Xo,` attribute is set to the same class you use when registering the model (here
`),Oi=s(Xo,"CODE",{});var $7=n(Oi);Im=t($7,"NewModelConfig"),$7.forEach(r),Na=t(Xo,")."),Xo.forEach(r),this.h()},h(){c(Os,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(mo,ge){b(mo,J,ge),e(J,Be),e(J,de),e(de,fe),e(J,no),e(J,ce),e(ce,_e),e(J,Go),e(J,Ii),e(Ii,Pm),e(J,ca),e(J,ji),e(ji,Di),e(J,D5),b(mo,$m,ge),b(mo,ye,ge),e(ye,co),e(ye,Ni),e(Ni,qs),e(ye,N5),e(ye,Os),e(Os,Gs),e(ye,q5),e(ye,qi),e(qi,Xs),e(ye,O5),e(ye,Oi),e(Oi,Im),e(ye,Na)},d(mo){mo&&r(J),mo&&r($m),mo&&r(ye)}}}function U1r($i){let J,Be,de,fe,no;return{c(){J=a("p"),Be=o("Passing "),de=a("code"),fe=o("use_auth_token=True"),no=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Be=t(_e,"Passing "),de=s(_e,"CODE",{});var Go=n(de);fe=t(Go,"use_auth_token=True"),Go.forEach(r),no=t(_e," is required when you want to use a private model."),_e.forEach(r)},m(ce,_e){b(ce,J,_e),e(J,Be),e(J,de),e(de,fe),e(J,no)},d(ce){ce&&r(J)}}}function J1r($i){let J,Be,de,fe,no;return{c(){J=a("p"),Be=o("Passing "),de=a("code"),fe=o("use_auth_token=True"),no=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Be=t(_e,"Passing "),de=s(_e,"CODE",{});var Go=n(de);fe=t(Go,"use_auth_token=True"),Go.forEach(r),no=t(_e," is required when you want to use a private model."),_e.forEach(r)},m(ce,_e){b(ce,J,_e),e(J,Be),e(J,de),e(de,fe),e(J,no)},d(ce){ce&&r(J)}}}function Y1r($i){let J,Be,de,fe,no,ce,_e,Go,Ii,Pm,ca,ji,Di,D5,$m,ye,co,Ni,qs,N5,Os,Gs,q5,qi,Xs,O5,Oi,Im,Na,mo,ge,k7,Gi,R7,S7,Xo,qa,P7,jm,$7,DSe,EBe,Xi,Dm,JW,G5,NSe,YW,qSe,yBe,Vs,OSe,KW,GSe,XSe,ZW,VSe,zSe,wBe,X5,ABe,I7,WSe,LBe,Nm,BBe,Vi,qm,eQ,V5,QSe,oQ,HSe,xBe,Vo,z5,USe,W5,JSe,j7,YSe,KSe,ZSe,Q5,ePe,tQ,oPe,tPe,rPe,fo,H5,aPe,rQ,sPe,nPe,zi,lPe,aQ,iPe,dPe,sQ,cPe,mPe,fPe,v,Om,nQ,gPe,hPe,D7,uPe,pPe,_Pe,Gm,lQ,bPe,vPe,N7,TPe,FPe,CPe,Xm,iQ,MPe,EPe,q7,yPe,wPe,APe,Vm,dQ,LPe,BPe,O7,xPe,kPe,RPe,zm,cQ,SPe,PPe,G7,$Pe,IPe,jPe,Wm,mQ,DPe,NPe,X7,qPe,OPe,GPe,Qm,fQ,XPe,VPe,V7,zPe,WPe,QPe,Hm,gQ,HPe,UPe,z7,JPe,YPe,KPe,Um,hQ,ZPe,e$e,W7,o$e,t$e,r$e,Jm,uQ,a$e,s$e,Q7,n$e,l$e,i$e,Ym,pQ,d$e,c$e,H7,m$e,f$e,g$e,Km,_Q,h$e,u$e,U7,p$e,_$e,b$e,Zm,bQ,v$e,T$e,J7,F$e,C$e,M$e,ef,vQ,E$e,y$e,Y7,w$e,A$e,L$e,of,TQ,B$e,x$e,K7,k$e,R$e,S$e,tf,FQ,P$e,$$e,Z7,I$e,j$e,D$e,rf,CQ,N$e,q$e,e9,O$e,G$e,X$e,af,MQ,V$e,z$e,o9,W$e,Q$e,H$e,sf,EQ,U$e,J$e,t9,Y$e,K$e,Z$e,nf,yQ,eIe,oIe,r9,tIe,rIe,aIe,lf,wQ,sIe,nIe,a9,lIe,iIe,dIe,df,AQ,cIe,mIe,s9,fIe,gIe,hIe,cf,LQ,uIe,pIe,n9,_Ie,bIe,vIe,mf,BQ,TIe,FIe,l9,CIe,MIe,EIe,ff,xQ,yIe,wIe,i9,AIe,LIe,BIe,gf,kQ,xIe,kIe,d9,RIe,SIe,PIe,hf,RQ,$Ie,IIe,c9,jIe,DIe,NIe,uf,SQ,qIe,OIe,m9,GIe,XIe,VIe,pf,PQ,zIe,WIe,f9,QIe,HIe,UIe,_f,$Q,JIe,YIe,g9,KIe,ZIe,eje,bf,IQ,oje,tje,h9,rje,aje,sje,vf,jQ,nje,lje,u9,ije,dje,cje,Tf,DQ,mje,fje,p9,gje,hje,uje,Ff,NQ,pje,_je,_9,bje,vje,Tje,Cf,qQ,Fje,Cje,b9,Mje,Eje,yje,Mf,OQ,wje,Aje,v9,Lje,Bje,xje,Ef,GQ,kje,Rje,T9,Sje,Pje,$je,yf,XQ,Ije,jje,F9,Dje,Nje,qje,wf,VQ,Oje,Gje,C9,Xje,Vje,zje,Af,zQ,Wje,Qje,M9,Hje,Uje,Jje,Lf,WQ,Yje,Kje,E9,Zje,eDe,oDe,Bf,QQ,tDe,rDe,y9,aDe,sDe,nDe,xf,HQ,lDe,iDe,w9,dDe,cDe,mDe,kf,UQ,fDe,gDe,A9,hDe,uDe,pDe,Rf,JQ,_De,bDe,L9,vDe,TDe,FDe,Sf,YQ,CDe,MDe,B9,EDe,yDe,wDe,Pf,KQ,ADe,LDe,x9,BDe,xDe,kDe,$f,ZQ,RDe,SDe,k9,PDe,$De,IDe,If,eH,jDe,DDe,R9,NDe,qDe,ODe,jf,oH,GDe,XDe,S9,VDe,zDe,WDe,Df,tH,QDe,HDe,P9,UDe,JDe,YDe,Nf,rH,KDe,ZDe,$9,eNe,oNe,tNe,qf,aH,rNe,aNe,I9,sNe,nNe,lNe,Of,sH,iNe,dNe,j9,cNe,mNe,fNe,Gf,nH,gNe,hNe,D9,uNe,pNe,_Ne,Xf,lH,bNe,vNe,N9,TNe,FNe,CNe,Vf,iH,MNe,ENe,q9,yNe,wNe,ANe,zf,dH,LNe,BNe,O9,xNe,kNe,RNe,Wf,cH,SNe,PNe,G9,$Ne,INe,jNe,Qf,mH,DNe,NNe,X9,qNe,ONe,GNe,Hf,fH,XNe,VNe,V9,zNe,WNe,QNe,Uf,gH,HNe,UNe,z9,JNe,YNe,KNe,Jf,hH,ZNe,eqe,W9,oqe,tqe,rqe,Yf,uH,aqe,sqe,Q9,nqe,lqe,iqe,Kf,pH,dqe,cqe,H9,mqe,fqe,gqe,Zf,_H,hqe,uqe,U9,pqe,_qe,bqe,eg,bH,vqe,Tqe,J9,Fqe,Cqe,Mqe,og,vH,Eqe,yqe,Y9,wqe,Aqe,Lqe,tg,TH,Bqe,xqe,K9,kqe,Rqe,Sqe,rg,FH,Pqe,$qe,Z9,Iqe,jqe,Dqe,ag,CH,Nqe,qqe,eB,Oqe,Gqe,Xqe,sg,MH,Vqe,zqe,oB,Wqe,Qqe,Hqe,ng,EH,Uqe,Jqe,tB,Yqe,Kqe,Zqe,lg,yH,eOe,oOe,rB,tOe,rOe,aOe,ig,wH,sOe,nOe,aB,lOe,iOe,dOe,dg,AH,cOe,mOe,sB,fOe,gOe,hOe,cg,LH,uOe,pOe,nB,_Oe,bOe,vOe,mg,BH,TOe,FOe,lB,COe,MOe,EOe,fg,xH,yOe,wOe,iB,AOe,LOe,BOe,gg,kH,xOe,kOe,dB,ROe,SOe,POe,hg,RH,$Oe,IOe,cB,jOe,DOe,NOe,ug,SH,qOe,OOe,mB,GOe,XOe,VOe,pg,PH,zOe,WOe,fB,QOe,HOe,UOe,_g,$H,JOe,YOe,gB,KOe,ZOe,eGe,bg,IH,oGe,tGe,hB,rGe,aGe,sGe,vg,jH,nGe,lGe,uB,iGe,dGe,cGe,Tg,DH,mGe,fGe,pB,gGe,hGe,uGe,Fg,NH,pGe,_Ge,_B,bGe,vGe,TGe,Cg,qH,FGe,CGe,bB,MGe,EGe,yGe,Mg,OH,wGe,AGe,vB,LGe,BGe,xGe,Eg,GH,kGe,RGe,TB,SGe,PGe,$Ge,yg,XH,IGe,jGe,FB,DGe,NGe,qGe,wg,VH,OGe,GGe,CB,XGe,VGe,zGe,Ag,zH,WGe,QGe,MB,HGe,UGe,JGe,WH,YGe,KGe,U5,ZGe,Lg,J5,eXe,QH,oXe,kBe,Wi,Bg,HH,Y5,tXe,UH,rXe,RBe,zo,K5,aXe,Z5,sXe,EB,nXe,lXe,iXe,ey,dXe,JH,cXe,mXe,fXe,go,oy,gXe,YH,hXe,uXe,Oa,pXe,KH,_Xe,bXe,ZH,vXe,TXe,eU,FXe,CXe,MXe,E,zs,oU,EXe,yXe,yB,wXe,AXe,wB,LXe,BXe,xXe,Ws,tU,kXe,RXe,AB,SXe,PXe,LB,$Xe,IXe,jXe,Qs,rU,DXe,NXe,BB,qXe,OXe,xB,GXe,XXe,VXe,xg,aU,zXe,WXe,kB,QXe,HXe,UXe,Hs,sU,JXe,YXe,RB,KXe,ZXe,SB,eVe,oVe,tVe,kg,nU,rVe,aVe,PB,sVe,nVe,lVe,Rg,lU,iVe,dVe,$B,cVe,mVe,fVe,Sg,iU,gVe,hVe,IB,uVe,pVe,_Ve,Us,dU,bVe,vVe,jB,TVe,FVe,DB,CVe,MVe,EVe,Js,cU,yVe,wVe,NB,AVe,LVe,qB,BVe,xVe,kVe,Ys,mU,RVe,SVe,OB,PVe,$Ve,GB,IVe,jVe,DVe,Pg,fU,NVe,qVe,XB,OVe,GVe,XVe,$g,gU,VVe,zVe,VB,WVe,QVe,HVe,Ks,hU,UVe,JVe,zB,YVe,KVe,WB,ZVe,eze,oze,Ig,uU,tze,rze,QB,aze,sze,nze,Zs,pU,lze,ize,HB,dze,cze,UB,mze,fze,gze,en,_U,hze,uze,JB,pze,_ze,YB,bze,vze,Tze,on,bU,Fze,Cze,KB,Mze,Eze,vU,yze,wze,Aze,jg,TU,Lze,Bze,ZB,xze,kze,Rze,tn,FU,Sze,Pze,ex,$ze,Ize,ox,jze,Dze,Nze,Dg,CU,qze,Oze,tx,Gze,Xze,Vze,rn,MU,zze,Wze,rx,Qze,Hze,ax,Uze,Jze,Yze,an,EU,Kze,Zze,sx,eWe,oWe,nx,tWe,rWe,aWe,sn,yU,sWe,nWe,lx,lWe,iWe,ix,dWe,cWe,mWe,Ng,wU,fWe,gWe,dx,hWe,uWe,pWe,nn,AU,_We,bWe,cx,vWe,TWe,mx,FWe,CWe,MWe,qg,LU,EWe,yWe,fx,wWe,AWe,LWe,ln,BU,BWe,xWe,gx,kWe,RWe,hx,SWe,PWe,$We,dn,xU,IWe,jWe,ux,DWe,NWe,px,qWe,OWe,GWe,cn,kU,XWe,VWe,_x,zWe,WWe,bx,QWe,HWe,UWe,mn,RU,JWe,YWe,vx,KWe,ZWe,Tx,eQe,oQe,tQe,Og,SU,rQe,aQe,Fx,sQe,nQe,lQe,fn,PU,iQe,dQe,Cx,cQe,mQe,Mx,fQe,gQe,hQe,gn,$U,uQe,pQe,Ex,_Qe,bQe,yx,vQe,TQe,FQe,hn,IU,CQe,MQe,wx,EQe,yQe,Ax,wQe,AQe,LQe,un,jU,BQe,xQe,Lx,kQe,RQe,Bx,SQe,PQe,$Qe,pn,DU,IQe,jQe,xx,DQe,NQe,kx,qQe,OQe,GQe,_n,NU,XQe,VQe,Rx,zQe,WQe,Sx,QQe,HQe,UQe,Gg,qU,JQe,YQe,Px,KQe,ZQe,eHe,bn,OU,oHe,tHe,$x,rHe,aHe,Ix,sHe,nHe,lHe,Xg,GU,iHe,dHe,jx,cHe,mHe,fHe,Vg,XU,gHe,hHe,Dx,uHe,pHe,_He,vn,VU,bHe,vHe,Nx,THe,FHe,qx,CHe,MHe,EHe,Tn,zU,yHe,wHe,Ox,AHe,LHe,Gx,BHe,xHe,kHe,zg,WU,RHe,SHe,Xx,PHe,$He,IHe,Fn,QU,jHe,DHe,Vx,NHe,qHe,zx,OHe,GHe,XHe,Cn,HU,VHe,zHe,Wx,WHe,QHe,Qx,HHe,UHe,JHe,Mn,UU,YHe,KHe,Hx,ZHe,eUe,Ux,oUe,tUe,rUe,En,JU,aUe,sUe,Jx,nUe,lUe,Yx,iUe,dUe,cUe,yn,YU,mUe,fUe,Kx,gUe,hUe,Zx,uUe,pUe,_Ue,Wg,KU,bUe,vUe,ek,TUe,FUe,CUe,Qg,ZU,MUe,EUe,ok,yUe,wUe,AUe,Hg,eJ,LUe,BUe,tk,xUe,kUe,RUe,Ug,oJ,SUe,PUe,rk,$Ue,IUe,jUe,wn,tJ,DUe,NUe,ak,qUe,OUe,sk,GUe,XUe,VUe,Jg,rJ,zUe,WUe,nk,QUe,HUe,UUe,An,aJ,JUe,YUe,lk,KUe,ZUe,ik,eJe,oJe,tJe,Ln,sJ,rJe,aJe,dk,sJe,nJe,ck,lJe,iJe,dJe,Bn,nJ,cJe,mJe,mk,fJe,gJe,fk,hJe,uJe,pJe,xn,lJ,_Je,bJe,gk,vJe,TJe,hk,FJe,CJe,MJe,kn,iJ,EJe,yJe,uk,wJe,AJe,pk,LJe,BJe,xJe,Rn,dJ,kJe,RJe,_k,SJe,PJe,bk,$Je,IJe,jJe,Yg,cJ,DJe,NJe,vk,qJe,OJe,GJe,Kg,mJ,XJe,VJe,Tk,zJe,WJe,QJe,Sn,fJ,HJe,UJe,Fk,JJe,YJe,Ck,KJe,ZJe,eYe,Pn,gJ,oYe,tYe,Mk,rYe,aYe,Ek,sYe,nYe,lYe,$n,hJ,iYe,dYe,yk,cYe,mYe,wk,fYe,gYe,hYe,Zg,uJ,uYe,pYe,Ak,_Ye,bYe,vYe,eh,pJ,TYe,FYe,Lk,CYe,MYe,EYe,oh,_J,yYe,wYe,Bk,AYe,LYe,BYe,th,bJ,xYe,kYe,xk,RYe,SYe,PYe,In,vJ,$Ye,IYe,kk,jYe,DYe,Rk,NYe,qYe,OYe,rh,TJ,GYe,XYe,Sk,VYe,zYe,WYe,ah,FJ,QYe,HYe,Pk,UYe,JYe,YYe,jn,CJ,KYe,ZYe,$k,eKe,oKe,Ik,tKe,rKe,aKe,Dn,MJ,sKe,nKe,jk,lKe,iKe,Dk,dKe,cKe,mKe,EJ,fKe,gKe,ty,hKe,sh,ry,uKe,yJ,pKe,SBe,Qi,nh,wJ,ay,_Ke,AJ,bKe,PBe,Wo,sy,vKe,ny,TKe,Nk,FKe,CKe,MKe,ly,EKe,LJ,yKe,wKe,AKe,xe,iy,LKe,BJ,BKe,xKe,Ga,kKe,xJ,RKe,SKe,kJ,PKe,$Ke,RJ,IKe,jKe,DKe,ae,lh,SJ,NKe,qKe,qk,OKe,GKe,XKe,ih,PJ,VKe,zKe,Ok,WKe,QKe,HKe,dh,$J,UKe,JKe,Gk,YKe,KKe,ZKe,ch,IJ,eZe,oZe,Xk,tZe,rZe,aZe,mh,jJ,sZe,nZe,Vk,lZe,iZe,dZe,fh,DJ,cZe,mZe,zk,fZe,gZe,hZe,gh,NJ,uZe,pZe,Wk,_Ze,bZe,vZe,hh,qJ,TZe,FZe,Qk,CZe,MZe,EZe,uh,OJ,yZe,wZe,Hk,AZe,LZe,BZe,ph,GJ,xZe,kZe,Uk,RZe,SZe,PZe,_h,XJ,$Ze,IZe,Jk,jZe,DZe,NZe,bh,VJ,qZe,OZe,Yk,GZe,XZe,VZe,vh,zJ,zZe,WZe,Kk,QZe,HZe,UZe,Th,WJ,JZe,YZe,Zk,KZe,ZZe,eeo,Fh,QJ,oeo,teo,eR,reo,aeo,seo,Ch,HJ,neo,leo,oR,ieo,deo,ceo,Mh,meo,UJ,feo,geo,dy,heo,Eh,cy,ueo,JJ,peo,$Be,Hi,yh,YJ,my,_eo,KJ,beo,IBe,Qo,fy,veo,gy,Teo,tR,Feo,Ceo,Meo,hy,Eeo,ZJ,yeo,weo,Aeo,ke,uy,Leo,eY,Beo,xeo,Ui,keo,oY,Reo,Seo,tY,Peo,$eo,Ieo,we,wh,rY,jeo,Deo,rR,Neo,qeo,Oeo,Ah,aY,Geo,Xeo,aR,Veo,zeo,Weo,Lh,sY,Qeo,Heo,sR,Ueo,Jeo,Yeo,Bh,nY,Keo,Zeo,nR,eoo,ooo,too,xh,lY,roo,aoo,lR,soo,noo,loo,kh,iY,ioo,doo,iR,coo,moo,foo,Rh,dY,goo,hoo,dR,uoo,poo,_oo,Sh,cY,boo,voo,cR,Too,Foo,Coo,Ph,Moo,mY,Eoo,yoo,py,woo,$h,_y,Aoo,fY,Loo,jBe,Ji,Ih,gY,by,Boo,hY,xoo,DBe,Ho,vy,koo,Yi,Roo,uY,Soo,Poo,pY,$oo,Ioo,joo,Ty,Doo,_Y,Noo,qoo,Ooo,Gt,Fy,Goo,bY,Xoo,Voo,Ki,zoo,vY,Woo,Qoo,TY,Hoo,Uoo,Joo,FY,Yoo,Koo,Cy,Zoo,Re,My,eto,CY,oto,tto,Xa,rto,MY,ato,sto,EY,nto,lto,yY,ito,dto,cto,F,jh,wY,mto,fto,mR,gto,hto,uto,Dh,AY,pto,_to,fR,bto,vto,Tto,Nh,LY,Fto,Cto,gR,Mto,Eto,yto,qh,BY,wto,Ato,hR,Lto,Bto,xto,Oh,xY,kto,Rto,uR,Sto,Pto,$to,Gh,kY,Ito,jto,pR,Dto,Nto,qto,Xh,RY,Oto,Gto,_R,Xto,Vto,zto,Vh,SY,Wto,Qto,bR,Hto,Uto,Jto,zh,PY,Yto,Kto,vR,Zto,ero,oro,Wh,$Y,tro,rro,TR,aro,sro,nro,Qh,IY,lro,iro,FR,dro,cro,mro,Hh,jY,fro,gro,CR,hro,uro,pro,Uh,DY,_ro,bro,MR,vro,Tro,Fro,Jh,NY,Cro,Mro,ER,Ero,yro,wro,Yh,qY,Aro,Lro,yR,Bro,xro,kro,Kh,OY,Rro,Sro,wR,Pro,$ro,Iro,Zh,GY,jro,Dro,AR,Nro,qro,Oro,eu,XY,Gro,Xro,LR,Vro,zro,Wro,ou,VY,Qro,Hro,BR,Uro,Jro,Yro,tu,zY,Kro,Zro,xR,eao,oao,tao,ru,WY,rao,aao,kR,sao,nao,lao,au,QY,iao,dao,RR,cao,mao,fao,su,HY,gao,hao,SR,uao,pao,_ao,nu,UY,bao,vao,PR,Tao,Fao,Cao,lu,JY,Mao,Eao,$R,yao,wao,Aao,iu,YY,Lao,Bao,IR,xao,kao,Rao,du,KY,Sao,Pao,jR,$ao,Iao,jao,Nn,ZY,Dao,Nao,DR,qao,Oao,NR,Gao,Xao,Vao,cu,eK,zao,Wao,qR,Qao,Hao,Uao,mu,oK,Jao,Yao,OR,Kao,Zao,eso,fu,tK,oso,tso,GR,rso,aso,sso,gu,rK,nso,lso,XR,iso,dso,cso,hu,aK,mso,fso,VR,gso,hso,uso,uu,sK,pso,_so,zR,bso,vso,Tso,pu,nK,Fso,Cso,WR,Mso,Eso,yso,_u,lK,wso,Aso,QR,Lso,Bso,xso,bu,iK,kso,Rso,HR,Sso,Pso,$so,vu,dK,Iso,jso,UR,Dso,Nso,qso,Tu,cK,Oso,Gso,JR,Xso,Vso,zso,Fu,mK,Wso,Qso,YR,Hso,Uso,Jso,Cu,fK,Yso,Kso,KR,Zso,eno,ono,Mu,gK,tno,rno,ZR,ano,sno,nno,Eu,hK,lno,ino,eS,dno,cno,mno,yu,uK,fno,gno,oS,hno,uno,pno,wu,pK,_no,bno,tS,vno,Tno,Fno,Au,_K,Cno,Mno,rS,Eno,yno,wno,Lu,bK,Ano,Lno,aS,Bno,xno,kno,Bu,vK,Rno,Sno,sS,Pno,$no,Ino,xu,TK,jno,Dno,nS,Nno,qno,Ono,ku,FK,Gno,Xno,lS,Vno,zno,Wno,Ru,CK,Qno,Hno,iS,Uno,Jno,Yno,Su,MK,Kno,Zno,dS,elo,olo,tlo,Pu,EK,rlo,alo,cS,slo,nlo,llo,$u,yK,ilo,dlo,mS,clo,mlo,flo,Iu,wK,glo,hlo,fS,ulo,plo,_lo,ju,AK,blo,vlo,gS,Tlo,Flo,Clo,Du,LK,Mlo,Elo,hS,ylo,wlo,Alo,Nu,BK,Llo,Blo,uS,xlo,klo,Rlo,qu,xK,Slo,Plo,pS,$lo,Ilo,jlo,Ou,kK,Dlo,Nlo,_S,qlo,Olo,Glo,Gu,RK,Xlo,Vlo,bS,zlo,Wlo,Qlo,Xu,SK,Hlo,Ulo,vS,Jlo,Ylo,Klo,Vu,PK,Zlo,eio,TS,oio,tio,rio,zu,$K,aio,sio,FS,nio,lio,iio,Wu,IK,dio,cio,CS,mio,fio,gio,Qu,jK,hio,uio,MS,pio,_io,bio,Hu,DK,vio,Tio,ES,Fio,Cio,Mio,Uu,NK,Eio,yio,yS,wio,Aio,Lio,Ju,qK,Bio,xio,wS,kio,Rio,Sio,Yu,OK,Pio,$io,AS,Iio,jio,Dio,Ku,GK,Nio,qio,LS,Oio,Gio,Xio,Zu,XK,Vio,zio,BS,Wio,Qio,Hio,ep,VK,Uio,Jio,xS,Yio,Kio,Zio,op,zK,edo,odo,kS,tdo,rdo,ado,tp,WK,sdo,ndo,RS,ldo,ido,ddo,rp,QK,cdo,mdo,SS,fdo,gdo,hdo,ap,HK,udo,pdo,PS,_do,bdo,vdo,sp,UK,Tdo,Fdo,$S,Cdo,Mdo,Edo,np,JK,ydo,wdo,IS,Ado,Ldo,Bdo,lp,YK,xdo,kdo,jS,Rdo,Sdo,Pdo,ip,KK,$do,Ido,DS,jdo,Ddo,Ndo,dp,ZK,qdo,Odo,NS,Gdo,Xdo,Vdo,cp,eZ,zdo,Wdo,qS,Qdo,Hdo,Udo,mp,oZ,Jdo,Ydo,OS,Kdo,Zdo,eco,fp,tZ,oco,tco,GS,rco,aco,sco,gp,rZ,nco,lco,XS,ico,dco,cco,hp,aZ,mco,fco,VS,gco,hco,uco,up,pco,sZ,_co,bco,nZ,vco,Tco,lZ,Fco,Cco,Ey,NBe,Zi,pp,iZ,yy,Mco,dZ,Eco,qBe,Uo,wy,yco,ed,wco,cZ,Aco,Lco,mZ,Bco,xco,kco,Ay,Rco,fZ,Sco,Pco,$co,Xt,Ly,Ico,gZ,jco,Dco,od,Nco,hZ,qco,Oco,uZ,Gco,Xco,Vco,pZ,zco,Wco,By,Qco,Se,xy,Hco,_Z,Uco,Jco,Va,Yco,bZ,Kco,Zco,vZ,emo,omo,TZ,tmo,rmo,amo,k,_p,FZ,smo,nmo,zS,lmo,imo,dmo,bp,CZ,cmo,mmo,WS,fmo,gmo,hmo,vp,MZ,umo,pmo,QS,_mo,bmo,vmo,Tp,EZ,Tmo,Fmo,HS,Cmo,Mmo,Emo,Fp,yZ,ymo,wmo,US,Amo,Lmo,Bmo,Cp,wZ,xmo,kmo,JS,Rmo,Smo,Pmo,Mp,AZ,$mo,Imo,YS,jmo,Dmo,Nmo,Ep,LZ,qmo,Omo,KS,Gmo,Xmo,Vmo,yp,BZ,zmo,Wmo,ZS,Qmo,Hmo,Umo,wp,xZ,Jmo,Ymo,eP,Kmo,Zmo,efo,Ap,kZ,ofo,tfo,oP,rfo,afo,sfo,Lp,RZ,nfo,lfo,tP,ifo,dfo,cfo,Bp,SZ,mfo,ffo,rP,gfo,hfo,ufo,xp,PZ,pfo,_fo,aP,bfo,vfo,Tfo,kp,$Z,Ffo,Cfo,sP,Mfo,Efo,yfo,Rp,IZ,wfo,Afo,nP,Lfo,Bfo,xfo,Sp,jZ,kfo,Rfo,lP,Sfo,Pfo,$fo,Pp,DZ,Ifo,jfo,iP,Dfo,Nfo,qfo,$p,NZ,Ofo,Gfo,dP,Xfo,Vfo,zfo,Ip,qZ,Wfo,Qfo,cP,Hfo,Ufo,Jfo,jp,OZ,Yfo,Kfo,mP,Zfo,ego,ogo,Dp,GZ,tgo,rgo,fP,ago,sgo,ngo,Np,XZ,lgo,igo,gP,dgo,cgo,mgo,qp,VZ,fgo,ggo,hP,hgo,ugo,pgo,Op,zZ,_go,bgo,uP,vgo,Tgo,Fgo,Gp,WZ,Cgo,Mgo,pP,Ego,ygo,wgo,Xp,QZ,Ago,Lgo,_P,Bgo,xgo,kgo,Vp,HZ,Rgo,Sgo,bP,Pgo,$go,Igo,zp,UZ,jgo,Dgo,vP,Ngo,qgo,Ogo,Wp,JZ,Ggo,Xgo,TP,Vgo,zgo,Wgo,Qp,YZ,Qgo,Hgo,FP,Ugo,Jgo,Ygo,Hp,KZ,Kgo,Zgo,CP,eho,oho,tho,Up,ZZ,rho,aho,MP,sho,nho,lho,Jp,eee,iho,dho,EP,cho,mho,fho,Yp,oee,gho,hho,yP,uho,pho,_ho,Kp,tee,bho,vho,wP,Tho,Fho,Cho,Zp,ree,Mho,Eho,AP,yho,who,Aho,e_,aee,Lho,Bho,LP,xho,kho,Rho,o_,see,Sho,Pho,BP,$ho,Iho,jho,t_,Dho,nee,Nho,qho,lee,Oho,Gho,iee,Xho,Vho,ky,OBe,td,r_,dee,Ry,zho,cee,Who,GBe,Jo,Sy,Qho,rd,Hho,mee,Uho,Jho,fee,Yho,Kho,Zho,Py,euo,gee,ouo,tuo,ruo,Vt,$y,auo,hee,suo,nuo,ad,luo,uee,iuo,duo,pee,cuo,muo,fuo,_ee,guo,huo,Iy,uuo,Pe,jy,puo,bee,_uo,buo,za,vuo,vee,Tuo,Fuo,Tee,Cuo,Muo,Fee,Euo,yuo,wuo,$,a_,Cee,Auo,Luo,xP,Buo,xuo,kuo,s_,Mee,Ruo,Suo,kP,Puo,$uo,Iuo,n_,Eee,juo,Duo,RP,Nuo,quo,Ouo,l_,yee,Guo,Xuo,SP,Vuo,zuo,Wuo,i_,wee,Quo,Huo,PP,Uuo,Juo,Yuo,d_,Aee,Kuo,Zuo,$P,epo,opo,tpo,c_,Lee,rpo,apo,IP,spo,npo,lpo,m_,Bee,ipo,dpo,jP,cpo,mpo,fpo,f_,xee,gpo,hpo,DP,upo,ppo,_po,g_,kee,bpo,vpo,NP,Tpo,Fpo,Cpo,h_,Ree,Mpo,Epo,qP,ypo,wpo,Apo,u_,See,Lpo,Bpo,OP,xpo,kpo,Rpo,p_,Pee,Spo,Ppo,GP,$po,Ipo,jpo,__,$ee,Dpo,Npo,XP,qpo,Opo,Gpo,b_,Iee,Xpo,Vpo,VP,zpo,Wpo,Qpo,v_,jee,Hpo,Upo,zP,Jpo,Ypo,Kpo,T_,Dee,Zpo,e_o,WP,o_o,t_o,r_o,F_,Nee,a_o,s_o,QP,n_o,l_o,i_o,C_,qee,d_o,c_o,HP,m_o,f_o,g_o,M_,Oee,h_o,u_o,UP,p_o,__o,b_o,E_,Gee,v_o,T_o,JP,F_o,C_o,M_o,y_,Xee,E_o,y_o,YP,w_o,A_o,L_o,w_,Vee,B_o,x_o,KP,k_o,R_o,S_o,A_,zee,P_o,$_o,ZP,I_o,j_o,D_o,L_,Wee,N_o,q_o,e$,O_o,G_o,X_o,B_,Qee,V_o,z_o,o$,W_o,Q_o,H_o,x_,Hee,U_o,J_o,t$,Y_o,K_o,Z_o,k_,Uee,ebo,obo,r$,tbo,rbo,abo,R_,Jee,sbo,nbo,a$,lbo,ibo,dbo,S_,Yee,cbo,mbo,s$,fbo,gbo,hbo,P_,Kee,ubo,pbo,n$,_bo,bbo,vbo,$_,Zee,Tbo,Fbo,l$,Cbo,Mbo,Ebo,I_,eoe,ybo,wbo,i$,Abo,Lbo,Bbo,j_,ooe,xbo,kbo,d$,Rbo,Sbo,Pbo,D_,toe,$bo,Ibo,c$,jbo,Dbo,Nbo,N_,qbo,roe,Obo,Gbo,aoe,Xbo,Vbo,soe,zbo,Wbo,Dy,XBe,sd,q_,noe,Ny,Qbo,loe,Hbo,VBe,Yo,qy,Ubo,nd,Jbo,ioe,Ybo,Kbo,doe,Zbo,e2o,o2o,Oy,t2o,coe,r2o,a2o,s2o,zt,Gy,n2o,moe,l2o,i2o,ld,d2o,foe,c2o,m2o,goe,f2o,g2o,h2o,hoe,u2o,p2o,Xy,_2o,$e,Vy,b2o,uoe,v2o,T2o,Wa,F2o,poe,C2o,M2o,_oe,E2o,y2o,boe,w2o,A2o,L2o,I,O_,voe,B2o,x2o,m$,k2o,R2o,S2o,G_,Toe,P2o,$2o,f$,I2o,j2o,D2o,X_,Foe,N2o,q2o,g$,O2o,G2o,X2o,V_,Coe,V2o,z2o,h$,W2o,Q2o,H2o,z_,Moe,U2o,J2o,u$,Y2o,K2o,Z2o,W_,Eoe,evo,ovo,p$,tvo,rvo,avo,Q_,yoe,svo,nvo,_$,lvo,ivo,dvo,H_,woe,cvo,mvo,b$,fvo,gvo,hvo,U_,Aoe,uvo,pvo,v$,_vo,bvo,vvo,J_,Loe,Tvo,Fvo,T$,Cvo,Mvo,Evo,Y_,Boe,yvo,wvo,F$,Avo,Lvo,Bvo,K_,xoe,xvo,kvo,C$,Rvo,Svo,Pvo,Z_,koe,$vo,Ivo,M$,jvo,Dvo,Nvo,eb,Roe,qvo,Ovo,E$,Gvo,Xvo,Vvo,ob,Soe,zvo,Wvo,y$,Qvo,Hvo,Uvo,tb,Poe,Jvo,Yvo,w$,Kvo,Zvo,eTo,rb,$oe,oTo,tTo,A$,rTo,aTo,sTo,ab,Ioe,nTo,lTo,L$,iTo,dTo,cTo,sb,joe,mTo,fTo,B$,gTo,hTo,uTo,nb,Doe,pTo,_To,x$,bTo,vTo,TTo,lb,Noe,FTo,CTo,k$,MTo,ETo,yTo,ib,qoe,wTo,ATo,R$,LTo,BTo,xTo,db,Ooe,kTo,RTo,S$,STo,PTo,$To,cb,Goe,ITo,jTo,P$,DTo,NTo,qTo,mb,Xoe,OTo,GTo,$$,XTo,VTo,zTo,fb,Voe,WTo,QTo,I$,HTo,UTo,JTo,gb,zoe,YTo,KTo,j$,ZTo,e1o,o1o,hb,Woe,t1o,r1o,D$,a1o,s1o,n1o,ub,Qoe,l1o,i1o,N$,d1o,c1o,m1o,pb,Hoe,f1o,g1o,q$,h1o,u1o,p1o,_b,Uoe,_1o,b1o,Joe,v1o,T1o,F1o,bb,Yoe,C1o,M1o,O$,E1o,y1o,w1o,vb,Koe,A1o,L1o,G$,B1o,x1o,k1o,Tb,Zoe,R1o,S1o,X$,P1o,$1o,I1o,Fb,ete,j1o,D1o,V$,N1o,q1o,O1o,Cb,G1o,ote,X1o,V1o,tte,z1o,W1o,rte,Q1o,H1o,zy,zBe,id,Mb,ate,Wy,U1o,ste,J1o,WBe,Ko,Qy,Y1o,dd,K1o,nte,Z1o,eFo,lte,oFo,tFo,rFo,Hy,aFo,ite,sFo,nFo,lFo,Wt,Uy,iFo,dte,dFo,cFo,cd,mFo,cte,fFo,gFo,mte,hFo,uFo,pFo,fte,_Fo,bFo,Jy,vFo,Ie,Yy,TFo,gte,FFo,CFo,Qa,MFo,hte,EFo,yFo,ute,wFo,AFo,pte,LFo,BFo,xFo,se,Eb,_te,kFo,RFo,z$,SFo,PFo,$Fo,yb,bte,IFo,jFo,W$,DFo,NFo,qFo,wb,vte,OFo,GFo,Q$,XFo,VFo,zFo,Ab,Tte,WFo,QFo,H$,HFo,UFo,JFo,Lb,Fte,YFo,KFo,U$,ZFo,eCo,oCo,Bb,Cte,tCo,rCo,J$,aCo,sCo,nCo,xb,Mte,lCo,iCo,Y$,dCo,cCo,mCo,kb,Ete,fCo,gCo,K$,hCo,uCo,pCo,Rb,yte,_Co,bCo,Z$,vCo,TCo,FCo,Sb,wte,CCo,MCo,eI,ECo,yCo,wCo,Pb,Ate,ACo,LCo,oI,BCo,xCo,kCo,$b,Lte,RCo,SCo,tI,PCo,$Co,ICo,Ib,Bte,jCo,DCo,rI,NCo,qCo,OCo,jb,xte,GCo,XCo,aI,VCo,zCo,WCo,Db,kte,QCo,HCo,sI,UCo,JCo,YCo,Nb,Rte,KCo,ZCo,nI,e4o,o4o,t4o,qb,r4o,Ste,a4o,s4o,Pte,n4o,l4o,$te,i4o,d4o,Ky,QBe,md,Ob,Ite,Zy,c4o,jte,m4o,HBe,Zo,ew,f4o,fd,g4o,Dte,h4o,u4o,Nte,p4o,_4o,b4o,ow,v4o,qte,T4o,F4o,C4o,Qt,tw,M4o,Ote,E4o,y4o,gd,w4o,Gte,A4o,L4o,Xte,B4o,x4o,k4o,Vte,R4o,S4o,rw,P4o,je,aw,$4o,zte,I4o,j4o,Ha,D4o,Wte,N4o,q4o,Qte,O4o,G4o,Hte,X4o,V4o,z4o,A,Gb,Ute,W4o,Q4o,lI,H4o,U4o,J4o,Xb,Jte,Y4o,K4o,iI,Z4o,eMo,oMo,Vb,Yte,tMo,rMo,dI,aMo,sMo,nMo,zb,Kte,lMo,iMo,cI,dMo,cMo,mMo,Wb,Zte,fMo,gMo,mI,hMo,uMo,pMo,Qb,ere,_Mo,bMo,fI,vMo,TMo,FMo,Hb,ore,CMo,MMo,gI,EMo,yMo,wMo,Ub,tre,AMo,LMo,hI,BMo,xMo,kMo,Jb,rre,RMo,SMo,uI,PMo,$Mo,IMo,Yb,are,jMo,DMo,pI,NMo,qMo,OMo,Kb,sre,GMo,XMo,_I,VMo,zMo,WMo,Zb,nre,QMo,HMo,bI,UMo,JMo,YMo,e2,lre,KMo,ZMo,vI,eEo,oEo,tEo,o2,ire,rEo,aEo,TI,sEo,nEo,lEo,t2,dre,iEo,dEo,FI,cEo,mEo,fEo,r2,cre,gEo,hEo,CI,uEo,pEo,_Eo,a2,mre,bEo,vEo,MI,TEo,FEo,CEo,s2,fre,MEo,EEo,EI,yEo,wEo,AEo,n2,gre,LEo,BEo,yI,xEo,kEo,REo,l2,hre,SEo,PEo,wI,$Eo,IEo,jEo,i2,ure,DEo,NEo,AI,qEo,OEo,GEo,d2,pre,XEo,VEo,LI,zEo,WEo,QEo,c2,_re,HEo,UEo,BI,JEo,YEo,KEo,m2,bre,ZEo,e3o,xI,o3o,t3o,r3o,f2,vre,a3o,s3o,kI,n3o,l3o,i3o,g2,Tre,d3o,c3o,RI,m3o,f3o,g3o,h2,Fre,h3o,u3o,SI,p3o,_3o,b3o,u2,Cre,v3o,T3o,PI,F3o,C3o,M3o,p2,Mre,E3o,y3o,$I,w3o,A3o,L3o,_2,Ere,B3o,x3o,II,k3o,R3o,S3o,b2,yre,P3o,$3o,jI,I3o,j3o,D3o,v2,wre,N3o,q3o,DI,O3o,G3o,X3o,T2,Are,V3o,z3o,NI,W3o,Q3o,H3o,F2,Lre,U3o,J3o,qI,Y3o,K3o,Z3o,C2,Bre,e5o,o5o,OI,t5o,r5o,a5o,M2,xre,s5o,n5o,GI,l5o,i5o,d5o,E2,kre,c5o,m5o,XI,f5o,g5o,h5o,y2,Rre,u5o,p5o,VI,_5o,b5o,v5o,w2,Sre,T5o,F5o,zI,C5o,M5o,E5o,A2,Pre,y5o,w5o,WI,A5o,L5o,B5o,L2,$re,x5o,k5o,QI,R5o,S5o,P5o,B2,Ire,$5o,I5o,HI,j5o,D5o,N5o,x2,jre,q5o,O5o,UI,G5o,X5o,V5o,k2,Dre,z5o,W5o,JI,Q5o,H5o,U5o,R2,Nre,J5o,Y5o,YI,K5o,Z5o,eyo,S2,qre,oyo,tyo,KI,ryo,ayo,syo,P2,nyo,Ore,lyo,iyo,Gre,dyo,cyo,Xre,myo,fyo,sw,UBe,hd,$2,Vre,nw,gyo,zre,hyo,JBe,et,lw,uyo,ud,pyo,Wre,_yo,byo,Qre,vyo,Tyo,Fyo,iw,Cyo,Hre,Myo,Eyo,yyo,Ht,dw,wyo,Ure,Ayo,Lyo,pd,Byo,Jre,xyo,kyo,Yre,Ryo,Syo,Pyo,Kre,$yo,Iyo,cw,jyo,De,mw,Dyo,Zre,Nyo,qyo,Ua,Oyo,eae,Gyo,Xyo,oae,Vyo,zyo,tae,Wyo,Qyo,Hyo,O,I2,rae,Uyo,Jyo,ZI,Yyo,Kyo,Zyo,j2,aae,ewo,owo,ej,two,rwo,awo,D2,sae,swo,nwo,oj,lwo,iwo,dwo,N2,nae,cwo,mwo,tj,fwo,gwo,hwo,q2,lae,uwo,pwo,rj,_wo,bwo,vwo,O2,iae,Two,Fwo,aj,Cwo,Mwo,Ewo,G2,dae,ywo,wwo,sj,Awo,Lwo,Bwo,X2,cae,xwo,kwo,nj,Rwo,Swo,Pwo,V2,mae,$wo,Iwo,lj,jwo,Dwo,Nwo,z2,fae,qwo,Owo,ij,Gwo,Xwo,Vwo,W2,gae,zwo,Wwo,dj,Qwo,Hwo,Uwo,Q2,hae,Jwo,Ywo,cj,Kwo,Zwo,e6o,H2,uae,o6o,t6o,mj,r6o,a6o,s6o,U2,pae,n6o,l6o,fj,i6o,d6o,c6o,J2,_ae,m6o,f6o,gj,g6o,h6o,u6o,Y2,bae,p6o,_6o,hj,b6o,v6o,T6o,K2,vae,F6o,C6o,uj,M6o,E6o,y6o,Z2,Tae,w6o,A6o,pj,L6o,B6o,x6o,ev,Fae,k6o,R6o,_j,S6o,P6o,$6o,ov,Cae,I6o,j6o,bj,D6o,N6o,q6o,tv,Mae,O6o,G6o,vj,X6o,V6o,z6o,rv,Eae,W6o,Q6o,Tj,H6o,U6o,J6o,av,yae,Y6o,K6o,Fj,Z6o,eAo,oAo,sv,wae,tAo,rAo,Cj,aAo,sAo,nAo,nv,Aae,lAo,iAo,Mj,dAo,cAo,mAo,lv,Lae,fAo,gAo,Ej,hAo,uAo,pAo,iv,Bae,_Ao,bAo,yj,vAo,TAo,FAo,dv,xae,CAo,MAo,wj,EAo,yAo,wAo,cv,AAo,kae,LAo,BAo,Rae,xAo,kAo,Sae,RAo,SAo,fw,YBe,_d,mv,Pae,gw,PAo,$ae,$Ao,KBe,ot,hw,IAo,bd,jAo,Iae,DAo,NAo,jae,qAo,OAo,GAo,uw,XAo,Dae,VAo,zAo,WAo,Ut,pw,QAo,Nae,HAo,UAo,vd,JAo,qae,YAo,KAo,Oae,ZAo,e0o,o0o,Gae,t0o,r0o,_w,a0o,Ne,bw,s0o,Xae,n0o,l0o,Ja,i0o,Vae,d0o,c0o,zae,m0o,f0o,Wae,g0o,h0o,u0o,da,fv,Qae,p0o,_0o,Aj,b0o,v0o,T0o,gv,Hae,F0o,C0o,Lj,M0o,E0o,y0o,hv,Uae,w0o,A0o,Bj,L0o,B0o,x0o,uv,Jae,k0o,R0o,xj,S0o,P0o,$0o,pv,Yae,I0o,j0o,kj,D0o,N0o,q0o,_v,O0o,Kae,G0o,X0o,Zae,V0o,z0o,ese,W0o,Q0o,vw,ZBe,Td,bv,ose,Tw,H0o,tse,U0o,exe,tt,Fw,J0o,Fd,Y0o,rse,K0o,Z0o,ase,eLo,oLo,tLo,Cw,rLo,sse,aLo,sLo,nLo,Jt,Mw,lLo,nse,iLo,dLo,Cd,cLo,lse,mLo,fLo,ise,gLo,hLo,uLo,dse,pLo,_Lo,Ew,bLo,qe,yw,vLo,cse,TLo,FLo,Ya,CLo,mse,MLo,ELo,fse,yLo,wLo,gse,ALo,LLo,BLo,N,vv,hse,xLo,kLo,Rj,RLo,SLo,PLo,Tv,use,$Lo,ILo,Sj,jLo,DLo,NLo,Fv,pse,qLo,OLo,Pj,GLo,XLo,VLo,Cv,_se,zLo,WLo,$j,QLo,HLo,ULo,Mv,bse,JLo,YLo,Ij,KLo,ZLo,e8o,Ev,vse,o8o,t8o,jj,r8o,a8o,s8o,yv,Tse,n8o,l8o,Dj,i8o,d8o,c8o,wv,Fse,m8o,f8o,Nj,g8o,h8o,u8o,Av,Cse,p8o,_8o,qj,b8o,v8o,T8o,Lv,Mse,F8o,C8o,Oj,M8o,E8o,y8o,Bv,Ese,w8o,A8o,Gj,L8o,B8o,x8o,xv,yse,k8o,R8o,Xj,S8o,P8o,$8o,kv,wse,I8o,j8o,Vj,D8o,N8o,q8o,Rv,Ase,O8o,G8o,zj,X8o,V8o,z8o,Sv,Lse,W8o,Q8o,Wj,H8o,U8o,J8o,Pv,Bse,Y8o,K8o,Qj,Z8o,e7o,o7o,$v,xse,t7o,r7o,Hj,a7o,s7o,n7o,Iv,kse,l7o,i7o,Uj,d7o,c7o,m7o,jv,Rse,f7o,g7o,Jj,h7o,u7o,p7o,Dv,Sse,_7o,b7o,Yj,v7o,T7o,F7o,Nv,Pse,C7o,M7o,Kj,E7o,y7o,w7o,qv,$se,A7o,L7o,Zj,B7o,x7o,k7o,Ov,Ise,R7o,S7o,eD,P7o,$7o,I7o,Gv,jse,j7o,D7o,oD,N7o,q7o,O7o,Xv,Dse,G7o,X7o,tD,V7o,z7o,W7o,Vv,Nse,Q7o,H7o,rD,U7o,J7o,Y7o,zv,qse,K7o,Z7o,aD,e9o,o9o,t9o,Wv,Ose,r9o,a9o,sD,s9o,n9o,l9o,Qv,Gse,i9o,d9o,nD,c9o,m9o,f9o,Hv,Xse,g9o,h9o,lD,u9o,p9o,_9o,Uv,Vse,b9o,v9o,iD,T9o,F9o,C9o,Jv,zse,M9o,E9o,dD,y9o,w9o,A9o,Yv,Wse,L9o,B9o,cD,x9o,k9o,R9o,Kv,S9o,Qse,P9o,$9o,Hse,I9o,j9o,Use,D9o,N9o,ww,oxe,Md,Zv,Jse,Aw,q9o,Yse,O9o,txe,rt,Lw,G9o,Ed,X9o,Kse,V9o,z9o,Zse,W9o,Q9o,H9o,Bw,U9o,ene,J9o,Y9o,K9o,Yt,xw,Z9o,one,eBo,oBo,yd,tBo,tne,rBo,aBo,rne,sBo,nBo,lBo,ane,iBo,dBo,kw,cBo,Oe,Rw,mBo,sne,fBo,gBo,Ka,hBo,nne,uBo,pBo,lne,_Bo,bBo,ine,vBo,TBo,FBo,R,eT,dne,CBo,MBo,mD,EBo,yBo,wBo,oT,cne,ABo,LBo,fD,BBo,xBo,kBo,tT,mne,RBo,SBo,gD,PBo,$Bo,IBo,rT,fne,jBo,DBo,hD,NBo,qBo,OBo,aT,gne,GBo,XBo,uD,VBo,zBo,WBo,sT,hne,QBo,HBo,pD,UBo,JBo,YBo,nT,une,KBo,ZBo,_D,exo,oxo,txo,lT,pne,rxo,axo,bD,sxo,nxo,lxo,iT,_ne,ixo,dxo,vD,cxo,mxo,fxo,dT,bne,gxo,hxo,TD,uxo,pxo,_xo,cT,vne,bxo,vxo,FD,Txo,Fxo,Cxo,mT,Tne,Mxo,Exo,CD,yxo,wxo,Axo,fT,Fne,Lxo,Bxo,MD,xxo,kxo,Rxo,gT,Cne,Sxo,Pxo,ED,$xo,Ixo,jxo,hT,Mne,Dxo,Nxo,yD,qxo,Oxo,Gxo,uT,Ene,Xxo,Vxo,wD,zxo,Wxo,Qxo,pT,yne,Hxo,Uxo,AD,Jxo,Yxo,Kxo,_T,wne,Zxo,eko,LD,oko,tko,rko,bT,Ane,ako,sko,BD,nko,lko,iko,vT,Lne,dko,cko,xD,mko,fko,gko,TT,Bne,hko,uko,kD,pko,_ko,bko,FT,xne,vko,Tko,RD,Fko,Cko,Mko,CT,kne,Eko,yko,SD,wko,Ako,Lko,MT,Rne,Bko,xko,PD,kko,Rko,Sko,ET,Sne,Pko,$ko,$D,Iko,jko,Dko,yT,Pne,Nko,qko,ID,Oko,Gko,Xko,wT,$ne,Vko,zko,jD,Wko,Qko,Hko,AT,Ine,Uko,Jko,DD,Yko,Kko,Zko,LT,jne,eRo,oRo,ND,tRo,rRo,aRo,BT,Dne,sRo,nRo,qD,lRo,iRo,dRo,xT,Nne,cRo,mRo,OD,fRo,gRo,hRo,kT,qne,uRo,pRo,GD,_Ro,bRo,vRo,RT,One,TRo,FRo,XD,CRo,MRo,ERo,ST,Gne,yRo,wRo,VD,ARo,LRo,BRo,PT,Xne,xRo,kRo,zD,RRo,SRo,PRo,$T,Vne,$Ro,IRo,WD,jRo,DRo,NRo,IT,zne,qRo,ORo,QD,GRo,XRo,VRo,jT,Wne,zRo,WRo,HD,QRo,HRo,URo,DT,Qne,JRo,YRo,UD,KRo,ZRo,eSo,NT,oSo,Hne,tSo,rSo,Une,aSo,sSo,Jne,nSo,lSo,Sw,rxe,wd,qT,Yne,Pw,iSo,Kne,dSo,axe,at,$w,cSo,Ad,mSo,Zne,fSo,gSo,ele,hSo,uSo,pSo,Iw,_So,ole,bSo,vSo,TSo,Kt,jw,FSo,tle,CSo,MSo,Ld,ESo,rle,ySo,wSo,ale,ASo,LSo,BSo,sle,xSo,kSo,Dw,RSo,Ge,Nw,SSo,nle,PSo,$So,Za,ISo,lle,jSo,DSo,ile,NSo,qSo,dle,OSo,GSo,XSo,cle,OT,mle,VSo,zSo,JD,WSo,QSo,HSo,GT,USo,fle,JSo,YSo,gle,KSo,ZSo,hle,ePo,oPo,qw,sxe,Bd,XT,ule,Ow,tPo,ple,rPo,nxe,st,Gw,aPo,xd,sPo,_le,nPo,lPo,ble,iPo,dPo,cPo,Xw,mPo,vle,fPo,gPo,hPo,Zt,Vw,uPo,Tle,pPo,_Po,kd,bPo,Fle,vPo,TPo,Cle,FPo,CPo,MPo,Mle,EPo,yPo,zw,wPo,Xe,Ww,APo,Ele,LPo,BPo,es,xPo,yle,kPo,RPo,wle,SPo,PPo,Ale,$Po,IPo,jPo,be,VT,Lle,DPo,NPo,YD,qPo,OPo,GPo,zT,Ble,XPo,VPo,KD,zPo,WPo,QPo,qn,xle,HPo,UPo,ZD,JPo,YPo,eN,KPo,ZPo,e$o,WT,kle,o$o,t$o,oN,r$o,a$o,s$o,ma,Rle,n$o,l$o,tN,i$o,d$o,rN,c$o,m$o,aN,f$o,g$o,h$o,QT,Sle,u$o,p$o,sN,_$o,b$o,v$o,HT,Ple,T$o,F$o,nN,C$o,M$o,E$o,UT,$le,y$o,w$o,lN,A$o,L$o,B$o,JT,Ile,x$o,k$o,iN,R$o,S$o,P$o,YT,$$o,jle,I$o,j$o,Dle,D$o,N$o,Nle,q$o,O$o,Qw,lxe,Rd,KT,qle,Hw,G$o,Ole,X$o,ixe,nt,Uw,V$o,Sd,z$o,Gle,W$o,Q$o,Xle,H$o,U$o,J$o,Jw,Y$o,Vle,K$o,Z$o,eIo,er,Yw,oIo,zle,tIo,rIo,Pd,aIo,Wle,sIo,nIo,Qle,lIo,iIo,dIo,Hle,cIo,mIo,Kw,fIo,Ve,Zw,gIo,Ule,hIo,uIo,os,pIo,Jle,_Io,bIo,Yle,vIo,TIo,Kle,FIo,CIo,MIo,Zle,ZT,eie,EIo,yIo,dN,wIo,AIo,LIo,e1,BIo,oie,xIo,kIo,tie,RIo,SIo,rie,PIo,$Io,e6,dxe,$d,o1,aie,o6,IIo,sie,jIo,cxe,lt,t6,DIo,Id,NIo,nie,qIo,OIo,lie,GIo,XIo,VIo,r6,zIo,iie,WIo,QIo,HIo,or,a6,UIo,die,JIo,YIo,jd,KIo,cie,ZIo,ejo,mie,ojo,tjo,rjo,fie,ajo,sjo,s6,njo,ze,n6,ljo,gie,ijo,djo,ts,cjo,hie,mjo,fjo,uie,gjo,hjo,pie,ujo,pjo,_jo,Ae,t1,_ie,bjo,vjo,cN,Tjo,Fjo,Cjo,r1,bie,Mjo,Ejo,mN,yjo,wjo,Ajo,a1,vie,Ljo,Bjo,fN,xjo,kjo,Rjo,s1,Tie,Sjo,Pjo,gN,$jo,Ijo,jjo,n1,Fie,Djo,Njo,hN,qjo,Ojo,Gjo,l1,Cie,Xjo,Vjo,uN,zjo,Wjo,Qjo,i1,Mie,Hjo,Ujo,pN,Jjo,Yjo,Kjo,d1,Eie,Zjo,eDo,_N,oDo,tDo,rDo,c1,aDo,yie,sDo,nDo,wie,lDo,iDo,Aie,dDo,cDo,l6,mxe,Dd,m1,Lie,i6,mDo,Bie,fDo,fxe,it,d6,gDo,Nd,hDo,xie,uDo,pDo,kie,_Do,bDo,vDo,c6,TDo,Rie,FDo,CDo,MDo,tr,m6,EDo,Sie,yDo,wDo,qd,ADo,Pie,LDo,BDo,$ie,xDo,kDo,RDo,Iie,SDo,PDo,f6,$Do,We,g6,IDo,jie,jDo,DDo,rs,NDo,Die,qDo,ODo,Nie,GDo,XDo,qie,VDo,zDo,WDo,as,f1,Oie,QDo,HDo,bN,UDo,JDo,YDo,g1,Gie,KDo,ZDo,vN,eNo,oNo,tNo,h1,Xie,rNo,aNo,TN,sNo,nNo,lNo,u1,Vie,iNo,dNo,FN,cNo,mNo,fNo,p1,gNo,zie,hNo,uNo,Wie,pNo,_No,Qie,bNo,vNo,h6,gxe,Od,_1,Hie,u6,TNo,Uie,FNo,hxe,dt,p6,CNo,Gd,MNo,Jie,ENo,yNo,Yie,wNo,ANo,LNo,_6,BNo,Kie,xNo,kNo,RNo,rr,b6,SNo,Zie,PNo,$No,Xd,INo,ede,jNo,DNo,ode,NNo,qNo,ONo,tde,GNo,XNo,v6,VNo,Qe,T6,zNo,rde,WNo,QNo,ss,HNo,ade,UNo,JNo,sde,YNo,KNo,nde,ZNo,eqo,oqo,Le,b1,lde,tqo,rqo,CN,aqo,sqo,nqo,v1,ide,lqo,iqo,MN,dqo,cqo,mqo,T1,dde,fqo,gqo,EN,hqo,uqo,pqo,F1,cde,_qo,bqo,yN,vqo,Tqo,Fqo,C1,mde,Cqo,Mqo,wN,Eqo,yqo,wqo,M1,fde,Aqo,Lqo,AN,Bqo,xqo,kqo,E1,gde,Rqo,Sqo,LN,Pqo,$qo,Iqo,y1,hde,jqo,Dqo,BN,Nqo,qqo,Oqo,w1,Gqo,ude,Xqo,Vqo,pde,zqo,Wqo,_de,Qqo,Hqo,F6,uxe,Vd,A1,bde,C6,Uqo,vde,Jqo,pxe,ct,M6,Yqo,zd,Kqo,Tde,Zqo,eOo,Fde,oOo,tOo,rOo,E6,aOo,Cde,sOo,nOo,lOo,ar,y6,iOo,Mde,dOo,cOo,Wd,mOo,Ede,fOo,gOo,yde,hOo,uOo,pOo,wde,_Oo,bOo,w6,vOo,He,A6,TOo,Ade,FOo,COo,ns,MOo,Lde,EOo,yOo,Bde,wOo,AOo,xde,LOo,BOo,xOo,L6,L1,kde,kOo,ROo,xN,SOo,POo,$Oo,B1,Rde,IOo,jOo,kN,DOo,NOo,qOo,x1,OOo,Sde,GOo,XOo,Pde,VOo,zOo,$de,WOo,QOo,B6,_xe,Qd,k1,Ide,x6,HOo,jde,UOo,bxe,mt,k6,JOo,Hd,YOo,Dde,KOo,ZOo,Nde,eGo,oGo,tGo,R6,rGo,qde,aGo,sGo,nGo,sr,S6,lGo,Ode,iGo,dGo,Ud,cGo,Gde,mGo,fGo,Xde,gGo,hGo,uGo,Vde,pGo,_Go,P6,bGo,Ue,$6,vGo,zde,TGo,FGo,ls,CGo,Wde,MGo,EGo,Qde,yGo,wGo,Hde,AGo,LGo,BGo,is,R1,Ude,xGo,kGo,RN,RGo,SGo,PGo,S1,Jde,$Go,IGo,SN,jGo,DGo,NGo,P1,Yde,qGo,OGo,PN,GGo,XGo,VGo,$1,Kde,zGo,WGo,$N,QGo,HGo,UGo,I1,JGo,Zde,YGo,KGo,ece,ZGo,eXo,oce,oXo,tXo,I6,vxe,Jd,j1,tce,j6,rXo,rce,aXo,Txe,ft,D6,sXo,Yd,nXo,ace,lXo,iXo,sce,dXo,cXo,mXo,N6,fXo,nce,gXo,hXo,uXo,nr,q6,pXo,lce,_Xo,bXo,Kd,vXo,ice,TXo,FXo,dce,CXo,MXo,EXo,cce,yXo,wXo,O6,AXo,Je,G6,LXo,mce,BXo,xXo,ds,kXo,fce,RXo,SXo,gce,PXo,$Xo,hce,IXo,jXo,DXo,Zd,D1,uce,NXo,qXo,IN,OXo,GXo,XXo,N1,pce,VXo,zXo,jN,WXo,QXo,HXo,q1,_ce,UXo,JXo,DN,YXo,KXo,ZXo,O1,eVo,bce,oVo,tVo,vce,rVo,aVo,Tce,sVo,nVo,X6,Fxe,ec,G1,Fce,V6,lVo,Cce,iVo,Cxe,gt,z6,dVo,oc,cVo,Mce,mVo,fVo,Ece,gVo,hVo,uVo,W6,pVo,yce,_Vo,bVo,vVo,lr,Q6,TVo,wce,FVo,CVo,tc,MVo,Ace,EVo,yVo,Lce,wVo,AVo,LVo,Bce,BVo,xVo,H6,kVo,Ye,U6,RVo,xce,SVo,PVo,cs,$Vo,kce,IVo,jVo,Rce,DVo,NVo,Sce,qVo,OVo,GVo,Pce,X1,$ce,XVo,VVo,NN,zVo,WVo,QVo,V1,HVo,Ice,UVo,JVo,jce,YVo,KVo,Dce,ZVo,ezo,J6,Mxe,rc,z1,Nce,Y6,ozo,qce,tzo,Exe,ht,K6,rzo,ac,azo,Oce,szo,nzo,Gce,lzo,izo,dzo,Z6,czo,Xce,mzo,fzo,gzo,ir,eA,hzo,Vce,uzo,pzo,sc,_zo,zce,bzo,vzo,Wce,Tzo,Fzo,Czo,Qce,Mzo,Ezo,oA,yzo,Ke,tA,wzo,Hce,Azo,Lzo,ms,Bzo,Uce,xzo,kzo,Jce,Rzo,Szo,Yce,Pzo,$zo,Izo,Kce,W1,Zce,jzo,Dzo,qN,Nzo,qzo,Ozo,Q1,Gzo,eme,Xzo,Vzo,ome,zzo,Wzo,tme,Qzo,Hzo,rA,yxe,nc,H1,rme,aA,Uzo,ame,Jzo,wxe,ut,sA,Yzo,lc,Kzo,sme,Zzo,eWo,nme,oWo,tWo,rWo,nA,aWo,lme,sWo,nWo,lWo,dr,lA,iWo,ime,dWo,cWo,ic,mWo,dme,fWo,gWo,cme,hWo,uWo,pWo,mme,_Wo,bWo,iA,vWo,Ze,dA,TWo,fme,FWo,CWo,fs,MWo,gme,EWo,yWo,hme,wWo,AWo,ume,LWo,BWo,xWo,cA,U1,pme,kWo,RWo,ON,SWo,PWo,$Wo,J1,_me,IWo,jWo,GN,DWo,NWo,qWo,Y1,OWo,bme,GWo,XWo,vme,VWo,zWo,Tme,WWo,QWo,mA,Axe,dc,K1,Fme,fA,HWo,Cme,UWo,Lxe,pt,gA,JWo,cc,YWo,Mme,KWo,ZWo,Eme,eQo,oQo,tQo,hA,rQo,yme,aQo,sQo,nQo,cr,uA,lQo,wme,iQo,dQo,mc,cQo,Ame,mQo,fQo,Lme,gQo,hQo,uQo,Bme,pQo,_Qo,pA,bQo,eo,_A,vQo,xme,TQo,FQo,gs,CQo,kme,MQo,EQo,Rme,yQo,wQo,Sme,AQo,LQo,BQo,Pme,Z1,$me,xQo,kQo,XN,RQo,SQo,PQo,eF,$Qo,Ime,IQo,jQo,jme,DQo,NQo,Dme,qQo,OQo,bA,Bxe,fc,oF,Nme,vA,GQo,qme,XQo,xxe,_t,TA,VQo,gc,zQo,Ome,WQo,QQo,Gme,HQo,UQo,JQo,FA,YQo,Xme,KQo,ZQo,eHo,mr,CA,oHo,Vme,tHo,rHo,hc,aHo,zme,sHo,nHo,Wme,lHo,iHo,dHo,Qme,cHo,mHo,MA,fHo,ho,EA,gHo,Hme,hHo,uHo,hs,pHo,Ume,_Ho,bHo,Jme,vHo,THo,Yme,FHo,CHo,MHo,B,tF,Kme,EHo,yHo,VN,wHo,AHo,LHo,rF,Zme,BHo,xHo,zN,kHo,RHo,SHo,aF,efe,PHo,$Ho,WN,IHo,jHo,DHo,sF,ofe,NHo,qHo,QN,OHo,GHo,XHo,nF,tfe,VHo,zHo,HN,WHo,QHo,HHo,lF,rfe,UHo,JHo,UN,YHo,KHo,ZHo,iF,afe,eUo,oUo,JN,tUo,rUo,aUo,dF,sfe,sUo,nUo,YN,lUo,iUo,dUo,cF,nfe,cUo,mUo,KN,fUo,gUo,hUo,mF,lfe,uUo,pUo,ZN,_Uo,bUo,vUo,fF,ife,TUo,FUo,eq,CUo,MUo,EUo,gF,dfe,yUo,wUo,oq,AUo,LUo,BUo,hF,cfe,xUo,kUo,tq,RUo,SUo,PUo,uF,mfe,$Uo,IUo,rq,jUo,DUo,NUo,pF,ffe,qUo,OUo,aq,GUo,XUo,VUo,_F,gfe,zUo,WUo,sq,QUo,HUo,UUo,On,hfe,JUo,YUo,nq,KUo,ZUo,lq,eJo,oJo,tJo,bF,ufe,rJo,aJo,iq,sJo,nJo,lJo,vF,pfe,iJo,dJo,dq,cJo,mJo,fJo,TF,_fe,gJo,hJo,cq,uJo,pJo,_Jo,FF,bfe,bJo,vJo,mq,TJo,FJo,CJo,CF,vfe,MJo,EJo,fq,yJo,wJo,AJo,MF,Tfe,LJo,BJo,gq,xJo,kJo,RJo,EF,Ffe,SJo,PJo,hq,$Jo,IJo,jJo,yF,Cfe,DJo,NJo,uq,qJo,OJo,GJo,wF,Mfe,XJo,VJo,pq,zJo,WJo,QJo,AF,Efe,HJo,UJo,_q,JJo,YJo,KJo,LF,yfe,ZJo,eYo,bq,oYo,tYo,rYo,BF,wfe,aYo,sYo,vq,nYo,lYo,iYo,xF,Afe,dYo,cYo,Tq,mYo,fYo,gYo,kF,Lfe,hYo,uYo,Fq,pYo,_Yo,bYo,RF,Bfe,vYo,TYo,Cq,FYo,CYo,MYo,SF,xfe,EYo,yYo,Mq,wYo,AYo,LYo,PF,kfe,BYo,xYo,Eq,kYo,RYo,SYo,$F,Rfe,PYo,$Yo,yq,IYo,jYo,DYo,IF,Sfe,NYo,qYo,wq,OYo,GYo,XYo,jF,Pfe,VYo,zYo,Aq,WYo,QYo,HYo,DF,$fe,UYo,JYo,Lq,YYo,KYo,ZYo,NF,Ife,eKo,oKo,Bq,tKo,rKo,aKo,qF,jfe,sKo,nKo,xq,lKo,iKo,dKo,OF,Dfe,cKo,mKo,kq,fKo,gKo,hKo,GF,Nfe,uKo,pKo,Rq,_Ko,bKo,vKo,qfe,TKo,FKo,yA,kxe,uc,XF,Ofe,wA,CKo,Gfe,MKo,Rxe,bt,AA,EKo,pc,yKo,Xfe,wKo,AKo,Vfe,LKo,BKo,xKo,LA,kKo,zfe,RKo,SKo,PKo,fr,BA,$Ko,Wfe,IKo,jKo,_c,DKo,Qfe,NKo,qKo,Hfe,OKo,GKo,XKo,Ufe,VKo,zKo,xA,WKo,uo,kA,QKo,Jfe,HKo,UKo,us,JKo,Yfe,YKo,KKo,Kfe,ZKo,eZo,Zfe,oZo,tZo,rZo,H,VF,ege,aZo,sZo,Sq,nZo,lZo,iZo,zF,oge,dZo,cZo,Pq,mZo,fZo,gZo,WF,tge,hZo,uZo,$q,pZo,_Zo,bZo,QF,rge,vZo,TZo,Iq,FZo,CZo,MZo,HF,age,EZo,yZo,jq,wZo,AZo,LZo,UF,sge,BZo,xZo,Dq,kZo,RZo,SZo,JF,nge,PZo,$Zo,Nq,IZo,jZo,DZo,YF,lge,NZo,qZo,qq,OZo,GZo,XZo,KF,ige,VZo,zZo,Oq,WZo,QZo,HZo,ZF,dge,UZo,JZo,Gq,YZo,KZo,ZZo,eC,cge,eet,oet,Xq,tet,ret,aet,oC,mge,set,net,Vq,iet,det,cet,tC,fge,met,fet,zq,get,het,uet,rC,gge,pet,_et,Wq,bet,vet,Tet,aC,hge,Fet,Cet,Qq,Met,Eet,yet,sC,uge,wet,Aet,Hq,Let,Bet,xet,nC,pge,ket,Ret,Uq,Set,Pet,$et,lC,_ge,Iet,jet,Jq,Det,Net,qet,iC,bge,Oet,Get,Yq,Xet,Vet,zet,dC,vge,Wet,Qet,Kq,Het,Uet,Jet,cC,Tge,Yet,Ket,Zq,Zet,eot,oot,mC,Fge,tot,rot,eO,aot,sot,not,Cge,lot,iot,RA,Sxe,bc,fC,Mge,SA,dot,Ege,cot,Pxe,vt,PA,mot,vc,fot,yge,got,hot,wge,uot,pot,_ot,$A,bot,Age,vot,Tot,Fot,gr,IA,Cot,Lge,Mot,Eot,Tc,yot,Bge,wot,Aot,xge,Lot,Bot,xot,kge,kot,Rot,jA,Sot,po,DA,Pot,Rge,$ot,Iot,ps,jot,Sge,Dot,Not,Pge,qot,Oot,$ge,Got,Xot,Vot,he,gC,Ige,zot,Wot,oO,Qot,Hot,Uot,hC,jge,Jot,Yot,tO,Kot,Zot,ett,uC,Dge,ott,ttt,rO,rtt,att,stt,pC,Nge,ntt,ltt,aO,itt,dtt,ctt,_C,qge,mtt,ftt,sO,gtt,htt,utt,bC,Oge,ptt,_tt,nO,btt,vtt,Ttt,vC,Gge,Ftt,Ctt,lO,Mtt,Ett,ytt,TC,Xge,wtt,Att,iO,Ltt,Btt,xtt,FC,Vge,ktt,Rtt,dO,Stt,Ptt,$tt,CC,zge,Itt,jtt,cO,Dtt,Ntt,qtt,Wge,Ott,Gtt,NA,$xe,Fc,MC,Qge,qA,Xtt,Hge,Vtt,Ixe,Tt,OA,ztt,Cc,Wtt,Uge,Qtt,Htt,Jge,Utt,Jtt,Ytt,GA,Ktt,Yge,Ztt,ert,ort,hr,XA,trt,Kge,rrt,art,Mc,srt,Zge,nrt,lrt,ehe,irt,drt,crt,ohe,mrt,frt,VA,grt,_o,zA,hrt,the,urt,prt,_s,_rt,rhe,brt,vrt,ahe,Trt,Frt,she,Crt,Mrt,Ert,WA,EC,nhe,yrt,wrt,mO,Art,Lrt,Brt,yC,lhe,xrt,krt,fO,Rrt,Srt,Prt,ihe,$rt,Irt,QA,jxe,Ec,wC,dhe,HA,jrt,che,Drt,Dxe,Ft,UA,Nrt,yc,qrt,mhe,Ort,Grt,fhe,Xrt,Vrt,zrt,JA,Wrt,ghe,Qrt,Hrt,Urt,ur,YA,Jrt,hhe,Yrt,Krt,wc,Zrt,uhe,eat,oat,phe,tat,rat,aat,_he,sat,nat,KA,lat,bo,ZA,iat,bhe,dat,cat,bs,mat,vhe,fat,gat,The,hat,uat,Fhe,pat,_at,bat,Y,AC,Che,vat,Tat,gO,Fat,Cat,Mat,LC,Mhe,Eat,yat,hO,wat,Aat,Lat,BC,Ehe,Bat,xat,uO,kat,Rat,Sat,xC,yhe,Pat,$at,pO,Iat,jat,Dat,kC,whe,Nat,qat,_O,Oat,Gat,Xat,RC,Ahe,Vat,zat,bO,Wat,Qat,Hat,SC,Lhe,Uat,Jat,vO,Yat,Kat,Zat,PC,Bhe,est,ost,TO,tst,rst,ast,$C,xhe,sst,nst,FO,lst,ist,dst,IC,khe,cst,mst,CO,fst,gst,hst,jC,Rhe,ust,pst,MO,_st,bst,vst,DC,She,Tst,Fst,EO,Cst,Mst,Est,NC,Phe,yst,wst,yO,Ast,Lst,Bst,qC,$he,xst,kst,wO,Rst,Sst,Pst,OC,Ihe,$st,Ist,AO,jst,Dst,Nst,GC,jhe,qst,Ost,LO,Gst,Xst,Vst,XC,Dhe,zst,Wst,BO,Qst,Hst,Ust,VC,Nhe,Jst,Yst,xO,Kst,Zst,ent,zC,qhe,ont,tnt,kO,rnt,ant,snt,WC,Ohe,nnt,lnt,RO,int,dnt,cnt,Ghe,mnt,fnt,e0,Nxe,Ac,QC,Xhe,o0,gnt,Vhe,hnt,qxe,Ct,t0,unt,Lc,pnt,zhe,_nt,bnt,Whe,vnt,Tnt,Fnt,r0,Cnt,Qhe,Mnt,Ent,ynt,pr,a0,wnt,Hhe,Ant,Lnt,Bc,Bnt,Uhe,xnt,knt,Jhe,Rnt,Snt,Pnt,Yhe,$nt,Int,s0,jnt,vo,n0,Dnt,Khe,Nnt,qnt,vs,Ont,Zhe,Gnt,Xnt,eue,Vnt,znt,oue,Wnt,Qnt,Hnt,ue,HC,tue,Unt,Jnt,SO,Ynt,Knt,Znt,UC,rue,elt,olt,PO,tlt,rlt,alt,JC,aue,slt,nlt,$O,llt,ilt,dlt,YC,sue,clt,mlt,IO,flt,glt,hlt,KC,nue,ult,plt,jO,_lt,blt,vlt,ZC,lue,Tlt,Flt,DO,Clt,Mlt,Elt,e4,iue,ylt,wlt,NO,Alt,Llt,Blt,o4,due,xlt,klt,qO,Rlt,Slt,Plt,t4,cue,$lt,Ilt,OO,jlt,Dlt,Nlt,r4,mue,qlt,Olt,GO,Glt,Xlt,Vlt,fue,zlt,Wlt,l0,Oxe,xc,a4,gue,i0,Qlt,hue,Hlt,Gxe,Mt,d0,Ult,kc,Jlt,uue,Ylt,Klt,pue,Zlt,eit,oit,c0,tit,_ue,rit,ait,sit,_r,m0,nit,bue,lit,iit,Rc,dit,vue,cit,mit,Tue,fit,git,hit,Fue,uit,pit,f0,_it,To,g0,bit,Cue,vit,Tit,Ts,Fit,Mue,Cit,Mit,Eue,Eit,yit,yue,wit,Ait,Lit,V,s4,wue,Bit,xit,XO,kit,Rit,Sit,n4,Aue,Pit,$it,VO,Iit,jit,Dit,l4,Lue,Nit,qit,zO,Oit,Git,Xit,i4,Bue,Vit,zit,WO,Wit,Qit,Hit,d4,xue,Uit,Jit,QO,Yit,Kit,Zit,c4,kue,edt,odt,HO,tdt,rdt,adt,m4,Rue,sdt,ndt,UO,ldt,idt,ddt,f4,Sue,cdt,mdt,JO,fdt,gdt,hdt,g4,Pue,udt,pdt,YO,_dt,bdt,vdt,h4,$ue,Tdt,Fdt,KO,Cdt,Mdt,Edt,u4,Iue,ydt,wdt,ZO,Adt,Ldt,Bdt,p4,jue,xdt,kdt,eG,Rdt,Sdt,Pdt,_4,Due,$dt,Idt,oG,jdt,Ddt,Ndt,b4,Nue,qdt,Odt,tG,Gdt,Xdt,Vdt,v4,que,zdt,Wdt,rG,Qdt,Hdt,Udt,T4,Oue,Jdt,Ydt,aG,Kdt,Zdt,ect,F4,Gue,oct,tct,sG,rct,act,sct,C4,Xue,nct,lct,nG,ict,dct,cct,M4,Vue,mct,fct,lG,gct,hct,uct,E4,zue,pct,_ct,iG,bct,vct,Tct,y4,Wue,Fct,Cct,dG,Mct,Ect,yct,w4,Que,wct,Act,cG,Lct,Bct,xct,A4,Hue,kct,Rct,mG,Sct,Pct,$ct,L4,Uue,Ict,jct,fG,Dct,Nct,qct,B4,Jue,Oct,Gct,gG,Xct,Vct,zct,Yue,Wct,Qct,h0,Xxe,Sc,x4,Kue,u0,Hct,Zue,Uct,Vxe,Et,p0,Jct,Pc,Yct,epe,Kct,Zct,ope,emt,omt,tmt,_0,rmt,tpe,amt,smt,nmt,br,b0,lmt,rpe,imt,dmt,$c,cmt,ape,mmt,fmt,spe,gmt,hmt,umt,npe,pmt,_mt,v0,bmt,Fo,T0,vmt,lpe,Tmt,Fmt,Fs,Cmt,ipe,Mmt,Emt,dpe,ymt,wmt,cpe,Amt,Lmt,Bmt,re,k4,mpe,xmt,kmt,hG,Rmt,Smt,Pmt,R4,fpe,$mt,Imt,uG,jmt,Dmt,Nmt,S4,gpe,qmt,Omt,pG,Gmt,Xmt,Vmt,P4,hpe,zmt,Wmt,_G,Qmt,Hmt,Umt,$4,upe,Jmt,Ymt,bG,Kmt,Zmt,eft,I4,ppe,oft,tft,vG,rft,aft,sft,j4,_pe,nft,lft,TG,ift,dft,cft,D4,bpe,mft,fft,FG,gft,hft,uft,N4,vpe,pft,_ft,CG,bft,vft,Tft,q4,Tpe,Fft,Cft,MG,Mft,Eft,yft,O4,Fpe,wft,Aft,EG,Lft,Bft,xft,G4,Cpe,kft,Rft,yG,Sft,Pft,$ft,X4,Mpe,Ift,jft,wG,Dft,Nft,qft,V4,Epe,Oft,Gft,AG,Xft,Vft,zft,z4,ype,Wft,Qft,LG,Hft,Uft,Jft,W4,wpe,Yft,Kft,BG,Zft,egt,ogt,Q4,Ape,tgt,rgt,xG,agt,sgt,ngt,Lpe,lgt,igt,F0,zxe,Ic,H4,Bpe,C0,dgt,xpe,cgt,Wxe,yt,M0,mgt,jc,fgt,kpe,ggt,hgt,Rpe,ugt,pgt,_gt,E0,bgt,Spe,vgt,Tgt,Fgt,vr,y0,Cgt,Ppe,Mgt,Egt,Dc,ygt,$pe,wgt,Agt,Ipe,Lgt,Bgt,xgt,jpe,kgt,Rgt,w0,Sgt,Co,A0,Pgt,Dpe,$gt,Igt,Cs,jgt,Npe,Dgt,Ngt,qpe,qgt,Ogt,Ope,Ggt,Xgt,Vgt,Gpe,U4,Xpe,zgt,Wgt,kG,Qgt,Hgt,Ugt,Vpe,Jgt,Ygt,L0,Qxe,Nc,J4,zpe,B0,Kgt,Wpe,Zgt,Hxe,wt,x0,eht,qc,oht,Qpe,tht,rht,Hpe,aht,sht,nht,k0,lht,Upe,iht,dht,cht,Tr,R0,mht,Jpe,fht,ght,Oc,hht,Ype,uht,pht,Kpe,_ht,bht,vht,Zpe,Tht,Fht,S0,Cht,Mo,P0,Mht,e_e,Eht,yht,Ms,wht,o_e,Aht,Lht,t_e,Bht,xht,r_e,kht,Rht,Sht,K,Y4,a_e,Pht,$ht,RG,Iht,jht,Dht,K4,s_e,Nht,qht,SG,Oht,Ght,Xht,Z4,n_e,Vht,zht,PG,Wht,Qht,Hht,eM,l_e,Uht,Jht,$G,Yht,Kht,Zht,oM,i_e,eut,out,IG,tut,rut,aut,tM,d_e,sut,nut,jG,lut,iut,dut,rM,c_e,cut,mut,DG,fut,gut,hut,aM,m_e,uut,put,NG,_ut,but,vut,sM,f_e,Tut,Fut,qG,Cut,Mut,Eut,nM,g_e,yut,wut,OG,Aut,Lut,But,lM,h_e,xut,kut,GG,Rut,Sut,Put,iM,u_e,$ut,Iut,XG,jut,Dut,Nut,dM,p_e,qut,Out,VG,Gut,Xut,Vut,cM,__e,zut,Wut,zG,Qut,Hut,Uut,mM,b_e,Jut,Yut,WG,Kut,Zut,ept,fM,v_e,opt,tpt,QG,rpt,apt,spt,gM,T_e,npt,lpt,HG,ipt,dpt,cpt,hM,F_e,mpt,fpt,UG,gpt,hpt,upt,uM,C_e,ppt,_pt,JG,bpt,vpt,Tpt,pM,M_e,Fpt,Cpt,YG,Mpt,Ept,ypt,E_e,wpt,Apt,$0,Uxe,Gc,_M,y_e,I0,Lpt,w_e,Bpt,Jxe,At,j0,xpt,Xc,kpt,A_e,Rpt,Spt,L_e,Ppt,$pt,Ipt,D0,jpt,B_e,Dpt,Npt,qpt,Fr,N0,Opt,x_e,Gpt,Xpt,Vc,Vpt,k_e,zpt,Wpt,R_e,Qpt,Hpt,Upt,S_e,Jpt,Ypt,q0,Kpt,Eo,O0,Zpt,P_e,e_t,o_t,Es,t_t,$_e,r_t,a_t,I_e,s_t,n_t,j_e,l_t,i_t,d_t,Z,bM,D_e,c_t,m_t,KG,f_t,g_t,h_t,vM,N_e,u_t,p_t,ZG,__t,b_t,v_t,TM,q_e,T_t,F_t,eX,C_t,M_t,E_t,FM,O_e,y_t,w_t,oX,A_t,L_t,B_t,CM,G_e,x_t,k_t,tX,R_t,S_t,P_t,MM,X_e,$_t,I_t,rX,j_t,D_t,N_t,EM,V_e,q_t,O_t,aX,G_t,X_t,V_t,yM,z_e,z_t,W_t,sX,Q_t,H_t,U_t,wM,W_e,J_t,Y_t,nX,K_t,Z_t,ebt,AM,Q_e,obt,tbt,lX,rbt,abt,sbt,LM,H_e,nbt,lbt,iX,ibt,dbt,cbt,BM,U_e,mbt,fbt,dX,gbt,hbt,ubt,xM,J_e,pbt,_bt,cX,bbt,vbt,Tbt,kM,Y_e,Fbt,Cbt,mX,Mbt,Ebt,ybt,RM,K_e,wbt,Abt,fX,Lbt,Bbt,xbt,SM,Z_e,kbt,Rbt,gX,Sbt,Pbt,$bt,PM,ebe,Ibt,jbt,hX,Dbt,Nbt,qbt,$M,obe,Obt,Gbt,uX,Xbt,Vbt,zbt,IM,tbe,Wbt,Qbt,pX,Hbt,Ubt,Jbt,rbe,Ybt,Kbt,G0,Yxe,zc,jM,abe,X0,Zbt,sbe,e2t,Kxe,Lt,V0,o2t,Wc,t2t,nbe,r2t,a2t,lbe,s2t,n2t,l2t,z0,i2t,ibe,d2t,c2t,m2t,Cr,W0,f2t,dbe,g2t,h2t,Qc,u2t,cbe,p2t,_2t,mbe,b2t,v2t,T2t,fbe,F2t,C2t,Q0,M2t,yo,H0,E2t,gbe,y2t,w2t,ys,A2t,hbe,L2t,B2t,ube,x2t,k2t,pbe,R2t,S2t,P2t,_be,DM,bbe,$2t,I2t,_X,j2t,D2t,N2t,vbe,q2t,O2t,U0,Zxe,Hc,NM,Tbe,J0,G2t,Fbe,X2t,eke,Bt,Y0,V2t,Uc,z2t,Cbe,W2t,Q2t,Mbe,H2t,U2t,J2t,K0,Y2t,Ebe,K2t,Z2t,evt,Mr,Z0,ovt,ybe,tvt,rvt,Jc,avt,wbe,svt,nvt,Abe,lvt,ivt,dvt,Lbe,cvt,mvt,eL,fvt,wo,oL,gvt,Bbe,hvt,uvt,ws,pvt,xbe,_vt,bvt,kbe,vvt,Tvt,Rbe,Fvt,Cvt,Mvt,Sbe,qM,Pbe,Evt,yvt,bX,wvt,Avt,Lvt,$be,Bvt,xvt,tL,oke,Yc,OM,Ibe,rL,kvt,jbe,Rvt,tke,xt,aL,Svt,Kc,Pvt,Dbe,$vt,Ivt,Nbe,jvt,Dvt,Nvt,sL,qvt,qbe,Ovt,Gvt,Xvt,Er,nL,Vvt,Obe,zvt,Wvt,Zc,Qvt,Gbe,Hvt,Uvt,Xbe,Jvt,Yvt,Kvt,Vbe,Zvt,eTt,lL,oTt,Ao,iL,tTt,zbe,rTt,aTt,As,sTt,Wbe,nTt,lTt,Qbe,iTt,dTt,Hbe,cTt,mTt,fTt,z,GM,Ube,gTt,hTt,vX,uTt,pTt,_Tt,XM,Jbe,bTt,vTt,TX,TTt,FTt,CTt,VM,Ybe,MTt,ETt,FX,yTt,wTt,ATt,zM,Kbe,LTt,BTt,CX,xTt,kTt,RTt,WM,Zbe,STt,PTt,MX,$Tt,ITt,jTt,QM,e2e,DTt,NTt,EX,qTt,OTt,GTt,HM,o2e,XTt,VTt,yX,zTt,WTt,QTt,UM,t2e,HTt,UTt,wX,JTt,YTt,KTt,JM,r2e,ZTt,e1t,AX,o1t,t1t,r1t,YM,a2e,a1t,s1t,LX,n1t,l1t,i1t,KM,s2e,d1t,c1t,BX,m1t,f1t,g1t,ZM,n2e,h1t,u1t,xX,p1t,_1t,b1t,eE,l2e,v1t,T1t,kX,F1t,C1t,M1t,oE,i2e,E1t,y1t,RX,w1t,A1t,L1t,tE,d2e,B1t,x1t,SX,k1t,R1t,S1t,rE,c2e,P1t,$1t,PX,I1t,j1t,D1t,aE,m2e,N1t,q1t,$X,O1t,G1t,X1t,sE,f2e,V1t,z1t,IX,W1t,Q1t,H1t,nE,g2e,U1t,J1t,jX,Y1t,K1t,Z1t,lE,h2e,eFt,oFt,DX,tFt,rFt,aFt,iE,u2e,sFt,nFt,NX,lFt,iFt,dFt,dE,p2e,cFt,mFt,qX,fFt,gFt,hFt,cE,_2e,uFt,pFt,OX,_Ft,bFt,vFt,mE,b2e,TFt,FFt,GX,CFt,MFt,EFt,v2e,yFt,wFt,dL,rke,em,fE,T2e,cL,AFt,F2e,LFt,ake,kt,mL,BFt,om,xFt,C2e,kFt,RFt,M2e,SFt,PFt,$Ft,fL,IFt,E2e,jFt,DFt,NFt,yr,gL,qFt,y2e,OFt,GFt,tm,XFt,w2e,VFt,zFt,A2e,WFt,QFt,HFt,L2e,UFt,JFt,hL,YFt,Lo,uL,KFt,B2e,ZFt,eCt,Ls,oCt,x2e,tCt,rCt,k2e,aCt,sCt,R2e,nCt,lCt,iCt,Bs,gE,S2e,dCt,cCt,XX,mCt,fCt,gCt,hE,P2e,hCt,uCt,VX,pCt,_Ct,bCt,uE,$2e,vCt,TCt,zX,FCt,CCt,MCt,pE,I2e,ECt,yCt,WX,wCt,ACt,LCt,j2e,BCt,xCt,pL,ske,rm,_E,D2e,_L,kCt,N2e,RCt,nke,Rt,bL,SCt,am,PCt,q2e,$Ct,ICt,O2e,jCt,DCt,NCt,vL,qCt,G2e,OCt,GCt,XCt,wr,TL,VCt,X2e,zCt,WCt,sm,QCt,V2e,HCt,UCt,z2e,JCt,YCt,KCt,W2e,ZCt,e4t,FL,o4t,Bo,CL,t4t,Q2e,r4t,a4t,xs,s4t,H2e,n4t,l4t,U2e,i4t,d4t,J2e,c4t,m4t,f4t,me,bE,Y2e,g4t,h4t,QX,u4t,p4t,_4t,vE,K2e,b4t,v4t,HX,T4t,F4t,C4t,TE,Z2e,M4t,E4t,UX,y4t,w4t,A4t,FE,eve,L4t,B4t,JX,x4t,k4t,R4t,CE,ove,S4t,P4t,YX,$4t,I4t,j4t,ME,tve,D4t,N4t,KX,q4t,O4t,G4t,EE,rve,X4t,V4t,ZX,z4t,W4t,Q4t,yE,ave,H4t,U4t,eV,J4t,Y4t,K4t,wE,sve,Z4t,eMt,oV,oMt,tMt,rMt,AE,nve,aMt,sMt,tV,nMt,lMt,iMt,LE,lve,dMt,cMt,rV,mMt,fMt,gMt,ive,hMt,uMt,ML,lke,nm,BE,dve,EL,pMt,cve,_Mt,ike,St,yL,bMt,lm,vMt,mve,TMt,FMt,fve,CMt,MMt,EMt,wL,yMt,gve,wMt,AMt,LMt,Ar,AL,BMt,hve,xMt,kMt,im,RMt,uve,SMt,PMt,pve,$Mt,IMt,jMt,_ve,DMt,NMt,LL,qMt,xo,BL,OMt,bve,GMt,XMt,ks,VMt,vve,zMt,WMt,Tve,QMt,HMt,Fve,UMt,JMt,YMt,ve,xE,Cve,KMt,ZMt,aV,eEt,oEt,tEt,kE,Mve,rEt,aEt,sV,sEt,nEt,lEt,RE,Eve,iEt,dEt,nV,cEt,mEt,fEt,SE,yve,gEt,hEt,lV,uEt,pEt,_Et,PE,wve,bEt,vEt,iV,TEt,FEt,CEt,$E,Ave,MEt,EEt,dV,yEt,wEt,AEt,IE,Lve,LEt,BEt,cV,xEt,kEt,REt,jE,Bve,SEt,PEt,mV,$Et,IEt,jEt,DE,xve,DEt,NEt,fV,qEt,OEt,GEt,kve,XEt,VEt,xL,dke,dm,NE,Rve,kL,zEt,Sve,WEt,cke,Pt,RL,QEt,cm,HEt,Pve,UEt,JEt,$ve,YEt,KEt,ZEt,SL,e3t,Ive,o3t,t3t,r3t,Lr,PL,a3t,jve,s3t,n3t,mm,l3t,Dve,i3t,d3t,Nve,c3t,m3t,f3t,qve,g3t,h3t,$L,u3t,ko,IL,p3t,Ove,_3t,b3t,Rs,v3t,Gve,T3t,F3t,Xve,C3t,M3t,Vve,E3t,y3t,w3t,Te,qE,zve,A3t,L3t,gV,B3t,x3t,k3t,OE,Wve,R3t,S3t,hV,P3t,$3t,I3t,GE,Qve,j3t,D3t,uV,N3t,q3t,O3t,XE,Hve,G3t,X3t,pV,V3t,z3t,W3t,VE,Uve,Q3t,H3t,_V,U3t,J3t,Y3t,zE,Jve,K3t,Z3t,bV,e5t,o5t,t5t,WE,Yve,r5t,a5t,vV,s5t,n5t,l5t,QE,Kve,i5t,d5t,TV,c5t,m5t,f5t,HE,Zve,g5t,h5t,FV,u5t,p5t,_5t,eTe,b5t,v5t,jL,mke,fm,UE,oTe,DL,T5t,tTe,F5t,fke,$t,NL,C5t,gm,M5t,rTe,E5t,y5t,aTe,w5t,A5t,L5t,qL,B5t,sTe,x5t,k5t,R5t,Br,OL,S5t,nTe,P5t,$5t,hm,I5t,lTe,j5t,D5t,iTe,N5t,q5t,O5t,dTe,G5t,X5t,GL,V5t,Ro,XL,z5t,cTe,W5t,Q5t,Ss,H5t,mTe,U5t,J5t,fTe,Y5t,K5t,gTe,Z5t,eyt,oyt,Fe,JE,hTe,tyt,ryt,CV,ayt,syt,nyt,YE,uTe,lyt,iyt,MV,dyt,cyt,myt,KE,pTe,fyt,gyt,EV,hyt,uyt,pyt,ZE,_Te,_yt,byt,yV,vyt,Tyt,Fyt,e3,bTe,Cyt,Myt,wV,Eyt,yyt,wyt,o3,vTe,Ayt,Lyt,AV,Byt,xyt,kyt,t3,TTe,Ryt,Syt,LV,Pyt,$yt,Iyt,r3,FTe,jyt,Dyt,BV,Nyt,qyt,Oyt,a3,CTe,Gyt,Xyt,xV,Vyt,zyt,Wyt,MTe,Qyt,Hyt,VL,gke,um,s3,ETe,zL,Uyt,yTe,Jyt,hke,It,WL,Yyt,pm,Kyt,wTe,Zyt,ewt,ATe,owt,twt,rwt,QL,awt,LTe,swt,nwt,lwt,xr,HL,iwt,BTe,dwt,cwt,_m,mwt,xTe,fwt,gwt,kTe,hwt,uwt,pwt,RTe,_wt,bwt,UL,vwt,So,JL,Twt,STe,Fwt,Cwt,Ps,Mwt,PTe,Ewt,ywt,$Te,wwt,Awt,ITe,Lwt,Bwt,xwt,Ce,n3,jTe,kwt,Rwt,kV,Swt,Pwt,$wt,l3,DTe,Iwt,jwt,RV,Dwt,Nwt,qwt,i3,NTe,Owt,Gwt,SV,Xwt,Vwt,zwt,d3,qTe,Wwt,Qwt,PV,Hwt,Uwt,Jwt,c3,OTe,Ywt,Kwt,$V,Zwt,e6t,o6t,m3,GTe,t6t,r6t,IV,a6t,s6t,n6t,f3,XTe,l6t,i6t,jV,d6t,c6t,m6t,g3,VTe,f6t,g6t,DV,h6t,u6t,p6t,h3,zTe,_6t,b6t,NV,v6t,T6t,F6t,WTe,C6t,M6t,YL,uke,bm,u3,QTe,KL,E6t,HTe,y6t,pke,jt,ZL,w6t,vm,A6t,UTe,L6t,B6t,JTe,x6t,k6t,R6t,e8,S6t,YTe,P6t,$6t,I6t,kr,o8,j6t,KTe,D6t,N6t,Tm,q6t,ZTe,O6t,G6t,e1e,X6t,V6t,z6t,o1e,W6t,Q6t,t8,H6t,Po,r8,U6t,t1e,J6t,Y6t,$s,K6t,r1e,Z6t,eAt,a1e,oAt,tAt,s1e,rAt,aAt,sAt,lo,p3,n1e,nAt,lAt,qV,iAt,dAt,cAt,_3,l1e,mAt,fAt,OV,gAt,hAt,uAt,b3,i1e,pAt,_At,GV,bAt,vAt,TAt,v3,d1e,FAt,CAt,XV,MAt,EAt,yAt,T3,c1e,wAt,AAt,VV,LAt,BAt,xAt,F3,m1e,kAt,RAt,zV,SAt,PAt,$At,C3,f1e,IAt,jAt,WV,DAt,NAt,qAt,g1e,OAt,GAt,a8,_ke,Fm,M3,h1e,s8,XAt,u1e,VAt,bke,Dt,n8,zAt,Cm,WAt,p1e,QAt,HAt,_1e,UAt,JAt,YAt,l8,KAt,b1e,ZAt,e0t,o0t,Rr,i8,t0t,v1e,r0t,a0t,Mm,s0t,T1e,n0t,l0t,F1e,i0t,d0t,c0t,C1e,m0t,f0t,d8,g0t,$o,c8,h0t,M1e,u0t,p0t,Is,_0t,E1e,b0t,v0t,y1e,T0t,F0t,w1e,C0t,M0t,E0t,io,E3,A1e,y0t,w0t,QV,A0t,L0t,B0t,y3,L1e,x0t,k0t,HV,R0t,S0t,P0t,w3,B1e,$0t,I0t,UV,j0t,D0t,N0t,A3,x1e,q0t,O0t,JV,G0t,X0t,V0t,L3,k1e,z0t,W0t,YV,Q0t,H0t,U0t,B3,R1e,J0t,Y0t,KV,K0t,Z0t,eLt,x3,S1e,oLt,tLt,ZV,rLt,aLt,sLt,P1e,nLt,lLt,m8,vke,Em,k3,$1e,f8,iLt,I1e,dLt,Tke,Nt,g8,cLt,ym,mLt,j1e,fLt,gLt,D1e,hLt,uLt,pLt,h8,_Lt,N1e,bLt,vLt,TLt,Sr,u8,FLt,q1e,CLt,MLt,wm,ELt,O1e,yLt,wLt,G1e,ALt,LLt,BLt,X1e,xLt,kLt,p8,RLt,Io,_8,SLt,V1e,PLt,$Lt,js,ILt,z1e,jLt,DLt,W1e,NLt,qLt,Q1e,OLt,GLt,XLt,H1e,R3,U1e,VLt,zLt,ez,WLt,QLt,HLt,J1e,ULt,JLt,b8,Fke,Am,S3,Y1e,v8,YLt,K1e,KLt,Cke,qt,T8,ZLt,Lm,e8t,Z1e,o8t,t8t,eFe,r8t,a8t,s8t,F8,n8t,oFe,l8t,i8t,d8t,Pr,C8,c8t,tFe,m8t,f8t,Bm,g8t,rFe,h8t,u8t,aFe,p8t,_8t,b8t,sFe,v8t,T8t,M8,F8t,jo,E8,C8t,nFe,M8t,E8t,Ds,y8t,lFe,w8t,A8t,iFe,L8t,B8t,dFe,x8t,k8t,R8t,y8,P3,cFe,S8t,P8t,oz,$8t,I8t,j8t,$3,mFe,D8t,N8t,tz,q8t,O8t,G8t,fFe,X8t,V8t,w8,Mke,xm,I3,gFe,A8,z8t,hFe,W8t,Eke,Ot,L8,Q8t,km,H8t,uFe,U8t,J8t,pFe,Y8t,K8t,Z8t,B8,e7t,_Fe,o7t,t7t,r7t,$r,x8,a7t,bFe,s7t,n7t,Rm,l7t,vFe,i7t,d7t,TFe,c7t,m7t,f7t,FFe,g7t,h7t,k8,u7t,Do,R8,p7t,CFe,_7t,b7t,Ns,v7t,MFe,T7t,F7t,EFe,C7t,M7t,yFe,E7t,y7t,w7t,wFe,j3,AFe,A7t,L7t,rz,B7t,x7t,k7t,LFe,R7t,S7t,S8,yke;return ce=new X({}),Na=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased")',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),G5=new X({}),X5=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Nm=new P7t({props:{warning:"&lcub;true}",$$slots:{default:[H1r]},$$scope:{ctx:$i}}}),V5=new X({}),z5=new M({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L526"}}),H5=new M({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L549",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),U5=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),J5=new M({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L671",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),Y5=new X({}),K5=new M({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L352"}}),oy=new M({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L366",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),ty=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),ry=new M({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L562",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),ay=new X({}),sy=new M({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L170"}}),iy=new M({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L184",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Mh=new P7t({props:{$$slots:{default:[U1r]},$$scope:{ctx:$i}}}),dy=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),cy=new M({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L311",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),my=new X({}),fy=new M({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L71"}}),uy=new M({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Ph=new P7t({props:{$$slots:{default:[J1r]},$$scope:{ctx:$i}}}),py=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),_y=new M({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),by=new X({}),vy=new M({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L697"}}),Fy=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),My=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ey=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yy=new X({}),wy=new M({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L704"}}),Ly=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),By=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),xy=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ky=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ry=new X({}),Sy=new M({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L719"}}),$y=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),jy=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ny=new X({}),qy=new M({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L726"}}),Gy=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Xy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),Vy=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Wy=new X({}),Qy=new M({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L733"}}),Uy=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),Jy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),Yy=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ky=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Zy=new X({}),ew=new M({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L742"}}),tw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),aw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nw=new X({}),lw=new M({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L776"}}),dw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),cw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),mw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gw=new X({}),hw=new M({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L783"}}),pw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),_w=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),bw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vw=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Tw=new X({}),Fw=new M({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L769"}}),Mw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Ew=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),yw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ww=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Aw=new X({}),Lw=new M({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L751"}}),xw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),kw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),Rw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Sw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Pw=new X({}),$w=new M({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L758"}}),jw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),Nw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ow=new X({}),Gw=new M({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L792"}}),Vw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),zw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),Ww=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Qw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Hw=new X({}),Uw=new M({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L831"}}),Yw=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Kw=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),Zw=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),e6=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),o6=new X({}),t6=new M({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L838"}}),a6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),s6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),n6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),l6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),i6=new X({}),d6=new M({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L861"}}),m6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),f6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),g6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),u6=new X({}),p6=new M({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L845"}}),b6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),v6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),T6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C6=new X({}),M6=new M({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L852"}}),y6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),w6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),A6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),B6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),x6=new X({}),k6=new M({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L870"}}),S6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),P6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),$6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),I6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),j6=new X({}),D6=new M({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L877"}}),q6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),O6=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),G6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X6=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),V6=new X({}),z6=new M({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L824"}}),Q6=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),H6=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),U6=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J6=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y6=new X({}),K6=new M({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L799"}}),eA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),oA=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),tA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rA=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aA=new X({}),sA=new M({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L806"}}),lA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),iA=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),dA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mA=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fA=new X({}),gA=new M({props:{name:"class transformers.AutoModelForInstanceSegmentation",anchor:"transformers.AutoModelForInstanceSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L815"}}),uA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>
</ul>`,name:"config"}]}}),pA=new w({props:{code:`from transformers import AutoConfig, AutoModelForInstanceSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForInstanceSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_config(config)`}}),_A=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, AutoModelForInstanceSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForInstanceSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForInstanceSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForInstanceSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),vA=new X({}),TA=new M({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L373"}}),CA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),MA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),EA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),wA=new X({}),AA=new M({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L380"}}),BA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),xA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),kA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),RA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),SA=new X({}),PA=new M({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L395"}}),IA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),jA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),DA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),NA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qA=new X({}),OA=new M({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L402"}}),XA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),VA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),zA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),QA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),HA=new X({}),UA=new M({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L416"}}),YA=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),KA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),ZA=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),e0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),o0=new X({}),t0=new M({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L423"}}),a0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),s0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),n0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),l0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),i0=new X({}),d0=new M({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),m0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),f0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),g0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),u0=new X({}),p0=new M({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L468"}}),b0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),v0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),T0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C0=new X({}),M0=new M({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L448"}}),y0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),w0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),A0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B0=new X({}),x0=new M({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L459"}}),R0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),S0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),P0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I0=new X({}),j0=new M({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),N0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),q0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),O0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),G0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),X0=new X({}),V0=new M({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L409"}}),W0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Q0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),H0=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),U0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),J0=new X({}),Y0=new M({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L484"}}),Z0=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),eL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),oL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tL=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rL=new X({}),aL=new M({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L229"}}),nL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),lL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),iL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cL=new X({}),mL=new M({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L243"}}),gL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),hL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),uL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_L=new X({}),bL=new M({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L236"}}),TL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),FL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),CL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ML=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),EL=new X({}),yL=new M({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L250"}}),AL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),LL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),BL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),kL=new X({}),RL=new M({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),PL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),$L=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),IL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),jL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DL=new X({}),NL=new M({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),OL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),GL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),XL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),VL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zL=new X({}),WL=new M({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L275"}}),HL=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),UL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),JL=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),YL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),KL=new X({}),ZL=new M({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),o8=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),t8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),r8=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),s8=new X({}),n8=new M({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L291"}}),i8=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),d8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),c8=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),f8=new X({}),g8=new M({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),u8=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),p8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),_8=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v8=new X({}),T8=new M({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),C8=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),M8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),E8=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),w8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),A8=new X({}),L8=new M({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L316"}}),x8=new M({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),k8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),R8=new M({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),S8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Be=l(),de=a("h1"),fe=a("a"),no=a("span"),m(ce.$$.fragment),_e=l(),Go=a("span"),Ii=o("Auto Classes"),Pm=l(),ca=a("p"),ji=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Di=a("code"),D5=o("from_pretrained()"),$m=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),co=a("p"),Ni=o("Instantiating one of "),qs=a("a"),N5=o("AutoConfig"),Os=o(", "),Gs=a("a"),q5=o("AutoModel"),qi=o(`, and
`),Xs=a("a"),O5=o("AutoTokenizer"),Oi=o(" will directly create a class of the relevant architecture. For instance"),Im=l(),m(Na.$$.fragment),mo=l(),ge=a("p"),k7=o("will create a model that is an instance of "),Gi=a("a"),R7=o("BertModel"),S7=o("."),Xo=l(),qa=a("p"),P7=o("There is one class of "),jm=a("code"),$7=o("AutoModel"),DSe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),EBe=l(),Xi=a("h2"),Dm=a("a"),JW=a("span"),m(G5.$$.fragment),NSe=l(),YW=a("span"),qSe=o("Extending the Auto Classes"),yBe=l(),Vs=a("p"),OSe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),KW=a("code"),GSe=o("NewModel"),XSe=o(", make sure you have a "),ZW=a("code"),VSe=o("NewModelConfig"),zSe=o(` then you can add those to the auto
classes like this:`),wBe=l(),m(X5.$$.fragment),ABe=l(),I7=a("p"),WSe=o("You will then be able to use the auto classes like you would usually do!"),LBe=l(),m(Nm.$$.fragment),BBe=l(),Vi=a("h2"),qm=a("a"),eQ=a("span"),m(V5.$$.fragment),QSe=l(),oQ=a("span"),HSe=o("AutoConfig"),xBe=l(),Vo=a("div"),m(z5.$$.fragment),USe=l(),W5=a("p"),JSe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),j7=a("a"),YSe=o("from_pretrained()"),KSe=o(" class method."),ZSe=l(),Q5=a("p"),ePe=o("This class cannot be instantiated directly using "),tQ=a("code"),oPe=o("__init__()"),tPe=o(" (throws an error)."),rPe=l(),fo=a("div"),m(H5.$$.fragment),aPe=l(),rQ=a("p"),sPe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),nPe=l(),zi=a("p"),lPe=o("The configuration class to instantiate is selected based on the "),aQ=a("code"),iPe=o("model_type"),dPe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),sQ=a("code"),cPe=o("pretrained_model_name_or_path"),mPe=o(":"),fPe=l(),v=a("ul"),Om=a("li"),nQ=a("strong"),gPe=o("albert"),hPe=o(" \u2014 "),D7=a("a"),uPe=o("AlbertConfig"),pPe=o(" (ALBERT model)"),_Pe=l(),Gm=a("li"),lQ=a("strong"),bPe=o("bart"),vPe=o(" \u2014 "),N7=a("a"),TPe=o("BartConfig"),FPe=o(" (BART model)"),CPe=l(),Xm=a("li"),iQ=a("strong"),MPe=o("beit"),EPe=o(" \u2014 "),q7=a("a"),yPe=o("BeitConfig"),wPe=o(" (BEiT model)"),APe=l(),Vm=a("li"),dQ=a("strong"),LPe=o("bert"),BPe=o(" \u2014 "),O7=a("a"),xPe=o("BertConfig"),kPe=o(" (BERT model)"),RPe=l(),zm=a("li"),cQ=a("strong"),SPe=o("bert-generation"),PPe=o(" \u2014 "),G7=a("a"),$Pe=o("BertGenerationConfig"),IPe=o(" (Bert Generation model)"),jPe=l(),Wm=a("li"),mQ=a("strong"),DPe=o("big_bird"),NPe=o(" \u2014 "),X7=a("a"),qPe=o("BigBirdConfig"),OPe=o(" (BigBird model)"),GPe=l(),Qm=a("li"),fQ=a("strong"),XPe=o("bigbird_pegasus"),VPe=o(" \u2014 "),V7=a("a"),zPe=o("BigBirdPegasusConfig"),WPe=o(" (BigBirdPegasus model)"),QPe=l(),Hm=a("li"),gQ=a("strong"),HPe=o("blenderbot"),UPe=o(" \u2014 "),z7=a("a"),JPe=o("BlenderbotConfig"),YPe=o(" (Blenderbot model)"),KPe=l(),Um=a("li"),hQ=a("strong"),ZPe=o("blenderbot-small"),e$e=o(" \u2014 "),W7=a("a"),o$e=o("BlenderbotSmallConfig"),t$e=o(" (BlenderbotSmall model)"),r$e=l(),Jm=a("li"),uQ=a("strong"),a$e=o("camembert"),s$e=o(" \u2014 "),Q7=a("a"),n$e=o("CamembertConfig"),l$e=o(" (CamemBERT model)"),i$e=l(),Ym=a("li"),pQ=a("strong"),d$e=o("canine"),c$e=o(" \u2014 "),H7=a("a"),m$e=o("CanineConfig"),f$e=o(" (Canine model)"),g$e=l(),Km=a("li"),_Q=a("strong"),h$e=o("clip"),u$e=o(" \u2014 "),U7=a("a"),p$e=o("CLIPConfig"),_$e=o(" (CLIP model)"),b$e=l(),Zm=a("li"),bQ=a("strong"),v$e=o("convbert"),T$e=o(" \u2014 "),J7=a("a"),F$e=o("ConvBertConfig"),C$e=o(" (ConvBERT model)"),M$e=l(),ef=a("li"),vQ=a("strong"),E$e=o("convnext"),y$e=o(" \u2014 "),Y7=a("a"),w$e=o("ConvNextConfig"),A$e=o(" (ConvNext model)"),L$e=l(),of=a("li"),TQ=a("strong"),B$e=o("ctrl"),x$e=o(" \u2014 "),K7=a("a"),k$e=o("CTRLConfig"),R$e=o(" (CTRL model)"),S$e=l(),tf=a("li"),FQ=a("strong"),P$e=o("data2vec-audio"),$$e=o(" \u2014 "),Z7=a("a"),I$e=o("Data2VecAudioConfig"),j$e=o(" (Data2VecAudio model)"),D$e=l(),rf=a("li"),CQ=a("strong"),N$e=o("data2vec-text"),q$e=o(" \u2014 "),e9=a("a"),O$e=o("Data2VecTextConfig"),G$e=o(" (Data2VecText model)"),X$e=l(),af=a("li"),MQ=a("strong"),V$e=o("deberta"),z$e=o(" \u2014 "),o9=a("a"),W$e=o("DebertaConfig"),Q$e=o(" (DeBERTa model)"),H$e=l(),sf=a("li"),EQ=a("strong"),U$e=o("deberta-v2"),J$e=o(" \u2014 "),t9=a("a"),Y$e=o("DebertaV2Config"),K$e=o(" (DeBERTa-v2 model)"),Z$e=l(),nf=a("li"),yQ=a("strong"),eIe=o("deit"),oIe=o(" \u2014 "),r9=a("a"),tIe=o("DeiTConfig"),rIe=o(" (DeiT model)"),aIe=l(),lf=a("li"),wQ=a("strong"),sIe=o("detr"),nIe=o(" \u2014 "),a9=a("a"),lIe=o("DetrConfig"),iIe=o(" (DETR model)"),dIe=l(),df=a("li"),AQ=a("strong"),cIe=o("distilbert"),mIe=o(" \u2014 "),s9=a("a"),fIe=o("DistilBertConfig"),gIe=o(" (DistilBERT model)"),hIe=l(),cf=a("li"),LQ=a("strong"),uIe=o("dpr"),pIe=o(" \u2014 "),n9=a("a"),_Ie=o("DPRConfig"),bIe=o(" (DPR model)"),vIe=l(),mf=a("li"),BQ=a("strong"),TIe=o("electra"),FIe=o(" \u2014 "),l9=a("a"),CIe=o("ElectraConfig"),MIe=o(" (ELECTRA model)"),EIe=l(),ff=a("li"),xQ=a("strong"),yIe=o("encoder-decoder"),wIe=o(" \u2014 "),i9=a("a"),AIe=o("EncoderDecoderConfig"),LIe=o(" (Encoder decoder model)"),BIe=l(),gf=a("li"),kQ=a("strong"),xIe=o("flaubert"),kIe=o(" \u2014 "),d9=a("a"),RIe=o("FlaubertConfig"),SIe=o(" (FlauBERT model)"),PIe=l(),hf=a("li"),RQ=a("strong"),$Ie=o("fnet"),IIe=o(" \u2014 "),c9=a("a"),jIe=o("FNetConfig"),DIe=o(" (FNet model)"),NIe=l(),uf=a("li"),SQ=a("strong"),qIe=o("fsmt"),OIe=o(" \u2014 "),m9=a("a"),GIe=o("FSMTConfig"),XIe=o(" (FairSeq Machine-Translation model)"),VIe=l(),pf=a("li"),PQ=a("strong"),zIe=o("funnel"),WIe=o(" \u2014 "),f9=a("a"),QIe=o("FunnelConfig"),HIe=o(" (Funnel Transformer model)"),UIe=l(),_f=a("li"),$Q=a("strong"),JIe=o("gpt2"),YIe=o(" \u2014 "),g9=a("a"),KIe=o("GPT2Config"),ZIe=o(" (OpenAI GPT-2 model)"),eje=l(),bf=a("li"),IQ=a("strong"),oje=o("gpt_neo"),tje=o(" \u2014 "),h9=a("a"),rje=o("GPTNeoConfig"),aje=o(" (GPT Neo model)"),sje=l(),vf=a("li"),jQ=a("strong"),nje=o("gptj"),lje=o(" \u2014 "),u9=a("a"),ije=o("GPTJConfig"),dje=o(" (GPT-J model)"),cje=l(),Tf=a("li"),DQ=a("strong"),mje=o("hubert"),fje=o(" \u2014 "),p9=a("a"),gje=o("HubertConfig"),hje=o(" (Hubert model)"),uje=l(),Ff=a("li"),NQ=a("strong"),pje=o("ibert"),_je=o(" \u2014 "),_9=a("a"),bje=o("IBertConfig"),vje=o(" (I-BERT model)"),Tje=l(),Cf=a("li"),qQ=a("strong"),Fje=o("imagegpt"),Cje=o(" \u2014 "),b9=a("a"),Mje=o("ImageGPTConfig"),Eje=o(" (ImageGPT model)"),yje=l(),Mf=a("li"),OQ=a("strong"),wje=o("layoutlm"),Aje=o(" \u2014 "),v9=a("a"),Lje=o("LayoutLMConfig"),Bje=o(" (LayoutLM model)"),xje=l(),Ef=a("li"),GQ=a("strong"),kje=o("layoutlmv2"),Rje=o(" \u2014 "),T9=a("a"),Sje=o("LayoutLMv2Config"),Pje=o(" (LayoutLMv2 model)"),$je=l(),yf=a("li"),XQ=a("strong"),Ije=o("led"),jje=o(" \u2014 "),F9=a("a"),Dje=o("LEDConfig"),Nje=o(" (LED model)"),qje=l(),wf=a("li"),VQ=a("strong"),Oje=o("longformer"),Gje=o(" \u2014 "),C9=a("a"),Xje=o("LongformerConfig"),Vje=o(" (Longformer model)"),zje=l(),Af=a("li"),zQ=a("strong"),Wje=o("luke"),Qje=o(" \u2014 "),M9=a("a"),Hje=o("LukeConfig"),Uje=o(" (LUKE model)"),Jje=l(),Lf=a("li"),WQ=a("strong"),Yje=o("lxmert"),Kje=o(" \u2014 "),E9=a("a"),Zje=o("LxmertConfig"),eDe=o(" (LXMERT model)"),oDe=l(),Bf=a("li"),QQ=a("strong"),tDe=o("m2m_100"),rDe=o(" \u2014 "),y9=a("a"),aDe=o("M2M100Config"),sDe=o(" (M2M100 model)"),nDe=l(),xf=a("li"),HQ=a("strong"),lDe=o("marian"),iDe=o(" \u2014 "),w9=a("a"),dDe=o("MarianConfig"),cDe=o(" (Marian model)"),mDe=l(),kf=a("li"),UQ=a("strong"),fDe=o("maskformer"),gDe=o(" \u2014 "),A9=a("a"),hDe=o("MaskFormerConfig"),uDe=o(" (MaskFormer model)"),pDe=l(),Rf=a("li"),JQ=a("strong"),_De=o("mbart"),bDe=o(" \u2014 "),L9=a("a"),vDe=o("MBartConfig"),TDe=o(" (mBART model)"),FDe=l(),Sf=a("li"),YQ=a("strong"),CDe=o("megatron-bert"),MDe=o(" \u2014 "),B9=a("a"),EDe=o("MegatronBertConfig"),yDe=o(" (MegatronBert model)"),wDe=l(),Pf=a("li"),KQ=a("strong"),ADe=o("mobilebert"),LDe=o(" \u2014 "),x9=a("a"),BDe=o("MobileBertConfig"),xDe=o(" (MobileBERT model)"),kDe=l(),$f=a("li"),ZQ=a("strong"),RDe=o("mpnet"),SDe=o(" \u2014 "),k9=a("a"),PDe=o("MPNetConfig"),$De=o(" (MPNet model)"),IDe=l(),If=a("li"),eH=a("strong"),jDe=o("mt5"),DDe=o(" \u2014 "),R9=a("a"),NDe=o("MT5Config"),qDe=o(" (mT5 model)"),ODe=l(),jf=a("li"),oH=a("strong"),GDe=o("nystromformer"),XDe=o(" \u2014 "),S9=a("a"),VDe=o("NystromformerConfig"),zDe=o(" (Nystromformer model)"),WDe=l(),Df=a("li"),tH=a("strong"),QDe=o("openai-gpt"),HDe=o(" \u2014 "),P9=a("a"),UDe=o("OpenAIGPTConfig"),JDe=o(" (OpenAI GPT model)"),YDe=l(),Nf=a("li"),rH=a("strong"),KDe=o("pegasus"),ZDe=o(" \u2014 "),$9=a("a"),eNe=o("PegasusConfig"),oNe=o(" (Pegasus model)"),tNe=l(),qf=a("li"),aH=a("strong"),rNe=o("perceiver"),aNe=o(" \u2014 "),I9=a("a"),sNe=o("PerceiverConfig"),nNe=o(" (Perceiver model)"),lNe=l(),Of=a("li"),sH=a("strong"),iNe=o("plbart"),dNe=o(" \u2014 "),j9=a("a"),cNe=o("PLBartConfig"),mNe=o(" (PLBart model)"),fNe=l(),Gf=a("li"),nH=a("strong"),gNe=o("poolformer"),hNe=o(" \u2014 "),D9=a("a"),uNe=o("PoolFormerConfig"),pNe=o(" (PoolFormer model)"),_Ne=l(),Xf=a("li"),lH=a("strong"),bNe=o("prophetnet"),vNe=o(" \u2014 "),N9=a("a"),TNe=o("ProphetNetConfig"),FNe=o(" (ProphetNet model)"),CNe=l(),Vf=a("li"),iH=a("strong"),MNe=o("qdqbert"),ENe=o(" \u2014 "),q9=a("a"),yNe=o("QDQBertConfig"),wNe=o(" (QDQBert model)"),ANe=l(),zf=a("li"),dH=a("strong"),LNe=o("rag"),BNe=o(" \u2014 "),O9=a("a"),xNe=o("RagConfig"),kNe=o(" (RAG model)"),RNe=l(),Wf=a("li"),cH=a("strong"),SNe=o("realm"),PNe=o(" \u2014 "),G9=a("a"),$Ne=o("RealmConfig"),INe=o(" (Realm model)"),jNe=l(),Qf=a("li"),mH=a("strong"),DNe=o("reformer"),NNe=o(" \u2014 "),X9=a("a"),qNe=o("ReformerConfig"),ONe=o(" (Reformer model)"),GNe=l(),Hf=a("li"),fH=a("strong"),XNe=o("rembert"),VNe=o(" \u2014 "),V9=a("a"),zNe=o("RemBertConfig"),WNe=o(" (RemBERT model)"),QNe=l(),Uf=a("li"),gH=a("strong"),HNe=o("retribert"),UNe=o(" \u2014 "),z9=a("a"),JNe=o("RetriBertConfig"),YNe=o(" (RetriBERT model)"),KNe=l(),Jf=a("li"),hH=a("strong"),ZNe=o("roberta"),eqe=o(" \u2014 "),W9=a("a"),oqe=o("RobertaConfig"),tqe=o(" (RoBERTa model)"),rqe=l(),Yf=a("li"),uH=a("strong"),aqe=o("roformer"),sqe=o(" \u2014 "),Q9=a("a"),nqe=o("RoFormerConfig"),lqe=o(" (RoFormer model)"),iqe=l(),Kf=a("li"),pH=a("strong"),dqe=o("segformer"),cqe=o(" \u2014 "),H9=a("a"),mqe=o("SegformerConfig"),fqe=o(" (SegFormer model)"),gqe=l(),Zf=a("li"),_H=a("strong"),hqe=o("sew"),uqe=o(" \u2014 "),U9=a("a"),pqe=o("SEWConfig"),_qe=o(" (SEW model)"),bqe=l(),eg=a("li"),bH=a("strong"),vqe=o("sew-d"),Tqe=o(" \u2014 "),J9=a("a"),Fqe=o("SEWDConfig"),Cqe=o(" (SEW-D model)"),Mqe=l(),og=a("li"),vH=a("strong"),Eqe=o("speech-encoder-decoder"),yqe=o(" \u2014 "),Y9=a("a"),wqe=o("SpeechEncoderDecoderConfig"),Aqe=o(" (Speech Encoder decoder model)"),Lqe=l(),tg=a("li"),TH=a("strong"),Bqe=o("speech_to_text"),xqe=o(" \u2014 "),K9=a("a"),kqe=o("Speech2TextConfig"),Rqe=o(" (Speech2Text model)"),Sqe=l(),rg=a("li"),FH=a("strong"),Pqe=o("speech_to_text_2"),$qe=o(" \u2014 "),Z9=a("a"),Iqe=o("Speech2Text2Config"),jqe=o(" (Speech2Text2 model)"),Dqe=l(),ag=a("li"),CH=a("strong"),Nqe=o("splinter"),qqe=o(" \u2014 "),eB=a("a"),Oqe=o("SplinterConfig"),Gqe=o(" (Splinter model)"),Xqe=l(),sg=a("li"),MH=a("strong"),Vqe=o("squeezebert"),zqe=o(" \u2014 "),oB=a("a"),Wqe=o("SqueezeBertConfig"),Qqe=o(" (SqueezeBERT model)"),Hqe=l(),ng=a("li"),EH=a("strong"),Uqe=o("swin"),Jqe=o(" \u2014 "),tB=a("a"),Yqe=o("SwinConfig"),Kqe=o(" (Swin model)"),Zqe=l(),lg=a("li"),yH=a("strong"),eOe=o("t5"),oOe=o(" \u2014 "),rB=a("a"),tOe=o("T5Config"),rOe=o(" (T5 model)"),aOe=l(),ig=a("li"),wH=a("strong"),sOe=o("tapas"),nOe=o(" \u2014 "),aB=a("a"),lOe=o("TapasConfig"),iOe=o(" (TAPAS model)"),dOe=l(),dg=a("li"),AH=a("strong"),cOe=o("transfo-xl"),mOe=o(" \u2014 "),sB=a("a"),fOe=o("TransfoXLConfig"),gOe=o(" (Transformer-XL model)"),hOe=l(),cg=a("li"),LH=a("strong"),uOe=o("trocr"),pOe=o(" \u2014 "),nB=a("a"),_Oe=o("TrOCRConfig"),bOe=o(" (TrOCR model)"),vOe=l(),mg=a("li"),BH=a("strong"),TOe=o("unispeech"),FOe=o(" \u2014 "),lB=a("a"),COe=o("UniSpeechConfig"),MOe=o(" (UniSpeech model)"),EOe=l(),fg=a("li"),xH=a("strong"),yOe=o("unispeech-sat"),wOe=o(" \u2014 "),iB=a("a"),AOe=o("UniSpeechSatConfig"),LOe=o(" (UniSpeechSat model)"),BOe=l(),gg=a("li"),kH=a("strong"),xOe=o("vilt"),kOe=o(" \u2014 "),dB=a("a"),ROe=o("ViltConfig"),SOe=o(" (ViLT model)"),POe=l(),hg=a("li"),RH=a("strong"),$Oe=o("vision-encoder-decoder"),IOe=o(" \u2014 "),cB=a("a"),jOe=o("VisionEncoderDecoderConfig"),DOe=o(" (Vision Encoder decoder model)"),NOe=l(),ug=a("li"),SH=a("strong"),qOe=o("vision-text-dual-encoder"),OOe=o(" \u2014 "),mB=a("a"),GOe=o("VisionTextDualEncoderConfig"),XOe=o(" (VisionTextDualEncoder model)"),VOe=l(),pg=a("li"),PH=a("strong"),zOe=o("visual_bert"),WOe=o(" \u2014 "),fB=a("a"),QOe=o("VisualBertConfig"),HOe=o(" (VisualBert model)"),UOe=l(),_g=a("li"),$H=a("strong"),JOe=o("vit"),YOe=o(" \u2014 "),gB=a("a"),KOe=o("ViTConfig"),ZOe=o(" (ViT model)"),eGe=l(),bg=a("li"),IH=a("strong"),oGe=o("vit_mae"),tGe=o(" \u2014 "),hB=a("a"),rGe=o("ViTMAEConfig"),aGe=o(" (ViTMAE model)"),sGe=l(),vg=a("li"),jH=a("strong"),nGe=o("wav2vec2"),lGe=o(" \u2014 "),uB=a("a"),iGe=o("Wav2Vec2Config"),dGe=o(" (Wav2Vec2 model)"),cGe=l(),Tg=a("li"),DH=a("strong"),mGe=o("wavlm"),fGe=o(" \u2014 "),pB=a("a"),gGe=o("WavLMConfig"),hGe=o(" (WavLM model)"),uGe=l(),Fg=a("li"),NH=a("strong"),pGe=o("xglm"),_Ge=o(" \u2014 "),_B=a("a"),bGe=o("XGLMConfig"),vGe=o(" (XGLM model)"),TGe=l(),Cg=a("li"),qH=a("strong"),FGe=o("xlm"),CGe=o(" \u2014 "),bB=a("a"),MGe=o("XLMConfig"),EGe=o(" (XLM model)"),yGe=l(),Mg=a("li"),OH=a("strong"),wGe=o("xlm-prophetnet"),AGe=o(" \u2014 "),vB=a("a"),LGe=o("XLMProphetNetConfig"),BGe=o(" (XLMProphetNet model)"),xGe=l(),Eg=a("li"),GH=a("strong"),kGe=o("xlm-roberta"),RGe=o(" \u2014 "),TB=a("a"),SGe=o("XLMRobertaConfig"),PGe=o(" (XLM-RoBERTa model)"),$Ge=l(),yg=a("li"),XH=a("strong"),IGe=o("xlm-roberta-xl"),jGe=o(" \u2014 "),FB=a("a"),DGe=o("XLMRobertaXLConfig"),NGe=o(" (XLM-RoBERTa-XL model)"),qGe=l(),wg=a("li"),VH=a("strong"),OGe=o("xlnet"),GGe=o(" \u2014 "),CB=a("a"),XGe=o("XLNetConfig"),VGe=o(" (XLNet model)"),zGe=l(),Ag=a("li"),zH=a("strong"),WGe=o("yoso"),QGe=o(" \u2014 "),MB=a("a"),HGe=o("YosoConfig"),UGe=o(" (YOSO model)"),JGe=l(),WH=a("p"),YGe=o("Examples:"),KGe=l(),m(U5.$$.fragment),ZGe=l(),Lg=a("div"),m(J5.$$.fragment),eXe=l(),QH=a("p"),oXe=o("Register a new configuration for this class."),kBe=l(),Wi=a("h2"),Bg=a("a"),HH=a("span"),m(Y5.$$.fragment),tXe=l(),UH=a("span"),rXe=o("AutoTokenizer"),RBe=l(),zo=a("div"),m(K5.$$.fragment),aXe=l(),Z5=a("p"),sXe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),EB=a("a"),nXe=o("AutoTokenizer.from_pretrained()"),lXe=o(" class method."),iXe=l(),ey=a("p"),dXe=o("This class cannot be instantiated directly using "),JH=a("code"),cXe=o("__init__()"),mXe=o(" (throws an error)."),fXe=l(),go=a("div"),m(oy.$$.fragment),gXe=l(),YH=a("p"),hXe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),uXe=l(),Oa=a("p"),pXe=o("The tokenizer class to instantiate is selected based on the "),KH=a("code"),_Xe=o("model_type"),bXe=o(` property of the config object (either
passed as an argument or loaded from `),ZH=a("code"),vXe=o("pretrained_model_name_or_path"),TXe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eU=a("code"),FXe=o("pretrained_model_name_or_path"),CXe=o(":"),MXe=l(),E=a("ul"),zs=a("li"),oU=a("strong"),EXe=o("albert"),yXe=o(" \u2014 "),yB=a("a"),wXe=o("AlbertTokenizer"),AXe=o(" or "),wB=a("a"),LXe=o("AlbertTokenizerFast"),BXe=o(" (ALBERT model)"),xXe=l(),Ws=a("li"),tU=a("strong"),kXe=o("bart"),RXe=o(" \u2014 "),AB=a("a"),SXe=o("BartTokenizer"),PXe=o(" or "),LB=a("a"),$Xe=o("BartTokenizerFast"),IXe=o(" (BART model)"),jXe=l(),Qs=a("li"),rU=a("strong"),DXe=o("barthez"),NXe=o(" \u2014 "),BB=a("a"),qXe=o("BarthezTokenizer"),OXe=o(" or "),xB=a("a"),GXe=o("BarthezTokenizerFast"),XXe=o(" (BARThez model)"),VXe=l(),xg=a("li"),aU=a("strong"),zXe=o("bartpho"),WXe=o(" \u2014 "),kB=a("a"),QXe=o("BartphoTokenizer"),HXe=o(" (BARTpho model)"),UXe=l(),Hs=a("li"),sU=a("strong"),JXe=o("bert"),YXe=o(" \u2014 "),RB=a("a"),KXe=o("BertTokenizer"),ZXe=o(" or "),SB=a("a"),eVe=o("BertTokenizerFast"),oVe=o(" (BERT model)"),tVe=l(),kg=a("li"),nU=a("strong"),rVe=o("bert-generation"),aVe=o(" \u2014 "),PB=a("a"),sVe=o("BertGenerationTokenizer"),nVe=o(" (Bert Generation model)"),lVe=l(),Rg=a("li"),lU=a("strong"),iVe=o("bert-japanese"),dVe=o(" \u2014 "),$B=a("a"),cVe=o("BertJapaneseTokenizer"),mVe=o(" (BertJapanese model)"),fVe=l(),Sg=a("li"),iU=a("strong"),gVe=o("bertweet"),hVe=o(" \u2014 "),IB=a("a"),uVe=o("BertweetTokenizer"),pVe=o(" (Bertweet model)"),_Ve=l(),Us=a("li"),dU=a("strong"),bVe=o("big_bird"),vVe=o(" \u2014 "),jB=a("a"),TVe=o("BigBirdTokenizer"),FVe=o(" or "),DB=a("a"),CVe=o("BigBirdTokenizerFast"),MVe=o(" (BigBird model)"),EVe=l(),Js=a("li"),cU=a("strong"),yVe=o("bigbird_pegasus"),wVe=o(" \u2014 "),NB=a("a"),AVe=o("PegasusTokenizer"),LVe=o(" or "),qB=a("a"),BVe=o("PegasusTokenizerFast"),xVe=o(" (BigBirdPegasus model)"),kVe=l(),Ys=a("li"),mU=a("strong"),RVe=o("blenderbot"),SVe=o(" \u2014 "),OB=a("a"),PVe=o("BlenderbotTokenizer"),$Ve=o(" or "),GB=a("a"),IVe=o("BlenderbotTokenizerFast"),jVe=o(" (Blenderbot model)"),DVe=l(),Pg=a("li"),fU=a("strong"),NVe=o("blenderbot-small"),qVe=o(" \u2014 "),XB=a("a"),OVe=o("BlenderbotSmallTokenizer"),GVe=o(" (BlenderbotSmall model)"),XVe=l(),$g=a("li"),gU=a("strong"),VVe=o("byt5"),zVe=o(" \u2014 "),VB=a("a"),WVe=o("ByT5Tokenizer"),QVe=o(" (ByT5 model)"),HVe=l(),Ks=a("li"),hU=a("strong"),UVe=o("camembert"),JVe=o(" \u2014 "),zB=a("a"),YVe=o("CamembertTokenizer"),KVe=o(" or "),WB=a("a"),ZVe=o("CamembertTokenizerFast"),eze=o(" (CamemBERT model)"),oze=l(),Ig=a("li"),uU=a("strong"),tze=o("canine"),rze=o(" \u2014 "),QB=a("a"),aze=o("CanineTokenizer"),sze=o(" (Canine model)"),nze=l(),Zs=a("li"),pU=a("strong"),lze=o("clip"),ize=o(" \u2014 "),HB=a("a"),dze=o("CLIPTokenizer"),cze=o(" or "),UB=a("a"),mze=o("CLIPTokenizerFast"),fze=o(" (CLIP model)"),gze=l(),en=a("li"),_U=a("strong"),hze=o("convbert"),uze=o(" \u2014 "),JB=a("a"),pze=o("ConvBertTokenizer"),_ze=o(" or "),YB=a("a"),bze=o("ConvBertTokenizerFast"),vze=o(" (ConvBERT model)"),Tze=l(),on=a("li"),bU=a("strong"),Fze=o("cpm"),Cze=o(" \u2014 "),KB=a("a"),Mze=o("CpmTokenizer"),Eze=o(" or "),vU=a("code"),yze=o("CpmTokenizerFast"),wze=o(" (CPM model)"),Aze=l(),jg=a("li"),TU=a("strong"),Lze=o("ctrl"),Bze=o(" \u2014 "),ZB=a("a"),xze=o("CTRLTokenizer"),kze=o(" (CTRL model)"),Rze=l(),tn=a("li"),FU=a("strong"),Sze=o("deberta"),Pze=o(" \u2014 "),ex=a("a"),$ze=o("DebertaTokenizer"),Ize=o(" or "),ox=a("a"),jze=o("DebertaTokenizerFast"),Dze=o(" (DeBERTa model)"),Nze=l(),Dg=a("li"),CU=a("strong"),qze=o("deberta-v2"),Oze=o(" \u2014 "),tx=a("a"),Gze=o("DebertaV2Tokenizer"),Xze=o(" (DeBERTa-v2 model)"),Vze=l(),rn=a("li"),MU=a("strong"),zze=o("distilbert"),Wze=o(" \u2014 "),rx=a("a"),Qze=o("DistilBertTokenizer"),Hze=o(" or "),ax=a("a"),Uze=o("DistilBertTokenizerFast"),Jze=o(" (DistilBERT model)"),Yze=l(),an=a("li"),EU=a("strong"),Kze=o("dpr"),Zze=o(" \u2014 "),sx=a("a"),eWe=o("DPRQuestionEncoderTokenizer"),oWe=o(" or "),nx=a("a"),tWe=o("DPRQuestionEncoderTokenizerFast"),rWe=o(" (DPR model)"),aWe=l(),sn=a("li"),yU=a("strong"),sWe=o("electra"),nWe=o(" \u2014 "),lx=a("a"),lWe=o("ElectraTokenizer"),iWe=o(" or "),ix=a("a"),dWe=o("ElectraTokenizerFast"),cWe=o(" (ELECTRA model)"),mWe=l(),Ng=a("li"),wU=a("strong"),fWe=o("flaubert"),gWe=o(" \u2014 "),dx=a("a"),hWe=o("FlaubertTokenizer"),uWe=o(" (FlauBERT model)"),pWe=l(),nn=a("li"),AU=a("strong"),_We=o("fnet"),bWe=o(" \u2014 "),cx=a("a"),vWe=o("FNetTokenizer"),TWe=o(" or "),mx=a("a"),FWe=o("FNetTokenizerFast"),CWe=o(" (FNet model)"),MWe=l(),qg=a("li"),LU=a("strong"),EWe=o("fsmt"),yWe=o(" \u2014 "),fx=a("a"),wWe=o("FSMTTokenizer"),AWe=o(" (FairSeq Machine-Translation model)"),LWe=l(),ln=a("li"),BU=a("strong"),BWe=o("funnel"),xWe=o(" \u2014 "),gx=a("a"),kWe=o("FunnelTokenizer"),RWe=o(" or "),hx=a("a"),SWe=o("FunnelTokenizerFast"),PWe=o(" (Funnel Transformer model)"),$We=l(),dn=a("li"),xU=a("strong"),IWe=o("gpt2"),jWe=o(" \u2014 "),ux=a("a"),DWe=o("GPT2Tokenizer"),NWe=o(" or "),px=a("a"),qWe=o("GPT2TokenizerFast"),OWe=o(" (OpenAI GPT-2 model)"),GWe=l(),cn=a("li"),kU=a("strong"),XWe=o("gpt_neo"),VWe=o(" \u2014 "),_x=a("a"),zWe=o("GPT2Tokenizer"),WWe=o(" or "),bx=a("a"),QWe=o("GPT2TokenizerFast"),HWe=o(" (GPT Neo model)"),UWe=l(),mn=a("li"),RU=a("strong"),JWe=o("herbert"),YWe=o(" \u2014 "),vx=a("a"),KWe=o("HerbertTokenizer"),ZWe=o(" or "),Tx=a("a"),eQe=o("HerbertTokenizerFast"),oQe=o(" (HerBERT model)"),tQe=l(),Og=a("li"),SU=a("strong"),rQe=o("hubert"),aQe=o(" \u2014 "),Fx=a("a"),sQe=o("Wav2Vec2CTCTokenizer"),nQe=o(" (Hubert model)"),lQe=l(),fn=a("li"),PU=a("strong"),iQe=o("ibert"),dQe=o(" \u2014 "),Cx=a("a"),cQe=o("RobertaTokenizer"),mQe=o(" or "),Mx=a("a"),fQe=o("RobertaTokenizerFast"),gQe=o(" (I-BERT model)"),hQe=l(),gn=a("li"),$U=a("strong"),uQe=o("layoutlm"),pQe=o(" \u2014 "),Ex=a("a"),_Qe=o("LayoutLMTokenizer"),bQe=o(" or "),yx=a("a"),vQe=o("LayoutLMTokenizerFast"),TQe=o(" (LayoutLM model)"),FQe=l(),hn=a("li"),IU=a("strong"),CQe=o("layoutlmv2"),MQe=o(" \u2014 "),wx=a("a"),EQe=o("LayoutLMv2Tokenizer"),yQe=o(" or "),Ax=a("a"),wQe=o("LayoutLMv2TokenizerFast"),AQe=o(" (LayoutLMv2 model)"),LQe=l(),un=a("li"),jU=a("strong"),BQe=o("layoutxlm"),xQe=o(" \u2014 "),Lx=a("a"),kQe=o("LayoutXLMTokenizer"),RQe=o(" or "),Bx=a("a"),SQe=o("LayoutXLMTokenizerFast"),PQe=o(" (LayoutXLM model)"),$Qe=l(),pn=a("li"),DU=a("strong"),IQe=o("led"),jQe=o(" \u2014 "),xx=a("a"),DQe=o("LEDTokenizer"),NQe=o(" or "),kx=a("a"),qQe=o("LEDTokenizerFast"),OQe=o(" (LED model)"),GQe=l(),_n=a("li"),NU=a("strong"),XQe=o("longformer"),VQe=o(" \u2014 "),Rx=a("a"),zQe=o("LongformerTokenizer"),WQe=o(" or "),Sx=a("a"),QQe=o("LongformerTokenizerFast"),HQe=o(" (Longformer model)"),UQe=l(),Gg=a("li"),qU=a("strong"),JQe=o("luke"),YQe=o(" \u2014 "),Px=a("a"),KQe=o("LukeTokenizer"),ZQe=o(" (LUKE model)"),eHe=l(),bn=a("li"),OU=a("strong"),oHe=o("lxmert"),tHe=o(" \u2014 "),$x=a("a"),rHe=o("LxmertTokenizer"),aHe=o(" or "),Ix=a("a"),sHe=o("LxmertTokenizerFast"),nHe=o(" (LXMERT model)"),lHe=l(),Xg=a("li"),GU=a("strong"),iHe=o("m2m_100"),dHe=o(" \u2014 "),jx=a("a"),cHe=o("M2M100Tokenizer"),mHe=o(" (M2M100 model)"),fHe=l(),Vg=a("li"),XU=a("strong"),gHe=o("marian"),hHe=o(" \u2014 "),Dx=a("a"),uHe=o("MarianTokenizer"),pHe=o(" (Marian model)"),_He=l(),vn=a("li"),VU=a("strong"),bHe=o("mbart"),vHe=o(" \u2014 "),Nx=a("a"),THe=o("MBartTokenizer"),FHe=o(" or "),qx=a("a"),CHe=o("MBartTokenizerFast"),MHe=o(" (mBART model)"),EHe=l(),Tn=a("li"),zU=a("strong"),yHe=o("mbart50"),wHe=o(" \u2014 "),Ox=a("a"),AHe=o("MBart50Tokenizer"),LHe=o(" or "),Gx=a("a"),BHe=o("MBart50TokenizerFast"),xHe=o(" (mBART-50 model)"),kHe=l(),zg=a("li"),WU=a("strong"),RHe=o("mluke"),SHe=o(" \u2014 "),Xx=a("a"),PHe=o("MLukeTokenizer"),$He=o(" (mLUKE model)"),IHe=l(),Fn=a("li"),QU=a("strong"),jHe=o("mobilebert"),DHe=o(" \u2014 "),Vx=a("a"),NHe=o("MobileBertTokenizer"),qHe=o(" or "),zx=a("a"),OHe=o("MobileBertTokenizerFast"),GHe=o(" (MobileBERT model)"),XHe=l(),Cn=a("li"),HU=a("strong"),VHe=o("mpnet"),zHe=o(" \u2014 "),Wx=a("a"),WHe=o("MPNetTokenizer"),QHe=o(" or "),Qx=a("a"),HHe=o("MPNetTokenizerFast"),UHe=o(" (MPNet model)"),JHe=l(),Mn=a("li"),UU=a("strong"),YHe=o("mt5"),KHe=o(" \u2014 "),Hx=a("a"),ZHe=o("MT5Tokenizer"),eUe=o(" or "),Ux=a("a"),oUe=o("MT5TokenizerFast"),tUe=o(" (mT5 model)"),rUe=l(),En=a("li"),JU=a("strong"),aUe=o("openai-gpt"),sUe=o(" \u2014 "),Jx=a("a"),nUe=o("OpenAIGPTTokenizer"),lUe=o(" or "),Yx=a("a"),iUe=o("OpenAIGPTTokenizerFast"),dUe=o(" (OpenAI GPT model)"),cUe=l(),yn=a("li"),YU=a("strong"),mUe=o("pegasus"),fUe=o(" \u2014 "),Kx=a("a"),gUe=o("PegasusTokenizer"),hUe=o(" or "),Zx=a("a"),uUe=o("PegasusTokenizerFast"),pUe=o(" (Pegasus model)"),_Ue=l(),Wg=a("li"),KU=a("strong"),bUe=o("perceiver"),vUe=o(" \u2014 "),ek=a("a"),TUe=o("PerceiverTokenizer"),FUe=o(" (Perceiver model)"),CUe=l(),Qg=a("li"),ZU=a("strong"),MUe=o("phobert"),EUe=o(" \u2014 "),ok=a("a"),yUe=o("PhobertTokenizer"),wUe=o(" (PhoBERT model)"),AUe=l(),Hg=a("li"),eJ=a("strong"),LUe=o("plbart"),BUe=o(" \u2014 "),tk=a("a"),xUe=o("PLBartTokenizer"),kUe=o(" (PLBart model)"),RUe=l(),Ug=a("li"),oJ=a("strong"),SUe=o("prophetnet"),PUe=o(" \u2014 "),rk=a("a"),$Ue=o("ProphetNetTokenizer"),IUe=o(" (ProphetNet model)"),jUe=l(),wn=a("li"),tJ=a("strong"),DUe=o("qdqbert"),NUe=o(" \u2014 "),ak=a("a"),qUe=o("BertTokenizer"),OUe=o(" or "),sk=a("a"),GUe=o("BertTokenizerFast"),XUe=o(" (QDQBert model)"),VUe=l(),Jg=a("li"),rJ=a("strong"),zUe=o("rag"),WUe=o(" \u2014 "),nk=a("a"),QUe=o("RagTokenizer"),HUe=o(" (RAG model)"),UUe=l(),An=a("li"),aJ=a("strong"),JUe=o("realm"),YUe=o(" \u2014 "),lk=a("a"),KUe=o("RealmTokenizer"),ZUe=o(" or "),ik=a("a"),eJe=o("RealmTokenizerFast"),oJe=o(" (Realm model)"),tJe=l(),Ln=a("li"),sJ=a("strong"),rJe=o("reformer"),aJe=o(" \u2014 "),dk=a("a"),sJe=o("ReformerTokenizer"),nJe=o(" or "),ck=a("a"),lJe=o("ReformerTokenizerFast"),iJe=o(" (Reformer model)"),dJe=l(),Bn=a("li"),nJ=a("strong"),cJe=o("rembert"),mJe=o(" \u2014 "),mk=a("a"),fJe=o("RemBertTokenizer"),gJe=o(" or "),fk=a("a"),hJe=o("RemBertTokenizerFast"),uJe=o(" (RemBERT model)"),pJe=l(),xn=a("li"),lJ=a("strong"),_Je=o("retribert"),bJe=o(" \u2014 "),gk=a("a"),vJe=o("RetriBertTokenizer"),TJe=o(" or "),hk=a("a"),FJe=o("RetriBertTokenizerFast"),CJe=o(" (RetriBERT model)"),MJe=l(),kn=a("li"),iJ=a("strong"),EJe=o("roberta"),yJe=o(" \u2014 "),uk=a("a"),wJe=o("RobertaTokenizer"),AJe=o(" or "),pk=a("a"),LJe=o("RobertaTokenizerFast"),BJe=o(" (RoBERTa model)"),xJe=l(),Rn=a("li"),dJ=a("strong"),kJe=o("roformer"),RJe=o(" \u2014 "),_k=a("a"),SJe=o("RoFormerTokenizer"),PJe=o(" or "),bk=a("a"),$Je=o("RoFormerTokenizerFast"),IJe=o(" (RoFormer model)"),jJe=l(),Yg=a("li"),cJ=a("strong"),DJe=o("speech_to_text"),NJe=o(" \u2014 "),vk=a("a"),qJe=o("Speech2TextTokenizer"),OJe=o(" (Speech2Text model)"),GJe=l(),Kg=a("li"),mJ=a("strong"),XJe=o("speech_to_text_2"),VJe=o(" \u2014 "),Tk=a("a"),zJe=o("Speech2Text2Tokenizer"),WJe=o(" (Speech2Text2 model)"),QJe=l(),Sn=a("li"),fJ=a("strong"),HJe=o("splinter"),UJe=o(" \u2014 "),Fk=a("a"),JJe=o("SplinterTokenizer"),YJe=o(" or "),Ck=a("a"),KJe=o("SplinterTokenizerFast"),ZJe=o(" (Splinter model)"),eYe=l(),Pn=a("li"),gJ=a("strong"),oYe=o("squeezebert"),tYe=o(" \u2014 "),Mk=a("a"),rYe=o("SqueezeBertTokenizer"),aYe=o(" or "),Ek=a("a"),sYe=o("SqueezeBertTokenizerFast"),nYe=o(" (SqueezeBERT model)"),lYe=l(),$n=a("li"),hJ=a("strong"),iYe=o("t5"),dYe=o(" \u2014 "),yk=a("a"),cYe=o("T5Tokenizer"),mYe=o(" or "),wk=a("a"),fYe=o("T5TokenizerFast"),gYe=o(" (T5 model)"),hYe=l(),Zg=a("li"),uJ=a("strong"),uYe=o("tapas"),pYe=o(" \u2014 "),Ak=a("a"),_Ye=o("TapasTokenizer"),bYe=o(" (TAPAS model)"),vYe=l(),eh=a("li"),pJ=a("strong"),TYe=o("transfo-xl"),FYe=o(" \u2014 "),Lk=a("a"),CYe=o("TransfoXLTokenizer"),MYe=o(" (Transformer-XL model)"),EYe=l(),oh=a("li"),_J=a("strong"),yYe=o("wav2vec2"),wYe=o(" \u2014 "),Bk=a("a"),AYe=o("Wav2Vec2CTCTokenizer"),LYe=o(" (Wav2Vec2 model)"),BYe=l(),th=a("li"),bJ=a("strong"),xYe=o("wav2vec2_phoneme"),kYe=o(" \u2014 "),xk=a("a"),RYe=o("Wav2Vec2PhonemeCTCTokenizer"),SYe=o(" (Wav2Vec2Phoneme model)"),PYe=l(),In=a("li"),vJ=a("strong"),$Ye=o("xglm"),IYe=o(" \u2014 "),kk=a("a"),jYe=o("XGLMTokenizer"),DYe=o(" or "),Rk=a("a"),NYe=o("XGLMTokenizerFast"),qYe=o(" (XGLM model)"),OYe=l(),rh=a("li"),TJ=a("strong"),GYe=o("xlm"),XYe=o(" \u2014 "),Sk=a("a"),VYe=o("XLMTokenizer"),zYe=o(" (XLM model)"),WYe=l(),ah=a("li"),FJ=a("strong"),QYe=o("xlm-prophetnet"),HYe=o(" \u2014 "),Pk=a("a"),UYe=o("XLMProphetNetTokenizer"),JYe=o(" (XLMProphetNet model)"),YYe=l(),jn=a("li"),CJ=a("strong"),KYe=o("xlm-roberta"),ZYe=o(" \u2014 "),$k=a("a"),eKe=o("XLMRobertaTokenizer"),oKe=o(" or "),Ik=a("a"),tKe=o("XLMRobertaTokenizerFast"),rKe=o(" (XLM-RoBERTa model)"),aKe=l(),Dn=a("li"),MJ=a("strong"),sKe=o("xlnet"),nKe=o(" \u2014 "),jk=a("a"),lKe=o("XLNetTokenizer"),iKe=o(" or "),Dk=a("a"),dKe=o("XLNetTokenizerFast"),cKe=o(" (XLNet model)"),mKe=l(),EJ=a("p"),fKe=o("Examples:"),gKe=l(),m(ty.$$.fragment),hKe=l(),sh=a("div"),m(ry.$$.fragment),uKe=l(),yJ=a("p"),pKe=o("Register a new tokenizer in this mapping."),SBe=l(),Qi=a("h2"),nh=a("a"),wJ=a("span"),m(ay.$$.fragment),_Ke=l(),AJ=a("span"),bKe=o("AutoFeatureExtractor"),PBe=l(),Wo=a("div"),m(sy.$$.fragment),vKe=l(),ny=a("p"),TKe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Nk=a("a"),FKe=o("AutoFeatureExtractor.from_pretrained()"),CKe=o(" class method."),MKe=l(),ly=a("p"),EKe=o("This class cannot be instantiated directly using "),LJ=a("code"),yKe=o("__init__()"),wKe=o(" (throws an error)."),AKe=l(),xe=a("div"),m(iy.$$.fragment),LKe=l(),BJ=a("p"),BKe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),xKe=l(),Ga=a("p"),kKe=o("The feature extractor class to instantiate is selected based on the "),xJ=a("code"),RKe=o("model_type"),SKe=o(` property of the config object
(either passed as an argument or loaded from `),kJ=a("code"),PKe=o("pretrained_model_name_or_path"),$Ke=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),RJ=a("code"),IKe=o("pretrained_model_name_or_path"),jKe=o(":"),DKe=l(),ae=a("ul"),lh=a("li"),SJ=a("strong"),NKe=o("beit"),qKe=o(" \u2014 "),qk=a("a"),OKe=o("BeitFeatureExtractor"),GKe=o(" (BEiT model)"),XKe=l(),ih=a("li"),PJ=a("strong"),VKe=o("clip"),zKe=o(" \u2014 "),Ok=a("a"),WKe=o("CLIPFeatureExtractor"),QKe=o(" (CLIP model)"),HKe=l(),dh=a("li"),$J=a("strong"),UKe=o("convnext"),JKe=o(" \u2014 "),Gk=a("a"),YKe=o("ConvNextFeatureExtractor"),KKe=o(" (ConvNext model)"),ZKe=l(),ch=a("li"),IJ=a("strong"),eZe=o("deit"),oZe=o(" \u2014 "),Xk=a("a"),tZe=o("DeiTFeatureExtractor"),rZe=o(" (DeiT model)"),aZe=l(),mh=a("li"),jJ=a("strong"),sZe=o("detr"),nZe=o(" \u2014 "),Vk=a("a"),lZe=o("DetrFeatureExtractor"),iZe=o(" (DETR model)"),dZe=l(),fh=a("li"),DJ=a("strong"),cZe=o("hubert"),mZe=o(" \u2014 "),zk=a("a"),fZe=o("Wav2Vec2FeatureExtractor"),gZe=o(" (Hubert model)"),hZe=l(),gh=a("li"),NJ=a("strong"),uZe=o("layoutlmv2"),pZe=o(" \u2014 "),Wk=a("a"),_Ze=o("LayoutLMv2FeatureExtractor"),bZe=o(" (LayoutLMv2 model)"),vZe=l(),hh=a("li"),qJ=a("strong"),TZe=o("maskformer"),FZe=o(" \u2014 "),Qk=a("a"),CZe=o("MaskFormerFeatureExtractor"),MZe=o(" (MaskFormer model)"),EZe=l(),uh=a("li"),OJ=a("strong"),yZe=o("perceiver"),wZe=o(" \u2014 "),Hk=a("a"),AZe=o("PerceiverFeatureExtractor"),LZe=o(" (Perceiver model)"),BZe=l(),ph=a("li"),GJ=a("strong"),xZe=o("poolformer"),kZe=o(" \u2014 "),Uk=a("a"),RZe=o("PoolFormerFeatureExtractor"),SZe=o(" (PoolFormer model)"),PZe=l(),_h=a("li"),XJ=a("strong"),$Ze=o("segformer"),IZe=o(" \u2014 "),Jk=a("a"),jZe=o("SegformerFeatureExtractor"),DZe=o(" (SegFormer model)"),NZe=l(),bh=a("li"),VJ=a("strong"),qZe=o("speech_to_text"),OZe=o(" \u2014 "),Yk=a("a"),GZe=o("Speech2TextFeatureExtractor"),XZe=o(" (Speech2Text model)"),VZe=l(),vh=a("li"),zJ=a("strong"),zZe=o("swin"),WZe=o(" \u2014 "),Kk=a("a"),QZe=o("ViTFeatureExtractor"),HZe=o(" (Swin model)"),UZe=l(),Th=a("li"),WJ=a("strong"),JZe=o("vit"),YZe=o(" \u2014 "),Zk=a("a"),KZe=o("ViTFeatureExtractor"),ZZe=o(" (ViT model)"),eeo=l(),Fh=a("li"),QJ=a("strong"),oeo=o("vit_mae"),teo=o(" \u2014 "),eR=a("a"),reo=o("ViTFeatureExtractor"),aeo=o(" (ViTMAE model)"),seo=l(),Ch=a("li"),HJ=a("strong"),neo=o("wav2vec2"),leo=o(" \u2014 "),oR=a("a"),ieo=o("Wav2Vec2FeatureExtractor"),deo=o(" (Wav2Vec2 model)"),ceo=l(),m(Mh.$$.fragment),meo=l(),UJ=a("p"),feo=o("Examples:"),geo=l(),m(dy.$$.fragment),heo=l(),Eh=a("div"),m(cy.$$.fragment),ueo=l(),JJ=a("p"),peo=o("Register a new feature extractor for this class."),$Be=l(),Hi=a("h2"),yh=a("a"),YJ=a("span"),m(my.$$.fragment),_eo=l(),KJ=a("span"),beo=o("AutoProcessor"),IBe=l(),Qo=a("div"),m(fy.$$.fragment),veo=l(),gy=a("p"),Teo=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),tR=a("a"),Feo=o("AutoProcessor.from_pretrained()"),Ceo=o(" class method."),Meo=l(),hy=a("p"),Eeo=o("This class cannot be instantiated directly using "),ZJ=a("code"),yeo=o("__init__()"),weo=o(" (throws an error)."),Aeo=l(),ke=a("div"),m(uy.$$.fragment),Leo=l(),eY=a("p"),Beo=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),xeo=l(),Ui=a("p"),keo=o("The processor class to instantiate is selected based on the "),oY=a("code"),Reo=o("model_type"),Seo=o(` property of the config object (either
passed as an argument or loaded from `),tY=a("code"),Peo=o("pretrained_model_name_or_path"),$eo=o(" if possible):"),Ieo=l(),we=a("ul"),wh=a("li"),rY=a("strong"),jeo=o("clip"),Deo=o(" \u2014 "),rR=a("a"),Neo=o("CLIPProcessor"),qeo=o(" (CLIP model)"),Oeo=l(),Ah=a("li"),aY=a("strong"),Geo=o("layoutlmv2"),Xeo=o(" \u2014 "),aR=a("a"),Veo=o("LayoutLMv2Processor"),zeo=o(" (LayoutLMv2 model)"),Weo=l(),Lh=a("li"),sY=a("strong"),Qeo=o("layoutxlm"),Heo=o(" \u2014 "),sR=a("a"),Ueo=o("LayoutXLMProcessor"),Jeo=o(" (LayoutXLM model)"),Yeo=l(),Bh=a("li"),nY=a("strong"),Keo=o("speech_to_text"),Zeo=o(" \u2014 "),nR=a("a"),eoo=o("Speech2TextProcessor"),ooo=o(" (Speech2Text model)"),too=l(),xh=a("li"),lY=a("strong"),roo=o("speech_to_text_2"),aoo=o(" \u2014 "),lR=a("a"),soo=o("Speech2Text2Processor"),noo=o(" (Speech2Text2 model)"),loo=l(),kh=a("li"),iY=a("strong"),ioo=o("trocr"),doo=o(" \u2014 "),iR=a("a"),coo=o("TrOCRProcessor"),moo=o(" (TrOCR model)"),foo=l(),Rh=a("li"),dY=a("strong"),goo=o("vision-text-dual-encoder"),hoo=o(" \u2014 "),dR=a("a"),uoo=o("VisionTextDualEncoderProcessor"),poo=o(" (VisionTextDualEncoder model)"),_oo=l(),Sh=a("li"),cY=a("strong"),boo=o("wav2vec2"),voo=o(" \u2014 "),cR=a("a"),Too=o("Wav2Vec2Processor"),Foo=o(" (Wav2Vec2 model)"),Coo=l(),m(Ph.$$.fragment),Moo=l(),mY=a("p"),Eoo=o("Examples:"),yoo=l(),m(py.$$.fragment),woo=l(),$h=a("div"),m(_y.$$.fragment),Aoo=l(),fY=a("p"),Loo=o("Register a new processor for this class."),jBe=l(),Ji=a("h2"),Ih=a("a"),gY=a("span"),m(by.$$.fragment),Boo=l(),hY=a("span"),xoo=o("AutoModel"),DBe=l(),Ho=a("div"),m(vy.$$.fragment),koo=l(),Yi=a("p"),Roo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),uY=a("code"),Soo=o("from_pretrained()"),Poo=o("class method or the "),pY=a("code"),$oo=o("from_config()"),Ioo=o(`class
method.`),joo=l(),Ty=a("p"),Doo=o("This class cannot be instantiated directly using "),_Y=a("code"),Noo=o("__init__()"),qoo=o(" (throws an error)."),Ooo=l(),Gt=a("div"),m(Fy.$$.fragment),Goo=l(),bY=a("p"),Xoo=o("Instantiates one of the base model classes of the library from a configuration."),Voo=l(),Ki=a("p"),zoo=o(`Note:
Loading a model from its configuration file does `),vY=a("strong"),Woo=o("not"),Qoo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),TY=a("code"),Hoo=o("from_pretrained()"),Uoo=o("to load the model weights."),Joo=l(),FY=a("p"),Yoo=o("Examples:"),Koo=l(),m(Cy.$$.fragment),Zoo=l(),Re=a("div"),m(My.$$.fragment),eto=l(),CY=a("p"),oto=o("Instantiate one of the base model classes of the library from a pretrained model."),tto=l(),Xa=a("p"),rto=o("The model class to instantiate is selected based on the "),MY=a("code"),ato=o("model_type"),sto=o(` property of the config object (either
passed as an argument or loaded from `),EY=a("code"),nto=o("pretrained_model_name_or_path"),lto=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yY=a("code"),ito=o("pretrained_model_name_or_path"),dto=o(":"),cto=l(),F=a("ul"),jh=a("li"),wY=a("strong"),mto=o("albert"),fto=o(" \u2014 "),mR=a("a"),gto=o("AlbertModel"),hto=o(" (ALBERT model)"),uto=l(),Dh=a("li"),AY=a("strong"),pto=o("bart"),_to=o(" \u2014 "),fR=a("a"),bto=o("BartModel"),vto=o(" (BART model)"),Tto=l(),Nh=a("li"),LY=a("strong"),Fto=o("beit"),Cto=o(" \u2014 "),gR=a("a"),Mto=o("BeitModel"),Eto=o(" (BEiT model)"),yto=l(),qh=a("li"),BY=a("strong"),wto=o("bert"),Ato=o(" \u2014 "),hR=a("a"),Lto=o("BertModel"),Bto=o(" (BERT model)"),xto=l(),Oh=a("li"),xY=a("strong"),kto=o("bert-generation"),Rto=o(" \u2014 "),uR=a("a"),Sto=o("BertGenerationEncoder"),Pto=o(" (Bert Generation model)"),$to=l(),Gh=a("li"),kY=a("strong"),Ito=o("big_bird"),jto=o(" \u2014 "),pR=a("a"),Dto=o("BigBirdModel"),Nto=o(" (BigBird model)"),qto=l(),Xh=a("li"),RY=a("strong"),Oto=o("bigbird_pegasus"),Gto=o(" \u2014 "),_R=a("a"),Xto=o("BigBirdPegasusModel"),Vto=o(" (BigBirdPegasus model)"),zto=l(),Vh=a("li"),SY=a("strong"),Wto=o("blenderbot"),Qto=o(" \u2014 "),bR=a("a"),Hto=o("BlenderbotModel"),Uto=o(" (Blenderbot model)"),Jto=l(),zh=a("li"),PY=a("strong"),Yto=o("blenderbot-small"),Kto=o(" \u2014 "),vR=a("a"),Zto=o("BlenderbotSmallModel"),ero=o(" (BlenderbotSmall model)"),oro=l(),Wh=a("li"),$Y=a("strong"),tro=o("camembert"),rro=o(" \u2014 "),TR=a("a"),aro=o("CamembertModel"),sro=o(" (CamemBERT model)"),nro=l(),Qh=a("li"),IY=a("strong"),lro=o("canine"),iro=o(" \u2014 "),FR=a("a"),dro=o("CanineModel"),cro=o(" (Canine model)"),mro=l(),Hh=a("li"),jY=a("strong"),fro=o("clip"),gro=o(" \u2014 "),CR=a("a"),hro=o("CLIPModel"),uro=o(" (CLIP model)"),pro=l(),Uh=a("li"),DY=a("strong"),_ro=o("convbert"),bro=o(" \u2014 "),MR=a("a"),vro=o("ConvBertModel"),Tro=o(" (ConvBERT model)"),Fro=l(),Jh=a("li"),NY=a("strong"),Cro=o("convnext"),Mro=o(" \u2014 "),ER=a("a"),Ero=o("ConvNextModel"),yro=o(" (ConvNext model)"),wro=l(),Yh=a("li"),qY=a("strong"),Aro=o("ctrl"),Lro=o(" \u2014 "),yR=a("a"),Bro=o("CTRLModel"),xro=o(" (CTRL model)"),kro=l(),Kh=a("li"),OY=a("strong"),Rro=o("data2vec-audio"),Sro=o(" \u2014 "),wR=a("a"),Pro=o("Data2VecAudioModel"),$ro=o(" (Data2VecAudio model)"),Iro=l(),Zh=a("li"),GY=a("strong"),jro=o("data2vec-text"),Dro=o(" \u2014 "),AR=a("a"),Nro=o("Data2VecTextModel"),qro=o(" (Data2VecText model)"),Oro=l(),eu=a("li"),XY=a("strong"),Gro=o("deberta"),Xro=o(" \u2014 "),LR=a("a"),Vro=o("DebertaModel"),zro=o(" (DeBERTa model)"),Wro=l(),ou=a("li"),VY=a("strong"),Qro=o("deberta-v2"),Hro=o(" \u2014 "),BR=a("a"),Uro=o("DebertaV2Model"),Jro=o(" (DeBERTa-v2 model)"),Yro=l(),tu=a("li"),zY=a("strong"),Kro=o("deit"),Zro=o(" \u2014 "),xR=a("a"),eao=o("DeiTModel"),oao=o(" (DeiT model)"),tao=l(),ru=a("li"),WY=a("strong"),rao=o("detr"),aao=o(" \u2014 "),kR=a("a"),sao=o("DetrModel"),nao=o(" (DETR model)"),lao=l(),au=a("li"),QY=a("strong"),iao=o("distilbert"),dao=o(" \u2014 "),RR=a("a"),cao=o("DistilBertModel"),mao=o(" (DistilBERT model)"),fao=l(),su=a("li"),HY=a("strong"),gao=o("dpr"),hao=o(" \u2014 "),SR=a("a"),uao=o("DPRQuestionEncoder"),pao=o(" (DPR model)"),_ao=l(),nu=a("li"),UY=a("strong"),bao=o("electra"),vao=o(" \u2014 "),PR=a("a"),Tao=o("ElectraModel"),Fao=o(" (ELECTRA model)"),Cao=l(),lu=a("li"),JY=a("strong"),Mao=o("flaubert"),Eao=o(" \u2014 "),$R=a("a"),yao=o("FlaubertModel"),wao=o(" (FlauBERT model)"),Aao=l(),iu=a("li"),YY=a("strong"),Lao=o("fnet"),Bao=o(" \u2014 "),IR=a("a"),xao=o("FNetModel"),kao=o(" (FNet model)"),Rao=l(),du=a("li"),KY=a("strong"),Sao=o("fsmt"),Pao=o(" \u2014 "),jR=a("a"),$ao=o("FSMTModel"),Iao=o(" (FairSeq Machine-Translation model)"),jao=l(),Nn=a("li"),ZY=a("strong"),Dao=o("funnel"),Nao=o(" \u2014 "),DR=a("a"),qao=o("FunnelModel"),Oao=o(" or "),NR=a("a"),Gao=o("FunnelBaseModel"),Xao=o(" (Funnel Transformer model)"),Vao=l(),cu=a("li"),eK=a("strong"),zao=o("gpt2"),Wao=o(" \u2014 "),qR=a("a"),Qao=o("GPT2Model"),Hao=o(" (OpenAI GPT-2 model)"),Uao=l(),mu=a("li"),oK=a("strong"),Jao=o("gpt_neo"),Yao=o(" \u2014 "),OR=a("a"),Kao=o("GPTNeoModel"),Zao=o(" (GPT Neo model)"),eso=l(),fu=a("li"),tK=a("strong"),oso=o("gptj"),tso=o(" \u2014 "),GR=a("a"),rso=o("GPTJModel"),aso=o(" (GPT-J model)"),sso=l(),gu=a("li"),rK=a("strong"),nso=o("hubert"),lso=o(" \u2014 "),XR=a("a"),iso=o("HubertModel"),dso=o(" (Hubert model)"),cso=l(),hu=a("li"),aK=a("strong"),mso=o("ibert"),fso=o(" \u2014 "),VR=a("a"),gso=o("IBertModel"),hso=o(" (I-BERT model)"),uso=l(),uu=a("li"),sK=a("strong"),pso=o("imagegpt"),_so=o(" \u2014 "),zR=a("a"),bso=o("ImageGPTModel"),vso=o(" (ImageGPT model)"),Tso=l(),pu=a("li"),nK=a("strong"),Fso=o("layoutlm"),Cso=o(" \u2014 "),WR=a("a"),Mso=o("LayoutLMModel"),Eso=o(" (LayoutLM model)"),yso=l(),_u=a("li"),lK=a("strong"),wso=o("layoutlmv2"),Aso=o(" \u2014 "),QR=a("a"),Lso=o("LayoutLMv2Model"),Bso=o(" (LayoutLMv2 model)"),xso=l(),bu=a("li"),iK=a("strong"),kso=o("led"),Rso=o(" \u2014 "),HR=a("a"),Sso=o("LEDModel"),Pso=o(" (LED model)"),$so=l(),vu=a("li"),dK=a("strong"),Iso=o("longformer"),jso=o(" \u2014 "),UR=a("a"),Dso=o("LongformerModel"),Nso=o(" (Longformer model)"),qso=l(),Tu=a("li"),cK=a("strong"),Oso=o("luke"),Gso=o(" \u2014 "),JR=a("a"),Xso=o("LukeModel"),Vso=o(" (LUKE model)"),zso=l(),Fu=a("li"),mK=a("strong"),Wso=o("lxmert"),Qso=o(" \u2014 "),YR=a("a"),Hso=o("LxmertModel"),Uso=o(" (LXMERT model)"),Jso=l(),Cu=a("li"),fK=a("strong"),Yso=o("m2m_100"),Kso=o(" \u2014 "),KR=a("a"),Zso=o("M2M100Model"),eno=o(" (M2M100 model)"),ono=l(),Mu=a("li"),gK=a("strong"),tno=o("marian"),rno=o(" \u2014 "),ZR=a("a"),ano=o("MarianModel"),sno=o(" (Marian model)"),nno=l(),Eu=a("li"),hK=a("strong"),lno=o("maskformer"),ino=o(" \u2014 "),eS=a("a"),dno=o("MaskFormerModel"),cno=o(" (MaskFormer model)"),mno=l(),yu=a("li"),uK=a("strong"),fno=o("mbart"),gno=o(" \u2014 "),oS=a("a"),hno=o("MBartModel"),uno=o(" (mBART model)"),pno=l(),wu=a("li"),pK=a("strong"),_no=o("megatron-bert"),bno=o(" \u2014 "),tS=a("a"),vno=o("MegatronBertModel"),Tno=o(" (MegatronBert model)"),Fno=l(),Au=a("li"),_K=a("strong"),Cno=o("mobilebert"),Mno=o(" \u2014 "),rS=a("a"),Eno=o("MobileBertModel"),yno=o(" (MobileBERT model)"),wno=l(),Lu=a("li"),bK=a("strong"),Ano=o("mpnet"),Lno=o(" \u2014 "),aS=a("a"),Bno=o("MPNetModel"),xno=o(" (MPNet model)"),kno=l(),Bu=a("li"),vK=a("strong"),Rno=o("mt5"),Sno=o(" \u2014 "),sS=a("a"),Pno=o("MT5Model"),$no=o(" (mT5 model)"),Ino=l(),xu=a("li"),TK=a("strong"),jno=o("nystromformer"),Dno=o(" \u2014 "),nS=a("a"),Nno=o("NystromformerModel"),qno=o(" (Nystromformer model)"),Ono=l(),ku=a("li"),FK=a("strong"),Gno=o("openai-gpt"),Xno=o(" \u2014 "),lS=a("a"),Vno=o("OpenAIGPTModel"),zno=o(" (OpenAI GPT model)"),Wno=l(),Ru=a("li"),CK=a("strong"),Qno=o("pegasus"),Hno=o(" \u2014 "),iS=a("a"),Uno=o("PegasusModel"),Jno=o(" (Pegasus model)"),Yno=l(),Su=a("li"),MK=a("strong"),Kno=o("perceiver"),Zno=o(" \u2014 "),dS=a("a"),elo=o("PerceiverModel"),olo=o(" (Perceiver model)"),tlo=l(),Pu=a("li"),EK=a("strong"),rlo=o("plbart"),alo=o(" \u2014 "),cS=a("a"),slo=o("PLBartModel"),nlo=o(" (PLBart model)"),llo=l(),$u=a("li"),yK=a("strong"),ilo=o("poolformer"),dlo=o(" \u2014 "),mS=a("a"),clo=o("PoolFormerModel"),mlo=o(" (PoolFormer model)"),flo=l(),Iu=a("li"),wK=a("strong"),glo=o("prophetnet"),hlo=o(" \u2014 "),fS=a("a"),ulo=o("ProphetNetModel"),plo=o(" (ProphetNet model)"),_lo=l(),ju=a("li"),AK=a("strong"),blo=o("qdqbert"),vlo=o(" \u2014 "),gS=a("a"),Tlo=o("QDQBertModel"),Flo=o(" (QDQBert model)"),Clo=l(),Du=a("li"),LK=a("strong"),Mlo=o("reformer"),Elo=o(" \u2014 "),hS=a("a"),ylo=o("ReformerModel"),wlo=o(" (Reformer model)"),Alo=l(),Nu=a("li"),BK=a("strong"),Llo=o("rembert"),Blo=o(" \u2014 "),uS=a("a"),xlo=o("RemBertModel"),klo=o(" (RemBERT model)"),Rlo=l(),qu=a("li"),xK=a("strong"),Slo=o("retribert"),Plo=o(" \u2014 "),pS=a("a"),$lo=o("RetriBertModel"),Ilo=o(" (RetriBERT model)"),jlo=l(),Ou=a("li"),kK=a("strong"),Dlo=o("roberta"),Nlo=o(" \u2014 "),_S=a("a"),qlo=o("RobertaModel"),Olo=o(" (RoBERTa model)"),Glo=l(),Gu=a("li"),RK=a("strong"),Xlo=o("roformer"),Vlo=o(" \u2014 "),bS=a("a"),zlo=o("RoFormerModel"),Wlo=o(" (RoFormer model)"),Qlo=l(),Xu=a("li"),SK=a("strong"),Hlo=o("segformer"),Ulo=o(" \u2014 "),vS=a("a"),Jlo=o("SegformerModel"),Ylo=o(" (SegFormer model)"),Klo=l(),Vu=a("li"),PK=a("strong"),Zlo=o("sew"),eio=o(" \u2014 "),TS=a("a"),oio=o("SEWModel"),tio=o(" (SEW model)"),rio=l(),zu=a("li"),$K=a("strong"),aio=o("sew-d"),sio=o(" \u2014 "),FS=a("a"),nio=o("SEWDModel"),lio=o(" (SEW-D model)"),iio=l(),Wu=a("li"),IK=a("strong"),dio=o("speech_to_text"),cio=o(" \u2014 "),CS=a("a"),mio=o("Speech2TextModel"),fio=o(" (Speech2Text model)"),gio=l(),Qu=a("li"),jK=a("strong"),hio=o("splinter"),uio=o(" \u2014 "),MS=a("a"),pio=o("SplinterModel"),_io=o(" (Splinter model)"),bio=l(),Hu=a("li"),DK=a("strong"),vio=o("squeezebert"),Tio=o(" \u2014 "),ES=a("a"),Fio=o("SqueezeBertModel"),Cio=o(" (SqueezeBERT model)"),Mio=l(),Uu=a("li"),NK=a("strong"),Eio=o("swin"),yio=o(" \u2014 "),yS=a("a"),wio=o("SwinModel"),Aio=o(" (Swin model)"),Lio=l(),Ju=a("li"),qK=a("strong"),Bio=o("t5"),xio=o(" \u2014 "),wS=a("a"),kio=o("T5Model"),Rio=o(" (T5 model)"),Sio=l(),Yu=a("li"),OK=a("strong"),Pio=o("tapas"),$io=o(" \u2014 "),AS=a("a"),Iio=o("TapasModel"),jio=o(" (TAPAS model)"),Dio=l(),Ku=a("li"),GK=a("strong"),Nio=o("transfo-xl"),qio=o(" \u2014 "),LS=a("a"),Oio=o("TransfoXLModel"),Gio=o(" (Transformer-XL model)"),Xio=l(),Zu=a("li"),XK=a("strong"),Vio=o("unispeech"),zio=o(" \u2014 "),BS=a("a"),Wio=o("UniSpeechModel"),Qio=o(" (UniSpeech model)"),Hio=l(),ep=a("li"),VK=a("strong"),Uio=o("unispeech-sat"),Jio=o(" \u2014 "),xS=a("a"),Yio=o("UniSpeechSatModel"),Kio=o(" (UniSpeechSat model)"),Zio=l(),op=a("li"),zK=a("strong"),edo=o("vilt"),odo=o(" \u2014 "),kS=a("a"),tdo=o("ViltModel"),rdo=o(" (ViLT model)"),ado=l(),tp=a("li"),WK=a("strong"),sdo=o("vision-text-dual-encoder"),ndo=o(" \u2014 "),RS=a("a"),ldo=o("VisionTextDualEncoderModel"),ido=o(" (VisionTextDualEncoder model)"),ddo=l(),rp=a("li"),QK=a("strong"),cdo=o("visual_bert"),mdo=o(" \u2014 "),SS=a("a"),fdo=o("VisualBertModel"),gdo=o(" (VisualBert model)"),hdo=l(),ap=a("li"),HK=a("strong"),udo=o("vit"),pdo=o(" \u2014 "),PS=a("a"),_do=o("ViTModel"),bdo=o(" (ViT model)"),vdo=l(),sp=a("li"),UK=a("strong"),Tdo=o("vit_mae"),Fdo=o(" \u2014 "),$S=a("a"),Cdo=o("ViTMAEModel"),Mdo=o(" (ViTMAE model)"),Edo=l(),np=a("li"),JK=a("strong"),ydo=o("wav2vec2"),wdo=o(" \u2014 "),IS=a("a"),Ado=o("Wav2Vec2Model"),Ldo=o(" (Wav2Vec2 model)"),Bdo=l(),lp=a("li"),YK=a("strong"),xdo=o("wavlm"),kdo=o(" \u2014 "),jS=a("a"),Rdo=o("WavLMModel"),Sdo=o(" (WavLM model)"),Pdo=l(),ip=a("li"),KK=a("strong"),$do=o("xglm"),Ido=o(" \u2014 "),DS=a("a"),jdo=o("XGLMModel"),Ddo=o(" (XGLM model)"),Ndo=l(),dp=a("li"),ZK=a("strong"),qdo=o("xlm"),Odo=o(" \u2014 "),NS=a("a"),Gdo=o("XLMModel"),Xdo=o(" (XLM model)"),Vdo=l(),cp=a("li"),eZ=a("strong"),zdo=o("xlm-prophetnet"),Wdo=o(" \u2014 "),qS=a("a"),Qdo=o("XLMProphetNetModel"),Hdo=o(" (XLMProphetNet model)"),Udo=l(),mp=a("li"),oZ=a("strong"),Jdo=o("xlm-roberta"),Ydo=o(" \u2014 "),OS=a("a"),Kdo=o("XLMRobertaModel"),Zdo=o(" (XLM-RoBERTa model)"),eco=l(),fp=a("li"),tZ=a("strong"),oco=o("xlm-roberta-xl"),tco=o(" \u2014 "),GS=a("a"),rco=o("XLMRobertaXLModel"),aco=o(" (XLM-RoBERTa-XL model)"),sco=l(),gp=a("li"),rZ=a("strong"),nco=o("xlnet"),lco=o(" \u2014 "),XS=a("a"),ico=o("XLNetModel"),dco=o(" (XLNet model)"),cco=l(),hp=a("li"),aZ=a("strong"),mco=o("yoso"),fco=o(" \u2014 "),VS=a("a"),gco=o("YosoModel"),hco=o(" (YOSO model)"),uco=l(),up=a("p"),pco=o("The model is set in evaluation mode by default using "),sZ=a("code"),_co=o("model.eval()"),bco=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),nZ=a("code"),vco=o("model.train()"),Tco=l(),lZ=a("p"),Fco=o("Examples:"),Cco=l(),m(Ey.$$.fragment),NBe=l(),Zi=a("h2"),pp=a("a"),iZ=a("span"),m(yy.$$.fragment),Mco=l(),dZ=a("span"),Eco=o("AutoModelForPreTraining"),qBe=l(),Uo=a("div"),m(wy.$$.fragment),yco=l(),ed=a("p"),wco=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),cZ=a("code"),Aco=o("from_pretrained()"),Lco=o("class method or the "),mZ=a("code"),Bco=o("from_config()"),xco=o(`class
method.`),kco=l(),Ay=a("p"),Rco=o("This class cannot be instantiated directly using "),fZ=a("code"),Sco=o("__init__()"),Pco=o(" (throws an error)."),$co=l(),Xt=a("div"),m(Ly.$$.fragment),Ico=l(),gZ=a("p"),jco=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Dco=l(),od=a("p"),Nco=o(`Note:
Loading a model from its configuration file does `),hZ=a("strong"),qco=o("not"),Oco=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),uZ=a("code"),Gco=o("from_pretrained()"),Xco=o("to load the model weights."),Vco=l(),pZ=a("p"),zco=o("Examples:"),Wco=l(),m(By.$$.fragment),Qco=l(),Se=a("div"),m(xy.$$.fragment),Hco=l(),_Z=a("p"),Uco=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Jco=l(),Va=a("p"),Yco=o("The model class to instantiate is selected based on the "),bZ=a("code"),Kco=o("model_type"),Zco=o(` property of the config object (either
passed as an argument or loaded from `),vZ=a("code"),emo=o("pretrained_model_name_or_path"),omo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),TZ=a("code"),tmo=o("pretrained_model_name_or_path"),rmo=o(":"),amo=l(),k=a("ul"),_p=a("li"),FZ=a("strong"),smo=o("albert"),nmo=o(" \u2014 "),zS=a("a"),lmo=o("AlbertForPreTraining"),imo=o(" (ALBERT model)"),dmo=l(),bp=a("li"),CZ=a("strong"),cmo=o("bart"),mmo=o(" \u2014 "),WS=a("a"),fmo=o("BartForConditionalGeneration"),gmo=o(" (BART model)"),hmo=l(),vp=a("li"),MZ=a("strong"),umo=o("bert"),pmo=o(" \u2014 "),QS=a("a"),_mo=o("BertForPreTraining"),bmo=o(" (BERT model)"),vmo=l(),Tp=a("li"),EZ=a("strong"),Tmo=o("big_bird"),Fmo=o(" \u2014 "),HS=a("a"),Cmo=o("BigBirdForPreTraining"),Mmo=o(" (BigBird model)"),Emo=l(),Fp=a("li"),yZ=a("strong"),ymo=o("camembert"),wmo=o(" \u2014 "),US=a("a"),Amo=o("CamembertForMaskedLM"),Lmo=o(" (CamemBERT model)"),Bmo=l(),Cp=a("li"),wZ=a("strong"),xmo=o("ctrl"),kmo=o(" \u2014 "),JS=a("a"),Rmo=o("CTRLLMHeadModel"),Smo=o(" (CTRL model)"),Pmo=l(),Mp=a("li"),AZ=a("strong"),$mo=o("data2vec-text"),Imo=o(" \u2014 "),YS=a("a"),jmo=o("Data2VecTextForMaskedLM"),Dmo=o(" (Data2VecText model)"),Nmo=l(),Ep=a("li"),LZ=a("strong"),qmo=o("deberta"),Omo=o(" \u2014 "),KS=a("a"),Gmo=o("DebertaForMaskedLM"),Xmo=o(" (DeBERTa model)"),Vmo=l(),yp=a("li"),BZ=a("strong"),zmo=o("deberta-v2"),Wmo=o(" \u2014 "),ZS=a("a"),Qmo=o("DebertaV2ForMaskedLM"),Hmo=o(" (DeBERTa-v2 model)"),Umo=l(),wp=a("li"),xZ=a("strong"),Jmo=o("distilbert"),Ymo=o(" \u2014 "),eP=a("a"),Kmo=o("DistilBertForMaskedLM"),Zmo=o(" (DistilBERT model)"),efo=l(),Ap=a("li"),kZ=a("strong"),ofo=o("electra"),tfo=o(" \u2014 "),oP=a("a"),rfo=o("ElectraForPreTraining"),afo=o(" (ELECTRA model)"),sfo=l(),Lp=a("li"),RZ=a("strong"),nfo=o("flaubert"),lfo=o(" \u2014 "),tP=a("a"),ifo=o("FlaubertWithLMHeadModel"),dfo=o(" (FlauBERT model)"),cfo=l(),Bp=a("li"),SZ=a("strong"),mfo=o("fnet"),ffo=o(" \u2014 "),rP=a("a"),gfo=o("FNetForPreTraining"),hfo=o(" (FNet model)"),ufo=l(),xp=a("li"),PZ=a("strong"),pfo=o("fsmt"),_fo=o(" \u2014 "),aP=a("a"),bfo=o("FSMTForConditionalGeneration"),vfo=o(" (FairSeq Machine-Translation model)"),Tfo=l(),kp=a("li"),$Z=a("strong"),Ffo=o("funnel"),Cfo=o(" \u2014 "),sP=a("a"),Mfo=o("FunnelForPreTraining"),Efo=o(" (Funnel Transformer model)"),yfo=l(),Rp=a("li"),IZ=a("strong"),wfo=o("gpt2"),Afo=o(" \u2014 "),nP=a("a"),Lfo=o("GPT2LMHeadModel"),Bfo=o(" (OpenAI GPT-2 model)"),xfo=l(),Sp=a("li"),jZ=a("strong"),kfo=o("ibert"),Rfo=o(" \u2014 "),lP=a("a"),Sfo=o("IBertForMaskedLM"),Pfo=o(" (I-BERT model)"),$fo=l(),Pp=a("li"),DZ=a("strong"),Ifo=o("layoutlm"),jfo=o(" \u2014 "),iP=a("a"),Dfo=o("LayoutLMForMaskedLM"),Nfo=o(" (LayoutLM model)"),qfo=l(),$p=a("li"),NZ=a("strong"),Ofo=o("longformer"),Gfo=o(" \u2014 "),dP=a("a"),Xfo=o("LongformerForMaskedLM"),Vfo=o(" (Longformer model)"),zfo=l(),Ip=a("li"),qZ=a("strong"),Wfo=o("lxmert"),Qfo=o(" \u2014 "),cP=a("a"),Hfo=o("LxmertForPreTraining"),Ufo=o(" (LXMERT model)"),Jfo=l(),jp=a("li"),OZ=a("strong"),Yfo=o("megatron-bert"),Kfo=o(" \u2014 "),mP=a("a"),Zfo=o("MegatronBertForPreTraining"),ego=o(" (MegatronBert model)"),ogo=l(),Dp=a("li"),GZ=a("strong"),tgo=o("mobilebert"),rgo=o(" \u2014 "),fP=a("a"),ago=o("MobileBertForPreTraining"),sgo=o(" (MobileBERT model)"),ngo=l(),Np=a("li"),XZ=a("strong"),lgo=o("mpnet"),igo=o(" \u2014 "),gP=a("a"),dgo=o("MPNetForMaskedLM"),cgo=o(" (MPNet model)"),mgo=l(),qp=a("li"),VZ=a("strong"),fgo=o("openai-gpt"),ggo=o(" \u2014 "),hP=a("a"),hgo=o("OpenAIGPTLMHeadModel"),ugo=o(" (OpenAI GPT model)"),pgo=l(),Op=a("li"),zZ=a("strong"),_go=o("retribert"),bgo=o(" \u2014 "),uP=a("a"),vgo=o("RetriBertModel"),Tgo=o(" (RetriBERT model)"),Fgo=l(),Gp=a("li"),WZ=a("strong"),Cgo=o("roberta"),Mgo=o(" \u2014 "),pP=a("a"),Ego=o("RobertaForMaskedLM"),ygo=o(" (RoBERTa model)"),wgo=l(),Xp=a("li"),QZ=a("strong"),Ago=o("squeezebert"),Lgo=o(" \u2014 "),_P=a("a"),Bgo=o("SqueezeBertForMaskedLM"),xgo=o(" (SqueezeBERT model)"),kgo=l(),Vp=a("li"),HZ=a("strong"),Rgo=o("t5"),Sgo=o(" \u2014 "),bP=a("a"),Pgo=o("T5ForConditionalGeneration"),$go=o(" (T5 model)"),Igo=l(),zp=a("li"),UZ=a("strong"),jgo=o("tapas"),Dgo=o(" \u2014 "),vP=a("a"),Ngo=o("TapasForMaskedLM"),qgo=o(" (TAPAS model)"),Ogo=l(),Wp=a("li"),JZ=a("strong"),Ggo=o("transfo-xl"),Xgo=o(" \u2014 "),TP=a("a"),Vgo=o("TransfoXLLMHeadModel"),zgo=o(" (Transformer-XL model)"),Wgo=l(),Qp=a("li"),YZ=a("strong"),Qgo=o("unispeech"),Hgo=o(" \u2014 "),FP=a("a"),Ugo=o("UniSpeechForPreTraining"),Jgo=o(" (UniSpeech model)"),Ygo=l(),Hp=a("li"),KZ=a("strong"),Kgo=o("unispeech-sat"),Zgo=o(" \u2014 "),CP=a("a"),eho=o("UniSpeechSatForPreTraining"),oho=o(" (UniSpeechSat model)"),tho=l(),Up=a("li"),ZZ=a("strong"),rho=o("visual_bert"),aho=o(" \u2014 "),MP=a("a"),sho=o("VisualBertForPreTraining"),nho=o(" (VisualBert model)"),lho=l(),Jp=a("li"),eee=a("strong"),iho=o("vit_mae"),dho=o(" \u2014 "),EP=a("a"),cho=o("ViTMAEForPreTraining"),mho=o(" (ViTMAE model)"),fho=l(),Yp=a("li"),oee=a("strong"),gho=o("wav2vec2"),hho=o(" \u2014 "),yP=a("a"),uho=o("Wav2Vec2ForPreTraining"),pho=o(" (Wav2Vec2 model)"),_ho=l(),Kp=a("li"),tee=a("strong"),bho=o("xlm"),vho=o(" \u2014 "),wP=a("a"),Tho=o("XLMWithLMHeadModel"),Fho=o(" (XLM model)"),Cho=l(),Zp=a("li"),ree=a("strong"),Mho=o("xlm-roberta"),Eho=o(" \u2014 "),AP=a("a"),yho=o("XLMRobertaForMaskedLM"),who=o(" (XLM-RoBERTa model)"),Aho=l(),e_=a("li"),aee=a("strong"),Lho=o("xlm-roberta-xl"),Bho=o(" \u2014 "),LP=a("a"),xho=o("XLMRobertaXLForMaskedLM"),kho=o(" (XLM-RoBERTa-XL model)"),Rho=l(),o_=a("li"),see=a("strong"),Sho=o("xlnet"),Pho=o(" \u2014 "),BP=a("a"),$ho=o("XLNetLMHeadModel"),Iho=o(" (XLNet model)"),jho=l(),t_=a("p"),Dho=o("The model is set in evaluation mode by default using "),nee=a("code"),Nho=o("model.eval()"),qho=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lee=a("code"),Oho=o("model.train()"),Gho=l(),iee=a("p"),Xho=o("Examples:"),Vho=l(),m(ky.$$.fragment),OBe=l(),td=a("h2"),r_=a("a"),dee=a("span"),m(Ry.$$.fragment),zho=l(),cee=a("span"),Who=o("AutoModelForCausalLM"),GBe=l(),Jo=a("div"),m(Sy.$$.fragment),Qho=l(),rd=a("p"),Hho=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),mee=a("code"),Uho=o("from_pretrained()"),Jho=o("class method or the "),fee=a("code"),Yho=o("from_config()"),Kho=o(`class
method.`),Zho=l(),Py=a("p"),euo=o("This class cannot be instantiated directly using "),gee=a("code"),ouo=o("__init__()"),tuo=o(" (throws an error)."),ruo=l(),Vt=a("div"),m($y.$$.fragment),auo=l(),hee=a("p"),suo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),nuo=l(),ad=a("p"),luo=o(`Note:
Loading a model from its configuration file does `),uee=a("strong"),iuo=o("not"),duo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pee=a("code"),cuo=o("from_pretrained()"),muo=o("to load the model weights."),fuo=l(),_ee=a("p"),guo=o("Examples:"),huo=l(),m(Iy.$$.fragment),uuo=l(),Pe=a("div"),m(jy.$$.fragment),puo=l(),bee=a("p"),_uo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),buo=l(),za=a("p"),vuo=o("The model class to instantiate is selected based on the "),vee=a("code"),Tuo=o("model_type"),Fuo=o(` property of the config object (either
passed as an argument or loaded from `),Tee=a("code"),Cuo=o("pretrained_model_name_or_path"),Muo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fee=a("code"),Euo=o("pretrained_model_name_or_path"),yuo=o(":"),wuo=l(),$=a("ul"),a_=a("li"),Cee=a("strong"),Auo=o("bart"),Luo=o(" \u2014 "),xP=a("a"),Buo=o("BartForCausalLM"),xuo=o(" (BART model)"),kuo=l(),s_=a("li"),Mee=a("strong"),Ruo=o("bert"),Suo=o(" \u2014 "),kP=a("a"),Puo=o("BertLMHeadModel"),$uo=o(" (BERT model)"),Iuo=l(),n_=a("li"),Eee=a("strong"),juo=o("bert-generation"),Duo=o(" \u2014 "),RP=a("a"),Nuo=o("BertGenerationDecoder"),quo=o(" (Bert Generation model)"),Ouo=l(),l_=a("li"),yee=a("strong"),Guo=o("big_bird"),Xuo=o(" \u2014 "),SP=a("a"),Vuo=o("BigBirdForCausalLM"),zuo=o(" (BigBird model)"),Wuo=l(),i_=a("li"),wee=a("strong"),Quo=o("bigbird_pegasus"),Huo=o(" \u2014 "),PP=a("a"),Uuo=o("BigBirdPegasusForCausalLM"),Juo=o(" (BigBirdPegasus model)"),Yuo=l(),d_=a("li"),Aee=a("strong"),Kuo=o("blenderbot"),Zuo=o(" \u2014 "),$P=a("a"),epo=o("BlenderbotForCausalLM"),opo=o(" (Blenderbot model)"),tpo=l(),c_=a("li"),Lee=a("strong"),rpo=o("blenderbot-small"),apo=o(" \u2014 "),IP=a("a"),spo=o("BlenderbotSmallForCausalLM"),npo=o(" (BlenderbotSmall model)"),lpo=l(),m_=a("li"),Bee=a("strong"),ipo=o("camembert"),dpo=o(" \u2014 "),jP=a("a"),cpo=o("CamembertForCausalLM"),mpo=o(" (CamemBERT model)"),fpo=l(),f_=a("li"),xee=a("strong"),gpo=o("ctrl"),hpo=o(" \u2014 "),DP=a("a"),upo=o("CTRLLMHeadModel"),ppo=o(" (CTRL model)"),_po=l(),g_=a("li"),kee=a("strong"),bpo=o("data2vec-text"),vpo=o(" \u2014 "),NP=a("a"),Tpo=o("Data2VecTextForCausalLM"),Fpo=o(" (Data2VecText model)"),Cpo=l(),h_=a("li"),Ree=a("strong"),Mpo=o("electra"),Epo=o(" \u2014 "),qP=a("a"),ypo=o("ElectraForCausalLM"),wpo=o(" (ELECTRA model)"),Apo=l(),u_=a("li"),See=a("strong"),Lpo=o("gpt2"),Bpo=o(" \u2014 "),OP=a("a"),xpo=o("GPT2LMHeadModel"),kpo=o(" (OpenAI GPT-2 model)"),Rpo=l(),p_=a("li"),Pee=a("strong"),Spo=o("gpt_neo"),Ppo=o(" \u2014 "),GP=a("a"),$po=o("GPTNeoForCausalLM"),Ipo=o(" (GPT Neo model)"),jpo=l(),__=a("li"),$ee=a("strong"),Dpo=o("gptj"),Npo=o(" \u2014 "),XP=a("a"),qpo=o("GPTJForCausalLM"),Opo=o(" (GPT-J model)"),Gpo=l(),b_=a("li"),Iee=a("strong"),Xpo=o("marian"),Vpo=o(" \u2014 "),VP=a("a"),zpo=o("MarianForCausalLM"),Wpo=o(" (Marian model)"),Qpo=l(),v_=a("li"),jee=a("strong"),Hpo=o("mbart"),Upo=o(" \u2014 "),zP=a("a"),Jpo=o("MBartForCausalLM"),Ypo=o(" (mBART model)"),Kpo=l(),T_=a("li"),Dee=a("strong"),Zpo=o("megatron-bert"),e_o=o(" \u2014 "),WP=a("a"),o_o=o("MegatronBertForCausalLM"),t_o=o(" (MegatronBert model)"),r_o=l(),F_=a("li"),Nee=a("strong"),a_o=o("openai-gpt"),s_o=o(" \u2014 "),QP=a("a"),n_o=o("OpenAIGPTLMHeadModel"),l_o=o(" (OpenAI GPT model)"),i_o=l(),C_=a("li"),qee=a("strong"),d_o=o("pegasus"),c_o=o(" \u2014 "),HP=a("a"),m_o=o("PegasusForCausalLM"),f_o=o(" (Pegasus model)"),g_o=l(),M_=a("li"),Oee=a("strong"),h_o=o("plbart"),u_o=o(" \u2014 "),UP=a("a"),p_o=o("PLBartForCausalLM"),__o=o(" (PLBart model)"),b_o=l(),E_=a("li"),Gee=a("strong"),v_o=o("prophetnet"),T_o=o(" \u2014 "),JP=a("a"),F_o=o("ProphetNetForCausalLM"),C_o=o(" (ProphetNet model)"),M_o=l(),y_=a("li"),Xee=a("strong"),E_o=o("qdqbert"),y_o=o(" \u2014 "),YP=a("a"),w_o=o("QDQBertLMHeadModel"),A_o=o(" (QDQBert model)"),L_o=l(),w_=a("li"),Vee=a("strong"),B_o=o("reformer"),x_o=o(" \u2014 "),KP=a("a"),k_o=o("ReformerModelWithLMHead"),R_o=o(" (Reformer model)"),S_o=l(),A_=a("li"),zee=a("strong"),P_o=o("rembert"),$_o=o(" \u2014 "),ZP=a("a"),I_o=o("RemBertForCausalLM"),j_o=o(" (RemBERT model)"),D_o=l(),L_=a("li"),Wee=a("strong"),N_o=o("roberta"),q_o=o(" \u2014 "),e$=a("a"),O_o=o("RobertaForCausalLM"),G_o=o(" (RoBERTa model)"),X_o=l(),B_=a("li"),Qee=a("strong"),V_o=o("roformer"),z_o=o(" \u2014 "),o$=a("a"),W_o=o("RoFormerForCausalLM"),Q_o=o(" (RoFormer model)"),H_o=l(),x_=a("li"),Hee=a("strong"),U_o=o("speech_to_text_2"),J_o=o(" \u2014 "),t$=a("a"),Y_o=o("Speech2Text2ForCausalLM"),K_o=o(" (Speech2Text2 model)"),Z_o=l(),k_=a("li"),Uee=a("strong"),ebo=o("transfo-xl"),obo=o(" \u2014 "),r$=a("a"),tbo=o("TransfoXLLMHeadModel"),rbo=o(" (Transformer-XL model)"),abo=l(),R_=a("li"),Jee=a("strong"),sbo=o("trocr"),nbo=o(" \u2014 "),a$=a("a"),lbo=o("TrOCRForCausalLM"),ibo=o(" (TrOCR model)"),dbo=l(),S_=a("li"),Yee=a("strong"),cbo=o("xglm"),mbo=o(" \u2014 "),s$=a("a"),fbo=o("XGLMForCausalLM"),gbo=o(" (XGLM model)"),hbo=l(),P_=a("li"),Kee=a("strong"),ubo=o("xlm"),pbo=o(" \u2014 "),n$=a("a"),_bo=o("XLMWithLMHeadModel"),bbo=o(" (XLM model)"),vbo=l(),$_=a("li"),Zee=a("strong"),Tbo=o("xlm-prophetnet"),Fbo=o(" \u2014 "),l$=a("a"),Cbo=o("XLMProphetNetForCausalLM"),Mbo=o(" (XLMProphetNet model)"),Ebo=l(),I_=a("li"),eoe=a("strong"),ybo=o("xlm-roberta"),wbo=o(" \u2014 "),i$=a("a"),Abo=o("XLMRobertaForCausalLM"),Lbo=o(" (XLM-RoBERTa model)"),Bbo=l(),j_=a("li"),ooe=a("strong"),xbo=o("xlm-roberta-xl"),kbo=o(" \u2014 "),d$=a("a"),Rbo=o("XLMRobertaXLForCausalLM"),Sbo=o(" (XLM-RoBERTa-XL model)"),Pbo=l(),D_=a("li"),toe=a("strong"),$bo=o("xlnet"),Ibo=o(" \u2014 "),c$=a("a"),jbo=o("XLNetLMHeadModel"),Dbo=o(" (XLNet model)"),Nbo=l(),N_=a("p"),qbo=o("The model is set in evaluation mode by default using "),roe=a("code"),Obo=o("model.eval()"),Gbo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),aoe=a("code"),Xbo=o("model.train()"),Vbo=l(),soe=a("p"),zbo=o("Examples:"),Wbo=l(),m(Dy.$$.fragment),XBe=l(),sd=a("h2"),q_=a("a"),noe=a("span"),m(Ny.$$.fragment),Qbo=l(),loe=a("span"),Hbo=o("AutoModelForMaskedLM"),VBe=l(),Yo=a("div"),m(qy.$$.fragment),Ubo=l(),nd=a("p"),Jbo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),ioe=a("code"),Ybo=o("from_pretrained()"),Kbo=o("class method or the "),doe=a("code"),Zbo=o("from_config()"),e2o=o(`class
method.`),o2o=l(),Oy=a("p"),t2o=o("This class cannot be instantiated directly using "),coe=a("code"),r2o=o("__init__()"),a2o=o(" (throws an error)."),s2o=l(),zt=a("div"),m(Gy.$$.fragment),n2o=l(),moe=a("p"),l2o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),i2o=l(),ld=a("p"),d2o=o(`Note:
Loading a model from its configuration file does `),foe=a("strong"),c2o=o("not"),m2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),goe=a("code"),f2o=o("from_pretrained()"),g2o=o("to load the model weights."),h2o=l(),hoe=a("p"),u2o=o("Examples:"),p2o=l(),m(Xy.$$.fragment),_2o=l(),$e=a("div"),m(Vy.$$.fragment),b2o=l(),uoe=a("p"),v2o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),T2o=l(),Wa=a("p"),F2o=o("The model class to instantiate is selected based on the "),poe=a("code"),C2o=o("model_type"),M2o=o(` property of the config object (either
passed as an argument or loaded from `),_oe=a("code"),E2o=o("pretrained_model_name_or_path"),y2o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),boe=a("code"),w2o=o("pretrained_model_name_or_path"),A2o=o(":"),L2o=l(),I=a("ul"),O_=a("li"),voe=a("strong"),B2o=o("albert"),x2o=o(" \u2014 "),m$=a("a"),k2o=o("AlbertForMaskedLM"),R2o=o(" (ALBERT model)"),S2o=l(),G_=a("li"),Toe=a("strong"),P2o=o("bart"),$2o=o(" \u2014 "),f$=a("a"),I2o=o("BartForConditionalGeneration"),j2o=o(" (BART model)"),D2o=l(),X_=a("li"),Foe=a("strong"),N2o=o("bert"),q2o=o(" \u2014 "),g$=a("a"),O2o=o("BertForMaskedLM"),G2o=o(" (BERT model)"),X2o=l(),V_=a("li"),Coe=a("strong"),V2o=o("big_bird"),z2o=o(" \u2014 "),h$=a("a"),W2o=o("BigBirdForMaskedLM"),Q2o=o(" (BigBird model)"),H2o=l(),z_=a("li"),Moe=a("strong"),U2o=o("camembert"),J2o=o(" \u2014 "),u$=a("a"),Y2o=o("CamembertForMaskedLM"),K2o=o(" (CamemBERT model)"),Z2o=l(),W_=a("li"),Eoe=a("strong"),evo=o("convbert"),ovo=o(" \u2014 "),p$=a("a"),tvo=o("ConvBertForMaskedLM"),rvo=o(" (ConvBERT model)"),avo=l(),Q_=a("li"),yoe=a("strong"),svo=o("data2vec-text"),nvo=o(" \u2014 "),_$=a("a"),lvo=o("Data2VecTextForMaskedLM"),ivo=o(" (Data2VecText model)"),dvo=l(),H_=a("li"),woe=a("strong"),cvo=o("deberta"),mvo=o(" \u2014 "),b$=a("a"),fvo=o("DebertaForMaskedLM"),gvo=o(" (DeBERTa model)"),hvo=l(),U_=a("li"),Aoe=a("strong"),uvo=o("deberta-v2"),pvo=o(" \u2014 "),v$=a("a"),_vo=o("DebertaV2ForMaskedLM"),bvo=o(" (DeBERTa-v2 model)"),vvo=l(),J_=a("li"),Loe=a("strong"),Tvo=o("distilbert"),Fvo=o(" \u2014 "),T$=a("a"),Cvo=o("DistilBertForMaskedLM"),Mvo=o(" (DistilBERT model)"),Evo=l(),Y_=a("li"),Boe=a("strong"),yvo=o("electra"),wvo=o(" \u2014 "),F$=a("a"),Avo=o("ElectraForMaskedLM"),Lvo=o(" (ELECTRA model)"),Bvo=l(),K_=a("li"),xoe=a("strong"),xvo=o("flaubert"),kvo=o(" \u2014 "),C$=a("a"),Rvo=o("FlaubertWithLMHeadModel"),Svo=o(" (FlauBERT model)"),Pvo=l(),Z_=a("li"),koe=a("strong"),$vo=o("fnet"),Ivo=o(" \u2014 "),M$=a("a"),jvo=o("FNetForMaskedLM"),Dvo=o(" (FNet model)"),Nvo=l(),eb=a("li"),Roe=a("strong"),qvo=o("funnel"),Ovo=o(" \u2014 "),E$=a("a"),Gvo=o("FunnelForMaskedLM"),Xvo=o(" (Funnel Transformer model)"),Vvo=l(),ob=a("li"),Soe=a("strong"),zvo=o("ibert"),Wvo=o(" \u2014 "),y$=a("a"),Qvo=o("IBertForMaskedLM"),Hvo=o(" (I-BERT model)"),Uvo=l(),tb=a("li"),Poe=a("strong"),Jvo=o("layoutlm"),Yvo=o(" \u2014 "),w$=a("a"),Kvo=o("LayoutLMForMaskedLM"),Zvo=o(" (LayoutLM model)"),eTo=l(),rb=a("li"),$oe=a("strong"),oTo=o("longformer"),tTo=o(" \u2014 "),A$=a("a"),rTo=o("LongformerForMaskedLM"),aTo=o(" (Longformer model)"),sTo=l(),ab=a("li"),Ioe=a("strong"),nTo=o("mbart"),lTo=o(" \u2014 "),L$=a("a"),iTo=o("MBartForConditionalGeneration"),dTo=o(" (mBART model)"),cTo=l(),sb=a("li"),joe=a("strong"),mTo=o("megatron-bert"),fTo=o(" \u2014 "),B$=a("a"),gTo=o("MegatronBertForMaskedLM"),hTo=o(" (MegatronBert model)"),uTo=l(),nb=a("li"),Doe=a("strong"),pTo=o("mobilebert"),_To=o(" \u2014 "),x$=a("a"),bTo=o("MobileBertForMaskedLM"),vTo=o(" (MobileBERT model)"),TTo=l(),lb=a("li"),Noe=a("strong"),FTo=o("mpnet"),CTo=o(" \u2014 "),k$=a("a"),MTo=o("MPNetForMaskedLM"),ETo=o(" (MPNet model)"),yTo=l(),ib=a("li"),qoe=a("strong"),wTo=o("nystromformer"),ATo=o(" \u2014 "),R$=a("a"),LTo=o("NystromformerForMaskedLM"),BTo=o(" (Nystromformer model)"),xTo=l(),db=a("li"),Ooe=a("strong"),kTo=o("perceiver"),RTo=o(" \u2014 "),S$=a("a"),STo=o("PerceiverForMaskedLM"),PTo=o(" (Perceiver model)"),$To=l(),cb=a("li"),Goe=a("strong"),ITo=o("qdqbert"),jTo=o(" \u2014 "),P$=a("a"),DTo=o("QDQBertForMaskedLM"),NTo=o(" (QDQBert model)"),qTo=l(),mb=a("li"),Xoe=a("strong"),OTo=o("reformer"),GTo=o(" \u2014 "),$$=a("a"),XTo=o("ReformerForMaskedLM"),VTo=o(" (Reformer model)"),zTo=l(),fb=a("li"),Voe=a("strong"),WTo=o("rembert"),QTo=o(" \u2014 "),I$=a("a"),HTo=o("RemBertForMaskedLM"),UTo=o(" (RemBERT model)"),JTo=l(),gb=a("li"),zoe=a("strong"),YTo=o("roberta"),KTo=o(" \u2014 "),j$=a("a"),ZTo=o("RobertaForMaskedLM"),e1o=o(" (RoBERTa model)"),o1o=l(),hb=a("li"),Woe=a("strong"),t1o=o("roformer"),r1o=o(" \u2014 "),D$=a("a"),a1o=o("RoFormerForMaskedLM"),s1o=o(" (RoFormer model)"),n1o=l(),ub=a("li"),Qoe=a("strong"),l1o=o("squeezebert"),i1o=o(" \u2014 "),N$=a("a"),d1o=o("SqueezeBertForMaskedLM"),c1o=o(" (SqueezeBERT model)"),m1o=l(),pb=a("li"),Hoe=a("strong"),f1o=o("tapas"),g1o=o(" \u2014 "),q$=a("a"),h1o=o("TapasForMaskedLM"),u1o=o(" (TAPAS model)"),p1o=l(),_b=a("li"),Uoe=a("strong"),_1o=o("wav2vec2"),b1o=o(" \u2014 "),Joe=a("code"),v1o=o("Wav2Vec2ForMaskedLM"),T1o=o("(Wav2Vec2 model)"),F1o=l(),bb=a("li"),Yoe=a("strong"),C1o=o("xlm"),M1o=o(" \u2014 "),O$=a("a"),E1o=o("XLMWithLMHeadModel"),y1o=o(" (XLM model)"),w1o=l(),vb=a("li"),Koe=a("strong"),A1o=o("xlm-roberta"),L1o=o(" \u2014 "),G$=a("a"),B1o=o("XLMRobertaForMaskedLM"),x1o=o(" (XLM-RoBERTa model)"),k1o=l(),Tb=a("li"),Zoe=a("strong"),R1o=o("xlm-roberta-xl"),S1o=o(" \u2014 "),X$=a("a"),P1o=o("XLMRobertaXLForMaskedLM"),$1o=o(" (XLM-RoBERTa-XL model)"),I1o=l(),Fb=a("li"),ete=a("strong"),j1o=o("yoso"),D1o=o(" \u2014 "),V$=a("a"),N1o=o("YosoForMaskedLM"),q1o=o(" (YOSO model)"),O1o=l(),Cb=a("p"),G1o=o("The model is set in evaluation mode by default using "),ote=a("code"),X1o=o("model.eval()"),V1o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tte=a("code"),z1o=o("model.train()"),W1o=l(),rte=a("p"),Q1o=o("Examples:"),H1o=l(),m(zy.$$.fragment),zBe=l(),id=a("h2"),Mb=a("a"),ate=a("span"),m(Wy.$$.fragment),U1o=l(),ste=a("span"),J1o=o("AutoModelForSeq2SeqLM"),WBe=l(),Ko=a("div"),m(Qy.$$.fragment),Y1o=l(),dd=a("p"),K1o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),nte=a("code"),Z1o=o("from_pretrained()"),eFo=o("class method or the "),lte=a("code"),oFo=o("from_config()"),tFo=o(`class
method.`),rFo=l(),Hy=a("p"),aFo=o("This class cannot be instantiated directly using "),ite=a("code"),sFo=o("__init__()"),nFo=o(" (throws an error)."),lFo=l(),Wt=a("div"),m(Uy.$$.fragment),iFo=l(),dte=a("p"),dFo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),cFo=l(),cd=a("p"),mFo=o(`Note:
Loading a model from its configuration file does `),cte=a("strong"),fFo=o("not"),gFo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mte=a("code"),hFo=o("from_pretrained()"),uFo=o("to load the model weights."),pFo=l(),fte=a("p"),_Fo=o("Examples:"),bFo=l(),m(Jy.$$.fragment),vFo=l(),Ie=a("div"),m(Yy.$$.fragment),TFo=l(),gte=a("p"),FFo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),CFo=l(),Qa=a("p"),MFo=o("The model class to instantiate is selected based on the "),hte=a("code"),EFo=o("model_type"),yFo=o(` property of the config object (either
passed as an argument or loaded from `),ute=a("code"),wFo=o("pretrained_model_name_or_path"),AFo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pte=a("code"),LFo=o("pretrained_model_name_or_path"),BFo=o(":"),xFo=l(),se=a("ul"),Eb=a("li"),_te=a("strong"),kFo=o("bart"),RFo=o(" \u2014 "),z$=a("a"),SFo=o("BartForConditionalGeneration"),PFo=o(" (BART model)"),$Fo=l(),yb=a("li"),bte=a("strong"),IFo=o("bigbird_pegasus"),jFo=o(" \u2014 "),W$=a("a"),DFo=o("BigBirdPegasusForConditionalGeneration"),NFo=o(" (BigBirdPegasus model)"),qFo=l(),wb=a("li"),vte=a("strong"),OFo=o("blenderbot"),GFo=o(" \u2014 "),Q$=a("a"),XFo=o("BlenderbotForConditionalGeneration"),VFo=o(" (Blenderbot model)"),zFo=l(),Ab=a("li"),Tte=a("strong"),WFo=o("blenderbot-small"),QFo=o(" \u2014 "),H$=a("a"),HFo=o("BlenderbotSmallForConditionalGeneration"),UFo=o(" (BlenderbotSmall model)"),JFo=l(),Lb=a("li"),Fte=a("strong"),YFo=o("encoder-decoder"),KFo=o(" \u2014 "),U$=a("a"),ZFo=o("EncoderDecoderModel"),eCo=o(" (Encoder decoder model)"),oCo=l(),Bb=a("li"),Cte=a("strong"),tCo=o("fsmt"),rCo=o(" \u2014 "),J$=a("a"),aCo=o("FSMTForConditionalGeneration"),sCo=o(" (FairSeq Machine-Translation model)"),nCo=l(),xb=a("li"),Mte=a("strong"),lCo=o("led"),iCo=o(" \u2014 "),Y$=a("a"),dCo=o("LEDForConditionalGeneration"),cCo=o(" (LED model)"),mCo=l(),kb=a("li"),Ete=a("strong"),fCo=o("m2m_100"),gCo=o(" \u2014 "),K$=a("a"),hCo=o("M2M100ForConditionalGeneration"),uCo=o(" (M2M100 model)"),pCo=l(),Rb=a("li"),yte=a("strong"),_Co=o("marian"),bCo=o(" \u2014 "),Z$=a("a"),vCo=o("MarianMTModel"),TCo=o(" (Marian model)"),FCo=l(),Sb=a("li"),wte=a("strong"),CCo=o("mbart"),MCo=o(" \u2014 "),eI=a("a"),ECo=o("MBartForConditionalGeneration"),yCo=o(" (mBART model)"),wCo=l(),Pb=a("li"),Ate=a("strong"),ACo=o("mt5"),LCo=o(" \u2014 "),oI=a("a"),BCo=o("MT5ForConditionalGeneration"),xCo=o(" (mT5 model)"),kCo=l(),$b=a("li"),Lte=a("strong"),RCo=o("pegasus"),SCo=o(" \u2014 "),tI=a("a"),PCo=o("PegasusForConditionalGeneration"),$Co=o(" (Pegasus model)"),ICo=l(),Ib=a("li"),Bte=a("strong"),jCo=o("plbart"),DCo=o(" \u2014 "),rI=a("a"),NCo=o("PLBartForConditionalGeneration"),qCo=o(" (PLBart model)"),OCo=l(),jb=a("li"),xte=a("strong"),GCo=o("prophetnet"),XCo=o(" \u2014 "),aI=a("a"),VCo=o("ProphetNetForConditionalGeneration"),zCo=o(" (ProphetNet model)"),WCo=l(),Db=a("li"),kte=a("strong"),QCo=o("t5"),HCo=o(" \u2014 "),sI=a("a"),UCo=o("T5ForConditionalGeneration"),JCo=o(" (T5 model)"),YCo=l(),Nb=a("li"),Rte=a("strong"),KCo=o("xlm-prophetnet"),ZCo=o(" \u2014 "),nI=a("a"),e4o=o("XLMProphetNetForConditionalGeneration"),o4o=o(" (XLMProphetNet model)"),t4o=l(),qb=a("p"),r4o=o("The model is set in evaluation mode by default using "),Ste=a("code"),a4o=o("model.eval()"),s4o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pte=a("code"),n4o=o("model.train()"),l4o=l(),$te=a("p"),i4o=o("Examples:"),d4o=l(),m(Ky.$$.fragment),QBe=l(),md=a("h2"),Ob=a("a"),Ite=a("span"),m(Zy.$$.fragment),c4o=l(),jte=a("span"),m4o=o("AutoModelForSequenceClassification"),HBe=l(),Zo=a("div"),m(ew.$$.fragment),f4o=l(),fd=a("p"),g4o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Dte=a("code"),h4o=o("from_pretrained()"),u4o=o("class method or the "),Nte=a("code"),p4o=o("from_config()"),_4o=o(`class
method.`),b4o=l(),ow=a("p"),v4o=o("This class cannot be instantiated directly using "),qte=a("code"),T4o=o("__init__()"),F4o=o(" (throws an error)."),C4o=l(),Qt=a("div"),m(tw.$$.fragment),M4o=l(),Ote=a("p"),E4o=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),y4o=l(),gd=a("p"),w4o=o(`Note:
Loading a model from its configuration file does `),Gte=a("strong"),A4o=o("not"),L4o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xte=a("code"),B4o=o("from_pretrained()"),x4o=o("to load the model weights."),k4o=l(),Vte=a("p"),R4o=o("Examples:"),S4o=l(),m(rw.$$.fragment),P4o=l(),je=a("div"),m(aw.$$.fragment),$4o=l(),zte=a("p"),I4o=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),j4o=l(),Ha=a("p"),D4o=o("The model class to instantiate is selected based on the "),Wte=a("code"),N4o=o("model_type"),q4o=o(` property of the config object (either
passed as an argument or loaded from `),Qte=a("code"),O4o=o("pretrained_model_name_or_path"),G4o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hte=a("code"),X4o=o("pretrained_model_name_or_path"),V4o=o(":"),z4o=l(),A=a("ul"),Gb=a("li"),Ute=a("strong"),W4o=o("albert"),Q4o=o(" \u2014 "),lI=a("a"),H4o=o("AlbertForSequenceClassification"),U4o=o(" (ALBERT model)"),J4o=l(),Xb=a("li"),Jte=a("strong"),Y4o=o("bart"),K4o=o(" \u2014 "),iI=a("a"),Z4o=o("BartForSequenceClassification"),eMo=o(" (BART model)"),oMo=l(),Vb=a("li"),Yte=a("strong"),tMo=o("bert"),rMo=o(" \u2014 "),dI=a("a"),aMo=o("BertForSequenceClassification"),sMo=o(" (BERT model)"),nMo=l(),zb=a("li"),Kte=a("strong"),lMo=o("big_bird"),iMo=o(" \u2014 "),cI=a("a"),dMo=o("BigBirdForSequenceClassification"),cMo=o(" (BigBird model)"),mMo=l(),Wb=a("li"),Zte=a("strong"),fMo=o("bigbird_pegasus"),gMo=o(" \u2014 "),mI=a("a"),hMo=o("BigBirdPegasusForSequenceClassification"),uMo=o(" (BigBirdPegasus model)"),pMo=l(),Qb=a("li"),ere=a("strong"),_Mo=o("camembert"),bMo=o(" \u2014 "),fI=a("a"),vMo=o("CamembertForSequenceClassification"),TMo=o(" (CamemBERT model)"),FMo=l(),Hb=a("li"),ore=a("strong"),CMo=o("canine"),MMo=o(" \u2014 "),gI=a("a"),EMo=o("CanineForSequenceClassification"),yMo=o(" (Canine model)"),wMo=l(),Ub=a("li"),tre=a("strong"),AMo=o("convbert"),LMo=o(" \u2014 "),hI=a("a"),BMo=o("ConvBertForSequenceClassification"),xMo=o(" (ConvBERT model)"),kMo=l(),Jb=a("li"),rre=a("strong"),RMo=o("ctrl"),SMo=o(" \u2014 "),uI=a("a"),PMo=o("CTRLForSequenceClassification"),$Mo=o(" (CTRL model)"),IMo=l(),Yb=a("li"),are=a("strong"),jMo=o("data2vec-text"),DMo=o(" \u2014 "),pI=a("a"),NMo=o("Data2VecTextForSequenceClassification"),qMo=o(" (Data2VecText model)"),OMo=l(),Kb=a("li"),sre=a("strong"),GMo=o("deberta"),XMo=o(" \u2014 "),_I=a("a"),VMo=o("DebertaForSequenceClassification"),zMo=o(" (DeBERTa model)"),WMo=l(),Zb=a("li"),nre=a("strong"),QMo=o("deberta-v2"),HMo=o(" \u2014 "),bI=a("a"),UMo=o("DebertaV2ForSequenceClassification"),JMo=o(" (DeBERTa-v2 model)"),YMo=l(),e2=a("li"),lre=a("strong"),KMo=o("distilbert"),ZMo=o(" \u2014 "),vI=a("a"),eEo=o("DistilBertForSequenceClassification"),oEo=o(" (DistilBERT model)"),tEo=l(),o2=a("li"),ire=a("strong"),rEo=o("electra"),aEo=o(" \u2014 "),TI=a("a"),sEo=o("ElectraForSequenceClassification"),nEo=o(" (ELECTRA model)"),lEo=l(),t2=a("li"),dre=a("strong"),iEo=o("flaubert"),dEo=o(" \u2014 "),FI=a("a"),cEo=o("FlaubertForSequenceClassification"),mEo=o(" (FlauBERT model)"),fEo=l(),r2=a("li"),cre=a("strong"),gEo=o("fnet"),hEo=o(" \u2014 "),CI=a("a"),uEo=o("FNetForSequenceClassification"),pEo=o(" (FNet model)"),_Eo=l(),a2=a("li"),mre=a("strong"),bEo=o("funnel"),vEo=o(" \u2014 "),MI=a("a"),TEo=o("FunnelForSequenceClassification"),FEo=o(" (Funnel Transformer model)"),CEo=l(),s2=a("li"),fre=a("strong"),MEo=o("gpt2"),EEo=o(" \u2014 "),EI=a("a"),yEo=o("GPT2ForSequenceClassification"),wEo=o(" (OpenAI GPT-2 model)"),AEo=l(),n2=a("li"),gre=a("strong"),LEo=o("gpt_neo"),BEo=o(" \u2014 "),yI=a("a"),xEo=o("GPTNeoForSequenceClassification"),kEo=o(" (GPT Neo model)"),REo=l(),l2=a("li"),hre=a("strong"),SEo=o("gptj"),PEo=o(" \u2014 "),wI=a("a"),$Eo=o("GPTJForSequenceClassification"),IEo=o(" (GPT-J model)"),jEo=l(),i2=a("li"),ure=a("strong"),DEo=o("ibert"),NEo=o(" \u2014 "),AI=a("a"),qEo=o("IBertForSequenceClassification"),OEo=o(" (I-BERT model)"),GEo=l(),d2=a("li"),pre=a("strong"),XEo=o("layoutlm"),VEo=o(" \u2014 "),LI=a("a"),zEo=o("LayoutLMForSequenceClassification"),WEo=o(" (LayoutLM model)"),QEo=l(),c2=a("li"),_re=a("strong"),HEo=o("layoutlmv2"),UEo=o(" \u2014 "),BI=a("a"),JEo=o("LayoutLMv2ForSequenceClassification"),YEo=o(" (LayoutLMv2 model)"),KEo=l(),m2=a("li"),bre=a("strong"),ZEo=o("led"),e3o=o(" \u2014 "),xI=a("a"),o3o=o("LEDForSequenceClassification"),t3o=o(" (LED model)"),r3o=l(),f2=a("li"),vre=a("strong"),a3o=o("longformer"),s3o=o(" \u2014 "),kI=a("a"),n3o=o("LongformerForSequenceClassification"),l3o=o(" (Longformer model)"),i3o=l(),g2=a("li"),Tre=a("strong"),d3o=o("mbart"),c3o=o(" \u2014 "),RI=a("a"),m3o=o("MBartForSequenceClassification"),f3o=o(" (mBART model)"),g3o=l(),h2=a("li"),Fre=a("strong"),h3o=o("megatron-bert"),u3o=o(" \u2014 "),SI=a("a"),p3o=o("MegatronBertForSequenceClassification"),_3o=o(" (MegatronBert model)"),b3o=l(),u2=a("li"),Cre=a("strong"),v3o=o("mobilebert"),T3o=o(" \u2014 "),PI=a("a"),F3o=o("MobileBertForSequenceClassification"),C3o=o(" (MobileBERT model)"),M3o=l(),p2=a("li"),Mre=a("strong"),E3o=o("mpnet"),y3o=o(" \u2014 "),$I=a("a"),w3o=o("MPNetForSequenceClassification"),A3o=o(" (MPNet model)"),L3o=l(),_2=a("li"),Ere=a("strong"),B3o=o("nystromformer"),x3o=o(" \u2014 "),II=a("a"),k3o=o("NystromformerForSequenceClassification"),R3o=o(" (Nystromformer model)"),S3o=l(),b2=a("li"),yre=a("strong"),P3o=o("openai-gpt"),$3o=o(" \u2014 "),jI=a("a"),I3o=o("OpenAIGPTForSequenceClassification"),j3o=o(" (OpenAI GPT model)"),D3o=l(),v2=a("li"),wre=a("strong"),N3o=o("perceiver"),q3o=o(" \u2014 "),DI=a("a"),O3o=o("PerceiverForSequenceClassification"),G3o=o(" (Perceiver model)"),X3o=l(),T2=a("li"),Are=a("strong"),V3o=o("plbart"),z3o=o(" \u2014 "),NI=a("a"),W3o=o("PLBartForSequenceClassification"),Q3o=o(" (PLBart model)"),H3o=l(),F2=a("li"),Lre=a("strong"),U3o=o("qdqbert"),J3o=o(" \u2014 "),qI=a("a"),Y3o=o("QDQBertForSequenceClassification"),K3o=o(" (QDQBert model)"),Z3o=l(),C2=a("li"),Bre=a("strong"),e5o=o("reformer"),o5o=o(" \u2014 "),OI=a("a"),t5o=o("ReformerForSequenceClassification"),r5o=o(" (Reformer model)"),a5o=l(),M2=a("li"),xre=a("strong"),s5o=o("rembert"),n5o=o(" \u2014 "),GI=a("a"),l5o=o("RemBertForSequenceClassification"),i5o=o(" (RemBERT model)"),d5o=l(),E2=a("li"),kre=a("strong"),c5o=o("roberta"),m5o=o(" \u2014 "),XI=a("a"),f5o=o("RobertaForSequenceClassification"),g5o=o(" (RoBERTa model)"),h5o=l(),y2=a("li"),Rre=a("strong"),u5o=o("roformer"),p5o=o(" \u2014 "),VI=a("a"),_5o=o("RoFormerForSequenceClassification"),b5o=o(" (RoFormer model)"),v5o=l(),w2=a("li"),Sre=a("strong"),T5o=o("squeezebert"),F5o=o(" \u2014 "),zI=a("a"),C5o=o("SqueezeBertForSequenceClassification"),M5o=o(" (SqueezeBERT model)"),E5o=l(),A2=a("li"),Pre=a("strong"),y5o=o("tapas"),w5o=o(" \u2014 "),WI=a("a"),A5o=o("TapasForSequenceClassification"),L5o=o(" (TAPAS model)"),B5o=l(),L2=a("li"),$re=a("strong"),x5o=o("transfo-xl"),k5o=o(" \u2014 "),QI=a("a"),R5o=o("TransfoXLForSequenceClassification"),S5o=o(" (Transformer-XL model)"),P5o=l(),B2=a("li"),Ire=a("strong"),$5o=o("xlm"),I5o=o(" \u2014 "),HI=a("a"),j5o=o("XLMForSequenceClassification"),D5o=o(" (XLM model)"),N5o=l(),x2=a("li"),jre=a("strong"),q5o=o("xlm-roberta"),O5o=o(" \u2014 "),UI=a("a"),G5o=o("XLMRobertaForSequenceClassification"),X5o=o(" (XLM-RoBERTa model)"),V5o=l(),k2=a("li"),Dre=a("strong"),z5o=o("xlm-roberta-xl"),W5o=o(" \u2014 "),JI=a("a"),Q5o=o("XLMRobertaXLForSequenceClassification"),H5o=o(" (XLM-RoBERTa-XL model)"),U5o=l(),R2=a("li"),Nre=a("strong"),J5o=o("xlnet"),Y5o=o(" \u2014 "),YI=a("a"),K5o=o("XLNetForSequenceClassification"),Z5o=o(" (XLNet model)"),eyo=l(),S2=a("li"),qre=a("strong"),oyo=o("yoso"),tyo=o(" \u2014 "),KI=a("a"),ryo=o("YosoForSequenceClassification"),ayo=o(" (YOSO model)"),syo=l(),P2=a("p"),nyo=o("The model is set in evaluation mode by default using "),Ore=a("code"),lyo=o("model.eval()"),iyo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gre=a("code"),dyo=o("model.train()"),cyo=l(),Xre=a("p"),myo=o("Examples:"),fyo=l(),m(sw.$$.fragment),UBe=l(),hd=a("h2"),$2=a("a"),Vre=a("span"),m(nw.$$.fragment),gyo=l(),zre=a("span"),hyo=o("AutoModelForMultipleChoice"),JBe=l(),et=a("div"),m(lw.$$.fragment),uyo=l(),ud=a("p"),pyo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Wre=a("code"),_yo=o("from_pretrained()"),byo=o("class method or the "),Qre=a("code"),vyo=o("from_config()"),Tyo=o(`class
method.`),Fyo=l(),iw=a("p"),Cyo=o("This class cannot be instantiated directly using "),Hre=a("code"),Myo=o("__init__()"),Eyo=o(" (throws an error)."),yyo=l(),Ht=a("div"),m(dw.$$.fragment),wyo=l(),Ure=a("p"),Ayo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Lyo=l(),pd=a("p"),Byo=o(`Note:
Loading a model from its configuration file does `),Jre=a("strong"),xyo=o("not"),kyo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yre=a("code"),Ryo=o("from_pretrained()"),Syo=o("to load the model weights."),Pyo=l(),Kre=a("p"),$yo=o("Examples:"),Iyo=l(),m(cw.$$.fragment),jyo=l(),De=a("div"),m(mw.$$.fragment),Dyo=l(),Zre=a("p"),Nyo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),qyo=l(),Ua=a("p"),Oyo=o("The model class to instantiate is selected based on the "),eae=a("code"),Gyo=o("model_type"),Xyo=o(` property of the config object (either
passed as an argument or loaded from `),oae=a("code"),Vyo=o("pretrained_model_name_or_path"),zyo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tae=a("code"),Wyo=o("pretrained_model_name_or_path"),Qyo=o(":"),Hyo=l(),O=a("ul"),I2=a("li"),rae=a("strong"),Uyo=o("albert"),Jyo=o(" \u2014 "),ZI=a("a"),Yyo=o("AlbertForMultipleChoice"),Kyo=o(" (ALBERT model)"),Zyo=l(),j2=a("li"),aae=a("strong"),ewo=o("bert"),owo=o(" \u2014 "),ej=a("a"),two=o("BertForMultipleChoice"),rwo=o(" (BERT model)"),awo=l(),D2=a("li"),sae=a("strong"),swo=o("big_bird"),nwo=o(" \u2014 "),oj=a("a"),lwo=o("BigBirdForMultipleChoice"),iwo=o(" (BigBird model)"),dwo=l(),N2=a("li"),nae=a("strong"),cwo=o("camembert"),mwo=o(" \u2014 "),tj=a("a"),fwo=o("CamembertForMultipleChoice"),gwo=o(" (CamemBERT model)"),hwo=l(),q2=a("li"),lae=a("strong"),uwo=o("canine"),pwo=o(" \u2014 "),rj=a("a"),_wo=o("CanineForMultipleChoice"),bwo=o(" (Canine model)"),vwo=l(),O2=a("li"),iae=a("strong"),Two=o("convbert"),Fwo=o(" \u2014 "),aj=a("a"),Cwo=o("ConvBertForMultipleChoice"),Mwo=o(" (ConvBERT model)"),Ewo=l(),G2=a("li"),dae=a("strong"),ywo=o("data2vec-text"),wwo=o(" \u2014 "),sj=a("a"),Awo=o("Data2VecTextForMultipleChoice"),Lwo=o(" (Data2VecText model)"),Bwo=l(),X2=a("li"),cae=a("strong"),xwo=o("distilbert"),kwo=o(" \u2014 "),nj=a("a"),Rwo=o("DistilBertForMultipleChoice"),Swo=o(" (DistilBERT model)"),Pwo=l(),V2=a("li"),mae=a("strong"),$wo=o("electra"),Iwo=o(" \u2014 "),lj=a("a"),jwo=o("ElectraForMultipleChoice"),Dwo=o(" (ELECTRA model)"),Nwo=l(),z2=a("li"),fae=a("strong"),qwo=o("flaubert"),Owo=o(" \u2014 "),ij=a("a"),Gwo=o("FlaubertForMultipleChoice"),Xwo=o(" (FlauBERT model)"),Vwo=l(),W2=a("li"),gae=a("strong"),zwo=o("fnet"),Wwo=o(" \u2014 "),dj=a("a"),Qwo=o("FNetForMultipleChoice"),Hwo=o(" (FNet model)"),Uwo=l(),Q2=a("li"),hae=a("strong"),Jwo=o("funnel"),Ywo=o(" \u2014 "),cj=a("a"),Kwo=o("FunnelForMultipleChoice"),Zwo=o(" (Funnel Transformer model)"),e6o=l(),H2=a("li"),uae=a("strong"),o6o=o("ibert"),t6o=o(" \u2014 "),mj=a("a"),r6o=o("IBertForMultipleChoice"),a6o=o(" (I-BERT model)"),s6o=l(),U2=a("li"),pae=a("strong"),n6o=o("longformer"),l6o=o(" \u2014 "),fj=a("a"),i6o=o("LongformerForMultipleChoice"),d6o=o(" (Longformer model)"),c6o=l(),J2=a("li"),_ae=a("strong"),m6o=o("megatron-bert"),f6o=o(" \u2014 "),gj=a("a"),g6o=o("MegatronBertForMultipleChoice"),h6o=o(" (MegatronBert model)"),u6o=l(),Y2=a("li"),bae=a("strong"),p6o=o("mobilebert"),_6o=o(" \u2014 "),hj=a("a"),b6o=o("MobileBertForMultipleChoice"),v6o=o(" (MobileBERT model)"),T6o=l(),K2=a("li"),vae=a("strong"),F6o=o("mpnet"),C6o=o(" \u2014 "),uj=a("a"),M6o=o("MPNetForMultipleChoice"),E6o=o(" (MPNet model)"),y6o=l(),Z2=a("li"),Tae=a("strong"),w6o=o("nystromformer"),A6o=o(" \u2014 "),pj=a("a"),L6o=o("NystromformerForMultipleChoice"),B6o=o(" (Nystromformer model)"),x6o=l(),ev=a("li"),Fae=a("strong"),k6o=o("qdqbert"),R6o=o(" \u2014 "),_j=a("a"),S6o=o("QDQBertForMultipleChoice"),P6o=o(" (QDQBert model)"),$6o=l(),ov=a("li"),Cae=a("strong"),I6o=o("rembert"),j6o=o(" \u2014 "),bj=a("a"),D6o=o("RemBertForMultipleChoice"),N6o=o(" (RemBERT model)"),q6o=l(),tv=a("li"),Mae=a("strong"),O6o=o("roberta"),G6o=o(" \u2014 "),vj=a("a"),X6o=o("RobertaForMultipleChoice"),V6o=o(" (RoBERTa model)"),z6o=l(),rv=a("li"),Eae=a("strong"),W6o=o("roformer"),Q6o=o(" \u2014 "),Tj=a("a"),H6o=o("RoFormerForMultipleChoice"),U6o=o(" (RoFormer model)"),J6o=l(),av=a("li"),yae=a("strong"),Y6o=o("squeezebert"),K6o=o(" \u2014 "),Fj=a("a"),Z6o=o("SqueezeBertForMultipleChoice"),eAo=o(" (SqueezeBERT model)"),oAo=l(),sv=a("li"),wae=a("strong"),tAo=o("xlm"),rAo=o(" \u2014 "),Cj=a("a"),aAo=o("XLMForMultipleChoice"),sAo=o(" (XLM model)"),nAo=l(),nv=a("li"),Aae=a("strong"),lAo=o("xlm-roberta"),iAo=o(" \u2014 "),Mj=a("a"),dAo=o("XLMRobertaForMultipleChoice"),cAo=o(" (XLM-RoBERTa model)"),mAo=l(),lv=a("li"),Lae=a("strong"),fAo=o("xlm-roberta-xl"),gAo=o(" \u2014 "),Ej=a("a"),hAo=o("XLMRobertaXLForMultipleChoice"),uAo=o(" (XLM-RoBERTa-XL model)"),pAo=l(),iv=a("li"),Bae=a("strong"),_Ao=o("xlnet"),bAo=o(" \u2014 "),yj=a("a"),vAo=o("XLNetForMultipleChoice"),TAo=o(" (XLNet model)"),FAo=l(),dv=a("li"),xae=a("strong"),CAo=o("yoso"),MAo=o(" \u2014 "),wj=a("a"),EAo=o("YosoForMultipleChoice"),yAo=o(" (YOSO model)"),wAo=l(),cv=a("p"),AAo=o("The model is set in evaluation mode by default using "),kae=a("code"),LAo=o("model.eval()"),BAo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rae=a("code"),xAo=o("model.train()"),kAo=l(),Sae=a("p"),RAo=o("Examples:"),SAo=l(),m(fw.$$.fragment),YBe=l(),_d=a("h2"),mv=a("a"),Pae=a("span"),m(gw.$$.fragment),PAo=l(),$ae=a("span"),$Ao=o("AutoModelForNextSentencePrediction"),KBe=l(),ot=a("div"),m(hw.$$.fragment),IAo=l(),bd=a("p"),jAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Iae=a("code"),DAo=o("from_pretrained()"),NAo=o("class method or the "),jae=a("code"),qAo=o("from_config()"),OAo=o(`class
method.`),GAo=l(),uw=a("p"),XAo=o("This class cannot be instantiated directly using "),Dae=a("code"),VAo=o("__init__()"),zAo=o(" (throws an error)."),WAo=l(),Ut=a("div"),m(pw.$$.fragment),QAo=l(),Nae=a("p"),HAo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),UAo=l(),vd=a("p"),JAo=o(`Note:
Loading a model from its configuration file does `),qae=a("strong"),YAo=o("not"),KAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Oae=a("code"),ZAo=o("from_pretrained()"),e0o=o("to load the model weights."),o0o=l(),Gae=a("p"),t0o=o("Examples:"),r0o=l(),m(_w.$$.fragment),a0o=l(),Ne=a("div"),m(bw.$$.fragment),s0o=l(),Xae=a("p"),n0o=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),l0o=l(),Ja=a("p"),i0o=o("The model class to instantiate is selected based on the "),Vae=a("code"),d0o=o("model_type"),c0o=o(` property of the config object (either
passed as an argument or loaded from `),zae=a("code"),m0o=o("pretrained_model_name_or_path"),f0o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wae=a("code"),g0o=o("pretrained_model_name_or_path"),h0o=o(":"),u0o=l(),da=a("ul"),fv=a("li"),Qae=a("strong"),p0o=o("bert"),_0o=o(" \u2014 "),Aj=a("a"),b0o=o("BertForNextSentencePrediction"),v0o=o(" (BERT model)"),T0o=l(),gv=a("li"),Hae=a("strong"),F0o=o("fnet"),C0o=o(" \u2014 "),Lj=a("a"),M0o=o("FNetForNextSentencePrediction"),E0o=o(" (FNet model)"),y0o=l(),hv=a("li"),Uae=a("strong"),w0o=o("megatron-bert"),A0o=o(" \u2014 "),Bj=a("a"),L0o=o("MegatronBertForNextSentencePrediction"),B0o=o(" (MegatronBert model)"),x0o=l(),uv=a("li"),Jae=a("strong"),k0o=o("mobilebert"),R0o=o(" \u2014 "),xj=a("a"),S0o=o("MobileBertForNextSentencePrediction"),P0o=o(" (MobileBERT model)"),$0o=l(),pv=a("li"),Yae=a("strong"),I0o=o("qdqbert"),j0o=o(" \u2014 "),kj=a("a"),D0o=o("QDQBertForNextSentencePrediction"),N0o=o(" (QDQBert model)"),q0o=l(),_v=a("p"),O0o=o("The model is set in evaluation mode by default using "),Kae=a("code"),G0o=o("model.eval()"),X0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zae=a("code"),V0o=o("model.train()"),z0o=l(),ese=a("p"),W0o=o("Examples:"),Q0o=l(),m(vw.$$.fragment),ZBe=l(),Td=a("h2"),bv=a("a"),ose=a("span"),m(Tw.$$.fragment),H0o=l(),tse=a("span"),U0o=o("AutoModelForTokenClassification"),exe=l(),tt=a("div"),m(Fw.$$.fragment),J0o=l(),Fd=a("p"),Y0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),rse=a("code"),K0o=o("from_pretrained()"),Z0o=o("class method or the "),ase=a("code"),eLo=o("from_config()"),oLo=o(`class
method.`),tLo=l(),Cw=a("p"),rLo=o("This class cannot be instantiated directly using "),sse=a("code"),aLo=o("__init__()"),sLo=o(" (throws an error)."),nLo=l(),Jt=a("div"),m(Mw.$$.fragment),lLo=l(),nse=a("p"),iLo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),dLo=l(),Cd=a("p"),cLo=o(`Note:
Loading a model from its configuration file does `),lse=a("strong"),mLo=o("not"),fLo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ise=a("code"),gLo=o("from_pretrained()"),hLo=o("to load the model weights."),uLo=l(),dse=a("p"),pLo=o("Examples:"),_Lo=l(),m(Ew.$$.fragment),bLo=l(),qe=a("div"),m(yw.$$.fragment),vLo=l(),cse=a("p"),TLo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),FLo=l(),Ya=a("p"),CLo=o("The model class to instantiate is selected based on the "),mse=a("code"),MLo=o("model_type"),ELo=o(` property of the config object (either
passed as an argument or loaded from `),fse=a("code"),yLo=o("pretrained_model_name_or_path"),wLo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gse=a("code"),ALo=o("pretrained_model_name_or_path"),LLo=o(":"),BLo=l(),N=a("ul"),vv=a("li"),hse=a("strong"),xLo=o("albert"),kLo=o(" \u2014 "),Rj=a("a"),RLo=o("AlbertForTokenClassification"),SLo=o(" (ALBERT model)"),PLo=l(),Tv=a("li"),use=a("strong"),$Lo=o("bert"),ILo=o(" \u2014 "),Sj=a("a"),jLo=o("BertForTokenClassification"),DLo=o(" (BERT model)"),NLo=l(),Fv=a("li"),pse=a("strong"),qLo=o("big_bird"),OLo=o(" \u2014 "),Pj=a("a"),GLo=o("BigBirdForTokenClassification"),XLo=o(" (BigBird model)"),VLo=l(),Cv=a("li"),_se=a("strong"),zLo=o("camembert"),WLo=o(" \u2014 "),$j=a("a"),QLo=o("CamembertForTokenClassification"),HLo=o(" (CamemBERT model)"),ULo=l(),Mv=a("li"),bse=a("strong"),JLo=o("canine"),YLo=o(" \u2014 "),Ij=a("a"),KLo=o("CanineForTokenClassification"),ZLo=o(" (Canine model)"),e8o=l(),Ev=a("li"),vse=a("strong"),o8o=o("convbert"),t8o=o(" \u2014 "),jj=a("a"),r8o=o("ConvBertForTokenClassification"),a8o=o(" (ConvBERT model)"),s8o=l(),yv=a("li"),Tse=a("strong"),n8o=o("data2vec-text"),l8o=o(" \u2014 "),Dj=a("a"),i8o=o("Data2VecTextForTokenClassification"),d8o=o(" (Data2VecText model)"),c8o=l(),wv=a("li"),Fse=a("strong"),m8o=o("deberta"),f8o=o(" \u2014 "),Nj=a("a"),g8o=o("DebertaForTokenClassification"),h8o=o(" (DeBERTa model)"),u8o=l(),Av=a("li"),Cse=a("strong"),p8o=o("deberta-v2"),_8o=o(" \u2014 "),qj=a("a"),b8o=o("DebertaV2ForTokenClassification"),v8o=o(" (DeBERTa-v2 model)"),T8o=l(),Lv=a("li"),Mse=a("strong"),F8o=o("distilbert"),C8o=o(" \u2014 "),Oj=a("a"),M8o=o("DistilBertForTokenClassification"),E8o=o(" (DistilBERT model)"),y8o=l(),Bv=a("li"),Ese=a("strong"),w8o=o("electra"),A8o=o(" \u2014 "),Gj=a("a"),L8o=o("ElectraForTokenClassification"),B8o=o(" (ELECTRA model)"),x8o=l(),xv=a("li"),yse=a("strong"),k8o=o("flaubert"),R8o=o(" \u2014 "),Xj=a("a"),S8o=o("FlaubertForTokenClassification"),P8o=o(" (FlauBERT model)"),$8o=l(),kv=a("li"),wse=a("strong"),I8o=o("fnet"),j8o=o(" \u2014 "),Vj=a("a"),D8o=o("FNetForTokenClassification"),N8o=o(" (FNet model)"),q8o=l(),Rv=a("li"),Ase=a("strong"),O8o=o("funnel"),G8o=o(" \u2014 "),zj=a("a"),X8o=o("FunnelForTokenClassification"),V8o=o(" (Funnel Transformer model)"),z8o=l(),Sv=a("li"),Lse=a("strong"),W8o=o("gpt2"),Q8o=o(" \u2014 "),Wj=a("a"),H8o=o("GPT2ForTokenClassification"),U8o=o(" (OpenAI GPT-2 model)"),J8o=l(),Pv=a("li"),Bse=a("strong"),Y8o=o("ibert"),K8o=o(" \u2014 "),Qj=a("a"),Z8o=o("IBertForTokenClassification"),e7o=o(" (I-BERT model)"),o7o=l(),$v=a("li"),xse=a("strong"),t7o=o("layoutlm"),r7o=o(" \u2014 "),Hj=a("a"),a7o=o("LayoutLMForTokenClassification"),s7o=o(" (LayoutLM model)"),n7o=l(),Iv=a("li"),kse=a("strong"),l7o=o("layoutlmv2"),i7o=o(" \u2014 "),Uj=a("a"),d7o=o("LayoutLMv2ForTokenClassification"),c7o=o(" (LayoutLMv2 model)"),m7o=l(),jv=a("li"),Rse=a("strong"),f7o=o("longformer"),g7o=o(" \u2014 "),Jj=a("a"),h7o=o("LongformerForTokenClassification"),u7o=o(" (Longformer model)"),p7o=l(),Dv=a("li"),Sse=a("strong"),_7o=o("megatron-bert"),b7o=o(" \u2014 "),Yj=a("a"),v7o=o("MegatronBertForTokenClassification"),T7o=o(" (MegatronBert model)"),F7o=l(),Nv=a("li"),Pse=a("strong"),C7o=o("mobilebert"),M7o=o(" \u2014 "),Kj=a("a"),E7o=o("MobileBertForTokenClassification"),y7o=o(" (MobileBERT model)"),w7o=l(),qv=a("li"),$se=a("strong"),A7o=o("mpnet"),L7o=o(" \u2014 "),Zj=a("a"),B7o=o("MPNetForTokenClassification"),x7o=o(" (MPNet model)"),k7o=l(),Ov=a("li"),Ise=a("strong"),R7o=o("nystromformer"),S7o=o(" \u2014 "),eD=a("a"),P7o=o("NystromformerForTokenClassification"),$7o=o(" (Nystromformer model)"),I7o=l(),Gv=a("li"),jse=a("strong"),j7o=o("qdqbert"),D7o=o(" \u2014 "),oD=a("a"),N7o=o("QDQBertForTokenClassification"),q7o=o(" (QDQBert model)"),O7o=l(),Xv=a("li"),Dse=a("strong"),G7o=o("rembert"),X7o=o(" \u2014 "),tD=a("a"),V7o=o("RemBertForTokenClassification"),z7o=o(" (RemBERT model)"),W7o=l(),Vv=a("li"),Nse=a("strong"),Q7o=o("roberta"),H7o=o(" \u2014 "),rD=a("a"),U7o=o("RobertaForTokenClassification"),J7o=o(" (RoBERTa model)"),Y7o=l(),zv=a("li"),qse=a("strong"),K7o=o("roformer"),Z7o=o(" \u2014 "),aD=a("a"),e9o=o("RoFormerForTokenClassification"),o9o=o(" (RoFormer model)"),t9o=l(),Wv=a("li"),Ose=a("strong"),r9o=o("squeezebert"),a9o=o(" \u2014 "),sD=a("a"),s9o=o("SqueezeBertForTokenClassification"),n9o=o(" (SqueezeBERT model)"),l9o=l(),Qv=a("li"),Gse=a("strong"),i9o=o("xlm"),d9o=o(" \u2014 "),nD=a("a"),c9o=o("XLMForTokenClassification"),m9o=o(" (XLM model)"),f9o=l(),Hv=a("li"),Xse=a("strong"),g9o=o("xlm-roberta"),h9o=o(" \u2014 "),lD=a("a"),u9o=o("XLMRobertaForTokenClassification"),p9o=o(" (XLM-RoBERTa model)"),_9o=l(),Uv=a("li"),Vse=a("strong"),b9o=o("xlm-roberta-xl"),v9o=o(" \u2014 "),iD=a("a"),T9o=o("XLMRobertaXLForTokenClassification"),F9o=o(" (XLM-RoBERTa-XL model)"),C9o=l(),Jv=a("li"),zse=a("strong"),M9o=o("xlnet"),E9o=o(" \u2014 "),dD=a("a"),y9o=o("XLNetForTokenClassification"),w9o=o(" (XLNet model)"),A9o=l(),Yv=a("li"),Wse=a("strong"),L9o=o("yoso"),B9o=o(" \u2014 "),cD=a("a"),x9o=o("YosoForTokenClassification"),k9o=o(" (YOSO model)"),R9o=l(),Kv=a("p"),S9o=o("The model is set in evaluation mode by default using "),Qse=a("code"),P9o=o("model.eval()"),$9o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hse=a("code"),I9o=o("model.train()"),j9o=l(),Use=a("p"),D9o=o("Examples:"),N9o=l(),m(ww.$$.fragment),oxe=l(),Md=a("h2"),Zv=a("a"),Jse=a("span"),m(Aw.$$.fragment),q9o=l(),Yse=a("span"),O9o=o("AutoModelForQuestionAnswering"),txe=l(),rt=a("div"),m(Lw.$$.fragment),G9o=l(),Ed=a("p"),X9o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Kse=a("code"),V9o=o("from_pretrained()"),z9o=o("class method or the "),Zse=a("code"),W9o=o("from_config()"),Q9o=o(`class
method.`),H9o=l(),Bw=a("p"),U9o=o("This class cannot be instantiated directly using "),ene=a("code"),J9o=o("__init__()"),Y9o=o(" (throws an error)."),K9o=l(),Yt=a("div"),m(xw.$$.fragment),Z9o=l(),one=a("p"),eBo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),oBo=l(),yd=a("p"),tBo=o(`Note:
Loading a model from its configuration file does `),tne=a("strong"),rBo=o("not"),aBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rne=a("code"),sBo=o("from_pretrained()"),nBo=o("to load the model weights."),lBo=l(),ane=a("p"),iBo=o("Examples:"),dBo=l(),m(kw.$$.fragment),cBo=l(),Oe=a("div"),m(Rw.$$.fragment),mBo=l(),sne=a("p"),fBo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),gBo=l(),Ka=a("p"),hBo=o("The model class to instantiate is selected based on the "),nne=a("code"),uBo=o("model_type"),pBo=o(` property of the config object (either
passed as an argument or loaded from `),lne=a("code"),_Bo=o("pretrained_model_name_or_path"),bBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ine=a("code"),vBo=o("pretrained_model_name_or_path"),TBo=o(":"),FBo=l(),R=a("ul"),eT=a("li"),dne=a("strong"),CBo=o("albert"),MBo=o(" \u2014 "),mD=a("a"),EBo=o("AlbertForQuestionAnswering"),yBo=o(" (ALBERT model)"),wBo=l(),oT=a("li"),cne=a("strong"),ABo=o("bart"),LBo=o(" \u2014 "),fD=a("a"),BBo=o("BartForQuestionAnswering"),xBo=o(" (BART model)"),kBo=l(),tT=a("li"),mne=a("strong"),RBo=o("bert"),SBo=o(" \u2014 "),gD=a("a"),PBo=o("BertForQuestionAnswering"),$Bo=o(" (BERT model)"),IBo=l(),rT=a("li"),fne=a("strong"),jBo=o("big_bird"),DBo=o(" \u2014 "),hD=a("a"),NBo=o("BigBirdForQuestionAnswering"),qBo=o(" (BigBird model)"),OBo=l(),aT=a("li"),gne=a("strong"),GBo=o("bigbird_pegasus"),XBo=o(" \u2014 "),uD=a("a"),VBo=o("BigBirdPegasusForQuestionAnswering"),zBo=o(" (BigBirdPegasus model)"),WBo=l(),sT=a("li"),hne=a("strong"),QBo=o("camembert"),HBo=o(" \u2014 "),pD=a("a"),UBo=o("CamembertForQuestionAnswering"),JBo=o(" (CamemBERT model)"),YBo=l(),nT=a("li"),une=a("strong"),KBo=o("canine"),ZBo=o(" \u2014 "),_D=a("a"),exo=o("CanineForQuestionAnswering"),oxo=o(" (Canine model)"),txo=l(),lT=a("li"),pne=a("strong"),rxo=o("convbert"),axo=o(" \u2014 "),bD=a("a"),sxo=o("ConvBertForQuestionAnswering"),nxo=o(" (ConvBERT model)"),lxo=l(),iT=a("li"),_ne=a("strong"),ixo=o("data2vec-text"),dxo=o(" \u2014 "),vD=a("a"),cxo=o("Data2VecTextForQuestionAnswering"),mxo=o(" (Data2VecText model)"),fxo=l(),dT=a("li"),bne=a("strong"),gxo=o("deberta"),hxo=o(" \u2014 "),TD=a("a"),uxo=o("DebertaForQuestionAnswering"),pxo=o(" (DeBERTa model)"),_xo=l(),cT=a("li"),vne=a("strong"),bxo=o("deberta-v2"),vxo=o(" \u2014 "),FD=a("a"),Txo=o("DebertaV2ForQuestionAnswering"),Fxo=o(" (DeBERTa-v2 model)"),Cxo=l(),mT=a("li"),Tne=a("strong"),Mxo=o("distilbert"),Exo=o(" \u2014 "),CD=a("a"),yxo=o("DistilBertForQuestionAnswering"),wxo=o(" (DistilBERT model)"),Axo=l(),fT=a("li"),Fne=a("strong"),Lxo=o("electra"),Bxo=o(" \u2014 "),MD=a("a"),xxo=o("ElectraForQuestionAnswering"),kxo=o(" (ELECTRA model)"),Rxo=l(),gT=a("li"),Cne=a("strong"),Sxo=o("flaubert"),Pxo=o(" \u2014 "),ED=a("a"),$xo=o("FlaubertForQuestionAnsweringSimple"),Ixo=o(" (FlauBERT model)"),jxo=l(),hT=a("li"),Mne=a("strong"),Dxo=o("fnet"),Nxo=o(" \u2014 "),yD=a("a"),qxo=o("FNetForQuestionAnswering"),Oxo=o(" (FNet model)"),Gxo=l(),uT=a("li"),Ene=a("strong"),Xxo=o("funnel"),Vxo=o(" \u2014 "),wD=a("a"),zxo=o("FunnelForQuestionAnswering"),Wxo=o(" (Funnel Transformer model)"),Qxo=l(),pT=a("li"),yne=a("strong"),Hxo=o("gptj"),Uxo=o(" \u2014 "),AD=a("a"),Jxo=o("GPTJForQuestionAnswering"),Yxo=o(" (GPT-J model)"),Kxo=l(),_T=a("li"),wne=a("strong"),Zxo=o("ibert"),eko=o(" \u2014 "),LD=a("a"),oko=o("IBertForQuestionAnswering"),tko=o(" (I-BERT model)"),rko=l(),bT=a("li"),Ane=a("strong"),ako=o("layoutlmv2"),sko=o(" \u2014 "),BD=a("a"),nko=o("LayoutLMv2ForQuestionAnswering"),lko=o(" (LayoutLMv2 model)"),iko=l(),vT=a("li"),Lne=a("strong"),dko=o("led"),cko=o(" \u2014 "),xD=a("a"),mko=o("LEDForQuestionAnswering"),fko=o(" (LED model)"),gko=l(),TT=a("li"),Bne=a("strong"),hko=o("longformer"),uko=o(" \u2014 "),kD=a("a"),pko=o("LongformerForQuestionAnswering"),_ko=o(" (Longformer model)"),bko=l(),FT=a("li"),xne=a("strong"),vko=o("lxmert"),Tko=o(" \u2014 "),RD=a("a"),Fko=o("LxmertForQuestionAnswering"),Cko=o(" (LXMERT model)"),Mko=l(),CT=a("li"),kne=a("strong"),Eko=o("mbart"),yko=o(" \u2014 "),SD=a("a"),wko=o("MBartForQuestionAnswering"),Ako=o(" (mBART model)"),Lko=l(),MT=a("li"),Rne=a("strong"),Bko=o("megatron-bert"),xko=o(" \u2014 "),PD=a("a"),kko=o("MegatronBertForQuestionAnswering"),Rko=o(" (MegatronBert model)"),Sko=l(),ET=a("li"),Sne=a("strong"),Pko=o("mobilebert"),$ko=o(" \u2014 "),$D=a("a"),Iko=o("MobileBertForQuestionAnswering"),jko=o(" (MobileBERT model)"),Dko=l(),yT=a("li"),Pne=a("strong"),Nko=o("mpnet"),qko=o(" \u2014 "),ID=a("a"),Oko=o("MPNetForQuestionAnswering"),Gko=o(" (MPNet model)"),Xko=l(),wT=a("li"),$ne=a("strong"),Vko=o("nystromformer"),zko=o(" \u2014 "),jD=a("a"),Wko=o("NystromformerForQuestionAnswering"),Qko=o(" (Nystromformer model)"),Hko=l(),AT=a("li"),Ine=a("strong"),Uko=o("qdqbert"),Jko=o(" \u2014 "),DD=a("a"),Yko=o("QDQBertForQuestionAnswering"),Kko=o(" (QDQBert model)"),Zko=l(),LT=a("li"),jne=a("strong"),eRo=o("reformer"),oRo=o(" \u2014 "),ND=a("a"),tRo=o("ReformerForQuestionAnswering"),rRo=o(" (Reformer model)"),aRo=l(),BT=a("li"),Dne=a("strong"),sRo=o("rembert"),nRo=o(" \u2014 "),qD=a("a"),lRo=o("RemBertForQuestionAnswering"),iRo=o(" (RemBERT model)"),dRo=l(),xT=a("li"),Nne=a("strong"),cRo=o("roberta"),mRo=o(" \u2014 "),OD=a("a"),fRo=o("RobertaForQuestionAnswering"),gRo=o(" (RoBERTa model)"),hRo=l(),kT=a("li"),qne=a("strong"),uRo=o("roformer"),pRo=o(" \u2014 "),GD=a("a"),_Ro=o("RoFormerForQuestionAnswering"),bRo=o(" (RoFormer model)"),vRo=l(),RT=a("li"),One=a("strong"),TRo=o("splinter"),FRo=o(" \u2014 "),XD=a("a"),CRo=o("SplinterForQuestionAnswering"),MRo=o(" (Splinter model)"),ERo=l(),ST=a("li"),Gne=a("strong"),yRo=o("squeezebert"),wRo=o(" \u2014 "),VD=a("a"),ARo=o("SqueezeBertForQuestionAnswering"),LRo=o(" (SqueezeBERT model)"),BRo=l(),PT=a("li"),Xne=a("strong"),xRo=o("xlm"),kRo=o(" \u2014 "),zD=a("a"),RRo=o("XLMForQuestionAnsweringSimple"),SRo=o(" (XLM model)"),PRo=l(),$T=a("li"),Vne=a("strong"),$Ro=o("xlm-roberta"),IRo=o(" \u2014 "),WD=a("a"),jRo=o("XLMRobertaForQuestionAnswering"),DRo=o(" (XLM-RoBERTa model)"),NRo=l(),IT=a("li"),zne=a("strong"),qRo=o("xlm-roberta-xl"),ORo=o(" \u2014 "),QD=a("a"),GRo=o("XLMRobertaXLForQuestionAnswering"),XRo=o(" (XLM-RoBERTa-XL model)"),VRo=l(),jT=a("li"),Wne=a("strong"),zRo=o("xlnet"),WRo=o(" \u2014 "),HD=a("a"),QRo=o("XLNetForQuestionAnsweringSimple"),HRo=o(" (XLNet model)"),URo=l(),DT=a("li"),Qne=a("strong"),JRo=o("yoso"),YRo=o(" \u2014 "),UD=a("a"),KRo=o("YosoForQuestionAnswering"),ZRo=o(" (YOSO model)"),eSo=l(),NT=a("p"),oSo=o("The model is set in evaluation mode by default using "),Hne=a("code"),tSo=o("model.eval()"),rSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Une=a("code"),aSo=o("model.train()"),sSo=l(),Jne=a("p"),nSo=o("Examples:"),lSo=l(),m(Sw.$$.fragment),rxe=l(),wd=a("h2"),qT=a("a"),Yne=a("span"),m(Pw.$$.fragment),iSo=l(),Kne=a("span"),dSo=o("AutoModelForTableQuestionAnswering"),axe=l(),at=a("div"),m($w.$$.fragment),cSo=l(),Ad=a("p"),mSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Zne=a("code"),fSo=o("from_pretrained()"),gSo=o("class method or the "),ele=a("code"),hSo=o("from_config()"),uSo=o(`class
method.`),pSo=l(),Iw=a("p"),_So=o("This class cannot be instantiated directly using "),ole=a("code"),bSo=o("__init__()"),vSo=o(" (throws an error)."),TSo=l(),Kt=a("div"),m(jw.$$.fragment),FSo=l(),tle=a("p"),CSo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),MSo=l(),Ld=a("p"),ESo=o(`Note:
Loading a model from its configuration file does `),rle=a("strong"),ySo=o("not"),wSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=a("code"),ASo=o("from_pretrained()"),LSo=o("to load the model weights."),BSo=l(),sle=a("p"),xSo=o("Examples:"),kSo=l(),m(Dw.$$.fragment),RSo=l(),Ge=a("div"),m(Nw.$$.fragment),SSo=l(),nle=a("p"),PSo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),$So=l(),Za=a("p"),ISo=o("The model class to instantiate is selected based on the "),lle=a("code"),jSo=o("model_type"),DSo=o(` property of the config object (either
passed as an argument or loaded from `),ile=a("code"),NSo=o("pretrained_model_name_or_path"),qSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=a("code"),OSo=o("pretrained_model_name_or_path"),GSo=o(":"),XSo=l(),cle=a("ul"),OT=a("li"),mle=a("strong"),VSo=o("tapas"),zSo=o(" \u2014 "),JD=a("a"),WSo=o("TapasForQuestionAnswering"),QSo=o(" (TAPAS model)"),HSo=l(),GT=a("p"),USo=o("The model is set in evaluation mode by default using "),fle=a("code"),JSo=o("model.eval()"),YSo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gle=a("code"),KSo=o("model.train()"),ZSo=l(),hle=a("p"),ePo=o("Examples:"),oPo=l(),m(qw.$$.fragment),sxe=l(),Bd=a("h2"),XT=a("a"),ule=a("span"),m(Ow.$$.fragment),tPo=l(),ple=a("span"),rPo=o("AutoModelForImageClassification"),nxe=l(),st=a("div"),m(Gw.$$.fragment),aPo=l(),xd=a("p"),sPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),_le=a("code"),nPo=o("from_pretrained()"),lPo=o("class method or the "),ble=a("code"),iPo=o("from_config()"),dPo=o(`class
method.`),cPo=l(),Xw=a("p"),mPo=o("This class cannot be instantiated directly using "),vle=a("code"),fPo=o("__init__()"),gPo=o(" (throws an error)."),hPo=l(),Zt=a("div"),m(Vw.$$.fragment),uPo=l(),Tle=a("p"),pPo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),_Po=l(),kd=a("p"),bPo=o(`Note:
Loading a model from its configuration file does `),Fle=a("strong"),vPo=o("not"),TPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cle=a("code"),FPo=o("from_pretrained()"),CPo=o("to load the model weights."),MPo=l(),Mle=a("p"),EPo=o("Examples:"),yPo=l(),m(zw.$$.fragment),wPo=l(),Xe=a("div"),m(Ww.$$.fragment),APo=l(),Ele=a("p"),LPo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),BPo=l(),es=a("p"),xPo=o("The model class to instantiate is selected based on the "),yle=a("code"),kPo=o("model_type"),RPo=o(` property of the config object (either
passed as an argument or loaded from `),wle=a("code"),SPo=o("pretrained_model_name_or_path"),PPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ale=a("code"),$Po=o("pretrained_model_name_or_path"),IPo=o(":"),jPo=l(),be=a("ul"),VT=a("li"),Lle=a("strong"),DPo=o("beit"),NPo=o(" \u2014 "),YD=a("a"),qPo=o("BeitForImageClassification"),OPo=o(" (BEiT model)"),GPo=l(),zT=a("li"),Ble=a("strong"),XPo=o("convnext"),VPo=o(" \u2014 "),KD=a("a"),zPo=o("ConvNextForImageClassification"),WPo=o(" (ConvNext model)"),QPo=l(),qn=a("li"),xle=a("strong"),HPo=o("deit"),UPo=o(" \u2014 "),ZD=a("a"),JPo=o("DeiTForImageClassification"),YPo=o(" or "),eN=a("a"),KPo=o("DeiTForImageClassificationWithTeacher"),ZPo=o(" (DeiT model)"),e$o=l(),WT=a("li"),kle=a("strong"),o$o=o("imagegpt"),t$o=o(" \u2014 "),oN=a("a"),r$o=o("ImageGPTForImageClassification"),a$o=o(" (ImageGPT model)"),s$o=l(),ma=a("li"),Rle=a("strong"),n$o=o("perceiver"),l$o=o(" \u2014 "),tN=a("a"),i$o=o("PerceiverForImageClassificationLearned"),d$o=o(" or "),rN=a("a"),c$o=o("PerceiverForImageClassificationFourier"),m$o=o(" or "),aN=a("a"),f$o=o("PerceiverForImageClassificationConvProcessing"),g$o=o(" (Perceiver model)"),h$o=l(),QT=a("li"),Sle=a("strong"),u$o=o("poolformer"),p$o=o(" \u2014 "),sN=a("a"),_$o=o("PoolFormerForImageClassification"),b$o=o(" (PoolFormer model)"),v$o=l(),HT=a("li"),Ple=a("strong"),T$o=o("segformer"),F$o=o(" \u2014 "),nN=a("a"),C$o=o("SegformerForImageClassification"),M$o=o(" (SegFormer model)"),E$o=l(),UT=a("li"),$le=a("strong"),y$o=o("swin"),w$o=o(" \u2014 "),lN=a("a"),A$o=o("SwinForImageClassification"),L$o=o(" (Swin model)"),B$o=l(),JT=a("li"),Ile=a("strong"),x$o=o("vit"),k$o=o(" \u2014 "),iN=a("a"),R$o=o("ViTForImageClassification"),S$o=o(" (ViT model)"),P$o=l(),YT=a("p"),$$o=o("The model is set in evaluation mode by default using "),jle=a("code"),I$o=o("model.eval()"),j$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dle=a("code"),D$o=o("model.train()"),N$o=l(),Nle=a("p"),q$o=o("Examples:"),O$o=l(),m(Qw.$$.fragment),lxe=l(),Rd=a("h2"),KT=a("a"),qle=a("span"),m(Hw.$$.fragment),G$o=l(),Ole=a("span"),X$o=o("AutoModelForVision2Seq"),ixe=l(),nt=a("div"),m(Uw.$$.fragment),V$o=l(),Sd=a("p"),z$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Gle=a("code"),W$o=o("from_pretrained()"),Q$o=o("class method or the "),Xle=a("code"),H$o=o("from_config()"),U$o=o(`class
method.`),J$o=l(),Jw=a("p"),Y$o=o("This class cannot be instantiated directly using "),Vle=a("code"),K$o=o("__init__()"),Z$o=o(" (throws an error)."),eIo=l(),er=a("div"),m(Yw.$$.fragment),oIo=l(),zle=a("p"),tIo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),rIo=l(),Pd=a("p"),aIo=o(`Note:
Loading a model from its configuration file does `),Wle=a("strong"),sIo=o("not"),nIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Qle=a("code"),lIo=o("from_pretrained()"),iIo=o("to load the model weights."),dIo=l(),Hle=a("p"),cIo=o("Examples:"),mIo=l(),m(Kw.$$.fragment),fIo=l(),Ve=a("div"),m(Zw.$$.fragment),gIo=l(),Ule=a("p"),hIo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),uIo=l(),os=a("p"),pIo=o("The model class to instantiate is selected based on the "),Jle=a("code"),_Io=o("model_type"),bIo=o(` property of the config object (either
passed as an argument or loaded from `),Yle=a("code"),vIo=o("pretrained_model_name_or_path"),TIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Kle=a("code"),FIo=o("pretrained_model_name_or_path"),CIo=o(":"),MIo=l(),Zle=a("ul"),ZT=a("li"),eie=a("strong"),EIo=o("vision-encoder-decoder"),yIo=o(" \u2014 "),dN=a("a"),wIo=o("VisionEncoderDecoderModel"),AIo=o(" (Vision Encoder decoder model)"),LIo=l(),e1=a("p"),BIo=o("The model is set in evaluation mode by default using "),oie=a("code"),xIo=o("model.eval()"),kIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tie=a("code"),RIo=o("model.train()"),SIo=l(),rie=a("p"),PIo=o("Examples:"),$Io=l(),m(e6.$$.fragment),dxe=l(),$d=a("h2"),o1=a("a"),aie=a("span"),m(o6.$$.fragment),IIo=l(),sie=a("span"),jIo=o("AutoModelForAudioClassification"),cxe=l(),lt=a("div"),m(t6.$$.fragment),DIo=l(),Id=a("p"),NIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),nie=a("code"),qIo=o("from_pretrained()"),OIo=o("class method or the "),lie=a("code"),GIo=o("from_config()"),XIo=o(`class
method.`),VIo=l(),r6=a("p"),zIo=o("This class cannot be instantiated directly using "),iie=a("code"),WIo=o("__init__()"),QIo=o(" (throws an error)."),HIo=l(),or=a("div"),m(a6.$$.fragment),UIo=l(),die=a("p"),JIo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),YIo=l(),jd=a("p"),KIo=o(`Note:
Loading a model from its configuration file does `),cie=a("strong"),ZIo=o("not"),ejo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mie=a("code"),ojo=o("from_pretrained()"),tjo=o("to load the model weights."),rjo=l(),fie=a("p"),ajo=o("Examples:"),sjo=l(),m(s6.$$.fragment),njo=l(),ze=a("div"),m(n6.$$.fragment),ljo=l(),gie=a("p"),ijo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),djo=l(),ts=a("p"),cjo=o("The model class to instantiate is selected based on the "),hie=a("code"),mjo=o("model_type"),fjo=o(` property of the config object (either
passed as an argument or loaded from `),uie=a("code"),gjo=o("pretrained_model_name_or_path"),hjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=a("code"),ujo=o("pretrained_model_name_or_path"),pjo=o(":"),_jo=l(),Ae=a("ul"),t1=a("li"),_ie=a("strong"),bjo=o("data2vec-audio"),vjo=o(" \u2014 "),cN=a("a"),Tjo=o("Data2VecAudioForSequenceClassification"),Fjo=o(" (Data2VecAudio model)"),Cjo=l(),r1=a("li"),bie=a("strong"),Mjo=o("hubert"),Ejo=o(" \u2014 "),mN=a("a"),yjo=o("HubertForSequenceClassification"),wjo=o(" (Hubert model)"),Ajo=l(),a1=a("li"),vie=a("strong"),Ljo=o("sew"),Bjo=o(" \u2014 "),fN=a("a"),xjo=o("SEWForSequenceClassification"),kjo=o(" (SEW model)"),Rjo=l(),s1=a("li"),Tie=a("strong"),Sjo=o("sew-d"),Pjo=o(" \u2014 "),gN=a("a"),$jo=o("SEWDForSequenceClassification"),Ijo=o(" (SEW-D model)"),jjo=l(),n1=a("li"),Fie=a("strong"),Djo=o("unispeech"),Njo=o(" \u2014 "),hN=a("a"),qjo=o("UniSpeechForSequenceClassification"),Ojo=o(" (UniSpeech model)"),Gjo=l(),l1=a("li"),Cie=a("strong"),Xjo=o("unispeech-sat"),Vjo=o(" \u2014 "),uN=a("a"),zjo=o("UniSpeechSatForSequenceClassification"),Wjo=o(" (UniSpeechSat model)"),Qjo=l(),i1=a("li"),Mie=a("strong"),Hjo=o("wav2vec2"),Ujo=o(" \u2014 "),pN=a("a"),Jjo=o("Wav2Vec2ForSequenceClassification"),Yjo=o(" (Wav2Vec2 model)"),Kjo=l(),d1=a("li"),Eie=a("strong"),Zjo=o("wavlm"),eDo=o(" \u2014 "),_N=a("a"),oDo=o("WavLMForSequenceClassification"),tDo=o(" (WavLM model)"),rDo=l(),c1=a("p"),aDo=o("The model is set in evaluation mode by default using "),yie=a("code"),sDo=o("model.eval()"),nDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wie=a("code"),lDo=o("model.train()"),iDo=l(),Aie=a("p"),dDo=o("Examples:"),cDo=l(),m(l6.$$.fragment),mxe=l(),Dd=a("h2"),m1=a("a"),Lie=a("span"),m(i6.$$.fragment),mDo=l(),Bie=a("span"),fDo=o("AutoModelForAudioFrameClassification"),fxe=l(),it=a("div"),m(d6.$$.fragment),gDo=l(),Nd=a("p"),hDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),xie=a("code"),uDo=o("from_pretrained()"),pDo=o("class method or the "),kie=a("code"),_Do=o("from_config()"),bDo=o(`class
method.`),vDo=l(),c6=a("p"),TDo=o("This class cannot be instantiated directly using "),Rie=a("code"),FDo=o("__init__()"),CDo=o(" (throws an error)."),MDo=l(),tr=a("div"),m(m6.$$.fragment),EDo=l(),Sie=a("p"),yDo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),wDo=l(),qd=a("p"),ADo=o(`Note:
Loading a model from its configuration file does `),Pie=a("strong"),LDo=o("not"),BDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ie=a("code"),xDo=o("from_pretrained()"),kDo=o("to load the model weights."),RDo=l(),Iie=a("p"),SDo=o("Examples:"),PDo=l(),m(f6.$$.fragment),$Do=l(),We=a("div"),m(g6.$$.fragment),IDo=l(),jie=a("p"),jDo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),DDo=l(),rs=a("p"),NDo=o("The model class to instantiate is selected based on the "),Die=a("code"),qDo=o("model_type"),ODo=o(` property of the config object (either
passed as an argument or loaded from `),Nie=a("code"),GDo=o("pretrained_model_name_or_path"),XDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qie=a("code"),VDo=o("pretrained_model_name_or_path"),zDo=o(":"),WDo=l(),as=a("ul"),f1=a("li"),Oie=a("strong"),QDo=o("data2vec-audio"),HDo=o(" \u2014 "),bN=a("a"),UDo=o("Data2VecAudioForAudioFrameClassification"),JDo=o(" (Data2VecAudio model)"),YDo=l(),g1=a("li"),Gie=a("strong"),KDo=o("unispeech-sat"),ZDo=o(" \u2014 "),vN=a("a"),eNo=o("UniSpeechSatForAudioFrameClassification"),oNo=o(" (UniSpeechSat model)"),tNo=l(),h1=a("li"),Xie=a("strong"),rNo=o("wav2vec2"),aNo=o(" \u2014 "),TN=a("a"),sNo=o("Wav2Vec2ForAudioFrameClassification"),nNo=o(" (Wav2Vec2 model)"),lNo=l(),u1=a("li"),Vie=a("strong"),iNo=o("wavlm"),dNo=o(" \u2014 "),FN=a("a"),cNo=o("WavLMForAudioFrameClassification"),mNo=o(" (WavLM model)"),fNo=l(),p1=a("p"),gNo=o("The model is set in evaluation mode by default using "),zie=a("code"),hNo=o("model.eval()"),uNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wie=a("code"),pNo=o("model.train()"),_No=l(),Qie=a("p"),bNo=o("Examples:"),vNo=l(),m(h6.$$.fragment),gxe=l(),Od=a("h2"),_1=a("a"),Hie=a("span"),m(u6.$$.fragment),TNo=l(),Uie=a("span"),FNo=o("AutoModelForCTC"),hxe=l(),dt=a("div"),m(p6.$$.fragment),CNo=l(),Gd=a("p"),MNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Jie=a("code"),ENo=o("from_pretrained()"),yNo=o("class method or the "),Yie=a("code"),wNo=o("from_config()"),ANo=o(`class
method.`),LNo=l(),_6=a("p"),BNo=o("This class cannot be instantiated directly using "),Kie=a("code"),xNo=o("__init__()"),kNo=o(" (throws an error)."),RNo=l(),rr=a("div"),m(b6.$$.fragment),SNo=l(),Zie=a("p"),PNo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),$No=l(),Xd=a("p"),INo=o(`Note:
Loading a model from its configuration file does `),ede=a("strong"),jNo=o("not"),DNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ode=a("code"),NNo=o("from_pretrained()"),qNo=o("to load the model weights."),ONo=l(),tde=a("p"),GNo=o("Examples:"),XNo=l(),m(v6.$$.fragment),VNo=l(),Qe=a("div"),m(T6.$$.fragment),zNo=l(),rde=a("p"),WNo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),QNo=l(),ss=a("p"),HNo=o("The model class to instantiate is selected based on the "),ade=a("code"),UNo=o("model_type"),JNo=o(` property of the config object (either
passed as an argument or loaded from `),sde=a("code"),YNo=o("pretrained_model_name_or_path"),KNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nde=a("code"),ZNo=o("pretrained_model_name_or_path"),eqo=o(":"),oqo=l(),Le=a("ul"),b1=a("li"),lde=a("strong"),tqo=o("data2vec-audio"),rqo=o(" \u2014 "),CN=a("a"),aqo=o("Data2VecAudioForCTC"),sqo=o(" (Data2VecAudio model)"),nqo=l(),v1=a("li"),ide=a("strong"),lqo=o("hubert"),iqo=o(" \u2014 "),MN=a("a"),dqo=o("HubertForCTC"),cqo=o(" (Hubert model)"),mqo=l(),T1=a("li"),dde=a("strong"),fqo=o("sew"),gqo=o(" \u2014 "),EN=a("a"),hqo=o("SEWForCTC"),uqo=o(" (SEW model)"),pqo=l(),F1=a("li"),cde=a("strong"),_qo=o("sew-d"),bqo=o(" \u2014 "),yN=a("a"),vqo=o("SEWDForCTC"),Tqo=o(" (SEW-D model)"),Fqo=l(),C1=a("li"),mde=a("strong"),Cqo=o("unispeech"),Mqo=o(" \u2014 "),wN=a("a"),Eqo=o("UniSpeechForCTC"),yqo=o(" (UniSpeech model)"),wqo=l(),M1=a("li"),fde=a("strong"),Aqo=o("unispeech-sat"),Lqo=o(" \u2014 "),AN=a("a"),Bqo=o("UniSpeechSatForCTC"),xqo=o(" (UniSpeechSat model)"),kqo=l(),E1=a("li"),gde=a("strong"),Rqo=o("wav2vec2"),Sqo=o(" \u2014 "),LN=a("a"),Pqo=o("Wav2Vec2ForCTC"),$qo=o(" (Wav2Vec2 model)"),Iqo=l(),y1=a("li"),hde=a("strong"),jqo=o("wavlm"),Dqo=o(" \u2014 "),BN=a("a"),Nqo=o("WavLMForCTC"),qqo=o(" (WavLM model)"),Oqo=l(),w1=a("p"),Gqo=o("The model is set in evaluation mode by default using "),ude=a("code"),Xqo=o("model.eval()"),Vqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pde=a("code"),zqo=o("model.train()"),Wqo=l(),_de=a("p"),Qqo=o("Examples:"),Hqo=l(),m(F6.$$.fragment),uxe=l(),Vd=a("h2"),A1=a("a"),bde=a("span"),m(C6.$$.fragment),Uqo=l(),vde=a("span"),Jqo=o("AutoModelForSpeechSeq2Seq"),pxe=l(),ct=a("div"),m(M6.$$.fragment),Yqo=l(),zd=a("p"),Kqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Tde=a("code"),Zqo=o("from_pretrained()"),eOo=o("class method or the "),Fde=a("code"),oOo=o("from_config()"),tOo=o(`class
method.`),rOo=l(),E6=a("p"),aOo=o("This class cannot be instantiated directly using "),Cde=a("code"),sOo=o("__init__()"),nOo=o(" (throws an error)."),lOo=l(),ar=a("div"),m(y6.$$.fragment),iOo=l(),Mde=a("p"),dOo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),cOo=l(),Wd=a("p"),mOo=o(`Note:
Loading a model from its configuration file does `),Ede=a("strong"),fOo=o("not"),gOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),yde=a("code"),hOo=o("from_pretrained()"),uOo=o("to load the model weights."),pOo=l(),wde=a("p"),_Oo=o("Examples:"),bOo=l(),m(w6.$$.fragment),vOo=l(),He=a("div"),m(A6.$$.fragment),TOo=l(),Ade=a("p"),FOo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),COo=l(),ns=a("p"),MOo=o("The model class to instantiate is selected based on the "),Lde=a("code"),EOo=o("model_type"),yOo=o(` property of the config object (either
passed as an argument or loaded from `),Bde=a("code"),wOo=o("pretrained_model_name_or_path"),AOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xde=a("code"),LOo=o("pretrained_model_name_or_path"),BOo=o(":"),xOo=l(),L6=a("ul"),L1=a("li"),kde=a("strong"),kOo=o("speech-encoder-decoder"),ROo=o(" \u2014 "),xN=a("a"),SOo=o("SpeechEncoderDecoderModel"),POo=o(" (Speech Encoder decoder model)"),$Oo=l(),B1=a("li"),Rde=a("strong"),IOo=o("speech_to_text"),jOo=o(" \u2014 "),kN=a("a"),DOo=o("Speech2TextForConditionalGeneration"),NOo=o(" (Speech2Text model)"),qOo=l(),x1=a("p"),OOo=o("The model is set in evaluation mode by default using "),Sde=a("code"),GOo=o("model.eval()"),XOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pde=a("code"),VOo=o("model.train()"),zOo=l(),$de=a("p"),WOo=o("Examples:"),QOo=l(),m(B6.$$.fragment),_xe=l(),Qd=a("h2"),k1=a("a"),Ide=a("span"),m(x6.$$.fragment),HOo=l(),jde=a("span"),UOo=o("AutoModelForAudioXVector"),bxe=l(),mt=a("div"),m(k6.$$.fragment),JOo=l(),Hd=a("p"),YOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),Dde=a("code"),KOo=o("from_pretrained()"),ZOo=o("class method or the "),Nde=a("code"),eGo=o("from_config()"),oGo=o(`class
method.`),tGo=l(),R6=a("p"),rGo=o("This class cannot be instantiated directly using "),qde=a("code"),aGo=o("__init__()"),sGo=o(" (throws an error)."),nGo=l(),sr=a("div"),m(S6.$$.fragment),lGo=l(),Ode=a("p"),iGo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),dGo=l(),Ud=a("p"),cGo=o(`Note:
Loading a model from its configuration file does `),Gde=a("strong"),mGo=o("not"),fGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xde=a("code"),gGo=o("from_pretrained()"),hGo=o("to load the model weights."),uGo=l(),Vde=a("p"),pGo=o("Examples:"),_Go=l(),m(P6.$$.fragment),bGo=l(),Ue=a("div"),m($6.$$.fragment),vGo=l(),zde=a("p"),TGo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),FGo=l(),ls=a("p"),CGo=o("The model class to instantiate is selected based on the "),Wde=a("code"),MGo=o("model_type"),EGo=o(` property of the config object (either
passed as an argument or loaded from `),Qde=a("code"),yGo=o("pretrained_model_name_or_path"),wGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hde=a("code"),AGo=o("pretrained_model_name_or_path"),LGo=o(":"),BGo=l(),is=a("ul"),R1=a("li"),Ude=a("strong"),xGo=o("data2vec-audio"),kGo=o(" \u2014 "),RN=a("a"),RGo=o("Data2VecAudioForXVector"),SGo=o(" (Data2VecAudio model)"),PGo=l(),S1=a("li"),Jde=a("strong"),$Go=o("unispeech-sat"),IGo=o(" \u2014 "),SN=a("a"),jGo=o("UniSpeechSatForXVector"),DGo=o(" (UniSpeechSat model)"),NGo=l(),P1=a("li"),Yde=a("strong"),qGo=o("wav2vec2"),OGo=o(" \u2014 "),PN=a("a"),GGo=o("Wav2Vec2ForXVector"),XGo=o(" (Wav2Vec2 model)"),VGo=l(),$1=a("li"),Kde=a("strong"),zGo=o("wavlm"),WGo=o(" \u2014 "),$N=a("a"),QGo=o("WavLMForXVector"),HGo=o(" (WavLM model)"),UGo=l(),I1=a("p"),JGo=o("The model is set in evaluation mode by default using "),Zde=a("code"),YGo=o("model.eval()"),KGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ece=a("code"),ZGo=o("model.train()"),eXo=l(),oce=a("p"),oXo=o("Examples:"),tXo=l(),m(I6.$$.fragment),vxe=l(),Jd=a("h2"),j1=a("a"),tce=a("span"),m(j6.$$.fragment),rXo=l(),rce=a("span"),aXo=o("AutoModelForMaskedImageModeling"),Txe=l(),ft=a("div"),m(D6.$$.fragment),sXo=l(),Yd=a("p"),nXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),ace=a("code"),lXo=o("from_pretrained()"),iXo=o("class method or the "),sce=a("code"),dXo=o("from_config()"),cXo=o(`class
method.`),mXo=l(),N6=a("p"),fXo=o("This class cannot be instantiated directly using "),nce=a("code"),gXo=o("__init__()"),hXo=o(" (throws an error)."),uXo=l(),nr=a("div"),m(q6.$$.fragment),pXo=l(),lce=a("p"),_Xo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),bXo=l(),Kd=a("p"),vXo=o(`Note:
Loading a model from its configuration file does `),ice=a("strong"),TXo=o("not"),FXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dce=a("code"),CXo=o("from_pretrained()"),MXo=o("to load the model weights."),EXo=l(),cce=a("p"),yXo=o("Examples:"),wXo=l(),m(O6.$$.fragment),AXo=l(),Je=a("div"),m(G6.$$.fragment),LXo=l(),mce=a("p"),BXo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),xXo=l(),ds=a("p"),kXo=o("The model class to instantiate is selected based on the "),fce=a("code"),RXo=o("model_type"),SXo=o(` property of the config object (either
passed as an argument or loaded from `),gce=a("code"),PXo=o("pretrained_model_name_or_path"),$Xo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hce=a("code"),IXo=o("pretrained_model_name_or_path"),jXo=o(":"),DXo=l(),Zd=a("ul"),D1=a("li"),uce=a("strong"),NXo=o("deit"),qXo=o(" \u2014 "),IN=a("a"),OXo=o("DeiTForMaskedImageModeling"),GXo=o(" (DeiT model)"),XXo=l(),N1=a("li"),pce=a("strong"),VXo=o("swin"),zXo=o(" \u2014 "),jN=a("a"),WXo=o("SwinForMaskedImageModeling"),QXo=o(" (Swin model)"),HXo=l(),q1=a("li"),_ce=a("strong"),UXo=o("vit"),JXo=o(" \u2014 "),DN=a("a"),YXo=o("ViTForMaskedImageModeling"),KXo=o(" (ViT model)"),ZXo=l(),O1=a("p"),eVo=o("The model is set in evaluation mode by default using "),bce=a("code"),oVo=o("model.eval()"),tVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vce=a("code"),rVo=o("model.train()"),aVo=l(),Tce=a("p"),sVo=o("Examples:"),nVo=l(),m(X6.$$.fragment),Fxe=l(),ec=a("h2"),G1=a("a"),Fce=a("span"),m(V6.$$.fragment),lVo=l(),Cce=a("span"),iVo=o("AutoModelForObjectDetection"),Cxe=l(),gt=a("div"),m(z6.$$.fragment),dVo=l(),oc=a("p"),cVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Mce=a("code"),mVo=o("from_pretrained()"),fVo=o("class method or the "),Ece=a("code"),gVo=o("from_config()"),hVo=o(`class
method.`),uVo=l(),W6=a("p"),pVo=o("This class cannot be instantiated directly using "),yce=a("code"),_Vo=o("__init__()"),bVo=o(" (throws an error)."),vVo=l(),lr=a("div"),m(Q6.$$.fragment),TVo=l(),wce=a("p"),FVo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),CVo=l(),tc=a("p"),MVo=o(`Note:
Loading a model from its configuration file does `),Ace=a("strong"),EVo=o("not"),yVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lce=a("code"),wVo=o("from_pretrained()"),AVo=o("to load the model weights."),LVo=l(),Bce=a("p"),BVo=o("Examples:"),xVo=l(),m(H6.$$.fragment),kVo=l(),Ye=a("div"),m(U6.$$.fragment),RVo=l(),xce=a("p"),SVo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),PVo=l(),cs=a("p"),$Vo=o("The model class to instantiate is selected based on the "),kce=a("code"),IVo=o("model_type"),jVo=o(` property of the config object (either
passed as an argument or loaded from `),Rce=a("code"),DVo=o("pretrained_model_name_or_path"),NVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sce=a("code"),qVo=o("pretrained_model_name_or_path"),OVo=o(":"),GVo=l(),Pce=a("ul"),X1=a("li"),$ce=a("strong"),XVo=o("detr"),VVo=o(" \u2014 "),NN=a("a"),zVo=o("DetrForObjectDetection"),WVo=o(" (DETR model)"),QVo=l(),V1=a("p"),HVo=o("The model is set in evaluation mode by default using "),Ice=a("code"),UVo=o("model.eval()"),JVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jce=a("code"),YVo=o("model.train()"),KVo=l(),Dce=a("p"),ZVo=o("Examples:"),ezo=l(),m(J6.$$.fragment),Mxe=l(),rc=a("h2"),z1=a("a"),Nce=a("span"),m(Y6.$$.fragment),ozo=l(),qce=a("span"),tzo=o("AutoModelForImageSegmentation"),Exe=l(),ht=a("div"),m(K6.$$.fragment),rzo=l(),ac=a("p"),azo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Oce=a("code"),szo=o("from_pretrained()"),nzo=o("class method or the "),Gce=a("code"),lzo=o("from_config()"),izo=o(`class
method.`),dzo=l(),Z6=a("p"),czo=o("This class cannot be instantiated directly using "),Xce=a("code"),mzo=o("__init__()"),fzo=o(" (throws an error)."),gzo=l(),ir=a("div"),m(eA.$$.fragment),hzo=l(),Vce=a("p"),uzo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),pzo=l(),sc=a("p"),_zo=o(`Note:
Loading a model from its configuration file does `),zce=a("strong"),bzo=o("not"),vzo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wce=a("code"),Tzo=o("from_pretrained()"),Fzo=o("to load the model weights."),Czo=l(),Qce=a("p"),Mzo=o("Examples:"),Ezo=l(),m(oA.$$.fragment),yzo=l(),Ke=a("div"),m(tA.$$.fragment),wzo=l(),Hce=a("p"),Azo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),Lzo=l(),ms=a("p"),Bzo=o("The model class to instantiate is selected based on the "),Uce=a("code"),xzo=o("model_type"),kzo=o(` property of the config object (either
passed as an argument or loaded from `),Jce=a("code"),Rzo=o("pretrained_model_name_or_path"),Szo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yce=a("code"),Pzo=o("pretrained_model_name_or_path"),$zo=o(":"),Izo=l(),Kce=a("ul"),W1=a("li"),Zce=a("strong"),jzo=o("detr"),Dzo=o(" \u2014 "),qN=a("a"),Nzo=o("DetrForSegmentation"),qzo=o(" (DETR model)"),Ozo=l(),Q1=a("p"),Gzo=o("The model is set in evaluation mode by default using "),eme=a("code"),Xzo=o("model.eval()"),Vzo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ome=a("code"),zzo=o("model.train()"),Wzo=l(),tme=a("p"),Qzo=o("Examples:"),Hzo=l(),m(rA.$$.fragment),yxe=l(),nc=a("h2"),H1=a("a"),rme=a("span"),m(aA.$$.fragment),Uzo=l(),ame=a("span"),Jzo=o("AutoModelForSemanticSegmentation"),wxe=l(),ut=a("div"),m(sA.$$.fragment),Yzo=l(),lc=a("p"),Kzo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),sme=a("code"),Zzo=o("from_pretrained()"),eWo=o("class method or the "),nme=a("code"),oWo=o("from_config()"),tWo=o(`class
method.`),rWo=l(),nA=a("p"),aWo=o("This class cannot be instantiated directly using "),lme=a("code"),sWo=o("__init__()"),nWo=o(" (throws an error)."),lWo=l(),dr=a("div"),m(lA.$$.fragment),iWo=l(),ime=a("p"),dWo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),cWo=l(),ic=a("p"),mWo=o(`Note:
Loading a model from its configuration file does `),dme=a("strong"),fWo=o("not"),gWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cme=a("code"),hWo=o("from_pretrained()"),uWo=o("to load the model weights."),pWo=l(),mme=a("p"),_Wo=o("Examples:"),bWo=l(),m(iA.$$.fragment),vWo=l(),Ze=a("div"),m(dA.$$.fragment),TWo=l(),fme=a("p"),FWo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),CWo=l(),fs=a("p"),MWo=o("The model class to instantiate is selected based on the "),gme=a("code"),EWo=o("model_type"),yWo=o(` property of the config object (either
passed as an argument or loaded from `),hme=a("code"),wWo=o("pretrained_model_name_or_path"),AWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ume=a("code"),LWo=o("pretrained_model_name_or_path"),BWo=o(":"),xWo=l(),cA=a("ul"),U1=a("li"),pme=a("strong"),kWo=o("beit"),RWo=o(" \u2014 "),ON=a("a"),SWo=o("BeitForSemanticSegmentation"),PWo=o(" (BEiT model)"),$Wo=l(),J1=a("li"),_me=a("strong"),IWo=o("segformer"),jWo=o(" \u2014 "),GN=a("a"),DWo=o("SegformerForSemanticSegmentation"),NWo=o(" (SegFormer model)"),qWo=l(),Y1=a("p"),OWo=o("The model is set in evaluation mode by default using "),bme=a("code"),GWo=o("model.eval()"),XWo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vme=a("code"),VWo=o("model.train()"),zWo=l(),Tme=a("p"),WWo=o("Examples:"),QWo=l(),m(mA.$$.fragment),Axe=l(),dc=a("h2"),K1=a("a"),Fme=a("span"),m(fA.$$.fragment),HWo=l(),Cme=a("span"),UWo=o("AutoModelForInstanceSegmentation"),Lxe=l(),pt=a("div"),m(gA.$$.fragment),JWo=l(),cc=a("p"),YWo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the `),Mme=a("code"),KWo=o("from_pretrained()"),ZWo=o("class method or the "),Eme=a("code"),eQo=o("from_config()"),oQo=o(`class
method.`),tQo=l(),hA=a("p"),rQo=o("This class cannot be instantiated directly using "),yme=a("code"),aQo=o("__init__()"),sQo=o(" (throws an error)."),nQo=l(),cr=a("div"),m(uA.$$.fragment),lQo=l(),wme=a("p"),iQo=o("Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration."),dQo=l(),mc=a("p"),cQo=o(`Note:
Loading a model from its configuration file does `),Ame=a("strong"),mQo=o("not"),fQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lme=a("code"),gQo=o("from_pretrained()"),hQo=o("to load the model weights."),uQo=l(),Bme=a("p"),pQo=o("Examples:"),_Qo=l(),m(pA.$$.fragment),bQo=l(),eo=a("div"),m(_A.$$.fragment),vQo=l(),xme=a("p"),TQo=o("Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model."),FQo=l(),gs=a("p"),CQo=o("The model class to instantiate is selected based on the "),kme=a("code"),MQo=o("model_type"),EQo=o(` property of the config object (either
passed as an argument or loaded from `),Rme=a("code"),yQo=o("pretrained_model_name_or_path"),wQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sme=a("code"),AQo=o("pretrained_model_name_or_path"),LQo=o(":"),BQo=l(),Pme=a("ul"),Z1=a("li"),$me=a("strong"),xQo=o("maskformer"),kQo=o(" \u2014 "),XN=a("a"),RQo=o("MaskFormerForInstanceSegmentation"),SQo=o(" (MaskFormer model)"),PQo=l(),eF=a("p"),$Qo=o("The model is set in evaluation mode by default using "),Ime=a("code"),IQo=o("model.eval()"),jQo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jme=a("code"),DQo=o("model.train()"),NQo=l(),Dme=a("p"),qQo=o("Examples:"),OQo=l(),m(bA.$$.fragment),Bxe=l(),fc=a("h2"),oF=a("a"),Nme=a("span"),m(vA.$$.fragment),GQo=l(),qme=a("span"),XQo=o("TFAutoModel"),xxe=l(),_t=a("div"),m(TA.$$.fragment),VQo=l(),gc=a("p"),zQo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Ome=a("code"),WQo=o("from_pretrained()"),QQo=o("class method or the "),Gme=a("code"),HQo=o("from_config()"),UQo=o(`class
method.`),JQo=l(),FA=a("p"),YQo=o("This class cannot be instantiated directly using "),Xme=a("code"),KQo=o("__init__()"),ZQo=o(" (throws an error)."),eHo=l(),mr=a("div"),m(CA.$$.fragment),oHo=l(),Vme=a("p"),tHo=o("Instantiates one of the base model classes of the library from a configuration."),rHo=l(),hc=a("p"),aHo=o(`Note:
Loading a model from its configuration file does `),zme=a("strong"),sHo=o("not"),nHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wme=a("code"),lHo=o("from_pretrained()"),iHo=o("to load the model weights."),dHo=l(),Qme=a("p"),cHo=o("Examples:"),mHo=l(),m(MA.$$.fragment),fHo=l(),ho=a("div"),m(EA.$$.fragment),gHo=l(),Hme=a("p"),hHo=o("Instantiate one of the base model classes of the library from a pretrained model."),uHo=l(),hs=a("p"),pHo=o("The model class to instantiate is selected based on the "),Ume=a("code"),_Ho=o("model_type"),bHo=o(` property of the config object (either
passed as an argument or loaded from `),Jme=a("code"),vHo=o("pretrained_model_name_or_path"),THo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yme=a("code"),FHo=o("pretrained_model_name_or_path"),CHo=o(":"),MHo=l(),B=a("ul"),tF=a("li"),Kme=a("strong"),EHo=o("albert"),yHo=o(" \u2014 "),VN=a("a"),wHo=o("TFAlbertModel"),AHo=o(" (ALBERT model)"),LHo=l(),rF=a("li"),Zme=a("strong"),BHo=o("bart"),xHo=o(" \u2014 "),zN=a("a"),kHo=o("TFBartModel"),RHo=o(" (BART model)"),SHo=l(),aF=a("li"),efe=a("strong"),PHo=o("bert"),$Ho=o(" \u2014 "),WN=a("a"),IHo=o("TFBertModel"),jHo=o(" (BERT model)"),DHo=l(),sF=a("li"),ofe=a("strong"),NHo=o("blenderbot"),qHo=o(" \u2014 "),QN=a("a"),OHo=o("TFBlenderbotModel"),GHo=o(" (Blenderbot model)"),XHo=l(),nF=a("li"),tfe=a("strong"),VHo=o("blenderbot-small"),zHo=o(" \u2014 "),HN=a("a"),WHo=o("TFBlenderbotSmallModel"),QHo=o(" (BlenderbotSmall model)"),HHo=l(),lF=a("li"),rfe=a("strong"),UHo=o("camembert"),JHo=o(" \u2014 "),UN=a("a"),YHo=o("TFCamembertModel"),KHo=o(" (CamemBERT model)"),ZHo=l(),iF=a("li"),afe=a("strong"),eUo=o("clip"),oUo=o(" \u2014 "),JN=a("a"),tUo=o("TFCLIPModel"),rUo=o(" (CLIP model)"),aUo=l(),dF=a("li"),sfe=a("strong"),sUo=o("convbert"),nUo=o(" \u2014 "),YN=a("a"),lUo=o("TFConvBertModel"),iUo=o(" (ConvBERT model)"),dUo=l(),cF=a("li"),nfe=a("strong"),cUo=o("convnext"),mUo=o(" \u2014 "),KN=a("a"),fUo=o("TFConvNextModel"),gUo=o(" (ConvNext model)"),hUo=l(),mF=a("li"),lfe=a("strong"),uUo=o("ctrl"),pUo=o(" \u2014 "),ZN=a("a"),_Uo=o("TFCTRLModel"),bUo=o(" (CTRL model)"),vUo=l(),fF=a("li"),ife=a("strong"),TUo=o("deberta"),FUo=o(" \u2014 "),eq=a("a"),CUo=o("TFDebertaModel"),MUo=o(" (DeBERTa model)"),EUo=l(),gF=a("li"),dfe=a("strong"),yUo=o("deberta-v2"),wUo=o(" \u2014 "),oq=a("a"),AUo=o("TFDebertaV2Model"),LUo=o(" (DeBERTa-v2 model)"),BUo=l(),hF=a("li"),cfe=a("strong"),xUo=o("distilbert"),kUo=o(" \u2014 "),tq=a("a"),RUo=o("TFDistilBertModel"),SUo=o(" (DistilBERT model)"),PUo=l(),uF=a("li"),mfe=a("strong"),$Uo=o("dpr"),IUo=o(" \u2014 "),rq=a("a"),jUo=o("TFDPRQuestionEncoder"),DUo=o(" (DPR model)"),NUo=l(),pF=a("li"),ffe=a("strong"),qUo=o("electra"),OUo=o(" \u2014 "),aq=a("a"),GUo=o("TFElectraModel"),XUo=o(" (ELECTRA model)"),VUo=l(),_F=a("li"),gfe=a("strong"),zUo=o("flaubert"),WUo=o(" \u2014 "),sq=a("a"),QUo=o("TFFlaubertModel"),HUo=o(" (FlauBERT model)"),UUo=l(),On=a("li"),hfe=a("strong"),JUo=o("funnel"),YUo=o(" \u2014 "),nq=a("a"),KUo=o("TFFunnelModel"),ZUo=o(" or "),lq=a("a"),eJo=o("TFFunnelBaseModel"),oJo=o(" (Funnel Transformer model)"),tJo=l(),bF=a("li"),ufe=a("strong"),rJo=o("gpt2"),aJo=o(" \u2014 "),iq=a("a"),sJo=o("TFGPT2Model"),nJo=o(" (OpenAI GPT-2 model)"),lJo=l(),vF=a("li"),pfe=a("strong"),iJo=o("hubert"),dJo=o(" \u2014 "),dq=a("a"),cJo=o("TFHubertModel"),mJo=o(" (Hubert model)"),fJo=l(),TF=a("li"),_fe=a("strong"),gJo=o("layoutlm"),hJo=o(" \u2014 "),cq=a("a"),uJo=o("TFLayoutLMModel"),pJo=o(" (LayoutLM model)"),_Jo=l(),FF=a("li"),bfe=a("strong"),bJo=o("led"),vJo=o(" \u2014 "),mq=a("a"),TJo=o("TFLEDModel"),FJo=o(" (LED model)"),CJo=l(),CF=a("li"),vfe=a("strong"),MJo=o("longformer"),EJo=o(" \u2014 "),fq=a("a"),yJo=o("TFLongformerModel"),wJo=o(" (Longformer model)"),AJo=l(),MF=a("li"),Tfe=a("strong"),LJo=o("lxmert"),BJo=o(" \u2014 "),gq=a("a"),xJo=o("TFLxmertModel"),kJo=o(" (LXMERT model)"),RJo=l(),EF=a("li"),Ffe=a("strong"),SJo=o("marian"),PJo=o(" \u2014 "),hq=a("a"),$Jo=o("TFMarianModel"),IJo=o(" (Marian model)"),jJo=l(),yF=a("li"),Cfe=a("strong"),DJo=o("mbart"),NJo=o(" \u2014 "),uq=a("a"),qJo=o("TFMBartModel"),OJo=o(" (mBART model)"),GJo=l(),wF=a("li"),Mfe=a("strong"),XJo=o("mobilebert"),VJo=o(" \u2014 "),pq=a("a"),zJo=o("TFMobileBertModel"),WJo=o(" (MobileBERT model)"),QJo=l(),AF=a("li"),Efe=a("strong"),HJo=o("mpnet"),UJo=o(" \u2014 "),_q=a("a"),JJo=o("TFMPNetModel"),YJo=o(" (MPNet model)"),KJo=l(),LF=a("li"),yfe=a("strong"),ZJo=o("mt5"),eYo=o(" \u2014 "),bq=a("a"),oYo=o("TFMT5Model"),tYo=o(" (mT5 model)"),rYo=l(),BF=a("li"),wfe=a("strong"),aYo=o("openai-gpt"),sYo=o(" \u2014 "),vq=a("a"),nYo=o("TFOpenAIGPTModel"),lYo=o(" (OpenAI GPT model)"),iYo=l(),xF=a("li"),Afe=a("strong"),dYo=o("pegasus"),cYo=o(" \u2014 "),Tq=a("a"),mYo=o("TFPegasusModel"),fYo=o(" (Pegasus model)"),gYo=l(),kF=a("li"),Lfe=a("strong"),hYo=o("rembert"),uYo=o(" \u2014 "),Fq=a("a"),pYo=o("TFRemBertModel"),_Yo=o(" (RemBERT model)"),bYo=l(),RF=a("li"),Bfe=a("strong"),vYo=o("roberta"),TYo=o(" \u2014 "),Cq=a("a"),FYo=o("TFRobertaModel"),CYo=o(" (RoBERTa model)"),MYo=l(),SF=a("li"),xfe=a("strong"),EYo=o("roformer"),yYo=o(" \u2014 "),Mq=a("a"),wYo=o("TFRoFormerModel"),AYo=o(" (RoFormer model)"),LYo=l(),PF=a("li"),kfe=a("strong"),BYo=o("speech_to_text"),xYo=o(" \u2014 "),Eq=a("a"),kYo=o("TFSpeech2TextModel"),RYo=o(" (Speech2Text model)"),SYo=l(),$F=a("li"),Rfe=a("strong"),PYo=o("t5"),$Yo=o(" \u2014 "),yq=a("a"),IYo=o("TFT5Model"),jYo=o(" (T5 model)"),DYo=l(),IF=a("li"),Sfe=a("strong"),NYo=o("tapas"),qYo=o(" \u2014 "),wq=a("a"),OYo=o("TFTapasModel"),GYo=o(" (TAPAS model)"),XYo=l(),jF=a("li"),Pfe=a("strong"),VYo=o("transfo-xl"),zYo=o(" \u2014 "),Aq=a("a"),WYo=o("TFTransfoXLModel"),QYo=o(" (Transformer-XL model)"),HYo=l(),DF=a("li"),$fe=a("strong"),UYo=o("vit"),JYo=o(" \u2014 "),Lq=a("a"),YYo=o("TFViTModel"),KYo=o(" (ViT model)"),ZYo=l(),NF=a("li"),Ife=a("strong"),eKo=o("wav2vec2"),oKo=o(" \u2014 "),Bq=a("a"),tKo=o("TFWav2Vec2Model"),rKo=o(" (Wav2Vec2 model)"),aKo=l(),qF=a("li"),jfe=a("strong"),sKo=o("xlm"),nKo=o(" \u2014 "),xq=a("a"),lKo=o("TFXLMModel"),iKo=o(" (XLM model)"),dKo=l(),OF=a("li"),Dfe=a("strong"),cKo=o("xlm-roberta"),mKo=o(" \u2014 "),kq=a("a"),fKo=o("TFXLMRobertaModel"),gKo=o(" (XLM-RoBERTa model)"),hKo=l(),GF=a("li"),Nfe=a("strong"),uKo=o("xlnet"),pKo=o(" \u2014 "),Rq=a("a"),_Ko=o("TFXLNetModel"),bKo=o(" (XLNet model)"),vKo=l(),qfe=a("p"),TKo=o("Examples:"),FKo=l(),m(yA.$$.fragment),kxe=l(),uc=a("h2"),XF=a("a"),Ofe=a("span"),m(wA.$$.fragment),CKo=l(),Gfe=a("span"),MKo=o("TFAutoModelForPreTraining"),Rxe=l(),bt=a("div"),m(AA.$$.fragment),EKo=l(),pc=a("p"),yKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Xfe=a("code"),wKo=o("from_pretrained()"),AKo=o("class method or the "),Vfe=a("code"),LKo=o("from_config()"),BKo=o(`class
method.`),xKo=l(),LA=a("p"),kKo=o("This class cannot be instantiated directly using "),zfe=a("code"),RKo=o("__init__()"),SKo=o(" (throws an error)."),PKo=l(),fr=a("div"),m(BA.$$.fragment),$Ko=l(),Wfe=a("p"),IKo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),jKo=l(),_c=a("p"),DKo=o(`Note:
Loading a model from its configuration file does `),Qfe=a("strong"),NKo=o("not"),qKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hfe=a("code"),OKo=o("from_pretrained()"),GKo=o("to load the model weights."),XKo=l(),Ufe=a("p"),VKo=o("Examples:"),zKo=l(),m(xA.$$.fragment),WKo=l(),uo=a("div"),m(kA.$$.fragment),QKo=l(),Jfe=a("p"),HKo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),UKo=l(),us=a("p"),JKo=o("The model class to instantiate is selected based on the "),Yfe=a("code"),YKo=o("model_type"),KKo=o(` property of the config object (either
passed as an argument or loaded from `),Kfe=a("code"),ZKo=o("pretrained_model_name_or_path"),eZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zfe=a("code"),oZo=o("pretrained_model_name_or_path"),tZo=o(":"),rZo=l(),H=a("ul"),VF=a("li"),ege=a("strong"),aZo=o("albert"),sZo=o(" \u2014 "),Sq=a("a"),nZo=o("TFAlbertForPreTraining"),lZo=o(" (ALBERT model)"),iZo=l(),zF=a("li"),oge=a("strong"),dZo=o("bart"),cZo=o(" \u2014 "),Pq=a("a"),mZo=o("TFBartForConditionalGeneration"),fZo=o(" (BART model)"),gZo=l(),WF=a("li"),tge=a("strong"),hZo=o("bert"),uZo=o(" \u2014 "),$q=a("a"),pZo=o("TFBertForPreTraining"),_Zo=o(" (BERT model)"),bZo=l(),QF=a("li"),rge=a("strong"),vZo=o("camembert"),TZo=o(" \u2014 "),Iq=a("a"),FZo=o("TFCamembertForMaskedLM"),CZo=o(" (CamemBERT model)"),MZo=l(),HF=a("li"),age=a("strong"),EZo=o("ctrl"),yZo=o(" \u2014 "),jq=a("a"),wZo=o("TFCTRLLMHeadModel"),AZo=o(" (CTRL model)"),LZo=l(),UF=a("li"),sge=a("strong"),BZo=o("distilbert"),xZo=o(" \u2014 "),Dq=a("a"),kZo=o("TFDistilBertForMaskedLM"),RZo=o(" (DistilBERT model)"),SZo=l(),JF=a("li"),nge=a("strong"),PZo=o("electra"),$Zo=o(" \u2014 "),Nq=a("a"),IZo=o("TFElectraForPreTraining"),jZo=o(" (ELECTRA model)"),DZo=l(),YF=a("li"),lge=a("strong"),NZo=o("flaubert"),qZo=o(" \u2014 "),qq=a("a"),OZo=o("TFFlaubertWithLMHeadModel"),GZo=o(" (FlauBERT model)"),XZo=l(),KF=a("li"),ige=a("strong"),VZo=o("funnel"),zZo=o(" \u2014 "),Oq=a("a"),WZo=o("TFFunnelForPreTraining"),QZo=o(" (Funnel Transformer model)"),HZo=l(),ZF=a("li"),dge=a("strong"),UZo=o("gpt2"),JZo=o(" \u2014 "),Gq=a("a"),YZo=o("TFGPT2LMHeadModel"),KZo=o(" (OpenAI GPT-2 model)"),ZZo=l(),eC=a("li"),cge=a("strong"),eet=o("layoutlm"),oet=o(" \u2014 "),Xq=a("a"),tet=o("TFLayoutLMForMaskedLM"),ret=o(" (LayoutLM model)"),aet=l(),oC=a("li"),mge=a("strong"),set=o("lxmert"),net=o(" \u2014 "),Vq=a("a"),iet=o("TFLxmertForPreTraining"),det=o(" (LXMERT model)"),cet=l(),tC=a("li"),fge=a("strong"),met=o("mobilebert"),fet=o(" \u2014 "),zq=a("a"),get=o("TFMobileBertForPreTraining"),het=o(" (MobileBERT model)"),uet=l(),rC=a("li"),gge=a("strong"),pet=o("mpnet"),_et=o(" \u2014 "),Wq=a("a"),bet=o("TFMPNetForMaskedLM"),vet=o(" (MPNet model)"),Tet=l(),aC=a("li"),hge=a("strong"),Fet=o("openai-gpt"),Cet=o(" \u2014 "),Qq=a("a"),Met=o("TFOpenAIGPTLMHeadModel"),Eet=o(" (OpenAI GPT model)"),yet=l(),sC=a("li"),uge=a("strong"),wet=o("roberta"),Aet=o(" \u2014 "),Hq=a("a"),Let=o("TFRobertaForMaskedLM"),Bet=o(" (RoBERTa model)"),xet=l(),nC=a("li"),pge=a("strong"),ket=o("t5"),Ret=o(" \u2014 "),Uq=a("a"),Set=o("TFT5ForConditionalGeneration"),Pet=o(" (T5 model)"),$et=l(),lC=a("li"),_ge=a("strong"),Iet=o("tapas"),jet=o(" \u2014 "),Jq=a("a"),Det=o("TFTapasForMaskedLM"),Net=o(" (TAPAS model)"),qet=l(),iC=a("li"),bge=a("strong"),Oet=o("transfo-xl"),Get=o(" \u2014 "),Yq=a("a"),Xet=o("TFTransfoXLLMHeadModel"),Vet=o(" (Transformer-XL model)"),zet=l(),dC=a("li"),vge=a("strong"),Wet=o("xlm"),Qet=o(" \u2014 "),Kq=a("a"),Het=o("TFXLMWithLMHeadModel"),Uet=o(" (XLM model)"),Jet=l(),cC=a("li"),Tge=a("strong"),Yet=o("xlm-roberta"),Ket=o(" \u2014 "),Zq=a("a"),Zet=o("TFXLMRobertaForMaskedLM"),eot=o(" (XLM-RoBERTa model)"),oot=l(),mC=a("li"),Fge=a("strong"),tot=o("xlnet"),rot=o(" \u2014 "),eO=a("a"),aot=o("TFXLNetLMHeadModel"),sot=o(" (XLNet model)"),not=l(),Cge=a("p"),lot=o("Examples:"),iot=l(),m(RA.$$.fragment),Sxe=l(),bc=a("h2"),fC=a("a"),Mge=a("span"),m(SA.$$.fragment),dot=l(),Ege=a("span"),cot=o("TFAutoModelForCausalLM"),Pxe=l(),vt=a("div"),m(PA.$$.fragment),mot=l(),vc=a("p"),fot=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),yge=a("code"),got=o("from_pretrained()"),hot=o("class method or the "),wge=a("code"),uot=o("from_config()"),pot=o(`class
method.`),_ot=l(),$A=a("p"),bot=o("This class cannot be instantiated directly using "),Age=a("code"),vot=o("__init__()"),Tot=o(" (throws an error)."),Fot=l(),gr=a("div"),m(IA.$$.fragment),Cot=l(),Lge=a("p"),Mot=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Eot=l(),Tc=a("p"),yot=o(`Note:
Loading a model from its configuration file does `),Bge=a("strong"),wot=o("not"),Aot=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xge=a("code"),Lot=o("from_pretrained()"),Bot=o("to load the model weights."),xot=l(),kge=a("p"),kot=o("Examples:"),Rot=l(),m(jA.$$.fragment),Sot=l(),po=a("div"),m(DA.$$.fragment),Pot=l(),Rge=a("p"),$ot=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Iot=l(),ps=a("p"),jot=o("The model class to instantiate is selected based on the "),Sge=a("code"),Dot=o("model_type"),Not=o(` property of the config object (either
passed as an argument or loaded from `),Pge=a("code"),qot=o("pretrained_model_name_or_path"),Oot=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$ge=a("code"),Got=o("pretrained_model_name_or_path"),Xot=o(":"),Vot=l(),he=a("ul"),gC=a("li"),Ige=a("strong"),zot=o("bert"),Wot=o(" \u2014 "),oO=a("a"),Qot=o("TFBertLMHeadModel"),Hot=o(" (BERT model)"),Uot=l(),hC=a("li"),jge=a("strong"),Jot=o("ctrl"),Yot=o(" \u2014 "),tO=a("a"),Kot=o("TFCTRLLMHeadModel"),Zot=o(" (CTRL model)"),ett=l(),uC=a("li"),Dge=a("strong"),ott=o("gpt2"),ttt=o(" \u2014 "),rO=a("a"),rtt=o("TFGPT2LMHeadModel"),att=o(" (OpenAI GPT-2 model)"),stt=l(),pC=a("li"),Nge=a("strong"),ntt=o("openai-gpt"),ltt=o(" \u2014 "),aO=a("a"),itt=o("TFOpenAIGPTLMHeadModel"),dtt=o(" (OpenAI GPT model)"),ctt=l(),_C=a("li"),qge=a("strong"),mtt=o("rembert"),ftt=o(" \u2014 "),sO=a("a"),gtt=o("TFRemBertForCausalLM"),htt=o(" (RemBERT model)"),utt=l(),bC=a("li"),Oge=a("strong"),ptt=o("roberta"),_tt=o(" \u2014 "),nO=a("a"),btt=o("TFRobertaForCausalLM"),vtt=o(" (RoBERTa model)"),Ttt=l(),vC=a("li"),Gge=a("strong"),Ftt=o("roformer"),Ctt=o(" \u2014 "),lO=a("a"),Mtt=o("TFRoFormerForCausalLM"),Ett=o(" (RoFormer model)"),ytt=l(),TC=a("li"),Xge=a("strong"),wtt=o("transfo-xl"),Att=o(" \u2014 "),iO=a("a"),Ltt=o("TFTransfoXLLMHeadModel"),Btt=o(" (Transformer-XL model)"),xtt=l(),FC=a("li"),Vge=a("strong"),ktt=o("xlm"),Rtt=o(" \u2014 "),dO=a("a"),Stt=o("TFXLMWithLMHeadModel"),Ptt=o(" (XLM model)"),$tt=l(),CC=a("li"),zge=a("strong"),Itt=o("xlnet"),jtt=o(" \u2014 "),cO=a("a"),Dtt=o("TFXLNetLMHeadModel"),Ntt=o(" (XLNet model)"),qtt=l(),Wge=a("p"),Ott=o("Examples:"),Gtt=l(),m(NA.$$.fragment),$xe=l(),Fc=a("h2"),MC=a("a"),Qge=a("span"),m(qA.$$.fragment),Xtt=l(),Hge=a("span"),Vtt=o("TFAutoModelForImageClassification"),Ixe=l(),Tt=a("div"),m(OA.$$.fragment),ztt=l(),Cc=a("p"),Wtt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Uge=a("code"),Qtt=o("from_pretrained()"),Htt=o("class method or the "),Jge=a("code"),Utt=o("from_config()"),Jtt=o(`class
method.`),Ytt=l(),GA=a("p"),Ktt=o("This class cannot be instantiated directly using "),Yge=a("code"),Ztt=o("__init__()"),ert=o(" (throws an error)."),ort=l(),hr=a("div"),m(XA.$$.fragment),trt=l(),Kge=a("p"),rrt=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),art=l(),Mc=a("p"),srt=o(`Note:
Loading a model from its configuration file does `),Zge=a("strong"),nrt=o("not"),lrt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ehe=a("code"),irt=o("from_pretrained()"),drt=o("to load the model weights."),crt=l(),ohe=a("p"),mrt=o("Examples:"),frt=l(),m(VA.$$.fragment),grt=l(),_o=a("div"),m(zA.$$.fragment),hrt=l(),the=a("p"),urt=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),prt=l(),_s=a("p"),_rt=o("The model class to instantiate is selected based on the "),rhe=a("code"),brt=o("model_type"),vrt=o(` property of the config object (either
passed as an argument or loaded from `),ahe=a("code"),Trt=o("pretrained_model_name_or_path"),Frt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),she=a("code"),Crt=o("pretrained_model_name_or_path"),Mrt=o(":"),Ert=l(),WA=a("ul"),EC=a("li"),nhe=a("strong"),yrt=o("convnext"),wrt=o(" \u2014 "),mO=a("a"),Art=o("TFConvNextForImageClassification"),Lrt=o(" (ConvNext model)"),Brt=l(),yC=a("li"),lhe=a("strong"),xrt=o("vit"),krt=o(" \u2014 "),fO=a("a"),Rrt=o("TFViTForImageClassification"),Srt=o(" (ViT model)"),Prt=l(),ihe=a("p"),$rt=o("Examples:"),Irt=l(),m(QA.$$.fragment),jxe=l(),Ec=a("h2"),wC=a("a"),dhe=a("span"),m(HA.$$.fragment),jrt=l(),che=a("span"),Drt=o("TFAutoModelForMaskedLM"),Dxe=l(),Ft=a("div"),m(UA.$$.fragment),Nrt=l(),yc=a("p"),qrt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mhe=a("code"),Ort=o("from_pretrained()"),Grt=o("class method or the "),fhe=a("code"),Xrt=o("from_config()"),Vrt=o(`class
method.`),zrt=l(),JA=a("p"),Wrt=o("This class cannot be instantiated directly using "),ghe=a("code"),Qrt=o("__init__()"),Hrt=o(" (throws an error)."),Urt=l(),ur=a("div"),m(YA.$$.fragment),Jrt=l(),hhe=a("p"),Yrt=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Krt=l(),wc=a("p"),Zrt=o(`Note:
Loading a model from its configuration file does `),uhe=a("strong"),eat=o("not"),oat=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),phe=a("code"),tat=o("from_pretrained()"),rat=o("to load the model weights."),aat=l(),_he=a("p"),sat=o("Examples:"),nat=l(),m(KA.$$.fragment),lat=l(),bo=a("div"),m(ZA.$$.fragment),iat=l(),bhe=a("p"),dat=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),cat=l(),bs=a("p"),mat=o("The model class to instantiate is selected based on the "),vhe=a("code"),fat=o("model_type"),gat=o(` property of the config object (either
passed as an argument or loaded from `),The=a("code"),hat=o("pretrained_model_name_or_path"),uat=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fhe=a("code"),pat=o("pretrained_model_name_or_path"),_at=o(":"),bat=l(),Y=a("ul"),AC=a("li"),Che=a("strong"),vat=o("albert"),Tat=o(" \u2014 "),gO=a("a"),Fat=o("TFAlbertForMaskedLM"),Cat=o(" (ALBERT model)"),Mat=l(),LC=a("li"),Mhe=a("strong"),Eat=o("bert"),yat=o(" \u2014 "),hO=a("a"),wat=o("TFBertForMaskedLM"),Aat=o(" (BERT model)"),Lat=l(),BC=a("li"),Ehe=a("strong"),Bat=o("camembert"),xat=o(" \u2014 "),uO=a("a"),kat=o("TFCamembertForMaskedLM"),Rat=o(" (CamemBERT model)"),Sat=l(),xC=a("li"),yhe=a("strong"),Pat=o("convbert"),$at=o(" \u2014 "),pO=a("a"),Iat=o("TFConvBertForMaskedLM"),jat=o(" (ConvBERT model)"),Dat=l(),kC=a("li"),whe=a("strong"),Nat=o("deberta"),qat=o(" \u2014 "),_O=a("a"),Oat=o("TFDebertaForMaskedLM"),Gat=o(" (DeBERTa model)"),Xat=l(),RC=a("li"),Ahe=a("strong"),Vat=o("deberta-v2"),zat=o(" \u2014 "),bO=a("a"),Wat=o("TFDebertaV2ForMaskedLM"),Qat=o(" (DeBERTa-v2 model)"),Hat=l(),SC=a("li"),Lhe=a("strong"),Uat=o("distilbert"),Jat=o(" \u2014 "),vO=a("a"),Yat=o("TFDistilBertForMaskedLM"),Kat=o(" (DistilBERT model)"),Zat=l(),PC=a("li"),Bhe=a("strong"),est=o("electra"),ost=o(" \u2014 "),TO=a("a"),tst=o("TFElectraForMaskedLM"),rst=o(" (ELECTRA model)"),ast=l(),$C=a("li"),xhe=a("strong"),sst=o("flaubert"),nst=o(" \u2014 "),FO=a("a"),lst=o("TFFlaubertWithLMHeadModel"),ist=o(" (FlauBERT model)"),dst=l(),IC=a("li"),khe=a("strong"),cst=o("funnel"),mst=o(" \u2014 "),CO=a("a"),fst=o("TFFunnelForMaskedLM"),gst=o(" (Funnel Transformer model)"),hst=l(),jC=a("li"),Rhe=a("strong"),ust=o("layoutlm"),pst=o(" \u2014 "),MO=a("a"),_st=o("TFLayoutLMForMaskedLM"),bst=o(" (LayoutLM model)"),vst=l(),DC=a("li"),She=a("strong"),Tst=o("longformer"),Fst=o(" \u2014 "),EO=a("a"),Cst=o("TFLongformerForMaskedLM"),Mst=o(" (Longformer model)"),Est=l(),NC=a("li"),Phe=a("strong"),yst=o("mobilebert"),wst=o(" \u2014 "),yO=a("a"),Ast=o("TFMobileBertForMaskedLM"),Lst=o(" (MobileBERT model)"),Bst=l(),qC=a("li"),$he=a("strong"),xst=o("mpnet"),kst=o(" \u2014 "),wO=a("a"),Rst=o("TFMPNetForMaskedLM"),Sst=o(" (MPNet model)"),Pst=l(),OC=a("li"),Ihe=a("strong"),$st=o("rembert"),Ist=o(" \u2014 "),AO=a("a"),jst=o("TFRemBertForMaskedLM"),Dst=o(" (RemBERT model)"),Nst=l(),GC=a("li"),jhe=a("strong"),qst=o("roberta"),Ost=o(" \u2014 "),LO=a("a"),Gst=o("TFRobertaForMaskedLM"),Xst=o(" (RoBERTa model)"),Vst=l(),XC=a("li"),Dhe=a("strong"),zst=o("roformer"),Wst=o(" \u2014 "),BO=a("a"),Qst=o("TFRoFormerForMaskedLM"),Hst=o(" (RoFormer model)"),Ust=l(),VC=a("li"),Nhe=a("strong"),Jst=o("tapas"),Yst=o(" \u2014 "),xO=a("a"),Kst=o("TFTapasForMaskedLM"),Zst=o(" (TAPAS model)"),ent=l(),zC=a("li"),qhe=a("strong"),ont=o("xlm"),tnt=o(" \u2014 "),kO=a("a"),rnt=o("TFXLMWithLMHeadModel"),ant=o(" (XLM model)"),snt=l(),WC=a("li"),Ohe=a("strong"),nnt=o("xlm-roberta"),lnt=o(" \u2014 "),RO=a("a"),int=o("TFXLMRobertaForMaskedLM"),dnt=o(" (XLM-RoBERTa model)"),cnt=l(),Ghe=a("p"),mnt=o("Examples:"),fnt=l(),m(e0.$$.fragment),Nxe=l(),Ac=a("h2"),QC=a("a"),Xhe=a("span"),m(o0.$$.fragment),gnt=l(),Vhe=a("span"),hnt=o("TFAutoModelForSeq2SeqLM"),qxe=l(),Ct=a("div"),m(t0.$$.fragment),unt=l(),Lc=a("p"),pnt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),zhe=a("code"),_nt=o("from_pretrained()"),bnt=o("class method or the "),Whe=a("code"),vnt=o("from_config()"),Tnt=o(`class
method.`),Fnt=l(),r0=a("p"),Cnt=o("This class cannot be instantiated directly using "),Qhe=a("code"),Mnt=o("__init__()"),Ent=o(" (throws an error)."),ynt=l(),pr=a("div"),m(a0.$$.fragment),wnt=l(),Hhe=a("p"),Ant=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Lnt=l(),Bc=a("p"),Bnt=o(`Note:
Loading a model from its configuration file does `),Uhe=a("strong"),xnt=o("not"),knt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jhe=a("code"),Rnt=o("from_pretrained()"),Snt=o("to load the model weights."),Pnt=l(),Yhe=a("p"),$nt=o("Examples:"),Int=l(),m(s0.$$.fragment),jnt=l(),vo=a("div"),m(n0.$$.fragment),Dnt=l(),Khe=a("p"),Nnt=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),qnt=l(),vs=a("p"),Ont=o("The model class to instantiate is selected based on the "),Zhe=a("code"),Gnt=o("model_type"),Xnt=o(` property of the config object (either
passed as an argument or loaded from `),eue=a("code"),Vnt=o("pretrained_model_name_or_path"),znt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),oue=a("code"),Wnt=o("pretrained_model_name_or_path"),Qnt=o(":"),Hnt=l(),ue=a("ul"),HC=a("li"),tue=a("strong"),Unt=o("bart"),Jnt=o(" \u2014 "),SO=a("a"),Ynt=o("TFBartForConditionalGeneration"),Knt=o(" (BART model)"),Znt=l(),UC=a("li"),rue=a("strong"),elt=o("blenderbot"),olt=o(" \u2014 "),PO=a("a"),tlt=o("TFBlenderbotForConditionalGeneration"),rlt=o(" (Blenderbot model)"),alt=l(),JC=a("li"),aue=a("strong"),slt=o("blenderbot-small"),nlt=o(" \u2014 "),$O=a("a"),llt=o("TFBlenderbotSmallForConditionalGeneration"),ilt=o(" (BlenderbotSmall model)"),dlt=l(),YC=a("li"),sue=a("strong"),clt=o("encoder-decoder"),mlt=o(" \u2014 "),IO=a("a"),flt=o("TFEncoderDecoderModel"),glt=o(" (Encoder decoder model)"),hlt=l(),KC=a("li"),nue=a("strong"),ult=o("led"),plt=o(" \u2014 "),jO=a("a"),_lt=o("TFLEDForConditionalGeneration"),blt=o(" (LED model)"),vlt=l(),ZC=a("li"),lue=a("strong"),Tlt=o("marian"),Flt=o(" \u2014 "),DO=a("a"),Clt=o("TFMarianMTModel"),Mlt=o(" (Marian model)"),Elt=l(),e4=a("li"),iue=a("strong"),ylt=o("mbart"),wlt=o(" \u2014 "),NO=a("a"),Alt=o("TFMBartForConditionalGeneration"),Llt=o(" (mBART model)"),Blt=l(),o4=a("li"),due=a("strong"),xlt=o("mt5"),klt=o(" \u2014 "),qO=a("a"),Rlt=o("TFMT5ForConditionalGeneration"),Slt=o(" (mT5 model)"),Plt=l(),t4=a("li"),cue=a("strong"),$lt=o("pegasus"),Ilt=o(" \u2014 "),OO=a("a"),jlt=o("TFPegasusForConditionalGeneration"),Dlt=o(" (Pegasus model)"),Nlt=l(),r4=a("li"),mue=a("strong"),qlt=o("t5"),Olt=o(" \u2014 "),GO=a("a"),Glt=o("TFT5ForConditionalGeneration"),Xlt=o(" (T5 model)"),Vlt=l(),fue=a("p"),zlt=o("Examples:"),Wlt=l(),m(l0.$$.fragment),Oxe=l(),xc=a("h2"),a4=a("a"),gue=a("span"),m(i0.$$.fragment),Qlt=l(),hue=a("span"),Hlt=o("TFAutoModelForSequenceClassification"),Gxe=l(),Mt=a("div"),m(d0.$$.fragment),Ult=l(),kc=a("p"),Jlt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),uue=a("code"),Ylt=o("from_pretrained()"),Klt=o("class method or the "),pue=a("code"),Zlt=o("from_config()"),eit=o(`class
method.`),oit=l(),c0=a("p"),tit=o("This class cannot be instantiated directly using "),_ue=a("code"),rit=o("__init__()"),ait=o(" (throws an error)."),sit=l(),_r=a("div"),m(m0.$$.fragment),nit=l(),bue=a("p"),lit=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),iit=l(),Rc=a("p"),dit=o(`Note:
Loading a model from its configuration file does `),vue=a("strong"),cit=o("not"),mit=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Tue=a("code"),fit=o("from_pretrained()"),git=o("to load the model weights."),hit=l(),Fue=a("p"),uit=o("Examples:"),pit=l(),m(f0.$$.fragment),_it=l(),To=a("div"),m(g0.$$.fragment),bit=l(),Cue=a("p"),vit=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Tit=l(),Ts=a("p"),Fit=o("The model class to instantiate is selected based on the "),Mue=a("code"),Cit=o("model_type"),Mit=o(` property of the config object (either
passed as an argument or loaded from `),Eue=a("code"),Eit=o("pretrained_model_name_or_path"),yit=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yue=a("code"),wit=o("pretrained_model_name_or_path"),Ait=o(":"),Lit=l(),V=a("ul"),s4=a("li"),wue=a("strong"),Bit=o("albert"),xit=o(" \u2014 "),XO=a("a"),kit=o("TFAlbertForSequenceClassification"),Rit=o(" (ALBERT model)"),Sit=l(),n4=a("li"),Aue=a("strong"),Pit=o("bert"),$it=o(" \u2014 "),VO=a("a"),Iit=o("TFBertForSequenceClassification"),jit=o(" (BERT model)"),Dit=l(),l4=a("li"),Lue=a("strong"),Nit=o("camembert"),qit=o(" \u2014 "),zO=a("a"),Oit=o("TFCamembertForSequenceClassification"),Git=o(" (CamemBERT model)"),Xit=l(),i4=a("li"),Bue=a("strong"),Vit=o("convbert"),zit=o(" \u2014 "),WO=a("a"),Wit=o("TFConvBertForSequenceClassification"),Qit=o(" (ConvBERT model)"),Hit=l(),d4=a("li"),xue=a("strong"),Uit=o("ctrl"),Jit=o(" \u2014 "),QO=a("a"),Yit=o("TFCTRLForSequenceClassification"),Kit=o(" (CTRL model)"),Zit=l(),c4=a("li"),kue=a("strong"),edt=o("deberta"),odt=o(" \u2014 "),HO=a("a"),tdt=o("TFDebertaForSequenceClassification"),rdt=o(" (DeBERTa model)"),adt=l(),m4=a("li"),Rue=a("strong"),sdt=o("deberta-v2"),ndt=o(" \u2014 "),UO=a("a"),ldt=o("TFDebertaV2ForSequenceClassification"),idt=o(" (DeBERTa-v2 model)"),ddt=l(),f4=a("li"),Sue=a("strong"),cdt=o("distilbert"),mdt=o(" \u2014 "),JO=a("a"),fdt=o("TFDistilBertForSequenceClassification"),gdt=o(" (DistilBERT model)"),hdt=l(),g4=a("li"),Pue=a("strong"),udt=o("electra"),pdt=o(" \u2014 "),YO=a("a"),_dt=o("TFElectraForSequenceClassification"),bdt=o(" (ELECTRA model)"),vdt=l(),h4=a("li"),$ue=a("strong"),Tdt=o("flaubert"),Fdt=o(" \u2014 "),KO=a("a"),Cdt=o("TFFlaubertForSequenceClassification"),Mdt=o(" (FlauBERT model)"),Edt=l(),u4=a("li"),Iue=a("strong"),ydt=o("funnel"),wdt=o(" \u2014 "),ZO=a("a"),Adt=o("TFFunnelForSequenceClassification"),Ldt=o(" (Funnel Transformer model)"),Bdt=l(),p4=a("li"),jue=a("strong"),xdt=o("gpt2"),kdt=o(" \u2014 "),eG=a("a"),Rdt=o("TFGPT2ForSequenceClassification"),Sdt=o(" (OpenAI GPT-2 model)"),Pdt=l(),_4=a("li"),Due=a("strong"),$dt=o("layoutlm"),Idt=o(" \u2014 "),oG=a("a"),jdt=o("TFLayoutLMForSequenceClassification"),Ddt=o(" (LayoutLM model)"),Ndt=l(),b4=a("li"),Nue=a("strong"),qdt=o("longformer"),Odt=o(" \u2014 "),tG=a("a"),Gdt=o("TFLongformerForSequenceClassification"),Xdt=o(" (Longformer model)"),Vdt=l(),v4=a("li"),que=a("strong"),zdt=o("mobilebert"),Wdt=o(" \u2014 "),rG=a("a"),Qdt=o("TFMobileBertForSequenceClassification"),Hdt=o(" (MobileBERT model)"),Udt=l(),T4=a("li"),Oue=a("strong"),Jdt=o("mpnet"),Ydt=o(" \u2014 "),aG=a("a"),Kdt=o("TFMPNetForSequenceClassification"),Zdt=o(" (MPNet model)"),ect=l(),F4=a("li"),Gue=a("strong"),oct=o("openai-gpt"),tct=o(" \u2014 "),sG=a("a"),rct=o("TFOpenAIGPTForSequenceClassification"),act=o(" (OpenAI GPT model)"),sct=l(),C4=a("li"),Xue=a("strong"),nct=o("rembert"),lct=o(" \u2014 "),nG=a("a"),ict=o("TFRemBertForSequenceClassification"),dct=o(" (RemBERT model)"),cct=l(),M4=a("li"),Vue=a("strong"),mct=o("roberta"),fct=o(" \u2014 "),lG=a("a"),gct=o("TFRobertaForSequenceClassification"),hct=o(" (RoBERTa model)"),uct=l(),E4=a("li"),zue=a("strong"),pct=o("roformer"),_ct=o(" \u2014 "),iG=a("a"),bct=o("TFRoFormerForSequenceClassification"),vct=o(" (RoFormer model)"),Tct=l(),y4=a("li"),Wue=a("strong"),Fct=o("tapas"),Cct=o(" \u2014 "),dG=a("a"),Mct=o("TFTapasForSequenceClassification"),Ect=o(" (TAPAS model)"),yct=l(),w4=a("li"),Que=a("strong"),wct=o("transfo-xl"),Act=o(" \u2014 "),cG=a("a"),Lct=o("TFTransfoXLForSequenceClassification"),Bct=o(" (Transformer-XL model)"),xct=l(),A4=a("li"),Hue=a("strong"),kct=o("xlm"),Rct=o(" \u2014 "),mG=a("a"),Sct=o("TFXLMForSequenceClassification"),Pct=o(" (XLM model)"),$ct=l(),L4=a("li"),Uue=a("strong"),Ict=o("xlm-roberta"),jct=o(" \u2014 "),fG=a("a"),Dct=o("TFXLMRobertaForSequenceClassification"),Nct=o(" (XLM-RoBERTa model)"),qct=l(),B4=a("li"),Jue=a("strong"),Oct=o("xlnet"),Gct=o(" \u2014 "),gG=a("a"),Xct=o("TFXLNetForSequenceClassification"),Vct=o(" (XLNet model)"),zct=l(),Yue=a("p"),Wct=o("Examples:"),Qct=l(),m(h0.$$.fragment),Xxe=l(),Sc=a("h2"),x4=a("a"),Kue=a("span"),m(u0.$$.fragment),Hct=l(),Zue=a("span"),Uct=o("TFAutoModelForMultipleChoice"),Vxe=l(),Et=a("div"),m(p0.$$.fragment),Jct=l(),Pc=a("p"),Yct=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),epe=a("code"),Kct=o("from_pretrained()"),Zct=o("class method or the "),ope=a("code"),emt=o("from_config()"),omt=o(`class
method.`),tmt=l(),_0=a("p"),rmt=o("This class cannot be instantiated directly using "),tpe=a("code"),amt=o("__init__()"),smt=o(" (throws an error)."),nmt=l(),br=a("div"),m(b0.$$.fragment),lmt=l(),rpe=a("p"),imt=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),dmt=l(),$c=a("p"),cmt=o(`Note:
Loading a model from its configuration file does `),ape=a("strong"),mmt=o("not"),fmt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),spe=a("code"),gmt=o("from_pretrained()"),hmt=o("to load the model weights."),umt=l(),npe=a("p"),pmt=o("Examples:"),_mt=l(),m(v0.$$.fragment),bmt=l(),Fo=a("div"),m(T0.$$.fragment),vmt=l(),lpe=a("p"),Tmt=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Fmt=l(),Fs=a("p"),Cmt=o("The model class to instantiate is selected based on the "),ipe=a("code"),Mmt=o("model_type"),Emt=o(` property of the config object (either
passed as an argument or loaded from `),dpe=a("code"),ymt=o("pretrained_model_name_or_path"),wmt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cpe=a("code"),Amt=o("pretrained_model_name_or_path"),Lmt=o(":"),Bmt=l(),re=a("ul"),k4=a("li"),mpe=a("strong"),xmt=o("albert"),kmt=o(" \u2014 "),hG=a("a"),Rmt=o("TFAlbertForMultipleChoice"),Smt=o(" (ALBERT model)"),Pmt=l(),R4=a("li"),fpe=a("strong"),$mt=o("bert"),Imt=o(" \u2014 "),uG=a("a"),jmt=o("TFBertForMultipleChoice"),Dmt=o(" (BERT model)"),Nmt=l(),S4=a("li"),gpe=a("strong"),qmt=o("camembert"),Omt=o(" \u2014 "),pG=a("a"),Gmt=o("TFCamembertForMultipleChoice"),Xmt=o(" (CamemBERT model)"),Vmt=l(),P4=a("li"),hpe=a("strong"),zmt=o("convbert"),Wmt=o(" \u2014 "),_G=a("a"),Qmt=o("TFConvBertForMultipleChoice"),Hmt=o(" (ConvBERT model)"),Umt=l(),$4=a("li"),upe=a("strong"),Jmt=o("distilbert"),Ymt=o(" \u2014 "),bG=a("a"),Kmt=o("TFDistilBertForMultipleChoice"),Zmt=o(" (DistilBERT model)"),eft=l(),I4=a("li"),ppe=a("strong"),oft=o("electra"),tft=o(" \u2014 "),vG=a("a"),rft=o("TFElectraForMultipleChoice"),aft=o(" (ELECTRA model)"),sft=l(),j4=a("li"),_pe=a("strong"),nft=o("flaubert"),lft=o(" \u2014 "),TG=a("a"),ift=o("TFFlaubertForMultipleChoice"),dft=o(" (FlauBERT model)"),cft=l(),D4=a("li"),bpe=a("strong"),mft=o("funnel"),fft=o(" \u2014 "),FG=a("a"),gft=o("TFFunnelForMultipleChoice"),hft=o(" (Funnel Transformer model)"),uft=l(),N4=a("li"),vpe=a("strong"),pft=o("longformer"),_ft=o(" \u2014 "),CG=a("a"),bft=o("TFLongformerForMultipleChoice"),vft=o(" (Longformer model)"),Tft=l(),q4=a("li"),Tpe=a("strong"),Fft=o("mobilebert"),Cft=o(" \u2014 "),MG=a("a"),Mft=o("TFMobileBertForMultipleChoice"),Eft=o(" (MobileBERT model)"),yft=l(),O4=a("li"),Fpe=a("strong"),wft=o("mpnet"),Aft=o(" \u2014 "),EG=a("a"),Lft=o("TFMPNetForMultipleChoice"),Bft=o(" (MPNet model)"),xft=l(),G4=a("li"),Cpe=a("strong"),kft=o("rembert"),Rft=o(" \u2014 "),yG=a("a"),Sft=o("TFRemBertForMultipleChoice"),Pft=o(" (RemBERT model)"),$ft=l(),X4=a("li"),Mpe=a("strong"),Ift=o("roberta"),jft=o(" \u2014 "),wG=a("a"),Dft=o("TFRobertaForMultipleChoice"),Nft=o(" (RoBERTa model)"),qft=l(),V4=a("li"),Epe=a("strong"),Oft=o("roformer"),Gft=o(" \u2014 "),AG=a("a"),Xft=o("TFRoFormerForMultipleChoice"),Vft=o(" (RoFormer model)"),zft=l(),z4=a("li"),ype=a("strong"),Wft=o("xlm"),Qft=o(" \u2014 "),LG=a("a"),Hft=o("TFXLMForMultipleChoice"),Uft=o(" (XLM model)"),Jft=l(),W4=a("li"),wpe=a("strong"),Yft=o("xlm-roberta"),Kft=o(" \u2014 "),BG=a("a"),Zft=o("TFXLMRobertaForMultipleChoice"),egt=o(" (XLM-RoBERTa model)"),ogt=l(),Q4=a("li"),Ape=a("strong"),tgt=o("xlnet"),rgt=o(" \u2014 "),xG=a("a"),agt=o("TFXLNetForMultipleChoice"),sgt=o(" (XLNet model)"),ngt=l(),Lpe=a("p"),lgt=o("Examples:"),igt=l(),m(F0.$$.fragment),zxe=l(),Ic=a("h2"),H4=a("a"),Bpe=a("span"),m(C0.$$.fragment),dgt=l(),xpe=a("span"),cgt=o("TFAutoModelForTableQuestionAnswering"),Wxe=l(),yt=a("div"),m(M0.$$.fragment),mgt=l(),jc=a("p"),fgt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),kpe=a("code"),ggt=o("from_pretrained()"),hgt=o("class method or the "),Rpe=a("code"),ugt=o("from_config()"),pgt=o(`class
method.`),_gt=l(),E0=a("p"),bgt=o("This class cannot be instantiated directly using "),Spe=a("code"),vgt=o("__init__()"),Tgt=o(" (throws an error)."),Fgt=l(),vr=a("div"),m(y0.$$.fragment),Cgt=l(),Ppe=a("p"),Mgt=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Egt=l(),Dc=a("p"),ygt=o(`Note:
Loading a model from its configuration file does `),$pe=a("strong"),wgt=o("not"),Agt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ipe=a("code"),Lgt=o("from_pretrained()"),Bgt=o("to load the model weights."),xgt=l(),jpe=a("p"),kgt=o("Examples:"),Rgt=l(),m(w0.$$.fragment),Sgt=l(),Co=a("div"),m(A0.$$.fragment),Pgt=l(),Dpe=a("p"),$gt=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Igt=l(),Cs=a("p"),jgt=o("The model class to instantiate is selected based on the "),Npe=a("code"),Dgt=o("model_type"),Ngt=o(` property of the config object (either
passed as an argument or loaded from `),qpe=a("code"),qgt=o("pretrained_model_name_or_path"),Ogt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ope=a("code"),Ggt=o("pretrained_model_name_or_path"),Xgt=o(":"),Vgt=l(),Gpe=a("ul"),U4=a("li"),Xpe=a("strong"),zgt=o("tapas"),Wgt=o(" \u2014 "),kG=a("a"),Qgt=o("TFTapasForQuestionAnswering"),Hgt=o(" (TAPAS model)"),Ugt=l(),Vpe=a("p"),Jgt=o("Examples:"),Ygt=l(),m(L0.$$.fragment),Qxe=l(),Nc=a("h2"),J4=a("a"),zpe=a("span"),m(B0.$$.fragment),Kgt=l(),Wpe=a("span"),Zgt=o("TFAutoModelForTokenClassification"),Hxe=l(),wt=a("div"),m(x0.$$.fragment),eht=l(),qc=a("p"),oht=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Qpe=a("code"),tht=o("from_pretrained()"),rht=o("class method or the "),Hpe=a("code"),aht=o("from_config()"),sht=o(`class
method.`),nht=l(),k0=a("p"),lht=o("This class cannot be instantiated directly using "),Upe=a("code"),iht=o("__init__()"),dht=o(" (throws an error)."),cht=l(),Tr=a("div"),m(R0.$$.fragment),mht=l(),Jpe=a("p"),fht=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),ght=l(),Oc=a("p"),hht=o(`Note:
Loading a model from its configuration file does `),Ype=a("strong"),uht=o("not"),pht=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kpe=a("code"),_ht=o("from_pretrained()"),bht=o("to load the model weights."),vht=l(),Zpe=a("p"),Tht=o("Examples:"),Fht=l(),m(S0.$$.fragment),Cht=l(),Mo=a("div"),m(P0.$$.fragment),Mht=l(),e_e=a("p"),Eht=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),yht=l(),Ms=a("p"),wht=o("The model class to instantiate is selected based on the "),o_e=a("code"),Aht=o("model_type"),Lht=o(` property of the config object (either
passed as an argument or loaded from `),t_e=a("code"),Bht=o("pretrained_model_name_or_path"),xht=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r_e=a("code"),kht=o("pretrained_model_name_or_path"),Rht=o(":"),Sht=l(),K=a("ul"),Y4=a("li"),a_e=a("strong"),Pht=o("albert"),$ht=o(" \u2014 "),RG=a("a"),Iht=o("TFAlbertForTokenClassification"),jht=o(" (ALBERT model)"),Dht=l(),K4=a("li"),s_e=a("strong"),Nht=o("bert"),qht=o(" \u2014 "),SG=a("a"),Oht=o("TFBertForTokenClassification"),Ght=o(" (BERT model)"),Xht=l(),Z4=a("li"),n_e=a("strong"),Vht=o("camembert"),zht=o(" \u2014 "),PG=a("a"),Wht=o("TFCamembertForTokenClassification"),Qht=o(" (CamemBERT model)"),Hht=l(),eM=a("li"),l_e=a("strong"),Uht=o("convbert"),Jht=o(" \u2014 "),$G=a("a"),Yht=o("TFConvBertForTokenClassification"),Kht=o(" (ConvBERT model)"),Zht=l(),oM=a("li"),i_e=a("strong"),eut=o("deberta"),out=o(" \u2014 "),IG=a("a"),tut=o("TFDebertaForTokenClassification"),rut=o(" (DeBERTa model)"),aut=l(),tM=a("li"),d_e=a("strong"),sut=o("deberta-v2"),nut=o(" \u2014 "),jG=a("a"),lut=o("TFDebertaV2ForTokenClassification"),iut=o(" (DeBERTa-v2 model)"),dut=l(),rM=a("li"),c_e=a("strong"),cut=o("distilbert"),mut=o(" \u2014 "),DG=a("a"),fut=o("TFDistilBertForTokenClassification"),gut=o(" (DistilBERT model)"),hut=l(),aM=a("li"),m_e=a("strong"),uut=o("electra"),put=o(" \u2014 "),NG=a("a"),_ut=o("TFElectraForTokenClassification"),but=o(" (ELECTRA model)"),vut=l(),sM=a("li"),f_e=a("strong"),Tut=o("flaubert"),Fut=o(" \u2014 "),qG=a("a"),Cut=o("TFFlaubertForTokenClassification"),Mut=o(" (FlauBERT model)"),Eut=l(),nM=a("li"),g_e=a("strong"),yut=o("funnel"),wut=o(" \u2014 "),OG=a("a"),Aut=o("TFFunnelForTokenClassification"),Lut=o(" (Funnel Transformer model)"),But=l(),lM=a("li"),h_e=a("strong"),xut=o("layoutlm"),kut=o(" \u2014 "),GG=a("a"),Rut=o("TFLayoutLMForTokenClassification"),Sut=o(" (LayoutLM model)"),Put=l(),iM=a("li"),u_e=a("strong"),$ut=o("longformer"),Iut=o(" \u2014 "),XG=a("a"),jut=o("TFLongformerForTokenClassification"),Dut=o(" (Longformer model)"),Nut=l(),dM=a("li"),p_e=a("strong"),qut=o("mobilebert"),Out=o(" \u2014 "),VG=a("a"),Gut=o("TFMobileBertForTokenClassification"),Xut=o(" (MobileBERT model)"),Vut=l(),cM=a("li"),__e=a("strong"),zut=o("mpnet"),Wut=o(" \u2014 "),zG=a("a"),Qut=o("TFMPNetForTokenClassification"),Hut=o(" (MPNet model)"),Uut=l(),mM=a("li"),b_e=a("strong"),Jut=o("rembert"),Yut=o(" \u2014 "),WG=a("a"),Kut=o("TFRemBertForTokenClassification"),Zut=o(" (RemBERT model)"),ept=l(),fM=a("li"),v_e=a("strong"),opt=o("roberta"),tpt=o(" \u2014 "),QG=a("a"),rpt=o("TFRobertaForTokenClassification"),apt=o(" (RoBERTa model)"),spt=l(),gM=a("li"),T_e=a("strong"),npt=o("roformer"),lpt=o(" \u2014 "),HG=a("a"),ipt=o("TFRoFormerForTokenClassification"),dpt=o(" (RoFormer model)"),cpt=l(),hM=a("li"),F_e=a("strong"),mpt=o("xlm"),fpt=o(" \u2014 "),UG=a("a"),gpt=o("TFXLMForTokenClassification"),hpt=o(" (XLM model)"),upt=l(),uM=a("li"),C_e=a("strong"),ppt=o("xlm-roberta"),_pt=o(" \u2014 "),JG=a("a"),bpt=o("TFXLMRobertaForTokenClassification"),vpt=o(" (XLM-RoBERTa model)"),Tpt=l(),pM=a("li"),M_e=a("strong"),Fpt=o("xlnet"),Cpt=o(" \u2014 "),YG=a("a"),Mpt=o("TFXLNetForTokenClassification"),Ept=o(" (XLNet model)"),ypt=l(),E_e=a("p"),wpt=o("Examples:"),Apt=l(),m($0.$$.fragment),Uxe=l(),Gc=a("h2"),_M=a("a"),y_e=a("span"),m(I0.$$.fragment),Lpt=l(),w_e=a("span"),Bpt=o("TFAutoModelForQuestionAnswering"),Jxe=l(),At=a("div"),m(j0.$$.fragment),xpt=l(),Xc=a("p"),kpt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A_e=a("code"),Rpt=o("from_pretrained()"),Spt=o("class method or the "),L_e=a("code"),Ppt=o("from_config()"),$pt=o(`class
method.`),Ipt=l(),D0=a("p"),jpt=o("This class cannot be instantiated directly using "),B_e=a("code"),Dpt=o("__init__()"),Npt=o(" (throws an error)."),qpt=l(),Fr=a("div"),m(N0.$$.fragment),Opt=l(),x_e=a("p"),Gpt=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Xpt=l(),Vc=a("p"),Vpt=o(`Note:
Loading a model from its configuration file does `),k_e=a("strong"),zpt=o("not"),Wpt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),R_e=a("code"),Qpt=o("from_pretrained()"),Hpt=o("to load the model weights."),Upt=l(),S_e=a("p"),Jpt=o("Examples:"),Ypt=l(),m(q0.$$.fragment),Kpt=l(),Eo=a("div"),m(O0.$$.fragment),Zpt=l(),P_e=a("p"),e_t=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),o_t=l(),Es=a("p"),t_t=o("The model class to instantiate is selected based on the "),$_e=a("code"),r_t=o("model_type"),a_t=o(` property of the config object (either
passed as an argument or loaded from `),I_e=a("code"),s_t=o("pretrained_model_name_or_path"),n_t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j_e=a("code"),l_t=o("pretrained_model_name_or_path"),i_t=o(":"),d_t=l(),Z=a("ul"),bM=a("li"),D_e=a("strong"),c_t=o("albert"),m_t=o(" \u2014 "),KG=a("a"),f_t=o("TFAlbertForQuestionAnswering"),g_t=o(" (ALBERT model)"),h_t=l(),vM=a("li"),N_e=a("strong"),u_t=o("bert"),p_t=o(" \u2014 "),ZG=a("a"),__t=o("TFBertForQuestionAnswering"),b_t=o(" (BERT model)"),v_t=l(),TM=a("li"),q_e=a("strong"),T_t=o("camembert"),F_t=o(" \u2014 "),eX=a("a"),C_t=o("TFCamembertForQuestionAnswering"),M_t=o(" (CamemBERT model)"),E_t=l(),FM=a("li"),O_e=a("strong"),y_t=o("convbert"),w_t=o(" \u2014 "),oX=a("a"),A_t=o("TFConvBertForQuestionAnswering"),L_t=o(" (ConvBERT model)"),B_t=l(),CM=a("li"),G_e=a("strong"),x_t=o("deberta"),k_t=o(" \u2014 "),tX=a("a"),R_t=o("TFDebertaForQuestionAnswering"),S_t=o(" (DeBERTa model)"),P_t=l(),MM=a("li"),X_e=a("strong"),$_t=o("deberta-v2"),I_t=o(" \u2014 "),rX=a("a"),j_t=o("TFDebertaV2ForQuestionAnswering"),D_t=o(" (DeBERTa-v2 model)"),N_t=l(),EM=a("li"),V_e=a("strong"),q_t=o("distilbert"),O_t=o(" \u2014 "),aX=a("a"),G_t=o("TFDistilBertForQuestionAnswering"),X_t=o(" (DistilBERT model)"),V_t=l(),yM=a("li"),z_e=a("strong"),z_t=o("electra"),W_t=o(" \u2014 "),sX=a("a"),Q_t=o("TFElectraForQuestionAnswering"),H_t=o(" (ELECTRA model)"),U_t=l(),wM=a("li"),W_e=a("strong"),J_t=o("flaubert"),Y_t=o(" \u2014 "),nX=a("a"),K_t=o("TFFlaubertForQuestionAnsweringSimple"),Z_t=o(" (FlauBERT model)"),ebt=l(),AM=a("li"),Q_e=a("strong"),obt=o("funnel"),tbt=o(" \u2014 "),lX=a("a"),rbt=o("TFFunnelForQuestionAnswering"),abt=o(" (Funnel Transformer model)"),sbt=l(),LM=a("li"),H_e=a("strong"),nbt=o("longformer"),lbt=o(" \u2014 "),iX=a("a"),ibt=o("TFLongformerForQuestionAnswering"),dbt=o(" (Longformer model)"),cbt=l(),BM=a("li"),U_e=a("strong"),mbt=o("mobilebert"),fbt=o(" \u2014 "),dX=a("a"),gbt=o("TFMobileBertForQuestionAnswering"),hbt=o(" (MobileBERT model)"),ubt=l(),xM=a("li"),J_e=a("strong"),pbt=o("mpnet"),_bt=o(" \u2014 "),cX=a("a"),bbt=o("TFMPNetForQuestionAnswering"),vbt=o(" (MPNet model)"),Tbt=l(),kM=a("li"),Y_e=a("strong"),Fbt=o("rembert"),Cbt=o(" \u2014 "),mX=a("a"),Mbt=o("TFRemBertForQuestionAnswering"),Ebt=o(" (RemBERT model)"),ybt=l(),RM=a("li"),K_e=a("strong"),wbt=o("roberta"),Abt=o(" \u2014 "),fX=a("a"),Lbt=o("TFRobertaForQuestionAnswering"),Bbt=o(" (RoBERTa model)"),xbt=l(),SM=a("li"),Z_e=a("strong"),kbt=o("roformer"),Rbt=o(" \u2014 "),gX=a("a"),Sbt=o("TFRoFormerForQuestionAnswering"),Pbt=o(" (RoFormer model)"),$bt=l(),PM=a("li"),ebe=a("strong"),Ibt=o("xlm"),jbt=o(" \u2014 "),hX=a("a"),Dbt=o("TFXLMForQuestionAnsweringSimple"),Nbt=o(" (XLM model)"),qbt=l(),$M=a("li"),obe=a("strong"),Obt=o("xlm-roberta"),Gbt=o(" \u2014 "),uX=a("a"),Xbt=o("TFXLMRobertaForQuestionAnswering"),Vbt=o(" (XLM-RoBERTa model)"),zbt=l(),IM=a("li"),tbe=a("strong"),Wbt=o("xlnet"),Qbt=o(" \u2014 "),pX=a("a"),Hbt=o("TFXLNetForQuestionAnsweringSimple"),Ubt=o(" (XLNet model)"),Jbt=l(),rbe=a("p"),Ybt=o("Examples:"),Kbt=l(),m(G0.$$.fragment),Yxe=l(),zc=a("h2"),jM=a("a"),abe=a("span"),m(X0.$$.fragment),Zbt=l(),sbe=a("span"),e2t=o("TFAutoModelForVision2Seq"),Kxe=l(),Lt=a("div"),m(V0.$$.fragment),o2t=l(),Wc=a("p"),t2t=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),nbe=a("code"),r2t=o("from_pretrained()"),a2t=o("class method or the "),lbe=a("code"),s2t=o("from_config()"),n2t=o(`class
method.`),l2t=l(),z0=a("p"),i2t=o("This class cannot be instantiated directly using "),ibe=a("code"),d2t=o("__init__()"),c2t=o(" (throws an error)."),m2t=l(),Cr=a("div"),m(W0.$$.fragment),f2t=l(),dbe=a("p"),g2t=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),h2t=l(),Qc=a("p"),u2t=o(`Note:
Loading a model from its configuration file does `),cbe=a("strong"),p2t=o("not"),_2t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mbe=a("code"),b2t=o("from_pretrained()"),v2t=o("to load the model weights."),T2t=l(),fbe=a("p"),F2t=o("Examples:"),C2t=l(),m(Q0.$$.fragment),M2t=l(),yo=a("div"),m(H0.$$.fragment),E2t=l(),gbe=a("p"),y2t=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),w2t=l(),ys=a("p"),A2t=o("The model class to instantiate is selected based on the "),hbe=a("code"),L2t=o("model_type"),B2t=o(` property of the config object (either
passed as an argument or loaded from `),ube=a("code"),x2t=o("pretrained_model_name_or_path"),k2t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pbe=a("code"),R2t=o("pretrained_model_name_or_path"),S2t=o(":"),P2t=l(),_be=a("ul"),DM=a("li"),bbe=a("strong"),$2t=o("vision-encoder-decoder"),I2t=o(" \u2014 "),_X=a("a"),j2t=o("TFVisionEncoderDecoderModel"),D2t=o(" (Vision Encoder decoder model)"),N2t=l(),vbe=a("p"),q2t=o("Examples:"),O2t=l(),m(U0.$$.fragment),Zxe=l(),Hc=a("h2"),NM=a("a"),Tbe=a("span"),m(J0.$$.fragment),G2t=l(),Fbe=a("span"),X2t=o("TFAutoModelForSpeechSeq2Seq"),eke=l(),Bt=a("div"),m(Y0.$$.fragment),V2t=l(),Uc=a("p"),z2t=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Cbe=a("code"),W2t=o("from_pretrained()"),Q2t=o("class method or the "),Mbe=a("code"),H2t=o("from_config()"),U2t=o(`class
method.`),J2t=l(),K0=a("p"),Y2t=o("This class cannot be instantiated directly using "),Ebe=a("code"),K2t=o("__init__()"),Z2t=o(" (throws an error)."),evt=l(),Mr=a("div"),m(Z0.$$.fragment),ovt=l(),ybe=a("p"),tvt=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),rvt=l(),Jc=a("p"),avt=o(`Note:
Loading a model from its configuration file does `),wbe=a("strong"),svt=o("not"),nvt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Abe=a("code"),lvt=o("from_pretrained()"),ivt=o("to load the model weights."),dvt=l(),Lbe=a("p"),cvt=o("Examples:"),mvt=l(),m(eL.$$.fragment),fvt=l(),wo=a("div"),m(oL.$$.fragment),gvt=l(),Bbe=a("p"),hvt=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),uvt=l(),ws=a("p"),pvt=o("The model class to instantiate is selected based on the "),xbe=a("code"),_vt=o("model_type"),bvt=o(` property of the config object (either
passed as an argument or loaded from `),kbe=a("code"),vvt=o("pretrained_model_name_or_path"),Tvt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rbe=a("code"),Fvt=o("pretrained_model_name_or_path"),Cvt=o(":"),Mvt=l(),Sbe=a("ul"),qM=a("li"),Pbe=a("strong"),Evt=o("speech_to_text"),yvt=o(" \u2014 "),bX=a("a"),wvt=o("TFSpeech2TextForConditionalGeneration"),Avt=o(" (Speech2Text model)"),Lvt=l(),$be=a("p"),Bvt=o("Examples:"),xvt=l(),m(tL.$$.fragment),oke=l(),Yc=a("h2"),OM=a("a"),Ibe=a("span"),m(rL.$$.fragment),kvt=l(),jbe=a("span"),Rvt=o("FlaxAutoModel"),tke=l(),xt=a("div"),m(aL.$$.fragment),Svt=l(),Kc=a("p"),Pvt=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Dbe=a("code"),$vt=o("from_pretrained()"),Ivt=o("class method or the "),Nbe=a("code"),jvt=o("from_config()"),Dvt=o(`class
method.`),Nvt=l(),sL=a("p"),qvt=o("This class cannot be instantiated directly using "),qbe=a("code"),Ovt=o("__init__()"),Gvt=o(" (throws an error)."),Xvt=l(),Er=a("div"),m(nL.$$.fragment),Vvt=l(),Obe=a("p"),zvt=o("Instantiates one of the base model classes of the library from a configuration."),Wvt=l(),Zc=a("p"),Qvt=o(`Note:
Loading a model from its configuration file does `),Gbe=a("strong"),Hvt=o("not"),Uvt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xbe=a("code"),Jvt=o("from_pretrained()"),Yvt=o("to load the model weights."),Kvt=l(),Vbe=a("p"),Zvt=o("Examples:"),eTt=l(),m(lL.$$.fragment),oTt=l(),Ao=a("div"),m(iL.$$.fragment),tTt=l(),zbe=a("p"),rTt=o("Instantiate one of the base model classes of the library from a pretrained model."),aTt=l(),As=a("p"),sTt=o("The model class to instantiate is selected based on the "),Wbe=a("code"),nTt=o("model_type"),lTt=o(` property of the config object (either
passed as an argument or loaded from `),Qbe=a("code"),iTt=o("pretrained_model_name_or_path"),dTt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hbe=a("code"),cTt=o("pretrained_model_name_or_path"),mTt=o(":"),fTt=l(),z=a("ul"),GM=a("li"),Ube=a("strong"),gTt=o("albert"),hTt=o(" \u2014 "),vX=a("a"),uTt=o("FlaxAlbertModel"),pTt=o(" (ALBERT model)"),_Tt=l(),XM=a("li"),Jbe=a("strong"),bTt=o("bart"),vTt=o(" \u2014 "),TX=a("a"),TTt=o("FlaxBartModel"),FTt=o(" (BART model)"),CTt=l(),VM=a("li"),Ybe=a("strong"),MTt=o("beit"),ETt=o(" \u2014 "),FX=a("a"),yTt=o("FlaxBeitModel"),wTt=o(" (BEiT model)"),ATt=l(),zM=a("li"),Kbe=a("strong"),LTt=o("bert"),BTt=o(" \u2014 "),CX=a("a"),xTt=o("FlaxBertModel"),kTt=o(" (BERT model)"),RTt=l(),WM=a("li"),Zbe=a("strong"),STt=o("big_bird"),PTt=o(" \u2014 "),MX=a("a"),$Tt=o("FlaxBigBirdModel"),ITt=o(" (BigBird model)"),jTt=l(),QM=a("li"),e2e=a("strong"),DTt=o("blenderbot"),NTt=o(" \u2014 "),EX=a("a"),qTt=o("FlaxBlenderbotModel"),OTt=o(" (Blenderbot model)"),GTt=l(),HM=a("li"),o2e=a("strong"),XTt=o("blenderbot-small"),VTt=o(" \u2014 "),yX=a("a"),zTt=o("FlaxBlenderbotSmallModel"),WTt=o(" (BlenderbotSmall model)"),QTt=l(),UM=a("li"),t2e=a("strong"),HTt=o("clip"),UTt=o(" \u2014 "),wX=a("a"),JTt=o("FlaxCLIPModel"),YTt=o(" (CLIP model)"),KTt=l(),JM=a("li"),r2e=a("strong"),ZTt=o("distilbert"),e1t=o(" \u2014 "),AX=a("a"),o1t=o("FlaxDistilBertModel"),t1t=o(" (DistilBERT model)"),r1t=l(),YM=a("li"),a2e=a("strong"),a1t=o("electra"),s1t=o(" \u2014 "),LX=a("a"),n1t=o("FlaxElectraModel"),l1t=o(" (ELECTRA model)"),i1t=l(),KM=a("li"),s2e=a("strong"),d1t=o("gpt2"),c1t=o(" \u2014 "),BX=a("a"),m1t=o("FlaxGPT2Model"),f1t=o(" (OpenAI GPT-2 model)"),g1t=l(),ZM=a("li"),n2e=a("strong"),h1t=o("gpt_neo"),u1t=o(" \u2014 "),xX=a("a"),p1t=o("FlaxGPTNeoModel"),_1t=o(" (GPT Neo model)"),b1t=l(),eE=a("li"),l2e=a("strong"),v1t=o("gptj"),T1t=o(" \u2014 "),kX=a("a"),F1t=o("FlaxGPTJModel"),C1t=o(" (GPT-J model)"),M1t=l(),oE=a("li"),i2e=a("strong"),E1t=o("marian"),y1t=o(" \u2014 "),RX=a("a"),w1t=o("FlaxMarianModel"),A1t=o(" (Marian model)"),L1t=l(),tE=a("li"),d2e=a("strong"),B1t=o("mbart"),x1t=o(" \u2014 "),SX=a("a"),k1t=o("FlaxMBartModel"),R1t=o(" (mBART model)"),S1t=l(),rE=a("li"),c2e=a("strong"),P1t=o("mt5"),$1t=o(" \u2014 "),PX=a("a"),I1t=o("FlaxMT5Model"),j1t=o(" (mT5 model)"),D1t=l(),aE=a("li"),m2e=a("strong"),N1t=o("pegasus"),q1t=o(" \u2014 "),$X=a("a"),O1t=o("FlaxPegasusModel"),G1t=o(" (Pegasus model)"),X1t=l(),sE=a("li"),f2e=a("strong"),V1t=o("roberta"),z1t=o(" \u2014 "),IX=a("a"),W1t=o("FlaxRobertaModel"),Q1t=o(" (RoBERTa model)"),H1t=l(),nE=a("li"),g2e=a("strong"),U1t=o("roformer"),J1t=o(" \u2014 "),jX=a("a"),Y1t=o("FlaxRoFormerModel"),K1t=o(" (RoFormer model)"),Z1t=l(),lE=a("li"),h2e=a("strong"),eFt=o("t5"),oFt=o(" \u2014 "),DX=a("a"),tFt=o("FlaxT5Model"),rFt=o(" (T5 model)"),aFt=l(),iE=a("li"),u2e=a("strong"),sFt=o("vision-text-dual-encoder"),nFt=o(" \u2014 "),NX=a("a"),lFt=o("FlaxVisionTextDualEncoderModel"),iFt=o(" (VisionTextDualEncoder model)"),dFt=l(),dE=a("li"),p2e=a("strong"),cFt=o("vit"),mFt=o(" \u2014 "),qX=a("a"),fFt=o("FlaxViTModel"),gFt=o(" (ViT model)"),hFt=l(),cE=a("li"),_2e=a("strong"),uFt=o("wav2vec2"),pFt=o(" \u2014 "),OX=a("a"),_Ft=o("FlaxWav2Vec2Model"),bFt=o(" (Wav2Vec2 model)"),vFt=l(),mE=a("li"),b2e=a("strong"),TFt=o("xglm"),FFt=o(" \u2014 "),GX=a("a"),CFt=o("FlaxXGLMModel"),MFt=o(" (XGLM model)"),EFt=l(),v2e=a("p"),yFt=o("Examples:"),wFt=l(),m(dL.$$.fragment),rke=l(),em=a("h2"),fE=a("a"),T2e=a("span"),m(cL.$$.fragment),AFt=l(),F2e=a("span"),LFt=o("FlaxAutoModelForCausalLM"),ake=l(),kt=a("div"),m(mL.$$.fragment),BFt=l(),om=a("p"),xFt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),C2e=a("code"),kFt=o("from_pretrained()"),RFt=o("class method or the "),M2e=a("code"),SFt=o("from_config()"),PFt=o(`class
method.`),$Ft=l(),fL=a("p"),IFt=o("This class cannot be instantiated directly using "),E2e=a("code"),jFt=o("__init__()"),DFt=o(" (throws an error)."),NFt=l(),yr=a("div"),m(gL.$$.fragment),qFt=l(),y2e=a("p"),OFt=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),GFt=l(),tm=a("p"),XFt=o(`Note:
Loading a model from its configuration file does `),w2e=a("strong"),VFt=o("not"),zFt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),A2e=a("code"),WFt=o("from_pretrained()"),QFt=o("to load the model weights."),HFt=l(),L2e=a("p"),UFt=o("Examples:"),JFt=l(),m(hL.$$.fragment),YFt=l(),Lo=a("div"),m(uL.$$.fragment),KFt=l(),B2e=a("p"),ZFt=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),eCt=l(),Ls=a("p"),oCt=o("The model class to instantiate is selected based on the "),x2e=a("code"),tCt=o("model_type"),rCt=o(` property of the config object (either
passed as an argument or loaded from `),k2e=a("code"),aCt=o("pretrained_model_name_or_path"),sCt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),R2e=a("code"),nCt=o("pretrained_model_name_or_path"),lCt=o(":"),iCt=l(),Bs=a("ul"),gE=a("li"),S2e=a("strong"),dCt=o("gpt2"),cCt=o(" \u2014 "),XX=a("a"),mCt=o("FlaxGPT2LMHeadModel"),fCt=o(" (OpenAI GPT-2 model)"),gCt=l(),hE=a("li"),P2e=a("strong"),hCt=o("gpt_neo"),uCt=o(" \u2014 "),VX=a("a"),pCt=o("FlaxGPTNeoForCausalLM"),_Ct=o(" (GPT Neo model)"),bCt=l(),uE=a("li"),$2e=a("strong"),vCt=o("gptj"),TCt=o(" \u2014 "),zX=a("a"),FCt=o("FlaxGPTJForCausalLM"),CCt=o(" (GPT-J model)"),MCt=l(),pE=a("li"),I2e=a("strong"),ECt=o("xglm"),yCt=o(" \u2014 "),WX=a("a"),wCt=o("FlaxXGLMForCausalLM"),ACt=o(" (XGLM model)"),LCt=l(),j2e=a("p"),BCt=o("Examples:"),xCt=l(),m(pL.$$.fragment),ske=l(),rm=a("h2"),_E=a("a"),D2e=a("span"),m(_L.$$.fragment),kCt=l(),N2e=a("span"),RCt=o("FlaxAutoModelForPreTraining"),nke=l(),Rt=a("div"),m(bL.$$.fragment),SCt=l(),am=a("p"),PCt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),q2e=a("code"),$Ct=o("from_pretrained()"),ICt=o("class method or the "),O2e=a("code"),jCt=o("from_config()"),DCt=o(`class
method.`),NCt=l(),vL=a("p"),qCt=o("This class cannot be instantiated directly using "),G2e=a("code"),OCt=o("__init__()"),GCt=o(" (throws an error)."),XCt=l(),wr=a("div"),m(TL.$$.fragment),VCt=l(),X2e=a("p"),zCt=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),WCt=l(),sm=a("p"),QCt=o(`Note:
Loading a model from its configuration file does `),V2e=a("strong"),HCt=o("not"),UCt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z2e=a("code"),JCt=o("from_pretrained()"),YCt=o("to load the model weights."),KCt=l(),W2e=a("p"),ZCt=o("Examples:"),e4t=l(),m(FL.$$.fragment),o4t=l(),Bo=a("div"),m(CL.$$.fragment),t4t=l(),Q2e=a("p"),r4t=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),a4t=l(),xs=a("p"),s4t=o("The model class to instantiate is selected based on the "),H2e=a("code"),n4t=o("model_type"),l4t=o(` property of the config object (either
passed as an argument or loaded from `),U2e=a("code"),i4t=o("pretrained_model_name_or_path"),d4t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),J2e=a("code"),c4t=o("pretrained_model_name_or_path"),m4t=o(":"),f4t=l(),me=a("ul"),bE=a("li"),Y2e=a("strong"),g4t=o("albert"),h4t=o(" \u2014 "),QX=a("a"),u4t=o("FlaxAlbertForPreTraining"),p4t=o(" (ALBERT model)"),_4t=l(),vE=a("li"),K2e=a("strong"),b4t=o("bart"),v4t=o(" \u2014 "),HX=a("a"),T4t=o("FlaxBartForConditionalGeneration"),F4t=o(" (BART model)"),C4t=l(),TE=a("li"),Z2e=a("strong"),M4t=o("bert"),E4t=o(" \u2014 "),UX=a("a"),y4t=o("FlaxBertForPreTraining"),w4t=o(" (BERT model)"),A4t=l(),FE=a("li"),eve=a("strong"),L4t=o("big_bird"),B4t=o(" \u2014 "),JX=a("a"),x4t=o("FlaxBigBirdForPreTraining"),k4t=o(" (BigBird model)"),R4t=l(),CE=a("li"),ove=a("strong"),S4t=o("electra"),P4t=o(" \u2014 "),YX=a("a"),$4t=o("FlaxElectraForPreTraining"),I4t=o(" (ELECTRA model)"),j4t=l(),ME=a("li"),tve=a("strong"),D4t=o("mbart"),N4t=o(" \u2014 "),KX=a("a"),q4t=o("FlaxMBartForConditionalGeneration"),O4t=o(" (mBART model)"),G4t=l(),EE=a("li"),rve=a("strong"),X4t=o("mt5"),V4t=o(" \u2014 "),ZX=a("a"),z4t=o("FlaxMT5ForConditionalGeneration"),W4t=o(" (mT5 model)"),Q4t=l(),yE=a("li"),ave=a("strong"),H4t=o("roberta"),U4t=o(" \u2014 "),eV=a("a"),J4t=o("FlaxRobertaForMaskedLM"),Y4t=o(" (RoBERTa model)"),K4t=l(),wE=a("li"),sve=a("strong"),Z4t=o("roformer"),eMt=o(" \u2014 "),oV=a("a"),oMt=o("FlaxRoFormerForMaskedLM"),tMt=o(" (RoFormer model)"),rMt=l(),AE=a("li"),nve=a("strong"),aMt=o("t5"),sMt=o(" \u2014 "),tV=a("a"),nMt=o("FlaxT5ForConditionalGeneration"),lMt=o(" (T5 model)"),iMt=l(),LE=a("li"),lve=a("strong"),dMt=o("wav2vec2"),cMt=o(" \u2014 "),rV=a("a"),mMt=o("FlaxWav2Vec2ForPreTraining"),fMt=o(" (Wav2Vec2 model)"),gMt=l(),ive=a("p"),hMt=o("Examples:"),uMt=l(),m(ML.$$.fragment),lke=l(),nm=a("h2"),BE=a("a"),dve=a("span"),m(EL.$$.fragment),pMt=l(),cve=a("span"),_Mt=o("FlaxAutoModelForMaskedLM"),ike=l(),St=a("div"),m(yL.$$.fragment),bMt=l(),lm=a("p"),vMt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mve=a("code"),TMt=o("from_pretrained()"),FMt=o("class method or the "),fve=a("code"),CMt=o("from_config()"),MMt=o(`class
method.`),EMt=l(),wL=a("p"),yMt=o("This class cannot be instantiated directly using "),gve=a("code"),wMt=o("__init__()"),AMt=o(" (throws an error)."),LMt=l(),Ar=a("div"),m(AL.$$.fragment),BMt=l(),hve=a("p"),xMt=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),kMt=l(),im=a("p"),RMt=o(`Note:
Loading a model from its configuration file does `),uve=a("strong"),SMt=o("not"),PMt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pve=a("code"),$Mt=o("from_pretrained()"),IMt=o("to load the model weights."),jMt=l(),_ve=a("p"),DMt=o("Examples:"),NMt=l(),m(LL.$$.fragment),qMt=l(),xo=a("div"),m(BL.$$.fragment),OMt=l(),bve=a("p"),GMt=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),XMt=l(),ks=a("p"),VMt=o("The model class to instantiate is selected based on the "),vve=a("code"),zMt=o("model_type"),WMt=o(` property of the config object (either
passed as an argument or loaded from `),Tve=a("code"),QMt=o("pretrained_model_name_or_path"),HMt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fve=a("code"),UMt=o("pretrained_model_name_or_path"),JMt=o(":"),YMt=l(),ve=a("ul"),xE=a("li"),Cve=a("strong"),KMt=o("albert"),ZMt=o(" \u2014 "),aV=a("a"),eEt=o("FlaxAlbertForMaskedLM"),oEt=o(" (ALBERT model)"),tEt=l(),kE=a("li"),Mve=a("strong"),rEt=o("bart"),aEt=o(" \u2014 "),sV=a("a"),sEt=o("FlaxBartForConditionalGeneration"),nEt=o(" (BART model)"),lEt=l(),RE=a("li"),Eve=a("strong"),iEt=o("bert"),dEt=o(" \u2014 "),nV=a("a"),cEt=o("FlaxBertForMaskedLM"),mEt=o(" (BERT model)"),fEt=l(),SE=a("li"),yve=a("strong"),gEt=o("big_bird"),hEt=o(" \u2014 "),lV=a("a"),uEt=o("FlaxBigBirdForMaskedLM"),pEt=o(" (BigBird model)"),_Et=l(),PE=a("li"),wve=a("strong"),bEt=o("distilbert"),vEt=o(" \u2014 "),iV=a("a"),TEt=o("FlaxDistilBertForMaskedLM"),FEt=o(" (DistilBERT model)"),CEt=l(),$E=a("li"),Ave=a("strong"),MEt=o("electra"),EEt=o(" \u2014 "),dV=a("a"),yEt=o("FlaxElectraForMaskedLM"),wEt=o(" (ELECTRA model)"),AEt=l(),IE=a("li"),Lve=a("strong"),LEt=o("mbart"),BEt=o(" \u2014 "),cV=a("a"),xEt=o("FlaxMBartForConditionalGeneration"),kEt=o(" (mBART model)"),REt=l(),jE=a("li"),Bve=a("strong"),SEt=o("roberta"),PEt=o(" \u2014 "),mV=a("a"),$Et=o("FlaxRobertaForMaskedLM"),IEt=o(" (RoBERTa model)"),jEt=l(),DE=a("li"),xve=a("strong"),DEt=o("roformer"),NEt=o(" \u2014 "),fV=a("a"),qEt=o("FlaxRoFormerForMaskedLM"),OEt=o(" (RoFormer model)"),GEt=l(),kve=a("p"),XEt=o("Examples:"),VEt=l(),m(xL.$$.fragment),dke=l(),dm=a("h2"),NE=a("a"),Rve=a("span"),m(kL.$$.fragment),zEt=l(),Sve=a("span"),WEt=o("FlaxAutoModelForSeq2SeqLM"),cke=l(),Pt=a("div"),m(RL.$$.fragment),QEt=l(),cm=a("p"),HEt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pve=a("code"),UEt=o("from_pretrained()"),JEt=o("class method or the "),$ve=a("code"),YEt=o("from_config()"),KEt=o(`class
method.`),ZEt=l(),SL=a("p"),e3t=o("This class cannot be instantiated directly using "),Ive=a("code"),o3t=o("__init__()"),t3t=o(" (throws an error)."),r3t=l(),Lr=a("div"),m(PL.$$.fragment),a3t=l(),jve=a("p"),s3t=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),n3t=l(),mm=a("p"),l3t=o(`Note:
Loading a model from its configuration file does `),Dve=a("strong"),i3t=o("not"),d3t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Nve=a("code"),c3t=o("from_pretrained()"),m3t=o("to load the model weights."),f3t=l(),qve=a("p"),g3t=o("Examples:"),h3t=l(),m($L.$$.fragment),u3t=l(),ko=a("div"),m(IL.$$.fragment),p3t=l(),Ove=a("p"),_3t=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),b3t=l(),Rs=a("p"),v3t=o("The model class to instantiate is selected based on the "),Gve=a("code"),T3t=o("model_type"),F3t=o(` property of the config object (either
passed as an argument or loaded from `),Xve=a("code"),C3t=o("pretrained_model_name_or_path"),M3t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vve=a("code"),E3t=o("pretrained_model_name_or_path"),y3t=o(":"),w3t=l(),Te=a("ul"),qE=a("li"),zve=a("strong"),A3t=o("bart"),L3t=o(" \u2014 "),gV=a("a"),B3t=o("FlaxBartForConditionalGeneration"),x3t=o(" (BART model)"),k3t=l(),OE=a("li"),Wve=a("strong"),R3t=o("blenderbot"),S3t=o(" \u2014 "),hV=a("a"),P3t=o("FlaxBlenderbotForConditionalGeneration"),$3t=o(" (Blenderbot model)"),I3t=l(),GE=a("li"),Qve=a("strong"),j3t=o("blenderbot-small"),D3t=o(" \u2014 "),uV=a("a"),N3t=o("FlaxBlenderbotSmallForConditionalGeneration"),q3t=o(" (BlenderbotSmall model)"),O3t=l(),XE=a("li"),Hve=a("strong"),G3t=o("encoder-decoder"),X3t=o(" \u2014 "),pV=a("a"),V3t=o("FlaxEncoderDecoderModel"),z3t=o(" (Encoder decoder model)"),W3t=l(),VE=a("li"),Uve=a("strong"),Q3t=o("marian"),H3t=o(" \u2014 "),_V=a("a"),U3t=o("FlaxMarianMTModel"),J3t=o(" (Marian model)"),Y3t=l(),zE=a("li"),Jve=a("strong"),K3t=o("mbart"),Z3t=o(" \u2014 "),bV=a("a"),e5t=o("FlaxMBartForConditionalGeneration"),o5t=o(" (mBART model)"),t5t=l(),WE=a("li"),Yve=a("strong"),r5t=o("mt5"),a5t=o(" \u2014 "),vV=a("a"),s5t=o("FlaxMT5ForConditionalGeneration"),n5t=o(" (mT5 model)"),l5t=l(),QE=a("li"),Kve=a("strong"),i5t=o("pegasus"),d5t=o(" \u2014 "),TV=a("a"),c5t=o("FlaxPegasusForConditionalGeneration"),m5t=o(" (Pegasus model)"),f5t=l(),HE=a("li"),Zve=a("strong"),g5t=o("t5"),h5t=o(" \u2014 "),FV=a("a"),u5t=o("FlaxT5ForConditionalGeneration"),p5t=o(" (T5 model)"),_5t=l(),eTe=a("p"),b5t=o("Examples:"),v5t=l(),m(jL.$$.fragment),mke=l(),fm=a("h2"),UE=a("a"),oTe=a("span"),m(DL.$$.fragment),T5t=l(),tTe=a("span"),F5t=o("FlaxAutoModelForSequenceClassification"),fke=l(),$t=a("div"),m(NL.$$.fragment),C5t=l(),gm=a("p"),M5t=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),rTe=a("code"),E5t=o("from_pretrained()"),y5t=o("class method or the "),aTe=a("code"),w5t=o("from_config()"),A5t=o(`class
method.`),L5t=l(),qL=a("p"),B5t=o("This class cannot be instantiated directly using "),sTe=a("code"),x5t=o("__init__()"),k5t=o(" (throws an error)."),R5t=l(),Br=a("div"),m(OL.$$.fragment),S5t=l(),nTe=a("p"),P5t=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),$5t=l(),hm=a("p"),I5t=o(`Note:
Loading a model from its configuration file does `),lTe=a("strong"),j5t=o("not"),D5t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),iTe=a("code"),N5t=o("from_pretrained()"),q5t=o("to load the model weights."),O5t=l(),dTe=a("p"),G5t=o("Examples:"),X5t=l(),m(GL.$$.fragment),V5t=l(),Ro=a("div"),m(XL.$$.fragment),z5t=l(),cTe=a("p"),W5t=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Q5t=l(),Ss=a("p"),H5t=o("The model class to instantiate is selected based on the "),mTe=a("code"),U5t=o("model_type"),J5t=o(` property of the config object (either
passed as an argument or loaded from `),fTe=a("code"),Y5t=o("pretrained_model_name_or_path"),K5t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gTe=a("code"),Z5t=o("pretrained_model_name_or_path"),eyt=o(":"),oyt=l(),Fe=a("ul"),JE=a("li"),hTe=a("strong"),tyt=o("albert"),ryt=o(" \u2014 "),CV=a("a"),ayt=o("FlaxAlbertForSequenceClassification"),syt=o(" (ALBERT model)"),nyt=l(),YE=a("li"),uTe=a("strong"),lyt=o("bart"),iyt=o(" \u2014 "),MV=a("a"),dyt=o("FlaxBartForSequenceClassification"),cyt=o(" (BART model)"),myt=l(),KE=a("li"),pTe=a("strong"),fyt=o("bert"),gyt=o(" \u2014 "),EV=a("a"),hyt=o("FlaxBertForSequenceClassification"),uyt=o(" (BERT model)"),pyt=l(),ZE=a("li"),_Te=a("strong"),_yt=o("big_bird"),byt=o(" \u2014 "),yV=a("a"),vyt=o("FlaxBigBirdForSequenceClassification"),Tyt=o(" (BigBird model)"),Fyt=l(),e3=a("li"),bTe=a("strong"),Cyt=o("distilbert"),Myt=o(" \u2014 "),wV=a("a"),Eyt=o("FlaxDistilBertForSequenceClassification"),yyt=o(" (DistilBERT model)"),wyt=l(),o3=a("li"),vTe=a("strong"),Ayt=o("electra"),Lyt=o(" \u2014 "),AV=a("a"),Byt=o("FlaxElectraForSequenceClassification"),xyt=o(" (ELECTRA model)"),kyt=l(),t3=a("li"),TTe=a("strong"),Ryt=o("mbart"),Syt=o(" \u2014 "),LV=a("a"),Pyt=o("FlaxMBartForSequenceClassification"),$yt=o(" (mBART model)"),Iyt=l(),r3=a("li"),FTe=a("strong"),jyt=o("roberta"),Dyt=o(" \u2014 "),BV=a("a"),Nyt=o("FlaxRobertaForSequenceClassification"),qyt=o(" (RoBERTa model)"),Oyt=l(),a3=a("li"),CTe=a("strong"),Gyt=o("roformer"),Xyt=o(" \u2014 "),xV=a("a"),Vyt=o("FlaxRoFormerForSequenceClassification"),zyt=o(" (RoFormer model)"),Wyt=l(),MTe=a("p"),Qyt=o("Examples:"),Hyt=l(),m(VL.$$.fragment),gke=l(),um=a("h2"),s3=a("a"),ETe=a("span"),m(zL.$$.fragment),Uyt=l(),yTe=a("span"),Jyt=o("FlaxAutoModelForQuestionAnswering"),hke=l(),It=a("div"),m(WL.$$.fragment),Yyt=l(),pm=a("p"),Kyt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),wTe=a("code"),Zyt=o("from_pretrained()"),ewt=o("class method or the "),ATe=a("code"),owt=o("from_config()"),twt=o(`class
method.`),rwt=l(),QL=a("p"),awt=o("This class cannot be instantiated directly using "),LTe=a("code"),swt=o("__init__()"),nwt=o(" (throws an error)."),lwt=l(),xr=a("div"),m(HL.$$.fragment),iwt=l(),BTe=a("p"),dwt=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),cwt=l(),_m=a("p"),mwt=o(`Note:
Loading a model from its configuration file does `),xTe=a("strong"),fwt=o("not"),gwt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kTe=a("code"),hwt=o("from_pretrained()"),uwt=o("to load the model weights."),pwt=l(),RTe=a("p"),_wt=o("Examples:"),bwt=l(),m(UL.$$.fragment),vwt=l(),So=a("div"),m(JL.$$.fragment),Twt=l(),STe=a("p"),Fwt=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Cwt=l(),Ps=a("p"),Mwt=o("The model class to instantiate is selected based on the "),PTe=a("code"),Ewt=o("model_type"),ywt=o(` property of the config object (either
passed as an argument or loaded from `),$Te=a("code"),wwt=o("pretrained_model_name_or_path"),Awt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ITe=a("code"),Lwt=o("pretrained_model_name_or_path"),Bwt=o(":"),xwt=l(),Ce=a("ul"),n3=a("li"),jTe=a("strong"),kwt=o("albert"),Rwt=o(" \u2014 "),kV=a("a"),Swt=o("FlaxAlbertForQuestionAnswering"),Pwt=o(" (ALBERT model)"),$wt=l(),l3=a("li"),DTe=a("strong"),Iwt=o("bart"),jwt=o(" \u2014 "),RV=a("a"),Dwt=o("FlaxBartForQuestionAnswering"),Nwt=o(" (BART model)"),qwt=l(),i3=a("li"),NTe=a("strong"),Owt=o("bert"),Gwt=o(" \u2014 "),SV=a("a"),Xwt=o("FlaxBertForQuestionAnswering"),Vwt=o(" (BERT model)"),zwt=l(),d3=a("li"),qTe=a("strong"),Wwt=o("big_bird"),Qwt=o(" \u2014 "),PV=a("a"),Hwt=o("FlaxBigBirdForQuestionAnswering"),Uwt=o(" (BigBird model)"),Jwt=l(),c3=a("li"),OTe=a("strong"),Ywt=o("distilbert"),Kwt=o(" \u2014 "),$V=a("a"),Zwt=o("FlaxDistilBertForQuestionAnswering"),e6t=o(" (DistilBERT model)"),o6t=l(),m3=a("li"),GTe=a("strong"),t6t=o("electra"),r6t=o(" \u2014 "),IV=a("a"),a6t=o("FlaxElectraForQuestionAnswering"),s6t=o(" (ELECTRA model)"),n6t=l(),f3=a("li"),XTe=a("strong"),l6t=o("mbart"),i6t=o(" \u2014 "),jV=a("a"),d6t=o("FlaxMBartForQuestionAnswering"),c6t=o(" (mBART model)"),m6t=l(),g3=a("li"),VTe=a("strong"),f6t=o("roberta"),g6t=o(" \u2014 "),DV=a("a"),h6t=o("FlaxRobertaForQuestionAnswering"),u6t=o(" (RoBERTa model)"),p6t=l(),h3=a("li"),zTe=a("strong"),_6t=o("roformer"),b6t=o(" \u2014 "),NV=a("a"),v6t=o("FlaxRoFormerForQuestionAnswering"),T6t=o(" (RoFormer model)"),F6t=l(),WTe=a("p"),C6t=o("Examples:"),M6t=l(),m(YL.$$.fragment),uke=l(),bm=a("h2"),u3=a("a"),QTe=a("span"),m(KL.$$.fragment),E6t=l(),HTe=a("span"),y6t=o("FlaxAutoModelForTokenClassification"),pke=l(),jt=a("div"),m(ZL.$$.fragment),w6t=l(),vm=a("p"),A6t=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),UTe=a("code"),L6t=o("from_pretrained()"),B6t=o("class method or the "),JTe=a("code"),x6t=o("from_config()"),k6t=o(`class
method.`),R6t=l(),e8=a("p"),S6t=o("This class cannot be instantiated directly using "),YTe=a("code"),P6t=o("__init__()"),$6t=o(" (throws an error)."),I6t=l(),kr=a("div"),m(o8.$$.fragment),j6t=l(),KTe=a("p"),D6t=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),N6t=l(),Tm=a("p"),q6t=o(`Note:
Loading a model from its configuration file does `),ZTe=a("strong"),O6t=o("not"),G6t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),e1e=a("code"),X6t=o("from_pretrained()"),V6t=o("to load the model weights."),z6t=l(),o1e=a("p"),W6t=o("Examples:"),Q6t=l(),m(t8.$$.fragment),H6t=l(),Po=a("div"),m(r8.$$.fragment),U6t=l(),t1e=a("p"),J6t=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Y6t=l(),$s=a("p"),K6t=o("The model class to instantiate is selected based on the "),r1e=a("code"),Z6t=o("model_type"),eAt=o(` property of the config object (either
passed as an argument or loaded from `),a1e=a("code"),oAt=o("pretrained_model_name_or_path"),tAt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s1e=a("code"),rAt=o("pretrained_model_name_or_path"),aAt=o(":"),sAt=l(),lo=a("ul"),p3=a("li"),n1e=a("strong"),nAt=o("albert"),lAt=o(" \u2014 "),qV=a("a"),iAt=o("FlaxAlbertForTokenClassification"),dAt=o(" (ALBERT model)"),cAt=l(),_3=a("li"),l1e=a("strong"),mAt=o("bert"),fAt=o(" \u2014 "),OV=a("a"),gAt=o("FlaxBertForTokenClassification"),hAt=o(" (BERT model)"),uAt=l(),b3=a("li"),i1e=a("strong"),pAt=o("big_bird"),_At=o(" \u2014 "),GV=a("a"),bAt=o("FlaxBigBirdForTokenClassification"),vAt=o(" (BigBird model)"),TAt=l(),v3=a("li"),d1e=a("strong"),FAt=o("distilbert"),CAt=o(" \u2014 "),XV=a("a"),MAt=o("FlaxDistilBertForTokenClassification"),EAt=o(" (DistilBERT model)"),yAt=l(),T3=a("li"),c1e=a("strong"),wAt=o("electra"),AAt=o(" \u2014 "),VV=a("a"),LAt=o("FlaxElectraForTokenClassification"),BAt=o(" (ELECTRA model)"),xAt=l(),F3=a("li"),m1e=a("strong"),kAt=o("roberta"),RAt=o(" \u2014 "),zV=a("a"),SAt=o("FlaxRobertaForTokenClassification"),PAt=o(" (RoBERTa model)"),$At=l(),C3=a("li"),f1e=a("strong"),IAt=o("roformer"),jAt=o(" \u2014 "),WV=a("a"),DAt=o("FlaxRoFormerForTokenClassification"),NAt=o(" (RoFormer model)"),qAt=l(),g1e=a("p"),OAt=o("Examples:"),GAt=l(),m(a8.$$.fragment),_ke=l(),Fm=a("h2"),M3=a("a"),h1e=a("span"),m(s8.$$.fragment),XAt=l(),u1e=a("span"),VAt=o("FlaxAutoModelForMultipleChoice"),bke=l(),Dt=a("div"),m(n8.$$.fragment),zAt=l(),Cm=a("p"),WAt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),p1e=a("code"),QAt=o("from_pretrained()"),HAt=o("class method or the "),_1e=a("code"),UAt=o("from_config()"),JAt=o(`class
method.`),YAt=l(),l8=a("p"),KAt=o("This class cannot be instantiated directly using "),b1e=a("code"),ZAt=o("__init__()"),e0t=o(" (throws an error)."),o0t=l(),Rr=a("div"),m(i8.$$.fragment),t0t=l(),v1e=a("p"),r0t=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),a0t=l(),Mm=a("p"),s0t=o(`Note:
Loading a model from its configuration file does `),T1e=a("strong"),n0t=o("not"),l0t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),F1e=a("code"),i0t=o("from_pretrained()"),d0t=o("to load the model weights."),c0t=l(),C1e=a("p"),m0t=o("Examples:"),f0t=l(),m(d8.$$.fragment),g0t=l(),$o=a("div"),m(c8.$$.fragment),h0t=l(),M1e=a("p"),u0t=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),p0t=l(),Is=a("p"),_0t=o("The model class to instantiate is selected based on the "),E1e=a("code"),b0t=o("model_type"),v0t=o(` property of the config object (either
passed as an argument or loaded from `),y1e=a("code"),T0t=o("pretrained_model_name_or_path"),F0t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w1e=a("code"),C0t=o("pretrained_model_name_or_path"),M0t=o(":"),E0t=l(),io=a("ul"),E3=a("li"),A1e=a("strong"),y0t=o("albert"),w0t=o(" \u2014 "),QV=a("a"),A0t=o("FlaxAlbertForMultipleChoice"),L0t=o(" (ALBERT model)"),B0t=l(),y3=a("li"),L1e=a("strong"),x0t=o("bert"),k0t=o(" \u2014 "),HV=a("a"),R0t=o("FlaxBertForMultipleChoice"),S0t=o(" (BERT model)"),P0t=l(),w3=a("li"),B1e=a("strong"),$0t=o("big_bird"),I0t=o(" \u2014 "),UV=a("a"),j0t=o("FlaxBigBirdForMultipleChoice"),D0t=o(" (BigBird model)"),N0t=l(),A3=a("li"),x1e=a("strong"),q0t=o("distilbert"),O0t=o(" \u2014 "),JV=a("a"),G0t=o("FlaxDistilBertForMultipleChoice"),X0t=o(" (DistilBERT model)"),V0t=l(),L3=a("li"),k1e=a("strong"),z0t=o("electra"),W0t=o(" \u2014 "),YV=a("a"),Q0t=o("FlaxElectraForMultipleChoice"),H0t=o(" (ELECTRA model)"),U0t=l(),B3=a("li"),R1e=a("strong"),J0t=o("roberta"),Y0t=o(" \u2014 "),KV=a("a"),K0t=o("FlaxRobertaForMultipleChoice"),Z0t=o(" (RoBERTa model)"),eLt=l(),x3=a("li"),S1e=a("strong"),oLt=o("roformer"),tLt=o(" \u2014 "),ZV=a("a"),rLt=o("FlaxRoFormerForMultipleChoice"),aLt=o(" (RoFormer model)"),sLt=l(),P1e=a("p"),nLt=o("Examples:"),lLt=l(),m(m8.$$.fragment),vke=l(),Em=a("h2"),k3=a("a"),$1e=a("span"),m(f8.$$.fragment),iLt=l(),I1e=a("span"),dLt=o("FlaxAutoModelForNextSentencePrediction"),Tke=l(),Nt=a("div"),m(g8.$$.fragment),cLt=l(),ym=a("p"),mLt=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),j1e=a("code"),fLt=o("from_pretrained()"),gLt=o("class method or the "),D1e=a("code"),hLt=o("from_config()"),uLt=o(`class
method.`),pLt=l(),h8=a("p"),_Lt=o("This class cannot be instantiated directly using "),N1e=a("code"),bLt=o("__init__()"),vLt=o(" (throws an error)."),TLt=l(),Sr=a("div"),m(u8.$$.fragment),FLt=l(),q1e=a("p"),CLt=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),MLt=l(),wm=a("p"),ELt=o(`Note:
Loading a model from its configuration file does `),O1e=a("strong"),yLt=o("not"),wLt=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),G1e=a("code"),ALt=o("from_pretrained()"),LLt=o("to load the model weights."),BLt=l(),X1e=a("p"),xLt=o("Examples:"),kLt=l(),m(p8.$$.fragment),RLt=l(),Io=a("div"),m(_8.$$.fragment),SLt=l(),V1e=a("p"),PLt=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),$Lt=l(),js=a("p"),ILt=o("The model class to instantiate is selected based on the "),z1e=a("code"),jLt=o("model_type"),DLt=o(` property of the config object (either
passed as an argument or loaded from `),W1e=a("code"),NLt=o("pretrained_model_name_or_path"),qLt=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Q1e=a("code"),OLt=o("pretrained_model_name_or_path"),GLt=o(":"),XLt=l(),H1e=a("ul"),R3=a("li"),U1e=a("strong"),VLt=o("bert"),zLt=o(" \u2014 "),ez=a("a"),WLt=o("FlaxBertForNextSentencePrediction"),QLt=o(" (BERT model)"),HLt=l(),J1e=a("p"),ULt=o("Examples:"),JLt=l(),m(b8.$$.fragment),Fke=l(),Am=a("h2"),S3=a("a"),Y1e=a("span"),m(v8.$$.fragment),YLt=l(),K1e=a("span"),KLt=o("FlaxAutoModelForImageClassification"),Cke=l(),qt=a("div"),m(T8.$$.fragment),ZLt=l(),Lm=a("p"),e8t=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Z1e=a("code"),o8t=o("from_pretrained()"),t8t=o("class method or the "),eFe=a("code"),r8t=o("from_config()"),a8t=o(`class
method.`),s8t=l(),F8=a("p"),n8t=o("This class cannot be instantiated directly using "),oFe=a("code"),l8t=o("__init__()"),i8t=o(" (throws an error)."),d8t=l(),Pr=a("div"),m(C8.$$.fragment),c8t=l(),tFe=a("p"),m8t=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),f8t=l(),Bm=a("p"),g8t=o(`Note:
Loading a model from its configuration file does `),rFe=a("strong"),h8t=o("not"),u8t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),aFe=a("code"),p8t=o("from_pretrained()"),_8t=o("to load the model weights."),b8t=l(),sFe=a("p"),v8t=o("Examples:"),T8t=l(),m(M8.$$.fragment),F8t=l(),jo=a("div"),m(E8.$$.fragment),C8t=l(),nFe=a("p"),M8t=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),E8t=l(),Ds=a("p"),y8t=o("The model class to instantiate is selected based on the "),lFe=a("code"),w8t=o("model_type"),A8t=o(` property of the config object (either
passed as an argument or loaded from `),iFe=a("code"),L8t=o("pretrained_model_name_or_path"),B8t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dFe=a("code"),x8t=o("pretrained_model_name_or_path"),k8t=o(":"),R8t=l(),y8=a("ul"),P3=a("li"),cFe=a("strong"),S8t=o("beit"),P8t=o(" \u2014 "),oz=a("a"),$8t=o("FlaxBeitForImageClassification"),I8t=o(" (BEiT model)"),j8t=l(),$3=a("li"),mFe=a("strong"),D8t=o("vit"),N8t=o(" \u2014 "),tz=a("a"),q8t=o("FlaxViTForImageClassification"),O8t=o(" (ViT model)"),G8t=l(),fFe=a("p"),X8t=o("Examples:"),V8t=l(),m(w8.$$.fragment),Mke=l(),xm=a("h2"),I3=a("a"),gFe=a("span"),m(A8.$$.fragment),z8t=l(),hFe=a("span"),W8t=o("FlaxAutoModelForVision2Seq"),Eke=l(),Ot=a("div"),m(L8.$$.fragment),Q8t=l(),km=a("p"),H8t=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),uFe=a("code"),U8t=o("from_pretrained()"),J8t=o("class method or the "),pFe=a("code"),Y8t=o("from_config()"),K8t=o(`class
method.`),Z8t=l(),B8=a("p"),e7t=o("This class cannot be instantiated directly using "),_Fe=a("code"),o7t=o("__init__()"),t7t=o(" (throws an error)."),r7t=l(),$r=a("div"),m(x8.$$.fragment),a7t=l(),bFe=a("p"),s7t=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),n7t=l(),Rm=a("p"),l7t=o(`Note:
Loading a model from its configuration file does `),vFe=a("strong"),i7t=o("not"),d7t=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),TFe=a("code"),c7t=o("from_pretrained()"),m7t=o("to load the model weights."),f7t=l(),FFe=a("p"),g7t=o("Examples:"),h7t=l(),m(k8.$$.fragment),u7t=l(),Do=a("div"),m(R8.$$.fragment),p7t=l(),CFe=a("p"),_7t=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),b7t=l(),Ns=a("p"),v7t=o("The model class to instantiate is selected based on the "),MFe=a("code"),T7t=o("model_type"),F7t=o(` property of the config object (either
passed as an argument or loaded from `),EFe=a("code"),C7t=o("pretrained_model_name_or_path"),M7t=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yFe=a("code"),E7t=o("pretrained_model_name_or_path"),y7t=o(":"),w7t=l(),wFe=a("ul"),j3=a("li"),AFe=a("strong"),A7t=o("vision-encoder-decoder"),L7t=o(" \u2014 "),rz=a("a"),B7t=o("FlaxVisionEncoderDecoderModel"),x7t=o(" (Vision Encoder decoder model)"),k7t=l(),LFe=a("p"),R7t=o("Examples:"),S7t=l(),m(S8.$$.fragment),this.h()},l(d){const _=Q1r('[data-svelte="svelte-1phssyn"]',document.head);J=s(_,"META",{name:!0,content:!0}),_.forEach(r),Be=i(d),de=s(d,"H1",{class:!0});var P8=n(de);fe=s(P8,"A",{id:!0,class:!0,href:!0});var BFe=n(fe);no=s(BFe,"SPAN",{});var xFe=n(no);f(ce.$$.fragment,xFe),xFe.forEach(r),BFe.forEach(r),_e=i(P8),Go=s(P8,"SPAN",{});var $7t=n(Go);Ii=t($7t,"Auto Classes"),$7t.forEach(r),P8.forEach(r),Pm=i(d),ca=s(d,"P",{});var wke=n(ca);ji=t(wke,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Di=s(wke,"CODE",{});var I7t=n(Di);D5=t(I7t,"from_pretrained()"),I7t.forEach(r),$m=t(wke,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),wke.forEach(r),ye=i(d),co=s(d,"P",{});var D3=n(co);Ni=t(D3,"Instantiating one of "),qs=s(D3,"A",{href:!0});var j7t=n(qs);N5=t(j7t,"AutoConfig"),j7t.forEach(r),Os=t(D3,", "),Gs=s(D3,"A",{href:!0});var D7t=n(Gs);q5=t(D7t,"AutoModel"),D7t.forEach(r),qi=t(D3,`, and
`),Xs=s(D3,"A",{href:!0});var N7t=n(Xs);O5=t(N7t,"AutoTokenizer"),N7t.forEach(r),Oi=t(D3," will directly create a class of the relevant architecture. For instance"),D3.forEach(r),Im=i(d),f(Na.$$.fragment,d),mo=i(d),ge=s(d,"P",{});var Ake=n(ge);k7=t(Ake,"will create a model that is an instance of "),Gi=s(Ake,"A",{href:!0});var q7t=n(Gi);R7=t(q7t,"BertModel"),q7t.forEach(r),S7=t(Ake,"."),Ake.forEach(r),Xo=i(d),qa=s(d,"P",{});var Lke=n(qa);P7=t(Lke,"There is one class of "),jm=s(Lke,"CODE",{});var O7t=n(jm);$7=t(O7t,"AutoModel"),O7t.forEach(r),DSe=t(Lke," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),Lke.forEach(r),EBe=i(d),Xi=s(d,"H2",{class:!0});var Bke=n(Xi);Dm=s(Bke,"A",{id:!0,class:!0,href:!0});var G7t=n(Dm);JW=s(G7t,"SPAN",{});var X7t=n(JW);f(G5.$$.fragment,X7t),X7t.forEach(r),G7t.forEach(r),NSe=i(Bke),YW=s(Bke,"SPAN",{});var V7t=n(YW);qSe=t(V7t,"Extending the Auto Classes"),V7t.forEach(r),Bke.forEach(r),yBe=i(d),Vs=s(d,"P",{});var az=n(Vs);OSe=t(az,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),KW=s(az,"CODE",{});var z7t=n(KW);GSe=t(z7t,"NewModel"),z7t.forEach(r),XSe=t(az,", make sure you have a "),ZW=s(az,"CODE",{});var W7t=n(ZW);VSe=t(W7t,"NewModelConfig"),W7t.forEach(r),zSe=t(az,` then you can add those to the auto
classes like this:`),az.forEach(r),wBe=i(d),f(X5.$$.fragment,d),ABe=i(d),I7=s(d,"P",{});var Q7t=n(I7);WSe=t(Q7t,"You will then be able to use the auto classes like you would usually do!"),Q7t.forEach(r),LBe=i(d),f(Nm.$$.fragment,d),BBe=i(d),Vi=s(d,"H2",{class:!0});var xke=n(Vi);qm=s(xke,"A",{id:!0,class:!0,href:!0});var H7t=n(qm);eQ=s(H7t,"SPAN",{});var U7t=n(eQ);f(V5.$$.fragment,U7t),U7t.forEach(r),H7t.forEach(r),QSe=i(xke),oQ=s(xke,"SPAN",{});var J7t=n(oQ);HSe=t(J7t,"AutoConfig"),J7t.forEach(r),xke.forEach(r),xBe=i(d),Vo=s(d,"DIV",{class:!0});var Gn=n(Vo);f(z5.$$.fragment,Gn),USe=i(Gn),W5=s(Gn,"P",{});var kke=n(W5);JSe=t(kke,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),j7=s(kke,"A",{href:!0});var Y7t=n(j7);YSe=t(Y7t,"from_pretrained()"),Y7t.forEach(r),KSe=t(kke," class method."),kke.forEach(r),ZSe=i(Gn),Q5=s(Gn,"P",{});var Rke=n(Q5);ePe=t(Rke,"This class cannot be instantiated directly using "),tQ=s(Rke,"CODE",{});var K7t=n(tQ);oPe=t(K7t,"__init__()"),K7t.forEach(r),tPe=t(Rke," (throws an error)."),Rke.forEach(r),rPe=i(Gn),fo=s(Gn,"DIV",{class:!0});var fa=n(fo);f(H5.$$.fragment,fa),aPe=i(fa),rQ=s(fa,"P",{});var Z7t=n(rQ);sPe=t(Z7t,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),Z7t.forEach(r),nPe=i(fa),zi=s(fa,"P",{});var sz=n(zi);lPe=t(sz,"The configuration class to instantiate is selected based on the "),aQ=s(sz,"CODE",{});var e9t=n(aQ);iPe=t(e9t,"model_type"),e9t.forEach(r),dPe=t(sz,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),sQ=s(sz,"CODE",{});var o9t=n(sQ);cPe=t(o9t,"pretrained_model_name_or_path"),o9t.forEach(r),mPe=t(sz,":"),sz.forEach(r),fPe=i(fa),v=s(fa,"UL",{});var T=n(v);Om=s(T,"LI",{});var kFe=n(Om);nQ=s(kFe,"STRONG",{});var t9t=n(nQ);gPe=t(t9t,"albert"),t9t.forEach(r),hPe=t(kFe," \u2014 "),D7=s(kFe,"A",{href:!0});var r9t=n(D7);uPe=t(r9t,"AlbertConfig"),r9t.forEach(r),pPe=t(kFe," (ALBERT model)"),kFe.forEach(r),_Pe=i(T),Gm=s(T,"LI",{});var RFe=n(Gm);lQ=s(RFe,"STRONG",{});var a9t=n(lQ);bPe=t(a9t,"bart"),a9t.forEach(r),vPe=t(RFe," \u2014 "),N7=s(RFe,"A",{href:!0});var s9t=n(N7);TPe=t(s9t,"BartConfig"),s9t.forEach(r),FPe=t(RFe," (BART model)"),RFe.forEach(r),CPe=i(T),Xm=s(T,"LI",{});var SFe=n(Xm);iQ=s(SFe,"STRONG",{});var n9t=n(iQ);MPe=t(n9t,"beit"),n9t.forEach(r),EPe=t(SFe," \u2014 "),q7=s(SFe,"A",{href:!0});var l9t=n(q7);yPe=t(l9t,"BeitConfig"),l9t.forEach(r),wPe=t(SFe," (BEiT model)"),SFe.forEach(r),APe=i(T),Vm=s(T,"LI",{});var PFe=n(Vm);dQ=s(PFe,"STRONG",{});var i9t=n(dQ);LPe=t(i9t,"bert"),i9t.forEach(r),BPe=t(PFe," \u2014 "),O7=s(PFe,"A",{href:!0});var d9t=n(O7);xPe=t(d9t,"BertConfig"),d9t.forEach(r),kPe=t(PFe," (BERT model)"),PFe.forEach(r),RPe=i(T),zm=s(T,"LI",{});var $Fe=n(zm);cQ=s($Fe,"STRONG",{});var c9t=n(cQ);SPe=t(c9t,"bert-generation"),c9t.forEach(r),PPe=t($Fe," \u2014 "),G7=s($Fe,"A",{href:!0});var m9t=n(G7);$Pe=t(m9t,"BertGenerationConfig"),m9t.forEach(r),IPe=t($Fe," (Bert Generation model)"),$Fe.forEach(r),jPe=i(T),Wm=s(T,"LI",{});var IFe=n(Wm);mQ=s(IFe,"STRONG",{});var f9t=n(mQ);DPe=t(f9t,"big_bird"),f9t.forEach(r),NPe=t(IFe," \u2014 "),X7=s(IFe,"A",{href:!0});var g9t=n(X7);qPe=t(g9t,"BigBirdConfig"),g9t.forEach(r),OPe=t(IFe," (BigBird model)"),IFe.forEach(r),GPe=i(T),Qm=s(T,"LI",{});var jFe=n(Qm);fQ=s(jFe,"STRONG",{});var h9t=n(fQ);XPe=t(h9t,"bigbird_pegasus"),h9t.forEach(r),VPe=t(jFe," \u2014 "),V7=s(jFe,"A",{href:!0});var u9t=n(V7);zPe=t(u9t,"BigBirdPegasusConfig"),u9t.forEach(r),WPe=t(jFe," (BigBirdPegasus model)"),jFe.forEach(r),QPe=i(T),Hm=s(T,"LI",{});var DFe=n(Hm);gQ=s(DFe,"STRONG",{});var p9t=n(gQ);HPe=t(p9t,"blenderbot"),p9t.forEach(r),UPe=t(DFe," \u2014 "),z7=s(DFe,"A",{href:!0});var _9t=n(z7);JPe=t(_9t,"BlenderbotConfig"),_9t.forEach(r),YPe=t(DFe," (Blenderbot model)"),DFe.forEach(r),KPe=i(T),Um=s(T,"LI",{});var NFe=n(Um);hQ=s(NFe,"STRONG",{});var b9t=n(hQ);ZPe=t(b9t,"blenderbot-small"),b9t.forEach(r),e$e=t(NFe," \u2014 "),W7=s(NFe,"A",{href:!0});var v9t=n(W7);o$e=t(v9t,"BlenderbotSmallConfig"),v9t.forEach(r),t$e=t(NFe," (BlenderbotSmall model)"),NFe.forEach(r),r$e=i(T),Jm=s(T,"LI",{});var qFe=n(Jm);uQ=s(qFe,"STRONG",{});var T9t=n(uQ);a$e=t(T9t,"camembert"),T9t.forEach(r),s$e=t(qFe," \u2014 "),Q7=s(qFe,"A",{href:!0});var F9t=n(Q7);n$e=t(F9t,"CamembertConfig"),F9t.forEach(r),l$e=t(qFe," (CamemBERT model)"),qFe.forEach(r),i$e=i(T),Ym=s(T,"LI",{});var OFe=n(Ym);pQ=s(OFe,"STRONG",{});var C9t=n(pQ);d$e=t(C9t,"canine"),C9t.forEach(r),c$e=t(OFe," \u2014 "),H7=s(OFe,"A",{href:!0});var M9t=n(H7);m$e=t(M9t,"CanineConfig"),M9t.forEach(r),f$e=t(OFe," (Canine model)"),OFe.forEach(r),g$e=i(T),Km=s(T,"LI",{});var GFe=n(Km);_Q=s(GFe,"STRONG",{});var E9t=n(_Q);h$e=t(E9t,"clip"),E9t.forEach(r),u$e=t(GFe," \u2014 "),U7=s(GFe,"A",{href:!0});var y9t=n(U7);p$e=t(y9t,"CLIPConfig"),y9t.forEach(r),_$e=t(GFe," (CLIP model)"),GFe.forEach(r),b$e=i(T),Zm=s(T,"LI",{});var XFe=n(Zm);bQ=s(XFe,"STRONG",{});var w9t=n(bQ);v$e=t(w9t,"convbert"),w9t.forEach(r),T$e=t(XFe," \u2014 "),J7=s(XFe,"A",{href:!0});var A9t=n(J7);F$e=t(A9t,"ConvBertConfig"),A9t.forEach(r),C$e=t(XFe," (ConvBERT model)"),XFe.forEach(r),M$e=i(T),ef=s(T,"LI",{});var VFe=n(ef);vQ=s(VFe,"STRONG",{});var L9t=n(vQ);E$e=t(L9t,"convnext"),L9t.forEach(r),y$e=t(VFe," \u2014 "),Y7=s(VFe,"A",{href:!0});var B9t=n(Y7);w$e=t(B9t,"ConvNextConfig"),B9t.forEach(r),A$e=t(VFe," (ConvNext model)"),VFe.forEach(r),L$e=i(T),of=s(T,"LI",{});var zFe=n(of);TQ=s(zFe,"STRONG",{});var x9t=n(TQ);B$e=t(x9t,"ctrl"),x9t.forEach(r),x$e=t(zFe," \u2014 "),K7=s(zFe,"A",{href:!0});var k9t=n(K7);k$e=t(k9t,"CTRLConfig"),k9t.forEach(r),R$e=t(zFe," (CTRL model)"),zFe.forEach(r),S$e=i(T),tf=s(T,"LI",{});var WFe=n(tf);FQ=s(WFe,"STRONG",{});var R9t=n(FQ);P$e=t(R9t,"data2vec-audio"),R9t.forEach(r),$$e=t(WFe," \u2014 "),Z7=s(WFe,"A",{href:!0});var S9t=n(Z7);I$e=t(S9t,"Data2VecAudioConfig"),S9t.forEach(r),j$e=t(WFe," (Data2VecAudio model)"),WFe.forEach(r),D$e=i(T),rf=s(T,"LI",{});var QFe=n(rf);CQ=s(QFe,"STRONG",{});var P9t=n(CQ);N$e=t(P9t,"data2vec-text"),P9t.forEach(r),q$e=t(QFe," \u2014 "),e9=s(QFe,"A",{href:!0});var $9t=n(e9);O$e=t($9t,"Data2VecTextConfig"),$9t.forEach(r),G$e=t(QFe," (Data2VecText model)"),QFe.forEach(r),X$e=i(T),af=s(T,"LI",{});var HFe=n(af);MQ=s(HFe,"STRONG",{});var I9t=n(MQ);V$e=t(I9t,"deberta"),I9t.forEach(r),z$e=t(HFe," \u2014 "),o9=s(HFe,"A",{href:!0});var j9t=n(o9);W$e=t(j9t,"DebertaConfig"),j9t.forEach(r),Q$e=t(HFe," (DeBERTa model)"),HFe.forEach(r),H$e=i(T),sf=s(T,"LI",{});var UFe=n(sf);EQ=s(UFe,"STRONG",{});var D9t=n(EQ);U$e=t(D9t,"deberta-v2"),D9t.forEach(r),J$e=t(UFe," \u2014 "),t9=s(UFe,"A",{href:!0});var N9t=n(t9);Y$e=t(N9t,"DebertaV2Config"),N9t.forEach(r),K$e=t(UFe," (DeBERTa-v2 model)"),UFe.forEach(r),Z$e=i(T),nf=s(T,"LI",{});var JFe=n(nf);yQ=s(JFe,"STRONG",{});var q9t=n(yQ);eIe=t(q9t,"deit"),q9t.forEach(r),oIe=t(JFe," \u2014 "),r9=s(JFe,"A",{href:!0});var O9t=n(r9);tIe=t(O9t,"DeiTConfig"),O9t.forEach(r),rIe=t(JFe," (DeiT model)"),JFe.forEach(r),aIe=i(T),lf=s(T,"LI",{});var YFe=n(lf);wQ=s(YFe,"STRONG",{});var G9t=n(wQ);sIe=t(G9t,"detr"),G9t.forEach(r),nIe=t(YFe," \u2014 "),a9=s(YFe,"A",{href:!0});var X9t=n(a9);lIe=t(X9t,"DetrConfig"),X9t.forEach(r),iIe=t(YFe," (DETR model)"),YFe.forEach(r),dIe=i(T),df=s(T,"LI",{});var KFe=n(df);AQ=s(KFe,"STRONG",{});var V9t=n(AQ);cIe=t(V9t,"distilbert"),V9t.forEach(r),mIe=t(KFe," \u2014 "),s9=s(KFe,"A",{href:!0});var z9t=n(s9);fIe=t(z9t,"DistilBertConfig"),z9t.forEach(r),gIe=t(KFe," (DistilBERT model)"),KFe.forEach(r),hIe=i(T),cf=s(T,"LI",{});var ZFe=n(cf);LQ=s(ZFe,"STRONG",{});var W9t=n(LQ);uIe=t(W9t,"dpr"),W9t.forEach(r),pIe=t(ZFe," \u2014 "),n9=s(ZFe,"A",{href:!0});var Q9t=n(n9);_Ie=t(Q9t,"DPRConfig"),Q9t.forEach(r),bIe=t(ZFe," (DPR model)"),ZFe.forEach(r),vIe=i(T),mf=s(T,"LI",{});var eCe=n(mf);BQ=s(eCe,"STRONG",{});var H9t=n(BQ);TIe=t(H9t,"electra"),H9t.forEach(r),FIe=t(eCe," \u2014 "),l9=s(eCe,"A",{href:!0});var U9t=n(l9);CIe=t(U9t,"ElectraConfig"),U9t.forEach(r),MIe=t(eCe," (ELECTRA model)"),eCe.forEach(r),EIe=i(T),ff=s(T,"LI",{});var oCe=n(ff);xQ=s(oCe,"STRONG",{});var J9t=n(xQ);yIe=t(J9t,"encoder-decoder"),J9t.forEach(r),wIe=t(oCe," \u2014 "),i9=s(oCe,"A",{href:!0});var Y9t=n(i9);AIe=t(Y9t,"EncoderDecoderConfig"),Y9t.forEach(r),LIe=t(oCe," (Encoder decoder model)"),oCe.forEach(r),BIe=i(T),gf=s(T,"LI",{});var tCe=n(gf);kQ=s(tCe,"STRONG",{});var K9t=n(kQ);xIe=t(K9t,"flaubert"),K9t.forEach(r),kIe=t(tCe," \u2014 "),d9=s(tCe,"A",{href:!0});var Z9t=n(d9);RIe=t(Z9t,"FlaubertConfig"),Z9t.forEach(r),SIe=t(tCe," (FlauBERT model)"),tCe.forEach(r),PIe=i(T),hf=s(T,"LI",{});var rCe=n(hf);RQ=s(rCe,"STRONG",{});var eBt=n(RQ);$Ie=t(eBt,"fnet"),eBt.forEach(r),IIe=t(rCe," \u2014 "),c9=s(rCe,"A",{href:!0});var oBt=n(c9);jIe=t(oBt,"FNetConfig"),oBt.forEach(r),DIe=t(rCe," (FNet model)"),rCe.forEach(r),NIe=i(T),uf=s(T,"LI",{});var aCe=n(uf);SQ=s(aCe,"STRONG",{});var tBt=n(SQ);qIe=t(tBt,"fsmt"),tBt.forEach(r),OIe=t(aCe," \u2014 "),m9=s(aCe,"A",{href:!0});var rBt=n(m9);GIe=t(rBt,"FSMTConfig"),rBt.forEach(r),XIe=t(aCe," (FairSeq Machine-Translation model)"),aCe.forEach(r),VIe=i(T),pf=s(T,"LI",{});var sCe=n(pf);PQ=s(sCe,"STRONG",{});var aBt=n(PQ);zIe=t(aBt,"funnel"),aBt.forEach(r),WIe=t(sCe," \u2014 "),f9=s(sCe,"A",{href:!0});var sBt=n(f9);QIe=t(sBt,"FunnelConfig"),sBt.forEach(r),HIe=t(sCe," (Funnel Transformer model)"),sCe.forEach(r),UIe=i(T),_f=s(T,"LI",{});var nCe=n(_f);$Q=s(nCe,"STRONG",{});var nBt=n($Q);JIe=t(nBt,"gpt2"),nBt.forEach(r),YIe=t(nCe," \u2014 "),g9=s(nCe,"A",{href:!0});var lBt=n(g9);KIe=t(lBt,"GPT2Config"),lBt.forEach(r),ZIe=t(nCe," (OpenAI GPT-2 model)"),nCe.forEach(r),eje=i(T),bf=s(T,"LI",{});var lCe=n(bf);IQ=s(lCe,"STRONG",{});var iBt=n(IQ);oje=t(iBt,"gpt_neo"),iBt.forEach(r),tje=t(lCe," \u2014 "),h9=s(lCe,"A",{href:!0});var dBt=n(h9);rje=t(dBt,"GPTNeoConfig"),dBt.forEach(r),aje=t(lCe," (GPT Neo model)"),lCe.forEach(r),sje=i(T),vf=s(T,"LI",{});var iCe=n(vf);jQ=s(iCe,"STRONG",{});var cBt=n(jQ);nje=t(cBt,"gptj"),cBt.forEach(r),lje=t(iCe," \u2014 "),u9=s(iCe,"A",{href:!0});var mBt=n(u9);ije=t(mBt,"GPTJConfig"),mBt.forEach(r),dje=t(iCe," (GPT-J model)"),iCe.forEach(r),cje=i(T),Tf=s(T,"LI",{});var dCe=n(Tf);DQ=s(dCe,"STRONG",{});var fBt=n(DQ);mje=t(fBt,"hubert"),fBt.forEach(r),fje=t(dCe," \u2014 "),p9=s(dCe,"A",{href:!0});var gBt=n(p9);gje=t(gBt,"HubertConfig"),gBt.forEach(r),hje=t(dCe," (Hubert model)"),dCe.forEach(r),uje=i(T),Ff=s(T,"LI",{});var cCe=n(Ff);NQ=s(cCe,"STRONG",{});var hBt=n(NQ);pje=t(hBt,"ibert"),hBt.forEach(r),_je=t(cCe," \u2014 "),_9=s(cCe,"A",{href:!0});var uBt=n(_9);bje=t(uBt,"IBertConfig"),uBt.forEach(r),vje=t(cCe," (I-BERT model)"),cCe.forEach(r),Tje=i(T),Cf=s(T,"LI",{});var mCe=n(Cf);qQ=s(mCe,"STRONG",{});var pBt=n(qQ);Fje=t(pBt,"imagegpt"),pBt.forEach(r),Cje=t(mCe," \u2014 "),b9=s(mCe,"A",{href:!0});var _Bt=n(b9);Mje=t(_Bt,"ImageGPTConfig"),_Bt.forEach(r),Eje=t(mCe," (ImageGPT model)"),mCe.forEach(r),yje=i(T),Mf=s(T,"LI",{});var fCe=n(Mf);OQ=s(fCe,"STRONG",{});var bBt=n(OQ);wje=t(bBt,"layoutlm"),bBt.forEach(r),Aje=t(fCe," \u2014 "),v9=s(fCe,"A",{href:!0});var vBt=n(v9);Lje=t(vBt,"LayoutLMConfig"),vBt.forEach(r),Bje=t(fCe," (LayoutLM model)"),fCe.forEach(r),xje=i(T),Ef=s(T,"LI",{});var gCe=n(Ef);GQ=s(gCe,"STRONG",{});var TBt=n(GQ);kje=t(TBt,"layoutlmv2"),TBt.forEach(r),Rje=t(gCe," \u2014 "),T9=s(gCe,"A",{href:!0});var FBt=n(T9);Sje=t(FBt,"LayoutLMv2Config"),FBt.forEach(r),Pje=t(gCe," (LayoutLMv2 model)"),gCe.forEach(r),$je=i(T),yf=s(T,"LI",{});var hCe=n(yf);XQ=s(hCe,"STRONG",{});var CBt=n(XQ);Ije=t(CBt,"led"),CBt.forEach(r),jje=t(hCe," \u2014 "),F9=s(hCe,"A",{href:!0});var MBt=n(F9);Dje=t(MBt,"LEDConfig"),MBt.forEach(r),Nje=t(hCe," (LED model)"),hCe.forEach(r),qje=i(T),wf=s(T,"LI",{});var uCe=n(wf);VQ=s(uCe,"STRONG",{});var EBt=n(VQ);Oje=t(EBt,"longformer"),EBt.forEach(r),Gje=t(uCe," \u2014 "),C9=s(uCe,"A",{href:!0});var yBt=n(C9);Xje=t(yBt,"LongformerConfig"),yBt.forEach(r),Vje=t(uCe," (Longformer model)"),uCe.forEach(r),zje=i(T),Af=s(T,"LI",{});var pCe=n(Af);zQ=s(pCe,"STRONG",{});var wBt=n(zQ);Wje=t(wBt,"luke"),wBt.forEach(r),Qje=t(pCe," \u2014 "),M9=s(pCe,"A",{href:!0});var ABt=n(M9);Hje=t(ABt,"LukeConfig"),ABt.forEach(r),Uje=t(pCe," (LUKE model)"),pCe.forEach(r),Jje=i(T),Lf=s(T,"LI",{});var _Ce=n(Lf);WQ=s(_Ce,"STRONG",{});var LBt=n(WQ);Yje=t(LBt,"lxmert"),LBt.forEach(r),Kje=t(_Ce," \u2014 "),E9=s(_Ce,"A",{href:!0});var BBt=n(E9);Zje=t(BBt,"LxmertConfig"),BBt.forEach(r),eDe=t(_Ce," (LXMERT model)"),_Ce.forEach(r),oDe=i(T),Bf=s(T,"LI",{});var bCe=n(Bf);QQ=s(bCe,"STRONG",{});var xBt=n(QQ);tDe=t(xBt,"m2m_100"),xBt.forEach(r),rDe=t(bCe," \u2014 "),y9=s(bCe,"A",{href:!0});var kBt=n(y9);aDe=t(kBt,"M2M100Config"),kBt.forEach(r),sDe=t(bCe," (M2M100 model)"),bCe.forEach(r),nDe=i(T),xf=s(T,"LI",{});var vCe=n(xf);HQ=s(vCe,"STRONG",{});var RBt=n(HQ);lDe=t(RBt,"marian"),RBt.forEach(r),iDe=t(vCe," \u2014 "),w9=s(vCe,"A",{href:!0});var SBt=n(w9);dDe=t(SBt,"MarianConfig"),SBt.forEach(r),cDe=t(vCe," (Marian model)"),vCe.forEach(r),mDe=i(T),kf=s(T,"LI",{});var TCe=n(kf);UQ=s(TCe,"STRONG",{});var PBt=n(UQ);fDe=t(PBt,"maskformer"),PBt.forEach(r),gDe=t(TCe," \u2014 "),A9=s(TCe,"A",{href:!0});var $Bt=n(A9);hDe=t($Bt,"MaskFormerConfig"),$Bt.forEach(r),uDe=t(TCe," (MaskFormer model)"),TCe.forEach(r),pDe=i(T),Rf=s(T,"LI",{});var FCe=n(Rf);JQ=s(FCe,"STRONG",{});var IBt=n(JQ);_De=t(IBt,"mbart"),IBt.forEach(r),bDe=t(FCe," \u2014 "),L9=s(FCe,"A",{href:!0});var jBt=n(L9);vDe=t(jBt,"MBartConfig"),jBt.forEach(r),TDe=t(FCe," (mBART model)"),FCe.forEach(r),FDe=i(T),Sf=s(T,"LI",{});var CCe=n(Sf);YQ=s(CCe,"STRONG",{});var DBt=n(YQ);CDe=t(DBt,"megatron-bert"),DBt.forEach(r),MDe=t(CCe," \u2014 "),B9=s(CCe,"A",{href:!0});var NBt=n(B9);EDe=t(NBt,"MegatronBertConfig"),NBt.forEach(r),yDe=t(CCe," (MegatronBert model)"),CCe.forEach(r),wDe=i(T),Pf=s(T,"LI",{});var MCe=n(Pf);KQ=s(MCe,"STRONG",{});var qBt=n(KQ);ADe=t(qBt,"mobilebert"),qBt.forEach(r),LDe=t(MCe," \u2014 "),x9=s(MCe,"A",{href:!0});var OBt=n(x9);BDe=t(OBt,"MobileBertConfig"),OBt.forEach(r),xDe=t(MCe," (MobileBERT model)"),MCe.forEach(r),kDe=i(T),$f=s(T,"LI",{});var ECe=n($f);ZQ=s(ECe,"STRONG",{});var GBt=n(ZQ);RDe=t(GBt,"mpnet"),GBt.forEach(r),SDe=t(ECe," \u2014 "),k9=s(ECe,"A",{href:!0});var XBt=n(k9);PDe=t(XBt,"MPNetConfig"),XBt.forEach(r),$De=t(ECe," (MPNet model)"),ECe.forEach(r),IDe=i(T),If=s(T,"LI",{});var yCe=n(If);eH=s(yCe,"STRONG",{});var VBt=n(eH);jDe=t(VBt,"mt5"),VBt.forEach(r),DDe=t(yCe," \u2014 "),R9=s(yCe,"A",{href:!0});var zBt=n(R9);NDe=t(zBt,"MT5Config"),zBt.forEach(r),qDe=t(yCe," (mT5 model)"),yCe.forEach(r),ODe=i(T),jf=s(T,"LI",{});var wCe=n(jf);oH=s(wCe,"STRONG",{});var WBt=n(oH);GDe=t(WBt,"nystromformer"),WBt.forEach(r),XDe=t(wCe," \u2014 "),S9=s(wCe,"A",{href:!0});var QBt=n(S9);VDe=t(QBt,"NystromformerConfig"),QBt.forEach(r),zDe=t(wCe," (Nystromformer model)"),wCe.forEach(r),WDe=i(T),Df=s(T,"LI",{});var ACe=n(Df);tH=s(ACe,"STRONG",{});var HBt=n(tH);QDe=t(HBt,"openai-gpt"),HBt.forEach(r),HDe=t(ACe," \u2014 "),P9=s(ACe,"A",{href:!0});var UBt=n(P9);UDe=t(UBt,"OpenAIGPTConfig"),UBt.forEach(r),JDe=t(ACe," (OpenAI GPT model)"),ACe.forEach(r),YDe=i(T),Nf=s(T,"LI",{});var LCe=n(Nf);rH=s(LCe,"STRONG",{});var JBt=n(rH);KDe=t(JBt,"pegasus"),JBt.forEach(r),ZDe=t(LCe," \u2014 "),$9=s(LCe,"A",{href:!0});var YBt=n($9);eNe=t(YBt,"PegasusConfig"),YBt.forEach(r),oNe=t(LCe," (Pegasus model)"),LCe.forEach(r),tNe=i(T),qf=s(T,"LI",{});var BCe=n(qf);aH=s(BCe,"STRONG",{});var KBt=n(aH);rNe=t(KBt,"perceiver"),KBt.forEach(r),aNe=t(BCe," \u2014 "),I9=s(BCe,"A",{href:!0});var ZBt=n(I9);sNe=t(ZBt,"PerceiverConfig"),ZBt.forEach(r),nNe=t(BCe," (Perceiver model)"),BCe.forEach(r),lNe=i(T),Of=s(T,"LI",{});var xCe=n(Of);sH=s(xCe,"STRONG",{});var ext=n(sH);iNe=t(ext,"plbart"),ext.forEach(r),dNe=t(xCe," \u2014 "),j9=s(xCe,"A",{href:!0});var oxt=n(j9);cNe=t(oxt,"PLBartConfig"),oxt.forEach(r),mNe=t(xCe," (PLBart model)"),xCe.forEach(r),fNe=i(T),Gf=s(T,"LI",{});var kCe=n(Gf);nH=s(kCe,"STRONG",{});var txt=n(nH);gNe=t(txt,"poolformer"),txt.forEach(r),hNe=t(kCe," \u2014 "),D9=s(kCe,"A",{href:!0});var rxt=n(D9);uNe=t(rxt,"PoolFormerConfig"),rxt.forEach(r),pNe=t(kCe," (PoolFormer model)"),kCe.forEach(r),_Ne=i(T),Xf=s(T,"LI",{});var RCe=n(Xf);lH=s(RCe,"STRONG",{});var axt=n(lH);bNe=t(axt,"prophetnet"),axt.forEach(r),vNe=t(RCe," \u2014 "),N9=s(RCe,"A",{href:!0});var sxt=n(N9);TNe=t(sxt,"ProphetNetConfig"),sxt.forEach(r),FNe=t(RCe," (ProphetNet model)"),RCe.forEach(r),CNe=i(T),Vf=s(T,"LI",{});var SCe=n(Vf);iH=s(SCe,"STRONG",{});var nxt=n(iH);MNe=t(nxt,"qdqbert"),nxt.forEach(r),ENe=t(SCe," \u2014 "),q9=s(SCe,"A",{href:!0});var lxt=n(q9);yNe=t(lxt,"QDQBertConfig"),lxt.forEach(r),wNe=t(SCe," (QDQBert model)"),SCe.forEach(r),ANe=i(T),zf=s(T,"LI",{});var PCe=n(zf);dH=s(PCe,"STRONG",{});var ixt=n(dH);LNe=t(ixt,"rag"),ixt.forEach(r),BNe=t(PCe," \u2014 "),O9=s(PCe,"A",{href:!0});var dxt=n(O9);xNe=t(dxt,"RagConfig"),dxt.forEach(r),kNe=t(PCe," (RAG model)"),PCe.forEach(r),RNe=i(T),Wf=s(T,"LI",{});var $Ce=n(Wf);cH=s($Ce,"STRONG",{});var cxt=n(cH);SNe=t(cxt,"realm"),cxt.forEach(r),PNe=t($Ce," \u2014 "),G9=s($Ce,"A",{href:!0});var mxt=n(G9);$Ne=t(mxt,"RealmConfig"),mxt.forEach(r),INe=t($Ce," (Realm model)"),$Ce.forEach(r),jNe=i(T),Qf=s(T,"LI",{});var ICe=n(Qf);mH=s(ICe,"STRONG",{});var fxt=n(mH);DNe=t(fxt,"reformer"),fxt.forEach(r),NNe=t(ICe," \u2014 "),X9=s(ICe,"A",{href:!0});var gxt=n(X9);qNe=t(gxt,"ReformerConfig"),gxt.forEach(r),ONe=t(ICe," (Reformer model)"),ICe.forEach(r),GNe=i(T),Hf=s(T,"LI",{});var jCe=n(Hf);fH=s(jCe,"STRONG",{});var hxt=n(fH);XNe=t(hxt,"rembert"),hxt.forEach(r),VNe=t(jCe," \u2014 "),V9=s(jCe,"A",{href:!0});var uxt=n(V9);zNe=t(uxt,"RemBertConfig"),uxt.forEach(r),WNe=t(jCe," (RemBERT model)"),jCe.forEach(r),QNe=i(T),Uf=s(T,"LI",{});var DCe=n(Uf);gH=s(DCe,"STRONG",{});var pxt=n(gH);HNe=t(pxt,"retribert"),pxt.forEach(r),UNe=t(DCe," \u2014 "),z9=s(DCe,"A",{href:!0});var _xt=n(z9);JNe=t(_xt,"RetriBertConfig"),_xt.forEach(r),YNe=t(DCe," (RetriBERT model)"),DCe.forEach(r),KNe=i(T),Jf=s(T,"LI",{});var NCe=n(Jf);hH=s(NCe,"STRONG",{});var bxt=n(hH);ZNe=t(bxt,"roberta"),bxt.forEach(r),eqe=t(NCe," \u2014 "),W9=s(NCe,"A",{href:!0});var vxt=n(W9);oqe=t(vxt,"RobertaConfig"),vxt.forEach(r),tqe=t(NCe," (RoBERTa model)"),NCe.forEach(r),rqe=i(T),Yf=s(T,"LI",{});var qCe=n(Yf);uH=s(qCe,"STRONG",{});var Txt=n(uH);aqe=t(Txt,"roformer"),Txt.forEach(r),sqe=t(qCe," \u2014 "),Q9=s(qCe,"A",{href:!0});var Fxt=n(Q9);nqe=t(Fxt,"RoFormerConfig"),Fxt.forEach(r),lqe=t(qCe," (RoFormer model)"),qCe.forEach(r),iqe=i(T),Kf=s(T,"LI",{});var OCe=n(Kf);pH=s(OCe,"STRONG",{});var Cxt=n(pH);dqe=t(Cxt,"segformer"),Cxt.forEach(r),cqe=t(OCe," \u2014 "),H9=s(OCe,"A",{href:!0});var Mxt=n(H9);mqe=t(Mxt,"SegformerConfig"),Mxt.forEach(r),fqe=t(OCe," (SegFormer model)"),OCe.forEach(r),gqe=i(T),Zf=s(T,"LI",{});var GCe=n(Zf);_H=s(GCe,"STRONG",{});var Ext=n(_H);hqe=t(Ext,"sew"),Ext.forEach(r),uqe=t(GCe," \u2014 "),U9=s(GCe,"A",{href:!0});var yxt=n(U9);pqe=t(yxt,"SEWConfig"),yxt.forEach(r),_qe=t(GCe," (SEW model)"),GCe.forEach(r),bqe=i(T),eg=s(T,"LI",{});var XCe=n(eg);bH=s(XCe,"STRONG",{});var wxt=n(bH);vqe=t(wxt,"sew-d"),wxt.forEach(r),Tqe=t(XCe," \u2014 "),J9=s(XCe,"A",{href:!0});var Axt=n(J9);Fqe=t(Axt,"SEWDConfig"),Axt.forEach(r),Cqe=t(XCe," (SEW-D model)"),XCe.forEach(r),Mqe=i(T),og=s(T,"LI",{});var VCe=n(og);vH=s(VCe,"STRONG",{});var Lxt=n(vH);Eqe=t(Lxt,"speech-encoder-decoder"),Lxt.forEach(r),yqe=t(VCe," \u2014 "),Y9=s(VCe,"A",{href:!0});var Bxt=n(Y9);wqe=t(Bxt,"SpeechEncoderDecoderConfig"),Bxt.forEach(r),Aqe=t(VCe," (Speech Encoder decoder model)"),VCe.forEach(r),Lqe=i(T),tg=s(T,"LI",{});var zCe=n(tg);TH=s(zCe,"STRONG",{});var xxt=n(TH);Bqe=t(xxt,"speech_to_text"),xxt.forEach(r),xqe=t(zCe," \u2014 "),K9=s(zCe,"A",{href:!0});var kxt=n(K9);kqe=t(kxt,"Speech2TextConfig"),kxt.forEach(r),Rqe=t(zCe," (Speech2Text model)"),zCe.forEach(r),Sqe=i(T),rg=s(T,"LI",{});var WCe=n(rg);FH=s(WCe,"STRONG",{});var Rxt=n(FH);Pqe=t(Rxt,"speech_to_text_2"),Rxt.forEach(r),$qe=t(WCe," \u2014 "),Z9=s(WCe,"A",{href:!0});var Sxt=n(Z9);Iqe=t(Sxt,"Speech2Text2Config"),Sxt.forEach(r),jqe=t(WCe," (Speech2Text2 model)"),WCe.forEach(r),Dqe=i(T),ag=s(T,"LI",{});var QCe=n(ag);CH=s(QCe,"STRONG",{});var Pxt=n(CH);Nqe=t(Pxt,"splinter"),Pxt.forEach(r),qqe=t(QCe," \u2014 "),eB=s(QCe,"A",{href:!0});var $xt=n(eB);Oqe=t($xt,"SplinterConfig"),$xt.forEach(r),Gqe=t(QCe," (Splinter model)"),QCe.forEach(r),Xqe=i(T),sg=s(T,"LI",{});var HCe=n(sg);MH=s(HCe,"STRONG",{});var Ixt=n(MH);Vqe=t(Ixt,"squeezebert"),Ixt.forEach(r),zqe=t(HCe," \u2014 "),oB=s(HCe,"A",{href:!0});var jxt=n(oB);Wqe=t(jxt,"SqueezeBertConfig"),jxt.forEach(r),Qqe=t(HCe," (SqueezeBERT model)"),HCe.forEach(r),Hqe=i(T),ng=s(T,"LI",{});var UCe=n(ng);EH=s(UCe,"STRONG",{});var Dxt=n(EH);Uqe=t(Dxt,"swin"),Dxt.forEach(r),Jqe=t(UCe," \u2014 "),tB=s(UCe,"A",{href:!0});var Nxt=n(tB);Yqe=t(Nxt,"SwinConfig"),Nxt.forEach(r),Kqe=t(UCe," (Swin model)"),UCe.forEach(r),Zqe=i(T),lg=s(T,"LI",{});var JCe=n(lg);yH=s(JCe,"STRONG",{});var qxt=n(yH);eOe=t(qxt,"t5"),qxt.forEach(r),oOe=t(JCe," \u2014 "),rB=s(JCe,"A",{href:!0});var Oxt=n(rB);tOe=t(Oxt,"T5Config"),Oxt.forEach(r),rOe=t(JCe," (T5 model)"),JCe.forEach(r),aOe=i(T),ig=s(T,"LI",{});var YCe=n(ig);wH=s(YCe,"STRONG",{});var Gxt=n(wH);sOe=t(Gxt,"tapas"),Gxt.forEach(r),nOe=t(YCe," \u2014 "),aB=s(YCe,"A",{href:!0});var Xxt=n(aB);lOe=t(Xxt,"TapasConfig"),Xxt.forEach(r),iOe=t(YCe," (TAPAS model)"),YCe.forEach(r),dOe=i(T),dg=s(T,"LI",{});var KCe=n(dg);AH=s(KCe,"STRONG",{});var Vxt=n(AH);cOe=t(Vxt,"transfo-xl"),Vxt.forEach(r),mOe=t(KCe," \u2014 "),sB=s(KCe,"A",{href:!0});var zxt=n(sB);fOe=t(zxt,"TransfoXLConfig"),zxt.forEach(r),gOe=t(KCe," (Transformer-XL model)"),KCe.forEach(r),hOe=i(T),cg=s(T,"LI",{});var ZCe=n(cg);LH=s(ZCe,"STRONG",{});var Wxt=n(LH);uOe=t(Wxt,"trocr"),Wxt.forEach(r),pOe=t(ZCe," \u2014 "),nB=s(ZCe,"A",{href:!0});var Qxt=n(nB);_Oe=t(Qxt,"TrOCRConfig"),Qxt.forEach(r),bOe=t(ZCe," (TrOCR model)"),ZCe.forEach(r),vOe=i(T),mg=s(T,"LI",{});var e4e=n(mg);BH=s(e4e,"STRONG",{});var Hxt=n(BH);TOe=t(Hxt,"unispeech"),Hxt.forEach(r),FOe=t(e4e," \u2014 "),lB=s(e4e,"A",{href:!0});var Uxt=n(lB);COe=t(Uxt,"UniSpeechConfig"),Uxt.forEach(r),MOe=t(e4e," (UniSpeech model)"),e4e.forEach(r),EOe=i(T),fg=s(T,"LI",{});var o4e=n(fg);xH=s(o4e,"STRONG",{});var Jxt=n(xH);yOe=t(Jxt,"unispeech-sat"),Jxt.forEach(r),wOe=t(o4e," \u2014 "),iB=s(o4e,"A",{href:!0});var Yxt=n(iB);AOe=t(Yxt,"UniSpeechSatConfig"),Yxt.forEach(r),LOe=t(o4e," (UniSpeechSat model)"),o4e.forEach(r),BOe=i(T),gg=s(T,"LI",{});var t4e=n(gg);kH=s(t4e,"STRONG",{});var Kxt=n(kH);xOe=t(Kxt,"vilt"),Kxt.forEach(r),kOe=t(t4e," \u2014 "),dB=s(t4e,"A",{href:!0});var Zxt=n(dB);ROe=t(Zxt,"ViltConfig"),Zxt.forEach(r),SOe=t(t4e," (ViLT model)"),t4e.forEach(r),POe=i(T),hg=s(T,"LI",{});var r4e=n(hg);RH=s(r4e,"STRONG",{});var ekt=n(RH);$Oe=t(ekt,"vision-encoder-decoder"),ekt.forEach(r),IOe=t(r4e," \u2014 "),cB=s(r4e,"A",{href:!0});var okt=n(cB);jOe=t(okt,"VisionEncoderDecoderConfig"),okt.forEach(r),DOe=t(r4e," (Vision Encoder decoder model)"),r4e.forEach(r),NOe=i(T),ug=s(T,"LI",{});var a4e=n(ug);SH=s(a4e,"STRONG",{});var tkt=n(SH);qOe=t(tkt,"vision-text-dual-encoder"),tkt.forEach(r),OOe=t(a4e," \u2014 "),mB=s(a4e,"A",{href:!0});var rkt=n(mB);GOe=t(rkt,"VisionTextDualEncoderConfig"),rkt.forEach(r),XOe=t(a4e," (VisionTextDualEncoder model)"),a4e.forEach(r),VOe=i(T),pg=s(T,"LI",{});var s4e=n(pg);PH=s(s4e,"STRONG",{});var akt=n(PH);zOe=t(akt,"visual_bert"),akt.forEach(r),WOe=t(s4e," \u2014 "),fB=s(s4e,"A",{href:!0});var skt=n(fB);QOe=t(skt,"VisualBertConfig"),skt.forEach(r),HOe=t(s4e," (VisualBert model)"),s4e.forEach(r),UOe=i(T),_g=s(T,"LI",{});var n4e=n(_g);$H=s(n4e,"STRONG",{});var nkt=n($H);JOe=t(nkt,"vit"),nkt.forEach(r),YOe=t(n4e," \u2014 "),gB=s(n4e,"A",{href:!0});var lkt=n(gB);KOe=t(lkt,"ViTConfig"),lkt.forEach(r),ZOe=t(n4e," (ViT model)"),n4e.forEach(r),eGe=i(T),bg=s(T,"LI",{});var l4e=n(bg);IH=s(l4e,"STRONG",{});var ikt=n(IH);oGe=t(ikt,"vit_mae"),ikt.forEach(r),tGe=t(l4e," \u2014 "),hB=s(l4e,"A",{href:!0});var dkt=n(hB);rGe=t(dkt,"ViTMAEConfig"),dkt.forEach(r),aGe=t(l4e," (ViTMAE model)"),l4e.forEach(r),sGe=i(T),vg=s(T,"LI",{});var i4e=n(vg);jH=s(i4e,"STRONG",{});var ckt=n(jH);nGe=t(ckt,"wav2vec2"),ckt.forEach(r),lGe=t(i4e," \u2014 "),uB=s(i4e,"A",{href:!0});var mkt=n(uB);iGe=t(mkt,"Wav2Vec2Config"),mkt.forEach(r),dGe=t(i4e," (Wav2Vec2 model)"),i4e.forEach(r),cGe=i(T),Tg=s(T,"LI",{});var d4e=n(Tg);DH=s(d4e,"STRONG",{});var fkt=n(DH);mGe=t(fkt,"wavlm"),fkt.forEach(r),fGe=t(d4e," \u2014 "),pB=s(d4e,"A",{href:!0});var gkt=n(pB);gGe=t(gkt,"WavLMConfig"),gkt.forEach(r),hGe=t(d4e," (WavLM model)"),d4e.forEach(r),uGe=i(T),Fg=s(T,"LI",{});var c4e=n(Fg);NH=s(c4e,"STRONG",{});var hkt=n(NH);pGe=t(hkt,"xglm"),hkt.forEach(r),_Ge=t(c4e," \u2014 "),_B=s(c4e,"A",{href:!0});var ukt=n(_B);bGe=t(ukt,"XGLMConfig"),ukt.forEach(r),vGe=t(c4e," (XGLM model)"),c4e.forEach(r),TGe=i(T),Cg=s(T,"LI",{});var m4e=n(Cg);qH=s(m4e,"STRONG",{});var pkt=n(qH);FGe=t(pkt,"xlm"),pkt.forEach(r),CGe=t(m4e," \u2014 "),bB=s(m4e,"A",{href:!0});var _kt=n(bB);MGe=t(_kt,"XLMConfig"),_kt.forEach(r),EGe=t(m4e," (XLM model)"),m4e.forEach(r),yGe=i(T),Mg=s(T,"LI",{});var f4e=n(Mg);OH=s(f4e,"STRONG",{});var bkt=n(OH);wGe=t(bkt,"xlm-prophetnet"),bkt.forEach(r),AGe=t(f4e," \u2014 "),vB=s(f4e,"A",{href:!0});var vkt=n(vB);LGe=t(vkt,"XLMProphetNetConfig"),vkt.forEach(r),BGe=t(f4e," (XLMProphetNet model)"),f4e.forEach(r),xGe=i(T),Eg=s(T,"LI",{});var g4e=n(Eg);GH=s(g4e,"STRONG",{});var Tkt=n(GH);kGe=t(Tkt,"xlm-roberta"),Tkt.forEach(r),RGe=t(g4e," \u2014 "),TB=s(g4e,"A",{href:!0});var Fkt=n(TB);SGe=t(Fkt,"XLMRobertaConfig"),Fkt.forEach(r),PGe=t(g4e," (XLM-RoBERTa model)"),g4e.forEach(r),$Ge=i(T),yg=s(T,"LI",{});var h4e=n(yg);XH=s(h4e,"STRONG",{});var Ckt=n(XH);IGe=t(Ckt,"xlm-roberta-xl"),Ckt.forEach(r),jGe=t(h4e," \u2014 "),FB=s(h4e,"A",{href:!0});var Mkt=n(FB);DGe=t(Mkt,"XLMRobertaXLConfig"),Mkt.forEach(r),NGe=t(h4e," (XLM-RoBERTa-XL model)"),h4e.forEach(r),qGe=i(T),wg=s(T,"LI",{});var u4e=n(wg);VH=s(u4e,"STRONG",{});var Ekt=n(VH);OGe=t(Ekt,"xlnet"),Ekt.forEach(r),GGe=t(u4e," \u2014 "),CB=s(u4e,"A",{href:!0});var ykt=n(CB);XGe=t(ykt,"XLNetConfig"),ykt.forEach(r),VGe=t(u4e," (XLNet model)"),u4e.forEach(r),zGe=i(T),Ag=s(T,"LI",{});var p4e=n(Ag);zH=s(p4e,"STRONG",{});var wkt=n(zH);WGe=t(wkt,"yoso"),wkt.forEach(r),QGe=t(p4e," \u2014 "),MB=s(p4e,"A",{href:!0});var Akt=n(MB);HGe=t(Akt,"YosoConfig"),Akt.forEach(r),UGe=t(p4e," (YOSO model)"),p4e.forEach(r),T.forEach(r),JGe=i(fa),WH=s(fa,"P",{});var Lkt=n(WH);YGe=t(Lkt,"Examples:"),Lkt.forEach(r),KGe=i(fa),f(U5.$$.fragment,fa),fa.forEach(r),ZGe=i(Gn),Lg=s(Gn,"DIV",{class:!0});var Ske=n(Lg);f(J5.$$.fragment,Ske),eXe=i(Ske),QH=s(Ske,"P",{});var Bkt=n(QH);oXe=t(Bkt,"Register a new configuration for this class."),Bkt.forEach(r),Ske.forEach(r),Gn.forEach(r),kBe=i(d),Wi=s(d,"H2",{class:!0});var Pke=n(Wi);Bg=s(Pke,"A",{id:!0,class:!0,href:!0});var xkt=n(Bg);HH=s(xkt,"SPAN",{});var kkt=n(HH);f(Y5.$$.fragment,kkt),kkt.forEach(r),xkt.forEach(r),tXe=i(Pke),UH=s(Pke,"SPAN",{});var Rkt=n(UH);rXe=t(Rkt,"AutoTokenizer"),Rkt.forEach(r),Pke.forEach(r),RBe=i(d),zo=s(d,"DIV",{class:!0});var Xn=n(zo);f(K5.$$.fragment,Xn),aXe=i(Xn),Z5=s(Xn,"P",{});var $ke=n(Z5);sXe=t($ke,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),EB=s($ke,"A",{href:!0});var Skt=n(EB);nXe=t(Skt,"AutoTokenizer.from_pretrained()"),Skt.forEach(r),lXe=t($ke," class method."),$ke.forEach(r),iXe=i(Xn),ey=s(Xn,"P",{});var Ike=n(ey);dXe=t(Ike,"This class cannot be instantiated directly using "),JH=s(Ike,"CODE",{});var Pkt=n(JH);cXe=t(Pkt,"__init__()"),Pkt.forEach(r),mXe=t(Ike," (throws an error)."),Ike.forEach(r),fXe=i(Xn),go=s(Xn,"DIV",{class:!0});var ga=n(go);f(oy.$$.fragment,ga),gXe=i(ga),YH=s(ga,"P",{});var $kt=n(YH);hXe=t($kt,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),$kt.forEach(r),uXe=i(ga),Oa=s(ga,"P",{});var N3=n(Oa);pXe=t(N3,"The tokenizer class to instantiate is selected based on the "),KH=s(N3,"CODE",{});var Ikt=n(KH);_Xe=t(Ikt,"model_type"),Ikt.forEach(r),bXe=t(N3,` property of the config object (either
passed as an argument or loaded from `),ZH=s(N3,"CODE",{});var jkt=n(ZH);vXe=t(jkt,"pretrained_model_name_or_path"),jkt.forEach(r),TXe=t(N3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eU=s(N3,"CODE",{});var Dkt=n(eU);FXe=t(Dkt,"pretrained_model_name_or_path"),Dkt.forEach(r),CXe=t(N3,":"),N3.forEach(r),MXe=i(ga),E=s(ga,"UL",{});var y=n(E);zs=s(y,"LI",{});var $8=n(zs);oU=s($8,"STRONG",{});var Nkt=n(oU);EXe=t(Nkt,"albert"),Nkt.forEach(r),yXe=t($8," \u2014 "),yB=s($8,"A",{href:!0});var qkt=n(yB);wXe=t(qkt,"AlbertTokenizer"),qkt.forEach(r),AXe=t($8," or "),wB=s($8,"A",{href:!0});var Okt=n(wB);LXe=t(Okt,"AlbertTokenizerFast"),Okt.forEach(r),BXe=t($8," (ALBERT model)"),$8.forEach(r),xXe=i(y),Ws=s(y,"LI",{});var I8=n(Ws);tU=s(I8,"STRONG",{});var Gkt=n(tU);kXe=t(Gkt,"bart"),Gkt.forEach(r),RXe=t(I8," \u2014 "),AB=s(I8,"A",{href:!0});var Xkt=n(AB);SXe=t(Xkt,"BartTokenizer"),Xkt.forEach(r),PXe=t(I8," or "),LB=s(I8,"A",{href:!0});var Vkt=n(LB);$Xe=t(Vkt,"BartTokenizerFast"),Vkt.forEach(r),IXe=t(I8," (BART model)"),I8.forEach(r),jXe=i(y),Qs=s(y,"LI",{});var j8=n(Qs);rU=s(j8,"STRONG",{});var zkt=n(rU);DXe=t(zkt,"barthez"),zkt.forEach(r),NXe=t(j8," \u2014 "),BB=s(j8,"A",{href:!0});var Wkt=n(BB);qXe=t(Wkt,"BarthezTokenizer"),Wkt.forEach(r),OXe=t(j8," or "),xB=s(j8,"A",{href:!0});var Qkt=n(xB);GXe=t(Qkt,"BarthezTokenizerFast"),Qkt.forEach(r),XXe=t(j8," (BARThez model)"),j8.forEach(r),VXe=i(y),xg=s(y,"LI",{});var _4e=n(xg);aU=s(_4e,"STRONG",{});var Hkt=n(aU);zXe=t(Hkt,"bartpho"),Hkt.forEach(r),WXe=t(_4e," \u2014 "),kB=s(_4e,"A",{href:!0});var Ukt=n(kB);QXe=t(Ukt,"BartphoTokenizer"),Ukt.forEach(r),HXe=t(_4e," (BARTpho model)"),_4e.forEach(r),UXe=i(y),Hs=s(y,"LI",{});var D8=n(Hs);sU=s(D8,"STRONG",{});var Jkt=n(sU);JXe=t(Jkt,"bert"),Jkt.forEach(r),YXe=t(D8," \u2014 "),RB=s(D8,"A",{href:!0});var Ykt=n(RB);KXe=t(Ykt,"BertTokenizer"),Ykt.forEach(r),ZXe=t(D8," or "),SB=s(D8,"A",{href:!0});var Kkt=n(SB);eVe=t(Kkt,"BertTokenizerFast"),Kkt.forEach(r),oVe=t(D8," (BERT model)"),D8.forEach(r),tVe=i(y),kg=s(y,"LI",{});var b4e=n(kg);nU=s(b4e,"STRONG",{});var Zkt=n(nU);rVe=t(Zkt,"bert-generation"),Zkt.forEach(r),aVe=t(b4e," \u2014 "),PB=s(b4e,"A",{href:!0});var eRt=n(PB);sVe=t(eRt,"BertGenerationTokenizer"),eRt.forEach(r),nVe=t(b4e," (Bert Generation model)"),b4e.forEach(r),lVe=i(y),Rg=s(y,"LI",{});var v4e=n(Rg);lU=s(v4e,"STRONG",{});var oRt=n(lU);iVe=t(oRt,"bert-japanese"),oRt.forEach(r),dVe=t(v4e," \u2014 "),$B=s(v4e,"A",{href:!0});var tRt=n($B);cVe=t(tRt,"BertJapaneseTokenizer"),tRt.forEach(r),mVe=t(v4e," (BertJapanese model)"),v4e.forEach(r),fVe=i(y),Sg=s(y,"LI",{});var T4e=n(Sg);iU=s(T4e,"STRONG",{});var rRt=n(iU);gVe=t(rRt,"bertweet"),rRt.forEach(r),hVe=t(T4e," \u2014 "),IB=s(T4e,"A",{href:!0});var aRt=n(IB);uVe=t(aRt,"BertweetTokenizer"),aRt.forEach(r),pVe=t(T4e," (Bertweet model)"),T4e.forEach(r),_Ve=i(y),Us=s(y,"LI",{});var N8=n(Us);dU=s(N8,"STRONG",{});var sRt=n(dU);bVe=t(sRt,"big_bird"),sRt.forEach(r),vVe=t(N8," \u2014 "),jB=s(N8,"A",{href:!0});var nRt=n(jB);TVe=t(nRt,"BigBirdTokenizer"),nRt.forEach(r),FVe=t(N8," or "),DB=s(N8,"A",{href:!0});var lRt=n(DB);CVe=t(lRt,"BigBirdTokenizerFast"),lRt.forEach(r),MVe=t(N8," (BigBird model)"),N8.forEach(r),EVe=i(y),Js=s(y,"LI",{});var q8=n(Js);cU=s(q8,"STRONG",{});var iRt=n(cU);yVe=t(iRt,"bigbird_pegasus"),iRt.forEach(r),wVe=t(q8," \u2014 "),NB=s(q8,"A",{href:!0});var dRt=n(NB);AVe=t(dRt,"PegasusTokenizer"),dRt.forEach(r),LVe=t(q8," or "),qB=s(q8,"A",{href:!0});var cRt=n(qB);BVe=t(cRt,"PegasusTokenizerFast"),cRt.forEach(r),xVe=t(q8," (BigBirdPegasus model)"),q8.forEach(r),kVe=i(y),Ys=s(y,"LI",{});var O8=n(Ys);mU=s(O8,"STRONG",{});var mRt=n(mU);RVe=t(mRt,"blenderbot"),mRt.forEach(r),SVe=t(O8," \u2014 "),OB=s(O8,"A",{href:!0});var fRt=n(OB);PVe=t(fRt,"BlenderbotTokenizer"),fRt.forEach(r),$Ve=t(O8," or "),GB=s(O8,"A",{href:!0});var gRt=n(GB);IVe=t(gRt,"BlenderbotTokenizerFast"),gRt.forEach(r),jVe=t(O8," (Blenderbot model)"),O8.forEach(r),DVe=i(y),Pg=s(y,"LI",{});var F4e=n(Pg);fU=s(F4e,"STRONG",{});var hRt=n(fU);NVe=t(hRt,"blenderbot-small"),hRt.forEach(r),qVe=t(F4e," \u2014 "),XB=s(F4e,"A",{href:!0});var uRt=n(XB);OVe=t(uRt,"BlenderbotSmallTokenizer"),uRt.forEach(r),GVe=t(F4e," (BlenderbotSmall model)"),F4e.forEach(r),XVe=i(y),$g=s(y,"LI",{});var C4e=n($g);gU=s(C4e,"STRONG",{});var pRt=n(gU);VVe=t(pRt,"byt5"),pRt.forEach(r),zVe=t(C4e," \u2014 "),VB=s(C4e,"A",{href:!0});var _Rt=n(VB);WVe=t(_Rt,"ByT5Tokenizer"),_Rt.forEach(r),QVe=t(C4e," (ByT5 model)"),C4e.forEach(r),HVe=i(y),Ks=s(y,"LI",{});var G8=n(Ks);hU=s(G8,"STRONG",{});var bRt=n(hU);UVe=t(bRt,"camembert"),bRt.forEach(r),JVe=t(G8," \u2014 "),zB=s(G8,"A",{href:!0});var vRt=n(zB);YVe=t(vRt,"CamembertTokenizer"),vRt.forEach(r),KVe=t(G8," or "),WB=s(G8,"A",{href:!0});var TRt=n(WB);ZVe=t(TRt,"CamembertTokenizerFast"),TRt.forEach(r),eze=t(G8," (CamemBERT model)"),G8.forEach(r),oze=i(y),Ig=s(y,"LI",{});var M4e=n(Ig);uU=s(M4e,"STRONG",{});var FRt=n(uU);tze=t(FRt,"canine"),FRt.forEach(r),rze=t(M4e," \u2014 "),QB=s(M4e,"A",{href:!0});var CRt=n(QB);aze=t(CRt,"CanineTokenizer"),CRt.forEach(r),sze=t(M4e," (Canine model)"),M4e.forEach(r),nze=i(y),Zs=s(y,"LI",{});var X8=n(Zs);pU=s(X8,"STRONG",{});var MRt=n(pU);lze=t(MRt,"clip"),MRt.forEach(r),ize=t(X8," \u2014 "),HB=s(X8,"A",{href:!0});var ERt=n(HB);dze=t(ERt,"CLIPTokenizer"),ERt.forEach(r),cze=t(X8," or "),UB=s(X8,"A",{href:!0});var yRt=n(UB);mze=t(yRt,"CLIPTokenizerFast"),yRt.forEach(r),fze=t(X8," (CLIP model)"),X8.forEach(r),gze=i(y),en=s(y,"LI",{});var V8=n(en);_U=s(V8,"STRONG",{});var wRt=n(_U);hze=t(wRt,"convbert"),wRt.forEach(r),uze=t(V8," \u2014 "),JB=s(V8,"A",{href:!0});var ARt=n(JB);pze=t(ARt,"ConvBertTokenizer"),ARt.forEach(r),_ze=t(V8," or "),YB=s(V8,"A",{href:!0});var LRt=n(YB);bze=t(LRt,"ConvBertTokenizerFast"),LRt.forEach(r),vze=t(V8," (ConvBERT model)"),V8.forEach(r),Tze=i(y),on=s(y,"LI",{});var z8=n(on);bU=s(z8,"STRONG",{});var BRt=n(bU);Fze=t(BRt,"cpm"),BRt.forEach(r),Cze=t(z8," \u2014 "),KB=s(z8,"A",{href:!0});var xRt=n(KB);Mze=t(xRt,"CpmTokenizer"),xRt.forEach(r),Eze=t(z8," or "),vU=s(z8,"CODE",{});var kRt=n(vU);yze=t(kRt,"CpmTokenizerFast"),kRt.forEach(r),wze=t(z8," (CPM model)"),z8.forEach(r),Aze=i(y),jg=s(y,"LI",{});var E4e=n(jg);TU=s(E4e,"STRONG",{});var RRt=n(TU);Lze=t(RRt,"ctrl"),RRt.forEach(r),Bze=t(E4e," \u2014 "),ZB=s(E4e,"A",{href:!0});var SRt=n(ZB);xze=t(SRt,"CTRLTokenizer"),SRt.forEach(r),kze=t(E4e," (CTRL model)"),E4e.forEach(r),Rze=i(y),tn=s(y,"LI",{});var W8=n(tn);FU=s(W8,"STRONG",{});var PRt=n(FU);Sze=t(PRt,"deberta"),PRt.forEach(r),Pze=t(W8," \u2014 "),ex=s(W8,"A",{href:!0});var $Rt=n(ex);$ze=t($Rt,"DebertaTokenizer"),$Rt.forEach(r),Ize=t(W8," or "),ox=s(W8,"A",{href:!0});var IRt=n(ox);jze=t(IRt,"DebertaTokenizerFast"),IRt.forEach(r),Dze=t(W8," (DeBERTa model)"),W8.forEach(r),Nze=i(y),Dg=s(y,"LI",{});var y4e=n(Dg);CU=s(y4e,"STRONG",{});var jRt=n(CU);qze=t(jRt,"deberta-v2"),jRt.forEach(r),Oze=t(y4e," \u2014 "),tx=s(y4e,"A",{href:!0});var DRt=n(tx);Gze=t(DRt,"DebertaV2Tokenizer"),DRt.forEach(r),Xze=t(y4e," (DeBERTa-v2 model)"),y4e.forEach(r),Vze=i(y),rn=s(y,"LI",{});var Q8=n(rn);MU=s(Q8,"STRONG",{});var NRt=n(MU);zze=t(NRt,"distilbert"),NRt.forEach(r),Wze=t(Q8," \u2014 "),rx=s(Q8,"A",{href:!0});var qRt=n(rx);Qze=t(qRt,"DistilBertTokenizer"),qRt.forEach(r),Hze=t(Q8," or "),ax=s(Q8,"A",{href:!0});var ORt=n(ax);Uze=t(ORt,"DistilBertTokenizerFast"),ORt.forEach(r),Jze=t(Q8," (DistilBERT model)"),Q8.forEach(r),Yze=i(y),an=s(y,"LI",{});var H8=n(an);EU=s(H8,"STRONG",{});var GRt=n(EU);Kze=t(GRt,"dpr"),GRt.forEach(r),Zze=t(H8," \u2014 "),sx=s(H8,"A",{href:!0});var XRt=n(sx);eWe=t(XRt,"DPRQuestionEncoderTokenizer"),XRt.forEach(r),oWe=t(H8," or "),nx=s(H8,"A",{href:!0});var VRt=n(nx);tWe=t(VRt,"DPRQuestionEncoderTokenizerFast"),VRt.forEach(r),rWe=t(H8," (DPR model)"),H8.forEach(r),aWe=i(y),sn=s(y,"LI",{});var U8=n(sn);yU=s(U8,"STRONG",{});var zRt=n(yU);sWe=t(zRt,"electra"),zRt.forEach(r),nWe=t(U8," \u2014 "),lx=s(U8,"A",{href:!0});var WRt=n(lx);lWe=t(WRt,"ElectraTokenizer"),WRt.forEach(r),iWe=t(U8," or "),ix=s(U8,"A",{href:!0});var QRt=n(ix);dWe=t(QRt,"ElectraTokenizerFast"),QRt.forEach(r),cWe=t(U8," (ELECTRA model)"),U8.forEach(r),mWe=i(y),Ng=s(y,"LI",{});var w4e=n(Ng);wU=s(w4e,"STRONG",{});var HRt=n(wU);fWe=t(HRt,"flaubert"),HRt.forEach(r),gWe=t(w4e," \u2014 "),dx=s(w4e,"A",{href:!0});var URt=n(dx);hWe=t(URt,"FlaubertTokenizer"),URt.forEach(r),uWe=t(w4e," (FlauBERT model)"),w4e.forEach(r),pWe=i(y),nn=s(y,"LI",{});var J8=n(nn);AU=s(J8,"STRONG",{});var JRt=n(AU);_We=t(JRt,"fnet"),JRt.forEach(r),bWe=t(J8," \u2014 "),cx=s(J8,"A",{href:!0});var YRt=n(cx);vWe=t(YRt,"FNetTokenizer"),YRt.forEach(r),TWe=t(J8," or "),mx=s(J8,"A",{href:!0});var KRt=n(mx);FWe=t(KRt,"FNetTokenizerFast"),KRt.forEach(r),CWe=t(J8," (FNet model)"),J8.forEach(r),MWe=i(y),qg=s(y,"LI",{});var A4e=n(qg);LU=s(A4e,"STRONG",{});var ZRt=n(LU);EWe=t(ZRt,"fsmt"),ZRt.forEach(r),yWe=t(A4e," \u2014 "),fx=s(A4e,"A",{href:!0});var eSt=n(fx);wWe=t(eSt,"FSMTTokenizer"),eSt.forEach(r),AWe=t(A4e," (FairSeq Machine-Translation model)"),A4e.forEach(r),LWe=i(y),ln=s(y,"LI",{});var Y8=n(ln);BU=s(Y8,"STRONG",{});var oSt=n(BU);BWe=t(oSt,"funnel"),oSt.forEach(r),xWe=t(Y8," \u2014 "),gx=s(Y8,"A",{href:!0});var tSt=n(gx);kWe=t(tSt,"FunnelTokenizer"),tSt.forEach(r),RWe=t(Y8," or "),hx=s(Y8,"A",{href:!0});var rSt=n(hx);SWe=t(rSt,"FunnelTokenizerFast"),rSt.forEach(r),PWe=t(Y8," (Funnel Transformer model)"),Y8.forEach(r),$We=i(y),dn=s(y,"LI",{});var K8=n(dn);xU=s(K8,"STRONG",{});var aSt=n(xU);IWe=t(aSt,"gpt2"),aSt.forEach(r),jWe=t(K8," \u2014 "),ux=s(K8,"A",{href:!0});var sSt=n(ux);DWe=t(sSt,"GPT2Tokenizer"),sSt.forEach(r),NWe=t(K8," or "),px=s(K8,"A",{href:!0});var nSt=n(px);qWe=t(nSt,"GPT2TokenizerFast"),nSt.forEach(r),OWe=t(K8," (OpenAI GPT-2 model)"),K8.forEach(r),GWe=i(y),cn=s(y,"LI",{});var Z8=n(cn);kU=s(Z8,"STRONG",{});var lSt=n(kU);XWe=t(lSt,"gpt_neo"),lSt.forEach(r),VWe=t(Z8," \u2014 "),_x=s(Z8,"A",{href:!0});var iSt=n(_x);zWe=t(iSt,"GPT2Tokenizer"),iSt.forEach(r),WWe=t(Z8," or "),bx=s(Z8,"A",{href:!0});var dSt=n(bx);QWe=t(dSt,"GPT2TokenizerFast"),dSt.forEach(r),HWe=t(Z8," (GPT Neo model)"),Z8.forEach(r),UWe=i(y),mn=s(y,"LI",{});var e7=n(mn);RU=s(e7,"STRONG",{});var cSt=n(RU);JWe=t(cSt,"herbert"),cSt.forEach(r),YWe=t(e7," \u2014 "),vx=s(e7,"A",{href:!0});var mSt=n(vx);KWe=t(mSt,"HerbertTokenizer"),mSt.forEach(r),ZWe=t(e7," or "),Tx=s(e7,"A",{href:!0});var fSt=n(Tx);eQe=t(fSt,"HerbertTokenizerFast"),fSt.forEach(r),oQe=t(e7," (HerBERT model)"),e7.forEach(r),tQe=i(y),Og=s(y,"LI",{});var L4e=n(Og);SU=s(L4e,"STRONG",{});var gSt=n(SU);rQe=t(gSt,"hubert"),gSt.forEach(r),aQe=t(L4e," \u2014 "),Fx=s(L4e,"A",{href:!0});var hSt=n(Fx);sQe=t(hSt,"Wav2Vec2CTCTokenizer"),hSt.forEach(r),nQe=t(L4e," (Hubert model)"),L4e.forEach(r),lQe=i(y),fn=s(y,"LI",{});var o7=n(fn);PU=s(o7,"STRONG",{});var uSt=n(PU);iQe=t(uSt,"ibert"),uSt.forEach(r),dQe=t(o7," \u2014 "),Cx=s(o7,"A",{href:!0});var pSt=n(Cx);cQe=t(pSt,"RobertaTokenizer"),pSt.forEach(r),mQe=t(o7," or "),Mx=s(o7,"A",{href:!0});var _St=n(Mx);fQe=t(_St,"RobertaTokenizerFast"),_St.forEach(r),gQe=t(o7," (I-BERT model)"),o7.forEach(r),hQe=i(y),gn=s(y,"LI",{});var t7=n(gn);$U=s(t7,"STRONG",{});var bSt=n($U);uQe=t(bSt,"layoutlm"),bSt.forEach(r),pQe=t(t7," \u2014 "),Ex=s(t7,"A",{href:!0});var vSt=n(Ex);_Qe=t(vSt,"LayoutLMTokenizer"),vSt.forEach(r),bQe=t(t7," or "),yx=s(t7,"A",{href:!0});var TSt=n(yx);vQe=t(TSt,"LayoutLMTokenizerFast"),TSt.forEach(r),TQe=t(t7," (LayoutLM model)"),t7.forEach(r),FQe=i(y),hn=s(y,"LI",{});var r7=n(hn);IU=s(r7,"STRONG",{});var FSt=n(IU);CQe=t(FSt,"layoutlmv2"),FSt.forEach(r),MQe=t(r7," \u2014 "),wx=s(r7,"A",{href:!0});var CSt=n(wx);EQe=t(CSt,"LayoutLMv2Tokenizer"),CSt.forEach(r),yQe=t(r7," or "),Ax=s(r7,"A",{href:!0});var MSt=n(Ax);wQe=t(MSt,"LayoutLMv2TokenizerFast"),MSt.forEach(r),AQe=t(r7," (LayoutLMv2 model)"),r7.forEach(r),LQe=i(y),un=s(y,"LI",{});var a7=n(un);jU=s(a7,"STRONG",{});var ESt=n(jU);BQe=t(ESt,"layoutxlm"),ESt.forEach(r),xQe=t(a7," \u2014 "),Lx=s(a7,"A",{href:!0});var ySt=n(Lx);kQe=t(ySt,"LayoutXLMTokenizer"),ySt.forEach(r),RQe=t(a7," or "),Bx=s(a7,"A",{href:!0});var wSt=n(Bx);SQe=t(wSt,"LayoutXLMTokenizerFast"),wSt.forEach(r),PQe=t(a7," (LayoutXLM model)"),a7.forEach(r),$Qe=i(y),pn=s(y,"LI",{});var s7=n(pn);DU=s(s7,"STRONG",{});var ASt=n(DU);IQe=t(ASt,"led"),ASt.forEach(r),jQe=t(s7," \u2014 "),xx=s(s7,"A",{href:!0});var LSt=n(xx);DQe=t(LSt,"LEDTokenizer"),LSt.forEach(r),NQe=t(s7," or "),kx=s(s7,"A",{href:!0});var BSt=n(kx);qQe=t(BSt,"LEDTokenizerFast"),BSt.forEach(r),OQe=t(s7," (LED model)"),s7.forEach(r),GQe=i(y),_n=s(y,"LI",{});var n7=n(_n);NU=s(n7,"STRONG",{});var xSt=n(NU);XQe=t(xSt,"longformer"),xSt.forEach(r),VQe=t(n7," \u2014 "),Rx=s(n7,"A",{href:!0});var kSt=n(Rx);zQe=t(kSt,"LongformerTokenizer"),kSt.forEach(r),WQe=t(n7," or "),Sx=s(n7,"A",{href:!0});var RSt=n(Sx);QQe=t(RSt,"LongformerTokenizerFast"),RSt.forEach(r),HQe=t(n7," (Longformer model)"),n7.forEach(r),UQe=i(y),Gg=s(y,"LI",{});var B4e=n(Gg);qU=s(B4e,"STRONG",{});var SSt=n(qU);JQe=t(SSt,"luke"),SSt.forEach(r),YQe=t(B4e," \u2014 "),Px=s(B4e,"A",{href:!0});var PSt=n(Px);KQe=t(PSt,"LukeTokenizer"),PSt.forEach(r),ZQe=t(B4e," (LUKE model)"),B4e.forEach(r),eHe=i(y),bn=s(y,"LI",{});var l7=n(bn);OU=s(l7,"STRONG",{});var $St=n(OU);oHe=t($St,"lxmert"),$St.forEach(r),tHe=t(l7," \u2014 "),$x=s(l7,"A",{href:!0});var ISt=n($x);rHe=t(ISt,"LxmertTokenizer"),ISt.forEach(r),aHe=t(l7," or "),Ix=s(l7,"A",{href:!0});var jSt=n(Ix);sHe=t(jSt,"LxmertTokenizerFast"),jSt.forEach(r),nHe=t(l7," (LXMERT model)"),l7.forEach(r),lHe=i(y),Xg=s(y,"LI",{});var x4e=n(Xg);GU=s(x4e,"STRONG",{});var DSt=n(GU);iHe=t(DSt,"m2m_100"),DSt.forEach(r),dHe=t(x4e," \u2014 "),jx=s(x4e,"A",{href:!0});var NSt=n(jx);cHe=t(NSt,"M2M100Tokenizer"),NSt.forEach(r),mHe=t(x4e," (M2M100 model)"),x4e.forEach(r),fHe=i(y),Vg=s(y,"LI",{});var k4e=n(Vg);XU=s(k4e,"STRONG",{});var qSt=n(XU);gHe=t(qSt,"marian"),qSt.forEach(r),hHe=t(k4e," \u2014 "),Dx=s(k4e,"A",{href:!0});var OSt=n(Dx);uHe=t(OSt,"MarianTokenizer"),OSt.forEach(r),pHe=t(k4e," (Marian model)"),k4e.forEach(r),_He=i(y),vn=s(y,"LI",{});var i7=n(vn);VU=s(i7,"STRONG",{});var GSt=n(VU);bHe=t(GSt,"mbart"),GSt.forEach(r),vHe=t(i7," \u2014 "),Nx=s(i7,"A",{href:!0});var XSt=n(Nx);THe=t(XSt,"MBartTokenizer"),XSt.forEach(r),FHe=t(i7," or "),qx=s(i7,"A",{href:!0});var VSt=n(qx);CHe=t(VSt,"MBartTokenizerFast"),VSt.forEach(r),MHe=t(i7," (mBART model)"),i7.forEach(r),EHe=i(y),Tn=s(y,"LI",{});var d7=n(Tn);zU=s(d7,"STRONG",{});var zSt=n(zU);yHe=t(zSt,"mbart50"),zSt.forEach(r),wHe=t(d7," \u2014 "),Ox=s(d7,"A",{href:!0});var WSt=n(Ox);AHe=t(WSt,"MBart50Tokenizer"),WSt.forEach(r),LHe=t(d7," or "),Gx=s(d7,"A",{href:!0});var QSt=n(Gx);BHe=t(QSt,"MBart50TokenizerFast"),QSt.forEach(r),xHe=t(d7," (mBART-50 model)"),d7.forEach(r),kHe=i(y),zg=s(y,"LI",{});var R4e=n(zg);WU=s(R4e,"STRONG",{});var HSt=n(WU);RHe=t(HSt,"mluke"),HSt.forEach(r),SHe=t(R4e," \u2014 "),Xx=s(R4e,"A",{href:!0});var USt=n(Xx);PHe=t(USt,"MLukeTokenizer"),USt.forEach(r),$He=t(R4e," (mLUKE model)"),R4e.forEach(r),IHe=i(y),Fn=s(y,"LI",{});var c7=n(Fn);QU=s(c7,"STRONG",{});var JSt=n(QU);jHe=t(JSt,"mobilebert"),JSt.forEach(r),DHe=t(c7," \u2014 "),Vx=s(c7,"A",{href:!0});var YSt=n(Vx);NHe=t(YSt,"MobileBertTokenizer"),YSt.forEach(r),qHe=t(c7," or "),zx=s(c7,"A",{href:!0});var KSt=n(zx);OHe=t(KSt,"MobileBertTokenizerFast"),KSt.forEach(r),GHe=t(c7," (MobileBERT model)"),c7.forEach(r),XHe=i(y),Cn=s(y,"LI",{});var m7=n(Cn);HU=s(m7,"STRONG",{});var ZSt=n(HU);VHe=t(ZSt,"mpnet"),ZSt.forEach(r),zHe=t(m7," \u2014 "),Wx=s(m7,"A",{href:!0});var ePt=n(Wx);WHe=t(ePt,"MPNetTokenizer"),ePt.forEach(r),QHe=t(m7," or "),Qx=s(m7,"A",{href:!0});var oPt=n(Qx);HHe=t(oPt,"MPNetTokenizerFast"),oPt.forEach(r),UHe=t(m7," (MPNet model)"),m7.forEach(r),JHe=i(y),Mn=s(y,"LI",{});var f7=n(Mn);UU=s(f7,"STRONG",{});var tPt=n(UU);YHe=t(tPt,"mt5"),tPt.forEach(r),KHe=t(f7," \u2014 "),Hx=s(f7,"A",{href:!0});var rPt=n(Hx);ZHe=t(rPt,"MT5Tokenizer"),rPt.forEach(r),eUe=t(f7," or "),Ux=s(f7,"A",{href:!0});var aPt=n(Ux);oUe=t(aPt,"MT5TokenizerFast"),aPt.forEach(r),tUe=t(f7," (mT5 model)"),f7.forEach(r),rUe=i(y),En=s(y,"LI",{});var g7=n(En);JU=s(g7,"STRONG",{});var sPt=n(JU);aUe=t(sPt,"openai-gpt"),sPt.forEach(r),sUe=t(g7," \u2014 "),Jx=s(g7,"A",{href:!0});var nPt=n(Jx);nUe=t(nPt,"OpenAIGPTTokenizer"),nPt.forEach(r),lUe=t(g7," or "),Yx=s(g7,"A",{href:!0});var lPt=n(Yx);iUe=t(lPt,"OpenAIGPTTokenizerFast"),lPt.forEach(r),dUe=t(g7," (OpenAI GPT model)"),g7.forEach(r),cUe=i(y),yn=s(y,"LI",{});var h7=n(yn);YU=s(h7,"STRONG",{});var iPt=n(YU);mUe=t(iPt,"pegasus"),iPt.forEach(r),fUe=t(h7," \u2014 "),Kx=s(h7,"A",{href:!0});var dPt=n(Kx);gUe=t(dPt,"PegasusTokenizer"),dPt.forEach(r),hUe=t(h7," or "),Zx=s(h7,"A",{href:!0});var cPt=n(Zx);uUe=t(cPt,"PegasusTokenizerFast"),cPt.forEach(r),pUe=t(h7," (Pegasus model)"),h7.forEach(r),_Ue=i(y),Wg=s(y,"LI",{});var S4e=n(Wg);KU=s(S4e,"STRONG",{});var mPt=n(KU);bUe=t(mPt,"perceiver"),mPt.forEach(r),vUe=t(S4e," \u2014 "),ek=s(S4e,"A",{href:!0});var fPt=n(ek);TUe=t(fPt,"PerceiverTokenizer"),fPt.forEach(r),FUe=t(S4e," (Perceiver model)"),S4e.forEach(r),CUe=i(y),Qg=s(y,"LI",{});var P4e=n(Qg);ZU=s(P4e,"STRONG",{});var gPt=n(ZU);MUe=t(gPt,"phobert"),gPt.forEach(r),EUe=t(P4e," \u2014 "),ok=s(P4e,"A",{href:!0});var hPt=n(ok);yUe=t(hPt,"PhobertTokenizer"),hPt.forEach(r),wUe=t(P4e," (PhoBERT model)"),P4e.forEach(r),AUe=i(y),Hg=s(y,"LI",{});var $4e=n(Hg);eJ=s($4e,"STRONG",{});var uPt=n(eJ);LUe=t(uPt,"plbart"),uPt.forEach(r),BUe=t($4e," \u2014 "),tk=s($4e,"A",{href:!0});var pPt=n(tk);xUe=t(pPt,"PLBartTokenizer"),pPt.forEach(r),kUe=t($4e," (PLBart model)"),$4e.forEach(r),RUe=i(y),Ug=s(y,"LI",{});var I4e=n(Ug);oJ=s(I4e,"STRONG",{});var _Pt=n(oJ);SUe=t(_Pt,"prophetnet"),_Pt.forEach(r),PUe=t(I4e," \u2014 "),rk=s(I4e,"A",{href:!0});var bPt=n(rk);$Ue=t(bPt,"ProphetNetTokenizer"),bPt.forEach(r),IUe=t(I4e," (ProphetNet model)"),I4e.forEach(r),jUe=i(y),wn=s(y,"LI",{});var u7=n(wn);tJ=s(u7,"STRONG",{});var vPt=n(tJ);DUe=t(vPt,"qdqbert"),vPt.forEach(r),NUe=t(u7," \u2014 "),ak=s(u7,"A",{href:!0});var TPt=n(ak);qUe=t(TPt,"BertTokenizer"),TPt.forEach(r),OUe=t(u7," or "),sk=s(u7,"A",{href:!0});var FPt=n(sk);GUe=t(FPt,"BertTokenizerFast"),FPt.forEach(r),XUe=t(u7," (QDQBert model)"),u7.forEach(r),VUe=i(y),Jg=s(y,"LI",{});var j4e=n(Jg);rJ=s(j4e,"STRONG",{});var CPt=n(rJ);zUe=t(CPt,"rag"),CPt.forEach(r),WUe=t(j4e," \u2014 "),nk=s(j4e,"A",{href:!0});var MPt=n(nk);QUe=t(MPt,"RagTokenizer"),MPt.forEach(r),HUe=t(j4e," (RAG model)"),j4e.forEach(r),UUe=i(y),An=s(y,"LI",{});var p7=n(An);aJ=s(p7,"STRONG",{});var EPt=n(aJ);JUe=t(EPt,"realm"),EPt.forEach(r),YUe=t(p7," \u2014 "),lk=s(p7,"A",{href:!0});var yPt=n(lk);KUe=t(yPt,"RealmTokenizer"),yPt.forEach(r),ZUe=t(p7," or "),ik=s(p7,"A",{href:!0});var wPt=n(ik);eJe=t(wPt,"RealmTokenizerFast"),wPt.forEach(r),oJe=t(p7," (Realm model)"),p7.forEach(r),tJe=i(y),Ln=s(y,"LI",{});var _7=n(Ln);sJ=s(_7,"STRONG",{});var APt=n(sJ);rJe=t(APt,"reformer"),APt.forEach(r),aJe=t(_7," \u2014 "),dk=s(_7,"A",{href:!0});var LPt=n(dk);sJe=t(LPt,"ReformerTokenizer"),LPt.forEach(r),nJe=t(_7," or "),ck=s(_7,"A",{href:!0});var BPt=n(ck);lJe=t(BPt,"ReformerTokenizerFast"),BPt.forEach(r),iJe=t(_7," (Reformer model)"),_7.forEach(r),dJe=i(y),Bn=s(y,"LI",{});var b7=n(Bn);nJ=s(b7,"STRONG",{});var xPt=n(nJ);cJe=t(xPt,"rembert"),xPt.forEach(r),mJe=t(b7," \u2014 "),mk=s(b7,"A",{href:!0});var kPt=n(mk);fJe=t(kPt,"RemBertTokenizer"),kPt.forEach(r),gJe=t(b7," or "),fk=s(b7,"A",{href:!0});var RPt=n(fk);hJe=t(RPt,"RemBertTokenizerFast"),RPt.forEach(r),uJe=t(b7," (RemBERT model)"),b7.forEach(r),pJe=i(y),xn=s(y,"LI",{});var v7=n(xn);lJ=s(v7,"STRONG",{});var SPt=n(lJ);_Je=t(SPt,"retribert"),SPt.forEach(r),bJe=t(v7," \u2014 "),gk=s(v7,"A",{href:!0});var PPt=n(gk);vJe=t(PPt,"RetriBertTokenizer"),PPt.forEach(r),TJe=t(v7," or "),hk=s(v7,"A",{href:!0});var $Pt=n(hk);FJe=t($Pt,"RetriBertTokenizerFast"),$Pt.forEach(r),CJe=t(v7," (RetriBERT model)"),v7.forEach(r),MJe=i(y),kn=s(y,"LI",{});var T7=n(kn);iJ=s(T7,"STRONG",{});var IPt=n(iJ);EJe=t(IPt,"roberta"),IPt.forEach(r),yJe=t(T7," \u2014 "),uk=s(T7,"A",{href:!0});var jPt=n(uk);wJe=t(jPt,"RobertaTokenizer"),jPt.forEach(r),AJe=t(T7," or "),pk=s(T7,"A",{href:!0});var DPt=n(pk);LJe=t(DPt,"RobertaTokenizerFast"),DPt.forEach(r),BJe=t(T7," (RoBERTa model)"),T7.forEach(r),xJe=i(y),Rn=s(y,"LI",{});var F7=n(Rn);dJ=s(F7,"STRONG",{});var NPt=n(dJ);kJe=t(NPt,"roformer"),NPt.forEach(r),RJe=t(F7," \u2014 "),_k=s(F7,"A",{href:!0});var qPt=n(_k);SJe=t(qPt,"RoFormerTokenizer"),qPt.forEach(r),PJe=t(F7," or "),bk=s(F7,"A",{href:!0});var OPt=n(bk);$Je=t(OPt,"RoFormerTokenizerFast"),OPt.forEach(r),IJe=t(F7," (RoFormer model)"),F7.forEach(r),jJe=i(y),Yg=s(y,"LI",{});var D4e=n(Yg);cJ=s(D4e,"STRONG",{});var GPt=n(cJ);DJe=t(GPt,"speech_to_text"),GPt.forEach(r),NJe=t(D4e," \u2014 "),vk=s(D4e,"A",{href:!0});var XPt=n(vk);qJe=t(XPt,"Speech2TextTokenizer"),XPt.forEach(r),OJe=t(D4e," (Speech2Text model)"),D4e.forEach(r),GJe=i(y),Kg=s(y,"LI",{});var N4e=n(Kg);mJ=s(N4e,"STRONG",{});var VPt=n(mJ);XJe=t(VPt,"speech_to_text_2"),VPt.forEach(r),VJe=t(N4e," \u2014 "),Tk=s(N4e,"A",{href:!0});var zPt=n(Tk);zJe=t(zPt,"Speech2Text2Tokenizer"),zPt.forEach(r),WJe=t(N4e," (Speech2Text2 model)"),N4e.forEach(r),QJe=i(y),Sn=s(y,"LI",{});var C7=n(Sn);fJ=s(C7,"STRONG",{});var WPt=n(fJ);HJe=t(WPt,"splinter"),WPt.forEach(r),UJe=t(C7," \u2014 "),Fk=s(C7,"A",{href:!0});var QPt=n(Fk);JJe=t(QPt,"SplinterTokenizer"),QPt.forEach(r),YJe=t(C7," or "),Ck=s(C7,"A",{href:!0});var HPt=n(Ck);KJe=t(HPt,"SplinterTokenizerFast"),HPt.forEach(r),ZJe=t(C7," (Splinter model)"),C7.forEach(r),eYe=i(y),Pn=s(y,"LI",{});var M7=n(Pn);gJ=s(M7,"STRONG",{});var UPt=n(gJ);oYe=t(UPt,"squeezebert"),UPt.forEach(r),tYe=t(M7," \u2014 "),Mk=s(M7,"A",{href:!0});var JPt=n(Mk);rYe=t(JPt,"SqueezeBertTokenizer"),JPt.forEach(r),aYe=t(M7," or "),Ek=s(M7,"A",{href:!0});var YPt=n(Ek);sYe=t(YPt,"SqueezeBertTokenizerFast"),YPt.forEach(r),nYe=t(M7," (SqueezeBERT model)"),M7.forEach(r),lYe=i(y),$n=s(y,"LI",{});var E7=n($n);hJ=s(E7,"STRONG",{});var KPt=n(hJ);iYe=t(KPt,"t5"),KPt.forEach(r),dYe=t(E7," \u2014 "),yk=s(E7,"A",{href:!0});var ZPt=n(yk);cYe=t(ZPt,"T5Tokenizer"),ZPt.forEach(r),mYe=t(E7," or "),wk=s(E7,"A",{href:!0});var e$t=n(wk);fYe=t(e$t,"T5TokenizerFast"),e$t.forEach(r),gYe=t(E7," (T5 model)"),E7.forEach(r),hYe=i(y),Zg=s(y,"LI",{});var q4e=n(Zg);uJ=s(q4e,"STRONG",{});var o$t=n(uJ);uYe=t(o$t,"tapas"),o$t.forEach(r),pYe=t(q4e," \u2014 "),Ak=s(q4e,"A",{href:!0});var t$t=n(Ak);_Ye=t(t$t,"TapasTokenizer"),t$t.forEach(r),bYe=t(q4e," (TAPAS model)"),q4e.forEach(r),vYe=i(y),eh=s(y,"LI",{});var O4e=n(eh);pJ=s(O4e,"STRONG",{});var r$t=n(pJ);TYe=t(r$t,"transfo-xl"),r$t.forEach(r),FYe=t(O4e," \u2014 "),Lk=s(O4e,"A",{href:!0});var a$t=n(Lk);CYe=t(a$t,"TransfoXLTokenizer"),a$t.forEach(r),MYe=t(O4e," (Transformer-XL model)"),O4e.forEach(r),EYe=i(y),oh=s(y,"LI",{});var G4e=n(oh);_J=s(G4e,"STRONG",{});var s$t=n(_J);yYe=t(s$t,"wav2vec2"),s$t.forEach(r),wYe=t(G4e," \u2014 "),Bk=s(G4e,"A",{href:!0});var n$t=n(Bk);AYe=t(n$t,"Wav2Vec2CTCTokenizer"),n$t.forEach(r),LYe=t(G4e," (Wav2Vec2 model)"),G4e.forEach(r),BYe=i(y),th=s(y,"LI",{});var X4e=n(th);bJ=s(X4e,"STRONG",{});var l$t=n(bJ);xYe=t(l$t,"wav2vec2_phoneme"),l$t.forEach(r),kYe=t(X4e," \u2014 "),xk=s(X4e,"A",{href:!0});var i$t=n(xk);RYe=t(i$t,"Wav2Vec2PhonemeCTCTokenizer"),i$t.forEach(r),SYe=t(X4e," (Wav2Vec2Phoneme model)"),X4e.forEach(r),PYe=i(y),In=s(y,"LI",{});var y7=n(In);vJ=s(y7,"STRONG",{});var d$t=n(vJ);$Ye=t(d$t,"xglm"),d$t.forEach(r),IYe=t(y7," \u2014 "),kk=s(y7,"A",{href:!0});var c$t=n(kk);jYe=t(c$t,"XGLMTokenizer"),c$t.forEach(r),DYe=t(y7," or "),Rk=s(y7,"A",{href:!0});var m$t=n(Rk);NYe=t(m$t,"XGLMTokenizerFast"),m$t.forEach(r),qYe=t(y7," (XGLM model)"),y7.forEach(r),OYe=i(y),rh=s(y,"LI",{});var V4e=n(rh);TJ=s(V4e,"STRONG",{});var f$t=n(TJ);GYe=t(f$t,"xlm"),f$t.forEach(r),XYe=t(V4e," \u2014 "),Sk=s(V4e,"A",{href:!0});var g$t=n(Sk);VYe=t(g$t,"XLMTokenizer"),g$t.forEach(r),zYe=t(V4e," (XLM model)"),V4e.forEach(r),WYe=i(y),ah=s(y,"LI",{});var z4e=n(ah);FJ=s(z4e,"STRONG",{});var h$t=n(FJ);QYe=t(h$t,"xlm-prophetnet"),h$t.forEach(r),HYe=t(z4e," \u2014 "),Pk=s(z4e,"A",{href:!0});var u$t=n(Pk);UYe=t(u$t,"XLMProphetNetTokenizer"),u$t.forEach(r),JYe=t(z4e," (XLMProphetNet model)"),z4e.forEach(r),YYe=i(y),jn=s(y,"LI",{});var w7=n(jn);CJ=s(w7,"STRONG",{});var p$t=n(CJ);KYe=t(p$t,"xlm-roberta"),p$t.forEach(r),ZYe=t(w7," \u2014 "),$k=s(w7,"A",{href:!0});var _$t=n($k);eKe=t(_$t,"XLMRobertaTokenizer"),_$t.forEach(r),oKe=t(w7," or "),Ik=s(w7,"A",{href:!0});var b$t=n(Ik);tKe=t(b$t,"XLMRobertaTokenizerFast"),b$t.forEach(r),rKe=t(w7," (XLM-RoBERTa model)"),w7.forEach(r),aKe=i(y),Dn=s(y,"LI",{});var A7=n(Dn);MJ=s(A7,"STRONG",{});var v$t=n(MJ);sKe=t(v$t,"xlnet"),v$t.forEach(r),nKe=t(A7," \u2014 "),jk=s(A7,"A",{href:!0});var T$t=n(jk);lKe=t(T$t,"XLNetTokenizer"),T$t.forEach(r),iKe=t(A7," or "),Dk=s(A7,"A",{href:!0});var F$t=n(Dk);dKe=t(F$t,"XLNetTokenizerFast"),F$t.forEach(r),cKe=t(A7," (XLNet model)"),A7.forEach(r),y.forEach(r),mKe=i(ga),EJ=s(ga,"P",{});var C$t=n(EJ);fKe=t(C$t,"Examples:"),C$t.forEach(r),gKe=i(ga),f(ty.$$.fragment,ga),ga.forEach(r),hKe=i(Xn),sh=s(Xn,"DIV",{class:!0});var jke=n(sh);f(ry.$$.fragment,jke),uKe=i(jke),yJ=s(jke,"P",{});var M$t=n(yJ);pKe=t(M$t,"Register a new tokenizer in this mapping."),M$t.forEach(r),jke.forEach(r),Xn.forEach(r),SBe=i(d),Qi=s(d,"H2",{class:!0});var Dke=n(Qi);nh=s(Dke,"A",{id:!0,class:!0,href:!0});var E$t=n(nh);wJ=s(E$t,"SPAN",{});var y$t=n(wJ);f(ay.$$.fragment,y$t),y$t.forEach(r),E$t.forEach(r),_Ke=i(Dke),AJ=s(Dke,"SPAN",{});var w$t=n(AJ);bKe=t(w$t,"AutoFeatureExtractor"),w$t.forEach(r),Dke.forEach(r),PBe=i(d),Wo=s(d,"DIV",{class:!0});var Vn=n(Wo);f(sy.$$.fragment,Vn),vKe=i(Vn),ny=s(Vn,"P",{});var Nke=n(ny);TKe=t(Nke,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Nk=s(Nke,"A",{href:!0});var A$t=n(Nk);FKe=t(A$t,"AutoFeatureExtractor.from_pretrained()"),A$t.forEach(r),CKe=t(Nke," class method."),Nke.forEach(r),MKe=i(Vn),ly=s(Vn,"P",{});var qke=n(ly);EKe=t(qke,"This class cannot be instantiated directly using "),LJ=s(qke,"CODE",{});var L$t=n(LJ);yKe=t(L$t,"__init__()"),L$t.forEach(r),wKe=t(qke," (throws an error)."),qke.forEach(r),AKe=i(Vn),xe=s(Vn,"DIV",{class:!0});var Ir=n(xe);f(iy.$$.fragment,Ir),LKe=i(Ir),BJ=s(Ir,"P",{});var B$t=n(BJ);BKe=t(B$t,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),B$t.forEach(r),xKe=i(Ir),Ga=s(Ir,"P",{});var q3=n(Ga);kKe=t(q3,"The feature extractor class to instantiate is selected based on the "),xJ=s(q3,"CODE",{});var x$t=n(xJ);RKe=t(x$t,"model_type"),x$t.forEach(r),SKe=t(q3,` property of the config object
(either passed as an argument or loaded from `),kJ=s(q3,"CODE",{});var k$t=n(kJ);PKe=t(k$t,"pretrained_model_name_or_path"),k$t.forEach(r),$Ke=t(q3,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),RJ=s(q3,"CODE",{});var R$t=n(RJ);IKe=t(R$t,"pretrained_model_name_or_path"),R$t.forEach(r),jKe=t(q3,":"),q3.forEach(r),DKe=i(Ir),ae=s(Ir,"UL",{});var le=n(ae);lh=s(le,"LI",{});var W4e=n(lh);SJ=s(W4e,"STRONG",{});var S$t=n(SJ);NKe=t(S$t,"beit"),S$t.forEach(r),qKe=t(W4e," \u2014 "),qk=s(W4e,"A",{href:!0});var P$t=n(qk);OKe=t(P$t,"BeitFeatureExtractor"),P$t.forEach(r),GKe=t(W4e," (BEiT model)"),W4e.forEach(r),XKe=i(le),ih=s(le,"LI",{});var Q4e=n(ih);PJ=s(Q4e,"STRONG",{});var $$t=n(PJ);VKe=t($$t,"clip"),$$t.forEach(r),zKe=t(Q4e," \u2014 "),Ok=s(Q4e,"A",{href:!0});var I$t=n(Ok);WKe=t(I$t,"CLIPFeatureExtractor"),I$t.forEach(r),QKe=t(Q4e," (CLIP model)"),Q4e.forEach(r),HKe=i(le),dh=s(le,"LI",{});var H4e=n(dh);$J=s(H4e,"STRONG",{});var j$t=n($J);UKe=t(j$t,"convnext"),j$t.forEach(r),JKe=t(H4e," \u2014 "),Gk=s(H4e,"A",{href:!0});var D$t=n(Gk);YKe=t(D$t,"ConvNextFeatureExtractor"),D$t.forEach(r),KKe=t(H4e," (ConvNext model)"),H4e.forEach(r),ZKe=i(le),ch=s(le,"LI",{});var U4e=n(ch);IJ=s(U4e,"STRONG",{});var N$t=n(IJ);eZe=t(N$t,"deit"),N$t.forEach(r),oZe=t(U4e," \u2014 "),Xk=s(U4e,"A",{href:!0});var q$t=n(Xk);tZe=t(q$t,"DeiTFeatureExtractor"),q$t.forEach(r),rZe=t(U4e," (DeiT model)"),U4e.forEach(r),aZe=i(le),mh=s(le,"LI",{});var J4e=n(mh);jJ=s(J4e,"STRONG",{});var O$t=n(jJ);sZe=t(O$t,"detr"),O$t.forEach(r),nZe=t(J4e," \u2014 "),Vk=s(J4e,"A",{href:!0});var G$t=n(Vk);lZe=t(G$t,"DetrFeatureExtractor"),G$t.forEach(r),iZe=t(J4e," (DETR model)"),J4e.forEach(r),dZe=i(le),fh=s(le,"LI",{});var Y4e=n(fh);DJ=s(Y4e,"STRONG",{});var X$t=n(DJ);cZe=t(X$t,"hubert"),X$t.forEach(r),mZe=t(Y4e," \u2014 "),zk=s(Y4e,"A",{href:!0});var V$t=n(zk);fZe=t(V$t,"Wav2Vec2FeatureExtractor"),V$t.forEach(r),gZe=t(Y4e," (Hubert model)"),Y4e.forEach(r),hZe=i(le),gh=s(le,"LI",{});var K4e=n(gh);NJ=s(K4e,"STRONG",{});var z$t=n(NJ);uZe=t(z$t,"layoutlmv2"),z$t.forEach(r),pZe=t(K4e," \u2014 "),Wk=s(K4e,"A",{href:!0});var W$t=n(Wk);_Ze=t(W$t,"LayoutLMv2FeatureExtractor"),W$t.forEach(r),bZe=t(K4e," (LayoutLMv2 model)"),K4e.forEach(r),vZe=i(le),hh=s(le,"LI",{});var Z4e=n(hh);qJ=s(Z4e,"STRONG",{});var Q$t=n(qJ);TZe=t(Q$t,"maskformer"),Q$t.forEach(r),FZe=t(Z4e," \u2014 "),Qk=s(Z4e,"A",{href:!0});var H$t=n(Qk);CZe=t(H$t,"MaskFormerFeatureExtractor"),H$t.forEach(r),MZe=t(Z4e," (MaskFormer model)"),Z4e.forEach(r),EZe=i(le),uh=s(le,"LI",{});var eMe=n(uh);OJ=s(eMe,"STRONG",{});var U$t=n(OJ);yZe=t(U$t,"perceiver"),U$t.forEach(r),wZe=t(eMe," \u2014 "),Hk=s(eMe,"A",{href:!0});var J$t=n(Hk);AZe=t(J$t,"PerceiverFeatureExtractor"),J$t.forEach(r),LZe=t(eMe," (Perceiver model)"),eMe.forEach(r),BZe=i(le),ph=s(le,"LI",{});var oMe=n(ph);GJ=s(oMe,"STRONG",{});var Y$t=n(GJ);xZe=t(Y$t,"poolformer"),Y$t.forEach(r),kZe=t(oMe," \u2014 "),Uk=s(oMe,"A",{href:!0});var K$t=n(Uk);RZe=t(K$t,"PoolFormerFeatureExtractor"),K$t.forEach(r),SZe=t(oMe," (PoolFormer model)"),oMe.forEach(r),PZe=i(le),_h=s(le,"LI",{});var tMe=n(_h);XJ=s(tMe,"STRONG",{});var Z$t=n(XJ);$Ze=t(Z$t,"segformer"),Z$t.forEach(r),IZe=t(tMe," \u2014 "),Jk=s(tMe,"A",{href:!0});var eIt=n(Jk);jZe=t(eIt,"SegformerFeatureExtractor"),eIt.forEach(r),DZe=t(tMe," (SegFormer model)"),tMe.forEach(r),NZe=i(le),bh=s(le,"LI",{});var rMe=n(bh);VJ=s(rMe,"STRONG",{});var oIt=n(VJ);qZe=t(oIt,"speech_to_text"),oIt.forEach(r),OZe=t(rMe," \u2014 "),Yk=s(rMe,"A",{href:!0});var tIt=n(Yk);GZe=t(tIt,"Speech2TextFeatureExtractor"),tIt.forEach(r),XZe=t(rMe," (Speech2Text model)"),rMe.forEach(r),VZe=i(le),vh=s(le,"LI",{});var aMe=n(vh);zJ=s(aMe,"STRONG",{});var rIt=n(zJ);zZe=t(rIt,"swin"),rIt.forEach(r),WZe=t(aMe," \u2014 "),Kk=s(aMe,"A",{href:!0});var aIt=n(Kk);QZe=t(aIt,"ViTFeatureExtractor"),aIt.forEach(r),HZe=t(aMe," (Swin model)"),aMe.forEach(r),UZe=i(le),Th=s(le,"LI",{});var sMe=n(Th);WJ=s(sMe,"STRONG",{});var sIt=n(WJ);JZe=t(sIt,"vit"),sIt.forEach(r),YZe=t(sMe," \u2014 "),Zk=s(sMe,"A",{href:!0});var nIt=n(Zk);KZe=t(nIt,"ViTFeatureExtractor"),nIt.forEach(r),ZZe=t(sMe," (ViT model)"),sMe.forEach(r),eeo=i(le),Fh=s(le,"LI",{});var nMe=n(Fh);QJ=s(nMe,"STRONG",{});var lIt=n(QJ);oeo=t(lIt,"vit_mae"),lIt.forEach(r),teo=t(nMe," \u2014 "),eR=s(nMe,"A",{href:!0});var iIt=n(eR);reo=t(iIt,"ViTFeatureExtractor"),iIt.forEach(r),aeo=t(nMe," (ViTMAE model)"),nMe.forEach(r),seo=i(le),Ch=s(le,"LI",{});var lMe=n(Ch);HJ=s(lMe,"STRONG",{});var dIt=n(HJ);neo=t(dIt,"wav2vec2"),dIt.forEach(r),leo=t(lMe," \u2014 "),oR=s(lMe,"A",{href:!0});var cIt=n(oR);ieo=t(cIt,"Wav2Vec2FeatureExtractor"),cIt.forEach(r),deo=t(lMe," (Wav2Vec2 model)"),lMe.forEach(r),le.forEach(r),ceo=i(Ir),f(Mh.$$.fragment,Ir),meo=i(Ir),UJ=s(Ir,"P",{});var mIt=n(UJ);feo=t(mIt,"Examples:"),mIt.forEach(r),geo=i(Ir),f(dy.$$.fragment,Ir),Ir.forEach(r),heo=i(Vn),Eh=s(Vn,"DIV",{class:!0});var Oke=n(Eh);f(cy.$$.fragment,Oke),ueo=i(Oke),JJ=s(Oke,"P",{});var fIt=n(JJ);peo=t(fIt,"Register a new feature extractor for this class."),fIt.forEach(r),Oke.forEach(r),Vn.forEach(r),$Be=i(d),Hi=s(d,"H2",{class:!0});var Gke=n(Hi);yh=s(Gke,"A",{id:!0,class:!0,href:!0});var gIt=n(yh);YJ=s(gIt,"SPAN",{});var hIt=n(YJ);f(my.$$.fragment,hIt),hIt.forEach(r),gIt.forEach(r),_eo=i(Gke),KJ=s(Gke,"SPAN",{});var uIt=n(KJ);beo=t(uIt,"AutoProcessor"),uIt.forEach(r),Gke.forEach(r),IBe=i(d),Qo=s(d,"DIV",{class:!0});var zn=n(Qo);f(fy.$$.fragment,zn),veo=i(zn),gy=s(zn,"P",{});var Xke=n(gy);Teo=t(Xke,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),tR=s(Xke,"A",{href:!0});var pIt=n(tR);Feo=t(pIt,"AutoProcessor.from_pretrained()"),pIt.forEach(r),Ceo=t(Xke," class method."),Xke.forEach(r),Meo=i(zn),hy=s(zn,"P",{});var Vke=n(hy);Eeo=t(Vke,"This class cannot be instantiated directly using "),ZJ=s(Vke,"CODE",{});var _It=n(ZJ);yeo=t(_It,"__init__()"),_It.forEach(r),weo=t(Vke," (throws an error)."),Vke.forEach(r),Aeo=i(zn),ke=s(zn,"DIV",{class:!0});var jr=n(ke);f(uy.$$.fragment,jr),Leo=i(jr),eY=s(jr,"P",{});var bIt=n(eY);Beo=t(bIt,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),bIt.forEach(r),xeo=i(jr),Ui=s(jr,"P",{});var nz=n(Ui);keo=t(nz,"The processor class to instantiate is selected based on the "),oY=s(nz,"CODE",{});var vIt=n(oY);Reo=t(vIt,"model_type"),vIt.forEach(r),Seo=t(nz,` property of the config object (either
passed as an argument or loaded from `),tY=s(nz,"CODE",{});var TIt=n(tY);Peo=t(TIt,"pretrained_model_name_or_path"),TIt.forEach(r),$eo=t(nz," if possible):"),nz.forEach(r),Ieo=i(jr),we=s(jr,"UL",{});var No=n(we);wh=s(No,"LI",{});var iMe=n(wh);rY=s(iMe,"STRONG",{});var FIt=n(rY);jeo=t(FIt,"clip"),FIt.forEach(r),Deo=t(iMe," \u2014 "),rR=s(iMe,"A",{href:!0});var CIt=n(rR);Neo=t(CIt,"CLIPProcessor"),CIt.forEach(r),qeo=t(iMe," (CLIP model)"),iMe.forEach(r),Oeo=i(No),Ah=s(No,"LI",{});var dMe=n(Ah);aY=s(dMe,"STRONG",{});var MIt=n(aY);Geo=t(MIt,"layoutlmv2"),MIt.forEach(r),Xeo=t(dMe," \u2014 "),aR=s(dMe,"A",{href:!0});var EIt=n(aR);Veo=t(EIt,"LayoutLMv2Processor"),EIt.forEach(r),zeo=t(dMe," (LayoutLMv2 model)"),dMe.forEach(r),Weo=i(No),Lh=s(No,"LI",{});var cMe=n(Lh);sY=s(cMe,"STRONG",{});var yIt=n(sY);Qeo=t(yIt,"layoutxlm"),yIt.forEach(r),Heo=t(cMe," \u2014 "),sR=s(cMe,"A",{href:!0});var wIt=n(sR);Ueo=t(wIt,"LayoutXLMProcessor"),wIt.forEach(r),Jeo=t(cMe," (LayoutXLM model)"),cMe.forEach(r),Yeo=i(No),Bh=s(No,"LI",{});var mMe=n(Bh);nY=s(mMe,"STRONG",{});var AIt=n(nY);Keo=t(AIt,"speech_to_text"),AIt.forEach(r),Zeo=t(mMe," \u2014 "),nR=s(mMe,"A",{href:!0});var LIt=n(nR);eoo=t(LIt,"Speech2TextProcessor"),LIt.forEach(r),ooo=t(mMe," (Speech2Text model)"),mMe.forEach(r),too=i(No),xh=s(No,"LI",{});var fMe=n(xh);lY=s(fMe,"STRONG",{});var BIt=n(lY);roo=t(BIt,"speech_to_text_2"),BIt.forEach(r),aoo=t(fMe," \u2014 "),lR=s(fMe,"A",{href:!0});var xIt=n(lR);soo=t(xIt,"Speech2Text2Processor"),xIt.forEach(r),noo=t(fMe," (Speech2Text2 model)"),fMe.forEach(r),loo=i(No),kh=s(No,"LI",{});var gMe=n(kh);iY=s(gMe,"STRONG",{});var kIt=n(iY);ioo=t(kIt,"trocr"),kIt.forEach(r),doo=t(gMe," \u2014 "),iR=s(gMe,"A",{href:!0});var RIt=n(iR);coo=t(RIt,"TrOCRProcessor"),RIt.forEach(r),moo=t(gMe," (TrOCR model)"),gMe.forEach(r),foo=i(No),Rh=s(No,"LI",{});var hMe=n(Rh);dY=s(hMe,"STRONG",{});var SIt=n(dY);goo=t(SIt,"vision-text-dual-encoder"),SIt.forEach(r),hoo=t(hMe," \u2014 "),dR=s(hMe,"A",{href:!0});var PIt=n(dR);uoo=t(PIt,"VisionTextDualEncoderProcessor"),PIt.forEach(r),poo=t(hMe," (VisionTextDualEncoder model)"),hMe.forEach(r),_oo=i(No),Sh=s(No,"LI",{});var uMe=n(Sh);cY=s(uMe,"STRONG",{});var $It=n(cY);boo=t($It,"wav2vec2"),$It.forEach(r),voo=t(uMe," \u2014 "),cR=s(uMe,"A",{href:!0});var IIt=n(cR);Too=t(IIt,"Wav2Vec2Processor"),IIt.forEach(r),Foo=t(uMe," (Wav2Vec2 model)"),uMe.forEach(r),No.forEach(r),Coo=i(jr),f(Ph.$$.fragment,jr),Moo=i(jr),mY=s(jr,"P",{});var jIt=n(mY);Eoo=t(jIt,"Examples:"),jIt.forEach(r),yoo=i(jr),f(py.$$.fragment,jr),jr.forEach(r),woo=i(zn),$h=s(zn,"DIV",{class:!0});var zke=n($h);f(_y.$$.fragment,zke),Aoo=i(zke),fY=s(zke,"P",{});var DIt=n(fY);Loo=t(DIt,"Register a new processor for this class."),DIt.forEach(r),zke.forEach(r),zn.forEach(r),jBe=i(d),Ji=s(d,"H2",{class:!0});var Wke=n(Ji);Ih=s(Wke,"A",{id:!0,class:!0,href:!0});var NIt=n(Ih);gY=s(NIt,"SPAN",{});var qIt=n(gY);f(by.$$.fragment,qIt),qIt.forEach(r),NIt.forEach(r),Boo=i(Wke),hY=s(Wke,"SPAN",{});var OIt=n(hY);xoo=t(OIt,"AutoModel"),OIt.forEach(r),Wke.forEach(r),DBe=i(d),Ho=s(d,"DIV",{class:!0});var Wn=n(Ho);f(vy.$$.fragment,Wn),koo=i(Wn),Yi=s(Wn,"P",{});var lz=n(Yi);Roo=t(lz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),uY=s(lz,"CODE",{});var GIt=n(uY);Soo=t(GIt,"from_pretrained()"),GIt.forEach(r),Poo=t(lz,"class method or the "),pY=s(lz,"CODE",{});var XIt=n(pY);$oo=t(XIt,"from_config()"),XIt.forEach(r),Ioo=t(lz,`class
method.`),lz.forEach(r),joo=i(Wn),Ty=s(Wn,"P",{});var Qke=n(Ty);Doo=t(Qke,"This class cannot be instantiated directly using "),_Y=s(Qke,"CODE",{});var VIt=n(_Y);Noo=t(VIt,"__init__()"),VIt.forEach(r),qoo=t(Qke," (throws an error)."),Qke.forEach(r),Ooo=i(Wn),Gt=s(Wn,"DIV",{class:!0});var Qn=n(Gt);f(Fy.$$.fragment,Qn),Goo=i(Qn),bY=s(Qn,"P",{});var zIt=n(bY);Xoo=t(zIt,"Instantiates one of the base model classes of the library from a configuration."),zIt.forEach(r),Voo=i(Qn),Ki=s(Qn,"P",{});var iz=n(Ki);zoo=t(iz,`Note:
Loading a model from its configuration file does `),vY=s(iz,"STRONG",{});var WIt=n(vY);Woo=t(WIt,"not"),WIt.forEach(r),Qoo=t(iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),TY=s(iz,"CODE",{});var QIt=n(TY);Hoo=t(QIt,"from_pretrained()"),QIt.forEach(r),Uoo=t(iz,"to load the model weights."),iz.forEach(r),Joo=i(Qn),FY=s(Qn,"P",{});var HIt=n(FY);Yoo=t(HIt,"Examples:"),HIt.forEach(r),Koo=i(Qn),f(Cy.$$.fragment,Qn),Qn.forEach(r),Zoo=i(Wn),Re=s(Wn,"DIV",{class:!0});var Dr=n(Re);f(My.$$.fragment,Dr),eto=i(Dr),CY=s(Dr,"P",{});var UIt=n(CY);oto=t(UIt,"Instantiate one of the base model classes of the library from a pretrained model."),UIt.forEach(r),tto=i(Dr),Xa=s(Dr,"P",{});var O3=n(Xa);rto=t(O3,"The model class to instantiate is selected based on the "),MY=s(O3,"CODE",{});var JIt=n(MY);ato=t(JIt,"model_type"),JIt.forEach(r),sto=t(O3,` property of the config object (either
passed as an argument or loaded from `),EY=s(O3,"CODE",{});var YIt=n(EY);nto=t(YIt,"pretrained_model_name_or_path"),YIt.forEach(r),lto=t(O3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yY=s(O3,"CODE",{});var KIt=n(yY);ito=t(KIt,"pretrained_model_name_or_path"),KIt.forEach(r),dto=t(O3,":"),O3.forEach(r),cto=i(Dr),F=s(Dr,"UL",{});var C=n(F);jh=s(C,"LI",{});var pMe=n(jh);wY=s(pMe,"STRONG",{});var ZIt=n(wY);mto=t(ZIt,"albert"),ZIt.forEach(r),fto=t(pMe," \u2014 "),mR=s(pMe,"A",{href:!0});var ejt=n(mR);gto=t(ejt,"AlbertModel"),ejt.forEach(r),hto=t(pMe," (ALBERT model)"),pMe.forEach(r),uto=i(C),Dh=s(C,"LI",{});var _Me=n(Dh);AY=s(_Me,"STRONG",{});var ojt=n(AY);pto=t(ojt,"bart"),ojt.forEach(r),_to=t(_Me," \u2014 "),fR=s(_Me,"A",{href:!0});var tjt=n(fR);bto=t(tjt,"BartModel"),tjt.forEach(r),vto=t(_Me," (BART model)"),_Me.forEach(r),Tto=i(C),Nh=s(C,"LI",{});var bMe=n(Nh);LY=s(bMe,"STRONG",{});var rjt=n(LY);Fto=t(rjt,"beit"),rjt.forEach(r),Cto=t(bMe," \u2014 "),gR=s(bMe,"A",{href:!0});var ajt=n(gR);Mto=t(ajt,"BeitModel"),ajt.forEach(r),Eto=t(bMe," (BEiT model)"),bMe.forEach(r),yto=i(C),qh=s(C,"LI",{});var vMe=n(qh);BY=s(vMe,"STRONG",{});var sjt=n(BY);wto=t(sjt,"bert"),sjt.forEach(r),Ato=t(vMe," \u2014 "),hR=s(vMe,"A",{href:!0});var njt=n(hR);Lto=t(njt,"BertModel"),njt.forEach(r),Bto=t(vMe," (BERT model)"),vMe.forEach(r),xto=i(C),Oh=s(C,"LI",{});var TMe=n(Oh);xY=s(TMe,"STRONG",{});var ljt=n(xY);kto=t(ljt,"bert-generation"),ljt.forEach(r),Rto=t(TMe," \u2014 "),uR=s(TMe,"A",{href:!0});var ijt=n(uR);Sto=t(ijt,"BertGenerationEncoder"),ijt.forEach(r),Pto=t(TMe," (Bert Generation model)"),TMe.forEach(r),$to=i(C),Gh=s(C,"LI",{});var FMe=n(Gh);kY=s(FMe,"STRONG",{});var djt=n(kY);Ito=t(djt,"big_bird"),djt.forEach(r),jto=t(FMe," \u2014 "),pR=s(FMe,"A",{href:!0});var cjt=n(pR);Dto=t(cjt,"BigBirdModel"),cjt.forEach(r),Nto=t(FMe," (BigBird model)"),FMe.forEach(r),qto=i(C),Xh=s(C,"LI",{});var CMe=n(Xh);RY=s(CMe,"STRONG",{});var mjt=n(RY);Oto=t(mjt,"bigbird_pegasus"),mjt.forEach(r),Gto=t(CMe," \u2014 "),_R=s(CMe,"A",{href:!0});var fjt=n(_R);Xto=t(fjt,"BigBirdPegasusModel"),fjt.forEach(r),Vto=t(CMe," (BigBirdPegasus model)"),CMe.forEach(r),zto=i(C),Vh=s(C,"LI",{});var MMe=n(Vh);SY=s(MMe,"STRONG",{});var gjt=n(SY);Wto=t(gjt,"blenderbot"),gjt.forEach(r),Qto=t(MMe," \u2014 "),bR=s(MMe,"A",{href:!0});var hjt=n(bR);Hto=t(hjt,"BlenderbotModel"),hjt.forEach(r),Uto=t(MMe," (Blenderbot model)"),MMe.forEach(r),Jto=i(C),zh=s(C,"LI",{});var EMe=n(zh);PY=s(EMe,"STRONG",{});var ujt=n(PY);Yto=t(ujt,"blenderbot-small"),ujt.forEach(r),Kto=t(EMe," \u2014 "),vR=s(EMe,"A",{href:!0});var pjt=n(vR);Zto=t(pjt,"BlenderbotSmallModel"),pjt.forEach(r),ero=t(EMe," (BlenderbotSmall model)"),EMe.forEach(r),oro=i(C),Wh=s(C,"LI",{});var yMe=n(Wh);$Y=s(yMe,"STRONG",{});var _jt=n($Y);tro=t(_jt,"camembert"),_jt.forEach(r),rro=t(yMe," \u2014 "),TR=s(yMe,"A",{href:!0});var bjt=n(TR);aro=t(bjt,"CamembertModel"),bjt.forEach(r),sro=t(yMe," (CamemBERT model)"),yMe.forEach(r),nro=i(C),Qh=s(C,"LI",{});var wMe=n(Qh);IY=s(wMe,"STRONG",{});var vjt=n(IY);lro=t(vjt,"canine"),vjt.forEach(r),iro=t(wMe," \u2014 "),FR=s(wMe,"A",{href:!0});var Tjt=n(FR);dro=t(Tjt,"CanineModel"),Tjt.forEach(r),cro=t(wMe," (Canine model)"),wMe.forEach(r),mro=i(C),Hh=s(C,"LI",{});var AMe=n(Hh);jY=s(AMe,"STRONG",{});var Fjt=n(jY);fro=t(Fjt,"clip"),Fjt.forEach(r),gro=t(AMe," \u2014 "),CR=s(AMe,"A",{href:!0});var Cjt=n(CR);hro=t(Cjt,"CLIPModel"),Cjt.forEach(r),uro=t(AMe," (CLIP model)"),AMe.forEach(r),pro=i(C),Uh=s(C,"LI",{});var LMe=n(Uh);DY=s(LMe,"STRONG",{});var Mjt=n(DY);_ro=t(Mjt,"convbert"),Mjt.forEach(r),bro=t(LMe," \u2014 "),MR=s(LMe,"A",{href:!0});var Ejt=n(MR);vro=t(Ejt,"ConvBertModel"),Ejt.forEach(r),Tro=t(LMe," (ConvBERT model)"),LMe.forEach(r),Fro=i(C),Jh=s(C,"LI",{});var BMe=n(Jh);NY=s(BMe,"STRONG",{});var yjt=n(NY);Cro=t(yjt,"convnext"),yjt.forEach(r),Mro=t(BMe," \u2014 "),ER=s(BMe,"A",{href:!0});var wjt=n(ER);Ero=t(wjt,"ConvNextModel"),wjt.forEach(r),yro=t(BMe," (ConvNext model)"),BMe.forEach(r),wro=i(C),Yh=s(C,"LI",{});var xMe=n(Yh);qY=s(xMe,"STRONG",{});var Ajt=n(qY);Aro=t(Ajt,"ctrl"),Ajt.forEach(r),Lro=t(xMe," \u2014 "),yR=s(xMe,"A",{href:!0});var Ljt=n(yR);Bro=t(Ljt,"CTRLModel"),Ljt.forEach(r),xro=t(xMe," (CTRL model)"),xMe.forEach(r),kro=i(C),Kh=s(C,"LI",{});var kMe=n(Kh);OY=s(kMe,"STRONG",{});var Bjt=n(OY);Rro=t(Bjt,"data2vec-audio"),Bjt.forEach(r),Sro=t(kMe," \u2014 "),wR=s(kMe,"A",{href:!0});var xjt=n(wR);Pro=t(xjt,"Data2VecAudioModel"),xjt.forEach(r),$ro=t(kMe," (Data2VecAudio model)"),kMe.forEach(r),Iro=i(C),Zh=s(C,"LI",{});var RMe=n(Zh);GY=s(RMe,"STRONG",{});var kjt=n(GY);jro=t(kjt,"data2vec-text"),kjt.forEach(r),Dro=t(RMe," \u2014 "),AR=s(RMe,"A",{href:!0});var Rjt=n(AR);Nro=t(Rjt,"Data2VecTextModel"),Rjt.forEach(r),qro=t(RMe," (Data2VecText model)"),RMe.forEach(r),Oro=i(C),eu=s(C,"LI",{});var SMe=n(eu);XY=s(SMe,"STRONG",{});var Sjt=n(XY);Gro=t(Sjt,"deberta"),Sjt.forEach(r),Xro=t(SMe," \u2014 "),LR=s(SMe,"A",{href:!0});var Pjt=n(LR);Vro=t(Pjt,"DebertaModel"),Pjt.forEach(r),zro=t(SMe," (DeBERTa model)"),SMe.forEach(r),Wro=i(C),ou=s(C,"LI",{});var PMe=n(ou);VY=s(PMe,"STRONG",{});var $jt=n(VY);Qro=t($jt,"deberta-v2"),$jt.forEach(r),Hro=t(PMe," \u2014 "),BR=s(PMe,"A",{href:!0});var Ijt=n(BR);Uro=t(Ijt,"DebertaV2Model"),Ijt.forEach(r),Jro=t(PMe," (DeBERTa-v2 model)"),PMe.forEach(r),Yro=i(C),tu=s(C,"LI",{});var $Me=n(tu);zY=s($Me,"STRONG",{});var jjt=n(zY);Kro=t(jjt,"deit"),jjt.forEach(r),Zro=t($Me," \u2014 "),xR=s($Me,"A",{href:!0});var Djt=n(xR);eao=t(Djt,"DeiTModel"),Djt.forEach(r),oao=t($Me," (DeiT model)"),$Me.forEach(r),tao=i(C),ru=s(C,"LI",{});var IMe=n(ru);WY=s(IMe,"STRONG",{});var Njt=n(WY);rao=t(Njt,"detr"),Njt.forEach(r),aao=t(IMe," \u2014 "),kR=s(IMe,"A",{href:!0});var qjt=n(kR);sao=t(qjt,"DetrModel"),qjt.forEach(r),nao=t(IMe," (DETR model)"),IMe.forEach(r),lao=i(C),au=s(C,"LI",{});var jMe=n(au);QY=s(jMe,"STRONG",{});var Ojt=n(QY);iao=t(Ojt,"distilbert"),Ojt.forEach(r),dao=t(jMe," \u2014 "),RR=s(jMe,"A",{href:!0});var Gjt=n(RR);cao=t(Gjt,"DistilBertModel"),Gjt.forEach(r),mao=t(jMe," (DistilBERT model)"),jMe.forEach(r),fao=i(C),su=s(C,"LI",{});var DMe=n(su);HY=s(DMe,"STRONG",{});var Xjt=n(HY);gao=t(Xjt,"dpr"),Xjt.forEach(r),hao=t(DMe," \u2014 "),SR=s(DMe,"A",{href:!0});var Vjt=n(SR);uao=t(Vjt,"DPRQuestionEncoder"),Vjt.forEach(r),pao=t(DMe," (DPR model)"),DMe.forEach(r),_ao=i(C),nu=s(C,"LI",{});var NMe=n(nu);UY=s(NMe,"STRONG",{});var zjt=n(UY);bao=t(zjt,"electra"),zjt.forEach(r),vao=t(NMe," \u2014 "),PR=s(NMe,"A",{href:!0});var Wjt=n(PR);Tao=t(Wjt,"ElectraModel"),Wjt.forEach(r),Fao=t(NMe," (ELECTRA model)"),NMe.forEach(r),Cao=i(C),lu=s(C,"LI",{});var qMe=n(lu);JY=s(qMe,"STRONG",{});var Qjt=n(JY);Mao=t(Qjt,"flaubert"),Qjt.forEach(r),Eao=t(qMe," \u2014 "),$R=s(qMe,"A",{href:!0});var Hjt=n($R);yao=t(Hjt,"FlaubertModel"),Hjt.forEach(r),wao=t(qMe," (FlauBERT model)"),qMe.forEach(r),Aao=i(C),iu=s(C,"LI",{});var OMe=n(iu);YY=s(OMe,"STRONG",{});var Ujt=n(YY);Lao=t(Ujt,"fnet"),Ujt.forEach(r),Bao=t(OMe," \u2014 "),IR=s(OMe,"A",{href:!0});var Jjt=n(IR);xao=t(Jjt,"FNetModel"),Jjt.forEach(r),kao=t(OMe," (FNet model)"),OMe.forEach(r),Rao=i(C),du=s(C,"LI",{});var GMe=n(du);KY=s(GMe,"STRONG",{});var Yjt=n(KY);Sao=t(Yjt,"fsmt"),Yjt.forEach(r),Pao=t(GMe," \u2014 "),jR=s(GMe,"A",{href:!0});var Kjt=n(jR);$ao=t(Kjt,"FSMTModel"),Kjt.forEach(r),Iao=t(GMe," (FairSeq Machine-Translation model)"),GMe.forEach(r),jao=i(C),Nn=s(C,"LI",{});var L7=n(Nn);ZY=s(L7,"STRONG",{});var Zjt=n(ZY);Dao=t(Zjt,"funnel"),Zjt.forEach(r),Nao=t(L7," \u2014 "),DR=s(L7,"A",{href:!0});var eDt=n(DR);qao=t(eDt,"FunnelModel"),eDt.forEach(r),Oao=t(L7," or "),NR=s(L7,"A",{href:!0});var oDt=n(NR);Gao=t(oDt,"FunnelBaseModel"),oDt.forEach(r),Xao=t(L7," (Funnel Transformer model)"),L7.forEach(r),Vao=i(C),cu=s(C,"LI",{});var XMe=n(cu);eK=s(XMe,"STRONG",{});var tDt=n(eK);zao=t(tDt,"gpt2"),tDt.forEach(r),Wao=t(XMe," \u2014 "),qR=s(XMe,"A",{href:!0});var rDt=n(qR);Qao=t(rDt,"GPT2Model"),rDt.forEach(r),Hao=t(XMe," (OpenAI GPT-2 model)"),XMe.forEach(r),Uao=i(C),mu=s(C,"LI",{});var VMe=n(mu);oK=s(VMe,"STRONG",{});var aDt=n(oK);Jao=t(aDt,"gpt_neo"),aDt.forEach(r),Yao=t(VMe," \u2014 "),OR=s(VMe,"A",{href:!0});var sDt=n(OR);Kao=t(sDt,"GPTNeoModel"),sDt.forEach(r),Zao=t(VMe," (GPT Neo model)"),VMe.forEach(r),eso=i(C),fu=s(C,"LI",{});var zMe=n(fu);tK=s(zMe,"STRONG",{});var nDt=n(tK);oso=t(nDt,"gptj"),nDt.forEach(r),tso=t(zMe," \u2014 "),GR=s(zMe,"A",{href:!0});var lDt=n(GR);rso=t(lDt,"GPTJModel"),lDt.forEach(r),aso=t(zMe," (GPT-J model)"),zMe.forEach(r),sso=i(C),gu=s(C,"LI",{});var WMe=n(gu);rK=s(WMe,"STRONG",{});var iDt=n(rK);nso=t(iDt,"hubert"),iDt.forEach(r),lso=t(WMe," \u2014 "),XR=s(WMe,"A",{href:!0});var dDt=n(XR);iso=t(dDt,"HubertModel"),dDt.forEach(r),dso=t(WMe," (Hubert model)"),WMe.forEach(r),cso=i(C),hu=s(C,"LI",{});var QMe=n(hu);aK=s(QMe,"STRONG",{});var cDt=n(aK);mso=t(cDt,"ibert"),cDt.forEach(r),fso=t(QMe," \u2014 "),VR=s(QMe,"A",{href:!0});var mDt=n(VR);gso=t(mDt,"IBertModel"),mDt.forEach(r),hso=t(QMe," (I-BERT model)"),QMe.forEach(r),uso=i(C),uu=s(C,"LI",{});var HMe=n(uu);sK=s(HMe,"STRONG",{});var fDt=n(sK);pso=t(fDt,"imagegpt"),fDt.forEach(r),_so=t(HMe," \u2014 "),zR=s(HMe,"A",{href:!0});var gDt=n(zR);bso=t(gDt,"ImageGPTModel"),gDt.forEach(r),vso=t(HMe," (ImageGPT model)"),HMe.forEach(r),Tso=i(C),pu=s(C,"LI",{});var UMe=n(pu);nK=s(UMe,"STRONG",{});var hDt=n(nK);Fso=t(hDt,"layoutlm"),hDt.forEach(r),Cso=t(UMe," \u2014 "),WR=s(UMe,"A",{href:!0});var uDt=n(WR);Mso=t(uDt,"LayoutLMModel"),uDt.forEach(r),Eso=t(UMe," (LayoutLM model)"),UMe.forEach(r),yso=i(C),_u=s(C,"LI",{});var JMe=n(_u);lK=s(JMe,"STRONG",{});var pDt=n(lK);wso=t(pDt,"layoutlmv2"),pDt.forEach(r),Aso=t(JMe," \u2014 "),QR=s(JMe,"A",{href:!0});var _Dt=n(QR);Lso=t(_Dt,"LayoutLMv2Model"),_Dt.forEach(r),Bso=t(JMe," (LayoutLMv2 model)"),JMe.forEach(r),xso=i(C),bu=s(C,"LI",{});var YMe=n(bu);iK=s(YMe,"STRONG",{});var bDt=n(iK);kso=t(bDt,"led"),bDt.forEach(r),Rso=t(YMe," \u2014 "),HR=s(YMe,"A",{href:!0});var vDt=n(HR);Sso=t(vDt,"LEDModel"),vDt.forEach(r),Pso=t(YMe," (LED model)"),YMe.forEach(r),$so=i(C),vu=s(C,"LI",{});var KMe=n(vu);dK=s(KMe,"STRONG",{});var TDt=n(dK);Iso=t(TDt,"longformer"),TDt.forEach(r),jso=t(KMe," \u2014 "),UR=s(KMe,"A",{href:!0});var FDt=n(UR);Dso=t(FDt,"LongformerModel"),FDt.forEach(r),Nso=t(KMe," (Longformer model)"),KMe.forEach(r),qso=i(C),Tu=s(C,"LI",{});var ZMe=n(Tu);cK=s(ZMe,"STRONG",{});var CDt=n(cK);Oso=t(CDt,"luke"),CDt.forEach(r),Gso=t(ZMe," \u2014 "),JR=s(ZMe,"A",{href:!0});var MDt=n(JR);Xso=t(MDt,"LukeModel"),MDt.forEach(r),Vso=t(ZMe," (LUKE model)"),ZMe.forEach(r),zso=i(C),Fu=s(C,"LI",{});var eEe=n(Fu);mK=s(eEe,"STRONG",{});var EDt=n(mK);Wso=t(EDt,"lxmert"),EDt.forEach(r),Qso=t(eEe," \u2014 "),YR=s(eEe,"A",{href:!0});var yDt=n(YR);Hso=t(yDt,"LxmertModel"),yDt.forEach(r),Uso=t(eEe," (LXMERT model)"),eEe.forEach(r),Jso=i(C),Cu=s(C,"LI",{});var oEe=n(Cu);fK=s(oEe,"STRONG",{});var wDt=n(fK);Yso=t(wDt,"m2m_100"),wDt.forEach(r),Kso=t(oEe," \u2014 "),KR=s(oEe,"A",{href:!0});var ADt=n(KR);Zso=t(ADt,"M2M100Model"),ADt.forEach(r),eno=t(oEe," (M2M100 model)"),oEe.forEach(r),ono=i(C),Mu=s(C,"LI",{});var tEe=n(Mu);gK=s(tEe,"STRONG",{});var LDt=n(gK);tno=t(LDt,"marian"),LDt.forEach(r),rno=t(tEe," \u2014 "),ZR=s(tEe,"A",{href:!0});var BDt=n(ZR);ano=t(BDt,"MarianModel"),BDt.forEach(r),sno=t(tEe," (Marian model)"),tEe.forEach(r),nno=i(C),Eu=s(C,"LI",{});var rEe=n(Eu);hK=s(rEe,"STRONG",{});var xDt=n(hK);lno=t(xDt,"maskformer"),xDt.forEach(r),ino=t(rEe," \u2014 "),eS=s(rEe,"A",{href:!0});var kDt=n(eS);dno=t(kDt,"MaskFormerModel"),kDt.forEach(r),cno=t(rEe," (MaskFormer model)"),rEe.forEach(r),mno=i(C),yu=s(C,"LI",{});var aEe=n(yu);uK=s(aEe,"STRONG",{});var RDt=n(uK);fno=t(RDt,"mbart"),RDt.forEach(r),gno=t(aEe," \u2014 "),oS=s(aEe,"A",{href:!0});var SDt=n(oS);hno=t(SDt,"MBartModel"),SDt.forEach(r),uno=t(aEe," (mBART model)"),aEe.forEach(r),pno=i(C),wu=s(C,"LI",{});var sEe=n(wu);pK=s(sEe,"STRONG",{});var PDt=n(pK);_no=t(PDt,"megatron-bert"),PDt.forEach(r),bno=t(sEe," \u2014 "),tS=s(sEe,"A",{href:!0});var $Dt=n(tS);vno=t($Dt,"MegatronBertModel"),$Dt.forEach(r),Tno=t(sEe," (MegatronBert model)"),sEe.forEach(r),Fno=i(C),Au=s(C,"LI",{});var nEe=n(Au);_K=s(nEe,"STRONG",{});var IDt=n(_K);Cno=t(IDt,"mobilebert"),IDt.forEach(r),Mno=t(nEe," \u2014 "),rS=s(nEe,"A",{href:!0});var jDt=n(rS);Eno=t(jDt,"MobileBertModel"),jDt.forEach(r),yno=t(nEe," (MobileBERT model)"),nEe.forEach(r),wno=i(C),Lu=s(C,"LI",{});var lEe=n(Lu);bK=s(lEe,"STRONG",{});var DDt=n(bK);Ano=t(DDt,"mpnet"),DDt.forEach(r),Lno=t(lEe," \u2014 "),aS=s(lEe,"A",{href:!0});var NDt=n(aS);Bno=t(NDt,"MPNetModel"),NDt.forEach(r),xno=t(lEe," (MPNet model)"),lEe.forEach(r),kno=i(C),Bu=s(C,"LI",{});var iEe=n(Bu);vK=s(iEe,"STRONG",{});var qDt=n(vK);Rno=t(qDt,"mt5"),qDt.forEach(r),Sno=t(iEe," \u2014 "),sS=s(iEe,"A",{href:!0});var ODt=n(sS);Pno=t(ODt,"MT5Model"),ODt.forEach(r),$no=t(iEe," (mT5 model)"),iEe.forEach(r),Ino=i(C),xu=s(C,"LI",{});var dEe=n(xu);TK=s(dEe,"STRONG",{});var GDt=n(TK);jno=t(GDt,"nystromformer"),GDt.forEach(r),Dno=t(dEe," \u2014 "),nS=s(dEe,"A",{href:!0});var XDt=n(nS);Nno=t(XDt,"NystromformerModel"),XDt.forEach(r),qno=t(dEe," (Nystromformer model)"),dEe.forEach(r),Ono=i(C),ku=s(C,"LI",{});var cEe=n(ku);FK=s(cEe,"STRONG",{});var VDt=n(FK);Gno=t(VDt,"openai-gpt"),VDt.forEach(r),Xno=t(cEe," \u2014 "),lS=s(cEe,"A",{href:!0});var zDt=n(lS);Vno=t(zDt,"OpenAIGPTModel"),zDt.forEach(r),zno=t(cEe," (OpenAI GPT model)"),cEe.forEach(r),Wno=i(C),Ru=s(C,"LI",{});var mEe=n(Ru);CK=s(mEe,"STRONG",{});var WDt=n(CK);Qno=t(WDt,"pegasus"),WDt.forEach(r),Hno=t(mEe," \u2014 "),iS=s(mEe,"A",{href:!0});var QDt=n(iS);Uno=t(QDt,"PegasusModel"),QDt.forEach(r),Jno=t(mEe," (Pegasus model)"),mEe.forEach(r),Yno=i(C),Su=s(C,"LI",{});var fEe=n(Su);MK=s(fEe,"STRONG",{});var HDt=n(MK);Kno=t(HDt,"perceiver"),HDt.forEach(r),Zno=t(fEe," \u2014 "),dS=s(fEe,"A",{href:!0});var UDt=n(dS);elo=t(UDt,"PerceiverModel"),UDt.forEach(r),olo=t(fEe," (Perceiver model)"),fEe.forEach(r),tlo=i(C),Pu=s(C,"LI",{});var gEe=n(Pu);EK=s(gEe,"STRONG",{});var JDt=n(EK);rlo=t(JDt,"plbart"),JDt.forEach(r),alo=t(gEe," \u2014 "),cS=s(gEe,"A",{href:!0});var YDt=n(cS);slo=t(YDt,"PLBartModel"),YDt.forEach(r),nlo=t(gEe," (PLBart model)"),gEe.forEach(r),llo=i(C),$u=s(C,"LI",{});var hEe=n($u);yK=s(hEe,"STRONG",{});var KDt=n(yK);ilo=t(KDt,"poolformer"),KDt.forEach(r),dlo=t(hEe," \u2014 "),mS=s(hEe,"A",{href:!0});var ZDt=n(mS);clo=t(ZDt,"PoolFormerModel"),ZDt.forEach(r),mlo=t(hEe," (PoolFormer model)"),hEe.forEach(r),flo=i(C),Iu=s(C,"LI",{});var uEe=n(Iu);wK=s(uEe,"STRONG",{});var eNt=n(wK);glo=t(eNt,"prophetnet"),eNt.forEach(r),hlo=t(uEe," \u2014 "),fS=s(uEe,"A",{href:!0});var oNt=n(fS);ulo=t(oNt,"ProphetNetModel"),oNt.forEach(r),plo=t(uEe," (ProphetNet model)"),uEe.forEach(r),_lo=i(C),ju=s(C,"LI",{});var pEe=n(ju);AK=s(pEe,"STRONG",{});var tNt=n(AK);blo=t(tNt,"qdqbert"),tNt.forEach(r),vlo=t(pEe," \u2014 "),gS=s(pEe,"A",{href:!0});var rNt=n(gS);Tlo=t(rNt,"QDQBertModel"),rNt.forEach(r),Flo=t(pEe," (QDQBert model)"),pEe.forEach(r),Clo=i(C),Du=s(C,"LI",{});var _Ee=n(Du);LK=s(_Ee,"STRONG",{});var aNt=n(LK);Mlo=t(aNt,"reformer"),aNt.forEach(r),Elo=t(_Ee," \u2014 "),hS=s(_Ee,"A",{href:!0});var sNt=n(hS);ylo=t(sNt,"ReformerModel"),sNt.forEach(r),wlo=t(_Ee," (Reformer model)"),_Ee.forEach(r),Alo=i(C),Nu=s(C,"LI",{});var bEe=n(Nu);BK=s(bEe,"STRONG",{});var nNt=n(BK);Llo=t(nNt,"rembert"),nNt.forEach(r),Blo=t(bEe," \u2014 "),uS=s(bEe,"A",{href:!0});var lNt=n(uS);xlo=t(lNt,"RemBertModel"),lNt.forEach(r),klo=t(bEe," (RemBERT model)"),bEe.forEach(r),Rlo=i(C),qu=s(C,"LI",{});var vEe=n(qu);xK=s(vEe,"STRONG",{});var iNt=n(xK);Slo=t(iNt,"retribert"),iNt.forEach(r),Plo=t(vEe," \u2014 "),pS=s(vEe,"A",{href:!0});var dNt=n(pS);$lo=t(dNt,"RetriBertModel"),dNt.forEach(r),Ilo=t(vEe," (RetriBERT model)"),vEe.forEach(r),jlo=i(C),Ou=s(C,"LI",{});var TEe=n(Ou);kK=s(TEe,"STRONG",{});var cNt=n(kK);Dlo=t(cNt,"roberta"),cNt.forEach(r),Nlo=t(TEe," \u2014 "),_S=s(TEe,"A",{href:!0});var mNt=n(_S);qlo=t(mNt,"RobertaModel"),mNt.forEach(r),Olo=t(TEe," (RoBERTa model)"),TEe.forEach(r),Glo=i(C),Gu=s(C,"LI",{});var FEe=n(Gu);RK=s(FEe,"STRONG",{});var fNt=n(RK);Xlo=t(fNt,"roformer"),fNt.forEach(r),Vlo=t(FEe," \u2014 "),bS=s(FEe,"A",{href:!0});var gNt=n(bS);zlo=t(gNt,"RoFormerModel"),gNt.forEach(r),Wlo=t(FEe," (RoFormer model)"),FEe.forEach(r),Qlo=i(C),Xu=s(C,"LI",{});var CEe=n(Xu);SK=s(CEe,"STRONG",{});var hNt=n(SK);Hlo=t(hNt,"segformer"),hNt.forEach(r),Ulo=t(CEe," \u2014 "),vS=s(CEe,"A",{href:!0});var uNt=n(vS);Jlo=t(uNt,"SegformerModel"),uNt.forEach(r),Ylo=t(CEe," (SegFormer model)"),CEe.forEach(r),Klo=i(C),Vu=s(C,"LI",{});var MEe=n(Vu);PK=s(MEe,"STRONG",{});var pNt=n(PK);Zlo=t(pNt,"sew"),pNt.forEach(r),eio=t(MEe," \u2014 "),TS=s(MEe,"A",{href:!0});var _Nt=n(TS);oio=t(_Nt,"SEWModel"),_Nt.forEach(r),tio=t(MEe," (SEW model)"),MEe.forEach(r),rio=i(C),zu=s(C,"LI",{});var EEe=n(zu);$K=s(EEe,"STRONG",{});var bNt=n($K);aio=t(bNt,"sew-d"),bNt.forEach(r),sio=t(EEe," \u2014 "),FS=s(EEe,"A",{href:!0});var vNt=n(FS);nio=t(vNt,"SEWDModel"),vNt.forEach(r),lio=t(EEe," (SEW-D model)"),EEe.forEach(r),iio=i(C),Wu=s(C,"LI",{});var yEe=n(Wu);IK=s(yEe,"STRONG",{});var TNt=n(IK);dio=t(TNt,"speech_to_text"),TNt.forEach(r),cio=t(yEe," \u2014 "),CS=s(yEe,"A",{href:!0});var FNt=n(CS);mio=t(FNt,"Speech2TextModel"),FNt.forEach(r),fio=t(yEe," (Speech2Text model)"),yEe.forEach(r),gio=i(C),Qu=s(C,"LI",{});var wEe=n(Qu);jK=s(wEe,"STRONG",{});var CNt=n(jK);hio=t(CNt,"splinter"),CNt.forEach(r),uio=t(wEe," \u2014 "),MS=s(wEe,"A",{href:!0});var MNt=n(MS);pio=t(MNt,"SplinterModel"),MNt.forEach(r),_io=t(wEe," (Splinter model)"),wEe.forEach(r),bio=i(C),Hu=s(C,"LI",{});var AEe=n(Hu);DK=s(AEe,"STRONG",{});var ENt=n(DK);vio=t(ENt,"squeezebert"),ENt.forEach(r),Tio=t(AEe," \u2014 "),ES=s(AEe,"A",{href:!0});var yNt=n(ES);Fio=t(yNt,"SqueezeBertModel"),yNt.forEach(r),Cio=t(AEe," (SqueezeBERT model)"),AEe.forEach(r),Mio=i(C),Uu=s(C,"LI",{});var LEe=n(Uu);NK=s(LEe,"STRONG",{});var wNt=n(NK);Eio=t(wNt,"swin"),wNt.forEach(r),yio=t(LEe," \u2014 "),yS=s(LEe,"A",{href:!0});var ANt=n(yS);wio=t(ANt,"SwinModel"),ANt.forEach(r),Aio=t(LEe," (Swin model)"),LEe.forEach(r),Lio=i(C),Ju=s(C,"LI",{});var BEe=n(Ju);qK=s(BEe,"STRONG",{});var LNt=n(qK);Bio=t(LNt,"t5"),LNt.forEach(r),xio=t(BEe," \u2014 "),wS=s(BEe,"A",{href:!0});var BNt=n(wS);kio=t(BNt,"T5Model"),BNt.forEach(r),Rio=t(BEe," (T5 model)"),BEe.forEach(r),Sio=i(C),Yu=s(C,"LI",{});var xEe=n(Yu);OK=s(xEe,"STRONG",{});var xNt=n(OK);Pio=t(xNt,"tapas"),xNt.forEach(r),$io=t(xEe," \u2014 "),AS=s(xEe,"A",{href:!0});var kNt=n(AS);Iio=t(kNt,"TapasModel"),kNt.forEach(r),jio=t(xEe," (TAPAS model)"),xEe.forEach(r),Dio=i(C),Ku=s(C,"LI",{});var kEe=n(Ku);GK=s(kEe,"STRONG",{});var RNt=n(GK);Nio=t(RNt,"transfo-xl"),RNt.forEach(r),qio=t(kEe," \u2014 "),LS=s(kEe,"A",{href:!0});var SNt=n(LS);Oio=t(SNt,"TransfoXLModel"),SNt.forEach(r),Gio=t(kEe," (Transformer-XL model)"),kEe.forEach(r),Xio=i(C),Zu=s(C,"LI",{});var REe=n(Zu);XK=s(REe,"STRONG",{});var PNt=n(XK);Vio=t(PNt,"unispeech"),PNt.forEach(r),zio=t(REe," \u2014 "),BS=s(REe,"A",{href:!0});var $Nt=n(BS);Wio=t($Nt,"UniSpeechModel"),$Nt.forEach(r),Qio=t(REe," (UniSpeech model)"),REe.forEach(r),Hio=i(C),ep=s(C,"LI",{});var SEe=n(ep);VK=s(SEe,"STRONG",{});var INt=n(VK);Uio=t(INt,"unispeech-sat"),INt.forEach(r),Jio=t(SEe," \u2014 "),xS=s(SEe,"A",{href:!0});var jNt=n(xS);Yio=t(jNt,"UniSpeechSatModel"),jNt.forEach(r),Kio=t(SEe," (UniSpeechSat model)"),SEe.forEach(r),Zio=i(C),op=s(C,"LI",{});var PEe=n(op);zK=s(PEe,"STRONG",{});var DNt=n(zK);edo=t(DNt,"vilt"),DNt.forEach(r),odo=t(PEe," \u2014 "),kS=s(PEe,"A",{href:!0});var NNt=n(kS);tdo=t(NNt,"ViltModel"),NNt.forEach(r),rdo=t(PEe," (ViLT model)"),PEe.forEach(r),ado=i(C),tp=s(C,"LI",{});var $Ee=n(tp);WK=s($Ee,"STRONG",{});var qNt=n(WK);sdo=t(qNt,"vision-text-dual-encoder"),qNt.forEach(r),ndo=t($Ee," \u2014 "),RS=s($Ee,"A",{href:!0});var ONt=n(RS);ldo=t(ONt,"VisionTextDualEncoderModel"),ONt.forEach(r),ido=t($Ee," (VisionTextDualEncoder model)"),$Ee.forEach(r),ddo=i(C),rp=s(C,"LI",{});var IEe=n(rp);QK=s(IEe,"STRONG",{});var GNt=n(QK);cdo=t(GNt,"visual_bert"),GNt.forEach(r),mdo=t(IEe," \u2014 "),SS=s(IEe,"A",{href:!0});var XNt=n(SS);fdo=t(XNt,"VisualBertModel"),XNt.forEach(r),gdo=t(IEe," (VisualBert model)"),IEe.forEach(r),hdo=i(C),ap=s(C,"LI",{});var jEe=n(ap);HK=s(jEe,"STRONG",{});var VNt=n(HK);udo=t(VNt,"vit"),VNt.forEach(r),pdo=t(jEe," \u2014 "),PS=s(jEe,"A",{href:!0});var zNt=n(PS);_do=t(zNt,"ViTModel"),zNt.forEach(r),bdo=t(jEe," (ViT model)"),jEe.forEach(r),vdo=i(C),sp=s(C,"LI",{});var DEe=n(sp);UK=s(DEe,"STRONG",{});var WNt=n(UK);Tdo=t(WNt,"vit_mae"),WNt.forEach(r),Fdo=t(DEe," \u2014 "),$S=s(DEe,"A",{href:!0});var QNt=n($S);Cdo=t(QNt,"ViTMAEModel"),QNt.forEach(r),Mdo=t(DEe," (ViTMAE model)"),DEe.forEach(r),Edo=i(C),np=s(C,"LI",{});var NEe=n(np);JK=s(NEe,"STRONG",{});var HNt=n(JK);ydo=t(HNt,"wav2vec2"),HNt.forEach(r),wdo=t(NEe," \u2014 "),IS=s(NEe,"A",{href:!0});var UNt=n(IS);Ado=t(UNt,"Wav2Vec2Model"),UNt.forEach(r),Ldo=t(NEe," (Wav2Vec2 model)"),NEe.forEach(r),Bdo=i(C),lp=s(C,"LI",{});var qEe=n(lp);YK=s(qEe,"STRONG",{});var JNt=n(YK);xdo=t(JNt,"wavlm"),JNt.forEach(r),kdo=t(qEe," \u2014 "),jS=s(qEe,"A",{href:!0});var YNt=n(jS);Rdo=t(YNt,"WavLMModel"),YNt.forEach(r),Sdo=t(qEe," (WavLM model)"),qEe.forEach(r),Pdo=i(C),ip=s(C,"LI",{});var OEe=n(ip);KK=s(OEe,"STRONG",{});var KNt=n(KK);$do=t(KNt,"xglm"),KNt.forEach(r),Ido=t(OEe," \u2014 "),DS=s(OEe,"A",{href:!0});var ZNt=n(DS);jdo=t(ZNt,"XGLMModel"),ZNt.forEach(r),Ddo=t(OEe," (XGLM model)"),OEe.forEach(r),Ndo=i(C),dp=s(C,"LI",{});var GEe=n(dp);ZK=s(GEe,"STRONG",{});var eqt=n(ZK);qdo=t(eqt,"xlm"),eqt.forEach(r),Odo=t(GEe," \u2014 "),NS=s(GEe,"A",{href:!0});var oqt=n(NS);Gdo=t(oqt,"XLMModel"),oqt.forEach(r),Xdo=t(GEe," (XLM model)"),GEe.forEach(r),Vdo=i(C),cp=s(C,"LI",{});var XEe=n(cp);eZ=s(XEe,"STRONG",{});var tqt=n(eZ);zdo=t(tqt,"xlm-prophetnet"),tqt.forEach(r),Wdo=t(XEe," \u2014 "),qS=s(XEe,"A",{href:!0});var rqt=n(qS);Qdo=t(rqt,"XLMProphetNetModel"),rqt.forEach(r),Hdo=t(XEe," (XLMProphetNet model)"),XEe.forEach(r),Udo=i(C),mp=s(C,"LI",{});var VEe=n(mp);oZ=s(VEe,"STRONG",{});var aqt=n(oZ);Jdo=t(aqt,"xlm-roberta"),aqt.forEach(r),Ydo=t(VEe," \u2014 "),OS=s(VEe,"A",{href:!0});var sqt=n(OS);Kdo=t(sqt,"XLMRobertaModel"),sqt.forEach(r),Zdo=t(VEe," (XLM-RoBERTa model)"),VEe.forEach(r),eco=i(C),fp=s(C,"LI",{});var zEe=n(fp);tZ=s(zEe,"STRONG",{});var nqt=n(tZ);oco=t(nqt,"xlm-roberta-xl"),nqt.forEach(r),tco=t(zEe," \u2014 "),GS=s(zEe,"A",{href:!0});var lqt=n(GS);rco=t(lqt,"XLMRobertaXLModel"),lqt.forEach(r),aco=t(zEe," (XLM-RoBERTa-XL model)"),zEe.forEach(r),sco=i(C),gp=s(C,"LI",{});var WEe=n(gp);rZ=s(WEe,"STRONG",{});var iqt=n(rZ);nco=t(iqt,"xlnet"),iqt.forEach(r),lco=t(WEe," \u2014 "),XS=s(WEe,"A",{href:!0});var dqt=n(XS);ico=t(dqt,"XLNetModel"),dqt.forEach(r),dco=t(WEe," (XLNet model)"),WEe.forEach(r),cco=i(C),hp=s(C,"LI",{});var QEe=n(hp);aZ=s(QEe,"STRONG",{});var cqt=n(aZ);mco=t(cqt,"yoso"),cqt.forEach(r),fco=t(QEe," \u2014 "),VS=s(QEe,"A",{href:!0});var mqt=n(VS);gco=t(mqt,"YosoModel"),mqt.forEach(r),hco=t(QEe," (YOSO model)"),QEe.forEach(r),C.forEach(r),uco=i(Dr),up=s(Dr,"P",{});var HEe=n(up);pco=t(HEe,"The model is set in evaluation mode by default using "),sZ=s(HEe,"CODE",{});var fqt=n(sZ);_co=t(fqt,"model.eval()"),fqt.forEach(r),bco=t(HEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),nZ=s(HEe,"CODE",{});var gqt=n(nZ);vco=t(gqt,"model.train()"),gqt.forEach(r),HEe.forEach(r),Tco=i(Dr),lZ=s(Dr,"P",{});var hqt=n(lZ);Fco=t(hqt,"Examples:"),hqt.forEach(r),Cco=i(Dr),f(Ey.$$.fragment,Dr),Dr.forEach(r),Wn.forEach(r),NBe=i(d),Zi=s(d,"H2",{class:!0});var Hke=n(Zi);pp=s(Hke,"A",{id:!0,class:!0,href:!0});var uqt=n(pp);iZ=s(uqt,"SPAN",{});var pqt=n(iZ);f(yy.$$.fragment,pqt),pqt.forEach(r),uqt.forEach(r),Mco=i(Hke),dZ=s(Hke,"SPAN",{});var _qt=n(dZ);Eco=t(_qt,"AutoModelForPreTraining"),_qt.forEach(r),Hke.forEach(r),qBe=i(d),Uo=s(d,"DIV",{class:!0});var Hn=n(Uo);f(wy.$$.fragment,Hn),yco=i(Hn),ed=s(Hn,"P",{});var dz=n(ed);wco=t(dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),cZ=s(dz,"CODE",{});var bqt=n(cZ);Aco=t(bqt,"from_pretrained()"),bqt.forEach(r),Lco=t(dz,"class method or the "),mZ=s(dz,"CODE",{});var vqt=n(mZ);Bco=t(vqt,"from_config()"),vqt.forEach(r),xco=t(dz,`class
method.`),dz.forEach(r),kco=i(Hn),Ay=s(Hn,"P",{});var Uke=n(Ay);Rco=t(Uke,"This class cannot be instantiated directly using "),fZ=s(Uke,"CODE",{});var Tqt=n(fZ);Sco=t(Tqt,"__init__()"),Tqt.forEach(r),Pco=t(Uke," (throws an error)."),Uke.forEach(r),$co=i(Hn),Xt=s(Hn,"DIV",{class:!0});var Un=n(Xt);f(Ly.$$.fragment,Un),Ico=i(Un),gZ=s(Un,"P",{});var Fqt=n(gZ);jco=t(Fqt,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Fqt.forEach(r),Dco=i(Un),od=s(Un,"P",{});var cz=n(od);Nco=t(cz,`Note:
Loading a model from its configuration file does `),hZ=s(cz,"STRONG",{});var Cqt=n(hZ);qco=t(Cqt,"not"),Cqt.forEach(r),Oco=t(cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),uZ=s(cz,"CODE",{});var Mqt=n(uZ);Gco=t(Mqt,"from_pretrained()"),Mqt.forEach(r),Xco=t(cz,"to load the model weights."),cz.forEach(r),Vco=i(Un),pZ=s(Un,"P",{});var Eqt=n(pZ);zco=t(Eqt,"Examples:"),Eqt.forEach(r),Wco=i(Un),f(By.$$.fragment,Un),Un.forEach(r),Qco=i(Hn),Se=s(Hn,"DIV",{class:!0});var Nr=n(Se);f(xy.$$.fragment,Nr),Hco=i(Nr),_Z=s(Nr,"P",{});var yqt=n(_Z);Uco=t(yqt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),yqt.forEach(r),Jco=i(Nr),Va=s(Nr,"P",{});var G3=n(Va);Yco=t(G3,"The model class to instantiate is selected based on the "),bZ=s(G3,"CODE",{});var wqt=n(bZ);Kco=t(wqt,"model_type"),wqt.forEach(r),Zco=t(G3,` property of the config object (either
passed as an argument or loaded from `),vZ=s(G3,"CODE",{});var Aqt=n(vZ);emo=t(Aqt,"pretrained_model_name_or_path"),Aqt.forEach(r),omo=t(G3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),TZ=s(G3,"CODE",{});var Lqt=n(TZ);tmo=t(Lqt,"pretrained_model_name_or_path"),Lqt.forEach(r),rmo=t(G3,":"),G3.forEach(r),amo=i(Nr),k=s(Nr,"UL",{});var S=n(k);_p=s(S,"LI",{});var UEe=n(_p);FZ=s(UEe,"STRONG",{});var Bqt=n(FZ);smo=t(Bqt,"albert"),Bqt.forEach(r),nmo=t(UEe," \u2014 "),zS=s(UEe,"A",{href:!0});var xqt=n(zS);lmo=t(xqt,"AlbertForPreTraining"),xqt.forEach(r),imo=t(UEe," (ALBERT model)"),UEe.forEach(r),dmo=i(S),bp=s(S,"LI",{});var JEe=n(bp);CZ=s(JEe,"STRONG",{});var kqt=n(CZ);cmo=t(kqt,"bart"),kqt.forEach(r),mmo=t(JEe," \u2014 "),WS=s(JEe,"A",{href:!0});var Rqt=n(WS);fmo=t(Rqt,"BartForConditionalGeneration"),Rqt.forEach(r),gmo=t(JEe," (BART model)"),JEe.forEach(r),hmo=i(S),vp=s(S,"LI",{});var YEe=n(vp);MZ=s(YEe,"STRONG",{});var Sqt=n(MZ);umo=t(Sqt,"bert"),Sqt.forEach(r),pmo=t(YEe," \u2014 "),QS=s(YEe,"A",{href:!0});var Pqt=n(QS);_mo=t(Pqt,"BertForPreTraining"),Pqt.forEach(r),bmo=t(YEe," (BERT model)"),YEe.forEach(r),vmo=i(S),Tp=s(S,"LI",{});var KEe=n(Tp);EZ=s(KEe,"STRONG",{});var $qt=n(EZ);Tmo=t($qt,"big_bird"),$qt.forEach(r),Fmo=t(KEe," \u2014 "),HS=s(KEe,"A",{href:!0});var Iqt=n(HS);Cmo=t(Iqt,"BigBirdForPreTraining"),Iqt.forEach(r),Mmo=t(KEe," (BigBird model)"),KEe.forEach(r),Emo=i(S),Fp=s(S,"LI",{});var ZEe=n(Fp);yZ=s(ZEe,"STRONG",{});var jqt=n(yZ);ymo=t(jqt,"camembert"),jqt.forEach(r),wmo=t(ZEe," \u2014 "),US=s(ZEe,"A",{href:!0});var Dqt=n(US);Amo=t(Dqt,"CamembertForMaskedLM"),Dqt.forEach(r),Lmo=t(ZEe," (CamemBERT model)"),ZEe.forEach(r),Bmo=i(S),Cp=s(S,"LI",{});var e3e=n(Cp);wZ=s(e3e,"STRONG",{});var Nqt=n(wZ);xmo=t(Nqt,"ctrl"),Nqt.forEach(r),kmo=t(e3e," \u2014 "),JS=s(e3e,"A",{href:!0});var qqt=n(JS);Rmo=t(qqt,"CTRLLMHeadModel"),qqt.forEach(r),Smo=t(e3e," (CTRL model)"),e3e.forEach(r),Pmo=i(S),Mp=s(S,"LI",{});var o3e=n(Mp);AZ=s(o3e,"STRONG",{});var Oqt=n(AZ);$mo=t(Oqt,"data2vec-text"),Oqt.forEach(r),Imo=t(o3e," \u2014 "),YS=s(o3e,"A",{href:!0});var Gqt=n(YS);jmo=t(Gqt,"Data2VecTextForMaskedLM"),Gqt.forEach(r),Dmo=t(o3e," (Data2VecText model)"),o3e.forEach(r),Nmo=i(S),Ep=s(S,"LI",{});var t3e=n(Ep);LZ=s(t3e,"STRONG",{});var Xqt=n(LZ);qmo=t(Xqt,"deberta"),Xqt.forEach(r),Omo=t(t3e," \u2014 "),KS=s(t3e,"A",{href:!0});var Vqt=n(KS);Gmo=t(Vqt,"DebertaForMaskedLM"),Vqt.forEach(r),Xmo=t(t3e," (DeBERTa model)"),t3e.forEach(r),Vmo=i(S),yp=s(S,"LI",{});var r3e=n(yp);BZ=s(r3e,"STRONG",{});var zqt=n(BZ);zmo=t(zqt,"deberta-v2"),zqt.forEach(r),Wmo=t(r3e," \u2014 "),ZS=s(r3e,"A",{href:!0});var Wqt=n(ZS);Qmo=t(Wqt,"DebertaV2ForMaskedLM"),Wqt.forEach(r),Hmo=t(r3e," (DeBERTa-v2 model)"),r3e.forEach(r),Umo=i(S),wp=s(S,"LI",{});var a3e=n(wp);xZ=s(a3e,"STRONG",{});var Qqt=n(xZ);Jmo=t(Qqt,"distilbert"),Qqt.forEach(r),Ymo=t(a3e," \u2014 "),eP=s(a3e,"A",{href:!0});var Hqt=n(eP);Kmo=t(Hqt,"DistilBertForMaskedLM"),Hqt.forEach(r),Zmo=t(a3e," (DistilBERT model)"),a3e.forEach(r),efo=i(S),Ap=s(S,"LI",{});var s3e=n(Ap);kZ=s(s3e,"STRONG",{});var Uqt=n(kZ);ofo=t(Uqt,"electra"),Uqt.forEach(r),tfo=t(s3e," \u2014 "),oP=s(s3e,"A",{href:!0});var Jqt=n(oP);rfo=t(Jqt,"ElectraForPreTraining"),Jqt.forEach(r),afo=t(s3e," (ELECTRA model)"),s3e.forEach(r),sfo=i(S),Lp=s(S,"LI",{});var n3e=n(Lp);RZ=s(n3e,"STRONG",{});var Yqt=n(RZ);nfo=t(Yqt,"flaubert"),Yqt.forEach(r),lfo=t(n3e," \u2014 "),tP=s(n3e,"A",{href:!0});var Kqt=n(tP);ifo=t(Kqt,"FlaubertWithLMHeadModel"),Kqt.forEach(r),dfo=t(n3e," (FlauBERT model)"),n3e.forEach(r),cfo=i(S),Bp=s(S,"LI",{});var l3e=n(Bp);SZ=s(l3e,"STRONG",{});var Zqt=n(SZ);mfo=t(Zqt,"fnet"),Zqt.forEach(r),ffo=t(l3e," \u2014 "),rP=s(l3e,"A",{href:!0});var eOt=n(rP);gfo=t(eOt,"FNetForPreTraining"),eOt.forEach(r),hfo=t(l3e," (FNet model)"),l3e.forEach(r),ufo=i(S),xp=s(S,"LI",{});var i3e=n(xp);PZ=s(i3e,"STRONG",{});var oOt=n(PZ);pfo=t(oOt,"fsmt"),oOt.forEach(r),_fo=t(i3e," \u2014 "),aP=s(i3e,"A",{href:!0});var tOt=n(aP);bfo=t(tOt,"FSMTForConditionalGeneration"),tOt.forEach(r),vfo=t(i3e," (FairSeq Machine-Translation model)"),i3e.forEach(r),Tfo=i(S),kp=s(S,"LI",{});var d3e=n(kp);$Z=s(d3e,"STRONG",{});var rOt=n($Z);Ffo=t(rOt,"funnel"),rOt.forEach(r),Cfo=t(d3e," \u2014 "),sP=s(d3e,"A",{href:!0});var aOt=n(sP);Mfo=t(aOt,"FunnelForPreTraining"),aOt.forEach(r),Efo=t(d3e," (Funnel Transformer model)"),d3e.forEach(r),yfo=i(S),Rp=s(S,"LI",{});var c3e=n(Rp);IZ=s(c3e,"STRONG",{});var sOt=n(IZ);wfo=t(sOt,"gpt2"),sOt.forEach(r),Afo=t(c3e," \u2014 "),nP=s(c3e,"A",{href:!0});var nOt=n(nP);Lfo=t(nOt,"GPT2LMHeadModel"),nOt.forEach(r),Bfo=t(c3e," (OpenAI GPT-2 model)"),c3e.forEach(r),xfo=i(S),Sp=s(S,"LI",{});var m3e=n(Sp);jZ=s(m3e,"STRONG",{});var lOt=n(jZ);kfo=t(lOt,"ibert"),lOt.forEach(r),Rfo=t(m3e," \u2014 "),lP=s(m3e,"A",{href:!0});var iOt=n(lP);Sfo=t(iOt,"IBertForMaskedLM"),iOt.forEach(r),Pfo=t(m3e," (I-BERT model)"),m3e.forEach(r),$fo=i(S),Pp=s(S,"LI",{});var f3e=n(Pp);DZ=s(f3e,"STRONG",{});var dOt=n(DZ);Ifo=t(dOt,"layoutlm"),dOt.forEach(r),jfo=t(f3e," \u2014 "),iP=s(f3e,"A",{href:!0});var cOt=n(iP);Dfo=t(cOt,"LayoutLMForMaskedLM"),cOt.forEach(r),Nfo=t(f3e," (LayoutLM model)"),f3e.forEach(r),qfo=i(S),$p=s(S,"LI",{});var g3e=n($p);NZ=s(g3e,"STRONG",{});var mOt=n(NZ);Ofo=t(mOt,"longformer"),mOt.forEach(r),Gfo=t(g3e," \u2014 "),dP=s(g3e,"A",{href:!0});var fOt=n(dP);Xfo=t(fOt,"LongformerForMaskedLM"),fOt.forEach(r),Vfo=t(g3e," (Longformer model)"),g3e.forEach(r),zfo=i(S),Ip=s(S,"LI",{});var h3e=n(Ip);qZ=s(h3e,"STRONG",{});var gOt=n(qZ);Wfo=t(gOt,"lxmert"),gOt.forEach(r),Qfo=t(h3e," \u2014 "),cP=s(h3e,"A",{href:!0});var hOt=n(cP);Hfo=t(hOt,"LxmertForPreTraining"),hOt.forEach(r),Ufo=t(h3e," (LXMERT model)"),h3e.forEach(r),Jfo=i(S),jp=s(S,"LI",{});var u3e=n(jp);OZ=s(u3e,"STRONG",{});var uOt=n(OZ);Yfo=t(uOt,"megatron-bert"),uOt.forEach(r),Kfo=t(u3e," \u2014 "),mP=s(u3e,"A",{href:!0});var pOt=n(mP);Zfo=t(pOt,"MegatronBertForPreTraining"),pOt.forEach(r),ego=t(u3e," (MegatronBert model)"),u3e.forEach(r),ogo=i(S),Dp=s(S,"LI",{});var p3e=n(Dp);GZ=s(p3e,"STRONG",{});var _Ot=n(GZ);tgo=t(_Ot,"mobilebert"),_Ot.forEach(r),rgo=t(p3e," \u2014 "),fP=s(p3e,"A",{href:!0});var bOt=n(fP);ago=t(bOt,"MobileBertForPreTraining"),bOt.forEach(r),sgo=t(p3e," (MobileBERT model)"),p3e.forEach(r),ngo=i(S),Np=s(S,"LI",{});var _3e=n(Np);XZ=s(_3e,"STRONG",{});var vOt=n(XZ);lgo=t(vOt,"mpnet"),vOt.forEach(r),igo=t(_3e," \u2014 "),gP=s(_3e,"A",{href:!0});var TOt=n(gP);dgo=t(TOt,"MPNetForMaskedLM"),TOt.forEach(r),cgo=t(_3e," (MPNet model)"),_3e.forEach(r),mgo=i(S),qp=s(S,"LI",{});var b3e=n(qp);VZ=s(b3e,"STRONG",{});var FOt=n(VZ);fgo=t(FOt,"openai-gpt"),FOt.forEach(r),ggo=t(b3e," \u2014 "),hP=s(b3e,"A",{href:!0});var COt=n(hP);hgo=t(COt,"OpenAIGPTLMHeadModel"),COt.forEach(r),ugo=t(b3e," (OpenAI GPT model)"),b3e.forEach(r),pgo=i(S),Op=s(S,"LI",{});var v3e=n(Op);zZ=s(v3e,"STRONG",{});var MOt=n(zZ);_go=t(MOt,"retribert"),MOt.forEach(r),bgo=t(v3e," \u2014 "),uP=s(v3e,"A",{href:!0});var EOt=n(uP);vgo=t(EOt,"RetriBertModel"),EOt.forEach(r),Tgo=t(v3e," (RetriBERT model)"),v3e.forEach(r),Fgo=i(S),Gp=s(S,"LI",{});var T3e=n(Gp);WZ=s(T3e,"STRONG",{});var yOt=n(WZ);Cgo=t(yOt,"roberta"),yOt.forEach(r),Mgo=t(T3e," \u2014 "),pP=s(T3e,"A",{href:!0});var wOt=n(pP);Ego=t(wOt,"RobertaForMaskedLM"),wOt.forEach(r),ygo=t(T3e," (RoBERTa model)"),T3e.forEach(r),wgo=i(S),Xp=s(S,"LI",{});var F3e=n(Xp);QZ=s(F3e,"STRONG",{});var AOt=n(QZ);Ago=t(AOt,"squeezebert"),AOt.forEach(r),Lgo=t(F3e," \u2014 "),_P=s(F3e,"A",{href:!0});var LOt=n(_P);Bgo=t(LOt,"SqueezeBertForMaskedLM"),LOt.forEach(r),xgo=t(F3e," (SqueezeBERT model)"),F3e.forEach(r),kgo=i(S),Vp=s(S,"LI",{});var C3e=n(Vp);HZ=s(C3e,"STRONG",{});var BOt=n(HZ);Rgo=t(BOt,"t5"),BOt.forEach(r),Sgo=t(C3e," \u2014 "),bP=s(C3e,"A",{href:!0});var xOt=n(bP);Pgo=t(xOt,"T5ForConditionalGeneration"),xOt.forEach(r),$go=t(C3e," (T5 model)"),C3e.forEach(r),Igo=i(S),zp=s(S,"LI",{});var M3e=n(zp);UZ=s(M3e,"STRONG",{});var kOt=n(UZ);jgo=t(kOt,"tapas"),kOt.forEach(r),Dgo=t(M3e," \u2014 "),vP=s(M3e,"A",{href:!0});var ROt=n(vP);Ngo=t(ROt,"TapasForMaskedLM"),ROt.forEach(r),qgo=t(M3e," (TAPAS model)"),M3e.forEach(r),Ogo=i(S),Wp=s(S,"LI",{});var E3e=n(Wp);JZ=s(E3e,"STRONG",{});var SOt=n(JZ);Ggo=t(SOt,"transfo-xl"),SOt.forEach(r),Xgo=t(E3e," \u2014 "),TP=s(E3e,"A",{href:!0});var POt=n(TP);Vgo=t(POt,"TransfoXLLMHeadModel"),POt.forEach(r),zgo=t(E3e," (Transformer-XL model)"),E3e.forEach(r),Wgo=i(S),Qp=s(S,"LI",{});var y3e=n(Qp);YZ=s(y3e,"STRONG",{});var $Ot=n(YZ);Qgo=t($Ot,"unispeech"),$Ot.forEach(r),Hgo=t(y3e," \u2014 "),FP=s(y3e,"A",{href:!0});var IOt=n(FP);Ugo=t(IOt,"UniSpeechForPreTraining"),IOt.forEach(r),Jgo=t(y3e," (UniSpeech model)"),y3e.forEach(r),Ygo=i(S),Hp=s(S,"LI",{});var w3e=n(Hp);KZ=s(w3e,"STRONG",{});var jOt=n(KZ);Kgo=t(jOt,"unispeech-sat"),jOt.forEach(r),Zgo=t(w3e," \u2014 "),CP=s(w3e,"A",{href:!0});var DOt=n(CP);eho=t(DOt,"UniSpeechSatForPreTraining"),DOt.forEach(r),oho=t(w3e," (UniSpeechSat model)"),w3e.forEach(r),tho=i(S),Up=s(S,"LI",{});var A3e=n(Up);ZZ=s(A3e,"STRONG",{});var NOt=n(ZZ);rho=t(NOt,"visual_bert"),NOt.forEach(r),aho=t(A3e," \u2014 "),MP=s(A3e,"A",{href:!0});var qOt=n(MP);sho=t(qOt,"VisualBertForPreTraining"),qOt.forEach(r),nho=t(A3e," (VisualBert model)"),A3e.forEach(r),lho=i(S),Jp=s(S,"LI",{});var L3e=n(Jp);eee=s(L3e,"STRONG",{});var OOt=n(eee);iho=t(OOt,"vit_mae"),OOt.forEach(r),dho=t(L3e," \u2014 "),EP=s(L3e,"A",{href:!0});var GOt=n(EP);cho=t(GOt,"ViTMAEForPreTraining"),GOt.forEach(r),mho=t(L3e," (ViTMAE model)"),L3e.forEach(r),fho=i(S),Yp=s(S,"LI",{});var B3e=n(Yp);oee=s(B3e,"STRONG",{});var XOt=n(oee);gho=t(XOt,"wav2vec2"),XOt.forEach(r),hho=t(B3e," \u2014 "),yP=s(B3e,"A",{href:!0});var VOt=n(yP);uho=t(VOt,"Wav2Vec2ForPreTraining"),VOt.forEach(r),pho=t(B3e," (Wav2Vec2 model)"),B3e.forEach(r),_ho=i(S),Kp=s(S,"LI",{});var x3e=n(Kp);tee=s(x3e,"STRONG",{});var zOt=n(tee);bho=t(zOt,"xlm"),zOt.forEach(r),vho=t(x3e," \u2014 "),wP=s(x3e,"A",{href:!0});var WOt=n(wP);Tho=t(WOt,"XLMWithLMHeadModel"),WOt.forEach(r),Fho=t(x3e," (XLM model)"),x3e.forEach(r),Cho=i(S),Zp=s(S,"LI",{});var k3e=n(Zp);ree=s(k3e,"STRONG",{});var QOt=n(ree);Mho=t(QOt,"xlm-roberta"),QOt.forEach(r),Eho=t(k3e," \u2014 "),AP=s(k3e,"A",{href:!0});var HOt=n(AP);yho=t(HOt,"XLMRobertaForMaskedLM"),HOt.forEach(r),who=t(k3e," (XLM-RoBERTa model)"),k3e.forEach(r),Aho=i(S),e_=s(S,"LI",{});var R3e=n(e_);aee=s(R3e,"STRONG",{});var UOt=n(aee);Lho=t(UOt,"xlm-roberta-xl"),UOt.forEach(r),Bho=t(R3e," \u2014 "),LP=s(R3e,"A",{href:!0});var JOt=n(LP);xho=t(JOt,"XLMRobertaXLForMaskedLM"),JOt.forEach(r),kho=t(R3e," (XLM-RoBERTa-XL model)"),R3e.forEach(r),Rho=i(S),o_=s(S,"LI",{});var S3e=n(o_);see=s(S3e,"STRONG",{});var YOt=n(see);Sho=t(YOt,"xlnet"),YOt.forEach(r),Pho=t(S3e," \u2014 "),BP=s(S3e,"A",{href:!0});var KOt=n(BP);$ho=t(KOt,"XLNetLMHeadModel"),KOt.forEach(r),Iho=t(S3e," (XLNet model)"),S3e.forEach(r),S.forEach(r),jho=i(Nr),t_=s(Nr,"P",{});var P3e=n(t_);Dho=t(P3e,"The model is set in evaluation mode by default using "),nee=s(P3e,"CODE",{});var ZOt=n(nee);Nho=t(ZOt,"model.eval()"),ZOt.forEach(r),qho=t(P3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lee=s(P3e,"CODE",{});var eGt=n(lee);Oho=t(eGt,"model.train()"),eGt.forEach(r),P3e.forEach(r),Gho=i(Nr),iee=s(Nr,"P",{});var oGt=n(iee);Xho=t(oGt,"Examples:"),oGt.forEach(r),Vho=i(Nr),f(ky.$$.fragment,Nr),Nr.forEach(r),Hn.forEach(r),OBe=i(d),td=s(d,"H2",{class:!0});var Jke=n(td);r_=s(Jke,"A",{id:!0,class:!0,href:!0});var tGt=n(r_);dee=s(tGt,"SPAN",{});var rGt=n(dee);f(Ry.$$.fragment,rGt),rGt.forEach(r),tGt.forEach(r),zho=i(Jke),cee=s(Jke,"SPAN",{});var aGt=n(cee);Who=t(aGt,"AutoModelForCausalLM"),aGt.forEach(r),Jke.forEach(r),GBe=i(d),Jo=s(d,"DIV",{class:!0});var Jn=n(Jo);f(Sy.$$.fragment,Jn),Qho=i(Jn),rd=s(Jn,"P",{});var mz=n(rd);Hho=t(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),mee=s(mz,"CODE",{});var sGt=n(mee);Uho=t(sGt,"from_pretrained()"),sGt.forEach(r),Jho=t(mz,"class method or the "),fee=s(mz,"CODE",{});var nGt=n(fee);Yho=t(nGt,"from_config()"),nGt.forEach(r),Kho=t(mz,`class
method.`),mz.forEach(r),Zho=i(Jn),Py=s(Jn,"P",{});var Yke=n(Py);euo=t(Yke,"This class cannot be instantiated directly using "),gee=s(Yke,"CODE",{});var lGt=n(gee);ouo=t(lGt,"__init__()"),lGt.forEach(r),tuo=t(Yke," (throws an error)."),Yke.forEach(r),ruo=i(Jn),Vt=s(Jn,"DIV",{class:!0});var Yn=n(Vt);f($y.$$.fragment,Yn),auo=i(Yn),hee=s(Yn,"P",{});var iGt=n(hee);suo=t(iGt,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),iGt.forEach(r),nuo=i(Yn),ad=s(Yn,"P",{});var fz=n(ad);luo=t(fz,`Note:
Loading a model from its configuration file does `),uee=s(fz,"STRONG",{});var dGt=n(uee);iuo=t(dGt,"not"),dGt.forEach(r),duo=t(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),pee=s(fz,"CODE",{});var cGt=n(pee);cuo=t(cGt,"from_pretrained()"),cGt.forEach(r),muo=t(fz,"to load the model weights."),fz.forEach(r),fuo=i(Yn),_ee=s(Yn,"P",{});var mGt=n(_ee);guo=t(mGt,"Examples:"),mGt.forEach(r),huo=i(Yn),f(Iy.$$.fragment,Yn),Yn.forEach(r),uuo=i(Jn),Pe=s(Jn,"DIV",{class:!0});var qr=n(Pe);f(jy.$$.fragment,qr),puo=i(qr),bee=s(qr,"P",{});var fGt=n(bee);_uo=t(fGt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),fGt.forEach(r),buo=i(qr),za=s(qr,"P",{});var X3=n(za);vuo=t(X3,"The model class to instantiate is selected based on the "),vee=s(X3,"CODE",{});var gGt=n(vee);Tuo=t(gGt,"model_type"),gGt.forEach(r),Fuo=t(X3,` property of the config object (either
passed as an argument or loaded from `),Tee=s(X3,"CODE",{});var hGt=n(Tee);Cuo=t(hGt,"pretrained_model_name_or_path"),hGt.forEach(r),Muo=t(X3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fee=s(X3,"CODE",{});var uGt=n(Fee);Euo=t(uGt,"pretrained_model_name_or_path"),uGt.forEach(r),yuo=t(X3,":"),X3.forEach(r),wuo=i(qr),$=s(qr,"UL",{});var j=n($);a_=s(j,"LI",{});var $3e=n(a_);Cee=s($3e,"STRONG",{});var pGt=n(Cee);Auo=t(pGt,"bart"),pGt.forEach(r),Luo=t($3e," \u2014 "),xP=s($3e,"A",{href:!0});var _Gt=n(xP);Buo=t(_Gt,"BartForCausalLM"),_Gt.forEach(r),xuo=t($3e," (BART model)"),$3e.forEach(r),kuo=i(j),s_=s(j,"LI",{});var I3e=n(s_);Mee=s(I3e,"STRONG",{});var bGt=n(Mee);Ruo=t(bGt,"bert"),bGt.forEach(r),Suo=t(I3e," \u2014 "),kP=s(I3e,"A",{href:!0});var vGt=n(kP);Puo=t(vGt,"BertLMHeadModel"),vGt.forEach(r),$uo=t(I3e," (BERT model)"),I3e.forEach(r),Iuo=i(j),n_=s(j,"LI",{});var j3e=n(n_);Eee=s(j3e,"STRONG",{});var TGt=n(Eee);juo=t(TGt,"bert-generation"),TGt.forEach(r),Duo=t(j3e," \u2014 "),RP=s(j3e,"A",{href:!0});var FGt=n(RP);Nuo=t(FGt,"BertGenerationDecoder"),FGt.forEach(r),quo=t(j3e," (Bert Generation model)"),j3e.forEach(r),Ouo=i(j),l_=s(j,"LI",{});var D3e=n(l_);yee=s(D3e,"STRONG",{});var CGt=n(yee);Guo=t(CGt,"big_bird"),CGt.forEach(r),Xuo=t(D3e," \u2014 "),SP=s(D3e,"A",{href:!0});var MGt=n(SP);Vuo=t(MGt,"BigBirdForCausalLM"),MGt.forEach(r),zuo=t(D3e," (BigBird model)"),D3e.forEach(r),Wuo=i(j),i_=s(j,"LI",{});var N3e=n(i_);wee=s(N3e,"STRONG",{});var EGt=n(wee);Quo=t(EGt,"bigbird_pegasus"),EGt.forEach(r),Huo=t(N3e," \u2014 "),PP=s(N3e,"A",{href:!0});var yGt=n(PP);Uuo=t(yGt,"BigBirdPegasusForCausalLM"),yGt.forEach(r),Juo=t(N3e," (BigBirdPegasus model)"),N3e.forEach(r),Yuo=i(j),d_=s(j,"LI",{});var q3e=n(d_);Aee=s(q3e,"STRONG",{});var wGt=n(Aee);Kuo=t(wGt,"blenderbot"),wGt.forEach(r),Zuo=t(q3e," \u2014 "),$P=s(q3e,"A",{href:!0});var AGt=n($P);epo=t(AGt,"BlenderbotForCausalLM"),AGt.forEach(r),opo=t(q3e," (Blenderbot model)"),q3e.forEach(r),tpo=i(j),c_=s(j,"LI",{});var O3e=n(c_);Lee=s(O3e,"STRONG",{});var LGt=n(Lee);rpo=t(LGt,"blenderbot-small"),LGt.forEach(r),apo=t(O3e," \u2014 "),IP=s(O3e,"A",{href:!0});var BGt=n(IP);spo=t(BGt,"BlenderbotSmallForCausalLM"),BGt.forEach(r),npo=t(O3e," (BlenderbotSmall model)"),O3e.forEach(r),lpo=i(j),m_=s(j,"LI",{});var G3e=n(m_);Bee=s(G3e,"STRONG",{});var xGt=n(Bee);ipo=t(xGt,"camembert"),xGt.forEach(r),dpo=t(G3e," \u2014 "),jP=s(G3e,"A",{href:!0});var kGt=n(jP);cpo=t(kGt,"CamembertForCausalLM"),kGt.forEach(r),mpo=t(G3e," (CamemBERT model)"),G3e.forEach(r),fpo=i(j),f_=s(j,"LI",{});var X3e=n(f_);xee=s(X3e,"STRONG",{});var RGt=n(xee);gpo=t(RGt,"ctrl"),RGt.forEach(r),hpo=t(X3e," \u2014 "),DP=s(X3e,"A",{href:!0});var SGt=n(DP);upo=t(SGt,"CTRLLMHeadModel"),SGt.forEach(r),ppo=t(X3e," (CTRL model)"),X3e.forEach(r),_po=i(j),g_=s(j,"LI",{});var V3e=n(g_);kee=s(V3e,"STRONG",{});var PGt=n(kee);bpo=t(PGt,"data2vec-text"),PGt.forEach(r),vpo=t(V3e," \u2014 "),NP=s(V3e,"A",{href:!0});var $Gt=n(NP);Tpo=t($Gt,"Data2VecTextForCausalLM"),$Gt.forEach(r),Fpo=t(V3e," (Data2VecText model)"),V3e.forEach(r),Cpo=i(j),h_=s(j,"LI",{});var z3e=n(h_);Ree=s(z3e,"STRONG",{});var IGt=n(Ree);Mpo=t(IGt,"electra"),IGt.forEach(r),Epo=t(z3e," \u2014 "),qP=s(z3e,"A",{href:!0});var jGt=n(qP);ypo=t(jGt,"ElectraForCausalLM"),jGt.forEach(r),wpo=t(z3e," (ELECTRA model)"),z3e.forEach(r),Apo=i(j),u_=s(j,"LI",{});var W3e=n(u_);See=s(W3e,"STRONG",{});var DGt=n(See);Lpo=t(DGt,"gpt2"),DGt.forEach(r),Bpo=t(W3e," \u2014 "),OP=s(W3e,"A",{href:!0});var NGt=n(OP);xpo=t(NGt,"GPT2LMHeadModel"),NGt.forEach(r),kpo=t(W3e," (OpenAI GPT-2 model)"),W3e.forEach(r),Rpo=i(j),p_=s(j,"LI",{});var Q3e=n(p_);Pee=s(Q3e,"STRONG",{});var qGt=n(Pee);Spo=t(qGt,"gpt_neo"),qGt.forEach(r),Ppo=t(Q3e," \u2014 "),GP=s(Q3e,"A",{href:!0});var OGt=n(GP);$po=t(OGt,"GPTNeoForCausalLM"),OGt.forEach(r),Ipo=t(Q3e," (GPT Neo model)"),Q3e.forEach(r),jpo=i(j),__=s(j,"LI",{});var H3e=n(__);$ee=s(H3e,"STRONG",{});var GGt=n($ee);Dpo=t(GGt,"gptj"),GGt.forEach(r),Npo=t(H3e," \u2014 "),XP=s(H3e,"A",{href:!0});var XGt=n(XP);qpo=t(XGt,"GPTJForCausalLM"),XGt.forEach(r),Opo=t(H3e," (GPT-J model)"),H3e.forEach(r),Gpo=i(j),b_=s(j,"LI",{});var U3e=n(b_);Iee=s(U3e,"STRONG",{});var VGt=n(Iee);Xpo=t(VGt,"marian"),VGt.forEach(r),Vpo=t(U3e," \u2014 "),VP=s(U3e,"A",{href:!0});var zGt=n(VP);zpo=t(zGt,"MarianForCausalLM"),zGt.forEach(r),Wpo=t(U3e," (Marian model)"),U3e.forEach(r),Qpo=i(j),v_=s(j,"LI",{});var J3e=n(v_);jee=s(J3e,"STRONG",{});var WGt=n(jee);Hpo=t(WGt,"mbart"),WGt.forEach(r),Upo=t(J3e," \u2014 "),zP=s(J3e,"A",{href:!0});var QGt=n(zP);Jpo=t(QGt,"MBartForCausalLM"),QGt.forEach(r),Ypo=t(J3e," (mBART model)"),J3e.forEach(r),Kpo=i(j),T_=s(j,"LI",{});var Y3e=n(T_);Dee=s(Y3e,"STRONG",{});var HGt=n(Dee);Zpo=t(HGt,"megatron-bert"),HGt.forEach(r),e_o=t(Y3e," \u2014 "),WP=s(Y3e,"A",{href:!0});var UGt=n(WP);o_o=t(UGt,"MegatronBertForCausalLM"),UGt.forEach(r),t_o=t(Y3e," (MegatronBert model)"),Y3e.forEach(r),r_o=i(j),F_=s(j,"LI",{});var K3e=n(F_);Nee=s(K3e,"STRONG",{});var JGt=n(Nee);a_o=t(JGt,"openai-gpt"),JGt.forEach(r),s_o=t(K3e," \u2014 "),QP=s(K3e,"A",{href:!0});var YGt=n(QP);n_o=t(YGt,"OpenAIGPTLMHeadModel"),YGt.forEach(r),l_o=t(K3e," (OpenAI GPT model)"),K3e.forEach(r),i_o=i(j),C_=s(j,"LI",{});var Z3e=n(C_);qee=s(Z3e,"STRONG",{});var KGt=n(qee);d_o=t(KGt,"pegasus"),KGt.forEach(r),c_o=t(Z3e," \u2014 "),HP=s(Z3e,"A",{href:!0});var ZGt=n(HP);m_o=t(ZGt,"PegasusForCausalLM"),ZGt.forEach(r),f_o=t(Z3e," (Pegasus model)"),Z3e.forEach(r),g_o=i(j),M_=s(j,"LI",{});var e5e=n(M_);Oee=s(e5e,"STRONG",{});var eXt=n(Oee);h_o=t(eXt,"plbart"),eXt.forEach(r),u_o=t(e5e," \u2014 "),UP=s(e5e,"A",{href:!0});var oXt=n(UP);p_o=t(oXt,"PLBartForCausalLM"),oXt.forEach(r),__o=t(e5e," (PLBart model)"),e5e.forEach(r),b_o=i(j),E_=s(j,"LI",{});var o5e=n(E_);Gee=s(o5e,"STRONG",{});var tXt=n(Gee);v_o=t(tXt,"prophetnet"),tXt.forEach(r),T_o=t(o5e," \u2014 "),JP=s(o5e,"A",{href:!0});var rXt=n(JP);F_o=t(rXt,"ProphetNetForCausalLM"),rXt.forEach(r),C_o=t(o5e," (ProphetNet model)"),o5e.forEach(r),M_o=i(j),y_=s(j,"LI",{});var t5e=n(y_);Xee=s(t5e,"STRONG",{});var aXt=n(Xee);E_o=t(aXt,"qdqbert"),aXt.forEach(r),y_o=t(t5e," \u2014 "),YP=s(t5e,"A",{href:!0});var sXt=n(YP);w_o=t(sXt,"QDQBertLMHeadModel"),sXt.forEach(r),A_o=t(t5e," (QDQBert model)"),t5e.forEach(r),L_o=i(j),w_=s(j,"LI",{});var r5e=n(w_);Vee=s(r5e,"STRONG",{});var nXt=n(Vee);B_o=t(nXt,"reformer"),nXt.forEach(r),x_o=t(r5e," \u2014 "),KP=s(r5e,"A",{href:!0});var lXt=n(KP);k_o=t(lXt,"ReformerModelWithLMHead"),lXt.forEach(r),R_o=t(r5e," (Reformer model)"),r5e.forEach(r),S_o=i(j),A_=s(j,"LI",{});var a5e=n(A_);zee=s(a5e,"STRONG",{});var iXt=n(zee);P_o=t(iXt,"rembert"),iXt.forEach(r),$_o=t(a5e," \u2014 "),ZP=s(a5e,"A",{href:!0});var dXt=n(ZP);I_o=t(dXt,"RemBertForCausalLM"),dXt.forEach(r),j_o=t(a5e," (RemBERT model)"),a5e.forEach(r),D_o=i(j),L_=s(j,"LI",{});var s5e=n(L_);Wee=s(s5e,"STRONG",{});var cXt=n(Wee);N_o=t(cXt,"roberta"),cXt.forEach(r),q_o=t(s5e," \u2014 "),e$=s(s5e,"A",{href:!0});var mXt=n(e$);O_o=t(mXt,"RobertaForCausalLM"),mXt.forEach(r),G_o=t(s5e," (RoBERTa model)"),s5e.forEach(r),X_o=i(j),B_=s(j,"LI",{});var n5e=n(B_);Qee=s(n5e,"STRONG",{});var fXt=n(Qee);V_o=t(fXt,"roformer"),fXt.forEach(r),z_o=t(n5e," \u2014 "),o$=s(n5e,"A",{href:!0});var gXt=n(o$);W_o=t(gXt,"RoFormerForCausalLM"),gXt.forEach(r),Q_o=t(n5e," (RoFormer model)"),n5e.forEach(r),H_o=i(j),x_=s(j,"LI",{});var l5e=n(x_);Hee=s(l5e,"STRONG",{});var hXt=n(Hee);U_o=t(hXt,"speech_to_text_2"),hXt.forEach(r),J_o=t(l5e," \u2014 "),t$=s(l5e,"A",{href:!0});var uXt=n(t$);Y_o=t(uXt,"Speech2Text2ForCausalLM"),uXt.forEach(r),K_o=t(l5e," (Speech2Text2 model)"),l5e.forEach(r),Z_o=i(j),k_=s(j,"LI",{});var i5e=n(k_);Uee=s(i5e,"STRONG",{});var pXt=n(Uee);ebo=t(pXt,"transfo-xl"),pXt.forEach(r),obo=t(i5e," \u2014 "),r$=s(i5e,"A",{href:!0});var _Xt=n(r$);tbo=t(_Xt,"TransfoXLLMHeadModel"),_Xt.forEach(r),rbo=t(i5e," (Transformer-XL model)"),i5e.forEach(r),abo=i(j),R_=s(j,"LI",{});var d5e=n(R_);Jee=s(d5e,"STRONG",{});var bXt=n(Jee);sbo=t(bXt,"trocr"),bXt.forEach(r),nbo=t(d5e," \u2014 "),a$=s(d5e,"A",{href:!0});var vXt=n(a$);lbo=t(vXt,"TrOCRForCausalLM"),vXt.forEach(r),ibo=t(d5e," (TrOCR model)"),d5e.forEach(r),dbo=i(j),S_=s(j,"LI",{});var c5e=n(S_);Yee=s(c5e,"STRONG",{});var TXt=n(Yee);cbo=t(TXt,"xglm"),TXt.forEach(r),mbo=t(c5e," \u2014 "),s$=s(c5e,"A",{href:!0});var FXt=n(s$);fbo=t(FXt,"XGLMForCausalLM"),FXt.forEach(r),gbo=t(c5e," (XGLM model)"),c5e.forEach(r),hbo=i(j),P_=s(j,"LI",{});var m5e=n(P_);Kee=s(m5e,"STRONG",{});var CXt=n(Kee);ubo=t(CXt,"xlm"),CXt.forEach(r),pbo=t(m5e," \u2014 "),n$=s(m5e,"A",{href:!0});var MXt=n(n$);_bo=t(MXt,"XLMWithLMHeadModel"),MXt.forEach(r),bbo=t(m5e," (XLM model)"),m5e.forEach(r),vbo=i(j),$_=s(j,"LI",{});var f5e=n($_);Zee=s(f5e,"STRONG",{});var EXt=n(Zee);Tbo=t(EXt,"xlm-prophetnet"),EXt.forEach(r),Fbo=t(f5e," \u2014 "),l$=s(f5e,"A",{href:!0});var yXt=n(l$);Cbo=t(yXt,"XLMProphetNetForCausalLM"),yXt.forEach(r),Mbo=t(f5e," (XLMProphetNet model)"),f5e.forEach(r),Ebo=i(j),I_=s(j,"LI",{});var g5e=n(I_);eoe=s(g5e,"STRONG",{});var wXt=n(eoe);ybo=t(wXt,"xlm-roberta"),wXt.forEach(r),wbo=t(g5e," \u2014 "),i$=s(g5e,"A",{href:!0});var AXt=n(i$);Abo=t(AXt,"XLMRobertaForCausalLM"),AXt.forEach(r),Lbo=t(g5e," (XLM-RoBERTa model)"),g5e.forEach(r),Bbo=i(j),j_=s(j,"LI",{});var h5e=n(j_);ooe=s(h5e,"STRONG",{});var LXt=n(ooe);xbo=t(LXt,"xlm-roberta-xl"),LXt.forEach(r),kbo=t(h5e," \u2014 "),d$=s(h5e,"A",{href:!0});var BXt=n(d$);Rbo=t(BXt,"XLMRobertaXLForCausalLM"),BXt.forEach(r),Sbo=t(h5e," (XLM-RoBERTa-XL model)"),h5e.forEach(r),Pbo=i(j),D_=s(j,"LI",{});var u5e=n(D_);toe=s(u5e,"STRONG",{});var xXt=n(toe);$bo=t(xXt,"xlnet"),xXt.forEach(r),Ibo=t(u5e," \u2014 "),c$=s(u5e,"A",{href:!0});var kXt=n(c$);jbo=t(kXt,"XLNetLMHeadModel"),kXt.forEach(r),Dbo=t(u5e," (XLNet model)"),u5e.forEach(r),j.forEach(r),Nbo=i(qr),N_=s(qr,"P",{});var p5e=n(N_);qbo=t(p5e,"The model is set in evaluation mode by default using "),roe=s(p5e,"CODE",{});var RXt=n(roe);Obo=t(RXt,"model.eval()"),RXt.forEach(r),Gbo=t(p5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),aoe=s(p5e,"CODE",{});var SXt=n(aoe);Xbo=t(SXt,"model.train()"),SXt.forEach(r),p5e.forEach(r),Vbo=i(qr),soe=s(qr,"P",{});var PXt=n(soe);zbo=t(PXt,"Examples:"),PXt.forEach(r),Wbo=i(qr),f(Dy.$$.fragment,qr),qr.forEach(r),Jn.forEach(r),XBe=i(d),sd=s(d,"H2",{class:!0});var Kke=n(sd);q_=s(Kke,"A",{id:!0,class:!0,href:!0});var $Xt=n(q_);noe=s($Xt,"SPAN",{});var IXt=n(noe);f(Ny.$$.fragment,IXt),IXt.forEach(r),$Xt.forEach(r),Qbo=i(Kke),loe=s(Kke,"SPAN",{});var jXt=n(loe);Hbo=t(jXt,"AutoModelForMaskedLM"),jXt.forEach(r),Kke.forEach(r),VBe=i(d),Yo=s(d,"DIV",{class:!0});var Kn=n(Yo);f(qy.$$.fragment,Kn),Ubo=i(Kn),nd=s(Kn,"P",{});var gz=n(nd);Jbo=t(gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),ioe=s(gz,"CODE",{});var DXt=n(ioe);Ybo=t(DXt,"from_pretrained()"),DXt.forEach(r),Kbo=t(gz,"class method or the "),doe=s(gz,"CODE",{});var NXt=n(doe);Zbo=t(NXt,"from_config()"),NXt.forEach(r),e2o=t(gz,`class
method.`),gz.forEach(r),o2o=i(Kn),Oy=s(Kn,"P",{});var Zke=n(Oy);t2o=t(Zke,"This class cannot be instantiated directly using "),coe=s(Zke,"CODE",{});var qXt=n(coe);r2o=t(qXt,"__init__()"),qXt.forEach(r),a2o=t(Zke," (throws an error)."),Zke.forEach(r),s2o=i(Kn),zt=s(Kn,"DIV",{class:!0});var Zn=n(zt);f(Gy.$$.fragment,Zn),n2o=i(Zn),moe=s(Zn,"P",{});var OXt=n(moe);l2o=t(OXt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),OXt.forEach(r),i2o=i(Zn),ld=s(Zn,"P",{});var hz=n(ld);d2o=t(hz,`Note:
Loading a model from its configuration file does `),foe=s(hz,"STRONG",{});var GXt=n(foe);c2o=t(GXt,"not"),GXt.forEach(r),m2o=t(hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),goe=s(hz,"CODE",{});var XXt=n(goe);f2o=t(XXt,"from_pretrained()"),XXt.forEach(r),g2o=t(hz,"to load the model weights."),hz.forEach(r),h2o=i(Zn),hoe=s(Zn,"P",{});var VXt=n(hoe);u2o=t(VXt,"Examples:"),VXt.forEach(r),p2o=i(Zn),f(Xy.$$.fragment,Zn),Zn.forEach(r),_2o=i(Kn),$e=s(Kn,"DIV",{class:!0});var Or=n($e);f(Vy.$$.fragment,Or),b2o=i(Or),uoe=s(Or,"P",{});var zXt=n(uoe);v2o=t(zXt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),zXt.forEach(r),T2o=i(Or),Wa=s(Or,"P",{});var V3=n(Wa);F2o=t(V3,"The model class to instantiate is selected based on the "),poe=s(V3,"CODE",{});var WXt=n(poe);C2o=t(WXt,"model_type"),WXt.forEach(r),M2o=t(V3,` property of the config object (either
passed as an argument or loaded from `),_oe=s(V3,"CODE",{});var QXt=n(_oe);E2o=t(QXt,"pretrained_model_name_or_path"),QXt.forEach(r),y2o=t(V3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),boe=s(V3,"CODE",{});var HXt=n(boe);w2o=t(HXt,"pretrained_model_name_or_path"),HXt.forEach(r),A2o=t(V3,":"),V3.forEach(r),L2o=i(Or),I=s(Or,"UL",{});var D=n(I);O_=s(D,"LI",{});var _5e=n(O_);voe=s(_5e,"STRONG",{});var UXt=n(voe);B2o=t(UXt,"albert"),UXt.forEach(r),x2o=t(_5e," \u2014 "),m$=s(_5e,"A",{href:!0});var JXt=n(m$);k2o=t(JXt,"AlbertForMaskedLM"),JXt.forEach(r),R2o=t(_5e," (ALBERT model)"),_5e.forEach(r),S2o=i(D),G_=s(D,"LI",{});var b5e=n(G_);Toe=s(b5e,"STRONG",{});var YXt=n(Toe);P2o=t(YXt,"bart"),YXt.forEach(r),$2o=t(b5e," \u2014 "),f$=s(b5e,"A",{href:!0});var KXt=n(f$);I2o=t(KXt,"BartForConditionalGeneration"),KXt.forEach(r),j2o=t(b5e," (BART model)"),b5e.forEach(r),D2o=i(D),X_=s(D,"LI",{});var v5e=n(X_);Foe=s(v5e,"STRONG",{});var ZXt=n(Foe);N2o=t(ZXt,"bert"),ZXt.forEach(r),q2o=t(v5e," \u2014 "),g$=s(v5e,"A",{href:!0});var eVt=n(g$);O2o=t(eVt,"BertForMaskedLM"),eVt.forEach(r),G2o=t(v5e," (BERT model)"),v5e.forEach(r),X2o=i(D),V_=s(D,"LI",{});var T5e=n(V_);Coe=s(T5e,"STRONG",{});var oVt=n(Coe);V2o=t(oVt,"big_bird"),oVt.forEach(r),z2o=t(T5e," \u2014 "),h$=s(T5e,"A",{href:!0});var tVt=n(h$);W2o=t(tVt,"BigBirdForMaskedLM"),tVt.forEach(r),Q2o=t(T5e," (BigBird model)"),T5e.forEach(r),H2o=i(D),z_=s(D,"LI",{});var F5e=n(z_);Moe=s(F5e,"STRONG",{});var rVt=n(Moe);U2o=t(rVt,"camembert"),rVt.forEach(r),J2o=t(F5e," \u2014 "),u$=s(F5e,"A",{href:!0});var aVt=n(u$);Y2o=t(aVt,"CamembertForMaskedLM"),aVt.forEach(r),K2o=t(F5e," (CamemBERT model)"),F5e.forEach(r),Z2o=i(D),W_=s(D,"LI",{});var C5e=n(W_);Eoe=s(C5e,"STRONG",{});var sVt=n(Eoe);evo=t(sVt,"convbert"),sVt.forEach(r),ovo=t(C5e," \u2014 "),p$=s(C5e,"A",{href:!0});var nVt=n(p$);tvo=t(nVt,"ConvBertForMaskedLM"),nVt.forEach(r),rvo=t(C5e," (ConvBERT model)"),C5e.forEach(r),avo=i(D),Q_=s(D,"LI",{});var M5e=n(Q_);yoe=s(M5e,"STRONG",{});var lVt=n(yoe);svo=t(lVt,"data2vec-text"),lVt.forEach(r),nvo=t(M5e," \u2014 "),_$=s(M5e,"A",{href:!0});var iVt=n(_$);lvo=t(iVt,"Data2VecTextForMaskedLM"),iVt.forEach(r),ivo=t(M5e," (Data2VecText model)"),M5e.forEach(r),dvo=i(D),H_=s(D,"LI",{});var E5e=n(H_);woe=s(E5e,"STRONG",{});var dVt=n(woe);cvo=t(dVt,"deberta"),dVt.forEach(r),mvo=t(E5e," \u2014 "),b$=s(E5e,"A",{href:!0});var cVt=n(b$);fvo=t(cVt,"DebertaForMaskedLM"),cVt.forEach(r),gvo=t(E5e," (DeBERTa model)"),E5e.forEach(r),hvo=i(D),U_=s(D,"LI",{});var y5e=n(U_);Aoe=s(y5e,"STRONG",{});var mVt=n(Aoe);uvo=t(mVt,"deberta-v2"),mVt.forEach(r),pvo=t(y5e," \u2014 "),v$=s(y5e,"A",{href:!0});var fVt=n(v$);_vo=t(fVt,"DebertaV2ForMaskedLM"),fVt.forEach(r),bvo=t(y5e," (DeBERTa-v2 model)"),y5e.forEach(r),vvo=i(D),J_=s(D,"LI",{});var w5e=n(J_);Loe=s(w5e,"STRONG",{});var gVt=n(Loe);Tvo=t(gVt,"distilbert"),gVt.forEach(r),Fvo=t(w5e," \u2014 "),T$=s(w5e,"A",{href:!0});var hVt=n(T$);Cvo=t(hVt,"DistilBertForMaskedLM"),hVt.forEach(r),Mvo=t(w5e," (DistilBERT model)"),w5e.forEach(r),Evo=i(D),Y_=s(D,"LI",{});var A5e=n(Y_);Boe=s(A5e,"STRONG",{});var uVt=n(Boe);yvo=t(uVt,"electra"),uVt.forEach(r),wvo=t(A5e," \u2014 "),F$=s(A5e,"A",{href:!0});var pVt=n(F$);Avo=t(pVt,"ElectraForMaskedLM"),pVt.forEach(r),Lvo=t(A5e," (ELECTRA model)"),A5e.forEach(r),Bvo=i(D),K_=s(D,"LI",{});var L5e=n(K_);xoe=s(L5e,"STRONG",{});var _Vt=n(xoe);xvo=t(_Vt,"flaubert"),_Vt.forEach(r),kvo=t(L5e," \u2014 "),C$=s(L5e,"A",{href:!0});var bVt=n(C$);Rvo=t(bVt,"FlaubertWithLMHeadModel"),bVt.forEach(r),Svo=t(L5e," (FlauBERT model)"),L5e.forEach(r),Pvo=i(D),Z_=s(D,"LI",{});var B5e=n(Z_);koe=s(B5e,"STRONG",{});var vVt=n(koe);$vo=t(vVt,"fnet"),vVt.forEach(r),Ivo=t(B5e," \u2014 "),M$=s(B5e,"A",{href:!0});var TVt=n(M$);jvo=t(TVt,"FNetForMaskedLM"),TVt.forEach(r),Dvo=t(B5e," (FNet model)"),B5e.forEach(r),Nvo=i(D),eb=s(D,"LI",{});var x5e=n(eb);Roe=s(x5e,"STRONG",{});var FVt=n(Roe);qvo=t(FVt,"funnel"),FVt.forEach(r),Ovo=t(x5e," \u2014 "),E$=s(x5e,"A",{href:!0});var CVt=n(E$);Gvo=t(CVt,"FunnelForMaskedLM"),CVt.forEach(r),Xvo=t(x5e," (Funnel Transformer model)"),x5e.forEach(r),Vvo=i(D),ob=s(D,"LI",{});var k5e=n(ob);Soe=s(k5e,"STRONG",{});var MVt=n(Soe);zvo=t(MVt,"ibert"),MVt.forEach(r),Wvo=t(k5e," \u2014 "),y$=s(k5e,"A",{href:!0});var EVt=n(y$);Qvo=t(EVt,"IBertForMaskedLM"),EVt.forEach(r),Hvo=t(k5e," (I-BERT model)"),k5e.forEach(r),Uvo=i(D),tb=s(D,"LI",{});var R5e=n(tb);Poe=s(R5e,"STRONG",{});var yVt=n(Poe);Jvo=t(yVt,"layoutlm"),yVt.forEach(r),Yvo=t(R5e," \u2014 "),w$=s(R5e,"A",{href:!0});var wVt=n(w$);Kvo=t(wVt,"LayoutLMForMaskedLM"),wVt.forEach(r),Zvo=t(R5e," (LayoutLM model)"),R5e.forEach(r),eTo=i(D),rb=s(D,"LI",{});var S5e=n(rb);$oe=s(S5e,"STRONG",{});var AVt=n($oe);oTo=t(AVt,"longformer"),AVt.forEach(r),tTo=t(S5e," \u2014 "),A$=s(S5e,"A",{href:!0});var LVt=n(A$);rTo=t(LVt,"LongformerForMaskedLM"),LVt.forEach(r),aTo=t(S5e," (Longformer model)"),S5e.forEach(r),sTo=i(D),ab=s(D,"LI",{});var P5e=n(ab);Ioe=s(P5e,"STRONG",{});var BVt=n(Ioe);nTo=t(BVt,"mbart"),BVt.forEach(r),lTo=t(P5e," \u2014 "),L$=s(P5e,"A",{href:!0});var xVt=n(L$);iTo=t(xVt,"MBartForConditionalGeneration"),xVt.forEach(r),dTo=t(P5e," (mBART model)"),P5e.forEach(r),cTo=i(D),sb=s(D,"LI",{});var $5e=n(sb);joe=s($5e,"STRONG",{});var kVt=n(joe);mTo=t(kVt,"megatron-bert"),kVt.forEach(r),fTo=t($5e," \u2014 "),B$=s($5e,"A",{href:!0});var RVt=n(B$);gTo=t(RVt,"MegatronBertForMaskedLM"),RVt.forEach(r),hTo=t($5e," (MegatronBert model)"),$5e.forEach(r),uTo=i(D),nb=s(D,"LI",{});var I5e=n(nb);Doe=s(I5e,"STRONG",{});var SVt=n(Doe);pTo=t(SVt,"mobilebert"),SVt.forEach(r),_To=t(I5e," \u2014 "),x$=s(I5e,"A",{href:!0});var PVt=n(x$);bTo=t(PVt,"MobileBertForMaskedLM"),PVt.forEach(r),vTo=t(I5e," (MobileBERT model)"),I5e.forEach(r),TTo=i(D),lb=s(D,"LI",{});var j5e=n(lb);Noe=s(j5e,"STRONG",{});var $Vt=n(Noe);FTo=t($Vt,"mpnet"),$Vt.forEach(r),CTo=t(j5e," \u2014 "),k$=s(j5e,"A",{href:!0});var IVt=n(k$);MTo=t(IVt,"MPNetForMaskedLM"),IVt.forEach(r),ETo=t(j5e," (MPNet model)"),j5e.forEach(r),yTo=i(D),ib=s(D,"LI",{});var D5e=n(ib);qoe=s(D5e,"STRONG",{});var jVt=n(qoe);wTo=t(jVt,"nystromformer"),jVt.forEach(r),ATo=t(D5e," \u2014 "),R$=s(D5e,"A",{href:!0});var DVt=n(R$);LTo=t(DVt,"NystromformerForMaskedLM"),DVt.forEach(r),BTo=t(D5e," (Nystromformer model)"),D5e.forEach(r),xTo=i(D),db=s(D,"LI",{});var N5e=n(db);Ooe=s(N5e,"STRONG",{});var NVt=n(Ooe);kTo=t(NVt,"perceiver"),NVt.forEach(r),RTo=t(N5e," \u2014 "),S$=s(N5e,"A",{href:!0});var qVt=n(S$);STo=t(qVt,"PerceiverForMaskedLM"),qVt.forEach(r),PTo=t(N5e," (Perceiver model)"),N5e.forEach(r),$To=i(D),cb=s(D,"LI",{});var q5e=n(cb);Goe=s(q5e,"STRONG",{});var OVt=n(Goe);ITo=t(OVt,"qdqbert"),OVt.forEach(r),jTo=t(q5e," \u2014 "),P$=s(q5e,"A",{href:!0});var GVt=n(P$);DTo=t(GVt,"QDQBertForMaskedLM"),GVt.forEach(r),NTo=t(q5e," (QDQBert model)"),q5e.forEach(r),qTo=i(D),mb=s(D,"LI",{});var O5e=n(mb);Xoe=s(O5e,"STRONG",{});var XVt=n(Xoe);OTo=t(XVt,"reformer"),XVt.forEach(r),GTo=t(O5e," \u2014 "),$$=s(O5e,"A",{href:!0});var VVt=n($$);XTo=t(VVt,"ReformerForMaskedLM"),VVt.forEach(r),VTo=t(O5e," (Reformer model)"),O5e.forEach(r),zTo=i(D),fb=s(D,"LI",{});var G5e=n(fb);Voe=s(G5e,"STRONG",{});var zVt=n(Voe);WTo=t(zVt,"rembert"),zVt.forEach(r),QTo=t(G5e," \u2014 "),I$=s(G5e,"A",{href:!0});var WVt=n(I$);HTo=t(WVt,"RemBertForMaskedLM"),WVt.forEach(r),UTo=t(G5e," (RemBERT model)"),G5e.forEach(r),JTo=i(D),gb=s(D,"LI",{});var X5e=n(gb);zoe=s(X5e,"STRONG",{});var QVt=n(zoe);YTo=t(QVt,"roberta"),QVt.forEach(r),KTo=t(X5e," \u2014 "),j$=s(X5e,"A",{href:!0});var HVt=n(j$);ZTo=t(HVt,"RobertaForMaskedLM"),HVt.forEach(r),e1o=t(X5e," (RoBERTa model)"),X5e.forEach(r),o1o=i(D),hb=s(D,"LI",{});var V5e=n(hb);Woe=s(V5e,"STRONG",{});var UVt=n(Woe);t1o=t(UVt,"roformer"),UVt.forEach(r),r1o=t(V5e," \u2014 "),D$=s(V5e,"A",{href:!0});var JVt=n(D$);a1o=t(JVt,"RoFormerForMaskedLM"),JVt.forEach(r),s1o=t(V5e," (RoFormer model)"),V5e.forEach(r),n1o=i(D),ub=s(D,"LI",{});var z5e=n(ub);Qoe=s(z5e,"STRONG",{});var YVt=n(Qoe);l1o=t(YVt,"squeezebert"),YVt.forEach(r),i1o=t(z5e," \u2014 "),N$=s(z5e,"A",{href:!0});var KVt=n(N$);d1o=t(KVt,"SqueezeBertForMaskedLM"),KVt.forEach(r),c1o=t(z5e," (SqueezeBERT model)"),z5e.forEach(r),m1o=i(D),pb=s(D,"LI",{});var W5e=n(pb);Hoe=s(W5e,"STRONG",{});var ZVt=n(Hoe);f1o=t(ZVt,"tapas"),ZVt.forEach(r),g1o=t(W5e," \u2014 "),q$=s(W5e,"A",{href:!0});var ezt=n(q$);h1o=t(ezt,"TapasForMaskedLM"),ezt.forEach(r),u1o=t(W5e," (TAPAS model)"),W5e.forEach(r),p1o=i(D),_b=s(D,"LI",{});var Q5e=n(_b);Uoe=s(Q5e,"STRONG",{});var ozt=n(Uoe);_1o=t(ozt,"wav2vec2"),ozt.forEach(r),b1o=t(Q5e," \u2014 "),Joe=s(Q5e,"CODE",{});var tzt=n(Joe);v1o=t(tzt,"Wav2Vec2ForMaskedLM"),tzt.forEach(r),T1o=t(Q5e,"(Wav2Vec2 model)"),Q5e.forEach(r),F1o=i(D),bb=s(D,"LI",{});var H5e=n(bb);Yoe=s(H5e,"STRONG",{});var rzt=n(Yoe);C1o=t(rzt,"xlm"),rzt.forEach(r),M1o=t(H5e," \u2014 "),O$=s(H5e,"A",{href:!0});var azt=n(O$);E1o=t(azt,"XLMWithLMHeadModel"),azt.forEach(r),y1o=t(H5e," (XLM model)"),H5e.forEach(r),w1o=i(D),vb=s(D,"LI",{});var U5e=n(vb);Koe=s(U5e,"STRONG",{});var szt=n(Koe);A1o=t(szt,"xlm-roberta"),szt.forEach(r),L1o=t(U5e," \u2014 "),G$=s(U5e,"A",{href:!0});var nzt=n(G$);B1o=t(nzt,"XLMRobertaForMaskedLM"),nzt.forEach(r),x1o=t(U5e," (XLM-RoBERTa model)"),U5e.forEach(r),k1o=i(D),Tb=s(D,"LI",{});var J5e=n(Tb);Zoe=s(J5e,"STRONG",{});var lzt=n(Zoe);R1o=t(lzt,"xlm-roberta-xl"),lzt.forEach(r),S1o=t(J5e," \u2014 "),X$=s(J5e,"A",{href:!0});var izt=n(X$);P1o=t(izt,"XLMRobertaXLForMaskedLM"),izt.forEach(r),$1o=t(J5e," (XLM-RoBERTa-XL model)"),J5e.forEach(r),I1o=i(D),Fb=s(D,"LI",{});var Y5e=n(Fb);ete=s(Y5e,"STRONG",{});var dzt=n(ete);j1o=t(dzt,"yoso"),dzt.forEach(r),D1o=t(Y5e," \u2014 "),V$=s(Y5e,"A",{href:!0});var czt=n(V$);N1o=t(czt,"YosoForMaskedLM"),czt.forEach(r),q1o=t(Y5e," (YOSO model)"),Y5e.forEach(r),D.forEach(r),O1o=i(Or),Cb=s(Or,"P",{});var K5e=n(Cb);G1o=t(K5e,"The model is set in evaluation mode by default using "),ote=s(K5e,"CODE",{});var mzt=n(ote);X1o=t(mzt,"model.eval()"),mzt.forEach(r),V1o=t(K5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tte=s(K5e,"CODE",{});var fzt=n(tte);z1o=t(fzt,"model.train()"),fzt.forEach(r),K5e.forEach(r),W1o=i(Or),rte=s(Or,"P",{});var gzt=n(rte);Q1o=t(gzt,"Examples:"),gzt.forEach(r),H1o=i(Or),f(zy.$$.fragment,Or),Or.forEach(r),Kn.forEach(r),zBe=i(d),id=s(d,"H2",{class:!0});var eRe=n(id);Mb=s(eRe,"A",{id:!0,class:!0,href:!0});var hzt=n(Mb);ate=s(hzt,"SPAN",{});var uzt=n(ate);f(Wy.$$.fragment,uzt),uzt.forEach(r),hzt.forEach(r),U1o=i(eRe),ste=s(eRe,"SPAN",{});var pzt=n(ste);J1o=t(pzt,"AutoModelForSeq2SeqLM"),pzt.forEach(r),eRe.forEach(r),WBe=i(d),Ko=s(d,"DIV",{class:!0});var el=n(Ko);f(Qy.$$.fragment,el),Y1o=i(el),dd=s(el,"P",{});var uz=n(dd);K1o=t(uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),nte=s(uz,"CODE",{});var _zt=n(nte);Z1o=t(_zt,"from_pretrained()"),_zt.forEach(r),eFo=t(uz,"class method or the "),lte=s(uz,"CODE",{});var bzt=n(lte);oFo=t(bzt,"from_config()"),bzt.forEach(r),tFo=t(uz,`class
method.`),uz.forEach(r),rFo=i(el),Hy=s(el,"P",{});var oRe=n(Hy);aFo=t(oRe,"This class cannot be instantiated directly using "),ite=s(oRe,"CODE",{});var vzt=n(ite);sFo=t(vzt,"__init__()"),vzt.forEach(r),nFo=t(oRe," (throws an error)."),oRe.forEach(r),lFo=i(el),Wt=s(el,"DIV",{class:!0});var ol=n(Wt);f(Uy.$$.fragment,ol),iFo=i(ol),dte=s(ol,"P",{});var Tzt=n(dte);dFo=t(Tzt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Tzt.forEach(r),cFo=i(ol),cd=s(ol,"P",{});var pz=n(cd);mFo=t(pz,`Note:
Loading a model from its configuration file does `),cte=s(pz,"STRONG",{});var Fzt=n(cte);fFo=t(Fzt,"not"),Fzt.forEach(r),gFo=t(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mte=s(pz,"CODE",{});var Czt=n(mte);hFo=t(Czt,"from_pretrained()"),Czt.forEach(r),uFo=t(pz,"to load the model weights."),pz.forEach(r),pFo=i(ol),fte=s(ol,"P",{});var Mzt=n(fte);_Fo=t(Mzt,"Examples:"),Mzt.forEach(r),bFo=i(ol),f(Jy.$$.fragment,ol),ol.forEach(r),vFo=i(el),Ie=s(el,"DIV",{class:!0});var Gr=n(Ie);f(Yy.$$.fragment,Gr),TFo=i(Gr),gte=s(Gr,"P",{});var Ezt=n(gte);FFo=t(Ezt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Ezt.forEach(r),CFo=i(Gr),Qa=s(Gr,"P",{});var z3=n(Qa);MFo=t(z3,"The model class to instantiate is selected based on the "),hte=s(z3,"CODE",{});var yzt=n(hte);EFo=t(yzt,"model_type"),yzt.forEach(r),yFo=t(z3,` property of the config object (either
passed as an argument or loaded from `),ute=s(z3,"CODE",{});var wzt=n(ute);wFo=t(wzt,"pretrained_model_name_or_path"),wzt.forEach(r),AFo=t(z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pte=s(z3,"CODE",{});var Azt=n(pte);LFo=t(Azt,"pretrained_model_name_or_path"),Azt.forEach(r),BFo=t(z3,":"),z3.forEach(r),xFo=i(Gr),se=s(Gr,"UL",{});var ie=n(se);Eb=s(ie,"LI",{});var Z5e=n(Eb);_te=s(Z5e,"STRONG",{});var Lzt=n(_te);kFo=t(Lzt,"bart"),Lzt.forEach(r),RFo=t(Z5e," \u2014 "),z$=s(Z5e,"A",{href:!0});var Bzt=n(z$);SFo=t(Bzt,"BartForConditionalGeneration"),Bzt.forEach(r),PFo=t(Z5e," (BART model)"),Z5e.forEach(r),$Fo=i(ie),yb=s(ie,"LI",{});var eye=n(yb);bte=s(eye,"STRONG",{});var xzt=n(bte);IFo=t(xzt,"bigbird_pegasus"),xzt.forEach(r),jFo=t(eye," \u2014 "),W$=s(eye,"A",{href:!0});var kzt=n(W$);DFo=t(kzt,"BigBirdPegasusForConditionalGeneration"),kzt.forEach(r),NFo=t(eye," (BigBirdPegasus model)"),eye.forEach(r),qFo=i(ie),wb=s(ie,"LI",{});var oye=n(wb);vte=s(oye,"STRONG",{});var Rzt=n(vte);OFo=t(Rzt,"blenderbot"),Rzt.forEach(r),GFo=t(oye," \u2014 "),Q$=s(oye,"A",{href:!0});var Szt=n(Q$);XFo=t(Szt,"BlenderbotForConditionalGeneration"),Szt.forEach(r),VFo=t(oye," (Blenderbot model)"),oye.forEach(r),zFo=i(ie),Ab=s(ie,"LI",{});var tye=n(Ab);Tte=s(tye,"STRONG",{});var Pzt=n(Tte);WFo=t(Pzt,"blenderbot-small"),Pzt.forEach(r),QFo=t(tye," \u2014 "),H$=s(tye,"A",{href:!0});var $zt=n(H$);HFo=t($zt,"BlenderbotSmallForConditionalGeneration"),$zt.forEach(r),UFo=t(tye," (BlenderbotSmall model)"),tye.forEach(r),JFo=i(ie),Lb=s(ie,"LI",{});var rye=n(Lb);Fte=s(rye,"STRONG",{});var Izt=n(Fte);YFo=t(Izt,"encoder-decoder"),Izt.forEach(r),KFo=t(rye," \u2014 "),U$=s(rye,"A",{href:!0});var jzt=n(U$);ZFo=t(jzt,"EncoderDecoderModel"),jzt.forEach(r),eCo=t(rye," (Encoder decoder model)"),rye.forEach(r),oCo=i(ie),Bb=s(ie,"LI",{});var aye=n(Bb);Cte=s(aye,"STRONG",{});var Dzt=n(Cte);tCo=t(Dzt,"fsmt"),Dzt.forEach(r),rCo=t(aye," \u2014 "),J$=s(aye,"A",{href:!0});var Nzt=n(J$);aCo=t(Nzt,"FSMTForConditionalGeneration"),Nzt.forEach(r),sCo=t(aye," (FairSeq Machine-Translation model)"),aye.forEach(r),nCo=i(ie),xb=s(ie,"LI",{});var sye=n(xb);Mte=s(sye,"STRONG",{});var qzt=n(Mte);lCo=t(qzt,"led"),qzt.forEach(r),iCo=t(sye," \u2014 "),Y$=s(sye,"A",{href:!0});var Ozt=n(Y$);dCo=t(Ozt,"LEDForConditionalGeneration"),Ozt.forEach(r),cCo=t(sye," (LED model)"),sye.forEach(r),mCo=i(ie),kb=s(ie,"LI",{});var nye=n(kb);Ete=s(nye,"STRONG",{});var Gzt=n(Ete);fCo=t(Gzt,"m2m_100"),Gzt.forEach(r),gCo=t(nye," \u2014 "),K$=s(nye,"A",{href:!0});var Xzt=n(K$);hCo=t(Xzt,"M2M100ForConditionalGeneration"),Xzt.forEach(r),uCo=t(nye," (M2M100 model)"),nye.forEach(r),pCo=i(ie),Rb=s(ie,"LI",{});var lye=n(Rb);yte=s(lye,"STRONG",{});var Vzt=n(yte);_Co=t(Vzt,"marian"),Vzt.forEach(r),bCo=t(lye," \u2014 "),Z$=s(lye,"A",{href:!0});var zzt=n(Z$);vCo=t(zzt,"MarianMTModel"),zzt.forEach(r),TCo=t(lye," (Marian model)"),lye.forEach(r),FCo=i(ie),Sb=s(ie,"LI",{});var iye=n(Sb);wte=s(iye,"STRONG",{});var Wzt=n(wte);CCo=t(Wzt,"mbart"),Wzt.forEach(r),MCo=t(iye," \u2014 "),eI=s(iye,"A",{href:!0});var Qzt=n(eI);ECo=t(Qzt,"MBartForConditionalGeneration"),Qzt.forEach(r),yCo=t(iye," (mBART model)"),iye.forEach(r),wCo=i(ie),Pb=s(ie,"LI",{});var dye=n(Pb);Ate=s(dye,"STRONG",{});var Hzt=n(Ate);ACo=t(Hzt,"mt5"),Hzt.forEach(r),LCo=t(dye," \u2014 "),oI=s(dye,"A",{href:!0});var Uzt=n(oI);BCo=t(Uzt,"MT5ForConditionalGeneration"),Uzt.forEach(r),xCo=t(dye," (mT5 model)"),dye.forEach(r),kCo=i(ie),$b=s(ie,"LI",{});var cye=n($b);Lte=s(cye,"STRONG",{});var Jzt=n(Lte);RCo=t(Jzt,"pegasus"),Jzt.forEach(r),SCo=t(cye," \u2014 "),tI=s(cye,"A",{href:!0});var Yzt=n(tI);PCo=t(Yzt,"PegasusForConditionalGeneration"),Yzt.forEach(r),$Co=t(cye," (Pegasus model)"),cye.forEach(r),ICo=i(ie),Ib=s(ie,"LI",{});var mye=n(Ib);Bte=s(mye,"STRONG",{});var Kzt=n(Bte);jCo=t(Kzt,"plbart"),Kzt.forEach(r),DCo=t(mye," \u2014 "),rI=s(mye,"A",{href:!0});var Zzt=n(rI);NCo=t(Zzt,"PLBartForConditionalGeneration"),Zzt.forEach(r),qCo=t(mye," (PLBart model)"),mye.forEach(r),OCo=i(ie),jb=s(ie,"LI",{});var fye=n(jb);xte=s(fye,"STRONG",{});var eWt=n(xte);GCo=t(eWt,"prophetnet"),eWt.forEach(r),XCo=t(fye," \u2014 "),aI=s(fye,"A",{href:!0});var oWt=n(aI);VCo=t(oWt,"ProphetNetForConditionalGeneration"),oWt.forEach(r),zCo=t(fye," (ProphetNet model)"),fye.forEach(r),WCo=i(ie),Db=s(ie,"LI",{});var gye=n(Db);kte=s(gye,"STRONG",{});var tWt=n(kte);QCo=t(tWt,"t5"),tWt.forEach(r),HCo=t(gye," \u2014 "),sI=s(gye,"A",{href:!0});var rWt=n(sI);UCo=t(rWt,"T5ForConditionalGeneration"),rWt.forEach(r),JCo=t(gye," (T5 model)"),gye.forEach(r),YCo=i(ie),Nb=s(ie,"LI",{});var hye=n(Nb);Rte=s(hye,"STRONG",{});var aWt=n(Rte);KCo=t(aWt,"xlm-prophetnet"),aWt.forEach(r),ZCo=t(hye," \u2014 "),nI=s(hye,"A",{href:!0});var sWt=n(nI);e4o=t(sWt,"XLMProphetNetForConditionalGeneration"),sWt.forEach(r),o4o=t(hye," (XLMProphetNet model)"),hye.forEach(r),ie.forEach(r),t4o=i(Gr),qb=s(Gr,"P",{});var uye=n(qb);r4o=t(uye,"The model is set in evaluation mode by default using "),Ste=s(uye,"CODE",{});var nWt=n(Ste);a4o=t(nWt,"model.eval()"),nWt.forEach(r),s4o=t(uye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pte=s(uye,"CODE",{});var lWt=n(Pte);n4o=t(lWt,"model.train()"),lWt.forEach(r),uye.forEach(r),l4o=i(Gr),$te=s(Gr,"P",{});var iWt=n($te);i4o=t(iWt,"Examples:"),iWt.forEach(r),d4o=i(Gr),f(Ky.$$.fragment,Gr),Gr.forEach(r),el.forEach(r),QBe=i(d),md=s(d,"H2",{class:!0});var tRe=n(md);Ob=s(tRe,"A",{id:!0,class:!0,href:!0});var dWt=n(Ob);Ite=s(dWt,"SPAN",{});var cWt=n(Ite);f(Zy.$$.fragment,cWt),cWt.forEach(r),dWt.forEach(r),c4o=i(tRe),jte=s(tRe,"SPAN",{});var mWt=n(jte);m4o=t(mWt,"AutoModelForSequenceClassification"),mWt.forEach(r),tRe.forEach(r),HBe=i(d),Zo=s(d,"DIV",{class:!0});var tl=n(Zo);f(ew.$$.fragment,tl),f4o=i(tl),fd=s(tl,"P",{});var _z=n(fd);g4o=t(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Dte=s(_z,"CODE",{});var fWt=n(Dte);h4o=t(fWt,"from_pretrained()"),fWt.forEach(r),u4o=t(_z,"class method or the "),Nte=s(_z,"CODE",{});var gWt=n(Nte);p4o=t(gWt,"from_config()"),gWt.forEach(r),_4o=t(_z,`class
method.`),_z.forEach(r),b4o=i(tl),ow=s(tl,"P",{});var rRe=n(ow);v4o=t(rRe,"This class cannot be instantiated directly using "),qte=s(rRe,"CODE",{});var hWt=n(qte);T4o=t(hWt,"__init__()"),hWt.forEach(r),F4o=t(rRe," (throws an error)."),rRe.forEach(r),C4o=i(tl),Qt=s(tl,"DIV",{class:!0});var rl=n(Qt);f(tw.$$.fragment,rl),M4o=i(rl),Ote=s(rl,"P",{});var uWt=n(Ote);E4o=t(uWt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),uWt.forEach(r),y4o=i(rl),gd=s(rl,"P",{});var bz=n(gd);w4o=t(bz,`Note:
Loading a model from its configuration file does `),Gte=s(bz,"STRONG",{});var pWt=n(Gte);A4o=t(pWt,"not"),pWt.forEach(r),L4o=t(bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xte=s(bz,"CODE",{});var _Wt=n(Xte);B4o=t(_Wt,"from_pretrained()"),_Wt.forEach(r),x4o=t(bz,"to load the model weights."),bz.forEach(r),k4o=i(rl),Vte=s(rl,"P",{});var bWt=n(Vte);R4o=t(bWt,"Examples:"),bWt.forEach(r),S4o=i(rl),f(rw.$$.fragment,rl),rl.forEach(r),P4o=i(tl),je=s(tl,"DIV",{class:!0});var Xr=n(je);f(aw.$$.fragment,Xr),$4o=i(Xr),zte=s(Xr,"P",{});var vWt=n(zte);I4o=t(vWt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),vWt.forEach(r),j4o=i(Xr),Ha=s(Xr,"P",{});var W3=n(Ha);D4o=t(W3,"The model class to instantiate is selected based on the "),Wte=s(W3,"CODE",{});var TWt=n(Wte);N4o=t(TWt,"model_type"),TWt.forEach(r),q4o=t(W3,` property of the config object (either
passed as an argument or loaded from `),Qte=s(W3,"CODE",{});var FWt=n(Qte);O4o=t(FWt,"pretrained_model_name_or_path"),FWt.forEach(r),G4o=t(W3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hte=s(W3,"CODE",{});var CWt=n(Hte);X4o=t(CWt,"pretrained_model_name_or_path"),CWt.forEach(r),V4o=t(W3,":"),W3.forEach(r),z4o=i(Xr),A=s(Xr,"UL",{});var L=n(A);Gb=s(L,"LI",{});var pye=n(Gb);Ute=s(pye,"STRONG",{});var MWt=n(Ute);W4o=t(MWt,"albert"),MWt.forEach(r),Q4o=t(pye," \u2014 "),lI=s(pye,"A",{href:!0});var EWt=n(lI);H4o=t(EWt,"AlbertForSequenceClassification"),EWt.forEach(r),U4o=t(pye," (ALBERT model)"),pye.forEach(r),J4o=i(L),Xb=s(L,"LI",{});var _ye=n(Xb);Jte=s(_ye,"STRONG",{});var yWt=n(Jte);Y4o=t(yWt,"bart"),yWt.forEach(r),K4o=t(_ye," \u2014 "),iI=s(_ye,"A",{href:!0});var wWt=n(iI);Z4o=t(wWt,"BartForSequenceClassification"),wWt.forEach(r),eMo=t(_ye," (BART model)"),_ye.forEach(r),oMo=i(L),Vb=s(L,"LI",{});var bye=n(Vb);Yte=s(bye,"STRONG",{});var AWt=n(Yte);tMo=t(AWt,"bert"),AWt.forEach(r),rMo=t(bye," \u2014 "),dI=s(bye,"A",{href:!0});var LWt=n(dI);aMo=t(LWt,"BertForSequenceClassification"),LWt.forEach(r),sMo=t(bye," (BERT model)"),bye.forEach(r),nMo=i(L),zb=s(L,"LI",{});var vye=n(zb);Kte=s(vye,"STRONG",{});var BWt=n(Kte);lMo=t(BWt,"big_bird"),BWt.forEach(r),iMo=t(vye," \u2014 "),cI=s(vye,"A",{href:!0});var xWt=n(cI);dMo=t(xWt,"BigBirdForSequenceClassification"),xWt.forEach(r),cMo=t(vye," (BigBird model)"),vye.forEach(r),mMo=i(L),Wb=s(L,"LI",{});var Tye=n(Wb);Zte=s(Tye,"STRONG",{});var kWt=n(Zte);fMo=t(kWt,"bigbird_pegasus"),kWt.forEach(r),gMo=t(Tye," \u2014 "),mI=s(Tye,"A",{href:!0});var RWt=n(mI);hMo=t(RWt,"BigBirdPegasusForSequenceClassification"),RWt.forEach(r),uMo=t(Tye," (BigBirdPegasus model)"),Tye.forEach(r),pMo=i(L),Qb=s(L,"LI",{});var Fye=n(Qb);ere=s(Fye,"STRONG",{});var SWt=n(ere);_Mo=t(SWt,"camembert"),SWt.forEach(r),bMo=t(Fye," \u2014 "),fI=s(Fye,"A",{href:!0});var PWt=n(fI);vMo=t(PWt,"CamembertForSequenceClassification"),PWt.forEach(r),TMo=t(Fye," (CamemBERT model)"),Fye.forEach(r),FMo=i(L),Hb=s(L,"LI",{});var Cye=n(Hb);ore=s(Cye,"STRONG",{});var $Wt=n(ore);CMo=t($Wt,"canine"),$Wt.forEach(r),MMo=t(Cye," \u2014 "),gI=s(Cye,"A",{href:!0});var IWt=n(gI);EMo=t(IWt,"CanineForSequenceClassification"),IWt.forEach(r),yMo=t(Cye," (Canine model)"),Cye.forEach(r),wMo=i(L),Ub=s(L,"LI",{});var Mye=n(Ub);tre=s(Mye,"STRONG",{});var jWt=n(tre);AMo=t(jWt,"convbert"),jWt.forEach(r),LMo=t(Mye," \u2014 "),hI=s(Mye,"A",{href:!0});var DWt=n(hI);BMo=t(DWt,"ConvBertForSequenceClassification"),DWt.forEach(r),xMo=t(Mye," (ConvBERT model)"),Mye.forEach(r),kMo=i(L),Jb=s(L,"LI",{});var Eye=n(Jb);rre=s(Eye,"STRONG",{});var NWt=n(rre);RMo=t(NWt,"ctrl"),NWt.forEach(r),SMo=t(Eye," \u2014 "),uI=s(Eye,"A",{href:!0});var qWt=n(uI);PMo=t(qWt,"CTRLForSequenceClassification"),qWt.forEach(r),$Mo=t(Eye," (CTRL model)"),Eye.forEach(r),IMo=i(L),Yb=s(L,"LI",{});var yye=n(Yb);are=s(yye,"STRONG",{});var OWt=n(are);jMo=t(OWt,"data2vec-text"),OWt.forEach(r),DMo=t(yye," \u2014 "),pI=s(yye,"A",{href:!0});var GWt=n(pI);NMo=t(GWt,"Data2VecTextForSequenceClassification"),GWt.forEach(r),qMo=t(yye," (Data2VecText model)"),yye.forEach(r),OMo=i(L),Kb=s(L,"LI",{});var wye=n(Kb);sre=s(wye,"STRONG",{});var XWt=n(sre);GMo=t(XWt,"deberta"),XWt.forEach(r),XMo=t(wye," \u2014 "),_I=s(wye,"A",{href:!0});var VWt=n(_I);VMo=t(VWt,"DebertaForSequenceClassification"),VWt.forEach(r),zMo=t(wye," (DeBERTa model)"),wye.forEach(r),WMo=i(L),Zb=s(L,"LI",{});var Aye=n(Zb);nre=s(Aye,"STRONG",{});var zWt=n(nre);QMo=t(zWt,"deberta-v2"),zWt.forEach(r),HMo=t(Aye," \u2014 "),bI=s(Aye,"A",{href:!0});var WWt=n(bI);UMo=t(WWt,"DebertaV2ForSequenceClassification"),WWt.forEach(r),JMo=t(Aye," (DeBERTa-v2 model)"),Aye.forEach(r),YMo=i(L),e2=s(L,"LI",{});var Lye=n(e2);lre=s(Lye,"STRONG",{});var QWt=n(lre);KMo=t(QWt,"distilbert"),QWt.forEach(r),ZMo=t(Lye," \u2014 "),vI=s(Lye,"A",{href:!0});var HWt=n(vI);eEo=t(HWt,"DistilBertForSequenceClassification"),HWt.forEach(r),oEo=t(Lye," (DistilBERT model)"),Lye.forEach(r),tEo=i(L),o2=s(L,"LI",{});var Bye=n(o2);ire=s(Bye,"STRONG",{});var UWt=n(ire);rEo=t(UWt,"electra"),UWt.forEach(r),aEo=t(Bye," \u2014 "),TI=s(Bye,"A",{href:!0});var JWt=n(TI);sEo=t(JWt,"ElectraForSequenceClassification"),JWt.forEach(r),nEo=t(Bye," (ELECTRA model)"),Bye.forEach(r),lEo=i(L),t2=s(L,"LI",{});var xye=n(t2);dre=s(xye,"STRONG",{});var YWt=n(dre);iEo=t(YWt,"flaubert"),YWt.forEach(r),dEo=t(xye," \u2014 "),FI=s(xye,"A",{href:!0});var KWt=n(FI);cEo=t(KWt,"FlaubertForSequenceClassification"),KWt.forEach(r),mEo=t(xye," (FlauBERT model)"),xye.forEach(r),fEo=i(L),r2=s(L,"LI",{});var kye=n(r2);cre=s(kye,"STRONG",{});var ZWt=n(cre);gEo=t(ZWt,"fnet"),ZWt.forEach(r),hEo=t(kye," \u2014 "),CI=s(kye,"A",{href:!0});var eQt=n(CI);uEo=t(eQt,"FNetForSequenceClassification"),eQt.forEach(r),pEo=t(kye," (FNet model)"),kye.forEach(r),_Eo=i(L),a2=s(L,"LI",{});var Rye=n(a2);mre=s(Rye,"STRONG",{});var oQt=n(mre);bEo=t(oQt,"funnel"),oQt.forEach(r),vEo=t(Rye," \u2014 "),MI=s(Rye,"A",{href:!0});var tQt=n(MI);TEo=t(tQt,"FunnelForSequenceClassification"),tQt.forEach(r),FEo=t(Rye," (Funnel Transformer model)"),Rye.forEach(r),CEo=i(L),s2=s(L,"LI",{});var Sye=n(s2);fre=s(Sye,"STRONG",{});var rQt=n(fre);MEo=t(rQt,"gpt2"),rQt.forEach(r),EEo=t(Sye," \u2014 "),EI=s(Sye,"A",{href:!0});var aQt=n(EI);yEo=t(aQt,"GPT2ForSequenceClassification"),aQt.forEach(r),wEo=t(Sye," (OpenAI GPT-2 model)"),Sye.forEach(r),AEo=i(L),n2=s(L,"LI",{});var Pye=n(n2);gre=s(Pye,"STRONG",{});var sQt=n(gre);LEo=t(sQt,"gpt_neo"),sQt.forEach(r),BEo=t(Pye," \u2014 "),yI=s(Pye,"A",{href:!0});var nQt=n(yI);xEo=t(nQt,"GPTNeoForSequenceClassification"),nQt.forEach(r),kEo=t(Pye," (GPT Neo model)"),Pye.forEach(r),REo=i(L),l2=s(L,"LI",{});var $ye=n(l2);hre=s($ye,"STRONG",{});var lQt=n(hre);SEo=t(lQt,"gptj"),lQt.forEach(r),PEo=t($ye," \u2014 "),wI=s($ye,"A",{href:!0});var iQt=n(wI);$Eo=t(iQt,"GPTJForSequenceClassification"),iQt.forEach(r),IEo=t($ye," (GPT-J model)"),$ye.forEach(r),jEo=i(L),i2=s(L,"LI",{});var Iye=n(i2);ure=s(Iye,"STRONG",{});var dQt=n(ure);DEo=t(dQt,"ibert"),dQt.forEach(r),NEo=t(Iye," \u2014 "),AI=s(Iye,"A",{href:!0});var cQt=n(AI);qEo=t(cQt,"IBertForSequenceClassification"),cQt.forEach(r),OEo=t(Iye," (I-BERT model)"),Iye.forEach(r),GEo=i(L),d2=s(L,"LI",{});var jye=n(d2);pre=s(jye,"STRONG",{});var mQt=n(pre);XEo=t(mQt,"layoutlm"),mQt.forEach(r),VEo=t(jye," \u2014 "),LI=s(jye,"A",{href:!0});var fQt=n(LI);zEo=t(fQt,"LayoutLMForSequenceClassification"),fQt.forEach(r),WEo=t(jye," (LayoutLM model)"),jye.forEach(r),QEo=i(L),c2=s(L,"LI",{});var Dye=n(c2);_re=s(Dye,"STRONG",{});var gQt=n(_re);HEo=t(gQt,"layoutlmv2"),gQt.forEach(r),UEo=t(Dye," \u2014 "),BI=s(Dye,"A",{href:!0});var hQt=n(BI);JEo=t(hQt,"LayoutLMv2ForSequenceClassification"),hQt.forEach(r),YEo=t(Dye," (LayoutLMv2 model)"),Dye.forEach(r),KEo=i(L),m2=s(L,"LI",{});var Nye=n(m2);bre=s(Nye,"STRONG",{});var uQt=n(bre);ZEo=t(uQt,"led"),uQt.forEach(r),e3o=t(Nye," \u2014 "),xI=s(Nye,"A",{href:!0});var pQt=n(xI);o3o=t(pQt,"LEDForSequenceClassification"),pQt.forEach(r),t3o=t(Nye," (LED model)"),Nye.forEach(r),r3o=i(L),f2=s(L,"LI",{});var qye=n(f2);vre=s(qye,"STRONG",{});var _Qt=n(vre);a3o=t(_Qt,"longformer"),_Qt.forEach(r),s3o=t(qye," \u2014 "),kI=s(qye,"A",{href:!0});var bQt=n(kI);n3o=t(bQt,"LongformerForSequenceClassification"),bQt.forEach(r),l3o=t(qye," (Longformer model)"),qye.forEach(r),i3o=i(L),g2=s(L,"LI",{});var Oye=n(g2);Tre=s(Oye,"STRONG",{});var vQt=n(Tre);d3o=t(vQt,"mbart"),vQt.forEach(r),c3o=t(Oye," \u2014 "),RI=s(Oye,"A",{href:!0});var TQt=n(RI);m3o=t(TQt,"MBartForSequenceClassification"),TQt.forEach(r),f3o=t(Oye," (mBART model)"),Oye.forEach(r),g3o=i(L),h2=s(L,"LI",{});var Gye=n(h2);Fre=s(Gye,"STRONG",{});var FQt=n(Fre);h3o=t(FQt,"megatron-bert"),FQt.forEach(r),u3o=t(Gye," \u2014 "),SI=s(Gye,"A",{href:!0});var CQt=n(SI);p3o=t(CQt,"MegatronBertForSequenceClassification"),CQt.forEach(r),_3o=t(Gye," (MegatronBert model)"),Gye.forEach(r),b3o=i(L),u2=s(L,"LI",{});var Xye=n(u2);Cre=s(Xye,"STRONG",{});var MQt=n(Cre);v3o=t(MQt,"mobilebert"),MQt.forEach(r),T3o=t(Xye," \u2014 "),PI=s(Xye,"A",{href:!0});var EQt=n(PI);F3o=t(EQt,"MobileBertForSequenceClassification"),EQt.forEach(r),C3o=t(Xye," (MobileBERT model)"),Xye.forEach(r),M3o=i(L),p2=s(L,"LI",{});var Vye=n(p2);Mre=s(Vye,"STRONG",{});var yQt=n(Mre);E3o=t(yQt,"mpnet"),yQt.forEach(r),y3o=t(Vye," \u2014 "),$I=s(Vye,"A",{href:!0});var wQt=n($I);w3o=t(wQt,"MPNetForSequenceClassification"),wQt.forEach(r),A3o=t(Vye," (MPNet model)"),Vye.forEach(r),L3o=i(L),_2=s(L,"LI",{});var zye=n(_2);Ere=s(zye,"STRONG",{});var AQt=n(Ere);B3o=t(AQt,"nystromformer"),AQt.forEach(r),x3o=t(zye," \u2014 "),II=s(zye,"A",{href:!0});var LQt=n(II);k3o=t(LQt,"NystromformerForSequenceClassification"),LQt.forEach(r),R3o=t(zye," (Nystromformer model)"),zye.forEach(r),S3o=i(L),b2=s(L,"LI",{});var Wye=n(b2);yre=s(Wye,"STRONG",{});var BQt=n(yre);P3o=t(BQt,"openai-gpt"),BQt.forEach(r),$3o=t(Wye," \u2014 "),jI=s(Wye,"A",{href:!0});var xQt=n(jI);I3o=t(xQt,"OpenAIGPTForSequenceClassification"),xQt.forEach(r),j3o=t(Wye," (OpenAI GPT model)"),Wye.forEach(r),D3o=i(L),v2=s(L,"LI",{});var Qye=n(v2);wre=s(Qye,"STRONG",{});var kQt=n(wre);N3o=t(kQt,"perceiver"),kQt.forEach(r),q3o=t(Qye," \u2014 "),DI=s(Qye,"A",{href:!0});var RQt=n(DI);O3o=t(RQt,"PerceiverForSequenceClassification"),RQt.forEach(r),G3o=t(Qye," (Perceiver model)"),Qye.forEach(r),X3o=i(L),T2=s(L,"LI",{});var Hye=n(T2);Are=s(Hye,"STRONG",{});var SQt=n(Are);V3o=t(SQt,"plbart"),SQt.forEach(r),z3o=t(Hye," \u2014 "),NI=s(Hye,"A",{href:!0});var PQt=n(NI);W3o=t(PQt,"PLBartForSequenceClassification"),PQt.forEach(r),Q3o=t(Hye," (PLBart model)"),Hye.forEach(r),H3o=i(L),F2=s(L,"LI",{});var Uye=n(F2);Lre=s(Uye,"STRONG",{});var $Qt=n(Lre);U3o=t($Qt,"qdqbert"),$Qt.forEach(r),J3o=t(Uye," \u2014 "),qI=s(Uye,"A",{href:!0});var IQt=n(qI);Y3o=t(IQt,"QDQBertForSequenceClassification"),IQt.forEach(r),K3o=t(Uye," (QDQBert model)"),Uye.forEach(r),Z3o=i(L),C2=s(L,"LI",{});var Jye=n(C2);Bre=s(Jye,"STRONG",{});var jQt=n(Bre);e5o=t(jQt,"reformer"),jQt.forEach(r),o5o=t(Jye," \u2014 "),OI=s(Jye,"A",{href:!0});var DQt=n(OI);t5o=t(DQt,"ReformerForSequenceClassification"),DQt.forEach(r),r5o=t(Jye," (Reformer model)"),Jye.forEach(r),a5o=i(L),M2=s(L,"LI",{});var Yye=n(M2);xre=s(Yye,"STRONG",{});var NQt=n(xre);s5o=t(NQt,"rembert"),NQt.forEach(r),n5o=t(Yye," \u2014 "),GI=s(Yye,"A",{href:!0});var qQt=n(GI);l5o=t(qQt,"RemBertForSequenceClassification"),qQt.forEach(r),i5o=t(Yye," (RemBERT model)"),Yye.forEach(r),d5o=i(L),E2=s(L,"LI",{});var Kye=n(E2);kre=s(Kye,"STRONG",{});var OQt=n(kre);c5o=t(OQt,"roberta"),OQt.forEach(r),m5o=t(Kye," \u2014 "),XI=s(Kye,"A",{href:!0});var GQt=n(XI);f5o=t(GQt,"RobertaForSequenceClassification"),GQt.forEach(r),g5o=t(Kye," (RoBERTa model)"),Kye.forEach(r),h5o=i(L),y2=s(L,"LI",{});var Zye=n(y2);Rre=s(Zye,"STRONG",{});var XQt=n(Rre);u5o=t(XQt,"roformer"),XQt.forEach(r),p5o=t(Zye," \u2014 "),VI=s(Zye,"A",{href:!0});var VQt=n(VI);_5o=t(VQt,"RoFormerForSequenceClassification"),VQt.forEach(r),b5o=t(Zye," (RoFormer model)"),Zye.forEach(r),v5o=i(L),w2=s(L,"LI",{});var ewe=n(w2);Sre=s(ewe,"STRONG",{});var zQt=n(Sre);T5o=t(zQt,"squeezebert"),zQt.forEach(r),F5o=t(ewe," \u2014 "),zI=s(ewe,"A",{href:!0});var WQt=n(zI);C5o=t(WQt,"SqueezeBertForSequenceClassification"),WQt.forEach(r),M5o=t(ewe," (SqueezeBERT model)"),ewe.forEach(r),E5o=i(L),A2=s(L,"LI",{});var owe=n(A2);Pre=s(owe,"STRONG",{});var QQt=n(Pre);y5o=t(QQt,"tapas"),QQt.forEach(r),w5o=t(owe," \u2014 "),WI=s(owe,"A",{href:!0});var HQt=n(WI);A5o=t(HQt,"TapasForSequenceClassification"),HQt.forEach(r),L5o=t(owe," (TAPAS model)"),owe.forEach(r),B5o=i(L),L2=s(L,"LI",{});var twe=n(L2);$re=s(twe,"STRONG",{});var UQt=n($re);x5o=t(UQt,"transfo-xl"),UQt.forEach(r),k5o=t(twe," \u2014 "),QI=s(twe,"A",{href:!0});var JQt=n(QI);R5o=t(JQt,"TransfoXLForSequenceClassification"),JQt.forEach(r),S5o=t(twe," (Transformer-XL model)"),twe.forEach(r),P5o=i(L),B2=s(L,"LI",{});var rwe=n(B2);Ire=s(rwe,"STRONG",{});var YQt=n(Ire);$5o=t(YQt,"xlm"),YQt.forEach(r),I5o=t(rwe," \u2014 "),HI=s(rwe,"A",{href:!0});var KQt=n(HI);j5o=t(KQt,"XLMForSequenceClassification"),KQt.forEach(r),D5o=t(rwe," (XLM model)"),rwe.forEach(r),N5o=i(L),x2=s(L,"LI",{});var awe=n(x2);jre=s(awe,"STRONG",{});var ZQt=n(jre);q5o=t(ZQt,"xlm-roberta"),ZQt.forEach(r),O5o=t(awe," \u2014 "),UI=s(awe,"A",{href:!0});var eHt=n(UI);G5o=t(eHt,"XLMRobertaForSequenceClassification"),eHt.forEach(r),X5o=t(awe," (XLM-RoBERTa model)"),awe.forEach(r),V5o=i(L),k2=s(L,"LI",{});var swe=n(k2);Dre=s(swe,"STRONG",{});var oHt=n(Dre);z5o=t(oHt,"xlm-roberta-xl"),oHt.forEach(r),W5o=t(swe," \u2014 "),JI=s(swe,"A",{href:!0});var tHt=n(JI);Q5o=t(tHt,"XLMRobertaXLForSequenceClassification"),tHt.forEach(r),H5o=t(swe," (XLM-RoBERTa-XL model)"),swe.forEach(r),U5o=i(L),R2=s(L,"LI",{});var nwe=n(R2);Nre=s(nwe,"STRONG",{});var rHt=n(Nre);J5o=t(rHt,"xlnet"),rHt.forEach(r),Y5o=t(nwe," \u2014 "),YI=s(nwe,"A",{href:!0});var aHt=n(YI);K5o=t(aHt,"XLNetForSequenceClassification"),aHt.forEach(r),Z5o=t(nwe," (XLNet model)"),nwe.forEach(r),eyo=i(L),S2=s(L,"LI",{});var lwe=n(S2);qre=s(lwe,"STRONG",{});var sHt=n(qre);oyo=t(sHt,"yoso"),sHt.forEach(r),tyo=t(lwe," \u2014 "),KI=s(lwe,"A",{href:!0});var nHt=n(KI);ryo=t(nHt,"YosoForSequenceClassification"),nHt.forEach(r),ayo=t(lwe," (YOSO model)"),lwe.forEach(r),L.forEach(r),syo=i(Xr),P2=s(Xr,"P",{});var iwe=n(P2);nyo=t(iwe,"The model is set in evaluation mode by default using "),Ore=s(iwe,"CODE",{});var lHt=n(Ore);lyo=t(lHt,"model.eval()"),lHt.forEach(r),iyo=t(iwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Gre=s(iwe,"CODE",{});var iHt=n(Gre);dyo=t(iHt,"model.train()"),iHt.forEach(r),iwe.forEach(r),cyo=i(Xr),Xre=s(Xr,"P",{});var dHt=n(Xre);myo=t(dHt,"Examples:"),dHt.forEach(r),fyo=i(Xr),f(sw.$$.fragment,Xr),Xr.forEach(r),tl.forEach(r),UBe=i(d),hd=s(d,"H2",{class:!0});var aRe=n(hd);$2=s(aRe,"A",{id:!0,class:!0,href:!0});var cHt=n($2);Vre=s(cHt,"SPAN",{});var mHt=n(Vre);f(nw.$$.fragment,mHt),mHt.forEach(r),cHt.forEach(r),gyo=i(aRe),zre=s(aRe,"SPAN",{});var fHt=n(zre);hyo=t(fHt,"AutoModelForMultipleChoice"),fHt.forEach(r),aRe.forEach(r),JBe=i(d),et=s(d,"DIV",{class:!0});var al=n(et);f(lw.$$.fragment,al),uyo=i(al),ud=s(al,"P",{});var vz=n(ud);pyo=t(vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Wre=s(vz,"CODE",{});var gHt=n(Wre);_yo=t(gHt,"from_pretrained()"),gHt.forEach(r),byo=t(vz,"class method or the "),Qre=s(vz,"CODE",{});var hHt=n(Qre);vyo=t(hHt,"from_config()"),hHt.forEach(r),Tyo=t(vz,`class
method.`),vz.forEach(r),Fyo=i(al),iw=s(al,"P",{});var sRe=n(iw);Cyo=t(sRe,"This class cannot be instantiated directly using "),Hre=s(sRe,"CODE",{});var uHt=n(Hre);Myo=t(uHt,"__init__()"),uHt.forEach(r),Eyo=t(sRe," (throws an error)."),sRe.forEach(r),yyo=i(al),Ht=s(al,"DIV",{class:!0});var sl=n(Ht);f(dw.$$.fragment,sl),wyo=i(sl),Ure=s(sl,"P",{});var pHt=n(Ure);Ayo=t(pHt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),pHt.forEach(r),Lyo=i(sl),pd=s(sl,"P",{});var Tz=n(pd);Byo=t(Tz,`Note:
Loading a model from its configuration file does `),Jre=s(Tz,"STRONG",{});var _Ht=n(Jre);xyo=t(_Ht,"not"),_Ht.forEach(r),kyo=t(Tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yre=s(Tz,"CODE",{});var bHt=n(Yre);Ryo=t(bHt,"from_pretrained()"),bHt.forEach(r),Syo=t(Tz,"to load the model weights."),Tz.forEach(r),Pyo=i(sl),Kre=s(sl,"P",{});var vHt=n(Kre);$yo=t(vHt,"Examples:"),vHt.forEach(r),Iyo=i(sl),f(cw.$$.fragment,sl),sl.forEach(r),jyo=i(al),De=s(al,"DIV",{class:!0});var Vr=n(De);f(mw.$$.fragment,Vr),Dyo=i(Vr),Zre=s(Vr,"P",{});var THt=n(Zre);Nyo=t(THt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),THt.forEach(r),qyo=i(Vr),Ua=s(Vr,"P",{});var Q3=n(Ua);Oyo=t(Q3,"The model class to instantiate is selected based on the "),eae=s(Q3,"CODE",{});var FHt=n(eae);Gyo=t(FHt,"model_type"),FHt.forEach(r),Xyo=t(Q3,` property of the config object (either
passed as an argument or loaded from `),oae=s(Q3,"CODE",{});var CHt=n(oae);Vyo=t(CHt,"pretrained_model_name_or_path"),CHt.forEach(r),zyo=t(Q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tae=s(Q3,"CODE",{});var MHt=n(tae);Wyo=t(MHt,"pretrained_model_name_or_path"),MHt.forEach(r),Qyo=t(Q3,":"),Q3.forEach(r),Hyo=i(Vr),O=s(Vr,"UL",{});var G=n(O);I2=s(G,"LI",{});var dwe=n(I2);rae=s(dwe,"STRONG",{});var EHt=n(rae);Uyo=t(EHt,"albert"),EHt.forEach(r),Jyo=t(dwe," \u2014 "),ZI=s(dwe,"A",{href:!0});var yHt=n(ZI);Yyo=t(yHt,"AlbertForMultipleChoice"),yHt.forEach(r),Kyo=t(dwe," (ALBERT model)"),dwe.forEach(r),Zyo=i(G),j2=s(G,"LI",{});var cwe=n(j2);aae=s(cwe,"STRONG",{});var wHt=n(aae);ewo=t(wHt,"bert"),wHt.forEach(r),owo=t(cwe," \u2014 "),ej=s(cwe,"A",{href:!0});var AHt=n(ej);two=t(AHt,"BertForMultipleChoice"),AHt.forEach(r),rwo=t(cwe," (BERT model)"),cwe.forEach(r),awo=i(G),D2=s(G,"LI",{});var mwe=n(D2);sae=s(mwe,"STRONG",{});var LHt=n(sae);swo=t(LHt,"big_bird"),LHt.forEach(r),nwo=t(mwe," \u2014 "),oj=s(mwe,"A",{href:!0});var BHt=n(oj);lwo=t(BHt,"BigBirdForMultipleChoice"),BHt.forEach(r),iwo=t(mwe," (BigBird model)"),mwe.forEach(r),dwo=i(G),N2=s(G,"LI",{});var fwe=n(N2);nae=s(fwe,"STRONG",{});var xHt=n(nae);cwo=t(xHt,"camembert"),xHt.forEach(r),mwo=t(fwe," \u2014 "),tj=s(fwe,"A",{href:!0});var kHt=n(tj);fwo=t(kHt,"CamembertForMultipleChoice"),kHt.forEach(r),gwo=t(fwe," (CamemBERT model)"),fwe.forEach(r),hwo=i(G),q2=s(G,"LI",{});var gwe=n(q2);lae=s(gwe,"STRONG",{});var RHt=n(lae);uwo=t(RHt,"canine"),RHt.forEach(r),pwo=t(gwe," \u2014 "),rj=s(gwe,"A",{href:!0});var SHt=n(rj);_wo=t(SHt,"CanineForMultipleChoice"),SHt.forEach(r),bwo=t(gwe," (Canine model)"),gwe.forEach(r),vwo=i(G),O2=s(G,"LI",{});var hwe=n(O2);iae=s(hwe,"STRONG",{});var PHt=n(iae);Two=t(PHt,"convbert"),PHt.forEach(r),Fwo=t(hwe," \u2014 "),aj=s(hwe,"A",{href:!0});var $Ht=n(aj);Cwo=t($Ht,"ConvBertForMultipleChoice"),$Ht.forEach(r),Mwo=t(hwe," (ConvBERT model)"),hwe.forEach(r),Ewo=i(G),G2=s(G,"LI",{});var uwe=n(G2);dae=s(uwe,"STRONG",{});var IHt=n(dae);ywo=t(IHt,"data2vec-text"),IHt.forEach(r),wwo=t(uwe," \u2014 "),sj=s(uwe,"A",{href:!0});var jHt=n(sj);Awo=t(jHt,"Data2VecTextForMultipleChoice"),jHt.forEach(r),Lwo=t(uwe," (Data2VecText model)"),uwe.forEach(r),Bwo=i(G),X2=s(G,"LI",{});var pwe=n(X2);cae=s(pwe,"STRONG",{});var DHt=n(cae);xwo=t(DHt,"distilbert"),DHt.forEach(r),kwo=t(pwe," \u2014 "),nj=s(pwe,"A",{href:!0});var NHt=n(nj);Rwo=t(NHt,"DistilBertForMultipleChoice"),NHt.forEach(r),Swo=t(pwe," (DistilBERT model)"),pwe.forEach(r),Pwo=i(G),V2=s(G,"LI",{});var _we=n(V2);mae=s(_we,"STRONG",{});var qHt=n(mae);$wo=t(qHt,"electra"),qHt.forEach(r),Iwo=t(_we," \u2014 "),lj=s(_we,"A",{href:!0});var OHt=n(lj);jwo=t(OHt,"ElectraForMultipleChoice"),OHt.forEach(r),Dwo=t(_we," (ELECTRA model)"),_we.forEach(r),Nwo=i(G),z2=s(G,"LI",{});var bwe=n(z2);fae=s(bwe,"STRONG",{});var GHt=n(fae);qwo=t(GHt,"flaubert"),GHt.forEach(r),Owo=t(bwe," \u2014 "),ij=s(bwe,"A",{href:!0});var XHt=n(ij);Gwo=t(XHt,"FlaubertForMultipleChoice"),XHt.forEach(r),Xwo=t(bwe," (FlauBERT model)"),bwe.forEach(r),Vwo=i(G),W2=s(G,"LI",{});var vwe=n(W2);gae=s(vwe,"STRONG",{});var VHt=n(gae);zwo=t(VHt,"fnet"),VHt.forEach(r),Wwo=t(vwe," \u2014 "),dj=s(vwe,"A",{href:!0});var zHt=n(dj);Qwo=t(zHt,"FNetForMultipleChoice"),zHt.forEach(r),Hwo=t(vwe," (FNet model)"),vwe.forEach(r),Uwo=i(G),Q2=s(G,"LI",{});var Twe=n(Q2);hae=s(Twe,"STRONG",{});var WHt=n(hae);Jwo=t(WHt,"funnel"),WHt.forEach(r),Ywo=t(Twe," \u2014 "),cj=s(Twe,"A",{href:!0});var QHt=n(cj);Kwo=t(QHt,"FunnelForMultipleChoice"),QHt.forEach(r),Zwo=t(Twe," (Funnel Transformer model)"),Twe.forEach(r),e6o=i(G),H2=s(G,"LI",{});var Fwe=n(H2);uae=s(Fwe,"STRONG",{});var HHt=n(uae);o6o=t(HHt,"ibert"),HHt.forEach(r),t6o=t(Fwe," \u2014 "),mj=s(Fwe,"A",{href:!0});var UHt=n(mj);r6o=t(UHt,"IBertForMultipleChoice"),UHt.forEach(r),a6o=t(Fwe," (I-BERT model)"),Fwe.forEach(r),s6o=i(G),U2=s(G,"LI",{});var Cwe=n(U2);pae=s(Cwe,"STRONG",{});var JHt=n(pae);n6o=t(JHt,"longformer"),JHt.forEach(r),l6o=t(Cwe," \u2014 "),fj=s(Cwe,"A",{href:!0});var YHt=n(fj);i6o=t(YHt,"LongformerForMultipleChoice"),YHt.forEach(r),d6o=t(Cwe," (Longformer model)"),Cwe.forEach(r),c6o=i(G),J2=s(G,"LI",{});var Mwe=n(J2);_ae=s(Mwe,"STRONG",{});var KHt=n(_ae);m6o=t(KHt,"megatron-bert"),KHt.forEach(r),f6o=t(Mwe," \u2014 "),gj=s(Mwe,"A",{href:!0});var ZHt=n(gj);g6o=t(ZHt,"MegatronBertForMultipleChoice"),ZHt.forEach(r),h6o=t(Mwe," (MegatronBert model)"),Mwe.forEach(r),u6o=i(G),Y2=s(G,"LI",{});var Ewe=n(Y2);bae=s(Ewe,"STRONG",{});var eUt=n(bae);p6o=t(eUt,"mobilebert"),eUt.forEach(r),_6o=t(Ewe," \u2014 "),hj=s(Ewe,"A",{href:!0});var oUt=n(hj);b6o=t(oUt,"MobileBertForMultipleChoice"),oUt.forEach(r),v6o=t(Ewe," (MobileBERT model)"),Ewe.forEach(r),T6o=i(G),K2=s(G,"LI",{});var ywe=n(K2);vae=s(ywe,"STRONG",{});var tUt=n(vae);F6o=t(tUt,"mpnet"),tUt.forEach(r),C6o=t(ywe," \u2014 "),uj=s(ywe,"A",{href:!0});var rUt=n(uj);M6o=t(rUt,"MPNetForMultipleChoice"),rUt.forEach(r),E6o=t(ywe," (MPNet model)"),ywe.forEach(r),y6o=i(G),Z2=s(G,"LI",{});var wwe=n(Z2);Tae=s(wwe,"STRONG",{});var aUt=n(Tae);w6o=t(aUt,"nystromformer"),aUt.forEach(r),A6o=t(wwe," \u2014 "),pj=s(wwe,"A",{href:!0});var sUt=n(pj);L6o=t(sUt,"NystromformerForMultipleChoice"),sUt.forEach(r),B6o=t(wwe," (Nystromformer model)"),wwe.forEach(r),x6o=i(G),ev=s(G,"LI",{});var Awe=n(ev);Fae=s(Awe,"STRONG",{});var nUt=n(Fae);k6o=t(nUt,"qdqbert"),nUt.forEach(r),R6o=t(Awe," \u2014 "),_j=s(Awe,"A",{href:!0});var lUt=n(_j);S6o=t(lUt,"QDQBertForMultipleChoice"),lUt.forEach(r),P6o=t(Awe," (QDQBert model)"),Awe.forEach(r),$6o=i(G),ov=s(G,"LI",{});var Lwe=n(ov);Cae=s(Lwe,"STRONG",{});var iUt=n(Cae);I6o=t(iUt,"rembert"),iUt.forEach(r),j6o=t(Lwe," \u2014 "),bj=s(Lwe,"A",{href:!0});var dUt=n(bj);D6o=t(dUt,"RemBertForMultipleChoice"),dUt.forEach(r),N6o=t(Lwe," (RemBERT model)"),Lwe.forEach(r),q6o=i(G),tv=s(G,"LI",{});var Bwe=n(tv);Mae=s(Bwe,"STRONG",{});var cUt=n(Mae);O6o=t(cUt,"roberta"),cUt.forEach(r),G6o=t(Bwe," \u2014 "),vj=s(Bwe,"A",{href:!0});var mUt=n(vj);X6o=t(mUt,"RobertaForMultipleChoice"),mUt.forEach(r),V6o=t(Bwe," (RoBERTa model)"),Bwe.forEach(r),z6o=i(G),rv=s(G,"LI",{});var xwe=n(rv);Eae=s(xwe,"STRONG",{});var fUt=n(Eae);W6o=t(fUt,"roformer"),fUt.forEach(r),Q6o=t(xwe," \u2014 "),Tj=s(xwe,"A",{href:!0});var gUt=n(Tj);H6o=t(gUt,"RoFormerForMultipleChoice"),gUt.forEach(r),U6o=t(xwe," (RoFormer model)"),xwe.forEach(r),J6o=i(G),av=s(G,"LI",{});var kwe=n(av);yae=s(kwe,"STRONG",{});var hUt=n(yae);Y6o=t(hUt,"squeezebert"),hUt.forEach(r),K6o=t(kwe," \u2014 "),Fj=s(kwe,"A",{href:!0});var uUt=n(Fj);Z6o=t(uUt,"SqueezeBertForMultipleChoice"),uUt.forEach(r),eAo=t(kwe," (SqueezeBERT model)"),kwe.forEach(r),oAo=i(G),sv=s(G,"LI",{});var Rwe=n(sv);wae=s(Rwe,"STRONG",{});var pUt=n(wae);tAo=t(pUt,"xlm"),pUt.forEach(r),rAo=t(Rwe," \u2014 "),Cj=s(Rwe,"A",{href:!0});var _Ut=n(Cj);aAo=t(_Ut,"XLMForMultipleChoice"),_Ut.forEach(r),sAo=t(Rwe," (XLM model)"),Rwe.forEach(r),nAo=i(G),nv=s(G,"LI",{});var Swe=n(nv);Aae=s(Swe,"STRONG",{});var bUt=n(Aae);lAo=t(bUt,"xlm-roberta"),bUt.forEach(r),iAo=t(Swe," \u2014 "),Mj=s(Swe,"A",{href:!0});var vUt=n(Mj);dAo=t(vUt,"XLMRobertaForMultipleChoice"),vUt.forEach(r),cAo=t(Swe," (XLM-RoBERTa model)"),Swe.forEach(r),mAo=i(G),lv=s(G,"LI",{});var Pwe=n(lv);Lae=s(Pwe,"STRONG",{});var TUt=n(Lae);fAo=t(TUt,"xlm-roberta-xl"),TUt.forEach(r),gAo=t(Pwe," \u2014 "),Ej=s(Pwe,"A",{href:!0});var FUt=n(Ej);hAo=t(FUt,"XLMRobertaXLForMultipleChoice"),FUt.forEach(r),uAo=t(Pwe," (XLM-RoBERTa-XL model)"),Pwe.forEach(r),pAo=i(G),iv=s(G,"LI",{});var $we=n(iv);Bae=s($we,"STRONG",{});var CUt=n(Bae);_Ao=t(CUt,"xlnet"),CUt.forEach(r),bAo=t($we," \u2014 "),yj=s($we,"A",{href:!0});var MUt=n(yj);vAo=t(MUt,"XLNetForMultipleChoice"),MUt.forEach(r),TAo=t($we," (XLNet model)"),$we.forEach(r),FAo=i(G),dv=s(G,"LI",{});var Iwe=n(dv);xae=s(Iwe,"STRONG",{});var EUt=n(xae);CAo=t(EUt,"yoso"),EUt.forEach(r),MAo=t(Iwe," \u2014 "),wj=s(Iwe,"A",{href:!0});var yUt=n(wj);EAo=t(yUt,"YosoForMultipleChoice"),yUt.forEach(r),yAo=t(Iwe," (YOSO model)"),Iwe.forEach(r),G.forEach(r),wAo=i(Vr),cv=s(Vr,"P",{});var jwe=n(cv);AAo=t(jwe,"The model is set in evaluation mode by default using "),kae=s(jwe,"CODE",{});var wUt=n(kae);LAo=t(wUt,"model.eval()"),wUt.forEach(r),BAo=t(jwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rae=s(jwe,"CODE",{});var AUt=n(Rae);xAo=t(AUt,"model.train()"),AUt.forEach(r),jwe.forEach(r),kAo=i(Vr),Sae=s(Vr,"P",{});var LUt=n(Sae);RAo=t(LUt,"Examples:"),LUt.forEach(r),SAo=i(Vr),f(fw.$$.fragment,Vr),Vr.forEach(r),al.forEach(r),YBe=i(d),_d=s(d,"H2",{class:!0});var nRe=n(_d);mv=s(nRe,"A",{id:!0,class:!0,href:!0});var BUt=n(mv);Pae=s(BUt,"SPAN",{});var xUt=n(Pae);f(gw.$$.fragment,xUt),xUt.forEach(r),BUt.forEach(r),PAo=i(nRe),$ae=s(nRe,"SPAN",{});var kUt=n($ae);$Ao=t(kUt,"AutoModelForNextSentencePrediction"),kUt.forEach(r),nRe.forEach(r),KBe=i(d),ot=s(d,"DIV",{class:!0});var nl=n(ot);f(hw.$$.fragment,nl),IAo=i(nl),bd=s(nl,"P",{});var Fz=n(bd);jAo=t(Fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Iae=s(Fz,"CODE",{});var RUt=n(Iae);DAo=t(RUt,"from_pretrained()"),RUt.forEach(r),NAo=t(Fz,"class method or the "),jae=s(Fz,"CODE",{});var SUt=n(jae);qAo=t(SUt,"from_config()"),SUt.forEach(r),OAo=t(Fz,`class
method.`),Fz.forEach(r),GAo=i(nl),uw=s(nl,"P",{});var lRe=n(uw);XAo=t(lRe,"This class cannot be instantiated directly using "),Dae=s(lRe,"CODE",{});var PUt=n(Dae);VAo=t(PUt,"__init__()"),PUt.forEach(r),zAo=t(lRe," (throws an error)."),lRe.forEach(r),WAo=i(nl),Ut=s(nl,"DIV",{class:!0});var ll=n(Ut);f(pw.$$.fragment,ll),QAo=i(ll),Nae=s(ll,"P",{});var $Ut=n(Nae);HAo=t($Ut,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),$Ut.forEach(r),UAo=i(ll),vd=s(ll,"P",{});var Cz=n(vd);JAo=t(Cz,`Note:
Loading a model from its configuration file does `),qae=s(Cz,"STRONG",{});var IUt=n(qae);YAo=t(IUt,"not"),IUt.forEach(r),KAo=t(Cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Oae=s(Cz,"CODE",{});var jUt=n(Oae);ZAo=t(jUt,"from_pretrained()"),jUt.forEach(r),e0o=t(Cz,"to load the model weights."),Cz.forEach(r),o0o=i(ll),Gae=s(ll,"P",{});var DUt=n(Gae);t0o=t(DUt,"Examples:"),DUt.forEach(r),r0o=i(ll),f(_w.$$.fragment,ll),ll.forEach(r),a0o=i(nl),Ne=s(nl,"DIV",{class:!0});var zr=n(Ne);f(bw.$$.fragment,zr),s0o=i(zr),Xae=s(zr,"P",{});var NUt=n(Xae);n0o=t(NUt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),NUt.forEach(r),l0o=i(zr),Ja=s(zr,"P",{});var H3=n(Ja);i0o=t(H3,"The model class to instantiate is selected based on the "),Vae=s(H3,"CODE",{});var qUt=n(Vae);d0o=t(qUt,"model_type"),qUt.forEach(r),c0o=t(H3,` property of the config object (either
passed as an argument or loaded from `),zae=s(H3,"CODE",{});var OUt=n(zae);m0o=t(OUt,"pretrained_model_name_or_path"),OUt.forEach(r),f0o=t(H3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wae=s(H3,"CODE",{});var GUt=n(Wae);g0o=t(GUt,"pretrained_model_name_or_path"),GUt.forEach(r),h0o=t(H3,":"),H3.forEach(r),u0o=i(zr),da=s(zr,"UL",{});var il=n(da);fv=s(il,"LI",{});var Dwe=n(fv);Qae=s(Dwe,"STRONG",{});var XUt=n(Qae);p0o=t(XUt,"bert"),XUt.forEach(r),_0o=t(Dwe," \u2014 "),Aj=s(Dwe,"A",{href:!0});var VUt=n(Aj);b0o=t(VUt,"BertForNextSentencePrediction"),VUt.forEach(r),v0o=t(Dwe," (BERT model)"),Dwe.forEach(r),T0o=i(il),gv=s(il,"LI",{});var Nwe=n(gv);Hae=s(Nwe,"STRONG",{});var zUt=n(Hae);F0o=t(zUt,"fnet"),zUt.forEach(r),C0o=t(Nwe," \u2014 "),Lj=s(Nwe,"A",{href:!0});var WUt=n(Lj);M0o=t(WUt,"FNetForNextSentencePrediction"),WUt.forEach(r),E0o=t(Nwe," (FNet model)"),Nwe.forEach(r),y0o=i(il),hv=s(il,"LI",{});var qwe=n(hv);Uae=s(qwe,"STRONG",{});var QUt=n(Uae);w0o=t(QUt,"megatron-bert"),QUt.forEach(r),A0o=t(qwe," \u2014 "),Bj=s(qwe,"A",{href:!0});var HUt=n(Bj);L0o=t(HUt,"MegatronBertForNextSentencePrediction"),HUt.forEach(r),B0o=t(qwe," (MegatronBert model)"),qwe.forEach(r),x0o=i(il),uv=s(il,"LI",{});var Owe=n(uv);Jae=s(Owe,"STRONG",{});var UUt=n(Jae);k0o=t(UUt,"mobilebert"),UUt.forEach(r),R0o=t(Owe," \u2014 "),xj=s(Owe,"A",{href:!0});var JUt=n(xj);S0o=t(JUt,"MobileBertForNextSentencePrediction"),JUt.forEach(r),P0o=t(Owe," (MobileBERT model)"),Owe.forEach(r),$0o=i(il),pv=s(il,"LI",{});var Gwe=n(pv);Yae=s(Gwe,"STRONG",{});var YUt=n(Yae);I0o=t(YUt,"qdqbert"),YUt.forEach(r),j0o=t(Gwe," \u2014 "),kj=s(Gwe,"A",{href:!0});var KUt=n(kj);D0o=t(KUt,"QDQBertForNextSentencePrediction"),KUt.forEach(r),N0o=t(Gwe," (QDQBert model)"),Gwe.forEach(r),il.forEach(r),q0o=i(zr),_v=s(zr,"P",{});var Xwe=n(_v);O0o=t(Xwe,"The model is set in evaluation mode by default using "),Kae=s(Xwe,"CODE",{});var ZUt=n(Kae);G0o=t(ZUt,"model.eval()"),ZUt.forEach(r),X0o=t(Xwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zae=s(Xwe,"CODE",{});var eJt=n(Zae);V0o=t(eJt,"model.train()"),eJt.forEach(r),Xwe.forEach(r),z0o=i(zr),ese=s(zr,"P",{});var oJt=n(ese);W0o=t(oJt,"Examples:"),oJt.forEach(r),Q0o=i(zr),f(vw.$$.fragment,zr),zr.forEach(r),nl.forEach(r),ZBe=i(d),Td=s(d,"H2",{class:!0});var iRe=n(Td);bv=s(iRe,"A",{id:!0,class:!0,href:!0});var tJt=n(bv);ose=s(tJt,"SPAN",{});var rJt=n(ose);f(Tw.$$.fragment,rJt),rJt.forEach(r),tJt.forEach(r),H0o=i(iRe),tse=s(iRe,"SPAN",{});var aJt=n(tse);U0o=t(aJt,"AutoModelForTokenClassification"),aJt.forEach(r),iRe.forEach(r),exe=i(d),tt=s(d,"DIV",{class:!0});var dl=n(tt);f(Fw.$$.fragment,dl),J0o=i(dl),Fd=s(dl,"P",{});var Mz=n(Fd);Y0o=t(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),rse=s(Mz,"CODE",{});var sJt=n(rse);K0o=t(sJt,"from_pretrained()"),sJt.forEach(r),Z0o=t(Mz,"class method or the "),ase=s(Mz,"CODE",{});var nJt=n(ase);eLo=t(nJt,"from_config()"),nJt.forEach(r),oLo=t(Mz,`class
method.`),Mz.forEach(r),tLo=i(dl),Cw=s(dl,"P",{});var dRe=n(Cw);rLo=t(dRe,"This class cannot be instantiated directly using "),sse=s(dRe,"CODE",{});var lJt=n(sse);aLo=t(lJt,"__init__()"),lJt.forEach(r),sLo=t(dRe," (throws an error)."),dRe.forEach(r),nLo=i(dl),Jt=s(dl,"DIV",{class:!0});var cl=n(Jt);f(Mw.$$.fragment,cl),lLo=i(cl),nse=s(cl,"P",{});var iJt=n(nse);iLo=t(iJt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),iJt.forEach(r),dLo=i(cl),Cd=s(cl,"P",{});var Ez=n(Cd);cLo=t(Ez,`Note:
Loading a model from its configuration file does `),lse=s(Ez,"STRONG",{});var dJt=n(lse);mLo=t(dJt,"not"),dJt.forEach(r),fLo=t(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),ise=s(Ez,"CODE",{});var cJt=n(ise);gLo=t(cJt,"from_pretrained()"),cJt.forEach(r),hLo=t(Ez,"to load the model weights."),Ez.forEach(r),uLo=i(cl),dse=s(cl,"P",{});var mJt=n(dse);pLo=t(mJt,"Examples:"),mJt.forEach(r),_Lo=i(cl),f(Ew.$$.fragment,cl),cl.forEach(r),bLo=i(dl),qe=s(dl,"DIV",{class:!0});var Wr=n(qe);f(yw.$$.fragment,Wr),vLo=i(Wr),cse=s(Wr,"P",{});var fJt=n(cse);TLo=t(fJt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),fJt.forEach(r),FLo=i(Wr),Ya=s(Wr,"P",{});var U3=n(Ya);CLo=t(U3,"The model class to instantiate is selected based on the "),mse=s(U3,"CODE",{});var gJt=n(mse);MLo=t(gJt,"model_type"),gJt.forEach(r),ELo=t(U3,` property of the config object (either
passed as an argument or loaded from `),fse=s(U3,"CODE",{});var hJt=n(fse);yLo=t(hJt,"pretrained_model_name_or_path"),hJt.forEach(r),wLo=t(U3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gse=s(U3,"CODE",{});var uJt=n(gse);ALo=t(uJt,"pretrained_model_name_or_path"),uJt.forEach(r),LLo=t(U3,":"),U3.forEach(r),BLo=i(Wr),N=s(Wr,"UL",{});var q=n(N);vv=s(q,"LI",{});var Vwe=n(vv);hse=s(Vwe,"STRONG",{});var pJt=n(hse);xLo=t(pJt,"albert"),pJt.forEach(r),kLo=t(Vwe," \u2014 "),Rj=s(Vwe,"A",{href:!0});var _Jt=n(Rj);RLo=t(_Jt,"AlbertForTokenClassification"),_Jt.forEach(r),SLo=t(Vwe," (ALBERT model)"),Vwe.forEach(r),PLo=i(q),Tv=s(q,"LI",{});var zwe=n(Tv);use=s(zwe,"STRONG",{});var bJt=n(use);$Lo=t(bJt,"bert"),bJt.forEach(r),ILo=t(zwe," \u2014 "),Sj=s(zwe,"A",{href:!0});var vJt=n(Sj);jLo=t(vJt,"BertForTokenClassification"),vJt.forEach(r),DLo=t(zwe," (BERT model)"),zwe.forEach(r),NLo=i(q),Fv=s(q,"LI",{});var Wwe=n(Fv);pse=s(Wwe,"STRONG",{});var TJt=n(pse);qLo=t(TJt,"big_bird"),TJt.forEach(r),OLo=t(Wwe," \u2014 "),Pj=s(Wwe,"A",{href:!0});var FJt=n(Pj);GLo=t(FJt,"BigBirdForTokenClassification"),FJt.forEach(r),XLo=t(Wwe," (BigBird model)"),Wwe.forEach(r),VLo=i(q),Cv=s(q,"LI",{});var Qwe=n(Cv);_se=s(Qwe,"STRONG",{});var CJt=n(_se);zLo=t(CJt,"camembert"),CJt.forEach(r),WLo=t(Qwe," \u2014 "),$j=s(Qwe,"A",{href:!0});var MJt=n($j);QLo=t(MJt,"CamembertForTokenClassification"),MJt.forEach(r),HLo=t(Qwe," (CamemBERT model)"),Qwe.forEach(r),ULo=i(q),Mv=s(q,"LI",{});var Hwe=n(Mv);bse=s(Hwe,"STRONG",{});var EJt=n(bse);JLo=t(EJt,"canine"),EJt.forEach(r),YLo=t(Hwe," \u2014 "),Ij=s(Hwe,"A",{href:!0});var yJt=n(Ij);KLo=t(yJt,"CanineForTokenClassification"),yJt.forEach(r),ZLo=t(Hwe," (Canine model)"),Hwe.forEach(r),e8o=i(q),Ev=s(q,"LI",{});var Uwe=n(Ev);vse=s(Uwe,"STRONG",{});var wJt=n(vse);o8o=t(wJt,"convbert"),wJt.forEach(r),t8o=t(Uwe," \u2014 "),jj=s(Uwe,"A",{href:!0});var AJt=n(jj);r8o=t(AJt,"ConvBertForTokenClassification"),AJt.forEach(r),a8o=t(Uwe," (ConvBERT model)"),Uwe.forEach(r),s8o=i(q),yv=s(q,"LI",{});var Jwe=n(yv);Tse=s(Jwe,"STRONG",{});var LJt=n(Tse);n8o=t(LJt,"data2vec-text"),LJt.forEach(r),l8o=t(Jwe," \u2014 "),Dj=s(Jwe,"A",{href:!0});var BJt=n(Dj);i8o=t(BJt,"Data2VecTextForTokenClassification"),BJt.forEach(r),d8o=t(Jwe," (Data2VecText model)"),Jwe.forEach(r),c8o=i(q),wv=s(q,"LI",{});var Ywe=n(wv);Fse=s(Ywe,"STRONG",{});var xJt=n(Fse);m8o=t(xJt,"deberta"),xJt.forEach(r),f8o=t(Ywe," \u2014 "),Nj=s(Ywe,"A",{href:!0});var kJt=n(Nj);g8o=t(kJt,"DebertaForTokenClassification"),kJt.forEach(r),h8o=t(Ywe," (DeBERTa model)"),Ywe.forEach(r),u8o=i(q),Av=s(q,"LI",{});var Kwe=n(Av);Cse=s(Kwe,"STRONG",{});var RJt=n(Cse);p8o=t(RJt,"deberta-v2"),RJt.forEach(r),_8o=t(Kwe," \u2014 "),qj=s(Kwe,"A",{href:!0});var SJt=n(qj);b8o=t(SJt,"DebertaV2ForTokenClassification"),SJt.forEach(r),v8o=t(Kwe," (DeBERTa-v2 model)"),Kwe.forEach(r),T8o=i(q),Lv=s(q,"LI",{});var Zwe=n(Lv);Mse=s(Zwe,"STRONG",{});var PJt=n(Mse);F8o=t(PJt,"distilbert"),PJt.forEach(r),C8o=t(Zwe," \u2014 "),Oj=s(Zwe,"A",{href:!0});var $Jt=n(Oj);M8o=t($Jt,"DistilBertForTokenClassification"),$Jt.forEach(r),E8o=t(Zwe," (DistilBERT model)"),Zwe.forEach(r),y8o=i(q),Bv=s(q,"LI",{});var e6e=n(Bv);Ese=s(e6e,"STRONG",{});var IJt=n(Ese);w8o=t(IJt,"electra"),IJt.forEach(r),A8o=t(e6e," \u2014 "),Gj=s(e6e,"A",{href:!0});var jJt=n(Gj);L8o=t(jJt,"ElectraForTokenClassification"),jJt.forEach(r),B8o=t(e6e," (ELECTRA model)"),e6e.forEach(r),x8o=i(q),xv=s(q,"LI",{});var o6e=n(xv);yse=s(o6e,"STRONG",{});var DJt=n(yse);k8o=t(DJt,"flaubert"),DJt.forEach(r),R8o=t(o6e," \u2014 "),Xj=s(o6e,"A",{href:!0});var NJt=n(Xj);S8o=t(NJt,"FlaubertForTokenClassification"),NJt.forEach(r),P8o=t(o6e," (FlauBERT model)"),o6e.forEach(r),$8o=i(q),kv=s(q,"LI",{});var t6e=n(kv);wse=s(t6e,"STRONG",{});var qJt=n(wse);I8o=t(qJt,"fnet"),qJt.forEach(r),j8o=t(t6e," \u2014 "),Vj=s(t6e,"A",{href:!0});var OJt=n(Vj);D8o=t(OJt,"FNetForTokenClassification"),OJt.forEach(r),N8o=t(t6e," (FNet model)"),t6e.forEach(r),q8o=i(q),Rv=s(q,"LI",{});var r6e=n(Rv);Ase=s(r6e,"STRONG",{});var GJt=n(Ase);O8o=t(GJt,"funnel"),GJt.forEach(r),G8o=t(r6e," \u2014 "),zj=s(r6e,"A",{href:!0});var XJt=n(zj);X8o=t(XJt,"FunnelForTokenClassification"),XJt.forEach(r),V8o=t(r6e," (Funnel Transformer model)"),r6e.forEach(r),z8o=i(q),Sv=s(q,"LI",{});var a6e=n(Sv);Lse=s(a6e,"STRONG",{});var VJt=n(Lse);W8o=t(VJt,"gpt2"),VJt.forEach(r),Q8o=t(a6e," \u2014 "),Wj=s(a6e,"A",{href:!0});var zJt=n(Wj);H8o=t(zJt,"GPT2ForTokenClassification"),zJt.forEach(r),U8o=t(a6e," (OpenAI GPT-2 model)"),a6e.forEach(r),J8o=i(q),Pv=s(q,"LI",{});var s6e=n(Pv);Bse=s(s6e,"STRONG",{});var WJt=n(Bse);Y8o=t(WJt,"ibert"),WJt.forEach(r),K8o=t(s6e," \u2014 "),Qj=s(s6e,"A",{href:!0});var QJt=n(Qj);Z8o=t(QJt,"IBertForTokenClassification"),QJt.forEach(r),e7o=t(s6e," (I-BERT model)"),s6e.forEach(r),o7o=i(q),$v=s(q,"LI",{});var n6e=n($v);xse=s(n6e,"STRONG",{});var HJt=n(xse);t7o=t(HJt,"layoutlm"),HJt.forEach(r),r7o=t(n6e," \u2014 "),Hj=s(n6e,"A",{href:!0});var UJt=n(Hj);a7o=t(UJt,"LayoutLMForTokenClassification"),UJt.forEach(r),s7o=t(n6e," (LayoutLM model)"),n6e.forEach(r),n7o=i(q),Iv=s(q,"LI",{});var l6e=n(Iv);kse=s(l6e,"STRONG",{});var JJt=n(kse);l7o=t(JJt,"layoutlmv2"),JJt.forEach(r),i7o=t(l6e," \u2014 "),Uj=s(l6e,"A",{href:!0});var YJt=n(Uj);d7o=t(YJt,"LayoutLMv2ForTokenClassification"),YJt.forEach(r),c7o=t(l6e," (LayoutLMv2 model)"),l6e.forEach(r),m7o=i(q),jv=s(q,"LI",{});var i6e=n(jv);Rse=s(i6e,"STRONG",{});var KJt=n(Rse);f7o=t(KJt,"longformer"),KJt.forEach(r),g7o=t(i6e," \u2014 "),Jj=s(i6e,"A",{href:!0});var ZJt=n(Jj);h7o=t(ZJt,"LongformerForTokenClassification"),ZJt.forEach(r),u7o=t(i6e," (Longformer model)"),i6e.forEach(r),p7o=i(q),Dv=s(q,"LI",{});var d6e=n(Dv);Sse=s(d6e,"STRONG",{});var eYt=n(Sse);_7o=t(eYt,"megatron-bert"),eYt.forEach(r),b7o=t(d6e," \u2014 "),Yj=s(d6e,"A",{href:!0});var oYt=n(Yj);v7o=t(oYt,"MegatronBertForTokenClassification"),oYt.forEach(r),T7o=t(d6e," (MegatronBert model)"),d6e.forEach(r),F7o=i(q),Nv=s(q,"LI",{});var c6e=n(Nv);Pse=s(c6e,"STRONG",{});var tYt=n(Pse);C7o=t(tYt,"mobilebert"),tYt.forEach(r),M7o=t(c6e," \u2014 "),Kj=s(c6e,"A",{href:!0});var rYt=n(Kj);E7o=t(rYt,"MobileBertForTokenClassification"),rYt.forEach(r),y7o=t(c6e," (MobileBERT model)"),c6e.forEach(r),w7o=i(q),qv=s(q,"LI",{});var m6e=n(qv);$se=s(m6e,"STRONG",{});var aYt=n($se);A7o=t(aYt,"mpnet"),aYt.forEach(r),L7o=t(m6e," \u2014 "),Zj=s(m6e,"A",{href:!0});var sYt=n(Zj);B7o=t(sYt,"MPNetForTokenClassification"),sYt.forEach(r),x7o=t(m6e," (MPNet model)"),m6e.forEach(r),k7o=i(q),Ov=s(q,"LI",{});var f6e=n(Ov);Ise=s(f6e,"STRONG",{});var nYt=n(Ise);R7o=t(nYt,"nystromformer"),nYt.forEach(r),S7o=t(f6e," \u2014 "),eD=s(f6e,"A",{href:!0});var lYt=n(eD);P7o=t(lYt,"NystromformerForTokenClassification"),lYt.forEach(r),$7o=t(f6e," (Nystromformer model)"),f6e.forEach(r),I7o=i(q),Gv=s(q,"LI",{});var g6e=n(Gv);jse=s(g6e,"STRONG",{});var iYt=n(jse);j7o=t(iYt,"qdqbert"),iYt.forEach(r),D7o=t(g6e," \u2014 "),oD=s(g6e,"A",{href:!0});var dYt=n(oD);N7o=t(dYt,"QDQBertForTokenClassification"),dYt.forEach(r),q7o=t(g6e," (QDQBert model)"),g6e.forEach(r),O7o=i(q),Xv=s(q,"LI",{});var h6e=n(Xv);Dse=s(h6e,"STRONG",{});var cYt=n(Dse);G7o=t(cYt,"rembert"),cYt.forEach(r),X7o=t(h6e," \u2014 "),tD=s(h6e,"A",{href:!0});var mYt=n(tD);V7o=t(mYt,"RemBertForTokenClassification"),mYt.forEach(r),z7o=t(h6e," (RemBERT model)"),h6e.forEach(r),W7o=i(q),Vv=s(q,"LI",{});var u6e=n(Vv);Nse=s(u6e,"STRONG",{});var fYt=n(Nse);Q7o=t(fYt,"roberta"),fYt.forEach(r),H7o=t(u6e," \u2014 "),rD=s(u6e,"A",{href:!0});var gYt=n(rD);U7o=t(gYt,"RobertaForTokenClassification"),gYt.forEach(r),J7o=t(u6e," (RoBERTa model)"),u6e.forEach(r),Y7o=i(q),zv=s(q,"LI",{});var p6e=n(zv);qse=s(p6e,"STRONG",{});var hYt=n(qse);K7o=t(hYt,"roformer"),hYt.forEach(r),Z7o=t(p6e," \u2014 "),aD=s(p6e,"A",{href:!0});var uYt=n(aD);e9o=t(uYt,"RoFormerForTokenClassification"),uYt.forEach(r),o9o=t(p6e," (RoFormer model)"),p6e.forEach(r),t9o=i(q),Wv=s(q,"LI",{});var _6e=n(Wv);Ose=s(_6e,"STRONG",{});var pYt=n(Ose);r9o=t(pYt,"squeezebert"),pYt.forEach(r),a9o=t(_6e," \u2014 "),sD=s(_6e,"A",{href:!0});var _Yt=n(sD);s9o=t(_Yt,"SqueezeBertForTokenClassification"),_Yt.forEach(r),n9o=t(_6e," (SqueezeBERT model)"),_6e.forEach(r),l9o=i(q),Qv=s(q,"LI",{});var b6e=n(Qv);Gse=s(b6e,"STRONG",{});var bYt=n(Gse);i9o=t(bYt,"xlm"),bYt.forEach(r),d9o=t(b6e," \u2014 "),nD=s(b6e,"A",{href:!0});var vYt=n(nD);c9o=t(vYt,"XLMForTokenClassification"),vYt.forEach(r),m9o=t(b6e," (XLM model)"),b6e.forEach(r),f9o=i(q),Hv=s(q,"LI",{});var v6e=n(Hv);Xse=s(v6e,"STRONG",{});var TYt=n(Xse);g9o=t(TYt,"xlm-roberta"),TYt.forEach(r),h9o=t(v6e," \u2014 "),lD=s(v6e,"A",{href:!0});var FYt=n(lD);u9o=t(FYt,"XLMRobertaForTokenClassification"),FYt.forEach(r),p9o=t(v6e," (XLM-RoBERTa model)"),v6e.forEach(r),_9o=i(q),Uv=s(q,"LI",{});var T6e=n(Uv);Vse=s(T6e,"STRONG",{});var CYt=n(Vse);b9o=t(CYt,"xlm-roberta-xl"),CYt.forEach(r),v9o=t(T6e," \u2014 "),iD=s(T6e,"A",{href:!0});var MYt=n(iD);T9o=t(MYt,"XLMRobertaXLForTokenClassification"),MYt.forEach(r),F9o=t(T6e," (XLM-RoBERTa-XL model)"),T6e.forEach(r),C9o=i(q),Jv=s(q,"LI",{});var F6e=n(Jv);zse=s(F6e,"STRONG",{});var EYt=n(zse);M9o=t(EYt,"xlnet"),EYt.forEach(r),E9o=t(F6e," \u2014 "),dD=s(F6e,"A",{href:!0});var yYt=n(dD);y9o=t(yYt,"XLNetForTokenClassification"),yYt.forEach(r),w9o=t(F6e," (XLNet model)"),F6e.forEach(r),A9o=i(q),Yv=s(q,"LI",{});var C6e=n(Yv);Wse=s(C6e,"STRONG",{});var wYt=n(Wse);L9o=t(wYt,"yoso"),wYt.forEach(r),B9o=t(C6e," \u2014 "),cD=s(C6e,"A",{href:!0});var AYt=n(cD);x9o=t(AYt,"YosoForTokenClassification"),AYt.forEach(r),k9o=t(C6e," (YOSO model)"),C6e.forEach(r),q.forEach(r),R9o=i(Wr),Kv=s(Wr,"P",{});var M6e=n(Kv);S9o=t(M6e,"The model is set in evaluation mode by default using "),Qse=s(M6e,"CODE",{});var LYt=n(Qse);P9o=t(LYt,"model.eval()"),LYt.forEach(r),$9o=t(M6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hse=s(M6e,"CODE",{});var BYt=n(Hse);I9o=t(BYt,"model.train()"),BYt.forEach(r),M6e.forEach(r),j9o=i(Wr),Use=s(Wr,"P",{});var xYt=n(Use);D9o=t(xYt,"Examples:"),xYt.forEach(r),N9o=i(Wr),f(ww.$$.fragment,Wr),Wr.forEach(r),dl.forEach(r),oxe=i(d),Md=s(d,"H2",{class:!0});var cRe=n(Md);Zv=s(cRe,"A",{id:!0,class:!0,href:!0});var kYt=n(Zv);Jse=s(kYt,"SPAN",{});var RYt=n(Jse);f(Aw.$$.fragment,RYt),RYt.forEach(r),kYt.forEach(r),q9o=i(cRe),Yse=s(cRe,"SPAN",{});var SYt=n(Yse);O9o=t(SYt,"AutoModelForQuestionAnswering"),SYt.forEach(r),cRe.forEach(r),txe=i(d),rt=s(d,"DIV",{class:!0});var ml=n(rt);f(Lw.$$.fragment,ml),G9o=i(ml),Ed=s(ml,"P",{});var yz=n(Ed);X9o=t(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Kse=s(yz,"CODE",{});var PYt=n(Kse);V9o=t(PYt,"from_pretrained()"),PYt.forEach(r),z9o=t(yz,"class method or the "),Zse=s(yz,"CODE",{});var $Yt=n(Zse);W9o=t($Yt,"from_config()"),$Yt.forEach(r),Q9o=t(yz,`class
method.`),yz.forEach(r),H9o=i(ml),Bw=s(ml,"P",{});var mRe=n(Bw);U9o=t(mRe,"This class cannot be instantiated directly using "),ene=s(mRe,"CODE",{});var IYt=n(ene);J9o=t(IYt,"__init__()"),IYt.forEach(r),Y9o=t(mRe," (throws an error)."),mRe.forEach(r),K9o=i(ml),Yt=s(ml,"DIV",{class:!0});var fl=n(Yt);f(xw.$$.fragment,fl),Z9o=i(fl),one=s(fl,"P",{});var jYt=n(one);eBo=t(jYt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),jYt.forEach(r),oBo=i(fl),yd=s(fl,"P",{});var wz=n(yd);tBo=t(wz,`Note:
Loading a model from its configuration file does `),tne=s(wz,"STRONG",{});var DYt=n(tne);rBo=t(DYt,"not"),DYt.forEach(r),aBo=t(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),rne=s(wz,"CODE",{});var NYt=n(rne);sBo=t(NYt,"from_pretrained()"),NYt.forEach(r),nBo=t(wz,"to load the model weights."),wz.forEach(r),lBo=i(fl),ane=s(fl,"P",{});var qYt=n(ane);iBo=t(qYt,"Examples:"),qYt.forEach(r),dBo=i(fl),f(kw.$$.fragment,fl),fl.forEach(r),cBo=i(ml),Oe=s(ml,"DIV",{class:!0});var Qr=n(Oe);f(Rw.$$.fragment,Qr),mBo=i(Qr),sne=s(Qr,"P",{});var OYt=n(sne);fBo=t(OYt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),OYt.forEach(r),gBo=i(Qr),Ka=s(Qr,"P",{});var J3=n(Ka);hBo=t(J3,"The model class to instantiate is selected based on the "),nne=s(J3,"CODE",{});var GYt=n(nne);uBo=t(GYt,"model_type"),GYt.forEach(r),pBo=t(J3,` property of the config object (either
passed as an argument or loaded from `),lne=s(J3,"CODE",{});var XYt=n(lne);_Bo=t(XYt,"pretrained_model_name_or_path"),XYt.forEach(r),bBo=t(J3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ine=s(J3,"CODE",{});var VYt=n(ine);vBo=t(VYt,"pretrained_model_name_or_path"),VYt.forEach(r),TBo=t(J3,":"),J3.forEach(r),FBo=i(Qr),R=s(Qr,"UL",{});var P=n(R);eT=s(P,"LI",{});var E6e=n(eT);dne=s(E6e,"STRONG",{});var zYt=n(dne);CBo=t(zYt,"albert"),zYt.forEach(r),MBo=t(E6e," \u2014 "),mD=s(E6e,"A",{href:!0});var WYt=n(mD);EBo=t(WYt,"AlbertForQuestionAnswering"),WYt.forEach(r),yBo=t(E6e," (ALBERT model)"),E6e.forEach(r),wBo=i(P),oT=s(P,"LI",{});var y6e=n(oT);cne=s(y6e,"STRONG",{});var QYt=n(cne);ABo=t(QYt,"bart"),QYt.forEach(r),LBo=t(y6e," \u2014 "),fD=s(y6e,"A",{href:!0});var HYt=n(fD);BBo=t(HYt,"BartForQuestionAnswering"),HYt.forEach(r),xBo=t(y6e," (BART model)"),y6e.forEach(r),kBo=i(P),tT=s(P,"LI",{});var w6e=n(tT);mne=s(w6e,"STRONG",{});var UYt=n(mne);RBo=t(UYt,"bert"),UYt.forEach(r),SBo=t(w6e," \u2014 "),gD=s(w6e,"A",{href:!0});var JYt=n(gD);PBo=t(JYt,"BertForQuestionAnswering"),JYt.forEach(r),$Bo=t(w6e," (BERT model)"),w6e.forEach(r),IBo=i(P),rT=s(P,"LI",{});var A6e=n(rT);fne=s(A6e,"STRONG",{});var YYt=n(fne);jBo=t(YYt,"big_bird"),YYt.forEach(r),DBo=t(A6e," \u2014 "),hD=s(A6e,"A",{href:!0});var KYt=n(hD);NBo=t(KYt,"BigBirdForQuestionAnswering"),KYt.forEach(r),qBo=t(A6e," (BigBird model)"),A6e.forEach(r),OBo=i(P),aT=s(P,"LI",{});var L6e=n(aT);gne=s(L6e,"STRONG",{});var ZYt=n(gne);GBo=t(ZYt,"bigbird_pegasus"),ZYt.forEach(r),XBo=t(L6e," \u2014 "),uD=s(L6e,"A",{href:!0});var eKt=n(uD);VBo=t(eKt,"BigBirdPegasusForQuestionAnswering"),eKt.forEach(r),zBo=t(L6e," (BigBirdPegasus model)"),L6e.forEach(r),WBo=i(P),sT=s(P,"LI",{});var B6e=n(sT);hne=s(B6e,"STRONG",{});var oKt=n(hne);QBo=t(oKt,"camembert"),oKt.forEach(r),HBo=t(B6e," \u2014 "),pD=s(B6e,"A",{href:!0});var tKt=n(pD);UBo=t(tKt,"CamembertForQuestionAnswering"),tKt.forEach(r),JBo=t(B6e," (CamemBERT model)"),B6e.forEach(r),YBo=i(P),nT=s(P,"LI",{});var x6e=n(nT);une=s(x6e,"STRONG",{});var rKt=n(une);KBo=t(rKt,"canine"),rKt.forEach(r),ZBo=t(x6e," \u2014 "),_D=s(x6e,"A",{href:!0});var aKt=n(_D);exo=t(aKt,"CanineForQuestionAnswering"),aKt.forEach(r),oxo=t(x6e," (Canine model)"),x6e.forEach(r),txo=i(P),lT=s(P,"LI",{});var k6e=n(lT);pne=s(k6e,"STRONG",{});var sKt=n(pne);rxo=t(sKt,"convbert"),sKt.forEach(r),axo=t(k6e," \u2014 "),bD=s(k6e,"A",{href:!0});var nKt=n(bD);sxo=t(nKt,"ConvBertForQuestionAnswering"),nKt.forEach(r),nxo=t(k6e," (ConvBERT model)"),k6e.forEach(r),lxo=i(P),iT=s(P,"LI",{});var R6e=n(iT);_ne=s(R6e,"STRONG",{});var lKt=n(_ne);ixo=t(lKt,"data2vec-text"),lKt.forEach(r),dxo=t(R6e," \u2014 "),vD=s(R6e,"A",{href:!0});var iKt=n(vD);cxo=t(iKt,"Data2VecTextForQuestionAnswering"),iKt.forEach(r),mxo=t(R6e," (Data2VecText model)"),R6e.forEach(r),fxo=i(P),dT=s(P,"LI",{});var S6e=n(dT);bne=s(S6e,"STRONG",{});var dKt=n(bne);gxo=t(dKt,"deberta"),dKt.forEach(r),hxo=t(S6e," \u2014 "),TD=s(S6e,"A",{href:!0});var cKt=n(TD);uxo=t(cKt,"DebertaForQuestionAnswering"),cKt.forEach(r),pxo=t(S6e," (DeBERTa model)"),S6e.forEach(r),_xo=i(P),cT=s(P,"LI",{});var P6e=n(cT);vne=s(P6e,"STRONG",{});var mKt=n(vne);bxo=t(mKt,"deberta-v2"),mKt.forEach(r),vxo=t(P6e," \u2014 "),FD=s(P6e,"A",{href:!0});var fKt=n(FD);Txo=t(fKt,"DebertaV2ForQuestionAnswering"),fKt.forEach(r),Fxo=t(P6e," (DeBERTa-v2 model)"),P6e.forEach(r),Cxo=i(P),mT=s(P,"LI",{});var $6e=n(mT);Tne=s($6e,"STRONG",{});var gKt=n(Tne);Mxo=t(gKt,"distilbert"),gKt.forEach(r),Exo=t($6e," \u2014 "),CD=s($6e,"A",{href:!0});var hKt=n(CD);yxo=t(hKt,"DistilBertForQuestionAnswering"),hKt.forEach(r),wxo=t($6e," (DistilBERT model)"),$6e.forEach(r),Axo=i(P),fT=s(P,"LI",{});var I6e=n(fT);Fne=s(I6e,"STRONG",{});var uKt=n(Fne);Lxo=t(uKt,"electra"),uKt.forEach(r),Bxo=t(I6e," \u2014 "),MD=s(I6e,"A",{href:!0});var pKt=n(MD);xxo=t(pKt,"ElectraForQuestionAnswering"),pKt.forEach(r),kxo=t(I6e," (ELECTRA model)"),I6e.forEach(r),Rxo=i(P),gT=s(P,"LI",{});var j6e=n(gT);Cne=s(j6e,"STRONG",{});var _Kt=n(Cne);Sxo=t(_Kt,"flaubert"),_Kt.forEach(r),Pxo=t(j6e," \u2014 "),ED=s(j6e,"A",{href:!0});var bKt=n(ED);$xo=t(bKt,"FlaubertForQuestionAnsweringSimple"),bKt.forEach(r),Ixo=t(j6e," (FlauBERT model)"),j6e.forEach(r),jxo=i(P),hT=s(P,"LI",{});var D6e=n(hT);Mne=s(D6e,"STRONG",{});var vKt=n(Mne);Dxo=t(vKt,"fnet"),vKt.forEach(r),Nxo=t(D6e," \u2014 "),yD=s(D6e,"A",{href:!0});var TKt=n(yD);qxo=t(TKt,"FNetForQuestionAnswering"),TKt.forEach(r),Oxo=t(D6e," (FNet model)"),D6e.forEach(r),Gxo=i(P),uT=s(P,"LI",{});var N6e=n(uT);Ene=s(N6e,"STRONG",{});var FKt=n(Ene);Xxo=t(FKt,"funnel"),FKt.forEach(r),Vxo=t(N6e," \u2014 "),wD=s(N6e,"A",{href:!0});var CKt=n(wD);zxo=t(CKt,"FunnelForQuestionAnswering"),CKt.forEach(r),Wxo=t(N6e," (Funnel Transformer model)"),N6e.forEach(r),Qxo=i(P),pT=s(P,"LI",{});var q6e=n(pT);yne=s(q6e,"STRONG",{});var MKt=n(yne);Hxo=t(MKt,"gptj"),MKt.forEach(r),Uxo=t(q6e," \u2014 "),AD=s(q6e,"A",{href:!0});var EKt=n(AD);Jxo=t(EKt,"GPTJForQuestionAnswering"),EKt.forEach(r),Yxo=t(q6e," (GPT-J model)"),q6e.forEach(r),Kxo=i(P),_T=s(P,"LI",{});var O6e=n(_T);wne=s(O6e,"STRONG",{});var yKt=n(wne);Zxo=t(yKt,"ibert"),yKt.forEach(r),eko=t(O6e," \u2014 "),LD=s(O6e,"A",{href:!0});var wKt=n(LD);oko=t(wKt,"IBertForQuestionAnswering"),wKt.forEach(r),tko=t(O6e," (I-BERT model)"),O6e.forEach(r),rko=i(P),bT=s(P,"LI",{});var G6e=n(bT);Ane=s(G6e,"STRONG",{});var AKt=n(Ane);ako=t(AKt,"layoutlmv2"),AKt.forEach(r),sko=t(G6e," \u2014 "),BD=s(G6e,"A",{href:!0});var LKt=n(BD);nko=t(LKt,"LayoutLMv2ForQuestionAnswering"),LKt.forEach(r),lko=t(G6e," (LayoutLMv2 model)"),G6e.forEach(r),iko=i(P),vT=s(P,"LI",{});var X6e=n(vT);Lne=s(X6e,"STRONG",{});var BKt=n(Lne);dko=t(BKt,"led"),BKt.forEach(r),cko=t(X6e," \u2014 "),xD=s(X6e,"A",{href:!0});var xKt=n(xD);mko=t(xKt,"LEDForQuestionAnswering"),xKt.forEach(r),fko=t(X6e," (LED model)"),X6e.forEach(r),gko=i(P),TT=s(P,"LI",{});var V6e=n(TT);Bne=s(V6e,"STRONG",{});var kKt=n(Bne);hko=t(kKt,"longformer"),kKt.forEach(r),uko=t(V6e," \u2014 "),kD=s(V6e,"A",{href:!0});var RKt=n(kD);pko=t(RKt,"LongformerForQuestionAnswering"),RKt.forEach(r),_ko=t(V6e," (Longformer model)"),V6e.forEach(r),bko=i(P),FT=s(P,"LI",{});var z6e=n(FT);xne=s(z6e,"STRONG",{});var SKt=n(xne);vko=t(SKt,"lxmert"),SKt.forEach(r),Tko=t(z6e," \u2014 "),RD=s(z6e,"A",{href:!0});var PKt=n(RD);Fko=t(PKt,"LxmertForQuestionAnswering"),PKt.forEach(r),Cko=t(z6e," (LXMERT model)"),z6e.forEach(r),Mko=i(P),CT=s(P,"LI",{});var W6e=n(CT);kne=s(W6e,"STRONG",{});var $Kt=n(kne);Eko=t($Kt,"mbart"),$Kt.forEach(r),yko=t(W6e," \u2014 "),SD=s(W6e,"A",{href:!0});var IKt=n(SD);wko=t(IKt,"MBartForQuestionAnswering"),IKt.forEach(r),Ako=t(W6e," (mBART model)"),W6e.forEach(r),Lko=i(P),MT=s(P,"LI",{});var Q6e=n(MT);Rne=s(Q6e,"STRONG",{});var jKt=n(Rne);Bko=t(jKt,"megatron-bert"),jKt.forEach(r),xko=t(Q6e," \u2014 "),PD=s(Q6e,"A",{href:!0});var DKt=n(PD);kko=t(DKt,"MegatronBertForQuestionAnswering"),DKt.forEach(r),Rko=t(Q6e," (MegatronBert model)"),Q6e.forEach(r),Sko=i(P),ET=s(P,"LI",{});var H6e=n(ET);Sne=s(H6e,"STRONG",{});var NKt=n(Sne);Pko=t(NKt,"mobilebert"),NKt.forEach(r),$ko=t(H6e," \u2014 "),$D=s(H6e,"A",{href:!0});var qKt=n($D);Iko=t(qKt,"MobileBertForQuestionAnswering"),qKt.forEach(r),jko=t(H6e," (MobileBERT model)"),H6e.forEach(r),Dko=i(P),yT=s(P,"LI",{});var U6e=n(yT);Pne=s(U6e,"STRONG",{});var OKt=n(Pne);Nko=t(OKt,"mpnet"),OKt.forEach(r),qko=t(U6e," \u2014 "),ID=s(U6e,"A",{href:!0});var GKt=n(ID);Oko=t(GKt,"MPNetForQuestionAnswering"),GKt.forEach(r),Gko=t(U6e," (MPNet model)"),U6e.forEach(r),Xko=i(P),wT=s(P,"LI",{});var J6e=n(wT);$ne=s(J6e,"STRONG",{});var XKt=n($ne);Vko=t(XKt,"nystromformer"),XKt.forEach(r),zko=t(J6e," \u2014 "),jD=s(J6e,"A",{href:!0});var VKt=n(jD);Wko=t(VKt,"NystromformerForQuestionAnswering"),VKt.forEach(r),Qko=t(J6e," (Nystromformer model)"),J6e.forEach(r),Hko=i(P),AT=s(P,"LI",{});var Y6e=n(AT);Ine=s(Y6e,"STRONG",{});var zKt=n(Ine);Uko=t(zKt,"qdqbert"),zKt.forEach(r),Jko=t(Y6e," \u2014 "),DD=s(Y6e,"A",{href:!0});var WKt=n(DD);Yko=t(WKt,"QDQBertForQuestionAnswering"),WKt.forEach(r),Kko=t(Y6e," (QDQBert model)"),Y6e.forEach(r),Zko=i(P),LT=s(P,"LI",{});var K6e=n(LT);jne=s(K6e,"STRONG",{});var QKt=n(jne);eRo=t(QKt,"reformer"),QKt.forEach(r),oRo=t(K6e," \u2014 "),ND=s(K6e,"A",{href:!0});var HKt=n(ND);tRo=t(HKt,"ReformerForQuestionAnswering"),HKt.forEach(r),rRo=t(K6e," (Reformer model)"),K6e.forEach(r),aRo=i(P),BT=s(P,"LI",{});var Z6e=n(BT);Dne=s(Z6e,"STRONG",{});var UKt=n(Dne);sRo=t(UKt,"rembert"),UKt.forEach(r),nRo=t(Z6e," \u2014 "),qD=s(Z6e,"A",{href:!0});var JKt=n(qD);lRo=t(JKt,"RemBertForQuestionAnswering"),JKt.forEach(r),iRo=t(Z6e," (RemBERT model)"),Z6e.forEach(r),dRo=i(P),xT=s(P,"LI",{});var eAe=n(xT);Nne=s(eAe,"STRONG",{});var YKt=n(Nne);cRo=t(YKt,"roberta"),YKt.forEach(r),mRo=t(eAe," \u2014 "),OD=s(eAe,"A",{href:!0});var KKt=n(OD);fRo=t(KKt,"RobertaForQuestionAnswering"),KKt.forEach(r),gRo=t(eAe," (RoBERTa model)"),eAe.forEach(r),hRo=i(P),kT=s(P,"LI",{});var oAe=n(kT);qne=s(oAe,"STRONG",{});var ZKt=n(qne);uRo=t(ZKt,"roformer"),ZKt.forEach(r),pRo=t(oAe," \u2014 "),GD=s(oAe,"A",{href:!0});var eZt=n(GD);_Ro=t(eZt,"RoFormerForQuestionAnswering"),eZt.forEach(r),bRo=t(oAe," (RoFormer model)"),oAe.forEach(r),vRo=i(P),RT=s(P,"LI",{});var tAe=n(RT);One=s(tAe,"STRONG",{});var oZt=n(One);TRo=t(oZt,"splinter"),oZt.forEach(r),FRo=t(tAe," \u2014 "),XD=s(tAe,"A",{href:!0});var tZt=n(XD);CRo=t(tZt,"SplinterForQuestionAnswering"),tZt.forEach(r),MRo=t(tAe," (Splinter model)"),tAe.forEach(r),ERo=i(P),ST=s(P,"LI",{});var rAe=n(ST);Gne=s(rAe,"STRONG",{});var rZt=n(Gne);yRo=t(rZt,"squeezebert"),rZt.forEach(r),wRo=t(rAe," \u2014 "),VD=s(rAe,"A",{href:!0});var aZt=n(VD);ARo=t(aZt,"SqueezeBertForQuestionAnswering"),aZt.forEach(r),LRo=t(rAe," (SqueezeBERT model)"),rAe.forEach(r),BRo=i(P),PT=s(P,"LI",{});var aAe=n(PT);Xne=s(aAe,"STRONG",{});var sZt=n(Xne);xRo=t(sZt,"xlm"),sZt.forEach(r),kRo=t(aAe," \u2014 "),zD=s(aAe,"A",{href:!0});var nZt=n(zD);RRo=t(nZt,"XLMForQuestionAnsweringSimple"),nZt.forEach(r),SRo=t(aAe," (XLM model)"),aAe.forEach(r),PRo=i(P),$T=s(P,"LI",{});var sAe=n($T);Vne=s(sAe,"STRONG",{});var lZt=n(Vne);$Ro=t(lZt,"xlm-roberta"),lZt.forEach(r),IRo=t(sAe," \u2014 "),WD=s(sAe,"A",{href:!0});var iZt=n(WD);jRo=t(iZt,"XLMRobertaForQuestionAnswering"),iZt.forEach(r),DRo=t(sAe," (XLM-RoBERTa model)"),sAe.forEach(r),NRo=i(P),IT=s(P,"LI",{});var nAe=n(IT);zne=s(nAe,"STRONG",{});var dZt=n(zne);qRo=t(dZt,"xlm-roberta-xl"),dZt.forEach(r),ORo=t(nAe," \u2014 "),QD=s(nAe,"A",{href:!0});var cZt=n(QD);GRo=t(cZt,"XLMRobertaXLForQuestionAnswering"),cZt.forEach(r),XRo=t(nAe," (XLM-RoBERTa-XL model)"),nAe.forEach(r),VRo=i(P),jT=s(P,"LI",{});var lAe=n(jT);Wne=s(lAe,"STRONG",{});var mZt=n(Wne);zRo=t(mZt,"xlnet"),mZt.forEach(r),WRo=t(lAe," \u2014 "),HD=s(lAe,"A",{href:!0});var fZt=n(HD);QRo=t(fZt,"XLNetForQuestionAnsweringSimple"),fZt.forEach(r),HRo=t(lAe," (XLNet model)"),lAe.forEach(r),URo=i(P),DT=s(P,"LI",{});var iAe=n(DT);Qne=s(iAe,"STRONG",{});var gZt=n(Qne);JRo=t(gZt,"yoso"),gZt.forEach(r),YRo=t(iAe," \u2014 "),UD=s(iAe,"A",{href:!0});var hZt=n(UD);KRo=t(hZt,"YosoForQuestionAnswering"),hZt.forEach(r),ZRo=t(iAe," (YOSO model)"),iAe.forEach(r),P.forEach(r),eSo=i(Qr),NT=s(Qr,"P",{});var dAe=n(NT);oSo=t(dAe,"The model is set in evaluation mode by default using "),Hne=s(dAe,"CODE",{});var uZt=n(Hne);tSo=t(uZt,"model.eval()"),uZt.forEach(r),rSo=t(dAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Une=s(dAe,"CODE",{});var pZt=n(Une);aSo=t(pZt,"model.train()"),pZt.forEach(r),dAe.forEach(r),sSo=i(Qr),Jne=s(Qr,"P",{});var _Zt=n(Jne);nSo=t(_Zt,"Examples:"),_Zt.forEach(r),lSo=i(Qr),f(Sw.$$.fragment,Qr),Qr.forEach(r),ml.forEach(r),rxe=i(d),wd=s(d,"H2",{class:!0});var fRe=n(wd);qT=s(fRe,"A",{id:!0,class:!0,href:!0});var bZt=n(qT);Yne=s(bZt,"SPAN",{});var vZt=n(Yne);f(Pw.$$.fragment,vZt),vZt.forEach(r),bZt.forEach(r),iSo=i(fRe),Kne=s(fRe,"SPAN",{});var TZt=n(Kne);dSo=t(TZt,"AutoModelForTableQuestionAnswering"),TZt.forEach(r),fRe.forEach(r),axe=i(d),at=s(d,"DIV",{class:!0});var gl=n(at);f($w.$$.fragment,gl),cSo=i(gl),Ad=s(gl,"P",{});var Az=n(Ad);mSo=t(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Zne=s(Az,"CODE",{});var FZt=n(Zne);fSo=t(FZt,"from_pretrained()"),FZt.forEach(r),gSo=t(Az,"class method or the "),ele=s(Az,"CODE",{});var CZt=n(ele);hSo=t(CZt,"from_config()"),CZt.forEach(r),uSo=t(Az,`class
method.`),Az.forEach(r),pSo=i(gl),Iw=s(gl,"P",{});var gRe=n(Iw);_So=t(gRe,"This class cannot be instantiated directly using "),ole=s(gRe,"CODE",{});var MZt=n(ole);bSo=t(MZt,"__init__()"),MZt.forEach(r),vSo=t(gRe," (throws an error)."),gRe.forEach(r),TSo=i(gl),Kt=s(gl,"DIV",{class:!0});var hl=n(Kt);f(jw.$$.fragment,hl),FSo=i(hl),tle=s(hl,"P",{});var EZt=n(tle);CSo=t(EZt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),EZt.forEach(r),MSo=i(hl),Ld=s(hl,"P",{});var Lz=n(Ld);ESo=t(Lz,`Note:
Loading a model from its configuration file does `),rle=s(Lz,"STRONG",{});var yZt=n(rle);ySo=t(yZt,"not"),yZt.forEach(r),wSo=t(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=s(Lz,"CODE",{});var wZt=n(ale);ASo=t(wZt,"from_pretrained()"),wZt.forEach(r),LSo=t(Lz,"to load the model weights."),Lz.forEach(r),BSo=i(hl),sle=s(hl,"P",{});var AZt=n(sle);xSo=t(AZt,"Examples:"),AZt.forEach(r),kSo=i(hl),f(Dw.$$.fragment,hl),hl.forEach(r),RSo=i(gl),Ge=s(gl,"DIV",{class:!0});var Hr=n(Ge);f(Nw.$$.fragment,Hr),SSo=i(Hr),nle=s(Hr,"P",{});var LZt=n(nle);PSo=t(LZt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),LZt.forEach(r),$So=i(Hr),Za=s(Hr,"P",{});var Y3=n(Za);ISo=t(Y3,"The model class to instantiate is selected based on the "),lle=s(Y3,"CODE",{});var BZt=n(lle);jSo=t(BZt,"model_type"),BZt.forEach(r),DSo=t(Y3,` property of the config object (either
passed as an argument or loaded from `),ile=s(Y3,"CODE",{});var xZt=n(ile);NSo=t(xZt,"pretrained_model_name_or_path"),xZt.forEach(r),qSo=t(Y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=s(Y3,"CODE",{});var kZt=n(dle);OSo=t(kZt,"pretrained_model_name_or_path"),kZt.forEach(r),GSo=t(Y3,":"),Y3.forEach(r),XSo=i(Hr),cle=s(Hr,"UL",{});var RZt=n(cle);OT=s(RZt,"LI",{});var cAe=n(OT);mle=s(cAe,"STRONG",{});var SZt=n(mle);VSo=t(SZt,"tapas"),SZt.forEach(r),zSo=t(cAe," \u2014 "),JD=s(cAe,"A",{href:!0});var PZt=n(JD);WSo=t(PZt,"TapasForQuestionAnswering"),PZt.forEach(r),QSo=t(cAe," (TAPAS model)"),cAe.forEach(r),RZt.forEach(r),HSo=i(Hr),GT=s(Hr,"P",{});var mAe=n(GT);USo=t(mAe,"The model is set in evaluation mode by default using "),fle=s(mAe,"CODE",{});var $Zt=n(fle);JSo=t($Zt,"model.eval()"),$Zt.forEach(r),YSo=t(mAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),gle=s(mAe,"CODE",{});var IZt=n(gle);KSo=t(IZt,"model.train()"),IZt.forEach(r),mAe.forEach(r),ZSo=i(Hr),hle=s(Hr,"P",{});var jZt=n(hle);ePo=t(jZt,"Examples:"),jZt.forEach(r),oPo=i(Hr),f(qw.$$.fragment,Hr),Hr.forEach(r),gl.forEach(r),sxe=i(d),Bd=s(d,"H2",{class:!0});var hRe=n(Bd);XT=s(hRe,"A",{id:!0,class:!0,href:!0});var DZt=n(XT);ule=s(DZt,"SPAN",{});var NZt=n(ule);f(Ow.$$.fragment,NZt),NZt.forEach(r),DZt.forEach(r),tPo=i(hRe),ple=s(hRe,"SPAN",{});var qZt=n(ple);rPo=t(qZt,"AutoModelForImageClassification"),qZt.forEach(r),hRe.forEach(r),nxe=i(d),st=s(d,"DIV",{class:!0});var ul=n(st);f(Gw.$$.fragment,ul),aPo=i(ul),xd=s(ul,"P",{});var Bz=n(xd);sPo=t(Bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),_le=s(Bz,"CODE",{});var OZt=n(_le);nPo=t(OZt,"from_pretrained()"),OZt.forEach(r),lPo=t(Bz,"class method or the "),ble=s(Bz,"CODE",{});var GZt=n(ble);iPo=t(GZt,"from_config()"),GZt.forEach(r),dPo=t(Bz,`class
method.`),Bz.forEach(r),cPo=i(ul),Xw=s(ul,"P",{});var uRe=n(Xw);mPo=t(uRe,"This class cannot be instantiated directly using "),vle=s(uRe,"CODE",{});var XZt=n(vle);fPo=t(XZt,"__init__()"),XZt.forEach(r),gPo=t(uRe," (throws an error)."),uRe.forEach(r),hPo=i(ul),Zt=s(ul,"DIV",{class:!0});var pl=n(Zt);f(Vw.$$.fragment,pl),uPo=i(pl),Tle=s(pl,"P",{});var VZt=n(Tle);pPo=t(VZt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),VZt.forEach(r),_Po=i(pl),kd=s(pl,"P",{});var xz=n(kd);bPo=t(xz,`Note:
Loading a model from its configuration file does `),Fle=s(xz,"STRONG",{});var zZt=n(Fle);vPo=t(zZt,"not"),zZt.forEach(r),TPo=t(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cle=s(xz,"CODE",{});var WZt=n(Cle);FPo=t(WZt,"from_pretrained()"),WZt.forEach(r),CPo=t(xz,"to load the model weights."),xz.forEach(r),MPo=i(pl),Mle=s(pl,"P",{});var QZt=n(Mle);EPo=t(QZt,"Examples:"),QZt.forEach(r),yPo=i(pl),f(zw.$$.fragment,pl),pl.forEach(r),wPo=i(ul),Xe=s(ul,"DIV",{class:!0});var Ur=n(Xe);f(Ww.$$.fragment,Ur),APo=i(Ur),Ele=s(Ur,"P",{});var HZt=n(Ele);LPo=t(HZt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),HZt.forEach(r),BPo=i(Ur),es=s(Ur,"P",{});var K3=n(es);xPo=t(K3,"The model class to instantiate is selected based on the "),yle=s(K3,"CODE",{});var UZt=n(yle);kPo=t(UZt,"model_type"),UZt.forEach(r),RPo=t(K3,` property of the config object (either
passed as an argument or loaded from `),wle=s(K3,"CODE",{});var JZt=n(wle);SPo=t(JZt,"pretrained_model_name_or_path"),JZt.forEach(r),PPo=t(K3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ale=s(K3,"CODE",{});var YZt=n(Ale);$Po=t(YZt,"pretrained_model_name_or_path"),YZt.forEach(r),IPo=t(K3,":"),K3.forEach(r),jPo=i(Ur),be=s(Ur,"UL",{});var oo=n(be);VT=s(oo,"LI",{});var fAe=n(VT);Lle=s(fAe,"STRONG",{});var KZt=n(Lle);DPo=t(KZt,"beit"),KZt.forEach(r),NPo=t(fAe," \u2014 "),YD=s(fAe,"A",{href:!0});var ZZt=n(YD);qPo=t(ZZt,"BeitForImageClassification"),ZZt.forEach(r),OPo=t(fAe," (BEiT model)"),fAe.forEach(r),GPo=i(oo),zT=s(oo,"LI",{});var gAe=n(zT);Ble=s(gAe,"STRONG",{});var eer=n(Ble);XPo=t(eer,"convnext"),eer.forEach(r),VPo=t(gAe," \u2014 "),KD=s(gAe,"A",{href:!0});var oer=n(KD);zPo=t(oer,"ConvNextForImageClassification"),oer.forEach(r),WPo=t(gAe," (ConvNext model)"),gAe.forEach(r),QPo=i(oo),qn=s(oo,"LI",{});var B7=n(qn);xle=s(B7,"STRONG",{});var ter=n(xle);HPo=t(ter,"deit"),ter.forEach(r),UPo=t(B7," \u2014 "),ZD=s(B7,"A",{href:!0});var rer=n(ZD);JPo=t(rer,"DeiTForImageClassification"),rer.forEach(r),YPo=t(B7," or "),eN=s(B7,"A",{href:!0});var aer=n(eN);KPo=t(aer,"DeiTForImageClassificationWithTeacher"),aer.forEach(r),ZPo=t(B7," (DeiT model)"),B7.forEach(r),e$o=i(oo),WT=s(oo,"LI",{});var hAe=n(WT);kle=s(hAe,"STRONG",{});var ser=n(kle);o$o=t(ser,"imagegpt"),ser.forEach(r),t$o=t(hAe," \u2014 "),oN=s(hAe,"A",{href:!0});var ner=n(oN);r$o=t(ner,"ImageGPTForImageClassification"),ner.forEach(r),a$o=t(hAe," (ImageGPT model)"),hAe.forEach(r),s$o=i(oo),ma=s(oo,"LI",{});var Sm=n(ma);Rle=s(Sm,"STRONG",{});var ler=n(Rle);n$o=t(ler,"perceiver"),ler.forEach(r),l$o=t(Sm," \u2014 "),tN=s(Sm,"A",{href:!0});var ier=n(tN);i$o=t(ier,"PerceiverForImageClassificationLearned"),ier.forEach(r),d$o=t(Sm," or "),rN=s(Sm,"A",{href:!0});var der=n(rN);c$o=t(der,"PerceiverForImageClassificationFourier"),der.forEach(r),m$o=t(Sm," or "),aN=s(Sm,"A",{href:!0});var cer=n(aN);f$o=t(cer,"PerceiverForImageClassificationConvProcessing"),cer.forEach(r),g$o=t(Sm," (Perceiver model)"),Sm.forEach(r),h$o=i(oo),QT=s(oo,"LI",{});var uAe=n(QT);Sle=s(uAe,"STRONG",{});var mer=n(Sle);u$o=t(mer,"poolformer"),mer.forEach(r),p$o=t(uAe," \u2014 "),sN=s(uAe,"A",{href:!0});var fer=n(sN);_$o=t(fer,"PoolFormerForImageClassification"),fer.forEach(r),b$o=t(uAe," (PoolFormer model)"),uAe.forEach(r),v$o=i(oo),HT=s(oo,"LI",{});var pAe=n(HT);Ple=s(pAe,"STRONG",{});var ger=n(Ple);T$o=t(ger,"segformer"),ger.forEach(r),F$o=t(pAe," \u2014 "),nN=s(pAe,"A",{href:!0});var her=n(nN);C$o=t(her,"SegformerForImageClassification"),her.forEach(r),M$o=t(pAe," (SegFormer model)"),pAe.forEach(r),E$o=i(oo),UT=s(oo,"LI",{});var _Ae=n(UT);$le=s(_Ae,"STRONG",{});var uer=n($le);y$o=t(uer,"swin"),uer.forEach(r),w$o=t(_Ae," \u2014 "),lN=s(_Ae,"A",{href:!0});var per=n(lN);A$o=t(per,"SwinForImageClassification"),per.forEach(r),L$o=t(_Ae," (Swin model)"),_Ae.forEach(r),B$o=i(oo),JT=s(oo,"LI",{});var bAe=n(JT);Ile=s(bAe,"STRONG",{});var _er=n(Ile);x$o=t(_er,"vit"),_er.forEach(r),k$o=t(bAe," \u2014 "),iN=s(bAe,"A",{href:!0});var ber=n(iN);R$o=t(ber,"ViTForImageClassification"),ber.forEach(r),S$o=t(bAe," (ViT model)"),bAe.forEach(r),oo.forEach(r),P$o=i(Ur),YT=s(Ur,"P",{});var vAe=n(YT);$$o=t(vAe,"The model is set in evaluation mode by default using "),jle=s(vAe,"CODE",{});var ver=n(jle);I$o=t(ver,"model.eval()"),ver.forEach(r),j$o=t(vAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dle=s(vAe,"CODE",{});var Ter=n(Dle);D$o=t(Ter,"model.train()"),Ter.forEach(r),vAe.forEach(r),N$o=i(Ur),Nle=s(Ur,"P",{});var Fer=n(Nle);q$o=t(Fer,"Examples:"),Fer.forEach(r),O$o=i(Ur),f(Qw.$$.fragment,Ur),Ur.forEach(r),ul.forEach(r),lxe=i(d),Rd=s(d,"H2",{class:!0});var pRe=n(Rd);KT=s(pRe,"A",{id:!0,class:!0,href:!0});var Cer=n(KT);qle=s(Cer,"SPAN",{});var Mer=n(qle);f(Hw.$$.fragment,Mer),Mer.forEach(r),Cer.forEach(r),G$o=i(pRe),Ole=s(pRe,"SPAN",{});var Eer=n(Ole);X$o=t(Eer,"AutoModelForVision2Seq"),Eer.forEach(r),pRe.forEach(r),ixe=i(d),nt=s(d,"DIV",{class:!0});var _l=n(nt);f(Uw.$$.fragment,_l),V$o=i(_l),Sd=s(_l,"P",{});var kz=n(Sd);z$o=t(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Gle=s(kz,"CODE",{});var yer=n(Gle);W$o=t(yer,"from_pretrained()"),yer.forEach(r),Q$o=t(kz,"class method or the "),Xle=s(kz,"CODE",{});var wer=n(Xle);H$o=t(wer,"from_config()"),wer.forEach(r),U$o=t(kz,`class
method.`),kz.forEach(r),J$o=i(_l),Jw=s(_l,"P",{});var _Re=n(Jw);Y$o=t(_Re,"This class cannot be instantiated directly using "),Vle=s(_Re,"CODE",{});var Aer=n(Vle);K$o=t(Aer,"__init__()"),Aer.forEach(r),Z$o=t(_Re," (throws an error)."),_Re.forEach(r),eIo=i(_l),er=s(_l,"DIV",{class:!0});var bl=n(er);f(Yw.$$.fragment,bl),oIo=i(bl),zle=s(bl,"P",{});var Ler=n(zle);tIo=t(Ler,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Ler.forEach(r),rIo=i(bl),Pd=s(bl,"P",{});var Rz=n(Pd);aIo=t(Rz,`Note:
Loading a model from its configuration file does `),Wle=s(Rz,"STRONG",{});var Ber=n(Wle);sIo=t(Ber,"not"),Ber.forEach(r),nIo=t(Rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Qle=s(Rz,"CODE",{});var xer=n(Qle);lIo=t(xer,"from_pretrained()"),xer.forEach(r),iIo=t(Rz,"to load the model weights."),Rz.forEach(r),dIo=i(bl),Hle=s(bl,"P",{});var ker=n(Hle);cIo=t(ker,"Examples:"),ker.forEach(r),mIo=i(bl),f(Kw.$$.fragment,bl),bl.forEach(r),fIo=i(_l),Ve=s(_l,"DIV",{class:!0});var Jr=n(Ve);f(Zw.$$.fragment,Jr),gIo=i(Jr),Ule=s(Jr,"P",{});var Rer=n(Ule);hIo=t(Rer,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Rer.forEach(r),uIo=i(Jr),os=s(Jr,"P",{});var Z3=n(os);pIo=t(Z3,"The model class to instantiate is selected based on the "),Jle=s(Z3,"CODE",{});var Ser=n(Jle);_Io=t(Ser,"model_type"),Ser.forEach(r),bIo=t(Z3,` property of the config object (either
passed as an argument or loaded from `),Yle=s(Z3,"CODE",{});var Per=n(Yle);vIo=t(Per,"pretrained_model_name_or_path"),Per.forEach(r),TIo=t(Z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Kle=s(Z3,"CODE",{});var $er=n(Kle);FIo=t($er,"pretrained_model_name_or_path"),$er.forEach(r),CIo=t(Z3,":"),Z3.forEach(r),MIo=i(Jr),Zle=s(Jr,"UL",{});var Ier=n(Zle);ZT=s(Ier,"LI",{});var TAe=n(ZT);eie=s(TAe,"STRONG",{});var jer=n(eie);EIo=t(jer,"vision-encoder-decoder"),jer.forEach(r),yIo=t(TAe," \u2014 "),dN=s(TAe,"A",{href:!0});var Der=n(dN);wIo=t(Der,"VisionEncoderDecoderModel"),Der.forEach(r),AIo=t(TAe," (Vision Encoder decoder model)"),TAe.forEach(r),Ier.forEach(r),LIo=i(Jr),e1=s(Jr,"P",{});var FAe=n(e1);BIo=t(FAe,"The model is set in evaluation mode by default using "),oie=s(FAe,"CODE",{});var Ner=n(oie);xIo=t(Ner,"model.eval()"),Ner.forEach(r),kIo=t(FAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tie=s(FAe,"CODE",{});var qer=n(tie);RIo=t(qer,"model.train()"),qer.forEach(r),FAe.forEach(r),SIo=i(Jr),rie=s(Jr,"P",{});var Oer=n(rie);PIo=t(Oer,"Examples:"),Oer.forEach(r),$Io=i(Jr),f(e6.$$.fragment,Jr),Jr.forEach(r),_l.forEach(r),dxe=i(d),$d=s(d,"H2",{class:!0});var bRe=n($d);o1=s(bRe,"A",{id:!0,class:!0,href:!0});var Ger=n(o1);aie=s(Ger,"SPAN",{});var Xer=n(aie);f(o6.$$.fragment,Xer),Xer.forEach(r),Ger.forEach(r),IIo=i(bRe),sie=s(bRe,"SPAN",{});var Ver=n(sie);jIo=t(Ver,"AutoModelForAudioClassification"),Ver.forEach(r),bRe.forEach(r),cxe=i(d),lt=s(d,"DIV",{class:!0});var vl=n(lt);f(t6.$$.fragment,vl),DIo=i(vl),Id=s(vl,"P",{});var Sz=n(Id);NIo=t(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),nie=s(Sz,"CODE",{});var zer=n(nie);qIo=t(zer,"from_pretrained()"),zer.forEach(r),OIo=t(Sz,"class method or the "),lie=s(Sz,"CODE",{});var Wer=n(lie);GIo=t(Wer,"from_config()"),Wer.forEach(r),XIo=t(Sz,`class
method.`),Sz.forEach(r),VIo=i(vl),r6=s(vl,"P",{});var vRe=n(r6);zIo=t(vRe,"This class cannot be instantiated directly using "),iie=s(vRe,"CODE",{});var Qer=n(iie);WIo=t(Qer,"__init__()"),Qer.forEach(r),QIo=t(vRe," (throws an error)."),vRe.forEach(r),HIo=i(vl),or=s(vl,"DIV",{class:!0});var Tl=n(or);f(a6.$$.fragment,Tl),UIo=i(Tl),die=s(Tl,"P",{});var Her=n(die);JIo=t(Her,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),Her.forEach(r),YIo=i(Tl),jd=s(Tl,"P",{});var Pz=n(jd);KIo=t(Pz,`Note:
Loading a model from its configuration file does `),cie=s(Pz,"STRONG",{});var Uer=n(cie);ZIo=t(Uer,"not"),Uer.forEach(r),ejo=t(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),mie=s(Pz,"CODE",{});var Jer=n(mie);ojo=t(Jer,"from_pretrained()"),Jer.forEach(r),tjo=t(Pz,"to load the model weights."),Pz.forEach(r),rjo=i(Tl),fie=s(Tl,"P",{});var Yer=n(fie);ajo=t(Yer,"Examples:"),Yer.forEach(r),sjo=i(Tl),f(s6.$$.fragment,Tl),Tl.forEach(r),njo=i(vl),ze=s(vl,"DIV",{class:!0});var Yr=n(ze);f(n6.$$.fragment,Yr),ljo=i(Yr),gie=s(Yr,"P",{});var Ker=n(gie);ijo=t(Ker,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),Ker.forEach(r),djo=i(Yr),ts=s(Yr,"P",{});var e5=n(ts);cjo=t(e5,"The model class to instantiate is selected based on the "),hie=s(e5,"CODE",{});var Zer=n(hie);mjo=t(Zer,"model_type"),Zer.forEach(r),fjo=t(e5,` property of the config object (either
passed as an argument or loaded from `),uie=s(e5,"CODE",{});var eor=n(uie);gjo=t(eor,"pretrained_model_name_or_path"),eor.forEach(r),hjo=t(e5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pie=s(e5,"CODE",{});var oor=n(pie);ujo=t(oor,"pretrained_model_name_or_path"),oor.forEach(r),pjo=t(e5,":"),e5.forEach(r),_jo=i(Yr),Ae=s(Yr,"UL",{});var qo=n(Ae);t1=s(qo,"LI",{});var CAe=n(t1);_ie=s(CAe,"STRONG",{});var tor=n(_ie);bjo=t(tor,"data2vec-audio"),tor.forEach(r),vjo=t(CAe," \u2014 "),cN=s(CAe,"A",{href:!0});var ror=n(cN);Tjo=t(ror,"Data2VecAudioForSequenceClassification"),ror.forEach(r),Fjo=t(CAe," (Data2VecAudio model)"),CAe.forEach(r),Cjo=i(qo),r1=s(qo,"LI",{});var MAe=n(r1);bie=s(MAe,"STRONG",{});var aor=n(bie);Mjo=t(aor,"hubert"),aor.forEach(r),Ejo=t(MAe," \u2014 "),mN=s(MAe,"A",{href:!0});var sor=n(mN);yjo=t(sor,"HubertForSequenceClassification"),sor.forEach(r),wjo=t(MAe," (Hubert model)"),MAe.forEach(r),Ajo=i(qo),a1=s(qo,"LI",{});var EAe=n(a1);vie=s(EAe,"STRONG",{});var nor=n(vie);Ljo=t(nor,"sew"),nor.forEach(r),Bjo=t(EAe," \u2014 "),fN=s(EAe,"A",{href:!0});var lor=n(fN);xjo=t(lor,"SEWForSequenceClassification"),lor.forEach(r),kjo=t(EAe," (SEW model)"),EAe.forEach(r),Rjo=i(qo),s1=s(qo,"LI",{});var yAe=n(s1);Tie=s(yAe,"STRONG",{});var ior=n(Tie);Sjo=t(ior,"sew-d"),ior.forEach(r),Pjo=t(yAe," \u2014 "),gN=s(yAe,"A",{href:!0});var dor=n(gN);$jo=t(dor,"SEWDForSequenceClassification"),dor.forEach(r),Ijo=t(yAe," (SEW-D model)"),yAe.forEach(r),jjo=i(qo),n1=s(qo,"LI",{});var wAe=n(n1);Fie=s(wAe,"STRONG",{});var cor=n(Fie);Djo=t(cor,"unispeech"),cor.forEach(r),Njo=t(wAe," \u2014 "),hN=s(wAe,"A",{href:!0});var mor=n(hN);qjo=t(mor,"UniSpeechForSequenceClassification"),mor.forEach(r),Ojo=t(wAe," (UniSpeech model)"),wAe.forEach(r),Gjo=i(qo),l1=s(qo,"LI",{});var AAe=n(l1);Cie=s(AAe,"STRONG",{});var gor=n(Cie);Xjo=t(gor,"unispeech-sat"),gor.forEach(r),Vjo=t(AAe," \u2014 "),uN=s(AAe,"A",{href:!0});var hor=n(uN);zjo=t(hor,"UniSpeechSatForSequenceClassification"),hor.forEach(r),Wjo=t(AAe," (UniSpeechSat model)"),AAe.forEach(r),Qjo=i(qo),i1=s(qo,"LI",{});var LAe=n(i1);Mie=s(LAe,"STRONG",{});var uor=n(Mie);Hjo=t(uor,"wav2vec2"),uor.forEach(r),Ujo=t(LAe," \u2014 "),pN=s(LAe,"A",{href:!0});var por=n(pN);Jjo=t(por,"Wav2Vec2ForSequenceClassification"),por.forEach(r),Yjo=t(LAe," (Wav2Vec2 model)"),LAe.forEach(r),Kjo=i(qo),d1=s(qo,"LI",{});var BAe=n(d1);Eie=s(BAe,"STRONG",{});var _or=n(Eie);Zjo=t(_or,"wavlm"),_or.forEach(r),eDo=t(BAe," \u2014 "),_N=s(BAe,"A",{href:!0});var bor=n(_N);oDo=t(bor,"WavLMForSequenceClassification"),bor.forEach(r),tDo=t(BAe," (WavLM model)"),BAe.forEach(r),qo.forEach(r),rDo=i(Yr),c1=s(Yr,"P",{});var xAe=n(c1);aDo=t(xAe,"The model is set in evaluation mode by default using "),yie=s(xAe,"CODE",{});var vor=n(yie);sDo=t(vor,"model.eval()"),vor.forEach(r),nDo=t(xAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wie=s(xAe,"CODE",{});var Tor=n(wie);lDo=t(Tor,"model.train()"),Tor.forEach(r),xAe.forEach(r),iDo=i(Yr),Aie=s(Yr,"P",{});var For=n(Aie);dDo=t(For,"Examples:"),For.forEach(r),cDo=i(Yr),f(l6.$$.fragment,Yr),Yr.forEach(r),vl.forEach(r),mxe=i(d),Dd=s(d,"H2",{class:!0});var TRe=n(Dd);m1=s(TRe,"A",{id:!0,class:!0,href:!0});var Cor=n(m1);Lie=s(Cor,"SPAN",{});var Mor=n(Lie);f(i6.$$.fragment,Mor),Mor.forEach(r),Cor.forEach(r),mDo=i(TRe),Bie=s(TRe,"SPAN",{});var Eor=n(Bie);fDo=t(Eor,"AutoModelForAudioFrameClassification"),Eor.forEach(r),TRe.forEach(r),fxe=i(d),it=s(d,"DIV",{class:!0});var Fl=n(it);f(d6.$$.fragment,Fl),gDo=i(Fl),Nd=s(Fl,"P",{});var $z=n(Nd);hDo=t($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),xie=s($z,"CODE",{});var yor=n(xie);uDo=t(yor,"from_pretrained()"),yor.forEach(r),pDo=t($z,"class method or the "),kie=s($z,"CODE",{});var wor=n(kie);_Do=t(wor,"from_config()"),wor.forEach(r),bDo=t($z,`class
method.`),$z.forEach(r),vDo=i(Fl),c6=s(Fl,"P",{});var FRe=n(c6);TDo=t(FRe,"This class cannot be instantiated directly using "),Rie=s(FRe,"CODE",{});var Aor=n(Rie);FDo=t(Aor,"__init__()"),Aor.forEach(r),CDo=t(FRe," (throws an error)."),FRe.forEach(r),MDo=i(Fl),tr=s(Fl,"DIV",{class:!0});var Cl=n(tr);f(m6.$$.fragment,Cl),EDo=i(Cl),Sie=s(Cl,"P",{});var Lor=n(Sie);yDo=t(Lor,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),Lor.forEach(r),wDo=i(Cl),qd=s(Cl,"P",{});var Iz=n(qd);ADo=t(Iz,`Note:
Loading a model from its configuration file does `),Pie=s(Iz,"STRONG",{});var Bor=n(Pie);LDo=t(Bor,"not"),Bor.forEach(r),BDo=t(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ie=s(Iz,"CODE",{});var xor=n($ie);xDo=t(xor,"from_pretrained()"),xor.forEach(r),kDo=t(Iz,"to load the model weights."),Iz.forEach(r),RDo=i(Cl),Iie=s(Cl,"P",{});var kor=n(Iie);SDo=t(kor,"Examples:"),kor.forEach(r),PDo=i(Cl),f(f6.$$.fragment,Cl),Cl.forEach(r),$Do=i(Fl),We=s(Fl,"DIV",{class:!0});var Kr=n(We);f(g6.$$.fragment,Kr),IDo=i(Kr),jie=s(Kr,"P",{});var Ror=n(jie);jDo=t(Ror,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),Ror.forEach(r),DDo=i(Kr),rs=s(Kr,"P",{});var o5=n(rs);NDo=t(o5,"The model class to instantiate is selected based on the "),Die=s(o5,"CODE",{});var Sor=n(Die);qDo=t(Sor,"model_type"),Sor.forEach(r),ODo=t(o5,` property of the config object (either
passed as an argument or loaded from `),Nie=s(o5,"CODE",{});var Por=n(Nie);GDo=t(Por,"pretrained_model_name_or_path"),Por.forEach(r),XDo=t(o5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qie=s(o5,"CODE",{});var $or=n(qie);VDo=t($or,"pretrained_model_name_or_path"),$or.forEach(r),zDo=t(o5,":"),o5.forEach(r),WDo=i(Kr),as=s(Kr,"UL",{});var t5=n(as);f1=s(t5,"LI",{});var kAe=n(f1);Oie=s(kAe,"STRONG",{});var Ior=n(Oie);QDo=t(Ior,"data2vec-audio"),Ior.forEach(r),HDo=t(kAe," \u2014 "),bN=s(kAe,"A",{href:!0});var jor=n(bN);UDo=t(jor,"Data2VecAudioForAudioFrameClassification"),jor.forEach(r),JDo=t(kAe," (Data2VecAudio model)"),kAe.forEach(r),YDo=i(t5),g1=s(t5,"LI",{});var RAe=n(g1);Gie=s(RAe,"STRONG",{});var Dor=n(Gie);KDo=t(Dor,"unispeech-sat"),Dor.forEach(r),ZDo=t(RAe," \u2014 "),vN=s(RAe,"A",{href:!0});var Nor=n(vN);eNo=t(Nor,"UniSpeechSatForAudioFrameClassification"),Nor.forEach(r),oNo=t(RAe," (UniSpeechSat model)"),RAe.forEach(r),tNo=i(t5),h1=s(t5,"LI",{});var SAe=n(h1);Xie=s(SAe,"STRONG",{});var qor=n(Xie);rNo=t(qor,"wav2vec2"),qor.forEach(r),aNo=t(SAe," \u2014 "),TN=s(SAe,"A",{href:!0});var Oor=n(TN);sNo=t(Oor,"Wav2Vec2ForAudioFrameClassification"),Oor.forEach(r),nNo=t(SAe," (Wav2Vec2 model)"),SAe.forEach(r),lNo=i(t5),u1=s(t5,"LI",{});var PAe=n(u1);Vie=s(PAe,"STRONG",{});var Gor=n(Vie);iNo=t(Gor,"wavlm"),Gor.forEach(r),dNo=t(PAe," \u2014 "),FN=s(PAe,"A",{href:!0});var Xor=n(FN);cNo=t(Xor,"WavLMForAudioFrameClassification"),Xor.forEach(r),mNo=t(PAe," (WavLM model)"),PAe.forEach(r),t5.forEach(r),fNo=i(Kr),p1=s(Kr,"P",{});var $Ae=n(p1);gNo=t($Ae,"The model is set in evaluation mode by default using "),zie=s($Ae,"CODE",{});var Vor=n(zie);hNo=t(Vor,"model.eval()"),Vor.forEach(r),uNo=t($Ae,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wie=s($Ae,"CODE",{});var zor=n(Wie);pNo=t(zor,"model.train()"),zor.forEach(r),$Ae.forEach(r),_No=i(Kr),Qie=s(Kr,"P",{});var Wor=n(Qie);bNo=t(Wor,"Examples:"),Wor.forEach(r),vNo=i(Kr),f(h6.$$.fragment,Kr),Kr.forEach(r),Fl.forEach(r),gxe=i(d),Od=s(d,"H2",{class:!0});var CRe=n(Od);_1=s(CRe,"A",{id:!0,class:!0,href:!0});var Qor=n(_1);Hie=s(Qor,"SPAN",{});var Hor=n(Hie);f(u6.$$.fragment,Hor),Hor.forEach(r),Qor.forEach(r),TNo=i(CRe),Uie=s(CRe,"SPAN",{});var Uor=n(Uie);FNo=t(Uor,"AutoModelForCTC"),Uor.forEach(r),CRe.forEach(r),hxe=i(d),dt=s(d,"DIV",{class:!0});var Ml=n(dt);f(p6.$$.fragment,Ml),CNo=i(Ml),Gd=s(Ml,"P",{});var jz=n(Gd);MNo=t(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Jie=s(jz,"CODE",{});var Jor=n(Jie);ENo=t(Jor,"from_pretrained()"),Jor.forEach(r),yNo=t(jz,"class method or the "),Yie=s(jz,"CODE",{});var Yor=n(Yie);wNo=t(Yor,"from_config()"),Yor.forEach(r),ANo=t(jz,`class
method.`),jz.forEach(r),LNo=i(Ml),_6=s(Ml,"P",{});var MRe=n(_6);BNo=t(MRe,"This class cannot be instantiated directly using "),Kie=s(MRe,"CODE",{});var Kor=n(Kie);xNo=t(Kor,"__init__()"),Kor.forEach(r),kNo=t(MRe," (throws an error)."),MRe.forEach(r),RNo=i(Ml),rr=s(Ml,"DIV",{class:!0});var El=n(rr);f(b6.$$.fragment,El),SNo=i(El),Zie=s(El,"P",{});var Zor=n(Zie);PNo=t(Zor,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),Zor.forEach(r),$No=i(El),Xd=s(El,"P",{});var Dz=n(Xd);INo=t(Dz,`Note:
Loading a model from its configuration file does `),ede=s(Dz,"STRONG",{});var etr=n(ede);jNo=t(etr,"not"),etr.forEach(r),DNo=t(Dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ode=s(Dz,"CODE",{});var otr=n(ode);NNo=t(otr,"from_pretrained()"),otr.forEach(r),qNo=t(Dz,"to load the model weights."),Dz.forEach(r),ONo=i(El),tde=s(El,"P",{});var ttr=n(tde);GNo=t(ttr,"Examples:"),ttr.forEach(r),XNo=i(El),f(v6.$$.fragment,El),El.forEach(r),VNo=i(Ml),Qe=s(Ml,"DIV",{class:!0});var Zr=n(Qe);f(T6.$$.fragment,Zr),zNo=i(Zr),rde=s(Zr,"P",{});var rtr=n(rde);WNo=t(rtr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),rtr.forEach(r),QNo=i(Zr),ss=s(Zr,"P",{});var r5=n(ss);HNo=t(r5,"The model class to instantiate is selected based on the "),ade=s(r5,"CODE",{});var atr=n(ade);UNo=t(atr,"model_type"),atr.forEach(r),JNo=t(r5,` property of the config object (either
passed as an argument or loaded from `),sde=s(r5,"CODE",{});var str=n(sde);YNo=t(str,"pretrained_model_name_or_path"),str.forEach(r),KNo=t(r5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nde=s(r5,"CODE",{});var ntr=n(nde);ZNo=t(ntr,"pretrained_model_name_or_path"),ntr.forEach(r),eqo=t(r5,":"),r5.forEach(r),oqo=i(Zr),Le=s(Zr,"UL",{});var Oo=n(Le);b1=s(Oo,"LI",{});var IAe=n(b1);lde=s(IAe,"STRONG",{});var ltr=n(lde);tqo=t(ltr,"data2vec-audio"),ltr.forEach(r),rqo=t(IAe," \u2014 "),CN=s(IAe,"A",{href:!0});var itr=n(CN);aqo=t(itr,"Data2VecAudioForCTC"),itr.forEach(r),sqo=t(IAe," (Data2VecAudio model)"),IAe.forEach(r),nqo=i(Oo),v1=s(Oo,"LI",{});var jAe=n(v1);ide=s(jAe,"STRONG",{});var dtr=n(ide);lqo=t(dtr,"hubert"),dtr.forEach(r),iqo=t(jAe," \u2014 "),MN=s(jAe,"A",{href:!0});var ctr=n(MN);dqo=t(ctr,"HubertForCTC"),ctr.forEach(r),cqo=t(jAe," (Hubert model)"),jAe.forEach(r),mqo=i(Oo),T1=s(Oo,"LI",{});var DAe=n(T1);dde=s(DAe,"STRONG",{});var mtr=n(dde);fqo=t(mtr,"sew"),mtr.forEach(r),gqo=t(DAe," \u2014 "),EN=s(DAe,"A",{href:!0});var ftr=n(EN);hqo=t(ftr,"SEWForCTC"),ftr.forEach(r),uqo=t(DAe," (SEW model)"),DAe.forEach(r),pqo=i(Oo),F1=s(Oo,"LI",{});var NAe=n(F1);cde=s(NAe,"STRONG",{});var gtr=n(cde);_qo=t(gtr,"sew-d"),gtr.forEach(r),bqo=t(NAe," \u2014 "),yN=s(NAe,"A",{href:!0});var htr=n(yN);vqo=t(htr,"SEWDForCTC"),htr.forEach(r),Tqo=t(NAe," (SEW-D model)"),NAe.forEach(r),Fqo=i(Oo),C1=s(Oo,"LI",{});var qAe=n(C1);mde=s(qAe,"STRONG",{});var utr=n(mde);Cqo=t(utr,"unispeech"),utr.forEach(r),Mqo=t(qAe," \u2014 "),wN=s(qAe,"A",{href:!0});var ptr=n(wN);Eqo=t(ptr,"UniSpeechForCTC"),ptr.forEach(r),yqo=t(qAe," (UniSpeech model)"),qAe.forEach(r),wqo=i(Oo),M1=s(Oo,"LI",{});var OAe=n(M1);fde=s(OAe,"STRONG",{});var _tr=n(fde);Aqo=t(_tr,"unispeech-sat"),_tr.forEach(r),Lqo=t(OAe," \u2014 "),AN=s(OAe,"A",{href:!0});var btr=n(AN);Bqo=t(btr,"UniSpeechSatForCTC"),btr.forEach(r),xqo=t(OAe," (UniSpeechSat model)"),OAe.forEach(r),kqo=i(Oo),E1=s(Oo,"LI",{});var GAe=n(E1);gde=s(GAe,"STRONG",{});var vtr=n(gde);Rqo=t(vtr,"wav2vec2"),vtr.forEach(r),Sqo=t(GAe," \u2014 "),LN=s(GAe,"A",{href:!0});var Ttr=n(LN);Pqo=t(Ttr,"Wav2Vec2ForCTC"),Ttr.forEach(r),$qo=t(GAe," (Wav2Vec2 model)"),GAe.forEach(r),Iqo=i(Oo),y1=s(Oo,"LI",{});var XAe=n(y1);hde=s(XAe,"STRONG",{});var Ftr=n(hde);jqo=t(Ftr,"wavlm"),Ftr.forEach(r),Dqo=t(XAe," \u2014 "),BN=s(XAe,"A",{href:!0});var Ctr=n(BN);Nqo=t(Ctr,"WavLMForCTC"),Ctr.forEach(r),qqo=t(XAe," (WavLM model)"),XAe.forEach(r),Oo.forEach(r),Oqo=i(Zr),w1=s(Zr,"P",{});var VAe=n(w1);Gqo=t(VAe,"The model is set in evaluation mode by default using "),ude=s(VAe,"CODE",{});var Mtr=n(ude);Xqo=t(Mtr,"model.eval()"),Mtr.forEach(r),Vqo=t(VAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pde=s(VAe,"CODE",{});var Etr=n(pde);zqo=t(Etr,"model.train()"),Etr.forEach(r),VAe.forEach(r),Wqo=i(Zr),_de=s(Zr,"P",{});var ytr=n(_de);Qqo=t(ytr,"Examples:"),ytr.forEach(r),Hqo=i(Zr),f(F6.$$.fragment,Zr),Zr.forEach(r),Ml.forEach(r),uxe=i(d),Vd=s(d,"H2",{class:!0});var ERe=n(Vd);A1=s(ERe,"A",{id:!0,class:!0,href:!0});var wtr=n(A1);bde=s(wtr,"SPAN",{});var Atr=n(bde);f(C6.$$.fragment,Atr),Atr.forEach(r),wtr.forEach(r),Uqo=i(ERe),vde=s(ERe,"SPAN",{});var Ltr=n(vde);Jqo=t(Ltr,"AutoModelForSpeechSeq2Seq"),Ltr.forEach(r),ERe.forEach(r),pxe=i(d),ct=s(d,"DIV",{class:!0});var yl=n(ct);f(M6.$$.fragment,yl),Yqo=i(yl),zd=s(yl,"P",{});var Nz=n(zd);Kqo=t(Nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Tde=s(Nz,"CODE",{});var Btr=n(Tde);Zqo=t(Btr,"from_pretrained()"),Btr.forEach(r),eOo=t(Nz,"class method or the "),Fde=s(Nz,"CODE",{});var xtr=n(Fde);oOo=t(xtr,"from_config()"),xtr.forEach(r),tOo=t(Nz,`class
method.`),Nz.forEach(r),rOo=i(yl),E6=s(yl,"P",{});var yRe=n(E6);aOo=t(yRe,"This class cannot be instantiated directly using "),Cde=s(yRe,"CODE",{});var ktr=n(Cde);sOo=t(ktr,"__init__()"),ktr.forEach(r),nOo=t(yRe," (throws an error)."),yRe.forEach(r),lOo=i(yl),ar=s(yl,"DIV",{class:!0});var wl=n(ar);f(y6.$$.fragment,wl),iOo=i(wl),Mde=s(wl,"P",{});var Rtr=n(Mde);dOo=t(Rtr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Rtr.forEach(r),cOo=i(wl),Wd=s(wl,"P",{});var qz=n(Wd);mOo=t(qz,`Note:
Loading a model from its configuration file does `),Ede=s(qz,"STRONG",{});var Str=n(Ede);fOo=t(Str,"not"),Str.forEach(r),gOo=t(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),yde=s(qz,"CODE",{});var Ptr=n(yde);hOo=t(Ptr,"from_pretrained()"),Ptr.forEach(r),uOo=t(qz,"to load the model weights."),qz.forEach(r),pOo=i(wl),wde=s(wl,"P",{});var $tr=n(wde);_Oo=t($tr,"Examples:"),$tr.forEach(r),bOo=i(wl),f(w6.$$.fragment,wl),wl.forEach(r),vOo=i(yl),He=s(yl,"DIV",{class:!0});var ea=n(He);f(A6.$$.fragment,ea),TOo=i(ea),Ade=s(ea,"P",{});var Itr=n(Ade);FOo=t(Itr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Itr.forEach(r),COo=i(ea),ns=s(ea,"P",{});var a5=n(ns);MOo=t(a5,"The model class to instantiate is selected based on the "),Lde=s(a5,"CODE",{});var jtr=n(Lde);EOo=t(jtr,"model_type"),jtr.forEach(r),yOo=t(a5,` property of the config object (either
passed as an argument or loaded from `),Bde=s(a5,"CODE",{});var Dtr=n(Bde);wOo=t(Dtr,"pretrained_model_name_or_path"),Dtr.forEach(r),AOo=t(a5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xde=s(a5,"CODE",{});var Ntr=n(xde);LOo=t(Ntr,"pretrained_model_name_or_path"),Ntr.forEach(r),BOo=t(a5,":"),a5.forEach(r),xOo=i(ea),L6=s(ea,"UL",{});var wRe=n(L6);L1=s(wRe,"LI",{});var zAe=n(L1);kde=s(zAe,"STRONG",{});var qtr=n(kde);kOo=t(qtr,"speech-encoder-decoder"),qtr.forEach(r),ROo=t(zAe," \u2014 "),xN=s(zAe,"A",{href:!0});var Otr=n(xN);SOo=t(Otr,"SpeechEncoderDecoderModel"),Otr.forEach(r),POo=t(zAe," (Speech Encoder decoder model)"),zAe.forEach(r),$Oo=i(wRe),B1=s(wRe,"LI",{});var WAe=n(B1);Rde=s(WAe,"STRONG",{});var Gtr=n(Rde);IOo=t(Gtr,"speech_to_text"),Gtr.forEach(r),jOo=t(WAe," \u2014 "),kN=s(WAe,"A",{href:!0});var Xtr=n(kN);DOo=t(Xtr,"Speech2TextForConditionalGeneration"),Xtr.forEach(r),NOo=t(WAe," (Speech2Text model)"),WAe.forEach(r),wRe.forEach(r),qOo=i(ea),x1=s(ea,"P",{});var QAe=n(x1);OOo=t(QAe,"The model is set in evaluation mode by default using "),Sde=s(QAe,"CODE",{});var Vtr=n(Sde);GOo=t(Vtr,"model.eval()"),Vtr.forEach(r),XOo=t(QAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Pde=s(QAe,"CODE",{});var ztr=n(Pde);VOo=t(ztr,"model.train()"),ztr.forEach(r),QAe.forEach(r),zOo=i(ea),$de=s(ea,"P",{});var Wtr=n($de);WOo=t(Wtr,"Examples:"),Wtr.forEach(r),QOo=i(ea),f(B6.$$.fragment,ea),ea.forEach(r),yl.forEach(r),_xe=i(d),Qd=s(d,"H2",{class:!0});var ARe=n(Qd);k1=s(ARe,"A",{id:!0,class:!0,href:!0});var Qtr=n(k1);Ide=s(Qtr,"SPAN",{});var Htr=n(Ide);f(x6.$$.fragment,Htr),Htr.forEach(r),Qtr.forEach(r),HOo=i(ARe),jde=s(ARe,"SPAN",{});var Utr=n(jde);UOo=t(Utr,"AutoModelForAudioXVector"),Utr.forEach(r),ARe.forEach(r),bxe=i(d),mt=s(d,"DIV",{class:!0});var Al=n(mt);f(k6.$$.fragment,Al),JOo=i(Al),Hd=s(Al,"P",{});var Oz=n(Hd);YOo=t(Oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),Dde=s(Oz,"CODE",{});var Jtr=n(Dde);KOo=t(Jtr,"from_pretrained()"),Jtr.forEach(r),ZOo=t(Oz,"class method or the "),Nde=s(Oz,"CODE",{});var Ytr=n(Nde);eGo=t(Ytr,"from_config()"),Ytr.forEach(r),oGo=t(Oz,`class
method.`),Oz.forEach(r),tGo=i(Al),R6=s(Al,"P",{});var LRe=n(R6);rGo=t(LRe,"This class cannot be instantiated directly using "),qde=s(LRe,"CODE",{});var Ktr=n(qde);aGo=t(Ktr,"__init__()"),Ktr.forEach(r),sGo=t(LRe," (throws an error)."),LRe.forEach(r),nGo=i(Al),sr=s(Al,"DIV",{class:!0});var Ll=n(sr);f(S6.$$.fragment,Ll),lGo=i(Ll),Ode=s(Ll,"P",{});var Ztr=n(Ode);iGo=t(Ztr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),Ztr.forEach(r),dGo=i(Ll),Ud=s(Ll,"P",{});var Gz=n(Ud);cGo=t(Gz,`Note:
Loading a model from its configuration file does `),Gde=s(Gz,"STRONG",{});var err=n(Gde);mGo=t(err,"not"),err.forEach(r),fGo=t(Gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xde=s(Gz,"CODE",{});var orr=n(Xde);gGo=t(orr,"from_pretrained()"),orr.forEach(r),hGo=t(Gz,"to load the model weights."),Gz.forEach(r),uGo=i(Ll),Vde=s(Ll,"P",{});var trr=n(Vde);pGo=t(trr,"Examples:"),trr.forEach(r),_Go=i(Ll),f(P6.$$.fragment,Ll),Ll.forEach(r),bGo=i(Al),Ue=s(Al,"DIV",{class:!0});var oa=n(Ue);f($6.$$.fragment,oa),vGo=i(oa),zde=s(oa,"P",{});var rrr=n(zde);TGo=t(rrr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),rrr.forEach(r),FGo=i(oa),ls=s(oa,"P",{});var s5=n(ls);CGo=t(s5,"The model class to instantiate is selected based on the "),Wde=s(s5,"CODE",{});var arr=n(Wde);MGo=t(arr,"model_type"),arr.forEach(r),EGo=t(s5,` property of the config object (either
passed as an argument or loaded from `),Qde=s(s5,"CODE",{});var srr=n(Qde);yGo=t(srr,"pretrained_model_name_or_path"),srr.forEach(r),wGo=t(s5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hde=s(s5,"CODE",{});var nrr=n(Hde);AGo=t(nrr,"pretrained_model_name_or_path"),nrr.forEach(r),LGo=t(s5,":"),s5.forEach(r),BGo=i(oa),is=s(oa,"UL",{});var n5=n(is);R1=s(n5,"LI",{});var HAe=n(R1);Ude=s(HAe,"STRONG",{});var lrr=n(Ude);xGo=t(lrr,"data2vec-audio"),lrr.forEach(r),kGo=t(HAe," \u2014 "),RN=s(HAe,"A",{href:!0});var irr=n(RN);RGo=t(irr,"Data2VecAudioForXVector"),irr.forEach(r),SGo=t(HAe," (Data2VecAudio model)"),HAe.forEach(r),PGo=i(n5),S1=s(n5,"LI",{});var UAe=n(S1);Jde=s(UAe,"STRONG",{});var drr=n(Jde);$Go=t(drr,"unispeech-sat"),drr.forEach(r),IGo=t(UAe," \u2014 "),SN=s(UAe,"A",{href:!0});var crr=n(SN);jGo=t(crr,"UniSpeechSatForXVector"),crr.forEach(r),DGo=t(UAe," (UniSpeechSat model)"),UAe.forEach(r),NGo=i(n5),P1=s(n5,"LI",{});var JAe=n(P1);Yde=s(JAe,"STRONG",{});var mrr=n(Yde);qGo=t(mrr,"wav2vec2"),mrr.forEach(r),OGo=t(JAe," \u2014 "),PN=s(JAe,"A",{href:!0});var frr=n(PN);GGo=t(frr,"Wav2Vec2ForXVector"),frr.forEach(r),XGo=t(JAe," (Wav2Vec2 model)"),JAe.forEach(r),VGo=i(n5),$1=s(n5,"LI",{});var YAe=n($1);Kde=s(YAe,"STRONG",{});var grr=n(Kde);zGo=t(grr,"wavlm"),grr.forEach(r),WGo=t(YAe," \u2014 "),$N=s(YAe,"A",{href:!0});var hrr=n($N);QGo=t(hrr,"WavLMForXVector"),hrr.forEach(r),HGo=t(YAe," (WavLM model)"),YAe.forEach(r),n5.forEach(r),UGo=i(oa),I1=s(oa,"P",{});var KAe=n(I1);JGo=t(KAe,"The model is set in evaluation mode by default using "),Zde=s(KAe,"CODE",{});var urr=n(Zde);YGo=t(urr,"model.eval()"),urr.forEach(r),KGo=t(KAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ece=s(KAe,"CODE",{});var prr=n(ece);ZGo=t(prr,"model.train()"),prr.forEach(r),KAe.forEach(r),eXo=i(oa),oce=s(oa,"P",{});var _rr=n(oce);oXo=t(_rr,"Examples:"),_rr.forEach(r),tXo=i(oa),f(I6.$$.fragment,oa),oa.forEach(r),Al.forEach(r),vxe=i(d),Jd=s(d,"H2",{class:!0});var BRe=n(Jd);j1=s(BRe,"A",{id:!0,class:!0,href:!0});var brr=n(j1);tce=s(brr,"SPAN",{});var vrr=n(tce);f(j6.$$.fragment,vrr),vrr.forEach(r),brr.forEach(r),rXo=i(BRe),rce=s(BRe,"SPAN",{});var Trr=n(rce);aXo=t(Trr,"AutoModelForMaskedImageModeling"),Trr.forEach(r),BRe.forEach(r),Txe=i(d),ft=s(d,"DIV",{class:!0});var Bl=n(ft);f(D6.$$.fragment,Bl),sXo=i(Bl),Yd=s(Bl,"P",{});var Xz=n(Yd);nXo=t(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),ace=s(Xz,"CODE",{});var Frr=n(ace);lXo=t(Frr,"from_pretrained()"),Frr.forEach(r),iXo=t(Xz,"class method or the "),sce=s(Xz,"CODE",{});var Crr=n(sce);dXo=t(Crr,"from_config()"),Crr.forEach(r),cXo=t(Xz,`class
method.`),Xz.forEach(r),mXo=i(Bl),N6=s(Bl,"P",{});var xRe=n(N6);fXo=t(xRe,"This class cannot be instantiated directly using "),nce=s(xRe,"CODE",{});var Mrr=n(nce);gXo=t(Mrr,"__init__()"),Mrr.forEach(r),hXo=t(xRe," (throws an error)."),xRe.forEach(r),uXo=i(Bl),nr=s(Bl,"DIV",{class:!0});var xl=n(nr);f(q6.$$.fragment,xl),pXo=i(xl),lce=s(xl,"P",{});var Err=n(lce);_Xo=t(Err,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),Err.forEach(r),bXo=i(xl),Kd=s(xl,"P",{});var Vz=n(Kd);vXo=t(Vz,`Note:
Loading a model from its configuration file does `),ice=s(Vz,"STRONG",{});var yrr=n(ice);TXo=t(yrr,"not"),yrr.forEach(r),FXo=t(Vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),dce=s(Vz,"CODE",{});var wrr=n(dce);CXo=t(wrr,"from_pretrained()"),wrr.forEach(r),MXo=t(Vz,"to load the model weights."),Vz.forEach(r),EXo=i(xl),cce=s(xl,"P",{});var Arr=n(cce);yXo=t(Arr,"Examples:"),Arr.forEach(r),wXo=i(xl),f(O6.$$.fragment,xl),xl.forEach(r),AXo=i(Bl),Je=s(Bl,"DIV",{class:!0});var ta=n(Je);f(G6.$$.fragment,ta),LXo=i(ta),mce=s(ta,"P",{});var Lrr=n(mce);BXo=t(Lrr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),Lrr.forEach(r),xXo=i(ta),ds=s(ta,"P",{});var l5=n(ds);kXo=t(l5,"The model class to instantiate is selected based on the "),fce=s(l5,"CODE",{});var Brr=n(fce);RXo=t(Brr,"model_type"),Brr.forEach(r),SXo=t(l5,` property of the config object (either
passed as an argument or loaded from `),gce=s(l5,"CODE",{});var xrr=n(gce);PXo=t(xrr,"pretrained_model_name_or_path"),xrr.forEach(r),$Xo=t(l5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hce=s(l5,"CODE",{});var krr=n(hce);IXo=t(krr,"pretrained_model_name_or_path"),krr.forEach(r),jXo=t(l5,":"),l5.forEach(r),DXo=i(ta),Zd=s(ta,"UL",{});var zz=n(Zd);D1=s(zz,"LI",{});var ZAe=n(D1);uce=s(ZAe,"STRONG",{});var Rrr=n(uce);NXo=t(Rrr,"deit"),Rrr.forEach(r),qXo=t(ZAe," \u2014 "),IN=s(ZAe,"A",{href:!0});var Srr=n(IN);OXo=t(Srr,"DeiTForMaskedImageModeling"),Srr.forEach(r),GXo=t(ZAe," (DeiT model)"),ZAe.forEach(r),XXo=i(zz),N1=s(zz,"LI",{});var e0e=n(N1);pce=s(e0e,"STRONG",{});var Prr=n(pce);VXo=t(Prr,"swin"),Prr.forEach(r),zXo=t(e0e," \u2014 "),jN=s(e0e,"A",{href:!0});var $rr=n(jN);WXo=t($rr,"SwinForMaskedImageModeling"),$rr.forEach(r),QXo=t(e0e," (Swin model)"),e0e.forEach(r),HXo=i(zz),q1=s(zz,"LI",{});var o0e=n(q1);_ce=s(o0e,"STRONG",{});var Irr=n(_ce);UXo=t(Irr,"vit"),Irr.forEach(r),JXo=t(o0e," \u2014 "),DN=s(o0e,"A",{href:!0});var jrr=n(DN);YXo=t(jrr,"ViTForMaskedImageModeling"),jrr.forEach(r),KXo=t(o0e," (ViT model)"),o0e.forEach(r),zz.forEach(r),ZXo=i(ta),O1=s(ta,"P",{});var t0e=n(O1);eVo=t(t0e,"The model is set in evaluation mode by default using "),bce=s(t0e,"CODE",{});var Drr=n(bce);oVo=t(Drr,"model.eval()"),Drr.forEach(r),tVo=t(t0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vce=s(t0e,"CODE",{});var Nrr=n(vce);rVo=t(Nrr,"model.train()"),Nrr.forEach(r),t0e.forEach(r),aVo=i(ta),Tce=s(ta,"P",{});var qrr=n(Tce);sVo=t(qrr,"Examples:"),qrr.forEach(r),nVo=i(ta),f(X6.$$.fragment,ta),ta.forEach(r),Bl.forEach(r),Fxe=i(d),ec=s(d,"H2",{class:!0});var kRe=n(ec);G1=s(kRe,"A",{id:!0,class:!0,href:!0});var Orr=n(G1);Fce=s(Orr,"SPAN",{});var Grr=n(Fce);f(V6.$$.fragment,Grr),Grr.forEach(r),Orr.forEach(r),lVo=i(kRe),Cce=s(kRe,"SPAN",{});var Xrr=n(Cce);iVo=t(Xrr,"AutoModelForObjectDetection"),Xrr.forEach(r),kRe.forEach(r),Cxe=i(d),gt=s(d,"DIV",{class:!0});var kl=n(gt);f(z6.$$.fragment,kl),dVo=i(kl),oc=s(kl,"P",{});var Wz=n(oc);cVo=t(Wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Mce=s(Wz,"CODE",{});var Vrr=n(Mce);mVo=t(Vrr,"from_pretrained()"),Vrr.forEach(r),fVo=t(Wz,"class method or the "),Ece=s(Wz,"CODE",{});var zrr=n(Ece);gVo=t(zrr,"from_config()"),zrr.forEach(r),hVo=t(Wz,`class
method.`),Wz.forEach(r),uVo=i(kl),W6=s(kl,"P",{});var RRe=n(W6);pVo=t(RRe,"This class cannot be instantiated directly using "),yce=s(RRe,"CODE",{});var Wrr=n(yce);_Vo=t(Wrr,"__init__()"),Wrr.forEach(r),bVo=t(RRe," (throws an error)."),RRe.forEach(r),vVo=i(kl),lr=s(kl,"DIV",{class:!0});var Rl=n(lr);f(Q6.$$.fragment,Rl),TVo=i(Rl),wce=s(Rl,"P",{});var Qrr=n(wce);FVo=t(Qrr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),Qrr.forEach(r),CVo=i(Rl),tc=s(Rl,"P",{});var Qz=n(tc);MVo=t(Qz,`Note:
Loading a model from its configuration file does `),Ace=s(Qz,"STRONG",{});var Hrr=n(Ace);EVo=t(Hrr,"not"),Hrr.forEach(r),yVo=t(Qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lce=s(Qz,"CODE",{});var Urr=n(Lce);wVo=t(Urr,"from_pretrained()"),Urr.forEach(r),AVo=t(Qz,"to load the model weights."),Qz.forEach(r),LVo=i(Rl),Bce=s(Rl,"P",{});var Jrr=n(Bce);BVo=t(Jrr,"Examples:"),Jrr.forEach(r),xVo=i(Rl),f(H6.$$.fragment,Rl),Rl.forEach(r),kVo=i(kl),Ye=s(kl,"DIV",{class:!0});var ra=n(Ye);f(U6.$$.fragment,ra),RVo=i(ra),xce=s(ra,"P",{});var Yrr=n(xce);SVo=t(Yrr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),Yrr.forEach(r),PVo=i(ra),cs=s(ra,"P",{});var i5=n(cs);$Vo=t(i5,"The model class to instantiate is selected based on the "),kce=s(i5,"CODE",{});var Krr=n(kce);IVo=t(Krr,"model_type"),Krr.forEach(r),jVo=t(i5,` property of the config object (either
passed as an argument or loaded from `),Rce=s(i5,"CODE",{});var Zrr=n(Rce);DVo=t(Zrr,"pretrained_model_name_or_path"),Zrr.forEach(r),NVo=t(i5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sce=s(i5,"CODE",{});var ear=n(Sce);qVo=t(ear,"pretrained_model_name_or_path"),ear.forEach(r),OVo=t(i5,":"),i5.forEach(r),GVo=i(ra),Pce=s(ra,"UL",{});var oar=n(Pce);X1=s(oar,"LI",{});var r0e=n(X1);$ce=s(r0e,"STRONG",{});var tar=n($ce);XVo=t(tar,"detr"),tar.forEach(r),VVo=t(r0e," \u2014 "),NN=s(r0e,"A",{href:!0});var rar=n(NN);zVo=t(rar,"DetrForObjectDetection"),rar.forEach(r),WVo=t(r0e," (DETR model)"),r0e.forEach(r),oar.forEach(r),QVo=i(ra),V1=s(ra,"P",{});var a0e=n(V1);HVo=t(a0e,"The model is set in evaluation mode by default using "),Ice=s(a0e,"CODE",{});var aar=n(Ice);UVo=t(aar,"model.eval()"),aar.forEach(r),JVo=t(a0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jce=s(a0e,"CODE",{});var sar=n(jce);YVo=t(sar,"model.train()"),sar.forEach(r),a0e.forEach(r),KVo=i(ra),Dce=s(ra,"P",{});var nar=n(Dce);ZVo=t(nar,"Examples:"),nar.forEach(r),ezo=i(ra),f(J6.$$.fragment,ra),ra.forEach(r),kl.forEach(r),Mxe=i(d),rc=s(d,"H2",{class:!0});var SRe=n(rc);z1=s(SRe,"A",{id:!0,class:!0,href:!0});var lar=n(z1);Nce=s(lar,"SPAN",{});var iar=n(Nce);f(Y6.$$.fragment,iar),iar.forEach(r),lar.forEach(r),ozo=i(SRe),qce=s(SRe,"SPAN",{});var dar=n(qce);tzo=t(dar,"AutoModelForImageSegmentation"),dar.forEach(r),SRe.forEach(r),Exe=i(d),ht=s(d,"DIV",{class:!0});var Sl=n(ht);f(K6.$$.fragment,Sl),rzo=i(Sl),ac=s(Sl,"P",{});var Hz=n(ac);azo=t(Hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Oce=s(Hz,"CODE",{});var car=n(Oce);szo=t(car,"from_pretrained()"),car.forEach(r),nzo=t(Hz,"class method or the "),Gce=s(Hz,"CODE",{});var mar=n(Gce);lzo=t(mar,"from_config()"),mar.forEach(r),izo=t(Hz,`class
method.`),Hz.forEach(r),dzo=i(Sl),Z6=s(Sl,"P",{});var PRe=n(Z6);czo=t(PRe,"This class cannot be instantiated directly using "),Xce=s(PRe,"CODE",{});var far=n(Xce);mzo=t(far,"__init__()"),far.forEach(r),fzo=t(PRe," (throws an error)."),PRe.forEach(r),gzo=i(Sl),ir=s(Sl,"DIV",{class:!0});var Pl=n(ir);f(eA.$$.fragment,Pl),hzo=i(Pl),Vce=s(Pl,"P",{});var gar=n(Vce);uzo=t(gar,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),gar.forEach(r),pzo=i(Pl),sc=s(Pl,"P",{});var Uz=n(sc);_zo=t(Uz,`Note:
Loading a model from its configuration file does `),zce=s(Uz,"STRONG",{});var har=n(zce);bzo=t(har,"not"),har.forEach(r),vzo=t(Uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wce=s(Uz,"CODE",{});var uar=n(Wce);Tzo=t(uar,"from_pretrained()"),uar.forEach(r),Fzo=t(Uz,"to load the model weights."),Uz.forEach(r),Czo=i(Pl),Qce=s(Pl,"P",{});var par=n(Qce);Mzo=t(par,"Examples:"),par.forEach(r),Ezo=i(Pl),f(oA.$$.fragment,Pl),Pl.forEach(r),yzo=i(Sl),Ke=s(Sl,"DIV",{class:!0});var aa=n(Ke);f(tA.$$.fragment,aa),wzo=i(aa),Hce=s(aa,"P",{});var _ar=n(Hce);Azo=t(_ar,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),_ar.forEach(r),Lzo=i(aa),ms=s(aa,"P",{});var d5=n(ms);Bzo=t(d5,"The model class to instantiate is selected based on the "),Uce=s(d5,"CODE",{});var bar=n(Uce);xzo=t(bar,"model_type"),bar.forEach(r),kzo=t(d5,` property of the config object (either
passed as an argument or loaded from `),Jce=s(d5,"CODE",{});var Tar=n(Jce);Rzo=t(Tar,"pretrained_model_name_or_path"),Tar.forEach(r),Szo=t(d5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yce=s(d5,"CODE",{});var Far=n(Yce);Pzo=t(Far,"pretrained_model_name_or_path"),Far.forEach(r),$zo=t(d5,":"),d5.forEach(r),Izo=i(aa),Kce=s(aa,"UL",{});var Car=n(Kce);W1=s(Car,"LI",{});var s0e=n(W1);Zce=s(s0e,"STRONG",{});var Mar=n(Zce);jzo=t(Mar,"detr"),Mar.forEach(r),Dzo=t(s0e," \u2014 "),qN=s(s0e,"A",{href:!0});var Ear=n(qN);Nzo=t(Ear,"DetrForSegmentation"),Ear.forEach(r),qzo=t(s0e," (DETR model)"),s0e.forEach(r),Car.forEach(r),Ozo=i(aa),Q1=s(aa,"P",{});var n0e=n(Q1);Gzo=t(n0e,"The model is set in evaluation mode by default using "),eme=s(n0e,"CODE",{});var yar=n(eme);Xzo=t(yar,"model.eval()"),yar.forEach(r),Vzo=t(n0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ome=s(n0e,"CODE",{});var war=n(ome);zzo=t(war,"model.train()"),war.forEach(r),n0e.forEach(r),Wzo=i(aa),tme=s(aa,"P",{});var Aar=n(tme);Qzo=t(Aar,"Examples:"),Aar.forEach(r),Hzo=i(aa),f(rA.$$.fragment,aa),aa.forEach(r),Sl.forEach(r),yxe=i(d),nc=s(d,"H2",{class:!0});var $Re=n(nc);H1=s($Re,"A",{id:!0,class:!0,href:!0});var Lar=n(H1);rme=s(Lar,"SPAN",{});var Bar=n(rme);f(aA.$$.fragment,Bar),Bar.forEach(r),Lar.forEach(r),Uzo=i($Re),ame=s($Re,"SPAN",{});var xar=n(ame);Jzo=t(xar,"AutoModelForSemanticSegmentation"),xar.forEach(r),$Re.forEach(r),wxe=i(d),ut=s(d,"DIV",{class:!0});var $l=n(ut);f(sA.$$.fragment,$l),Yzo=i($l),lc=s($l,"P",{});var Jz=n(lc);Kzo=t(Jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),sme=s(Jz,"CODE",{});var kar=n(sme);Zzo=t(kar,"from_pretrained()"),kar.forEach(r),eWo=t(Jz,"class method or the "),nme=s(Jz,"CODE",{});var Rar=n(nme);oWo=t(Rar,"from_config()"),Rar.forEach(r),tWo=t(Jz,`class
method.`),Jz.forEach(r),rWo=i($l),nA=s($l,"P",{});var IRe=n(nA);aWo=t(IRe,"This class cannot be instantiated directly using "),lme=s(IRe,"CODE",{});var Sar=n(lme);sWo=t(Sar,"__init__()"),Sar.forEach(r),nWo=t(IRe," (throws an error)."),IRe.forEach(r),lWo=i($l),dr=s($l,"DIV",{class:!0});var Il=n(dr);f(lA.$$.fragment,Il),iWo=i(Il),ime=s(Il,"P",{});var Par=n(ime);dWo=t(Par,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Par.forEach(r),cWo=i(Il),ic=s(Il,"P",{});var Yz=n(ic);mWo=t(Yz,`Note:
Loading a model from its configuration file does `),dme=s(Yz,"STRONG",{});var $ar=n(dme);fWo=t($ar,"not"),$ar.forEach(r),gWo=t(Yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cme=s(Yz,"CODE",{});var Iar=n(cme);hWo=t(Iar,"from_pretrained()"),Iar.forEach(r),uWo=t(Yz,"to load the model weights."),Yz.forEach(r),pWo=i(Il),mme=s(Il,"P",{});var jar=n(mme);_Wo=t(jar,"Examples:"),jar.forEach(r),bWo=i(Il),f(iA.$$.fragment,Il),Il.forEach(r),vWo=i($l),Ze=s($l,"DIV",{class:!0});var sa=n(Ze);f(dA.$$.fragment,sa),TWo=i(sa),fme=s(sa,"P",{});var Dar=n(fme);FWo=t(Dar,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),Dar.forEach(r),CWo=i(sa),fs=s(sa,"P",{});var c5=n(fs);MWo=t(c5,"The model class to instantiate is selected based on the "),gme=s(c5,"CODE",{});var Nar=n(gme);EWo=t(Nar,"model_type"),Nar.forEach(r),yWo=t(c5,` property of the config object (either
passed as an argument or loaded from `),hme=s(c5,"CODE",{});var qar=n(hme);wWo=t(qar,"pretrained_model_name_or_path"),qar.forEach(r),AWo=t(c5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ume=s(c5,"CODE",{});var Oar=n(ume);LWo=t(Oar,"pretrained_model_name_or_path"),Oar.forEach(r),BWo=t(c5,":"),c5.forEach(r),xWo=i(sa),cA=s(sa,"UL",{});var jRe=n(cA);U1=s(jRe,"LI",{});var l0e=n(U1);pme=s(l0e,"STRONG",{});var Gar=n(pme);kWo=t(Gar,"beit"),Gar.forEach(r),RWo=t(l0e," \u2014 "),ON=s(l0e,"A",{href:!0});var Xar=n(ON);SWo=t(Xar,"BeitForSemanticSegmentation"),Xar.forEach(r),PWo=t(l0e," (BEiT model)"),l0e.forEach(r),$Wo=i(jRe),J1=s(jRe,"LI",{});var i0e=n(J1);_me=s(i0e,"STRONG",{});var Var=n(_me);IWo=t(Var,"segformer"),Var.forEach(r),jWo=t(i0e," \u2014 "),GN=s(i0e,"A",{href:!0});var zar=n(GN);DWo=t(zar,"SegformerForSemanticSegmentation"),zar.forEach(r),NWo=t(i0e," (SegFormer model)"),i0e.forEach(r),jRe.forEach(r),qWo=i(sa),Y1=s(sa,"P",{});var d0e=n(Y1);OWo=t(d0e,"The model is set in evaluation mode by default using "),bme=s(d0e,"CODE",{});var War=n(bme);GWo=t(War,"model.eval()"),War.forEach(r),XWo=t(d0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vme=s(d0e,"CODE",{});var Qar=n(vme);VWo=t(Qar,"model.train()"),Qar.forEach(r),d0e.forEach(r),zWo=i(sa),Tme=s(sa,"P",{});var Har=n(Tme);WWo=t(Har,"Examples:"),Har.forEach(r),QWo=i(sa),f(mA.$$.fragment,sa),sa.forEach(r),$l.forEach(r),Axe=i(d),dc=s(d,"H2",{class:!0});var DRe=n(dc);K1=s(DRe,"A",{id:!0,class:!0,href:!0});var Uar=n(K1);Fme=s(Uar,"SPAN",{});var Jar=n(Fme);f(fA.$$.fragment,Jar),Jar.forEach(r),Uar.forEach(r),HWo=i(DRe),Cme=s(DRe,"SPAN",{});var Yar=n(Cme);UWo=t(Yar,"AutoModelForInstanceSegmentation"),Yar.forEach(r),DRe.forEach(r),Lxe=i(d),pt=s(d,"DIV",{class:!0});var jl=n(pt);f(gA.$$.fragment,jl),JWo=i(jl),cc=s(jl,"P",{});var Kz=n(cc);YWo=t(Kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the `),Mme=s(Kz,"CODE",{});var Kar=n(Mme);KWo=t(Kar,"from_pretrained()"),Kar.forEach(r),ZWo=t(Kz,"class method or the "),Eme=s(Kz,"CODE",{});var Zar=n(Eme);eQo=t(Zar,"from_config()"),Zar.forEach(r),oQo=t(Kz,`class
method.`),Kz.forEach(r),tQo=i(jl),hA=s(jl,"P",{});var NRe=n(hA);rQo=t(NRe,"This class cannot be instantiated directly using "),yme=s(NRe,"CODE",{});var esr=n(yme);aQo=t(esr,"__init__()"),esr.forEach(r),sQo=t(NRe," (throws an error)."),NRe.forEach(r),nQo=i(jl),cr=s(jl,"DIV",{class:!0});var Dl=n(cr);f(uA.$$.fragment,Dl),lQo=i(Dl),wme=s(Dl,"P",{});var osr=n(wme);iQo=t(osr,"Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration."),osr.forEach(r),dQo=i(Dl),mc=s(Dl,"P",{});var Zz=n(mc);cQo=t(Zz,`Note:
Loading a model from its configuration file does `),Ame=s(Zz,"STRONG",{});var tsr=n(Ame);mQo=t(tsr,"not"),tsr.forEach(r),fQo=t(Zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lme=s(Zz,"CODE",{});var rsr=n(Lme);gQo=t(rsr,"from_pretrained()"),rsr.forEach(r),hQo=t(Zz,"to load the model weights."),Zz.forEach(r),uQo=i(Dl),Bme=s(Dl,"P",{});var asr=n(Bme);pQo=t(asr,"Examples:"),asr.forEach(r),_Qo=i(Dl),f(pA.$$.fragment,Dl),Dl.forEach(r),bQo=i(jl),eo=s(jl,"DIV",{class:!0});var na=n(eo);f(_A.$$.fragment,na),vQo=i(na),xme=s(na,"P",{});var ssr=n(xme);TQo=t(ssr,"Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model."),ssr.forEach(r),FQo=i(na),gs=s(na,"P",{});var m5=n(gs);CQo=t(m5,"The model class to instantiate is selected based on the "),kme=s(m5,"CODE",{});var nsr=n(kme);MQo=t(nsr,"model_type"),nsr.forEach(r),EQo=t(m5,` property of the config object (either
passed as an argument or loaded from `),Rme=s(m5,"CODE",{});var lsr=n(Rme);yQo=t(lsr,"pretrained_model_name_or_path"),lsr.forEach(r),wQo=t(m5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sme=s(m5,"CODE",{});var isr=n(Sme);AQo=t(isr,"pretrained_model_name_or_path"),isr.forEach(r),LQo=t(m5,":"),m5.forEach(r),BQo=i(na),Pme=s(na,"UL",{});var dsr=n(Pme);Z1=s(dsr,"LI",{});var c0e=n(Z1);$me=s(c0e,"STRONG",{});var csr=n($me);xQo=t(csr,"maskformer"),csr.forEach(r),kQo=t(c0e," \u2014 "),XN=s(c0e,"A",{href:!0});var msr=n(XN);RQo=t(msr,"MaskFormerForInstanceSegmentation"),msr.forEach(r),SQo=t(c0e," (MaskFormer model)"),c0e.forEach(r),dsr.forEach(r),PQo=i(na),eF=s(na,"P",{});var m0e=n(eF);$Qo=t(m0e,"The model is set in evaluation mode by default using "),Ime=s(m0e,"CODE",{});var fsr=n(Ime);IQo=t(fsr,"model.eval()"),fsr.forEach(r),jQo=t(m0e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jme=s(m0e,"CODE",{});var gsr=n(jme);DQo=t(gsr,"model.train()"),gsr.forEach(r),m0e.forEach(r),NQo=i(na),Dme=s(na,"P",{});var hsr=n(Dme);qQo=t(hsr,"Examples:"),hsr.forEach(r),OQo=i(na),f(bA.$$.fragment,na),na.forEach(r),jl.forEach(r),Bxe=i(d),fc=s(d,"H2",{class:!0});var qRe=n(fc);oF=s(qRe,"A",{id:!0,class:!0,href:!0});var usr=n(oF);Nme=s(usr,"SPAN",{});var psr=n(Nme);f(vA.$$.fragment,psr),psr.forEach(r),usr.forEach(r),GQo=i(qRe),qme=s(qRe,"SPAN",{});var _sr=n(qme);XQo=t(_sr,"TFAutoModel"),_sr.forEach(r),qRe.forEach(r),xxe=i(d),_t=s(d,"DIV",{class:!0});var Nl=n(_t);f(TA.$$.fragment,Nl),VQo=i(Nl),gc=s(Nl,"P",{});var eW=n(gc);zQo=t(eW,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Ome=s(eW,"CODE",{});var bsr=n(Ome);WQo=t(bsr,"from_pretrained()"),bsr.forEach(r),QQo=t(eW,"class method or the "),Gme=s(eW,"CODE",{});var vsr=n(Gme);HQo=t(vsr,"from_config()"),vsr.forEach(r),UQo=t(eW,`class
method.`),eW.forEach(r),JQo=i(Nl),FA=s(Nl,"P",{});var ORe=n(FA);YQo=t(ORe,"This class cannot be instantiated directly using "),Xme=s(ORe,"CODE",{});var Tsr=n(Xme);KQo=t(Tsr,"__init__()"),Tsr.forEach(r),ZQo=t(ORe," (throws an error)."),ORe.forEach(r),eHo=i(Nl),mr=s(Nl,"DIV",{class:!0});var ql=n(mr);f(CA.$$.fragment,ql),oHo=i(ql),Vme=s(ql,"P",{});var Fsr=n(Vme);tHo=t(Fsr,"Instantiates one of the base model classes of the library from a configuration."),Fsr.forEach(r),rHo=i(ql),hc=s(ql,"P",{});var oW=n(hc);aHo=t(oW,`Note:
Loading a model from its configuration file does `),zme=s(oW,"STRONG",{});var Csr=n(zme);sHo=t(Csr,"not"),Csr.forEach(r),nHo=t(oW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wme=s(oW,"CODE",{});var Msr=n(Wme);lHo=t(Msr,"from_pretrained()"),Msr.forEach(r),iHo=t(oW,"to load the model weights."),oW.forEach(r),dHo=i(ql),Qme=s(ql,"P",{});var Esr=n(Qme);cHo=t(Esr,"Examples:"),Esr.forEach(r),mHo=i(ql),f(MA.$$.fragment,ql),ql.forEach(r),fHo=i(Nl),ho=s(Nl,"DIV",{class:!0});var ha=n(ho);f(EA.$$.fragment,ha),gHo=i(ha),Hme=s(ha,"P",{});var ysr=n(Hme);hHo=t(ysr,"Instantiate one of the base model classes of the library from a pretrained model."),ysr.forEach(r),uHo=i(ha),hs=s(ha,"P",{});var f5=n(hs);pHo=t(f5,"The model class to instantiate is selected based on the "),Ume=s(f5,"CODE",{});var wsr=n(Ume);_Ho=t(wsr,"model_type"),wsr.forEach(r),bHo=t(f5,` property of the config object (either
passed as an argument or loaded from `),Jme=s(f5,"CODE",{});var Asr=n(Jme);vHo=t(Asr,"pretrained_model_name_or_path"),Asr.forEach(r),THo=t(f5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yme=s(f5,"CODE",{});var Lsr=n(Yme);FHo=t(Lsr,"pretrained_model_name_or_path"),Lsr.forEach(r),CHo=t(f5,":"),f5.forEach(r),MHo=i(ha),B=s(ha,"UL",{});var x=n(B);tF=s(x,"LI",{});var f0e=n(tF);Kme=s(f0e,"STRONG",{});var Bsr=n(Kme);EHo=t(Bsr,"albert"),Bsr.forEach(r),yHo=t(f0e," \u2014 "),VN=s(f0e,"A",{href:!0});var xsr=n(VN);wHo=t(xsr,"TFAlbertModel"),xsr.forEach(r),AHo=t(f0e," (ALBERT model)"),f0e.forEach(r),LHo=i(x),rF=s(x,"LI",{});var g0e=n(rF);Zme=s(g0e,"STRONG",{});var ksr=n(Zme);BHo=t(ksr,"bart"),ksr.forEach(r),xHo=t(g0e," \u2014 "),zN=s(g0e,"A",{href:!0});var Rsr=n(zN);kHo=t(Rsr,"TFBartModel"),Rsr.forEach(r),RHo=t(g0e," (BART model)"),g0e.forEach(r),SHo=i(x),aF=s(x,"LI",{});var h0e=n(aF);efe=s(h0e,"STRONG",{});var Ssr=n(efe);PHo=t(Ssr,"bert"),Ssr.forEach(r),$Ho=t(h0e," \u2014 "),WN=s(h0e,"A",{href:!0});var Psr=n(WN);IHo=t(Psr,"TFBertModel"),Psr.forEach(r),jHo=t(h0e," (BERT model)"),h0e.forEach(r),DHo=i(x),sF=s(x,"LI",{});var u0e=n(sF);ofe=s(u0e,"STRONG",{});var $sr=n(ofe);NHo=t($sr,"blenderbot"),$sr.forEach(r),qHo=t(u0e," \u2014 "),QN=s(u0e,"A",{href:!0});var Isr=n(QN);OHo=t(Isr,"TFBlenderbotModel"),Isr.forEach(r),GHo=t(u0e," (Blenderbot model)"),u0e.forEach(r),XHo=i(x),nF=s(x,"LI",{});var p0e=n(nF);tfe=s(p0e,"STRONG",{});var jsr=n(tfe);VHo=t(jsr,"blenderbot-small"),jsr.forEach(r),zHo=t(p0e," \u2014 "),HN=s(p0e,"A",{href:!0});var Dsr=n(HN);WHo=t(Dsr,"TFBlenderbotSmallModel"),Dsr.forEach(r),QHo=t(p0e," (BlenderbotSmall model)"),p0e.forEach(r),HHo=i(x),lF=s(x,"LI",{});var _0e=n(lF);rfe=s(_0e,"STRONG",{});var Nsr=n(rfe);UHo=t(Nsr,"camembert"),Nsr.forEach(r),JHo=t(_0e," \u2014 "),UN=s(_0e,"A",{href:!0});var qsr=n(UN);YHo=t(qsr,"TFCamembertModel"),qsr.forEach(r),KHo=t(_0e," (CamemBERT model)"),_0e.forEach(r),ZHo=i(x),iF=s(x,"LI",{});var b0e=n(iF);afe=s(b0e,"STRONG",{});var Osr=n(afe);eUo=t(Osr,"clip"),Osr.forEach(r),oUo=t(b0e," \u2014 "),JN=s(b0e,"A",{href:!0});var Gsr=n(JN);tUo=t(Gsr,"TFCLIPModel"),Gsr.forEach(r),rUo=t(b0e," (CLIP model)"),b0e.forEach(r),aUo=i(x),dF=s(x,"LI",{});var v0e=n(dF);sfe=s(v0e,"STRONG",{});var Xsr=n(sfe);sUo=t(Xsr,"convbert"),Xsr.forEach(r),nUo=t(v0e," \u2014 "),YN=s(v0e,"A",{href:!0});var Vsr=n(YN);lUo=t(Vsr,"TFConvBertModel"),Vsr.forEach(r),iUo=t(v0e," (ConvBERT model)"),v0e.forEach(r),dUo=i(x),cF=s(x,"LI",{});var T0e=n(cF);nfe=s(T0e,"STRONG",{});var zsr=n(nfe);cUo=t(zsr,"convnext"),zsr.forEach(r),mUo=t(T0e," \u2014 "),KN=s(T0e,"A",{href:!0});var Wsr=n(KN);fUo=t(Wsr,"TFConvNextModel"),Wsr.forEach(r),gUo=t(T0e," (ConvNext model)"),T0e.forEach(r),hUo=i(x),mF=s(x,"LI",{});var F0e=n(mF);lfe=s(F0e,"STRONG",{});var Qsr=n(lfe);uUo=t(Qsr,"ctrl"),Qsr.forEach(r),pUo=t(F0e," \u2014 "),ZN=s(F0e,"A",{href:!0});var Hsr=n(ZN);_Uo=t(Hsr,"TFCTRLModel"),Hsr.forEach(r),bUo=t(F0e," (CTRL model)"),F0e.forEach(r),vUo=i(x),fF=s(x,"LI",{});var C0e=n(fF);ife=s(C0e,"STRONG",{});var Usr=n(ife);TUo=t(Usr,"deberta"),Usr.forEach(r),FUo=t(C0e," \u2014 "),eq=s(C0e,"A",{href:!0});var Jsr=n(eq);CUo=t(Jsr,"TFDebertaModel"),Jsr.forEach(r),MUo=t(C0e," (DeBERTa model)"),C0e.forEach(r),EUo=i(x),gF=s(x,"LI",{});var M0e=n(gF);dfe=s(M0e,"STRONG",{});var Ysr=n(dfe);yUo=t(Ysr,"deberta-v2"),Ysr.forEach(r),wUo=t(M0e," \u2014 "),oq=s(M0e,"A",{href:!0});var Ksr=n(oq);AUo=t(Ksr,"TFDebertaV2Model"),Ksr.forEach(r),LUo=t(M0e," (DeBERTa-v2 model)"),M0e.forEach(r),BUo=i(x),hF=s(x,"LI",{});var E0e=n(hF);cfe=s(E0e,"STRONG",{});var Zsr=n(cfe);xUo=t(Zsr,"distilbert"),Zsr.forEach(r),kUo=t(E0e," \u2014 "),tq=s(E0e,"A",{href:!0});var enr=n(tq);RUo=t(enr,"TFDistilBertModel"),enr.forEach(r),SUo=t(E0e," (DistilBERT model)"),E0e.forEach(r),PUo=i(x),uF=s(x,"LI",{});var y0e=n(uF);mfe=s(y0e,"STRONG",{});var onr=n(mfe);$Uo=t(onr,"dpr"),onr.forEach(r),IUo=t(y0e," \u2014 "),rq=s(y0e,"A",{href:!0});var tnr=n(rq);jUo=t(tnr,"TFDPRQuestionEncoder"),tnr.forEach(r),DUo=t(y0e," (DPR model)"),y0e.forEach(r),NUo=i(x),pF=s(x,"LI",{});var w0e=n(pF);ffe=s(w0e,"STRONG",{});var rnr=n(ffe);qUo=t(rnr,"electra"),rnr.forEach(r),OUo=t(w0e," \u2014 "),aq=s(w0e,"A",{href:!0});var anr=n(aq);GUo=t(anr,"TFElectraModel"),anr.forEach(r),XUo=t(w0e," (ELECTRA model)"),w0e.forEach(r),VUo=i(x),_F=s(x,"LI",{});var A0e=n(_F);gfe=s(A0e,"STRONG",{});var snr=n(gfe);zUo=t(snr,"flaubert"),snr.forEach(r),WUo=t(A0e," \u2014 "),sq=s(A0e,"A",{href:!0});var nnr=n(sq);QUo=t(nnr,"TFFlaubertModel"),nnr.forEach(r),HUo=t(A0e," (FlauBERT model)"),A0e.forEach(r),UUo=i(x),On=s(x,"LI",{});var x7=n(On);hfe=s(x7,"STRONG",{});var lnr=n(hfe);JUo=t(lnr,"funnel"),lnr.forEach(r),YUo=t(x7," \u2014 "),nq=s(x7,"A",{href:!0});var inr=n(nq);KUo=t(inr,"TFFunnelModel"),inr.forEach(r),ZUo=t(x7," or "),lq=s(x7,"A",{href:!0});var dnr=n(lq);eJo=t(dnr,"TFFunnelBaseModel"),dnr.forEach(r),oJo=t(x7," (Funnel Transformer model)"),x7.forEach(r),tJo=i(x),bF=s(x,"LI",{});var L0e=n(bF);ufe=s(L0e,"STRONG",{});var cnr=n(ufe);rJo=t(cnr,"gpt2"),cnr.forEach(r),aJo=t(L0e," \u2014 "),iq=s(L0e,"A",{href:!0});var mnr=n(iq);sJo=t(mnr,"TFGPT2Model"),mnr.forEach(r),nJo=t(L0e," (OpenAI GPT-2 model)"),L0e.forEach(r),lJo=i(x),vF=s(x,"LI",{});var B0e=n(vF);pfe=s(B0e,"STRONG",{});var fnr=n(pfe);iJo=t(fnr,"hubert"),fnr.forEach(r),dJo=t(B0e," \u2014 "),dq=s(B0e,"A",{href:!0});var gnr=n(dq);cJo=t(gnr,"TFHubertModel"),gnr.forEach(r),mJo=t(B0e," (Hubert model)"),B0e.forEach(r),fJo=i(x),TF=s(x,"LI",{});var x0e=n(TF);_fe=s(x0e,"STRONG",{});var hnr=n(_fe);gJo=t(hnr,"layoutlm"),hnr.forEach(r),hJo=t(x0e," \u2014 "),cq=s(x0e,"A",{href:!0});var unr=n(cq);uJo=t(unr,"TFLayoutLMModel"),unr.forEach(r),pJo=t(x0e," (LayoutLM model)"),x0e.forEach(r),_Jo=i(x),FF=s(x,"LI",{});var k0e=n(FF);bfe=s(k0e,"STRONG",{});var pnr=n(bfe);bJo=t(pnr,"led"),pnr.forEach(r),vJo=t(k0e," \u2014 "),mq=s(k0e,"A",{href:!0});var _nr=n(mq);TJo=t(_nr,"TFLEDModel"),_nr.forEach(r),FJo=t(k0e," (LED model)"),k0e.forEach(r),CJo=i(x),CF=s(x,"LI",{});var R0e=n(CF);vfe=s(R0e,"STRONG",{});var bnr=n(vfe);MJo=t(bnr,"longformer"),bnr.forEach(r),EJo=t(R0e," \u2014 "),fq=s(R0e,"A",{href:!0});var vnr=n(fq);yJo=t(vnr,"TFLongformerModel"),vnr.forEach(r),wJo=t(R0e," (Longformer model)"),R0e.forEach(r),AJo=i(x),MF=s(x,"LI",{});var S0e=n(MF);Tfe=s(S0e,"STRONG",{});var Tnr=n(Tfe);LJo=t(Tnr,"lxmert"),Tnr.forEach(r),BJo=t(S0e," \u2014 "),gq=s(S0e,"A",{href:!0});var Fnr=n(gq);xJo=t(Fnr,"TFLxmertModel"),Fnr.forEach(r),kJo=t(S0e," (LXMERT model)"),S0e.forEach(r),RJo=i(x),EF=s(x,"LI",{});var P0e=n(EF);Ffe=s(P0e,"STRONG",{});var Cnr=n(Ffe);SJo=t(Cnr,"marian"),Cnr.forEach(r),PJo=t(P0e," \u2014 "),hq=s(P0e,"A",{href:!0});var Mnr=n(hq);$Jo=t(Mnr,"TFMarianModel"),Mnr.forEach(r),IJo=t(P0e," (Marian model)"),P0e.forEach(r),jJo=i(x),yF=s(x,"LI",{});var $0e=n(yF);Cfe=s($0e,"STRONG",{});var Enr=n(Cfe);DJo=t(Enr,"mbart"),Enr.forEach(r),NJo=t($0e," \u2014 "),uq=s($0e,"A",{href:!0});var ynr=n(uq);qJo=t(ynr,"TFMBartModel"),ynr.forEach(r),OJo=t($0e," (mBART model)"),$0e.forEach(r),GJo=i(x),wF=s(x,"LI",{});var I0e=n(wF);Mfe=s(I0e,"STRONG",{});var wnr=n(Mfe);XJo=t(wnr,"mobilebert"),wnr.forEach(r),VJo=t(I0e," \u2014 "),pq=s(I0e,"A",{href:!0});var Anr=n(pq);zJo=t(Anr,"TFMobileBertModel"),Anr.forEach(r),WJo=t(I0e," (MobileBERT model)"),I0e.forEach(r),QJo=i(x),AF=s(x,"LI",{});var j0e=n(AF);Efe=s(j0e,"STRONG",{});var Lnr=n(Efe);HJo=t(Lnr,"mpnet"),Lnr.forEach(r),UJo=t(j0e," \u2014 "),_q=s(j0e,"A",{href:!0});var Bnr=n(_q);JJo=t(Bnr,"TFMPNetModel"),Bnr.forEach(r),YJo=t(j0e," (MPNet model)"),j0e.forEach(r),KJo=i(x),LF=s(x,"LI",{});var D0e=n(LF);yfe=s(D0e,"STRONG",{});var xnr=n(yfe);ZJo=t(xnr,"mt5"),xnr.forEach(r),eYo=t(D0e," \u2014 "),bq=s(D0e,"A",{href:!0});var knr=n(bq);oYo=t(knr,"TFMT5Model"),knr.forEach(r),tYo=t(D0e," (mT5 model)"),D0e.forEach(r),rYo=i(x),BF=s(x,"LI",{});var N0e=n(BF);wfe=s(N0e,"STRONG",{});var Rnr=n(wfe);aYo=t(Rnr,"openai-gpt"),Rnr.forEach(r),sYo=t(N0e," \u2014 "),vq=s(N0e,"A",{href:!0});var Snr=n(vq);nYo=t(Snr,"TFOpenAIGPTModel"),Snr.forEach(r),lYo=t(N0e," (OpenAI GPT model)"),N0e.forEach(r),iYo=i(x),xF=s(x,"LI",{});var q0e=n(xF);Afe=s(q0e,"STRONG",{});var Pnr=n(Afe);dYo=t(Pnr,"pegasus"),Pnr.forEach(r),cYo=t(q0e," \u2014 "),Tq=s(q0e,"A",{href:!0});var $nr=n(Tq);mYo=t($nr,"TFPegasusModel"),$nr.forEach(r),fYo=t(q0e," (Pegasus model)"),q0e.forEach(r),gYo=i(x),kF=s(x,"LI",{});var O0e=n(kF);Lfe=s(O0e,"STRONG",{});var Inr=n(Lfe);hYo=t(Inr,"rembert"),Inr.forEach(r),uYo=t(O0e," \u2014 "),Fq=s(O0e,"A",{href:!0});var jnr=n(Fq);pYo=t(jnr,"TFRemBertModel"),jnr.forEach(r),_Yo=t(O0e," (RemBERT model)"),O0e.forEach(r),bYo=i(x),RF=s(x,"LI",{});var G0e=n(RF);Bfe=s(G0e,"STRONG",{});var Dnr=n(Bfe);vYo=t(Dnr,"roberta"),Dnr.forEach(r),TYo=t(G0e," \u2014 "),Cq=s(G0e,"A",{href:!0});var Nnr=n(Cq);FYo=t(Nnr,"TFRobertaModel"),Nnr.forEach(r),CYo=t(G0e," (RoBERTa model)"),G0e.forEach(r),MYo=i(x),SF=s(x,"LI",{});var X0e=n(SF);xfe=s(X0e,"STRONG",{});var qnr=n(xfe);EYo=t(qnr,"roformer"),qnr.forEach(r),yYo=t(X0e," \u2014 "),Mq=s(X0e,"A",{href:!0});var Onr=n(Mq);wYo=t(Onr,"TFRoFormerModel"),Onr.forEach(r),AYo=t(X0e," (RoFormer model)"),X0e.forEach(r),LYo=i(x),PF=s(x,"LI",{});var V0e=n(PF);kfe=s(V0e,"STRONG",{});var Gnr=n(kfe);BYo=t(Gnr,"speech_to_text"),Gnr.forEach(r),xYo=t(V0e," \u2014 "),Eq=s(V0e,"A",{href:!0});var Xnr=n(Eq);kYo=t(Xnr,"TFSpeech2TextModel"),Xnr.forEach(r),RYo=t(V0e," (Speech2Text model)"),V0e.forEach(r),SYo=i(x),$F=s(x,"LI",{});var z0e=n($F);Rfe=s(z0e,"STRONG",{});var Vnr=n(Rfe);PYo=t(Vnr,"t5"),Vnr.forEach(r),$Yo=t(z0e," \u2014 "),yq=s(z0e,"A",{href:!0});var znr=n(yq);IYo=t(znr,"TFT5Model"),znr.forEach(r),jYo=t(z0e," (T5 model)"),z0e.forEach(r),DYo=i(x),IF=s(x,"LI",{});var W0e=n(IF);Sfe=s(W0e,"STRONG",{});var Wnr=n(Sfe);NYo=t(Wnr,"tapas"),Wnr.forEach(r),qYo=t(W0e," \u2014 "),wq=s(W0e,"A",{href:!0});var Qnr=n(wq);OYo=t(Qnr,"TFTapasModel"),Qnr.forEach(r),GYo=t(W0e," (TAPAS model)"),W0e.forEach(r),XYo=i(x),jF=s(x,"LI",{});var Q0e=n(jF);Pfe=s(Q0e,"STRONG",{});var Hnr=n(Pfe);VYo=t(Hnr,"transfo-xl"),Hnr.forEach(r),zYo=t(Q0e," \u2014 "),Aq=s(Q0e,"A",{href:!0});var Unr=n(Aq);WYo=t(Unr,"TFTransfoXLModel"),Unr.forEach(r),QYo=t(Q0e," (Transformer-XL model)"),Q0e.forEach(r),HYo=i(x),DF=s(x,"LI",{});var H0e=n(DF);$fe=s(H0e,"STRONG",{});var Jnr=n($fe);UYo=t(Jnr,"vit"),Jnr.forEach(r),JYo=t(H0e," \u2014 "),Lq=s(H0e,"A",{href:!0});var Ynr=n(Lq);YYo=t(Ynr,"TFViTModel"),Ynr.forEach(r),KYo=t(H0e," (ViT model)"),H0e.forEach(r),ZYo=i(x),NF=s(x,"LI",{});var U0e=n(NF);Ife=s(U0e,"STRONG",{});var Knr=n(Ife);eKo=t(Knr,"wav2vec2"),Knr.forEach(r),oKo=t(U0e," \u2014 "),Bq=s(U0e,"A",{href:!0});var Znr=n(Bq);tKo=t(Znr,"TFWav2Vec2Model"),Znr.forEach(r),rKo=t(U0e," (Wav2Vec2 model)"),U0e.forEach(r),aKo=i(x),qF=s(x,"LI",{});var J0e=n(qF);jfe=s(J0e,"STRONG",{});var elr=n(jfe);sKo=t(elr,"xlm"),elr.forEach(r),nKo=t(J0e," \u2014 "),xq=s(J0e,"A",{href:!0});var olr=n(xq);lKo=t(olr,"TFXLMModel"),olr.forEach(r),iKo=t(J0e," (XLM model)"),J0e.forEach(r),dKo=i(x),OF=s(x,"LI",{});var Y0e=n(OF);Dfe=s(Y0e,"STRONG",{});var tlr=n(Dfe);cKo=t(tlr,"xlm-roberta"),tlr.forEach(r),mKo=t(Y0e," \u2014 "),kq=s(Y0e,"A",{href:!0});var rlr=n(kq);fKo=t(rlr,"TFXLMRobertaModel"),rlr.forEach(r),gKo=t(Y0e," (XLM-RoBERTa model)"),Y0e.forEach(r),hKo=i(x),GF=s(x,"LI",{});var K0e=n(GF);Nfe=s(K0e,"STRONG",{});var alr=n(Nfe);uKo=t(alr,"xlnet"),alr.forEach(r),pKo=t(K0e," \u2014 "),Rq=s(K0e,"A",{href:!0});var slr=n(Rq);_Ko=t(slr,"TFXLNetModel"),slr.forEach(r),bKo=t(K0e," (XLNet model)"),K0e.forEach(r),x.forEach(r),vKo=i(ha),qfe=s(ha,"P",{});var nlr=n(qfe);TKo=t(nlr,"Examples:"),nlr.forEach(r),FKo=i(ha),f(yA.$$.fragment,ha),ha.forEach(r),Nl.forEach(r),kxe=i(d),uc=s(d,"H2",{class:!0});var GRe=n(uc);XF=s(GRe,"A",{id:!0,class:!0,href:!0});var llr=n(XF);Ofe=s(llr,"SPAN",{});var ilr=n(Ofe);f(wA.$$.fragment,ilr),ilr.forEach(r),llr.forEach(r),CKo=i(GRe),Gfe=s(GRe,"SPAN",{});var dlr=n(Gfe);MKo=t(dlr,"TFAutoModelForPreTraining"),dlr.forEach(r),GRe.forEach(r),Rxe=i(d),bt=s(d,"DIV",{class:!0});var Ol=n(bt);f(AA.$$.fragment,Ol),EKo=i(Ol),pc=s(Ol,"P",{});var tW=n(pc);yKo=t(tW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Xfe=s(tW,"CODE",{});var clr=n(Xfe);wKo=t(clr,"from_pretrained()"),clr.forEach(r),AKo=t(tW,"class method or the "),Vfe=s(tW,"CODE",{});var mlr=n(Vfe);LKo=t(mlr,"from_config()"),mlr.forEach(r),BKo=t(tW,`class
method.`),tW.forEach(r),xKo=i(Ol),LA=s(Ol,"P",{});var XRe=n(LA);kKo=t(XRe,"This class cannot be instantiated directly using "),zfe=s(XRe,"CODE",{});var flr=n(zfe);RKo=t(flr,"__init__()"),flr.forEach(r),SKo=t(XRe," (throws an error)."),XRe.forEach(r),PKo=i(Ol),fr=s(Ol,"DIV",{class:!0});var Gl=n(fr);f(BA.$$.fragment,Gl),$Ko=i(Gl),Wfe=s(Gl,"P",{});var glr=n(Wfe);IKo=t(glr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),glr.forEach(r),jKo=i(Gl),_c=s(Gl,"P",{});var rW=n(_c);DKo=t(rW,`Note:
Loading a model from its configuration file does `),Qfe=s(rW,"STRONG",{});var hlr=n(Qfe);NKo=t(hlr,"not"),hlr.forEach(r),qKo=t(rW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hfe=s(rW,"CODE",{});var ulr=n(Hfe);OKo=t(ulr,"from_pretrained()"),ulr.forEach(r),GKo=t(rW,"to load the model weights."),rW.forEach(r),XKo=i(Gl),Ufe=s(Gl,"P",{});var plr=n(Ufe);VKo=t(plr,"Examples:"),plr.forEach(r),zKo=i(Gl),f(xA.$$.fragment,Gl),Gl.forEach(r),WKo=i(Ol),uo=s(Ol,"DIV",{class:!0});var ua=n(uo);f(kA.$$.fragment,ua),QKo=i(ua),Jfe=s(ua,"P",{});var _lr=n(Jfe);HKo=t(_lr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),_lr.forEach(r),UKo=i(ua),us=s(ua,"P",{});var g5=n(us);JKo=t(g5,"The model class to instantiate is selected based on the "),Yfe=s(g5,"CODE",{});var blr=n(Yfe);YKo=t(blr,"model_type"),blr.forEach(r),KKo=t(g5,` property of the config object (either
passed as an argument or loaded from `),Kfe=s(g5,"CODE",{});var vlr=n(Kfe);ZKo=t(vlr,"pretrained_model_name_or_path"),vlr.forEach(r),eZo=t(g5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zfe=s(g5,"CODE",{});var Tlr=n(Zfe);oZo=t(Tlr,"pretrained_model_name_or_path"),Tlr.forEach(r),tZo=t(g5,":"),g5.forEach(r),rZo=i(ua),H=s(ua,"UL",{});var U=n(H);VF=s(U,"LI",{});var Z0e=n(VF);ege=s(Z0e,"STRONG",{});var Flr=n(ege);aZo=t(Flr,"albert"),Flr.forEach(r),sZo=t(Z0e," \u2014 "),Sq=s(Z0e,"A",{href:!0});var Clr=n(Sq);nZo=t(Clr,"TFAlbertForPreTraining"),Clr.forEach(r),lZo=t(Z0e," (ALBERT model)"),Z0e.forEach(r),iZo=i(U),zF=s(U,"LI",{});var eLe=n(zF);oge=s(eLe,"STRONG",{});var Mlr=n(oge);dZo=t(Mlr,"bart"),Mlr.forEach(r),cZo=t(eLe," \u2014 "),Pq=s(eLe,"A",{href:!0});var Elr=n(Pq);mZo=t(Elr,"TFBartForConditionalGeneration"),Elr.forEach(r),fZo=t(eLe," (BART model)"),eLe.forEach(r),gZo=i(U),WF=s(U,"LI",{});var oLe=n(WF);tge=s(oLe,"STRONG",{});var ylr=n(tge);hZo=t(ylr,"bert"),ylr.forEach(r),uZo=t(oLe," \u2014 "),$q=s(oLe,"A",{href:!0});var wlr=n($q);pZo=t(wlr,"TFBertForPreTraining"),wlr.forEach(r),_Zo=t(oLe," (BERT model)"),oLe.forEach(r),bZo=i(U),QF=s(U,"LI",{});var tLe=n(QF);rge=s(tLe,"STRONG",{});var Alr=n(rge);vZo=t(Alr,"camembert"),Alr.forEach(r),TZo=t(tLe," \u2014 "),Iq=s(tLe,"A",{href:!0});var Llr=n(Iq);FZo=t(Llr,"TFCamembertForMaskedLM"),Llr.forEach(r),CZo=t(tLe," (CamemBERT model)"),tLe.forEach(r),MZo=i(U),HF=s(U,"LI",{});var rLe=n(HF);age=s(rLe,"STRONG",{});var Blr=n(age);EZo=t(Blr,"ctrl"),Blr.forEach(r),yZo=t(rLe," \u2014 "),jq=s(rLe,"A",{href:!0});var xlr=n(jq);wZo=t(xlr,"TFCTRLLMHeadModel"),xlr.forEach(r),AZo=t(rLe," (CTRL model)"),rLe.forEach(r),LZo=i(U),UF=s(U,"LI",{});var aLe=n(UF);sge=s(aLe,"STRONG",{});var klr=n(sge);BZo=t(klr,"distilbert"),klr.forEach(r),xZo=t(aLe," \u2014 "),Dq=s(aLe,"A",{href:!0});var Rlr=n(Dq);kZo=t(Rlr,"TFDistilBertForMaskedLM"),Rlr.forEach(r),RZo=t(aLe," (DistilBERT model)"),aLe.forEach(r),SZo=i(U),JF=s(U,"LI",{});var sLe=n(JF);nge=s(sLe,"STRONG",{});var Slr=n(nge);PZo=t(Slr,"electra"),Slr.forEach(r),$Zo=t(sLe," \u2014 "),Nq=s(sLe,"A",{href:!0});var Plr=n(Nq);IZo=t(Plr,"TFElectraForPreTraining"),Plr.forEach(r),jZo=t(sLe," (ELECTRA model)"),sLe.forEach(r),DZo=i(U),YF=s(U,"LI",{});var nLe=n(YF);lge=s(nLe,"STRONG",{});var $lr=n(lge);NZo=t($lr,"flaubert"),$lr.forEach(r),qZo=t(nLe," \u2014 "),qq=s(nLe,"A",{href:!0});var Ilr=n(qq);OZo=t(Ilr,"TFFlaubertWithLMHeadModel"),Ilr.forEach(r),GZo=t(nLe," (FlauBERT model)"),nLe.forEach(r),XZo=i(U),KF=s(U,"LI",{});var lLe=n(KF);ige=s(lLe,"STRONG",{});var jlr=n(ige);VZo=t(jlr,"funnel"),jlr.forEach(r),zZo=t(lLe," \u2014 "),Oq=s(lLe,"A",{href:!0});var Dlr=n(Oq);WZo=t(Dlr,"TFFunnelForPreTraining"),Dlr.forEach(r),QZo=t(lLe," (Funnel Transformer model)"),lLe.forEach(r),HZo=i(U),ZF=s(U,"LI",{});var iLe=n(ZF);dge=s(iLe,"STRONG",{});var Nlr=n(dge);UZo=t(Nlr,"gpt2"),Nlr.forEach(r),JZo=t(iLe," \u2014 "),Gq=s(iLe,"A",{href:!0});var qlr=n(Gq);YZo=t(qlr,"TFGPT2LMHeadModel"),qlr.forEach(r),KZo=t(iLe," (OpenAI GPT-2 model)"),iLe.forEach(r),ZZo=i(U),eC=s(U,"LI",{});var dLe=n(eC);cge=s(dLe,"STRONG",{});var Olr=n(cge);eet=t(Olr,"layoutlm"),Olr.forEach(r),oet=t(dLe," \u2014 "),Xq=s(dLe,"A",{href:!0});var Glr=n(Xq);tet=t(Glr,"TFLayoutLMForMaskedLM"),Glr.forEach(r),ret=t(dLe," (LayoutLM model)"),dLe.forEach(r),aet=i(U),oC=s(U,"LI",{});var cLe=n(oC);mge=s(cLe,"STRONG",{});var Xlr=n(mge);set=t(Xlr,"lxmert"),Xlr.forEach(r),net=t(cLe," \u2014 "),Vq=s(cLe,"A",{href:!0});var Vlr=n(Vq);iet=t(Vlr,"TFLxmertForPreTraining"),Vlr.forEach(r),det=t(cLe," (LXMERT model)"),cLe.forEach(r),cet=i(U),tC=s(U,"LI",{});var mLe=n(tC);fge=s(mLe,"STRONG",{});var zlr=n(fge);met=t(zlr,"mobilebert"),zlr.forEach(r),fet=t(mLe," \u2014 "),zq=s(mLe,"A",{href:!0});var Wlr=n(zq);get=t(Wlr,"TFMobileBertForPreTraining"),Wlr.forEach(r),het=t(mLe," (MobileBERT model)"),mLe.forEach(r),uet=i(U),rC=s(U,"LI",{});var fLe=n(rC);gge=s(fLe,"STRONG",{});var Qlr=n(gge);pet=t(Qlr,"mpnet"),Qlr.forEach(r),_et=t(fLe," \u2014 "),Wq=s(fLe,"A",{href:!0});var Hlr=n(Wq);bet=t(Hlr,"TFMPNetForMaskedLM"),Hlr.forEach(r),vet=t(fLe," (MPNet model)"),fLe.forEach(r),Tet=i(U),aC=s(U,"LI",{});var gLe=n(aC);hge=s(gLe,"STRONG",{});var Ulr=n(hge);Fet=t(Ulr,"openai-gpt"),Ulr.forEach(r),Cet=t(gLe," \u2014 "),Qq=s(gLe,"A",{href:!0});var Jlr=n(Qq);Met=t(Jlr,"TFOpenAIGPTLMHeadModel"),Jlr.forEach(r),Eet=t(gLe," (OpenAI GPT model)"),gLe.forEach(r),yet=i(U),sC=s(U,"LI",{});var hLe=n(sC);uge=s(hLe,"STRONG",{});var Ylr=n(uge);wet=t(Ylr,"roberta"),Ylr.forEach(r),Aet=t(hLe," \u2014 "),Hq=s(hLe,"A",{href:!0});var Klr=n(Hq);Let=t(Klr,"TFRobertaForMaskedLM"),Klr.forEach(r),Bet=t(hLe," (RoBERTa model)"),hLe.forEach(r),xet=i(U),nC=s(U,"LI",{});var uLe=n(nC);pge=s(uLe,"STRONG",{});var Zlr=n(pge);ket=t(Zlr,"t5"),Zlr.forEach(r),Ret=t(uLe," \u2014 "),Uq=s(uLe,"A",{href:!0});var eir=n(Uq);Set=t(eir,"TFT5ForConditionalGeneration"),eir.forEach(r),Pet=t(uLe," (T5 model)"),uLe.forEach(r),$et=i(U),lC=s(U,"LI",{});var pLe=n(lC);_ge=s(pLe,"STRONG",{});var oir=n(_ge);Iet=t(oir,"tapas"),oir.forEach(r),jet=t(pLe," \u2014 "),Jq=s(pLe,"A",{href:!0});var tir=n(Jq);Det=t(tir,"TFTapasForMaskedLM"),tir.forEach(r),Net=t(pLe," (TAPAS model)"),pLe.forEach(r),qet=i(U),iC=s(U,"LI",{});var _Le=n(iC);bge=s(_Le,"STRONG",{});var rir=n(bge);Oet=t(rir,"transfo-xl"),rir.forEach(r),Get=t(_Le," \u2014 "),Yq=s(_Le,"A",{href:!0});var air=n(Yq);Xet=t(air,"TFTransfoXLLMHeadModel"),air.forEach(r),Vet=t(_Le," (Transformer-XL model)"),_Le.forEach(r),zet=i(U),dC=s(U,"LI",{});var bLe=n(dC);vge=s(bLe,"STRONG",{});var sir=n(vge);Wet=t(sir,"xlm"),sir.forEach(r),Qet=t(bLe," \u2014 "),Kq=s(bLe,"A",{href:!0});var nir=n(Kq);Het=t(nir,"TFXLMWithLMHeadModel"),nir.forEach(r),Uet=t(bLe," (XLM model)"),bLe.forEach(r),Jet=i(U),cC=s(U,"LI",{});var vLe=n(cC);Tge=s(vLe,"STRONG",{});var lir=n(Tge);Yet=t(lir,"xlm-roberta"),lir.forEach(r),Ket=t(vLe," \u2014 "),Zq=s(vLe,"A",{href:!0});var iir=n(Zq);Zet=t(iir,"TFXLMRobertaForMaskedLM"),iir.forEach(r),eot=t(vLe," (XLM-RoBERTa model)"),vLe.forEach(r),oot=i(U),mC=s(U,"LI",{});var TLe=n(mC);Fge=s(TLe,"STRONG",{});var dir=n(Fge);tot=t(dir,"xlnet"),dir.forEach(r),rot=t(TLe," \u2014 "),eO=s(TLe,"A",{href:!0});var cir=n(eO);aot=t(cir,"TFXLNetLMHeadModel"),cir.forEach(r),sot=t(TLe," (XLNet model)"),TLe.forEach(r),U.forEach(r),not=i(ua),Cge=s(ua,"P",{});var mir=n(Cge);lot=t(mir,"Examples:"),mir.forEach(r),iot=i(ua),f(RA.$$.fragment,ua),ua.forEach(r),Ol.forEach(r),Sxe=i(d),bc=s(d,"H2",{class:!0});var VRe=n(bc);fC=s(VRe,"A",{id:!0,class:!0,href:!0});var fir=n(fC);Mge=s(fir,"SPAN",{});var gir=n(Mge);f(SA.$$.fragment,gir),gir.forEach(r),fir.forEach(r),dot=i(VRe),Ege=s(VRe,"SPAN",{});var hir=n(Ege);cot=t(hir,"TFAutoModelForCausalLM"),hir.forEach(r),VRe.forEach(r),Pxe=i(d),vt=s(d,"DIV",{class:!0});var Xl=n(vt);f(PA.$$.fragment,Xl),mot=i(Xl),vc=s(Xl,"P",{});var aW=n(vc);fot=t(aW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),yge=s(aW,"CODE",{});var uir=n(yge);got=t(uir,"from_pretrained()"),uir.forEach(r),hot=t(aW,"class method or the "),wge=s(aW,"CODE",{});var pir=n(wge);uot=t(pir,"from_config()"),pir.forEach(r),pot=t(aW,`class
method.`),aW.forEach(r),_ot=i(Xl),$A=s(Xl,"P",{});var zRe=n($A);bot=t(zRe,"This class cannot be instantiated directly using "),Age=s(zRe,"CODE",{});var _ir=n(Age);vot=t(_ir,"__init__()"),_ir.forEach(r),Tot=t(zRe," (throws an error)."),zRe.forEach(r),Fot=i(Xl),gr=s(Xl,"DIV",{class:!0});var Vl=n(gr);f(IA.$$.fragment,Vl),Cot=i(Vl),Lge=s(Vl,"P",{});var bir=n(Lge);Mot=t(bir,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),bir.forEach(r),Eot=i(Vl),Tc=s(Vl,"P",{});var sW=n(Tc);yot=t(sW,`Note:
Loading a model from its configuration file does `),Bge=s(sW,"STRONG",{});var vir=n(Bge);wot=t(vir,"not"),vir.forEach(r),Aot=t(sW,` load the model weights. It only affects the
model\u2019s configuration. Use `),xge=s(sW,"CODE",{});var Tir=n(xge);Lot=t(Tir,"from_pretrained()"),Tir.forEach(r),Bot=t(sW,"to load the model weights."),sW.forEach(r),xot=i(Vl),kge=s(Vl,"P",{});var Fir=n(kge);kot=t(Fir,"Examples:"),Fir.forEach(r),Rot=i(Vl),f(jA.$$.fragment,Vl),Vl.forEach(r),Sot=i(Xl),po=s(Xl,"DIV",{class:!0});var pa=n(po);f(DA.$$.fragment,pa),Pot=i(pa),Rge=s(pa,"P",{});var Cir=n(Rge);$ot=t(Cir,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Cir.forEach(r),Iot=i(pa),ps=s(pa,"P",{});var h5=n(ps);jot=t(h5,"The model class to instantiate is selected based on the "),Sge=s(h5,"CODE",{});var Mir=n(Sge);Dot=t(Mir,"model_type"),Mir.forEach(r),Not=t(h5,` property of the config object (either
passed as an argument or loaded from `),Pge=s(h5,"CODE",{});var Eir=n(Pge);qot=t(Eir,"pretrained_model_name_or_path"),Eir.forEach(r),Oot=t(h5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$ge=s(h5,"CODE",{});var yir=n($ge);Got=t(yir,"pretrained_model_name_or_path"),yir.forEach(r),Xot=t(h5,":"),h5.forEach(r),Vot=i(pa),he=s(pa,"UL",{});var Me=n(he);gC=s(Me,"LI",{});var FLe=n(gC);Ige=s(FLe,"STRONG",{});var wir=n(Ige);zot=t(wir,"bert"),wir.forEach(r),Wot=t(FLe," \u2014 "),oO=s(FLe,"A",{href:!0});var Air=n(oO);Qot=t(Air,"TFBertLMHeadModel"),Air.forEach(r),Hot=t(FLe," (BERT model)"),FLe.forEach(r),Uot=i(Me),hC=s(Me,"LI",{});var CLe=n(hC);jge=s(CLe,"STRONG",{});var Lir=n(jge);Jot=t(Lir,"ctrl"),Lir.forEach(r),Yot=t(CLe," \u2014 "),tO=s(CLe,"A",{href:!0});var Bir=n(tO);Kot=t(Bir,"TFCTRLLMHeadModel"),Bir.forEach(r),Zot=t(CLe," (CTRL model)"),CLe.forEach(r),ett=i(Me),uC=s(Me,"LI",{});var MLe=n(uC);Dge=s(MLe,"STRONG",{});var xir=n(Dge);ott=t(xir,"gpt2"),xir.forEach(r),ttt=t(MLe," \u2014 "),rO=s(MLe,"A",{href:!0});var kir=n(rO);rtt=t(kir,"TFGPT2LMHeadModel"),kir.forEach(r),att=t(MLe," (OpenAI GPT-2 model)"),MLe.forEach(r),stt=i(Me),pC=s(Me,"LI",{});var ELe=n(pC);Nge=s(ELe,"STRONG",{});var Rir=n(Nge);ntt=t(Rir,"openai-gpt"),Rir.forEach(r),ltt=t(ELe," \u2014 "),aO=s(ELe,"A",{href:!0});var Sir=n(aO);itt=t(Sir,"TFOpenAIGPTLMHeadModel"),Sir.forEach(r),dtt=t(ELe," (OpenAI GPT model)"),ELe.forEach(r),ctt=i(Me),_C=s(Me,"LI",{});var yLe=n(_C);qge=s(yLe,"STRONG",{});var Pir=n(qge);mtt=t(Pir,"rembert"),Pir.forEach(r),ftt=t(yLe," \u2014 "),sO=s(yLe,"A",{href:!0});var $ir=n(sO);gtt=t($ir,"TFRemBertForCausalLM"),$ir.forEach(r),htt=t(yLe," (RemBERT model)"),yLe.forEach(r),utt=i(Me),bC=s(Me,"LI",{});var wLe=n(bC);Oge=s(wLe,"STRONG",{});var Iir=n(Oge);ptt=t(Iir,"roberta"),Iir.forEach(r),_tt=t(wLe," \u2014 "),nO=s(wLe,"A",{href:!0});var jir=n(nO);btt=t(jir,"TFRobertaForCausalLM"),jir.forEach(r),vtt=t(wLe," (RoBERTa model)"),wLe.forEach(r),Ttt=i(Me),vC=s(Me,"LI",{});var ALe=n(vC);Gge=s(ALe,"STRONG",{});var Dir=n(Gge);Ftt=t(Dir,"roformer"),Dir.forEach(r),Ctt=t(ALe," \u2014 "),lO=s(ALe,"A",{href:!0});var Nir=n(lO);Mtt=t(Nir,"TFRoFormerForCausalLM"),Nir.forEach(r),Ett=t(ALe," (RoFormer model)"),ALe.forEach(r),ytt=i(Me),TC=s(Me,"LI",{});var LLe=n(TC);Xge=s(LLe,"STRONG",{});var qir=n(Xge);wtt=t(qir,"transfo-xl"),qir.forEach(r),Att=t(LLe," \u2014 "),iO=s(LLe,"A",{href:!0});var Oir=n(iO);Ltt=t(Oir,"TFTransfoXLLMHeadModel"),Oir.forEach(r),Btt=t(LLe," (Transformer-XL model)"),LLe.forEach(r),xtt=i(Me),FC=s(Me,"LI",{});var BLe=n(FC);Vge=s(BLe,"STRONG",{});var Gir=n(Vge);ktt=t(Gir,"xlm"),Gir.forEach(r),Rtt=t(BLe," \u2014 "),dO=s(BLe,"A",{href:!0});var Xir=n(dO);Stt=t(Xir,"TFXLMWithLMHeadModel"),Xir.forEach(r),Ptt=t(BLe," (XLM model)"),BLe.forEach(r),$tt=i(Me),CC=s(Me,"LI",{});var xLe=n(CC);zge=s(xLe,"STRONG",{});var Vir=n(zge);Itt=t(Vir,"xlnet"),Vir.forEach(r),jtt=t(xLe," \u2014 "),cO=s(xLe,"A",{href:!0});var zir=n(cO);Dtt=t(zir,"TFXLNetLMHeadModel"),zir.forEach(r),Ntt=t(xLe," (XLNet model)"),xLe.forEach(r),Me.forEach(r),qtt=i(pa),Wge=s(pa,"P",{});var Wir=n(Wge);Ott=t(Wir,"Examples:"),Wir.forEach(r),Gtt=i(pa),f(NA.$$.fragment,pa),pa.forEach(r),Xl.forEach(r),$xe=i(d),Fc=s(d,"H2",{class:!0});var WRe=n(Fc);MC=s(WRe,"A",{id:!0,class:!0,href:!0});var Qir=n(MC);Qge=s(Qir,"SPAN",{});var Hir=n(Qge);f(qA.$$.fragment,Hir),Hir.forEach(r),Qir.forEach(r),Xtt=i(WRe),Hge=s(WRe,"SPAN",{});var Uir=n(Hge);Vtt=t(Uir,"TFAutoModelForImageClassification"),Uir.forEach(r),WRe.forEach(r),Ixe=i(d),Tt=s(d,"DIV",{class:!0});var zl=n(Tt);f(OA.$$.fragment,zl),ztt=i(zl),Cc=s(zl,"P",{});var nW=n(Cc);Wtt=t(nW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Uge=s(nW,"CODE",{});var Jir=n(Uge);Qtt=t(Jir,"from_pretrained()"),Jir.forEach(r),Htt=t(nW,"class method or the "),Jge=s(nW,"CODE",{});var Yir=n(Jge);Utt=t(Yir,"from_config()"),Yir.forEach(r),Jtt=t(nW,`class
method.`),nW.forEach(r),Ytt=i(zl),GA=s(zl,"P",{});var QRe=n(GA);Ktt=t(QRe,"This class cannot be instantiated directly using "),Yge=s(QRe,"CODE",{});var Kir=n(Yge);Ztt=t(Kir,"__init__()"),Kir.forEach(r),ert=t(QRe," (throws an error)."),QRe.forEach(r),ort=i(zl),hr=s(zl,"DIV",{class:!0});var Wl=n(hr);f(XA.$$.fragment,Wl),trt=i(Wl),Kge=s(Wl,"P",{});var Zir=n(Kge);rrt=t(Zir,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Zir.forEach(r),art=i(Wl),Mc=s(Wl,"P",{});var lW=n(Mc);srt=t(lW,`Note:
Loading a model from its configuration file does `),Zge=s(lW,"STRONG",{});var edr=n(Zge);nrt=t(edr,"not"),edr.forEach(r),lrt=t(lW,` load the model weights. It only affects the
model\u2019s configuration. Use `),ehe=s(lW,"CODE",{});var odr=n(ehe);irt=t(odr,"from_pretrained()"),odr.forEach(r),drt=t(lW,"to load the model weights."),lW.forEach(r),crt=i(Wl),ohe=s(Wl,"P",{});var tdr=n(ohe);mrt=t(tdr,"Examples:"),tdr.forEach(r),frt=i(Wl),f(VA.$$.fragment,Wl),Wl.forEach(r),grt=i(zl),_o=s(zl,"DIV",{class:!0});var _a=n(_o);f(zA.$$.fragment,_a),hrt=i(_a),the=s(_a,"P",{});var rdr=n(the);urt=t(rdr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),rdr.forEach(r),prt=i(_a),_s=s(_a,"P",{});var u5=n(_s);_rt=t(u5,"The model class to instantiate is selected based on the "),rhe=s(u5,"CODE",{});var adr=n(rhe);brt=t(adr,"model_type"),adr.forEach(r),vrt=t(u5,` property of the config object (either
passed as an argument or loaded from `),ahe=s(u5,"CODE",{});var sdr=n(ahe);Trt=t(sdr,"pretrained_model_name_or_path"),sdr.forEach(r),Frt=t(u5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),she=s(u5,"CODE",{});var ndr=n(she);Crt=t(ndr,"pretrained_model_name_or_path"),ndr.forEach(r),Mrt=t(u5,":"),u5.forEach(r),Ert=i(_a),WA=s(_a,"UL",{});var HRe=n(WA);EC=s(HRe,"LI",{});var kLe=n(EC);nhe=s(kLe,"STRONG",{});var ldr=n(nhe);yrt=t(ldr,"convnext"),ldr.forEach(r),wrt=t(kLe," \u2014 "),mO=s(kLe,"A",{href:!0});var idr=n(mO);Art=t(idr,"TFConvNextForImageClassification"),idr.forEach(r),Lrt=t(kLe," (ConvNext model)"),kLe.forEach(r),Brt=i(HRe),yC=s(HRe,"LI",{});var RLe=n(yC);lhe=s(RLe,"STRONG",{});var ddr=n(lhe);xrt=t(ddr,"vit"),ddr.forEach(r),krt=t(RLe," \u2014 "),fO=s(RLe,"A",{href:!0});var cdr=n(fO);Rrt=t(cdr,"TFViTForImageClassification"),cdr.forEach(r),Srt=t(RLe," (ViT model)"),RLe.forEach(r),HRe.forEach(r),Prt=i(_a),ihe=s(_a,"P",{});var mdr=n(ihe);$rt=t(mdr,"Examples:"),mdr.forEach(r),Irt=i(_a),f(QA.$$.fragment,_a),_a.forEach(r),zl.forEach(r),jxe=i(d),Ec=s(d,"H2",{class:!0});var URe=n(Ec);wC=s(URe,"A",{id:!0,class:!0,href:!0});var fdr=n(wC);dhe=s(fdr,"SPAN",{});var gdr=n(dhe);f(HA.$$.fragment,gdr),gdr.forEach(r),fdr.forEach(r),jrt=i(URe),che=s(URe,"SPAN",{});var hdr=n(che);Drt=t(hdr,"TFAutoModelForMaskedLM"),hdr.forEach(r),URe.forEach(r),Dxe=i(d),Ft=s(d,"DIV",{class:!0});var Ql=n(Ft);f(UA.$$.fragment,Ql),Nrt=i(Ql),yc=s(Ql,"P",{});var iW=n(yc);qrt=t(iW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mhe=s(iW,"CODE",{});var udr=n(mhe);Ort=t(udr,"from_pretrained()"),udr.forEach(r),Grt=t(iW,"class method or the "),fhe=s(iW,"CODE",{});var pdr=n(fhe);Xrt=t(pdr,"from_config()"),pdr.forEach(r),Vrt=t(iW,`class
method.`),iW.forEach(r),zrt=i(Ql),JA=s(Ql,"P",{});var JRe=n(JA);Wrt=t(JRe,"This class cannot be instantiated directly using "),ghe=s(JRe,"CODE",{});var _dr=n(ghe);Qrt=t(_dr,"__init__()"),_dr.forEach(r),Hrt=t(JRe," (throws an error)."),JRe.forEach(r),Urt=i(Ql),ur=s(Ql,"DIV",{class:!0});var Hl=n(ur);f(YA.$$.fragment,Hl),Jrt=i(Hl),hhe=s(Hl,"P",{});var bdr=n(hhe);Yrt=t(bdr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),bdr.forEach(r),Krt=i(Hl),wc=s(Hl,"P",{});var dW=n(wc);Zrt=t(dW,`Note:
Loading a model from its configuration file does `),uhe=s(dW,"STRONG",{});var vdr=n(uhe);eat=t(vdr,"not"),vdr.forEach(r),oat=t(dW,` load the model weights. It only affects the
model\u2019s configuration. Use `),phe=s(dW,"CODE",{});var Tdr=n(phe);tat=t(Tdr,"from_pretrained()"),Tdr.forEach(r),rat=t(dW,"to load the model weights."),dW.forEach(r),aat=i(Hl),_he=s(Hl,"P",{});var Fdr=n(_he);sat=t(Fdr,"Examples:"),Fdr.forEach(r),nat=i(Hl),f(KA.$$.fragment,Hl),Hl.forEach(r),lat=i(Ql),bo=s(Ql,"DIV",{class:!0});var ba=n(bo);f(ZA.$$.fragment,ba),iat=i(ba),bhe=s(ba,"P",{});var Cdr=n(bhe);dat=t(Cdr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Cdr.forEach(r),cat=i(ba),bs=s(ba,"P",{});var p5=n(bs);mat=t(p5,"The model class to instantiate is selected based on the "),vhe=s(p5,"CODE",{});var Mdr=n(vhe);fat=t(Mdr,"model_type"),Mdr.forEach(r),gat=t(p5,` property of the config object (either
passed as an argument or loaded from `),The=s(p5,"CODE",{});var Edr=n(The);hat=t(Edr,"pretrained_model_name_or_path"),Edr.forEach(r),uat=t(p5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fhe=s(p5,"CODE",{});var ydr=n(Fhe);pat=t(ydr,"pretrained_model_name_or_path"),ydr.forEach(r),_at=t(p5,":"),p5.forEach(r),bat=i(ba),Y=s(ba,"UL",{});var ee=n(Y);AC=s(ee,"LI",{});var SLe=n(AC);Che=s(SLe,"STRONG",{});var wdr=n(Che);vat=t(wdr,"albert"),wdr.forEach(r),Tat=t(SLe," \u2014 "),gO=s(SLe,"A",{href:!0});var Adr=n(gO);Fat=t(Adr,"TFAlbertForMaskedLM"),Adr.forEach(r),Cat=t(SLe," (ALBERT model)"),SLe.forEach(r),Mat=i(ee),LC=s(ee,"LI",{});var PLe=n(LC);Mhe=s(PLe,"STRONG",{});var Ldr=n(Mhe);Eat=t(Ldr,"bert"),Ldr.forEach(r),yat=t(PLe," \u2014 "),hO=s(PLe,"A",{href:!0});var Bdr=n(hO);wat=t(Bdr,"TFBertForMaskedLM"),Bdr.forEach(r),Aat=t(PLe," (BERT model)"),PLe.forEach(r),Lat=i(ee),BC=s(ee,"LI",{});var $Le=n(BC);Ehe=s($Le,"STRONG",{});var xdr=n(Ehe);Bat=t(xdr,"camembert"),xdr.forEach(r),xat=t($Le," \u2014 "),uO=s($Le,"A",{href:!0});var kdr=n(uO);kat=t(kdr,"TFCamembertForMaskedLM"),kdr.forEach(r),Rat=t($Le," (CamemBERT model)"),$Le.forEach(r),Sat=i(ee),xC=s(ee,"LI",{});var ILe=n(xC);yhe=s(ILe,"STRONG",{});var Rdr=n(yhe);Pat=t(Rdr,"convbert"),Rdr.forEach(r),$at=t(ILe," \u2014 "),pO=s(ILe,"A",{href:!0});var Sdr=n(pO);Iat=t(Sdr,"TFConvBertForMaskedLM"),Sdr.forEach(r),jat=t(ILe," (ConvBERT model)"),ILe.forEach(r),Dat=i(ee),kC=s(ee,"LI",{});var jLe=n(kC);whe=s(jLe,"STRONG",{});var Pdr=n(whe);Nat=t(Pdr,"deberta"),Pdr.forEach(r),qat=t(jLe," \u2014 "),_O=s(jLe,"A",{href:!0});var $dr=n(_O);Oat=t($dr,"TFDebertaForMaskedLM"),$dr.forEach(r),Gat=t(jLe," (DeBERTa model)"),jLe.forEach(r),Xat=i(ee),RC=s(ee,"LI",{});var DLe=n(RC);Ahe=s(DLe,"STRONG",{});var Idr=n(Ahe);Vat=t(Idr,"deberta-v2"),Idr.forEach(r),zat=t(DLe," \u2014 "),bO=s(DLe,"A",{href:!0});var jdr=n(bO);Wat=t(jdr,"TFDebertaV2ForMaskedLM"),jdr.forEach(r),Qat=t(DLe," (DeBERTa-v2 model)"),DLe.forEach(r),Hat=i(ee),SC=s(ee,"LI",{});var NLe=n(SC);Lhe=s(NLe,"STRONG",{});var Ddr=n(Lhe);Uat=t(Ddr,"distilbert"),Ddr.forEach(r),Jat=t(NLe," \u2014 "),vO=s(NLe,"A",{href:!0});var Ndr=n(vO);Yat=t(Ndr,"TFDistilBertForMaskedLM"),Ndr.forEach(r),Kat=t(NLe," (DistilBERT model)"),NLe.forEach(r),Zat=i(ee),PC=s(ee,"LI",{});var qLe=n(PC);Bhe=s(qLe,"STRONG",{});var qdr=n(Bhe);est=t(qdr,"electra"),qdr.forEach(r),ost=t(qLe," \u2014 "),TO=s(qLe,"A",{href:!0});var Odr=n(TO);tst=t(Odr,"TFElectraForMaskedLM"),Odr.forEach(r),rst=t(qLe," (ELECTRA model)"),qLe.forEach(r),ast=i(ee),$C=s(ee,"LI",{});var OLe=n($C);xhe=s(OLe,"STRONG",{});var Gdr=n(xhe);sst=t(Gdr,"flaubert"),Gdr.forEach(r),nst=t(OLe," \u2014 "),FO=s(OLe,"A",{href:!0});var Xdr=n(FO);lst=t(Xdr,"TFFlaubertWithLMHeadModel"),Xdr.forEach(r),ist=t(OLe," (FlauBERT model)"),OLe.forEach(r),dst=i(ee),IC=s(ee,"LI",{});var GLe=n(IC);khe=s(GLe,"STRONG",{});var Vdr=n(khe);cst=t(Vdr,"funnel"),Vdr.forEach(r),mst=t(GLe," \u2014 "),CO=s(GLe,"A",{href:!0});var zdr=n(CO);fst=t(zdr,"TFFunnelForMaskedLM"),zdr.forEach(r),gst=t(GLe," (Funnel Transformer model)"),GLe.forEach(r),hst=i(ee),jC=s(ee,"LI",{});var XLe=n(jC);Rhe=s(XLe,"STRONG",{});var Wdr=n(Rhe);ust=t(Wdr,"layoutlm"),Wdr.forEach(r),pst=t(XLe," \u2014 "),MO=s(XLe,"A",{href:!0});var Qdr=n(MO);_st=t(Qdr,"TFLayoutLMForMaskedLM"),Qdr.forEach(r),bst=t(XLe," (LayoutLM model)"),XLe.forEach(r),vst=i(ee),DC=s(ee,"LI",{});var VLe=n(DC);She=s(VLe,"STRONG",{});var Hdr=n(She);Tst=t(Hdr,"longformer"),Hdr.forEach(r),Fst=t(VLe," \u2014 "),EO=s(VLe,"A",{href:!0});var Udr=n(EO);Cst=t(Udr,"TFLongformerForMaskedLM"),Udr.forEach(r),Mst=t(VLe," (Longformer model)"),VLe.forEach(r),Est=i(ee),NC=s(ee,"LI",{});var zLe=n(NC);Phe=s(zLe,"STRONG",{});var Jdr=n(Phe);yst=t(Jdr,"mobilebert"),Jdr.forEach(r),wst=t(zLe," \u2014 "),yO=s(zLe,"A",{href:!0});var Ydr=n(yO);Ast=t(Ydr,"TFMobileBertForMaskedLM"),Ydr.forEach(r),Lst=t(zLe," (MobileBERT model)"),zLe.forEach(r),Bst=i(ee),qC=s(ee,"LI",{});var WLe=n(qC);$he=s(WLe,"STRONG",{});var Kdr=n($he);xst=t(Kdr,"mpnet"),Kdr.forEach(r),kst=t(WLe," \u2014 "),wO=s(WLe,"A",{href:!0});var Zdr=n(wO);Rst=t(Zdr,"TFMPNetForMaskedLM"),Zdr.forEach(r),Sst=t(WLe," (MPNet model)"),WLe.forEach(r),Pst=i(ee),OC=s(ee,"LI",{});var QLe=n(OC);Ihe=s(QLe,"STRONG",{});var ecr=n(Ihe);$st=t(ecr,"rembert"),ecr.forEach(r),Ist=t(QLe," \u2014 "),AO=s(QLe,"A",{href:!0});var ocr=n(AO);jst=t(ocr,"TFRemBertForMaskedLM"),ocr.forEach(r),Dst=t(QLe," (RemBERT model)"),QLe.forEach(r),Nst=i(ee),GC=s(ee,"LI",{});var HLe=n(GC);jhe=s(HLe,"STRONG",{});var tcr=n(jhe);qst=t(tcr,"roberta"),tcr.forEach(r),Ost=t(HLe," \u2014 "),LO=s(HLe,"A",{href:!0});var rcr=n(LO);Gst=t(rcr,"TFRobertaForMaskedLM"),rcr.forEach(r),Xst=t(HLe," (RoBERTa model)"),HLe.forEach(r),Vst=i(ee),XC=s(ee,"LI",{});var ULe=n(XC);Dhe=s(ULe,"STRONG",{});var acr=n(Dhe);zst=t(acr,"roformer"),acr.forEach(r),Wst=t(ULe," \u2014 "),BO=s(ULe,"A",{href:!0});var scr=n(BO);Qst=t(scr,"TFRoFormerForMaskedLM"),scr.forEach(r),Hst=t(ULe," (RoFormer model)"),ULe.forEach(r),Ust=i(ee),VC=s(ee,"LI",{});var JLe=n(VC);Nhe=s(JLe,"STRONG",{});var ncr=n(Nhe);Jst=t(ncr,"tapas"),ncr.forEach(r),Yst=t(JLe," \u2014 "),xO=s(JLe,"A",{href:!0});var lcr=n(xO);Kst=t(lcr,"TFTapasForMaskedLM"),lcr.forEach(r),Zst=t(JLe," (TAPAS model)"),JLe.forEach(r),ent=i(ee),zC=s(ee,"LI",{});var YLe=n(zC);qhe=s(YLe,"STRONG",{});var icr=n(qhe);ont=t(icr,"xlm"),icr.forEach(r),tnt=t(YLe," \u2014 "),kO=s(YLe,"A",{href:!0});var dcr=n(kO);rnt=t(dcr,"TFXLMWithLMHeadModel"),dcr.forEach(r),ant=t(YLe," (XLM model)"),YLe.forEach(r),snt=i(ee),WC=s(ee,"LI",{});var KLe=n(WC);Ohe=s(KLe,"STRONG",{});var ccr=n(Ohe);nnt=t(ccr,"xlm-roberta"),ccr.forEach(r),lnt=t(KLe," \u2014 "),RO=s(KLe,"A",{href:!0});var mcr=n(RO);int=t(mcr,"TFXLMRobertaForMaskedLM"),mcr.forEach(r),dnt=t(KLe," (XLM-RoBERTa model)"),KLe.forEach(r),ee.forEach(r),cnt=i(ba),Ghe=s(ba,"P",{});var fcr=n(Ghe);mnt=t(fcr,"Examples:"),fcr.forEach(r),fnt=i(ba),f(e0.$$.fragment,ba),ba.forEach(r),Ql.forEach(r),Nxe=i(d),Ac=s(d,"H2",{class:!0});var YRe=n(Ac);QC=s(YRe,"A",{id:!0,class:!0,href:!0});var gcr=n(QC);Xhe=s(gcr,"SPAN",{});var hcr=n(Xhe);f(o0.$$.fragment,hcr),hcr.forEach(r),gcr.forEach(r),gnt=i(YRe),Vhe=s(YRe,"SPAN",{});var ucr=n(Vhe);hnt=t(ucr,"TFAutoModelForSeq2SeqLM"),ucr.forEach(r),YRe.forEach(r),qxe=i(d),Ct=s(d,"DIV",{class:!0});var Ul=n(Ct);f(t0.$$.fragment,Ul),unt=i(Ul),Lc=s(Ul,"P",{});var cW=n(Lc);pnt=t(cW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),zhe=s(cW,"CODE",{});var pcr=n(zhe);_nt=t(pcr,"from_pretrained()"),pcr.forEach(r),bnt=t(cW,"class method or the "),Whe=s(cW,"CODE",{});var _cr=n(Whe);vnt=t(_cr,"from_config()"),_cr.forEach(r),Tnt=t(cW,`class
method.`),cW.forEach(r),Fnt=i(Ul),r0=s(Ul,"P",{});var KRe=n(r0);Cnt=t(KRe,"This class cannot be instantiated directly using "),Qhe=s(KRe,"CODE",{});var bcr=n(Qhe);Mnt=t(bcr,"__init__()"),bcr.forEach(r),Ent=t(KRe," (throws an error)."),KRe.forEach(r),ynt=i(Ul),pr=s(Ul,"DIV",{class:!0});var Jl=n(pr);f(a0.$$.fragment,Jl),wnt=i(Jl),Hhe=s(Jl,"P",{});var vcr=n(Hhe);Ant=t(vcr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),vcr.forEach(r),Lnt=i(Jl),Bc=s(Jl,"P",{});var mW=n(Bc);Bnt=t(mW,`Note:
Loading a model from its configuration file does `),Uhe=s(mW,"STRONG",{});var Tcr=n(Uhe);xnt=t(Tcr,"not"),Tcr.forEach(r),knt=t(mW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jhe=s(mW,"CODE",{});var Fcr=n(Jhe);Rnt=t(Fcr,"from_pretrained()"),Fcr.forEach(r),Snt=t(mW,"to load the model weights."),mW.forEach(r),Pnt=i(Jl),Yhe=s(Jl,"P",{});var Ccr=n(Yhe);$nt=t(Ccr,"Examples:"),Ccr.forEach(r),Int=i(Jl),f(s0.$$.fragment,Jl),Jl.forEach(r),jnt=i(Ul),vo=s(Ul,"DIV",{class:!0});var va=n(vo);f(n0.$$.fragment,va),Dnt=i(va),Khe=s(va,"P",{});var Mcr=n(Khe);Nnt=t(Mcr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Mcr.forEach(r),qnt=i(va),vs=s(va,"P",{});var _5=n(vs);Ont=t(_5,"The model class to instantiate is selected based on the "),Zhe=s(_5,"CODE",{});var Ecr=n(Zhe);Gnt=t(Ecr,"model_type"),Ecr.forEach(r),Xnt=t(_5,` property of the config object (either
passed as an argument or loaded from `),eue=s(_5,"CODE",{});var ycr=n(eue);Vnt=t(ycr,"pretrained_model_name_or_path"),ycr.forEach(r),znt=t(_5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),oue=s(_5,"CODE",{});var wcr=n(oue);Wnt=t(wcr,"pretrained_model_name_or_path"),wcr.forEach(r),Qnt=t(_5,":"),_5.forEach(r),Hnt=i(va),ue=s(va,"UL",{});var Ee=n(ue);HC=s(Ee,"LI",{});var ZLe=n(HC);tue=s(ZLe,"STRONG",{});var Acr=n(tue);Unt=t(Acr,"bart"),Acr.forEach(r),Jnt=t(ZLe," \u2014 "),SO=s(ZLe,"A",{href:!0});var Lcr=n(SO);Ynt=t(Lcr,"TFBartForConditionalGeneration"),Lcr.forEach(r),Knt=t(ZLe," (BART model)"),ZLe.forEach(r),Znt=i(Ee),UC=s(Ee,"LI",{});var e8e=n(UC);rue=s(e8e,"STRONG",{});var Bcr=n(rue);elt=t(Bcr,"blenderbot"),Bcr.forEach(r),olt=t(e8e," \u2014 "),PO=s(e8e,"A",{href:!0});var xcr=n(PO);tlt=t(xcr,"TFBlenderbotForConditionalGeneration"),xcr.forEach(r),rlt=t(e8e," (Blenderbot model)"),e8e.forEach(r),alt=i(Ee),JC=s(Ee,"LI",{});var o8e=n(JC);aue=s(o8e,"STRONG",{});var kcr=n(aue);slt=t(kcr,"blenderbot-small"),kcr.forEach(r),nlt=t(o8e," \u2014 "),$O=s(o8e,"A",{href:!0});var Rcr=n($O);llt=t(Rcr,"TFBlenderbotSmallForConditionalGeneration"),Rcr.forEach(r),ilt=t(o8e," (BlenderbotSmall model)"),o8e.forEach(r),dlt=i(Ee),YC=s(Ee,"LI",{});var t8e=n(YC);sue=s(t8e,"STRONG",{});var Scr=n(sue);clt=t(Scr,"encoder-decoder"),Scr.forEach(r),mlt=t(t8e," \u2014 "),IO=s(t8e,"A",{href:!0});var Pcr=n(IO);flt=t(Pcr,"TFEncoderDecoderModel"),Pcr.forEach(r),glt=t(t8e," (Encoder decoder model)"),t8e.forEach(r),hlt=i(Ee),KC=s(Ee,"LI",{});var r8e=n(KC);nue=s(r8e,"STRONG",{});var $cr=n(nue);ult=t($cr,"led"),$cr.forEach(r),plt=t(r8e," \u2014 "),jO=s(r8e,"A",{href:!0});var Icr=n(jO);_lt=t(Icr,"TFLEDForConditionalGeneration"),Icr.forEach(r),blt=t(r8e," (LED model)"),r8e.forEach(r),vlt=i(Ee),ZC=s(Ee,"LI",{});var a8e=n(ZC);lue=s(a8e,"STRONG",{});var jcr=n(lue);Tlt=t(jcr,"marian"),jcr.forEach(r),Flt=t(a8e," \u2014 "),DO=s(a8e,"A",{href:!0});var Dcr=n(DO);Clt=t(Dcr,"TFMarianMTModel"),Dcr.forEach(r),Mlt=t(a8e," (Marian model)"),a8e.forEach(r),Elt=i(Ee),e4=s(Ee,"LI",{});var s8e=n(e4);iue=s(s8e,"STRONG",{});var Ncr=n(iue);ylt=t(Ncr,"mbart"),Ncr.forEach(r),wlt=t(s8e," \u2014 "),NO=s(s8e,"A",{href:!0});var qcr=n(NO);Alt=t(qcr,"TFMBartForConditionalGeneration"),qcr.forEach(r),Llt=t(s8e," (mBART model)"),s8e.forEach(r),Blt=i(Ee),o4=s(Ee,"LI",{});var n8e=n(o4);due=s(n8e,"STRONG",{});var Ocr=n(due);xlt=t(Ocr,"mt5"),Ocr.forEach(r),klt=t(n8e," \u2014 "),qO=s(n8e,"A",{href:!0});var Gcr=n(qO);Rlt=t(Gcr,"TFMT5ForConditionalGeneration"),Gcr.forEach(r),Slt=t(n8e," (mT5 model)"),n8e.forEach(r),Plt=i(Ee),t4=s(Ee,"LI",{});var l8e=n(t4);cue=s(l8e,"STRONG",{});var Xcr=n(cue);$lt=t(Xcr,"pegasus"),Xcr.forEach(r),Ilt=t(l8e," \u2014 "),OO=s(l8e,"A",{href:!0});var Vcr=n(OO);jlt=t(Vcr,"TFPegasusForConditionalGeneration"),Vcr.forEach(r),Dlt=t(l8e," (Pegasus model)"),l8e.forEach(r),Nlt=i(Ee),r4=s(Ee,"LI",{});var i8e=n(r4);mue=s(i8e,"STRONG",{});var zcr=n(mue);qlt=t(zcr,"t5"),zcr.forEach(r),Olt=t(i8e," \u2014 "),GO=s(i8e,"A",{href:!0});var Wcr=n(GO);Glt=t(Wcr,"TFT5ForConditionalGeneration"),Wcr.forEach(r),Xlt=t(i8e," (T5 model)"),i8e.forEach(r),Ee.forEach(r),Vlt=i(va),fue=s(va,"P",{});var Qcr=n(fue);zlt=t(Qcr,"Examples:"),Qcr.forEach(r),Wlt=i(va),f(l0.$$.fragment,va),va.forEach(r),Ul.forEach(r),Oxe=i(d),xc=s(d,"H2",{class:!0});var ZRe=n(xc);a4=s(ZRe,"A",{id:!0,class:!0,href:!0});var Hcr=n(a4);gue=s(Hcr,"SPAN",{});var Ucr=n(gue);f(i0.$$.fragment,Ucr),Ucr.forEach(r),Hcr.forEach(r),Qlt=i(ZRe),hue=s(ZRe,"SPAN",{});var Jcr=n(hue);Hlt=t(Jcr,"TFAutoModelForSequenceClassification"),Jcr.forEach(r),ZRe.forEach(r),Gxe=i(d),Mt=s(d,"DIV",{class:!0});var Yl=n(Mt);f(d0.$$.fragment,Yl),Ult=i(Yl),kc=s(Yl,"P",{});var fW=n(kc);Jlt=t(fW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),uue=s(fW,"CODE",{});var Ycr=n(uue);Ylt=t(Ycr,"from_pretrained()"),Ycr.forEach(r),Klt=t(fW,"class method or the "),pue=s(fW,"CODE",{});var Kcr=n(pue);Zlt=t(Kcr,"from_config()"),Kcr.forEach(r),eit=t(fW,`class
method.`),fW.forEach(r),oit=i(Yl),c0=s(Yl,"P",{});var eSe=n(c0);tit=t(eSe,"This class cannot be instantiated directly using "),_ue=s(eSe,"CODE",{});var Zcr=n(_ue);rit=t(Zcr,"__init__()"),Zcr.forEach(r),ait=t(eSe," (throws an error)."),eSe.forEach(r),sit=i(Yl),_r=s(Yl,"DIV",{class:!0});var Kl=n(_r);f(m0.$$.fragment,Kl),nit=i(Kl),bue=s(Kl,"P",{});var emr=n(bue);lit=t(emr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),emr.forEach(r),iit=i(Kl),Rc=s(Kl,"P",{});var gW=n(Rc);dit=t(gW,`Note:
Loading a model from its configuration file does `),vue=s(gW,"STRONG",{});var omr=n(vue);cit=t(omr,"not"),omr.forEach(r),mit=t(gW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Tue=s(gW,"CODE",{});var tmr=n(Tue);fit=t(tmr,"from_pretrained()"),tmr.forEach(r),git=t(gW,"to load the model weights."),gW.forEach(r),hit=i(Kl),Fue=s(Kl,"P",{});var rmr=n(Fue);uit=t(rmr,"Examples:"),rmr.forEach(r),pit=i(Kl),f(f0.$$.fragment,Kl),Kl.forEach(r),_it=i(Yl),To=s(Yl,"DIV",{class:!0});var Ta=n(To);f(g0.$$.fragment,Ta),bit=i(Ta),Cue=s(Ta,"P",{});var amr=n(Cue);vit=t(amr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),amr.forEach(r),Tit=i(Ta),Ts=s(Ta,"P",{});var b5=n(Ts);Fit=t(b5,"The model class to instantiate is selected based on the "),Mue=s(b5,"CODE",{});var smr=n(Mue);Cit=t(smr,"model_type"),smr.forEach(r),Mit=t(b5,` property of the config object (either
passed as an argument or loaded from `),Eue=s(b5,"CODE",{});var nmr=n(Eue);Eit=t(nmr,"pretrained_model_name_or_path"),nmr.forEach(r),yit=t(b5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yue=s(b5,"CODE",{});var lmr=n(yue);wit=t(lmr,"pretrained_model_name_or_path"),lmr.forEach(r),Ait=t(b5,":"),b5.forEach(r),Lit=i(Ta),V=s(Ta,"UL",{});var W=n(V);s4=s(W,"LI",{});var d8e=n(s4);wue=s(d8e,"STRONG",{});var imr=n(wue);Bit=t(imr,"albert"),imr.forEach(r),xit=t(d8e," \u2014 "),XO=s(d8e,"A",{href:!0});var dmr=n(XO);kit=t(dmr,"TFAlbertForSequenceClassification"),dmr.forEach(r),Rit=t(d8e," (ALBERT model)"),d8e.forEach(r),Sit=i(W),n4=s(W,"LI",{});var c8e=n(n4);Aue=s(c8e,"STRONG",{});var cmr=n(Aue);Pit=t(cmr,"bert"),cmr.forEach(r),$it=t(c8e," \u2014 "),VO=s(c8e,"A",{href:!0});var mmr=n(VO);Iit=t(mmr,"TFBertForSequenceClassification"),mmr.forEach(r),jit=t(c8e," (BERT model)"),c8e.forEach(r),Dit=i(W),l4=s(W,"LI",{});var m8e=n(l4);Lue=s(m8e,"STRONG",{});var fmr=n(Lue);Nit=t(fmr,"camembert"),fmr.forEach(r),qit=t(m8e," \u2014 "),zO=s(m8e,"A",{href:!0});var gmr=n(zO);Oit=t(gmr,"TFCamembertForSequenceClassification"),gmr.forEach(r),Git=t(m8e," (CamemBERT model)"),m8e.forEach(r),Xit=i(W),i4=s(W,"LI",{});var f8e=n(i4);Bue=s(f8e,"STRONG",{});var hmr=n(Bue);Vit=t(hmr,"convbert"),hmr.forEach(r),zit=t(f8e," \u2014 "),WO=s(f8e,"A",{href:!0});var umr=n(WO);Wit=t(umr,"TFConvBertForSequenceClassification"),umr.forEach(r),Qit=t(f8e," (ConvBERT model)"),f8e.forEach(r),Hit=i(W),d4=s(W,"LI",{});var g8e=n(d4);xue=s(g8e,"STRONG",{});var pmr=n(xue);Uit=t(pmr,"ctrl"),pmr.forEach(r),Jit=t(g8e," \u2014 "),QO=s(g8e,"A",{href:!0});var _mr=n(QO);Yit=t(_mr,"TFCTRLForSequenceClassification"),_mr.forEach(r),Kit=t(g8e," (CTRL model)"),g8e.forEach(r),Zit=i(W),c4=s(W,"LI",{});var h8e=n(c4);kue=s(h8e,"STRONG",{});var bmr=n(kue);edt=t(bmr,"deberta"),bmr.forEach(r),odt=t(h8e," \u2014 "),HO=s(h8e,"A",{href:!0});var vmr=n(HO);tdt=t(vmr,"TFDebertaForSequenceClassification"),vmr.forEach(r),rdt=t(h8e," (DeBERTa model)"),h8e.forEach(r),adt=i(W),m4=s(W,"LI",{});var u8e=n(m4);Rue=s(u8e,"STRONG",{});var Tmr=n(Rue);sdt=t(Tmr,"deberta-v2"),Tmr.forEach(r),ndt=t(u8e," \u2014 "),UO=s(u8e,"A",{href:!0});var Fmr=n(UO);ldt=t(Fmr,"TFDebertaV2ForSequenceClassification"),Fmr.forEach(r),idt=t(u8e," (DeBERTa-v2 model)"),u8e.forEach(r),ddt=i(W),f4=s(W,"LI",{});var p8e=n(f4);Sue=s(p8e,"STRONG",{});var Cmr=n(Sue);cdt=t(Cmr,"distilbert"),Cmr.forEach(r),mdt=t(p8e," \u2014 "),JO=s(p8e,"A",{href:!0});var Mmr=n(JO);fdt=t(Mmr,"TFDistilBertForSequenceClassification"),Mmr.forEach(r),gdt=t(p8e," (DistilBERT model)"),p8e.forEach(r),hdt=i(W),g4=s(W,"LI",{});var _8e=n(g4);Pue=s(_8e,"STRONG",{});var Emr=n(Pue);udt=t(Emr,"electra"),Emr.forEach(r),pdt=t(_8e," \u2014 "),YO=s(_8e,"A",{href:!0});var ymr=n(YO);_dt=t(ymr,"TFElectraForSequenceClassification"),ymr.forEach(r),bdt=t(_8e," (ELECTRA model)"),_8e.forEach(r),vdt=i(W),h4=s(W,"LI",{});var b8e=n(h4);$ue=s(b8e,"STRONG",{});var wmr=n($ue);Tdt=t(wmr,"flaubert"),wmr.forEach(r),Fdt=t(b8e," \u2014 "),KO=s(b8e,"A",{href:!0});var Amr=n(KO);Cdt=t(Amr,"TFFlaubertForSequenceClassification"),Amr.forEach(r),Mdt=t(b8e," (FlauBERT model)"),b8e.forEach(r),Edt=i(W),u4=s(W,"LI",{});var v8e=n(u4);Iue=s(v8e,"STRONG",{});var Lmr=n(Iue);ydt=t(Lmr,"funnel"),Lmr.forEach(r),wdt=t(v8e," \u2014 "),ZO=s(v8e,"A",{href:!0});var Bmr=n(ZO);Adt=t(Bmr,"TFFunnelForSequenceClassification"),Bmr.forEach(r),Ldt=t(v8e," (Funnel Transformer model)"),v8e.forEach(r),Bdt=i(W),p4=s(W,"LI",{});var T8e=n(p4);jue=s(T8e,"STRONG",{});var xmr=n(jue);xdt=t(xmr,"gpt2"),xmr.forEach(r),kdt=t(T8e," \u2014 "),eG=s(T8e,"A",{href:!0});var kmr=n(eG);Rdt=t(kmr,"TFGPT2ForSequenceClassification"),kmr.forEach(r),Sdt=t(T8e," (OpenAI GPT-2 model)"),T8e.forEach(r),Pdt=i(W),_4=s(W,"LI",{});var F8e=n(_4);Due=s(F8e,"STRONG",{});var Rmr=n(Due);$dt=t(Rmr,"layoutlm"),Rmr.forEach(r),Idt=t(F8e," \u2014 "),oG=s(F8e,"A",{href:!0});var Smr=n(oG);jdt=t(Smr,"TFLayoutLMForSequenceClassification"),Smr.forEach(r),Ddt=t(F8e," (LayoutLM model)"),F8e.forEach(r),Ndt=i(W),b4=s(W,"LI",{});var C8e=n(b4);Nue=s(C8e,"STRONG",{});var Pmr=n(Nue);qdt=t(Pmr,"longformer"),Pmr.forEach(r),Odt=t(C8e," \u2014 "),tG=s(C8e,"A",{href:!0});var $mr=n(tG);Gdt=t($mr,"TFLongformerForSequenceClassification"),$mr.forEach(r),Xdt=t(C8e," (Longformer model)"),C8e.forEach(r),Vdt=i(W),v4=s(W,"LI",{});var M8e=n(v4);que=s(M8e,"STRONG",{});var Imr=n(que);zdt=t(Imr,"mobilebert"),Imr.forEach(r),Wdt=t(M8e," \u2014 "),rG=s(M8e,"A",{href:!0});var jmr=n(rG);Qdt=t(jmr,"TFMobileBertForSequenceClassification"),jmr.forEach(r),Hdt=t(M8e," (MobileBERT model)"),M8e.forEach(r),Udt=i(W),T4=s(W,"LI",{});var E8e=n(T4);Oue=s(E8e,"STRONG",{});var Dmr=n(Oue);Jdt=t(Dmr,"mpnet"),Dmr.forEach(r),Ydt=t(E8e," \u2014 "),aG=s(E8e,"A",{href:!0});var Nmr=n(aG);Kdt=t(Nmr,"TFMPNetForSequenceClassification"),Nmr.forEach(r),Zdt=t(E8e," (MPNet model)"),E8e.forEach(r),ect=i(W),F4=s(W,"LI",{});var y8e=n(F4);Gue=s(y8e,"STRONG",{});var qmr=n(Gue);oct=t(qmr,"openai-gpt"),qmr.forEach(r),tct=t(y8e," \u2014 "),sG=s(y8e,"A",{href:!0});var Omr=n(sG);rct=t(Omr,"TFOpenAIGPTForSequenceClassification"),Omr.forEach(r),act=t(y8e," (OpenAI GPT model)"),y8e.forEach(r),sct=i(W),C4=s(W,"LI",{});var w8e=n(C4);Xue=s(w8e,"STRONG",{});var Gmr=n(Xue);nct=t(Gmr,"rembert"),Gmr.forEach(r),lct=t(w8e," \u2014 "),nG=s(w8e,"A",{href:!0});var Xmr=n(nG);ict=t(Xmr,"TFRemBertForSequenceClassification"),Xmr.forEach(r),dct=t(w8e," (RemBERT model)"),w8e.forEach(r),cct=i(W),M4=s(W,"LI",{});var A8e=n(M4);Vue=s(A8e,"STRONG",{});var Vmr=n(Vue);mct=t(Vmr,"roberta"),Vmr.forEach(r),fct=t(A8e," \u2014 "),lG=s(A8e,"A",{href:!0});var zmr=n(lG);gct=t(zmr,"TFRobertaForSequenceClassification"),zmr.forEach(r),hct=t(A8e," (RoBERTa model)"),A8e.forEach(r),uct=i(W),E4=s(W,"LI",{});var L8e=n(E4);zue=s(L8e,"STRONG",{});var Wmr=n(zue);pct=t(Wmr,"roformer"),Wmr.forEach(r),_ct=t(L8e," \u2014 "),iG=s(L8e,"A",{href:!0});var Qmr=n(iG);bct=t(Qmr,"TFRoFormerForSequenceClassification"),Qmr.forEach(r),vct=t(L8e," (RoFormer model)"),L8e.forEach(r),Tct=i(W),y4=s(W,"LI",{});var B8e=n(y4);Wue=s(B8e,"STRONG",{});var Hmr=n(Wue);Fct=t(Hmr,"tapas"),Hmr.forEach(r),Cct=t(B8e," \u2014 "),dG=s(B8e,"A",{href:!0});var Umr=n(dG);Mct=t(Umr,"TFTapasForSequenceClassification"),Umr.forEach(r),Ect=t(B8e," (TAPAS model)"),B8e.forEach(r),yct=i(W),w4=s(W,"LI",{});var x8e=n(w4);Que=s(x8e,"STRONG",{});var Jmr=n(Que);wct=t(Jmr,"transfo-xl"),Jmr.forEach(r),Act=t(x8e," \u2014 "),cG=s(x8e,"A",{href:!0});var Ymr=n(cG);Lct=t(Ymr,"TFTransfoXLForSequenceClassification"),Ymr.forEach(r),Bct=t(x8e," (Transformer-XL model)"),x8e.forEach(r),xct=i(W),A4=s(W,"LI",{});var k8e=n(A4);Hue=s(k8e,"STRONG",{});var Kmr=n(Hue);kct=t(Kmr,"xlm"),Kmr.forEach(r),Rct=t(k8e," \u2014 "),mG=s(k8e,"A",{href:!0});var Zmr=n(mG);Sct=t(Zmr,"TFXLMForSequenceClassification"),Zmr.forEach(r),Pct=t(k8e," (XLM model)"),k8e.forEach(r),$ct=i(W),L4=s(W,"LI",{});var R8e=n(L4);Uue=s(R8e,"STRONG",{});var efr=n(Uue);Ict=t(efr,"xlm-roberta"),efr.forEach(r),jct=t(R8e," \u2014 "),fG=s(R8e,"A",{href:!0});var ofr=n(fG);Dct=t(ofr,"TFXLMRobertaForSequenceClassification"),ofr.forEach(r),Nct=t(R8e," (XLM-RoBERTa model)"),R8e.forEach(r),qct=i(W),B4=s(W,"LI",{});var S8e=n(B4);Jue=s(S8e,"STRONG",{});var tfr=n(Jue);Oct=t(tfr,"xlnet"),tfr.forEach(r),Gct=t(S8e," \u2014 "),gG=s(S8e,"A",{href:!0});var rfr=n(gG);Xct=t(rfr,"TFXLNetForSequenceClassification"),rfr.forEach(r),Vct=t(S8e," (XLNet model)"),S8e.forEach(r),W.forEach(r),zct=i(Ta),Yue=s(Ta,"P",{});var afr=n(Yue);Wct=t(afr,"Examples:"),afr.forEach(r),Qct=i(Ta),f(h0.$$.fragment,Ta),Ta.forEach(r),Yl.forEach(r),Xxe=i(d),Sc=s(d,"H2",{class:!0});var oSe=n(Sc);x4=s(oSe,"A",{id:!0,class:!0,href:!0});var sfr=n(x4);Kue=s(sfr,"SPAN",{});var nfr=n(Kue);f(u0.$$.fragment,nfr),nfr.forEach(r),sfr.forEach(r),Hct=i(oSe),Zue=s(oSe,"SPAN",{});var lfr=n(Zue);Uct=t(lfr,"TFAutoModelForMultipleChoice"),lfr.forEach(r),oSe.forEach(r),Vxe=i(d),Et=s(d,"DIV",{class:!0});var Zl=n(Et);f(p0.$$.fragment,Zl),Jct=i(Zl),Pc=s(Zl,"P",{});var hW=n(Pc);Yct=t(hW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),epe=s(hW,"CODE",{});var ifr=n(epe);Kct=t(ifr,"from_pretrained()"),ifr.forEach(r),Zct=t(hW,"class method or the "),ope=s(hW,"CODE",{});var dfr=n(ope);emt=t(dfr,"from_config()"),dfr.forEach(r),omt=t(hW,`class
method.`),hW.forEach(r),tmt=i(Zl),_0=s(Zl,"P",{});var tSe=n(_0);rmt=t(tSe,"This class cannot be instantiated directly using "),tpe=s(tSe,"CODE",{});var cfr=n(tpe);amt=t(cfr,"__init__()"),cfr.forEach(r),smt=t(tSe," (throws an error)."),tSe.forEach(r),nmt=i(Zl),br=s(Zl,"DIV",{class:!0});var ei=n(br);f(b0.$$.fragment,ei),lmt=i(ei),rpe=s(ei,"P",{});var mfr=n(rpe);imt=t(mfr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),mfr.forEach(r),dmt=i(ei),$c=s(ei,"P",{});var uW=n($c);cmt=t(uW,`Note:
Loading a model from its configuration file does `),ape=s(uW,"STRONG",{});var ffr=n(ape);mmt=t(ffr,"not"),ffr.forEach(r),fmt=t(uW,` load the model weights. It only affects the
model\u2019s configuration. Use `),spe=s(uW,"CODE",{});var gfr=n(spe);gmt=t(gfr,"from_pretrained()"),gfr.forEach(r),hmt=t(uW,"to load the model weights."),uW.forEach(r),umt=i(ei),npe=s(ei,"P",{});var hfr=n(npe);pmt=t(hfr,"Examples:"),hfr.forEach(r),_mt=i(ei),f(v0.$$.fragment,ei),ei.forEach(r),bmt=i(Zl),Fo=s(Zl,"DIV",{class:!0});var Fa=n(Fo);f(T0.$$.fragment,Fa),vmt=i(Fa),lpe=s(Fa,"P",{});var ufr=n(lpe);Tmt=t(ufr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),ufr.forEach(r),Fmt=i(Fa),Fs=s(Fa,"P",{});var v5=n(Fs);Cmt=t(v5,"The model class to instantiate is selected based on the "),ipe=s(v5,"CODE",{});var pfr=n(ipe);Mmt=t(pfr,"model_type"),pfr.forEach(r),Emt=t(v5,` property of the config object (either
passed as an argument or loaded from `),dpe=s(v5,"CODE",{});var _fr=n(dpe);ymt=t(_fr,"pretrained_model_name_or_path"),_fr.forEach(r),wmt=t(v5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cpe=s(v5,"CODE",{});var bfr=n(cpe);Amt=t(bfr,"pretrained_model_name_or_path"),bfr.forEach(r),Lmt=t(v5,":"),v5.forEach(r),Bmt=i(Fa),re=s(Fa,"UL",{});var ne=n(re);k4=s(ne,"LI",{});var P8e=n(k4);mpe=s(P8e,"STRONG",{});var vfr=n(mpe);xmt=t(vfr,"albert"),vfr.forEach(r),kmt=t(P8e," \u2014 "),hG=s(P8e,"A",{href:!0});var Tfr=n(hG);Rmt=t(Tfr,"TFAlbertForMultipleChoice"),Tfr.forEach(r),Smt=t(P8e," (ALBERT model)"),P8e.forEach(r),Pmt=i(ne),R4=s(ne,"LI",{});var $8e=n(R4);fpe=s($8e,"STRONG",{});var Ffr=n(fpe);$mt=t(Ffr,"bert"),Ffr.forEach(r),Imt=t($8e," \u2014 "),uG=s($8e,"A",{href:!0});var Cfr=n(uG);jmt=t(Cfr,"TFBertForMultipleChoice"),Cfr.forEach(r),Dmt=t($8e," (BERT model)"),$8e.forEach(r),Nmt=i(ne),S4=s(ne,"LI",{});var I8e=n(S4);gpe=s(I8e,"STRONG",{});var Mfr=n(gpe);qmt=t(Mfr,"camembert"),Mfr.forEach(r),Omt=t(I8e," \u2014 "),pG=s(I8e,"A",{href:!0});var Efr=n(pG);Gmt=t(Efr,"TFCamembertForMultipleChoice"),Efr.forEach(r),Xmt=t(I8e," (CamemBERT model)"),I8e.forEach(r),Vmt=i(ne),P4=s(ne,"LI",{});var j8e=n(P4);hpe=s(j8e,"STRONG",{});var yfr=n(hpe);zmt=t(yfr,"convbert"),yfr.forEach(r),Wmt=t(j8e," \u2014 "),_G=s(j8e,"A",{href:!0});var wfr=n(_G);Qmt=t(wfr,"TFConvBertForMultipleChoice"),wfr.forEach(r),Hmt=t(j8e," (ConvBERT model)"),j8e.forEach(r),Umt=i(ne),$4=s(ne,"LI",{});var D8e=n($4);upe=s(D8e,"STRONG",{});var Afr=n(upe);Jmt=t(Afr,"distilbert"),Afr.forEach(r),Ymt=t(D8e," \u2014 "),bG=s(D8e,"A",{href:!0});var Lfr=n(bG);Kmt=t(Lfr,"TFDistilBertForMultipleChoice"),Lfr.forEach(r),Zmt=t(D8e," (DistilBERT model)"),D8e.forEach(r),eft=i(ne),I4=s(ne,"LI",{});var N8e=n(I4);ppe=s(N8e,"STRONG",{});var Bfr=n(ppe);oft=t(Bfr,"electra"),Bfr.forEach(r),tft=t(N8e," \u2014 "),vG=s(N8e,"A",{href:!0});var xfr=n(vG);rft=t(xfr,"TFElectraForMultipleChoice"),xfr.forEach(r),aft=t(N8e," (ELECTRA model)"),N8e.forEach(r),sft=i(ne),j4=s(ne,"LI",{});var q8e=n(j4);_pe=s(q8e,"STRONG",{});var kfr=n(_pe);nft=t(kfr,"flaubert"),kfr.forEach(r),lft=t(q8e," \u2014 "),TG=s(q8e,"A",{href:!0});var Rfr=n(TG);ift=t(Rfr,"TFFlaubertForMultipleChoice"),Rfr.forEach(r),dft=t(q8e," (FlauBERT model)"),q8e.forEach(r),cft=i(ne),D4=s(ne,"LI",{});var O8e=n(D4);bpe=s(O8e,"STRONG",{});var Sfr=n(bpe);mft=t(Sfr,"funnel"),Sfr.forEach(r),fft=t(O8e," \u2014 "),FG=s(O8e,"A",{href:!0});var Pfr=n(FG);gft=t(Pfr,"TFFunnelForMultipleChoice"),Pfr.forEach(r),hft=t(O8e," (Funnel Transformer model)"),O8e.forEach(r),uft=i(ne),N4=s(ne,"LI",{});var G8e=n(N4);vpe=s(G8e,"STRONG",{});var $fr=n(vpe);pft=t($fr,"longformer"),$fr.forEach(r),_ft=t(G8e," \u2014 "),CG=s(G8e,"A",{href:!0});var Ifr=n(CG);bft=t(Ifr,"TFLongformerForMultipleChoice"),Ifr.forEach(r),vft=t(G8e," (Longformer model)"),G8e.forEach(r),Tft=i(ne),q4=s(ne,"LI",{});var X8e=n(q4);Tpe=s(X8e,"STRONG",{});var jfr=n(Tpe);Fft=t(jfr,"mobilebert"),jfr.forEach(r),Cft=t(X8e," \u2014 "),MG=s(X8e,"A",{href:!0});var Dfr=n(MG);Mft=t(Dfr,"TFMobileBertForMultipleChoice"),Dfr.forEach(r),Eft=t(X8e," (MobileBERT model)"),X8e.forEach(r),yft=i(ne),O4=s(ne,"LI",{});var V8e=n(O4);Fpe=s(V8e,"STRONG",{});var Nfr=n(Fpe);wft=t(Nfr,"mpnet"),Nfr.forEach(r),Aft=t(V8e," \u2014 "),EG=s(V8e,"A",{href:!0});var qfr=n(EG);Lft=t(qfr,"TFMPNetForMultipleChoice"),qfr.forEach(r),Bft=t(V8e," (MPNet model)"),V8e.forEach(r),xft=i(ne),G4=s(ne,"LI",{});var z8e=n(G4);Cpe=s(z8e,"STRONG",{});var Ofr=n(Cpe);kft=t(Ofr,"rembert"),Ofr.forEach(r),Rft=t(z8e," \u2014 "),yG=s(z8e,"A",{href:!0});var Gfr=n(yG);Sft=t(Gfr,"TFRemBertForMultipleChoice"),Gfr.forEach(r),Pft=t(z8e," (RemBERT model)"),z8e.forEach(r),$ft=i(ne),X4=s(ne,"LI",{});var W8e=n(X4);Mpe=s(W8e,"STRONG",{});var Xfr=n(Mpe);Ift=t(Xfr,"roberta"),Xfr.forEach(r),jft=t(W8e," \u2014 "),wG=s(W8e,"A",{href:!0});var Vfr=n(wG);Dft=t(Vfr,"TFRobertaForMultipleChoice"),Vfr.forEach(r),Nft=t(W8e," (RoBERTa model)"),W8e.forEach(r),qft=i(ne),V4=s(ne,"LI",{});var Q8e=n(V4);Epe=s(Q8e,"STRONG",{});var zfr=n(Epe);Oft=t(zfr,"roformer"),zfr.forEach(r),Gft=t(Q8e," \u2014 "),AG=s(Q8e,"A",{href:!0});var Wfr=n(AG);Xft=t(Wfr,"TFRoFormerForMultipleChoice"),Wfr.forEach(r),Vft=t(Q8e," (RoFormer model)"),Q8e.forEach(r),zft=i(ne),z4=s(ne,"LI",{});var H8e=n(z4);ype=s(H8e,"STRONG",{});var Qfr=n(ype);Wft=t(Qfr,"xlm"),Qfr.forEach(r),Qft=t(H8e," \u2014 "),LG=s(H8e,"A",{href:!0});var Hfr=n(LG);Hft=t(Hfr,"TFXLMForMultipleChoice"),Hfr.forEach(r),Uft=t(H8e," (XLM model)"),H8e.forEach(r),Jft=i(ne),W4=s(ne,"LI",{});var U8e=n(W4);wpe=s(U8e,"STRONG",{});var Ufr=n(wpe);Yft=t(Ufr,"xlm-roberta"),Ufr.forEach(r),Kft=t(U8e," \u2014 "),BG=s(U8e,"A",{href:!0});var Jfr=n(BG);Zft=t(Jfr,"TFXLMRobertaForMultipleChoice"),Jfr.forEach(r),egt=t(U8e," (XLM-RoBERTa model)"),U8e.forEach(r),ogt=i(ne),Q4=s(ne,"LI",{});var J8e=n(Q4);Ape=s(J8e,"STRONG",{});var Yfr=n(Ape);tgt=t(Yfr,"xlnet"),Yfr.forEach(r),rgt=t(J8e," \u2014 "),xG=s(J8e,"A",{href:!0});var Kfr=n(xG);agt=t(Kfr,"TFXLNetForMultipleChoice"),Kfr.forEach(r),sgt=t(J8e," (XLNet model)"),J8e.forEach(r),ne.forEach(r),ngt=i(Fa),Lpe=s(Fa,"P",{});var Zfr=n(Lpe);lgt=t(Zfr,"Examples:"),Zfr.forEach(r),igt=i(Fa),f(F0.$$.fragment,Fa),Fa.forEach(r),Zl.forEach(r),zxe=i(d),Ic=s(d,"H2",{class:!0});var rSe=n(Ic);H4=s(rSe,"A",{id:!0,class:!0,href:!0});var egr=n(H4);Bpe=s(egr,"SPAN",{});var ogr=n(Bpe);f(C0.$$.fragment,ogr),ogr.forEach(r),egr.forEach(r),dgt=i(rSe),xpe=s(rSe,"SPAN",{});var tgr=n(xpe);cgt=t(tgr,"TFAutoModelForTableQuestionAnswering"),tgr.forEach(r),rSe.forEach(r),Wxe=i(d),yt=s(d,"DIV",{class:!0});var oi=n(yt);f(M0.$$.fragment,oi),mgt=i(oi),jc=s(oi,"P",{});var pW=n(jc);fgt=t(pW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),kpe=s(pW,"CODE",{});var rgr=n(kpe);ggt=t(rgr,"from_pretrained()"),rgr.forEach(r),hgt=t(pW,"class method or the "),Rpe=s(pW,"CODE",{});var agr=n(Rpe);ugt=t(agr,"from_config()"),agr.forEach(r),pgt=t(pW,`class
method.`),pW.forEach(r),_gt=i(oi),E0=s(oi,"P",{});var aSe=n(E0);bgt=t(aSe,"This class cannot be instantiated directly using "),Spe=s(aSe,"CODE",{});var sgr=n(Spe);vgt=t(sgr,"__init__()"),sgr.forEach(r),Tgt=t(aSe," (throws an error)."),aSe.forEach(r),Fgt=i(oi),vr=s(oi,"DIV",{class:!0});var ti=n(vr);f(y0.$$.fragment,ti),Cgt=i(ti),Ppe=s(ti,"P",{});var ngr=n(Ppe);Mgt=t(ngr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),ngr.forEach(r),Egt=i(ti),Dc=s(ti,"P",{});var _W=n(Dc);ygt=t(_W,`Note:
Loading a model from its configuration file does `),$pe=s(_W,"STRONG",{});var lgr=n($pe);wgt=t(lgr,"not"),lgr.forEach(r),Agt=t(_W,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ipe=s(_W,"CODE",{});var igr=n(Ipe);Lgt=t(igr,"from_pretrained()"),igr.forEach(r),Bgt=t(_W,"to load the model weights."),_W.forEach(r),xgt=i(ti),jpe=s(ti,"P",{});var dgr=n(jpe);kgt=t(dgr,"Examples:"),dgr.forEach(r),Rgt=i(ti),f(w0.$$.fragment,ti),ti.forEach(r),Sgt=i(oi),Co=s(oi,"DIV",{class:!0});var Ca=n(Co);f(A0.$$.fragment,Ca),Pgt=i(Ca),Dpe=s(Ca,"P",{});var cgr=n(Dpe);$gt=t(cgr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),cgr.forEach(r),Igt=i(Ca),Cs=s(Ca,"P",{});var T5=n(Cs);jgt=t(T5,"The model class to instantiate is selected based on the "),Npe=s(T5,"CODE",{});var mgr=n(Npe);Dgt=t(mgr,"model_type"),mgr.forEach(r),Ngt=t(T5,` property of the config object (either
passed as an argument or loaded from `),qpe=s(T5,"CODE",{});var fgr=n(qpe);qgt=t(fgr,"pretrained_model_name_or_path"),fgr.forEach(r),Ogt=t(T5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ope=s(T5,"CODE",{});var ggr=n(Ope);Ggt=t(ggr,"pretrained_model_name_or_path"),ggr.forEach(r),Xgt=t(T5,":"),T5.forEach(r),Vgt=i(Ca),Gpe=s(Ca,"UL",{});var hgr=n(Gpe);U4=s(hgr,"LI",{});var Y8e=n(U4);Xpe=s(Y8e,"STRONG",{});var ugr=n(Xpe);zgt=t(ugr,"tapas"),ugr.forEach(r),Wgt=t(Y8e," \u2014 "),kG=s(Y8e,"A",{href:!0});var pgr=n(kG);Qgt=t(pgr,"TFTapasForQuestionAnswering"),pgr.forEach(r),Hgt=t(Y8e," (TAPAS model)"),Y8e.forEach(r),hgr.forEach(r),Ugt=i(Ca),Vpe=s(Ca,"P",{});var _gr=n(Vpe);Jgt=t(_gr,"Examples:"),_gr.forEach(r),Ygt=i(Ca),f(L0.$$.fragment,Ca),Ca.forEach(r),oi.forEach(r),Qxe=i(d),Nc=s(d,"H2",{class:!0});var sSe=n(Nc);J4=s(sSe,"A",{id:!0,class:!0,href:!0});var bgr=n(J4);zpe=s(bgr,"SPAN",{});var vgr=n(zpe);f(B0.$$.fragment,vgr),vgr.forEach(r),bgr.forEach(r),Kgt=i(sSe),Wpe=s(sSe,"SPAN",{});var Tgr=n(Wpe);Zgt=t(Tgr,"TFAutoModelForTokenClassification"),Tgr.forEach(r),sSe.forEach(r),Hxe=i(d),wt=s(d,"DIV",{class:!0});var ri=n(wt);f(x0.$$.fragment,ri),eht=i(ri),qc=s(ri,"P",{});var bW=n(qc);oht=t(bW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Qpe=s(bW,"CODE",{});var Fgr=n(Qpe);tht=t(Fgr,"from_pretrained()"),Fgr.forEach(r),rht=t(bW,"class method or the "),Hpe=s(bW,"CODE",{});var Cgr=n(Hpe);aht=t(Cgr,"from_config()"),Cgr.forEach(r),sht=t(bW,`class
method.`),bW.forEach(r),nht=i(ri),k0=s(ri,"P",{});var nSe=n(k0);lht=t(nSe,"This class cannot be instantiated directly using "),Upe=s(nSe,"CODE",{});var Mgr=n(Upe);iht=t(Mgr,"__init__()"),Mgr.forEach(r),dht=t(nSe," (throws an error)."),nSe.forEach(r),cht=i(ri),Tr=s(ri,"DIV",{class:!0});var ai=n(Tr);f(R0.$$.fragment,ai),mht=i(ai),Jpe=s(ai,"P",{});var Egr=n(Jpe);fht=t(Egr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Egr.forEach(r),ght=i(ai),Oc=s(ai,"P",{});var vW=n(Oc);hht=t(vW,`Note:
Loading a model from its configuration file does `),Ype=s(vW,"STRONG",{});var ygr=n(Ype);uht=t(ygr,"not"),ygr.forEach(r),pht=t(vW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kpe=s(vW,"CODE",{});var wgr=n(Kpe);_ht=t(wgr,"from_pretrained()"),wgr.forEach(r),bht=t(vW,"to load the model weights."),vW.forEach(r),vht=i(ai),Zpe=s(ai,"P",{});var Agr=n(Zpe);Tht=t(Agr,"Examples:"),Agr.forEach(r),Fht=i(ai),f(S0.$$.fragment,ai),ai.forEach(r),Cht=i(ri),Mo=s(ri,"DIV",{class:!0});var Ma=n(Mo);f(P0.$$.fragment,Ma),Mht=i(Ma),e_e=s(Ma,"P",{});var Lgr=n(e_e);Eht=t(Lgr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Lgr.forEach(r),yht=i(Ma),Ms=s(Ma,"P",{});var F5=n(Ms);wht=t(F5,"The model class to instantiate is selected based on the "),o_e=s(F5,"CODE",{});var Bgr=n(o_e);Aht=t(Bgr,"model_type"),Bgr.forEach(r),Lht=t(F5,` property of the config object (either
passed as an argument or loaded from `),t_e=s(F5,"CODE",{});var xgr=n(t_e);Bht=t(xgr,"pretrained_model_name_or_path"),xgr.forEach(r),xht=t(F5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r_e=s(F5,"CODE",{});var kgr=n(r_e);kht=t(kgr,"pretrained_model_name_or_path"),kgr.forEach(r),Rht=t(F5,":"),F5.forEach(r),Sht=i(Ma),K=s(Ma,"UL",{});var oe=n(K);Y4=s(oe,"LI",{});var K8e=n(Y4);a_e=s(K8e,"STRONG",{});var Rgr=n(a_e);Pht=t(Rgr,"albert"),Rgr.forEach(r),$ht=t(K8e," \u2014 "),RG=s(K8e,"A",{href:!0});var Sgr=n(RG);Iht=t(Sgr,"TFAlbertForTokenClassification"),Sgr.forEach(r),jht=t(K8e," (ALBERT model)"),K8e.forEach(r),Dht=i(oe),K4=s(oe,"LI",{});var Z8e=n(K4);s_e=s(Z8e,"STRONG",{});var Pgr=n(s_e);Nht=t(Pgr,"bert"),Pgr.forEach(r),qht=t(Z8e," \u2014 "),SG=s(Z8e,"A",{href:!0});var $gr=n(SG);Oht=t($gr,"TFBertForTokenClassification"),$gr.forEach(r),Ght=t(Z8e," (BERT model)"),Z8e.forEach(r),Xht=i(oe),Z4=s(oe,"LI",{});var e7e=n(Z4);n_e=s(e7e,"STRONG",{});var Igr=n(n_e);Vht=t(Igr,"camembert"),Igr.forEach(r),zht=t(e7e," \u2014 "),PG=s(e7e,"A",{href:!0});var jgr=n(PG);Wht=t(jgr,"TFCamembertForTokenClassification"),jgr.forEach(r),Qht=t(e7e," (CamemBERT model)"),e7e.forEach(r),Hht=i(oe),eM=s(oe,"LI",{});var o7e=n(eM);l_e=s(o7e,"STRONG",{});var Dgr=n(l_e);Uht=t(Dgr,"convbert"),Dgr.forEach(r),Jht=t(o7e," \u2014 "),$G=s(o7e,"A",{href:!0});var Ngr=n($G);Yht=t(Ngr,"TFConvBertForTokenClassification"),Ngr.forEach(r),Kht=t(o7e," (ConvBERT model)"),o7e.forEach(r),Zht=i(oe),oM=s(oe,"LI",{});var t7e=n(oM);i_e=s(t7e,"STRONG",{});var qgr=n(i_e);eut=t(qgr,"deberta"),qgr.forEach(r),out=t(t7e," \u2014 "),IG=s(t7e,"A",{href:!0});var Ogr=n(IG);tut=t(Ogr,"TFDebertaForTokenClassification"),Ogr.forEach(r),rut=t(t7e," (DeBERTa model)"),t7e.forEach(r),aut=i(oe),tM=s(oe,"LI",{});var r7e=n(tM);d_e=s(r7e,"STRONG",{});var Ggr=n(d_e);sut=t(Ggr,"deberta-v2"),Ggr.forEach(r),nut=t(r7e," \u2014 "),jG=s(r7e,"A",{href:!0});var Xgr=n(jG);lut=t(Xgr,"TFDebertaV2ForTokenClassification"),Xgr.forEach(r),iut=t(r7e," (DeBERTa-v2 model)"),r7e.forEach(r),dut=i(oe),rM=s(oe,"LI",{});var a7e=n(rM);c_e=s(a7e,"STRONG",{});var Vgr=n(c_e);cut=t(Vgr,"distilbert"),Vgr.forEach(r),mut=t(a7e," \u2014 "),DG=s(a7e,"A",{href:!0});var zgr=n(DG);fut=t(zgr,"TFDistilBertForTokenClassification"),zgr.forEach(r),gut=t(a7e," (DistilBERT model)"),a7e.forEach(r),hut=i(oe),aM=s(oe,"LI",{});var s7e=n(aM);m_e=s(s7e,"STRONG",{});var Wgr=n(m_e);uut=t(Wgr,"electra"),Wgr.forEach(r),put=t(s7e," \u2014 "),NG=s(s7e,"A",{href:!0});var Qgr=n(NG);_ut=t(Qgr,"TFElectraForTokenClassification"),Qgr.forEach(r),but=t(s7e," (ELECTRA model)"),s7e.forEach(r),vut=i(oe),sM=s(oe,"LI",{});var n7e=n(sM);f_e=s(n7e,"STRONG",{});var Hgr=n(f_e);Tut=t(Hgr,"flaubert"),Hgr.forEach(r),Fut=t(n7e," \u2014 "),qG=s(n7e,"A",{href:!0});var Ugr=n(qG);Cut=t(Ugr,"TFFlaubertForTokenClassification"),Ugr.forEach(r),Mut=t(n7e," (FlauBERT model)"),n7e.forEach(r),Eut=i(oe),nM=s(oe,"LI",{});var l7e=n(nM);g_e=s(l7e,"STRONG",{});var Jgr=n(g_e);yut=t(Jgr,"funnel"),Jgr.forEach(r),wut=t(l7e," \u2014 "),OG=s(l7e,"A",{href:!0});var Ygr=n(OG);Aut=t(Ygr,"TFFunnelForTokenClassification"),Ygr.forEach(r),Lut=t(l7e," (Funnel Transformer model)"),l7e.forEach(r),But=i(oe),lM=s(oe,"LI",{});var i7e=n(lM);h_e=s(i7e,"STRONG",{});var Kgr=n(h_e);xut=t(Kgr,"layoutlm"),Kgr.forEach(r),kut=t(i7e," \u2014 "),GG=s(i7e,"A",{href:!0});var Zgr=n(GG);Rut=t(Zgr,"TFLayoutLMForTokenClassification"),Zgr.forEach(r),Sut=t(i7e," (LayoutLM model)"),i7e.forEach(r),Put=i(oe),iM=s(oe,"LI",{});var d7e=n(iM);u_e=s(d7e,"STRONG",{});var ehr=n(u_e);$ut=t(ehr,"longformer"),ehr.forEach(r),Iut=t(d7e," \u2014 "),XG=s(d7e,"A",{href:!0});var ohr=n(XG);jut=t(ohr,"TFLongformerForTokenClassification"),ohr.forEach(r),Dut=t(d7e," (Longformer model)"),d7e.forEach(r),Nut=i(oe),dM=s(oe,"LI",{});var c7e=n(dM);p_e=s(c7e,"STRONG",{});var thr=n(p_e);qut=t(thr,"mobilebert"),thr.forEach(r),Out=t(c7e," \u2014 "),VG=s(c7e,"A",{href:!0});var rhr=n(VG);Gut=t(rhr,"TFMobileBertForTokenClassification"),rhr.forEach(r),Xut=t(c7e," (MobileBERT model)"),c7e.forEach(r),Vut=i(oe),cM=s(oe,"LI",{});var m7e=n(cM);__e=s(m7e,"STRONG",{});var ahr=n(__e);zut=t(ahr,"mpnet"),ahr.forEach(r),Wut=t(m7e," \u2014 "),zG=s(m7e,"A",{href:!0});var shr=n(zG);Qut=t(shr,"TFMPNetForTokenClassification"),shr.forEach(r),Hut=t(m7e," (MPNet model)"),m7e.forEach(r),Uut=i(oe),mM=s(oe,"LI",{});var f7e=n(mM);b_e=s(f7e,"STRONG",{});var nhr=n(b_e);Jut=t(nhr,"rembert"),nhr.forEach(r),Yut=t(f7e," \u2014 "),WG=s(f7e,"A",{href:!0});var lhr=n(WG);Kut=t(lhr,"TFRemBertForTokenClassification"),lhr.forEach(r),Zut=t(f7e," (RemBERT model)"),f7e.forEach(r),ept=i(oe),fM=s(oe,"LI",{});var g7e=n(fM);v_e=s(g7e,"STRONG",{});var ihr=n(v_e);opt=t(ihr,"roberta"),ihr.forEach(r),tpt=t(g7e," \u2014 "),QG=s(g7e,"A",{href:!0});var dhr=n(QG);rpt=t(dhr,"TFRobertaForTokenClassification"),dhr.forEach(r),apt=t(g7e," (RoBERTa model)"),g7e.forEach(r),spt=i(oe),gM=s(oe,"LI",{});var h7e=n(gM);T_e=s(h7e,"STRONG",{});var chr=n(T_e);npt=t(chr,"roformer"),chr.forEach(r),lpt=t(h7e," \u2014 "),HG=s(h7e,"A",{href:!0});var mhr=n(HG);ipt=t(mhr,"TFRoFormerForTokenClassification"),mhr.forEach(r),dpt=t(h7e," (RoFormer model)"),h7e.forEach(r),cpt=i(oe),hM=s(oe,"LI",{});var u7e=n(hM);F_e=s(u7e,"STRONG",{});var fhr=n(F_e);mpt=t(fhr,"xlm"),fhr.forEach(r),fpt=t(u7e," \u2014 "),UG=s(u7e,"A",{href:!0});var ghr=n(UG);gpt=t(ghr,"TFXLMForTokenClassification"),ghr.forEach(r),hpt=t(u7e," (XLM model)"),u7e.forEach(r),upt=i(oe),uM=s(oe,"LI",{});var p7e=n(uM);C_e=s(p7e,"STRONG",{});var hhr=n(C_e);ppt=t(hhr,"xlm-roberta"),hhr.forEach(r),_pt=t(p7e," \u2014 "),JG=s(p7e,"A",{href:!0});var uhr=n(JG);bpt=t(uhr,"TFXLMRobertaForTokenClassification"),uhr.forEach(r),vpt=t(p7e," (XLM-RoBERTa model)"),p7e.forEach(r),Tpt=i(oe),pM=s(oe,"LI",{});var _7e=n(pM);M_e=s(_7e,"STRONG",{});var phr=n(M_e);Fpt=t(phr,"xlnet"),phr.forEach(r),Cpt=t(_7e," \u2014 "),YG=s(_7e,"A",{href:!0});var _hr=n(YG);Mpt=t(_hr,"TFXLNetForTokenClassification"),_hr.forEach(r),Ept=t(_7e," (XLNet model)"),_7e.forEach(r),oe.forEach(r),ypt=i(Ma),E_e=s(Ma,"P",{});var bhr=n(E_e);wpt=t(bhr,"Examples:"),bhr.forEach(r),Apt=i(Ma),f($0.$$.fragment,Ma),Ma.forEach(r),ri.forEach(r),Uxe=i(d),Gc=s(d,"H2",{class:!0});var lSe=n(Gc);_M=s(lSe,"A",{id:!0,class:!0,href:!0});var vhr=n(_M);y_e=s(vhr,"SPAN",{});var Thr=n(y_e);f(I0.$$.fragment,Thr),Thr.forEach(r),vhr.forEach(r),Lpt=i(lSe),w_e=s(lSe,"SPAN",{});var Fhr=n(w_e);Bpt=t(Fhr,"TFAutoModelForQuestionAnswering"),Fhr.forEach(r),lSe.forEach(r),Jxe=i(d),At=s(d,"DIV",{class:!0});var si=n(At);f(j0.$$.fragment,si),xpt=i(si),Xc=s(si,"P",{});var TW=n(Xc);kpt=t(TW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A_e=s(TW,"CODE",{});var Chr=n(A_e);Rpt=t(Chr,"from_pretrained()"),Chr.forEach(r),Spt=t(TW,"class method or the "),L_e=s(TW,"CODE",{});var Mhr=n(L_e);Ppt=t(Mhr,"from_config()"),Mhr.forEach(r),$pt=t(TW,`class
method.`),TW.forEach(r),Ipt=i(si),D0=s(si,"P",{});var iSe=n(D0);jpt=t(iSe,"This class cannot be instantiated directly using "),B_e=s(iSe,"CODE",{});var Ehr=n(B_e);Dpt=t(Ehr,"__init__()"),Ehr.forEach(r),Npt=t(iSe," (throws an error)."),iSe.forEach(r),qpt=i(si),Fr=s(si,"DIV",{class:!0});var ni=n(Fr);f(N0.$$.fragment,ni),Opt=i(ni),x_e=s(ni,"P",{});var yhr=n(x_e);Gpt=t(yhr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),yhr.forEach(r),Xpt=i(ni),Vc=s(ni,"P",{});var FW=n(Vc);Vpt=t(FW,`Note:
Loading a model from its configuration file does `),k_e=s(FW,"STRONG",{});var whr=n(k_e);zpt=t(whr,"not"),whr.forEach(r),Wpt=t(FW,` load the model weights. It only affects the
model\u2019s configuration. Use `),R_e=s(FW,"CODE",{});var Ahr=n(R_e);Qpt=t(Ahr,"from_pretrained()"),Ahr.forEach(r),Hpt=t(FW,"to load the model weights."),FW.forEach(r),Upt=i(ni),S_e=s(ni,"P",{});var Lhr=n(S_e);Jpt=t(Lhr,"Examples:"),Lhr.forEach(r),Ypt=i(ni),f(q0.$$.fragment,ni),ni.forEach(r),Kpt=i(si),Eo=s(si,"DIV",{class:!0});var Ea=n(Eo);f(O0.$$.fragment,Ea),Zpt=i(Ea),P_e=s(Ea,"P",{});var Bhr=n(P_e);e_t=t(Bhr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Bhr.forEach(r),o_t=i(Ea),Es=s(Ea,"P",{});var C5=n(Es);t_t=t(C5,"The model class to instantiate is selected based on the "),$_e=s(C5,"CODE",{});var xhr=n($_e);r_t=t(xhr,"model_type"),xhr.forEach(r),a_t=t(C5,` property of the config object (either
passed as an argument or loaded from `),I_e=s(C5,"CODE",{});var khr=n(I_e);s_t=t(khr,"pretrained_model_name_or_path"),khr.forEach(r),n_t=t(C5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j_e=s(C5,"CODE",{});var Rhr=n(j_e);l_t=t(Rhr,"pretrained_model_name_or_path"),Rhr.forEach(r),i_t=t(C5,":"),C5.forEach(r),d_t=i(Ea),Z=s(Ea,"UL",{});var te=n(Z);bM=s(te,"LI",{});var b7e=n(bM);D_e=s(b7e,"STRONG",{});var Shr=n(D_e);c_t=t(Shr,"albert"),Shr.forEach(r),m_t=t(b7e," \u2014 "),KG=s(b7e,"A",{href:!0});var Phr=n(KG);f_t=t(Phr,"TFAlbertForQuestionAnswering"),Phr.forEach(r),g_t=t(b7e," (ALBERT model)"),b7e.forEach(r),h_t=i(te),vM=s(te,"LI",{});var v7e=n(vM);N_e=s(v7e,"STRONG",{});var $hr=n(N_e);u_t=t($hr,"bert"),$hr.forEach(r),p_t=t(v7e," \u2014 "),ZG=s(v7e,"A",{href:!0});var Ihr=n(ZG);__t=t(Ihr,"TFBertForQuestionAnswering"),Ihr.forEach(r),b_t=t(v7e," (BERT model)"),v7e.forEach(r),v_t=i(te),TM=s(te,"LI",{});var T7e=n(TM);q_e=s(T7e,"STRONG",{});var jhr=n(q_e);T_t=t(jhr,"camembert"),jhr.forEach(r),F_t=t(T7e," \u2014 "),eX=s(T7e,"A",{href:!0});var Dhr=n(eX);C_t=t(Dhr,"TFCamembertForQuestionAnswering"),Dhr.forEach(r),M_t=t(T7e," (CamemBERT model)"),T7e.forEach(r),E_t=i(te),FM=s(te,"LI",{});var F7e=n(FM);O_e=s(F7e,"STRONG",{});var Nhr=n(O_e);y_t=t(Nhr,"convbert"),Nhr.forEach(r),w_t=t(F7e," \u2014 "),oX=s(F7e,"A",{href:!0});var qhr=n(oX);A_t=t(qhr,"TFConvBertForQuestionAnswering"),qhr.forEach(r),L_t=t(F7e," (ConvBERT model)"),F7e.forEach(r),B_t=i(te),CM=s(te,"LI",{});var C7e=n(CM);G_e=s(C7e,"STRONG",{});var Ohr=n(G_e);x_t=t(Ohr,"deberta"),Ohr.forEach(r),k_t=t(C7e," \u2014 "),tX=s(C7e,"A",{href:!0});var Ghr=n(tX);R_t=t(Ghr,"TFDebertaForQuestionAnswering"),Ghr.forEach(r),S_t=t(C7e," (DeBERTa model)"),C7e.forEach(r),P_t=i(te),MM=s(te,"LI",{});var M7e=n(MM);X_e=s(M7e,"STRONG",{});var Xhr=n(X_e);$_t=t(Xhr,"deberta-v2"),Xhr.forEach(r),I_t=t(M7e," \u2014 "),rX=s(M7e,"A",{href:!0});var Vhr=n(rX);j_t=t(Vhr,"TFDebertaV2ForQuestionAnswering"),Vhr.forEach(r),D_t=t(M7e," (DeBERTa-v2 model)"),M7e.forEach(r),N_t=i(te),EM=s(te,"LI",{});var E7e=n(EM);V_e=s(E7e,"STRONG",{});var zhr=n(V_e);q_t=t(zhr,"distilbert"),zhr.forEach(r),O_t=t(E7e," \u2014 "),aX=s(E7e,"A",{href:!0});var Whr=n(aX);G_t=t(Whr,"TFDistilBertForQuestionAnswering"),Whr.forEach(r),X_t=t(E7e," (DistilBERT model)"),E7e.forEach(r),V_t=i(te),yM=s(te,"LI",{});var y7e=n(yM);z_e=s(y7e,"STRONG",{});var Qhr=n(z_e);z_t=t(Qhr,"electra"),Qhr.forEach(r),W_t=t(y7e," \u2014 "),sX=s(y7e,"A",{href:!0});var Hhr=n(sX);Q_t=t(Hhr,"TFElectraForQuestionAnswering"),Hhr.forEach(r),H_t=t(y7e," (ELECTRA model)"),y7e.forEach(r),U_t=i(te),wM=s(te,"LI",{});var w7e=n(wM);W_e=s(w7e,"STRONG",{});var Uhr=n(W_e);J_t=t(Uhr,"flaubert"),Uhr.forEach(r),Y_t=t(w7e," \u2014 "),nX=s(w7e,"A",{href:!0});var Jhr=n(nX);K_t=t(Jhr,"TFFlaubertForQuestionAnsweringSimple"),Jhr.forEach(r),Z_t=t(w7e," (FlauBERT model)"),w7e.forEach(r),ebt=i(te),AM=s(te,"LI",{});var A7e=n(AM);Q_e=s(A7e,"STRONG",{});var Yhr=n(Q_e);obt=t(Yhr,"funnel"),Yhr.forEach(r),tbt=t(A7e," \u2014 "),lX=s(A7e,"A",{href:!0});var Khr=n(lX);rbt=t(Khr,"TFFunnelForQuestionAnswering"),Khr.forEach(r),abt=t(A7e," (Funnel Transformer model)"),A7e.forEach(r),sbt=i(te),LM=s(te,"LI",{});var L7e=n(LM);H_e=s(L7e,"STRONG",{});var Zhr=n(H_e);nbt=t(Zhr,"longformer"),Zhr.forEach(r),lbt=t(L7e," \u2014 "),iX=s(L7e,"A",{href:!0});var eur=n(iX);ibt=t(eur,"TFLongformerForQuestionAnswering"),eur.forEach(r),dbt=t(L7e," (Longformer model)"),L7e.forEach(r),cbt=i(te),BM=s(te,"LI",{});var B7e=n(BM);U_e=s(B7e,"STRONG",{});var our=n(U_e);mbt=t(our,"mobilebert"),our.forEach(r),fbt=t(B7e," \u2014 "),dX=s(B7e,"A",{href:!0});var tur=n(dX);gbt=t(tur,"TFMobileBertForQuestionAnswering"),tur.forEach(r),hbt=t(B7e," (MobileBERT model)"),B7e.forEach(r),ubt=i(te),xM=s(te,"LI",{});var x7e=n(xM);J_e=s(x7e,"STRONG",{});var rur=n(J_e);pbt=t(rur,"mpnet"),rur.forEach(r),_bt=t(x7e," \u2014 "),cX=s(x7e,"A",{href:!0});var aur=n(cX);bbt=t(aur,"TFMPNetForQuestionAnswering"),aur.forEach(r),vbt=t(x7e," (MPNet model)"),x7e.forEach(r),Tbt=i(te),kM=s(te,"LI",{});var k7e=n(kM);Y_e=s(k7e,"STRONG",{});var sur=n(Y_e);Fbt=t(sur,"rembert"),sur.forEach(r),Cbt=t(k7e," \u2014 "),mX=s(k7e,"A",{href:!0});var nur=n(mX);Mbt=t(nur,"TFRemBertForQuestionAnswering"),nur.forEach(r),Ebt=t(k7e," (RemBERT model)"),k7e.forEach(r),ybt=i(te),RM=s(te,"LI",{});var R7e=n(RM);K_e=s(R7e,"STRONG",{});var lur=n(K_e);wbt=t(lur,"roberta"),lur.forEach(r),Abt=t(R7e," \u2014 "),fX=s(R7e,"A",{href:!0});var iur=n(fX);Lbt=t(iur,"TFRobertaForQuestionAnswering"),iur.forEach(r),Bbt=t(R7e," (RoBERTa model)"),R7e.forEach(r),xbt=i(te),SM=s(te,"LI",{});var S7e=n(SM);Z_e=s(S7e,"STRONG",{});var dur=n(Z_e);kbt=t(dur,"roformer"),dur.forEach(r),Rbt=t(S7e," \u2014 "),gX=s(S7e,"A",{href:!0});var cur=n(gX);Sbt=t(cur,"TFRoFormerForQuestionAnswering"),cur.forEach(r),Pbt=t(S7e," (RoFormer model)"),S7e.forEach(r),$bt=i(te),PM=s(te,"LI",{});var P7e=n(PM);ebe=s(P7e,"STRONG",{});var mur=n(ebe);Ibt=t(mur,"xlm"),mur.forEach(r),jbt=t(P7e," \u2014 "),hX=s(P7e,"A",{href:!0});var fur=n(hX);Dbt=t(fur,"TFXLMForQuestionAnsweringSimple"),fur.forEach(r),Nbt=t(P7e," (XLM model)"),P7e.forEach(r),qbt=i(te),$M=s(te,"LI",{});var $7e=n($M);obe=s($7e,"STRONG",{});var gur=n(obe);Obt=t(gur,"xlm-roberta"),gur.forEach(r),Gbt=t($7e," \u2014 "),uX=s($7e,"A",{href:!0});var hur=n(uX);Xbt=t(hur,"TFXLMRobertaForQuestionAnswering"),hur.forEach(r),Vbt=t($7e," (XLM-RoBERTa model)"),$7e.forEach(r),zbt=i(te),IM=s(te,"LI",{});var I7e=n(IM);tbe=s(I7e,"STRONG",{});var uur=n(tbe);Wbt=t(uur,"xlnet"),uur.forEach(r),Qbt=t(I7e," \u2014 "),pX=s(I7e,"A",{href:!0});var pur=n(pX);Hbt=t(pur,"TFXLNetForQuestionAnsweringSimple"),pur.forEach(r),Ubt=t(I7e," (XLNet model)"),I7e.forEach(r),te.forEach(r),Jbt=i(Ea),rbe=s(Ea,"P",{});var _ur=n(rbe);Ybt=t(_ur,"Examples:"),_ur.forEach(r),Kbt=i(Ea),f(G0.$$.fragment,Ea),Ea.forEach(r),si.forEach(r),Yxe=i(d),zc=s(d,"H2",{class:!0});var dSe=n(zc);jM=s(dSe,"A",{id:!0,class:!0,href:!0});var bur=n(jM);abe=s(bur,"SPAN",{});var vur=n(abe);f(X0.$$.fragment,vur),vur.forEach(r),bur.forEach(r),Zbt=i(dSe),sbe=s(dSe,"SPAN",{});var Tur=n(sbe);e2t=t(Tur,"TFAutoModelForVision2Seq"),Tur.forEach(r),dSe.forEach(r),Kxe=i(d),Lt=s(d,"DIV",{class:!0});var li=n(Lt);f(V0.$$.fragment,li),o2t=i(li),Wc=s(li,"P",{});var CW=n(Wc);t2t=t(CW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),nbe=s(CW,"CODE",{});var Fur=n(nbe);r2t=t(Fur,"from_pretrained()"),Fur.forEach(r),a2t=t(CW,"class method or the "),lbe=s(CW,"CODE",{});var Cur=n(lbe);s2t=t(Cur,"from_config()"),Cur.forEach(r),n2t=t(CW,`class
method.`),CW.forEach(r),l2t=i(li),z0=s(li,"P",{});var cSe=n(z0);i2t=t(cSe,"This class cannot be instantiated directly using "),ibe=s(cSe,"CODE",{});var Mur=n(ibe);d2t=t(Mur,"__init__()"),Mur.forEach(r),c2t=t(cSe," (throws an error)."),cSe.forEach(r),m2t=i(li),Cr=s(li,"DIV",{class:!0});var ii=n(Cr);f(W0.$$.fragment,ii),f2t=i(ii),dbe=s(ii,"P",{});var Eur=n(dbe);g2t=t(Eur,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Eur.forEach(r),h2t=i(ii),Qc=s(ii,"P",{});var MW=n(Qc);u2t=t(MW,`Note:
Loading a model from its configuration file does `),cbe=s(MW,"STRONG",{});var yur=n(cbe);p2t=t(yur,"not"),yur.forEach(r),_2t=t(MW,` load the model weights. It only affects the
model\u2019s configuration. Use `),mbe=s(MW,"CODE",{});var wur=n(mbe);b2t=t(wur,"from_pretrained()"),wur.forEach(r),v2t=t(MW,"to load the model weights."),MW.forEach(r),T2t=i(ii),fbe=s(ii,"P",{});var Aur=n(fbe);F2t=t(Aur,"Examples:"),Aur.forEach(r),C2t=i(ii),f(Q0.$$.fragment,ii),ii.forEach(r),M2t=i(li),yo=s(li,"DIV",{class:!0});var ya=n(yo);f(H0.$$.fragment,ya),E2t=i(ya),gbe=s(ya,"P",{});var Lur=n(gbe);y2t=t(Lur,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),Lur.forEach(r),w2t=i(ya),ys=s(ya,"P",{});var M5=n(ys);A2t=t(M5,"The model class to instantiate is selected based on the "),hbe=s(M5,"CODE",{});var Bur=n(hbe);L2t=t(Bur,"model_type"),Bur.forEach(r),B2t=t(M5,` property of the config object (either
passed as an argument or loaded from `),ube=s(M5,"CODE",{});var xur=n(ube);x2t=t(xur,"pretrained_model_name_or_path"),xur.forEach(r),k2t=t(M5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pbe=s(M5,"CODE",{});var kur=n(pbe);R2t=t(kur,"pretrained_model_name_or_path"),kur.forEach(r),S2t=t(M5,":"),M5.forEach(r),P2t=i(ya),_be=s(ya,"UL",{});var Rur=n(_be);DM=s(Rur,"LI",{});var j7e=n(DM);bbe=s(j7e,"STRONG",{});var Sur=n(bbe);$2t=t(Sur,"vision-encoder-decoder"),Sur.forEach(r),I2t=t(j7e," \u2014 "),_X=s(j7e,"A",{href:!0});var Pur=n(_X);j2t=t(Pur,"TFVisionEncoderDecoderModel"),Pur.forEach(r),D2t=t(j7e," (Vision Encoder decoder model)"),j7e.forEach(r),Rur.forEach(r),N2t=i(ya),vbe=s(ya,"P",{});var $ur=n(vbe);q2t=t($ur,"Examples:"),$ur.forEach(r),O2t=i(ya),f(U0.$$.fragment,ya),ya.forEach(r),li.forEach(r),Zxe=i(d),Hc=s(d,"H2",{class:!0});var mSe=n(Hc);NM=s(mSe,"A",{id:!0,class:!0,href:!0});var Iur=n(NM);Tbe=s(Iur,"SPAN",{});var jur=n(Tbe);f(J0.$$.fragment,jur),jur.forEach(r),Iur.forEach(r),G2t=i(mSe),Fbe=s(mSe,"SPAN",{});var Dur=n(Fbe);X2t=t(Dur,"TFAutoModelForSpeechSeq2Seq"),Dur.forEach(r),mSe.forEach(r),eke=i(d),Bt=s(d,"DIV",{class:!0});var di=n(Bt);f(Y0.$$.fragment,di),V2t=i(di),Uc=s(di,"P",{});var EW=n(Uc);z2t=t(EW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Cbe=s(EW,"CODE",{});var Nur=n(Cbe);W2t=t(Nur,"from_pretrained()"),Nur.forEach(r),Q2t=t(EW,"class method or the "),Mbe=s(EW,"CODE",{});var qur=n(Mbe);H2t=t(qur,"from_config()"),qur.forEach(r),U2t=t(EW,`class
method.`),EW.forEach(r),J2t=i(di),K0=s(di,"P",{});var fSe=n(K0);Y2t=t(fSe,"This class cannot be instantiated directly using "),Ebe=s(fSe,"CODE",{});var Our=n(Ebe);K2t=t(Our,"__init__()"),Our.forEach(r),Z2t=t(fSe," (throws an error)."),fSe.forEach(r),evt=i(di),Mr=s(di,"DIV",{class:!0});var ci=n(Mr);f(Z0.$$.fragment,ci),ovt=i(ci),ybe=s(ci,"P",{});var Gur=n(ybe);tvt=t(Gur,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Gur.forEach(r),rvt=i(ci),Jc=s(ci,"P",{});var yW=n(Jc);avt=t(yW,`Note:
Loading a model from its configuration file does `),wbe=s(yW,"STRONG",{});var Xur=n(wbe);svt=t(Xur,"not"),Xur.forEach(r),nvt=t(yW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Abe=s(yW,"CODE",{});var Vur=n(Abe);lvt=t(Vur,"from_pretrained()"),Vur.forEach(r),ivt=t(yW,"to load the model weights."),yW.forEach(r),dvt=i(ci),Lbe=s(ci,"P",{});var zur=n(Lbe);cvt=t(zur,"Examples:"),zur.forEach(r),mvt=i(ci),f(eL.$$.fragment,ci),ci.forEach(r),fvt=i(di),wo=s(di,"DIV",{class:!0});var wa=n(wo);f(oL.$$.fragment,wa),gvt=i(wa),Bbe=s(wa,"P",{});var Wur=n(Bbe);hvt=t(Wur,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Wur.forEach(r),uvt=i(wa),ws=s(wa,"P",{});var E5=n(ws);pvt=t(E5,"The model class to instantiate is selected based on the "),xbe=s(E5,"CODE",{});var Qur=n(xbe);_vt=t(Qur,"model_type"),Qur.forEach(r),bvt=t(E5,` property of the config object (either
passed as an argument or loaded from `),kbe=s(E5,"CODE",{});var Hur=n(kbe);vvt=t(Hur,"pretrained_model_name_or_path"),Hur.forEach(r),Tvt=t(E5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rbe=s(E5,"CODE",{});var Uur=n(Rbe);Fvt=t(Uur,"pretrained_model_name_or_path"),Uur.forEach(r),Cvt=t(E5,":"),E5.forEach(r),Mvt=i(wa),Sbe=s(wa,"UL",{});var Jur=n(Sbe);qM=s(Jur,"LI",{});var D7e=n(qM);Pbe=s(D7e,"STRONG",{});var Yur=n(Pbe);Evt=t(Yur,"speech_to_text"),Yur.forEach(r),yvt=t(D7e," \u2014 "),bX=s(D7e,"A",{href:!0});var Kur=n(bX);wvt=t(Kur,"TFSpeech2TextForConditionalGeneration"),Kur.forEach(r),Avt=t(D7e," (Speech2Text model)"),D7e.forEach(r),Jur.forEach(r),Lvt=i(wa),$be=s(wa,"P",{});var Zur=n($be);Bvt=t(Zur,"Examples:"),Zur.forEach(r),xvt=i(wa),f(tL.$$.fragment,wa),wa.forEach(r),di.forEach(r),oke=i(d),Yc=s(d,"H2",{class:!0});var gSe=n(Yc);OM=s(gSe,"A",{id:!0,class:!0,href:!0});var epr=n(OM);Ibe=s(epr,"SPAN",{});var opr=n(Ibe);f(rL.$$.fragment,opr),opr.forEach(r),epr.forEach(r),kvt=i(gSe),jbe=s(gSe,"SPAN",{});var tpr=n(jbe);Rvt=t(tpr,"FlaxAutoModel"),tpr.forEach(r),gSe.forEach(r),tke=i(d),xt=s(d,"DIV",{class:!0});var mi=n(xt);f(aL.$$.fragment,mi),Svt=i(mi),Kc=s(mi,"P",{});var wW=n(Kc);Pvt=t(wW,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Dbe=s(wW,"CODE",{});var rpr=n(Dbe);$vt=t(rpr,"from_pretrained()"),rpr.forEach(r),Ivt=t(wW,"class method or the "),Nbe=s(wW,"CODE",{});var apr=n(Nbe);jvt=t(apr,"from_config()"),apr.forEach(r),Dvt=t(wW,`class
method.`),wW.forEach(r),Nvt=i(mi),sL=s(mi,"P",{});var hSe=n(sL);qvt=t(hSe,"This class cannot be instantiated directly using "),qbe=s(hSe,"CODE",{});var spr=n(qbe);Ovt=t(spr,"__init__()"),spr.forEach(r),Gvt=t(hSe," (throws an error)."),hSe.forEach(r),Xvt=i(mi),Er=s(mi,"DIV",{class:!0});var fi=n(Er);f(nL.$$.fragment,fi),Vvt=i(fi),Obe=s(fi,"P",{});var npr=n(Obe);zvt=t(npr,"Instantiates one of the base model classes of the library from a configuration."),npr.forEach(r),Wvt=i(fi),Zc=s(fi,"P",{});var AW=n(Zc);Qvt=t(AW,`Note:
Loading a model from its configuration file does `),Gbe=s(AW,"STRONG",{});var lpr=n(Gbe);Hvt=t(lpr,"not"),lpr.forEach(r),Uvt=t(AW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xbe=s(AW,"CODE",{});var ipr=n(Xbe);Jvt=t(ipr,"from_pretrained()"),ipr.forEach(r),Yvt=t(AW,"to load the model weights."),AW.forEach(r),Kvt=i(fi),Vbe=s(fi,"P",{});var dpr=n(Vbe);Zvt=t(dpr,"Examples:"),dpr.forEach(r),eTt=i(fi),f(lL.$$.fragment,fi),fi.forEach(r),oTt=i(mi),Ao=s(mi,"DIV",{class:!0});var Aa=n(Ao);f(iL.$$.fragment,Aa),tTt=i(Aa),zbe=s(Aa,"P",{});var cpr=n(zbe);rTt=t(cpr,"Instantiate one of the base model classes of the library from a pretrained model."),cpr.forEach(r),aTt=i(Aa),As=s(Aa,"P",{});var y5=n(As);sTt=t(y5,"The model class to instantiate is selected based on the "),Wbe=s(y5,"CODE",{});var mpr=n(Wbe);nTt=t(mpr,"model_type"),mpr.forEach(r),lTt=t(y5,` property of the config object (either
passed as an argument or loaded from `),Qbe=s(y5,"CODE",{});var fpr=n(Qbe);iTt=t(fpr,"pretrained_model_name_or_path"),fpr.forEach(r),dTt=t(y5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hbe=s(y5,"CODE",{});var gpr=n(Hbe);cTt=t(gpr,"pretrained_model_name_or_path"),gpr.forEach(r),mTt=t(y5,":"),y5.forEach(r),fTt=i(Aa),z=s(Aa,"UL",{});var Q=n(z);GM=s(Q,"LI",{});var N7e=n(GM);Ube=s(N7e,"STRONG",{});var hpr=n(Ube);gTt=t(hpr,"albert"),hpr.forEach(r),hTt=t(N7e," \u2014 "),vX=s(N7e,"A",{href:!0});var upr=n(vX);uTt=t(upr,"FlaxAlbertModel"),upr.forEach(r),pTt=t(N7e," (ALBERT model)"),N7e.forEach(r),_Tt=i(Q),XM=s(Q,"LI",{});var q7e=n(XM);Jbe=s(q7e,"STRONG",{});var ppr=n(Jbe);bTt=t(ppr,"bart"),ppr.forEach(r),vTt=t(q7e," \u2014 "),TX=s(q7e,"A",{href:!0});var _pr=n(TX);TTt=t(_pr,"FlaxBartModel"),_pr.forEach(r),FTt=t(q7e," (BART model)"),q7e.forEach(r),CTt=i(Q),VM=s(Q,"LI",{});var O7e=n(VM);Ybe=s(O7e,"STRONG",{});var bpr=n(Ybe);MTt=t(bpr,"beit"),bpr.forEach(r),ETt=t(O7e," \u2014 "),FX=s(O7e,"A",{href:!0});var vpr=n(FX);yTt=t(vpr,"FlaxBeitModel"),vpr.forEach(r),wTt=t(O7e," (BEiT model)"),O7e.forEach(r),ATt=i(Q),zM=s(Q,"LI",{});var G7e=n(zM);Kbe=s(G7e,"STRONG",{});var Tpr=n(Kbe);LTt=t(Tpr,"bert"),Tpr.forEach(r),BTt=t(G7e," \u2014 "),CX=s(G7e,"A",{href:!0});var Fpr=n(CX);xTt=t(Fpr,"FlaxBertModel"),Fpr.forEach(r),kTt=t(G7e," (BERT model)"),G7e.forEach(r),RTt=i(Q),WM=s(Q,"LI",{});var X7e=n(WM);Zbe=s(X7e,"STRONG",{});var Cpr=n(Zbe);STt=t(Cpr,"big_bird"),Cpr.forEach(r),PTt=t(X7e," \u2014 "),MX=s(X7e,"A",{href:!0});var Mpr=n(MX);$Tt=t(Mpr,"FlaxBigBirdModel"),Mpr.forEach(r),ITt=t(X7e," (BigBird model)"),X7e.forEach(r),jTt=i(Q),QM=s(Q,"LI",{});var V7e=n(QM);e2e=s(V7e,"STRONG",{});var Epr=n(e2e);DTt=t(Epr,"blenderbot"),Epr.forEach(r),NTt=t(V7e," \u2014 "),EX=s(V7e,"A",{href:!0});var ypr=n(EX);qTt=t(ypr,"FlaxBlenderbotModel"),ypr.forEach(r),OTt=t(V7e," (Blenderbot model)"),V7e.forEach(r),GTt=i(Q),HM=s(Q,"LI",{});var z7e=n(HM);o2e=s(z7e,"STRONG",{});var wpr=n(o2e);XTt=t(wpr,"blenderbot-small"),wpr.forEach(r),VTt=t(z7e," \u2014 "),yX=s(z7e,"A",{href:!0});var Apr=n(yX);zTt=t(Apr,"FlaxBlenderbotSmallModel"),Apr.forEach(r),WTt=t(z7e," (BlenderbotSmall model)"),z7e.forEach(r),QTt=i(Q),UM=s(Q,"LI",{});var W7e=n(UM);t2e=s(W7e,"STRONG",{});var Lpr=n(t2e);HTt=t(Lpr,"clip"),Lpr.forEach(r),UTt=t(W7e," \u2014 "),wX=s(W7e,"A",{href:!0});var Bpr=n(wX);JTt=t(Bpr,"FlaxCLIPModel"),Bpr.forEach(r),YTt=t(W7e," (CLIP model)"),W7e.forEach(r),KTt=i(Q),JM=s(Q,"LI",{});var Q7e=n(JM);r2e=s(Q7e,"STRONG",{});var xpr=n(r2e);ZTt=t(xpr,"distilbert"),xpr.forEach(r),e1t=t(Q7e," \u2014 "),AX=s(Q7e,"A",{href:!0});var kpr=n(AX);o1t=t(kpr,"FlaxDistilBertModel"),kpr.forEach(r),t1t=t(Q7e," (DistilBERT model)"),Q7e.forEach(r),r1t=i(Q),YM=s(Q,"LI",{});var H7e=n(YM);a2e=s(H7e,"STRONG",{});var Rpr=n(a2e);a1t=t(Rpr,"electra"),Rpr.forEach(r),s1t=t(H7e," \u2014 "),LX=s(H7e,"A",{href:!0});var Spr=n(LX);n1t=t(Spr,"FlaxElectraModel"),Spr.forEach(r),l1t=t(H7e," (ELECTRA model)"),H7e.forEach(r),i1t=i(Q),KM=s(Q,"LI",{});var U7e=n(KM);s2e=s(U7e,"STRONG",{});var Ppr=n(s2e);d1t=t(Ppr,"gpt2"),Ppr.forEach(r),c1t=t(U7e," \u2014 "),BX=s(U7e,"A",{href:!0});var $pr=n(BX);m1t=t($pr,"FlaxGPT2Model"),$pr.forEach(r),f1t=t(U7e," (OpenAI GPT-2 model)"),U7e.forEach(r),g1t=i(Q),ZM=s(Q,"LI",{});var J7e=n(ZM);n2e=s(J7e,"STRONG",{});var Ipr=n(n2e);h1t=t(Ipr,"gpt_neo"),Ipr.forEach(r),u1t=t(J7e," \u2014 "),xX=s(J7e,"A",{href:!0});var jpr=n(xX);p1t=t(jpr,"FlaxGPTNeoModel"),jpr.forEach(r),_1t=t(J7e," (GPT Neo model)"),J7e.forEach(r),b1t=i(Q),eE=s(Q,"LI",{});var Y7e=n(eE);l2e=s(Y7e,"STRONG",{});var Dpr=n(l2e);v1t=t(Dpr,"gptj"),Dpr.forEach(r),T1t=t(Y7e," \u2014 "),kX=s(Y7e,"A",{href:!0});var Npr=n(kX);F1t=t(Npr,"FlaxGPTJModel"),Npr.forEach(r),C1t=t(Y7e," (GPT-J model)"),Y7e.forEach(r),M1t=i(Q),oE=s(Q,"LI",{});var K7e=n(oE);i2e=s(K7e,"STRONG",{});var qpr=n(i2e);E1t=t(qpr,"marian"),qpr.forEach(r),y1t=t(K7e," \u2014 "),RX=s(K7e,"A",{href:!0});var Opr=n(RX);w1t=t(Opr,"FlaxMarianModel"),Opr.forEach(r),A1t=t(K7e," (Marian model)"),K7e.forEach(r),L1t=i(Q),tE=s(Q,"LI",{});var Z7e=n(tE);d2e=s(Z7e,"STRONG",{});var Gpr=n(d2e);B1t=t(Gpr,"mbart"),Gpr.forEach(r),x1t=t(Z7e," \u2014 "),SX=s(Z7e,"A",{href:!0});var Xpr=n(SX);k1t=t(Xpr,"FlaxMBartModel"),Xpr.forEach(r),R1t=t(Z7e," (mBART model)"),Z7e.forEach(r),S1t=i(Q),rE=s(Q,"LI",{});var e9e=n(rE);c2e=s(e9e,"STRONG",{});var Vpr=n(c2e);P1t=t(Vpr,"mt5"),Vpr.forEach(r),$1t=t(e9e," \u2014 "),PX=s(e9e,"A",{href:!0});var zpr=n(PX);I1t=t(zpr,"FlaxMT5Model"),zpr.forEach(r),j1t=t(e9e," (mT5 model)"),e9e.forEach(r),D1t=i(Q),aE=s(Q,"LI",{});var o9e=n(aE);m2e=s(o9e,"STRONG",{});var Wpr=n(m2e);N1t=t(Wpr,"pegasus"),Wpr.forEach(r),q1t=t(o9e," \u2014 "),$X=s(o9e,"A",{href:!0});var Qpr=n($X);O1t=t(Qpr,"FlaxPegasusModel"),Qpr.forEach(r),G1t=t(o9e," (Pegasus model)"),o9e.forEach(r),X1t=i(Q),sE=s(Q,"LI",{});var t9e=n(sE);f2e=s(t9e,"STRONG",{});var Hpr=n(f2e);V1t=t(Hpr,"roberta"),Hpr.forEach(r),z1t=t(t9e," \u2014 "),IX=s(t9e,"A",{href:!0});var Upr=n(IX);W1t=t(Upr,"FlaxRobertaModel"),Upr.forEach(r),Q1t=t(t9e," (RoBERTa model)"),t9e.forEach(r),H1t=i(Q),nE=s(Q,"LI",{});var r9e=n(nE);g2e=s(r9e,"STRONG",{});var Jpr=n(g2e);U1t=t(Jpr,"roformer"),Jpr.forEach(r),J1t=t(r9e," \u2014 "),jX=s(r9e,"A",{href:!0});var Ypr=n(jX);Y1t=t(Ypr,"FlaxRoFormerModel"),Ypr.forEach(r),K1t=t(r9e," (RoFormer model)"),r9e.forEach(r),Z1t=i(Q),lE=s(Q,"LI",{});var a9e=n(lE);h2e=s(a9e,"STRONG",{});var Kpr=n(h2e);eFt=t(Kpr,"t5"),Kpr.forEach(r),oFt=t(a9e," \u2014 "),DX=s(a9e,"A",{href:!0});var Zpr=n(DX);tFt=t(Zpr,"FlaxT5Model"),Zpr.forEach(r),rFt=t(a9e," (T5 model)"),a9e.forEach(r),aFt=i(Q),iE=s(Q,"LI",{});var s9e=n(iE);u2e=s(s9e,"STRONG",{});var e_r=n(u2e);sFt=t(e_r,"vision-text-dual-encoder"),e_r.forEach(r),nFt=t(s9e," \u2014 "),NX=s(s9e,"A",{href:!0});var o_r=n(NX);lFt=t(o_r,"FlaxVisionTextDualEncoderModel"),o_r.forEach(r),iFt=t(s9e," (VisionTextDualEncoder model)"),s9e.forEach(r),dFt=i(Q),dE=s(Q,"LI",{});var n9e=n(dE);p2e=s(n9e,"STRONG",{});var t_r=n(p2e);cFt=t(t_r,"vit"),t_r.forEach(r),mFt=t(n9e," \u2014 "),qX=s(n9e,"A",{href:!0});var r_r=n(qX);fFt=t(r_r,"FlaxViTModel"),r_r.forEach(r),gFt=t(n9e," (ViT model)"),n9e.forEach(r),hFt=i(Q),cE=s(Q,"LI",{});var l9e=n(cE);_2e=s(l9e,"STRONG",{});var a_r=n(_2e);uFt=t(a_r,"wav2vec2"),a_r.forEach(r),pFt=t(l9e," \u2014 "),OX=s(l9e,"A",{href:!0});var s_r=n(OX);_Ft=t(s_r,"FlaxWav2Vec2Model"),s_r.forEach(r),bFt=t(l9e," (Wav2Vec2 model)"),l9e.forEach(r),vFt=i(Q),mE=s(Q,"LI",{});var i9e=n(mE);b2e=s(i9e,"STRONG",{});var n_r=n(b2e);TFt=t(n_r,"xglm"),n_r.forEach(r),FFt=t(i9e," \u2014 "),GX=s(i9e,"A",{href:!0});var l_r=n(GX);CFt=t(l_r,"FlaxXGLMModel"),l_r.forEach(r),MFt=t(i9e," (XGLM model)"),i9e.forEach(r),Q.forEach(r),EFt=i(Aa),v2e=s(Aa,"P",{});var i_r=n(v2e);yFt=t(i_r,"Examples:"),i_r.forEach(r),wFt=i(Aa),f(dL.$$.fragment,Aa),Aa.forEach(r),mi.forEach(r),rke=i(d),em=s(d,"H2",{class:!0});var uSe=n(em);fE=s(uSe,"A",{id:!0,class:!0,href:!0});var d_r=n(fE);T2e=s(d_r,"SPAN",{});var c_r=n(T2e);f(cL.$$.fragment,c_r),c_r.forEach(r),d_r.forEach(r),AFt=i(uSe),F2e=s(uSe,"SPAN",{});var m_r=n(F2e);LFt=t(m_r,"FlaxAutoModelForCausalLM"),m_r.forEach(r),uSe.forEach(r),ake=i(d),kt=s(d,"DIV",{class:!0});var gi=n(kt);f(mL.$$.fragment,gi),BFt=i(gi),om=s(gi,"P",{});var LW=n(om);xFt=t(LW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),C2e=s(LW,"CODE",{});var f_r=n(C2e);kFt=t(f_r,"from_pretrained()"),f_r.forEach(r),RFt=t(LW,"class method or the "),M2e=s(LW,"CODE",{});var g_r=n(M2e);SFt=t(g_r,"from_config()"),g_r.forEach(r),PFt=t(LW,`class
method.`),LW.forEach(r),$Ft=i(gi),fL=s(gi,"P",{});var pSe=n(fL);IFt=t(pSe,"This class cannot be instantiated directly using "),E2e=s(pSe,"CODE",{});var h_r=n(E2e);jFt=t(h_r,"__init__()"),h_r.forEach(r),DFt=t(pSe," (throws an error)."),pSe.forEach(r),NFt=i(gi),yr=s(gi,"DIV",{class:!0});var hi=n(yr);f(gL.$$.fragment,hi),qFt=i(hi),y2e=s(hi,"P",{});var u_r=n(y2e);OFt=t(u_r,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),u_r.forEach(r),GFt=i(hi),tm=s(hi,"P",{});var BW=n(tm);XFt=t(BW,`Note:
Loading a model from its configuration file does `),w2e=s(BW,"STRONG",{});var p_r=n(w2e);VFt=t(p_r,"not"),p_r.forEach(r),zFt=t(BW,` load the model weights. It only affects the
model\u2019s configuration. Use `),A2e=s(BW,"CODE",{});var __r=n(A2e);WFt=t(__r,"from_pretrained()"),__r.forEach(r),QFt=t(BW,"to load the model weights."),BW.forEach(r),HFt=i(hi),L2e=s(hi,"P",{});var b_r=n(L2e);UFt=t(b_r,"Examples:"),b_r.forEach(r),JFt=i(hi),f(hL.$$.fragment,hi),hi.forEach(r),YFt=i(gi),Lo=s(gi,"DIV",{class:!0});var La=n(Lo);f(uL.$$.fragment,La),KFt=i(La),B2e=s(La,"P",{});var v_r=n(B2e);ZFt=t(v_r,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),v_r.forEach(r),eCt=i(La),Ls=s(La,"P",{});var w5=n(Ls);oCt=t(w5,"The model class to instantiate is selected based on the "),x2e=s(w5,"CODE",{});var T_r=n(x2e);tCt=t(T_r,"model_type"),T_r.forEach(r),rCt=t(w5,` property of the config object (either
passed as an argument or loaded from `),k2e=s(w5,"CODE",{});var F_r=n(k2e);aCt=t(F_r,"pretrained_model_name_or_path"),F_r.forEach(r),sCt=t(w5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),R2e=s(w5,"CODE",{});var C_r=n(R2e);nCt=t(C_r,"pretrained_model_name_or_path"),C_r.forEach(r),lCt=t(w5,":"),w5.forEach(r),iCt=i(La),Bs=s(La,"UL",{});var A5=n(Bs);gE=s(A5,"LI",{});var d9e=n(gE);S2e=s(d9e,"STRONG",{});var M_r=n(S2e);dCt=t(M_r,"gpt2"),M_r.forEach(r),cCt=t(d9e," \u2014 "),XX=s(d9e,"A",{href:!0});var E_r=n(XX);mCt=t(E_r,"FlaxGPT2LMHeadModel"),E_r.forEach(r),fCt=t(d9e," (OpenAI GPT-2 model)"),d9e.forEach(r),gCt=i(A5),hE=s(A5,"LI",{});var c9e=n(hE);P2e=s(c9e,"STRONG",{});var y_r=n(P2e);hCt=t(y_r,"gpt_neo"),y_r.forEach(r),uCt=t(c9e," \u2014 "),VX=s(c9e,"A",{href:!0});var w_r=n(VX);pCt=t(w_r,"FlaxGPTNeoForCausalLM"),w_r.forEach(r),_Ct=t(c9e," (GPT Neo model)"),c9e.forEach(r),bCt=i(A5),uE=s(A5,"LI",{});var m9e=n(uE);$2e=s(m9e,"STRONG",{});var A_r=n($2e);vCt=t(A_r,"gptj"),A_r.forEach(r),TCt=t(m9e," \u2014 "),zX=s(m9e,"A",{href:!0});var L_r=n(zX);FCt=t(L_r,"FlaxGPTJForCausalLM"),L_r.forEach(r),CCt=t(m9e," (GPT-J model)"),m9e.forEach(r),MCt=i(A5),pE=s(A5,"LI",{});var f9e=n(pE);I2e=s(f9e,"STRONG",{});var B_r=n(I2e);ECt=t(B_r,"xglm"),B_r.forEach(r),yCt=t(f9e," \u2014 "),WX=s(f9e,"A",{href:!0});var x_r=n(WX);wCt=t(x_r,"FlaxXGLMForCausalLM"),x_r.forEach(r),ACt=t(f9e," (XGLM model)"),f9e.forEach(r),A5.forEach(r),LCt=i(La),j2e=s(La,"P",{});var k_r=n(j2e);BCt=t(k_r,"Examples:"),k_r.forEach(r),xCt=i(La),f(pL.$$.fragment,La),La.forEach(r),gi.forEach(r),ske=i(d),rm=s(d,"H2",{class:!0});var _Se=n(rm);_E=s(_Se,"A",{id:!0,class:!0,href:!0});var R_r=n(_E);D2e=s(R_r,"SPAN",{});var S_r=n(D2e);f(_L.$$.fragment,S_r),S_r.forEach(r),R_r.forEach(r),kCt=i(_Se),N2e=s(_Se,"SPAN",{});var P_r=n(N2e);RCt=t(P_r,"FlaxAutoModelForPreTraining"),P_r.forEach(r),_Se.forEach(r),nke=i(d),Rt=s(d,"DIV",{class:!0});var ui=n(Rt);f(bL.$$.fragment,ui),SCt=i(ui),am=s(ui,"P",{});var xW=n(am);PCt=t(xW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),q2e=s(xW,"CODE",{});var $_r=n(q2e);$Ct=t($_r,"from_pretrained()"),$_r.forEach(r),ICt=t(xW,"class method or the "),O2e=s(xW,"CODE",{});var I_r=n(O2e);jCt=t(I_r,"from_config()"),I_r.forEach(r),DCt=t(xW,`class
method.`),xW.forEach(r),NCt=i(ui),vL=s(ui,"P",{});var bSe=n(vL);qCt=t(bSe,"This class cannot be instantiated directly using "),G2e=s(bSe,"CODE",{});var j_r=n(G2e);OCt=t(j_r,"__init__()"),j_r.forEach(r),GCt=t(bSe," (throws an error)."),bSe.forEach(r),XCt=i(ui),wr=s(ui,"DIV",{class:!0});var pi=n(wr);f(TL.$$.fragment,pi),VCt=i(pi),X2e=s(pi,"P",{});var D_r=n(X2e);zCt=t(D_r,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),D_r.forEach(r),WCt=i(pi),sm=s(pi,"P",{});var kW=n(sm);QCt=t(kW,`Note:
Loading a model from its configuration file does `),V2e=s(kW,"STRONG",{});var N_r=n(V2e);HCt=t(N_r,"not"),N_r.forEach(r),UCt=t(kW,` load the model weights. It only affects the
model\u2019s configuration. Use `),z2e=s(kW,"CODE",{});var q_r=n(z2e);JCt=t(q_r,"from_pretrained()"),q_r.forEach(r),YCt=t(kW,"to load the model weights."),kW.forEach(r),KCt=i(pi),W2e=s(pi,"P",{});var O_r=n(W2e);ZCt=t(O_r,"Examples:"),O_r.forEach(r),e4t=i(pi),f(FL.$$.fragment,pi),pi.forEach(r),o4t=i(ui),Bo=s(ui,"DIV",{class:!0});var Ba=n(Bo);f(CL.$$.fragment,Ba),t4t=i(Ba),Q2e=s(Ba,"P",{});var G_r=n(Q2e);r4t=t(G_r,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),G_r.forEach(r),a4t=i(Ba),xs=s(Ba,"P",{});var L5=n(xs);s4t=t(L5,"The model class to instantiate is selected based on the "),H2e=s(L5,"CODE",{});var X_r=n(H2e);n4t=t(X_r,"model_type"),X_r.forEach(r),l4t=t(L5,` property of the config object (either
passed as an argument or loaded from `),U2e=s(L5,"CODE",{});var V_r=n(U2e);i4t=t(V_r,"pretrained_model_name_or_path"),V_r.forEach(r),d4t=t(L5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),J2e=s(L5,"CODE",{});var z_r=n(J2e);c4t=t(z_r,"pretrained_model_name_or_path"),z_r.forEach(r),m4t=t(L5,":"),L5.forEach(r),f4t=i(Ba),me=s(Ba,"UL",{});var pe=n(me);bE=s(pe,"LI",{});var g9e=n(bE);Y2e=s(g9e,"STRONG",{});var W_r=n(Y2e);g4t=t(W_r,"albert"),W_r.forEach(r),h4t=t(g9e," \u2014 "),QX=s(g9e,"A",{href:!0});var Q_r=n(QX);u4t=t(Q_r,"FlaxAlbertForPreTraining"),Q_r.forEach(r),p4t=t(g9e," (ALBERT model)"),g9e.forEach(r),_4t=i(pe),vE=s(pe,"LI",{});var h9e=n(vE);K2e=s(h9e,"STRONG",{});var H_r=n(K2e);b4t=t(H_r,"bart"),H_r.forEach(r),v4t=t(h9e," \u2014 "),HX=s(h9e,"A",{href:!0});var U_r=n(HX);T4t=t(U_r,"FlaxBartForConditionalGeneration"),U_r.forEach(r),F4t=t(h9e," (BART model)"),h9e.forEach(r),C4t=i(pe),TE=s(pe,"LI",{});var u9e=n(TE);Z2e=s(u9e,"STRONG",{});var J_r=n(Z2e);M4t=t(J_r,"bert"),J_r.forEach(r),E4t=t(u9e," \u2014 "),UX=s(u9e,"A",{href:!0});var Y_r=n(UX);y4t=t(Y_r,"FlaxBertForPreTraining"),Y_r.forEach(r),w4t=t(u9e," (BERT model)"),u9e.forEach(r),A4t=i(pe),FE=s(pe,"LI",{});var p9e=n(FE);eve=s(p9e,"STRONG",{});var K_r=n(eve);L4t=t(K_r,"big_bird"),K_r.forEach(r),B4t=t(p9e," \u2014 "),JX=s(p9e,"A",{href:!0});var Z_r=n(JX);x4t=t(Z_r,"FlaxBigBirdForPreTraining"),Z_r.forEach(r),k4t=t(p9e," (BigBird model)"),p9e.forEach(r),R4t=i(pe),CE=s(pe,"LI",{});var _9e=n(CE);ove=s(_9e,"STRONG",{});var ebr=n(ove);S4t=t(ebr,"electra"),ebr.forEach(r),P4t=t(_9e," \u2014 "),YX=s(_9e,"A",{href:!0});var obr=n(YX);$4t=t(obr,"FlaxElectraForPreTraining"),obr.forEach(r),I4t=t(_9e," (ELECTRA model)"),_9e.forEach(r),j4t=i(pe),ME=s(pe,"LI",{});var b9e=n(ME);tve=s(b9e,"STRONG",{});var tbr=n(tve);D4t=t(tbr,"mbart"),tbr.forEach(r),N4t=t(b9e," \u2014 "),KX=s(b9e,"A",{href:!0});var rbr=n(KX);q4t=t(rbr,"FlaxMBartForConditionalGeneration"),rbr.forEach(r),O4t=t(b9e," (mBART model)"),b9e.forEach(r),G4t=i(pe),EE=s(pe,"LI",{});var v9e=n(EE);rve=s(v9e,"STRONG",{});var abr=n(rve);X4t=t(abr,"mt5"),abr.forEach(r),V4t=t(v9e," \u2014 "),ZX=s(v9e,"A",{href:!0});var sbr=n(ZX);z4t=t(sbr,"FlaxMT5ForConditionalGeneration"),sbr.forEach(r),W4t=t(v9e," (mT5 model)"),v9e.forEach(r),Q4t=i(pe),yE=s(pe,"LI",{});var T9e=n(yE);ave=s(T9e,"STRONG",{});var nbr=n(ave);H4t=t(nbr,"roberta"),nbr.forEach(r),U4t=t(T9e," \u2014 "),eV=s(T9e,"A",{href:!0});var lbr=n(eV);J4t=t(lbr,"FlaxRobertaForMaskedLM"),lbr.forEach(r),Y4t=t(T9e," (RoBERTa model)"),T9e.forEach(r),K4t=i(pe),wE=s(pe,"LI",{});var F9e=n(wE);sve=s(F9e,"STRONG",{});var ibr=n(sve);Z4t=t(ibr,"roformer"),ibr.forEach(r),eMt=t(F9e," \u2014 "),oV=s(F9e,"A",{href:!0});var dbr=n(oV);oMt=t(dbr,"FlaxRoFormerForMaskedLM"),dbr.forEach(r),tMt=t(F9e," (RoFormer model)"),F9e.forEach(r),rMt=i(pe),AE=s(pe,"LI",{});var C9e=n(AE);nve=s(C9e,"STRONG",{});var cbr=n(nve);aMt=t(cbr,"t5"),cbr.forEach(r),sMt=t(C9e," \u2014 "),tV=s(C9e,"A",{href:!0});var mbr=n(tV);nMt=t(mbr,"FlaxT5ForConditionalGeneration"),mbr.forEach(r),lMt=t(C9e," (T5 model)"),C9e.forEach(r),iMt=i(pe),LE=s(pe,"LI",{});var M9e=n(LE);lve=s(M9e,"STRONG",{});var fbr=n(lve);dMt=t(fbr,"wav2vec2"),fbr.forEach(r),cMt=t(M9e," \u2014 "),rV=s(M9e,"A",{href:!0});var gbr=n(rV);mMt=t(gbr,"FlaxWav2Vec2ForPreTraining"),gbr.forEach(r),fMt=t(M9e," (Wav2Vec2 model)"),M9e.forEach(r),pe.forEach(r),gMt=i(Ba),ive=s(Ba,"P",{});var hbr=n(ive);hMt=t(hbr,"Examples:"),hbr.forEach(r),uMt=i(Ba),f(ML.$$.fragment,Ba),Ba.forEach(r),ui.forEach(r),lke=i(d),nm=s(d,"H2",{class:!0});var vSe=n(nm);BE=s(vSe,"A",{id:!0,class:!0,href:!0});var ubr=n(BE);dve=s(ubr,"SPAN",{});var pbr=n(dve);f(EL.$$.fragment,pbr),pbr.forEach(r),ubr.forEach(r),pMt=i(vSe),cve=s(vSe,"SPAN",{});var _br=n(cve);_Mt=t(_br,"FlaxAutoModelForMaskedLM"),_br.forEach(r),vSe.forEach(r),ike=i(d),St=s(d,"DIV",{class:!0});var _i=n(St);f(yL.$$.fragment,_i),bMt=i(_i),lm=s(_i,"P",{});var RW=n(lm);vMt=t(RW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),mve=s(RW,"CODE",{});var bbr=n(mve);TMt=t(bbr,"from_pretrained()"),bbr.forEach(r),FMt=t(RW,"class method or the "),fve=s(RW,"CODE",{});var vbr=n(fve);CMt=t(vbr,"from_config()"),vbr.forEach(r),MMt=t(RW,`class
method.`),RW.forEach(r),EMt=i(_i),wL=s(_i,"P",{});var TSe=n(wL);yMt=t(TSe,"This class cannot be instantiated directly using "),gve=s(TSe,"CODE",{});var Tbr=n(gve);wMt=t(Tbr,"__init__()"),Tbr.forEach(r),AMt=t(TSe," (throws an error)."),TSe.forEach(r),LMt=i(_i),Ar=s(_i,"DIV",{class:!0});var bi=n(Ar);f(AL.$$.fragment,bi),BMt=i(bi),hve=s(bi,"P",{});var Fbr=n(hve);xMt=t(Fbr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Fbr.forEach(r),kMt=i(bi),im=s(bi,"P",{});var SW=n(im);RMt=t(SW,`Note:
Loading a model from its configuration file does `),uve=s(SW,"STRONG",{});var Cbr=n(uve);SMt=t(Cbr,"not"),Cbr.forEach(r),PMt=t(SW,` load the model weights. It only affects the
model\u2019s configuration. Use `),pve=s(SW,"CODE",{});var Mbr=n(pve);$Mt=t(Mbr,"from_pretrained()"),Mbr.forEach(r),IMt=t(SW,"to load the model weights."),SW.forEach(r),jMt=i(bi),_ve=s(bi,"P",{});var Ebr=n(_ve);DMt=t(Ebr,"Examples:"),Ebr.forEach(r),NMt=i(bi),f(LL.$$.fragment,bi),bi.forEach(r),qMt=i(_i),xo=s(_i,"DIV",{class:!0});var xa=n(xo);f(BL.$$.fragment,xa),OMt=i(xa),bve=s(xa,"P",{});var ybr=n(bve);GMt=t(ybr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),ybr.forEach(r),XMt=i(xa),ks=s(xa,"P",{});var B5=n(ks);VMt=t(B5,"The model class to instantiate is selected based on the "),vve=s(B5,"CODE",{});var wbr=n(vve);zMt=t(wbr,"model_type"),wbr.forEach(r),WMt=t(B5,` property of the config object (either
passed as an argument or loaded from `),Tve=s(B5,"CODE",{});var Abr=n(Tve);QMt=t(Abr,"pretrained_model_name_or_path"),Abr.forEach(r),HMt=t(B5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fve=s(B5,"CODE",{});var Lbr=n(Fve);UMt=t(Lbr,"pretrained_model_name_or_path"),Lbr.forEach(r),JMt=t(B5,":"),B5.forEach(r),YMt=i(xa),ve=s(xa,"UL",{});var to=n(ve);xE=s(to,"LI",{});var E9e=n(xE);Cve=s(E9e,"STRONG",{});var Bbr=n(Cve);KMt=t(Bbr,"albert"),Bbr.forEach(r),ZMt=t(E9e," \u2014 "),aV=s(E9e,"A",{href:!0});var xbr=n(aV);eEt=t(xbr,"FlaxAlbertForMaskedLM"),xbr.forEach(r),oEt=t(E9e," (ALBERT model)"),E9e.forEach(r),tEt=i(to),kE=s(to,"LI",{});var y9e=n(kE);Mve=s(y9e,"STRONG",{});var kbr=n(Mve);rEt=t(kbr,"bart"),kbr.forEach(r),aEt=t(y9e," \u2014 "),sV=s(y9e,"A",{href:!0});var Rbr=n(sV);sEt=t(Rbr,"FlaxBartForConditionalGeneration"),Rbr.forEach(r),nEt=t(y9e," (BART model)"),y9e.forEach(r),lEt=i(to),RE=s(to,"LI",{});var w9e=n(RE);Eve=s(w9e,"STRONG",{});var Sbr=n(Eve);iEt=t(Sbr,"bert"),Sbr.forEach(r),dEt=t(w9e," \u2014 "),nV=s(w9e,"A",{href:!0});var Pbr=n(nV);cEt=t(Pbr,"FlaxBertForMaskedLM"),Pbr.forEach(r),mEt=t(w9e," (BERT model)"),w9e.forEach(r),fEt=i(to),SE=s(to,"LI",{});var A9e=n(SE);yve=s(A9e,"STRONG",{});var $br=n(yve);gEt=t($br,"big_bird"),$br.forEach(r),hEt=t(A9e," \u2014 "),lV=s(A9e,"A",{href:!0});var Ibr=n(lV);uEt=t(Ibr,"FlaxBigBirdForMaskedLM"),Ibr.forEach(r),pEt=t(A9e," (BigBird model)"),A9e.forEach(r),_Et=i(to),PE=s(to,"LI",{});var L9e=n(PE);wve=s(L9e,"STRONG",{});var jbr=n(wve);bEt=t(jbr,"distilbert"),jbr.forEach(r),vEt=t(L9e," \u2014 "),iV=s(L9e,"A",{href:!0});var Dbr=n(iV);TEt=t(Dbr,"FlaxDistilBertForMaskedLM"),Dbr.forEach(r),FEt=t(L9e," (DistilBERT model)"),L9e.forEach(r),CEt=i(to),$E=s(to,"LI",{});var B9e=n($E);Ave=s(B9e,"STRONG",{});var Nbr=n(Ave);MEt=t(Nbr,"electra"),Nbr.forEach(r),EEt=t(B9e," \u2014 "),dV=s(B9e,"A",{href:!0});var qbr=n(dV);yEt=t(qbr,"FlaxElectraForMaskedLM"),qbr.forEach(r),wEt=t(B9e," (ELECTRA model)"),B9e.forEach(r),AEt=i(to),IE=s(to,"LI",{});var x9e=n(IE);Lve=s(x9e,"STRONG",{});var Obr=n(Lve);LEt=t(Obr,"mbart"),Obr.forEach(r),BEt=t(x9e," \u2014 "),cV=s(x9e,"A",{href:!0});var Gbr=n(cV);xEt=t(Gbr,"FlaxMBartForConditionalGeneration"),Gbr.forEach(r),kEt=t(x9e," (mBART model)"),x9e.forEach(r),REt=i(to),jE=s(to,"LI",{});var k9e=n(jE);Bve=s(k9e,"STRONG",{});var Xbr=n(Bve);SEt=t(Xbr,"roberta"),Xbr.forEach(r),PEt=t(k9e," \u2014 "),mV=s(k9e,"A",{href:!0});var Vbr=n(mV);$Et=t(Vbr,"FlaxRobertaForMaskedLM"),Vbr.forEach(r),IEt=t(k9e," (RoBERTa model)"),k9e.forEach(r),jEt=i(to),DE=s(to,"LI",{});var R9e=n(DE);xve=s(R9e,"STRONG",{});var zbr=n(xve);DEt=t(zbr,"roformer"),zbr.forEach(r),NEt=t(R9e," \u2014 "),fV=s(R9e,"A",{href:!0});var Wbr=n(fV);qEt=t(Wbr,"FlaxRoFormerForMaskedLM"),Wbr.forEach(r),OEt=t(R9e," (RoFormer model)"),R9e.forEach(r),to.forEach(r),GEt=i(xa),kve=s(xa,"P",{});var Qbr=n(kve);XEt=t(Qbr,"Examples:"),Qbr.forEach(r),VEt=i(xa),f(xL.$$.fragment,xa),xa.forEach(r),_i.forEach(r),dke=i(d),dm=s(d,"H2",{class:!0});var FSe=n(dm);NE=s(FSe,"A",{id:!0,class:!0,href:!0});var Hbr=n(NE);Rve=s(Hbr,"SPAN",{});var Ubr=n(Rve);f(kL.$$.fragment,Ubr),Ubr.forEach(r),Hbr.forEach(r),zEt=i(FSe),Sve=s(FSe,"SPAN",{});var Jbr=n(Sve);WEt=t(Jbr,"FlaxAutoModelForSeq2SeqLM"),Jbr.forEach(r),FSe.forEach(r),cke=i(d),Pt=s(d,"DIV",{class:!0});var vi=n(Pt);f(RL.$$.fragment,vi),QEt=i(vi),cm=s(vi,"P",{});var PW=n(cm);HEt=t(PW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pve=s(PW,"CODE",{});var Ybr=n(Pve);UEt=t(Ybr,"from_pretrained()"),Ybr.forEach(r),JEt=t(PW,"class method or the "),$ve=s(PW,"CODE",{});var Kbr=n($ve);YEt=t(Kbr,"from_config()"),Kbr.forEach(r),KEt=t(PW,`class
method.`),PW.forEach(r),ZEt=i(vi),SL=s(vi,"P",{});var CSe=n(SL);e3t=t(CSe,"This class cannot be instantiated directly using "),Ive=s(CSe,"CODE",{});var Zbr=n(Ive);o3t=t(Zbr,"__init__()"),Zbr.forEach(r),t3t=t(CSe," (throws an error)."),CSe.forEach(r),r3t=i(vi),Lr=s(vi,"DIV",{class:!0});var Ti=n(Lr);f(PL.$$.fragment,Ti),a3t=i(Ti),jve=s(Ti,"P",{});var e2r=n(jve);s3t=t(e2r,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),e2r.forEach(r),n3t=i(Ti),mm=s(Ti,"P",{});var $W=n(mm);l3t=t($W,`Note:
Loading a model from its configuration file does `),Dve=s($W,"STRONG",{});var o2r=n(Dve);i3t=t(o2r,"not"),o2r.forEach(r),d3t=t($W,` load the model weights. It only affects the
model\u2019s configuration. Use `),Nve=s($W,"CODE",{});var t2r=n(Nve);c3t=t(t2r,"from_pretrained()"),t2r.forEach(r),m3t=t($W,"to load the model weights."),$W.forEach(r),f3t=i(Ti),qve=s(Ti,"P",{});var r2r=n(qve);g3t=t(r2r,"Examples:"),r2r.forEach(r),h3t=i(Ti),f($L.$$.fragment,Ti),Ti.forEach(r),u3t=i(vi),ko=s(vi,"DIV",{class:!0});var ka=n(ko);f(IL.$$.fragment,ka),p3t=i(ka),Ove=s(ka,"P",{});var a2r=n(Ove);_3t=t(a2r,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),a2r.forEach(r),b3t=i(ka),Rs=s(ka,"P",{});var x5=n(Rs);v3t=t(x5,"The model class to instantiate is selected based on the "),Gve=s(x5,"CODE",{});var s2r=n(Gve);T3t=t(s2r,"model_type"),s2r.forEach(r),F3t=t(x5,` property of the config object (either
passed as an argument or loaded from `),Xve=s(x5,"CODE",{});var n2r=n(Xve);C3t=t(n2r,"pretrained_model_name_or_path"),n2r.forEach(r),M3t=t(x5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vve=s(x5,"CODE",{});var l2r=n(Vve);E3t=t(l2r,"pretrained_model_name_or_path"),l2r.forEach(r),y3t=t(x5,":"),x5.forEach(r),w3t=i(ka),Te=s(ka,"UL",{});var ro=n(Te);qE=s(ro,"LI",{});var S9e=n(qE);zve=s(S9e,"STRONG",{});var i2r=n(zve);A3t=t(i2r,"bart"),i2r.forEach(r),L3t=t(S9e," \u2014 "),gV=s(S9e,"A",{href:!0});var d2r=n(gV);B3t=t(d2r,"FlaxBartForConditionalGeneration"),d2r.forEach(r),x3t=t(S9e," (BART model)"),S9e.forEach(r),k3t=i(ro),OE=s(ro,"LI",{});var P9e=n(OE);Wve=s(P9e,"STRONG",{});var c2r=n(Wve);R3t=t(c2r,"blenderbot"),c2r.forEach(r),S3t=t(P9e," \u2014 "),hV=s(P9e,"A",{href:!0});var m2r=n(hV);P3t=t(m2r,"FlaxBlenderbotForConditionalGeneration"),m2r.forEach(r),$3t=t(P9e," (Blenderbot model)"),P9e.forEach(r),I3t=i(ro),GE=s(ro,"LI",{});var $9e=n(GE);Qve=s($9e,"STRONG",{});var f2r=n(Qve);j3t=t(f2r,"blenderbot-small"),f2r.forEach(r),D3t=t($9e," \u2014 "),uV=s($9e,"A",{href:!0});var g2r=n(uV);N3t=t(g2r,"FlaxBlenderbotSmallForConditionalGeneration"),g2r.forEach(r),q3t=t($9e," (BlenderbotSmall model)"),$9e.forEach(r),O3t=i(ro),XE=s(ro,"LI",{});var I9e=n(XE);Hve=s(I9e,"STRONG",{});var h2r=n(Hve);G3t=t(h2r,"encoder-decoder"),h2r.forEach(r),X3t=t(I9e," \u2014 "),pV=s(I9e,"A",{href:!0});var u2r=n(pV);V3t=t(u2r,"FlaxEncoderDecoderModel"),u2r.forEach(r),z3t=t(I9e," (Encoder decoder model)"),I9e.forEach(r),W3t=i(ro),VE=s(ro,"LI",{});var j9e=n(VE);Uve=s(j9e,"STRONG",{});var p2r=n(Uve);Q3t=t(p2r,"marian"),p2r.forEach(r),H3t=t(j9e," \u2014 "),_V=s(j9e,"A",{href:!0});var _2r=n(_V);U3t=t(_2r,"FlaxMarianMTModel"),_2r.forEach(r),J3t=t(j9e," (Marian model)"),j9e.forEach(r),Y3t=i(ro),zE=s(ro,"LI",{});var D9e=n(zE);Jve=s(D9e,"STRONG",{});var b2r=n(Jve);K3t=t(b2r,"mbart"),b2r.forEach(r),Z3t=t(D9e," \u2014 "),bV=s(D9e,"A",{href:!0});var v2r=n(bV);e5t=t(v2r,"FlaxMBartForConditionalGeneration"),v2r.forEach(r),o5t=t(D9e," (mBART model)"),D9e.forEach(r),t5t=i(ro),WE=s(ro,"LI",{});var N9e=n(WE);Yve=s(N9e,"STRONG",{});var T2r=n(Yve);r5t=t(T2r,"mt5"),T2r.forEach(r),a5t=t(N9e," \u2014 "),vV=s(N9e,"A",{href:!0});var F2r=n(vV);s5t=t(F2r,"FlaxMT5ForConditionalGeneration"),F2r.forEach(r),n5t=t(N9e," (mT5 model)"),N9e.forEach(r),l5t=i(ro),QE=s(ro,"LI",{});var q9e=n(QE);Kve=s(q9e,"STRONG",{});var C2r=n(Kve);i5t=t(C2r,"pegasus"),C2r.forEach(r),d5t=t(q9e," \u2014 "),TV=s(q9e,"A",{href:!0});var M2r=n(TV);c5t=t(M2r,"FlaxPegasusForConditionalGeneration"),M2r.forEach(r),m5t=t(q9e," (Pegasus model)"),q9e.forEach(r),f5t=i(ro),HE=s(ro,"LI",{});var O9e=n(HE);Zve=s(O9e,"STRONG",{});var E2r=n(Zve);g5t=t(E2r,"t5"),E2r.forEach(r),h5t=t(O9e," \u2014 "),FV=s(O9e,"A",{href:!0});var y2r=n(FV);u5t=t(y2r,"FlaxT5ForConditionalGeneration"),y2r.forEach(r),p5t=t(O9e," (T5 model)"),O9e.forEach(r),ro.forEach(r),_5t=i(ka),eTe=s(ka,"P",{});var w2r=n(eTe);b5t=t(w2r,"Examples:"),w2r.forEach(r),v5t=i(ka),f(jL.$$.fragment,ka),ka.forEach(r),vi.forEach(r),mke=i(d),fm=s(d,"H2",{class:!0});var MSe=n(fm);UE=s(MSe,"A",{id:!0,class:!0,href:!0});var A2r=n(UE);oTe=s(A2r,"SPAN",{});var L2r=n(oTe);f(DL.$$.fragment,L2r),L2r.forEach(r),A2r.forEach(r),T5t=i(MSe),tTe=s(MSe,"SPAN",{});var B2r=n(tTe);F5t=t(B2r,"FlaxAutoModelForSequenceClassification"),B2r.forEach(r),MSe.forEach(r),fke=i(d),$t=s(d,"DIV",{class:!0});var Fi=n($t);f(NL.$$.fragment,Fi),C5t=i(Fi),gm=s(Fi,"P",{});var IW=n(gm);M5t=t(IW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),rTe=s(IW,"CODE",{});var x2r=n(rTe);E5t=t(x2r,"from_pretrained()"),x2r.forEach(r),y5t=t(IW,"class method or the "),aTe=s(IW,"CODE",{});var k2r=n(aTe);w5t=t(k2r,"from_config()"),k2r.forEach(r),A5t=t(IW,`class
method.`),IW.forEach(r),L5t=i(Fi),qL=s(Fi,"P",{});var ESe=n(qL);B5t=t(ESe,"This class cannot be instantiated directly using "),sTe=s(ESe,"CODE",{});var R2r=n(sTe);x5t=t(R2r,"__init__()"),R2r.forEach(r),k5t=t(ESe," (throws an error)."),ESe.forEach(r),R5t=i(Fi),Br=s(Fi,"DIV",{class:!0});var Ci=n(Br);f(OL.$$.fragment,Ci),S5t=i(Ci),nTe=s(Ci,"P",{});var S2r=n(nTe);P5t=t(S2r,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),S2r.forEach(r),$5t=i(Ci),hm=s(Ci,"P",{});var jW=n(hm);I5t=t(jW,`Note:
Loading a model from its configuration file does `),lTe=s(jW,"STRONG",{});var P2r=n(lTe);j5t=t(P2r,"not"),P2r.forEach(r),D5t=t(jW,` load the model weights. It only affects the
model\u2019s configuration. Use `),iTe=s(jW,"CODE",{});var $2r=n(iTe);N5t=t($2r,"from_pretrained()"),$2r.forEach(r),q5t=t(jW,"to load the model weights."),jW.forEach(r),O5t=i(Ci),dTe=s(Ci,"P",{});var I2r=n(dTe);G5t=t(I2r,"Examples:"),I2r.forEach(r),X5t=i(Ci),f(GL.$$.fragment,Ci),Ci.forEach(r),V5t=i(Fi),Ro=s(Fi,"DIV",{class:!0});var Ra=n(Ro);f(XL.$$.fragment,Ra),z5t=i(Ra),cTe=s(Ra,"P",{});var j2r=n(cTe);W5t=t(j2r,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),j2r.forEach(r),Q5t=i(Ra),Ss=s(Ra,"P",{});var k5=n(Ss);H5t=t(k5,"The model class to instantiate is selected based on the "),mTe=s(k5,"CODE",{});var D2r=n(mTe);U5t=t(D2r,"model_type"),D2r.forEach(r),J5t=t(k5,` property of the config object (either
passed as an argument or loaded from `),fTe=s(k5,"CODE",{});var N2r=n(fTe);Y5t=t(N2r,"pretrained_model_name_or_path"),N2r.forEach(r),K5t=t(k5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gTe=s(k5,"CODE",{});var q2r=n(gTe);Z5t=t(q2r,"pretrained_model_name_or_path"),q2r.forEach(r),eyt=t(k5,":"),k5.forEach(r),oyt=i(Ra),Fe=s(Ra,"UL",{});var ao=n(Fe);JE=s(ao,"LI",{});var G9e=n(JE);hTe=s(G9e,"STRONG",{});var O2r=n(hTe);tyt=t(O2r,"albert"),O2r.forEach(r),ryt=t(G9e," \u2014 "),CV=s(G9e,"A",{href:!0});var G2r=n(CV);ayt=t(G2r,"FlaxAlbertForSequenceClassification"),G2r.forEach(r),syt=t(G9e," (ALBERT model)"),G9e.forEach(r),nyt=i(ao),YE=s(ao,"LI",{});var X9e=n(YE);uTe=s(X9e,"STRONG",{});var X2r=n(uTe);lyt=t(X2r,"bart"),X2r.forEach(r),iyt=t(X9e," \u2014 "),MV=s(X9e,"A",{href:!0});var V2r=n(MV);dyt=t(V2r,"FlaxBartForSequenceClassification"),V2r.forEach(r),cyt=t(X9e," (BART model)"),X9e.forEach(r),myt=i(ao),KE=s(ao,"LI",{});var V9e=n(KE);pTe=s(V9e,"STRONG",{});var z2r=n(pTe);fyt=t(z2r,"bert"),z2r.forEach(r),gyt=t(V9e," \u2014 "),EV=s(V9e,"A",{href:!0});var W2r=n(EV);hyt=t(W2r,"FlaxBertForSequenceClassification"),W2r.forEach(r),uyt=t(V9e," (BERT model)"),V9e.forEach(r),pyt=i(ao),ZE=s(ao,"LI",{});var z9e=n(ZE);_Te=s(z9e,"STRONG",{});var Q2r=n(_Te);_yt=t(Q2r,"big_bird"),Q2r.forEach(r),byt=t(z9e," \u2014 "),yV=s(z9e,"A",{href:!0});var H2r=n(yV);vyt=t(H2r,"FlaxBigBirdForSequenceClassification"),H2r.forEach(r),Tyt=t(z9e," (BigBird model)"),z9e.forEach(r),Fyt=i(ao),e3=s(ao,"LI",{});var W9e=n(e3);bTe=s(W9e,"STRONG",{});var U2r=n(bTe);Cyt=t(U2r,"distilbert"),U2r.forEach(r),Myt=t(W9e," \u2014 "),wV=s(W9e,"A",{href:!0});var J2r=n(wV);Eyt=t(J2r,"FlaxDistilBertForSequenceClassification"),J2r.forEach(r),yyt=t(W9e," (DistilBERT model)"),W9e.forEach(r),wyt=i(ao),o3=s(ao,"LI",{});var Q9e=n(o3);vTe=s(Q9e,"STRONG",{});var Y2r=n(vTe);Ayt=t(Y2r,"electra"),Y2r.forEach(r),Lyt=t(Q9e," \u2014 "),AV=s(Q9e,"A",{href:!0});var K2r=n(AV);Byt=t(K2r,"FlaxElectraForSequenceClassification"),K2r.forEach(r),xyt=t(Q9e," (ELECTRA model)"),Q9e.forEach(r),kyt=i(ao),t3=s(ao,"LI",{});var H9e=n(t3);TTe=s(H9e,"STRONG",{});var Z2r=n(TTe);Ryt=t(Z2r,"mbart"),Z2r.forEach(r),Syt=t(H9e," \u2014 "),LV=s(H9e,"A",{href:!0});var evr=n(LV);Pyt=t(evr,"FlaxMBartForSequenceClassification"),evr.forEach(r),$yt=t(H9e," (mBART model)"),H9e.forEach(r),Iyt=i(ao),r3=s(ao,"LI",{});var U9e=n(r3);FTe=s(U9e,"STRONG",{});var ovr=n(FTe);jyt=t(ovr,"roberta"),ovr.forEach(r),Dyt=t(U9e," \u2014 "),BV=s(U9e,"A",{href:!0});var tvr=n(BV);Nyt=t(tvr,"FlaxRobertaForSequenceClassification"),tvr.forEach(r),qyt=t(U9e," (RoBERTa model)"),U9e.forEach(r),Oyt=i(ao),a3=s(ao,"LI",{});var J9e=n(a3);CTe=s(J9e,"STRONG",{});var rvr=n(CTe);Gyt=t(rvr,"roformer"),rvr.forEach(r),Xyt=t(J9e," \u2014 "),xV=s(J9e,"A",{href:!0});var avr=n(xV);Vyt=t(avr,"FlaxRoFormerForSequenceClassification"),avr.forEach(r),zyt=t(J9e," (RoFormer model)"),J9e.forEach(r),ao.forEach(r),Wyt=i(Ra),MTe=s(Ra,"P",{});var svr=n(MTe);Qyt=t(svr,"Examples:"),svr.forEach(r),Hyt=i(Ra),f(VL.$$.fragment,Ra),Ra.forEach(r),Fi.forEach(r),gke=i(d),um=s(d,"H2",{class:!0});var ySe=n(um);s3=s(ySe,"A",{id:!0,class:!0,href:!0});var nvr=n(s3);ETe=s(nvr,"SPAN",{});var lvr=n(ETe);f(zL.$$.fragment,lvr),lvr.forEach(r),nvr.forEach(r),Uyt=i(ySe),yTe=s(ySe,"SPAN",{});var ivr=n(yTe);Jyt=t(ivr,"FlaxAutoModelForQuestionAnswering"),ivr.forEach(r),ySe.forEach(r),hke=i(d),It=s(d,"DIV",{class:!0});var Mi=n(It);f(WL.$$.fragment,Mi),Yyt=i(Mi),pm=s(Mi,"P",{});var DW=n(pm);Kyt=t(DW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),wTe=s(DW,"CODE",{});var dvr=n(wTe);Zyt=t(dvr,"from_pretrained()"),dvr.forEach(r),ewt=t(DW,"class method or the "),ATe=s(DW,"CODE",{});var cvr=n(ATe);owt=t(cvr,"from_config()"),cvr.forEach(r),twt=t(DW,`class
method.`),DW.forEach(r),rwt=i(Mi),QL=s(Mi,"P",{});var wSe=n(QL);awt=t(wSe,"This class cannot be instantiated directly using "),LTe=s(wSe,"CODE",{});var mvr=n(LTe);swt=t(mvr,"__init__()"),mvr.forEach(r),nwt=t(wSe," (throws an error)."),wSe.forEach(r),lwt=i(Mi),xr=s(Mi,"DIV",{class:!0});var Ei=n(xr);f(HL.$$.fragment,Ei),iwt=i(Ei),BTe=s(Ei,"P",{});var fvr=n(BTe);dwt=t(fvr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),fvr.forEach(r),cwt=i(Ei),_m=s(Ei,"P",{});var NW=n(_m);mwt=t(NW,`Note:
Loading a model from its configuration file does `),xTe=s(NW,"STRONG",{});var gvr=n(xTe);fwt=t(gvr,"not"),gvr.forEach(r),gwt=t(NW,` load the model weights. It only affects the
model\u2019s configuration. Use `),kTe=s(NW,"CODE",{});var hvr=n(kTe);hwt=t(hvr,"from_pretrained()"),hvr.forEach(r),uwt=t(NW,"to load the model weights."),NW.forEach(r),pwt=i(Ei),RTe=s(Ei,"P",{});var uvr=n(RTe);_wt=t(uvr,"Examples:"),uvr.forEach(r),bwt=i(Ei),f(UL.$$.fragment,Ei),Ei.forEach(r),vwt=i(Mi),So=s(Mi,"DIV",{class:!0});var Sa=n(So);f(JL.$$.fragment,Sa),Twt=i(Sa),STe=s(Sa,"P",{});var pvr=n(STe);Fwt=t(pvr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),pvr.forEach(r),Cwt=i(Sa),Ps=s(Sa,"P",{});var R5=n(Ps);Mwt=t(R5,"The model class to instantiate is selected based on the "),PTe=s(R5,"CODE",{});var _vr=n(PTe);Ewt=t(_vr,"model_type"),_vr.forEach(r),ywt=t(R5,` property of the config object (either
passed as an argument or loaded from `),$Te=s(R5,"CODE",{});var bvr=n($Te);wwt=t(bvr,"pretrained_model_name_or_path"),bvr.forEach(r),Awt=t(R5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ITe=s(R5,"CODE",{});var vvr=n(ITe);Lwt=t(vvr,"pretrained_model_name_or_path"),vvr.forEach(r),Bwt=t(R5,":"),R5.forEach(r),xwt=i(Sa),Ce=s(Sa,"UL",{});var so=n(Ce);n3=s(so,"LI",{});var Y9e=n(n3);jTe=s(Y9e,"STRONG",{});var Tvr=n(jTe);kwt=t(Tvr,"albert"),Tvr.forEach(r),Rwt=t(Y9e," \u2014 "),kV=s(Y9e,"A",{href:!0});var Fvr=n(kV);Swt=t(Fvr,"FlaxAlbertForQuestionAnswering"),Fvr.forEach(r),Pwt=t(Y9e," (ALBERT model)"),Y9e.forEach(r),$wt=i(so),l3=s(so,"LI",{});var K9e=n(l3);DTe=s(K9e,"STRONG",{});var Cvr=n(DTe);Iwt=t(Cvr,"bart"),Cvr.forEach(r),jwt=t(K9e," \u2014 "),RV=s(K9e,"A",{href:!0});var Mvr=n(RV);Dwt=t(Mvr,"FlaxBartForQuestionAnswering"),Mvr.forEach(r),Nwt=t(K9e," (BART model)"),K9e.forEach(r),qwt=i(so),i3=s(so,"LI",{});var Z9e=n(i3);NTe=s(Z9e,"STRONG",{});var Evr=n(NTe);Owt=t(Evr,"bert"),Evr.forEach(r),Gwt=t(Z9e," \u2014 "),SV=s(Z9e,"A",{href:!0});var yvr=n(SV);Xwt=t(yvr,"FlaxBertForQuestionAnswering"),yvr.forEach(r),Vwt=t(Z9e," (BERT model)"),Z9e.forEach(r),zwt=i(so),d3=s(so,"LI",{});var eBe=n(d3);qTe=s(eBe,"STRONG",{});var wvr=n(qTe);Wwt=t(wvr,"big_bird"),wvr.forEach(r),Qwt=t(eBe," \u2014 "),PV=s(eBe,"A",{href:!0});var Avr=n(PV);Hwt=t(Avr,"FlaxBigBirdForQuestionAnswering"),Avr.forEach(r),Uwt=t(eBe," (BigBird model)"),eBe.forEach(r),Jwt=i(so),c3=s(so,"LI",{});var oBe=n(c3);OTe=s(oBe,"STRONG",{});var Lvr=n(OTe);Ywt=t(Lvr,"distilbert"),Lvr.forEach(r),Kwt=t(oBe," \u2014 "),$V=s(oBe,"A",{href:!0});var Bvr=n($V);Zwt=t(Bvr,"FlaxDistilBertForQuestionAnswering"),Bvr.forEach(r),e6t=t(oBe," (DistilBERT model)"),oBe.forEach(r),o6t=i(so),m3=s(so,"LI",{});var tBe=n(m3);GTe=s(tBe,"STRONG",{});var xvr=n(GTe);t6t=t(xvr,"electra"),xvr.forEach(r),r6t=t(tBe," \u2014 "),IV=s(tBe,"A",{href:!0});var kvr=n(IV);a6t=t(kvr,"FlaxElectraForQuestionAnswering"),kvr.forEach(r),s6t=t(tBe," (ELECTRA model)"),tBe.forEach(r),n6t=i(so),f3=s(so,"LI",{});var rBe=n(f3);XTe=s(rBe,"STRONG",{});var Rvr=n(XTe);l6t=t(Rvr,"mbart"),Rvr.forEach(r),i6t=t(rBe," \u2014 "),jV=s(rBe,"A",{href:!0});var Svr=n(jV);d6t=t(Svr,"FlaxMBartForQuestionAnswering"),Svr.forEach(r),c6t=t(rBe," (mBART model)"),rBe.forEach(r),m6t=i(so),g3=s(so,"LI",{});var aBe=n(g3);VTe=s(aBe,"STRONG",{});var Pvr=n(VTe);f6t=t(Pvr,"roberta"),Pvr.forEach(r),g6t=t(aBe," \u2014 "),DV=s(aBe,"A",{href:!0});var $vr=n(DV);h6t=t($vr,"FlaxRobertaForQuestionAnswering"),$vr.forEach(r),u6t=t(aBe," (RoBERTa model)"),aBe.forEach(r),p6t=i(so),h3=s(so,"LI",{});var sBe=n(h3);zTe=s(sBe,"STRONG",{});var Ivr=n(zTe);_6t=t(Ivr,"roformer"),Ivr.forEach(r),b6t=t(sBe," \u2014 "),NV=s(sBe,"A",{href:!0});var jvr=n(NV);v6t=t(jvr,"FlaxRoFormerForQuestionAnswering"),jvr.forEach(r),T6t=t(sBe," (RoFormer model)"),sBe.forEach(r),so.forEach(r),F6t=i(Sa),WTe=s(Sa,"P",{});var Dvr=n(WTe);C6t=t(Dvr,"Examples:"),Dvr.forEach(r),M6t=i(Sa),f(YL.$$.fragment,Sa),Sa.forEach(r),Mi.forEach(r),uke=i(d),bm=s(d,"H2",{class:!0});var ASe=n(bm);u3=s(ASe,"A",{id:!0,class:!0,href:!0});var Nvr=n(u3);QTe=s(Nvr,"SPAN",{});var qvr=n(QTe);f(KL.$$.fragment,qvr),qvr.forEach(r),Nvr.forEach(r),E6t=i(ASe),HTe=s(ASe,"SPAN",{});var Ovr=n(HTe);y6t=t(Ovr,"FlaxAutoModelForTokenClassification"),Ovr.forEach(r),ASe.forEach(r),pke=i(d),jt=s(d,"DIV",{class:!0});var yi=n(jt);f(ZL.$$.fragment,yi),w6t=i(yi),vm=s(yi,"P",{});var qW=n(vm);A6t=t(qW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),UTe=s(qW,"CODE",{});var Gvr=n(UTe);L6t=t(Gvr,"from_pretrained()"),Gvr.forEach(r),B6t=t(qW,"class method or the "),JTe=s(qW,"CODE",{});var Xvr=n(JTe);x6t=t(Xvr,"from_config()"),Xvr.forEach(r),k6t=t(qW,`class
method.`),qW.forEach(r),R6t=i(yi),e8=s(yi,"P",{});var LSe=n(e8);S6t=t(LSe,"This class cannot be instantiated directly using "),YTe=s(LSe,"CODE",{});var Vvr=n(YTe);P6t=t(Vvr,"__init__()"),Vvr.forEach(r),$6t=t(LSe," (throws an error)."),LSe.forEach(r),I6t=i(yi),kr=s(yi,"DIV",{class:!0});var wi=n(kr);f(o8.$$.fragment,wi),j6t=i(wi),KTe=s(wi,"P",{});var zvr=n(KTe);D6t=t(zvr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),zvr.forEach(r),N6t=i(wi),Tm=s(wi,"P",{});var OW=n(Tm);q6t=t(OW,`Note:
Loading a model from its configuration file does `),ZTe=s(OW,"STRONG",{});var Wvr=n(ZTe);O6t=t(Wvr,"not"),Wvr.forEach(r),G6t=t(OW,` load the model weights. It only affects the
model\u2019s configuration. Use `),e1e=s(OW,"CODE",{});var Qvr=n(e1e);X6t=t(Qvr,"from_pretrained()"),Qvr.forEach(r),V6t=t(OW,"to load the model weights."),OW.forEach(r),z6t=i(wi),o1e=s(wi,"P",{});var Hvr=n(o1e);W6t=t(Hvr,"Examples:"),Hvr.forEach(r),Q6t=i(wi),f(t8.$$.fragment,wi),wi.forEach(r),H6t=i(yi),Po=s(yi,"DIV",{class:!0});var Pa=n(Po);f(r8.$$.fragment,Pa),U6t=i(Pa),t1e=s(Pa,"P",{});var Uvr=n(t1e);J6t=t(Uvr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Uvr.forEach(r),Y6t=i(Pa),$s=s(Pa,"P",{});var S5=n($s);K6t=t(S5,"The model class to instantiate is selected based on the "),r1e=s(S5,"CODE",{});var Jvr=n(r1e);Z6t=t(Jvr,"model_type"),Jvr.forEach(r),eAt=t(S5,` property of the config object (either
passed as an argument or loaded from `),a1e=s(S5,"CODE",{});var Yvr=n(a1e);oAt=t(Yvr,"pretrained_model_name_or_path"),Yvr.forEach(r),tAt=t(S5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s1e=s(S5,"CODE",{});var Kvr=n(s1e);rAt=t(Kvr,"pretrained_model_name_or_path"),Kvr.forEach(r),aAt=t(S5,":"),S5.forEach(r),sAt=i(Pa),lo=s(Pa,"UL",{});var la=n(lo);p3=s(la,"LI",{});var nBe=n(p3);n1e=s(nBe,"STRONG",{});var Zvr=n(n1e);nAt=t(Zvr,"albert"),Zvr.forEach(r),lAt=t(nBe," \u2014 "),qV=s(nBe,"A",{href:!0});var eTr=n(qV);iAt=t(eTr,"FlaxAlbertForTokenClassification"),eTr.forEach(r),dAt=t(nBe," (ALBERT model)"),nBe.forEach(r),cAt=i(la),_3=s(la,"LI",{});var lBe=n(_3);l1e=s(lBe,"STRONG",{});var oTr=n(l1e);mAt=t(oTr,"bert"),oTr.forEach(r),fAt=t(lBe," \u2014 "),OV=s(lBe,"A",{href:!0});var tTr=n(OV);gAt=t(tTr,"FlaxBertForTokenClassification"),tTr.forEach(r),hAt=t(lBe," (BERT model)"),lBe.forEach(r),uAt=i(la),b3=s(la,"LI",{});var iBe=n(b3);i1e=s(iBe,"STRONG",{});var rTr=n(i1e);pAt=t(rTr,"big_bird"),rTr.forEach(r),_At=t(iBe," \u2014 "),GV=s(iBe,"A",{href:!0});var aTr=n(GV);bAt=t(aTr,"FlaxBigBirdForTokenClassification"),aTr.forEach(r),vAt=t(iBe," (BigBird model)"),iBe.forEach(r),TAt=i(la),v3=s(la,"LI",{});var dBe=n(v3);d1e=s(dBe,"STRONG",{});var sTr=n(d1e);FAt=t(sTr,"distilbert"),sTr.forEach(r),CAt=t(dBe," \u2014 "),XV=s(dBe,"A",{href:!0});var nTr=n(XV);MAt=t(nTr,"FlaxDistilBertForTokenClassification"),nTr.forEach(r),EAt=t(dBe," (DistilBERT model)"),dBe.forEach(r),yAt=i(la),T3=s(la,"LI",{});var cBe=n(T3);c1e=s(cBe,"STRONG",{});var lTr=n(c1e);wAt=t(lTr,"electra"),lTr.forEach(r),AAt=t(cBe," \u2014 "),VV=s(cBe,"A",{href:!0});var iTr=n(VV);LAt=t(iTr,"FlaxElectraForTokenClassification"),iTr.forEach(r),BAt=t(cBe," (ELECTRA model)"),cBe.forEach(r),xAt=i(la),F3=s(la,"LI",{});var mBe=n(F3);m1e=s(mBe,"STRONG",{});var dTr=n(m1e);kAt=t(dTr,"roberta"),dTr.forEach(r),RAt=t(mBe," \u2014 "),zV=s(mBe,"A",{href:!0});var cTr=n(zV);SAt=t(cTr,"FlaxRobertaForTokenClassification"),cTr.forEach(r),PAt=t(mBe," (RoBERTa model)"),mBe.forEach(r),$At=i(la),C3=s(la,"LI",{});var fBe=n(C3);f1e=s(fBe,"STRONG",{});var mTr=n(f1e);IAt=t(mTr,"roformer"),mTr.forEach(r),jAt=t(fBe," \u2014 "),WV=s(fBe,"A",{href:!0});var fTr=n(WV);DAt=t(fTr,"FlaxRoFormerForTokenClassification"),fTr.forEach(r),NAt=t(fBe," (RoFormer model)"),fBe.forEach(r),la.forEach(r),qAt=i(Pa),g1e=s(Pa,"P",{});var gTr=n(g1e);OAt=t(gTr,"Examples:"),gTr.forEach(r),GAt=i(Pa),f(a8.$$.fragment,Pa),Pa.forEach(r),yi.forEach(r),_ke=i(d),Fm=s(d,"H2",{class:!0});var BSe=n(Fm);M3=s(BSe,"A",{id:!0,class:!0,href:!0});var hTr=n(M3);h1e=s(hTr,"SPAN",{});var uTr=n(h1e);f(s8.$$.fragment,uTr),uTr.forEach(r),hTr.forEach(r),XAt=i(BSe),u1e=s(BSe,"SPAN",{});var pTr=n(u1e);VAt=t(pTr,"FlaxAutoModelForMultipleChoice"),pTr.forEach(r),BSe.forEach(r),bke=i(d),Dt=s(d,"DIV",{class:!0});var Ai=n(Dt);f(n8.$$.fragment,Ai),zAt=i(Ai),Cm=s(Ai,"P",{});var GW=n(Cm);WAt=t(GW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),p1e=s(GW,"CODE",{});var _Tr=n(p1e);QAt=t(_Tr,"from_pretrained()"),_Tr.forEach(r),HAt=t(GW,"class method or the "),_1e=s(GW,"CODE",{});var bTr=n(_1e);UAt=t(bTr,"from_config()"),bTr.forEach(r),JAt=t(GW,`class
method.`),GW.forEach(r),YAt=i(Ai),l8=s(Ai,"P",{});var xSe=n(l8);KAt=t(xSe,"This class cannot be instantiated directly using "),b1e=s(xSe,"CODE",{});var vTr=n(b1e);ZAt=t(vTr,"__init__()"),vTr.forEach(r),e0t=t(xSe," (throws an error)."),xSe.forEach(r),o0t=i(Ai),Rr=s(Ai,"DIV",{class:!0});var Li=n(Rr);f(i8.$$.fragment,Li),t0t=i(Li),v1e=s(Li,"P",{});var TTr=n(v1e);r0t=t(TTr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),TTr.forEach(r),a0t=i(Li),Mm=s(Li,"P",{});var XW=n(Mm);s0t=t(XW,`Note:
Loading a model from its configuration file does `),T1e=s(XW,"STRONG",{});var FTr=n(T1e);n0t=t(FTr,"not"),FTr.forEach(r),l0t=t(XW,` load the model weights. It only affects the
model\u2019s configuration. Use `),F1e=s(XW,"CODE",{});var CTr=n(F1e);i0t=t(CTr,"from_pretrained()"),CTr.forEach(r),d0t=t(XW,"to load the model weights."),XW.forEach(r),c0t=i(Li),C1e=s(Li,"P",{});var MTr=n(C1e);m0t=t(MTr,"Examples:"),MTr.forEach(r),f0t=i(Li),f(d8.$$.fragment,Li),Li.forEach(r),g0t=i(Ai),$o=s(Ai,"DIV",{class:!0});var $a=n($o);f(c8.$$.fragment,$a),h0t=i($a),M1e=s($a,"P",{});var ETr=n(M1e);u0t=t(ETr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),ETr.forEach(r),p0t=i($a),Is=s($a,"P",{});var P5=n(Is);_0t=t(P5,"The model class to instantiate is selected based on the "),E1e=s(P5,"CODE",{});var yTr=n(E1e);b0t=t(yTr,"model_type"),yTr.forEach(r),v0t=t(P5,` property of the config object (either
passed as an argument or loaded from `),y1e=s(P5,"CODE",{});var wTr=n(y1e);T0t=t(wTr,"pretrained_model_name_or_path"),wTr.forEach(r),F0t=t(P5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),w1e=s(P5,"CODE",{});var ATr=n(w1e);C0t=t(ATr,"pretrained_model_name_or_path"),ATr.forEach(r),M0t=t(P5,":"),P5.forEach(r),E0t=i($a),io=s($a,"UL",{});var ia=n(io);E3=s(ia,"LI",{});var gBe=n(E3);A1e=s(gBe,"STRONG",{});var LTr=n(A1e);y0t=t(LTr,"albert"),LTr.forEach(r),w0t=t(gBe," \u2014 "),QV=s(gBe,"A",{href:!0});var BTr=n(QV);A0t=t(BTr,"FlaxAlbertForMultipleChoice"),BTr.forEach(r),L0t=t(gBe," (ALBERT model)"),gBe.forEach(r),B0t=i(ia),y3=s(ia,"LI",{});var hBe=n(y3);L1e=s(hBe,"STRONG",{});var xTr=n(L1e);x0t=t(xTr,"bert"),xTr.forEach(r),k0t=t(hBe," \u2014 "),HV=s(hBe,"A",{href:!0});var kTr=n(HV);R0t=t(kTr,"FlaxBertForMultipleChoice"),kTr.forEach(r),S0t=t(hBe," (BERT model)"),hBe.forEach(r),P0t=i(ia),w3=s(ia,"LI",{});var uBe=n(w3);B1e=s(uBe,"STRONG",{});var RTr=n(B1e);$0t=t(RTr,"big_bird"),RTr.forEach(r),I0t=t(uBe," \u2014 "),UV=s(uBe,"A",{href:!0});var STr=n(UV);j0t=t(STr,"FlaxBigBirdForMultipleChoice"),STr.forEach(r),D0t=t(uBe," (BigBird model)"),uBe.forEach(r),N0t=i(ia),A3=s(ia,"LI",{});var pBe=n(A3);x1e=s(pBe,"STRONG",{});var PTr=n(x1e);q0t=t(PTr,"distilbert"),PTr.forEach(r),O0t=t(pBe," \u2014 "),JV=s(pBe,"A",{href:!0});var $Tr=n(JV);G0t=t($Tr,"FlaxDistilBertForMultipleChoice"),$Tr.forEach(r),X0t=t(pBe," (DistilBERT model)"),pBe.forEach(r),V0t=i(ia),L3=s(ia,"LI",{});var _Be=n(L3);k1e=s(_Be,"STRONG",{});var ITr=n(k1e);z0t=t(ITr,"electra"),ITr.forEach(r),W0t=t(_Be," \u2014 "),YV=s(_Be,"A",{href:!0});var jTr=n(YV);Q0t=t(jTr,"FlaxElectraForMultipleChoice"),jTr.forEach(r),H0t=t(_Be," (ELECTRA model)"),_Be.forEach(r),U0t=i(ia),B3=s(ia,"LI",{});var bBe=n(B3);R1e=s(bBe,"STRONG",{});var DTr=n(R1e);J0t=t(DTr,"roberta"),DTr.forEach(r),Y0t=t(bBe," \u2014 "),KV=s(bBe,"A",{href:!0});var NTr=n(KV);K0t=t(NTr,"FlaxRobertaForMultipleChoice"),NTr.forEach(r),Z0t=t(bBe," (RoBERTa model)"),bBe.forEach(r),eLt=i(ia),x3=s(ia,"LI",{});var vBe=n(x3);S1e=s(vBe,"STRONG",{});var qTr=n(S1e);oLt=t(qTr,"roformer"),qTr.forEach(r),tLt=t(vBe," \u2014 "),ZV=s(vBe,"A",{href:!0});var OTr=n(ZV);rLt=t(OTr,"FlaxRoFormerForMultipleChoice"),OTr.forEach(r),aLt=t(vBe," (RoFormer model)"),vBe.forEach(r),ia.forEach(r),sLt=i($a),P1e=s($a,"P",{});var GTr=n(P1e);nLt=t(GTr,"Examples:"),GTr.forEach(r),lLt=i($a),f(m8.$$.fragment,$a),$a.forEach(r),Ai.forEach(r),vke=i(d),Em=s(d,"H2",{class:!0});var kSe=n(Em);k3=s(kSe,"A",{id:!0,class:!0,href:!0});var XTr=n(k3);$1e=s(XTr,"SPAN",{});var VTr=n($1e);f(f8.$$.fragment,VTr),VTr.forEach(r),XTr.forEach(r),iLt=i(kSe),I1e=s(kSe,"SPAN",{});var zTr=n(I1e);dLt=t(zTr,"FlaxAutoModelForNextSentencePrediction"),zTr.forEach(r),kSe.forEach(r),Tke=i(d),Nt=s(d,"DIV",{class:!0});var Bi=n(Nt);f(g8.$$.fragment,Bi),cLt=i(Bi),ym=s(Bi,"P",{});var VW=n(ym);mLt=t(VW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),j1e=s(VW,"CODE",{});var WTr=n(j1e);fLt=t(WTr,"from_pretrained()"),WTr.forEach(r),gLt=t(VW,"class method or the "),D1e=s(VW,"CODE",{});var QTr=n(D1e);hLt=t(QTr,"from_config()"),QTr.forEach(r),uLt=t(VW,`class
method.`),VW.forEach(r),pLt=i(Bi),h8=s(Bi,"P",{});var RSe=n(h8);_Lt=t(RSe,"This class cannot be instantiated directly using "),N1e=s(RSe,"CODE",{});var HTr=n(N1e);bLt=t(HTr,"__init__()"),HTr.forEach(r),vLt=t(RSe," (throws an error)."),RSe.forEach(r),TLt=i(Bi),Sr=s(Bi,"DIV",{class:!0});var xi=n(Sr);f(u8.$$.fragment,xi),FLt=i(xi),q1e=s(xi,"P",{});var UTr=n(q1e);CLt=t(UTr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),UTr.forEach(r),MLt=i(xi),wm=s(xi,"P",{});var zW=n(wm);ELt=t(zW,`Note:
Loading a model from its configuration file does `),O1e=s(zW,"STRONG",{});var JTr=n(O1e);yLt=t(JTr,"not"),JTr.forEach(r),wLt=t(zW,` load the model weights. It only affects the
model\u2019s configuration. Use `),G1e=s(zW,"CODE",{});var YTr=n(G1e);ALt=t(YTr,"from_pretrained()"),YTr.forEach(r),LLt=t(zW,"to load the model weights."),zW.forEach(r),BLt=i(xi),X1e=s(xi,"P",{});var KTr=n(X1e);xLt=t(KTr,"Examples:"),KTr.forEach(r),kLt=i(xi),f(p8.$$.fragment,xi),xi.forEach(r),RLt=i(Bi),Io=s(Bi,"DIV",{class:!0});var Ia=n(Io);f(_8.$$.fragment,Ia),SLt=i(Ia),V1e=s(Ia,"P",{});var ZTr=n(V1e);PLt=t(ZTr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),ZTr.forEach(r),$Lt=i(Ia),js=s(Ia,"P",{});var $5=n(js);ILt=t($5,"The model class to instantiate is selected based on the "),z1e=s($5,"CODE",{});var e1r=n(z1e);jLt=t(e1r,"model_type"),e1r.forEach(r),DLt=t($5,` property of the config object (either
passed as an argument or loaded from `),W1e=s($5,"CODE",{});var o1r=n(W1e);NLt=t(o1r,"pretrained_model_name_or_path"),o1r.forEach(r),qLt=t($5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Q1e=s($5,"CODE",{});var t1r=n(Q1e);OLt=t(t1r,"pretrained_model_name_or_path"),t1r.forEach(r),GLt=t($5,":"),$5.forEach(r),XLt=i(Ia),H1e=s(Ia,"UL",{});var r1r=n(H1e);R3=s(r1r,"LI",{});var TBe=n(R3);U1e=s(TBe,"STRONG",{});var a1r=n(U1e);VLt=t(a1r,"bert"),a1r.forEach(r),zLt=t(TBe," \u2014 "),ez=s(TBe,"A",{href:!0});var s1r=n(ez);WLt=t(s1r,"FlaxBertForNextSentencePrediction"),s1r.forEach(r),QLt=t(TBe," (BERT model)"),TBe.forEach(r),r1r.forEach(r),HLt=i(Ia),J1e=s(Ia,"P",{});var n1r=n(J1e);ULt=t(n1r,"Examples:"),n1r.forEach(r),JLt=i(Ia),f(b8.$$.fragment,Ia),Ia.forEach(r),Bi.forEach(r),Fke=i(d),Am=s(d,"H2",{class:!0});var SSe=n(Am);S3=s(SSe,"A",{id:!0,class:!0,href:!0});var l1r=n(S3);Y1e=s(l1r,"SPAN",{});var i1r=n(Y1e);f(v8.$$.fragment,i1r),i1r.forEach(r),l1r.forEach(r),YLt=i(SSe),K1e=s(SSe,"SPAN",{});var d1r=n(K1e);KLt=t(d1r,"FlaxAutoModelForImageClassification"),d1r.forEach(r),SSe.forEach(r),Cke=i(d),qt=s(d,"DIV",{class:!0});var ki=n(qt);f(T8.$$.fragment,ki),ZLt=i(ki),Lm=s(ki,"P",{});var WW=n(Lm);e8t=t(WW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Z1e=s(WW,"CODE",{});var c1r=n(Z1e);o8t=t(c1r,"from_pretrained()"),c1r.forEach(r),t8t=t(WW,"class method or the "),eFe=s(WW,"CODE",{});var m1r=n(eFe);r8t=t(m1r,"from_config()"),m1r.forEach(r),a8t=t(WW,`class
method.`),WW.forEach(r),s8t=i(ki),F8=s(ki,"P",{});var PSe=n(F8);n8t=t(PSe,"This class cannot be instantiated directly using "),oFe=s(PSe,"CODE",{});var f1r=n(oFe);l8t=t(f1r,"__init__()"),f1r.forEach(r),i8t=t(PSe," (throws an error)."),PSe.forEach(r),d8t=i(ki),Pr=s(ki,"DIV",{class:!0});var Ri=n(Pr);f(C8.$$.fragment,Ri),c8t=i(Ri),tFe=s(Ri,"P",{});var g1r=n(tFe);m8t=t(g1r,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),g1r.forEach(r),f8t=i(Ri),Bm=s(Ri,"P",{});var QW=n(Bm);g8t=t(QW,`Note:
Loading a model from its configuration file does `),rFe=s(QW,"STRONG",{});var h1r=n(rFe);h8t=t(h1r,"not"),h1r.forEach(r),u8t=t(QW,` load the model weights. It only affects the
model\u2019s configuration. Use `),aFe=s(QW,"CODE",{});var u1r=n(aFe);p8t=t(u1r,"from_pretrained()"),u1r.forEach(r),_8t=t(QW,"to load the model weights."),QW.forEach(r),b8t=i(Ri),sFe=s(Ri,"P",{});var p1r=n(sFe);v8t=t(p1r,"Examples:"),p1r.forEach(r),T8t=i(Ri),f(M8.$$.fragment,Ri),Ri.forEach(r),F8t=i(ki),jo=s(ki,"DIV",{class:!0});var ja=n(jo);f(E8.$$.fragment,ja),C8t=i(ja),nFe=s(ja,"P",{});var _1r=n(nFe);M8t=t(_1r,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),_1r.forEach(r),E8t=i(ja),Ds=s(ja,"P",{});var I5=n(Ds);y8t=t(I5,"The model class to instantiate is selected based on the "),lFe=s(I5,"CODE",{});var b1r=n(lFe);w8t=t(b1r,"model_type"),b1r.forEach(r),A8t=t(I5,` property of the config object (either
passed as an argument or loaded from `),iFe=s(I5,"CODE",{});var v1r=n(iFe);L8t=t(v1r,"pretrained_model_name_or_path"),v1r.forEach(r),B8t=t(I5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dFe=s(I5,"CODE",{});var T1r=n(dFe);x8t=t(T1r,"pretrained_model_name_or_path"),T1r.forEach(r),k8t=t(I5,":"),I5.forEach(r),R8t=i(ja),y8=s(ja,"UL",{});var $Se=n(y8);P3=s($Se,"LI",{});var FBe=n(P3);cFe=s(FBe,"STRONG",{});var F1r=n(cFe);S8t=t(F1r,"beit"),F1r.forEach(r),P8t=t(FBe," \u2014 "),oz=s(FBe,"A",{href:!0});var C1r=n(oz);$8t=t(C1r,"FlaxBeitForImageClassification"),C1r.forEach(r),I8t=t(FBe," (BEiT model)"),FBe.forEach(r),j8t=i($Se),$3=s($Se,"LI",{});var CBe=n($3);mFe=s(CBe,"STRONG",{});var M1r=n(mFe);D8t=t(M1r,"vit"),M1r.forEach(r),N8t=t(CBe," \u2014 "),tz=s(CBe,"A",{href:!0});var E1r=n(tz);q8t=t(E1r,"FlaxViTForImageClassification"),E1r.forEach(r),O8t=t(CBe," (ViT model)"),CBe.forEach(r),$Se.forEach(r),G8t=i(ja),fFe=s(ja,"P",{});var y1r=n(fFe);X8t=t(y1r,"Examples:"),y1r.forEach(r),V8t=i(ja),f(w8.$$.fragment,ja),ja.forEach(r),ki.forEach(r),Mke=i(d),xm=s(d,"H2",{class:!0});var ISe=n(xm);I3=s(ISe,"A",{id:!0,class:!0,href:!0});var w1r=n(I3);gFe=s(w1r,"SPAN",{});var A1r=n(gFe);f(A8.$$.fragment,A1r),A1r.forEach(r),w1r.forEach(r),z8t=i(ISe),hFe=s(ISe,"SPAN",{});var L1r=n(hFe);W8t=t(L1r,"FlaxAutoModelForVision2Seq"),L1r.forEach(r),ISe.forEach(r),Eke=i(d),Ot=s(d,"DIV",{class:!0});var Si=n(Ot);f(L8.$$.fragment,Si),Q8t=i(Si),km=s(Si,"P",{});var HW=n(km);H8t=t(HW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),uFe=s(HW,"CODE",{});var B1r=n(uFe);U8t=t(B1r,"from_pretrained()"),B1r.forEach(r),J8t=t(HW,"class method or the "),pFe=s(HW,"CODE",{});var x1r=n(pFe);Y8t=t(x1r,"from_config()"),x1r.forEach(r),K8t=t(HW,`class
method.`),HW.forEach(r),Z8t=i(Si),B8=s(Si,"P",{});var jSe=n(B8);e7t=t(jSe,"This class cannot be instantiated directly using "),_Fe=s(jSe,"CODE",{});var k1r=n(_Fe);o7t=t(k1r,"__init__()"),k1r.forEach(r),t7t=t(jSe," (throws an error)."),jSe.forEach(r),r7t=i(Si),$r=s(Si,"DIV",{class:!0});var Pi=n($r);f(x8.$$.fragment,Pi),a7t=i(Pi),bFe=s(Pi,"P",{});var R1r=n(bFe);s7t=t(R1r,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),R1r.forEach(r),n7t=i(Pi),Rm=s(Pi,"P",{});var UW=n(Rm);l7t=t(UW,`Note:
Loading a model from its configuration file does `),vFe=s(UW,"STRONG",{});var S1r=n(vFe);i7t=t(S1r,"not"),S1r.forEach(r),d7t=t(UW,` load the model weights. It only affects the
model\u2019s configuration. Use `),TFe=s(UW,"CODE",{});var P1r=n(TFe);c7t=t(P1r,"from_pretrained()"),P1r.forEach(r),m7t=t(UW,"to load the model weights."),UW.forEach(r),f7t=i(Pi),FFe=s(Pi,"P",{});var $1r=n(FFe);g7t=t($1r,"Examples:"),$1r.forEach(r),h7t=i(Pi),f(k8.$$.fragment,Pi),Pi.forEach(r),u7t=i(Si),Do=s(Si,"DIV",{class:!0});var Da=n(Do);f(R8.$$.fragment,Da),p7t=i(Da),CFe=s(Da,"P",{});var I1r=n(CFe);_7t=t(I1r,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),I1r.forEach(r),b7t=i(Da),Ns=s(Da,"P",{});var j5=n(Ns);v7t=t(j5,"The model class to instantiate is selected based on the "),MFe=s(j5,"CODE",{});var j1r=n(MFe);T7t=t(j1r,"model_type"),j1r.forEach(r),F7t=t(j5,` property of the config object (either
passed as an argument or loaded from `),EFe=s(j5,"CODE",{});var D1r=n(EFe);C7t=t(D1r,"pretrained_model_name_or_path"),D1r.forEach(r),M7t=t(j5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yFe=s(j5,"CODE",{});var N1r=n(yFe);E7t=t(N1r,"pretrained_model_name_or_path"),N1r.forEach(r),y7t=t(j5,":"),j5.forEach(r),w7t=i(Da),wFe=s(Da,"UL",{});var q1r=n(wFe);j3=s(q1r,"LI",{});var MBe=n(j3);AFe=s(MBe,"STRONG",{});var O1r=n(AFe);A7t=t(O1r,"vision-encoder-decoder"),O1r.forEach(r),L7t=t(MBe," \u2014 "),rz=s(MBe,"A",{href:!0});var G1r=n(rz);B7t=t(G1r,"FlaxVisionEncoderDecoderModel"),G1r.forEach(r),x7t=t(MBe," (Vision Encoder decoder model)"),MBe.forEach(r),q1r.forEach(r),k7t=i(Da),LFe=s(Da,"P",{});var X1r=n(LFe);R7t=t(X1r,"Examples:"),X1r.forEach(r),S7t=i(Da),f(S8.$$.fragment,Da),Da.forEach(r),Si.forEach(r),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(K1r)),c(fe,"id","auto-classes"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#auto-classes"),c(de,"class","relative group"),c(qs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),c(Gs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),c(Xs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),c(Gi,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(Dm,"id","extending-the-auto-classes"),c(Dm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Dm,"href","#extending-the-auto-classes"),c(Xi,"class","relative group"),c(qm,"id","transformers.AutoConfig"),c(qm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qm,"href","#transformers.AutoConfig"),c(Vi,"class","relative group"),c(j7,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(D7,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),c(N7,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),c(q7,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),c(O7,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),c(G7,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(X7,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig"),c(V7,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(z7,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(W7,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(Q7,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),c(H7,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),c(U7,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),c(J7,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),c(Y7,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig"),c(K7,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),c(Z7,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig"),c(e9,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig"),c(o9,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),c(t9,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(r9,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),c(a9,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),c(s9,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),c(n9,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),c(l9,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),c(i9,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(d9,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),c(c9,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),c(m9,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),c(f9,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),c(g9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),c(h9,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(u9,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),c(p9,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),c(_9,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),c(b9,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(v9,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(T9,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(F9,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),c(C9,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),c(M9,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),c(E9,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),c(y9,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),c(w9,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),c(A9,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig"),c(L9,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),c(B9,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(x9,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(k9,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),c(R9,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),c(S9,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(P9,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c($9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),c(I9,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),c(j9,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig"),c(D9,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(N9,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(q9,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(O9,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),c(G9,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmConfig"),c(X9,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),c(V9,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),c(z9,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),c(W9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),c(Q9,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),c(H9,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),c(U9,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),c(J9,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig"),c(Y9,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(K9,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(Z9,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(eB,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),c(oB,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(tB,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig"),c(rB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),c(aB,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),c(sB,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(nB,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),c(lB,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(iB,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(dB,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig"),c(cB,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(mB,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(fB,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(gB,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),c(hB,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(uB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(pB,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig"),c(_B,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig"),c(bB,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),c(vB,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(TB,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(FB,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(CB,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),c(MB,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig"),c(fo,"class","docstring"),c(Lg,"class","docstring"),c(Vo,"class","docstring"),c(Bg,"id","transformers.AutoTokenizer"),c(Bg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bg,"href","#transformers.AutoTokenizer"),c(Wi,"class","relative group"),c(EB,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(yB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),c(wB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(AB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),c(LB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),c(BB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),c(xB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(kB,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(RB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(SB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(PB,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c($B,"href","/docs/transformers/master/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(IB,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(jB,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(DB,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(NB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(qB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(OB,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(GB,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(XB,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(VB,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(zB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),c(WB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(QB,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),c(HB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),c(UB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(JB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(YB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(KB,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),c(ZB,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(ex,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),c(ox,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(tx,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(rx,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(ax,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(sx,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(nx,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(lx,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),c(ix,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(dx,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(cx,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),c(mx,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(fx,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(gx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),c(hx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(ux,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(px,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(_x,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(bx,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(vx,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizer"),c(Tx,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(Fx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Cx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Mx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Ex,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(yx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(wx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(Ax,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(Lx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(Bx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(xx,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),c(kx,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),c(Rx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),c(Sx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(Px,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),c($x,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(Ix,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(jx,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(Dx,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),c(Nx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),c(qx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(Ox,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(Gx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(Xx,"href","/docs/transformers/master/en/model_doc/mluke#transformers.MLukeTokenizer"),c(Vx,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(zx,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(Wx,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(Qx,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(Hx,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),c(Ux,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),c(Jx,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(Yx,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(Kx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(Zx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(ek,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(ok,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),c(tk,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartTokenizer"),c(rk,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(ak,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(sk,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(nk,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),c(lk,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmTokenizer"),c(ik,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmTokenizerFast"),c(dk,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),c(ck,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(mk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),c(fk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(gk,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(hk,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(uk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(pk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(_k,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(bk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(vk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Tk,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Fk,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),c(Ck,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(Mk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Ek,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(yk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),c(wk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),c(Ak,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),c(Lk,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Bk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(xk,"href","/docs/transformers/master/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(kk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizer"),c(Rk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Sk,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),c(Pk,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c($k,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Ik,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(jk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Dk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(go,"class","docstring"),c(sh,"class","docstring"),c(zo,"class","docstring"),c(nh,"id","transformers.AutoFeatureExtractor"),c(nh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(nh,"href","#transformers.AutoFeatureExtractor"),c(Qi,"class","relative group"),c(Nk,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(qk,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(Ok,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Gk,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Xk,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Vk,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(zk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Wk,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(Qk,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor"),c(Hk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(Uk,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(Jk,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(Yk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(Kk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(Zk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(eR,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(oR,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(xe,"class","docstring"),c(Eh,"class","docstring"),c(Wo,"class","docstring"),c(yh,"id","transformers.AutoProcessor"),c(yh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yh,"href","#transformers.AutoProcessor"),c(Hi,"class","relative group"),c(tR,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(rR,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),c(aR,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(sR,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(nR,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(lR,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(iR,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),c(dR,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(cR,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(ke,"class","docstring"),c($h,"class","docstring"),c(Qo,"class","docstring"),c(Ih,"id","transformers.AutoModel"),c(Ih,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ih,"href","#transformers.AutoModel"),c(Ji,"class","relative group"),c(Gt,"class","docstring"),c(mR,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),c(fR,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),c(gR,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),c(hR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(uR,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(pR,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel"),c(_R,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(bR,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(vR,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(TR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),c(FR,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),c(CR,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),c(MR,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),c(ER,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel"),c(yR,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),c(wR,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel"),c(AR,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel"),c(LR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),c(BR,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(xR,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),c(kR,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),c(RR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),c(SR,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(PR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),c($R,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),c(IR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),c(jR,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),c(DR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(NR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),c(qR,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),c(OR,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(GR,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),c(XR,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),c(VR,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),c(zR,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(WR,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(QR,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(HR,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),c(UR,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),c(JR,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),c(YR,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),c(KR,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),c(ZR,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),c(eS,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerModel"),c(oS,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),c(tS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(rS,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),c(aS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),c(sS,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),c(nS,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel"),c(lS,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(iS,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),c(dS,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),c(cS,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel"),c(mS,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel"),c(fS,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(gS,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel"),c(hS,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),c(uS,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),c(pS,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(_S,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),c(bS,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),c(vS,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),c(TS,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),c(FS,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel"),c(CS,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(MS,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),c(ES,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(yS,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinModel"),c(wS,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),c(AS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),c(LS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(BS,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),c(xS,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(kS,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel"),c(RS,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(SS,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),c(PS,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),c($S,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(IS,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(jS,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel"),c(DS,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel"),c(NS,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),c(qS,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(OS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(GS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(XS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),c(VS,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel"),c(Re,"class","docstring"),c(Ho,"class","docstring"),c(pp,"id","transformers.AutoModelForPreTraining"),c(pp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pp,"href","#transformers.AutoModelForPreTraining"),c(Zi,"class","relative group"),c(Xt,"class","docstring"),c(zS,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),c(WS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(QS,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),c(HS,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(US,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(JS,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(YS,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),c(KS,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(ZS,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(eP,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(oP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),c(tP,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(rP,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),c(aP,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(sP,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(nP,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(lP,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(iP,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(dP,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(cP,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(mP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(fP,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(gP,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(hP,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(uP,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(pP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(_P,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(bP,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(vP,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(TP,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(FP,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(CP,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(MP,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(EP,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(yP,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(wP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(AP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(LP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(BP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Se,"class","docstring"),c(Uo,"class","docstring"),c(r_,"id","transformers.AutoModelForCausalLM"),c(r_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(r_,"href","#transformers.AutoModelForCausalLM"),c(td,"class","relative group"),c(Vt,"class","docstring"),c(xP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),c(kP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),c(RP,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(SP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(PP,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c($P,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(IP,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(jP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(DP,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(NP,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM"),c(qP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM"),c(OP,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(GP,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(XP,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(VP,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),c(zP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),c(WP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(QP,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(HP,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(UP,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(JP,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(YP,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(KP,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(ZP,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(e$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(o$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(t$,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(r$,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(a$,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(s$,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(n$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(l$,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(i$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(d$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(c$,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Pe,"class","docstring"),c(Jo,"class","docstring"),c(q_,"id","transformers.AutoModelForMaskedLM"),c(q_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q_,"href","#transformers.AutoModelForMaskedLM"),c(sd,"class","relative group"),c(zt,"class","docstring"),c(m$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(f$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(g$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),c(h$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(u$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(p$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(_$,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),c(b$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(v$,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(T$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(F$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(C$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(M$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(E$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(y$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(w$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(A$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(L$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(B$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(x$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(k$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(R$,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(S$,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(P$,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c($$,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(I$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(j$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(D$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(N$,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(q$,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(O$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(G$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(X$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(V$,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM"),c($e,"class","docstring"),c(Yo,"class","docstring"),c(Mb,"id","transformers.AutoModelForSeq2SeqLM"),c(Mb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mb,"href","#transformers.AutoModelForSeq2SeqLM"),c(id,"class","relative group"),c(Wt,"class","docstring"),c(z$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(W$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(Q$,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(H$,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(U$,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(J$,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(Y$,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(K$,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(Z$,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),c(eI,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(oI,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(tI,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(rI,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(aI,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(sI,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(nI,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Ie,"class","docstring"),c(Ko,"class","docstring"),c(Ob,"id","transformers.AutoModelForSequenceClassification"),c(Ob,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ob,"href","#transformers.AutoModelForSequenceClassification"),c(md,"class","relative group"),c(Qt,"class","docstring"),c(lI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(iI,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),c(dI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),c(cI,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(mI,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(fI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(gI,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(hI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(uI,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(pI,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification"),c(_I,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(bI,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(vI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(TI,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(FI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(CI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(MI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(EI,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(yI,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(wI,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(AI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(LI,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(BI,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(xI,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),c(kI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(RI,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(SI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(PI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c($I,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(II,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(jI,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(DI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(NI,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c(qI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(OI,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(GI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(XI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(VI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(zI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(WI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(QI,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(HI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(UI,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(JI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(YI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(KI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c(je,"class","docstring"),c(Zo,"class","docstring"),c($2,"id","transformers.AutoModelForMultipleChoice"),c($2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($2,"href","#transformers.AutoModelForMultipleChoice"),c(hd,"class","relative group"),c(Ht,"class","docstring"),c(ZI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(ej,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),c(oj,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(tj,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(rj,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(aj,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(sj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice"),c(nj,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(lj,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(ij,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(dj,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(cj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(mj,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(fj,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(gj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(hj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(uj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(pj,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(_j,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(bj,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(vj,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(Tj,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(Fj,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(Cj,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(Mj,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(Ej,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(yj,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(wj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(De,"class","docstring"),c(et,"class","docstring"),c(mv,"id","transformers.AutoModelForNextSentencePrediction"),c(mv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(mv,"href","#transformers.AutoModelForNextSentencePrediction"),c(_d,"class","relative group"),c(Ut,"class","docstring"),c(Aj,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(Lj,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(Bj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(xj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(kj,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(Ne,"class","docstring"),c(ot,"class","docstring"),c(bv,"id","transformers.AutoModelForTokenClassification"),c(bv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bv,"href","#transformers.AutoModelForTokenClassification"),c(Td,"class","relative group"),c(Jt,"class","docstring"),c(Rj,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(Sj,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),c(Pj,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c($j,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(Ij,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),c(jj,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(Dj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification"),c(Nj,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(qj,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(Oj,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(Gj,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(Xj,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(Vj,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(zj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(Wj,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(Qj,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(Hj,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(Uj,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(Jj,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(Yj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(Kj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(Zj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(eD,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(oD,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(tD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(rD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(aD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(sD,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(nD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(lD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(iD,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(dD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(cD,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(qe,"class","docstring"),c(tt,"class","docstring"),c(Zv,"id","transformers.AutoModelForQuestionAnswering"),c(Zv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zv,"href","#transformers.AutoModelForQuestionAnswering"),c(Md,"class","relative group"),c(Yt,"class","docstring"),c(mD,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(fD,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(gD,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(hD,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(uD,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(pD,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(_D,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(bD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(vD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering"),c(TD,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(FD,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(CD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(MD,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(ED,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(yD,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(wD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(AD,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(LD,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(BD,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(xD,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(kD,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(RD,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(SD,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(PD,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c($D,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(ID,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(jD,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(DD,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(ND,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(qD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(OD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(GD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(XD,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(VD,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(zD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(WD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(QD,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(HD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(UD,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(Oe,"class","docstring"),c(rt,"class","docstring"),c(qT,"id","transformers.AutoModelForTableQuestionAnswering"),c(qT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qT,"href","#transformers.AutoModelForTableQuestionAnswering"),c(wd,"class","relative group"),c(Kt,"class","docstring"),c(JD,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(Ge,"class","docstring"),c(at,"class","docstring"),c(XT,"id","transformers.AutoModelForImageClassification"),c(XT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(XT,"href","#transformers.AutoModelForImageClassification"),c(Bd,"class","relative group"),c(Zt,"class","docstring"),c(YD,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),c(KD,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(ZD,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),c(eN,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(oN,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(tN,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(rN,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(aN,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(sN,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(nN,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(lN,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification"),c(iN,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),c(Xe,"class","docstring"),c(st,"class","docstring"),c(KT,"id","transformers.AutoModelForVision2Seq"),c(KT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(KT,"href","#transformers.AutoModelForVision2Seq"),c(Rd,"class","relative group"),c(er,"class","docstring"),c(dN,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Ve,"class","docstring"),c(nt,"class","docstring"),c(o1,"id","transformers.AutoModelForAudioClassification"),c(o1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(o1,"href","#transformers.AutoModelForAudioClassification"),c($d,"class","relative group"),c(or,"class","docstring"),c(cN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification"),c(mN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(fN,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(gN,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(hN,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(uN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(pN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(_N,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(ze,"class","docstring"),c(lt,"class","docstring"),c(m1,"id","transformers.AutoModelForAudioFrameClassification"),c(m1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(m1,"href","#transformers.AutoModelForAudioFrameClassification"),c(Dd,"class","relative group"),c(tr,"class","docstring"),c(bN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification"),c(vN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(TN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(FN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(We,"class","docstring"),c(it,"class","docstring"),c(_1,"id","transformers.AutoModelForCTC"),c(_1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_1,"href","#transformers.AutoModelForCTC"),c(Od,"class","relative group"),c(rr,"class","docstring"),c(CN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC"),c(MN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),c(EN,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),c(yN,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC"),c(wN,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(AN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(LN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(BN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Qe,"class","docstring"),c(dt,"class","docstring"),c(A1,"id","transformers.AutoModelForSpeechSeq2Seq"),c(A1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A1,"href","#transformers.AutoModelForSpeechSeq2Seq"),c(Vd,"class","relative group"),c(ar,"class","docstring"),c(xN,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(kN,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(He,"class","docstring"),c(ct,"class","docstring"),c(k1,"id","transformers.AutoModelForAudioXVector"),c(k1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k1,"href","#transformers.AutoModelForAudioXVector"),c(Qd,"class","relative group"),c(sr,"class","docstring"),c(RN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector"),c(SN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(PN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c($N,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Ue,"class","docstring"),c(mt,"class","docstring"),c(j1,"id","transformers.AutoModelForMaskedImageModeling"),c(j1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j1,"href","#transformers.AutoModelForMaskedImageModeling"),c(Jd,"class","relative group"),c(nr,"class","docstring"),c(IN,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(jN,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(DN,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(Je,"class","docstring"),c(ft,"class","docstring"),c(G1,"id","transformers.AutoModelForObjectDetection"),c(G1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G1,"href","#transformers.AutoModelForObjectDetection"),c(ec,"class","relative group"),c(lr,"class","docstring"),c(NN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ye,"class","docstring"),c(gt,"class","docstring"),c(z1,"id","transformers.AutoModelForImageSegmentation"),c(z1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z1,"href","#transformers.AutoModelForImageSegmentation"),c(rc,"class","relative group"),c(ir,"class","docstring"),c(qN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),c(Ke,"class","docstring"),c(ht,"class","docstring"),c(H1,"id","transformers.AutoModelForSemanticSegmentation"),c(H1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H1,"href","#transformers.AutoModelForSemanticSegmentation"),c(nc,"class","relative group"),c(dr,"class","docstring"),c(ON,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(GN,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ze,"class","docstring"),c(ut,"class","docstring"),c(K1,"id","transformers.AutoModelForInstanceSegmentation"),c(K1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K1,"href","#transformers.AutoModelForInstanceSegmentation"),c(dc,"class","relative group"),c(cr,"class","docstring"),c(XN,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation"),c(eo,"class","docstring"),c(pt,"class","docstring"),c(oF,"id","transformers.TFAutoModel"),c(oF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oF,"href","#transformers.TFAutoModel"),c(fc,"class","relative group"),c(mr,"class","docstring"),c(VN,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),c(zN,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),c(WN,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),c(QN,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(HN,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(UN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),c(JN,"href","/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel"),c(YN,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),c(KN,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel"),c(ZN,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),c(eq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),c(oq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(tq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(rq,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(aq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),c(sq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(nq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),c(lq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(iq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),c(dq,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),c(cq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(mq,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),c(fq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),c(gq,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),c(hq,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),c(uq,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),c(pq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(_q,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),c(bq,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),c(vq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(Tq,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),c(Fq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),c(Cq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),c(Mq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),c(Eq,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(yq,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),c(wq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),c(Aq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(Lq,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),c(Bq,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(xq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),c(kq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(Rq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),c(ho,"class","docstring"),c(_t,"class","docstring"),c(XF,"id","transformers.TFAutoModelForPreTraining"),c(XF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(XF,"href","#transformers.TFAutoModelForPreTraining"),c(uc,"class","relative group"),c(fr,"class","docstring"),c(Sq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(Pq,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c($q,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),c(Iq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(jq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(Dq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(Nq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(qq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(Oq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(Gq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(Xq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(Vq,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(zq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(Wq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(Qq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(Hq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(Uq,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(Jq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(Yq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(Kq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(Zq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(eO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(uo,"class","docstring"),c(bt,"class","docstring"),c(fC,"id","transformers.TFAutoModelForCausalLM"),c(fC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fC,"href","#transformers.TFAutoModelForCausalLM"),c(bc,"class","relative group"),c(gr,"class","docstring"),c(oO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(tO,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(rO,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(aO,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(sO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(nO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(lO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(iO,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(dO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(cO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(po,"class","docstring"),c(vt,"class","docstring"),c(MC,"id","transformers.TFAutoModelForImageClassification"),c(MC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(MC,"href","#transformers.TFAutoModelForImageClassification"),c(Fc,"class","relative group"),c(hr,"class","docstring"),c(mO,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),c(fO,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),c(_o,"class","docstring"),c(Tt,"class","docstring"),c(wC,"id","transformers.TFAutoModelForMaskedLM"),c(wC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wC,"href","#transformers.TFAutoModelForMaskedLM"),c(Ec,"class","relative group"),c(ur,"class","docstring"),c(gO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(hO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(uO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(pO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(_O,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(bO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(vO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(TO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(FO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(CO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(MO,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(EO,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(yO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(wO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(AO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(LO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(BO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(xO,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(kO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(RO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(bo,"class","docstring"),c(Ft,"class","docstring"),c(QC,"id","transformers.TFAutoModelForSeq2SeqLM"),c(QC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(QC,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(Ac,"class","relative group"),c(pr,"class","docstring"),c(SO,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(PO,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c($O,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(IO,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(jO,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(DO,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),c(NO,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(qO,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(OO,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(GO,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(vo,"class","docstring"),c(Ct,"class","docstring"),c(a4,"id","transformers.TFAutoModelForSequenceClassification"),c(a4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(a4,"href","#transformers.TFAutoModelForSequenceClassification"),c(xc,"class","relative group"),c(_r,"class","docstring"),c(XO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(VO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(zO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(WO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(QO,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(HO,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(UO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(JO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(YO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(KO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(ZO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(eG,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(oG,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(tG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(rG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(aG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(sG,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(nG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(lG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(iG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(dG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(cG,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(mG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(fG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(gG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(To,"class","docstring"),c(Mt,"class","docstring"),c(x4,"id","transformers.TFAutoModelForMultipleChoice"),c(x4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x4,"href","#transformers.TFAutoModelForMultipleChoice"),c(Sc,"class","relative group"),c(br,"class","docstring"),c(hG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(uG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(pG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(_G,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(bG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(vG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(TG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(FG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(CG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(MG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(EG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(yG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(wG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(AG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(LG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(BG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(xG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(Fo,"class","docstring"),c(Et,"class","docstring"),c(H4,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(H4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H4,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(Ic,"class","relative group"),c(vr,"class","docstring"),c(kG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Co,"class","docstring"),c(yt,"class","docstring"),c(J4,"id","transformers.TFAutoModelForTokenClassification"),c(J4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J4,"href","#transformers.TFAutoModelForTokenClassification"),c(Nc,"class","relative group"),c(Tr,"class","docstring"),c(RG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(SG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(PG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c($G,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(IG,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(jG,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(DG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(NG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(qG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(OG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(GG,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(XG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(VG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(zG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(WG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(QG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(HG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(UG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(JG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(YG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Mo,"class","docstring"),c(wt,"class","docstring"),c(_M,"id","transformers.TFAutoModelForQuestionAnswering"),c(_M,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_M,"href","#transformers.TFAutoModelForQuestionAnswering"),c(Gc,"class","relative group"),c(Fr,"class","docstring"),c(KG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(ZG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(eX,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(oX,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(tX,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(rX,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(aX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(sX,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(nX,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(lX,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(iX,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(dX,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(cX,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(mX,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(fX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(gX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(hX,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(uX,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(pX,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Eo,"class","docstring"),c(At,"class","docstring"),c(jM,"id","transformers.TFAutoModelForVision2Seq"),c(jM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jM,"href","#transformers.TFAutoModelForVision2Seq"),c(zc,"class","relative group"),c(Cr,"class","docstring"),c(_X,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(yo,"class","docstring"),c(Lt,"class","docstring"),c(NM,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(NM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(NM,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(Hc,"class","relative group"),c(Mr,"class","docstring"),c(bX,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(wo,"class","docstring"),c(Bt,"class","docstring"),c(OM,"id","transformers.FlaxAutoModel"),c(OM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(OM,"href","#transformers.FlaxAutoModel"),c(Yc,"class","relative group"),c(Er,"class","docstring"),c(vX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),c(TX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),c(FX,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),c(CX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),c(MX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(EX,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(yX,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(wX,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),c(AX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(LX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),c(BX,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(xX,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(kX,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(RX,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),c(SX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),c(PX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),c($X,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(IX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(jX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(DX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),c(NX,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(qX,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),c(OX,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(GX,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(Ao,"class","docstring"),c(xt,"class","docstring"),c(fE,"id","transformers.FlaxAutoModelForCausalLM"),c(fE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fE,"href","#transformers.FlaxAutoModelForCausalLM"),c(em,"class","relative group"),c(yr,"class","docstring"),c(XX,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(VX,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(zX,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(WX,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Lo,"class","docstring"),c(kt,"class","docstring"),c(_E,"id","transformers.FlaxAutoModelForPreTraining"),c(_E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_E,"href","#transformers.FlaxAutoModelForPreTraining"),c(rm,"class","relative group"),c(wr,"class","docstring"),c(QX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(HX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(UX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(JX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(YX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(KX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(ZX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(eV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(oV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(tV,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(rV,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Bo,"class","docstring"),c(Rt,"class","docstring"),c(BE,"id","transformers.FlaxAutoModelForMaskedLM"),c(BE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(BE,"href","#transformers.FlaxAutoModelForMaskedLM"),c(nm,"class","relative group"),c(Ar,"class","docstring"),c(aV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(sV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(nV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(lV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(iV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(dV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(cV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(mV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(fV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(xo,"class","docstring"),c(St,"class","docstring"),c(NE,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(NE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(NE,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(dm,"class","relative group"),c(Lr,"class","docstring"),c(gV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(hV,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(uV,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(pV,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(_V,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(bV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(vV,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(TV,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(FV,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(Pt,"class","docstring"),c(UE,"id","transformers.FlaxAutoModelForSequenceClassification"),c(UE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(UE,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(fm,"class","relative group"),c(Br,"class","docstring"),c(CV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(MV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(EV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(yV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(wV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(AV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(LV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(BV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(xV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(Ro,"class","docstring"),c($t,"class","docstring"),c(s3,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(s3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s3,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(um,"class","relative group"),c(xr,"class","docstring"),c(kV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(RV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(SV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(PV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c($V,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(IV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(jV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(DV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(NV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(So,"class","docstring"),c(It,"class","docstring"),c(u3,"id","transformers.FlaxAutoModelForTokenClassification"),c(u3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u3,"href","#transformers.FlaxAutoModelForTokenClassification"),c(bm,"class","relative group"),c(kr,"class","docstring"),c(qV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(OV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(GV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(XV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(VV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(zV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(WV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(Po,"class","docstring"),c(jt,"class","docstring"),c(M3,"id","transformers.FlaxAutoModelForMultipleChoice"),c(M3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M3,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(Fm,"class","relative group"),c(Rr,"class","docstring"),c(QV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(HV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(UV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(JV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(YV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(KV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(ZV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c($o,"class","docstring"),c(Dt,"class","docstring"),c(k3,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(k3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k3,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(Em,"class","relative group"),c(Sr,"class","docstring"),c(ez,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c(Io,"class","docstring"),c(Nt,"class","docstring"),c(S3,"id","transformers.FlaxAutoModelForImageClassification"),c(S3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S3,"href","#transformers.FlaxAutoModelForImageClassification"),c(Am,"class","relative group"),c(Pr,"class","docstring"),c(oz,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(tz,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(jo,"class","docstring"),c(qt,"class","docstring"),c(I3,"id","transformers.FlaxAutoModelForVision2Seq"),c(I3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(I3,"href","#transformers.FlaxAutoModelForVision2Seq"),c(xm,"class","relative group"),c($r,"class","docstring"),c(rz,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(Do,"class","docstring"),c(Ot,"class","docstring")},m(d,_){e(document.head,J),b(d,Be,_),b(d,de,_),e(de,fe),e(fe,no),g(ce,no,null),e(de,_e),e(de,Go),e(Go,Ii),b(d,Pm,_),b(d,ca,_),e(ca,ji),e(ca,Di),e(Di,D5),e(ca,$m),b(d,ye,_),b(d,co,_),e(co,Ni),e(co,qs),e(qs,N5),e(co,Os),e(co,Gs),e(Gs,q5),e(co,qi),e(co,Xs),e(Xs,O5),e(co,Oi),b(d,Im,_),g(Na,d,_),b(d,mo,_),b(d,ge,_),e(ge,k7),e(ge,Gi),e(Gi,R7),e(ge,S7),b(d,Xo,_),b(d,qa,_),e(qa,P7),e(qa,jm),e(jm,$7),e(qa,DSe),b(d,EBe,_),b(d,Xi,_),e(Xi,Dm),e(Dm,JW),g(G5,JW,null),e(Xi,NSe),e(Xi,YW),e(YW,qSe),b(d,yBe,_),b(d,Vs,_),e(Vs,OSe),e(Vs,KW),e(KW,GSe),e(Vs,XSe),e(Vs,ZW),e(ZW,VSe),e(Vs,zSe),b(d,wBe,_),g(X5,d,_),b(d,ABe,_),b(d,I7,_),e(I7,WSe),b(d,LBe,_),g(Nm,d,_),b(d,BBe,_),b(d,Vi,_),e(Vi,qm),e(qm,eQ),g(V5,eQ,null),e(Vi,QSe),e(Vi,oQ),e(oQ,HSe),b(d,xBe,_),b(d,Vo,_),g(z5,Vo,null),e(Vo,USe),e(Vo,W5),e(W5,JSe),e(W5,j7),e(j7,YSe),e(W5,KSe),e(Vo,ZSe),e(Vo,Q5),e(Q5,ePe),e(Q5,tQ),e(tQ,oPe),e(Q5,tPe),e(Vo,rPe),e(Vo,fo),g(H5,fo,null),e(fo,aPe),e(fo,rQ),e(rQ,sPe),e(fo,nPe),e(fo,zi),e(zi,lPe),e(zi,aQ),e(aQ,iPe),e(zi,dPe),e(zi,sQ),e(sQ,cPe),e(zi,mPe),e(fo,fPe),e(fo,v),e(v,Om),e(Om,nQ),e(nQ,gPe),e(Om,hPe),e(Om,D7),e(D7,uPe),e(Om,pPe),e(v,_Pe),e(v,Gm),e(Gm,lQ),e(lQ,bPe),e(Gm,vPe),e(Gm,N7),e(N7,TPe),e(Gm,FPe),e(v,CPe),e(v,Xm),e(Xm,iQ),e(iQ,MPe),e(Xm,EPe),e(Xm,q7),e(q7,yPe),e(Xm,wPe),e(v,APe),e(v,Vm),e(Vm,dQ),e(dQ,LPe),e(Vm,BPe),e(Vm,O7),e(O7,xPe),e(Vm,kPe),e(v,RPe),e(v,zm),e(zm,cQ),e(cQ,SPe),e(zm,PPe),e(zm,G7),e(G7,$Pe),e(zm,IPe),e(v,jPe),e(v,Wm),e(Wm,mQ),e(mQ,DPe),e(Wm,NPe),e(Wm,X7),e(X7,qPe),e(Wm,OPe),e(v,GPe),e(v,Qm),e(Qm,fQ),e(fQ,XPe),e(Qm,VPe),e(Qm,V7),e(V7,zPe),e(Qm,WPe),e(v,QPe),e(v,Hm),e(Hm,gQ),e(gQ,HPe),e(Hm,UPe),e(Hm,z7),e(z7,JPe),e(Hm,YPe),e(v,KPe),e(v,Um),e(Um,hQ),e(hQ,ZPe),e(Um,e$e),e(Um,W7),e(W7,o$e),e(Um,t$e),e(v,r$e),e(v,Jm),e(Jm,uQ),e(uQ,a$e),e(Jm,s$e),e(Jm,Q7),e(Q7,n$e),e(Jm,l$e),e(v,i$e),e(v,Ym),e(Ym,pQ),e(pQ,d$e),e(Ym,c$e),e(Ym,H7),e(H7,m$e),e(Ym,f$e),e(v,g$e),e(v,Km),e(Km,_Q),e(_Q,h$e),e(Km,u$e),e(Km,U7),e(U7,p$e),e(Km,_$e),e(v,b$e),e(v,Zm),e(Zm,bQ),e(bQ,v$e),e(Zm,T$e),e(Zm,J7),e(J7,F$e),e(Zm,C$e),e(v,M$e),e(v,ef),e(ef,vQ),e(vQ,E$e),e(ef,y$e),e(ef,Y7),e(Y7,w$e),e(ef,A$e),e(v,L$e),e(v,of),e(of,TQ),e(TQ,B$e),e(of,x$e),e(of,K7),e(K7,k$e),e(of,R$e),e(v,S$e),e(v,tf),e(tf,FQ),e(FQ,P$e),e(tf,$$e),e(tf,Z7),e(Z7,I$e),e(tf,j$e),e(v,D$e),e(v,rf),e(rf,CQ),e(CQ,N$e),e(rf,q$e),e(rf,e9),e(e9,O$e),e(rf,G$e),e(v,X$e),e(v,af),e(af,MQ),e(MQ,V$e),e(af,z$e),e(af,o9),e(o9,W$e),e(af,Q$e),e(v,H$e),e(v,sf),e(sf,EQ),e(EQ,U$e),e(sf,J$e),e(sf,t9),e(t9,Y$e),e(sf,K$e),e(v,Z$e),e(v,nf),e(nf,yQ),e(yQ,eIe),e(nf,oIe),e(nf,r9),e(r9,tIe),e(nf,rIe),e(v,aIe),e(v,lf),e(lf,wQ),e(wQ,sIe),e(lf,nIe),e(lf,a9),e(a9,lIe),e(lf,iIe),e(v,dIe),e(v,df),e(df,AQ),e(AQ,cIe),e(df,mIe),e(df,s9),e(s9,fIe),e(df,gIe),e(v,hIe),e(v,cf),e(cf,LQ),e(LQ,uIe),e(cf,pIe),e(cf,n9),e(n9,_Ie),e(cf,bIe),e(v,vIe),e(v,mf),e(mf,BQ),e(BQ,TIe),e(mf,FIe),e(mf,l9),e(l9,CIe),e(mf,MIe),e(v,EIe),e(v,ff),e(ff,xQ),e(xQ,yIe),e(ff,wIe),e(ff,i9),e(i9,AIe),e(ff,LIe),e(v,BIe),e(v,gf),e(gf,kQ),e(kQ,xIe),e(gf,kIe),e(gf,d9),e(d9,RIe),e(gf,SIe),e(v,PIe),e(v,hf),e(hf,RQ),e(RQ,$Ie),e(hf,IIe),e(hf,c9),e(c9,jIe),e(hf,DIe),e(v,NIe),e(v,uf),e(uf,SQ),e(SQ,qIe),e(uf,OIe),e(uf,m9),e(m9,GIe),e(uf,XIe),e(v,VIe),e(v,pf),e(pf,PQ),e(PQ,zIe),e(pf,WIe),e(pf,f9),e(f9,QIe),e(pf,HIe),e(v,UIe),e(v,_f),e(_f,$Q),e($Q,JIe),e(_f,YIe),e(_f,g9),e(g9,KIe),e(_f,ZIe),e(v,eje),e(v,bf),e(bf,IQ),e(IQ,oje),e(bf,tje),e(bf,h9),e(h9,rje),e(bf,aje),e(v,sje),e(v,vf),e(vf,jQ),e(jQ,nje),e(vf,lje),e(vf,u9),e(u9,ije),e(vf,dje),e(v,cje),e(v,Tf),e(Tf,DQ),e(DQ,mje),e(Tf,fje),e(Tf,p9),e(p9,gje),e(Tf,hje),e(v,uje),e(v,Ff),e(Ff,NQ),e(NQ,pje),e(Ff,_je),e(Ff,_9),e(_9,bje),e(Ff,vje),e(v,Tje),e(v,Cf),e(Cf,qQ),e(qQ,Fje),e(Cf,Cje),e(Cf,b9),e(b9,Mje),e(Cf,Eje),e(v,yje),e(v,Mf),e(Mf,OQ),e(OQ,wje),e(Mf,Aje),e(Mf,v9),e(v9,Lje),e(Mf,Bje),e(v,xje),e(v,Ef),e(Ef,GQ),e(GQ,kje),e(Ef,Rje),e(Ef,T9),e(T9,Sje),e(Ef,Pje),e(v,$je),e(v,yf),e(yf,XQ),e(XQ,Ije),e(yf,jje),e(yf,F9),e(F9,Dje),e(yf,Nje),e(v,qje),e(v,wf),e(wf,VQ),e(VQ,Oje),e(wf,Gje),e(wf,C9),e(C9,Xje),e(wf,Vje),e(v,zje),e(v,Af),e(Af,zQ),e(zQ,Wje),e(Af,Qje),e(Af,M9),e(M9,Hje),e(Af,Uje),e(v,Jje),e(v,Lf),e(Lf,WQ),e(WQ,Yje),e(Lf,Kje),e(Lf,E9),e(E9,Zje),e(Lf,eDe),e(v,oDe),e(v,Bf),e(Bf,QQ),e(QQ,tDe),e(Bf,rDe),e(Bf,y9),e(y9,aDe),e(Bf,sDe),e(v,nDe),e(v,xf),e(xf,HQ),e(HQ,lDe),e(xf,iDe),e(xf,w9),e(w9,dDe),e(xf,cDe),e(v,mDe),e(v,kf),e(kf,UQ),e(UQ,fDe),e(kf,gDe),e(kf,A9),e(A9,hDe),e(kf,uDe),e(v,pDe),e(v,Rf),e(Rf,JQ),e(JQ,_De),e(Rf,bDe),e(Rf,L9),e(L9,vDe),e(Rf,TDe),e(v,FDe),e(v,Sf),e(Sf,YQ),e(YQ,CDe),e(Sf,MDe),e(Sf,B9),e(B9,EDe),e(Sf,yDe),e(v,wDe),e(v,Pf),e(Pf,KQ),e(KQ,ADe),e(Pf,LDe),e(Pf,x9),e(x9,BDe),e(Pf,xDe),e(v,kDe),e(v,$f),e($f,ZQ),e(ZQ,RDe),e($f,SDe),e($f,k9),e(k9,PDe),e($f,$De),e(v,IDe),e(v,If),e(If,eH),e(eH,jDe),e(If,DDe),e(If,R9),e(R9,NDe),e(If,qDe),e(v,ODe),e(v,jf),e(jf,oH),e(oH,GDe),e(jf,XDe),e(jf,S9),e(S9,VDe),e(jf,zDe),e(v,WDe),e(v,Df),e(Df,tH),e(tH,QDe),e(Df,HDe),e(Df,P9),e(P9,UDe),e(Df,JDe),e(v,YDe),e(v,Nf),e(Nf,rH),e(rH,KDe),e(Nf,ZDe),e(Nf,$9),e($9,eNe),e(Nf,oNe),e(v,tNe),e(v,qf),e(qf,aH),e(aH,rNe),e(qf,aNe),e(qf,I9),e(I9,sNe),e(qf,nNe),e(v,lNe),e(v,Of),e(Of,sH),e(sH,iNe),e(Of,dNe),e(Of,j9),e(j9,cNe),e(Of,mNe),e(v,fNe),e(v,Gf),e(Gf,nH),e(nH,gNe),e(Gf,hNe),e(Gf,D9),e(D9,uNe),e(Gf,pNe),e(v,_Ne),e(v,Xf),e(Xf,lH),e(lH,bNe),e(Xf,vNe),e(Xf,N9),e(N9,TNe),e(Xf,FNe),e(v,CNe),e(v,Vf),e(Vf,iH),e(iH,MNe),e(Vf,ENe),e(Vf,q9),e(q9,yNe),e(Vf,wNe),e(v,ANe),e(v,zf),e(zf,dH),e(dH,LNe),e(zf,BNe),e(zf,O9),e(O9,xNe),e(zf,kNe),e(v,RNe),e(v,Wf),e(Wf,cH),e(cH,SNe),e(Wf,PNe),e(Wf,G9),e(G9,$Ne),e(Wf,INe),e(v,jNe),e(v,Qf),e(Qf,mH),e(mH,DNe),e(Qf,NNe),e(Qf,X9),e(X9,qNe),e(Qf,ONe),e(v,GNe),e(v,Hf),e(Hf,fH),e(fH,XNe),e(Hf,VNe),e(Hf,V9),e(V9,zNe),e(Hf,WNe),e(v,QNe),e(v,Uf),e(Uf,gH),e(gH,HNe),e(Uf,UNe),e(Uf,z9),e(z9,JNe),e(Uf,YNe),e(v,KNe),e(v,Jf),e(Jf,hH),e(hH,ZNe),e(Jf,eqe),e(Jf,W9),e(W9,oqe),e(Jf,tqe),e(v,rqe),e(v,Yf),e(Yf,uH),e(uH,aqe),e(Yf,sqe),e(Yf,Q9),e(Q9,nqe),e(Yf,lqe),e(v,iqe),e(v,Kf),e(Kf,pH),e(pH,dqe),e(Kf,cqe),e(Kf,H9),e(H9,mqe),e(Kf,fqe),e(v,gqe),e(v,Zf),e(Zf,_H),e(_H,hqe),e(Zf,uqe),e(Zf,U9),e(U9,pqe),e(Zf,_qe),e(v,bqe),e(v,eg),e(eg,bH),e(bH,vqe),e(eg,Tqe),e(eg,J9),e(J9,Fqe),e(eg,Cqe),e(v,Mqe),e(v,og),e(og,vH),e(vH,Eqe),e(og,yqe),e(og,Y9),e(Y9,wqe),e(og,Aqe),e(v,Lqe),e(v,tg),e(tg,TH),e(TH,Bqe),e(tg,xqe),e(tg,K9),e(K9,kqe),e(tg,Rqe),e(v,Sqe),e(v,rg),e(rg,FH),e(FH,Pqe),e(rg,$qe),e(rg,Z9),e(Z9,Iqe),e(rg,jqe),e(v,Dqe),e(v,ag),e(ag,CH),e(CH,Nqe),e(ag,qqe),e(ag,eB),e(eB,Oqe),e(ag,Gqe),e(v,Xqe),e(v,sg),e(sg,MH),e(MH,Vqe),e(sg,zqe),e(sg,oB),e(oB,Wqe),e(sg,Qqe),e(v,Hqe),e(v,ng),e(ng,EH),e(EH,Uqe),e(ng,Jqe),e(ng,tB),e(tB,Yqe),e(ng,Kqe),e(v,Zqe),e(v,lg),e(lg,yH),e(yH,eOe),e(lg,oOe),e(lg,rB),e(rB,tOe),e(lg,rOe),e(v,aOe),e(v,ig),e(ig,wH),e(wH,sOe),e(ig,nOe),e(ig,aB),e(aB,lOe),e(ig,iOe),e(v,dOe),e(v,dg),e(dg,AH),e(AH,cOe),e(dg,mOe),e(dg,sB),e(sB,fOe),e(dg,gOe),e(v,hOe),e(v,cg),e(cg,LH),e(LH,uOe),e(cg,pOe),e(cg,nB),e(nB,_Oe),e(cg,bOe),e(v,vOe),e(v,mg),e(mg,BH),e(BH,TOe),e(mg,FOe),e(mg,lB),e(lB,COe),e(mg,MOe),e(v,EOe),e(v,fg),e(fg,xH),e(xH,yOe),e(fg,wOe),e(fg,iB),e(iB,AOe),e(fg,LOe),e(v,BOe),e(v,gg),e(gg,kH),e(kH,xOe),e(gg,kOe),e(gg,dB),e(dB,ROe),e(gg,SOe),e(v,POe),e(v,hg),e(hg,RH),e(RH,$Oe),e(hg,IOe),e(hg,cB),e(cB,jOe),e(hg,DOe),e(v,NOe),e(v,ug),e(ug,SH),e(SH,qOe),e(ug,OOe),e(ug,mB),e(mB,GOe),e(ug,XOe),e(v,VOe),e(v,pg),e(pg,PH),e(PH,zOe),e(pg,WOe),e(pg,fB),e(fB,QOe),e(pg,HOe),e(v,UOe),e(v,_g),e(_g,$H),e($H,JOe),e(_g,YOe),e(_g,gB),e(gB,KOe),e(_g,ZOe),e(v,eGe),e(v,bg),e(bg,IH),e(IH,oGe),e(bg,tGe),e(bg,hB),e(hB,rGe),e(bg,aGe),e(v,sGe),e(v,vg),e(vg,jH),e(jH,nGe),e(vg,lGe),e(vg,uB),e(uB,iGe),e(vg,dGe),e(v,cGe),e(v,Tg),e(Tg,DH),e(DH,mGe),e(Tg,fGe),e(Tg,pB),e(pB,gGe),e(Tg,hGe),e(v,uGe),e(v,Fg),e(Fg,NH),e(NH,pGe),e(Fg,_Ge),e(Fg,_B),e(_B,bGe),e(Fg,vGe),e(v,TGe),e(v,Cg),e(Cg,qH),e(qH,FGe),e(Cg,CGe),e(Cg,bB),e(bB,MGe),e(Cg,EGe),e(v,yGe),e(v,Mg),e(Mg,OH),e(OH,wGe),e(Mg,AGe),e(Mg,vB),e(vB,LGe),e(Mg,BGe),e(v,xGe),e(v,Eg),e(Eg,GH),e(GH,kGe),e(Eg,RGe),e(Eg,TB),e(TB,SGe),e(Eg,PGe),e(v,$Ge),e(v,yg),e(yg,XH),e(XH,IGe),e(yg,jGe),e(yg,FB),e(FB,DGe),e(yg,NGe),e(v,qGe),e(v,wg),e(wg,VH),e(VH,OGe),e(wg,GGe),e(wg,CB),e(CB,XGe),e(wg,VGe),e(v,zGe),e(v,Ag),e(Ag,zH),e(zH,WGe),e(Ag,QGe),e(Ag,MB),e(MB,HGe),e(Ag,UGe),e(fo,JGe),e(fo,WH),e(WH,YGe),e(fo,KGe),g(U5,fo,null),e(Vo,ZGe),e(Vo,Lg),g(J5,Lg,null),e(Lg,eXe),e(Lg,QH),e(QH,oXe),b(d,kBe,_),b(d,Wi,_),e(Wi,Bg),e(Bg,HH),g(Y5,HH,null),e(Wi,tXe),e(Wi,UH),e(UH,rXe),b(d,RBe,_),b(d,zo,_),g(K5,zo,null),e(zo,aXe),e(zo,Z5),e(Z5,sXe),e(Z5,EB),e(EB,nXe),e(Z5,lXe),e(zo,iXe),e(zo,ey),e(ey,dXe),e(ey,JH),e(JH,cXe),e(ey,mXe),e(zo,fXe),e(zo,go),g(oy,go,null),e(go,gXe),e(go,YH),e(YH,hXe),e(go,uXe),e(go,Oa),e(Oa,pXe),e(Oa,KH),e(KH,_Xe),e(Oa,bXe),e(Oa,ZH),e(ZH,vXe),e(Oa,TXe),e(Oa,eU),e(eU,FXe),e(Oa,CXe),e(go,MXe),e(go,E),e(E,zs),e(zs,oU),e(oU,EXe),e(zs,yXe),e(zs,yB),e(yB,wXe),e(zs,AXe),e(zs,wB),e(wB,LXe),e(zs,BXe),e(E,xXe),e(E,Ws),e(Ws,tU),e(tU,kXe),e(Ws,RXe),e(Ws,AB),e(AB,SXe),e(Ws,PXe),e(Ws,LB),e(LB,$Xe),e(Ws,IXe),e(E,jXe),e(E,Qs),e(Qs,rU),e(rU,DXe),e(Qs,NXe),e(Qs,BB),e(BB,qXe),e(Qs,OXe),e(Qs,xB),e(xB,GXe),e(Qs,XXe),e(E,VXe),e(E,xg),e(xg,aU),e(aU,zXe),e(xg,WXe),e(xg,kB),e(kB,QXe),e(xg,HXe),e(E,UXe),e(E,Hs),e(Hs,sU),e(sU,JXe),e(Hs,YXe),e(Hs,RB),e(RB,KXe),e(Hs,ZXe),e(Hs,SB),e(SB,eVe),e(Hs,oVe),e(E,tVe),e(E,kg),e(kg,nU),e(nU,rVe),e(kg,aVe),e(kg,PB),e(PB,sVe),e(kg,nVe),e(E,lVe),e(E,Rg),e(Rg,lU),e(lU,iVe),e(Rg,dVe),e(Rg,$B),e($B,cVe),e(Rg,mVe),e(E,fVe),e(E,Sg),e(Sg,iU),e(iU,gVe),e(Sg,hVe),e(Sg,IB),e(IB,uVe),e(Sg,pVe),e(E,_Ve),e(E,Us),e(Us,dU),e(dU,bVe),e(Us,vVe),e(Us,jB),e(jB,TVe),e(Us,FVe),e(Us,DB),e(DB,CVe),e(Us,MVe),e(E,EVe),e(E,Js),e(Js,cU),e(cU,yVe),e(Js,wVe),e(Js,NB),e(NB,AVe),e(Js,LVe),e(Js,qB),e(qB,BVe),e(Js,xVe),e(E,kVe),e(E,Ys),e(Ys,mU),e(mU,RVe),e(Ys,SVe),e(Ys,OB),e(OB,PVe),e(Ys,$Ve),e(Ys,GB),e(GB,IVe),e(Ys,jVe),e(E,DVe),e(E,Pg),e(Pg,fU),e(fU,NVe),e(Pg,qVe),e(Pg,XB),e(XB,OVe),e(Pg,GVe),e(E,XVe),e(E,$g),e($g,gU),e(gU,VVe),e($g,zVe),e($g,VB),e(VB,WVe),e($g,QVe),e(E,HVe),e(E,Ks),e(Ks,hU),e(hU,UVe),e(Ks,JVe),e(Ks,zB),e(zB,YVe),e(Ks,KVe),e(Ks,WB),e(WB,ZVe),e(Ks,eze),e(E,oze),e(E,Ig),e(Ig,uU),e(uU,tze),e(Ig,rze),e(Ig,QB),e(QB,aze),e(Ig,sze),e(E,nze),e(E,Zs),e(Zs,pU),e(pU,lze),e(Zs,ize),e(Zs,HB),e(HB,dze),e(Zs,cze),e(Zs,UB),e(UB,mze),e(Zs,fze),e(E,gze),e(E,en),e(en,_U),e(_U,hze),e(en,uze),e(en,JB),e(JB,pze),e(en,_ze),e(en,YB),e(YB,bze),e(en,vze),e(E,Tze),e(E,on),e(on,bU),e(bU,Fze),e(on,Cze),e(on,KB),e(KB,Mze),e(on,Eze),e(on,vU),e(vU,yze),e(on,wze),e(E,Aze),e(E,jg),e(jg,TU),e(TU,Lze),e(jg,Bze),e(jg,ZB),e(ZB,xze),e(jg,kze),e(E,Rze),e(E,tn),e(tn,FU),e(FU,Sze),e(tn,Pze),e(tn,ex),e(ex,$ze),e(tn,Ize),e(tn,ox),e(ox,jze),e(tn,Dze),e(E,Nze),e(E,Dg),e(Dg,CU),e(CU,qze),e(Dg,Oze),e(Dg,tx),e(tx,Gze),e(Dg,Xze),e(E,Vze),e(E,rn),e(rn,MU),e(MU,zze),e(rn,Wze),e(rn,rx),e(rx,Qze),e(rn,Hze),e(rn,ax),e(ax,Uze),e(rn,Jze),e(E,Yze),e(E,an),e(an,EU),e(EU,Kze),e(an,Zze),e(an,sx),e(sx,eWe),e(an,oWe),e(an,nx),e(nx,tWe),e(an,rWe),e(E,aWe),e(E,sn),e(sn,yU),e(yU,sWe),e(sn,nWe),e(sn,lx),e(lx,lWe),e(sn,iWe),e(sn,ix),e(ix,dWe),e(sn,cWe),e(E,mWe),e(E,Ng),e(Ng,wU),e(wU,fWe),e(Ng,gWe),e(Ng,dx),e(dx,hWe),e(Ng,uWe),e(E,pWe),e(E,nn),e(nn,AU),e(AU,_We),e(nn,bWe),e(nn,cx),e(cx,vWe),e(nn,TWe),e(nn,mx),e(mx,FWe),e(nn,CWe),e(E,MWe),e(E,qg),e(qg,LU),e(LU,EWe),e(qg,yWe),e(qg,fx),e(fx,wWe),e(qg,AWe),e(E,LWe),e(E,ln),e(ln,BU),e(BU,BWe),e(ln,xWe),e(ln,gx),e(gx,kWe),e(ln,RWe),e(ln,hx),e(hx,SWe),e(ln,PWe),e(E,$We),e(E,dn),e(dn,xU),e(xU,IWe),e(dn,jWe),e(dn,ux),e(ux,DWe),e(dn,NWe),e(dn,px),e(px,qWe),e(dn,OWe),e(E,GWe),e(E,cn),e(cn,kU),e(kU,XWe),e(cn,VWe),e(cn,_x),e(_x,zWe),e(cn,WWe),e(cn,bx),e(bx,QWe),e(cn,HWe),e(E,UWe),e(E,mn),e(mn,RU),e(RU,JWe),e(mn,YWe),e(mn,vx),e(vx,KWe),e(mn,ZWe),e(mn,Tx),e(Tx,eQe),e(mn,oQe),e(E,tQe),e(E,Og),e(Og,SU),e(SU,rQe),e(Og,aQe),e(Og,Fx),e(Fx,sQe),e(Og,nQe),e(E,lQe),e(E,fn),e(fn,PU),e(PU,iQe),e(fn,dQe),e(fn,Cx),e(Cx,cQe),e(fn,mQe),e(fn,Mx),e(Mx,fQe),e(fn,gQe),e(E,hQe),e(E,gn),e(gn,$U),e($U,uQe),e(gn,pQe),e(gn,Ex),e(Ex,_Qe),e(gn,bQe),e(gn,yx),e(yx,vQe),e(gn,TQe),e(E,FQe),e(E,hn),e(hn,IU),e(IU,CQe),e(hn,MQe),e(hn,wx),e(wx,EQe),e(hn,yQe),e(hn,Ax),e(Ax,wQe),e(hn,AQe),e(E,LQe),e(E,un),e(un,jU),e(jU,BQe),e(un,xQe),e(un,Lx),e(Lx,kQe),e(un,RQe),e(un,Bx),e(Bx,SQe),e(un,PQe),e(E,$Qe),e(E,pn),e(pn,DU),e(DU,IQe),e(pn,jQe),e(pn,xx),e(xx,DQe),e(pn,NQe),e(pn,kx),e(kx,qQe),e(pn,OQe),e(E,GQe),e(E,_n),e(_n,NU),e(NU,XQe),e(_n,VQe),e(_n,Rx),e(Rx,zQe),e(_n,WQe),e(_n,Sx),e(Sx,QQe),e(_n,HQe),e(E,UQe),e(E,Gg),e(Gg,qU),e(qU,JQe),e(Gg,YQe),e(Gg,Px),e(Px,KQe),e(Gg,ZQe),e(E,eHe),e(E,bn),e(bn,OU),e(OU,oHe),e(bn,tHe),e(bn,$x),e($x,rHe),e(bn,aHe),e(bn,Ix),e(Ix,sHe),e(bn,nHe),e(E,lHe),e(E,Xg),e(Xg,GU),e(GU,iHe),e(Xg,dHe),e(Xg,jx),e(jx,cHe),e(Xg,mHe),e(E,fHe),e(E,Vg),e(Vg,XU),e(XU,gHe),e(Vg,hHe),e(Vg,Dx),e(Dx,uHe),e(Vg,pHe),e(E,_He),e(E,vn),e(vn,VU),e(VU,bHe),e(vn,vHe),e(vn,Nx),e(Nx,THe),e(vn,FHe),e(vn,qx),e(qx,CHe),e(vn,MHe),e(E,EHe),e(E,Tn),e(Tn,zU),e(zU,yHe),e(Tn,wHe),e(Tn,Ox),e(Ox,AHe),e(Tn,LHe),e(Tn,Gx),e(Gx,BHe),e(Tn,xHe),e(E,kHe),e(E,zg),e(zg,WU),e(WU,RHe),e(zg,SHe),e(zg,Xx),e(Xx,PHe),e(zg,$He),e(E,IHe),e(E,Fn),e(Fn,QU),e(QU,jHe),e(Fn,DHe),e(Fn,Vx),e(Vx,NHe),e(Fn,qHe),e(Fn,zx),e(zx,OHe),e(Fn,GHe),e(E,XHe),e(E,Cn),e(Cn,HU),e(HU,VHe),e(Cn,zHe),e(Cn,Wx),e(Wx,WHe),e(Cn,QHe),e(Cn,Qx),e(Qx,HHe),e(Cn,UHe),e(E,JHe),e(E,Mn),e(Mn,UU),e(UU,YHe),e(Mn,KHe),e(Mn,Hx),e(Hx,ZHe),e(Mn,eUe),e(Mn,Ux),e(Ux,oUe),e(Mn,tUe),e(E,rUe),e(E,En),e(En,JU),e(JU,aUe),e(En,sUe),e(En,Jx),e(Jx,nUe),e(En,lUe),e(En,Yx),e(Yx,iUe),e(En,dUe),e(E,cUe),e(E,yn),e(yn,YU),e(YU,mUe),e(yn,fUe),e(yn,Kx),e(Kx,gUe),e(yn,hUe),e(yn,Zx),e(Zx,uUe),e(yn,pUe),e(E,_Ue),e(E,Wg),e(Wg,KU),e(KU,bUe),e(Wg,vUe),e(Wg,ek),e(ek,TUe),e(Wg,FUe),e(E,CUe),e(E,Qg),e(Qg,ZU),e(ZU,MUe),e(Qg,EUe),e(Qg,ok),e(ok,yUe),e(Qg,wUe),e(E,AUe),e(E,Hg),e(Hg,eJ),e(eJ,LUe),e(Hg,BUe),e(Hg,tk),e(tk,xUe),e(Hg,kUe),e(E,RUe),e(E,Ug),e(Ug,oJ),e(oJ,SUe),e(Ug,PUe),e(Ug,rk),e(rk,$Ue),e(Ug,IUe),e(E,jUe),e(E,wn),e(wn,tJ),e(tJ,DUe),e(wn,NUe),e(wn,ak),e(ak,qUe),e(wn,OUe),e(wn,sk),e(sk,GUe),e(wn,XUe),e(E,VUe),e(E,Jg),e(Jg,rJ),e(rJ,zUe),e(Jg,WUe),e(Jg,nk),e(nk,QUe),e(Jg,HUe),e(E,UUe),e(E,An),e(An,aJ),e(aJ,JUe),e(An,YUe),e(An,lk),e(lk,KUe),e(An,ZUe),e(An,ik),e(ik,eJe),e(An,oJe),e(E,tJe),e(E,Ln),e(Ln,sJ),e(sJ,rJe),e(Ln,aJe),e(Ln,dk),e(dk,sJe),e(Ln,nJe),e(Ln,ck),e(ck,lJe),e(Ln,iJe),e(E,dJe),e(E,Bn),e(Bn,nJ),e(nJ,cJe),e(Bn,mJe),e(Bn,mk),e(mk,fJe),e(Bn,gJe),e(Bn,fk),e(fk,hJe),e(Bn,uJe),e(E,pJe),e(E,xn),e(xn,lJ),e(lJ,_Je),e(xn,bJe),e(xn,gk),e(gk,vJe),e(xn,TJe),e(xn,hk),e(hk,FJe),e(xn,CJe),e(E,MJe),e(E,kn),e(kn,iJ),e(iJ,EJe),e(kn,yJe),e(kn,uk),e(uk,wJe),e(kn,AJe),e(kn,pk),e(pk,LJe),e(kn,BJe),e(E,xJe),e(E,Rn),e(Rn,dJ),e(dJ,kJe),e(Rn,RJe),e(Rn,_k),e(_k,SJe),e(Rn,PJe),e(Rn,bk),e(bk,$Je),e(Rn,IJe),e(E,jJe),e(E,Yg),e(Yg,cJ),e(cJ,DJe),e(Yg,NJe),e(Yg,vk),e(vk,qJe),e(Yg,OJe),e(E,GJe),e(E,Kg),e(Kg,mJ),e(mJ,XJe),e(Kg,VJe),e(Kg,Tk),e(Tk,zJe),e(Kg,WJe),e(E,QJe),e(E,Sn),e(Sn,fJ),e(fJ,HJe),e(Sn,UJe),e(Sn,Fk),e(Fk,JJe),e(Sn,YJe),e(Sn,Ck),e(Ck,KJe),e(Sn,ZJe),e(E,eYe),e(E,Pn),e(Pn,gJ),e(gJ,oYe),e(Pn,tYe),e(Pn,Mk),e(Mk,rYe),e(Pn,aYe),e(Pn,Ek),e(Ek,sYe),e(Pn,nYe),e(E,lYe),e(E,$n),e($n,hJ),e(hJ,iYe),e($n,dYe),e($n,yk),e(yk,cYe),e($n,mYe),e($n,wk),e(wk,fYe),e($n,gYe),e(E,hYe),e(E,Zg),e(Zg,uJ),e(uJ,uYe),e(Zg,pYe),e(Zg,Ak),e(Ak,_Ye),e(Zg,bYe),e(E,vYe),e(E,eh),e(eh,pJ),e(pJ,TYe),e(eh,FYe),e(eh,Lk),e(Lk,CYe),e(eh,MYe),e(E,EYe),e(E,oh),e(oh,_J),e(_J,yYe),e(oh,wYe),e(oh,Bk),e(Bk,AYe),e(oh,LYe),e(E,BYe),e(E,th),e(th,bJ),e(bJ,xYe),e(th,kYe),e(th,xk),e(xk,RYe),e(th,SYe),e(E,PYe),e(E,In),e(In,vJ),e(vJ,$Ye),e(In,IYe),e(In,kk),e(kk,jYe),e(In,DYe),e(In,Rk),e(Rk,NYe),e(In,qYe),e(E,OYe),e(E,rh),e(rh,TJ),e(TJ,GYe),e(rh,XYe),e(rh,Sk),e(Sk,VYe),e(rh,zYe),e(E,WYe),e(E,ah),e(ah,FJ),e(FJ,QYe),e(ah,HYe),e(ah,Pk),e(Pk,UYe),e(ah,JYe),e(E,YYe),e(E,jn),e(jn,CJ),e(CJ,KYe),e(jn,ZYe),e(jn,$k),e($k,eKe),e(jn,oKe),e(jn,Ik),e(Ik,tKe),e(jn,rKe),e(E,aKe),e(E,Dn),e(Dn,MJ),e(MJ,sKe),e(Dn,nKe),e(Dn,jk),e(jk,lKe),e(Dn,iKe),e(Dn,Dk),e(Dk,dKe),e(Dn,cKe),e(go,mKe),e(go,EJ),e(EJ,fKe),e(go,gKe),g(ty,go,null),e(zo,hKe),e(zo,sh),g(ry,sh,null),e(sh,uKe),e(sh,yJ),e(yJ,pKe),b(d,SBe,_),b(d,Qi,_),e(Qi,nh),e(nh,wJ),g(ay,wJ,null),e(Qi,_Ke),e(Qi,AJ),e(AJ,bKe),b(d,PBe,_),b(d,Wo,_),g(sy,Wo,null),e(Wo,vKe),e(Wo,ny),e(ny,TKe),e(ny,Nk),e(Nk,FKe),e(ny,CKe),e(Wo,MKe),e(Wo,ly),e(ly,EKe),e(ly,LJ),e(LJ,yKe),e(ly,wKe),e(Wo,AKe),e(Wo,xe),g(iy,xe,null),e(xe,LKe),e(xe,BJ),e(BJ,BKe),e(xe,xKe),e(xe,Ga),e(Ga,kKe),e(Ga,xJ),e(xJ,RKe),e(Ga,SKe),e(Ga,kJ),e(kJ,PKe),e(Ga,$Ke),e(Ga,RJ),e(RJ,IKe),e(Ga,jKe),e(xe,DKe),e(xe,ae),e(ae,lh),e(lh,SJ),e(SJ,NKe),e(lh,qKe),e(lh,qk),e(qk,OKe),e(lh,GKe),e(ae,XKe),e(ae,ih),e(ih,PJ),e(PJ,VKe),e(ih,zKe),e(ih,Ok),e(Ok,WKe),e(ih,QKe),e(ae,HKe),e(ae,dh),e(dh,$J),e($J,UKe),e(dh,JKe),e(dh,Gk),e(Gk,YKe),e(dh,KKe),e(ae,ZKe),e(ae,ch),e(ch,IJ),e(IJ,eZe),e(ch,oZe),e(ch,Xk),e(Xk,tZe),e(ch,rZe),e(ae,aZe),e(ae,mh),e(mh,jJ),e(jJ,sZe),e(mh,nZe),e(mh,Vk),e(Vk,lZe),e(mh,iZe),e(ae,dZe),e(ae,fh),e(fh,DJ),e(DJ,cZe),e(fh,mZe),e(fh,zk),e(zk,fZe),e(fh,gZe),e(ae,hZe),e(ae,gh),e(gh,NJ),e(NJ,uZe),e(gh,pZe),e(gh,Wk),e(Wk,_Ze),e(gh,bZe),e(ae,vZe),e(ae,hh),e(hh,qJ),e(qJ,TZe),e(hh,FZe),e(hh,Qk),e(Qk,CZe),e(hh,MZe),e(ae,EZe),e(ae,uh),e(uh,OJ),e(OJ,yZe),e(uh,wZe),e(uh,Hk),e(Hk,AZe),e(uh,LZe),e(ae,BZe),e(ae,ph),e(ph,GJ),e(GJ,xZe),e(ph,kZe),e(ph,Uk),e(Uk,RZe),e(ph,SZe),e(ae,PZe),e(ae,_h),e(_h,XJ),e(XJ,$Ze),e(_h,IZe),e(_h,Jk),e(Jk,jZe),e(_h,DZe),e(ae,NZe),e(ae,bh),e(bh,VJ),e(VJ,qZe),e(bh,OZe),e(bh,Yk),e(Yk,GZe),e(bh,XZe),e(ae,VZe),e(ae,vh),e(vh,zJ),e(zJ,zZe),e(vh,WZe),e(vh,Kk),e(Kk,QZe),e(vh,HZe),e(ae,UZe),e(ae,Th),e(Th,WJ),e(WJ,JZe),e(Th,YZe),e(Th,Zk),e(Zk,KZe),e(Th,ZZe),e(ae,eeo),e(ae,Fh),e(Fh,QJ),e(QJ,oeo),e(Fh,teo),e(Fh,eR),e(eR,reo),e(Fh,aeo),e(ae,seo),e(ae,Ch),e(Ch,HJ),e(HJ,neo),e(Ch,leo),e(Ch,oR),e(oR,ieo),e(Ch,deo),e(xe,ceo),g(Mh,xe,null),e(xe,meo),e(xe,UJ),e(UJ,feo),e(xe,geo),g(dy,xe,null),e(Wo,heo),e(Wo,Eh),g(cy,Eh,null),e(Eh,ueo),e(Eh,JJ),e(JJ,peo),b(d,$Be,_),b(d,Hi,_),e(Hi,yh),e(yh,YJ),g(my,YJ,null),e(Hi,_eo),e(Hi,KJ),e(KJ,beo),b(d,IBe,_),b(d,Qo,_),g(fy,Qo,null),e(Qo,veo),e(Qo,gy),e(gy,Teo),e(gy,tR),e(tR,Feo),e(gy,Ceo),e(Qo,Meo),e(Qo,hy),e(hy,Eeo),e(hy,ZJ),e(ZJ,yeo),e(hy,weo),e(Qo,Aeo),e(Qo,ke),g(uy,ke,null),e(ke,Leo),e(ke,eY),e(eY,Beo),e(ke,xeo),e(ke,Ui),e(Ui,keo),e(Ui,oY),e(oY,Reo),e(Ui,Seo),e(Ui,tY),e(tY,Peo),e(Ui,$eo),e(ke,Ieo),e(ke,we),e(we,wh),e(wh,rY),e(rY,jeo),e(wh,Deo),e(wh,rR),e(rR,Neo),e(wh,qeo),e(we,Oeo),e(we,Ah),e(Ah,aY),e(aY,Geo),e(Ah,Xeo),e(Ah,aR),e(aR,Veo),e(Ah,zeo),e(we,Weo),e(we,Lh),e(Lh,sY),e(sY,Qeo),e(Lh,Heo),e(Lh,sR),e(sR,Ueo),e(Lh,Jeo),e(we,Yeo),e(we,Bh),e(Bh,nY),e(nY,Keo),e(Bh,Zeo),e(Bh,nR),e(nR,eoo),e(Bh,ooo),e(we,too),e(we,xh),e(xh,lY),e(lY,roo),e(xh,aoo),e(xh,lR),e(lR,soo),e(xh,noo),e(we,loo),e(we,kh),e(kh,iY),e(iY,ioo),e(kh,doo),e(kh,iR),e(iR,coo),e(kh,moo),e(we,foo),e(we,Rh),e(Rh,dY),e(dY,goo),e(Rh,hoo),e(Rh,dR),e(dR,uoo),e(Rh,poo),e(we,_oo),e(we,Sh),e(Sh,cY),e(cY,boo),e(Sh,voo),e(Sh,cR),e(cR,Too),e(Sh,Foo),e(ke,Coo),g(Ph,ke,null),e(ke,Moo),e(ke,mY),e(mY,Eoo),e(ke,yoo),g(py,ke,null),e(Qo,woo),e(Qo,$h),g(_y,$h,null),e($h,Aoo),e($h,fY),e(fY,Loo),b(d,jBe,_),b(d,Ji,_),e(Ji,Ih),e(Ih,gY),g(by,gY,null),e(Ji,Boo),e(Ji,hY),e(hY,xoo),b(d,DBe,_),b(d,Ho,_),g(vy,Ho,null),e(Ho,koo),e(Ho,Yi),e(Yi,Roo),e(Yi,uY),e(uY,Soo),e(Yi,Poo),e(Yi,pY),e(pY,$oo),e(Yi,Ioo),e(Ho,joo),e(Ho,Ty),e(Ty,Doo),e(Ty,_Y),e(_Y,Noo),e(Ty,qoo),e(Ho,Ooo),e(Ho,Gt),g(Fy,Gt,null),e(Gt,Goo),e(Gt,bY),e(bY,Xoo),e(Gt,Voo),e(Gt,Ki),e(Ki,zoo),e(Ki,vY),e(vY,Woo),e(Ki,Qoo),e(Ki,TY),e(TY,Hoo),e(Ki,Uoo),e(Gt,Joo),e(Gt,FY),e(FY,Yoo),e(Gt,Koo),g(Cy,Gt,null),e(Ho,Zoo),e(Ho,Re),g(My,Re,null),e(Re,eto),e(Re,CY),e(CY,oto),e(Re,tto),e(Re,Xa),e(Xa,rto),e(Xa,MY),e(MY,ato),e(Xa,sto),e(Xa,EY),e(EY,nto),e(Xa,lto),e(Xa,yY),e(yY,ito),e(Xa,dto),e(Re,cto),e(Re,F),e(F,jh),e(jh,wY),e(wY,mto),e(jh,fto),e(jh,mR),e(mR,gto),e(jh,hto),e(F,uto),e(F,Dh),e(Dh,AY),e(AY,pto),e(Dh,_to),e(Dh,fR),e(fR,bto),e(Dh,vto),e(F,Tto),e(F,Nh),e(Nh,LY),e(LY,Fto),e(Nh,Cto),e(Nh,gR),e(gR,Mto),e(Nh,Eto),e(F,yto),e(F,qh),e(qh,BY),e(BY,wto),e(qh,Ato),e(qh,hR),e(hR,Lto),e(qh,Bto),e(F,xto),e(F,Oh),e(Oh,xY),e(xY,kto),e(Oh,Rto),e(Oh,uR),e(uR,Sto),e(Oh,Pto),e(F,$to),e(F,Gh),e(Gh,kY),e(kY,Ito),e(Gh,jto),e(Gh,pR),e(pR,Dto),e(Gh,Nto),e(F,qto),e(F,Xh),e(Xh,RY),e(RY,Oto),e(Xh,Gto),e(Xh,_R),e(_R,Xto),e(Xh,Vto),e(F,zto),e(F,Vh),e(Vh,SY),e(SY,Wto),e(Vh,Qto),e(Vh,bR),e(bR,Hto),e(Vh,Uto),e(F,Jto),e(F,zh),e(zh,PY),e(PY,Yto),e(zh,Kto),e(zh,vR),e(vR,Zto),e(zh,ero),e(F,oro),e(F,Wh),e(Wh,$Y),e($Y,tro),e(Wh,rro),e(Wh,TR),e(TR,aro),e(Wh,sro),e(F,nro),e(F,Qh),e(Qh,IY),e(IY,lro),e(Qh,iro),e(Qh,FR),e(FR,dro),e(Qh,cro),e(F,mro),e(F,Hh),e(Hh,jY),e(jY,fro),e(Hh,gro),e(Hh,CR),e(CR,hro),e(Hh,uro),e(F,pro),e(F,Uh),e(Uh,DY),e(DY,_ro),e(Uh,bro),e(Uh,MR),e(MR,vro),e(Uh,Tro),e(F,Fro),e(F,Jh),e(Jh,NY),e(NY,Cro),e(Jh,Mro),e(Jh,ER),e(ER,Ero),e(Jh,yro),e(F,wro),e(F,Yh),e(Yh,qY),e(qY,Aro),e(Yh,Lro),e(Yh,yR),e(yR,Bro),e(Yh,xro),e(F,kro),e(F,Kh),e(Kh,OY),e(OY,Rro),e(Kh,Sro),e(Kh,wR),e(wR,Pro),e(Kh,$ro),e(F,Iro),e(F,Zh),e(Zh,GY),e(GY,jro),e(Zh,Dro),e(Zh,AR),e(AR,Nro),e(Zh,qro),e(F,Oro),e(F,eu),e(eu,XY),e(XY,Gro),e(eu,Xro),e(eu,LR),e(LR,Vro),e(eu,zro),e(F,Wro),e(F,ou),e(ou,VY),e(VY,Qro),e(ou,Hro),e(ou,BR),e(BR,Uro),e(ou,Jro),e(F,Yro),e(F,tu),e(tu,zY),e(zY,Kro),e(tu,Zro),e(tu,xR),e(xR,eao),e(tu,oao),e(F,tao),e(F,ru),e(ru,WY),e(WY,rao),e(ru,aao),e(ru,kR),e(kR,sao),e(ru,nao),e(F,lao),e(F,au),e(au,QY),e(QY,iao),e(au,dao),e(au,RR),e(RR,cao),e(au,mao),e(F,fao),e(F,su),e(su,HY),e(HY,gao),e(su,hao),e(su,SR),e(SR,uao),e(su,pao),e(F,_ao),e(F,nu),e(nu,UY),e(UY,bao),e(nu,vao),e(nu,PR),e(PR,Tao),e(nu,Fao),e(F,Cao),e(F,lu),e(lu,JY),e(JY,Mao),e(lu,Eao),e(lu,$R),e($R,yao),e(lu,wao),e(F,Aao),e(F,iu),e(iu,YY),e(YY,Lao),e(iu,Bao),e(iu,IR),e(IR,xao),e(iu,kao),e(F,Rao),e(F,du),e(du,KY),e(KY,Sao),e(du,Pao),e(du,jR),e(jR,$ao),e(du,Iao),e(F,jao),e(F,Nn),e(Nn,ZY),e(ZY,Dao),e(Nn,Nao),e(Nn,DR),e(DR,qao),e(Nn,Oao),e(Nn,NR),e(NR,Gao),e(Nn,Xao),e(F,Vao),e(F,cu),e(cu,eK),e(eK,zao),e(cu,Wao),e(cu,qR),e(qR,Qao),e(cu,Hao),e(F,Uao),e(F,mu),e(mu,oK),e(oK,Jao),e(mu,Yao),e(mu,OR),e(OR,Kao),e(mu,Zao),e(F,eso),e(F,fu),e(fu,tK),e(tK,oso),e(fu,tso),e(fu,GR),e(GR,rso),e(fu,aso),e(F,sso),e(F,gu),e(gu,rK),e(rK,nso),e(gu,lso),e(gu,XR),e(XR,iso),e(gu,dso),e(F,cso),e(F,hu),e(hu,aK),e(aK,mso),e(hu,fso),e(hu,VR),e(VR,gso),e(hu,hso),e(F,uso),e(F,uu),e(uu,sK),e(sK,pso),e(uu,_so),e(uu,zR),e(zR,bso),e(uu,vso),e(F,Tso),e(F,pu),e(pu,nK),e(nK,Fso),e(pu,Cso),e(pu,WR),e(WR,Mso),e(pu,Eso),e(F,yso),e(F,_u),e(_u,lK),e(lK,wso),e(_u,Aso),e(_u,QR),e(QR,Lso),e(_u,Bso),e(F,xso),e(F,bu),e(bu,iK),e(iK,kso),e(bu,Rso),e(bu,HR),e(HR,Sso),e(bu,Pso),e(F,$so),e(F,vu),e(vu,dK),e(dK,Iso),e(vu,jso),e(vu,UR),e(UR,Dso),e(vu,Nso),e(F,qso),e(F,Tu),e(Tu,cK),e(cK,Oso),e(Tu,Gso),e(Tu,JR),e(JR,Xso),e(Tu,Vso),e(F,zso),e(F,Fu),e(Fu,mK),e(mK,Wso),e(Fu,Qso),e(Fu,YR),e(YR,Hso),e(Fu,Uso),e(F,Jso),e(F,Cu),e(Cu,fK),e(fK,Yso),e(Cu,Kso),e(Cu,KR),e(KR,Zso),e(Cu,eno),e(F,ono),e(F,Mu),e(Mu,gK),e(gK,tno),e(Mu,rno),e(Mu,ZR),e(ZR,ano),e(Mu,sno),e(F,nno),e(F,Eu),e(Eu,hK),e(hK,lno),e(Eu,ino),e(Eu,eS),e(eS,dno),e(Eu,cno),e(F,mno),e(F,yu),e(yu,uK),e(uK,fno),e(yu,gno),e(yu,oS),e(oS,hno),e(yu,uno),e(F,pno),e(F,wu),e(wu,pK),e(pK,_no),e(wu,bno),e(wu,tS),e(tS,vno),e(wu,Tno),e(F,Fno),e(F,Au),e(Au,_K),e(_K,Cno),e(Au,Mno),e(Au,rS),e(rS,Eno),e(Au,yno),e(F,wno),e(F,Lu),e(Lu,bK),e(bK,Ano),e(Lu,Lno),e(Lu,aS),e(aS,Bno),e(Lu,xno),e(F,kno),e(F,Bu),e(Bu,vK),e(vK,Rno),e(Bu,Sno),e(Bu,sS),e(sS,Pno),e(Bu,$no),e(F,Ino),e(F,xu),e(xu,TK),e(TK,jno),e(xu,Dno),e(xu,nS),e(nS,Nno),e(xu,qno),e(F,Ono),e(F,ku),e(ku,FK),e(FK,Gno),e(ku,Xno),e(ku,lS),e(lS,Vno),e(ku,zno),e(F,Wno),e(F,Ru),e(Ru,CK),e(CK,Qno),e(Ru,Hno),e(Ru,iS),e(iS,Uno),e(Ru,Jno),e(F,Yno),e(F,Su),e(Su,MK),e(MK,Kno),e(Su,Zno),e(Su,dS),e(dS,elo),e(Su,olo),e(F,tlo),e(F,Pu),e(Pu,EK),e(EK,rlo),e(Pu,alo),e(Pu,cS),e(cS,slo),e(Pu,nlo),e(F,llo),e(F,$u),e($u,yK),e(yK,ilo),e($u,dlo),e($u,mS),e(mS,clo),e($u,mlo),e(F,flo),e(F,Iu),e(Iu,wK),e(wK,glo),e(Iu,hlo),e(Iu,fS),e(fS,ulo),e(Iu,plo),e(F,_lo),e(F,ju),e(ju,AK),e(AK,blo),e(ju,vlo),e(ju,gS),e(gS,Tlo),e(ju,Flo),e(F,Clo),e(F,Du),e(Du,LK),e(LK,Mlo),e(Du,Elo),e(Du,hS),e(hS,ylo),e(Du,wlo),e(F,Alo),e(F,Nu),e(Nu,BK),e(BK,Llo),e(Nu,Blo),e(Nu,uS),e(uS,xlo),e(Nu,klo),e(F,Rlo),e(F,qu),e(qu,xK),e(xK,Slo),e(qu,Plo),e(qu,pS),e(pS,$lo),e(qu,Ilo),e(F,jlo),e(F,Ou),e(Ou,kK),e(kK,Dlo),e(Ou,Nlo),e(Ou,_S),e(_S,qlo),e(Ou,Olo),e(F,Glo),e(F,Gu),e(Gu,RK),e(RK,Xlo),e(Gu,Vlo),e(Gu,bS),e(bS,zlo),e(Gu,Wlo),e(F,Qlo),e(F,Xu),e(Xu,SK),e(SK,Hlo),e(Xu,Ulo),e(Xu,vS),e(vS,Jlo),e(Xu,Ylo),e(F,Klo),e(F,Vu),e(Vu,PK),e(PK,Zlo),e(Vu,eio),e(Vu,TS),e(TS,oio),e(Vu,tio),e(F,rio),e(F,zu),e(zu,$K),e($K,aio),e(zu,sio),e(zu,FS),e(FS,nio),e(zu,lio),e(F,iio),e(F,Wu),e(Wu,IK),e(IK,dio),e(Wu,cio),e(Wu,CS),e(CS,mio),e(Wu,fio),e(F,gio),e(F,Qu),e(Qu,jK),e(jK,hio),e(Qu,uio),e(Qu,MS),e(MS,pio),e(Qu,_io),e(F,bio),e(F,Hu),e(Hu,DK),e(DK,vio),e(Hu,Tio),e(Hu,ES),e(ES,Fio),e(Hu,Cio),e(F,Mio),e(F,Uu),e(Uu,NK),e(NK,Eio),e(Uu,yio),e(Uu,yS),e(yS,wio),e(Uu,Aio),e(F,Lio),e(F,Ju),e(Ju,qK),e(qK,Bio),e(Ju,xio),e(Ju,wS),e(wS,kio),e(Ju,Rio),e(F,Sio),e(F,Yu),e(Yu,OK),e(OK,Pio),e(Yu,$io),e(Yu,AS),e(AS,Iio),e(Yu,jio),e(F,Dio),e(F,Ku),e(Ku,GK),e(GK,Nio),e(Ku,qio),e(Ku,LS),e(LS,Oio),e(Ku,Gio),e(F,Xio),e(F,Zu),e(Zu,XK),e(XK,Vio),e(Zu,zio),e(Zu,BS),e(BS,Wio),e(Zu,Qio),e(F,Hio),e(F,ep),e(ep,VK),e(VK,Uio),e(ep,Jio),e(ep,xS),e(xS,Yio),e(ep,Kio),e(F,Zio),e(F,op),e(op,zK),e(zK,edo),e(op,odo),e(op,kS),e(kS,tdo),e(op,rdo),e(F,ado),e(F,tp),e(tp,WK),e(WK,sdo),e(tp,ndo),e(tp,RS),e(RS,ldo),e(tp,ido),e(F,ddo),e(F,rp),e(rp,QK),e(QK,cdo),e(rp,mdo),e(rp,SS),e(SS,fdo),e(rp,gdo),e(F,hdo),e(F,ap),e(ap,HK),e(HK,udo),e(ap,pdo),e(ap,PS),e(PS,_do),e(ap,bdo),e(F,vdo),e(F,sp),e(sp,UK),e(UK,Tdo),e(sp,Fdo),e(sp,$S),e($S,Cdo),e(sp,Mdo),e(F,Edo),e(F,np),e(np,JK),e(JK,ydo),e(np,wdo),e(np,IS),e(IS,Ado),e(np,Ldo),e(F,Bdo),e(F,lp),e(lp,YK),e(YK,xdo),e(lp,kdo),e(lp,jS),e(jS,Rdo),e(lp,Sdo),e(F,Pdo),e(F,ip),e(ip,KK),e(KK,$do),e(ip,Ido),e(ip,DS),e(DS,jdo),e(ip,Ddo),e(F,Ndo),e(F,dp),e(dp,ZK),e(ZK,qdo),e(dp,Odo),e(dp,NS),e(NS,Gdo),e(dp,Xdo),e(F,Vdo),e(F,cp),e(cp,eZ),e(eZ,zdo),e(cp,Wdo),e(cp,qS),e(qS,Qdo),e(cp,Hdo),e(F,Udo),e(F,mp),e(mp,oZ),e(oZ,Jdo),e(mp,Ydo),e(mp,OS),e(OS,Kdo),e(mp,Zdo),e(F,eco),e(F,fp),e(fp,tZ),e(tZ,oco),e(fp,tco),e(fp,GS),e(GS,rco),e(fp,aco),e(F,sco),e(F,gp),e(gp,rZ),e(rZ,nco),e(gp,lco),e(gp,XS),e(XS,ico),e(gp,dco),e(F,cco),e(F,hp),e(hp,aZ),e(aZ,mco),e(hp,fco),e(hp,VS),e(VS,gco),e(hp,hco),e(Re,uco),e(Re,up),e(up,pco),e(up,sZ),e(sZ,_co),e(up,bco),e(up,nZ),e(nZ,vco),e(Re,Tco),e(Re,lZ),e(lZ,Fco),e(Re,Cco),g(Ey,Re,null),b(d,NBe,_),b(d,Zi,_),e(Zi,pp),e(pp,iZ),g(yy,iZ,null),e(Zi,Mco),e(Zi,dZ),e(dZ,Eco),b(d,qBe,_),b(d,Uo,_),g(wy,Uo,null),e(Uo,yco),e(Uo,ed),e(ed,wco),e(ed,cZ),e(cZ,Aco),e(ed,Lco),e(ed,mZ),e(mZ,Bco),e(ed,xco),e(Uo,kco),e(Uo,Ay),e(Ay,Rco),e(Ay,fZ),e(fZ,Sco),e(Ay,Pco),e(Uo,$co),e(Uo,Xt),g(Ly,Xt,null),e(Xt,Ico),e(Xt,gZ),e(gZ,jco),e(Xt,Dco),e(Xt,od),e(od,Nco),e(od,hZ),e(hZ,qco),e(od,Oco),e(od,uZ),e(uZ,Gco),e(od,Xco),e(Xt,Vco),e(Xt,pZ),e(pZ,zco),e(Xt,Wco),g(By,Xt,null),e(Uo,Qco),e(Uo,Se),g(xy,Se,null),e(Se,Hco),e(Se,_Z),e(_Z,Uco),e(Se,Jco),e(Se,Va),e(Va,Yco),e(Va,bZ),e(bZ,Kco),e(Va,Zco),e(Va,vZ),e(vZ,emo),e(Va,omo),e(Va,TZ),e(TZ,tmo),e(Va,rmo),e(Se,amo),e(Se,k),e(k,_p),e(_p,FZ),e(FZ,smo),e(_p,nmo),e(_p,zS),e(zS,lmo),e(_p,imo),e(k,dmo),e(k,bp),e(bp,CZ),e(CZ,cmo),e(bp,mmo),e(bp,WS),e(WS,fmo),e(bp,gmo),e(k,hmo),e(k,vp),e(vp,MZ),e(MZ,umo),e(vp,pmo),e(vp,QS),e(QS,_mo),e(vp,bmo),e(k,vmo),e(k,Tp),e(Tp,EZ),e(EZ,Tmo),e(Tp,Fmo),e(Tp,HS),e(HS,Cmo),e(Tp,Mmo),e(k,Emo),e(k,Fp),e(Fp,yZ),e(yZ,ymo),e(Fp,wmo),e(Fp,US),e(US,Amo),e(Fp,Lmo),e(k,Bmo),e(k,Cp),e(Cp,wZ),e(wZ,xmo),e(Cp,kmo),e(Cp,JS),e(JS,Rmo),e(Cp,Smo),e(k,Pmo),e(k,Mp),e(Mp,AZ),e(AZ,$mo),e(Mp,Imo),e(Mp,YS),e(YS,jmo),e(Mp,Dmo),e(k,Nmo),e(k,Ep),e(Ep,LZ),e(LZ,qmo),e(Ep,Omo),e(Ep,KS),e(KS,Gmo),e(Ep,Xmo),e(k,Vmo),e(k,yp),e(yp,BZ),e(BZ,zmo),e(yp,Wmo),e(yp,ZS),e(ZS,Qmo),e(yp,Hmo),e(k,Umo),e(k,wp),e(wp,xZ),e(xZ,Jmo),e(wp,Ymo),e(wp,eP),e(eP,Kmo),e(wp,Zmo),e(k,efo),e(k,Ap),e(Ap,kZ),e(kZ,ofo),e(Ap,tfo),e(Ap,oP),e(oP,rfo),e(Ap,afo),e(k,sfo),e(k,Lp),e(Lp,RZ),e(RZ,nfo),e(Lp,lfo),e(Lp,tP),e(tP,ifo),e(Lp,dfo),e(k,cfo),e(k,Bp),e(Bp,SZ),e(SZ,mfo),e(Bp,ffo),e(Bp,rP),e(rP,gfo),e(Bp,hfo),e(k,ufo),e(k,xp),e(xp,PZ),e(PZ,pfo),e(xp,_fo),e(xp,aP),e(aP,bfo),e(xp,vfo),e(k,Tfo),e(k,kp),e(kp,$Z),e($Z,Ffo),e(kp,Cfo),e(kp,sP),e(sP,Mfo),e(kp,Efo),e(k,yfo),e(k,Rp),e(Rp,IZ),e(IZ,wfo),e(Rp,Afo),e(Rp,nP),e(nP,Lfo),e(Rp,Bfo),e(k,xfo),e(k,Sp),e(Sp,jZ),e(jZ,kfo),e(Sp,Rfo),e(Sp,lP),e(lP,Sfo),e(Sp,Pfo),e(k,$fo),e(k,Pp),e(Pp,DZ),e(DZ,Ifo),e(Pp,jfo),e(Pp,iP),e(iP,Dfo),e(Pp,Nfo),e(k,qfo),e(k,$p),e($p,NZ),e(NZ,Ofo),e($p,Gfo),e($p,dP),e(dP,Xfo),e($p,Vfo),e(k,zfo),e(k,Ip),e(Ip,qZ),e(qZ,Wfo),e(Ip,Qfo),e(Ip,cP),e(cP,Hfo),e(Ip,Ufo),e(k,Jfo),e(k,jp),e(jp,OZ),e(OZ,Yfo),e(jp,Kfo),e(jp,mP),e(mP,Zfo),e(jp,ego),e(k,ogo),e(k,Dp),e(Dp,GZ),e(GZ,tgo),e(Dp,rgo),e(Dp,fP),e(fP,ago),e(Dp,sgo),e(k,ngo),e(k,Np),e(Np,XZ),e(XZ,lgo),e(Np,igo),e(Np,gP),e(gP,dgo),e(Np,cgo),e(k,mgo),e(k,qp),e(qp,VZ),e(VZ,fgo),e(qp,ggo),e(qp,hP),e(hP,hgo),e(qp,ugo),e(k,pgo),e(k,Op),e(Op,zZ),e(zZ,_go),e(Op,bgo),e(Op,uP),e(uP,vgo),e(Op,Tgo),e(k,Fgo),e(k,Gp),e(Gp,WZ),e(WZ,Cgo),e(Gp,Mgo),e(Gp,pP),e(pP,Ego),e(Gp,ygo),e(k,wgo),e(k,Xp),e(Xp,QZ),e(QZ,Ago),e(Xp,Lgo),e(Xp,_P),e(_P,Bgo),e(Xp,xgo),e(k,kgo),e(k,Vp),e(Vp,HZ),e(HZ,Rgo),e(Vp,Sgo),e(Vp,bP),e(bP,Pgo),e(Vp,$go),e(k,Igo),e(k,zp),e(zp,UZ),e(UZ,jgo),e(zp,Dgo),e(zp,vP),e(vP,Ngo),e(zp,qgo),e(k,Ogo),e(k,Wp),e(Wp,JZ),e(JZ,Ggo),e(Wp,Xgo),e(Wp,TP),e(TP,Vgo),e(Wp,zgo),e(k,Wgo),e(k,Qp),e(Qp,YZ),e(YZ,Qgo),e(Qp,Hgo),e(Qp,FP),e(FP,Ugo),e(Qp,Jgo),e(k,Ygo),e(k,Hp),e(Hp,KZ),e(KZ,Kgo),e(Hp,Zgo),e(Hp,CP),e(CP,eho),e(Hp,oho),e(k,tho),e(k,Up),e(Up,ZZ),e(ZZ,rho),e(Up,aho),e(Up,MP),e(MP,sho),e(Up,nho),e(k,lho),e(k,Jp),e(Jp,eee),e(eee,iho),e(Jp,dho),e(Jp,EP),e(EP,cho),e(Jp,mho),e(k,fho),e(k,Yp),e(Yp,oee),e(oee,gho),e(Yp,hho),e(Yp,yP),e(yP,uho),e(Yp,pho),e(k,_ho),e(k,Kp),e(Kp,tee),e(tee,bho),e(Kp,vho),e(Kp,wP),e(wP,Tho),e(Kp,Fho),e(k,Cho),e(k,Zp),e(Zp,ree),e(ree,Mho),e(Zp,Eho),e(Zp,AP),e(AP,yho),e(Zp,who),e(k,Aho),e(k,e_),e(e_,aee),e(aee,Lho),e(e_,Bho),e(e_,LP),e(LP,xho),e(e_,kho),e(k,Rho),e(k,o_),e(o_,see),e(see,Sho),e(o_,Pho),e(o_,BP),e(BP,$ho),e(o_,Iho),e(Se,jho),e(Se,t_),e(t_,Dho),e(t_,nee),e(nee,Nho),e(t_,qho),e(t_,lee),e(lee,Oho),e(Se,Gho),e(Se,iee),e(iee,Xho),e(Se,Vho),g(ky,Se,null),b(d,OBe,_),b(d,td,_),e(td,r_),e(r_,dee),g(Ry,dee,null),e(td,zho),e(td,cee),e(cee,Who),b(d,GBe,_),b(d,Jo,_),g(Sy,Jo,null),e(Jo,Qho),e(Jo,rd),e(rd,Hho),e(rd,mee),e(mee,Uho),e(rd,Jho),e(rd,fee),e(fee,Yho),e(rd,Kho),e(Jo,Zho),e(Jo,Py),e(Py,euo),e(Py,gee),e(gee,ouo),e(Py,tuo),e(Jo,ruo),e(Jo,Vt),g($y,Vt,null),e(Vt,auo),e(Vt,hee),e(hee,suo),e(Vt,nuo),e(Vt,ad),e(ad,luo),e(ad,uee),e(uee,iuo),e(ad,duo),e(ad,pee),e(pee,cuo),e(ad,muo),e(Vt,fuo),e(Vt,_ee),e(_ee,guo),e(Vt,huo),g(Iy,Vt,null),e(Jo,uuo),e(Jo,Pe),g(jy,Pe,null),e(Pe,puo),e(Pe,bee),e(bee,_uo),e(Pe,buo),e(Pe,za),e(za,vuo),e(za,vee),e(vee,Tuo),e(za,Fuo),e(za,Tee),e(Tee,Cuo),e(za,Muo),e(za,Fee),e(Fee,Euo),e(za,yuo),e(Pe,wuo),e(Pe,$),e($,a_),e(a_,Cee),e(Cee,Auo),e(a_,Luo),e(a_,xP),e(xP,Buo),e(a_,xuo),e($,kuo),e($,s_),e(s_,Mee),e(Mee,Ruo),e(s_,Suo),e(s_,kP),e(kP,Puo),e(s_,$uo),e($,Iuo),e($,n_),e(n_,Eee),e(Eee,juo),e(n_,Duo),e(n_,RP),e(RP,Nuo),e(n_,quo),e($,Ouo),e($,l_),e(l_,yee),e(yee,Guo),e(l_,Xuo),e(l_,SP),e(SP,Vuo),e(l_,zuo),e($,Wuo),e($,i_),e(i_,wee),e(wee,Quo),e(i_,Huo),e(i_,PP),e(PP,Uuo),e(i_,Juo),e($,Yuo),e($,d_),e(d_,Aee),e(Aee,Kuo),e(d_,Zuo),e(d_,$P),e($P,epo),e(d_,opo),e($,tpo),e($,c_),e(c_,Lee),e(Lee,rpo),e(c_,apo),e(c_,IP),e(IP,spo),e(c_,npo),e($,lpo),e($,m_),e(m_,Bee),e(Bee,ipo),e(m_,dpo),e(m_,jP),e(jP,cpo),e(m_,mpo),e($,fpo),e($,f_),e(f_,xee),e(xee,gpo),e(f_,hpo),e(f_,DP),e(DP,upo),e(f_,ppo),e($,_po),e($,g_),e(g_,kee),e(kee,bpo),e(g_,vpo),e(g_,NP),e(NP,Tpo),e(g_,Fpo),e($,Cpo),e($,h_),e(h_,Ree),e(Ree,Mpo),e(h_,Epo),e(h_,qP),e(qP,ypo),e(h_,wpo),e($,Apo),e($,u_),e(u_,See),e(See,Lpo),e(u_,Bpo),e(u_,OP),e(OP,xpo),e(u_,kpo),e($,Rpo),e($,p_),e(p_,Pee),e(Pee,Spo),e(p_,Ppo),e(p_,GP),e(GP,$po),e(p_,Ipo),e($,jpo),e($,__),e(__,$ee),e($ee,Dpo),e(__,Npo),e(__,XP),e(XP,qpo),e(__,Opo),e($,Gpo),e($,b_),e(b_,Iee),e(Iee,Xpo),e(b_,Vpo),e(b_,VP),e(VP,zpo),e(b_,Wpo),e($,Qpo),e($,v_),e(v_,jee),e(jee,Hpo),e(v_,Upo),e(v_,zP),e(zP,Jpo),e(v_,Ypo),e($,Kpo),e($,T_),e(T_,Dee),e(Dee,Zpo),e(T_,e_o),e(T_,WP),e(WP,o_o),e(T_,t_o),e($,r_o),e($,F_),e(F_,Nee),e(Nee,a_o),e(F_,s_o),e(F_,QP),e(QP,n_o),e(F_,l_o),e($,i_o),e($,C_),e(C_,qee),e(qee,d_o),e(C_,c_o),e(C_,HP),e(HP,m_o),e(C_,f_o),e($,g_o),e($,M_),e(M_,Oee),e(Oee,h_o),e(M_,u_o),e(M_,UP),e(UP,p_o),e(M_,__o),e($,b_o),e($,E_),e(E_,Gee),e(Gee,v_o),e(E_,T_o),e(E_,JP),e(JP,F_o),e(E_,C_o),e($,M_o),e($,y_),e(y_,Xee),e(Xee,E_o),e(y_,y_o),e(y_,YP),e(YP,w_o),e(y_,A_o),e($,L_o),e($,w_),e(w_,Vee),e(Vee,B_o),e(w_,x_o),e(w_,KP),e(KP,k_o),e(w_,R_o),e($,S_o),e($,A_),e(A_,zee),e(zee,P_o),e(A_,$_o),e(A_,ZP),e(ZP,I_o),e(A_,j_o),e($,D_o),e($,L_),e(L_,Wee),e(Wee,N_o),e(L_,q_o),e(L_,e$),e(e$,O_o),e(L_,G_o),e($,X_o),e($,B_),e(B_,Qee),e(Qee,V_o),e(B_,z_o),e(B_,o$),e(o$,W_o),e(B_,Q_o),e($,H_o),e($,x_),e(x_,Hee),e(Hee,U_o),e(x_,J_o),e(x_,t$),e(t$,Y_o),e(x_,K_o),e($,Z_o),e($,k_),e(k_,Uee),e(Uee,ebo),e(k_,obo),e(k_,r$),e(r$,tbo),e(k_,rbo),e($,abo),e($,R_),e(R_,Jee),e(Jee,sbo),e(R_,nbo),e(R_,a$),e(a$,lbo),e(R_,ibo),e($,dbo),e($,S_),e(S_,Yee),e(Yee,cbo),e(S_,mbo),e(S_,s$),e(s$,fbo),e(S_,gbo),e($,hbo),e($,P_),e(P_,Kee),e(Kee,ubo),e(P_,pbo),e(P_,n$),e(n$,_bo),e(P_,bbo),e($,vbo),e($,$_),e($_,Zee),e(Zee,Tbo),e($_,Fbo),e($_,l$),e(l$,Cbo),e($_,Mbo),e($,Ebo),e($,I_),e(I_,eoe),e(eoe,ybo),e(I_,wbo),e(I_,i$),e(i$,Abo),e(I_,Lbo),e($,Bbo),e($,j_),e(j_,ooe),e(ooe,xbo),e(j_,kbo),e(j_,d$),e(d$,Rbo),e(j_,Sbo),e($,Pbo),e($,D_),e(D_,toe),e(toe,$bo),e(D_,Ibo),e(D_,c$),e(c$,jbo),e(D_,Dbo),e(Pe,Nbo),e(Pe,N_),e(N_,qbo),e(N_,roe),e(roe,Obo),e(N_,Gbo),e(N_,aoe),e(aoe,Xbo),e(Pe,Vbo),e(Pe,soe),e(soe,zbo),e(Pe,Wbo),g(Dy,Pe,null),b(d,XBe,_),b(d,sd,_),e(sd,q_),e(q_,noe),g(Ny,noe,null),e(sd,Qbo),e(sd,loe),e(loe,Hbo),b(d,VBe,_),b(d,Yo,_),g(qy,Yo,null),e(Yo,Ubo),e(Yo,nd),e(nd,Jbo),e(nd,ioe),e(ioe,Ybo),e(nd,Kbo),e(nd,doe),e(doe,Zbo),e(nd,e2o),e(Yo,o2o),e(Yo,Oy),e(Oy,t2o),e(Oy,coe),e(coe,r2o),e(Oy,a2o),e(Yo,s2o),e(Yo,zt),g(Gy,zt,null),e(zt,n2o),e(zt,moe),e(moe,l2o),e(zt,i2o),e(zt,ld),e(ld,d2o),e(ld,foe),e(foe,c2o),e(ld,m2o),e(ld,goe),e(goe,f2o),e(ld,g2o),e(zt,h2o),e(zt,hoe),e(hoe,u2o),e(zt,p2o),g(Xy,zt,null),e(Yo,_2o),e(Yo,$e),g(Vy,$e,null),e($e,b2o),e($e,uoe),e(uoe,v2o),e($e,T2o),e($e,Wa),e(Wa,F2o),e(Wa,poe),e(poe,C2o),e(Wa,M2o),e(Wa,_oe),e(_oe,E2o),e(Wa,y2o),e(Wa,boe),e(boe,w2o),e(Wa,A2o),e($e,L2o),e($e,I),e(I,O_),e(O_,voe),e(voe,B2o),e(O_,x2o),e(O_,m$),e(m$,k2o),e(O_,R2o),e(I,S2o),e(I,G_),e(G_,Toe),e(Toe,P2o),e(G_,$2o),e(G_,f$),e(f$,I2o),e(G_,j2o),e(I,D2o),e(I,X_),e(X_,Foe),e(Foe,N2o),e(X_,q2o),e(X_,g$),e(g$,O2o),e(X_,G2o),e(I,X2o),e(I,V_),e(V_,Coe),e(Coe,V2o),e(V_,z2o),e(V_,h$),e(h$,W2o),e(V_,Q2o),e(I,H2o),e(I,z_),e(z_,Moe),e(Moe,U2o),e(z_,J2o),e(z_,u$),e(u$,Y2o),e(z_,K2o),e(I,Z2o),e(I,W_),e(W_,Eoe),e(Eoe,evo),e(W_,ovo),e(W_,p$),e(p$,tvo),e(W_,rvo),e(I,avo),e(I,Q_),e(Q_,yoe),e(yoe,svo),e(Q_,nvo),e(Q_,_$),e(_$,lvo),e(Q_,ivo),e(I,dvo),e(I,H_),e(H_,woe),e(woe,cvo),e(H_,mvo),e(H_,b$),e(b$,fvo),e(H_,gvo),e(I,hvo),e(I,U_),e(U_,Aoe),e(Aoe,uvo),e(U_,pvo),e(U_,v$),e(v$,_vo),e(U_,bvo),e(I,vvo),e(I,J_),e(J_,Loe),e(Loe,Tvo),e(J_,Fvo),e(J_,T$),e(T$,Cvo),e(J_,Mvo),e(I,Evo),e(I,Y_),e(Y_,Boe),e(Boe,yvo),e(Y_,wvo),e(Y_,F$),e(F$,Avo),e(Y_,Lvo),e(I,Bvo),e(I,K_),e(K_,xoe),e(xoe,xvo),e(K_,kvo),e(K_,C$),e(C$,Rvo),e(K_,Svo),e(I,Pvo),e(I,Z_),e(Z_,koe),e(koe,$vo),e(Z_,Ivo),e(Z_,M$),e(M$,jvo),e(Z_,Dvo),e(I,Nvo),e(I,eb),e(eb,Roe),e(Roe,qvo),e(eb,Ovo),e(eb,E$),e(E$,Gvo),e(eb,Xvo),e(I,Vvo),e(I,ob),e(ob,Soe),e(Soe,zvo),e(ob,Wvo),e(ob,y$),e(y$,Qvo),e(ob,Hvo),e(I,Uvo),e(I,tb),e(tb,Poe),e(Poe,Jvo),e(tb,Yvo),e(tb,w$),e(w$,Kvo),e(tb,Zvo),e(I,eTo),e(I,rb),e(rb,$oe),e($oe,oTo),e(rb,tTo),e(rb,A$),e(A$,rTo),e(rb,aTo),e(I,sTo),e(I,ab),e(ab,Ioe),e(Ioe,nTo),e(ab,lTo),e(ab,L$),e(L$,iTo),e(ab,dTo),e(I,cTo),e(I,sb),e(sb,joe),e(joe,mTo),e(sb,fTo),e(sb,B$),e(B$,gTo),e(sb,hTo),e(I,uTo),e(I,nb),e(nb,Doe),e(Doe,pTo),e(nb,_To),e(nb,x$),e(x$,bTo),e(nb,vTo),e(I,TTo),e(I,lb),e(lb,Noe),e(Noe,FTo),e(lb,CTo),e(lb,k$),e(k$,MTo),e(lb,ETo),e(I,yTo),e(I,ib),e(ib,qoe),e(qoe,wTo),e(ib,ATo),e(ib,R$),e(R$,LTo),e(ib,BTo),e(I,xTo),e(I,db),e(db,Ooe),e(Ooe,kTo),e(db,RTo),e(db,S$),e(S$,STo),e(db,PTo),e(I,$To),e(I,cb),e(cb,Goe),e(Goe,ITo),e(cb,jTo),e(cb,P$),e(P$,DTo),e(cb,NTo),e(I,qTo),e(I,mb),e(mb,Xoe),e(Xoe,OTo),e(mb,GTo),e(mb,$$),e($$,XTo),e(mb,VTo),e(I,zTo),e(I,fb),e(fb,Voe),e(Voe,WTo),e(fb,QTo),e(fb,I$),e(I$,HTo),e(fb,UTo),e(I,JTo),e(I,gb),e(gb,zoe),e(zoe,YTo),e(gb,KTo),e(gb,j$),e(j$,ZTo),e(gb,e1o),e(I,o1o),e(I,hb),e(hb,Woe),e(Woe,t1o),e(hb,r1o),e(hb,D$),e(D$,a1o),e(hb,s1o),e(I,n1o),e(I,ub),e(ub,Qoe),e(Qoe,l1o),e(ub,i1o),e(ub,N$),e(N$,d1o),e(ub,c1o),e(I,m1o),e(I,pb),e(pb,Hoe),e(Hoe,f1o),e(pb,g1o),e(pb,q$),e(q$,h1o),e(pb,u1o),e(I,p1o),e(I,_b),e(_b,Uoe),e(Uoe,_1o),e(_b,b1o),e(_b,Joe),e(Joe,v1o),e(_b,T1o),e(I,F1o),e(I,bb),e(bb,Yoe),e(Yoe,C1o),e(bb,M1o),e(bb,O$),e(O$,E1o),e(bb,y1o),e(I,w1o),e(I,vb),e(vb,Koe),e(Koe,A1o),e(vb,L1o),e(vb,G$),e(G$,B1o),e(vb,x1o),e(I,k1o),e(I,Tb),e(Tb,Zoe),e(Zoe,R1o),e(Tb,S1o),e(Tb,X$),e(X$,P1o),e(Tb,$1o),e(I,I1o),e(I,Fb),e(Fb,ete),e(ete,j1o),e(Fb,D1o),e(Fb,V$),e(V$,N1o),e(Fb,q1o),e($e,O1o),e($e,Cb),e(Cb,G1o),e(Cb,ote),e(ote,X1o),e(Cb,V1o),e(Cb,tte),e(tte,z1o),e($e,W1o),e($e,rte),e(rte,Q1o),e($e,H1o),g(zy,$e,null),b(d,zBe,_),b(d,id,_),e(id,Mb),e(Mb,ate),g(Wy,ate,null),e(id,U1o),e(id,ste),e(ste,J1o),b(d,WBe,_),b(d,Ko,_),g(Qy,Ko,null),e(Ko,Y1o),e(Ko,dd),e(dd,K1o),e(dd,nte),e(nte,Z1o),e(dd,eFo),e(dd,lte),e(lte,oFo),e(dd,tFo),e(Ko,rFo),e(Ko,Hy),e(Hy,aFo),e(Hy,ite),e(ite,sFo),e(Hy,nFo),e(Ko,lFo),e(Ko,Wt),g(Uy,Wt,null),e(Wt,iFo),e(Wt,dte),e(dte,dFo),e(Wt,cFo),e(Wt,cd),e(cd,mFo),e(cd,cte),e(cte,fFo),e(cd,gFo),e(cd,mte),e(mte,hFo),e(cd,uFo),e(Wt,pFo),e(Wt,fte),e(fte,_Fo),e(Wt,bFo),g(Jy,Wt,null),e(Ko,vFo),e(Ko,Ie),g(Yy,Ie,null),e(Ie,TFo),e(Ie,gte),e(gte,FFo),e(Ie,CFo),e(Ie,Qa),e(Qa,MFo),e(Qa,hte),e(hte,EFo),e(Qa,yFo),e(Qa,ute),e(ute,wFo),e(Qa,AFo),e(Qa,pte),e(pte,LFo),e(Qa,BFo),e(Ie,xFo),e(Ie,se),e(se,Eb),e(Eb,_te),e(_te,kFo),e(Eb,RFo),e(Eb,z$),e(z$,SFo),e(Eb,PFo),e(se,$Fo),e(se,yb),e(yb,bte),e(bte,IFo),e(yb,jFo),e(yb,W$),e(W$,DFo),e(yb,NFo),e(se,qFo),e(se,wb),e(wb,vte),e(vte,OFo),e(wb,GFo),e(wb,Q$),e(Q$,XFo),e(wb,VFo),e(se,zFo),e(se,Ab),e(Ab,Tte),e(Tte,WFo),e(Ab,QFo),e(Ab,H$),e(H$,HFo),e(Ab,UFo),e(se,JFo),e(se,Lb),e(Lb,Fte),e(Fte,YFo),e(Lb,KFo),e(Lb,U$),e(U$,ZFo),e(Lb,eCo),e(se,oCo),e(se,Bb),e(Bb,Cte),e(Cte,tCo),e(Bb,rCo),e(Bb,J$),e(J$,aCo),e(Bb,sCo),e(se,nCo),e(se,xb),e(xb,Mte),e(Mte,lCo),e(xb,iCo),e(xb,Y$),e(Y$,dCo),e(xb,cCo),e(se,mCo),e(se,kb),e(kb,Ete),e(Ete,fCo),e(kb,gCo),e(kb,K$),e(K$,hCo),e(kb,uCo),e(se,pCo),e(se,Rb),e(Rb,yte),e(yte,_Co),e(Rb,bCo),e(Rb,Z$),e(Z$,vCo),e(Rb,TCo),e(se,FCo),e(se,Sb),e(Sb,wte),e(wte,CCo),e(Sb,MCo),e(Sb,eI),e(eI,ECo),e(Sb,yCo),e(se,wCo),e(se,Pb),e(Pb,Ate),e(Ate,ACo),e(Pb,LCo),e(Pb,oI),e(oI,BCo),e(Pb,xCo),e(se,kCo),e(se,$b),e($b,Lte),e(Lte,RCo),e($b,SCo),e($b,tI),e(tI,PCo),e($b,$Co),e(se,ICo),e(se,Ib),e(Ib,Bte),e(Bte,jCo),e(Ib,DCo),e(Ib,rI),e(rI,NCo),e(Ib,qCo),e(se,OCo),e(se,jb),e(jb,xte),e(xte,GCo),e(jb,XCo),e(jb,aI),e(aI,VCo),e(jb,zCo),e(se,WCo),e(se,Db),e(Db,kte),e(kte,QCo),e(Db,HCo),e(Db,sI),e(sI,UCo),e(Db,JCo),e(se,YCo),e(se,Nb),e(Nb,Rte),e(Rte,KCo),e(Nb,ZCo),e(Nb,nI),e(nI,e4o),e(Nb,o4o),e(Ie,t4o),e(Ie,qb),e(qb,r4o),e(qb,Ste),e(Ste,a4o),e(qb,s4o),e(qb,Pte),e(Pte,n4o),e(Ie,l4o),e(Ie,$te),e($te,i4o),e(Ie,d4o),g(Ky,Ie,null),b(d,QBe,_),b(d,md,_),e(md,Ob),e(Ob,Ite),g(Zy,Ite,null),e(md,c4o),e(md,jte),e(jte,m4o),b(d,HBe,_),b(d,Zo,_),g(ew,Zo,null),e(Zo,f4o),e(Zo,fd),e(fd,g4o),e(fd,Dte),e(Dte,h4o),e(fd,u4o),e(fd,Nte),e(Nte,p4o),e(fd,_4o),e(Zo,b4o),e(Zo,ow),e(ow,v4o),e(ow,qte),e(qte,T4o),e(ow,F4o),e(Zo,C4o),e(Zo,Qt),g(tw,Qt,null),e(Qt,M4o),e(Qt,Ote),e(Ote,E4o),e(Qt,y4o),e(Qt,gd),e(gd,w4o),e(gd,Gte),e(Gte,A4o),e(gd,L4o),e(gd,Xte),e(Xte,B4o),e(gd,x4o),e(Qt,k4o),e(Qt,Vte),e(Vte,R4o),e(Qt,S4o),g(rw,Qt,null),e(Zo,P4o),e(Zo,je),g(aw,je,null),e(je,$4o),e(je,zte),e(zte,I4o),e(je,j4o),e(je,Ha),e(Ha,D4o),e(Ha,Wte),e(Wte,N4o),e(Ha,q4o),e(Ha,Qte),e(Qte,O4o),e(Ha,G4o),e(Ha,Hte),e(Hte,X4o),e(Ha,V4o),e(je,z4o),e(je,A),e(A,Gb),e(Gb,Ute),e(Ute,W4o),e(Gb,Q4o),e(Gb,lI),e(lI,H4o),e(Gb,U4o),e(A,J4o),e(A,Xb),e(Xb,Jte),e(Jte,Y4o),e(Xb,K4o),e(Xb,iI),e(iI,Z4o),e(Xb,eMo),e(A,oMo),e(A,Vb),e(Vb,Yte),e(Yte,tMo),e(Vb,rMo),e(Vb,dI),e(dI,aMo),e(Vb,sMo),e(A,nMo),e(A,zb),e(zb,Kte),e(Kte,lMo),e(zb,iMo),e(zb,cI),e(cI,dMo),e(zb,cMo),e(A,mMo),e(A,Wb),e(Wb,Zte),e(Zte,fMo),e(Wb,gMo),e(Wb,mI),e(mI,hMo),e(Wb,uMo),e(A,pMo),e(A,Qb),e(Qb,ere),e(ere,_Mo),e(Qb,bMo),e(Qb,fI),e(fI,vMo),e(Qb,TMo),e(A,FMo),e(A,Hb),e(Hb,ore),e(ore,CMo),e(Hb,MMo),e(Hb,gI),e(gI,EMo),e(Hb,yMo),e(A,wMo),e(A,Ub),e(Ub,tre),e(tre,AMo),e(Ub,LMo),e(Ub,hI),e(hI,BMo),e(Ub,xMo),e(A,kMo),e(A,Jb),e(Jb,rre),e(rre,RMo),e(Jb,SMo),e(Jb,uI),e(uI,PMo),e(Jb,$Mo),e(A,IMo),e(A,Yb),e(Yb,are),e(are,jMo),e(Yb,DMo),e(Yb,pI),e(pI,NMo),e(Yb,qMo),e(A,OMo),e(A,Kb),e(Kb,sre),e(sre,GMo),e(Kb,XMo),e(Kb,_I),e(_I,VMo),e(Kb,zMo),e(A,WMo),e(A,Zb),e(Zb,nre),e(nre,QMo),e(Zb,HMo),e(Zb,bI),e(bI,UMo),e(Zb,JMo),e(A,YMo),e(A,e2),e(e2,lre),e(lre,KMo),e(e2,ZMo),e(e2,vI),e(vI,eEo),e(e2,oEo),e(A,tEo),e(A,o2),e(o2,ire),e(ire,rEo),e(o2,aEo),e(o2,TI),e(TI,sEo),e(o2,nEo),e(A,lEo),e(A,t2),e(t2,dre),e(dre,iEo),e(t2,dEo),e(t2,FI),e(FI,cEo),e(t2,mEo),e(A,fEo),e(A,r2),e(r2,cre),e(cre,gEo),e(r2,hEo),e(r2,CI),e(CI,uEo),e(r2,pEo),e(A,_Eo),e(A,a2),e(a2,mre),e(mre,bEo),e(a2,vEo),e(a2,MI),e(MI,TEo),e(a2,FEo),e(A,CEo),e(A,s2),e(s2,fre),e(fre,MEo),e(s2,EEo),e(s2,EI),e(EI,yEo),e(s2,wEo),e(A,AEo),e(A,n2),e(n2,gre),e(gre,LEo),e(n2,BEo),e(n2,yI),e(yI,xEo),e(n2,kEo),e(A,REo),e(A,l2),e(l2,hre),e(hre,SEo),e(l2,PEo),e(l2,wI),e(wI,$Eo),e(l2,IEo),e(A,jEo),e(A,i2),e(i2,ure),e(ure,DEo),e(i2,NEo),e(i2,AI),e(AI,qEo),e(i2,OEo),e(A,GEo),e(A,d2),e(d2,pre),e(pre,XEo),e(d2,VEo),e(d2,LI),e(LI,zEo),e(d2,WEo),e(A,QEo),e(A,c2),e(c2,_re),e(_re,HEo),e(c2,UEo),e(c2,BI),e(BI,JEo),e(c2,YEo),e(A,KEo),e(A,m2),e(m2,bre),e(bre,ZEo),e(m2,e3o),e(m2,xI),e(xI,o3o),e(m2,t3o),e(A,r3o),e(A,f2),e(f2,vre),e(vre,a3o),e(f2,s3o),e(f2,kI),e(kI,n3o),e(f2,l3o),e(A,i3o),e(A,g2),e(g2,Tre),e(Tre,d3o),e(g2,c3o),e(g2,RI),e(RI,m3o),e(g2,f3o),e(A,g3o),e(A,h2),e(h2,Fre),e(Fre,h3o),e(h2,u3o),e(h2,SI),e(SI,p3o),e(h2,_3o),e(A,b3o),e(A,u2),e(u2,Cre),e(Cre,v3o),e(u2,T3o),e(u2,PI),e(PI,F3o),e(u2,C3o),e(A,M3o),e(A,p2),e(p2,Mre),e(Mre,E3o),e(p2,y3o),e(p2,$I),e($I,w3o),e(p2,A3o),e(A,L3o),e(A,_2),e(_2,Ere),e(Ere,B3o),e(_2,x3o),e(_2,II),e(II,k3o),e(_2,R3o),e(A,S3o),e(A,b2),e(b2,yre),e(yre,P3o),e(b2,$3o),e(b2,jI),e(jI,I3o),e(b2,j3o),e(A,D3o),e(A,v2),e(v2,wre),e(wre,N3o),e(v2,q3o),e(v2,DI),e(DI,O3o),e(v2,G3o),e(A,X3o),e(A,T2),e(T2,Are),e(Are,V3o),e(T2,z3o),e(T2,NI),e(NI,W3o),e(T2,Q3o),e(A,H3o),e(A,F2),e(F2,Lre),e(Lre,U3o),e(F2,J3o),e(F2,qI),e(qI,Y3o),e(F2,K3o),e(A,Z3o),e(A,C2),e(C2,Bre),e(Bre,e5o),e(C2,o5o),e(C2,OI),e(OI,t5o),e(C2,r5o),e(A,a5o),e(A,M2),e(M2,xre),e(xre,s5o),e(M2,n5o),e(M2,GI),e(GI,l5o),e(M2,i5o),e(A,d5o),e(A,E2),e(E2,kre),e(kre,c5o),e(E2,m5o),e(E2,XI),e(XI,f5o),e(E2,g5o),e(A,h5o),e(A,y2),e(y2,Rre),e(Rre,u5o),e(y2,p5o),e(y2,VI),e(VI,_5o),e(y2,b5o),e(A,v5o),e(A,w2),e(w2,Sre),e(Sre,T5o),e(w2,F5o),e(w2,zI),e(zI,C5o),e(w2,M5o),e(A,E5o),e(A,A2),e(A2,Pre),e(Pre,y5o),e(A2,w5o),e(A2,WI),e(WI,A5o),e(A2,L5o),e(A,B5o),e(A,L2),e(L2,$re),e($re,x5o),e(L2,k5o),e(L2,QI),e(QI,R5o),e(L2,S5o),e(A,P5o),e(A,B2),e(B2,Ire),e(Ire,$5o),e(B2,I5o),e(B2,HI),e(HI,j5o),e(B2,D5o),e(A,N5o),e(A,x2),e(x2,jre),e(jre,q5o),e(x2,O5o),e(x2,UI),e(UI,G5o),e(x2,X5o),e(A,V5o),e(A,k2),e(k2,Dre),e(Dre,z5o),e(k2,W5o),e(k2,JI),e(JI,Q5o),e(k2,H5o),e(A,U5o),e(A,R2),e(R2,Nre),e(Nre,J5o),e(R2,Y5o),e(R2,YI),e(YI,K5o),e(R2,Z5o),e(A,eyo),e(A,S2),e(S2,qre),e(qre,oyo),e(S2,tyo),e(S2,KI),e(KI,ryo),e(S2,ayo),e(je,syo),e(je,P2),e(P2,nyo),e(P2,Ore),e(Ore,lyo),e(P2,iyo),e(P2,Gre),e(Gre,dyo),e(je,cyo),e(je,Xre),e(Xre,myo),e(je,fyo),g(sw,je,null),b(d,UBe,_),b(d,hd,_),e(hd,$2),e($2,Vre),g(nw,Vre,null),e(hd,gyo),e(hd,zre),e(zre,hyo),b(d,JBe,_),b(d,et,_),g(lw,et,null),e(et,uyo),e(et,ud),e(ud,pyo),e(ud,Wre),e(Wre,_yo),e(ud,byo),e(ud,Qre),e(Qre,vyo),e(ud,Tyo),e(et,Fyo),e(et,iw),e(iw,Cyo),e(iw,Hre),e(Hre,Myo),e(iw,Eyo),e(et,yyo),e(et,Ht),g(dw,Ht,null),e(Ht,wyo),e(Ht,Ure),e(Ure,Ayo),e(Ht,Lyo),e(Ht,pd),e(pd,Byo),e(pd,Jre),e(Jre,xyo),e(pd,kyo),e(pd,Yre),e(Yre,Ryo),e(pd,Syo),e(Ht,Pyo),e(Ht,Kre),e(Kre,$yo),e(Ht,Iyo),g(cw,Ht,null),e(et,jyo),e(et,De),g(mw,De,null),e(De,Dyo),e(De,Zre),e(Zre,Nyo),e(De,qyo),e(De,Ua),e(Ua,Oyo),e(Ua,eae),e(eae,Gyo),e(Ua,Xyo),e(Ua,oae),e(oae,Vyo),e(Ua,zyo),e(Ua,tae),e(tae,Wyo),e(Ua,Qyo),e(De,Hyo),e(De,O),e(O,I2),e(I2,rae),e(rae,Uyo),e(I2,Jyo),e(I2,ZI),e(ZI,Yyo),e(I2,Kyo),e(O,Zyo),e(O,j2),e(j2,aae),e(aae,ewo),e(j2,owo),e(j2,ej),e(ej,two),e(j2,rwo),e(O,awo),e(O,D2),e(D2,sae),e(sae,swo),e(D2,nwo),e(D2,oj),e(oj,lwo),e(D2,iwo),e(O,dwo),e(O,N2),e(N2,nae),e(nae,cwo),e(N2,mwo),e(N2,tj),e(tj,fwo),e(N2,gwo),e(O,hwo),e(O,q2),e(q2,lae),e(lae,uwo),e(q2,pwo),e(q2,rj),e(rj,_wo),e(q2,bwo),e(O,vwo),e(O,O2),e(O2,iae),e(iae,Two),e(O2,Fwo),e(O2,aj),e(aj,Cwo),e(O2,Mwo),e(O,Ewo),e(O,G2),e(G2,dae),e(dae,ywo),e(G2,wwo),e(G2,sj),e(sj,Awo),e(G2,Lwo),e(O,Bwo),e(O,X2),e(X2,cae),e(cae,xwo),e(X2,kwo),e(X2,nj),e(nj,Rwo),e(X2,Swo),e(O,Pwo),e(O,V2),e(V2,mae),e(mae,$wo),e(V2,Iwo),e(V2,lj),e(lj,jwo),e(V2,Dwo),e(O,Nwo),e(O,z2),e(z2,fae),e(fae,qwo),e(z2,Owo),e(z2,ij),e(ij,Gwo),e(z2,Xwo),e(O,Vwo),e(O,W2),e(W2,gae),e(gae,zwo),e(W2,Wwo),e(W2,dj),e(dj,Qwo),e(W2,Hwo),e(O,Uwo),e(O,Q2),e(Q2,hae),e(hae,Jwo),e(Q2,Ywo),e(Q2,cj),e(cj,Kwo),e(Q2,Zwo),e(O,e6o),e(O,H2),e(H2,uae),e(uae,o6o),e(H2,t6o),e(H2,mj),e(mj,r6o),e(H2,a6o),e(O,s6o),e(O,U2),e(U2,pae),e(pae,n6o),e(U2,l6o),e(U2,fj),e(fj,i6o),e(U2,d6o),e(O,c6o),e(O,J2),e(J2,_ae),e(_ae,m6o),e(J2,f6o),e(J2,gj),e(gj,g6o),e(J2,h6o),e(O,u6o),e(O,Y2),e(Y2,bae),e(bae,p6o),e(Y2,_6o),e(Y2,hj),e(hj,b6o),e(Y2,v6o),e(O,T6o),e(O,K2),e(K2,vae),e(vae,F6o),e(K2,C6o),e(K2,uj),e(uj,M6o),e(K2,E6o),e(O,y6o),e(O,Z2),e(Z2,Tae),e(Tae,w6o),e(Z2,A6o),e(Z2,pj),e(pj,L6o),e(Z2,B6o),e(O,x6o),e(O,ev),e(ev,Fae),e(Fae,k6o),e(ev,R6o),e(ev,_j),e(_j,S6o),e(ev,P6o),e(O,$6o),e(O,ov),e(ov,Cae),e(Cae,I6o),e(ov,j6o),e(ov,bj),e(bj,D6o),e(ov,N6o),e(O,q6o),e(O,tv),e(tv,Mae),e(Mae,O6o),e(tv,G6o),e(tv,vj),e(vj,X6o),e(tv,V6o),e(O,z6o),e(O,rv),e(rv,Eae),e(Eae,W6o),e(rv,Q6o),e(rv,Tj),e(Tj,H6o),e(rv,U6o),e(O,J6o),e(O,av),e(av,yae),e(yae,Y6o),e(av,K6o),e(av,Fj),e(Fj,Z6o),e(av,eAo),e(O,oAo),e(O,sv),e(sv,wae),e(wae,tAo),e(sv,rAo),e(sv,Cj),e(Cj,aAo),e(sv,sAo),e(O,nAo),e(O,nv),e(nv,Aae),e(Aae,lAo),e(nv,iAo),e(nv,Mj),e(Mj,dAo),e(nv,cAo),e(O,mAo),e(O,lv),e(lv,Lae),e(Lae,fAo),e(lv,gAo),e(lv,Ej),e(Ej,hAo),e(lv,uAo),e(O,pAo),e(O,iv),e(iv,Bae),e(Bae,_Ao),e(iv,bAo),e(iv,yj),e(yj,vAo),e(iv,TAo),e(O,FAo),e(O,dv),e(dv,xae),e(xae,CAo),e(dv,MAo),e(dv,wj),e(wj,EAo),e(dv,yAo),e(De,wAo),e(De,cv),e(cv,AAo),e(cv,kae),e(kae,LAo),e(cv,BAo),e(cv,Rae),e(Rae,xAo),e(De,kAo),e(De,Sae),e(Sae,RAo),e(De,SAo),g(fw,De,null),b(d,YBe,_),b(d,_d,_),e(_d,mv),e(mv,Pae),g(gw,Pae,null),e(_d,PAo),e(_d,$ae),e($ae,$Ao),b(d,KBe,_),b(d,ot,_),g(hw,ot,null),e(ot,IAo),e(ot,bd),e(bd,jAo),e(bd,Iae),e(Iae,DAo),e(bd,NAo),e(bd,jae),e(jae,qAo),e(bd,OAo),e(ot,GAo),e(ot,uw),e(uw,XAo),e(uw,Dae),e(Dae,VAo),e(uw,zAo),e(ot,WAo),e(ot,Ut),g(pw,Ut,null),e(Ut,QAo),e(Ut,Nae),e(Nae,HAo),e(Ut,UAo),e(Ut,vd),e(vd,JAo),e(vd,qae),e(qae,YAo),e(vd,KAo),e(vd,Oae),e(Oae,ZAo),e(vd,e0o),e(Ut,o0o),e(Ut,Gae),e(Gae,t0o),e(Ut,r0o),g(_w,Ut,null),e(ot,a0o),e(ot,Ne),g(bw,Ne,null),e(Ne,s0o),e(Ne,Xae),e(Xae,n0o),e(Ne,l0o),e(Ne,Ja),e(Ja,i0o),e(Ja,Vae),e(Vae,d0o),e(Ja,c0o),e(Ja,zae),e(zae,m0o),e(Ja,f0o),e(Ja,Wae),e(Wae,g0o),e(Ja,h0o),e(Ne,u0o),e(Ne,da),e(da,fv),e(fv,Qae),e(Qae,p0o),e(fv,_0o),e(fv,Aj),e(Aj,b0o),e(fv,v0o),e(da,T0o),e(da,gv),e(gv,Hae),e(Hae,F0o),e(gv,C0o),e(gv,Lj),e(Lj,M0o),e(gv,E0o),e(da,y0o),e(da,hv),e(hv,Uae),e(Uae,w0o),e(hv,A0o),e(hv,Bj),e(Bj,L0o),e(hv,B0o),e(da,x0o),e(da,uv),e(uv,Jae),e(Jae,k0o),e(uv,R0o),e(uv,xj),e(xj,S0o),e(uv,P0o),e(da,$0o),e(da,pv),e(pv,Yae),e(Yae,I0o),e(pv,j0o),e(pv,kj),e(kj,D0o),e(pv,N0o),e(Ne,q0o),e(Ne,_v),e(_v,O0o),e(_v,Kae),e(Kae,G0o),e(_v,X0o),e(_v,Zae),e(Zae,V0o),e(Ne,z0o),e(Ne,ese),e(ese,W0o),e(Ne,Q0o),g(vw,Ne,null),b(d,ZBe,_),b(d,Td,_),e(Td,bv),e(bv,ose),g(Tw,ose,null),e(Td,H0o),e(Td,tse),e(tse,U0o),b(d,exe,_),b(d,tt,_),g(Fw,tt,null),e(tt,J0o),e(tt,Fd),e(Fd,Y0o),e(Fd,rse),e(rse,K0o),e(Fd,Z0o),e(Fd,ase),e(ase,eLo),e(Fd,oLo),e(tt,tLo),e(tt,Cw),e(Cw,rLo),e(Cw,sse),e(sse,aLo),e(Cw,sLo),e(tt,nLo),e(tt,Jt),g(Mw,Jt,null),e(Jt,lLo),e(Jt,nse),e(nse,iLo),e(Jt,dLo),e(Jt,Cd),e(Cd,cLo),e(Cd,lse),e(lse,mLo),e(Cd,fLo),e(Cd,ise),e(ise,gLo),e(Cd,hLo),e(Jt,uLo),e(Jt,dse),e(dse,pLo),e(Jt,_Lo),g(Ew,Jt,null),e(tt,bLo),e(tt,qe),g(yw,qe,null),e(qe,vLo),e(qe,cse),e(cse,TLo),e(qe,FLo),e(qe,Ya),e(Ya,CLo),e(Ya,mse),e(mse,MLo),e(Ya,ELo),e(Ya,fse),e(fse,yLo),e(Ya,wLo),e(Ya,gse),e(gse,ALo),e(Ya,LLo),e(qe,BLo),e(qe,N),e(N,vv),e(vv,hse),e(hse,xLo),e(vv,kLo),e(vv,Rj),e(Rj,RLo),e(vv,SLo),e(N,PLo),e(N,Tv),e(Tv,use),e(use,$Lo),e(Tv,ILo),e(Tv,Sj),e(Sj,jLo),e(Tv,DLo),e(N,NLo),e(N,Fv),e(Fv,pse),e(pse,qLo),e(Fv,OLo),e(Fv,Pj),e(Pj,GLo),e(Fv,XLo),e(N,VLo),e(N,Cv),e(Cv,_se),e(_se,zLo),e(Cv,WLo),e(Cv,$j),e($j,QLo),e(Cv,HLo),e(N,ULo),e(N,Mv),e(Mv,bse),e(bse,JLo),e(Mv,YLo),e(Mv,Ij),e(Ij,KLo),e(Mv,ZLo),e(N,e8o),e(N,Ev),e(Ev,vse),e(vse,o8o),e(Ev,t8o),e(Ev,jj),e(jj,r8o),e(Ev,a8o),e(N,s8o),e(N,yv),e(yv,Tse),e(Tse,n8o),e(yv,l8o),e(yv,Dj),e(Dj,i8o),e(yv,d8o),e(N,c8o),e(N,wv),e(wv,Fse),e(Fse,m8o),e(wv,f8o),e(wv,Nj),e(Nj,g8o),e(wv,h8o),e(N,u8o),e(N,Av),e(Av,Cse),e(Cse,p8o),e(Av,_8o),e(Av,qj),e(qj,b8o),e(Av,v8o),e(N,T8o),e(N,Lv),e(Lv,Mse),e(Mse,F8o),e(Lv,C8o),e(Lv,Oj),e(Oj,M8o),e(Lv,E8o),e(N,y8o),e(N,Bv),e(Bv,Ese),e(Ese,w8o),e(Bv,A8o),e(Bv,Gj),e(Gj,L8o),e(Bv,B8o),e(N,x8o),e(N,xv),e(xv,yse),e(yse,k8o),e(xv,R8o),e(xv,Xj),e(Xj,S8o),e(xv,P8o),e(N,$8o),e(N,kv),e(kv,wse),e(wse,I8o),e(kv,j8o),e(kv,Vj),e(Vj,D8o),e(kv,N8o),e(N,q8o),e(N,Rv),e(Rv,Ase),e(Ase,O8o),e(Rv,G8o),e(Rv,zj),e(zj,X8o),e(Rv,V8o),e(N,z8o),e(N,Sv),e(Sv,Lse),e(Lse,W8o),e(Sv,Q8o),e(Sv,Wj),e(Wj,H8o),e(Sv,U8o),e(N,J8o),e(N,Pv),e(Pv,Bse),e(Bse,Y8o),e(Pv,K8o),e(Pv,Qj),e(Qj,Z8o),e(Pv,e7o),e(N,o7o),e(N,$v),e($v,xse),e(xse,t7o),e($v,r7o),e($v,Hj),e(Hj,a7o),e($v,s7o),e(N,n7o),e(N,Iv),e(Iv,kse),e(kse,l7o),e(Iv,i7o),e(Iv,Uj),e(Uj,d7o),e(Iv,c7o),e(N,m7o),e(N,jv),e(jv,Rse),e(Rse,f7o),e(jv,g7o),e(jv,Jj),e(Jj,h7o),e(jv,u7o),e(N,p7o),e(N,Dv),e(Dv,Sse),e(Sse,_7o),e(Dv,b7o),e(Dv,Yj),e(Yj,v7o),e(Dv,T7o),e(N,F7o),e(N,Nv),e(Nv,Pse),e(Pse,C7o),e(Nv,M7o),e(Nv,Kj),e(Kj,E7o),e(Nv,y7o),e(N,w7o),e(N,qv),e(qv,$se),e($se,A7o),e(qv,L7o),e(qv,Zj),e(Zj,B7o),e(qv,x7o),e(N,k7o),e(N,Ov),e(Ov,Ise),e(Ise,R7o),e(Ov,S7o),e(Ov,eD),e(eD,P7o),e(Ov,$7o),e(N,I7o),e(N,Gv),e(Gv,jse),e(jse,j7o),e(Gv,D7o),e(Gv,oD),e(oD,N7o),e(Gv,q7o),e(N,O7o),e(N,Xv),e(Xv,Dse),e(Dse,G7o),e(Xv,X7o),e(Xv,tD),e(tD,V7o),e(Xv,z7o),e(N,W7o),e(N,Vv),e(Vv,Nse),e(Nse,Q7o),e(Vv,H7o),e(Vv,rD),e(rD,U7o),e(Vv,J7o),e(N,Y7o),e(N,zv),e(zv,qse),e(qse,K7o),e(zv,Z7o),e(zv,aD),e(aD,e9o),e(zv,o9o),e(N,t9o),e(N,Wv),e(Wv,Ose),e(Ose,r9o),e(Wv,a9o),e(Wv,sD),e(sD,s9o),e(Wv,n9o),e(N,l9o),e(N,Qv),e(Qv,Gse),e(Gse,i9o),e(Qv,d9o),e(Qv,nD),e(nD,c9o),e(Qv,m9o),e(N,f9o),e(N,Hv),e(Hv,Xse),e(Xse,g9o),e(Hv,h9o),e(Hv,lD),e(lD,u9o),e(Hv,p9o),e(N,_9o),e(N,Uv),e(Uv,Vse),e(Vse,b9o),e(Uv,v9o),e(Uv,iD),e(iD,T9o),e(Uv,F9o),e(N,C9o),e(N,Jv),e(Jv,zse),e(zse,M9o),e(Jv,E9o),e(Jv,dD),e(dD,y9o),e(Jv,w9o),e(N,A9o),e(N,Yv),e(Yv,Wse),e(Wse,L9o),e(Yv,B9o),e(Yv,cD),e(cD,x9o),e(Yv,k9o),e(qe,R9o),e(qe,Kv),e(Kv,S9o),e(Kv,Qse),e(Qse,P9o),e(Kv,$9o),e(Kv,Hse),e(Hse,I9o),e(qe,j9o),e(qe,Use),e(Use,D9o),e(qe,N9o),g(ww,qe,null),b(d,oxe,_),b(d,Md,_),e(Md,Zv),e(Zv,Jse),g(Aw,Jse,null),e(Md,q9o),e(Md,Yse),e(Yse,O9o),b(d,txe,_),b(d,rt,_),g(Lw,rt,null),e(rt,G9o),e(rt,Ed),e(Ed,X9o),e(Ed,Kse),e(Kse,V9o),e(Ed,z9o),e(Ed,Zse),e(Zse,W9o),e(Ed,Q9o),e(rt,H9o),e(rt,Bw),e(Bw,U9o),e(Bw,ene),e(ene,J9o),e(Bw,Y9o),e(rt,K9o),e(rt,Yt),g(xw,Yt,null),e(Yt,Z9o),e(Yt,one),e(one,eBo),e(Yt,oBo),e(Yt,yd),e(yd,tBo),e(yd,tne),e(tne,rBo),e(yd,aBo),e(yd,rne),e(rne,sBo),e(yd,nBo),e(Yt,lBo),e(Yt,ane),e(ane,iBo),e(Yt,dBo),g(kw,Yt,null),e(rt,cBo),e(rt,Oe),g(Rw,Oe,null),e(Oe,mBo),e(Oe,sne),e(sne,fBo),e(Oe,gBo),e(Oe,Ka),e(Ka,hBo),e(Ka,nne),e(nne,uBo),e(Ka,pBo),e(Ka,lne),e(lne,_Bo),e(Ka,bBo),e(Ka,ine),e(ine,vBo),e(Ka,TBo),e(Oe,FBo),e(Oe,R),e(R,eT),e(eT,dne),e(dne,CBo),e(eT,MBo),e(eT,mD),e(mD,EBo),e(eT,yBo),e(R,wBo),e(R,oT),e(oT,cne),e(cne,ABo),e(oT,LBo),e(oT,fD),e(fD,BBo),e(oT,xBo),e(R,kBo),e(R,tT),e(tT,mne),e(mne,RBo),e(tT,SBo),e(tT,gD),e(gD,PBo),e(tT,$Bo),e(R,IBo),e(R,rT),e(rT,fne),e(fne,jBo),e(rT,DBo),e(rT,hD),e(hD,NBo),e(rT,qBo),e(R,OBo),e(R,aT),e(aT,gne),e(gne,GBo),e(aT,XBo),e(aT,uD),e(uD,VBo),e(aT,zBo),e(R,WBo),e(R,sT),e(sT,hne),e(hne,QBo),e(sT,HBo),e(sT,pD),e(pD,UBo),e(sT,JBo),e(R,YBo),e(R,nT),e(nT,une),e(une,KBo),e(nT,ZBo),e(nT,_D),e(_D,exo),e(nT,oxo),e(R,txo),e(R,lT),e(lT,pne),e(pne,rxo),e(lT,axo),e(lT,bD),e(bD,sxo),e(lT,nxo),e(R,lxo),e(R,iT),e(iT,_ne),e(_ne,ixo),e(iT,dxo),e(iT,vD),e(vD,cxo),e(iT,mxo),e(R,fxo),e(R,dT),e(dT,bne),e(bne,gxo),e(dT,hxo),e(dT,TD),e(TD,uxo),e(dT,pxo),e(R,_xo),e(R,cT),e(cT,vne),e(vne,bxo),e(cT,vxo),e(cT,FD),e(FD,Txo),e(cT,Fxo),e(R,Cxo),e(R,mT),e(mT,Tne),e(Tne,Mxo),e(mT,Exo),e(mT,CD),e(CD,yxo),e(mT,wxo),e(R,Axo),e(R,fT),e(fT,Fne),e(Fne,Lxo),e(fT,Bxo),e(fT,MD),e(MD,xxo),e(fT,kxo),e(R,Rxo),e(R,gT),e(gT,Cne),e(Cne,Sxo),e(gT,Pxo),e(gT,ED),e(ED,$xo),e(gT,Ixo),e(R,jxo),e(R,hT),e(hT,Mne),e(Mne,Dxo),e(hT,Nxo),e(hT,yD),e(yD,qxo),e(hT,Oxo),e(R,Gxo),e(R,uT),e(uT,Ene),e(Ene,Xxo),e(uT,Vxo),e(uT,wD),e(wD,zxo),e(uT,Wxo),e(R,Qxo),e(R,pT),e(pT,yne),e(yne,Hxo),e(pT,Uxo),e(pT,AD),e(AD,Jxo),e(pT,Yxo),e(R,Kxo),e(R,_T),e(_T,wne),e(wne,Zxo),e(_T,eko),e(_T,LD),e(LD,oko),e(_T,tko),e(R,rko),e(R,bT),e(bT,Ane),e(Ane,ako),e(bT,sko),e(bT,BD),e(BD,nko),e(bT,lko),e(R,iko),e(R,vT),e(vT,Lne),e(Lne,dko),e(vT,cko),e(vT,xD),e(xD,mko),e(vT,fko),e(R,gko),e(R,TT),e(TT,Bne),e(Bne,hko),e(TT,uko),e(TT,kD),e(kD,pko),e(TT,_ko),e(R,bko),e(R,FT),e(FT,xne),e(xne,vko),e(FT,Tko),e(FT,RD),e(RD,Fko),e(FT,Cko),e(R,Mko),e(R,CT),e(CT,kne),e(kne,Eko),e(CT,yko),e(CT,SD),e(SD,wko),e(CT,Ako),e(R,Lko),e(R,MT),e(MT,Rne),e(Rne,Bko),e(MT,xko),e(MT,PD),e(PD,kko),e(MT,Rko),e(R,Sko),e(R,ET),e(ET,Sne),e(Sne,Pko),e(ET,$ko),e(ET,$D),e($D,Iko),e(ET,jko),e(R,Dko),e(R,yT),e(yT,Pne),e(Pne,Nko),e(yT,qko),e(yT,ID),e(ID,Oko),e(yT,Gko),e(R,Xko),e(R,wT),e(wT,$ne),e($ne,Vko),e(wT,zko),e(wT,jD),e(jD,Wko),e(wT,Qko),e(R,Hko),e(R,AT),e(AT,Ine),e(Ine,Uko),e(AT,Jko),e(AT,DD),e(DD,Yko),e(AT,Kko),e(R,Zko),e(R,LT),e(LT,jne),e(jne,eRo),e(LT,oRo),e(LT,ND),e(ND,tRo),e(LT,rRo),e(R,aRo),e(R,BT),e(BT,Dne),e(Dne,sRo),e(BT,nRo),e(BT,qD),e(qD,lRo),e(BT,iRo),e(R,dRo),e(R,xT),e(xT,Nne),e(Nne,cRo),e(xT,mRo),e(xT,OD),e(OD,fRo),e(xT,gRo),e(R,hRo),e(R,kT),e(kT,qne),e(qne,uRo),e(kT,pRo),e(kT,GD),e(GD,_Ro),e(kT,bRo),e(R,vRo),e(R,RT),e(RT,One),e(One,TRo),e(RT,FRo),e(RT,XD),e(XD,CRo),e(RT,MRo),e(R,ERo),e(R,ST),e(ST,Gne),e(Gne,yRo),e(ST,wRo),e(ST,VD),e(VD,ARo),e(ST,LRo),e(R,BRo),e(R,PT),e(PT,Xne),e(Xne,xRo),e(PT,kRo),e(PT,zD),e(zD,RRo),e(PT,SRo),e(R,PRo),e(R,$T),e($T,Vne),e(Vne,$Ro),e($T,IRo),e($T,WD),e(WD,jRo),e($T,DRo),e(R,NRo),e(R,IT),e(IT,zne),e(zne,qRo),e(IT,ORo),e(IT,QD),e(QD,GRo),e(IT,XRo),e(R,VRo),e(R,jT),e(jT,Wne),e(Wne,zRo),e(jT,WRo),e(jT,HD),e(HD,QRo),e(jT,HRo),e(R,URo),e(R,DT),e(DT,Qne),e(Qne,JRo),e(DT,YRo),e(DT,UD),e(UD,KRo),e(DT,ZRo),e(Oe,eSo),e(Oe,NT),e(NT,oSo),e(NT,Hne),e(Hne,tSo),e(NT,rSo),e(NT,Une),e(Une,aSo),e(Oe,sSo),e(Oe,Jne),e(Jne,nSo),e(Oe,lSo),g(Sw,Oe,null),b(d,rxe,_),b(d,wd,_),e(wd,qT),e(qT,Yne),g(Pw,Yne,null),e(wd,iSo),e(wd,Kne),e(Kne,dSo),b(d,axe,_),b(d,at,_),g($w,at,null),e(at,cSo),e(at,Ad),e(Ad,mSo),e(Ad,Zne),e(Zne,fSo),e(Ad,gSo),e(Ad,ele),e(ele,hSo),e(Ad,uSo),e(at,pSo),e(at,Iw),e(Iw,_So),e(Iw,ole),e(ole,bSo),e(Iw,vSo),e(at,TSo),e(at,Kt),g(jw,Kt,null),e(Kt,FSo),e(Kt,tle),e(tle,CSo),e(Kt,MSo),e(Kt,Ld),e(Ld,ESo),e(Ld,rle),e(rle,ySo),e(Ld,wSo),e(Ld,ale),e(ale,ASo),e(Ld,LSo),e(Kt,BSo),e(Kt,sle),e(sle,xSo),e(Kt,kSo),g(Dw,Kt,null),e(at,RSo),e(at,Ge),g(Nw,Ge,null),e(Ge,SSo),e(Ge,nle),e(nle,PSo),e(Ge,$So),e(Ge,Za),e(Za,ISo),e(Za,lle),e(lle,jSo),e(Za,DSo),e(Za,ile),e(ile,NSo),e(Za,qSo),e(Za,dle),e(dle,OSo),e(Za,GSo),e(Ge,XSo),e(Ge,cle),e(cle,OT),e(OT,mle),e(mle,VSo),e(OT,zSo),e(OT,JD),e(JD,WSo),e(OT,QSo),e(Ge,HSo),e(Ge,GT),e(GT,USo),e(GT,fle),e(fle,JSo),e(GT,YSo),e(GT,gle),e(gle,KSo),e(Ge,ZSo),e(Ge,hle),e(hle,ePo),e(Ge,oPo),g(qw,Ge,null),b(d,sxe,_),b(d,Bd,_),e(Bd,XT),e(XT,ule),g(Ow,ule,null),e(Bd,tPo),e(Bd,ple),e(ple,rPo),b(d,nxe,_),b(d,st,_),g(Gw,st,null),e(st,aPo),e(st,xd),e(xd,sPo),e(xd,_le),e(_le,nPo),e(xd,lPo),e(xd,ble),e(ble,iPo),e(xd,dPo),e(st,cPo),e(st,Xw),e(Xw,mPo),e(Xw,vle),e(vle,fPo),e(Xw,gPo),e(st,hPo),e(st,Zt),g(Vw,Zt,null),e(Zt,uPo),e(Zt,Tle),e(Tle,pPo),e(Zt,_Po),e(Zt,kd),e(kd,bPo),e(kd,Fle),e(Fle,vPo),e(kd,TPo),e(kd,Cle),e(Cle,FPo),e(kd,CPo),e(Zt,MPo),e(Zt,Mle),e(Mle,EPo),e(Zt,yPo),g(zw,Zt,null),e(st,wPo),e(st,Xe),g(Ww,Xe,null),e(Xe,APo),e(Xe,Ele),e(Ele,LPo),e(Xe,BPo),e(Xe,es),e(es,xPo),e(es,yle),e(yle,kPo),e(es,RPo),e(es,wle),e(wle,SPo),e(es,PPo),e(es,Ale),e(Ale,$Po),e(es,IPo),e(Xe,jPo),e(Xe,be),e(be,VT),e(VT,Lle),e(Lle,DPo),e(VT,NPo),e(VT,YD),e(YD,qPo),e(VT,OPo),e(be,GPo),e(be,zT),e(zT,Ble),e(Ble,XPo),e(zT,VPo),e(zT,KD),e(KD,zPo),e(zT,WPo),e(be,QPo),e(be,qn),e(qn,xle),e(xle,HPo),e(qn,UPo),e(qn,ZD),e(ZD,JPo),e(qn,YPo),e(qn,eN),e(eN,KPo),e(qn,ZPo),e(be,e$o),e(be,WT),e(WT,kle),e(kle,o$o),e(WT,t$o),e(WT,oN),e(oN,r$o),e(WT,a$o),e(be,s$o),e(be,ma),e(ma,Rle),e(Rle,n$o),e(ma,l$o),e(ma,tN),e(tN,i$o),e(ma,d$o),e(ma,rN),e(rN,c$o),e(ma,m$o),e(ma,aN),e(aN,f$o),e(ma,g$o),e(be,h$o),e(be,QT),e(QT,Sle),e(Sle,u$o),e(QT,p$o),e(QT,sN),e(sN,_$o),e(QT,b$o),e(be,v$o),e(be,HT),e(HT,Ple),e(Ple,T$o),e(HT,F$o),e(HT,nN),e(nN,C$o),e(HT,M$o),e(be,E$o),e(be,UT),e(UT,$le),e($le,y$o),e(UT,w$o),e(UT,lN),e(lN,A$o),e(UT,L$o),e(be,B$o),e(be,JT),e(JT,Ile),e(Ile,x$o),e(JT,k$o),e(JT,iN),e(iN,R$o),e(JT,S$o),e(Xe,P$o),e(Xe,YT),e(YT,$$o),e(YT,jle),e(jle,I$o),e(YT,j$o),e(YT,Dle),e(Dle,D$o),e(Xe,N$o),e(Xe,Nle),e(Nle,q$o),e(Xe,O$o),g(Qw,Xe,null),b(d,lxe,_),b(d,Rd,_),e(Rd,KT),e(KT,qle),g(Hw,qle,null),e(Rd,G$o),e(Rd,Ole),e(Ole,X$o),b(d,ixe,_),b(d,nt,_),g(Uw,nt,null),e(nt,V$o),e(nt,Sd),e(Sd,z$o),e(Sd,Gle),e(Gle,W$o),e(Sd,Q$o),e(Sd,Xle),e(Xle,H$o),e(Sd,U$o),e(nt,J$o),e(nt,Jw),e(Jw,Y$o),e(Jw,Vle),e(Vle,K$o),e(Jw,Z$o),e(nt,eIo),e(nt,er),g(Yw,er,null),e(er,oIo),e(er,zle),e(zle,tIo),e(er,rIo),e(er,Pd),e(Pd,aIo),e(Pd,Wle),e(Wle,sIo),e(Pd,nIo),e(Pd,Qle),e(Qle,lIo),e(Pd,iIo),e(er,dIo),e(er,Hle),e(Hle,cIo),e(er,mIo),g(Kw,er,null),e(nt,fIo),e(nt,Ve),g(Zw,Ve,null),e(Ve,gIo),e(Ve,Ule),e(Ule,hIo),e(Ve,uIo),e(Ve,os),e(os,pIo),e(os,Jle),e(Jle,_Io),e(os,bIo),e(os,Yle),e(Yle,vIo),e(os,TIo),e(os,Kle),e(Kle,FIo),e(os,CIo),e(Ve,MIo),e(Ve,Zle),e(Zle,ZT),e(ZT,eie),e(eie,EIo),e(ZT,yIo),e(ZT,dN),e(dN,wIo),e(ZT,AIo),e(Ve,LIo),e(Ve,e1),e(e1,BIo),e(e1,oie),e(oie,xIo),e(e1,kIo),e(e1,tie),e(tie,RIo),e(Ve,SIo),e(Ve,rie),e(rie,PIo),e(Ve,$Io),g(e6,Ve,null),b(d,dxe,_),b(d,$d,_),e($d,o1),e(o1,aie),g(o6,aie,null),e($d,IIo),e($d,sie),e(sie,jIo),b(d,cxe,_),b(d,lt,_),g(t6,lt,null),e(lt,DIo),e(lt,Id),e(Id,NIo),e(Id,nie),e(nie,qIo),e(Id,OIo),e(Id,lie),e(lie,GIo),e(Id,XIo),e(lt,VIo),e(lt,r6),e(r6,zIo),e(r6,iie),e(iie,WIo),e(r6,QIo),e(lt,HIo),e(lt,or),g(a6,or,null),e(or,UIo),e(or,die),e(die,JIo),e(or,YIo),e(or,jd),e(jd,KIo),e(jd,cie),e(cie,ZIo),e(jd,ejo),e(jd,mie),e(mie,ojo),e(jd,tjo),e(or,rjo),e(or,fie),e(fie,ajo),e(or,sjo),g(s6,or,null),e(lt,njo),e(lt,ze),g(n6,ze,null),e(ze,ljo),e(ze,gie),e(gie,ijo),e(ze,djo),e(ze,ts),e(ts,cjo),e(ts,hie),e(hie,mjo),e(ts,fjo),e(ts,uie),e(uie,gjo),e(ts,hjo),e(ts,pie),e(pie,ujo),e(ts,pjo),e(ze,_jo),e(ze,Ae),e(Ae,t1),e(t1,_ie),e(_ie,bjo),e(t1,vjo),e(t1,cN),e(cN,Tjo),e(t1,Fjo),e(Ae,Cjo),e(Ae,r1),e(r1,bie),e(bie,Mjo),e(r1,Ejo),e(r1,mN),e(mN,yjo),e(r1,wjo),e(Ae,Ajo),e(Ae,a1),e(a1,vie),e(vie,Ljo),e(a1,Bjo),e(a1,fN),e(fN,xjo),e(a1,kjo),e(Ae,Rjo),e(Ae,s1),e(s1,Tie),e(Tie,Sjo),e(s1,Pjo),e(s1,gN),e(gN,$jo),e(s1,Ijo),e(Ae,jjo),e(Ae,n1),e(n1,Fie),e(Fie,Djo),e(n1,Njo),e(n1,hN),e(hN,qjo),e(n1,Ojo),e(Ae,Gjo),e(Ae,l1),e(l1,Cie),e(Cie,Xjo),e(l1,Vjo),e(l1,uN),e(uN,zjo),e(l1,Wjo),e(Ae,Qjo),e(Ae,i1),e(i1,Mie),e(Mie,Hjo),e(i1,Ujo),e(i1,pN),e(pN,Jjo),e(i1,Yjo),e(Ae,Kjo),e(Ae,d1),e(d1,Eie),e(Eie,Zjo),e(d1,eDo),e(d1,_N),e(_N,oDo),e(d1,tDo),e(ze,rDo),e(ze,c1),e(c1,aDo),e(c1,yie),e(yie,sDo),e(c1,nDo),e(c1,wie),e(wie,lDo),e(ze,iDo),e(ze,Aie),e(Aie,dDo),e(ze,cDo),g(l6,ze,null),b(d,mxe,_),b(d,Dd,_),e(Dd,m1),e(m1,Lie),g(i6,Lie,null),e(Dd,mDo),e(Dd,Bie),e(Bie,fDo),b(d,fxe,_),b(d,it,_),g(d6,it,null),e(it,gDo),e(it,Nd),e(Nd,hDo),e(Nd,xie),e(xie,uDo),e(Nd,pDo),e(Nd,kie),e(kie,_Do),e(Nd,bDo),e(it,vDo),e(it,c6),e(c6,TDo),e(c6,Rie),e(Rie,FDo),e(c6,CDo),e(it,MDo),e(it,tr),g(m6,tr,null),e(tr,EDo),e(tr,Sie),e(Sie,yDo),e(tr,wDo),e(tr,qd),e(qd,ADo),e(qd,Pie),e(Pie,LDo),e(qd,BDo),e(qd,$ie),e($ie,xDo),e(qd,kDo),e(tr,RDo),e(tr,Iie),e(Iie,SDo),e(tr,PDo),g(f6,tr,null),e(it,$Do),e(it,We),g(g6,We,null),e(We,IDo),e(We,jie),e(jie,jDo),e(We,DDo),e(We,rs),e(rs,NDo),e(rs,Die),e(Die,qDo),e(rs,ODo),e(rs,Nie),e(Nie,GDo),e(rs,XDo),e(rs,qie),e(qie,VDo),e(rs,zDo),e(We,WDo),e(We,as),e(as,f1),e(f1,Oie),e(Oie,QDo),e(f1,HDo),e(f1,bN),e(bN,UDo),e(f1,JDo),e(as,YDo),e(as,g1),e(g1,Gie),e(Gie,KDo),e(g1,ZDo),e(g1,vN),e(vN,eNo),e(g1,oNo),e(as,tNo),e(as,h1),e(h1,Xie),e(Xie,rNo),e(h1,aNo),e(h1,TN),e(TN,sNo),e(h1,nNo),e(as,lNo),e(as,u1),e(u1,Vie),e(Vie,iNo),e(u1,dNo),e(u1,FN),e(FN,cNo),e(u1,mNo),e(We,fNo),e(We,p1),e(p1,gNo),e(p1,zie),e(zie,hNo),e(p1,uNo),e(p1,Wie),e(Wie,pNo),e(We,_No),e(We,Qie),e(Qie,bNo),e(We,vNo),g(h6,We,null),b(d,gxe,_),b(d,Od,_),e(Od,_1),e(_1,Hie),g(u6,Hie,null),e(Od,TNo),e(Od,Uie),e(Uie,FNo),b(d,hxe,_),b(d,dt,_),g(p6,dt,null),e(dt,CNo),e(dt,Gd),e(Gd,MNo),e(Gd,Jie),e(Jie,ENo),e(Gd,yNo),e(Gd,Yie),e(Yie,wNo),e(Gd,ANo),e(dt,LNo),e(dt,_6),e(_6,BNo),e(_6,Kie),e(Kie,xNo),e(_6,kNo),e(dt,RNo),e(dt,rr),g(b6,rr,null),e(rr,SNo),e(rr,Zie),e(Zie,PNo),e(rr,$No),e(rr,Xd),e(Xd,INo),e(Xd,ede),e(ede,jNo),e(Xd,DNo),e(Xd,ode),e(ode,NNo),e(Xd,qNo),e(rr,ONo),e(rr,tde),e(tde,GNo),e(rr,XNo),g(v6,rr,null),e(dt,VNo),e(dt,Qe),g(T6,Qe,null),e(Qe,zNo),e(Qe,rde),e(rde,WNo),e(Qe,QNo),e(Qe,ss),e(ss,HNo),e(ss,ade),e(ade,UNo),e(ss,JNo),e(ss,sde),e(sde,YNo),e(ss,KNo),e(ss,nde),e(nde,ZNo),e(ss,eqo),e(Qe,oqo),e(Qe,Le),e(Le,b1),e(b1,lde),e(lde,tqo),e(b1,rqo),e(b1,CN),e(CN,aqo),e(b1,sqo),e(Le,nqo),e(Le,v1),e(v1,ide),e(ide,lqo),e(v1,iqo),e(v1,MN),e(MN,dqo),e(v1,cqo),e(Le,mqo),e(Le,T1),e(T1,dde),e(dde,fqo),e(T1,gqo),e(T1,EN),e(EN,hqo),e(T1,uqo),e(Le,pqo),e(Le,F1),e(F1,cde),e(cde,_qo),e(F1,bqo),e(F1,yN),e(yN,vqo),e(F1,Tqo),e(Le,Fqo),e(Le,C1),e(C1,mde),e(mde,Cqo),e(C1,Mqo),e(C1,wN),e(wN,Eqo),e(C1,yqo),e(Le,wqo),e(Le,M1),e(M1,fde),e(fde,Aqo),e(M1,Lqo),e(M1,AN),e(AN,Bqo),e(M1,xqo),e(Le,kqo),e(Le,E1),e(E1,gde),e(gde,Rqo),e(E1,Sqo),e(E1,LN),e(LN,Pqo),e(E1,$qo),e(Le,Iqo),e(Le,y1),e(y1,hde),e(hde,jqo),e(y1,Dqo),e(y1,BN),e(BN,Nqo),e(y1,qqo),e(Qe,Oqo),e(Qe,w1),e(w1,Gqo),e(w1,ude),e(ude,Xqo),e(w1,Vqo),e(w1,pde),e(pde,zqo),e(Qe,Wqo),e(Qe,_de),e(_de,Qqo),e(Qe,Hqo),g(F6,Qe,null),b(d,uxe,_),b(d,Vd,_),e(Vd,A1),e(A1,bde),g(C6,bde,null),e(Vd,Uqo),e(Vd,vde),e(vde,Jqo),b(d,pxe,_),b(d,ct,_),g(M6,ct,null),e(ct,Yqo),e(ct,zd),e(zd,Kqo),e(zd,Tde),e(Tde,Zqo),e(zd,eOo),e(zd,Fde),e(Fde,oOo),e(zd,tOo),e(ct,rOo),e(ct,E6),e(E6,aOo),e(E6,Cde),e(Cde,sOo),e(E6,nOo),e(ct,lOo),e(ct,ar),g(y6,ar,null),e(ar,iOo),e(ar,Mde),e(Mde,dOo),e(ar,cOo),e(ar,Wd),e(Wd,mOo),e(Wd,Ede),e(Ede,fOo),e(Wd,gOo),e(Wd,yde),e(yde,hOo),e(Wd,uOo),e(ar,pOo),e(ar,wde),e(wde,_Oo),e(ar,bOo),g(w6,ar,null),e(ct,vOo),e(ct,He),g(A6,He,null),e(He,TOo),e(He,Ade),e(Ade,FOo),e(He,COo),e(He,ns),e(ns,MOo),e(ns,Lde),e(Lde,EOo),e(ns,yOo),e(ns,Bde),e(Bde,wOo),e(ns,AOo),e(ns,xde),e(xde,LOo),e(ns,BOo),e(He,xOo),e(He,L6),e(L6,L1),e(L1,kde),e(kde,kOo),e(L1,ROo),e(L1,xN),e(xN,SOo),e(L1,POo),e(L6,$Oo),e(L6,B1),e(B1,Rde),e(Rde,IOo),e(B1,jOo),e(B1,kN),e(kN,DOo),e(B1,NOo),e(He,qOo),e(He,x1),e(x1,OOo),e(x1,Sde),e(Sde,GOo),e(x1,XOo),e(x1,Pde),e(Pde,VOo),e(He,zOo),e(He,$de),e($de,WOo),e(He,QOo),g(B6,He,null),b(d,_xe,_),b(d,Qd,_),e(Qd,k1),e(k1,Ide),g(x6,Ide,null),e(Qd,HOo),e(Qd,jde),e(jde,UOo),b(d,bxe,_),b(d,mt,_),g(k6,mt,null),e(mt,JOo),e(mt,Hd),e(Hd,YOo),e(Hd,Dde),e(Dde,KOo),e(Hd,ZOo),e(Hd,Nde),e(Nde,eGo),e(Hd,oGo),e(mt,tGo),e(mt,R6),e(R6,rGo),e(R6,qde),e(qde,aGo),e(R6,sGo),e(mt,nGo),e(mt,sr),g(S6,sr,null),e(sr,lGo),e(sr,Ode),e(Ode,iGo),e(sr,dGo),e(sr,Ud),e(Ud,cGo),e(Ud,Gde),e(Gde,mGo),e(Ud,fGo),e(Ud,Xde),e(Xde,gGo),e(Ud,hGo),e(sr,uGo),e(sr,Vde),e(Vde,pGo),e(sr,_Go),g(P6,sr,null),e(mt,bGo),e(mt,Ue),g($6,Ue,null),e(Ue,vGo),e(Ue,zde),e(zde,TGo),e(Ue,FGo),e(Ue,ls),e(ls,CGo),e(ls,Wde),e(Wde,MGo),e(ls,EGo),e(ls,Qde),e(Qde,yGo),e(ls,wGo),e(ls,Hde),e(Hde,AGo),e(ls,LGo),e(Ue,BGo),e(Ue,is),e(is,R1),e(R1,Ude),e(Ude,xGo),e(R1,kGo),e(R1,RN),e(RN,RGo),e(R1,SGo),e(is,PGo),e(is,S1),e(S1,Jde),e(Jde,$Go),e(S1,IGo),e(S1,SN),e(SN,jGo),e(S1,DGo),e(is,NGo),e(is,P1),e(P1,Yde),e(Yde,qGo),e(P1,OGo),e(P1,PN),e(PN,GGo),e(P1,XGo),e(is,VGo),e(is,$1),e($1,Kde),e(Kde,zGo),e($1,WGo),e($1,$N),e($N,QGo),e($1,HGo),e(Ue,UGo),e(Ue,I1),e(I1,JGo),e(I1,Zde),e(Zde,YGo),e(I1,KGo),e(I1,ece),e(ece,ZGo),e(Ue,eXo),e(Ue,oce),e(oce,oXo),e(Ue,tXo),g(I6,Ue,null),b(d,vxe,_),b(d,Jd,_),e(Jd,j1),e(j1,tce),g(j6,tce,null),e(Jd,rXo),e(Jd,rce),e(rce,aXo),b(d,Txe,_),b(d,ft,_),g(D6,ft,null),e(ft,sXo),e(ft,Yd),e(Yd,nXo),e(Yd,ace),e(ace,lXo),e(Yd,iXo),e(Yd,sce),e(sce,dXo),e(Yd,cXo),e(ft,mXo),e(ft,N6),e(N6,fXo),e(N6,nce),e(nce,gXo),e(N6,hXo),e(ft,uXo),e(ft,nr),g(q6,nr,null),e(nr,pXo),e(nr,lce),e(lce,_Xo),e(nr,bXo),e(nr,Kd),e(Kd,vXo),e(Kd,ice),e(ice,TXo),e(Kd,FXo),e(Kd,dce),e(dce,CXo),e(Kd,MXo),e(nr,EXo),e(nr,cce),e(cce,yXo),e(nr,wXo),g(O6,nr,null),e(ft,AXo),e(ft,Je),g(G6,Je,null),e(Je,LXo),e(Je,mce),e(mce,BXo),e(Je,xXo),e(Je,ds),e(ds,kXo),e(ds,fce),e(fce,RXo),e(ds,SXo),e(ds,gce),e(gce,PXo),e(ds,$Xo),e(ds,hce),e(hce,IXo),e(ds,jXo),e(Je,DXo),e(Je,Zd),e(Zd,D1),e(D1,uce),e(uce,NXo),e(D1,qXo),e(D1,IN),e(IN,OXo),e(D1,GXo),e(Zd,XXo),e(Zd,N1),e(N1,pce),e(pce,VXo),e(N1,zXo),e(N1,jN),e(jN,WXo),e(N1,QXo),e(Zd,HXo),e(Zd,q1),e(q1,_ce),e(_ce,UXo),e(q1,JXo),e(q1,DN),e(DN,YXo),e(q1,KXo),e(Je,ZXo),e(Je,O1),e(O1,eVo),e(O1,bce),e(bce,oVo),e(O1,tVo),e(O1,vce),e(vce,rVo),e(Je,aVo),e(Je,Tce),e(Tce,sVo),e(Je,nVo),g(X6,Je,null),b(d,Fxe,_),b(d,ec,_),e(ec,G1),e(G1,Fce),g(V6,Fce,null),e(ec,lVo),e(ec,Cce),e(Cce,iVo),b(d,Cxe,_),b(d,gt,_),g(z6,gt,null),e(gt,dVo),e(gt,oc),e(oc,cVo),e(oc,Mce),e(Mce,mVo),e(oc,fVo),e(oc,Ece),e(Ece,gVo),e(oc,hVo),e(gt,uVo),e(gt,W6),e(W6,pVo),e(W6,yce),e(yce,_Vo),e(W6,bVo),e(gt,vVo),e(gt,lr),g(Q6,lr,null),e(lr,TVo),e(lr,wce),e(wce,FVo),e(lr,CVo),e(lr,tc),e(tc,MVo),e(tc,Ace),e(Ace,EVo),e(tc,yVo),e(tc,Lce),e(Lce,wVo),e(tc,AVo),e(lr,LVo),e(lr,Bce),e(Bce,BVo),e(lr,xVo),g(H6,lr,null),e(gt,kVo),e(gt,Ye),g(U6,Ye,null),e(Ye,RVo),e(Ye,xce),e(xce,SVo),e(Ye,PVo),e(Ye,cs),e(cs,$Vo),e(cs,kce),e(kce,IVo),e(cs,jVo),e(cs,Rce),e(Rce,DVo),e(cs,NVo),e(cs,Sce),e(Sce,qVo),e(cs,OVo),e(Ye,GVo),e(Ye,Pce),e(Pce,X1),e(X1,$ce),e($ce,XVo),e(X1,VVo),e(X1,NN),e(NN,zVo),e(X1,WVo),e(Ye,QVo),e(Ye,V1),e(V1,HVo),e(V1,Ice),e(Ice,UVo),e(V1,JVo),e(V1,jce),e(jce,YVo),e(Ye,KVo),e(Ye,Dce),e(Dce,ZVo),e(Ye,ezo),g(J6,Ye,null),b(d,Mxe,_),b(d,rc,_),e(rc,z1),e(z1,Nce),g(Y6,Nce,null),e(rc,ozo),e(rc,qce),e(qce,tzo),b(d,Exe,_),b(d,ht,_),g(K6,ht,null),e(ht,rzo),e(ht,ac),e(ac,azo),e(ac,Oce),e(Oce,szo),e(ac,nzo),e(ac,Gce),e(Gce,lzo),e(ac,izo),e(ht,dzo),e(ht,Z6),e(Z6,czo),e(Z6,Xce),e(Xce,mzo),e(Z6,fzo),e(ht,gzo),e(ht,ir),g(eA,ir,null),e(ir,hzo),e(ir,Vce),e(Vce,uzo),e(ir,pzo),e(ir,sc),e(sc,_zo),e(sc,zce),e(zce,bzo),e(sc,vzo),e(sc,Wce),e(Wce,Tzo),e(sc,Fzo),e(ir,Czo),e(ir,Qce),e(Qce,Mzo),e(ir,Ezo),g(oA,ir,null),e(ht,yzo),e(ht,Ke),g(tA,Ke,null),e(Ke,wzo),e(Ke,Hce),e(Hce,Azo),e(Ke,Lzo),e(Ke,ms),e(ms,Bzo),e(ms,Uce),e(Uce,xzo),e(ms,kzo),e(ms,Jce),e(Jce,Rzo),e(ms,Szo),e(ms,Yce),e(Yce,Pzo),e(ms,$zo),e(Ke,Izo),e(Ke,Kce),e(Kce,W1),e(W1,Zce),e(Zce,jzo),e(W1,Dzo),e(W1,qN),e(qN,Nzo),e(W1,qzo),e(Ke,Ozo),e(Ke,Q1),e(Q1,Gzo),e(Q1,eme),e(eme,Xzo),e(Q1,Vzo),e(Q1,ome),e(ome,zzo),e(Ke,Wzo),e(Ke,tme),e(tme,Qzo),e(Ke,Hzo),g(rA,Ke,null),b(d,yxe,_),b(d,nc,_),e(nc,H1),e(H1,rme),g(aA,rme,null),e(nc,Uzo),e(nc,ame),e(ame,Jzo),b(d,wxe,_),b(d,ut,_),g(sA,ut,null),e(ut,Yzo),e(ut,lc),e(lc,Kzo),e(lc,sme),e(sme,Zzo),e(lc,eWo),e(lc,nme),e(nme,oWo),e(lc,tWo),e(ut,rWo),e(ut,nA),e(nA,aWo),e(nA,lme),e(lme,sWo),e(nA,nWo),e(ut,lWo),e(ut,dr),g(lA,dr,null),e(dr,iWo),e(dr,ime),e(ime,dWo),e(dr,cWo),e(dr,ic),e(ic,mWo),e(ic,dme),e(dme,fWo),e(ic,gWo),e(ic,cme),e(cme,hWo),e(ic,uWo),e(dr,pWo),e(dr,mme),e(mme,_Wo),e(dr,bWo),g(iA,dr,null),e(ut,vWo),e(ut,Ze),g(dA,Ze,null),e(Ze,TWo),e(Ze,fme),e(fme,FWo),e(Ze,CWo),e(Ze,fs),e(fs,MWo),e(fs,gme),e(gme,EWo),e(fs,yWo),e(fs,hme),e(hme,wWo),e(fs,AWo),e(fs,ume),e(ume,LWo),e(fs,BWo),e(Ze,xWo),e(Ze,cA),e(cA,U1),e(U1,pme),e(pme,kWo),e(U1,RWo),e(U1,ON),e(ON,SWo),e(U1,PWo),e(cA,$Wo),e(cA,J1),e(J1,_me),e(_me,IWo),e(J1,jWo),e(J1,GN),e(GN,DWo),e(J1,NWo),e(Ze,qWo),e(Ze,Y1),e(Y1,OWo),e(Y1,bme),e(bme,GWo),e(Y1,XWo),e(Y1,vme),e(vme,VWo),e(Ze,zWo),e(Ze,Tme),e(Tme,WWo),e(Ze,QWo),g(mA,Ze,null),b(d,Axe,_),b(d,dc,_),e(dc,K1),e(K1,Fme),g(fA,Fme,null),e(dc,HWo),e(dc,Cme),e(Cme,UWo),b(d,Lxe,_),b(d,pt,_),g(gA,pt,null),e(pt,JWo),e(pt,cc),e(cc,YWo),e(cc,Mme),e(Mme,KWo),e(cc,ZWo),e(cc,Eme),e(Eme,eQo),e(cc,oQo),e(pt,tQo),e(pt,hA),e(hA,rQo),e(hA,yme),e(yme,aQo),e(hA,sQo),e(pt,nQo),e(pt,cr),g(uA,cr,null),e(cr,lQo),e(cr,wme),e(wme,iQo),e(cr,dQo),e(cr,mc),e(mc,cQo),e(mc,Ame),e(Ame,mQo),e(mc,fQo),e(mc,Lme),e(Lme,gQo),e(mc,hQo),e(cr,uQo),e(cr,Bme),e(Bme,pQo),e(cr,_Qo),g(pA,cr,null),e(pt,bQo),e(pt,eo),g(_A,eo,null),e(eo,vQo),e(eo,xme),e(xme,TQo),e(eo,FQo),e(eo,gs),e(gs,CQo),e(gs,kme),e(kme,MQo),e(gs,EQo),e(gs,Rme),e(Rme,yQo),e(gs,wQo),e(gs,Sme),e(Sme,AQo),e(gs,LQo),e(eo,BQo),e(eo,Pme),e(Pme,Z1),e(Z1,$me),e($me,xQo),e(Z1,kQo),e(Z1,XN),e(XN,RQo),e(Z1,SQo),e(eo,PQo),e(eo,eF),e(eF,$Qo),e(eF,Ime),e(Ime,IQo),e(eF,jQo),e(eF,jme),e(jme,DQo),e(eo,NQo),e(eo,Dme),e(Dme,qQo),e(eo,OQo),g(bA,eo,null),b(d,Bxe,_),b(d,fc,_),e(fc,oF),e(oF,Nme),g(vA,Nme,null),e(fc,GQo),e(fc,qme),e(qme,XQo),b(d,xxe,_),b(d,_t,_),g(TA,_t,null),e(_t,VQo),e(_t,gc),e(gc,zQo),e(gc,Ome),e(Ome,WQo),e(gc,QQo),e(gc,Gme),e(Gme,HQo),e(gc,UQo),e(_t,JQo),e(_t,FA),e(FA,YQo),e(FA,Xme),e(Xme,KQo),e(FA,ZQo),e(_t,eHo),e(_t,mr),g(CA,mr,null),e(mr,oHo),e(mr,Vme),e(Vme,tHo),e(mr,rHo),e(mr,hc),e(hc,aHo),e(hc,zme),e(zme,sHo),e(hc,nHo),e(hc,Wme),e(Wme,lHo),e(hc,iHo),e(mr,dHo),e(mr,Qme),e(Qme,cHo),e(mr,mHo),g(MA,mr,null),e(_t,fHo),e(_t,ho),g(EA,ho,null),e(ho,gHo),e(ho,Hme),e(Hme,hHo),e(ho,uHo),e(ho,hs),e(hs,pHo),e(hs,Ume),e(Ume,_Ho),e(hs,bHo),e(hs,Jme),e(Jme,vHo),e(hs,THo),e(hs,Yme),e(Yme,FHo),e(hs,CHo),e(ho,MHo),e(ho,B),e(B,tF),e(tF,Kme),e(Kme,EHo),e(tF,yHo),e(tF,VN),e(VN,wHo),e(tF,AHo),e(B,LHo),e(B,rF),e(rF,Zme),e(Zme,BHo),e(rF,xHo),e(rF,zN),e(zN,kHo),e(rF,RHo),e(B,SHo),e(B,aF),e(aF,efe),e(efe,PHo),e(aF,$Ho),e(aF,WN),e(WN,IHo),e(aF,jHo),e(B,DHo),e(B,sF),e(sF,ofe),e(ofe,NHo),e(sF,qHo),e(sF,QN),e(QN,OHo),e(sF,GHo),e(B,XHo),e(B,nF),e(nF,tfe),e(tfe,VHo),e(nF,zHo),e(nF,HN),e(HN,WHo),e(nF,QHo),e(B,HHo),e(B,lF),e(lF,rfe),e(rfe,UHo),e(lF,JHo),e(lF,UN),e(UN,YHo),e(lF,KHo),e(B,ZHo),e(B,iF),e(iF,afe),e(afe,eUo),e(iF,oUo),e(iF,JN),e(JN,tUo),e(iF,rUo),e(B,aUo),e(B,dF),e(dF,sfe),e(sfe,sUo),e(dF,nUo),e(dF,YN),e(YN,lUo),e(dF,iUo),e(B,dUo),e(B,cF),e(cF,nfe),e(nfe,cUo),e(cF,mUo),e(cF,KN),e(KN,fUo),e(cF,gUo),e(B,hUo),e(B,mF),e(mF,lfe),e(lfe,uUo),e(mF,pUo),e(mF,ZN),e(ZN,_Uo),e(mF,bUo),e(B,vUo),e(B,fF),e(fF,ife),e(ife,TUo),e(fF,FUo),e(fF,eq),e(eq,CUo),e(fF,MUo),e(B,EUo),e(B,gF),e(gF,dfe),e(dfe,yUo),e(gF,wUo),e(gF,oq),e(oq,AUo),e(gF,LUo),e(B,BUo),e(B,hF),e(hF,cfe),e(cfe,xUo),e(hF,kUo),e(hF,tq),e(tq,RUo),e(hF,SUo),e(B,PUo),e(B,uF),e(uF,mfe),e(mfe,$Uo),e(uF,IUo),e(uF,rq),e(rq,jUo),e(uF,DUo),e(B,NUo),e(B,pF),e(pF,ffe),e(ffe,qUo),e(pF,OUo),e(pF,aq),e(aq,GUo),e(pF,XUo),e(B,VUo),e(B,_F),e(_F,gfe),e(gfe,zUo),e(_F,WUo),e(_F,sq),e(sq,QUo),e(_F,HUo),e(B,UUo),e(B,On),e(On,hfe),e(hfe,JUo),e(On,YUo),e(On,nq),e(nq,KUo),e(On,ZUo),e(On,lq),e(lq,eJo),e(On,oJo),e(B,tJo),e(B,bF),e(bF,ufe),e(ufe,rJo),e(bF,aJo),e(bF,iq),e(iq,sJo),e(bF,nJo),e(B,lJo),e(B,vF),e(vF,pfe),e(pfe,iJo),e(vF,dJo),e(vF,dq),e(dq,cJo),e(vF,mJo),e(B,fJo),e(B,TF),e(TF,_fe),e(_fe,gJo),e(TF,hJo),e(TF,cq),e(cq,uJo),e(TF,pJo),e(B,_Jo),e(B,FF),e(FF,bfe),e(bfe,bJo),e(FF,vJo),e(FF,mq),e(mq,TJo),e(FF,FJo),e(B,CJo),e(B,CF),e(CF,vfe),e(vfe,MJo),e(CF,EJo),e(CF,fq),e(fq,yJo),e(CF,wJo),e(B,AJo),e(B,MF),e(MF,Tfe),e(Tfe,LJo),e(MF,BJo),e(MF,gq),e(gq,xJo),e(MF,kJo),e(B,RJo),e(B,EF),e(EF,Ffe),e(Ffe,SJo),e(EF,PJo),e(EF,hq),e(hq,$Jo),e(EF,IJo),e(B,jJo),e(B,yF),e(yF,Cfe),e(Cfe,DJo),e(yF,NJo),e(yF,uq),e(uq,qJo),e(yF,OJo),e(B,GJo),e(B,wF),e(wF,Mfe),e(Mfe,XJo),e(wF,VJo),e(wF,pq),e(pq,zJo),e(wF,WJo),e(B,QJo),e(B,AF),e(AF,Efe),e(Efe,HJo),e(AF,UJo),e(AF,_q),e(_q,JJo),e(AF,YJo),e(B,KJo),e(B,LF),e(LF,yfe),e(yfe,ZJo),e(LF,eYo),e(LF,bq),e(bq,oYo),e(LF,tYo),e(B,rYo),e(B,BF),e(BF,wfe),e(wfe,aYo),e(BF,sYo),e(BF,vq),e(vq,nYo),e(BF,lYo),e(B,iYo),e(B,xF),e(xF,Afe),e(Afe,dYo),e(xF,cYo),e(xF,Tq),e(Tq,mYo),e(xF,fYo),e(B,gYo),e(B,kF),e(kF,Lfe),e(Lfe,hYo),e(kF,uYo),e(kF,Fq),e(Fq,pYo),e(kF,_Yo),e(B,bYo),e(B,RF),e(RF,Bfe),e(Bfe,vYo),e(RF,TYo),e(RF,Cq),e(Cq,FYo),e(RF,CYo),e(B,MYo),e(B,SF),e(SF,xfe),e(xfe,EYo),e(SF,yYo),e(SF,Mq),e(Mq,wYo),e(SF,AYo),e(B,LYo),e(B,PF),e(PF,kfe),e(kfe,BYo),e(PF,xYo),e(PF,Eq),e(Eq,kYo),e(PF,RYo),e(B,SYo),e(B,$F),e($F,Rfe),e(Rfe,PYo),e($F,$Yo),e($F,yq),e(yq,IYo),e($F,jYo),e(B,DYo),e(B,IF),e(IF,Sfe),e(Sfe,NYo),e(IF,qYo),e(IF,wq),e(wq,OYo),e(IF,GYo),e(B,XYo),e(B,jF),e(jF,Pfe),e(Pfe,VYo),e(jF,zYo),e(jF,Aq),e(Aq,WYo),e(jF,QYo),e(B,HYo),e(B,DF),e(DF,$fe),e($fe,UYo),e(DF,JYo),e(DF,Lq),e(Lq,YYo),e(DF,KYo),e(B,ZYo),e(B,NF),e(NF,Ife),e(Ife,eKo),e(NF,oKo),e(NF,Bq),e(Bq,tKo),e(NF,rKo),e(B,aKo),e(B,qF),e(qF,jfe),e(jfe,sKo),e(qF,nKo),e(qF,xq),e(xq,lKo),e(qF,iKo),e(B,dKo),e(B,OF),e(OF,Dfe),e(Dfe,cKo),e(OF,mKo),e(OF,kq),e(kq,fKo),e(OF,gKo),e(B,hKo),e(B,GF),e(GF,Nfe),e(Nfe,uKo),e(GF,pKo),e(GF,Rq),e(Rq,_Ko),e(GF,bKo),e(ho,vKo),e(ho,qfe),e(qfe,TKo),e(ho,FKo),g(yA,ho,null),b(d,kxe,_),b(d,uc,_),e(uc,XF),e(XF,Ofe),g(wA,Ofe,null),e(uc,CKo),e(uc,Gfe),e(Gfe,MKo),b(d,Rxe,_),b(d,bt,_),g(AA,bt,null),e(bt,EKo),e(bt,pc),e(pc,yKo),e(pc,Xfe),e(Xfe,wKo),e(pc,AKo),e(pc,Vfe),e(Vfe,LKo),e(pc,BKo),e(bt,xKo),e(bt,LA),e(LA,kKo),e(LA,zfe),e(zfe,RKo),e(LA,SKo),e(bt,PKo),e(bt,fr),g(BA,fr,null),e(fr,$Ko),e(fr,Wfe),e(Wfe,IKo),e(fr,jKo),e(fr,_c),e(_c,DKo),e(_c,Qfe),e(Qfe,NKo),e(_c,qKo),e(_c,Hfe),e(Hfe,OKo),e(_c,GKo),e(fr,XKo),e(fr,Ufe),e(Ufe,VKo),e(fr,zKo),g(xA,fr,null),e(bt,WKo),e(bt,uo),g(kA,uo,null),e(uo,QKo),e(uo,Jfe),e(Jfe,HKo),e(uo,UKo),e(uo,us),e(us,JKo),e(us,Yfe),e(Yfe,YKo),e(us,KKo),e(us,Kfe),e(Kfe,ZKo),e(us,eZo),e(us,Zfe),e(Zfe,oZo),e(us,tZo),e(uo,rZo),e(uo,H),e(H,VF),e(VF,ege),e(ege,aZo),e(VF,sZo),e(VF,Sq),e(Sq,nZo),e(VF,lZo),e(H,iZo),e(H,zF),e(zF,oge),e(oge,dZo),e(zF,cZo),e(zF,Pq),e(Pq,mZo),e(zF,fZo),e(H,gZo),e(H,WF),e(WF,tge),e(tge,hZo),e(WF,uZo),e(WF,$q),e($q,pZo),e(WF,_Zo),e(H,bZo),e(H,QF),e(QF,rge),e(rge,vZo),e(QF,TZo),e(QF,Iq),e(Iq,FZo),e(QF,CZo),e(H,MZo),e(H,HF),e(HF,age),e(age,EZo),e(HF,yZo),e(HF,jq),e(jq,wZo),e(HF,AZo),e(H,LZo),e(H,UF),e(UF,sge),e(sge,BZo),e(UF,xZo),e(UF,Dq),e(Dq,kZo),e(UF,RZo),e(H,SZo),e(H,JF),e(JF,nge),e(nge,PZo),e(JF,$Zo),e(JF,Nq),e(Nq,IZo),e(JF,jZo),e(H,DZo),e(H,YF),e(YF,lge),e(lge,NZo),e(YF,qZo),e(YF,qq),e(qq,OZo),e(YF,GZo),e(H,XZo),e(H,KF),e(KF,ige),e(ige,VZo),e(KF,zZo),e(KF,Oq),e(Oq,WZo),e(KF,QZo),e(H,HZo),e(H,ZF),e(ZF,dge),e(dge,UZo),e(ZF,JZo),e(ZF,Gq),e(Gq,YZo),e(ZF,KZo),e(H,ZZo),e(H,eC),e(eC,cge),e(cge,eet),e(eC,oet),e(eC,Xq),e(Xq,tet),e(eC,ret),e(H,aet),e(H,oC),e(oC,mge),e(mge,set),e(oC,net),e(oC,Vq),e(Vq,iet),e(oC,det),e(H,cet),e(H,tC),e(tC,fge),e(fge,met),e(tC,fet),e(tC,zq),e(zq,get),e(tC,het),e(H,uet),e(H,rC),e(rC,gge),e(gge,pet),e(rC,_et),e(rC,Wq),e(Wq,bet),e(rC,vet),e(H,Tet),e(H,aC),e(aC,hge),e(hge,Fet),e(aC,Cet),e(aC,Qq),e(Qq,Met),e(aC,Eet),e(H,yet),e(H,sC),e(sC,uge),e(uge,wet),e(sC,Aet),e(sC,Hq),e(Hq,Let),e(sC,Bet),e(H,xet),e(H,nC),e(nC,pge),e(pge,ket),e(nC,Ret),e(nC,Uq),e(Uq,Set),e(nC,Pet),e(H,$et),e(H,lC),e(lC,_ge),e(_ge,Iet),e(lC,jet),e(lC,Jq),e(Jq,Det),e(lC,Net),e(H,qet),e(H,iC),e(iC,bge),e(bge,Oet),e(iC,Get),e(iC,Yq),e(Yq,Xet),e(iC,Vet),e(H,zet),e(H,dC),e(dC,vge),e(vge,Wet),e(dC,Qet),e(dC,Kq),e(Kq,Het),e(dC,Uet),e(H,Jet),e(H,cC),e(cC,Tge),e(Tge,Yet),e(cC,Ket),e(cC,Zq),e(Zq,Zet),e(cC,eot),e(H,oot),e(H,mC),e(mC,Fge),e(Fge,tot),e(mC,rot),e(mC,eO),e(eO,aot),e(mC,sot),e(uo,not),e(uo,Cge),e(Cge,lot),e(uo,iot),g(RA,uo,null),b(d,Sxe,_),b(d,bc,_),e(bc,fC),e(fC,Mge),g(SA,Mge,null),e(bc,dot),e(bc,Ege),e(Ege,cot),b(d,Pxe,_),b(d,vt,_),g(PA,vt,null),e(vt,mot),e(vt,vc),e(vc,fot),e(vc,yge),e(yge,got),e(vc,hot),e(vc,wge),e(wge,uot),e(vc,pot),e(vt,_ot),e(vt,$A),e($A,bot),e($A,Age),e(Age,vot),e($A,Tot),e(vt,Fot),e(vt,gr),g(IA,gr,null),e(gr,Cot),e(gr,Lge),e(Lge,Mot),e(gr,Eot),e(gr,Tc),e(Tc,yot),e(Tc,Bge),e(Bge,wot),e(Tc,Aot),e(Tc,xge),e(xge,Lot),e(Tc,Bot),e(gr,xot),e(gr,kge),e(kge,kot),e(gr,Rot),g(jA,gr,null),e(vt,Sot),e(vt,po),g(DA,po,null),e(po,Pot),e(po,Rge),e(Rge,$ot),e(po,Iot),e(po,ps),e(ps,jot),e(ps,Sge),e(Sge,Dot),e(ps,Not),e(ps,Pge),e(Pge,qot),e(ps,Oot),e(ps,$ge),e($ge,Got),e(ps,Xot),e(po,Vot),e(po,he),e(he,gC),e(gC,Ige),e(Ige,zot),e(gC,Wot),e(gC,oO),e(oO,Qot),e(gC,Hot),e(he,Uot),e(he,hC),e(hC,jge),e(jge,Jot),e(hC,Yot),e(hC,tO),e(tO,Kot),e(hC,Zot),e(he,ett),e(he,uC),e(uC,Dge),e(Dge,ott),e(uC,ttt),e(uC,rO),e(rO,rtt),e(uC,att),e(he,stt),e(he,pC),e(pC,Nge),e(Nge,ntt),e(pC,ltt),e(pC,aO),e(aO,itt),e(pC,dtt),e(he,ctt),e(he,_C),e(_C,qge),e(qge,mtt),e(_C,ftt),e(_C,sO),e(sO,gtt),e(_C,htt),e(he,utt),e(he,bC),e(bC,Oge),e(Oge,ptt),e(bC,_tt),e(bC,nO),e(nO,btt),e(bC,vtt),e(he,Ttt),e(he,vC),e(vC,Gge),e(Gge,Ftt),e(vC,Ctt),e(vC,lO),e(lO,Mtt),e(vC,Ett),e(he,ytt),e(he,TC),e(TC,Xge),e(Xge,wtt),e(TC,Att),e(TC,iO),e(iO,Ltt),e(TC,Btt),e(he,xtt),e(he,FC),e(FC,Vge),e(Vge,ktt),e(FC,Rtt),e(FC,dO),e(dO,Stt),e(FC,Ptt),e(he,$tt),e(he,CC),e(CC,zge),e(zge,Itt),e(CC,jtt),e(CC,cO),e(cO,Dtt),e(CC,Ntt),e(po,qtt),e(po,Wge),e(Wge,Ott),e(po,Gtt),g(NA,po,null),b(d,$xe,_),b(d,Fc,_),e(Fc,MC),e(MC,Qge),g(qA,Qge,null),e(Fc,Xtt),e(Fc,Hge),e(Hge,Vtt),b(d,Ixe,_),b(d,Tt,_),g(OA,Tt,null),e(Tt,ztt),e(Tt,Cc),e(Cc,Wtt),e(Cc,Uge),e(Uge,Qtt),e(Cc,Htt),e(Cc,Jge),e(Jge,Utt),e(Cc,Jtt),e(Tt,Ytt),e(Tt,GA),e(GA,Ktt),e(GA,Yge),e(Yge,Ztt),e(GA,ert),e(Tt,ort),e(Tt,hr),g(XA,hr,null),e(hr,trt),e(hr,Kge),e(Kge,rrt),e(hr,art),e(hr,Mc),e(Mc,srt),e(Mc,Zge),e(Zge,nrt),e(Mc,lrt),e(Mc,ehe),e(ehe,irt),e(Mc,drt),e(hr,crt),e(hr,ohe),e(ohe,mrt),e(hr,frt),g(VA,hr,null),e(Tt,grt),e(Tt,_o),g(zA,_o,null),e(_o,hrt),e(_o,the),e(the,urt),e(_o,prt),e(_o,_s),e(_s,_rt),e(_s,rhe),e(rhe,brt),e(_s,vrt),e(_s,ahe),e(ahe,Trt),e(_s,Frt),e(_s,she),e(she,Crt),e(_s,Mrt),e(_o,Ert),e(_o,WA),e(WA,EC),e(EC,nhe),e(nhe,yrt),e(EC,wrt),e(EC,mO),e(mO,Art),e(EC,Lrt),e(WA,Brt),e(WA,yC),e(yC,lhe),e(lhe,xrt),e(yC,krt),e(yC,fO),e(fO,Rrt),e(yC,Srt),e(_o,Prt),e(_o,ihe),e(ihe,$rt),e(_o,Irt),g(QA,_o,null),b(d,jxe,_),b(d,Ec,_),e(Ec,wC),e(wC,dhe),g(HA,dhe,null),e(Ec,jrt),e(Ec,che),e(che,Drt),b(d,Dxe,_),b(d,Ft,_),g(UA,Ft,null),e(Ft,Nrt),e(Ft,yc),e(yc,qrt),e(yc,mhe),e(mhe,Ort),e(yc,Grt),e(yc,fhe),e(fhe,Xrt),e(yc,Vrt),e(Ft,zrt),e(Ft,JA),e(JA,Wrt),e(JA,ghe),e(ghe,Qrt),e(JA,Hrt),e(Ft,Urt),e(Ft,ur),g(YA,ur,null),e(ur,Jrt),e(ur,hhe),e(hhe,Yrt),e(ur,Krt),e(ur,wc),e(wc,Zrt),e(wc,uhe),e(uhe,eat),e(wc,oat),e(wc,phe),e(phe,tat),e(wc,rat),e(ur,aat),e(ur,_he),e(_he,sat),e(ur,nat),g(KA,ur,null),e(Ft,lat),e(Ft,bo),g(ZA,bo,null),e(bo,iat),e(bo,bhe),e(bhe,dat),e(bo,cat),e(bo,bs),e(bs,mat),e(bs,vhe),e(vhe,fat),e(bs,gat),e(bs,The),e(The,hat),e(bs,uat),e(bs,Fhe),e(Fhe,pat),e(bs,_at),e(bo,bat),e(bo,Y),e(Y,AC),e(AC,Che),e(Che,vat),e(AC,Tat),e(AC,gO),e(gO,Fat),e(AC,Cat),e(Y,Mat),e(Y,LC),e(LC,Mhe),e(Mhe,Eat),e(LC,yat),e(LC,hO),e(hO,wat),e(LC,Aat),e(Y,Lat),e(Y,BC),e(BC,Ehe),e(Ehe,Bat),e(BC,xat),e(BC,uO),e(uO,kat),e(BC,Rat),e(Y,Sat),e(Y,xC),e(xC,yhe),e(yhe,Pat),e(xC,$at),e(xC,pO),e(pO,Iat),e(xC,jat),e(Y,Dat),e(Y,kC),e(kC,whe),e(whe,Nat),e(kC,qat),e(kC,_O),e(_O,Oat),e(kC,Gat),e(Y,Xat),e(Y,RC),e(RC,Ahe),e(Ahe,Vat),e(RC,zat),e(RC,bO),e(bO,Wat),e(RC,Qat),e(Y,Hat),e(Y,SC),e(SC,Lhe),e(Lhe,Uat),e(SC,Jat),e(SC,vO),e(vO,Yat),e(SC,Kat),e(Y,Zat),e(Y,PC),e(PC,Bhe),e(Bhe,est),e(PC,ost),e(PC,TO),e(TO,tst),e(PC,rst),e(Y,ast),e(Y,$C),e($C,xhe),e(xhe,sst),e($C,nst),e($C,FO),e(FO,lst),e($C,ist),e(Y,dst),e(Y,IC),e(IC,khe),e(khe,cst),e(IC,mst),e(IC,CO),e(CO,fst),e(IC,gst),e(Y,hst),e(Y,jC),e(jC,Rhe),e(Rhe,ust),e(jC,pst),e(jC,MO),e(MO,_st),e(jC,bst),e(Y,vst),e(Y,DC),e(DC,She),e(She,Tst),e(DC,Fst),e(DC,EO),e(EO,Cst),e(DC,Mst),e(Y,Est),e(Y,NC),e(NC,Phe),e(Phe,yst),e(NC,wst),e(NC,yO),e(yO,Ast),e(NC,Lst),e(Y,Bst),e(Y,qC),e(qC,$he),e($he,xst),e(qC,kst),e(qC,wO),e(wO,Rst),e(qC,Sst),e(Y,Pst),e(Y,OC),e(OC,Ihe),e(Ihe,$st),e(OC,Ist),e(OC,AO),e(AO,jst),e(OC,Dst),e(Y,Nst),e(Y,GC),e(GC,jhe),e(jhe,qst),e(GC,Ost),e(GC,LO),e(LO,Gst),e(GC,Xst),e(Y,Vst),e(Y,XC),e(XC,Dhe),e(Dhe,zst),e(XC,Wst),e(XC,BO),e(BO,Qst),e(XC,Hst),e(Y,Ust),e(Y,VC),e(VC,Nhe),e(Nhe,Jst),e(VC,Yst),e(VC,xO),e(xO,Kst),e(VC,Zst),e(Y,ent),e(Y,zC),e(zC,qhe),e(qhe,ont),e(zC,tnt),e(zC,kO),e(kO,rnt),e(zC,ant),e(Y,snt),e(Y,WC),e(WC,Ohe),e(Ohe,nnt),e(WC,lnt),e(WC,RO),e(RO,int),e(WC,dnt),e(bo,cnt),e(bo,Ghe),e(Ghe,mnt),e(bo,fnt),g(e0,bo,null),b(d,Nxe,_),b(d,Ac,_),e(Ac,QC),e(QC,Xhe),g(o0,Xhe,null),e(Ac,gnt),e(Ac,Vhe),e(Vhe,hnt),b(d,qxe,_),b(d,Ct,_),g(t0,Ct,null),e(Ct,unt),e(Ct,Lc),e(Lc,pnt),e(Lc,zhe),e(zhe,_nt),e(Lc,bnt),e(Lc,Whe),e(Whe,vnt),e(Lc,Tnt),e(Ct,Fnt),e(Ct,r0),e(r0,Cnt),e(r0,Qhe),e(Qhe,Mnt),e(r0,Ent),e(Ct,ynt),e(Ct,pr),g(a0,pr,null),e(pr,wnt),e(pr,Hhe),e(Hhe,Ant),e(pr,Lnt),e(pr,Bc),e(Bc,Bnt),e(Bc,Uhe),e(Uhe,xnt),e(Bc,knt),e(Bc,Jhe),e(Jhe,Rnt),e(Bc,Snt),e(pr,Pnt),e(pr,Yhe),e(Yhe,$nt),e(pr,Int),g(s0,pr,null),e(Ct,jnt),e(Ct,vo),g(n0,vo,null),e(vo,Dnt),e(vo,Khe),e(Khe,Nnt),e(vo,qnt),e(vo,vs),e(vs,Ont),e(vs,Zhe),e(Zhe,Gnt),e(vs,Xnt),e(vs,eue),e(eue,Vnt),e(vs,znt),e(vs,oue),e(oue,Wnt),e(vs,Qnt),e(vo,Hnt),e(vo,ue),e(ue,HC),e(HC,tue),e(tue,Unt),e(HC,Jnt),e(HC,SO),e(SO,Ynt),e(HC,Knt),e(ue,Znt),e(ue,UC),e(UC,rue),e(rue,elt),e(UC,olt),e(UC,PO),e(PO,tlt),e(UC,rlt),e(ue,alt),e(ue,JC),e(JC,aue),e(aue,slt),e(JC,nlt),e(JC,$O),e($O,llt),e(JC,ilt),e(ue,dlt),e(ue,YC),e(YC,sue),e(sue,clt),e(YC,mlt),e(YC,IO),e(IO,flt),e(YC,glt),e(ue,hlt),e(ue,KC),e(KC,nue),e(nue,ult),e(KC,plt),e(KC,jO),e(jO,_lt),e(KC,blt),e(ue,vlt),e(ue,ZC),e(ZC,lue),e(lue,Tlt),e(ZC,Flt),e(ZC,DO),e(DO,Clt),e(ZC,Mlt),e(ue,Elt),e(ue,e4),e(e4,iue),e(iue,ylt),e(e4,wlt),e(e4,NO),e(NO,Alt),e(e4,Llt),e(ue,Blt),e(ue,o4),e(o4,due),e(due,xlt),e(o4,klt),e(o4,qO),e(qO,Rlt),e(o4,Slt),e(ue,Plt),e(ue,t4),e(t4,cue),e(cue,$lt),e(t4,Ilt),e(t4,OO),e(OO,jlt),e(t4,Dlt),e(ue,Nlt),e(ue,r4),e(r4,mue),e(mue,qlt),e(r4,Olt),e(r4,GO),e(GO,Glt),e(r4,Xlt),e(vo,Vlt),e(vo,fue),e(fue,zlt),e(vo,Wlt),g(l0,vo,null),b(d,Oxe,_),b(d,xc,_),e(xc,a4),e(a4,gue),g(i0,gue,null),e(xc,Qlt),e(xc,hue),e(hue,Hlt),b(d,Gxe,_),b(d,Mt,_),g(d0,Mt,null),e(Mt,Ult),e(Mt,kc),e(kc,Jlt),e(kc,uue),e(uue,Ylt),e(kc,Klt),e(kc,pue),e(pue,Zlt),e(kc,eit),e(Mt,oit),e(Mt,c0),e(c0,tit),e(c0,_ue),e(_ue,rit),e(c0,ait),e(Mt,sit),e(Mt,_r),g(m0,_r,null),e(_r,nit),e(_r,bue),e(bue,lit),e(_r,iit),e(_r,Rc),e(Rc,dit),e(Rc,vue),e(vue,cit),e(Rc,mit),e(Rc,Tue),e(Tue,fit),e(Rc,git),e(_r,hit),e(_r,Fue),e(Fue,uit),e(_r,pit),g(f0,_r,null),e(Mt,_it),e(Mt,To),g(g0,To,null),e(To,bit),e(To,Cue),e(Cue,vit),e(To,Tit),e(To,Ts),e(Ts,Fit),e(Ts,Mue),e(Mue,Cit),e(Ts,Mit),e(Ts,Eue),e(Eue,Eit),e(Ts,yit),e(Ts,yue),e(yue,wit),e(Ts,Ait),e(To,Lit),e(To,V),e(V,s4),e(s4,wue),e(wue,Bit),e(s4,xit),e(s4,XO),e(XO,kit),e(s4,Rit),e(V,Sit),e(V,n4),e(n4,Aue),e(Aue,Pit),e(n4,$it),e(n4,VO),e(VO,Iit),e(n4,jit),e(V,Dit),e(V,l4),e(l4,Lue),e(Lue,Nit),e(l4,qit),e(l4,zO),e(zO,Oit),e(l4,Git),e(V,Xit),e(V,i4),e(i4,Bue),e(Bue,Vit),e(i4,zit),e(i4,WO),e(WO,Wit),e(i4,Qit),e(V,Hit),e(V,d4),e(d4,xue),e(xue,Uit),e(d4,Jit),e(d4,QO),e(QO,Yit),e(d4,Kit),e(V,Zit),e(V,c4),e(c4,kue),e(kue,edt),e(c4,odt),e(c4,HO),e(HO,tdt),e(c4,rdt),e(V,adt),e(V,m4),e(m4,Rue),e(Rue,sdt),e(m4,ndt),e(m4,UO),e(UO,ldt),e(m4,idt),e(V,ddt),e(V,f4),e(f4,Sue),e(Sue,cdt),e(f4,mdt),e(f4,JO),e(JO,fdt),e(f4,gdt),e(V,hdt),e(V,g4),e(g4,Pue),e(Pue,udt),e(g4,pdt),e(g4,YO),e(YO,_dt),e(g4,bdt),e(V,vdt),e(V,h4),e(h4,$ue),e($ue,Tdt),e(h4,Fdt),e(h4,KO),e(KO,Cdt),e(h4,Mdt),e(V,Edt),e(V,u4),e(u4,Iue),e(Iue,ydt),e(u4,wdt),e(u4,ZO),e(ZO,Adt),e(u4,Ldt),e(V,Bdt),e(V,p4),e(p4,jue),e(jue,xdt),e(p4,kdt),e(p4,eG),e(eG,Rdt),e(p4,Sdt),e(V,Pdt),e(V,_4),e(_4,Due),e(Due,$dt),e(_4,Idt),e(_4,oG),e(oG,jdt),e(_4,Ddt),e(V,Ndt),e(V,b4),e(b4,Nue),e(Nue,qdt),e(b4,Odt),e(b4,tG),e(tG,Gdt),e(b4,Xdt),e(V,Vdt),e(V,v4),e(v4,que),e(que,zdt),e(v4,Wdt),e(v4,rG),e(rG,Qdt),e(v4,Hdt),e(V,Udt),e(V,T4),e(T4,Oue),e(Oue,Jdt),e(T4,Ydt),e(T4,aG),e(aG,Kdt),e(T4,Zdt),e(V,ect),e(V,F4),e(F4,Gue),e(Gue,oct),e(F4,tct),e(F4,sG),e(sG,rct),e(F4,act),e(V,sct),e(V,C4),e(C4,Xue),e(Xue,nct),e(C4,lct),e(C4,nG),e(nG,ict),e(C4,dct),e(V,cct),e(V,M4),e(M4,Vue),e(Vue,mct),e(M4,fct),e(M4,lG),e(lG,gct),e(M4,hct),e(V,uct),e(V,E4),e(E4,zue),e(zue,pct),e(E4,_ct),e(E4,iG),e(iG,bct),e(E4,vct),e(V,Tct),e(V,y4),e(y4,Wue),e(Wue,Fct),e(y4,Cct),e(y4,dG),e(dG,Mct),e(y4,Ect),e(V,yct),e(V,w4),e(w4,Que),e(Que,wct),e(w4,Act),e(w4,cG),e(cG,Lct),e(w4,Bct),e(V,xct),e(V,A4),e(A4,Hue),e(Hue,kct),e(A4,Rct),e(A4,mG),e(mG,Sct),e(A4,Pct),e(V,$ct),e(V,L4),e(L4,Uue),e(Uue,Ict),e(L4,jct),e(L4,fG),e(fG,Dct),e(L4,Nct),e(V,qct),e(V,B4),e(B4,Jue),e(Jue,Oct),e(B4,Gct),e(B4,gG),e(gG,Xct),e(B4,Vct),e(To,zct),e(To,Yue),e(Yue,Wct),e(To,Qct),g(h0,To,null),b(d,Xxe,_),b(d,Sc,_),e(Sc,x4),e(x4,Kue),g(u0,Kue,null),e(Sc,Hct),e(Sc,Zue),e(Zue,Uct),b(d,Vxe,_),b(d,Et,_),g(p0,Et,null),e(Et,Jct),e(Et,Pc),e(Pc,Yct),e(Pc,epe),e(epe,Kct),e(Pc,Zct),e(Pc,ope),e(ope,emt),e(Pc,omt),e(Et,tmt),e(Et,_0),e(_0,rmt),e(_0,tpe),e(tpe,amt),e(_0,smt),e(Et,nmt),e(Et,br),g(b0,br,null),e(br,lmt),e(br,rpe),e(rpe,imt),e(br,dmt),e(br,$c),e($c,cmt),e($c,ape),e(ape,mmt),e($c,fmt),e($c,spe),e(spe,gmt),e($c,hmt),e(br,umt),e(br,npe),e(npe,pmt),e(br,_mt),g(v0,br,null),e(Et,bmt),e(Et,Fo),g(T0,Fo,null),e(Fo,vmt),e(Fo,lpe),e(lpe,Tmt),e(Fo,Fmt),e(Fo,Fs),e(Fs,Cmt),e(Fs,ipe),e(ipe,Mmt),e(Fs,Emt),e(Fs,dpe),e(dpe,ymt),e(Fs,wmt),e(Fs,cpe),e(cpe,Amt),e(Fs,Lmt),e(Fo,Bmt),e(Fo,re),e(re,k4),e(k4,mpe),e(mpe,xmt),e(k4,kmt),e(k4,hG),e(hG,Rmt),e(k4,Smt),e(re,Pmt),e(re,R4),e(R4,fpe),e(fpe,$mt),e(R4,Imt),e(R4,uG),e(uG,jmt),e(R4,Dmt),e(re,Nmt),e(re,S4),e(S4,gpe),e(gpe,qmt),e(S4,Omt),e(S4,pG),e(pG,Gmt),e(S4,Xmt),e(re,Vmt),e(re,P4),e(P4,hpe),e(hpe,zmt),e(P4,Wmt),e(P4,_G),e(_G,Qmt),e(P4,Hmt),e(re,Umt),e(re,$4),e($4,upe),e(upe,Jmt),e($4,Ymt),e($4,bG),e(bG,Kmt),e($4,Zmt),e(re,eft),e(re,I4),e(I4,ppe),e(ppe,oft),e(I4,tft),e(I4,vG),e(vG,rft),e(I4,aft),e(re,sft),e(re,j4),e(j4,_pe),e(_pe,nft),e(j4,lft),e(j4,TG),e(TG,ift),e(j4,dft),e(re,cft),e(re,D4),e(D4,bpe),e(bpe,mft),e(D4,fft),e(D4,FG),e(FG,gft),e(D4,hft),e(re,uft),e(re,N4),e(N4,vpe),e(vpe,pft),e(N4,_ft),e(N4,CG),e(CG,bft),e(N4,vft),e(re,Tft),e(re,q4),e(q4,Tpe),e(Tpe,Fft),e(q4,Cft),e(q4,MG),e(MG,Mft),e(q4,Eft),e(re,yft),e(re,O4),e(O4,Fpe),e(Fpe,wft),e(O4,Aft),e(O4,EG),e(EG,Lft),e(O4,Bft),e(re,xft),e(re,G4),e(G4,Cpe),e(Cpe,kft),e(G4,Rft),e(G4,yG),e(yG,Sft),e(G4,Pft),e(re,$ft),e(re,X4),e(X4,Mpe),e(Mpe,Ift),e(X4,jft),e(X4,wG),e(wG,Dft),e(X4,Nft),e(re,qft),e(re,V4),e(V4,Epe),e(Epe,Oft),e(V4,Gft),e(V4,AG),e(AG,Xft),e(V4,Vft),e(re,zft),e(re,z4),e(z4,ype),e(ype,Wft),e(z4,Qft),e(z4,LG),e(LG,Hft),e(z4,Uft),e(re,Jft),e(re,W4),e(W4,wpe),e(wpe,Yft),e(W4,Kft),e(W4,BG),e(BG,Zft),e(W4,egt),e(re,ogt),e(re,Q4),e(Q4,Ape),e(Ape,tgt),e(Q4,rgt),e(Q4,xG),e(xG,agt),e(Q4,sgt),e(Fo,ngt),e(Fo,Lpe),e(Lpe,lgt),e(Fo,igt),g(F0,Fo,null),b(d,zxe,_),b(d,Ic,_),e(Ic,H4),e(H4,Bpe),g(C0,Bpe,null),e(Ic,dgt),e(Ic,xpe),e(xpe,cgt),b(d,Wxe,_),b(d,yt,_),g(M0,yt,null),e(yt,mgt),e(yt,jc),e(jc,fgt),e(jc,kpe),e(kpe,ggt),e(jc,hgt),e(jc,Rpe),e(Rpe,ugt),e(jc,pgt),e(yt,_gt),e(yt,E0),e(E0,bgt),e(E0,Spe),e(Spe,vgt),e(E0,Tgt),e(yt,Fgt),e(yt,vr),g(y0,vr,null),e(vr,Cgt),e(vr,Ppe),e(Ppe,Mgt),e(vr,Egt),e(vr,Dc),e(Dc,ygt),e(Dc,$pe),e($pe,wgt),e(Dc,Agt),e(Dc,Ipe),e(Ipe,Lgt),e(Dc,Bgt),e(vr,xgt),e(vr,jpe),e(jpe,kgt),e(vr,Rgt),g(w0,vr,null),e(yt,Sgt),e(yt,Co),g(A0,Co,null),e(Co,Pgt),e(Co,Dpe),e(Dpe,$gt),e(Co,Igt),e(Co,Cs),e(Cs,jgt),e(Cs,Npe),e(Npe,Dgt),e(Cs,Ngt),e(Cs,qpe),e(qpe,qgt),e(Cs,Ogt),e(Cs,Ope),e(Ope,Ggt),e(Cs,Xgt),e(Co,Vgt),e(Co,Gpe),e(Gpe,U4),e(U4,Xpe),e(Xpe,zgt),e(U4,Wgt),e(U4,kG),e(kG,Qgt),e(U4,Hgt),e(Co,Ugt),e(Co,Vpe),e(Vpe,Jgt),e(Co,Ygt),g(L0,Co,null),b(d,Qxe,_),b(d,Nc,_),e(Nc,J4),e(J4,zpe),g(B0,zpe,null),e(Nc,Kgt),e(Nc,Wpe),e(Wpe,Zgt),b(d,Hxe,_),b(d,wt,_),g(x0,wt,null),e(wt,eht),e(wt,qc),e(qc,oht),e(qc,Qpe),e(Qpe,tht),e(qc,rht),e(qc,Hpe),e(Hpe,aht),e(qc,sht),e(wt,nht),e(wt,k0),e(k0,lht),e(k0,Upe),e(Upe,iht),e(k0,dht),e(wt,cht),e(wt,Tr),g(R0,Tr,null),e(Tr,mht),e(Tr,Jpe),e(Jpe,fht),e(Tr,ght),e(Tr,Oc),e(Oc,hht),e(Oc,Ype),e(Ype,uht),e(Oc,pht),e(Oc,Kpe),e(Kpe,_ht),e(Oc,bht),e(Tr,vht),e(Tr,Zpe),e(Zpe,Tht),e(Tr,Fht),g(S0,Tr,null),e(wt,Cht),e(wt,Mo),g(P0,Mo,null),e(Mo,Mht),e(Mo,e_e),e(e_e,Eht),e(Mo,yht),e(Mo,Ms),e(Ms,wht),e(Ms,o_e),e(o_e,Aht),e(Ms,Lht),e(Ms,t_e),e(t_e,Bht),e(Ms,xht),e(Ms,r_e),e(r_e,kht),e(Ms,Rht),e(Mo,Sht),e(Mo,K),e(K,Y4),e(Y4,a_e),e(a_e,Pht),e(Y4,$ht),e(Y4,RG),e(RG,Iht),e(Y4,jht),e(K,Dht),e(K,K4),e(K4,s_e),e(s_e,Nht),e(K4,qht),e(K4,SG),e(SG,Oht),e(K4,Ght),e(K,Xht),e(K,Z4),e(Z4,n_e),e(n_e,Vht),e(Z4,zht),e(Z4,PG),e(PG,Wht),e(Z4,Qht),e(K,Hht),e(K,eM),e(eM,l_e),e(l_e,Uht),e(eM,Jht),e(eM,$G),e($G,Yht),e(eM,Kht),e(K,Zht),e(K,oM),e(oM,i_e),e(i_e,eut),e(oM,out),e(oM,IG),e(IG,tut),e(oM,rut),e(K,aut),e(K,tM),e(tM,d_e),e(d_e,sut),e(tM,nut),e(tM,jG),e(jG,lut),e(tM,iut),e(K,dut),e(K,rM),e(rM,c_e),e(c_e,cut),e(rM,mut),e(rM,DG),e(DG,fut),e(rM,gut),e(K,hut),e(K,aM),e(aM,m_e),e(m_e,uut),e(aM,put),e(aM,NG),e(NG,_ut),e(aM,but),e(K,vut),e(K,sM),e(sM,f_e),e(f_e,Tut),e(sM,Fut),e(sM,qG),e(qG,Cut),e(sM,Mut),e(K,Eut),e(K,nM),e(nM,g_e),e(g_e,yut),e(nM,wut),e(nM,OG),e(OG,Aut),e(nM,Lut),e(K,But),e(K,lM),e(lM,h_e),e(h_e,xut),e(lM,kut),e(lM,GG),e(GG,Rut),e(lM,Sut),e(K,Put),e(K,iM),e(iM,u_e),e(u_e,$ut),e(iM,Iut),e(iM,XG),e(XG,jut),e(iM,Dut),e(K,Nut),e(K,dM),e(dM,p_e),e(p_e,qut),e(dM,Out),e(dM,VG),e(VG,Gut),e(dM,Xut),e(K,Vut),e(K,cM),e(cM,__e),e(__e,zut),e(cM,Wut),e(cM,zG),e(zG,Qut),e(cM,Hut),e(K,Uut),e(K,mM),e(mM,b_e),e(b_e,Jut),e(mM,Yut),e(mM,WG),e(WG,Kut),e(mM,Zut),e(K,ept),e(K,fM),e(fM,v_e),e(v_e,opt),e(fM,tpt),e(fM,QG),e(QG,rpt),e(fM,apt),e(K,spt),e(K,gM),e(gM,T_e),e(T_e,npt),e(gM,lpt),e(gM,HG),e(HG,ipt),e(gM,dpt),e(K,cpt),e(K,hM),e(hM,F_e),e(F_e,mpt),e(hM,fpt),e(hM,UG),e(UG,gpt),e(hM,hpt),e(K,upt),e(K,uM),e(uM,C_e),e(C_e,ppt),e(uM,_pt),e(uM,JG),e(JG,bpt),e(uM,vpt),e(K,Tpt),e(K,pM),e(pM,M_e),e(M_e,Fpt),e(pM,Cpt),e(pM,YG),e(YG,Mpt),e(pM,Ept),e(Mo,ypt),e(Mo,E_e),e(E_e,wpt),e(Mo,Apt),g($0,Mo,null),b(d,Uxe,_),b(d,Gc,_),e(Gc,_M),e(_M,y_e),g(I0,y_e,null),e(Gc,Lpt),e(Gc,w_e),e(w_e,Bpt),b(d,Jxe,_),b(d,At,_),g(j0,At,null),e(At,xpt),e(At,Xc),e(Xc,kpt),e(Xc,A_e),e(A_e,Rpt),e(Xc,Spt),e(Xc,L_e),e(L_e,Ppt),e(Xc,$pt),e(At,Ipt),e(At,D0),e(D0,jpt),e(D0,B_e),e(B_e,Dpt),e(D0,Npt),e(At,qpt),e(At,Fr),g(N0,Fr,null),e(Fr,Opt),e(Fr,x_e),e(x_e,Gpt),e(Fr,Xpt),e(Fr,Vc),e(Vc,Vpt),e(Vc,k_e),e(k_e,zpt),e(Vc,Wpt),e(Vc,R_e),e(R_e,Qpt),e(Vc,Hpt),e(Fr,Upt),e(Fr,S_e),e(S_e,Jpt),e(Fr,Ypt),g(q0,Fr,null),e(At,Kpt),e(At,Eo),g(O0,Eo,null),e(Eo,Zpt),e(Eo,P_e),e(P_e,e_t),e(Eo,o_t),e(Eo,Es),e(Es,t_t),e(Es,$_e),e($_e,r_t),e(Es,a_t),e(Es,I_e),e(I_e,s_t),e(Es,n_t),e(Es,j_e),e(j_e,l_t),e(Es,i_t),e(Eo,d_t),e(Eo,Z),e(Z,bM),e(bM,D_e),e(D_e,c_t),e(bM,m_t),e(bM,KG),e(KG,f_t),e(bM,g_t),e(Z,h_t),e(Z,vM),e(vM,N_e),e(N_e,u_t),e(vM,p_t),e(vM,ZG),e(ZG,__t),e(vM,b_t),e(Z,v_t),e(Z,TM),e(TM,q_e),e(q_e,T_t),e(TM,F_t),e(TM,eX),e(eX,C_t),e(TM,M_t),e(Z,E_t),e(Z,FM),e(FM,O_e),e(O_e,y_t),e(FM,w_t),e(FM,oX),e(oX,A_t),e(FM,L_t),e(Z,B_t),e(Z,CM),e(CM,G_e),e(G_e,x_t),e(CM,k_t),e(CM,tX),e(tX,R_t),e(CM,S_t),e(Z,P_t),e(Z,MM),e(MM,X_e),e(X_e,$_t),e(MM,I_t),e(MM,rX),e(rX,j_t),e(MM,D_t),e(Z,N_t),e(Z,EM),e(EM,V_e),e(V_e,q_t),e(EM,O_t),e(EM,aX),e(aX,G_t),e(EM,X_t),e(Z,V_t),e(Z,yM),e(yM,z_e),e(z_e,z_t),e(yM,W_t),e(yM,sX),e(sX,Q_t),e(yM,H_t),e(Z,U_t),e(Z,wM),e(wM,W_e),e(W_e,J_t),e(wM,Y_t),e(wM,nX),e(nX,K_t),e(wM,Z_t),e(Z,ebt),e(Z,AM),e(AM,Q_e),e(Q_e,obt),e(AM,tbt),e(AM,lX),e(lX,rbt),e(AM,abt),e(Z,sbt),e(Z,LM),e(LM,H_e),e(H_e,nbt),e(LM,lbt),e(LM,iX),e(iX,ibt),e(LM,dbt),e(Z,cbt),e(Z,BM),e(BM,U_e),e(U_e,mbt),e(BM,fbt),e(BM,dX),e(dX,gbt),e(BM,hbt),e(Z,ubt),e(Z,xM),e(xM,J_e),e(J_e,pbt),e(xM,_bt),e(xM,cX),e(cX,bbt),e(xM,vbt),e(Z,Tbt),e(Z,kM),e(kM,Y_e),e(Y_e,Fbt),e(kM,Cbt),e(kM,mX),e(mX,Mbt),e(kM,Ebt),e(Z,ybt),e(Z,RM),e(RM,K_e),e(K_e,wbt),e(RM,Abt),e(RM,fX),e(fX,Lbt),e(RM,Bbt),e(Z,xbt),e(Z,SM),e(SM,Z_e),e(Z_e,kbt),e(SM,Rbt),e(SM,gX),e(gX,Sbt),e(SM,Pbt),e(Z,$bt),e(Z,PM),e(PM,ebe),e(ebe,Ibt),e(PM,jbt),e(PM,hX),e(hX,Dbt),e(PM,Nbt),e(Z,qbt),e(Z,$M),e($M,obe),e(obe,Obt),e($M,Gbt),e($M,uX),e(uX,Xbt),e($M,Vbt),e(Z,zbt),e(Z,IM),e(IM,tbe),e(tbe,Wbt),e(IM,Qbt),e(IM,pX),e(pX,Hbt),e(IM,Ubt),e(Eo,Jbt),e(Eo,rbe),e(rbe,Ybt),e(Eo,Kbt),g(G0,Eo,null),b(d,Yxe,_),b(d,zc,_),e(zc,jM),e(jM,abe),g(X0,abe,null),e(zc,Zbt),e(zc,sbe),e(sbe,e2t),b(d,Kxe,_),b(d,Lt,_),g(V0,Lt,null),e(Lt,o2t),e(Lt,Wc),e(Wc,t2t),e(Wc,nbe),e(nbe,r2t),e(Wc,a2t),e(Wc,lbe),e(lbe,s2t),e(Wc,n2t),e(Lt,l2t),e(Lt,z0),e(z0,i2t),e(z0,ibe),e(ibe,d2t),e(z0,c2t),e(Lt,m2t),e(Lt,Cr),g(W0,Cr,null),e(Cr,f2t),e(Cr,dbe),e(dbe,g2t),e(Cr,h2t),e(Cr,Qc),e(Qc,u2t),e(Qc,cbe),e(cbe,p2t),e(Qc,_2t),e(Qc,mbe),e(mbe,b2t),e(Qc,v2t),e(Cr,T2t),e(Cr,fbe),e(fbe,F2t),e(Cr,C2t),g(Q0,Cr,null),e(Lt,M2t),e(Lt,yo),g(H0,yo,null),e(yo,E2t),e(yo,gbe),e(gbe,y2t),e(yo,w2t),e(yo,ys),e(ys,A2t),e(ys,hbe),e(hbe,L2t),e(ys,B2t),e(ys,ube),e(ube,x2t),e(ys,k2t),e(ys,pbe),e(pbe,R2t),e(ys,S2t),e(yo,P2t),e(yo,_be),e(_be,DM),e(DM,bbe),e(bbe,$2t),e(DM,I2t),e(DM,_X),e(_X,j2t),e(DM,D2t),e(yo,N2t),e(yo,vbe),e(vbe,q2t),e(yo,O2t),g(U0,yo,null),b(d,Zxe,_),b(d,Hc,_),e(Hc,NM),e(NM,Tbe),g(J0,Tbe,null),e(Hc,G2t),e(Hc,Fbe),e(Fbe,X2t),b(d,eke,_),b(d,Bt,_),g(Y0,Bt,null),e(Bt,V2t),e(Bt,Uc),e(Uc,z2t),e(Uc,Cbe),e(Cbe,W2t),e(Uc,Q2t),e(Uc,Mbe),e(Mbe,H2t),e(Uc,U2t),e(Bt,J2t),e(Bt,K0),e(K0,Y2t),e(K0,Ebe),e(Ebe,K2t),e(K0,Z2t),e(Bt,evt),e(Bt,Mr),g(Z0,Mr,null),e(Mr,ovt),e(Mr,ybe),e(ybe,tvt),e(Mr,rvt),e(Mr,Jc),e(Jc,avt),e(Jc,wbe),e(wbe,svt),e(Jc,nvt),e(Jc,Abe),e(Abe,lvt),e(Jc,ivt),e(Mr,dvt),e(Mr,Lbe),e(Lbe,cvt),e(Mr,mvt),g(eL,Mr,null),e(Bt,fvt),e(Bt,wo),g(oL,wo,null),e(wo,gvt),e(wo,Bbe),e(Bbe,hvt),e(wo,uvt),e(wo,ws),e(ws,pvt),e(ws,xbe),e(xbe,_vt),e(ws,bvt),e(ws,kbe),e(kbe,vvt),e(ws,Tvt),e(ws,Rbe),e(Rbe,Fvt),e(ws,Cvt),e(wo,Mvt),e(wo,Sbe),e(Sbe,qM),e(qM,Pbe),e(Pbe,Evt),e(qM,yvt),e(qM,bX),e(bX,wvt),e(qM,Avt),e(wo,Lvt),e(wo,$be),e($be,Bvt),e(wo,xvt),g(tL,wo,null),b(d,oke,_),b(d,Yc,_),e(Yc,OM),e(OM,Ibe),g(rL,Ibe,null),e(Yc,kvt),e(Yc,jbe),e(jbe,Rvt),b(d,tke,_),b(d,xt,_),g(aL,xt,null),e(xt,Svt),e(xt,Kc),e(Kc,Pvt),e(Kc,Dbe),e(Dbe,$vt),e(Kc,Ivt),e(Kc,Nbe),e(Nbe,jvt),e(Kc,Dvt),e(xt,Nvt),e(xt,sL),e(sL,qvt),e(sL,qbe),e(qbe,Ovt),e(sL,Gvt),e(xt,Xvt),e(xt,Er),g(nL,Er,null),e(Er,Vvt),e(Er,Obe),e(Obe,zvt),e(Er,Wvt),e(Er,Zc),e(Zc,Qvt),e(Zc,Gbe),e(Gbe,Hvt),e(Zc,Uvt),e(Zc,Xbe),e(Xbe,Jvt),e(Zc,Yvt),e(Er,Kvt),e(Er,Vbe),e(Vbe,Zvt),e(Er,eTt),g(lL,Er,null),e(xt,oTt),e(xt,Ao),g(iL,Ao,null),e(Ao,tTt),e(Ao,zbe),e(zbe,rTt),e(Ao,aTt),e(Ao,As),e(As,sTt),e(As,Wbe),e(Wbe,nTt),e(As,lTt),e(As,Qbe),e(Qbe,iTt),e(As,dTt),e(As,Hbe),e(Hbe,cTt),e(As,mTt),e(Ao,fTt),e(Ao,z),e(z,GM),e(GM,Ube),e(Ube,gTt),e(GM,hTt),e(GM,vX),e(vX,uTt),e(GM,pTt),e(z,_Tt),e(z,XM),e(XM,Jbe),e(Jbe,bTt),e(XM,vTt),e(XM,TX),e(TX,TTt),e(XM,FTt),e(z,CTt),e(z,VM),e(VM,Ybe),e(Ybe,MTt),e(VM,ETt),e(VM,FX),e(FX,yTt),e(VM,wTt),e(z,ATt),e(z,zM),e(zM,Kbe),e(Kbe,LTt),e(zM,BTt),e(zM,CX),e(CX,xTt),e(zM,kTt),e(z,RTt),e(z,WM),e(WM,Zbe),e(Zbe,STt),e(WM,PTt),e(WM,MX),e(MX,$Tt),e(WM,ITt),e(z,jTt),e(z,QM),e(QM,e2e),e(e2e,DTt),e(QM,NTt),e(QM,EX),e(EX,qTt),e(QM,OTt),e(z,GTt),e(z,HM),e(HM,o2e),e(o2e,XTt),e(HM,VTt),e(HM,yX),e(yX,zTt),e(HM,WTt),e(z,QTt),e(z,UM),e(UM,t2e),e(t2e,HTt),e(UM,UTt),e(UM,wX),e(wX,JTt),e(UM,YTt),e(z,KTt),e(z,JM),e(JM,r2e),e(r2e,ZTt),e(JM,e1t),e(JM,AX),e(AX,o1t),e(JM,t1t),e(z,r1t),e(z,YM),e(YM,a2e),e(a2e,a1t),e(YM,s1t),e(YM,LX),e(LX,n1t),e(YM,l1t),e(z,i1t),e(z,KM),e(KM,s2e),e(s2e,d1t),e(KM,c1t),e(KM,BX),e(BX,m1t),e(KM,f1t),e(z,g1t),e(z,ZM),e(ZM,n2e),e(n2e,h1t),e(ZM,u1t),e(ZM,xX),e(xX,p1t),e(ZM,_1t),e(z,b1t),e(z,eE),e(eE,l2e),e(l2e,v1t),e(eE,T1t),e(eE,kX),e(kX,F1t),e(eE,C1t),e(z,M1t),e(z,oE),e(oE,i2e),e(i2e,E1t),e(oE,y1t),e(oE,RX),e(RX,w1t),e(oE,A1t),e(z,L1t),e(z,tE),e(tE,d2e),e(d2e,B1t),e(tE,x1t),e(tE,SX),e(SX,k1t),e(tE,R1t),e(z,S1t),e(z,rE),e(rE,c2e),e(c2e,P1t),e(rE,$1t),e(rE,PX),e(PX,I1t),e(rE,j1t),e(z,D1t),e(z,aE),e(aE,m2e),e(m2e,N1t),e(aE,q1t),e(aE,$X),e($X,O1t),e(aE,G1t),e(z,X1t),e(z,sE),e(sE,f2e),e(f2e,V1t),e(sE,z1t),e(sE,IX),e(IX,W1t),e(sE,Q1t),e(z,H1t),e(z,nE),e(nE,g2e),e(g2e,U1t),e(nE,J1t),e(nE,jX),e(jX,Y1t),e(nE,K1t),e(z,Z1t),e(z,lE),e(lE,h2e),e(h2e,eFt),e(lE,oFt),e(lE,DX),e(DX,tFt),e(lE,rFt),e(z,aFt),e(z,iE),e(iE,u2e),e(u2e,sFt),e(iE,nFt),e(iE,NX),e(NX,lFt),e(iE,iFt),e(z,dFt),e(z,dE),e(dE,p2e),e(p2e,cFt),e(dE,mFt),e(dE,qX),e(qX,fFt),e(dE,gFt),e(z,hFt),e(z,cE),e(cE,_2e),e(_2e,uFt),e(cE,pFt),e(cE,OX),e(OX,_Ft),e(cE,bFt),e(z,vFt),e(z,mE),e(mE,b2e),e(b2e,TFt),e(mE,FFt),e(mE,GX),e(GX,CFt),e(mE,MFt),e(Ao,EFt),e(Ao,v2e),e(v2e,yFt),e(Ao,wFt),g(dL,Ao,null),b(d,rke,_),b(d,em,_),e(em,fE),e(fE,T2e),g(cL,T2e,null),e(em,AFt),e(em,F2e),e(F2e,LFt),b(d,ake,_),b(d,kt,_),g(mL,kt,null),e(kt,BFt),e(kt,om),e(om,xFt),e(om,C2e),e(C2e,kFt),e(om,RFt),e(om,M2e),e(M2e,SFt),e(om,PFt),e(kt,$Ft),e(kt,fL),e(fL,IFt),e(fL,E2e),e(E2e,jFt),e(fL,DFt),e(kt,NFt),e(kt,yr),g(gL,yr,null),e(yr,qFt),e(yr,y2e),e(y2e,OFt),e(yr,GFt),e(yr,tm),e(tm,XFt),e(tm,w2e),e(w2e,VFt),e(tm,zFt),e(tm,A2e),e(A2e,WFt),e(tm,QFt),e(yr,HFt),e(yr,L2e),e(L2e,UFt),e(yr,JFt),g(hL,yr,null),e(kt,YFt),e(kt,Lo),g(uL,Lo,null),e(Lo,KFt),e(Lo,B2e),e(B2e,ZFt),e(Lo,eCt),e(Lo,Ls),e(Ls,oCt),e(Ls,x2e),e(x2e,tCt),e(Ls,rCt),e(Ls,k2e),e(k2e,aCt),e(Ls,sCt),e(Ls,R2e),e(R2e,nCt),e(Ls,lCt),e(Lo,iCt),e(Lo,Bs),e(Bs,gE),e(gE,S2e),e(S2e,dCt),e(gE,cCt),e(gE,XX),e(XX,mCt),e(gE,fCt),e(Bs,gCt),e(Bs,hE),e(hE,P2e),e(P2e,hCt),e(hE,uCt),e(hE,VX),e(VX,pCt),e(hE,_Ct),e(Bs,bCt),e(Bs,uE),e(uE,$2e),e($2e,vCt),e(uE,TCt),e(uE,zX),e(zX,FCt),e(uE,CCt),e(Bs,MCt),e(Bs,pE),e(pE,I2e),e(I2e,ECt),e(pE,yCt),e(pE,WX),e(WX,wCt),e(pE,ACt),e(Lo,LCt),e(Lo,j2e),e(j2e,BCt),e(Lo,xCt),g(pL,Lo,null),b(d,ske,_),b(d,rm,_),e(rm,_E),e(_E,D2e),g(_L,D2e,null),e(rm,kCt),e(rm,N2e),e(N2e,RCt),b(d,nke,_),b(d,Rt,_),g(bL,Rt,null),e(Rt,SCt),e(Rt,am),e(am,PCt),e(am,q2e),e(q2e,$Ct),e(am,ICt),e(am,O2e),e(O2e,jCt),e(am,DCt),e(Rt,NCt),e(Rt,vL),e(vL,qCt),e(vL,G2e),e(G2e,OCt),e(vL,GCt),e(Rt,XCt),e(Rt,wr),g(TL,wr,null),e(wr,VCt),e(wr,X2e),e(X2e,zCt),e(wr,WCt),e(wr,sm),e(sm,QCt),e(sm,V2e),e(V2e,HCt),e(sm,UCt),e(sm,z2e),e(z2e,JCt),e(sm,YCt),e(wr,KCt),e(wr,W2e),e(W2e,ZCt),e(wr,e4t),g(FL,wr,null),e(Rt,o4t),e(Rt,Bo),g(CL,Bo,null),e(Bo,t4t),e(Bo,Q2e),e(Q2e,r4t),e(Bo,a4t),e(Bo,xs),e(xs,s4t),e(xs,H2e),e(H2e,n4t),e(xs,l4t),e(xs,U2e),e(U2e,i4t),e(xs,d4t),e(xs,J2e),e(J2e,c4t),e(xs,m4t),e(Bo,f4t),e(Bo,me),e(me,bE),e(bE,Y2e),e(Y2e,g4t),e(bE,h4t),e(bE,QX),e(QX,u4t),e(bE,p4t),e(me,_4t),e(me,vE),e(vE,K2e),e(K2e,b4t),e(vE,v4t),e(vE,HX),e(HX,T4t),e(vE,F4t),e(me,C4t),e(me,TE),e(TE,Z2e),e(Z2e,M4t),e(TE,E4t),e(TE,UX),e(UX,y4t),e(TE,w4t),e(me,A4t),e(me,FE),e(FE,eve),e(eve,L4t),e(FE,B4t),e(FE,JX),e(JX,x4t),e(FE,k4t),e(me,R4t),e(me,CE),e(CE,ove),e(ove,S4t),e(CE,P4t),e(CE,YX),e(YX,$4t),e(CE,I4t),e(me,j4t),e(me,ME),e(ME,tve),e(tve,D4t),e(ME,N4t),e(ME,KX),e(KX,q4t),e(ME,O4t),e(me,G4t),e(me,EE),e(EE,rve),e(rve,X4t),e(EE,V4t),e(EE,ZX),e(ZX,z4t),e(EE,W4t),e(me,Q4t),e(me,yE),e(yE,ave),e(ave,H4t),e(yE,U4t),e(yE,eV),e(eV,J4t),e(yE,Y4t),e(me,K4t),e(me,wE),e(wE,sve),e(sve,Z4t),e(wE,eMt),e(wE,oV),e(oV,oMt),e(wE,tMt),e(me,rMt),e(me,AE),e(AE,nve),e(nve,aMt),e(AE,sMt),e(AE,tV),e(tV,nMt),e(AE,lMt),e(me,iMt),e(me,LE),e(LE,lve),e(lve,dMt),e(LE,cMt),e(LE,rV),e(rV,mMt),e(LE,fMt),e(Bo,gMt),e(Bo,ive),e(ive,hMt),e(Bo,uMt),g(ML,Bo,null),b(d,lke,_),b(d,nm,_),e(nm,BE),e(BE,dve),g(EL,dve,null),e(nm,pMt),e(nm,cve),e(cve,_Mt),b(d,ike,_),b(d,St,_),g(yL,St,null),e(St,bMt),e(St,lm),e(lm,vMt),e(lm,mve),e(mve,TMt),e(lm,FMt),e(lm,fve),e(fve,CMt),e(lm,MMt),e(St,EMt),e(St,wL),e(wL,yMt),e(wL,gve),e(gve,wMt),e(wL,AMt),e(St,LMt),e(St,Ar),g(AL,Ar,null),e(Ar,BMt),e(Ar,hve),e(hve,xMt),e(Ar,kMt),e(Ar,im),e(im,RMt),e(im,uve),e(uve,SMt),e(im,PMt),e(im,pve),e(pve,$Mt),e(im,IMt),e(Ar,jMt),e(Ar,_ve),e(_ve,DMt),e(Ar,NMt),g(LL,Ar,null),e(St,qMt),e(St,xo),g(BL,xo,null),e(xo,OMt),e(xo,bve),e(bve,GMt),e(xo,XMt),e(xo,ks),e(ks,VMt),e(ks,vve),e(vve,zMt),e(ks,WMt),e(ks,Tve),e(Tve,QMt),e(ks,HMt),e(ks,Fve),e(Fve,UMt),e(ks,JMt),e(xo,YMt),e(xo,ve),e(ve,xE),e(xE,Cve),e(Cve,KMt),e(xE,ZMt),e(xE,aV),e(aV,eEt),e(xE,oEt),e(ve,tEt),e(ve,kE),e(kE,Mve),e(Mve,rEt),e(kE,aEt),e(kE,sV),e(sV,sEt),e(kE,nEt),e(ve,lEt),e(ve,RE),e(RE,Eve),e(Eve,iEt),e(RE,dEt),e(RE,nV),e(nV,cEt),e(RE,mEt),e(ve,fEt),e(ve,SE),e(SE,yve),e(yve,gEt),e(SE,hEt),e(SE,lV),e(lV,uEt),e(SE,pEt),e(ve,_Et),e(ve,PE),e(PE,wve),e(wve,bEt),e(PE,vEt),e(PE,iV),e(iV,TEt),e(PE,FEt),e(ve,CEt),e(ve,$E),e($E,Ave),e(Ave,MEt),e($E,EEt),e($E,dV),e(dV,yEt),e($E,wEt),e(ve,AEt),e(ve,IE),e(IE,Lve),e(Lve,LEt),e(IE,BEt),e(IE,cV),e(cV,xEt),e(IE,kEt),e(ve,REt),e(ve,jE),e(jE,Bve),e(Bve,SEt),e(jE,PEt),e(jE,mV),e(mV,$Et),e(jE,IEt),e(ve,jEt),e(ve,DE),e(DE,xve),e(xve,DEt),e(DE,NEt),e(DE,fV),e(fV,qEt),e(DE,OEt),e(xo,GEt),e(xo,kve),e(kve,XEt),e(xo,VEt),g(xL,xo,null),b(d,dke,_),b(d,dm,_),e(dm,NE),e(NE,Rve),g(kL,Rve,null),e(dm,zEt),e(dm,Sve),e(Sve,WEt),b(d,cke,_),b(d,Pt,_),g(RL,Pt,null),e(Pt,QEt),e(Pt,cm),e(cm,HEt),e(cm,Pve),e(Pve,UEt),e(cm,JEt),e(cm,$ve),e($ve,YEt),e(cm,KEt),e(Pt,ZEt),e(Pt,SL),e(SL,e3t),e(SL,Ive),e(Ive,o3t),e(SL,t3t),e(Pt,r3t),e(Pt,Lr),g(PL,Lr,null),e(Lr,a3t),e(Lr,jve),e(jve,s3t),e(Lr,n3t),e(Lr,mm),e(mm,l3t),e(mm,Dve),e(Dve,i3t),e(mm,d3t),e(mm,Nve),e(Nve,c3t),e(mm,m3t),e(Lr,f3t),e(Lr,qve),e(qve,g3t),e(Lr,h3t),g($L,Lr,null),e(Pt,u3t),e(Pt,ko),g(IL,ko,null),e(ko,p3t),e(ko,Ove),e(Ove,_3t),e(ko,b3t),e(ko,Rs),e(Rs,v3t),e(Rs,Gve),e(Gve,T3t),e(Rs,F3t),e(Rs,Xve),e(Xve,C3t),e(Rs,M3t),e(Rs,Vve),e(Vve,E3t),e(Rs,y3t),e(ko,w3t),e(ko,Te),e(Te,qE),e(qE,zve),e(zve,A3t),e(qE,L3t),e(qE,gV),e(gV,B3t),e(qE,x3t),e(Te,k3t),e(Te,OE),e(OE,Wve),e(Wve,R3t),e(OE,S3t),e(OE,hV),e(hV,P3t),e(OE,$3t),e(Te,I3t),e(Te,GE),e(GE,Qve),e(Qve,j3t),e(GE,D3t),e(GE,uV),e(uV,N3t),e(GE,q3t),e(Te,O3t),e(Te,XE),e(XE,Hve),e(Hve,G3t),e(XE,X3t),e(XE,pV),e(pV,V3t),e(XE,z3t),e(Te,W3t),e(Te,VE),e(VE,Uve),e(Uve,Q3t),e(VE,H3t),e(VE,_V),e(_V,U3t),e(VE,J3t),e(Te,Y3t),e(Te,zE),e(zE,Jve),e(Jve,K3t),e(zE,Z3t),e(zE,bV),e(bV,e5t),e(zE,o5t),e(Te,t5t),e(Te,WE),e(WE,Yve),e(Yve,r5t),e(WE,a5t),e(WE,vV),e(vV,s5t),e(WE,n5t),e(Te,l5t),e(Te,QE),e(QE,Kve),e(Kve,i5t),e(QE,d5t),e(QE,TV),e(TV,c5t),e(QE,m5t),e(Te,f5t),e(Te,HE),e(HE,Zve),e(Zve,g5t),e(HE,h5t),e(HE,FV),e(FV,u5t),e(HE,p5t),e(ko,_5t),e(ko,eTe),e(eTe,b5t),e(ko,v5t),g(jL,ko,null),b(d,mke,_),b(d,fm,_),e(fm,UE),e(UE,oTe),g(DL,oTe,null),e(fm,T5t),e(fm,tTe),e(tTe,F5t),b(d,fke,_),b(d,$t,_),g(NL,$t,null),e($t,C5t),e($t,gm),e(gm,M5t),e(gm,rTe),e(rTe,E5t),e(gm,y5t),e(gm,aTe),e(aTe,w5t),e(gm,A5t),e($t,L5t),e($t,qL),e(qL,B5t),e(qL,sTe),e(sTe,x5t),e(qL,k5t),e($t,R5t),e($t,Br),g(OL,Br,null),e(Br,S5t),e(Br,nTe),e(nTe,P5t),e(Br,$5t),e(Br,hm),e(hm,I5t),e(hm,lTe),e(lTe,j5t),e(hm,D5t),e(hm,iTe),e(iTe,N5t),e(hm,q5t),e(Br,O5t),e(Br,dTe),e(dTe,G5t),e(Br,X5t),g(GL,Br,null),e($t,V5t),e($t,Ro),g(XL,Ro,null),e(Ro,z5t),e(Ro,cTe),e(cTe,W5t),e(Ro,Q5t),e(Ro,Ss),e(Ss,H5t),e(Ss,mTe),e(mTe,U5t),e(Ss,J5t),e(Ss,fTe),e(fTe,Y5t),e(Ss,K5t),e(Ss,gTe),e(gTe,Z5t),e(Ss,eyt),e(Ro,oyt),e(Ro,Fe),e(Fe,JE),e(JE,hTe),e(hTe,tyt),e(JE,ryt),e(JE,CV),e(CV,ayt),e(JE,syt),e(Fe,nyt),e(Fe,YE),e(YE,uTe),e(uTe,lyt),e(YE,iyt),e(YE,MV),e(MV,dyt),e(YE,cyt),e(Fe,myt),e(Fe,KE),e(KE,pTe),e(pTe,fyt),e(KE,gyt),e(KE,EV),e(EV,hyt),e(KE,uyt),e(Fe,pyt),e(Fe,ZE),e(ZE,_Te),e(_Te,_yt),e(ZE,byt),e(ZE,yV),e(yV,vyt),e(ZE,Tyt),e(Fe,Fyt),e(Fe,e3),e(e3,bTe),e(bTe,Cyt),e(e3,Myt),e(e3,wV),e(wV,Eyt),e(e3,yyt),e(Fe,wyt),e(Fe,o3),e(o3,vTe),e(vTe,Ayt),e(o3,Lyt),e(o3,AV),e(AV,Byt),e(o3,xyt),e(Fe,kyt),e(Fe,t3),e(t3,TTe),e(TTe,Ryt),e(t3,Syt),e(t3,LV),e(LV,Pyt),e(t3,$yt),e(Fe,Iyt),e(Fe,r3),e(r3,FTe),e(FTe,jyt),e(r3,Dyt),e(r3,BV),e(BV,Nyt),e(r3,qyt),e(Fe,Oyt),e(Fe,a3),e(a3,CTe),e(CTe,Gyt),e(a3,Xyt),e(a3,xV),e(xV,Vyt),e(a3,zyt),e(Ro,Wyt),e(Ro,MTe),e(MTe,Qyt),e(Ro,Hyt),g(VL,Ro,null),b(d,gke,_),b(d,um,_),e(um,s3),e(s3,ETe),g(zL,ETe,null),e(um,Uyt),e(um,yTe),e(yTe,Jyt),b(d,hke,_),b(d,It,_),g(WL,It,null),e(It,Yyt),e(It,pm),e(pm,Kyt),e(pm,wTe),e(wTe,Zyt),e(pm,ewt),e(pm,ATe),e(ATe,owt),e(pm,twt),e(It,rwt),e(It,QL),e(QL,awt),e(QL,LTe),e(LTe,swt),e(QL,nwt),e(It,lwt),e(It,xr),g(HL,xr,null),e(xr,iwt),e(xr,BTe),e(BTe,dwt),e(xr,cwt),e(xr,_m),e(_m,mwt),e(_m,xTe),e(xTe,fwt),e(_m,gwt),e(_m,kTe),e(kTe,hwt),e(_m,uwt),e(xr,pwt),e(xr,RTe),e(RTe,_wt),e(xr,bwt),g(UL,xr,null),e(It,vwt),e(It,So),g(JL,So,null),e(So,Twt),e(So,STe),e(STe,Fwt),e(So,Cwt),e(So,Ps),e(Ps,Mwt),e(Ps,PTe),e(PTe,Ewt),e(Ps,ywt),e(Ps,$Te),e($Te,wwt),e(Ps,Awt),e(Ps,ITe),e(ITe,Lwt),e(Ps,Bwt),e(So,xwt),e(So,Ce),e(Ce,n3),e(n3,jTe),e(jTe,kwt),e(n3,Rwt),e(n3,kV),e(kV,Swt),e(n3,Pwt),e(Ce,$wt),e(Ce,l3),e(l3,DTe),e(DTe,Iwt),e(l3,jwt),e(l3,RV),e(RV,Dwt),e(l3,Nwt),e(Ce,qwt),e(Ce,i3),e(i3,NTe),e(NTe,Owt),e(i3,Gwt),e(i3,SV),e(SV,Xwt),e(i3,Vwt),e(Ce,zwt),e(Ce,d3),e(d3,qTe),e(qTe,Wwt),e(d3,Qwt),e(d3,PV),e(PV,Hwt),e(d3,Uwt),e(Ce,Jwt),e(Ce,c3),e(c3,OTe),e(OTe,Ywt),e(c3,Kwt),e(c3,$V),e($V,Zwt),e(c3,e6t),e(Ce,o6t),e(Ce,m3),e(m3,GTe),e(GTe,t6t),e(m3,r6t),e(m3,IV),e(IV,a6t),e(m3,s6t),e(Ce,n6t),e(Ce,f3),e(f3,XTe),e(XTe,l6t),e(f3,i6t),e(f3,jV),e(jV,d6t),e(f3,c6t),e(Ce,m6t),e(Ce,g3),e(g3,VTe),e(VTe,f6t),e(g3,g6t),e(g3,DV),e(DV,h6t),e(g3,u6t),e(Ce,p6t),e(Ce,h3),e(h3,zTe),e(zTe,_6t),e(h3,b6t),e(h3,NV),e(NV,v6t),e(h3,T6t),e(So,F6t),e(So,WTe),e(WTe,C6t),e(So,M6t),g(YL,So,null),b(d,uke,_),b(d,bm,_),e(bm,u3),e(u3,QTe),g(KL,QTe,null),e(bm,E6t),e(bm,HTe),e(HTe,y6t),b(d,pke,_),b(d,jt,_),g(ZL,jt,null),e(jt,w6t),e(jt,vm),e(vm,A6t),e(vm,UTe),e(UTe,L6t),e(vm,B6t),e(vm,JTe),e(JTe,x6t),e(vm,k6t),e(jt,R6t),e(jt,e8),e(e8,S6t),e(e8,YTe),e(YTe,P6t),e(e8,$6t),e(jt,I6t),e(jt,kr),g(o8,kr,null),e(kr,j6t),e(kr,KTe),e(KTe,D6t),e(kr,N6t),e(kr,Tm),e(Tm,q6t),e(Tm,ZTe),e(ZTe,O6t),e(Tm,G6t),e(Tm,e1e),e(e1e,X6t),e(Tm,V6t),e(kr,z6t),e(kr,o1e),e(o1e,W6t),e(kr,Q6t),g(t8,kr,null),e(jt,H6t),e(jt,Po),g(r8,Po,null),e(Po,U6t),e(Po,t1e),e(t1e,J6t),e(Po,Y6t),e(Po,$s),e($s,K6t),e($s,r1e),e(r1e,Z6t),e($s,eAt),e($s,a1e),e(a1e,oAt),e($s,tAt),e($s,s1e),e(s1e,rAt),e($s,aAt),e(Po,sAt),e(Po,lo),e(lo,p3),e(p3,n1e),e(n1e,nAt),e(p3,lAt),e(p3,qV),e(qV,iAt),e(p3,dAt),e(lo,cAt),e(lo,_3),e(_3,l1e),e(l1e,mAt),e(_3,fAt),e(_3,OV),e(OV,gAt),e(_3,hAt),e(lo,uAt),e(lo,b3),e(b3,i1e),e(i1e,pAt),e(b3,_At),e(b3,GV),e(GV,bAt),e(b3,vAt),e(lo,TAt),e(lo,v3),e(v3,d1e),e(d1e,FAt),e(v3,CAt),e(v3,XV),e(XV,MAt),e(v3,EAt),e(lo,yAt),e(lo,T3),e(T3,c1e),e(c1e,wAt),e(T3,AAt),e(T3,VV),e(VV,LAt),e(T3,BAt),e(lo,xAt),e(lo,F3),e(F3,m1e),e(m1e,kAt),e(F3,RAt),e(F3,zV),e(zV,SAt),e(F3,PAt),e(lo,$At),e(lo,C3),e(C3,f1e),e(f1e,IAt),e(C3,jAt),e(C3,WV),e(WV,DAt),e(C3,NAt),e(Po,qAt),e(Po,g1e),e(g1e,OAt),e(Po,GAt),g(a8,Po,null),b(d,_ke,_),b(d,Fm,_),e(Fm,M3),e(M3,h1e),g(s8,h1e,null),e(Fm,XAt),e(Fm,u1e),e(u1e,VAt),b(d,bke,_),b(d,Dt,_),g(n8,Dt,null),e(Dt,zAt),e(Dt,Cm),e(Cm,WAt),e(Cm,p1e),e(p1e,QAt),e(Cm,HAt),e(Cm,_1e),e(_1e,UAt),e(Cm,JAt),e(Dt,YAt),e(Dt,l8),e(l8,KAt),e(l8,b1e),e(b1e,ZAt),e(l8,e0t),e(Dt,o0t),e(Dt,Rr),g(i8,Rr,null),e(Rr,t0t),e(Rr,v1e),e(v1e,r0t),e(Rr,a0t),e(Rr,Mm),e(Mm,s0t),e(Mm,T1e),e(T1e,n0t),e(Mm,l0t),e(Mm,F1e),e(F1e,i0t),e(Mm,d0t),e(Rr,c0t),e(Rr,C1e),e(C1e,m0t),e(Rr,f0t),g(d8,Rr,null),e(Dt,g0t),e(Dt,$o),g(c8,$o,null),e($o,h0t),e($o,M1e),e(M1e,u0t),e($o,p0t),e($o,Is),e(Is,_0t),e(Is,E1e),e(E1e,b0t),e(Is,v0t),e(Is,y1e),e(y1e,T0t),e(Is,F0t),e(Is,w1e),e(w1e,C0t),e(Is,M0t),e($o,E0t),e($o,io),e(io,E3),e(E3,A1e),e(A1e,y0t),e(E3,w0t),e(E3,QV),e(QV,A0t),e(E3,L0t),e(io,B0t),e(io,y3),e(y3,L1e),e(L1e,x0t),e(y3,k0t),e(y3,HV),e(HV,R0t),e(y3,S0t),e(io,P0t),e(io,w3),e(w3,B1e),e(B1e,$0t),e(w3,I0t),e(w3,UV),e(UV,j0t),e(w3,D0t),e(io,N0t),e(io,A3),e(A3,x1e),e(x1e,q0t),e(A3,O0t),e(A3,JV),e(JV,G0t),e(A3,X0t),e(io,V0t),e(io,L3),e(L3,k1e),e(k1e,z0t),e(L3,W0t),e(L3,YV),e(YV,Q0t),e(L3,H0t),e(io,U0t),e(io,B3),e(B3,R1e),e(R1e,J0t),e(B3,Y0t),e(B3,KV),e(KV,K0t),e(B3,Z0t),e(io,eLt),e(io,x3),e(x3,S1e),e(S1e,oLt),e(x3,tLt),e(x3,ZV),e(ZV,rLt),e(x3,aLt),e($o,sLt),e($o,P1e),e(P1e,nLt),e($o,lLt),g(m8,$o,null),b(d,vke,_),b(d,Em,_),e(Em,k3),e(k3,$1e),g(f8,$1e,null),e(Em,iLt),e(Em,I1e),e(I1e,dLt),b(d,Tke,_),b(d,Nt,_),g(g8,Nt,null),e(Nt,cLt),e(Nt,ym),e(ym,mLt),e(ym,j1e),e(j1e,fLt),e(ym,gLt),e(ym,D1e),e(D1e,hLt),e(ym,uLt),e(Nt,pLt),e(Nt,h8),e(h8,_Lt),e(h8,N1e),e(N1e,bLt),e(h8,vLt),e(Nt,TLt),e(Nt,Sr),g(u8,Sr,null),e(Sr,FLt),e(Sr,q1e),e(q1e,CLt),e(Sr,MLt),e(Sr,wm),e(wm,ELt),e(wm,O1e),e(O1e,yLt),e(wm,wLt),e(wm,G1e),e(G1e,ALt),e(wm,LLt),e(Sr,BLt),e(Sr,X1e),e(X1e,xLt),e(Sr,kLt),g(p8,Sr,null),e(Nt,RLt),e(Nt,Io),g(_8,Io,null),e(Io,SLt),e(Io,V1e),e(V1e,PLt),e(Io,$Lt),e(Io,js),e(js,ILt),e(js,z1e),e(z1e,jLt),e(js,DLt),e(js,W1e),e(W1e,NLt),e(js,qLt),e(js,Q1e),e(Q1e,OLt),e(js,GLt),e(Io,XLt),e(Io,H1e),e(H1e,R3),e(R3,U1e),e(U1e,VLt),e(R3,zLt),e(R3,ez),e(ez,WLt),e(R3,QLt),e(Io,HLt),e(Io,J1e),e(J1e,ULt),e(Io,JLt),g(b8,Io,null),b(d,Fke,_),b(d,Am,_),e(Am,S3),e(S3,Y1e),g(v8,Y1e,null),e(Am,YLt),e(Am,K1e),e(K1e,KLt),b(d,Cke,_),b(d,qt,_),g(T8,qt,null),e(qt,ZLt),e(qt,Lm),e(Lm,e8t),e(Lm,Z1e),e(Z1e,o8t),e(Lm,t8t),e(Lm,eFe),e(eFe,r8t),e(Lm,a8t),e(qt,s8t),e(qt,F8),e(F8,n8t),e(F8,oFe),e(oFe,l8t),e(F8,i8t),e(qt,d8t),e(qt,Pr),g(C8,Pr,null),e(Pr,c8t),e(Pr,tFe),e(tFe,m8t),e(Pr,f8t),e(Pr,Bm),e(Bm,g8t),e(Bm,rFe),e(rFe,h8t),e(Bm,u8t),e(Bm,aFe),e(aFe,p8t),e(Bm,_8t),e(Pr,b8t),e(Pr,sFe),e(sFe,v8t),e(Pr,T8t),g(M8,Pr,null),e(qt,F8t),e(qt,jo),g(E8,jo,null),e(jo,C8t),e(jo,nFe),e(nFe,M8t),e(jo,E8t),e(jo,Ds),e(Ds,y8t),e(Ds,lFe),e(lFe,w8t),e(Ds,A8t),e(Ds,iFe),e(iFe,L8t),e(Ds,B8t),e(Ds,dFe),e(dFe,x8t),e(Ds,k8t),e(jo,R8t),e(jo,y8),e(y8,P3),e(P3,cFe),e(cFe,S8t),e(P3,P8t),e(P3,oz),e(oz,$8t),e(P3,I8t),e(y8,j8t),e(y8,$3),e($3,mFe),e(mFe,D8t),e($3,N8t),e($3,tz),e(tz,q8t),e($3,O8t),e(jo,G8t),e(jo,fFe),e(fFe,X8t),e(jo,V8t),g(w8,jo,null),b(d,Mke,_),b(d,xm,_),e(xm,I3),e(I3,gFe),g(A8,gFe,null),e(xm,z8t),e(xm,hFe),e(hFe,W8t),b(d,Eke,_),b(d,Ot,_),g(L8,Ot,null),e(Ot,Q8t),e(Ot,km),e(km,H8t),e(km,uFe),e(uFe,U8t),e(km,J8t),e(km,pFe),e(pFe,Y8t),e(km,K8t),e(Ot,Z8t),e(Ot,B8),e(B8,e7t),e(B8,_Fe),e(_Fe,o7t),e(B8,t7t),e(Ot,r7t),e(Ot,$r),g(x8,$r,null),e($r,a7t),e($r,bFe),e(bFe,s7t),e($r,n7t),e($r,Rm),e(Rm,l7t),e(Rm,vFe),e(vFe,i7t),e(Rm,d7t),e(Rm,TFe),e(TFe,c7t),e(Rm,m7t),e($r,f7t),e($r,FFe),e(FFe,g7t),e($r,h7t),g(k8,$r,null),e(Ot,u7t),e(Ot,Do),g(R8,Do,null),e(Do,p7t),e(Do,CFe),e(CFe,_7t),e(Do,b7t),e(Do,Ns),e(Ns,v7t),e(Ns,MFe),e(MFe,T7t),e(Ns,F7t),e(Ns,EFe),e(EFe,C7t),e(Ns,M7t),e(Ns,yFe),e(yFe,E7t),e(Ns,y7t),e(Do,w7t),e(Do,wFe),e(wFe,j3),e(j3,AFe),e(AFe,A7t),e(j3,L7t),e(j3,rz),e(rz,B7t),e(j3,x7t),e(Do,k7t),e(Do,LFe),e(LFe,R7t),e(Do,S7t),g(S8,Do,null),yke=!0},p(d,[_]){const P8={};_&2&&(P8.$$scope={dirty:_,ctx:d}),Nm.$set(P8);const BFe={};_&2&&(BFe.$$scope={dirty:_,ctx:d}),Mh.$set(BFe);const xFe={};_&2&&(xFe.$$scope={dirty:_,ctx:d}),Ph.$set(xFe)},i(d){yke||(h(ce.$$.fragment,d),h(Na.$$.fragment,d),h(G5.$$.fragment,d),h(X5.$$.fragment,d),h(Nm.$$.fragment,d),h(V5.$$.fragment,d),h(z5.$$.fragment,d),h(H5.$$.fragment,d),h(U5.$$.fragment,d),h(J5.$$.fragment,d),h(Y5.$$.fragment,d),h(K5.$$.fragment,d),h(oy.$$.fragment,d),h(ty.$$.fragment,d),h(ry.$$.fragment,d),h(ay.$$.fragment,d),h(sy.$$.fragment,d),h(iy.$$.fragment,d),h(Mh.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(my.$$.fragment,d),h(fy.$$.fragment,d),h(uy.$$.fragment,d),h(Ph.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(xy.$$.fragment,d),h(ky.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Dy.$$.fragment,d),h(Ny.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Xy.$$.fragment,d),h(Vy.$$.fragment,d),h(zy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(tw.$$.fragment,d),h(rw.$$.fragment,d),h(aw.$$.fragment,d),h(sw.$$.fragment,d),h(nw.$$.fragment,d),h(lw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(mw.$$.fragment,d),h(fw.$$.fragment,d),h(gw.$$.fragment,d),h(hw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Lw.$$.fragment,d),h(xw.$$.fragment,d),h(kw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(jw.$$.fragment,d),h(Dw.$$.fragment,d),h(Nw.$$.fragment,d),h(qw.$$.fragment,d),h(Ow.$$.fragment,d),h(Gw.$$.fragment,d),h(Vw.$$.fragment,d),h(zw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(e6.$$.fragment,d),h(o6.$$.fragment,d),h(t6.$$.fragment,d),h(a6.$$.fragment,d),h(s6.$$.fragment,d),h(n6.$$.fragment,d),h(l6.$$.fragment,d),h(i6.$$.fragment,d),h(d6.$$.fragment,d),h(m6.$$.fragment,d),h(f6.$$.fragment,d),h(g6.$$.fragment,d),h(h6.$$.fragment,d),h(u6.$$.fragment,d),h(p6.$$.fragment,d),h(b6.$$.fragment,d),h(v6.$$.fragment,d),h(T6.$$.fragment,d),h(F6.$$.fragment,d),h(C6.$$.fragment,d),h(M6.$$.fragment,d),h(y6.$$.fragment,d),h(w6.$$.fragment,d),h(A6.$$.fragment,d),h(B6.$$.fragment,d),h(x6.$$.fragment,d),h(k6.$$.fragment,d),h(S6.$$.fragment,d),h(P6.$$.fragment,d),h($6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(D6.$$.fragment,d),h(q6.$$.fragment,d),h(O6.$$.fragment,d),h(G6.$$.fragment,d),h(X6.$$.fragment,d),h(V6.$$.fragment,d),h(z6.$$.fragment,d),h(Q6.$$.fragment,d),h(H6.$$.fragment,d),h(U6.$$.fragment,d),h(J6.$$.fragment,d),h(Y6.$$.fragment,d),h(K6.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(tA.$$.fragment,d),h(rA.$$.fragment,d),h(aA.$$.fragment,d),h(sA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(dA.$$.fragment,d),h(mA.$$.fragment,d),h(fA.$$.fragment,d),h(gA.$$.fragment,d),h(uA.$$.fragment,d),h(pA.$$.fragment,d),h(_A.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(CA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(AA.$$.fragment,d),h(BA.$$.fragment,d),h(xA.$$.fragment,d),h(kA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h(PA.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(DA.$$.fragment,d),h(NA.$$.fragment,d),h(qA.$$.fragment,d),h(OA.$$.fragment,d),h(XA.$$.fragment,d),h(VA.$$.fragment,d),h(zA.$$.fragment,d),h(QA.$$.fragment,d),h(HA.$$.fragment,d),h(UA.$$.fragment,d),h(YA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(e0.$$.fragment,d),h(o0.$$.fragment,d),h(t0.$$.fragment,d),h(a0.$$.fragment,d),h(s0.$$.fragment,d),h(n0.$$.fragment,d),h(l0.$$.fragment,d),h(i0.$$.fragment,d),h(d0.$$.fragment,d),h(m0.$$.fragment,d),h(f0.$$.fragment,d),h(g0.$$.fragment,d),h(h0.$$.fragment,d),h(u0.$$.fragment,d),h(p0.$$.fragment,d),h(b0.$$.fragment,d),h(v0.$$.fragment,d),h(T0.$$.fragment,d),h(F0.$$.fragment,d),h(C0.$$.fragment,d),h(M0.$$.fragment,d),h(y0.$$.fragment,d),h(w0.$$.fragment,d),h(A0.$$.fragment,d),h(L0.$$.fragment,d),h(B0.$$.fragment,d),h(x0.$$.fragment,d),h(R0.$$.fragment,d),h(S0.$$.fragment,d),h(P0.$$.fragment,d),h($0.$$.fragment,d),h(I0.$$.fragment,d),h(j0.$$.fragment,d),h(N0.$$.fragment,d),h(q0.$$.fragment,d),h(O0.$$.fragment,d),h(G0.$$.fragment,d),h(X0.$$.fragment,d),h(V0.$$.fragment,d),h(W0.$$.fragment,d),h(Q0.$$.fragment,d),h(H0.$$.fragment,d),h(U0.$$.fragment,d),h(J0.$$.fragment,d),h(Y0.$$.fragment,d),h(Z0.$$.fragment,d),h(eL.$$.fragment,d),h(oL.$$.fragment,d),h(tL.$$.fragment,d),h(rL.$$.fragment,d),h(aL.$$.fragment,d),h(nL.$$.fragment,d),h(lL.$$.fragment,d),h(iL.$$.fragment,d),h(dL.$$.fragment,d),h(cL.$$.fragment,d),h(mL.$$.fragment,d),h(gL.$$.fragment,d),h(hL.$$.fragment,d),h(uL.$$.fragment,d),h(pL.$$.fragment,d),h(_L.$$.fragment,d),h(bL.$$.fragment,d),h(TL.$$.fragment,d),h(FL.$$.fragment,d),h(CL.$$.fragment,d),h(ML.$$.fragment,d),h(EL.$$.fragment,d),h(yL.$$.fragment,d),h(AL.$$.fragment,d),h(LL.$$.fragment,d),h(BL.$$.fragment,d),h(xL.$$.fragment,d),h(kL.$$.fragment,d),h(RL.$$.fragment,d),h(PL.$$.fragment,d),h($L.$$.fragment,d),h(IL.$$.fragment,d),h(jL.$$.fragment,d),h(DL.$$.fragment,d),h(NL.$$.fragment,d),h(OL.$$.fragment,d),h(GL.$$.fragment,d),h(XL.$$.fragment,d),h(VL.$$.fragment,d),h(zL.$$.fragment,d),h(WL.$$.fragment,d),h(HL.$$.fragment,d),h(UL.$$.fragment,d),h(JL.$$.fragment,d),h(YL.$$.fragment,d),h(KL.$$.fragment,d),h(ZL.$$.fragment,d),h(o8.$$.fragment,d),h(t8.$$.fragment,d),h(r8.$$.fragment,d),h(a8.$$.fragment,d),h(s8.$$.fragment,d),h(n8.$$.fragment,d),h(i8.$$.fragment,d),h(d8.$$.fragment,d),h(c8.$$.fragment,d),h(m8.$$.fragment,d),h(f8.$$.fragment,d),h(g8.$$.fragment,d),h(u8.$$.fragment,d),h(p8.$$.fragment,d),h(_8.$$.fragment,d),h(b8.$$.fragment,d),h(v8.$$.fragment,d),h(T8.$$.fragment,d),h(C8.$$.fragment,d),h(M8.$$.fragment,d),h(E8.$$.fragment,d),h(w8.$$.fragment,d),h(A8.$$.fragment,d),h(L8.$$.fragment,d),h(x8.$$.fragment,d),h(k8.$$.fragment,d),h(R8.$$.fragment,d),h(S8.$$.fragment,d),yke=!0)},o(d){u(ce.$$.fragment,d),u(Na.$$.fragment,d),u(G5.$$.fragment,d),u(X5.$$.fragment,d),u(Nm.$$.fragment,d),u(V5.$$.fragment,d),u(z5.$$.fragment,d),u(H5.$$.fragment,d),u(U5.$$.fragment,d),u(J5.$$.fragment,d),u(Y5.$$.fragment,d),u(K5.$$.fragment,d),u(oy.$$.fragment,d),u(ty.$$.fragment,d),u(ry.$$.fragment,d),u(ay.$$.fragment,d),u(sy.$$.fragment,d),u(iy.$$.fragment,d),u(Mh.$$.fragment,d),u(dy.$$.fragment,d),u(cy.$$.fragment,d),u(my.$$.fragment,d),u(fy.$$.fragment,d),u(uy.$$.fragment,d),u(Ph.$$.fragment,d),u(py.$$.fragment,d),u(_y.$$.fragment,d),u(by.$$.fragment,d),u(vy.$$.fragment,d),u(Fy.$$.fragment,d),u(Cy.$$.fragment,d),u(My.$$.fragment,d),u(Ey.$$.fragment,d),u(yy.$$.fragment,d),u(wy.$$.fragment,d),u(Ly.$$.fragment,d),u(By.$$.fragment,d),u(xy.$$.fragment,d),u(ky.$$.fragment,d),u(Ry.$$.fragment,d),u(Sy.$$.fragment,d),u($y.$$.fragment,d),u(Iy.$$.fragment,d),u(jy.$$.fragment,d),u(Dy.$$.fragment,d),u(Ny.$$.fragment,d),u(qy.$$.fragment,d),u(Gy.$$.fragment,d),u(Xy.$$.fragment,d),u(Vy.$$.fragment,d),u(zy.$$.fragment,d),u(Wy.$$.fragment,d),u(Qy.$$.fragment,d),u(Uy.$$.fragment,d),u(Jy.$$.fragment,d),u(Yy.$$.fragment,d),u(Ky.$$.fragment,d),u(Zy.$$.fragment,d),u(ew.$$.fragment,d),u(tw.$$.fragment,d),u(rw.$$.fragment,d),u(aw.$$.fragment,d),u(sw.$$.fragment,d),u(nw.$$.fragment,d),u(lw.$$.fragment,d),u(dw.$$.fragment,d),u(cw.$$.fragment,d),u(mw.$$.fragment,d),u(fw.$$.fragment,d),u(gw.$$.fragment,d),u(hw.$$.fragment,d),u(pw.$$.fragment,d),u(_w.$$.fragment,d),u(bw.$$.fragment,d),u(vw.$$.fragment,d),u(Tw.$$.fragment,d),u(Fw.$$.fragment,d),u(Mw.$$.fragment,d),u(Ew.$$.fragment,d),u(yw.$$.fragment,d),u(ww.$$.fragment,d),u(Aw.$$.fragment,d),u(Lw.$$.fragment,d),u(xw.$$.fragment,d),u(kw.$$.fragment,d),u(Rw.$$.fragment,d),u(Sw.$$.fragment,d),u(Pw.$$.fragment,d),u($w.$$.fragment,d),u(jw.$$.fragment,d),u(Dw.$$.fragment,d),u(Nw.$$.fragment,d),u(qw.$$.fragment,d),u(Ow.$$.fragment,d),u(Gw.$$.fragment,d),u(Vw.$$.fragment,d),u(zw.$$.fragment,d),u(Ww.$$.fragment,d),u(Qw.$$.fragment,d),u(Hw.$$.fragment,d),u(Uw.$$.fragment,d),u(Yw.$$.fragment,d),u(Kw.$$.fragment,d),u(Zw.$$.fragment,d),u(e6.$$.fragment,d),u(o6.$$.fragment,d),u(t6.$$.fragment,d),u(a6.$$.fragment,d),u(s6.$$.fragment,d),u(n6.$$.fragment,d),u(l6.$$.fragment,d),u(i6.$$.fragment,d),u(d6.$$.fragment,d),u(m6.$$.fragment,d),u(f6.$$.fragment,d),u(g6.$$.fragment,d),u(h6.$$.fragment,d),u(u6.$$.fragment,d),u(p6.$$.fragment,d),u(b6.$$.fragment,d),u(v6.$$.fragment,d),u(T6.$$.fragment,d),u(F6.$$.fragment,d),u(C6.$$.fragment,d),u(M6.$$.fragment,d),u(y6.$$.fragment,d),u(w6.$$.fragment,d),u(A6.$$.fragment,d),u(B6.$$.fragment,d),u(x6.$$.fragment,d),u(k6.$$.fragment,d),u(S6.$$.fragment,d),u(P6.$$.fragment,d),u($6.$$.fragment,d),u(I6.$$.fragment,d),u(j6.$$.fragment,d),u(D6.$$.fragment,d),u(q6.$$.fragment,d),u(O6.$$.fragment,d),u(G6.$$.fragment,d),u(X6.$$.fragment,d),u(V6.$$.fragment,d),u(z6.$$.fragment,d),u(Q6.$$.fragment,d),u(H6.$$.fragment,d),u(U6.$$.fragment,d),u(J6.$$.fragment,d),u(Y6.$$.fragment,d),u(K6.$$.fragment,d),u(eA.$$.fragment,d),u(oA.$$.fragment,d),u(tA.$$.fragment,d),u(rA.$$.fragment,d),u(aA.$$.fragment,d),u(sA.$$.fragment,d),u(lA.$$.fragment,d),u(iA.$$.fragment,d),u(dA.$$.fragment,d),u(mA.$$.fragment,d),u(fA.$$.fragment,d),u(gA.$$.fragment,d),u(uA.$$.fragment,d),u(pA.$$.fragment,d),u(_A.$$.fragment,d),u(bA.$$.fragment,d),u(vA.$$.fragment,d),u(TA.$$.fragment,d),u(CA.$$.fragment,d),u(MA.$$.fragment,d),u(EA.$$.fragment,d),u(yA.$$.fragment,d),u(wA.$$.fragment,d),u(AA.$$.fragment,d),u(BA.$$.fragment,d),u(xA.$$.fragment,d),u(kA.$$.fragment,d),u(RA.$$.fragment,d),u(SA.$$.fragment,d),u(PA.$$.fragment,d),u(IA.$$.fragment,d),u(jA.$$.fragment,d),u(DA.$$.fragment,d),u(NA.$$.fragment,d),u(qA.$$.fragment,d),u(OA.$$.fragment,d),u(XA.$$.fragment,d),u(VA.$$.fragment,d),u(zA.$$.fragment,d),u(QA.$$.fragment,d),u(HA.$$.fragment,d),u(UA.$$.fragment,d),u(YA.$$.fragment,d),u(KA.$$.fragment,d),u(ZA.$$.fragment,d),u(e0.$$.fragment,d),u(o0.$$.fragment,d),u(t0.$$.fragment,d),u(a0.$$.fragment,d),u(s0.$$.fragment,d),u(n0.$$.fragment,d),u(l0.$$.fragment,d),u(i0.$$.fragment,d),u(d0.$$.fragment,d),u(m0.$$.fragment,d),u(f0.$$.fragment,d),u(g0.$$.fragment,d),u(h0.$$.fragment,d),u(u0.$$.fragment,d),u(p0.$$.fragment,d),u(b0.$$.fragment,d),u(v0.$$.fragment,d),u(T0.$$.fragment,d),u(F0.$$.fragment,d),u(C0.$$.fragment,d),u(M0.$$.fragment,d),u(y0.$$.fragment,d),u(w0.$$.fragment,d),u(A0.$$.fragment,d),u(L0.$$.fragment,d),u(B0.$$.fragment,d),u(x0.$$.fragment,d),u(R0.$$.fragment,d),u(S0.$$.fragment,d),u(P0.$$.fragment,d),u($0.$$.fragment,d),u(I0.$$.fragment,d),u(j0.$$.fragment,d),u(N0.$$.fragment,d),u(q0.$$.fragment,d),u(O0.$$.fragment,d),u(G0.$$.fragment,d),u(X0.$$.fragment,d),u(V0.$$.fragment,d),u(W0.$$.fragment,d),u(Q0.$$.fragment,d),u(H0.$$.fragment,d),u(U0.$$.fragment,d),u(J0.$$.fragment,d),u(Y0.$$.fragment,d),u(Z0.$$.fragment,d),u(eL.$$.fragment,d),u(oL.$$.fragment,d),u(tL.$$.fragment,d),u(rL.$$.fragment,d),u(aL.$$.fragment,d),u(nL.$$.fragment,d),u(lL.$$.fragment,d),u(iL.$$.fragment,d),u(dL.$$.fragment,d),u(cL.$$.fragment,d),u(mL.$$.fragment,d),u(gL.$$.fragment,d),u(hL.$$.fragment,d),u(uL.$$.fragment,d),u(pL.$$.fragment,d),u(_L.$$.fragment,d),u(bL.$$.fragment,d),u(TL.$$.fragment,d),u(FL.$$.fragment,d),u(CL.$$.fragment,d),u(ML.$$.fragment,d),u(EL.$$.fragment,d),u(yL.$$.fragment,d),u(AL.$$.fragment,d),u(LL.$$.fragment,d),u(BL.$$.fragment,d),u(xL.$$.fragment,d),u(kL.$$.fragment,d),u(RL.$$.fragment,d),u(PL.$$.fragment,d),u($L.$$.fragment,d),u(IL.$$.fragment,d),u(jL.$$.fragment,d),u(DL.$$.fragment,d),u(NL.$$.fragment,d),u(OL.$$.fragment,d),u(GL.$$.fragment,d),u(XL.$$.fragment,d),u(VL.$$.fragment,d),u(zL.$$.fragment,d),u(WL.$$.fragment,d),u(HL.$$.fragment,d),u(UL.$$.fragment,d),u(JL.$$.fragment,d),u(YL.$$.fragment,d),u(KL.$$.fragment,d),u(ZL.$$.fragment,d),u(o8.$$.fragment,d),u(t8.$$.fragment,d),u(r8.$$.fragment,d),u(a8.$$.fragment,d),u(s8.$$.fragment,d),u(n8.$$.fragment,d),u(i8.$$.fragment,d),u(d8.$$.fragment,d),u(c8.$$.fragment,d),u(m8.$$.fragment,d),u(f8.$$.fragment,d),u(g8.$$.fragment,d),u(u8.$$.fragment,d),u(p8.$$.fragment,d),u(_8.$$.fragment,d),u(b8.$$.fragment,d),u(v8.$$.fragment,d),u(T8.$$.fragment,d),u(C8.$$.fragment,d),u(M8.$$.fragment,d),u(E8.$$.fragment,d),u(w8.$$.fragment,d),u(A8.$$.fragment,d),u(L8.$$.fragment,d),u(x8.$$.fragment,d),u(k8.$$.fragment,d),u(R8.$$.fragment,d),u(S8.$$.fragment,d),yke=!1},d(d){r(J),d&&r(Be),d&&r(de),p(ce),d&&r(Pm),d&&r(ca),d&&r(ye),d&&r(co),d&&r(Im),p(Na,d),d&&r(mo),d&&r(ge),d&&r(Xo),d&&r(qa),d&&r(EBe),d&&r(Xi),p(G5),d&&r(yBe),d&&r(Vs),d&&r(wBe),p(X5,d),d&&r(ABe),d&&r(I7),d&&r(LBe),p(Nm,d),d&&r(BBe),d&&r(Vi),p(V5),d&&r(xBe),d&&r(Vo),p(z5),p(H5),p(U5),p(J5),d&&r(kBe),d&&r(Wi),p(Y5),d&&r(RBe),d&&r(zo),p(K5),p(oy),p(ty),p(ry),d&&r(SBe),d&&r(Qi),p(ay),d&&r(PBe),d&&r(Wo),p(sy),p(iy),p(Mh),p(dy),p(cy),d&&r($Be),d&&r(Hi),p(my),d&&r(IBe),d&&r(Qo),p(fy),p(uy),p(Ph),p(py),p(_y),d&&r(jBe),d&&r(Ji),p(by),d&&r(DBe),d&&r(Ho),p(vy),p(Fy),p(Cy),p(My),p(Ey),d&&r(NBe),d&&r(Zi),p(yy),d&&r(qBe),d&&r(Uo),p(wy),p(Ly),p(By),p(xy),p(ky),d&&r(OBe),d&&r(td),p(Ry),d&&r(GBe),d&&r(Jo),p(Sy),p($y),p(Iy),p(jy),p(Dy),d&&r(XBe),d&&r(sd),p(Ny),d&&r(VBe),d&&r(Yo),p(qy),p(Gy),p(Xy),p(Vy),p(zy),d&&r(zBe),d&&r(id),p(Wy),d&&r(WBe),d&&r(Ko),p(Qy),p(Uy),p(Jy),p(Yy),p(Ky),d&&r(QBe),d&&r(md),p(Zy),d&&r(HBe),d&&r(Zo),p(ew),p(tw),p(rw),p(aw),p(sw),d&&r(UBe),d&&r(hd),p(nw),d&&r(JBe),d&&r(et),p(lw),p(dw),p(cw),p(mw),p(fw),d&&r(YBe),d&&r(_d),p(gw),d&&r(KBe),d&&r(ot),p(hw),p(pw),p(_w),p(bw),p(vw),d&&r(ZBe),d&&r(Td),p(Tw),d&&r(exe),d&&r(tt),p(Fw),p(Mw),p(Ew),p(yw),p(ww),d&&r(oxe),d&&r(Md),p(Aw),d&&r(txe),d&&r(rt),p(Lw),p(xw),p(kw),p(Rw),p(Sw),d&&r(rxe),d&&r(wd),p(Pw),d&&r(axe),d&&r(at),p($w),p(jw),p(Dw),p(Nw),p(qw),d&&r(sxe),d&&r(Bd),p(Ow),d&&r(nxe),d&&r(st),p(Gw),p(Vw),p(zw),p(Ww),p(Qw),d&&r(lxe),d&&r(Rd),p(Hw),d&&r(ixe),d&&r(nt),p(Uw),p(Yw),p(Kw),p(Zw),p(e6),d&&r(dxe),d&&r($d),p(o6),d&&r(cxe),d&&r(lt),p(t6),p(a6),p(s6),p(n6),p(l6),d&&r(mxe),d&&r(Dd),p(i6),d&&r(fxe),d&&r(it),p(d6),p(m6),p(f6),p(g6),p(h6),d&&r(gxe),d&&r(Od),p(u6),d&&r(hxe),d&&r(dt),p(p6),p(b6),p(v6),p(T6),p(F6),d&&r(uxe),d&&r(Vd),p(C6),d&&r(pxe),d&&r(ct),p(M6),p(y6),p(w6),p(A6),p(B6),d&&r(_xe),d&&r(Qd),p(x6),d&&r(bxe),d&&r(mt),p(k6),p(S6),p(P6),p($6),p(I6),d&&r(vxe),d&&r(Jd),p(j6),d&&r(Txe),d&&r(ft),p(D6),p(q6),p(O6),p(G6),p(X6),d&&r(Fxe),d&&r(ec),p(V6),d&&r(Cxe),d&&r(gt),p(z6),p(Q6),p(H6),p(U6),p(J6),d&&r(Mxe),d&&r(rc),p(Y6),d&&r(Exe),d&&r(ht),p(K6),p(eA),p(oA),p(tA),p(rA),d&&r(yxe),d&&r(nc),p(aA),d&&r(wxe),d&&r(ut),p(sA),p(lA),p(iA),p(dA),p(mA),d&&r(Axe),d&&r(dc),p(fA),d&&r(Lxe),d&&r(pt),p(gA),p(uA),p(pA),p(_A),p(bA),d&&r(Bxe),d&&r(fc),p(vA),d&&r(xxe),d&&r(_t),p(TA),p(CA),p(MA),p(EA),p(yA),d&&r(kxe),d&&r(uc),p(wA),d&&r(Rxe),d&&r(bt),p(AA),p(BA),p(xA),p(kA),p(RA),d&&r(Sxe),d&&r(bc),p(SA),d&&r(Pxe),d&&r(vt),p(PA),p(IA),p(jA),p(DA),p(NA),d&&r($xe),d&&r(Fc),p(qA),d&&r(Ixe),d&&r(Tt),p(OA),p(XA),p(VA),p(zA),p(QA),d&&r(jxe),d&&r(Ec),p(HA),d&&r(Dxe),d&&r(Ft),p(UA),p(YA),p(KA),p(ZA),p(e0),d&&r(Nxe),d&&r(Ac),p(o0),d&&r(qxe),d&&r(Ct),p(t0),p(a0),p(s0),p(n0),p(l0),d&&r(Oxe),d&&r(xc),p(i0),d&&r(Gxe),d&&r(Mt),p(d0),p(m0),p(f0),p(g0),p(h0),d&&r(Xxe),d&&r(Sc),p(u0),d&&r(Vxe),d&&r(Et),p(p0),p(b0),p(v0),p(T0),p(F0),d&&r(zxe),d&&r(Ic),p(C0),d&&r(Wxe),d&&r(yt),p(M0),p(y0),p(w0),p(A0),p(L0),d&&r(Qxe),d&&r(Nc),p(B0),d&&r(Hxe),d&&r(wt),p(x0),p(R0),p(S0),p(P0),p($0),d&&r(Uxe),d&&r(Gc),p(I0),d&&r(Jxe),d&&r(At),p(j0),p(N0),p(q0),p(O0),p(G0),d&&r(Yxe),d&&r(zc),p(X0),d&&r(Kxe),d&&r(Lt),p(V0),p(W0),p(Q0),p(H0),p(U0),d&&r(Zxe),d&&r(Hc),p(J0),d&&r(eke),d&&r(Bt),p(Y0),p(Z0),p(eL),p(oL),p(tL),d&&r(oke),d&&r(Yc),p(rL),d&&r(tke),d&&r(xt),p(aL),p(nL),p(lL),p(iL),p(dL),d&&r(rke),d&&r(em),p(cL),d&&r(ake),d&&r(kt),p(mL),p(gL),p(hL),p(uL),p(pL),d&&r(ske),d&&r(rm),p(_L),d&&r(nke),d&&r(Rt),p(bL),p(TL),p(FL),p(CL),p(ML),d&&r(lke),d&&r(nm),p(EL),d&&r(ike),d&&r(St),p(yL),p(AL),p(LL),p(BL),p(xL),d&&r(dke),d&&r(dm),p(kL),d&&r(cke),d&&r(Pt),p(RL),p(PL),p($L),p(IL),p(jL),d&&r(mke),d&&r(fm),p(DL),d&&r(fke),d&&r($t),p(NL),p(OL),p(GL),p(XL),p(VL),d&&r(gke),d&&r(um),p(zL),d&&r(hke),d&&r(It),p(WL),p(HL),p(UL),p(JL),p(YL),d&&r(uke),d&&r(bm),p(KL),d&&r(pke),d&&r(jt),p(ZL),p(o8),p(t8),p(r8),p(a8),d&&r(_ke),d&&r(Fm),p(s8),d&&r(bke),d&&r(Dt),p(n8),p(i8),p(d8),p(c8),p(m8),d&&r(vke),d&&r(Em),p(f8),d&&r(Tke),d&&r(Nt),p(g8),p(u8),p(p8),p(_8),p(b8),d&&r(Fke),d&&r(Am),p(v8),d&&r(Cke),d&&r(qt),p(T8),p(C8),p(M8),p(E8),p(w8),d&&r(Mke),d&&r(xm),p(A8),d&&r(Eke),d&&r(Ot),p(L8),p(x8),p(k8),p(R8),p(S8)}}}const K1r={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.AutoModelForInstanceSegmentation",title:"AutoModelForInstanceSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function Z1r($i,J,Be){let{fw:de}=J;return $i.$$set=fe=>{"fw"in fe&&Be(0,de=fe.fw)},[de]}class nFr extends V1r{constructor(J){super();z1r(this,J,Z1r,Y1r,W1r,{fw:0})}}export{nFr as default,K1r as metadata};
