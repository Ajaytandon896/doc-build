import{S as Mmt,i as Emt,s as ymt,e as a,k as l,w as m,t as o,R as wmt,c as s,d as t,m as i,a as n,x as f,h as r,b as c,F as e,g as b,y as g,q as h,o as u,B as p}from"../../chunks/vendor-9daddcfa.js";import{T as g3r}from"../../chunks/Tip-c0a70391.js";import{D as y}from"../../chunks/Docstring-ea6f8b76.js";import{C as w}from"../../chunks/CodeBlock-37b92346.js";import{I as z}from"../../chunks/IconCopyLink-a413fd1b.js";import"../../chunks/CopyButton-6099fd9d.js";function Amt(pi){let J,Ae,le,fe,oo,ce,_e,No,_i,cm,ra,bi,vi,w3,mm,Ee,no,Ti,Ls,A3,Bs,xs,L3,Fi,ks,B3,Ci,fm,ka;return{c(){J=a("p"),Ae=o("If your "),le=a("code"),fe=o("NewModelConfig"),oo=o(" is a subclass of "),ce=a("code"),_e=o("PretrainedConfig"),No=o(`, make sure its
`),_i=a("code"),cm=o("model_type"),ra=o(" attribute is set to the same key you use when registering the config (here "),bi=a("code"),vi=o('"new-model"'),w3=o(")."),mm=l(),Ee=a("p"),no=o("Likewise, if your "),Ti=a("code"),Ls=o("NewModel"),A3=o(" is a subclass of "),Bs=a("a"),xs=o("PreTrainedModel"),L3=o(`, make sure its
`),Fi=a("code"),ks=o("config_class"),B3=o(` attribute is set to the same class you use when registering the model (here
`),Ci=a("code"),fm=o("NewModelConfig"),ka=o(")."),this.h()},l(lo){J=s(lo,"P",{});var ge=n(J);Ae=r(ge,"If your "),le=s(ge,"CODE",{});var o7=n(le);fe=r(o7,"NewModelConfig"),o7.forEach(t),oo=r(ge," is a subclass of "),ce=s(ge,"CODE",{});var Mi=n(ce);_e=r(Mi,"PretrainedConfig"),Mi.forEach(t),No=r(ge,`, make sure its
`),_i=s(ge,"CODE",{});var r7=n(_i);cm=r(r7,"model_type"),r7.forEach(t),ra=r(ge," attribute is set to the same key you use when registering the config (here "),bi=s(ge,"CODE",{});var t7=n(bi);vi=r(t7,'"new-model"'),t7.forEach(t),w3=r(ge,")."),ge.forEach(t),mm=i(lo),Ee=s(lo,"P",{});var Do=n(Ee);no=r(Do,"Likewise, if your "),Ti=s(Do,"CODE",{});var Ra=n(Ti);Ls=r(Ra,"NewModel"),Ra.forEach(t),A3=r(Do," is a subclass of "),Bs=s(Do,"A",{href:!0});var a7=n(Bs);xs=r(a7,"PreTrainedModel"),a7.forEach(t),L3=r(Do,`, make sure its
`),Fi=s(Do,"CODE",{});var gm=n(Fi);ks=r(gm,"config_class"),gm.forEach(t),B3=r(Do,` attribute is set to the same class you use when registering the model (here
`),Ci=s(Do,"CODE",{});var s7=n(Ci);fm=r(s7,"NewModelConfig"),s7.forEach(t),ka=r(Do,")."),Do.forEach(t),this.h()},h(){c(Bs,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(lo,ge){b(lo,J,ge),e(J,Ae),e(J,le),e(le,fe),e(J,oo),e(J,ce),e(ce,_e),e(J,No),e(J,_i),e(_i,cm),e(J,ra),e(J,bi),e(bi,vi),e(J,w3),b(lo,mm,ge),b(lo,Ee,ge),e(Ee,no),e(Ee,Ti),e(Ti,Ls),e(Ee,A3),e(Ee,Bs),e(Bs,xs),e(Ee,L3),e(Ee,Fi),e(Fi,ks),e(Ee,B3),e(Ee,Ci),e(Ci,fm),e(Ee,ka)},d(lo){lo&&t(J),lo&&t(mm),lo&&t(Ee)}}}function Lmt(pi){let J,Ae,le,fe,oo;return{c(){J=a("p"),Ae=o("Passing "),le=a("code"),fe=o("use_auth_token=True"),oo=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Ae=r(_e,"Passing "),le=s(_e,"CODE",{});var No=n(le);fe=r(No,"use_auth_token=True"),No.forEach(t),oo=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Ae),e(J,le),e(le,fe),e(J,oo)},d(ce){ce&&t(J)}}}function Bmt(pi){let J,Ae,le,fe,oo;return{c(){J=a("p"),Ae=o("Passing "),le=a("code"),fe=o("use_auth_token=True"),oo=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Ae=r(_e,"Passing "),le=s(_e,"CODE",{});var No=n(le);fe=r(No,"use_auth_token=True"),No.forEach(t),oo=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Ae),e(J,le),e(le,fe),e(J,oo)},d(ce){ce&&t(J)}}}function xmt(pi){let J,Ae,le,fe,oo,ce,_e,No,_i,cm,ra,bi,vi,w3,mm,Ee,no,Ti,Ls,A3,Bs,xs,L3,Fi,ks,B3,Ci,fm,ka,lo,ge,o7,Mi,r7,t7,Do,Ra,a7,gm,s7,uBe,fLe,Ei,hm,wz,x3,pBe,Az,_Be,gLe,Rs,bBe,Lz,vBe,TBe,Bz,FBe,CBe,hLe,k3,uLe,n7,MBe,pLe,um,_Le,yi,pm,xz,R3,EBe,kz,yBe,bLe,qo,S3,wBe,P3,ABe,l7,LBe,BBe,xBe,$3,kBe,Rz,RBe,SBe,PBe,io,I3,$Be,Sz,IBe,jBe,wi,NBe,Pz,DBe,qBe,$z,GBe,OBe,XBe,v,_m,Iz,zBe,VBe,i7,WBe,QBe,HBe,bm,jz,UBe,JBe,d7,YBe,KBe,ZBe,vm,Nz,exe,oxe,c7,rxe,txe,axe,Tm,Dz,sxe,nxe,m7,lxe,ixe,dxe,Fm,qz,cxe,mxe,f7,fxe,gxe,hxe,Cm,Gz,uxe,pxe,g7,_xe,bxe,vxe,Mm,Oz,Txe,Fxe,h7,Cxe,Mxe,Exe,Em,Xz,yxe,wxe,u7,Axe,Lxe,Bxe,ym,zz,xxe,kxe,p7,Rxe,Sxe,Pxe,wm,Vz,$xe,Ixe,_7,jxe,Nxe,Dxe,Am,Wz,qxe,Gxe,b7,Oxe,Xxe,zxe,Lm,Qz,Vxe,Wxe,v7,Qxe,Hxe,Uxe,Bm,Hz,Jxe,Yxe,T7,Kxe,Zxe,eke,xm,Uz,oke,rke,F7,tke,ake,ske,km,Jz,nke,lke,C7,ike,dke,cke,Rm,Yz,mke,fke,M7,gke,hke,uke,Sm,Kz,pke,_ke,E7,bke,vke,Tke,Pm,Zz,Fke,Cke,y7,Mke,Eke,yke,$m,eV,wke,Ake,w7,Lke,Bke,xke,Im,oV,kke,Rke,A7,Ske,Pke,$ke,jm,rV,Ike,jke,L7,Nke,Dke,qke,Nm,tV,Gke,Oke,B7,Xke,zke,Vke,Dm,aV,Wke,Qke,x7,Hke,Uke,Jke,qm,sV,Yke,Kke,k7,Zke,eRe,oRe,Gm,nV,rRe,tRe,R7,aRe,sRe,nRe,Om,lV,lRe,iRe,S7,dRe,cRe,mRe,Xm,iV,fRe,gRe,P7,hRe,uRe,pRe,zm,dV,_Re,bRe,$7,vRe,TRe,FRe,Vm,cV,CRe,MRe,I7,ERe,yRe,wRe,Wm,mV,ARe,LRe,j7,BRe,xRe,kRe,Qm,fV,RRe,SRe,N7,PRe,$Re,IRe,Hm,gV,jRe,NRe,D7,DRe,qRe,GRe,Um,hV,ORe,XRe,q7,zRe,VRe,WRe,Jm,uV,QRe,HRe,G7,URe,JRe,YRe,Ym,pV,KRe,ZRe,O7,eSe,oSe,rSe,Km,_V,tSe,aSe,X7,sSe,nSe,lSe,Zm,bV,iSe,dSe,z7,cSe,mSe,fSe,ef,vV,gSe,hSe,V7,uSe,pSe,_Se,of,TV,bSe,vSe,W7,TSe,FSe,CSe,rf,FV,MSe,ESe,Q7,ySe,wSe,ASe,tf,CV,LSe,BSe,H7,xSe,kSe,RSe,af,MV,SSe,PSe,U7,$Se,ISe,jSe,sf,EV,NSe,DSe,J7,qSe,GSe,OSe,nf,yV,XSe,zSe,Y7,VSe,WSe,QSe,lf,wV,HSe,USe,K7,JSe,YSe,KSe,df,AV,ZSe,ePe,Z7,oPe,rPe,tPe,cf,LV,aPe,sPe,e8,nPe,lPe,iPe,mf,BV,dPe,cPe,o8,mPe,fPe,gPe,ff,xV,hPe,uPe,r8,pPe,_Pe,bPe,gf,kV,vPe,TPe,t8,FPe,CPe,MPe,hf,RV,EPe,yPe,a8,wPe,APe,LPe,uf,SV,BPe,xPe,PV,kPe,RPe,SPe,pf,$V,PPe,$Pe,s8,IPe,jPe,NPe,_f,IV,DPe,qPe,n8,GPe,OPe,XPe,bf,jV,zPe,VPe,l8,WPe,QPe,HPe,vf,NV,UPe,JPe,i8,YPe,KPe,ZPe,Tf,DV,e$e,o$e,d8,r$e,t$e,a$e,Ff,qV,s$e,n$e,c8,l$e,i$e,d$e,Cf,GV,c$e,m$e,m8,f$e,g$e,h$e,Mf,OV,u$e,p$e,f8,_$e,b$e,v$e,Ef,XV,T$e,F$e,g8,C$e,M$e,E$e,yf,zV,y$e,w$e,h8,A$e,L$e,B$e,wf,VV,x$e,k$e,u8,R$e,S$e,P$e,Af,WV,$$e,I$e,p8,j$e,N$e,D$e,Lf,QV,q$e,G$e,_8,O$e,X$e,z$e,Bf,HV,V$e,W$e,b8,Q$e,H$e,U$e,xf,UV,J$e,Y$e,v8,K$e,Z$e,eIe,kf,JV,oIe,rIe,T8,tIe,aIe,sIe,Rf,YV,nIe,lIe,F8,iIe,dIe,cIe,Sf,KV,mIe,fIe,C8,gIe,hIe,uIe,Pf,ZV,pIe,_Ie,M8,bIe,vIe,TIe,$f,eW,FIe,CIe,E8,MIe,EIe,yIe,If,oW,wIe,AIe,y8,LIe,BIe,xIe,jf,rW,kIe,RIe,w8,SIe,PIe,$Ie,Nf,tW,IIe,jIe,A8,NIe,DIe,qIe,Df,aW,GIe,OIe,L8,XIe,zIe,VIe,qf,sW,WIe,QIe,B8,HIe,UIe,JIe,Gf,nW,YIe,KIe,x8,ZIe,eje,oje,Of,lW,rje,tje,k8,aje,sje,nje,Xf,iW,lje,ije,R8,dje,cje,mje,zf,dW,fje,gje,S8,hje,uje,pje,Vf,cW,_je,bje,P8,vje,Tje,Fje,Wf,mW,Cje,Mje,$8,Eje,yje,wje,Qf,fW,Aje,Lje,I8,Bje,xje,kje,Hf,gW,Rje,Sje,j8,Pje,$je,Ije,Uf,hW,jje,Nje,N8,Dje,qje,Gje,Jf,uW,Oje,Xje,D8,zje,Vje,Wje,Yf,pW,Qje,Hje,q8,Uje,Jje,Yje,Kf,_W,Kje,Zje,G8,eNe,oNe,rNe,bW,tNe,aNe,j3,sNe,Zf,N3,nNe,vW,lNe,vLe,Ai,eg,TW,D3,iNe,FW,dNe,TLe,Go,q3,cNe,G3,mNe,O8,fNe,gNe,hNe,O3,uNe,CW,pNe,_Ne,bNe,co,X3,vNe,MW,TNe,FNe,Sa,CNe,EW,MNe,ENe,yW,yNe,wNe,wW,ANe,LNe,BNe,M,Ss,AW,xNe,kNe,X8,RNe,SNe,z8,PNe,$Ne,INe,Ps,LW,jNe,NNe,V8,DNe,qNe,W8,GNe,ONe,XNe,$s,BW,zNe,VNe,Q8,WNe,QNe,H8,HNe,UNe,JNe,og,xW,YNe,KNe,U8,ZNe,eDe,oDe,Is,kW,rDe,tDe,J8,aDe,sDe,Y8,nDe,lDe,iDe,rg,RW,dDe,cDe,K8,mDe,fDe,gDe,tg,SW,hDe,uDe,Z8,pDe,_De,bDe,ag,PW,vDe,TDe,e9,FDe,CDe,MDe,js,$W,EDe,yDe,o9,wDe,ADe,r9,LDe,BDe,xDe,Ns,IW,kDe,RDe,t9,SDe,PDe,a9,$De,IDe,jDe,Ds,jW,NDe,DDe,s9,qDe,GDe,n9,ODe,XDe,zDe,sg,NW,VDe,WDe,l9,QDe,HDe,UDe,ng,DW,JDe,YDe,i9,KDe,ZDe,eqe,qs,qW,oqe,rqe,d9,tqe,aqe,c9,sqe,nqe,lqe,lg,GW,iqe,dqe,m9,cqe,mqe,fqe,Gs,OW,gqe,hqe,f9,uqe,pqe,g9,_qe,bqe,vqe,Os,XW,Tqe,Fqe,h9,Cqe,Mqe,u9,Eqe,yqe,wqe,Xs,zW,Aqe,Lqe,p9,Bqe,xqe,VW,kqe,Rqe,Sqe,ig,WW,Pqe,$qe,_9,Iqe,jqe,Nqe,zs,QW,Dqe,qqe,b9,Gqe,Oqe,v9,Xqe,zqe,Vqe,dg,HW,Wqe,Qqe,T9,Hqe,Uqe,Jqe,Vs,UW,Yqe,Kqe,F9,Zqe,eGe,C9,oGe,rGe,tGe,Ws,JW,aGe,sGe,M9,nGe,lGe,E9,iGe,dGe,cGe,Qs,YW,mGe,fGe,y9,gGe,hGe,w9,uGe,pGe,_Ge,cg,KW,bGe,vGe,A9,TGe,FGe,CGe,Hs,ZW,MGe,EGe,L9,yGe,wGe,B9,AGe,LGe,BGe,mg,eQ,xGe,kGe,x9,RGe,SGe,PGe,Us,oQ,$Ge,IGe,k9,jGe,NGe,R9,DGe,qGe,GGe,Js,rQ,OGe,XGe,S9,zGe,VGe,P9,WGe,QGe,HGe,Ys,tQ,UGe,JGe,$9,YGe,KGe,I9,ZGe,eOe,oOe,Ks,aQ,rOe,tOe,j9,aOe,sOe,N9,nOe,lOe,iOe,fg,sQ,dOe,cOe,D9,mOe,fOe,gOe,Zs,nQ,hOe,uOe,q9,pOe,_Oe,G9,bOe,vOe,TOe,en,lQ,FOe,COe,O9,MOe,EOe,X9,yOe,wOe,AOe,on,iQ,LOe,BOe,z9,xOe,kOe,V9,ROe,SOe,POe,rn,dQ,$Oe,IOe,W9,jOe,NOe,Q9,DOe,qOe,GOe,tn,cQ,OOe,XOe,H9,zOe,VOe,U9,WOe,QOe,HOe,an,mQ,UOe,JOe,J9,YOe,KOe,Y9,ZOe,eXe,oXe,gg,fQ,rXe,tXe,K9,aXe,sXe,nXe,sn,gQ,lXe,iXe,Z9,dXe,cXe,eB,mXe,fXe,gXe,hg,hQ,hXe,uXe,oB,pXe,_Xe,bXe,ug,uQ,vXe,TXe,rB,FXe,CXe,MXe,nn,pQ,EXe,yXe,tB,wXe,AXe,aB,LXe,BXe,xXe,ln,_Q,kXe,RXe,sB,SXe,PXe,nB,$Xe,IXe,jXe,pg,bQ,NXe,DXe,lB,qXe,GXe,OXe,dn,vQ,XXe,zXe,iB,VXe,WXe,dB,QXe,HXe,UXe,cn,TQ,JXe,YXe,cB,KXe,ZXe,mB,eze,oze,rze,mn,FQ,tze,aze,fB,sze,nze,gB,lze,ize,dze,fn,CQ,cze,mze,hB,fze,gze,uB,hze,uze,pze,gn,MQ,_ze,bze,pB,vze,Tze,_B,Fze,Cze,Mze,_g,EQ,Eze,yze,bB,wze,Aze,Lze,bg,yQ,Bze,xze,vB,kze,Rze,Sze,vg,wQ,Pze,$ze,TB,Ize,jze,Nze,hn,AQ,Dze,qze,FB,Gze,Oze,CB,Xze,zze,Vze,Tg,LQ,Wze,Qze,MB,Hze,Uze,Jze,un,BQ,Yze,Kze,EB,Zze,eVe,yB,oVe,rVe,tVe,pn,xQ,aVe,sVe,wB,nVe,lVe,AB,iVe,dVe,cVe,_n,kQ,mVe,fVe,LB,gVe,hVe,BB,uVe,pVe,_Ve,bn,RQ,bVe,vVe,xB,TVe,FVe,kB,CVe,MVe,EVe,vn,SQ,yVe,wVe,RB,AVe,LVe,SB,BVe,xVe,kVe,Fg,PQ,RVe,SVe,PB,PVe,$Ve,IVe,Cg,$Q,jVe,NVe,$B,DVe,qVe,GVe,Tn,IQ,OVe,XVe,IB,zVe,VVe,jB,WVe,QVe,HVe,Fn,jQ,UVe,JVe,NB,YVe,KVe,DB,ZVe,eWe,oWe,Cn,NQ,rWe,tWe,qB,aWe,sWe,GB,nWe,lWe,iWe,Mg,DQ,dWe,cWe,OB,mWe,fWe,gWe,Eg,qQ,hWe,uWe,XB,pWe,_We,bWe,yg,GQ,vWe,TWe,zB,FWe,CWe,MWe,wg,OQ,EWe,yWe,VB,wWe,AWe,LWe,Mn,XQ,BWe,xWe,WB,kWe,RWe,QB,SWe,PWe,$We,Ag,zQ,IWe,jWe,HB,NWe,DWe,qWe,Lg,VQ,GWe,OWe,UB,XWe,zWe,VWe,En,WQ,WWe,QWe,JB,HWe,UWe,YB,JWe,YWe,KWe,yn,QQ,ZWe,eQe,KB,oQe,rQe,ZB,tQe,aQe,sQe,HQ,nQe,lQe,z3,iQe,Bg,V3,dQe,UQ,cQe,FLe,Li,xg,JQ,W3,mQe,YQ,fQe,CLe,Zt,Q3,gQe,H3,hQe,ex,uQe,pQe,_Qe,U3,bQe,KQ,vQe,TQe,FQe,Le,J3,CQe,ZQ,MQe,EQe,Pa,yQe,eH,wQe,AQe,oH,LQe,BQe,rH,xQe,kQe,RQe,ne,kg,tH,SQe,PQe,ox,$Qe,IQe,jQe,Rg,aH,NQe,DQe,rx,qQe,GQe,OQe,Sg,sH,XQe,zQe,tx,VQe,WQe,QQe,Pg,nH,HQe,UQe,ax,JQe,YQe,KQe,$g,lH,ZQe,eHe,sx,oHe,rHe,tHe,Ig,iH,aHe,sHe,nx,nHe,lHe,iHe,jg,dH,dHe,cHe,lx,mHe,fHe,gHe,Ng,cH,hHe,uHe,ix,pHe,_He,bHe,Dg,mH,vHe,THe,dx,FHe,CHe,MHe,qg,fH,EHe,yHe,cx,wHe,AHe,LHe,Gg,gH,BHe,xHe,mx,kHe,RHe,SHe,Og,hH,PHe,$He,fx,IHe,jHe,NHe,Xg,uH,DHe,qHe,gx,GHe,OHe,XHe,zg,pH,zHe,VHe,hx,WHe,QHe,HHe,Vg,UHe,_H,JHe,YHe,Y3,MLe,Bi,Wg,bH,K3,KHe,vH,ZHe,ELe,ea,Z3,eUe,e5,oUe,ux,rUe,tUe,aUe,o5,sUe,TH,nUe,lUe,iUe,Be,r5,dUe,FH,cUe,mUe,xi,fUe,CH,gUe,hUe,MH,uUe,pUe,_Ue,ye,Qg,EH,bUe,vUe,px,TUe,FUe,CUe,Hg,yH,MUe,EUe,_x,yUe,wUe,AUe,Ug,wH,LUe,BUe,bx,xUe,kUe,RUe,Jg,AH,SUe,PUe,vx,$Ue,IUe,jUe,Yg,LH,NUe,DUe,Tx,qUe,GUe,OUe,Kg,BH,XUe,zUe,Fx,VUe,WUe,QUe,Zg,xH,HUe,UUe,Cx,JUe,YUe,KUe,eh,kH,ZUe,eJe,Mx,oJe,rJe,tJe,oh,aJe,RH,sJe,nJe,t5,yLe,ki,rh,SH,a5,lJe,PH,iJe,wLe,Oo,s5,dJe,Ri,cJe,$H,mJe,fJe,IH,gJe,hJe,uJe,n5,pJe,jH,_Je,bJe,vJe,Pr,l5,TJe,NH,FJe,CJe,Si,MJe,DH,EJe,yJe,qH,wJe,AJe,LJe,GH,BJe,xJe,i5,kJe,xe,d5,RJe,OH,SJe,PJe,$a,$Je,XH,IJe,jJe,zH,NJe,DJe,VH,qJe,GJe,OJe,F,th,WH,XJe,zJe,Ex,VJe,WJe,QJe,ah,QH,HJe,UJe,yx,JJe,YJe,KJe,sh,HH,ZJe,eYe,wx,oYe,rYe,tYe,nh,UH,aYe,sYe,Ax,nYe,lYe,iYe,lh,JH,dYe,cYe,Lx,mYe,fYe,gYe,ih,YH,hYe,uYe,Bx,pYe,_Ye,bYe,dh,KH,vYe,TYe,xx,FYe,CYe,MYe,ch,ZH,EYe,yYe,kx,wYe,AYe,LYe,mh,eU,BYe,xYe,Rx,kYe,RYe,SYe,fh,oU,PYe,$Ye,Sx,IYe,jYe,NYe,gh,rU,DYe,qYe,Px,GYe,OYe,XYe,hh,tU,zYe,VYe,$x,WYe,QYe,HYe,uh,aU,UYe,JYe,Ix,YYe,KYe,ZYe,ph,sU,eKe,oKe,jx,rKe,tKe,aKe,_h,nU,sKe,nKe,Nx,lKe,iKe,dKe,bh,lU,cKe,mKe,Dx,fKe,gKe,hKe,vh,iU,uKe,pKe,qx,_Ke,bKe,vKe,Th,dU,TKe,FKe,Gx,CKe,MKe,EKe,Fh,cU,yKe,wKe,Ox,AKe,LKe,BKe,Ch,mU,xKe,kKe,Xx,RKe,SKe,PKe,Mh,fU,$Ke,IKe,zx,jKe,NKe,DKe,Eh,gU,qKe,GKe,Vx,OKe,XKe,zKe,yh,hU,VKe,WKe,Wx,QKe,HKe,UKe,wh,uU,JKe,YKe,Qx,KKe,ZKe,eZe,Ah,pU,oZe,rZe,Hx,tZe,aZe,sZe,wn,_U,nZe,lZe,Ux,iZe,dZe,Jx,cZe,mZe,fZe,Lh,bU,gZe,hZe,Yx,uZe,pZe,_Ze,Bh,vU,bZe,vZe,Kx,TZe,FZe,CZe,xh,TU,MZe,EZe,Zx,yZe,wZe,AZe,kh,FU,LZe,BZe,ek,xZe,kZe,RZe,Rh,CU,SZe,PZe,ok,$Ze,IZe,jZe,Sh,MU,NZe,DZe,rk,qZe,GZe,OZe,Ph,EU,XZe,zZe,tk,VZe,WZe,QZe,$h,yU,HZe,UZe,ak,JZe,YZe,KZe,Ih,wU,ZZe,eeo,sk,oeo,reo,teo,jh,AU,aeo,seo,nk,neo,leo,ieo,Nh,LU,deo,ceo,lk,meo,feo,geo,Dh,BU,heo,ueo,ik,peo,_eo,beo,qh,xU,veo,Teo,dk,Feo,Ceo,Meo,Gh,kU,Eeo,yeo,ck,weo,Aeo,Leo,Oh,RU,Beo,xeo,mk,keo,Reo,Seo,Xh,SU,Peo,$eo,fk,Ieo,jeo,Neo,zh,PU,Deo,qeo,gk,Geo,Oeo,Xeo,Vh,$U,zeo,Veo,hk,Weo,Qeo,Heo,Wh,IU,Ueo,Jeo,uk,Yeo,Keo,Zeo,Qh,jU,eoo,ooo,pk,roo,too,aoo,Hh,NU,soo,noo,_k,loo,ioo,doo,Uh,DU,coo,moo,bk,foo,goo,hoo,Jh,qU,uoo,poo,vk,_oo,boo,voo,Yh,GU,Too,Foo,Tk,Coo,Moo,Eoo,Kh,OU,yoo,woo,XU,Aoo,Loo,Boo,Zh,zU,xoo,koo,Fk,Roo,Soo,Poo,eu,VU,$oo,Ioo,Ck,joo,Noo,Doo,ou,WU,qoo,Goo,Mk,Ooo,Xoo,zoo,ru,QU,Voo,Woo,Ek,Qoo,Hoo,Uoo,tu,HU,Joo,Yoo,yk,Koo,Zoo,ero,au,UU,oro,rro,wk,tro,aro,sro,su,JU,nro,lro,Ak,iro,dro,cro,nu,YU,mro,fro,Lk,gro,hro,uro,lu,KU,pro,_ro,Bk,bro,vro,Tro,iu,ZU,Fro,Cro,xk,Mro,Ero,yro,du,eJ,wro,Aro,kk,Lro,Bro,xro,cu,oJ,kro,Rro,Rk,Sro,Pro,$ro,mu,rJ,Iro,jro,Sk,Nro,Dro,qro,fu,tJ,Gro,Oro,Pk,Xro,zro,Vro,gu,aJ,Wro,Qro,$k,Hro,Uro,Jro,hu,sJ,Yro,Kro,Ik,Zro,eto,oto,uu,nJ,rto,tto,jk,ato,sto,nto,pu,lJ,lto,ito,Nk,dto,cto,mto,_u,iJ,fto,gto,Dk,hto,uto,pto,bu,dJ,_to,bto,qk,vto,Tto,Fto,vu,cJ,Cto,Mto,Gk,Eto,yto,wto,Tu,mJ,Ato,Lto,Ok,Bto,xto,kto,Fu,fJ,Rto,Sto,Xk,Pto,$to,Ito,Cu,gJ,jto,Nto,zk,Dto,qto,Gto,Mu,hJ,Oto,Xto,Vk,zto,Vto,Wto,Eu,uJ,Qto,Hto,Wk,Uto,Jto,Yto,yu,pJ,Kto,Zto,Qk,eao,oao,rao,wu,_J,tao,aao,Hk,sao,nao,lao,Au,bJ,iao,dao,Uk,cao,mao,fao,Lu,vJ,gao,hao,Jk,uao,pao,_ao,Bu,TJ,bao,vao,Yk,Tao,Fao,Cao,xu,Mao,FJ,Eao,yao,CJ,wao,Aao,MJ,Lao,Bao,c5,ALe,Pi,ku,EJ,m5,xao,yJ,kao,LLe,Xo,f5,Rao,$i,Sao,wJ,Pao,$ao,AJ,Iao,jao,Nao,g5,Dao,LJ,qao,Gao,Oao,$r,h5,Xao,BJ,zao,Vao,Ii,Wao,xJ,Qao,Hao,kJ,Uao,Jao,Yao,RJ,Kao,Zao,u5,eso,ke,p5,oso,SJ,rso,tso,Ia,aso,PJ,sso,nso,$J,lso,iso,IJ,dso,cso,mso,k,Ru,jJ,fso,gso,Kk,hso,uso,pso,Su,NJ,_so,bso,Zk,vso,Tso,Fso,Pu,DJ,Cso,Mso,eR,Eso,yso,wso,$u,qJ,Aso,Lso,oR,Bso,xso,kso,Iu,GJ,Rso,Sso,rR,Pso,$so,Iso,ju,OJ,jso,Nso,tR,Dso,qso,Gso,Nu,XJ,Oso,Xso,aR,zso,Vso,Wso,Du,zJ,Qso,Hso,sR,Uso,Jso,Yso,qu,VJ,Kso,Zso,nR,eno,ono,rno,Gu,WJ,tno,ano,lR,sno,nno,lno,Ou,QJ,ino,dno,iR,cno,mno,fno,Xu,HJ,gno,hno,dR,uno,pno,_no,zu,UJ,bno,vno,cR,Tno,Fno,Cno,Vu,JJ,Mno,Eno,mR,yno,wno,Ano,Wu,YJ,Lno,Bno,fR,xno,kno,Rno,Qu,KJ,Sno,Pno,gR,$no,Ino,jno,Hu,ZJ,Nno,Dno,hR,qno,Gno,Ono,Uu,eY,Xno,zno,uR,Vno,Wno,Qno,Ju,oY,Hno,Uno,pR,Jno,Yno,Kno,Yu,rY,Zno,elo,_R,olo,rlo,tlo,Ku,tY,alo,slo,bR,nlo,llo,ilo,Zu,aY,dlo,clo,vR,mlo,flo,glo,ep,sY,hlo,ulo,TR,plo,_lo,blo,op,nY,vlo,Tlo,FR,Flo,Clo,Mlo,rp,lY,Elo,ylo,CR,wlo,Alo,Llo,tp,iY,Blo,xlo,MR,klo,Rlo,Slo,ap,dY,Plo,$lo,ER,Ilo,jlo,Nlo,sp,cY,Dlo,qlo,yR,Glo,Olo,Xlo,np,mY,zlo,Vlo,wR,Wlo,Qlo,Hlo,lp,fY,Ulo,Jlo,AR,Ylo,Klo,Zlo,ip,gY,eio,oio,LR,rio,tio,aio,dp,hY,sio,nio,BR,lio,iio,dio,cp,uY,cio,mio,xR,fio,gio,hio,mp,pY,uio,pio,kR,_io,bio,vio,fp,_Y,Tio,Fio,RR,Cio,Mio,Eio,gp,bY,yio,wio,SR,Aio,Lio,Bio,hp,vY,xio,kio,PR,Rio,Sio,Pio,up,TY,$io,Iio,$R,jio,Nio,Dio,pp,qio,FY,Gio,Oio,CY,Xio,zio,MY,Vio,Wio,_5,BLe,ji,_p,EY,b5,Qio,yY,Hio,xLe,zo,v5,Uio,Ni,Jio,wY,Yio,Kio,AY,Zio,edo,odo,T5,rdo,LY,tdo,ado,sdo,Ir,F5,ndo,BY,ldo,ido,Di,ddo,xY,cdo,mdo,kY,fdo,gdo,hdo,RY,udo,pdo,C5,_do,Re,M5,bdo,SY,vdo,Tdo,ja,Fdo,PY,Cdo,Mdo,$Y,Edo,ydo,IY,wdo,Ado,Ldo,I,bp,jY,Bdo,xdo,IR,kdo,Rdo,Sdo,vp,NY,Pdo,$do,jR,Ido,jdo,Ndo,Tp,DY,Ddo,qdo,NR,Gdo,Odo,Xdo,Fp,qY,zdo,Vdo,DR,Wdo,Qdo,Hdo,Cp,GY,Udo,Jdo,qR,Ydo,Kdo,Zdo,Mp,OY,eco,oco,GR,rco,tco,aco,Ep,XY,sco,nco,OR,lco,ico,dco,yp,zY,cco,mco,XR,fco,gco,hco,wp,VY,uco,pco,zR,_co,bco,vco,Ap,WY,Tco,Fco,VR,Cco,Mco,Eco,Lp,QY,yco,wco,WR,Aco,Lco,Bco,Bp,HY,xco,kco,QR,Rco,Sco,Pco,xp,UY,$co,Ico,HR,jco,Nco,Dco,kp,JY,qco,Gco,UR,Oco,Xco,zco,Rp,YY,Vco,Wco,JR,Qco,Hco,Uco,Sp,KY,Jco,Yco,YR,Kco,Zco,emo,Pp,ZY,omo,rmo,KR,tmo,amo,smo,$p,eK,nmo,lmo,ZR,imo,dmo,cmo,Ip,oK,mmo,fmo,eS,gmo,hmo,umo,jp,rK,pmo,_mo,tK,bmo,vmo,Tmo,Np,aK,Fmo,Cmo,oS,Mmo,Emo,ymo,Dp,sK,wmo,Amo,rS,Lmo,Bmo,xmo,qp,nK,kmo,Rmo,tS,Smo,Pmo,$mo,Gp,lK,Imo,jmo,aS,Nmo,Dmo,qmo,Op,iK,Gmo,Omo,sS,Xmo,zmo,Vmo,Xp,dK,Wmo,Qmo,nS,Hmo,Umo,Jmo,zp,cK,Ymo,Kmo,lS,Zmo,efo,ofo,Vp,mK,rfo,tfo,iS,afo,sfo,nfo,Wp,fK,lfo,ifo,dS,dfo,cfo,mfo,Qp,gK,ffo,gfo,cS,hfo,ufo,pfo,Hp,hK,_fo,bfo,mS,vfo,Tfo,Ffo,Up,uK,Cfo,Mfo,fS,Efo,yfo,wfo,Jp,pK,Afo,Lfo,gS,Bfo,xfo,kfo,Yp,Rfo,_K,Sfo,Pfo,bK,$fo,Ifo,vK,jfo,Nfo,E5,kLe,qi,Kp,TK,y5,Dfo,FK,qfo,RLe,Vo,w5,Gfo,Gi,Ofo,CK,Xfo,zfo,MK,Vfo,Wfo,Qfo,A5,Hfo,EK,Ufo,Jfo,Yfo,jr,L5,Kfo,yK,Zfo,ego,Oi,ogo,wK,rgo,tgo,AK,ago,sgo,ngo,LK,lgo,igo,B5,dgo,Se,x5,cgo,BK,mgo,fgo,Na,ggo,xK,hgo,ugo,kK,pgo,_go,RK,bgo,vgo,Tgo,$,Zp,SK,Fgo,Cgo,hS,Mgo,Ego,ygo,e_,PK,wgo,Ago,uS,Lgo,Bgo,xgo,o_,$K,kgo,Rgo,pS,Sgo,Pgo,$go,r_,IK,Igo,jgo,_S,Ngo,Dgo,qgo,t_,jK,Ggo,Ogo,bS,Xgo,zgo,Vgo,a_,NK,Wgo,Qgo,vS,Hgo,Ugo,Jgo,s_,DK,Ygo,Kgo,TS,Zgo,eho,oho,n_,qK,rho,tho,FS,aho,sho,nho,l_,GK,lho,iho,CS,dho,cho,mho,i_,OK,fho,gho,MS,hho,uho,pho,d_,XK,_ho,bho,ES,vho,Tho,Fho,c_,zK,Cho,Mho,yS,Eho,yho,who,m_,VK,Aho,Lho,wS,Bho,xho,kho,f_,WK,Rho,Sho,AS,Pho,$ho,Iho,g_,QK,jho,Nho,LS,Dho,qho,Gho,h_,HK,Oho,Xho,BS,zho,Vho,Who,u_,UK,Qho,Hho,xS,Uho,Jho,Yho,p_,JK,Kho,Zho,kS,euo,ouo,ruo,__,YK,tuo,auo,RS,suo,nuo,luo,b_,KK,iuo,duo,SS,cuo,muo,fuo,v_,ZK,guo,huo,PS,uuo,puo,_uo,T_,eZ,buo,vuo,$S,Tuo,Fuo,Cuo,F_,oZ,Muo,Euo,rZ,yuo,wuo,Auo,C_,tZ,Luo,Buo,IS,xuo,kuo,Ruo,M_,aZ,Suo,Puo,jS,$uo,Iuo,juo,E_,sZ,Nuo,Duo,NS,quo,Guo,Ouo,y_,nZ,Xuo,zuo,DS,Vuo,Wuo,Quo,w_,lZ,Huo,Uuo,qS,Juo,Yuo,Kuo,A_,iZ,Zuo,epo,GS,opo,rpo,tpo,L_,dZ,apo,spo,cZ,npo,lpo,ipo,B_,mZ,dpo,cpo,OS,mpo,fpo,gpo,x_,fZ,hpo,upo,XS,ppo,_po,bpo,k_,gZ,vpo,Tpo,zS,Fpo,Cpo,Mpo,R_,hZ,Epo,ypo,VS,wpo,Apo,Lpo,S_,Bpo,uZ,xpo,kpo,pZ,Rpo,Spo,_Z,Ppo,$po,k5,SLe,Xi,P_,bZ,R5,Ipo,vZ,jpo,PLe,Wo,S5,Npo,zi,Dpo,TZ,qpo,Gpo,FZ,Opo,Xpo,zpo,P5,Vpo,CZ,Wpo,Qpo,Hpo,Nr,$5,Upo,MZ,Jpo,Ypo,Vi,Kpo,EZ,Zpo,e_o,yZ,o_o,r_o,t_o,wZ,a_o,s_o,I5,n_o,Pe,j5,l_o,AZ,i_o,d_o,Da,c_o,LZ,m_o,f_o,BZ,g_o,h_o,xZ,u_o,p_o,__o,se,$_,kZ,b_o,v_o,WS,T_o,F_o,C_o,I_,RZ,M_o,E_o,QS,y_o,w_o,A_o,j_,SZ,L_o,B_o,HS,x_o,k_o,R_o,N_,PZ,S_o,P_o,US,$_o,I_o,j_o,D_,$Z,N_o,D_o,JS,q_o,G_o,O_o,q_,IZ,X_o,z_o,YS,V_o,W_o,Q_o,G_,jZ,H_o,U_o,KS,J_o,Y_o,K_o,O_,NZ,Z_o,ebo,ZS,obo,rbo,tbo,X_,DZ,abo,sbo,eP,nbo,lbo,ibo,z_,qZ,dbo,cbo,oP,mbo,fbo,gbo,V_,GZ,hbo,ubo,rP,pbo,_bo,bbo,W_,OZ,vbo,Tbo,tP,Fbo,Cbo,Mbo,Q_,XZ,Ebo,ybo,aP,wbo,Abo,Lbo,H_,zZ,Bbo,xbo,sP,kbo,Rbo,Sbo,U_,VZ,Pbo,$bo,nP,Ibo,jbo,Nbo,J_,Dbo,WZ,qbo,Gbo,QZ,Obo,Xbo,HZ,zbo,Vbo,N5,$Le,Wi,Y_,UZ,D5,Wbo,JZ,Qbo,ILe,Qo,q5,Hbo,Qi,Ubo,YZ,Jbo,Ybo,KZ,Kbo,Zbo,e2o,G5,o2o,ZZ,r2o,t2o,a2o,Dr,O5,s2o,eee,n2o,l2o,Hi,i2o,oee,d2o,c2o,ree,m2o,f2o,g2o,tee,h2o,u2o,X5,p2o,$e,z5,_2o,aee,b2o,v2o,qa,T2o,see,F2o,C2o,nee,M2o,E2o,lee,y2o,w2o,A2o,A,K_,iee,L2o,B2o,lP,x2o,k2o,R2o,Z_,dee,S2o,P2o,iP,$2o,I2o,j2o,eb,cee,N2o,D2o,dP,q2o,G2o,O2o,ob,mee,X2o,z2o,cP,V2o,W2o,Q2o,rb,fee,H2o,U2o,mP,J2o,Y2o,K2o,tb,gee,Z2o,evo,fP,ovo,rvo,tvo,ab,hee,avo,svo,gP,nvo,lvo,ivo,sb,uee,dvo,cvo,hP,mvo,fvo,gvo,nb,pee,hvo,uvo,uP,pvo,_vo,bvo,lb,_ee,vvo,Tvo,pP,Fvo,Cvo,Mvo,ib,bee,Evo,yvo,_P,wvo,Avo,Lvo,db,vee,Bvo,xvo,bP,kvo,Rvo,Svo,cb,Tee,Pvo,$vo,vP,Ivo,jvo,Nvo,mb,Fee,Dvo,qvo,TP,Gvo,Ovo,Xvo,fb,Cee,zvo,Vvo,FP,Wvo,Qvo,Hvo,gb,Mee,Uvo,Jvo,CP,Yvo,Kvo,Zvo,hb,Eee,eTo,oTo,MP,rTo,tTo,aTo,ub,yee,sTo,nTo,EP,lTo,iTo,dTo,pb,wee,cTo,mTo,yP,fTo,gTo,hTo,_b,Aee,uTo,pTo,wP,_To,bTo,vTo,bb,Lee,TTo,FTo,AP,CTo,MTo,ETo,vb,Bee,yTo,wTo,LP,ATo,LTo,BTo,Tb,xee,xTo,kTo,BP,RTo,STo,PTo,Fb,kee,$To,ITo,xP,jTo,NTo,DTo,Cb,Ree,qTo,GTo,kP,OTo,XTo,zTo,Mb,See,VTo,WTo,RP,QTo,HTo,UTo,Eb,Pee,JTo,YTo,SP,KTo,ZTo,e1o,yb,$ee,o1o,r1o,PP,t1o,a1o,s1o,wb,Iee,n1o,l1o,$P,i1o,d1o,c1o,Ab,jee,m1o,f1o,IP,g1o,h1o,u1o,Lb,Nee,p1o,_1o,jP,b1o,v1o,T1o,Bb,Dee,F1o,C1o,qee,M1o,E1o,y1o,xb,Gee,w1o,A1o,NP,L1o,B1o,x1o,kb,Oee,k1o,R1o,DP,S1o,P1o,$1o,Rb,Xee,I1o,j1o,qP,N1o,D1o,q1o,Sb,zee,G1o,O1o,GP,X1o,z1o,V1o,Pb,Vee,W1o,Q1o,OP,H1o,U1o,J1o,$b,Wee,Y1o,K1o,XP,Z1o,eFo,oFo,Ib,Qee,rFo,tFo,zP,aFo,sFo,nFo,jb,Hee,lFo,iFo,VP,dFo,cFo,mFo,Nb,Uee,fFo,gFo,WP,hFo,uFo,pFo,Db,Jee,_Fo,bFo,QP,vFo,TFo,FFo,qb,Yee,CFo,MFo,HP,EFo,yFo,wFo,Gb,Kee,AFo,LFo,UP,BFo,xFo,kFo,Ob,RFo,Zee,SFo,PFo,eoe,$Fo,IFo,ooe,jFo,NFo,V5,jLe,Ui,Xb,roe,W5,DFo,toe,qFo,NLe,Ho,Q5,GFo,Ji,OFo,aoe,XFo,zFo,soe,VFo,WFo,QFo,H5,HFo,noe,UFo,JFo,YFo,qr,U5,KFo,loe,ZFo,eCo,Yi,oCo,ioe,rCo,tCo,doe,aCo,sCo,nCo,coe,lCo,iCo,J5,dCo,Ie,Y5,cCo,moe,mCo,fCo,Ga,gCo,foe,hCo,uCo,goe,pCo,_Co,hoe,bCo,vCo,TCo,G,zb,uoe,FCo,CCo,JP,MCo,ECo,yCo,Vb,poe,wCo,ACo,YP,LCo,BCo,xCo,Wb,_oe,kCo,RCo,KP,SCo,PCo,$Co,Qb,boe,ICo,jCo,ZP,NCo,DCo,qCo,Hb,voe,GCo,OCo,e$,XCo,zCo,VCo,Ub,Toe,WCo,QCo,o$,HCo,UCo,JCo,Jb,Foe,YCo,KCo,r$,ZCo,e4o,o4o,Yb,Coe,r4o,t4o,t$,a4o,s4o,n4o,Kb,Moe,l4o,i4o,a$,d4o,c4o,m4o,Zb,Eoe,f4o,g4o,s$,h4o,u4o,p4o,e2,yoe,_4o,b4o,n$,v4o,T4o,F4o,o2,woe,C4o,M4o,l$,E4o,y4o,w4o,r2,Aoe,A4o,L4o,i$,B4o,x4o,k4o,t2,Loe,R4o,S4o,d$,P4o,$4o,I4o,a2,Boe,j4o,N4o,c$,D4o,q4o,G4o,s2,xoe,O4o,X4o,m$,z4o,V4o,W4o,n2,koe,Q4o,H4o,f$,U4o,J4o,Y4o,l2,Roe,K4o,Z4o,Soe,eMo,oMo,rMo,i2,Poe,tMo,aMo,g$,sMo,nMo,lMo,d2,$oe,iMo,dMo,h$,cMo,mMo,fMo,c2,Ioe,gMo,hMo,u$,uMo,pMo,_Mo,m2,joe,bMo,vMo,p$,TMo,FMo,CMo,f2,Noe,MMo,EMo,_$,yMo,wMo,AMo,g2,Doe,LMo,BMo,b$,xMo,kMo,RMo,h2,qoe,SMo,PMo,v$,$Mo,IMo,jMo,u2,Goe,NMo,DMo,T$,qMo,GMo,OMo,p2,Ooe,XMo,zMo,F$,VMo,WMo,QMo,_2,HMo,Xoe,UMo,JMo,zoe,YMo,KMo,Voe,ZMo,eEo,K5,DLe,Ki,b2,Woe,Z5,oEo,Qoe,rEo,qLe,Uo,ey,tEo,Zi,aEo,Hoe,sEo,nEo,Uoe,lEo,iEo,dEo,oy,cEo,Joe,mEo,fEo,gEo,Gr,ry,hEo,Yoe,uEo,pEo,ed,_Eo,Koe,bEo,vEo,Zoe,TEo,FEo,CEo,ere,MEo,EEo,ty,yEo,je,ay,wEo,ore,AEo,LEo,Oa,BEo,rre,xEo,kEo,tre,REo,SEo,are,PEo,$Eo,IEo,oa,v2,sre,jEo,NEo,C$,DEo,qEo,GEo,T2,nre,OEo,XEo,M$,zEo,VEo,WEo,F2,lre,QEo,HEo,E$,UEo,JEo,YEo,C2,ire,KEo,ZEo,y$,e3o,o3o,r3o,M2,dre,t3o,a3o,cre,s3o,n3o,l3o,E2,i3o,mre,d3o,c3o,fre,m3o,f3o,gre,g3o,h3o,sy,GLe,od,y2,hre,ny,u3o,ure,p3o,OLe,Jo,ly,_3o,rd,b3o,pre,v3o,T3o,_re,F3o,C3o,M3o,iy,E3o,bre,y3o,w3o,A3o,Or,dy,L3o,vre,B3o,x3o,td,k3o,Tre,R3o,S3o,Fre,P3o,$3o,I3o,Cre,j3o,N3o,cy,D3o,Ne,my,q3o,Mre,G3o,O3o,Xa,X3o,Ere,z3o,V3o,yre,W3o,Q3o,wre,H3o,U3o,J3o,N,w2,Are,Y3o,K3o,w$,Z3o,e5o,o5o,A2,Lre,r5o,t5o,A$,a5o,s5o,n5o,L2,Bre,l5o,i5o,L$,d5o,c5o,m5o,B2,xre,f5o,g5o,B$,h5o,u5o,p5o,x2,kre,_5o,b5o,x$,v5o,T5o,F5o,k2,Rre,C5o,M5o,k$,E5o,y5o,w5o,R2,Sre,A5o,L5o,R$,B5o,x5o,k5o,S2,Pre,R5o,S5o,S$,P5o,$5o,I5o,P2,$re,j5o,N5o,P$,D5o,q5o,G5o,$2,Ire,O5o,X5o,$$,z5o,V5o,W5o,I2,jre,Q5o,H5o,I$,U5o,J5o,Y5o,j2,Nre,K5o,Z5o,j$,eyo,oyo,ryo,N2,Dre,tyo,ayo,N$,syo,nyo,lyo,D2,qre,iyo,dyo,D$,cyo,myo,fyo,q2,Gre,gyo,hyo,q$,uyo,pyo,_yo,G2,Ore,byo,vyo,G$,Tyo,Fyo,Cyo,O2,Xre,Myo,Eyo,O$,yyo,wyo,Ayo,X2,zre,Lyo,Byo,X$,xyo,kyo,Ryo,z2,Vre,Syo,Pyo,z$,$yo,Iyo,jyo,V2,Wre,Nyo,Dyo,V$,qyo,Gyo,Oyo,W2,Qre,Xyo,zyo,W$,Vyo,Wyo,Qyo,Q2,Hre,Hyo,Uyo,Q$,Jyo,Yyo,Kyo,H2,Ure,Zyo,ewo,Jre,owo,rwo,two,U2,Yre,awo,swo,H$,nwo,lwo,iwo,J2,Kre,dwo,cwo,U$,mwo,fwo,gwo,Y2,Zre,hwo,uwo,J$,pwo,_wo,bwo,K2,ete,vwo,Two,Y$,Fwo,Cwo,Mwo,Z2,ote,Ewo,ywo,K$,wwo,Awo,Lwo,ev,rte,Bwo,xwo,Z$,kwo,Rwo,Swo,ov,tte,Pwo,$wo,eI,Iwo,jwo,Nwo,rv,ate,Dwo,qwo,oI,Gwo,Owo,Xwo,tv,ste,zwo,Vwo,rI,Wwo,Qwo,Hwo,av,Uwo,nte,Jwo,Ywo,lte,Kwo,Zwo,ite,eAo,oAo,fy,XLe,ad,sv,dte,gy,rAo,cte,tAo,zLe,Yo,hy,aAo,sd,sAo,mte,nAo,lAo,fte,iAo,dAo,cAo,uy,mAo,gte,fAo,gAo,hAo,Xr,py,uAo,hte,pAo,_Ao,nd,bAo,ute,vAo,TAo,pte,FAo,CAo,MAo,_te,EAo,yAo,_y,wAo,De,by,AAo,bte,LAo,BAo,za,xAo,vte,kAo,RAo,Tte,SAo,PAo,Fte,$Ao,IAo,jAo,R,nv,Cte,NAo,DAo,tI,qAo,GAo,OAo,lv,Mte,XAo,zAo,aI,VAo,WAo,QAo,iv,Ete,HAo,UAo,sI,JAo,YAo,KAo,dv,yte,ZAo,e0o,nI,o0o,r0o,t0o,cv,wte,a0o,s0o,lI,n0o,l0o,i0o,mv,Ate,d0o,c0o,iI,m0o,f0o,g0o,fv,Lte,h0o,u0o,dI,p0o,_0o,b0o,gv,Bte,v0o,T0o,cI,F0o,C0o,M0o,hv,xte,E0o,y0o,mI,w0o,A0o,L0o,uv,kte,B0o,x0o,fI,k0o,R0o,S0o,pv,Rte,P0o,$0o,gI,I0o,j0o,N0o,_v,Ste,D0o,q0o,hI,G0o,O0o,X0o,bv,Pte,z0o,V0o,uI,W0o,Q0o,H0o,vv,$te,U0o,J0o,pI,Y0o,K0o,Z0o,Tv,Ite,e6o,o6o,_I,r6o,t6o,a6o,Fv,jte,s6o,n6o,bI,l6o,i6o,d6o,Cv,Nte,c6o,m6o,vI,f6o,g6o,h6o,Mv,Dte,u6o,p6o,TI,_6o,b6o,v6o,Ev,qte,T6o,F6o,FI,C6o,M6o,E6o,yv,Gte,y6o,w6o,CI,A6o,L6o,B6o,wv,Ote,x6o,k6o,MI,R6o,S6o,P6o,Av,Xte,$6o,I6o,EI,j6o,N6o,D6o,Lv,zte,q6o,G6o,yI,O6o,X6o,z6o,Bv,Vte,V6o,W6o,wI,Q6o,H6o,U6o,xv,Wte,J6o,Y6o,AI,K6o,Z6o,eLo,kv,Qte,oLo,rLo,LI,tLo,aLo,sLo,Rv,Hte,nLo,lLo,Ute,iLo,dLo,cLo,Sv,Jte,mLo,fLo,BI,gLo,hLo,uLo,Pv,Yte,pLo,_Lo,xI,bLo,vLo,TLo,$v,Kte,FLo,CLo,kI,MLo,ELo,yLo,Iv,Zte,wLo,ALo,RI,LLo,BLo,xLo,jv,eae,kLo,RLo,SI,SLo,PLo,$Lo,Nv,oae,ILo,jLo,PI,NLo,DLo,qLo,Dv,rae,GLo,OLo,$I,XLo,zLo,VLo,qv,tae,WLo,QLo,II,HLo,ULo,JLo,Gv,aae,YLo,KLo,jI,ZLo,e7o,o7o,Ov,sae,r7o,t7o,NI,a7o,s7o,n7o,Xv,nae,l7o,i7o,DI,d7o,c7o,m7o,zv,f7o,lae,g7o,h7o,iae,u7o,p7o,dae,_7o,b7o,vy,VLe,ld,Vv,cae,Ty,v7o,mae,T7o,WLe,Ko,Fy,F7o,id,C7o,fae,M7o,E7o,gae,y7o,w7o,A7o,Cy,L7o,hae,B7o,x7o,k7o,zr,My,R7o,uae,S7o,P7o,dd,$7o,pae,I7o,j7o,_ae,N7o,D7o,q7o,bae,G7o,O7o,Ey,X7o,qe,yy,z7o,vae,V7o,W7o,Va,Q7o,Tae,H7o,U7o,Fae,J7o,Y7o,Cae,K7o,Z7o,e8o,Mae,Wv,Eae,o8o,r8o,qI,t8o,a8o,s8o,Qv,n8o,yae,l8o,i8o,wae,d8o,c8o,Aae,m8o,f8o,wy,QLe,cd,Hv,Lae,Ay,g8o,Bae,h8o,HLe,Zo,Ly,u8o,md,p8o,xae,_8o,b8o,kae,v8o,T8o,F8o,By,C8o,Rae,M8o,E8o,y8o,Vr,xy,w8o,Sae,A8o,L8o,fd,B8o,Pae,x8o,k8o,$ae,R8o,S8o,P8o,Iae,$8o,I8o,ky,j8o,Ge,Ry,N8o,jae,D8o,q8o,Wa,G8o,Nae,O8o,X8o,Dae,z8o,V8o,qae,W8o,Q8o,H8o,we,Uv,Gae,U8o,J8o,GI,Y8o,K8o,Z8o,Jv,Oae,e9o,o9o,OI,r9o,t9o,a9o,An,Xae,s9o,n9o,XI,l9o,i9o,zI,d9o,c9o,m9o,Yv,zae,f9o,g9o,VI,h9o,u9o,p9o,ta,Vae,_9o,b9o,WI,v9o,T9o,QI,F9o,C9o,HI,M9o,E9o,y9o,Kv,Wae,w9o,A9o,UI,L9o,B9o,x9o,Zv,Qae,k9o,R9o,JI,S9o,P9o,$9o,eT,Hae,I9o,j9o,YI,N9o,D9o,q9o,oT,G9o,Uae,O9o,X9o,Jae,z9o,V9o,Yae,W9o,Q9o,Sy,ULe,gd,rT,Kae,Py,H9o,Zae,U9o,JLe,er,$y,J9o,hd,Y9o,ese,K9o,Z9o,ose,eBo,oBo,rBo,Iy,tBo,rse,aBo,sBo,nBo,Wr,jy,lBo,tse,iBo,dBo,ud,cBo,ase,mBo,fBo,sse,gBo,hBo,uBo,nse,pBo,_Bo,Ny,bBo,Oe,Dy,vBo,lse,TBo,FBo,Qa,CBo,ise,MBo,EBo,dse,yBo,wBo,cse,ABo,LBo,BBo,mse,tT,fse,xBo,kBo,KI,RBo,SBo,PBo,aT,$Bo,gse,IBo,jBo,hse,NBo,DBo,use,qBo,GBo,qy,YLe,pd,sT,pse,Gy,OBo,_se,XBo,KLe,or,Oy,zBo,_d,VBo,bse,WBo,QBo,vse,HBo,UBo,JBo,Xy,YBo,Tse,KBo,ZBo,exo,Qr,zy,oxo,Fse,rxo,txo,bd,axo,Cse,sxo,nxo,Mse,lxo,ixo,dxo,Ese,cxo,mxo,Vy,fxo,Xe,Wy,gxo,yse,hxo,uxo,Ha,pxo,wse,_xo,bxo,Ase,vxo,Txo,Lse,Fxo,Cxo,Mxo,ro,nT,Bse,Exo,yxo,ZI,wxo,Axo,Lxo,lT,xse,Bxo,xxo,ej,kxo,Rxo,Sxo,iT,kse,Pxo,$xo,oj,Ixo,jxo,Nxo,dT,Rse,Dxo,qxo,rj,Gxo,Oxo,Xxo,cT,Sse,zxo,Vxo,tj,Wxo,Qxo,Hxo,mT,Pse,Uxo,Jxo,aj,Yxo,Kxo,Zxo,fT,$se,eko,oko,sj,rko,tko,ako,gT,sko,Ise,nko,lko,jse,iko,dko,Nse,cko,mko,Qy,ZLe,vd,hT,Dse,Hy,fko,qse,gko,e7e,rr,Uy,hko,Td,uko,Gse,pko,_ko,Ose,bko,vko,Tko,Jy,Fko,Xse,Cko,Mko,Eko,Hr,Yy,yko,zse,wko,Ako,Fd,Lko,Vse,Bko,xko,Wse,kko,Rko,Sko,Qse,Pko,$ko,Ky,Iko,ze,Zy,jko,Hse,Nko,Dko,Ua,qko,Use,Gko,Oko,Jse,Xko,zko,Yse,Vko,Wko,Qko,Cd,uT,Kse,Hko,Uko,nj,Jko,Yko,Kko,pT,Zse,Zko,eRo,lj,oRo,rRo,tRo,_T,ene,aRo,sRo,ij,nRo,lRo,iRo,bT,dRo,one,cRo,mRo,rne,fRo,gRo,tne,hRo,uRo,ew,o7e,Md,vT,ane,ow,pRo,sne,_Ro,r7e,tr,rw,bRo,Ed,vRo,nne,TRo,FRo,lne,CRo,MRo,ERo,tw,yRo,ine,wRo,ARo,LRo,Ur,aw,BRo,dne,xRo,kRo,yd,RRo,cne,SRo,PRo,mne,$Ro,IRo,jRo,fne,NRo,DRo,sw,qRo,Ve,nw,GRo,gne,ORo,XRo,Ja,zRo,hne,VRo,WRo,une,QRo,HRo,pne,URo,JRo,YRo,to,TT,_ne,KRo,ZRo,dj,eSo,oSo,rSo,FT,bne,tSo,aSo,cj,sSo,nSo,lSo,CT,vne,iSo,dSo,mj,cSo,mSo,fSo,MT,Tne,gSo,hSo,fj,uSo,pSo,_So,ET,Fne,bSo,vSo,gj,TSo,FSo,CSo,yT,Cne,MSo,ESo,hj,ySo,wSo,ASo,wT,Mne,LSo,BSo,uj,xSo,kSo,RSo,AT,SSo,Ene,PSo,$So,yne,ISo,jSo,wne,NSo,DSo,lw,t7e,wd,LT,Ane,iw,qSo,Lne,GSo,a7e,ar,dw,OSo,Ad,XSo,Bne,zSo,VSo,xne,WSo,QSo,HSo,cw,USo,kne,JSo,YSo,KSo,Jr,mw,ZSo,Rne,ePo,oPo,Ld,rPo,Sne,tPo,aPo,Pne,sPo,nPo,lPo,$ne,iPo,dPo,fw,cPo,We,gw,mPo,Ine,fPo,gPo,Ya,hPo,jne,uPo,pPo,Nne,_Po,bPo,Dne,vPo,TPo,FPo,hw,BT,qne,CPo,MPo,pj,EPo,yPo,wPo,xT,Gne,APo,LPo,_j,BPo,xPo,kPo,kT,RPo,One,SPo,PPo,Xne,$Po,IPo,zne,jPo,NPo,uw,s7e,Bd,RT,Vne,pw,DPo,Wne,qPo,n7e,sr,_w,GPo,xd,OPo,Qne,XPo,zPo,Hne,VPo,WPo,QPo,bw,HPo,Une,UPo,JPo,YPo,Yr,vw,KPo,Jne,ZPo,e$o,kd,o$o,Yne,r$o,t$o,Kne,a$o,s$o,n$o,Zne,l$o,i$o,Tw,d$o,Qe,Fw,c$o,ele,m$o,f$o,Ka,g$o,ole,h$o,u$o,rle,p$o,_$o,tle,b$o,v$o,T$o,Rd,ST,ale,F$o,C$o,bj,M$o,E$o,y$o,PT,sle,w$o,A$o,vj,L$o,B$o,x$o,$T,nle,k$o,R$o,Tj,S$o,P$o,$$o,IT,I$o,lle,j$o,N$o,ile,D$o,q$o,dle,G$o,O$o,Cw,l7e,Sd,jT,cle,Mw,X$o,mle,z$o,i7e,nr,Ew,V$o,Pd,W$o,fle,Q$o,H$o,gle,U$o,J$o,Y$o,yw,K$o,hle,Z$o,eIo,oIo,Kr,ww,rIo,ule,tIo,aIo,$d,sIo,ple,nIo,lIo,_le,iIo,dIo,cIo,ble,mIo,fIo,Aw,gIo,He,Lw,hIo,vle,uIo,pIo,Za,_Io,Tle,bIo,vIo,Fle,TIo,FIo,Cle,CIo,MIo,EIo,Mle,NT,Ele,yIo,wIo,Fj,AIo,LIo,BIo,DT,xIo,yle,kIo,RIo,wle,SIo,PIo,Ale,$Io,IIo,Bw,d7e,Id,qT,Lle,xw,jIo,Ble,NIo,c7e,lr,kw,DIo,jd,qIo,xle,GIo,OIo,kle,XIo,zIo,VIo,Rw,WIo,Rle,QIo,HIo,UIo,Zr,Sw,JIo,Sle,YIo,KIo,Nd,ZIo,Ple,ejo,ojo,$le,rjo,tjo,ajo,Ile,sjo,njo,Pw,ljo,Ue,$w,ijo,jle,djo,cjo,es,mjo,Nle,fjo,gjo,Dle,hjo,ujo,qle,pjo,_jo,bjo,Gle,GT,Ole,vjo,Tjo,Cj,Fjo,Cjo,Mjo,OT,Ejo,Xle,yjo,wjo,zle,Ajo,Ljo,Vle,Bjo,xjo,Iw,m7e,Dd,XT,Wle,jw,kjo,Qle,Rjo,f7e,ir,Nw,Sjo,qd,Pjo,Hle,$jo,Ijo,Ule,jjo,Njo,Djo,Dw,qjo,Jle,Gjo,Ojo,Xjo,et,qw,zjo,Yle,Vjo,Wjo,Gd,Qjo,Kle,Hjo,Ujo,Zle,Jjo,Yjo,Kjo,eie,Zjo,eNo,Gw,oNo,Je,Ow,rNo,oie,tNo,aNo,os,sNo,rie,nNo,lNo,tie,iNo,dNo,aie,cNo,mNo,fNo,Xw,zT,sie,gNo,hNo,Mj,uNo,pNo,_No,VT,nie,bNo,vNo,Ej,TNo,FNo,CNo,WT,MNo,lie,ENo,yNo,iie,wNo,ANo,die,LNo,BNo,zw,g7e,Od,QT,cie,Vw,xNo,mie,kNo,h7e,dr,Ww,RNo,Xd,SNo,fie,PNo,$No,gie,INo,jNo,NNo,Qw,DNo,hie,qNo,GNo,ONo,ot,Hw,XNo,uie,zNo,VNo,zd,WNo,pie,QNo,HNo,_ie,UNo,JNo,YNo,bie,KNo,ZNo,Uw,eDo,mo,Jw,oDo,vie,rDo,tDo,rs,aDo,Tie,sDo,nDo,Fie,lDo,iDo,Cie,dDo,cDo,mDo,B,HT,Mie,fDo,gDo,yj,hDo,uDo,pDo,UT,Eie,_Do,bDo,wj,vDo,TDo,FDo,JT,yie,CDo,MDo,Aj,EDo,yDo,wDo,YT,wie,ADo,LDo,Lj,BDo,xDo,kDo,KT,Aie,RDo,SDo,Bj,PDo,$Do,IDo,ZT,Lie,jDo,NDo,xj,DDo,qDo,GDo,e1,Bie,ODo,XDo,kj,zDo,VDo,WDo,o1,xie,QDo,HDo,Rj,UDo,JDo,YDo,r1,kie,KDo,ZDo,Sj,eqo,oqo,rqo,t1,Rie,tqo,aqo,Pj,sqo,nqo,lqo,a1,Sie,iqo,dqo,$j,cqo,mqo,fqo,s1,Pie,gqo,hqo,Ij,uqo,pqo,_qo,n1,$ie,bqo,vqo,jj,Tqo,Fqo,Cqo,l1,Iie,Mqo,Eqo,Nj,yqo,wqo,Aqo,i1,jie,Lqo,Bqo,Dj,xqo,kqo,Rqo,Ln,Nie,Sqo,Pqo,qj,$qo,Iqo,Gj,jqo,Nqo,Dqo,d1,Die,qqo,Gqo,Oj,Oqo,Xqo,zqo,c1,qie,Vqo,Wqo,Xj,Qqo,Hqo,Uqo,m1,Gie,Jqo,Yqo,zj,Kqo,Zqo,eGo,f1,Oie,oGo,rGo,Vj,tGo,aGo,sGo,g1,Xie,nGo,lGo,Wj,iGo,dGo,cGo,h1,zie,mGo,fGo,Qj,gGo,hGo,uGo,u1,Vie,pGo,_Go,Hj,bGo,vGo,TGo,p1,Wie,FGo,CGo,Uj,MGo,EGo,yGo,_1,Qie,wGo,AGo,Jj,LGo,BGo,xGo,b1,Hie,kGo,RGo,Yj,SGo,PGo,$Go,v1,Uie,IGo,jGo,Kj,NGo,DGo,qGo,T1,Jie,GGo,OGo,Zj,XGo,zGo,VGo,F1,Yie,WGo,QGo,eN,HGo,UGo,JGo,C1,Kie,YGo,KGo,oN,ZGo,eOo,oOo,M1,Zie,rOo,tOo,rN,aOo,sOo,nOo,E1,ede,lOo,iOo,tN,dOo,cOo,mOo,y1,ode,fOo,gOo,aN,hOo,uOo,pOo,w1,rde,_Oo,bOo,sN,vOo,TOo,FOo,A1,tde,COo,MOo,nN,EOo,yOo,wOo,L1,ade,AOo,LOo,lN,BOo,xOo,kOo,B1,sde,ROo,SOo,iN,POo,$Oo,IOo,x1,nde,jOo,NOo,dN,DOo,qOo,GOo,k1,lde,OOo,XOo,cN,zOo,VOo,WOo,R1,ide,QOo,HOo,mN,UOo,JOo,YOo,S1,dde,KOo,ZOo,fN,eXo,oXo,rXo,cde,tXo,aXo,Yw,u7e,Vd,P1,mde,Kw,sXo,fde,nXo,p7e,cr,Zw,lXo,Wd,iXo,gde,dXo,cXo,hde,mXo,fXo,gXo,eA,hXo,ude,uXo,pXo,_Xo,rt,oA,bXo,pde,vXo,TXo,Qd,FXo,_de,CXo,MXo,bde,EXo,yXo,wXo,vde,AXo,LXo,rA,BXo,fo,tA,xXo,Tde,kXo,RXo,ts,SXo,Fde,PXo,$Xo,Cde,IXo,jXo,Mde,NXo,DXo,qXo,H,$1,Ede,GXo,OXo,gN,XXo,zXo,VXo,I1,yde,WXo,QXo,hN,HXo,UXo,JXo,j1,wde,YXo,KXo,uN,ZXo,ezo,ozo,N1,Ade,rzo,tzo,pN,azo,szo,nzo,D1,Lde,lzo,izo,_N,dzo,czo,mzo,q1,Bde,fzo,gzo,bN,hzo,uzo,pzo,G1,xde,_zo,bzo,vN,vzo,Tzo,Fzo,O1,kde,Czo,Mzo,TN,Ezo,yzo,wzo,X1,Rde,Azo,Lzo,FN,Bzo,xzo,kzo,z1,Sde,Rzo,Szo,CN,Pzo,$zo,Izo,V1,Pde,jzo,Nzo,MN,Dzo,qzo,Gzo,W1,$de,Ozo,Xzo,EN,zzo,Vzo,Wzo,Q1,Ide,Qzo,Hzo,yN,Uzo,Jzo,Yzo,H1,jde,Kzo,Zzo,wN,eVo,oVo,rVo,U1,Nde,tVo,aVo,AN,sVo,nVo,lVo,J1,Dde,iVo,dVo,LN,cVo,mVo,fVo,Y1,qde,gVo,hVo,BN,uVo,pVo,_Vo,K1,Gde,bVo,vVo,xN,TVo,FVo,CVo,Z1,Ode,MVo,EVo,kN,yVo,wVo,AVo,eF,Xde,LVo,BVo,RN,xVo,kVo,RVo,oF,zde,SVo,PVo,SN,$Vo,IVo,jVo,rF,Vde,NVo,DVo,PN,qVo,GVo,OVo,Wde,XVo,zVo,aA,_7e,Hd,tF,Qde,sA,VVo,Hde,WVo,b7e,mr,nA,QVo,Ud,HVo,Ude,UVo,JVo,Jde,YVo,KVo,ZVo,lA,eWo,Yde,oWo,rWo,tWo,tt,iA,aWo,Kde,sWo,nWo,Jd,lWo,Zde,iWo,dWo,ece,cWo,mWo,fWo,oce,gWo,hWo,dA,uWo,go,cA,pWo,rce,_Wo,bWo,as,vWo,tce,TWo,FWo,ace,CWo,MWo,sce,EWo,yWo,wWo,he,aF,nce,AWo,LWo,$N,BWo,xWo,kWo,sF,lce,RWo,SWo,IN,PWo,$Wo,IWo,nF,ice,jWo,NWo,jN,DWo,qWo,GWo,lF,dce,OWo,XWo,NN,zWo,VWo,WWo,iF,cce,QWo,HWo,DN,UWo,JWo,YWo,dF,mce,KWo,ZWo,qN,eQo,oQo,rQo,cF,fce,tQo,aQo,GN,sQo,nQo,lQo,mF,gce,iQo,dQo,ON,cQo,mQo,fQo,fF,hce,gQo,hQo,XN,uQo,pQo,_Qo,gF,uce,bQo,vQo,zN,TQo,FQo,CQo,pce,MQo,EQo,mA,v7e,Yd,hF,_ce,fA,yQo,bce,wQo,T7e,fr,gA,AQo,Kd,LQo,vce,BQo,xQo,Tce,kQo,RQo,SQo,hA,PQo,Fce,$Qo,IQo,jQo,at,uA,NQo,Cce,DQo,qQo,Zd,GQo,Mce,OQo,XQo,Ece,zQo,VQo,WQo,yce,QQo,HQo,pA,UQo,ho,_A,JQo,wce,YQo,KQo,ss,ZQo,Ace,eHo,oHo,Lce,rHo,tHo,Bce,aHo,sHo,nHo,xce,uF,kce,lHo,iHo,VN,dHo,cHo,mHo,Rce,fHo,gHo,bA,F7e,ec,pF,Sce,vA,hHo,Pce,uHo,C7e,gr,TA,pHo,oc,_Ho,$ce,bHo,vHo,Ice,THo,FHo,CHo,FA,MHo,jce,EHo,yHo,wHo,st,CA,AHo,Nce,LHo,BHo,rc,xHo,Dce,kHo,RHo,qce,SHo,PHo,$Ho,Gce,IHo,jHo,MA,NHo,uo,EA,DHo,Oce,qHo,GHo,ns,OHo,Xce,XHo,zHo,zce,VHo,WHo,Vce,QHo,HHo,UHo,Y,_F,Wce,JHo,YHo,WN,KHo,ZHo,eUo,bF,Qce,oUo,rUo,QN,tUo,aUo,sUo,vF,Hce,nUo,lUo,HN,iUo,dUo,cUo,TF,Uce,mUo,fUo,UN,gUo,hUo,uUo,FF,Jce,pUo,_Uo,JN,bUo,vUo,TUo,CF,Yce,FUo,CUo,YN,MUo,EUo,yUo,MF,Kce,wUo,AUo,KN,LUo,BUo,xUo,EF,Zce,kUo,RUo,ZN,SUo,PUo,$Uo,yF,eme,IUo,jUo,eD,NUo,DUo,qUo,wF,ome,GUo,OUo,oD,XUo,zUo,VUo,AF,rme,WUo,QUo,rD,HUo,UUo,JUo,LF,tme,YUo,KUo,tD,ZUo,eJo,oJo,BF,ame,rJo,tJo,aD,aJo,sJo,nJo,xF,sme,lJo,iJo,sD,dJo,cJo,mJo,kF,nme,fJo,gJo,nD,hJo,uJo,pJo,RF,lme,_Jo,bJo,lD,vJo,TJo,FJo,SF,ime,CJo,MJo,iD,EJo,yJo,wJo,PF,dme,AJo,LJo,dD,BJo,xJo,kJo,$F,cme,RJo,SJo,cD,PJo,$Jo,IJo,IF,mme,jJo,NJo,mD,DJo,qJo,GJo,fme,OJo,XJo,yA,M7e,tc,jF,gme,wA,zJo,hme,VJo,E7e,hr,AA,WJo,ac,QJo,ume,HJo,UJo,pme,JJo,YJo,KJo,LA,ZJo,_me,eYo,oYo,rYo,nt,BA,tYo,bme,aYo,sYo,sc,nYo,vme,lYo,iYo,Tme,dYo,cYo,mYo,Fme,fYo,gYo,xA,hYo,po,kA,uYo,Cme,pYo,_Yo,ls,bYo,Mme,vYo,TYo,Eme,FYo,CYo,yme,MYo,EYo,yYo,ue,NF,wme,wYo,AYo,fD,LYo,BYo,xYo,DF,Ame,kYo,RYo,gD,SYo,PYo,$Yo,qF,Lme,IYo,jYo,hD,NYo,DYo,qYo,GF,Bme,GYo,OYo,uD,XYo,zYo,VYo,OF,xme,WYo,QYo,pD,HYo,UYo,JYo,XF,kme,YYo,KYo,_D,ZYo,eKo,oKo,zF,Rme,rKo,tKo,bD,aKo,sKo,nKo,VF,Sme,lKo,iKo,vD,dKo,cKo,mKo,WF,Pme,fKo,gKo,TD,hKo,uKo,pKo,QF,$me,_Ko,bKo,FD,vKo,TKo,FKo,Ime,CKo,MKo,RA,y7e,nc,HF,jme,SA,EKo,Nme,yKo,w7e,ur,PA,wKo,lc,AKo,Dme,LKo,BKo,qme,xKo,kKo,RKo,$A,SKo,Gme,PKo,$Ko,IKo,lt,IA,jKo,Ome,NKo,DKo,ic,qKo,Xme,GKo,OKo,zme,XKo,zKo,VKo,Vme,WKo,QKo,jA,HKo,_o,NA,UKo,Wme,JKo,YKo,is,KKo,Qme,ZKo,eZo,Hme,oZo,rZo,Ume,tZo,aZo,sZo,X,UF,Jme,nZo,lZo,CD,iZo,dZo,cZo,JF,Yme,mZo,fZo,MD,gZo,hZo,uZo,YF,Kme,pZo,_Zo,ED,bZo,vZo,TZo,KF,Zme,FZo,CZo,yD,MZo,EZo,yZo,ZF,efe,wZo,AZo,wD,LZo,BZo,xZo,eC,ofe,kZo,RZo,AD,SZo,PZo,$Zo,oC,rfe,IZo,jZo,LD,NZo,DZo,qZo,rC,tfe,GZo,OZo,BD,XZo,zZo,VZo,tC,afe,WZo,QZo,xD,HZo,UZo,JZo,aC,sfe,YZo,KZo,kD,ZZo,eer,oer,sC,nfe,rer,ter,RD,aer,ser,ner,nC,lfe,ler,ier,SD,der,cer,mer,lC,ife,fer,ger,PD,her,uer,per,iC,dfe,_er,ber,$D,ver,Ter,Fer,dC,cfe,Cer,Mer,ID,Eer,yer,wer,cC,mfe,Aer,Ler,jD,Ber,xer,ker,mC,ffe,Rer,Ser,ND,Per,$er,Ier,fC,gfe,jer,Ner,DD,Der,qer,Ger,gC,hfe,Oer,Xer,qD,zer,Ver,Wer,hC,ufe,Qer,Her,GD,Uer,Jer,Yer,uC,pfe,Ker,Zer,OD,eor,oor,ror,pC,_fe,tor,aor,XD,sor,nor,lor,_C,bfe,ior,dor,zD,cor,mor,gor,bC,vfe,hor,uor,VD,por,_or,bor,vC,Tfe,vor,Tor,WD,For,Cor,Mor,Ffe,Eor,yor,DA,A7e,dc,TC,Cfe,qA,wor,Mfe,Aor,L7e,pr,GA,Lor,cc,Bor,Efe,xor,kor,yfe,Ror,Sor,Por,OA,$or,wfe,Ior,jor,Nor,it,XA,Dor,Afe,qor,Gor,mc,Oor,Lfe,Xor,zor,Bfe,Vor,Wor,Qor,xfe,Hor,Uor,zA,Jor,bo,VA,Yor,kfe,Kor,Zor,ds,err,Rfe,orr,rrr,Sfe,trr,arr,Pfe,srr,nrr,lrr,te,FC,$fe,irr,drr,QD,crr,mrr,frr,CC,Ife,grr,hrr,HD,urr,prr,_rr,MC,jfe,brr,vrr,UD,Trr,Frr,Crr,EC,Nfe,Mrr,Err,JD,yrr,wrr,Arr,yC,Dfe,Lrr,Brr,YD,xrr,krr,Rrr,wC,qfe,Srr,Prr,KD,$rr,Irr,jrr,AC,Gfe,Nrr,Drr,ZD,qrr,Grr,Orr,LC,Ofe,Xrr,zrr,eq,Vrr,Wrr,Qrr,BC,Xfe,Hrr,Urr,oq,Jrr,Yrr,Krr,xC,zfe,Zrr,etr,rq,otr,rtr,ttr,kC,Vfe,atr,str,tq,ntr,ltr,itr,RC,Wfe,dtr,ctr,aq,mtr,ftr,gtr,SC,Qfe,htr,utr,sq,ptr,_tr,btr,PC,Hfe,vtr,Ttr,nq,Ftr,Ctr,Mtr,$C,Ufe,Etr,ytr,lq,wtr,Atr,Ltr,IC,Jfe,Btr,xtr,iq,ktr,Rtr,Str,jC,Yfe,Ptr,$tr,dq,Itr,jtr,Ntr,Kfe,Dtr,qtr,WA,B7e,fc,NC,Zfe,QA,Gtr,ege,Otr,x7e,_r,HA,Xtr,gc,ztr,oge,Vtr,Wtr,rge,Qtr,Htr,Utr,UA,Jtr,tge,Ytr,Ktr,Ztr,dt,JA,ear,age,oar,rar,hc,tar,sge,aar,sar,nge,nar,lar,iar,lge,dar,car,YA,mar,vo,KA,far,ige,gar,har,cs,uar,dge,par,_ar,cge,bar,Tar,mge,Far,Car,Mar,fge,DC,gge,Ear,yar,cq,war,Aar,Lar,hge,Bar,xar,ZA,k7e,uc,qC,uge,e0,kar,pge,Rar,R7e,br,o0,Sar,pc,Par,_ge,$ar,Iar,bge,jar,Nar,Dar,r0,qar,vge,Gar,Oar,Xar,ct,t0,zar,Tge,Var,War,_c,Qar,Fge,Har,Uar,Cge,Jar,Yar,Kar,Mge,Zar,esr,a0,osr,To,s0,rsr,Ege,tsr,asr,ms,ssr,yge,nsr,lsr,wge,isr,dsr,Age,csr,msr,fsr,K,GC,Lge,gsr,hsr,mq,usr,psr,_sr,OC,Bge,bsr,vsr,fq,Tsr,Fsr,Csr,XC,xge,Msr,Esr,gq,ysr,wsr,Asr,zC,kge,Lsr,Bsr,hq,xsr,ksr,Rsr,VC,Rge,Ssr,Psr,uq,$sr,Isr,jsr,WC,Sge,Nsr,Dsr,pq,qsr,Gsr,Osr,QC,Pge,Xsr,zsr,_q,Vsr,Wsr,Qsr,HC,$ge,Hsr,Usr,bq,Jsr,Ysr,Ksr,UC,Ige,Zsr,enr,vq,onr,rnr,tnr,JC,jge,anr,snr,Tq,nnr,lnr,inr,YC,Nge,dnr,cnr,Fq,mnr,fnr,gnr,KC,Dge,hnr,unr,Cq,pnr,_nr,bnr,ZC,qge,vnr,Tnr,Mq,Fnr,Cnr,Mnr,e4,Gge,Enr,ynr,Eq,wnr,Anr,Lnr,o4,Oge,Bnr,xnr,yq,knr,Rnr,Snr,r4,Xge,Pnr,$nr,wq,Inr,jnr,Nnr,t4,zge,Dnr,qnr,Aq,Gnr,Onr,Xnr,a4,Vge,znr,Vnr,Lq,Wnr,Qnr,Hnr,s4,Wge,Unr,Jnr,Bq,Ynr,Knr,Znr,n4,Qge,elr,olr,xq,rlr,tlr,alr,Hge,slr,nlr,n0,S7e,bc,l4,Uge,l0,llr,Jge,ilr,P7e,vr,i0,dlr,vc,clr,Yge,mlr,flr,Kge,glr,hlr,ulr,d0,plr,Zge,_lr,blr,vlr,mt,c0,Tlr,ehe,Flr,Clr,Tc,Mlr,ohe,Elr,ylr,rhe,wlr,Alr,Llr,the,Blr,xlr,m0,klr,Fo,f0,Rlr,ahe,Slr,Plr,fs,$lr,she,Ilr,jlr,nhe,Nlr,Dlr,lhe,qlr,Glr,Olr,Z,i4,ihe,Xlr,zlr,kq,Vlr,Wlr,Qlr,d4,dhe,Hlr,Ulr,Rq,Jlr,Ylr,Klr,c4,che,Zlr,eir,Sq,oir,rir,tir,m4,mhe,air,sir,Pq,nir,lir,iir,f4,fhe,dir,cir,$q,mir,fir,gir,g4,ghe,hir,uir,Iq,pir,_ir,bir,h4,hhe,vir,Tir,jq,Fir,Cir,Mir,u4,uhe,Eir,yir,Nq,wir,Air,Lir,p4,phe,Bir,xir,Dq,kir,Rir,Sir,_4,_he,Pir,$ir,qq,Iir,jir,Nir,b4,bhe,Dir,qir,Gq,Gir,Oir,Xir,v4,vhe,zir,Vir,Oq,Wir,Qir,Hir,T4,The,Uir,Jir,Xq,Yir,Kir,Zir,F4,Fhe,edr,odr,zq,rdr,tdr,adr,C4,Che,sdr,ndr,Vq,ldr,idr,ddr,M4,Mhe,cdr,mdr,Wq,fdr,gdr,hdr,E4,Ehe,udr,pdr,Qq,_dr,bdr,vdr,y4,yhe,Tdr,Fdr,Hq,Cdr,Mdr,Edr,w4,whe,ydr,wdr,Uq,Adr,Ldr,Bdr,Ahe,xdr,kdr,g0,$7e,Fc,A4,Lhe,h0,Rdr,Bhe,Sdr,I7e,Tr,u0,Pdr,Cc,$dr,xhe,Idr,jdr,khe,Ndr,Ddr,qdr,p0,Gdr,Rhe,Odr,Xdr,zdr,ft,_0,Vdr,She,Wdr,Qdr,Mc,Hdr,Phe,Udr,Jdr,$he,Ydr,Kdr,Zdr,Ihe,ecr,ocr,b0,rcr,Co,v0,tcr,jhe,acr,scr,gs,ncr,Nhe,lcr,icr,Dhe,dcr,ccr,qhe,mcr,fcr,gcr,Ghe,L4,Ohe,hcr,ucr,Jq,pcr,_cr,bcr,Xhe,vcr,Tcr,T0,j7e,Ec,B4,zhe,F0,Fcr,Vhe,Ccr,N7e,Fr,C0,Mcr,yc,Ecr,Whe,ycr,wcr,Qhe,Acr,Lcr,Bcr,M0,xcr,Hhe,kcr,Rcr,Scr,gt,E0,Pcr,Uhe,$cr,Icr,wc,jcr,Jhe,Ncr,Dcr,Yhe,qcr,Gcr,Ocr,Khe,Xcr,zcr,y0,Vcr,Mo,w0,Wcr,Zhe,Qcr,Hcr,hs,Ucr,eue,Jcr,Ycr,oue,Kcr,Zcr,rue,emr,omr,rmr,tue,x4,aue,tmr,amr,Yq,smr,nmr,lmr,sue,imr,dmr,A0,D7e,Ac,k4,nue,L0,cmr,lue,mmr,q7e,Cr,B0,fmr,Lc,gmr,iue,hmr,umr,due,pmr,_mr,bmr,x0,vmr,cue,Tmr,Fmr,Cmr,ht,k0,Mmr,mue,Emr,ymr,Bc,wmr,fue,Amr,Lmr,gue,Bmr,xmr,kmr,hue,Rmr,Smr,R0,Pmr,Eo,S0,$mr,uue,Imr,jmr,us,Nmr,pue,Dmr,qmr,_ue,Gmr,Omr,bue,Xmr,zmr,Vmr,V,R4,vue,Wmr,Qmr,Kq,Hmr,Umr,Jmr,S4,Tue,Ymr,Kmr,Zq,Zmr,efr,ofr,P4,Fue,rfr,tfr,eG,afr,sfr,nfr,$4,Cue,lfr,ifr,oG,dfr,cfr,mfr,I4,Mue,ffr,gfr,rG,hfr,ufr,pfr,j4,Eue,_fr,bfr,tG,vfr,Tfr,Ffr,N4,yue,Cfr,Mfr,aG,Efr,yfr,wfr,D4,wue,Afr,Lfr,sG,Bfr,xfr,kfr,q4,Aue,Rfr,Sfr,nG,Pfr,$fr,Ifr,G4,Lue,jfr,Nfr,lG,Dfr,qfr,Gfr,O4,Bue,Ofr,Xfr,iG,zfr,Vfr,Wfr,X4,xue,Qfr,Hfr,dG,Ufr,Jfr,Yfr,z4,kue,Kfr,Zfr,cG,egr,ogr,rgr,V4,Rue,tgr,agr,mG,sgr,ngr,lgr,W4,Sue,igr,dgr,fG,cgr,mgr,fgr,Q4,Pue,ggr,hgr,gG,ugr,pgr,_gr,H4,$ue,bgr,vgr,hG,Tgr,Fgr,Cgr,U4,Iue,Mgr,Egr,uG,ygr,wgr,Agr,J4,jue,Lgr,Bgr,pG,xgr,kgr,Rgr,Y4,Nue,Sgr,Pgr,_G,$gr,Igr,jgr,K4,Due,Ngr,Dgr,bG,qgr,Ggr,Ogr,Z4,que,Xgr,zgr,vG,Vgr,Wgr,Qgr,eM,Gue,Hgr,Ugr,TG,Jgr,Ygr,Kgr,oM,Oue,Zgr,ehr,FG,ohr,rhr,thr,Xue,ahr,shr,P0,G7e,xc,rM,zue,$0,nhr,Vue,lhr,O7e,Mr,I0,ihr,kc,dhr,Wue,chr,mhr,Que,fhr,ghr,hhr,j0,uhr,Hue,phr,_hr,bhr,ut,N0,vhr,Uue,Thr,Fhr,Rc,Chr,Jue,Mhr,Ehr,Yue,yhr,whr,Ahr,Kue,Lhr,Bhr,D0,xhr,yo,q0,khr,Zue,Rhr,Shr,ps,Phr,epe,$hr,Ihr,ope,jhr,Nhr,rpe,Dhr,qhr,Ghr,_s,tM,tpe,Ohr,Xhr,CG,zhr,Vhr,Whr,aM,ape,Qhr,Hhr,MG,Uhr,Jhr,Yhr,sM,spe,Khr,Zhr,EG,eur,our,rur,nM,npe,tur,aur,yG,sur,nur,lur,lpe,iur,dur,G0,X7e,Sc,lM,ipe,O0,cur,dpe,mur,z7e,Er,X0,fur,Pc,gur,cpe,hur,uur,mpe,pur,_ur,bur,z0,vur,fpe,Tur,Fur,Cur,pt,V0,Mur,gpe,Eur,yur,$c,wur,hpe,Aur,Lur,upe,Bur,xur,kur,ppe,Rur,Sur,W0,Pur,wo,Q0,$ur,_pe,Iur,jur,bs,Nur,bpe,Dur,qur,vpe,Gur,Our,Tpe,Xur,zur,Vur,me,iM,Fpe,Wur,Qur,wG,Hur,Uur,Jur,dM,Cpe,Yur,Kur,AG,Zur,epr,opr,cM,Mpe,rpr,tpr,LG,apr,spr,npr,mM,Epe,lpr,ipr,BG,dpr,cpr,mpr,fM,ype,fpr,gpr,xG,hpr,upr,ppr,gM,wpe,_pr,bpr,kG,vpr,Tpr,Fpr,hM,Ape,Cpr,Mpr,RG,Epr,ypr,wpr,uM,Lpe,Apr,Lpr,SG,Bpr,xpr,kpr,pM,Bpe,Rpr,Spr,PG,Ppr,$pr,Ipr,_M,xpe,jpr,Npr,$G,Dpr,qpr,Gpr,bM,kpe,Opr,Xpr,IG,zpr,Vpr,Wpr,Rpe,Qpr,Hpr,H0,V7e,Ic,vM,Spe,U0,Upr,Ppe,Jpr,W7e,yr,J0,Ypr,jc,Kpr,$pe,Zpr,e_r,Ipe,o_r,r_r,t_r,Y0,a_r,jpe,s_r,n_r,l_r,_t,K0,i_r,Npe,d_r,c_r,Nc,m_r,Dpe,f_r,g_r,qpe,h_r,u_r,p_r,Gpe,__r,b_r,Z0,v_r,Ao,e6,T_r,Ope,F_r,C_r,vs,M_r,Xpe,E_r,y_r,zpe,w_r,A_r,Vpe,L_r,B_r,x_r,be,TM,Wpe,k_r,R_r,jG,S_r,P_r,$_r,FM,Qpe,I_r,j_r,NG,N_r,D_r,q_r,CM,Hpe,G_r,O_r,DG,X_r,z_r,V_r,MM,Upe,W_r,Q_r,qG,H_r,U_r,J_r,EM,Jpe,Y_r,K_r,GG,Z_r,ebr,obr,yM,Ype,rbr,tbr,OG,abr,sbr,nbr,wM,Kpe,lbr,ibr,XG,dbr,cbr,mbr,AM,Zpe,fbr,gbr,zG,hbr,ubr,pbr,LM,e_e,_br,bbr,VG,vbr,Tbr,Fbr,o_e,Cbr,Mbr,o6,Q7e,Dc,BM,r_e,r6,Ebr,t_e,ybr,H7e,wr,t6,wbr,qc,Abr,a_e,Lbr,Bbr,s_e,xbr,kbr,Rbr,a6,Sbr,n_e,Pbr,$br,Ibr,bt,s6,jbr,l_e,Nbr,Dbr,Gc,qbr,i_e,Gbr,Obr,d_e,Xbr,zbr,Vbr,c_e,Wbr,Qbr,n6,Hbr,Lo,l6,Ubr,m_e,Jbr,Ybr,Ts,Kbr,f_e,Zbr,e2r,g_e,o2r,r2r,h_e,t2r,a2r,s2r,ve,xM,u_e,n2r,l2r,WG,i2r,d2r,c2r,kM,p_e,m2r,f2r,QG,g2r,h2r,u2r,RM,__e,p2r,_2r,HG,b2r,v2r,T2r,SM,b_e,F2r,C2r,UG,M2r,E2r,y2r,PM,v_e,w2r,A2r,JG,L2r,B2r,x2r,$M,T_e,k2r,R2r,YG,S2r,P2r,$2r,IM,F_e,I2r,j2r,KG,N2r,D2r,q2r,jM,C_e,G2r,O2r,ZG,X2r,z2r,V2r,NM,M_e,W2r,Q2r,eO,H2r,U2r,J2r,E_e,Y2r,K2r,i6,U7e,Oc,DM,y_e,d6,Z2r,w_e,evr,J7e,Ar,c6,ovr,Xc,rvr,A_e,tvr,avr,L_e,svr,nvr,lvr,m6,ivr,B_e,dvr,cvr,mvr,vt,f6,fvr,x_e,gvr,hvr,zc,uvr,k_e,pvr,_vr,R_e,bvr,vvr,Tvr,S_e,Fvr,Cvr,g6,Mvr,Bo,h6,Evr,P_e,yvr,wvr,Fs,Avr,$_e,Lvr,Bvr,I_e,xvr,kvr,j_e,Rvr,Svr,Pvr,Te,qM,N_e,$vr,Ivr,oO,jvr,Nvr,Dvr,GM,D_e,qvr,Gvr,rO,Ovr,Xvr,zvr,OM,q_e,Vvr,Wvr,tO,Qvr,Hvr,Uvr,XM,G_e,Jvr,Yvr,aO,Kvr,Zvr,eTr,zM,O_e,oTr,rTr,sO,tTr,aTr,sTr,VM,X_e,nTr,lTr,nO,iTr,dTr,cTr,WM,z_e,mTr,fTr,lO,gTr,hTr,uTr,QM,V_e,pTr,_Tr,iO,bTr,vTr,TTr,HM,W_e,FTr,CTr,dO,MTr,ETr,yTr,Q_e,wTr,ATr,u6,Y7e,Vc,UM,H_e,p6,LTr,U_e,BTr,K7e,Lr,_6,xTr,Wc,kTr,J_e,RTr,STr,Y_e,PTr,$Tr,ITr,b6,jTr,K_e,NTr,DTr,qTr,Tt,v6,GTr,Z_e,OTr,XTr,Qc,zTr,ebe,VTr,WTr,obe,QTr,HTr,UTr,rbe,JTr,YTr,T6,KTr,xo,F6,ZTr,tbe,e1r,o1r,Cs,r1r,abe,t1r,a1r,sbe,s1r,n1r,nbe,l1r,i1r,d1r,Fe,JM,lbe,c1r,m1r,cO,f1r,g1r,h1r,YM,ibe,u1r,p1r,mO,_1r,b1r,v1r,KM,dbe,T1r,F1r,fO,C1r,M1r,E1r,ZM,cbe,y1r,w1r,gO,A1r,L1r,B1r,eE,mbe,x1r,k1r,hO,R1r,S1r,P1r,oE,fbe,$1r,I1r,uO,j1r,N1r,D1r,rE,gbe,q1r,G1r,pO,O1r,X1r,z1r,tE,hbe,V1r,W1r,_O,Q1r,H1r,U1r,aE,ube,J1r,Y1r,bO,K1r,Z1r,eFr,pbe,oFr,rFr,C6,Z7e,Hc,sE,_be,M6,tFr,bbe,aFr,e8e,Br,E6,sFr,Uc,nFr,vbe,lFr,iFr,Tbe,dFr,cFr,mFr,y6,fFr,Fbe,gFr,hFr,uFr,Ft,w6,pFr,Cbe,_Fr,bFr,Jc,vFr,Mbe,TFr,FFr,Ebe,CFr,MFr,EFr,ybe,yFr,wFr,A6,AFr,ko,L6,LFr,wbe,BFr,xFr,Ms,kFr,Abe,RFr,SFr,Lbe,PFr,$Fr,Bbe,IFr,jFr,NFr,ao,nE,xbe,DFr,qFr,vO,GFr,OFr,XFr,lE,kbe,zFr,VFr,TO,WFr,QFr,HFr,iE,Rbe,UFr,JFr,FO,YFr,KFr,ZFr,dE,Sbe,eCr,oCr,CO,rCr,tCr,aCr,cE,Pbe,sCr,nCr,MO,lCr,iCr,dCr,mE,$be,cCr,mCr,EO,fCr,gCr,hCr,fE,Ibe,uCr,pCr,yO,_Cr,bCr,vCr,jbe,TCr,FCr,B6,o8e,Yc,gE,Nbe,x6,CCr,Dbe,MCr,r8e,xr,k6,ECr,Kc,yCr,qbe,wCr,ACr,Gbe,LCr,BCr,xCr,R6,kCr,Obe,RCr,SCr,PCr,Ct,S6,$Cr,Xbe,ICr,jCr,Zc,NCr,zbe,DCr,qCr,Vbe,GCr,OCr,XCr,Wbe,zCr,VCr,P6,WCr,Ro,$6,QCr,Qbe,HCr,UCr,Es,JCr,Hbe,YCr,KCr,Ube,ZCr,e4r,Jbe,o4r,r4r,t4r,so,hE,Ybe,a4r,s4r,wO,n4r,l4r,i4r,uE,Kbe,d4r,c4r,AO,m4r,f4r,g4r,pE,Zbe,h4r,u4r,LO,p4r,_4r,b4r,_E,e2e,v4r,T4r,BO,F4r,C4r,M4r,bE,o2e,E4r,y4r,xO,w4r,A4r,L4r,vE,r2e,B4r,x4r,kO,k4r,R4r,S4r,TE,t2e,P4r,$4r,RO,I4r,j4r,N4r,a2e,D4r,q4r,I6,t8e,em,FE,s2e,j6,G4r,n2e,O4r,a8e,kr,N6,X4r,om,z4r,l2e,V4r,W4r,i2e,Q4r,H4r,U4r,D6,J4r,d2e,Y4r,K4r,Z4r,Mt,q6,eMr,c2e,oMr,rMr,rm,tMr,m2e,aMr,sMr,f2e,nMr,lMr,iMr,g2e,dMr,cMr,G6,mMr,So,O6,fMr,h2e,gMr,hMr,ys,uMr,u2e,pMr,_Mr,p2e,bMr,vMr,_2e,TMr,FMr,CMr,b2e,CE,v2e,MMr,EMr,SO,yMr,wMr,AMr,T2e,LMr,BMr,X6,s8e,tm,ME,F2e,z6,xMr,C2e,kMr,n8e,Rr,V6,RMr,am,SMr,M2e,PMr,$Mr,E2e,IMr,jMr,NMr,W6,DMr,y2e,qMr,GMr,OMr,Et,Q6,XMr,w2e,zMr,VMr,sm,WMr,A2e,QMr,HMr,L2e,UMr,JMr,YMr,B2e,KMr,ZMr,H6,eEr,Po,U6,oEr,x2e,rEr,tEr,ws,aEr,k2e,sEr,nEr,R2e,lEr,iEr,S2e,dEr,cEr,mEr,J6,EE,P2e,fEr,gEr,PO,hEr,uEr,pEr,yE,$2e,_Er,bEr,$O,vEr,TEr,FEr,I2e,CEr,MEr,Y6,l8e,nm,wE,j2e,K6,EEr,N2e,yEr,i8e,Sr,Z6,wEr,lm,AEr,D2e,LEr,BEr,q2e,xEr,kEr,REr,eL,SEr,G2e,PEr,$Er,IEr,yt,oL,jEr,O2e,NEr,DEr,im,qEr,X2e,GEr,OEr,z2e,XEr,zEr,VEr,V2e,WEr,QEr,rL,HEr,$o,tL,UEr,W2e,JEr,YEr,As,KEr,Q2e,ZEr,e3r,H2e,o3r,r3r,U2e,t3r,a3r,s3r,J2e,AE,Y2e,n3r,l3r,IO,i3r,d3r,c3r,K2e,m3r,f3r,aL,d8e;return ce=new z({}),ka=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased")',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),x3=new z({}),k3=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),um=new g3r({props:{warning:"&lcub;true}",$$slots:{default:[Amt]},$$scope:{ctx:pi}}}),R3=new z({}),S3=new y({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L509"}}),I3=new y({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L532",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),j3=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),N3=new y({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L654",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),D3=new z({}),q3=new y({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L350"}}),X3=new y({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L364",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),z3=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),V3=new y({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L554",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),W3=new z({}),Q3=new y({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L69"}}),J3=new y({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L83",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Vg=new g3r({props:{$$slots:{default:[Lmt]},$$scope:{ctx:pi}}}),Y3=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),K3=new z({}),Z3=new y({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L65"}}),r5=new y({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L79",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),oh=new g3r({props:{$$slots:{default:[Bmt]},$$scope:{ctx:pi}}}),t5=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),a5=new z({}),s5=new y({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L653"}}),l5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertModel</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),i5=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),d5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),c5=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m5=new z({}),f5=new y({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L660"}}),h5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),u5=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),p5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_5=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),b5=new z({}),v5=new y({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L675"}}),F5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertLMHeadModel</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),C5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),M5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),E5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),y5=new z({}),w5=new y({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L682"}}),L5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertForMaskedLM</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),B5=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),x5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),k5=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),R5=new z({}),S5=new y({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L689"}}),$5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),I5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),j5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),N5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),D5=new z({}),q5=new y({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L698"}}),O5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertForSequenceClassification</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),X5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),z5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),V5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),W5=new z({}),Q5=new y({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L732"}}),U5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertForMultipleChoice</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),J5=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),Y5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),K5=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Z5=new z({}),ey=new y({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L739"}}),ry=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertForNextSentencePrediction</code>(QDQBert model)</li>
</ul>`,name:"config"}]}}),ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),ay=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sy=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ny=new z({}),ly=new y({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L725"}}),dy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertForTokenClassification</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),my=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fy=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gy=new z({}),hy=new y({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L707"}}),py=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><code>QDQBertConfig</code>configuration class: <code>QDQBertForQuestionAnswering</code>(QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),_y=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),by=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vy=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ty=new z({}),Fy=new y({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L714"}}),My=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Ey=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),yy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wy=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ay=new z({}),Ly=new y({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L748"}}),xy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),ky=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),Ry=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Sy=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Py=new z({}),$y=new y({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L778"}}),jy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Ny=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),Dy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qy=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Gy=new z({}),Oy=new y({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L785"}}),zy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Vy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),Wy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Qy=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Hy=new z({}),Uy=new y({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L808"}}),Yy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Ky=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),Zy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ew=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ow=new z({}),rw=new y({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L792"}}),aw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),sw=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),nw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),lw=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),iw=new z({}),dw=new y({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L799"}}),mw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),fw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),gw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),pw=new z({}),_w=new y({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L817"}}),vw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Tw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),Fw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Mw=new z({}),Ew=new y({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L771"}}),ww=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Aw=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),Lw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Bw=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xw=new z({}),kw=new y({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L755"}}),Sw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),Pw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),$w=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jw=new z({}),Nw=new y({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L762"}}),qw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),Gw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),Ow=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),zw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Vw=new z({}),Ww=new y({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),Hw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Uw=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Jw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Kw=new z({}),Zw=new y({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),oA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),rA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),tA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sA=new z({}),nA=new y({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),iA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),dA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),cA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fA=new z({}),gA=new y({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),uA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),pA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),_A=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),vA=new z({}),TA=new y({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),CA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),MA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),EA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),wA=new z({}),AA=new y({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),BA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),xA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),kA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),RA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),SA=new z({}),PA=new y({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),IA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),jA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),NA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),DA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qA=new z({}),GA=new y({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),XA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),zA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),VA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),WA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),QA=new z({}),HA=new y({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),JA=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),YA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),KA=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ZA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e0=new z({}),o0=new y({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),t0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),a0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),s0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),n0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l0=new z({}),i0=new y({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),c0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),m0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),f0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),h0=new z({}),u0=new y({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),_0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),b0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),v0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),T0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),F0=new z({}),C0=new y({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),E0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),y0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),w0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),A0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),L0=new z({}),B0=new y({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),k0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),R0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),S0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),P0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$0=new z({}),I0=new y({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),N0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),D0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),q0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),G0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),O0=new z({}),X0=new y({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),V0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),W0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),Q0=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),H0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),U0=new z({}),J0=new y({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),K0=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Z0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),e6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),o6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),r6=new z({}),t6=new y({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),s6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),n6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),l6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),i6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),d6=new z({}),c6=new y({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),f6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),g6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),h6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),u6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),p6=new z({}),_6=new y({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),v6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),T6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),F6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),C6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),M6=new z({}),E6=new y({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),w6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),A6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),L6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),B6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),x6=new z({}),k6=new y({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),S6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),P6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),$6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),I6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),j6=new z({}),N6=new y({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),q6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),G6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),O6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),X6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),z6=new z({}),V6=new y({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),Q6=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),H6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),U6=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Y6=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),K6=new z({}),Z6=new y({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),oL=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),rL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),tL=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),le=a("h1"),fe=a("a"),oo=a("span"),m(ce.$$.fragment),_e=l(),No=a("span"),_i=o("Auto Classes"),cm=l(),ra=a("p"),bi=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),vi=a("code"),w3=o("from_pretrained()"),mm=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Ee=l(),no=a("p"),Ti=o("Instantiating one of "),Ls=a("a"),A3=o("AutoConfig"),Bs=o(", "),xs=a("a"),L3=o("AutoModel"),Fi=o(`, and
`),ks=a("a"),B3=o("AutoTokenizer"),Ci=o(" will directly create a class of the relevant architecture. For instance"),fm=l(),m(ka.$$.fragment),lo=l(),ge=a("p"),o7=o("will create a model that is an instance of "),Mi=a("a"),r7=o("BertModel"),t7=o("."),Do=l(),Ra=a("p"),a7=o("There is one class of "),gm=a("code"),s7=o("AutoModel"),uBe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),fLe=l(),Ei=a("h2"),hm=a("a"),wz=a("span"),m(x3.$$.fragment),pBe=l(),Az=a("span"),_Be=o("Extending the Auto Classes"),gLe=l(),Rs=a("p"),bBe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),Lz=a("code"),vBe=o("NewModel"),TBe=o(", make sure you have a "),Bz=a("code"),FBe=o("NewModelConfig"),CBe=o(` then you can add those to the auto
classes like this:`),hLe=l(),m(k3.$$.fragment),uLe=l(),n7=a("p"),MBe=o("You will then be able to use the auto classes like you would usually do!"),pLe=l(),m(um.$$.fragment),_Le=l(),yi=a("h2"),pm=a("a"),xz=a("span"),m(R3.$$.fragment),EBe=l(),kz=a("span"),yBe=o("AutoConfig"),bLe=l(),qo=a("div"),m(S3.$$.fragment),wBe=l(),P3=a("p"),ABe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),l7=a("a"),LBe=o("from_pretrained()"),BBe=o(" class method."),xBe=l(),$3=a("p"),kBe=o("This class cannot be instantiated directly using "),Rz=a("code"),RBe=o("__init__()"),SBe=o(" (throws an error)."),PBe=l(),io=a("div"),m(I3.$$.fragment),$Be=l(),Sz=a("p"),IBe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),jBe=l(),wi=a("p"),NBe=o("The configuration class to instantiate is selected based on the "),Pz=a("code"),DBe=o("model_type"),qBe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),$z=a("code"),GBe=o("pretrained_model_name_or_path"),OBe=o(":"),XBe=l(),v=a("ul"),_m=a("li"),Iz=a("strong"),zBe=o("albert"),VBe=o(" \u2014 "),i7=a("a"),WBe=o("AlbertConfig"),QBe=o(" (ALBERT model)"),HBe=l(),bm=a("li"),jz=a("strong"),UBe=o("bart"),JBe=o(" \u2014 "),d7=a("a"),YBe=o("BartConfig"),KBe=o(" (BART model)"),ZBe=l(),vm=a("li"),Nz=a("strong"),exe=o("beit"),oxe=o(" \u2014 "),c7=a("a"),rxe=o("BeitConfig"),txe=o(" (BEiT model)"),axe=l(),Tm=a("li"),Dz=a("strong"),sxe=o("bert"),nxe=o(" \u2014 "),m7=a("a"),lxe=o("BertConfig"),ixe=o(" (BERT model)"),dxe=l(),Fm=a("li"),qz=a("strong"),cxe=o("bert-generation"),mxe=o(" \u2014 "),f7=a("a"),fxe=o("BertGenerationConfig"),gxe=o(" (Bert Generation model)"),hxe=l(),Cm=a("li"),Gz=a("strong"),uxe=o("big_bird"),pxe=o(" \u2014 "),g7=a("a"),_xe=o("BigBirdConfig"),bxe=o(" (BigBird model)"),vxe=l(),Mm=a("li"),Oz=a("strong"),Txe=o("bigbird_pegasus"),Fxe=o(" \u2014 "),h7=a("a"),Cxe=o("BigBirdPegasusConfig"),Mxe=o(" (BigBirdPegasus model)"),Exe=l(),Em=a("li"),Xz=a("strong"),yxe=o("blenderbot"),wxe=o(" \u2014 "),u7=a("a"),Axe=o("BlenderbotConfig"),Lxe=o(" (Blenderbot model)"),Bxe=l(),ym=a("li"),zz=a("strong"),xxe=o("blenderbot-small"),kxe=o(" \u2014 "),p7=a("a"),Rxe=o("BlenderbotSmallConfig"),Sxe=o(" (BlenderbotSmall model)"),Pxe=l(),wm=a("li"),Vz=a("strong"),$xe=o("camembert"),Ixe=o(" \u2014 "),_7=a("a"),jxe=o("CamembertConfig"),Nxe=o(" (CamemBERT model)"),Dxe=l(),Am=a("li"),Wz=a("strong"),qxe=o("canine"),Gxe=o(" \u2014 "),b7=a("a"),Oxe=o("CanineConfig"),Xxe=o(" (Canine model)"),zxe=l(),Lm=a("li"),Qz=a("strong"),Vxe=o("clip"),Wxe=o(" \u2014 "),v7=a("a"),Qxe=o("CLIPConfig"),Hxe=o(" (CLIP model)"),Uxe=l(),Bm=a("li"),Hz=a("strong"),Jxe=o("convbert"),Yxe=o(" \u2014 "),T7=a("a"),Kxe=o("ConvBertConfig"),Zxe=o(" (ConvBERT model)"),eke=l(),xm=a("li"),Uz=a("strong"),oke=o("convnext"),rke=o(" \u2014 "),F7=a("a"),tke=o("ConvNextConfig"),ake=o(" (ConvNext model)"),ske=l(),km=a("li"),Jz=a("strong"),nke=o("ctrl"),lke=o(" \u2014 "),C7=a("a"),ike=o("CTRLConfig"),dke=o(" (CTRL model)"),cke=l(),Rm=a("li"),Yz=a("strong"),mke=o("deberta"),fke=o(" \u2014 "),M7=a("a"),gke=o("DebertaConfig"),hke=o(" (DeBERTa model)"),uke=l(),Sm=a("li"),Kz=a("strong"),pke=o("deberta-v2"),_ke=o(" \u2014 "),E7=a("a"),bke=o("DebertaV2Config"),vke=o(" (DeBERTa-v2 model)"),Tke=l(),Pm=a("li"),Zz=a("strong"),Fke=o("deit"),Cke=o(" \u2014 "),y7=a("a"),Mke=o("DeiTConfig"),Eke=o(" (DeiT model)"),yke=l(),$m=a("li"),eV=a("strong"),wke=o("detr"),Ake=o(" \u2014 "),w7=a("a"),Lke=o("DetrConfig"),Bke=o(" (DETR model)"),xke=l(),Im=a("li"),oV=a("strong"),kke=o("distilbert"),Rke=o(" \u2014 "),A7=a("a"),Ske=o("DistilBertConfig"),Pke=o(" (DistilBERT model)"),$ke=l(),jm=a("li"),rV=a("strong"),Ike=o("dpr"),jke=o(" \u2014 "),L7=a("a"),Nke=o("DPRConfig"),Dke=o(" (DPR model)"),qke=l(),Nm=a("li"),tV=a("strong"),Gke=o("electra"),Oke=o(" \u2014 "),B7=a("a"),Xke=o("ElectraConfig"),zke=o(" (ELECTRA model)"),Vke=l(),Dm=a("li"),aV=a("strong"),Wke=o("encoder-decoder"),Qke=o(" \u2014 "),x7=a("a"),Hke=o("EncoderDecoderConfig"),Uke=o(" (Encoder decoder model)"),Jke=l(),qm=a("li"),sV=a("strong"),Yke=o("flaubert"),Kke=o(" \u2014 "),k7=a("a"),Zke=o("FlaubertConfig"),eRe=o(" (FlauBERT model)"),oRe=l(),Gm=a("li"),nV=a("strong"),rRe=o("fnet"),tRe=o(" \u2014 "),R7=a("a"),aRe=o("FNetConfig"),sRe=o(" (FNet model)"),nRe=l(),Om=a("li"),lV=a("strong"),lRe=o("fsmt"),iRe=o(" \u2014 "),S7=a("a"),dRe=o("FSMTConfig"),cRe=o(" (FairSeq Machine-Translation model)"),mRe=l(),Xm=a("li"),iV=a("strong"),fRe=o("funnel"),gRe=o(" \u2014 "),P7=a("a"),hRe=o("FunnelConfig"),uRe=o(" (Funnel Transformer model)"),pRe=l(),zm=a("li"),dV=a("strong"),_Re=o("gpt2"),bRe=o(" \u2014 "),$7=a("a"),vRe=o("GPT2Config"),TRe=o(" (OpenAI GPT-2 model)"),FRe=l(),Vm=a("li"),cV=a("strong"),CRe=o("gpt_neo"),MRe=o(" \u2014 "),I7=a("a"),ERe=o("GPTNeoConfig"),yRe=o(" (GPT Neo model)"),wRe=l(),Wm=a("li"),mV=a("strong"),ARe=o("gptj"),LRe=o(" \u2014 "),j7=a("a"),BRe=o("GPTJConfig"),xRe=o(" (GPT-J model)"),kRe=l(),Qm=a("li"),fV=a("strong"),RRe=o("hubert"),SRe=o(" \u2014 "),N7=a("a"),PRe=o("HubertConfig"),$Re=o(" (Hubert model)"),IRe=l(),Hm=a("li"),gV=a("strong"),jRe=o("ibert"),NRe=o(" \u2014 "),D7=a("a"),DRe=o("IBertConfig"),qRe=o(" (I-BERT model)"),GRe=l(),Um=a("li"),hV=a("strong"),ORe=o("imagegpt"),XRe=o(" \u2014 "),q7=a("a"),zRe=o("ImageGPTConfig"),VRe=o(" (ImageGPT model)"),WRe=l(),Jm=a("li"),uV=a("strong"),QRe=o("layoutlm"),HRe=o(" \u2014 "),G7=a("a"),URe=o("LayoutLMConfig"),JRe=o(" (LayoutLM model)"),YRe=l(),Ym=a("li"),pV=a("strong"),KRe=o("layoutlmv2"),ZRe=o(" \u2014 "),O7=a("a"),eSe=o("LayoutLMv2Config"),oSe=o(" (LayoutLMv2 model)"),rSe=l(),Km=a("li"),_V=a("strong"),tSe=o("led"),aSe=o(" \u2014 "),X7=a("a"),sSe=o("LEDConfig"),nSe=o(" (LED model)"),lSe=l(),Zm=a("li"),bV=a("strong"),iSe=o("longformer"),dSe=o(" \u2014 "),z7=a("a"),cSe=o("LongformerConfig"),mSe=o(" (Longformer model)"),fSe=l(),ef=a("li"),vV=a("strong"),gSe=o("luke"),hSe=o(" \u2014 "),V7=a("a"),uSe=o("LukeConfig"),pSe=o(" (LUKE model)"),_Se=l(),of=a("li"),TV=a("strong"),bSe=o("lxmert"),vSe=o(" \u2014 "),W7=a("a"),TSe=o("LxmertConfig"),FSe=o(" (LXMERT model)"),CSe=l(),rf=a("li"),FV=a("strong"),MSe=o("m2m_100"),ESe=o(" \u2014 "),Q7=a("a"),ySe=o("M2M100Config"),wSe=o(" (M2M100 model)"),ASe=l(),tf=a("li"),CV=a("strong"),LSe=o("marian"),BSe=o(" \u2014 "),H7=a("a"),xSe=o("MarianConfig"),kSe=o(" (Marian model)"),RSe=l(),af=a("li"),MV=a("strong"),SSe=o("mbart"),PSe=o(" \u2014 "),U7=a("a"),$Se=o("MBartConfig"),ISe=o(" (mBART model)"),jSe=l(),sf=a("li"),EV=a("strong"),NSe=o("megatron-bert"),DSe=o(" \u2014 "),J7=a("a"),qSe=o("MegatronBertConfig"),GSe=o(" (MegatronBert model)"),OSe=l(),nf=a("li"),yV=a("strong"),XSe=o("mobilebert"),zSe=o(" \u2014 "),Y7=a("a"),VSe=o("MobileBertConfig"),WSe=o(" (MobileBERT model)"),QSe=l(),lf=a("li"),wV=a("strong"),HSe=o("mpnet"),USe=o(" \u2014 "),K7=a("a"),JSe=o("MPNetConfig"),YSe=o(" (MPNet model)"),KSe=l(),df=a("li"),AV=a("strong"),ZSe=o("mt5"),ePe=o(" \u2014 "),Z7=a("a"),oPe=o("MT5Config"),rPe=o(" (mT5 model)"),tPe=l(),cf=a("li"),LV=a("strong"),aPe=o("nystromformer"),sPe=o(" \u2014 "),e8=a("a"),nPe=o("NystromformerConfig"),lPe=o(" (Nystromformer model)"),iPe=l(),mf=a("li"),BV=a("strong"),dPe=o("openai-gpt"),cPe=o(" \u2014 "),o8=a("a"),mPe=o("OpenAIGPTConfig"),fPe=o(" (OpenAI GPT model)"),gPe=l(),ff=a("li"),xV=a("strong"),hPe=o("pegasus"),uPe=o(" \u2014 "),r8=a("a"),pPe=o("PegasusConfig"),_Pe=o(" (Pegasus model)"),bPe=l(),gf=a("li"),kV=a("strong"),vPe=o("perceiver"),TPe=o(" \u2014 "),t8=a("a"),FPe=o("PerceiverConfig"),CPe=o(" (Perceiver model)"),MPe=l(),hf=a("li"),RV=a("strong"),EPe=o("prophetnet"),yPe=o(" \u2014 "),a8=a("a"),wPe=o("ProphetNetConfig"),APe=o(" (ProphetNet model)"),LPe=l(),uf=a("li"),SV=a("strong"),BPe=o("qdqbert"),xPe=o(" \u2014 "),PV=a("code"),kPe=o("QDQBertConfig"),RPe=o("(QDQBert model)"),SPe=l(),pf=a("li"),$V=a("strong"),PPe=o("rag"),$Pe=o(" \u2014 "),s8=a("a"),IPe=o("RagConfig"),jPe=o(" (RAG model)"),NPe=l(),_f=a("li"),IV=a("strong"),DPe=o("realm"),qPe=o(" \u2014 "),n8=a("a"),GPe=o("RealmConfig"),OPe=o(" (Realm model)"),XPe=l(),bf=a("li"),jV=a("strong"),zPe=o("reformer"),VPe=o(" \u2014 "),l8=a("a"),WPe=o("ReformerConfig"),QPe=o(" (Reformer model)"),HPe=l(),vf=a("li"),NV=a("strong"),UPe=o("rembert"),JPe=o(" \u2014 "),i8=a("a"),YPe=o("RemBertConfig"),KPe=o(" (RemBERT model)"),ZPe=l(),Tf=a("li"),DV=a("strong"),e$e=o("retribert"),o$e=o(" \u2014 "),d8=a("a"),r$e=o("RetriBertConfig"),t$e=o(" (RetriBERT model)"),a$e=l(),Ff=a("li"),qV=a("strong"),s$e=o("roberta"),n$e=o(" \u2014 "),c8=a("a"),l$e=o("RobertaConfig"),i$e=o(" (RoBERTa model)"),d$e=l(),Cf=a("li"),GV=a("strong"),c$e=o("roformer"),m$e=o(" \u2014 "),m8=a("a"),f$e=o("RoFormerConfig"),g$e=o(" (RoFormer model)"),h$e=l(),Mf=a("li"),OV=a("strong"),u$e=o("segformer"),p$e=o(" \u2014 "),f8=a("a"),_$e=o("SegformerConfig"),b$e=o(" (SegFormer model)"),v$e=l(),Ef=a("li"),XV=a("strong"),T$e=o("sew"),F$e=o(" \u2014 "),g8=a("a"),C$e=o("SEWConfig"),M$e=o(" (SEW model)"),E$e=l(),yf=a("li"),zV=a("strong"),y$e=o("sew-d"),w$e=o(" \u2014 "),h8=a("a"),A$e=o("SEWDConfig"),L$e=o(" (SEW-D model)"),B$e=l(),wf=a("li"),VV=a("strong"),x$e=o("speech-encoder-decoder"),k$e=o(" \u2014 "),u8=a("a"),R$e=o("SpeechEncoderDecoderConfig"),S$e=o(" (Speech Encoder decoder model)"),P$e=l(),Af=a("li"),WV=a("strong"),$$e=o("speech_to_text"),I$e=o(" \u2014 "),p8=a("a"),j$e=o("Speech2TextConfig"),N$e=o(" (Speech2Text model)"),D$e=l(),Lf=a("li"),QV=a("strong"),q$e=o("speech_to_text_2"),G$e=o(" \u2014 "),_8=a("a"),O$e=o("Speech2Text2Config"),X$e=o(" (Speech2Text2 model)"),z$e=l(),Bf=a("li"),HV=a("strong"),V$e=o("splinter"),W$e=o(" \u2014 "),b8=a("a"),Q$e=o("SplinterConfig"),H$e=o(" (Splinter model)"),U$e=l(),xf=a("li"),UV=a("strong"),J$e=o("squeezebert"),Y$e=o(" \u2014 "),v8=a("a"),K$e=o("SqueezeBertConfig"),Z$e=o(" (SqueezeBERT model)"),eIe=l(),kf=a("li"),JV=a("strong"),oIe=o("swin"),rIe=o(" \u2014 "),T8=a("a"),tIe=o("SwinConfig"),aIe=o(" (Swin model)"),sIe=l(),Rf=a("li"),YV=a("strong"),nIe=o("t5"),lIe=o(" \u2014 "),F8=a("a"),iIe=o("T5Config"),dIe=o(" (T5 model)"),cIe=l(),Sf=a("li"),KV=a("strong"),mIe=o("tapas"),fIe=o(" \u2014 "),C8=a("a"),gIe=o("TapasConfig"),hIe=o(" (TAPAS model)"),uIe=l(),Pf=a("li"),ZV=a("strong"),pIe=o("transfo-xl"),_Ie=o(" \u2014 "),M8=a("a"),bIe=o("TransfoXLConfig"),vIe=o(" (Transformer-XL model)"),TIe=l(),$f=a("li"),eW=a("strong"),FIe=o("trocr"),CIe=o(" \u2014 "),E8=a("a"),MIe=o("TrOCRConfig"),EIe=o(" (TrOCR model)"),yIe=l(),If=a("li"),oW=a("strong"),wIe=o("unispeech"),AIe=o(" \u2014 "),y8=a("a"),LIe=o("UniSpeechConfig"),BIe=o(" (UniSpeech model)"),xIe=l(),jf=a("li"),rW=a("strong"),kIe=o("unispeech-sat"),RIe=o(" \u2014 "),w8=a("a"),SIe=o("UniSpeechSatConfig"),PIe=o(" (UniSpeechSat model)"),$Ie=l(),Nf=a("li"),tW=a("strong"),IIe=o("vilt"),jIe=o(" \u2014 "),A8=a("a"),NIe=o("ViltConfig"),DIe=o(" (ViLT model)"),qIe=l(),Df=a("li"),aW=a("strong"),GIe=o("vision-encoder-decoder"),OIe=o(" \u2014 "),L8=a("a"),XIe=o("VisionEncoderDecoderConfig"),zIe=o(" (Vision Encoder decoder model)"),VIe=l(),qf=a("li"),sW=a("strong"),WIe=o("vision-text-dual-encoder"),QIe=o(" \u2014 "),B8=a("a"),HIe=o("VisionTextDualEncoderConfig"),UIe=o(" (VisionTextDualEncoder model)"),JIe=l(),Gf=a("li"),nW=a("strong"),YIe=o("visual_bert"),KIe=o(" \u2014 "),x8=a("a"),ZIe=o("VisualBertConfig"),eje=o(" (VisualBert model)"),oje=l(),Of=a("li"),lW=a("strong"),rje=o("vit"),tje=o(" \u2014 "),k8=a("a"),aje=o("ViTConfig"),sje=o(" (ViT model)"),nje=l(),Xf=a("li"),iW=a("strong"),lje=o("vit_mae"),ije=o(" \u2014 "),R8=a("a"),dje=o("ViTMAEConfig"),cje=o(" (ViTMAE model)"),mje=l(),zf=a("li"),dW=a("strong"),fje=o("wav2vec2"),gje=o(" \u2014 "),S8=a("a"),hje=o("Wav2Vec2Config"),uje=o(" (Wav2Vec2 model)"),pje=l(),Vf=a("li"),cW=a("strong"),_je=o("wavlm"),bje=o(" \u2014 "),P8=a("a"),vje=o("WavLMConfig"),Tje=o(" (WavLM model)"),Fje=l(),Wf=a("li"),mW=a("strong"),Cje=o("xglm"),Mje=o(" \u2014 "),$8=a("a"),Eje=o("XGLMConfig"),yje=o(" (XGLM model)"),wje=l(),Qf=a("li"),fW=a("strong"),Aje=o("xlm"),Lje=o(" \u2014 "),I8=a("a"),Bje=o("XLMConfig"),xje=o(" (XLM model)"),kje=l(),Hf=a("li"),gW=a("strong"),Rje=o("xlm-prophetnet"),Sje=o(" \u2014 "),j8=a("a"),Pje=o("XLMProphetNetConfig"),$je=o(" (XLMProphetNet model)"),Ije=l(),Uf=a("li"),hW=a("strong"),jje=o("xlm-roberta"),Nje=o(" \u2014 "),N8=a("a"),Dje=o("XLMRobertaConfig"),qje=o(" (XLM-RoBERTa model)"),Gje=l(),Jf=a("li"),uW=a("strong"),Oje=o("xlm-roberta-xl"),Xje=o(" \u2014 "),D8=a("a"),zje=o("XLMRobertaXLConfig"),Vje=o(" (XLM-RoBERTa-XL model)"),Wje=l(),Yf=a("li"),pW=a("strong"),Qje=o("xlnet"),Hje=o(" \u2014 "),q8=a("a"),Uje=o("XLNetConfig"),Jje=o(" (XLNet model)"),Yje=l(),Kf=a("li"),_W=a("strong"),Kje=o("yoso"),Zje=o(" \u2014 "),G8=a("a"),eNe=o("YosoConfig"),oNe=o(" (YOSO model)"),rNe=l(),bW=a("p"),tNe=o("Examples:"),aNe=l(),m(j3.$$.fragment),sNe=l(),Zf=a("div"),m(N3.$$.fragment),nNe=l(),vW=a("p"),lNe=o("Register a new configuration for this class."),vLe=l(),Ai=a("h2"),eg=a("a"),TW=a("span"),m(D3.$$.fragment),iNe=l(),FW=a("span"),dNe=o("AutoTokenizer"),TLe=l(),Go=a("div"),m(q3.$$.fragment),cNe=l(),G3=a("p"),mNe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),O8=a("a"),fNe=o("AutoTokenizer.from_pretrained()"),gNe=o(" class method."),hNe=l(),O3=a("p"),uNe=o("This class cannot be instantiated directly using "),CW=a("code"),pNe=o("__init__()"),_Ne=o(" (throws an error)."),bNe=l(),co=a("div"),m(X3.$$.fragment),vNe=l(),MW=a("p"),TNe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),FNe=l(),Sa=a("p"),CNe=o("The tokenizer class to instantiate is selected based on the "),EW=a("code"),MNe=o("model_type"),ENe=o(` property of the config object (either
passed as an argument or loaded from `),yW=a("code"),yNe=o("pretrained_model_name_or_path"),wNe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wW=a("code"),ANe=o("pretrained_model_name_or_path"),LNe=o(":"),BNe=l(),M=a("ul"),Ss=a("li"),AW=a("strong"),xNe=o("albert"),kNe=o(" \u2014 "),X8=a("a"),RNe=o("AlbertTokenizer"),SNe=o(" or "),z8=a("a"),PNe=o("AlbertTokenizerFast"),$Ne=o(" (ALBERT model)"),INe=l(),Ps=a("li"),LW=a("strong"),jNe=o("bart"),NNe=o(" \u2014 "),V8=a("a"),DNe=o("BartTokenizer"),qNe=o(" or "),W8=a("a"),GNe=o("BartTokenizerFast"),ONe=o(" (BART model)"),XNe=l(),$s=a("li"),BW=a("strong"),zNe=o("barthez"),VNe=o(" \u2014 "),Q8=a("a"),WNe=o("BarthezTokenizer"),QNe=o(" or "),H8=a("a"),HNe=o("BarthezTokenizerFast"),UNe=o(" (BARThez model)"),JNe=l(),og=a("li"),xW=a("strong"),YNe=o("bartpho"),KNe=o(" \u2014 "),U8=a("a"),ZNe=o("BartphoTokenizer"),eDe=o(" (BARTpho model)"),oDe=l(),Is=a("li"),kW=a("strong"),rDe=o("bert"),tDe=o(" \u2014 "),J8=a("a"),aDe=o("BertTokenizer"),sDe=o(" or "),Y8=a("a"),nDe=o("BertTokenizerFast"),lDe=o(" (BERT model)"),iDe=l(),rg=a("li"),RW=a("strong"),dDe=o("bert-generation"),cDe=o(" \u2014 "),K8=a("a"),mDe=o("BertGenerationTokenizer"),fDe=o(" (Bert Generation model)"),gDe=l(),tg=a("li"),SW=a("strong"),hDe=o("bert-japanese"),uDe=o(" \u2014 "),Z8=a("a"),pDe=o("BertJapaneseTokenizer"),_De=o(" (BertJapanese model)"),bDe=l(),ag=a("li"),PW=a("strong"),vDe=o("bertweet"),TDe=o(" \u2014 "),e9=a("a"),FDe=o("BertweetTokenizer"),CDe=o(" (Bertweet model)"),MDe=l(),js=a("li"),$W=a("strong"),EDe=o("big_bird"),yDe=o(" \u2014 "),o9=a("a"),wDe=o("BigBirdTokenizer"),ADe=o(" or "),r9=a("a"),LDe=o("BigBirdTokenizerFast"),BDe=o(" (BigBird model)"),xDe=l(),Ns=a("li"),IW=a("strong"),kDe=o("bigbird_pegasus"),RDe=o(" \u2014 "),t9=a("a"),SDe=o("PegasusTokenizer"),PDe=o(" or "),a9=a("a"),$De=o("PegasusTokenizerFast"),IDe=o(" (BigBirdPegasus model)"),jDe=l(),Ds=a("li"),jW=a("strong"),NDe=o("blenderbot"),DDe=o(" \u2014 "),s9=a("a"),qDe=o("BlenderbotTokenizer"),GDe=o(" or "),n9=a("a"),ODe=o("BlenderbotTokenizerFast"),XDe=o(" (Blenderbot model)"),zDe=l(),sg=a("li"),NW=a("strong"),VDe=o("blenderbot-small"),WDe=o(" \u2014 "),l9=a("a"),QDe=o("BlenderbotSmallTokenizer"),HDe=o(" (BlenderbotSmall model)"),UDe=l(),ng=a("li"),DW=a("strong"),JDe=o("byt5"),YDe=o(" \u2014 "),i9=a("a"),KDe=o("ByT5Tokenizer"),ZDe=o(" (ByT5 model)"),eqe=l(),qs=a("li"),qW=a("strong"),oqe=o("camembert"),rqe=o(" \u2014 "),d9=a("a"),tqe=o("CamembertTokenizer"),aqe=o(" or "),c9=a("a"),sqe=o("CamembertTokenizerFast"),nqe=o(" (CamemBERT model)"),lqe=l(),lg=a("li"),GW=a("strong"),iqe=o("canine"),dqe=o(" \u2014 "),m9=a("a"),cqe=o("CanineTokenizer"),mqe=o(" (Canine model)"),fqe=l(),Gs=a("li"),OW=a("strong"),gqe=o("clip"),hqe=o(" \u2014 "),f9=a("a"),uqe=o("CLIPTokenizer"),pqe=o(" or "),g9=a("a"),_qe=o("CLIPTokenizerFast"),bqe=o(" (CLIP model)"),vqe=l(),Os=a("li"),XW=a("strong"),Tqe=o("convbert"),Fqe=o(" \u2014 "),h9=a("a"),Cqe=o("ConvBertTokenizer"),Mqe=o(" or "),u9=a("a"),Eqe=o("ConvBertTokenizerFast"),yqe=o(" (ConvBERT model)"),wqe=l(),Xs=a("li"),zW=a("strong"),Aqe=o("cpm"),Lqe=o(" \u2014 "),p9=a("a"),Bqe=o("CpmTokenizer"),xqe=o(" or "),VW=a("code"),kqe=o("CpmTokenizerFast"),Rqe=o(" (CPM model)"),Sqe=l(),ig=a("li"),WW=a("strong"),Pqe=o("ctrl"),$qe=o(" \u2014 "),_9=a("a"),Iqe=o("CTRLTokenizer"),jqe=o(" (CTRL model)"),Nqe=l(),zs=a("li"),QW=a("strong"),Dqe=o("deberta"),qqe=o(" \u2014 "),b9=a("a"),Gqe=o("DebertaTokenizer"),Oqe=o(" or "),v9=a("a"),Xqe=o("DebertaTokenizerFast"),zqe=o(" (DeBERTa model)"),Vqe=l(),dg=a("li"),HW=a("strong"),Wqe=o("deberta-v2"),Qqe=o(" \u2014 "),T9=a("a"),Hqe=o("DebertaV2Tokenizer"),Uqe=o(" (DeBERTa-v2 model)"),Jqe=l(),Vs=a("li"),UW=a("strong"),Yqe=o("distilbert"),Kqe=o(" \u2014 "),F9=a("a"),Zqe=o("DistilBertTokenizer"),eGe=o(" or "),C9=a("a"),oGe=o("DistilBertTokenizerFast"),rGe=o(" (DistilBERT model)"),tGe=l(),Ws=a("li"),JW=a("strong"),aGe=o("dpr"),sGe=o(" \u2014 "),M9=a("a"),nGe=o("DPRQuestionEncoderTokenizer"),lGe=o(" or "),E9=a("a"),iGe=o("DPRQuestionEncoderTokenizerFast"),dGe=o(" (DPR model)"),cGe=l(),Qs=a("li"),YW=a("strong"),mGe=o("electra"),fGe=o(" \u2014 "),y9=a("a"),gGe=o("ElectraTokenizer"),hGe=o(" or "),w9=a("a"),uGe=o("ElectraTokenizerFast"),pGe=o(" (ELECTRA model)"),_Ge=l(),cg=a("li"),KW=a("strong"),bGe=o("flaubert"),vGe=o(" \u2014 "),A9=a("a"),TGe=o("FlaubertTokenizer"),FGe=o(" (FlauBERT model)"),CGe=l(),Hs=a("li"),ZW=a("strong"),MGe=o("fnet"),EGe=o(" \u2014 "),L9=a("a"),yGe=o("FNetTokenizer"),wGe=o(" or "),B9=a("a"),AGe=o("FNetTokenizerFast"),LGe=o(" (FNet model)"),BGe=l(),mg=a("li"),eQ=a("strong"),xGe=o("fsmt"),kGe=o(" \u2014 "),x9=a("a"),RGe=o("FSMTTokenizer"),SGe=o(" (FairSeq Machine-Translation model)"),PGe=l(),Us=a("li"),oQ=a("strong"),$Ge=o("funnel"),IGe=o(" \u2014 "),k9=a("a"),jGe=o("FunnelTokenizer"),NGe=o(" or "),R9=a("a"),DGe=o("FunnelTokenizerFast"),qGe=o(" (Funnel Transformer model)"),GGe=l(),Js=a("li"),rQ=a("strong"),OGe=o("gpt2"),XGe=o(" \u2014 "),S9=a("a"),zGe=o("GPT2Tokenizer"),VGe=o(" or "),P9=a("a"),WGe=o("GPT2TokenizerFast"),QGe=o(" (OpenAI GPT-2 model)"),HGe=l(),Ys=a("li"),tQ=a("strong"),UGe=o("gpt_neo"),JGe=o(" \u2014 "),$9=a("a"),YGe=o("GPT2Tokenizer"),KGe=o(" or "),I9=a("a"),ZGe=o("GPT2TokenizerFast"),eOe=o(" (GPT Neo model)"),oOe=l(),Ks=a("li"),aQ=a("strong"),rOe=o("herbert"),tOe=o(" \u2014 "),j9=a("a"),aOe=o("HerbertTokenizer"),sOe=o(" or "),N9=a("a"),nOe=o("HerbertTokenizerFast"),lOe=o(" (HerBERT model)"),iOe=l(),fg=a("li"),sQ=a("strong"),dOe=o("hubert"),cOe=o(" \u2014 "),D9=a("a"),mOe=o("Wav2Vec2CTCTokenizer"),fOe=o(" (Hubert model)"),gOe=l(),Zs=a("li"),nQ=a("strong"),hOe=o("ibert"),uOe=o(" \u2014 "),q9=a("a"),pOe=o("RobertaTokenizer"),_Oe=o(" or "),G9=a("a"),bOe=o("RobertaTokenizerFast"),vOe=o(" (I-BERT model)"),TOe=l(),en=a("li"),lQ=a("strong"),FOe=o("layoutlm"),COe=o(" \u2014 "),O9=a("a"),MOe=o("LayoutLMTokenizer"),EOe=o(" or "),X9=a("a"),yOe=o("LayoutLMTokenizerFast"),wOe=o(" (LayoutLM model)"),AOe=l(),on=a("li"),iQ=a("strong"),LOe=o("layoutlmv2"),BOe=o(" \u2014 "),z9=a("a"),xOe=o("LayoutLMv2Tokenizer"),kOe=o(" or "),V9=a("a"),ROe=o("LayoutLMv2TokenizerFast"),SOe=o(" (LayoutLMv2 model)"),POe=l(),rn=a("li"),dQ=a("strong"),$Oe=o("layoutxlm"),IOe=o(" \u2014 "),W9=a("a"),jOe=o("LayoutXLMTokenizer"),NOe=o(" or "),Q9=a("a"),DOe=o("LayoutXLMTokenizerFast"),qOe=o(" (LayoutXLM model)"),GOe=l(),tn=a("li"),cQ=a("strong"),OOe=o("led"),XOe=o(" \u2014 "),H9=a("a"),zOe=o("LEDTokenizer"),VOe=o(" or "),U9=a("a"),WOe=o("LEDTokenizerFast"),QOe=o(" (LED model)"),HOe=l(),an=a("li"),mQ=a("strong"),UOe=o("longformer"),JOe=o(" \u2014 "),J9=a("a"),YOe=o("LongformerTokenizer"),KOe=o(" or "),Y9=a("a"),ZOe=o("LongformerTokenizerFast"),eXe=o(" (Longformer model)"),oXe=l(),gg=a("li"),fQ=a("strong"),rXe=o("luke"),tXe=o(" \u2014 "),K9=a("a"),aXe=o("LukeTokenizer"),sXe=o(" (LUKE model)"),nXe=l(),sn=a("li"),gQ=a("strong"),lXe=o("lxmert"),iXe=o(" \u2014 "),Z9=a("a"),dXe=o("LxmertTokenizer"),cXe=o(" or "),eB=a("a"),mXe=o("LxmertTokenizerFast"),fXe=o(" (LXMERT model)"),gXe=l(),hg=a("li"),hQ=a("strong"),hXe=o("m2m_100"),uXe=o(" \u2014 "),oB=a("a"),pXe=o("M2M100Tokenizer"),_Xe=o(" (M2M100 model)"),bXe=l(),ug=a("li"),uQ=a("strong"),vXe=o("marian"),TXe=o(" \u2014 "),rB=a("a"),FXe=o("MarianTokenizer"),CXe=o(" (Marian model)"),MXe=l(),nn=a("li"),pQ=a("strong"),EXe=o("mbart"),yXe=o(" \u2014 "),tB=a("a"),wXe=o("MBartTokenizer"),AXe=o(" or "),aB=a("a"),LXe=o("MBartTokenizerFast"),BXe=o(" (mBART model)"),xXe=l(),ln=a("li"),_Q=a("strong"),kXe=o("mbart50"),RXe=o(" \u2014 "),sB=a("a"),SXe=o("MBart50Tokenizer"),PXe=o(" or "),nB=a("a"),$Xe=o("MBart50TokenizerFast"),IXe=o(" (mBART-50 model)"),jXe=l(),pg=a("li"),bQ=a("strong"),NXe=o("mluke"),DXe=o(" \u2014 "),lB=a("a"),qXe=o("MLukeTokenizer"),GXe=o(" (mLUKE model)"),OXe=l(),dn=a("li"),vQ=a("strong"),XXe=o("mobilebert"),zXe=o(" \u2014 "),iB=a("a"),VXe=o("MobileBertTokenizer"),WXe=o(" or "),dB=a("a"),QXe=o("MobileBertTokenizerFast"),HXe=o(" (MobileBERT model)"),UXe=l(),cn=a("li"),TQ=a("strong"),JXe=o("mpnet"),YXe=o(" \u2014 "),cB=a("a"),KXe=o("MPNetTokenizer"),ZXe=o(" or "),mB=a("a"),eze=o("MPNetTokenizerFast"),oze=o(" (MPNet model)"),rze=l(),mn=a("li"),FQ=a("strong"),tze=o("mt5"),aze=o(" \u2014 "),fB=a("a"),sze=o("MT5Tokenizer"),nze=o(" or "),gB=a("a"),lze=o("MT5TokenizerFast"),ize=o(" (mT5 model)"),dze=l(),fn=a("li"),CQ=a("strong"),cze=o("openai-gpt"),mze=o(" \u2014 "),hB=a("a"),fze=o("OpenAIGPTTokenizer"),gze=o(" or "),uB=a("a"),hze=o("OpenAIGPTTokenizerFast"),uze=o(" (OpenAI GPT model)"),pze=l(),gn=a("li"),MQ=a("strong"),_ze=o("pegasus"),bze=o(" \u2014 "),pB=a("a"),vze=o("PegasusTokenizer"),Tze=o(" or "),_B=a("a"),Fze=o("PegasusTokenizerFast"),Cze=o(" (Pegasus model)"),Mze=l(),_g=a("li"),EQ=a("strong"),Eze=o("perceiver"),yze=o(" \u2014 "),bB=a("a"),wze=o("PerceiverTokenizer"),Aze=o(" (Perceiver model)"),Lze=l(),bg=a("li"),yQ=a("strong"),Bze=o("phobert"),xze=o(" \u2014 "),vB=a("a"),kze=o("PhobertTokenizer"),Rze=o(" (PhoBERT model)"),Sze=l(),vg=a("li"),wQ=a("strong"),Pze=o("prophetnet"),$ze=o(" \u2014 "),TB=a("a"),Ize=o("ProphetNetTokenizer"),jze=o(" (ProphetNet model)"),Nze=l(),hn=a("li"),AQ=a("strong"),Dze=o("qdqbert"),qze=o(" \u2014 "),FB=a("a"),Gze=o("BertTokenizer"),Oze=o(" or "),CB=a("a"),Xze=o("BertTokenizerFast"),zze=o(" (QDQBert model)"),Vze=l(),Tg=a("li"),LQ=a("strong"),Wze=o("rag"),Qze=o(" \u2014 "),MB=a("a"),Hze=o("RagTokenizer"),Uze=o(" (RAG model)"),Jze=l(),un=a("li"),BQ=a("strong"),Yze=o("reformer"),Kze=o(" \u2014 "),EB=a("a"),Zze=o("ReformerTokenizer"),eVe=o(" or "),yB=a("a"),oVe=o("ReformerTokenizerFast"),rVe=o(" (Reformer model)"),tVe=l(),pn=a("li"),xQ=a("strong"),aVe=o("rembert"),sVe=o(" \u2014 "),wB=a("a"),nVe=o("RemBertTokenizer"),lVe=o(" or "),AB=a("a"),iVe=o("RemBertTokenizerFast"),dVe=o(" (RemBERT model)"),cVe=l(),_n=a("li"),kQ=a("strong"),mVe=o("retribert"),fVe=o(" \u2014 "),LB=a("a"),gVe=o("RetriBertTokenizer"),hVe=o(" or "),BB=a("a"),uVe=o("RetriBertTokenizerFast"),pVe=o(" (RetriBERT model)"),_Ve=l(),bn=a("li"),RQ=a("strong"),bVe=o("roberta"),vVe=o(" \u2014 "),xB=a("a"),TVe=o("RobertaTokenizer"),FVe=o(" or "),kB=a("a"),CVe=o("RobertaTokenizerFast"),MVe=o(" (RoBERTa model)"),EVe=l(),vn=a("li"),SQ=a("strong"),yVe=o("roformer"),wVe=o(" \u2014 "),RB=a("a"),AVe=o("RoFormerTokenizer"),LVe=o(" or "),SB=a("a"),BVe=o("RoFormerTokenizerFast"),xVe=o(" (RoFormer model)"),kVe=l(),Fg=a("li"),PQ=a("strong"),RVe=o("speech_to_text"),SVe=o(" \u2014 "),PB=a("a"),PVe=o("Speech2TextTokenizer"),$Ve=o(" (Speech2Text model)"),IVe=l(),Cg=a("li"),$Q=a("strong"),jVe=o("speech_to_text_2"),NVe=o(" \u2014 "),$B=a("a"),DVe=o("Speech2Text2Tokenizer"),qVe=o(" (Speech2Text2 model)"),GVe=l(),Tn=a("li"),IQ=a("strong"),OVe=o("splinter"),XVe=o(" \u2014 "),IB=a("a"),zVe=o("SplinterTokenizer"),VVe=o(" or "),jB=a("a"),WVe=o("SplinterTokenizerFast"),QVe=o(" (Splinter model)"),HVe=l(),Fn=a("li"),jQ=a("strong"),UVe=o("squeezebert"),JVe=o(" \u2014 "),NB=a("a"),YVe=o("SqueezeBertTokenizer"),KVe=o(" or "),DB=a("a"),ZVe=o("SqueezeBertTokenizerFast"),eWe=o(" (SqueezeBERT model)"),oWe=l(),Cn=a("li"),NQ=a("strong"),rWe=o("t5"),tWe=o(" \u2014 "),qB=a("a"),aWe=o("T5Tokenizer"),sWe=o(" or "),GB=a("a"),nWe=o("T5TokenizerFast"),lWe=o(" (T5 model)"),iWe=l(),Mg=a("li"),DQ=a("strong"),dWe=o("tapas"),cWe=o(" \u2014 "),OB=a("a"),mWe=o("TapasTokenizer"),fWe=o(" (TAPAS model)"),gWe=l(),Eg=a("li"),qQ=a("strong"),hWe=o("transfo-xl"),uWe=o(" \u2014 "),XB=a("a"),pWe=o("TransfoXLTokenizer"),_We=o(" (Transformer-XL model)"),bWe=l(),yg=a("li"),GQ=a("strong"),vWe=o("wav2vec2"),TWe=o(" \u2014 "),zB=a("a"),FWe=o("Wav2Vec2CTCTokenizer"),CWe=o(" (Wav2Vec2 model)"),MWe=l(),wg=a("li"),OQ=a("strong"),EWe=o("wav2vec2_phoneme"),yWe=o(" \u2014 "),VB=a("a"),wWe=o("Wav2Vec2PhonemeCTCTokenizer"),AWe=o(" (Wav2Vec2Phoneme model)"),LWe=l(),Mn=a("li"),XQ=a("strong"),BWe=o("xglm"),xWe=o(" \u2014 "),WB=a("a"),kWe=o("XGLMTokenizer"),RWe=o(" or "),QB=a("a"),SWe=o("XGLMTokenizerFast"),PWe=o(" (XGLM model)"),$We=l(),Ag=a("li"),zQ=a("strong"),IWe=o("xlm"),jWe=o(" \u2014 "),HB=a("a"),NWe=o("XLMTokenizer"),DWe=o(" (XLM model)"),qWe=l(),Lg=a("li"),VQ=a("strong"),GWe=o("xlm-prophetnet"),OWe=o(" \u2014 "),UB=a("a"),XWe=o("XLMProphetNetTokenizer"),zWe=o(" (XLMProphetNet model)"),VWe=l(),En=a("li"),WQ=a("strong"),WWe=o("xlm-roberta"),QWe=o(" \u2014 "),JB=a("a"),HWe=o("XLMRobertaTokenizer"),UWe=o(" or "),YB=a("a"),JWe=o("XLMRobertaTokenizerFast"),YWe=o(" (XLM-RoBERTa model)"),KWe=l(),yn=a("li"),QQ=a("strong"),ZWe=o("xlnet"),eQe=o(" \u2014 "),KB=a("a"),oQe=o("XLNetTokenizer"),rQe=o(" or "),ZB=a("a"),tQe=o("XLNetTokenizerFast"),aQe=o(" (XLNet model)"),sQe=l(),HQ=a("p"),nQe=o("Examples:"),lQe=l(),m(z3.$$.fragment),iQe=l(),Bg=a("div"),m(V3.$$.fragment),dQe=l(),UQ=a("p"),cQe=o("Register a new tokenizer in this mapping."),FLe=l(),Li=a("h2"),xg=a("a"),JQ=a("span"),m(W3.$$.fragment),mQe=l(),YQ=a("span"),fQe=o("AutoFeatureExtractor"),CLe=l(),Zt=a("div"),m(Q3.$$.fragment),gQe=l(),H3=a("p"),hQe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),ex=a("a"),uQe=o("AutoFeatureExtractor.from_pretrained()"),pQe=o(" class method."),_Qe=l(),U3=a("p"),bQe=o("This class cannot be instantiated directly using "),KQ=a("code"),vQe=o("__init__()"),TQe=o(" (throws an error)."),FQe=l(),Le=a("div"),m(J3.$$.fragment),CQe=l(),ZQ=a("p"),MQe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),EQe=l(),Pa=a("p"),yQe=o("The feature extractor class to instantiate is selected based on the "),eH=a("code"),wQe=o("model_type"),AQe=o(` property of the config object
(either passed as an argument or loaded from `),oH=a("code"),LQe=o("pretrained_model_name_or_path"),BQe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),rH=a("code"),xQe=o("pretrained_model_name_or_path"),kQe=o(":"),RQe=l(),ne=a("ul"),kg=a("li"),tH=a("strong"),SQe=o("beit"),PQe=o(" \u2014 "),ox=a("a"),$Qe=o("BeitFeatureExtractor"),IQe=o(" (BEiT model)"),jQe=l(),Rg=a("li"),aH=a("strong"),NQe=o("clip"),DQe=o(" \u2014 "),rx=a("a"),qQe=o("CLIPFeatureExtractor"),GQe=o(" (CLIP model)"),OQe=l(),Sg=a("li"),sH=a("strong"),XQe=o("convnext"),zQe=o(" \u2014 "),tx=a("a"),VQe=o("ConvNextFeatureExtractor"),WQe=o(" (ConvNext model)"),QQe=l(),Pg=a("li"),nH=a("strong"),HQe=o("deit"),UQe=o(" \u2014 "),ax=a("a"),JQe=o("DeiTFeatureExtractor"),YQe=o(" (DeiT model)"),KQe=l(),$g=a("li"),lH=a("strong"),ZQe=o("detr"),eHe=o(" \u2014 "),sx=a("a"),oHe=o("DetrFeatureExtractor"),rHe=o(" (DETR model)"),tHe=l(),Ig=a("li"),iH=a("strong"),aHe=o("hubert"),sHe=o(" \u2014 "),nx=a("a"),nHe=o("Wav2Vec2FeatureExtractor"),lHe=o(" (Hubert model)"),iHe=l(),jg=a("li"),dH=a("strong"),dHe=o("layoutlmv2"),cHe=o(" \u2014 "),lx=a("a"),mHe=o("LayoutLMv2FeatureExtractor"),fHe=o(" (LayoutLMv2 model)"),gHe=l(),Ng=a("li"),cH=a("strong"),hHe=o("perceiver"),uHe=o(" \u2014 "),ix=a("a"),pHe=o("PerceiverFeatureExtractor"),_He=o(" (Perceiver model)"),bHe=l(),Dg=a("li"),mH=a("strong"),vHe=o("segformer"),THe=o(" \u2014 "),dx=a("a"),FHe=o("SegformerFeatureExtractor"),CHe=o(" (SegFormer model)"),MHe=l(),qg=a("li"),fH=a("strong"),EHe=o("speech_to_text"),yHe=o(" \u2014 "),cx=a("a"),wHe=o("Speech2TextFeatureExtractor"),AHe=o(" (Speech2Text model)"),LHe=l(),Gg=a("li"),gH=a("strong"),BHe=o("swin"),xHe=o(" \u2014 "),mx=a("a"),kHe=o("ViTFeatureExtractor"),RHe=o(" (Swin model)"),SHe=l(),Og=a("li"),hH=a("strong"),PHe=o("vit"),$He=o(" \u2014 "),fx=a("a"),IHe=o("ViTFeatureExtractor"),jHe=o(" (ViT model)"),NHe=l(),Xg=a("li"),uH=a("strong"),DHe=o("vit_mae"),qHe=o(" \u2014 "),gx=a("a"),GHe=o("ViTFeatureExtractor"),OHe=o(" (ViTMAE model)"),XHe=l(),zg=a("li"),pH=a("strong"),zHe=o("wav2vec2"),VHe=o(" \u2014 "),hx=a("a"),WHe=o("Wav2Vec2FeatureExtractor"),QHe=o(" (Wav2Vec2 model)"),HHe=l(),m(Vg.$$.fragment),UHe=l(),_H=a("p"),JHe=o("Examples:"),YHe=l(),m(Y3.$$.fragment),MLe=l(),Bi=a("h2"),Wg=a("a"),bH=a("span"),m(K3.$$.fragment),KHe=l(),vH=a("span"),ZHe=o("AutoProcessor"),ELe=l(),ea=a("div"),m(Z3.$$.fragment),eUe=l(),e5=a("p"),oUe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ux=a("a"),rUe=o("AutoProcessor.from_pretrained()"),tUe=o(" class method."),aUe=l(),o5=a("p"),sUe=o("This class cannot be instantiated directly using "),TH=a("code"),nUe=o("__init__()"),lUe=o(" (throws an error)."),iUe=l(),Be=a("div"),m(r5.$$.fragment),dUe=l(),FH=a("p"),cUe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),mUe=l(),xi=a("p"),fUe=o("The processor class to instantiate is selected based on the "),CH=a("code"),gUe=o("model_type"),hUe=o(` property of the config object (either
passed as an argument or loaded from `),MH=a("code"),uUe=o("pretrained_model_name_or_path"),pUe=o(" if possible):"),_Ue=l(),ye=a("ul"),Qg=a("li"),EH=a("strong"),bUe=o("clip"),vUe=o(" \u2014 "),px=a("a"),TUe=o("CLIPProcessor"),FUe=o(" (CLIP model)"),CUe=l(),Hg=a("li"),yH=a("strong"),MUe=o("layoutlmv2"),EUe=o(" \u2014 "),_x=a("a"),yUe=o("LayoutLMv2Processor"),wUe=o(" (LayoutLMv2 model)"),AUe=l(),Ug=a("li"),wH=a("strong"),LUe=o("layoutxlm"),BUe=o(" \u2014 "),bx=a("a"),xUe=o("LayoutXLMProcessor"),kUe=o(" (LayoutXLM model)"),RUe=l(),Jg=a("li"),AH=a("strong"),SUe=o("speech_to_text"),PUe=o(" \u2014 "),vx=a("a"),$Ue=o("Speech2TextProcessor"),IUe=o(" (Speech2Text model)"),jUe=l(),Yg=a("li"),LH=a("strong"),NUe=o("speech_to_text_2"),DUe=o(" \u2014 "),Tx=a("a"),qUe=o("Speech2Text2Processor"),GUe=o(" (Speech2Text2 model)"),OUe=l(),Kg=a("li"),BH=a("strong"),XUe=o("trocr"),zUe=o(" \u2014 "),Fx=a("a"),VUe=o("TrOCRProcessor"),WUe=o(" (TrOCR model)"),QUe=l(),Zg=a("li"),xH=a("strong"),HUe=o("vision-text-dual-encoder"),UUe=o(" \u2014 "),Cx=a("a"),JUe=o("VisionTextDualEncoderProcessor"),YUe=o(" (VisionTextDualEncoder model)"),KUe=l(),eh=a("li"),kH=a("strong"),ZUe=o("wav2vec2"),eJe=o(" \u2014 "),Mx=a("a"),oJe=o("Wav2Vec2Processor"),rJe=o(" (Wav2Vec2 model)"),tJe=l(),m(oh.$$.fragment),aJe=l(),RH=a("p"),sJe=o("Examples:"),nJe=l(),m(t5.$$.fragment),yLe=l(),ki=a("h2"),rh=a("a"),SH=a("span"),m(a5.$$.fragment),lJe=l(),PH=a("span"),iJe=o("AutoModel"),wLe=l(),Oo=a("div"),m(s5.$$.fragment),dJe=l(),Ri=a("p"),cJe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),$H=a("code"),mJe=o("from_pretrained()"),fJe=o("class method or the "),IH=a("code"),gJe=o("from_config()"),hJe=o(`class
method.`),uJe=l(),n5=a("p"),pJe=o("This class cannot be instantiated directly using "),jH=a("code"),_Je=o("__init__()"),bJe=o(" (throws an error)."),vJe=l(),Pr=a("div"),m(l5.$$.fragment),TJe=l(),NH=a("p"),FJe=o("Instantiates one of the base model classes of the library from a configuration."),CJe=l(),Si=a("p"),MJe=o(`Note:
Loading a model from its configuration file does `),DH=a("strong"),EJe=o("not"),yJe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qH=a("code"),wJe=o("from_pretrained()"),AJe=o("to load the model weights."),LJe=l(),GH=a("p"),BJe=o("Examples:"),xJe=l(),m(i5.$$.fragment),kJe=l(),xe=a("div"),m(d5.$$.fragment),RJe=l(),OH=a("p"),SJe=o("Instantiate one of the base model classes of the library from a pretrained model."),PJe=l(),$a=a("p"),$Je=o("The model class to instantiate is selected based on the "),XH=a("code"),IJe=o("model_type"),jJe=o(` property of the config object (either
passed as an argument or loaded from `),zH=a("code"),NJe=o("pretrained_model_name_or_path"),DJe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),VH=a("code"),qJe=o("pretrained_model_name_or_path"),GJe=o(":"),OJe=l(),F=a("ul"),th=a("li"),WH=a("strong"),XJe=o("albert"),zJe=o(" \u2014 "),Ex=a("a"),VJe=o("AlbertModel"),WJe=o(" (ALBERT model)"),QJe=l(),ah=a("li"),QH=a("strong"),HJe=o("bart"),UJe=o(" \u2014 "),yx=a("a"),JJe=o("BartModel"),YJe=o(" (BART model)"),KJe=l(),sh=a("li"),HH=a("strong"),ZJe=o("beit"),eYe=o(" \u2014 "),wx=a("a"),oYe=o("BeitModel"),rYe=o(" (BEiT model)"),tYe=l(),nh=a("li"),UH=a("strong"),aYe=o("bert"),sYe=o(" \u2014 "),Ax=a("a"),nYe=o("BertModel"),lYe=o(" (BERT model)"),iYe=l(),lh=a("li"),JH=a("strong"),dYe=o("bert-generation"),cYe=o(" \u2014 "),Lx=a("a"),mYe=o("BertGenerationEncoder"),fYe=o(" (Bert Generation model)"),gYe=l(),ih=a("li"),YH=a("strong"),hYe=o("big_bird"),uYe=o(" \u2014 "),Bx=a("a"),pYe=o("BigBirdModel"),_Ye=o(" (BigBird model)"),bYe=l(),dh=a("li"),KH=a("strong"),vYe=o("bigbird_pegasus"),TYe=o(" \u2014 "),xx=a("a"),FYe=o("BigBirdPegasusModel"),CYe=o(" (BigBirdPegasus model)"),MYe=l(),ch=a("li"),ZH=a("strong"),EYe=o("blenderbot"),yYe=o(" \u2014 "),kx=a("a"),wYe=o("BlenderbotModel"),AYe=o(" (Blenderbot model)"),LYe=l(),mh=a("li"),eU=a("strong"),BYe=o("blenderbot-small"),xYe=o(" \u2014 "),Rx=a("a"),kYe=o("BlenderbotSmallModel"),RYe=o(" (BlenderbotSmall model)"),SYe=l(),fh=a("li"),oU=a("strong"),PYe=o("camembert"),$Ye=o(" \u2014 "),Sx=a("a"),IYe=o("CamembertModel"),jYe=o(" (CamemBERT model)"),NYe=l(),gh=a("li"),rU=a("strong"),DYe=o("canine"),qYe=o(" \u2014 "),Px=a("a"),GYe=o("CanineModel"),OYe=o(" (Canine model)"),XYe=l(),hh=a("li"),tU=a("strong"),zYe=o("clip"),VYe=o(" \u2014 "),$x=a("a"),WYe=o("CLIPModel"),QYe=o(" (CLIP model)"),HYe=l(),uh=a("li"),aU=a("strong"),UYe=o("convbert"),JYe=o(" \u2014 "),Ix=a("a"),YYe=o("ConvBertModel"),KYe=o(" (ConvBERT model)"),ZYe=l(),ph=a("li"),sU=a("strong"),eKe=o("convnext"),oKe=o(" \u2014 "),jx=a("a"),rKe=o("ConvNextModel"),tKe=o(" (ConvNext model)"),aKe=l(),_h=a("li"),nU=a("strong"),sKe=o("ctrl"),nKe=o(" \u2014 "),Nx=a("a"),lKe=o("CTRLModel"),iKe=o(" (CTRL model)"),dKe=l(),bh=a("li"),lU=a("strong"),cKe=o("deberta"),mKe=o(" \u2014 "),Dx=a("a"),fKe=o("DebertaModel"),gKe=o(" (DeBERTa model)"),hKe=l(),vh=a("li"),iU=a("strong"),uKe=o("deberta-v2"),pKe=o(" \u2014 "),qx=a("a"),_Ke=o("DebertaV2Model"),bKe=o(" (DeBERTa-v2 model)"),vKe=l(),Th=a("li"),dU=a("strong"),TKe=o("deit"),FKe=o(" \u2014 "),Gx=a("a"),CKe=o("DeiTModel"),MKe=o(" (DeiT model)"),EKe=l(),Fh=a("li"),cU=a("strong"),yKe=o("detr"),wKe=o(" \u2014 "),Ox=a("a"),AKe=o("DetrModel"),LKe=o(" (DETR model)"),BKe=l(),Ch=a("li"),mU=a("strong"),xKe=o("distilbert"),kKe=o(" \u2014 "),Xx=a("a"),RKe=o("DistilBertModel"),SKe=o(" (DistilBERT model)"),PKe=l(),Mh=a("li"),fU=a("strong"),$Ke=o("dpr"),IKe=o(" \u2014 "),zx=a("a"),jKe=o("DPRQuestionEncoder"),NKe=o(" (DPR model)"),DKe=l(),Eh=a("li"),gU=a("strong"),qKe=o("electra"),GKe=o(" \u2014 "),Vx=a("a"),OKe=o("ElectraModel"),XKe=o(" (ELECTRA model)"),zKe=l(),yh=a("li"),hU=a("strong"),VKe=o("flaubert"),WKe=o(" \u2014 "),Wx=a("a"),QKe=o("FlaubertModel"),HKe=o(" (FlauBERT model)"),UKe=l(),wh=a("li"),uU=a("strong"),JKe=o("fnet"),YKe=o(" \u2014 "),Qx=a("a"),KKe=o("FNetModel"),ZKe=o(" (FNet model)"),eZe=l(),Ah=a("li"),pU=a("strong"),oZe=o("fsmt"),rZe=o(" \u2014 "),Hx=a("a"),tZe=o("FSMTModel"),aZe=o(" (FairSeq Machine-Translation model)"),sZe=l(),wn=a("li"),_U=a("strong"),nZe=o("funnel"),lZe=o(" \u2014 "),Ux=a("a"),iZe=o("FunnelModel"),dZe=o(" or "),Jx=a("a"),cZe=o("FunnelBaseModel"),mZe=o(" (Funnel Transformer model)"),fZe=l(),Lh=a("li"),bU=a("strong"),gZe=o("gpt2"),hZe=o(" \u2014 "),Yx=a("a"),uZe=o("GPT2Model"),pZe=o(" (OpenAI GPT-2 model)"),_Ze=l(),Bh=a("li"),vU=a("strong"),bZe=o("gpt_neo"),vZe=o(" \u2014 "),Kx=a("a"),TZe=o("GPTNeoModel"),FZe=o(" (GPT Neo model)"),CZe=l(),xh=a("li"),TU=a("strong"),MZe=o("gptj"),EZe=o(" \u2014 "),Zx=a("a"),yZe=o("GPTJModel"),wZe=o(" (GPT-J model)"),AZe=l(),kh=a("li"),FU=a("strong"),LZe=o("hubert"),BZe=o(" \u2014 "),ek=a("a"),xZe=o("HubertModel"),kZe=o(" (Hubert model)"),RZe=l(),Rh=a("li"),CU=a("strong"),SZe=o("ibert"),PZe=o(" \u2014 "),ok=a("a"),$Ze=o("IBertModel"),IZe=o(" (I-BERT model)"),jZe=l(),Sh=a("li"),MU=a("strong"),NZe=o("imagegpt"),DZe=o(" \u2014 "),rk=a("a"),qZe=o("ImageGPTModel"),GZe=o(" (ImageGPT model)"),OZe=l(),Ph=a("li"),EU=a("strong"),XZe=o("layoutlm"),zZe=o(" \u2014 "),tk=a("a"),VZe=o("LayoutLMModel"),WZe=o(" (LayoutLM model)"),QZe=l(),$h=a("li"),yU=a("strong"),HZe=o("layoutlmv2"),UZe=o(" \u2014 "),ak=a("a"),JZe=o("LayoutLMv2Model"),YZe=o(" (LayoutLMv2 model)"),KZe=l(),Ih=a("li"),wU=a("strong"),ZZe=o("led"),eeo=o(" \u2014 "),sk=a("a"),oeo=o("LEDModel"),reo=o(" (LED model)"),teo=l(),jh=a("li"),AU=a("strong"),aeo=o("longformer"),seo=o(" \u2014 "),nk=a("a"),neo=o("LongformerModel"),leo=o(" (Longformer model)"),ieo=l(),Nh=a("li"),LU=a("strong"),deo=o("luke"),ceo=o(" \u2014 "),lk=a("a"),meo=o("LukeModel"),feo=o(" (LUKE model)"),geo=l(),Dh=a("li"),BU=a("strong"),heo=o("lxmert"),ueo=o(" \u2014 "),ik=a("a"),peo=o("LxmertModel"),_eo=o(" (LXMERT model)"),beo=l(),qh=a("li"),xU=a("strong"),veo=o("m2m_100"),Teo=o(" \u2014 "),dk=a("a"),Feo=o("M2M100Model"),Ceo=o(" (M2M100 model)"),Meo=l(),Gh=a("li"),kU=a("strong"),Eeo=o("marian"),yeo=o(" \u2014 "),ck=a("a"),weo=o("MarianModel"),Aeo=o(" (Marian model)"),Leo=l(),Oh=a("li"),RU=a("strong"),Beo=o("mbart"),xeo=o(" \u2014 "),mk=a("a"),keo=o("MBartModel"),Reo=o(" (mBART model)"),Seo=l(),Xh=a("li"),SU=a("strong"),Peo=o("megatron-bert"),$eo=o(" \u2014 "),fk=a("a"),Ieo=o("MegatronBertModel"),jeo=o(" (MegatronBert model)"),Neo=l(),zh=a("li"),PU=a("strong"),Deo=o("mobilebert"),qeo=o(" \u2014 "),gk=a("a"),Geo=o("MobileBertModel"),Oeo=o(" (MobileBERT model)"),Xeo=l(),Vh=a("li"),$U=a("strong"),zeo=o("mpnet"),Veo=o(" \u2014 "),hk=a("a"),Weo=o("MPNetModel"),Qeo=o(" (MPNet model)"),Heo=l(),Wh=a("li"),IU=a("strong"),Ueo=o("mt5"),Jeo=o(" \u2014 "),uk=a("a"),Yeo=o("MT5Model"),Keo=o(" (mT5 model)"),Zeo=l(),Qh=a("li"),jU=a("strong"),eoo=o("nystromformer"),ooo=o(" \u2014 "),pk=a("a"),roo=o("NystromformerModel"),too=o(" (Nystromformer model)"),aoo=l(),Hh=a("li"),NU=a("strong"),soo=o("openai-gpt"),noo=o(" \u2014 "),_k=a("a"),loo=o("OpenAIGPTModel"),ioo=o(" (OpenAI GPT model)"),doo=l(),Uh=a("li"),DU=a("strong"),coo=o("pegasus"),moo=o(" \u2014 "),bk=a("a"),foo=o("PegasusModel"),goo=o(" (Pegasus model)"),hoo=l(),Jh=a("li"),qU=a("strong"),uoo=o("perceiver"),poo=o(" \u2014 "),vk=a("a"),_oo=o("PerceiverModel"),boo=o(" (Perceiver model)"),voo=l(),Yh=a("li"),GU=a("strong"),Too=o("prophetnet"),Foo=o(" \u2014 "),Tk=a("a"),Coo=o("ProphetNetModel"),Moo=o(" (ProphetNet model)"),Eoo=l(),Kh=a("li"),OU=a("strong"),yoo=o("qdqbert"),woo=o(" \u2014 "),XU=a("code"),Aoo=o("QDQBertModel"),Loo=o("(QDQBert model)"),Boo=l(),Zh=a("li"),zU=a("strong"),xoo=o("reformer"),koo=o(" \u2014 "),Fk=a("a"),Roo=o("ReformerModel"),Soo=o(" (Reformer model)"),Poo=l(),eu=a("li"),VU=a("strong"),$oo=o("rembert"),Ioo=o(" \u2014 "),Ck=a("a"),joo=o("RemBertModel"),Noo=o(" (RemBERT model)"),Doo=l(),ou=a("li"),WU=a("strong"),qoo=o("retribert"),Goo=o(" \u2014 "),Mk=a("a"),Ooo=o("RetriBertModel"),Xoo=o(" (RetriBERT model)"),zoo=l(),ru=a("li"),QU=a("strong"),Voo=o("roberta"),Woo=o(" \u2014 "),Ek=a("a"),Qoo=o("RobertaModel"),Hoo=o(" (RoBERTa model)"),Uoo=l(),tu=a("li"),HU=a("strong"),Joo=o("roformer"),Yoo=o(" \u2014 "),yk=a("a"),Koo=o("RoFormerModel"),Zoo=o(" (RoFormer model)"),ero=l(),au=a("li"),UU=a("strong"),oro=o("segformer"),rro=o(" \u2014 "),wk=a("a"),tro=o("SegformerModel"),aro=o(" (SegFormer model)"),sro=l(),su=a("li"),JU=a("strong"),nro=o("sew"),lro=o(" \u2014 "),Ak=a("a"),iro=o("SEWModel"),dro=o(" (SEW model)"),cro=l(),nu=a("li"),YU=a("strong"),mro=o("sew-d"),fro=o(" \u2014 "),Lk=a("a"),gro=o("SEWDModel"),hro=o(" (SEW-D model)"),uro=l(),lu=a("li"),KU=a("strong"),pro=o("speech_to_text"),_ro=o(" \u2014 "),Bk=a("a"),bro=o("Speech2TextModel"),vro=o(" (Speech2Text model)"),Tro=l(),iu=a("li"),ZU=a("strong"),Fro=o("splinter"),Cro=o(" \u2014 "),xk=a("a"),Mro=o("SplinterModel"),Ero=o(" (Splinter model)"),yro=l(),du=a("li"),eJ=a("strong"),wro=o("squeezebert"),Aro=o(" \u2014 "),kk=a("a"),Lro=o("SqueezeBertModel"),Bro=o(" (SqueezeBERT model)"),xro=l(),cu=a("li"),oJ=a("strong"),kro=o("swin"),Rro=o(" \u2014 "),Rk=a("a"),Sro=o("SwinModel"),Pro=o(" (Swin model)"),$ro=l(),mu=a("li"),rJ=a("strong"),Iro=o("t5"),jro=o(" \u2014 "),Sk=a("a"),Nro=o("T5Model"),Dro=o(" (T5 model)"),qro=l(),fu=a("li"),tJ=a("strong"),Gro=o("tapas"),Oro=o(" \u2014 "),Pk=a("a"),Xro=o("TapasModel"),zro=o(" (TAPAS model)"),Vro=l(),gu=a("li"),aJ=a("strong"),Wro=o("transfo-xl"),Qro=o(" \u2014 "),$k=a("a"),Hro=o("TransfoXLModel"),Uro=o(" (Transformer-XL model)"),Jro=l(),hu=a("li"),sJ=a("strong"),Yro=o("unispeech"),Kro=o(" \u2014 "),Ik=a("a"),Zro=o("UniSpeechModel"),eto=o(" (UniSpeech model)"),oto=l(),uu=a("li"),nJ=a("strong"),rto=o("unispeech-sat"),tto=o(" \u2014 "),jk=a("a"),ato=o("UniSpeechSatModel"),sto=o(" (UniSpeechSat model)"),nto=l(),pu=a("li"),lJ=a("strong"),lto=o("vilt"),ito=o(" \u2014 "),Nk=a("a"),dto=o("ViltModel"),cto=o(" (ViLT model)"),mto=l(),_u=a("li"),iJ=a("strong"),fto=o("vision-text-dual-encoder"),gto=o(" \u2014 "),Dk=a("a"),hto=o("VisionTextDualEncoderModel"),uto=o(" (VisionTextDualEncoder model)"),pto=l(),bu=a("li"),dJ=a("strong"),_to=o("visual_bert"),bto=o(" \u2014 "),qk=a("a"),vto=o("VisualBertModel"),Tto=o(" (VisualBert model)"),Fto=l(),vu=a("li"),cJ=a("strong"),Cto=o("vit"),Mto=o(" \u2014 "),Gk=a("a"),Eto=o("ViTModel"),yto=o(" (ViT model)"),wto=l(),Tu=a("li"),mJ=a("strong"),Ato=o("vit_mae"),Lto=o(" \u2014 "),Ok=a("a"),Bto=o("ViTMAEModel"),xto=o(" (ViTMAE model)"),kto=l(),Fu=a("li"),fJ=a("strong"),Rto=o("wav2vec2"),Sto=o(" \u2014 "),Xk=a("a"),Pto=o("Wav2Vec2Model"),$to=o(" (Wav2Vec2 model)"),Ito=l(),Cu=a("li"),gJ=a("strong"),jto=o("wavlm"),Nto=o(" \u2014 "),zk=a("a"),Dto=o("WavLMModel"),qto=o(" (WavLM model)"),Gto=l(),Mu=a("li"),hJ=a("strong"),Oto=o("xglm"),Xto=o(" \u2014 "),Vk=a("a"),zto=o("XGLMModel"),Vto=o(" (XGLM model)"),Wto=l(),Eu=a("li"),uJ=a("strong"),Qto=o("xlm"),Hto=o(" \u2014 "),Wk=a("a"),Uto=o("XLMModel"),Jto=o(" (XLM model)"),Yto=l(),yu=a("li"),pJ=a("strong"),Kto=o("xlm-prophetnet"),Zto=o(" \u2014 "),Qk=a("a"),eao=o("XLMProphetNetModel"),oao=o(" (XLMProphetNet model)"),rao=l(),wu=a("li"),_J=a("strong"),tao=o("xlm-roberta"),aao=o(" \u2014 "),Hk=a("a"),sao=o("XLMRobertaModel"),nao=o(" (XLM-RoBERTa model)"),lao=l(),Au=a("li"),bJ=a("strong"),iao=o("xlm-roberta-xl"),dao=o(" \u2014 "),Uk=a("a"),cao=o("XLMRobertaXLModel"),mao=o(" (XLM-RoBERTa-XL model)"),fao=l(),Lu=a("li"),vJ=a("strong"),gao=o("xlnet"),hao=o(" \u2014 "),Jk=a("a"),uao=o("XLNetModel"),pao=o(" (XLNet model)"),_ao=l(),Bu=a("li"),TJ=a("strong"),bao=o("yoso"),vao=o(" \u2014 "),Yk=a("a"),Tao=o("YosoModel"),Fao=o(" (YOSO model)"),Cao=l(),xu=a("p"),Mao=o("The model is set in evaluation mode by default using "),FJ=a("code"),Eao=o("model.eval()"),yao=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),CJ=a("code"),wao=o("model.train()"),Aao=l(),MJ=a("p"),Lao=o("Examples:"),Bao=l(),m(c5.$$.fragment),ALe=l(),Pi=a("h2"),ku=a("a"),EJ=a("span"),m(m5.$$.fragment),xao=l(),yJ=a("span"),kao=o("AutoModelForPreTraining"),LLe=l(),Xo=a("div"),m(f5.$$.fragment),Rao=l(),$i=a("p"),Sao=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),wJ=a("code"),Pao=o("from_pretrained()"),$ao=o("class method or the "),AJ=a("code"),Iao=o("from_config()"),jao=o(`class
method.`),Nao=l(),g5=a("p"),Dao=o("This class cannot be instantiated directly using "),LJ=a("code"),qao=o("__init__()"),Gao=o(" (throws an error)."),Oao=l(),$r=a("div"),m(h5.$$.fragment),Xao=l(),BJ=a("p"),zao=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Vao=l(),Ii=a("p"),Wao=o(`Note:
Loading a model from its configuration file does `),xJ=a("strong"),Qao=o("not"),Hao=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kJ=a("code"),Uao=o("from_pretrained()"),Jao=o("to load the model weights."),Yao=l(),RJ=a("p"),Kao=o("Examples:"),Zao=l(),m(u5.$$.fragment),eso=l(),ke=a("div"),m(p5.$$.fragment),oso=l(),SJ=a("p"),rso=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),tso=l(),Ia=a("p"),aso=o("The model class to instantiate is selected based on the "),PJ=a("code"),sso=o("model_type"),nso=o(` property of the config object (either
passed as an argument or loaded from `),$J=a("code"),lso=o("pretrained_model_name_or_path"),iso=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IJ=a("code"),dso=o("pretrained_model_name_or_path"),cso=o(":"),mso=l(),k=a("ul"),Ru=a("li"),jJ=a("strong"),fso=o("albert"),gso=o(" \u2014 "),Kk=a("a"),hso=o("AlbertForPreTraining"),uso=o(" (ALBERT model)"),pso=l(),Su=a("li"),NJ=a("strong"),_so=o("bart"),bso=o(" \u2014 "),Zk=a("a"),vso=o("BartForConditionalGeneration"),Tso=o(" (BART model)"),Fso=l(),Pu=a("li"),DJ=a("strong"),Cso=o("bert"),Mso=o(" \u2014 "),eR=a("a"),Eso=o("BertForPreTraining"),yso=o(" (BERT model)"),wso=l(),$u=a("li"),qJ=a("strong"),Aso=o("big_bird"),Lso=o(" \u2014 "),oR=a("a"),Bso=o("BigBirdForPreTraining"),xso=o(" (BigBird model)"),kso=l(),Iu=a("li"),GJ=a("strong"),Rso=o("camembert"),Sso=o(" \u2014 "),rR=a("a"),Pso=o("CamembertForMaskedLM"),$so=o(" (CamemBERT model)"),Iso=l(),ju=a("li"),OJ=a("strong"),jso=o("ctrl"),Nso=o(" \u2014 "),tR=a("a"),Dso=o("CTRLLMHeadModel"),qso=o(" (CTRL model)"),Gso=l(),Nu=a("li"),XJ=a("strong"),Oso=o("deberta"),Xso=o(" \u2014 "),aR=a("a"),zso=o("DebertaForMaskedLM"),Vso=o(" (DeBERTa model)"),Wso=l(),Du=a("li"),zJ=a("strong"),Qso=o("deberta-v2"),Hso=o(" \u2014 "),sR=a("a"),Uso=o("DebertaV2ForMaskedLM"),Jso=o(" (DeBERTa-v2 model)"),Yso=l(),qu=a("li"),VJ=a("strong"),Kso=o("distilbert"),Zso=o(" \u2014 "),nR=a("a"),eno=o("DistilBertForMaskedLM"),ono=o(" (DistilBERT model)"),rno=l(),Gu=a("li"),WJ=a("strong"),tno=o("electra"),ano=o(" \u2014 "),lR=a("a"),sno=o("ElectraForPreTraining"),nno=o(" (ELECTRA model)"),lno=l(),Ou=a("li"),QJ=a("strong"),ino=o("flaubert"),dno=o(" \u2014 "),iR=a("a"),cno=o("FlaubertWithLMHeadModel"),mno=o(" (FlauBERT model)"),fno=l(),Xu=a("li"),HJ=a("strong"),gno=o("fnet"),hno=o(" \u2014 "),dR=a("a"),uno=o("FNetForPreTraining"),pno=o(" (FNet model)"),_no=l(),zu=a("li"),UJ=a("strong"),bno=o("fsmt"),vno=o(" \u2014 "),cR=a("a"),Tno=o("FSMTForConditionalGeneration"),Fno=o(" (FairSeq Machine-Translation model)"),Cno=l(),Vu=a("li"),JJ=a("strong"),Mno=o("funnel"),Eno=o(" \u2014 "),mR=a("a"),yno=o("FunnelForPreTraining"),wno=o(" (Funnel Transformer model)"),Ano=l(),Wu=a("li"),YJ=a("strong"),Lno=o("gpt2"),Bno=o(" \u2014 "),fR=a("a"),xno=o("GPT2LMHeadModel"),kno=o(" (OpenAI GPT-2 model)"),Rno=l(),Qu=a("li"),KJ=a("strong"),Sno=o("ibert"),Pno=o(" \u2014 "),gR=a("a"),$no=o("IBertForMaskedLM"),Ino=o(" (I-BERT model)"),jno=l(),Hu=a("li"),ZJ=a("strong"),Nno=o("layoutlm"),Dno=o(" \u2014 "),hR=a("a"),qno=o("LayoutLMForMaskedLM"),Gno=o(" (LayoutLM model)"),Ono=l(),Uu=a("li"),eY=a("strong"),Xno=o("longformer"),zno=o(" \u2014 "),uR=a("a"),Vno=o("LongformerForMaskedLM"),Wno=o(" (Longformer model)"),Qno=l(),Ju=a("li"),oY=a("strong"),Hno=o("lxmert"),Uno=o(" \u2014 "),pR=a("a"),Jno=o("LxmertForPreTraining"),Yno=o(" (LXMERT model)"),Kno=l(),Yu=a("li"),rY=a("strong"),Zno=o("megatron-bert"),elo=o(" \u2014 "),_R=a("a"),olo=o("MegatronBertForPreTraining"),rlo=o(" (MegatronBert model)"),tlo=l(),Ku=a("li"),tY=a("strong"),alo=o("mobilebert"),slo=o(" \u2014 "),bR=a("a"),nlo=o("MobileBertForPreTraining"),llo=o(" (MobileBERT model)"),ilo=l(),Zu=a("li"),aY=a("strong"),dlo=o("mpnet"),clo=o(" \u2014 "),vR=a("a"),mlo=o("MPNetForMaskedLM"),flo=o(" (MPNet model)"),glo=l(),ep=a("li"),sY=a("strong"),hlo=o("openai-gpt"),ulo=o(" \u2014 "),TR=a("a"),plo=o("OpenAIGPTLMHeadModel"),_lo=o(" (OpenAI GPT model)"),blo=l(),op=a("li"),nY=a("strong"),vlo=o("retribert"),Tlo=o(" \u2014 "),FR=a("a"),Flo=o("RetriBertModel"),Clo=o(" (RetriBERT model)"),Mlo=l(),rp=a("li"),lY=a("strong"),Elo=o("roberta"),ylo=o(" \u2014 "),CR=a("a"),wlo=o("RobertaForMaskedLM"),Alo=o(" (RoBERTa model)"),Llo=l(),tp=a("li"),iY=a("strong"),Blo=o("squeezebert"),xlo=o(" \u2014 "),MR=a("a"),klo=o("SqueezeBertForMaskedLM"),Rlo=o(" (SqueezeBERT model)"),Slo=l(),ap=a("li"),dY=a("strong"),Plo=o("t5"),$lo=o(" \u2014 "),ER=a("a"),Ilo=o("T5ForConditionalGeneration"),jlo=o(" (T5 model)"),Nlo=l(),sp=a("li"),cY=a("strong"),Dlo=o("tapas"),qlo=o(" \u2014 "),yR=a("a"),Glo=o("TapasForMaskedLM"),Olo=o(" (TAPAS model)"),Xlo=l(),np=a("li"),mY=a("strong"),zlo=o("transfo-xl"),Vlo=o(" \u2014 "),wR=a("a"),Wlo=o("TransfoXLLMHeadModel"),Qlo=o(" (Transformer-XL model)"),Hlo=l(),lp=a("li"),fY=a("strong"),Ulo=o("unispeech"),Jlo=o(" \u2014 "),AR=a("a"),Ylo=o("UniSpeechForPreTraining"),Klo=o(" (UniSpeech model)"),Zlo=l(),ip=a("li"),gY=a("strong"),eio=o("unispeech-sat"),oio=o(" \u2014 "),LR=a("a"),rio=o("UniSpeechSatForPreTraining"),tio=o(" (UniSpeechSat model)"),aio=l(),dp=a("li"),hY=a("strong"),sio=o("visual_bert"),nio=o(" \u2014 "),BR=a("a"),lio=o("VisualBertForPreTraining"),iio=o(" (VisualBert model)"),dio=l(),cp=a("li"),uY=a("strong"),cio=o("vit_mae"),mio=o(" \u2014 "),xR=a("a"),fio=o("ViTMAEForPreTraining"),gio=o(" (ViTMAE model)"),hio=l(),mp=a("li"),pY=a("strong"),uio=o("wav2vec2"),pio=o(" \u2014 "),kR=a("a"),_io=o("Wav2Vec2ForPreTraining"),bio=o(" (Wav2Vec2 model)"),vio=l(),fp=a("li"),_Y=a("strong"),Tio=o("xlm"),Fio=o(" \u2014 "),RR=a("a"),Cio=o("XLMWithLMHeadModel"),Mio=o(" (XLM model)"),Eio=l(),gp=a("li"),bY=a("strong"),yio=o("xlm-roberta"),wio=o(" \u2014 "),SR=a("a"),Aio=o("XLMRobertaForMaskedLM"),Lio=o(" (XLM-RoBERTa model)"),Bio=l(),hp=a("li"),vY=a("strong"),xio=o("xlm-roberta-xl"),kio=o(" \u2014 "),PR=a("a"),Rio=o("XLMRobertaXLForMaskedLM"),Sio=o(" (XLM-RoBERTa-XL model)"),Pio=l(),up=a("li"),TY=a("strong"),$io=o("xlnet"),Iio=o(" \u2014 "),$R=a("a"),jio=o("XLNetLMHeadModel"),Nio=o(" (XLNet model)"),Dio=l(),pp=a("p"),qio=o("The model is set in evaluation mode by default using "),FY=a("code"),Gio=o("model.eval()"),Oio=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),CY=a("code"),Xio=o("model.train()"),zio=l(),MY=a("p"),Vio=o("Examples:"),Wio=l(),m(_5.$$.fragment),BLe=l(),ji=a("h2"),_p=a("a"),EY=a("span"),m(b5.$$.fragment),Qio=l(),yY=a("span"),Hio=o("AutoModelForCausalLM"),xLe=l(),zo=a("div"),m(v5.$$.fragment),Uio=l(),Ni=a("p"),Jio=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wY=a("code"),Yio=o("from_pretrained()"),Kio=o("class method or the "),AY=a("code"),Zio=o("from_config()"),edo=o(`class
method.`),odo=l(),T5=a("p"),rdo=o("This class cannot be instantiated directly using "),LY=a("code"),tdo=o("__init__()"),ado=o(" (throws an error)."),sdo=l(),Ir=a("div"),m(F5.$$.fragment),ndo=l(),BY=a("p"),ldo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ido=l(),Di=a("p"),ddo=o(`Note:
Loading a model from its configuration file does `),xY=a("strong"),cdo=o("not"),mdo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kY=a("code"),fdo=o("from_pretrained()"),gdo=o("to load the model weights."),hdo=l(),RY=a("p"),udo=o("Examples:"),pdo=l(),m(C5.$$.fragment),_do=l(),Re=a("div"),m(M5.$$.fragment),bdo=l(),SY=a("p"),vdo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Tdo=l(),ja=a("p"),Fdo=o("The model class to instantiate is selected based on the "),PY=a("code"),Cdo=o("model_type"),Mdo=o(` property of the config object (either
passed as an argument or loaded from `),$Y=a("code"),Edo=o("pretrained_model_name_or_path"),ydo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IY=a("code"),wdo=o("pretrained_model_name_or_path"),Ado=o(":"),Ldo=l(),I=a("ul"),bp=a("li"),jY=a("strong"),Bdo=o("bart"),xdo=o(" \u2014 "),IR=a("a"),kdo=o("BartForCausalLM"),Rdo=o(" (BART model)"),Sdo=l(),vp=a("li"),NY=a("strong"),Pdo=o("bert"),$do=o(" \u2014 "),jR=a("a"),Ido=o("BertLMHeadModel"),jdo=o(" (BERT model)"),Ndo=l(),Tp=a("li"),DY=a("strong"),Ddo=o("bert-generation"),qdo=o(" \u2014 "),NR=a("a"),Gdo=o("BertGenerationDecoder"),Odo=o(" (Bert Generation model)"),Xdo=l(),Fp=a("li"),qY=a("strong"),zdo=o("big_bird"),Vdo=o(" \u2014 "),DR=a("a"),Wdo=o("BigBirdForCausalLM"),Qdo=o(" (BigBird model)"),Hdo=l(),Cp=a("li"),GY=a("strong"),Udo=o("bigbird_pegasus"),Jdo=o(" \u2014 "),qR=a("a"),Ydo=o("BigBirdPegasusForCausalLM"),Kdo=o(" (BigBirdPegasus model)"),Zdo=l(),Mp=a("li"),OY=a("strong"),eco=o("blenderbot"),oco=o(" \u2014 "),GR=a("a"),rco=o("BlenderbotForCausalLM"),tco=o(" (Blenderbot model)"),aco=l(),Ep=a("li"),XY=a("strong"),sco=o("blenderbot-small"),nco=o(" \u2014 "),OR=a("a"),lco=o("BlenderbotSmallForCausalLM"),ico=o(" (BlenderbotSmall model)"),dco=l(),yp=a("li"),zY=a("strong"),cco=o("camembert"),mco=o(" \u2014 "),XR=a("a"),fco=o("CamembertForCausalLM"),gco=o(" (CamemBERT model)"),hco=l(),wp=a("li"),VY=a("strong"),uco=o("ctrl"),pco=o(" \u2014 "),zR=a("a"),_co=o("CTRLLMHeadModel"),bco=o(" (CTRL model)"),vco=l(),Ap=a("li"),WY=a("strong"),Tco=o("electra"),Fco=o(" \u2014 "),VR=a("a"),Cco=o("ElectraForCausalLM"),Mco=o(" (ELECTRA model)"),Eco=l(),Lp=a("li"),QY=a("strong"),yco=o("gpt2"),wco=o(" \u2014 "),WR=a("a"),Aco=o("GPT2LMHeadModel"),Lco=o(" (OpenAI GPT-2 model)"),Bco=l(),Bp=a("li"),HY=a("strong"),xco=o("gpt_neo"),kco=o(" \u2014 "),QR=a("a"),Rco=o("GPTNeoForCausalLM"),Sco=o(" (GPT Neo model)"),Pco=l(),xp=a("li"),UY=a("strong"),$co=o("gptj"),Ico=o(" \u2014 "),HR=a("a"),jco=o("GPTJForCausalLM"),Nco=o(" (GPT-J model)"),Dco=l(),kp=a("li"),JY=a("strong"),qco=o("marian"),Gco=o(" \u2014 "),UR=a("a"),Oco=o("MarianForCausalLM"),Xco=o(" (Marian model)"),zco=l(),Rp=a("li"),YY=a("strong"),Vco=o("mbart"),Wco=o(" \u2014 "),JR=a("a"),Qco=o("MBartForCausalLM"),Hco=o(" (mBART model)"),Uco=l(),Sp=a("li"),KY=a("strong"),Jco=o("megatron-bert"),Yco=o(" \u2014 "),YR=a("a"),Kco=o("MegatronBertForCausalLM"),Zco=o(" (MegatronBert model)"),emo=l(),Pp=a("li"),ZY=a("strong"),omo=o("openai-gpt"),rmo=o(" \u2014 "),KR=a("a"),tmo=o("OpenAIGPTLMHeadModel"),amo=o(" (OpenAI GPT model)"),smo=l(),$p=a("li"),eK=a("strong"),nmo=o("pegasus"),lmo=o(" \u2014 "),ZR=a("a"),imo=o("PegasusForCausalLM"),dmo=o(" (Pegasus model)"),cmo=l(),Ip=a("li"),oK=a("strong"),mmo=o("prophetnet"),fmo=o(" \u2014 "),eS=a("a"),gmo=o("ProphetNetForCausalLM"),hmo=o(" (ProphetNet model)"),umo=l(),jp=a("li"),rK=a("strong"),pmo=o("qdqbert"),_mo=o(" \u2014 "),tK=a("code"),bmo=o("QDQBertLMHeadModel"),vmo=o("(QDQBert model)"),Tmo=l(),Np=a("li"),aK=a("strong"),Fmo=o("reformer"),Cmo=o(" \u2014 "),oS=a("a"),Mmo=o("ReformerModelWithLMHead"),Emo=o(" (Reformer model)"),ymo=l(),Dp=a("li"),sK=a("strong"),wmo=o("rembert"),Amo=o(" \u2014 "),rS=a("a"),Lmo=o("RemBertForCausalLM"),Bmo=o(" (RemBERT model)"),xmo=l(),qp=a("li"),nK=a("strong"),kmo=o("roberta"),Rmo=o(" \u2014 "),tS=a("a"),Smo=o("RobertaForCausalLM"),Pmo=o(" (RoBERTa model)"),$mo=l(),Gp=a("li"),lK=a("strong"),Imo=o("roformer"),jmo=o(" \u2014 "),aS=a("a"),Nmo=o("RoFormerForCausalLM"),Dmo=o(" (RoFormer model)"),qmo=l(),Op=a("li"),iK=a("strong"),Gmo=o("speech_to_text_2"),Omo=o(" \u2014 "),sS=a("a"),Xmo=o("Speech2Text2ForCausalLM"),zmo=o(" (Speech2Text2 model)"),Vmo=l(),Xp=a("li"),dK=a("strong"),Wmo=o("transfo-xl"),Qmo=o(" \u2014 "),nS=a("a"),Hmo=o("TransfoXLLMHeadModel"),Umo=o(" (Transformer-XL model)"),Jmo=l(),zp=a("li"),cK=a("strong"),Ymo=o("trocr"),Kmo=o(" \u2014 "),lS=a("a"),Zmo=o("TrOCRForCausalLM"),efo=o(" (TrOCR model)"),ofo=l(),Vp=a("li"),mK=a("strong"),rfo=o("xglm"),tfo=o(" \u2014 "),iS=a("a"),afo=o("XGLMForCausalLM"),sfo=o(" (XGLM model)"),nfo=l(),Wp=a("li"),fK=a("strong"),lfo=o("xlm"),ifo=o(" \u2014 "),dS=a("a"),dfo=o("XLMWithLMHeadModel"),cfo=o(" (XLM model)"),mfo=l(),Qp=a("li"),gK=a("strong"),ffo=o("xlm-prophetnet"),gfo=o(" \u2014 "),cS=a("a"),hfo=o("XLMProphetNetForCausalLM"),ufo=o(" (XLMProphetNet model)"),pfo=l(),Hp=a("li"),hK=a("strong"),_fo=o("xlm-roberta"),bfo=o(" \u2014 "),mS=a("a"),vfo=o("XLMRobertaForCausalLM"),Tfo=o(" (XLM-RoBERTa model)"),Ffo=l(),Up=a("li"),uK=a("strong"),Cfo=o("xlm-roberta-xl"),Mfo=o(" \u2014 "),fS=a("a"),Efo=o("XLMRobertaXLForCausalLM"),yfo=o(" (XLM-RoBERTa-XL model)"),wfo=l(),Jp=a("li"),pK=a("strong"),Afo=o("xlnet"),Lfo=o(" \u2014 "),gS=a("a"),Bfo=o("XLNetLMHeadModel"),xfo=o(" (XLNet model)"),kfo=l(),Yp=a("p"),Rfo=o("The model is set in evaluation mode by default using "),_K=a("code"),Sfo=o("model.eval()"),Pfo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),bK=a("code"),$fo=o("model.train()"),Ifo=l(),vK=a("p"),jfo=o("Examples:"),Nfo=l(),m(E5.$$.fragment),kLe=l(),qi=a("h2"),Kp=a("a"),TK=a("span"),m(y5.$$.fragment),Dfo=l(),FK=a("span"),qfo=o("AutoModelForMaskedLM"),RLe=l(),Vo=a("div"),m(w5.$$.fragment),Gfo=l(),Gi=a("p"),Ofo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),CK=a("code"),Xfo=o("from_pretrained()"),zfo=o("class method or the "),MK=a("code"),Vfo=o("from_config()"),Wfo=o(`class
method.`),Qfo=l(),A5=a("p"),Hfo=o("This class cannot be instantiated directly using "),EK=a("code"),Ufo=o("__init__()"),Jfo=o(" (throws an error)."),Yfo=l(),jr=a("div"),m(L5.$$.fragment),Kfo=l(),yK=a("p"),Zfo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),ego=l(),Oi=a("p"),ogo=o(`Note:
Loading a model from its configuration file does `),wK=a("strong"),rgo=o("not"),tgo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),AK=a("code"),ago=o("from_pretrained()"),sgo=o("to load the model weights."),ngo=l(),LK=a("p"),lgo=o("Examples:"),igo=l(),m(B5.$$.fragment),dgo=l(),Se=a("div"),m(x5.$$.fragment),cgo=l(),BK=a("p"),mgo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),fgo=l(),Na=a("p"),ggo=o("The model class to instantiate is selected based on the "),xK=a("code"),hgo=o("model_type"),ugo=o(` property of the config object (either
passed as an argument or loaded from `),kK=a("code"),pgo=o("pretrained_model_name_or_path"),_go=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),RK=a("code"),bgo=o("pretrained_model_name_or_path"),vgo=o(":"),Tgo=l(),$=a("ul"),Zp=a("li"),SK=a("strong"),Fgo=o("albert"),Cgo=o(" \u2014 "),hS=a("a"),Mgo=o("AlbertForMaskedLM"),Ego=o(" (ALBERT model)"),ygo=l(),e_=a("li"),PK=a("strong"),wgo=o("bart"),Ago=o(" \u2014 "),uS=a("a"),Lgo=o("BartForConditionalGeneration"),Bgo=o(" (BART model)"),xgo=l(),o_=a("li"),$K=a("strong"),kgo=o("bert"),Rgo=o(" \u2014 "),pS=a("a"),Sgo=o("BertForMaskedLM"),Pgo=o(" (BERT model)"),$go=l(),r_=a("li"),IK=a("strong"),Igo=o("big_bird"),jgo=o(" \u2014 "),_S=a("a"),Ngo=o("BigBirdForMaskedLM"),Dgo=o(" (BigBird model)"),qgo=l(),t_=a("li"),jK=a("strong"),Ggo=o("camembert"),Ogo=o(" \u2014 "),bS=a("a"),Xgo=o("CamembertForMaskedLM"),zgo=o(" (CamemBERT model)"),Vgo=l(),a_=a("li"),NK=a("strong"),Wgo=o("convbert"),Qgo=o(" \u2014 "),vS=a("a"),Hgo=o("ConvBertForMaskedLM"),Ugo=o(" (ConvBERT model)"),Jgo=l(),s_=a("li"),DK=a("strong"),Ygo=o("deberta"),Kgo=o(" \u2014 "),TS=a("a"),Zgo=o("DebertaForMaskedLM"),eho=o(" (DeBERTa model)"),oho=l(),n_=a("li"),qK=a("strong"),rho=o("deberta-v2"),tho=o(" \u2014 "),FS=a("a"),aho=o("DebertaV2ForMaskedLM"),sho=o(" (DeBERTa-v2 model)"),nho=l(),l_=a("li"),GK=a("strong"),lho=o("distilbert"),iho=o(" \u2014 "),CS=a("a"),dho=o("DistilBertForMaskedLM"),cho=o(" (DistilBERT model)"),mho=l(),i_=a("li"),OK=a("strong"),fho=o("electra"),gho=o(" \u2014 "),MS=a("a"),hho=o("ElectraForMaskedLM"),uho=o(" (ELECTRA model)"),pho=l(),d_=a("li"),XK=a("strong"),_ho=o("flaubert"),bho=o(" \u2014 "),ES=a("a"),vho=o("FlaubertWithLMHeadModel"),Tho=o(" (FlauBERT model)"),Fho=l(),c_=a("li"),zK=a("strong"),Cho=o("fnet"),Mho=o(" \u2014 "),yS=a("a"),Eho=o("FNetForMaskedLM"),yho=o(" (FNet model)"),who=l(),m_=a("li"),VK=a("strong"),Aho=o("funnel"),Lho=o(" \u2014 "),wS=a("a"),Bho=o("FunnelForMaskedLM"),xho=o(" (Funnel Transformer model)"),kho=l(),f_=a("li"),WK=a("strong"),Rho=o("ibert"),Sho=o(" \u2014 "),AS=a("a"),Pho=o("IBertForMaskedLM"),$ho=o(" (I-BERT model)"),Iho=l(),g_=a("li"),QK=a("strong"),jho=o("layoutlm"),Nho=o(" \u2014 "),LS=a("a"),Dho=o("LayoutLMForMaskedLM"),qho=o(" (LayoutLM model)"),Gho=l(),h_=a("li"),HK=a("strong"),Oho=o("longformer"),Xho=o(" \u2014 "),BS=a("a"),zho=o("LongformerForMaskedLM"),Vho=o(" (Longformer model)"),Who=l(),u_=a("li"),UK=a("strong"),Qho=o("mbart"),Hho=o(" \u2014 "),xS=a("a"),Uho=o("MBartForConditionalGeneration"),Jho=o(" (mBART model)"),Yho=l(),p_=a("li"),JK=a("strong"),Kho=o("megatron-bert"),Zho=o(" \u2014 "),kS=a("a"),euo=o("MegatronBertForMaskedLM"),ouo=o(" (MegatronBert model)"),ruo=l(),__=a("li"),YK=a("strong"),tuo=o("mobilebert"),auo=o(" \u2014 "),RS=a("a"),suo=o("MobileBertForMaskedLM"),nuo=o(" (MobileBERT model)"),luo=l(),b_=a("li"),KK=a("strong"),iuo=o("mpnet"),duo=o(" \u2014 "),SS=a("a"),cuo=o("MPNetForMaskedLM"),muo=o(" (MPNet model)"),fuo=l(),v_=a("li"),ZK=a("strong"),guo=o("nystromformer"),huo=o(" \u2014 "),PS=a("a"),uuo=o("NystromformerForMaskedLM"),puo=o(" (Nystromformer model)"),_uo=l(),T_=a("li"),eZ=a("strong"),buo=o("perceiver"),vuo=o(" \u2014 "),$S=a("a"),Tuo=o("PerceiverForMaskedLM"),Fuo=o(" (Perceiver model)"),Cuo=l(),F_=a("li"),oZ=a("strong"),Muo=o("qdqbert"),Euo=o(" \u2014 "),rZ=a("code"),yuo=o("QDQBertForMaskedLM"),wuo=o("(QDQBert model)"),Auo=l(),C_=a("li"),tZ=a("strong"),Luo=o("reformer"),Buo=o(" \u2014 "),IS=a("a"),xuo=o("ReformerForMaskedLM"),kuo=o(" (Reformer model)"),Ruo=l(),M_=a("li"),aZ=a("strong"),Suo=o("rembert"),Puo=o(" \u2014 "),jS=a("a"),$uo=o("RemBertForMaskedLM"),Iuo=o(" (RemBERT model)"),juo=l(),E_=a("li"),sZ=a("strong"),Nuo=o("roberta"),Duo=o(" \u2014 "),NS=a("a"),quo=o("RobertaForMaskedLM"),Guo=o(" (RoBERTa model)"),Ouo=l(),y_=a("li"),nZ=a("strong"),Xuo=o("roformer"),zuo=o(" \u2014 "),DS=a("a"),Vuo=o("RoFormerForMaskedLM"),Wuo=o(" (RoFormer model)"),Quo=l(),w_=a("li"),lZ=a("strong"),Huo=o("squeezebert"),Uuo=o(" \u2014 "),qS=a("a"),Juo=o("SqueezeBertForMaskedLM"),Yuo=o(" (SqueezeBERT model)"),Kuo=l(),A_=a("li"),iZ=a("strong"),Zuo=o("tapas"),epo=o(" \u2014 "),GS=a("a"),opo=o("TapasForMaskedLM"),rpo=o(" (TAPAS model)"),tpo=l(),L_=a("li"),dZ=a("strong"),apo=o("wav2vec2"),spo=o(" \u2014 "),cZ=a("code"),npo=o("Wav2Vec2ForMaskedLM"),lpo=o("(Wav2Vec2 model)"),ipo=l(),B_=a("li"),mZ=a("strong"),dpo=o("xlm"),cpo=o(" \u2014 "),OS=a("a"),mpo=o("XLMWithLMHeadModel"),fpo=o(" (XLM model)"),gpo=l(),x_=a("li"),fZ=a("strong"),hpo=o("xlm-roberta"),upo=o(" \u2014 "),XS=a("a"),ppo=o("XLMRobertaForMaskedLM"),_po=o(" (XLM-RoBERTa model)"),bpo=l(),k_=a("li"),gZ=a("strong"),vpo=o("xlm-roberta-xl"),Tpo=o(" \u2014 "),zS=a("a"),Fpo=o("XLMRobertaXLForMaskedLM"),Cpo=o(" (XLM-RoBERTa-XL model)"),Mpo=l(),R_=a("li"),hZ=a("strong"),Epo=o("yoso"),ypo=o(" \u2014 "),VS=a("a"),wpo=o("YosoForMaskedLM"),Apo=o(" (YOSO model)"),Lpo=l(),S_=a("p"),Bpo=o("The model is set in evaluation mode by default using "),uZ=a("code"),xpo=o("model.eval()"),kpo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pZ=a("code"),Rpo=o("model.train()"),Spo=l(),_Z=a("p"),Ppo=o("Examples:"),$po=l(),m(k5.$$.fragment),SLe=l(),Xi=a("h2"),P_=a("a"),bZ=a("span"),m(R5.$$.fragment),Ipo=l(),vZ=a("span"),jpo=o("AutoModelForSeq2SeqLM"),PLe=l(),Wo=a("div"),m(S5.$$.fragment),Npo=l(),zi=a("p"),Dpo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),TZ=a("code"),qpo=o("from_pretrained()"),Gpo=o("class method or the "),FZ=a("code"),Opo=o("from_config()"),Xpo=o(`class
method.`),zpo=l(),P5=a("p"),Vpo=o("This class cannot be instantiated directly using "),CZ=a("code"),Wpo=o("__init__()"),Qpo=o(" (throws an error)."),Hpo=l(),Nr=a("div"),m($5.$$.fragment),Upo=l(),MZ=a("p"),Jpo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Ypo=l(),Vi=a("p"),Kpo=o(`Note:
Loading a model from its configuration file does `),EZ=a("strong"),Zpo=o("not"),e_o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),yZ=a("code"),o_o=o("from_pretrained()"),r_o=o("to load the model weights."),t_o=l(),wZ=a("p"),a_o=o("Examples:"),s_o=l(),m(I5.$$.fragment),n_o=l(),Pe=a("div"),m(j5.$$.fragment),l_o=l(),AZ=a("p"),i_o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),d_o=l(),Da=a("p"),c_o=o("The model class to instantiate is selected based on the "),LZ=a("code"),m_o=o("model_type"),f_o=o(` property of the config object (either
passed as an argument or loaded from `),BZ=a("code"),g_o=o("pretrained_model_name_or_path"),h_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xZ=a("code"),u_o=o("pretrained_model_name_or_path"),p_o=o(":"),__o=l(),se=a("ul"),$_=a("li"),kZ=a("strong"),b_o=o("bart"),v_o=o(" \u2014 "),WS=a("a"),T_o=o("BartForConditionalGeneration"),F_o=o(" (BART model)"),C_o=l(),I_=a("li"),RZ=a("strong"),M_o=o("bigbird_pegasus"),E_o=o(" \u2014 "),QS=a("a"),y_o=o("BigBirdPegasusForConditionalGeneration"),w_o=o(" (BigBirdPegasus model)"),A_o=l(),j_=a("li"),SZ=a("strong"),L_o=o("blenderbot"),B_o=o(" \u2014 "),HS=a("a"),x_o=o("BlenderbotForConditionalGeneration"),k_o=o(" (Blenderbot model)"),R_o=l(),N_=a("li"),PZ=a("strong"),S_o=o("blenderbot-small"),P_o=o(" \u2014 "),US=a("a"),$_o=o("BlenderbotSmallForConditionalGeneration"),I_o=o(" (BlenderbotSmall model)"),j_o=l(),D_=a("li"),$Z=a("strong"),N_o=o("encoder-decoder"),D_o=o(" \u2014 "),JS=a("a"),q_o=o("EncoderDecoderModel"),G_o=o(" (Encoder decoder model)"),O_o=l(),q_=a("li"),IZ=a("strong"),X_o=o("fsmt"),z_o=o(" \u2014 "),YS=a("a"),V_o=o("FSMTForConditionalGeneration"),W_o=o(" (FairSeq Machine-Translation model)"),Q_o=l(),G_=a("li"),jZ=a("strong"),H_o=o("led"),U_o=o(" \u2014 "),KS=a("a"),J_o=o("LEDForConditionalGeneration"),Y_o=o(" (LED model)"),K_o=l(),O_=a("li"),NZ=a("strong"),Z_o=o("m2m_100"),ebo=o(" \u2014 "),ZS=a("a"),obo=o("M2M100ForConditionalGeneration"),rbo=o(" (M2M100 model)"),tbo=l(),X_=a("li"),DZ=a("strong"),abo=o("marian"),sbo=o(" \u2014 "),eP=a("a"),nbo=o("MarianMTModel"),lbo=o(" (Marian model)"),ibo=l(),z_=a("li"),qZ=a("strong"),dbo=o("mbart"),cbo=o(" \u2014 "),oP=a("a"),mbo=o("MBartForConditionalGeneration"),fbo=o(" (mBART model)"),gbo=l(),V_=a("li"),GZ=a("strong"),hbo=o("mt5"),ubo=o(" \u2014 "),rP=a("a"),pbo=o("MT5ForConditionalGeneration"),_bo=o(" (mT5 model)"),bbo=l(),W_=a("li"),OZ=a("strong"),vbo=o("pegasus"),Tbo=o(" \u2014 "),tP=a("a"),Fbo=o("PegasusForConditionalGeneration"),Cbo=o(" (Pegasus model)"),Mbo=l(),Q_=a("li"),XZ=a("strong"),Ebo=o("prophetnet"),ybo=o(" \u2014 "),aP=a("a"),wbo=o("ProphetNetForConditionalGeneration"),Abo=o(" (ProphetNet model)"),Lbo=l(),H_=a("li"),zZ=a("strong"),Bbo=o("t5"),xbo=o(" \u2014 "),sP=a("a"),kbo=o("T5ForConditionalGeneration"),Rbo=o(" (T5 model)"),Sbo=l(),U_=a("li"),VZ=a("strong"),Pbo=o("xlm-prophetnet"),$bo=o(" \u2014 "),nP=a("a"),Ibo=o("XLMProphetNetForConditionalGeneration"),jbo=o(" (XLMProphetNet model)"),Nbo=l(),J_=a("p"),Dbo=o("The model is set in evaluation mode by default using "),WZ=a("code"),qbo=o("model.eval()"),Gbo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),QZ=a("code"),Obo=o("model.train()"),Xbo=l(),HZ=a("p"),zbo=o("Examples:"),Vbo=l(),m(N5.$$.fragment),$Le=l(),Wi=a("h2"),Y_=a("a"),UZ=a("span"),m(D5.$$.fragment),Wbo=l(),JZ=a("span"),Qbo=o("AutoModelForSequenceClassification"),ILe=l(),Qo=a("div"),m(q5.$$.fragment),Hbo=l(),Qi=a("p"),Ubo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),YZ=a("code"),Jbo=o("from_pretrained()"),Ybo=o("class method or the "),KZ=a("code"),Kbo=o("from_config()"),Zbo=o(`class
method.`),e2o=l(),G5=a("p"),o2o=o("This class cannot be instantiated directly using "),ZZ=a("code"),r2o=o("__init__()"),t2o=o(" (throws an error)."),a2o=l(),Dr=a("div"),m(O5.$$.fragment),s2o=l(),eee=a("p"),n2o=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),l2o=l(),Hi=a("p"),i2o=o(`Note:
Loading a model from its configuration file does `),oee=a("strong"),d2o=o("not"),c2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ree=a("code"),m2o=o("from_pretrained()"),f2o=o("to load the model weights."),g2o=l(),tee=a("p"),h2o=o("Examples:"),u2o=l(),m(X5.$$.fragment),p2o=l(),$e=a("div"),m(z5.$$.fragment),_2o=l(),aee=a("p"),b2o=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),v2o=l(),qa=a("p"),T2o=o("The model class to instantiate is selected based on the "),see=a("code"),F2o=o("model_type"),C2o=o(` property of the config object (either
passed as an argument or loaded from `),nee=a("code"),M2o=o("pretrained_model_name_or_path"),E2o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lee=a("code"),y2o=o("pretrained_model_name_or_path"),w2o=o(":"),A2o=l(),A=a("ul"),K_=a("li"),iee=a("strong"),L2o=o("albert"),B2o=o(" \u2014 "),lP=a("a"),x2o=o("AlbertForSequenceClassification"),k2o=o(" (ALBERT model)"),R2o=l(),Z_=a("li"),dee=a("strong"),S2o=o("bart"),P2o=o(" \u2014 "),iP=a("a"),$2o=o("BartForSequenceClassification"),I2o=o(" (BART model)"),j2o=l(),eb=a("li"),cee=a("strong"),N2o=o("bert"),D2o=o(" \u2014 "),dP=a("a"),q2o=o("BertForSequenceClassification"),G2o=o(" (BERT model)"),O2o=l(),ob=a("li"),mee=a("strong"),X2o=o("big_bird"),z2o=o(" \u2014 "),cP=a("a"),V2o=o("BigBirdForSequenceClassification"),W2o=o(" (BigBird model)"),Q2o=l(),rb=a("li"),fee=a("strong"),H2o=o("bigbird_pegasus"),U2o=o(" \u2014 "),mP=a("a"),J2o=o("BigBirdPegasusForSequenceClassification"),Y2o=o(" (BigBirdPegasus model)"),K2o=l(),tb=a("li"),gee=a("strong"),Z2o=o("camembert"),evo=o(" \u2014 "),fP=a("a"),ovo=o("CamembertForSequenceClassification"),rvo=o(" (CamemBERT model)"),tvo=l(),ab=a("li"),hee=a("strong"),avo=o("canine"),svo=o(" \u2014 "),gP=a("a"),nvo=o("CanineForSequenceClassification"),lvo=o(" (Canine model)"),ivo=l(),sb=a("li"),uee=a("strong"),dvo=o("convbert"),cvo=o(" \u2014 "),hP=a("a"),mvo=o("ConvBertForSequenceClassification"),fvo=o(" (ConvBERT model)"),gvo=l(),nb=a("li"),pee=a("strong"),hvo=o("ctrl"),uvo=o(" \u2014 "),uP=a("a"),pvo=o("CTRLForSequenceClassification"),_vo=o(" (CTRL model)"),bvo=l(),lb=a("li"),_ee=a("strong"),vvo=o("deberta"),Tvo=o(" \u2014 "),pP=a("a"),Fvo=o("DebertaForSequenceClassification"),Cvo=o(" (DeBERTa model)"),Mvo=l(),ib=a("li"),bee=a("strong"),Evo=o("deberta-v2"),yvo=o(" \u2014 "),_P=a("a"),wvo=o("DebertaV2ForSequenceClassification"),Avo=o(" (DeBERTa-v2 model)"),Lvo=l(),db=a("li"),vee=a("strong"),Bvo=o("distilbert"),xvo=o(" \u2014 "),bP=a("a"),kvo=o("DistilBertForSequenceClassification"),Rvo=o(" (DistilBERT model)"),Svo=l(),cb=a("li"),Tee=a("strong"),Pvo=o("electra"),$vo=o(" \u2014 "),vP=a("a"),Ivo=o("ElectraForSequenceClassification"),jvo=o(" (ELECTRA model)"),Nvo=l(),mb=a("li"),Fee=a("strong"),Dvo=o("flaubert"),qvo=o(" \u2014 "),TP=a("a"),Gvo=o("FlaubertForSequenceClassification"),Ovo=o(" (FlauBERT model)"),Xvo=l(),fb=a("li"),Cee=a("strong"),zvo=o("fnet"),Vvo=o(" \u2014 "),FP=a("a"),Wvo=o("FNetForSequenceClassification"),Qvo=o(" (FNet model)"),Hvo=l(),gb=a("li"),Mee=a("strong"),Uvo=o("funnel"),Jvo=o(" \u2014 "),CP=a("a"),Yvo=o("FunnelForSequenceClassification"),Kvo=o(" (Funnel Transformer model)"),Zvo=l(),hb=a("li"),Eee=a("strong"),eTo=o("gpt2"),oTo=o(" \u2014 "),MP=a("a"),rTo=o("GPT2ForSequenceClassification"),tTo=o(" (OpenAI GPT-2 model)"),aTo=l(),ub=a("li"),yee=a("strong"),sTo=o("gpt_neo"),nTo=o(" \u2014 "),EP=a("a"),lTo=o("GPTNeoForSequenceClassification"),iTo=o(" (GPT Neo model)"),dTo=l(),pb=a("li"),wee=a("strong"),cTo=o("gptj"),mTo=o(" \u2014 "),yP=a("a"),fTo=o("GPTJForSequenceClassification"),gTo=o(" (GPT-J model)"),hTo=l(),_b=a("li"),Aee=a("strong"),uTo=o("ibert"),pTo=o(" \u2014 "),wP=a("a"),_To=o("IBertForSequenceClassification"),bTo=o(" (I-BERT model)"),vTo=l(),bb=a("li"),Lee=a("strong"),TTo=o("layoutlm"),FTo=o(" \u2014 "),AP=a("a"),CTo=o("LayoutLMForSequenceClassification"),MTo=o(" (LayoutLM model)"),ETo=l(),vb=a("li"),Bee=a("strong"),yTo=o("layoutlmv2"),wTo=o(" \u2014 "),LP=a("a"),ATo=o("LayoutLMv2ForSequenceClassification"),LTo=o(" (LayoutLMv2 model)"),BTo=l(),Tb=a("li"),xee=a("strong"),xTo=o("led"),kTo=o(" \u2014 "),BP=a("a"),RTo=o("LEDForSequenceClassification"),STo=o(" (LED model)"),PTo=l(),Fb=a("li"),kee=a("strong"),$To=o("longformer"),ITo=o(" \u2014 "),xP=a("a"),jTo=o("LongformerForSequenceClassification"),NTo=o(" (Longformer model)"),DTo=l(),Cb=a("li"),Ree=a("strong"),qTo=o("mbart"),GTo=o(" \u2014 "),kP=a("a"),OTo=o("MBartForSequenceClassification"),XTo=o(" (mBART model)"),zTo=l(),Mb=a("li"),See=a("strong"),VTo=o("megatron-bert"),WTo=o(" \u2014 "),RP=a("a"),QTo=o("MegatronBertForSequenceClassification"),HTo=o(" (MegatronBert model)"),UTo=l(),Eb=a("li"),Pee=a("strong"),JTo=o("mobilebert"),YTo=o(" \u2014 "),SP=a("a"),KTo=o("MobileBertForSequenceClassification"),ZTo=o(" (MobileBERT model)"),e1o=l(),yb=a("li"),$ee=a("strong"),o1o=o("mpnet"),r1o=o(" \u2014 "),PP=a("a"),t1o=o("MPNetForSequenceClassification"),a1o=o(" (MPNet model)"),s1o=l(),wb=a("li"),Iee=a("strong"),n1o=o("nystromformer"),l1o=o(" \u2014 "),$P=a("a"),i1o=o("NystromformerForSequenceClassification"),d1o=o(" (Nystromformer model)"),c1o=l(),Ab=a("li"),jee=a("strong"),m1o=o("openai-gpt"),f1o=o(" \u2014 "),IP=a("a"),g1o=o("OpenAIGPTForSequenceClassification"),h1o=o(" (OpenAI GPT model)"),u1o=l(),Lb=a("li"),Nee=a("strong"),p1o=o("perceiver"),_1o=o(" \u2014 "),jP=a("a"),b1o=o("PerceiverForSequenceClassification"),v1o=o(" (Perceiver model)"),T1o=l(),Bb=a("li"),Dee=a("strong"),F1o=o("qdqbert"),C1o=o(" \u2014 "),qee=a("code"),M1o=o("QDQBertForSequenceClassification"),E1o=o("(QDQBert model)"),y1o=l(),xb=a("li"),Gee=a("strong"),w1o=o("reformer"),A1o=o(" \u2014 "),NP=a("a"),L1o=o("ReformerForSequenceClassification"),B1o=o(" (Reformer model)"),x1o=l(),kb=a("li"),Oee=a("strong"),k1o=o("rembert"),R1o=o(" \u2014 "),DP=a("a"),S1o=o("RemBertForSequenceClassification"),P1o=o(" (RemBERT model)"),$1o=l(),Rb=a("li"),Xee=a("strong"),I1o=o("roberta"),j1o=o(" \u2014 "),qP=a("a"),N1o=o("RobertaForSequenceClassification"),D1o=o(" (RoBERTa model)"),q1o=l(),Sb=a("li"),zee=a("strong"),G1o=o("roformer"),O1o=o(" \u2014 "),GP=a("a"),X1o=o("RoFormerForSequenceClassification"),z1o=o(" (RoFormer model)"),V1o=l(),Pb=a("li"),Vee=a("strong"),W1o=o("squeezebert"),Q1o=o(" \u2014 "),OP=a("a"),H1o=o("SqueezeBertForSequenceClassification"),U1o=o(" (SqueezeBERT model)"),J1o=l(),$b=a("li"),Wee=a("strong"),Y1o=o("tapas"),K1o=o(" \u2014 "),XP=a("a"),Z1o=o("TapasForSequenceClassification"),eFo=o(" (TAPAS model)"),oFo=l(),Ib=a("li"),Qee=a("strong"),rFo=o("transfo-xl"),tFo=o(" \u2014 "),zP=a("a"),aFo=o("TransfoXLForSequenceClassification"),sFo=o(" (Transformer-XL model)"),nFo=l(),jb=a("li"),Hee=a("strong"),lFo=o("xlm"),iFo=o(" \u2014 "),VP=a("a"),dFo=o("XLMForSequenceClassification"),cFo=o(" (XLM model)"),mFo=l(),Nb=a("li"),Uee=a("strong"),fFo=o("xlm-roberta"),gFo=o(" \u2014 "),WP=a("a"),hFo=o("XLMRobertaForSequenceClassification"),uFo=o(" (XLM-RoBERTa model)"),pFo=l(),Db=a("li"),Jee=a("strong"),_Fo=o("xlm-roberta-xl"),bFo=o(" \u2014 "),QP=a("a"),vFo=o("XLMRobertaXLForSequenceClassification"),TFo=o(" (XLM-RoBERTa-XL model)"),FFo=l(),qb=a("li"),Yee=a("strong"),CFo=o("xlnet"),MFo=o(" \u2014 "),HP=a("a"),EFo=o("XLNetForSequenceClassification"),yFo=o(" (XLNet model)"),wFo=l(),Gb=a("li"),Kee=a("strong"),AFo=o("yoso"),LFo=o(" \u2014 "),UP=a("a"),BFo=o("YosoForSequenceClassification"),xFo=o(" (YOSO model)"),kFo=l(),Ob=a("p"),RFo=o("The model is set in evaluation mode by default using "),Zee=a("code"),SFo=o("model.eval()"),PFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),eoe=a("code"),$Fo=o("model.train()"),IFo=l(),ooe=a("p"),jFo=o("Examples:"),NFo=l(),m(V5.$$.fragment),jLe=l(),Ui=a("h2"),Xb=a("a"),roe=a("span"),m(W5.$$.fragment),DFo=l(),toe=a("span"),qFo=o("AutoModelForMultipleChoice"),NLe=l(),Ho=a("div"),m(Q5.$$.fragment),GFo=l(),Ji=a("p"),OFo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),aoe=a("code"),XFo=o("from_pretrained()"),zFo=o("class method or the "),soe=a("code"),VFo=o("from_config()"),WFo=o(`class
method.`),QFo=l(),H5=a("p"),HFo=o("This class cannot be instantiated directly using "),noe=a("code"),UFo=o("__init__()"),JFo=o(" (throws an error)."),YFo=l(),qr=a("div"),m(U5.$$.fragment),KFo=l(),loe=a("p"),ZFo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),eCo=l(),Yi=a("p"),oCo=o(`Note:
Loading a model from its configuration file does `),ioe=a("strong"),rCo=o("not"),tCo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),doe=a("code"),aCo=o("from_pretrained()"),sCo=o("to load the model weights."),nCo=l(),coe=a("p"),lCo=o("Examples:"),iCo=l(),m(J5.$$.fragment),dCo=l(),Ie=a("div"),m(Y5.$$.fragment),cCo=l(),moe=a("p"),mCo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),fCo=l(),Ga=a("p"),gCo=o("The model class to instantiate is selected based on the "),foe=a("code"),hCo=o("model_type"),uCo=o(` property of the config object (either
passed as an argument or loaded from `),goe=a("code"),pCo=o("pretrained_model_name_or_path"),_Co=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hoe=a("code"),bCo=o("pretrained_model_name_or_path"),vCo=o(":"),TCo=l(),G=a("ul"),zb=a("li"),uoe=a("strong"),FCo=o("albert"),CCo=o(" \u2014 "),JP=a("a"),MCo=o("AlbertForMultipleChoice"),ECo=o(" (ALBERT model)"),yCo=l(),Vb=a("li"),poe=a("strong"),wCo=o("bert"),ACo=o(" \u2014 "),YP=a("a"),LCo=o("BertForMultipleChoice"),BCo=o(" (BERT model)"),xCo=l(),Wb=a("li"),_oe=a("strong"),kCo=o("big_bird"),RCo=o(" \u2014 "),KP=a("a"),SCo=o("BigBirdForMultipleChoice"),PCo=o(" (BigBird model)"),$Co=l(),Qb=a("li"),boe=a("strong"),ICo=o("camembert"),jCo=o(" \u2014 "),ZP=a("a"),NCo=o("CamembertForMultipleChoice"),DCo=o(" (CamemBERT model)"),qCo=l(),Hb=a("li"),voe=a("strong"),GCo=o("canine"),OCo=o(" \u2014 "),e$=a("a"),XCo=o("CanineForMultipleChoice"),zCo=o(" (Canine model)"),VCo=l(),Ub=a("li"),Toe=a("strong"),WCo=o("convbert"),QCo=o(" \u2014 "),o$=a("a"),HCo=o("ConvBertForMultipleChoice"),UCo=o(" (ConvBERT model)"),JCo=l(),Jb=a("li"),Foe=a("strong"),YCo=o("distilbert"),KCo=o(" \u2014 "),r$=a("a"),ZCo=o("DistilBertForMultipleChoice"),e4o=o(" (DistilBERT model)"),o4o=l(),Yb=a("li"),Coe=a("strong"),r4o=o("electra"),t4o=o(" \u2014 "),t$=a("a"),a4o=o("ElectraForMultipleChoice"),s4o=o(" (ELECTRA model)"),n4o=l(),Kb=a("li"),Moe=a("strong"),l4o=o("flaubert"),i4o=o(" \u2014 "),a$=a("a"),d4o=o("FlaubertForMultipleChoice"),c4o=o(" (FlauBERT model)"),m4o=l(),Zb=a("li"),Eoe=a("strong"),f4o=o("fnet"),g4o=o(" \u2014 "),s$=a("a"),h4o=o("FNetForMultipleChoice"),u4o=o(" (FNet model)"),p4o=l(),e2=a("li"),yoe=a("strong"),_4o=o("funnel"),b4o=o(" \u2014 "),n$=a("a"),v4o=o("FunnelForMultipleChoice"),T4o=o(" (Funnel Transformer model)"),F4o=l(),o2=a("li"),woe=a("strong"),C4o=o("ibert"),M4o=o(" \u2014 "),l$=a("a"),E4o=o("IBertForMultipleChoice"),y4o=o(" (I-BERT model)"),w4o=l(),r2=a("li"),Aoe=a("strong"),A4o=o("longformer"),L4o=o(" \u2014 "),i$=a("a"),B4o=o("LongformerForMultipleChoice"),x4o=o(" (Longformer model)"),k4o=l(),t2=a("li"),Loe=a("strong"),R4o=o("megatron-bert"),S4o=o(" \u2014 "),d$=a("a"),P4o=o("MegatronBertForMultipleChoice"),$4o=o(" (MegatronBert model)"),I4o=l(),a2=a("li"),Boe=a("strong"),j4o=o("mobilebert"),N4o=o(" \u2014 "),c$=a("a"),D4o=o("MobileBertForMultipleChoice"),q4o=o(" (MobileBERT model)"),G4o=l(),s2=a("li"),xoe=a("strong"),O4o=o("mpnet"),X4o=o(" \u2014 "),m$=a("a"),z4o=o("MPNetForMultipleChoice"),V4o=o(" (MPNet model)"),W4o=l(),n2=a("li"),koe=a("strong"),Q4o=o("nystromformer"),H4o=o(" \u2014 "),f$=a("a"),U4o=o("NystromformerForMultipleChoice"),J4o=o(" (Nystromformer model)"),Y4o=l(),l2=a("li"),Roe=a("strong"),K4o=o("qdqbert"),Z4o=o(" \u2014 "),Soe=a("code"),eMo=o("QDQBertForMultipleChoice"),oMo=o("(QDQBert model)"),rMo=l(),i2=a("li"),Poe=a("strong"),tMo=o("rembert"),aMo=o(" \u2014 "),g$=a("a"),sMo=o("RemBertForMultipleChoice"),nMo=o(" (RemBERT model)"),lMo=l(),d2=a("li"),$oe=a("strong"),iMo=o("roberta"),dMo=o(" \u2014 "),h$=a("a"),cMo=o("RobertaForMultipleChoice"),mMo=o(" (RoBERTa model)"),fMo=l(),c2=a("li"),Ioe=a("strong"),gMo=o("roformer"),hMo=o(" \u2014 "),u$=a("a"),uMo=o("RoFormerForMultipleChoice"),pMo=o(" (RoFormer model)"),_Mo=l(),m2=a("li"),joe=a("strong"),bMo=o("squeezebert"),vMo=o(" \u2014 "),p$=a("a"),TMo=o("SqueezeBertForMultipleChoice"),FMo=o(" (SqueezeBERT model)"),CMo=l(),f2=a("li"),Noe=a("strong"),MMo=o("xlm"),EMo=o(" \u2014 "),_$=a("a"),yMo=o("XLMForMultipleChoice"),wMo=o(" (XLM model)"),AMo=l(),g2=a("li"),Doe=a("strong"),LMo=o("xlm-roberta"),BMo=o(" \u2014 "),b$=a("a"),xMo=o("XLMRobertaForMultipleChoice"),kMo=o(" (XLM-RoBERTa model)"),RMo=l(),h2=a("li"),qoe=a("strong"),SMo=o("xlm-roberta-xl"),PMo=o(" \u2014 "),v$=a("a"),$Mo=o("XLMRobertaXLForMultipleChoice"),IMo=o(" (XLM-RoBERTa-XL model)"),jMo=l(),u2=a("li"),Goe=a("strong"),NMo=o("xlnet"),DMo=o(" \u2014 "),T$=a("a"),qMo=o("XLNetForMultipleChoice"),GMo=o(" (XLNet model)"),OMo=l(),p2=a("li"),Ooe=a("strong"),XMo=o("yoso"),zMo=o(" \u2014 "),F$=a("a"),VMo=o("YosoForMultipleChoice"),WMo=o(" (YOSO model)"),QMo=l(),_2=a("p"),HMo=o("The model is set in evaluation mode by default using "),Xoe=a("code"),UMo=o("model.eval()"),JMo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zoe=a("code"),YMo=o("model.train()"),KMo=l(),Voe=a("p"),ZMo=o("Examples:"),eEo=l(),m(K5.$$.fragment),DLe=l(),Ki=a("h2"),b2=a("a"),Woe=a("span"),m(Z5.$$.fragment),oEo=l(),Qoe=a("span"),rEo=o("AutoModelForNextSentencePrediction"),qLe=l(),Uo=a("div"),m(ey.$$.fragment),tEo=l(),Zi=a("p"),aEo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Hoe=a("code"),sEo=o("from_pretrained()"),nEo=o("class method or the "),Uoe=a("code"),lEo=o("from_config()"),iEo=o(`class
method.`),dEo=l(),oy=a("p"),cEo=o("This class cannot be instantiated directly using "),Joe=a("code"),mEo=o("__init__()"),fEo=o(" (throws an error)."),gEo=l(),Gr=a("div"),m(ry.$$.fragment),hEo=l(),Yoe=a("p"),uEo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),pEo=l(),ed=a("p"),_Eo=o(`Note:
Loading a model from its configuration file does `),Koe=a("strong"),bEo=o("not"),vEo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zoe=a("code"),TEo=o("from_pretrained()"),FEo=o("to load the model weights."),CEo=l(),ere=a("p"),MEo=o("Examples:"),EEo=l(),m(ty.$$.fragment),yEo=l(),je=a("div"),m(ay.$$.fragment),wEo=l(),ore=a("p"),AEo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),LEo=l(),Oa=a("p"),BEo=o("The model class to instantiate is selected based on the "),rre=a("code"),xEo=o("model_type"),kEo=o(` property of the config object (either
passed as an argument or loaded from `),tre=a("code"),REo=o("pretrained_model_name_or_path"),SEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),are=a("code"),PEo=o("pretrained_model_name_or_path"),$Eo=o(":"),IEo=l(),oa=a("ul"),v2=a("li"),sre=a("strong"),jEo=o("bert"),NEo=o(" \u2014 "),C$=a("a"),DEo=o("BertForNextSentencePrediction"),qEo=o(" (BERT model)"),GEo=l(),T2=a("li"),nre=a("strong"),OEo=o("fnet"),XEo=o(" \u2014 "),M$=a("a"),zEo=o("FNetForNextSentencePrediction"),VEo=o(" (FNet model)"),WEo=l(),F2=a("li"),lre=a("strong"),QEo=o("megatron-bert"),HEo=o(" \u2014 "),E$=a("a"),UEo=o("MegatronBertForNextSentencePrediction"),JEo=o(" (MegatronBert model)"),YEo=l(),C2=a("li"),ire=a("strong"),KEo=o("mobilebert"),ZEo=o(" \u2014 "),y$=a("a"),e3o=o("MobileBertForNextSentencePrediction"),o3o=o(" (MobileBERT model)"),r3o=l(),M2=a("li"),dre=a("strong"),t3o=o("qdqbert"),a3o=o(" \u2014 "),cre=a("code"),s3o=o("QDQBertForNextSentencePrediction"),n3o=o("(QDQBert model)"),l3o=l(),E2=a("p"),i3o=o("The model is set in evaluation mode by default using "),mre=a("code"),d3o=o("model.eval()"),c3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),fre=a("code"),m3o=o("model.train()"),f3o=l(),gre=a("p"),g3o=o("Examples:"),h3o=l(),m(sy.$$.fragment),GLe=l(),od=a("h2"),y2=a("a"),hre=a("span"),m(ny.$$.fragment),u3o=l(),ure=a("span"),p3o=o("AutoModelForTokenClassification"),OLe=l(),Jo=a("div"),m(ly.$$.fragment),_3o=l(),rd=a("p"),b3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),pre=a("code"),v3o=o("from_pretrained()"),T3o=o("class method or the "),_re=a("code"),F3o=o("from_config()"),C3o=o(`class
method.`),M3o=l(),iy=a("p"),E3o=o("This class cannot be instantiated directly using "),bre=a("code"),y3o=o("__init__()"),w3o=o(" (throws an error)."),A3o=l(),Or=a("div"),m(dy.$$.fragment),L3o=l(),vre=a("p"),B3o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),x3o=l(),td=a("p"),k3o=o(`Note:
Loading a model from its configuration file does `),Tre=a("strong"),R3o=o("not"),S3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fre=a("code"),P3o=o("from_pretrained()"),$3o=o("to load the model weights."),I3o=l(),Cre=a("p"),j3o=o("Examples:"),N3o=l(),m(cy.$$.fragment),D3o=l(),Ne=a("div"),m(my.$$.fragment),q3o=l(),Mre=a("p"),G3o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),O3o=l(),Xa=a("p"),X3o=o("The model class to instantiate is selected based on the "),Ere=a("code"),z3o=o("model_type"),V3o=o(` property of the config object (either
passed as an argument or loaded from `),yre=a("code"),W3o=o("pretrained_model_name_or_path"),Q3o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wre=a("code"),H3o=o("pretrained_model_name_or_path"),U3o=o(":"),J3o=l(),N=a("ul"),w2=a("li"),Are=a("strong"),Y3o=o("albert"),K3o=o(" \u2014 "),w$=a("a"),Z3o=o("AlbertForTokenClassification"),e5o=o(" (ALBERT model)"),o5o=l(),A2=a("li"),Lre=a("strong"),r5o=o("bert"),t5o=o(" \u2014 "),A$=a("a"),a5o=o("BertForTokenClassification"),s5o=o(" (BERT model)"),n5o=l(),L2=a("li"),Bre=a("strong"),l5o=o("big_bird"),i5o=o(" \u2014 "),L$=a("a"),d5o=o("BigBirdForTokenClassification"),c5o=o(" (BigBird model)"),m5o=l(),B2=a("li"),xre=a("strong"),f5o=o("camembert"),g5o=o(" \u2014 "),B$=a("a"),h5o=o("CamembertForTokenClassification"),u5o=o(" (CamemBERT model)"),p5o=l(),x2=a("li"),kre=a("strong"),_5o=o("canine"),b5o=o(" \u2014 "),x$=a("a"),v5o=o("CanineForTokenClassification"),T5o=o(" (Canine model)"),F5o=l(),k2=a("li"),Rre=a("strong"),C5o=o("convbert"),M5o=o(" \u2014 "),k$=a("a"),E5o=o("ConvBertForTokenClassification"),y5o=o(" (ConvBERT model)"),w5o=l(),R2=a("li"),Sre=a("strong"),A5o=o("deberta"),L5o=o(" \u2014 "),R$=a("a"),B5o=o("DebertaForTokenClassification"),x5o=o(" (DeBERTa model)"),k5o=l(),S2=a("li"),Pre=a("strong"),R5o=o("deberta-v2"),S5o=o(" \u2014 "),S$=a("a"),P5o=o("DebertaV2ForTokenClassification"),$5o=o(" (DeBERTa-v2 model)"),I5o=l(),P2=a("li"),$re=a("strong"),j5o=o("distilbert"),N5o=o(" \u2014 "),P$=a("a"),D5o=o("DistilBertForTokenClassification"),q5o=o(" (DistilBERT model)"),G5o=l(),$2=a("li"),Ire=a("strong"),O5o=o("electra"),X5o=o(" \u2014 "),$$=a("a"),z5o=o("ElectraForTokenClassification"),V5o=o(" (ELECTRA model)"),W5o=l(),I2=a("li"),jre=a("strong"),Q5o=o("flaubert"),H5o=o(" \u2014 "),I$=a("a"),U5o=o("FlaubertForTokenClassification"),J5o=o(" (FlauBERT model)"),Y5o=l(),j2=a("li"),Nre=a("strong"),K5o=o("fnet"),Z5o=o(" \u2014 "),j$=a("a"),eyo=o("FNetForTokenClassification"),oyo=o(" (FNet model)"),ryo=l(),N2=a("li"),Dre=a("strong"),tyo=o("funnel"),ayo=o(" \u2014 "),N$=a("a"),syo=o("FunnelForTokenClassification"),nyo=o(" (Funnel Transformer model)"),lyo=l(),D2=a("li"),qre=a("strong"),iyo=o("gpt2"),dyo=o(" \u2014 "),D$=a("a"),cyo=o("GPT2ForTokenClassification"),myo=o(" (OpenAI GPT-2 model)"),fyo=l(),q2=a("li"),Gre=a("strong"),gyo=o("ibert"),hyo=o(" \u2014 "),q$=a("a"),uyo=o("IBertForTokenClassification"),pyo=o(" (I-BERT model)"),_yo=l(),G2=a("li"),Ore=a("strong"),byo=o("layoutlm"),vyo=o(" \u2014 "),G$=a("a"),Tyo=o("LayoutLMForTokenClassification"),Fyo=o(" (LayoutLM model)"),Cyo=l(),O2=a("li"),Xre=a("strong"),Myo=o("layoutlmv2"),Eyo=o(" \u2014 "),O$=a("a"),yyo=o("LayoutLMv2ForTokenClassification"),wyo=o(" (LayoutLMv2 model)"),Ayo=l(),X2=a("li"),zre=a("strong"),Lyo=o("longformer"),Byo=o(" \u2014 "),X$=a("a"),xyo=o("LongformerForTokenClassification"),kyo=o(" (Longformer model)"),Ryo=l(),z2=a("li"),Vre=a("strong"),Syo=o("megatron-bert"),Pyo=o(" \u2014 "),z$=a("a"),$yo=o("MegatronBertForTokenClassification"),Iyo=o(" (MegatronBert model)"),jyo=l(),V2=a("li"),Wre=a("strong"),Nyo=o("mobilebert"),Dyo=o(" \u2014 "),V$=a("a"),qyo=o("MobileBertForTokenClassification"),Gyo=o(" (MobileBERT model)"),Oyo=l(),W2=a("li"),Qre=a("strong"),Xyo=o("mpnet"),zyo=o(" \u2014 "),W$=a("a"),Vyo=o("MPNetForTokenClassification"),Wyo=o(" (MPNet model)"),Qyo=l(),Q2=a("li"),Hre=a("strong"),Hyo=o("nystromformer"),Uyo=o(" \u2014 "),Q$=a("a"),Jyo=o("NystromformerForTokenClassification"),Yyo=o(" (Nystromformer model)"),Kyo=l(),H2=a("li"),Ure=a("strong"),Zyo=o("qdqbert"),ewo=o(" \u2014 "),Jre=a("code"),owo=o("QDQBertForTokenClassification"),rwo=o("(QDQBert model)"),two=l(),U2=a("li"),Yre=a("strong"),awo=o("rembert"),swo=o(" \u2014 "),H$=a("a"),nwo=o("RemBertForTokenClassification"),lwo=o(" (RemBERT model)"),iwo=l(),J2=a("li"),Kre=a("strong"),dwo=o("roberta"),cwo=o(" \u2014 "),U$=a("a"),mwo=o("RobertaForTokenClassification"),fwo=o(" (RoBERTa model)"),gwo=l(),Y2=a("li"),Zre=a("strong"),hwo=o("roformer"),uwo=o(" \u2014 "),J$=a("a"),pwo=o("RoFormerForTokenClassification"),_wo=o(" (RoFormer model)"),bwo=l(),K2=a("li"),ete=a("strong"),vwo=o("squeezebert"),Two=o(" \u2014 "),Y$=a("a"),Fwo=o("SqueezeBertForTokenClassification"),Cwo=o(" (SqueezeBERT model)"),Mwo=l(),Z2=a("li"),ote=a("strong"),Ewo=o("xlm"),ywo=o(" \u2014 "),K$=a("a"),wwo=o("XLMForTokenClassification"),Awo=o(" (XLM model)"),Lwo=l(),ev=a("li"),rte=a("strong"),Bwo=o("xlm-roberta"),xwo=o(" \u2014 "),Z$=a("a"),kwo=o("XLMRobertaForTokenClassification"),Rwo=o(" (XLM-RoBERTa model)"),Swo=l(),ov=a("li"),tte=a("strong"),Pwo=o("xlm-roberta-xl"),$wo=o(" \u2014 "),eI=a("a"),Iwo=o("XLMRobertaXLForTokenClassification"),jwo=o(" (XLM-RoBERTa-XL model)"),Nwo=l(),rv=a("li"),ate=a("strong"),Dwo=o("xlnet"),qwo=o(" \u2014 "),oI=a("a"),Gwo=o("XLNetForTokenClassification"),Owo=o(" (XLNet model)"),Xwo=l(),tv=a("li"),ste=a("strong"),zwo=o("yoso"),Vwo=o(" \u2014 "),rI=a("a"),Wwo=o("YosoForTokenClassification"),Qwo=o(" (YOSO model)"),Hwo=l(),av=a("p"),Uwo=o("The model is set in evaluation mode by default using "),nte=a("code"),Jwo=o("model.eval()"),Ywo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lte=a("code"),Kwo=o("model.train()"),Zwo=l(),ite=a("p"),eAo=o("Examples:"),oAo=l(),m(fy.$$.fragment),XLe=l(),ad=a("h2"),sv=a("a"),dte=a("span"),m(gy.$$.fragment),rAo=l(),cte=a("span"),tAo=o("AutoModelForQuestionAnswering"),zLe=l(),Yo=a("div"),m(hy.$$.fragment),aAo=l(),sd=a("p"),sAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),mte=a("code"),nAo=o("from_pretrained()"),lAo=o("class method or the "),fte=a("code"),iAo=o("from_config()"),dAo=o(`class
method.`),cAo=l(),uy=a("p"),mAo=o("This class cannot be instantiated directly using "),gte=a("code"),fAo=o("__init__()"),gAo=o(" (throws an error)."),hAo=l(),Xr=a("div"),m(py.$$.fragment),uAo=l(),hte=a("p"),pAo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),_Ao=l(),nd=a("p"),bAo=o(`Note:
Loading a model from its configuration file does `),ute=a("strong"),vAo=o("not"),TAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pte=a("code"),FAo=o("from_pretrained()"),CAo=o("to load the model weights."),MAo=l(),_te=a("p"),EAo=o("Examples:"),yAo=l(),m(_y.$$.fragment),wAo=l(),De=a("div"),m(by.$$.fragment),AAo=l(),bte=a("p"),LAo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),BAo=l(),za=a("p"),xAo=o("The model class to instantiate is selected based on the "),vte=a("code"),kAo=o("model_type"),RAo=o(` property of the config object (either
passed as an argument or loaded from `),Tte=a("code"),SAo=o("pretrained_model_name_or_path"),PAo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fte=a("code"),$Ao=o("pretrained_model_name_or_path"),IAo=o(":"),jAo=l(),R=a("ul"),nv=a("li"),Cte=a("strong"),NAo=o("albert"),DAo=o(" \u2014 "),tI=a("a"),qAo=o("AlbertForQuestionAnswering"),GAo=o(" (ALBERT model)"),OAo=l(),lv=a("li"),Mte=a("strong"),XAo=o("bart"),zAo=o(" \u2014 "),aI=a("a"),VAo=o("BartForQuestionAnswering"),WAo=o(" (BART model)"),QAo=l(),iv=a("li"),Ete=a("strong"),HAo=o("bert"),UAo=o(" \u2014 "),sI=a("a"),JAo=o("BertForQuestionAnswering"),YAo=o(" (BERT model)"),KAo=l(),dv=a("li"),yte=a("strong"),ZAo=o("big_bird"),e0o=o(" \u2014 "),nI=a("a"),o0o=o("BigBirdForQuestionAnswering"),r0o=o(" (BigBird model)"),t0o=l(),cv=a("li"),wte=a("strong"),a0o=o("bigbird_pegasus"),s0o=o(" \u2014 "),lI=a("a"),n0o=o("BigBirdPegasusForQuestionAnswering"),l0o=o(" (BigBirdPegasus model)"),i0o=l(),mv=a("li"),Ate=a("strong"),d0o=o("camembert"),c0o=o(" \u2014 "),iI=a("a"),m0o=o("CamembertForQuestionAnswering"),f0o=o(" (CamemBERT model)"),g0o=l(),fv=a("li"),Lte=a("strong"),h0o=o("canine"),u0o=o(" \u2014 "),dI=a("a"),p0o=o("CanineForQuestionAnswering"),_0o=o(" (Canine model)"),b0o=l(),gv=a("li"),Bte=a("strong"),v0o=o("convbert"),T0o=o(" \u2014 "),cI=a("a"),F0o=o("ConvBertForQuestionAnswering"),C0o=o(" (ConvBERT model)"),M0o=l(),hv=a("li"),xte=a("strong"),E0o=o("deberta"),y0o=o(" \u2014 "),mI=a("a"),w0o=o("DebertaForQuestionAnswering"),A0o=o(" (DeBERTa model)"),L0o=l(),uv=a("li"),kte=a("strong"),B0o=o("deberta-v2"),x0o=o(" \u2014 "),fI=a("a"),k0o=o("DebertaV2ForQuestionAnswering"),R0o=o(" (DeBERTa-v2 model)"),S0o=l(),pv=a("li"),Rte=a("strong"),P0o=o("distilbert"),$0o=o(" \u2014 "),gI=a("a"),I0o=o("DistilBertForQuestionAnswering"),j0o=o(" (DistilBERT model)"),N0o=l(),_v=a("li"),Ste=a("strong"),D0o=o("electra"),q0o=o(" \u2014 "),hI=a("a"),G0o=o("ElectraForQuestionAnswering"),O0o=o(" (ELECTRA model)"),X0o=l(),bv=a("li"),Pte=a("strong"),z0o=o("flaubert"),V0o=o(" \u2014 "),uI=a("a"),W0o=o("FlaubertForQuestionAnsweringSimple"),Q0o=o(" (FlauBERT model)"),H0o=l(),vv=a("li"),$te=a("strong"),U0o=o("fnet"),J0o=o(" \u2014 "),pI=a("a"),Y0o=o("FNetForQuestionAnswering"),K0o=o(" (FNet model)"),Z0o=l(),Tv=a("li"),Ite=a("strong"),e6o=o("funnel"),o6o=o(" \u2014 "),_I=a("a"),r6o=o("FunnelForQuestionAnswering"),t6o=o(" (Funnel Transformer model)"),a6o=l(),Fv=a("li"),jte=a("strong"),s6o=o("gptj"),n6o=o(" \u2014 "),bI=a("a"),l6o=o("GPTJForQuestionAnswering"),i6o=o(" (GPT-J model)"),d6o=l(),Cv=a("li"),Nte=a("strong"),c6o=o("ibert"),m6o=o(" \u2014 "),vI=a("a"),f6o=o("IBertForQuestionAnswering"),g6o=o(" (I-BERT model)"),h6o=l(),Mv=a("li"),Dte=a("strong"),u6o=o("layoutlmv2"),p6o=o(" \u2014 "),TI=a("a"),_6o=o("LayoutLMv2ForQuestionAnswering"),b6o=o(" (LayoutLMv2 model)"),v6o=l(),Ev=a("li"),qte=a("strong"),T6o=o("led"),F6o=o(" \u2014 "),FI=a("a"),C6o=o("LEDForQuestionAnswering"),M6o=o(" (LED model)"),E6o=l(),yv=a("li"),Gte=a("strong"),y6o=o("longformer"),w6o=o(" \u2014 "),CI=a("a"),A6o=o("LongformerForQuestionAnswering"),L6o=o(" (Longformer model)"),B6o=l(),wv=a("li"),Ote=a("strong"),x6o=o("lxmert"),k6o=o(" \u2014 "),MI=a("a"),R6o=o("LxmertForQuestionAnswering"),S6o=o(" (LXMERT model)"),P6o=l(),Av=a("li"),Xte=a("strong"),$6o=o("mbart"),I6o=o(" \u2014 "),EI=a("a"),j6o=o("MBartForQuestionAnswering"),N6o=o(" (mBART model)"),D6o=l(),Lv=a("li"),zte=a("strong"),q6o=o("megatron-bert"),G6o=o(" \u2014 "),yI=a("a"),O6o=o("MegatronBertForQuestionAnswering"),X6o=o(" (MegatronBert model)"),z6o=l(),Bv=a("li"),Vte=a("strong"),V6o=o("mobilebert"),W6o=o(" \u2014 "),wI=a("a"),Q6o=o("MobileBertForQuestionAnswering"),H6o=o(" (MobileBERT model)"),U6o=l(),xv=a("li"),Wte=a("strong"),J6o=o("mpnet"),Y6o=o(" \u2014 "),AI=a("a"),K6o=o("MPNetForQuestionAnswering"),Z6o=o(" (MPNet model)"),eLo=l(),kv=a("li"),Qte=a("strong"),oLo=o("nystromformer"),rLo=o(" \u2014 "),LI=a("a"),tLo=o("NystromformerForQuestionAnswering"),aLo=o(" (Nystromformer model)"),sLo=l(),Rv=a("li"),Hte=a("strong"),nLo=o("qdqbert"),lLo=o(" \u2014 "),Ute=a("code"),iLo=o("QDQBertForQuestionAnswering"),dLo=o("(QDQBert model)"),cLo=l(),Sv=a("li"),Jte=a("strong"),mLo=o("reformer"),fLo=o(" \u2014 "),BI=a("a"),gLo=o("ReformerForQuestionAnswering"),hLo=o(" (Reformer model)"),uLo=l(),Pv=a("li"),Yte=a("strong"),pLo=o("rembert"),_Lo=o(" \u2014 "),xI=a("a"),bLo=o("RemBertForQuestionAnswering"),vLo=o(" (RemBERT model)"),TLo=l(),$v=a("li"),Kte=a("strong"),FLo=o("roberta"),CLo=o(" \u2014 "),kI=a("a"),MLo=o("RobertaForQuestionAnswering"),ELo=o(" (RoBERTa model)"),yLo=l(),Iv=a("li"),Zte=a("strong"),wLo=o("roformer"),ALo=o(" \u2014 "),RI=a("a"),LLo=o("RoFormerForQuestionAnswering"),BLo=o(" (RoFormer model)"),xLo=l(),jv=a("li"),eae=a("strong"),kLo=o("splinter"),RLo=o(" \u2014 "),SI=a("a"),SLo=o("SplinterForQuestionAnswering"),PLo=o(" (Splinter model)"),$Lo=l(),Nv=a("li"),oae=a("strong"),ILo=o("squeezebert"),jLo=o(" \u2014 "),PI=a("a"),NLo=o("SqueezeBertForQuestionAnswering"),DLo=o(" (SqueezeBERT model)"),qLo=l(),Dv=a("li"),rae=a("strong"),GLo=o("xlm"),OLo=o(" \u2014 "),$I=a("a"),XLo=o("XLMForQuestionAnsweringSimple"),zLo=o(" (XLM model)"),VLo=l(),qv=a("li"),tae=a("strong"),WLo=o("xlm-roberta"),QLo=o(" \u2014 "),II=a("a"),HLo=o("XLMRobertaForQuestionAnswering"),ULo=o(" (XLM-RoBERTa model)"),JLo=l(),Gv=a("li"),aae=a("strong"),YLo=o("xlm-roberta-xl"),KLo=o(" \u2014 "),jI=a("a"),ZLo=o("XLMRobertaXLForQuestionAnswering"),e7o=o(" (XLM-RoBERTa-XL model)"),o7o=l(),Ov=a("li"),sae=a("strong"),r7o=o("xlnet"),t7o=o(" \u2014 "),NI=a("a"),a7o=o("XLNetForQuestionAnsweringSimple"),s7o=o(" (XLNet model)"),n7o=l(),Xv=a("li"),nae=a("strong"),l7o=o("yoso"),i7o=o(" \u2014 "),DI=a("a"),d7o=o("YosoForQuestionAnswering"),c7o=o(" (YOSO model)"),m7o=l(),zv=a("p"),f7o=o("The model is set in evaluation mode by default using "),lae=a("code"),g7o=o("model.eval()"),h7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iae=a("code"),u7o=o("model.train()"),p7o=l(),dae=a("p"),_7o=o("Examples:"),b7o=l(),m(vy.$$.fragment),VLe=l(),ld=a("h2"),Vv=a("a"),cae=a("span"),m(Ty.$$.fragment),v7o=l(),mae=a("span"),T7o=o("AutoModelForTableQuestionAnswering"),WLe=l(),Ko=a("div"),m(Fy.$$.fragment),F7o=l(),id=a("p"),C7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),fae=a("code"),M7o=o("from_pretrained()"),E7o=o("class method or the "),gae=a("code"),y7o=o("from_config()"),w7o=o(`class
method.`),A7o=l(),Cy=a("p"),L7o=o("This class cannot be instantiated directly using "),hae=a("code"),B7o=o("__init__()"),x7o=o(" (throws an error)."),k7o=l(),zr=a("div"),m(My.$$.fragment),R7o=l(),uae=a("p"),S7o=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),P7o=l(),dd=a("p"),$7o=o(`Note:
Loading a model from its configuration file does `),pae=a("strong"),I7o=o("not"),j7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ae=a("code"),N7o=o("from_pretrained()"),D7o=o("to load the model weights."),q7o=l(),bae=a("p"),G7o=o("Examples:"),O7o=l(),m(Ey.$$.fragment),X7o=l(),qe=a("div"),m(yy.$$.fragment),z7o=l(),vae=a("p"),V7o=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),W7o=l(),Va=a("p"),Q7o=o("The model class to instantiate is selected based on the "),Tae=a("code"),H7o=o("model_type"),U7o=o(` property of the config object (either
passed as an argument or loaded from `),Fae=a("code"),J7o=o("pretrained_model_name_or_path"),Y7o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cae=a("code"),K7o=o("pretrained_model_name_or_path"),Z7o=o(":"),e8o=l(),Mae=a("ul"),Wv=a("li"),Eae=a("strong"),o8o=o("tapas"),r8o=o(" \u2014 "),qI=a("a"),t8o=o("TapasForQuestionAnswering"),a8o=o(" (TAPAS model)"),s8o=l(),Qv=a("p"),n8o=o("The model is set in evaluation mode by default using "),yae=a("code"),l8o=o("model.eval()"),i8o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wae=a("code"),d8o=o("model.train()"),c8o=l(),Aae=a("p"),m8o=o("Examples:"),f8o=l(),m(wy.$$.fragment),QLe=l(),cd=a("h2"),Hv=a("a"),Lae=a("span"),m(Ay.$$.fragment),g8o=l(),Bae=a("span"),h8o=o("AutoModelForImageClassification"),HLe=l(),Zo=a("div"),m(Ly.$$.fragment),u8o=l(),md=a("p"),p8o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),xae=a("code"),_8o=o("from_pretrained()"),b8o=o("class method or the "),kae=a("code"),v8o=o("from_config()"),T8o=o(`class
method.`),F8o=l(),By=a("p"),C8o=o("This class cannot be instantiated directly using "),Rae=a("code"),M8o=o("__init__()"),E8o=o(" (throws an error)."),y8o=l(),Vr=a("div"),m(xy.$$.fragment),w8o=l(),Sae=a("p"),A8o=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),L8o=l(),fd=a("p"),B8o=o(`Note:
Loading a model from its configuration file does `),Pae=a("strong"),x8o=o("not"),k8o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ae=a("code"),R8o=o("from_pretrained()"),S8o=o("to load the model weights."),P8o=l(),Iae=a("p"),$8o=o("Examples:"),I8o=l(),m(ky.$$.fragment),j8o=l(),Ge=a("div"),m(Ry.$$.fragment),N8o=l(),jae=a("p"),D8o=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),q8o=l(),Wa=a("p"),G8o=o("The model class to instantiate is selected based on the "),Nae=a("code"),O8o=o("model_type"),X8o=o(` property of the config object (either
passed as an argument or loaded from `),Dae=a("code"),z8o=o("pretrained_model_name_or_path"),V8o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qae=a("code"),W8o=o("pretrained_model_name_or_path"),Q8o=o(":"),H8o=l(),we=a("ul"),Uv=a("li"),Gae=a("strong"),U8o=o("beit"),J8o=o(" \u2014 "),GI=a("a"),Y8o=o("BeitForImageClassification"),K8o=o(" (BEiT model)"),Z8o=l(),Jv=a("li"),Oae=a("strong"),e9o=o("convnext"),o9o=o(" \u2014 "),OI=a("a"),r9o=o("ConvNextForImageClassification"),t9o=o(" (ConvNext model)"),a9o=l(),An=a("li"),Xae=a("strong"),s9o=o("deit"),n9o=o(" \u2014 "),XI=a("a"),l9o=o("DeiTForImageClassification"),i9o=o(" or "),zI=a("a"),d9o=o("DeiTForImageClassificationWithTeacher"),c9o=o(" (DeiT model)"),m9o=l(),Yv=a("li"),zae=a("strong"),f9o=o("imagegpt"),g9o=o(" \u2014 "),VI=a("a"),h9o=o("ImageGPTForImageClassification"),u9o=o(" (ImageGPT model)"),p9o=l(),ta=a("li"),Vae=a("strong"),_9o=o("perceiver"),b9o=o(" \u2014 "),WI=a("a"),v9o=o("PerceiverForImageClassificationLearned"),T9o=o(" or "),QI=a("a"),F9o=o("PerceiverForImageClassificationFourier"),C9o=o(" or "),HI=a("a"),M9o=o("PerceiverForImageClassificationConvProcessing"),E9o=o(" (Perceiver model)"),y9o=l(),Kv=a("li"),Wae=a("strong"),w9o=o("segformer"),A9o=o(" \u2014 "),UI=a("a"),L9o=o("SegformerForImageClassification"),B9o=o(" (SegFormer model)"),x9o=l(),Zv=a("li"),Qae=a("strong"),k9o=o("swin"),R9o=o(" \u2014 "),JI=a("a"),S9o=o("SwinForImageClassification"),P9o=o(" (Swin model)"),$9o=l(),eT=a("li"),Hae=a("strong"),I9o=o("vit"),j9o=o(" \u2014 "),YI=a("a"),N9o=o("ViTForImageClassification"),D9o=o(" (ViT model)"),q9o=l(),oT=a("p"),G9o=o("The model is set in evaluation mode by default using "),Uae=a("code"),O9o=o("model.eval()"),X9o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jae=a("code"),z9o=o("model.train()"),V9o=l(),Yae=a("p"),W9o=o("Examples:"),Q9o=l(),m(Sy.$$.fragment),ULe=l(),gd=a("h2"),rT=a("a"),Kae=a("span"),m(Py.$$.fragment),H9o=l(),Zae=a("span"),U9o=o("AutoModelForVision2Seq"),JLe=l(),er=a("div"),m($y.$$.fragment),J9o=l(),hd=a("p"),Y9o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ese=a("code"),K9o=o("from_pretrained()"),Z9o=o("class method or the "),ose=a("code"),eBo=o("from_config()"),oBo=o(`class
method.`),rBo=l(),Iy=a("p"),tBo=o("This class cannot be instantiated directly using "),rse=a("code"),aBo=o("__init__()"),sBo=o(" (throws an error)."),nBo=l(),Wr=a("div"),m(jy.$$.fragment),lBo=l(),tse=a("p"),iBo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),dBo=l(),ud=a("p"),cBo=o(`Note:
Loading a model from its configuration file does `),ase=a("strong"),mBo=o("not"),fBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sse=a("code"),gBo=o("from_pretrained()"),hBo=o("to load the model weights."),uBo=l(),nse=a("p"),pBo=o("Examples:"),_Bo=l(),m(Ny.$$.fragment),bBo=l(),Oe=a("div"),m(Dy.$$.fragment),vBo=l(),lse=a("p"),TBo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),FBo=l(),Qa=a("p"),CBo=o("The model class to instantiate is selected based on the "),ise=a("code"),MBo=o("model_type"),EBo=o(` property of the config object (either
passed as an argument or loaded from `),dse=a("code"),yBo=o("pretrained_model_name_or_path"),wBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cse=a("code"),ABo=o("pretrained_model_name_or_path"),LBo=o(":"),BBo=l(),mse=a("ul"),tT=a("li"),fse=a("strong"),xBo=o("vision-encoder-decoder"),kBo=o(" \u2014 "),KI=a("a"),RBo=o("VisionEncoderDecoderModel"),SBo=o(" (Vision Encoder decoder model)"),PBo=l(),aT=a("p"),$Bo=o("The model is set in evaluation mode by default using "),gse=a("code"),IBo=o("model.eval()"),jBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hse=a("code"),NBo=o("model.train()"),DBo=l(),use=a("p"),qBo=o("Examples:"),GBo=l(),m(qy.$$.fragment),YLe=l(),pd=a("h2"),sT=a("a"),pse=a("span"),m(Gy.$$.fragment),OBo=l(),_se=a("span"),XBo=o("AutoModelForAudioClassification"),KLe=l(),or=a("div"),m(Oy.$$.fragment),zBo=l(),_d=a("p"),VBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),bse=a("code"),WBo=o("from_pretrained()"),QBo=o("class method or the "),vse=a("code"),HBo=o("from_config()"),UBo=o(`class
method.`),JBo=l(),Xy=a("p"),YBo=o("This class cannot be instantiated directly using "),Tse=a("code"),KBo=o("__init__()"),ZBo=o(" (throws an error)."),exo=l(),Qr=a("div"),m(zy.$$.fragment),oxo=l(),Fse=a("p"),rxo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),txo=l(),bd=a("p"),axo=o(`Note:
Loading a model from its configuration file does `),Cse=a("strong"),sxo=o("not"),nxo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mse=a("code"),lxo=o("from_pretrained()"),ixo=o("to load the model weights."),dxo=l(),Ese=a("p"),cxo=o("Examples:"),mxo=l(),m(Vy.$$.fragment),fxo=l(),Xe=a("div"),m(Wy.$$.fragment),gxo=l(),yse=a("p"),hxo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),uxo=l(),Ha=a("p"),pxo=o("The model class to instantiate is selected based on the "),wse=a("code"),_xo=o("model_type"),bxo=o(` property of the config object (either
passed as an argument or loaded from `),Ase=a("code"),vxo=o("pretrained_model_name_or_path"),Txo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lse=a("code"),Fxo=o("pretrained_model_name_or_path"),Cxo=o(":"),Mxo=l(),ro=a("ul"),nT=a("li"),Bse=a("strong"),Exo=o("hubert"),yxo=o(" \u2014 "),ZI=a("a"),wxo=o("HubertForSequenceClassification"),Axo=o(" (Hubert model)"),Lxo=l(),lT=a("li"),xse=a("strong"),Bxo=o("sew"),xxo=o(" \u2014 "),ej=a("a"),kxo=o("SEWForSequenceClassification"),Rxo=o(" (SEW model)"),Sxo=l(),iT=a("li"),kse=a("strong"),Pxo=o("sew-d"),$xo=o(" \u2014 "),oj=a("a"),Ixo=o("SEWDForSequenceClassification"),jxo=o(" (SEW-D model)"),Nxo=l(),dT=a("li"),Rse=a("strong"),Dxo=o("unispeech"),qxo=o(" \u2014 "),rj=a("a"),Gxo=o("UniSpeechForSequenceClassification"),Oxo=o(" (UniSpeech model)"),Xxo=l(),cT=a("li"),Sse=a("strong"),zxo=o("unispeech-sat"),Vxo=o(" \u2014 "),tj=a("a"),Wxo=o("UniSpeechSatForSequenceClassification"),Qxo=o(" (UniSpeechSat model)"),Hxo=l(),mT=a("li"),Pse=a("strong"),Uxo=o("wav2vec2"),Jxo=o(" \u2014 "),aj=a("a"),Yxo=o("Wav2Vec2ForSequenceClassification"),Kxo=o(" (Wav2Vec2 model)"),Zxo=l(),fT=a("li"),$se=a("strong"),eko=o("wavlm"),oko=o(" \u2014 "),sj=a("a"),rko=o("WavLMForSequenceClassification"),tko=o(" (WavLM model)"),ako=l(),gT=a("p"),sko=o("The model is set in evaluation mode by default using "),Ise=a("code"),nko=o("model.eval()"),lko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jse=a("code"),iko=o("model.train()"),dko=l(),Nse=a("p"),cko=o("Examples:"),mko=l(),m(Qy.$$.fragment),ZLe=l(),vd=a("h2"),hT=a("a"),Dse=a("span"),m(Hy.$$.fragment),fko=l(),qse=a("span"),gko=o("AutoModelForAudioFrameClassification"),e7e=l(),rr=a("div"),m(Uy.$$.fragment),hko=l(),Td=a("p"),uko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Gse=a("code"),pko=o("from_pretrained()"),_ko=o("class method or the "),Ose=a("code"),bko=o("from_config()"),vko=o(`class
method.`),Tko=l(),Jy=a("p"),Fko=o("This class cannot be instantiated directly using "),Xse=a("code"),Cko=o("__init__()"),Mko=o(" (throws an error)."),Eko=l(),Hr=a("div"),m(Yy.$$.fragment),yko=l(),zse=a("p"),wko=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),Ako=l(),Fd=a("p"),Lko=o(`Note:
Loading a model from its configuration file does `),Vse=a("strong"),Bko=o("not"),xko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=a("code"),kko=o("from_pretrained()"),Rko=o("to load the model weights."),Sko=l(),Qse=a("p"),Pko=o("Examples:"),$ko=l(),m(Ky.$$.fragment),Iko=l(),ze=a("div"),m(Zy.$$.fragment),jko=l(),Hse=a("p"),Nko=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),Dko=l(),Ua=a("p"),qko=o("The model class to instantiate is selected based on the "),Use=a("code"),Gko=o("model_type"),Oko=o(` property of the config object (either
passed as an argument or loaded from `),Jse=a("code"),Xko=o("pretrained_model_name_or_path"),zko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yse=a("code"),Vko=o("pretrained_model_name_or_path"),Wko=o(":"),Qko=l(),Cd=a("ul"),uT=a("li"),Kse=a("strong"),Hko=o("unispeech-sat"),Uko=o(" \u2014 "),nj=a("a"),Jko=o("UniSpeechSatForAudioFrameClassification"),Yko=o(" (UniSpeechSat model)"),Kko=l(),pT=a("li"),Zse=a("strong"),Zko=o("wav2vec2"),eRo=o(" \u2014 "),lj=a("a"),oRo=o("Wav2Vec2ForAudioFrameClassification"),rRo=o(" (Wav2Vec2 model)"),tRo=l(),_T=a("li"),ene=a("strong"),aRo=o("wavlm"),sRo=o(" \u2014 "),ij=a("a"),nRo=o("WavLMForAudioFrameClassification"),lRo=o(" (WavLM model)"),iRo=l(),bT=a("p"),dRo=o("The model is set in evaluation mode by default using "),one=a("code"),cRo=o("model.eval()"),mRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),rne=a("code"),fRo=o("model.train()"),gRo=l(),tne=a("p"),hRo=o("Examples:"),uRo=l(),m(ew.$$.fragment),o7e=l(),Md=a("h2"),vT=a("a"),ane=a("span"),m(ow.$$.fragment),pRo=l(),sne=a("span"),_Ro=o("AutoModelForCTC"),r7e=l(),tr=a("div"),m(rw.$$.fragment),bRo=l(),Ed=a("p"),vRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),nne=a("code"),TRo=o("from_pretrained()"),FRo=o("class method or the "),lne=a("code"),CRo=o("from_config()"),MRo=o(`class
method.`),ERo=l(),tw=a("p"),yRo=o("This class cannot be instantiated directly using "),ine=a("code"),wRo=o("__init__()"),ARo=o(" (throws an error)."),LRo=l(),Ur=a("div"),m(aw.$$.fragment),BRo=l(),dne=a("p"),xRo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),kRo=l(),yd=a("p"),RRo=o(`Note:
Loading a model from its configuration file does `),cne=a("strong"),SRo=o("not"),PRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mne=a("code"),$Ro=o("from_pretrained()"),IRo=o("to load the model weights."),jRo=l(),fne=a("p"),NRo=o("Examples:"),DRo=l(),m(sw.$$.fragment),qRo=l(),Ve=a("div"),m(nw.$$.fragment),GRo=l(),gne=a("p"),ORo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),XRo=l(),Ja=a("p"),zRo=o("The model class to instantiate is selected based on the "),hne=a("code"),VRo=o("model_type"),WRo=o(` property of the config object (either
passed as an argument or loaded from `),une=a("code"),QRo=o("pretrained_model_name_or_path"),HRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pne=a("code"),URo=o("pretrained_model_name_or_path"),JRo=o(":"),YRo=l(),to=a("ul"),TT=a("li"),_ne=a("strong"),KRo=o("hubert"),ZRo=o(" \u2014 "),dj=a("a"),eSo=o("HubertForCTC"),oSo=o(" (Hubert model)"),rSo=l(),FT=a("li"),bne=a("strong"),tSo=o("sew"),aSo=o(" \u2014 "),cj=a("a"),sSo=o("SEWForCTC"),nSo=o(" (SEW model)"),lSo=l(),CT=a("li"),vne=a("strong"),iSo=o("sew-d"),dSo=o(" \u2014 "),mj=a("a"),cSo=o("SEWDForCTC"),mSo=o(" (SEW-D model)"),fSo=l(),MT=a("li"),Tne=a("strong"),gSo=o("unispeech"),hSo=o(" \u2014 "),fj=a("a"),uSo=o("UniSpeechForCTC"),pSo=o(" (UniSpeech model)"),_So=l(),ET=a("li"),Fne=a("strong"),bSo=o("unispeech-sat"),vSo=o(" \u2014 "),gj=a("a"),TSo=o("UniSpeechSatForCTC"),FSo=o(" (UniSpeechSat model)"),CSo=l(),yT=a("li"),Cne=a("strong"),MSo=o("wav2vec2"),ESo=o(" \u2014 "),hj=a("a"),ySo=o("Wav2Vec2ForCTC"),wSo=o(" (Wav2Vec2 model)"),ASo=l(),wT=a("li"),Mne=a("strong"),LSo=o("wavlm"),BSo=o(" \u2014 "),uj=a("a"),xSo=o("WavLMForCTC"),kSo=o(" (WavLM model)"),RSo=l(),AT=a("p"),SSo=o("The model is set in evaluation mode by default using "),Ene=a("code"),PSo=o("model.eval()"),$So=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yne=a("code"),ISo=o("model.train()"),jSo=l(),wne=a("p"),NSo=o("Examples:"),DSo=l(),m(lw.$$.fragment),t7e=l(),wd=a("h2"),LT=a("a"),Ane=a("span"),m(iw.$$.fragment),qSo=l(),Lne=a("span"),GSo=o("AutoModelForSpeechSeq2Seq"),a7e=l(),ar=a("div"),m(dw.$$.fragment),OSo=l(),Ad=a("p"),XSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Bne=a("code"),zSo=o("from_pretrained()"),VSo=o("class method or the "),xne=a("code"),WSo=o("from_config()"),QSo=o(`class
method.`),HSo=l(),cw=a("p"),USo=o("This class cannot be instantiated directly using "),kne=a("code"),JSo=o("__init__()"),YSo=o(" (throws an error)."),KSo=l(),Jr=a("div"),m(mw.$$.fragment),ZSo=l(),Rne=a("p"),ePo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),oPo=l(),Ld=a("p"),rPo=o(`Note:
Loading a model from its configuration file does `),Sne=a("strong"),tPo=o("not"),aPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pne=a("code"),sPo=o("from_pretrained()"),nPo=o("to load the model weights."),lPo=l(),$ne=a("p"),iPo=o("Examples:"),dPo=l(),m(fw.$$.fragment),cPo=l(),We=a("div"),m(gw.$$.fragment),mPo=l(),Ine=a("p"),fPo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),gPo=l(),Ya=a("p"),hPo=o("The model class to instantiate is selected based on the "),jne=a("code"),uPo=o("model_type"),pPo=o(` property of the config object (either
passed as an argument or loaded from `),Nne=a("code"),_Po=o("pretrained_model_name_or_path"),bPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dne=a("code"),vPo=o("pretrained_model_name_or_path"),TPo=o(":"),FPo=l(),hw=a("ul"),BT=a("li"),qne=a("strong"),CPo=o("speech-encoder-decoder"),MPo=o(" \u2014 "),pj=a("a"),EPo=o("SpeechEncoderDecoderModel"),yPo=o(" (Speech Encoder decoder model)"),wPo=l(),xT=a("li"),Gne=a("strong"),APo=o("speech_to_text"),LPo=o(" \u2014 "),_j=a("a"),BPo=o("Speech2TextForConditionalGeneration"),xPo=o(" (Speech2Text model)"),kPo=l(),kT=a("p"),RPo=o("The model is set in evaluation mode by default using "),One=a("code"),SPo=o("model.eval()"),PPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xne=a("code"),$Po=o("model.train()"),IPo=l(),zne=a("p"),jPo=o("Examples:"),NPo=l(),m(uw.$$.fragment),s7e=l(),Bd=a("h2"),RT=a("a"),Vne=a("span"),m(pw.$$.fragment),DPo=l(),Wne=a("span"),qPo=o("AutoModelForAudioXVector"),n7e=l(),sr=a("div"),m(_w.$$.fragment),GPo=l(),xd=a("p"),OPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),Qne=a("code"),XPo=o("from_pretrained()"),zPo=o("class method or the "),Hne=a("code"),VPo=o("from_config()"),WPo=o(`class
method.`),QPo=l(),bw=a("p"),HPo=o("This class cannot be instantiated directly using "),Une=a("code"),UPo=o("__init__()"),JPo=o(" (throws an error)."),YPo=l(),Yr=a("div"),m(vw.$$.fragment),KPo=l(),Jne=a("p"),ZPo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),e$o=l(),kd=a("p"),o$o=o(`Note:
Loading a model from its configuration file does `),Yne=a("strong"),r$o=o("not"),t$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kne=a("code"),a$o=o("from_pretrained()"),s$o=o("to load the model weights."),n$o=l(),Zne=a("p"),l$o=o("Examples:"),i$o=l(),m(Tw.$$.fragment),d$o=l(),Qe=a("div"),m(Fw.$$.fragment),c$o=l(),ele=a("p"),m$o=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),f$o=l(),Ka=a("p"),g$o=o("The model class to instantiate is selected based on the "),ole=a("code"),h$o=o("model_type"),u$o=o(` property of the config object (either
passed as an argument or loaded from `),rle=a("code"),p$o=o("pretrained_model_name_or_path"),_$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tle=a("code"),b$o=o("pretrained_model_name_or_path"),v$o=o(":"),T$o=l(),Rd=a("ul"),ST=a("li"),ale=a("strong"),F$o=o("unispeech-sat"),C$o=o(" \u2014 "),bj=a("a"),M$o=o("UniSpeechSatForXVector"),E$o=o(" (UniSpeechSat model)"),y$o=l(),PT=a("li"),sle=a("strong"),w$o=o("wav2vec2"),A$o=o(" \u2014 "),vj=a("a"),L$o=o("Wav2Vec2ForXVector"),B$o=o(" (Wav2Vec2 model)"),x$o=l(),$T=a("li"),nle=a("strong"),k$o=o("wavlm"),R$o=o(" \u2014 "),Tj=a("a"),S$o=o("WavLMForXVector"),P$o=o(" (WavLM model)"),$$o=l(),IT=a("p"),I$o=o("The model is set in evaluation mode by default using "),lle=a("code"),j$o=o("model.eval()"),N$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ile=a("code"),D$o=o("model.train()"),q$o=l(),dle=a("p"),G$o=o("Examples:"),O$o=l(),m(Cw.$$.fragment),l7e=l(),Sd=a("h2"),jT=a("a"),cle=a("span"),m(Mw.$$.fragment),X$o=l(),mle=a("span"),z$o=o("AutoModelForObjectDetection"),i7e=l(),nr=a("div"),m(Ew.$$.fragment),V$o=l(),Pd=a("p"),W$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),fle=a("code"),Q$o=o("from_pretrained()"),H$o=o("class method or the "),gle=a("code"),U$o=o("from_config()"),J$o=o(`class
method.`),Y$o=l(),yw=a("p"),K$o=o("This class cannot be instantiated directly using "),hle=a("code"),Z$o=o("__init__()"),eIo=o(" (throws an error)."),oIo=l(),Kr=a("div"),m(ww.$$.fragment),rIo=l(),ule=a("p"),tIo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),aIo=l(),$d=a("p"),sIo=o(`Note:
Loading a model from its configuration file does `),ple=a("strong"),nIo=o("not"),lIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_le=a("code"),iIo=o("from_pretrained()"),dIo=o("to load the model weights."),cIo=l(),ble=a("p"),mIo=o("Examples:"),fIo=l(),m(Aw.$$.fragment),gIo=l(),He=a("div"),m(Lw.$$.fragment),hIo=l(),vle=a("p"),uIo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),pIo=l(),Za=a("p"),_Io=o("The model class to instantiate is selected based on the "),Tle=a("code"),bIo=o("model_type"),vIo=o(` property of the config object (either
passed as an argument or loaded from `),Fle=a("code"),TIo=o("pretrained_model_name_or_path"),FIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cle=a("code"),CIo=o("pretrained_model_name_or_path"),MIo=o(":"),EIo=l(),Mle=a("ul"),NT=a("li"),Ele=a("strong"),yIo=o("detr"),wIo=o(" \u2014 "),Fj=a("a"),AIo=o("DetrForObjectDetection"),LIo=o(" (DETR model)"),BIo=l(),DT=a("p"),xIo=o("The model is set in evaluation mode by default using "),yle=a("code"),kIo=o("model.eval()"),RIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wle=a("code"),SIo=o("model.train()"),PIo=l(),Ale=a("p"),$Io=o("Examples:"),IIo=l(),m(Bw.$$.fragment),d7e=l(),Id=a("h2"),qT=a("a"),Lle=a("span"),m(xw.$$.fragment),jIo=l(),Ble=a("span"),NIo=o("AutoModelForImageSegmentation"),c7e=l(),lr=a("div"),m(kw.$$.fragment),DIo=l(),jd=a("p"),qIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),xle=a("code"),GIo=o("from_pretrained()"),OIo=o("class method or the "),kle=a("code"),XIo=o("from_config()"),zIo=o(`class
method.`),VIo=l(),Rw=a("p"),WIo=o("This class cannot be instantiated directly using "),Rle=a("code"),QIo=o("__init__()"),HIo=o(" (throws an error)."),UIo=l(),Zr=a("div"),m(Sw.$$.fragment),JIo=l(),Sle=a("p"),YIo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),KIo=l(),Nd=a("p"),ZIo=o(`Note:
Loading a model from its configuration file does `),Ple=a("strong"),ejo=o("not"),ojo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$le=a("code"),rjo=o("from_pretrained()"),tjo=o("to load the model weights."),ajo=l(),Ile=a("p"),sjo=o("Examples:"),njo=l(),m(Pw.$$.fragment),ljo=l(),Ue=a("div"),m($w.$$.fragment),ijo=l(),jle=a("p"),djo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),cjo=l(),es=a("p"),mjo=o("The model class to instantiate is selected based on the "),Nle=a("code"),fjo=o("model_type"),gjo=o(` property of the config object (either
passed as an argument or loaded from `),Dle=a("code"),hjo=o("pretrained_model_name_or_path"),ujo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qle=a("code"),pjo=o("pretrained_model_name_or_path"),_jo=o(":"),bjo=l(),Gle=a("ul"),GT=a("li"),Ole=a("strong"),vjo=o("detr"),Tjo=o(" \u2014 "),Cj=a("a"),Fjo=o("DetrForSegmentation"),Cjo=o(" (DETR model)"),Mjo=l(),OT=a("p"),Ejo=o("The model is set in evaluation mode by default using "),Xle=a("code"),yjo=o("model.eval()"),wjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zle=a("code"),Ajo=o("model.train()"),Ljo=l(),Vle=a("p"),Bjo=o("Examples:"),xjo=l(),m(Iw.$$.fragment),m7e=l(),Dd=a("h2"),XT=a("a"),Wle=a("span"),m(jw.$$.fragment),kjo=l(),Qle=a("span"),Rjo=o("AutoModelForSemanticSegmentation"),f7e=l(),ir=a("div"),m(Nw.$$.fragment),Sjo=l(),qd=a("p"),Pjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Hle=a("code"),$jo=o("from_pretrained()"),Ijo=o("class method or the "),Ule=a("code"),jjo=o("from_config()"),Njo=o(`class
method.`),Djo=l(),Dw=a("p"),qjo=o("This class cannot be instantiated directly using "),Jle=a("code"),Gjo=o("__init__()"),Ojo=o(" (throws an error)."),Xjo=l(),et=a("div"),m(qw.$$.fragment),zjo=l(),Yle=a("p"),Vjo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Wjo=l(),Gd=a("p"),Qjo=o(`Note:
Loading a model from its configuration file does `),Kle=a("strong"),Hjo=o("not"),Ujo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zle=a("code"),Jjo=o("from_pretrained()"),Yjo=o("to load the model weights."),Kjo=l(),eie=a("p"),Zjo=o("Examples:"),eNo=l(),m(Gw.$$.fragment),oNo=l(),Je=a("div"),m(Ow.$$.fragment),rNo=l(),oie=a("p"),tNo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),aNo=l(),os=a("p"),sNo=o("The model class to instantiate is selected based on the "),rie=a("code"),nNo=o("model_type"),lNo=o(` property of the config object (either
passed as an argument or loaded from `),tie=a("code"),iNo=o("pretrained_model_name_or_path"),dNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aie=a("code"),cNo=o("pretrained_model_name_or_path"),mNo=o(":"),fNo=l(),Xw=a("ul"),zT=a("li"),sie=a("strong"),gNo=o("beit"),hNo=o(" \u2014 "),Mj=a("a"),uNo=o("BeitForSemanticSegmentation"),pNo=o(" (BEiT model)"),_No=l(),VT=a("li"),nie=a("strong"),bNo=o("segformer"),vNo=o(" \u2014 "),Ej=a("a"),TNo=o("SegformerForSemanticSegmentation"),FNo=o(" (SegFormer model)"),CNo=l(),WT=a("p"),MNo=o("The model is set in evaluation mode by default using "),lie=a("code"),ENo=o("model.eval()"),yNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iie=a("code"),wNo=o("model.train()"),ANo=l(),die=a("p"),LNo=o("Examples:"),BNo=l(),m(zw.$$.fragment),g7e=l(),Od=a("h2"),QT=a("a"),cie=a("span"),m(Vw.$$.fragment),xNo=l(),mie=a("span"),kNo=o("TFAutoModel"),h7e=l(),dr=a("div"),m(Ww.$$.fragment),RNo=l(),Xd=a("p"),SNo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),fie=a("code"),PNo=o("from_pretrained()"),$No=o("class method or the "),gie=a("code"),INo=o("from_config()"),jNo=o(`class
method.`),NNo=l(),Qw=a("p"),DNo=o("This class cannot be instantiated directly using "),hie=a("code"),qNo=o("__init__()"),GNo=o(" (throws an error)."),ONo=l(),ot=a("div"),m(Hw.$$.fragment),XNo=l(),uie=a("p"),zNo=o("Instantiates one of the base model classes of the library from a configuration."),VNo=l(),zd=a("p"),WNo=o(`Note:
Loading a model from its configuration file does `),pie=a("strong"),QNo=o("not"),HNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ie=a("code"),UNo=o("from_pretrained()"),JNo=o("to load the model weights."),YNo=l(),bie=a("p"),KNo=o("Examples:"),ZNo=l(),m(Uw.$$.fragment),eDo=l(),mo=a("div"),m(Jw.$$.fragment),oDo=l(),vie=a("p"),rDo=o("Instantiate one of the base model classes of the library from a pretrained model."),tDo=l(),rs=a("p"),aDo=o("The model class to instantiate is selected based on the "),Tie=a("code"),sDo=o("model_type"),nDo=o(` property of the config object (either
passed as an argument or loaded from `),Fie=a("code"),lDo=o("pretrained_model_name_or_path"),iDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cie=a("code"),dDo=o("pretrained_model_name_or_path"),cDo=o(":"),mDo=l(),B=a("ul"),HT=a("li"),Mie=a("strong"),fDo=o("albert"),gDo=o(" \u2014 "),yj=a("a"),hDo=o("TFAlbertModel"),uDo=o(" (ALBERT model)"),pDo=l(),UT=a("li"),Eie=a("strong"),_Do=o("bart"),bDo=o(" \u2014 "),wj=a("a"),vDo=o("TFBartModel"),TDo=o(" (BART model)"),FDo=l(),JT=a("li"),yie=a("strong"),CDo=o("bert"),MDo=o(" \u2014 "),Aj=a("a"),EDo=o("TFBertModel"),yDo=o(" (BERT model)"),wDo=l(),YT=a("li"),wie=a("strong"),ADo=o("blenderbot"),LDo=o(" \u2014 "),Lj=a("a"),BDo=o("TFBlenderbotModel"),xDo=o(" (Blenderbot model)"),kDo=l(),KT=a("li"),Aie=a("strong"),RDo=o("blenderbot-small"),SDo=o(" \u2014 "),Bj=a("a"),PDo=o("TFBlenderbotSmallModel"),$Do=o(" (BlenderbotSmall model)"),IDo=l(),ZT=a("li"),Lie=a("strong"),jDo=o("camembert"),NDo=o(" \u2014 "),xj=a("a"),DDo=o("TFCamembertModel"),qDo=o(" (CamemBERT model)"),GDo=l(),e1=a("li"),Bie=a("strong"),ODo=o("clip"),XDo=o(" \u2014 "),kj=a("a"),zDo=o("TFCLIPModel"),VDo=o(" (CLIP model)"),WDo=l(),o1=a("li"),xie=a("strong"),QDo=o("convbert"),HDo=o(" \u2014 "),Rj=a("a"),UDo=o("TFConvBertModel"),JDo=o(" (ConvBERT model)"),YDo=l(),r1=a("li"),kie=a("strong"),KDo=o("ctrl"),ZDo=o(" \u2014 "),Sj=a("a"),eqo=o("TFCTRLModel"),oqo=o(" (CTRL model)"),rqo=l(),t1=a("li"),Rie=a("strong"),tqo=o("deberta"),aqo=o(" \u2014 "),Pj=a("a"),sqo=o("TFDebertaModel"),nqo=o(" (DeBERTa model)"),lqo=l(),a1=a("li"),Sie=a("strong"),iqo=o("deberta-v2"),dqo=o(" \u2014 "),$j=a("a"),cqo=o("TFDebertaV2Model"),mqo=o(" (DeBERTa-v2 model)"),fqo=l(),s1=a("li"),Pie=a("strong"),gqo=o("distilbert"),hqo=o(" \u2014 "),Ij=a("a"),uqo=o("TFDistilBertModel"),pqo=o(" (DistilBERT model)"),_qo=l(),n1=a("li"),$ie=a("strong"),bqo=o("dpr"),vqo=o(" \u2014 "),jj=a("a"),Tqo=o("TFDPRQuestionEncoder"),Fqo=o(" (DPR model)"),Cqo=l(),l1=a("li"),Iie=a("strong"),Mqo=o("electra"),Eqo=o(" \u2014 "),Nj=a("a"),yqo=o("TFElectraModel"),wqo=o(" (ELECTRA model)"),Aqo=l(),i1=a("li"),jie=a("strong"),Lqo=o("flaubert"),Bqo=o(" \u2014 "),Dj=a("a"),xqo=o("TFFlaubertModel"),kqo=o(" (FlauBERT model)"),Rqo=l(),Ln=a("li"),Nie=a("strong"),Sqo=o("funnel"),Pqo=o(" \u2014 "),qj=a("a"),$qo=o("TFFunnelModel"),Iqo=o(" or "),Gj=a("a"),jqo=o("TFFunnelBaseModel"),Nqo=o(" (Funnel Transformer model)"),Dqo=l(),d1=a("li"),Die=a("strong"),qqo=o("gpt2"),Gqo=o(" \u2014 "),Oj=a("a"),Oqo=o("TFGPT2Model"),Xqo=o(" (OpenAI GPT-2 model)"),zqo=l(),c1=a("li"),qie=a("strong"),Vqo=o("hubert"),Wqo=o(" \u2014 "),Xj=a("a"),Qqo=o("TFHubertModel"),Hqo=o(" (Hubert model)"),Uqo=l(),m1=a("li"),Gie=a("strong"),Jqo=o("layoutlm"),Yqo=o(" \u2014 "),zj=a("a"),Kqo=o("TFLayoutLMModel"),Zqo=o(" (LayoutLM model)"),eGo=l(),f1=a("li"),Oie=a("strong"),oGo=o("led"),rGo=o(" \u2014 "),Vj=a("a"),tGo=o("TFLEDModel"),aGo=o(" (LED model)"),sGo=l(),g1=a("li"),Xie=a("strong"),nGo=o("longformer"),lGo=o(" \u2014 "),Wj=a("a"),iGo=o("TFLongformerModel"),dGo=o(" (Longformer model)"),cGo=l(),h1=a("li"),zie=a("strong"),mGo=o("lxmert"),fGo=o(" \u2014 "),Qj=a("a"),gGo=o("TFLxmertModel"),hGo=o(" (LXMERT model)"),uGo=l(),u1=a("li"),Vie=a("strong"),pGo=o("marian"),_Go=o(" \u2014 "),Hj=a("a"),bGo=o("TFMarianModel"),vGo=o(" (Marian model)"),TGo=l(),p1=a("li"),Wie=a("strong"),FGo=o("mbart"),CGo=o(" \u2014 "),Uj=a("a"),MGo=o("TFMBartModel"),EGo=o(" (mBART model)"),yGo=l(),_1=a("li"),Qie=a("strong"),wGo=o("mobilebert"),AGo=o(" \u2014 "),Jj=a("a"),LGo=o("TFMobileBertModel"),BGo=o(" (MobileBERT model)"),xGo=l(),b1=a("li"),Hie=a("strong"),kGo=o("mpnet"),RGo=o(" \u2014 "),Yj=a("a"),SGo=o("TFMPNetModel"),PGo=o(" (MPNet model)"),$Go=l(),v1=a("li"),Uie=a("strong"),IGo=o("mt5"),jGo=o(" \u2014 "),Kj=a("a"),NGo=o("TFMT5Model"),DGo=o(" (mT5 model)"),qGo=l(),T1=a("li"),Jie=a("strong"),GGo=o("openai-gpt"),OGo=o(" \u2014 "),Zj=a("a"),XGo=o("TFOpenAIGPTModel"),zGo=o(" (OpenAI GPT model)"),VGo=l(),F1=a("li"),Yie=a("strong"),WGo=o("pegasus"),QGo=o(" \u2014 "),eN=a("a"),HGo=o("TFPegasusModel"),UGo=o(" (Pegasus model)"),JGo=l(),C1=a("li"),Kie=a("strong"),YGo=o("rembert"),KGo=o(" \u2014 "),oN=a("a"),ZGo=o("TFRemBertModel"),eOo=o(" (RemBERT model)"),oOo=l(),M1=a("li"),Zie=a("strong"),rOo=o("roberta"),tOo=o(" \u2014 "),rN=a("a"),aOo=o("TFRobertaModel"),sOo=o(" (RoBERTa model)"),nOo=l(),E1=a("li"),ede=a("strong"),lOo=o("roformer"),iOo=o(" \u2014 "),tN=a("a"),dOo=o("TFRoFormerModel"),cOo=o(" (RoFormer model)"),mOo=l(),y1=a("li"),ode=a("strong"),fOo=o("speech_to_text"),gOo=o(" \u2014 "),aN=a("a"),hOo=o("TFSpeech2TextModel"),uOo=o(" (Speech2Text model)"),pOo=l(),w1=a("li"),rde=a("strong"),_Oo=o("t5"),bOo=o(" \u2014 "),sN=a("a"),vOo=o("TFT5Model"),TOo=o(" (T5 model)"),FOo=l(),A1=a("li"),tde=a("strong"),COo=o("tapas"),MOo=o(" \u2014 "),nN=a("a"),EOo=o("TFTapasModel"),yOo=o(" (TAPAS model)"),wOo=l(),L1=a("li"),ade=a("strong"),AOo=o("transfo-xl"),LOo=o(" \u2014 "),lN=a("a"),BOo=o("TFTransfoXLModel"),xOo=o(" (Transformer-XL model)"),kOo=l(),B1=a("li"),sde=a("strong"),ROo=o("vit"),SOo=o(" \u2014 "),iN=a("a"),POo=o("TFViTModel"),$Oo=o(" (ViT model)"),IOo=l(),x1=a("li"),nde=a("strong"),jOo=o("wav2vec2"),NOo=o(" \u2014 "),dN=a("a"),DOo=o("TFWav2Vec2Model"),qOo=o(" (Wav2Vec2 model)"),GOo=l(),k1=a("li"),lde=a("strong"),OOo=o("xlm"),XOo=o(" \u2014 "),cN=a("a"),zOo=o("TFXLMModel"),VOo=o(" (XLM model)"),WOo=l(),R1=a("li"),ide=a("strong"),QOo=o("xlm-roberta"),HOo=o(" \u2014 "),mN=a("a"),UOo=o("TFXLMRobertaModel"),JOo=o(" (XLM-RoBERTa model)"),YOo=l(),S1=a("li"),dde=a("strong"),KOo=o("xlnet"),ZOo=o(" \u2014 "),fN=a("a"),eXo=o("TFXLNetModel"),oXo=o(" (XLNet model)"),rXo=l(),cde=a("p"),tXo=o("Examples:"),aXo=l(),m(Yw.$$.fragment),u7e=l(),Vd=a("h2"),P1=a("a"),mde=a("span"),m(Kw.$$.fragment),sXo=l(),fde=a("span"),nXo=o("TFAutoModelForPreTraining"),p7e=l(),cr=a("div"),m(Zw.$$.fragment),lXo=l(),Wd=a("p"),iXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),gde=a("code"),dXo=o("from_pretrained()"),cXo=o("class method or the "),hde=a("code"),mXo=o("from_config()"),fXo=o(`class
method.`),gXo=l(),eA=a("p"),hXo=o("This class cannot be instantiated directly using "),ude=a("code"),uXo=o("__init__()"),pXo=o(" (throws an error)."),_Xo=l(),rt=a("div"),m(oA.$$.fragment),bXo=l(),pde=a("p"),vXo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),TXo=l(),Qd=a("p"),FXo=o(`Note:
Loading a model from its configuration file does `),_de=a("strong"),CXo=o("not"),MXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bde=a("code"),EXo=o("from_pretrained()"),yXo=o("to load the model weights."),wXo=l(),vde=a("p"),AXo=o("Examples:"),LXo=l(),m(rA.$$.fragment),BXo=l(),fo=a("div"),m(tA.$$.fragment),xXo=l(),Tde=a("p"),kXo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),RXo=l(),ts=a("p"),SXo=o("The model class to instantiate is selected based on the "),Fde=a("code"),PXo=o("model_type"),$Xo=o(` property of the config object (either
passed as an argument or loaded from `),Cde=a("code"),IXo=o("pretrained_model_name_or_path"),jXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mde=a("code"),NXo=o("pretrained_model_name_or_path"),DXo=o(":"),qXo=l(),H=a("ul"),$1=a("li"),Ede=a("strong"),GXo=o("albert"),OXo=o(" \u2014 "),gN=a("a"),XXo=o("TFAlbertForPreTraining"),zXo=o(" (ALBERT model)"),VXo=l(),I1=a("li"),yde=a("strong"),WXo=o("bart"),QXo=o(" \u2014 "),hN=a("a"),HXo=o("TFBartForConditionalGeneration"),UXo=o(" (BART model)"),JXo=l(),j1=a("li"),wde=a("strong"),YXo=o("bert"),KXo=o(" \u2014 "),uN=a("a"),ZXo=o("TFBertForPreTraining"),ezo=o(" (BERT model)"),ozo=l(),N1=a("li"),Ade=a("strong"),rzo=o("camembert"),tzo=o(" \u2014 "),pN=a("a"),azo=o("TFCamembertForMaskedLM"),szo=o(" (CamemBERT model)"),nzo=l(),D1=a("li"),Lde=a("strong"),lzo=o("ctrl"),izo=o(" \u2014 "),_N=a("a"),dzo=o("TFCTRLLMHeadModel"),czo=o(" (CTRL model)"),mzo=l(),q1=a("li"),Bde=a("strong"),fzo=o("distilbert"),gzo=o(" \u2014 "),bN=a("a"),hzo=o("TFDistilBertForMaskedLM"),uzo=o(" (DistilBERT model)"),pzo=l(),G1=a("li"),xde=a("strong"),_zo=o("electra"),bzo=o(" \u2014 "),vN=a("a"),vzo=o("TFElectraForPreTraining"),Tzo=o(" (ELECTRA model)"),Fzo=l(),O1=a("li"),kde=a("strong"),Czo=o("flaubert"),Mzo=o(" \u2014 "),TN=a("a"),Ezo=o("TFFlaubertWithLMHeadModel"),yzo=o(" (FlauBERT model)"),wzo=l(),X1=a("li"),Rde=a("strong"),Azo=o("funnel"),Lzo=o(" \u2014 "),FN=a("a"),Bzo=o("TFFunnelForPreTraining"),xzo=o(" (Funnel Transformer model)"),kzo=l(),z1=a("li"),Sde=a("strong"),Rzo=o("gpt2"),Szo=o(" \u2014 "),CN=a("a"),Pzo=o("TFGPT2LMHeadModel"),$zo=o(" (OpenAI GPT-2 model)"),Izo=l(),V1=a("li"),Pde=a("strong"),jzo=o("layoutlm"),Nzo=o(" \u2014 "),MN=a("a"),Dzo=o("TFLayoutLMForMaskedLM"),qzo=o(" (LayoutLM model)"),Gzo=l(),W1=a("li"),$de=a("strong"),Ozo=o("lxmert"),Xzo=o(" \u2014 "),EN=a("a"),zzo=o("TFLxmertForPreTraining"),Vzo=o(" (LXMERT model)"),Wzo=l(),Q1=a("li"),Ide=a("strong"),Qzo=o("mobilebert"),Hzo=o(" \u2014 "),yN=a("a"),Uzo=o("TFMobileBertForPreTraining"),Jzo=o(" (MobileBERT model)"),Yzo=l(),H1=a("li"),jde=a("strong"),Kzo=o("mpnet"),Zzo=o(" \u2014 "),wN=a("a"),eVo=o("TFMPNetForMaskedLM"),oVo=o(" (MPNet model)"),rVo=l(),U1=a("li"),Nde=a("strong"),tVo=o("openai-gpt"),aVo=o(" \u2014 "),AN=a("a"),sVo=o("TFOpenAIGPTLMHeadModel"),nVo=o(" (OpenAI GPT model)"),lVo=l(),J1=a("li"),Dde=a("strong"),iVo=o("roberta"),dVo=o(" \u2014 "),LN=a("a"),cVo=o("TFRobertaForMaskedLM"),mVo=o(" (RoBERTa model)"),fVo=l(),Y1=a("li"),qde=a("strong"),gVo=o("t5"),hVo=o(" \u2014 "),BN=a("a"),uVo=o("TFT5ForConditionalGeneration"),pVo=o(" (T5 model)"),_Vo=l(),K1=a("li"),Gde=a("strong"),bVo=o("tapas"),vVo=o(" \u2014 "),xN=a("a"),TVo=o("TFTapasForMaskedLM"),FVo=o(" (TAPAS model)"),CVo=l(),Z1=a("li"),Ode=a("strong"),MVo=o("transfo-xl"),EVo=o(" \u2014 "),kN=a("a"),yVo=o("TFTransfoXLLMHeadModel"),wVo=o(" (Transformer-XL model)"),AVo=l(),eF=a("li"),Xde=a("strong"),LVo=o("xlm"),BVo=o(" \u2014 "),RN=a("a"),xVo=o("TFXLMWithLMHeadModel"),kVo=o(" (XLM model)"),RVo=l(),oF=a("li"),zde=a("strong"),SVo=o("xlm-roberta"),PVo=o(" \u2014 "),SN=a("a"),$Vo=o("TFXLMRobertaForMaskedLM"),IVo=o(" (XLM-RoBERTa model)"),jVo=l(),rF=a("li"),Vde=a("strong"),NVo=o("xlnet"),DVo=o(" \u2014 "),PN=a("a"),qVo=o("TFXLNetLMHeadModel"),GVo=o(" (XLNet model)"),OVo=l(),Wde=a("p"),XVo=o("Examples:"),zVo=l(),m(aA.$$.fragment),_7e=l(),Hd=a("h2"),tF=a("a"),Qde=a("span"),m(sA.$$.fragment),VVo=l(),Hde=a("span"),WVo=o("TFAutoModelForCausalLM"),b7e=l(),mr=a("div"),m(nA.$$.fragment),QVo=l(),Ud=a("p"),HVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Ude=a("code"),UVo=o("from_pretrained()"),JVo=o("class method or the "),Jde=a("code"),YVo=o("from_config()"),KVo=o(`class
method.`),ZVo=l(),lA=a("p"),eWo=o("This class cannot be instantiated directly using "),Yde=a("code"),oWo=o("__init__()"),rWo=o(" (throws an error)."),tWo=l(),tt=a("div"),m(iA.$$.fragment),aWo=l(),Kde=a("p"),sWo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),nWo=l(),Jd=a("p"),lWo=o(`Note:
Loading a model from its configuration file does `),Zde=a("strong"),iWo=o("not"),dWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ece=a("code"),cWo=o("from_pretrained()"),mWo=o("to load the model weights."),fWo=l(),oce=a("p"),gWo=o("Examples:"),hWo=l(),m(dA.$$.fragment),uWo=l(),go=a("div"),m(cA.$$.fragment),pWo=l(),rce=a("p"),_Wo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),bWo=l(),as=a("p"),vWo=o("The model class to instantiate is selected based on the "),tce=a("code"),TWo=o("model_type"),FWo=o(` property of the config object (either
passed as an argument or loaded from `),ace=a("code"),CWo=o("pretrained_model_name_or_path"),MWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sce=a("code"),EWo=o("pretrained_model_name_or_path"),yWo=o(":"),wWo=l(),he=a("ul"),aF=a("li"),nce=a("strong"),AWo=o("bert"),LWo=o(" \u2014 "),$N=a("a"),BWo=o("TFBertLMHeadModel"),xWo=o(" (BERT model)"),kWo=l(),sF=a("li"),lce=a("strong"),RWo=o("ctrl"),SWo=o(" \u2014 "),IN=a("a"),PWo=o("TFCTRLLMHeadModel"),$Wo=o(" (CTRL model)"),IWo=l(),nF=a("li"),ice=a("strong"),jWo=o("gpt2"),NWo=o(" \u2014 "),jN=a("a"),DWo=o("TFGPT2LMHeadModel"),qWo=o(" (OpenAI GPT-2 model)"),GWo=l(),lF=a("li"),dce=a("strong"),OWo=o("openai-gpt"),XWo=o(" \u2014 "),NN=a("a"),zWo=o("TFOpenAIGPTLMHeadModel"),VWo=o(" (OpenAI GPT model)"),WWo=l(),iF=a("li"),cce=a("strong"),QWo=o("rembert"),HWo=o(" \u2014 "),DN=a("a"),UWo=o("TFRemBertForCausalLM"),JWo=o(" (RemBERT model)"),YWo=l(),dF=a("li"),mce=a("strong"),KWo=o("roberta"),ZWo=o(" \u2014 "),qN=a("a"),eQo=o("TFRobertaForCausalLM"),oQo=o(" (RoBERTa model)"),rQo=l(),cF=a("li"),fce=a("strong"),tQo=o("roformer"),aQo=o(" \u2014 "),GN=a("a"),sQo=o("TFRoFormerForCausalLM"),nQo=o(" (RoFormer model)"),lQo=l(),mF=a("li"),gce=a("strong"),iQo=o("transfo-xl"),dQo=o(" \u2014 "),ON=a("a"),cQo=o("TFTransfoXLLMHeadModel"),mQo=o(" (Transformer-XL model)"),fQo=l(),fF=a("li"),hce=a("strong"),gQo=o("xlm"),hQo=o(" \u2014 "),XN=a("a"),uQo=o("TFXLMWithLMHeadModel"),pQo=o(" (XLM model)"),_Qo=l(),gF=a("li"),uce=a("strong"),bQo=o("xlnet"),vQo=o(" \u2014 "),zN=a("a"),TQo=o("TFXLNetLMHeadModel"),FQo=o(" (XLNet model)"),CQo=l(),pce=a("p"),MQo=o("Examples:"),EQo=l(),m(mA.$$.fragment),v7e=l(),Yd=a("h2"),hF=a("a"),_ce=a("span"),m(fA.$$.fragment),yQo=l(),bce=a("span"),wQo=o("TFAutoModelForImageClassification"),T7e=l(),fr=a("div"),m(gA.$$.fragment),AQo=l(),Kd=a("p"),LQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),vce=a("code"),BQo=o("from_pretrained()"),xQo=o("class method or the "),Tce=a("code"),kQo=o("from_config()"),RQo=o(`class
method.`),SQo=l(),hA=a("p"),PQo=o("This class cannot be instantiated directly using "),Fce=a("code"),$Qo=o("__init__()"),IQo=o(" (throws an error)."),jQo=l(),at=a("div"),m(uA.$$.fragment),NQo=l(),Cce=a("p"),DQo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qQo=l(),Zd=a("p"),GQo=o(`Note:
Loading a model from its configuration file does `),Mce=a("strong"),OQo=o("not"),XQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ece=a("code"),zQo=o("from_pretrained()"),VQo=o("to load the model weights."),WQo=l(),yce=a("p"),QQo=o("Examples:"),HQo=l(),m(pA.$$.fragment),UQo=l(),ho=a("div"),m(_A.$$.fragment),JQo=l(),wce=a("p"),YQo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),KQo=l(),ss=a("p"),ZQo=o("The model class to instantiate is selected based on the "),Ace=a("code"),eHo=o("model_type"),oHo=o(` property of the config object (either
passed as an argument or loaded from `),Lce=a("code"),rHo=o("pretrained_model_name_or_path"),tHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bce=a("code"),aHo=o("pretrained_model_name_or_path"),sHo=o(":"),nHo=l(),xce=a("ul"),uF=a("li"),kce=a("strong"),lHo=o("vit"),iHo=o(" \u2014 "),VN=a("a"),dHo=o("TFViTForImageClassification"),cHo=o(" (ViT model)"),mHo=l(),Rce=a("p"),fHo=o("Examples:"),gHo=l(),m(bA.$$.fragment),F7e=l(),ec=a("h2"),pF=a("a"),Sce=a("span"),m(vA.$$.fragment),hHo=l(),Pce=a("span"),uHo=o("TFAutoModelForMaskedLM"),C7e=l(),gr=a("div"),m(TA.$$.fragment),pHo=l(),oc=a("p"),_Ho=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),$ce=a("code"),bHo=o("from_pretrained()"),vHo=o("class method or the "),Ice=a("code"),THo=o("from_config()"),FHo=o(`class
method.`),CHo=l(),FA=a("p"),MHo=o("This class cannot be instantiated directly using "),jce=a("code"),EHo=o("__init__()"),yHo=o(" (throws an error)."),wHo=l(),st=a("div"),m(CA.$$.fragment),AHo=l(),Nce=a("p"),LHo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),BHo=l(),rc=a("p"),xHo=o(`Note:
Loading a model from its configuration file does `),Dce=a("strong"),kHo=o("not"),RHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qce=a("code"),SHo=o("from_pretrained()"),PHo=o("to load the model weights."),$Ho=l(),Gce=a("p"),IHo=o("Examples:"),jHo=l(),m(MA.$$.fragment),NHo=l(),uo=a("div"),m(EA.$$.fragment),DHo=l(),Oce=a("p"),qHo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),GHo=l(),ns=a("p"),OHo=o("The model class to instantiate is selected based on the "),Xce=a("code"),XHo=o("model_type"),zHo=o(` property of the config object (either
passed as an argument or loaded from `),zce=a("code"),VHo=o("pretrained_model_name_or_path"),WHo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vce=a("code"),QHo=o("pretrained_model_name_or_path"),HHo=o(":"),UHo=l(),Y=a("ul"),_F=a("li"),Wce=a("strong"),JHo=o("albert"),YHo=o(" \u2014 "),WN=a("a"),KHo=o("TFAlbertForMaskedLM"),ZHo=o(" (ALBERT model)"),eUo=l(),bF=a("li"),Qce=a("strong"),oUo=o("bert"),rUo=o(" \u2014 "),QN=a("a"),tUo=o("TFBertForMaskedLM"),aUo=o(" (BERT model)"),sUo=l(),vF=a("li"),Hce=a("strong"),nUo=o("camembert"),lUo=o(" \u2014 "),HN=a("a"),iUo=o("TFCamembertForMaskedLM"),dUo=o(" (CamemBERT model)"),cUo=l(),TF=a("li"),Uce=a("strong"),mUo=o("convbert"),fUo=o(" \u2014 "),UN=a("a"),gUo=o("TFConvBertForMaskedLM"),hUo=o(" (ConvBERT model)"),uUo=l(),FF=a("li"),Jce=a("strong"),pUo=o("deberta"),_Uo=o(" \u2014 "),JN=a("a"),bUo=o("TFDebertaForMaskedLM"),vUo=o(" (DeBERTa model)"),TUo=l(),CF=a("li"),Yce=a("strong"),FUo=o("deberta-v2"),CUo=o(" \u2014 "),YN=a("a"),MUo=o("TFDebertaV2ForMaskedLM"),EUo=o(" (DeBERTa-v2 model)"),yUo=l(),MF=a("li"),Kce=a("strong"),wUo=o("distilbert"),AUo=o(" \u2014 "),KN=a("a"),LUo=o("TFDistilBertForMaskedLM"),BUo=o(" (DistilBERT model)"),xUo=l(),EF=a("li"),Zce=a("strong"),kUo=o("electra"),RUo=o(" \u2014 "),ZN=a("a"),SUo=o("TFElectraForMaskedLM"),PUo=o(" (ELECTRA model)"),$Uo=l(),yF=a("li"),eme=a("strong"),IUo=o("flaubert"),jUo=o(" \u2014 "),eD=a("a"),NUo=o("TFFlaubertWithLMHeadModel"),DUo=o(" (FlauBERT model)"),qUo=l(),wF=a("li"),ome=a("strong"),GUo=o("funnel"),OUo=o(" \u2014 "),oD=a("a"),XUo=o("TFFunnelForMaskedLM"),zUo=o(" (Funnel Transformer model)"),VUo=l(),AF=a("li"),rme=a("strong"),WUo=o("layoutlm"),QUo=o(" \u2014 "),rD=a("a"),HUo=o("TFLayoutLMForMaskedLM"),UUo=o(" (LayoutLM model)"),JUo=l(),LF=a("li"),tme=a("strong"),YUo=o("longformer"),KUo=o(" \u2014 "),tD=a("a"),ZUo=o("TFLongformerForMaskedLM"),eJo=o(" (Longformer model)"),oJo=l(),BF=a("li"),ame=a("strong"),rJo=o("mobilebert"),tJo=o(" \u2014 "),aD=a("a"),aJo=o("TFMobileBertForMaskedLM"),sJo=o(" (MobileBERT model)"),nJo=l(),xF=a("li"),sme=a("strong"),lJo=o("mpnet"),iJo=o(" \u2014 "),sD=a("a"),dJo=o("TFMPNetForMaskedLM"),cJo=o(" (MPNet model)"),mJo=l(),kF=a("li"),nme=a("strong"),fJo=o("rembert"),gJo=o(" \u2014 "),nD=a("a"),hJo=o("TFRemBertForMaskedLM"),uJo=o(" (RemBERT model)"),pJo=l(),RF=a("li"),lme=a("strong"),_Jo=o("roberta"),bJo=o(" \u2014 "),lD=a("a"),vJo=o("TFRobertaForMaskedLM"),TJo=o(" (RoBERTa model)"),FJo=l(),SF=a("li"),ime=a("strong"),CJo=o("roformer"),MJo=o(" \u2014 "),iD=a("a"),EJo=o("TFRoFormerForMaskedLM"),yJo=o(" (RoFormer model)"),wJo=l(),PF=a("li"),dme=a("strong"),AJo=o("tapas"),LJo=o(" \u2014 "),dD=a("a"),BJo=o("TFTapasForMaskedLM"),xJo=o(" (TAPAS model)"),kJo=l(),$F=a("li"),cme=a("strong"),RJo=o("xlm"),SJo=o(" \u2014 "),cD=a("a"),PJo=o("TFXLMWithLMHeadModel"),$Jo=o(" (XLM model)"),IJo=l(),IF=a("li"),mme=a("strong"),jJo=o("xlm-roberta"),NJo=o(" \u2014 "),mD=a("a"),DJo=o("TFXLMRobertaForMaskedLM"),qJo=o(" (XLM-RoBERTa model)"),GJo=l(),fme=a("p"),OJo=o("Examples:"),XJo=l(),m(yA.$$.fragment),M7e=l(),tc=a("h2"),jF=a("a"),gme=a("span"),m(wA.$$.fragment),zJo=l(),hme=a("span"),VJo=o("TFAutoModelForSeq2SeqLM"),E7e=l(),hr=a("div"),m(AA.$$.fragment),WJo=l(),ac=a("p"),QJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ume=a("code"),HJo=o("from_pretrained()"),UJo=o("class method or the "),pme=a("code"),JJo=o("from_config()"),YJo=o(`class
method.`),KJo=l(),LA=a("p"),ZJo=o("This class cannot be instantiated directly using "),_me=a("code"),eYo=o("__init__()"),oYo=o(" (throws an error)."),rYo=l(),nt=a("div"),m(BA.$$.fragment),tYo=l(),bme=a("p"),aYo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),sYo=l(),sc=a("p"),nYo=o(`Note:
Loading a model from its configuration file does `),vme=a("strong"),lYo=o("not"),iYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Tme=a("code"),dYo=o("from_pretrained()"),cYo=o("to load the model weights."),mYo=l(),Fme=a("p"),fYo=o("Examples:"),gYo=l(),m(xA.$$.fragment),hYo=l(),po=a("div"),m(kA.$$.fragment),uYo=l(),Cme=a("p"),pYo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),_Yo=l(),ls=a("p"),bYo=o("The model class to instantiate is selected based on the "),Mme=a("code"),vYo=o("model_type"),TYo=o(` property of the config object (either
passed as an argument or loaded from `),Eme=a("code"),FYo=o("pretrained_model_name_or_path"),CYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yme=a("code"),MYo=o("pretrained_model_name_or_path"),EYo=o(":"),yYo=l(),ue=a("ul"),NF=a("li"),wme=a("strong"),wYo=o("bart"),AYo=o(" \u2014 "),fD=a("a"),LYo=o("TFBartForConditionalGeneration"),BYo=o(" (BART model)"),xYo=l(),DF=a("li"),Ame=a("strong"),kYo=o("blenderbot"),RYo=o(" \u2014 "),gD=a("a"),SYo=o("TFBlenderbotForConditionalGeneration"),PYo=o(" (Blenderbot model)"),$Yo=l(),qF=a("li"),Lme=a("strong"),IYo=o("blenderbot-small"),jYo=o(" \u2014 "),hD=a("a"),NYo=o("TFBlenderbotSmallForConditionalGeneration"),DYo=o(" (BlenderbotSmall model)"),qYo=l(),GF=a("li"),Bme=a("strong"),GYo=o("encoder-decoder"),OYo=o(" \u2014 "),uD=a("a"),XYo=o("TFEncoderDecoderModel"),zYo=o(" (Encoder decoder model)"),VYo=l(),OF=a("li"),xme=a("strong"),WYo=o("led"),QYo=o(" \u2014 "),pD=a("a"),HYo=o("TFLEDForConditionalGeneration"),UYo=o(" (LED model)"),JYo=l(),XF=a("li"),kme=a("strong"),YYo=o("marian"),KYo=o(" \u2014 "),_D=a("a"),ZYo=o("TFMarianMTModel"),eKo=o(" (Marian model)"),oKo=l(),zF=a("li"),Rme=a("strong"),rKo=o("mbart"),tKo=o(" \u2014 "),bD=a("a"),aKo=o("TFMBartForConditionalGeneration"),sKo=o(" (mBART model)"),nKo=l(),VF=a("li"),Sme=a("strong"),lKo=o("mt5"),iKo=o(" \u2014 "),vD=a("a"),dKo=o("TFMT5ForConditionalGeneration"),cKo=o(" (mT5 model)"),mKo=l(),WF=a("li"),Pme=a("strong"),fKo=o("pegasus"),gKo=o(" \u2014 "),TD=a("a"),hKo=o("TFPegasusForConditionalGeneration"),uKo=o(" (Pegasus model)"),pKo=l(),QF=a("li"),$me=a("strong"),_Ko=o("t5"),bKo=o(" \u2014 "),FD=a("a"),vKo=o("TFT5ForConditionalGeneration"),TKo=o(" (T5 model)"),FKo=l(),Ime=a("p"),CKo=o("Examples:"),MKo=l(),m(RA.$$.fragment),y7e=l(),nc=a("h2"),HF=a("a"),jme=a("span"),m(SA.$$.fragment),EKo=l(),Nme=a("span"),yKo=o("TFAutoModelForSequenceClassification"),w7e=l(),ur=a("div"),m(PA.$$.fragment),wKo=l(),lc=a("p"),AKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Dme=a("code"),LKo=o("from_pretrained()"),BKo=o("class method or the "),qme=a("code"),xKo=o("from_config()"),kKo=o(`class
method.`),RKo=l(),$A=a("p"),SKo=o("This class cannot be instantiated directly using "),Gme=a("code"),PKo=o("__init__()"),$Ko=o(" (throws an error)."),IKo=l(),lt=a("div"),m(IA.$$.fragment),jKo=l(),Ome=a("p"),NKo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),DKo=l(),ic=a("p"),qKo=o(`Note:
Loading a model from its configuration file does `),Xme=a("strong"),GKo=o("not"),OKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zme=a("code"),XKo=o("from_pretrained()"),zKo=o("to load the model weights."),VKo=l(),Vme=a("p"),WKo=o("Examples:"),QKo=l(),m(jA.$$.fragment),HKo=l(),_o=a("div"),m(NA.$$.fragment),UKo=l(),Wme=a("p"),JKo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),YKo=l(),is=a("p"),KKo=o("The model class to instantiate is selected based on the "),Qme=a("code"),ZKo=o("model_type"),eZo=o(` property of the config object (either
passed as an argument or loaded from `),Hme=a("code"),oZo=o("pretrained_model_name_or_path"),rZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ume=a("code"),tZo=o("pretrained_model_name_or_path"),aZo=o(":"),sZo=l(),X=a("ul"),UF=a("li"),Jme=a("strong"),nZo=o("albert"),lZo=o(" \u2014 "),CD=a("a"),iZo=o("TFAlbertForSequenceClassification"),dZo=o(" (ALBERT model)"),cZo=l(),JF=a("li"),Yme=a("strong"),mZo=o("bert"),fZo=o(" \u2014 "),MD=a("a"),gZo=o("TFBertForSequenceClassification"),hZo=o(" (BERT model)"),uZo=l(),YF=a("li"),Kme=a("strong"),pZo=o("camembert"),_Zo=o(" \u2014 "),ED=a("a"),bZo=o("TFCamembertForSequenceClassification"),vZo=o(" (CamemBERT model)"),TZo=l(),KF=a("li"),Zme=a("strong"),FZo=o("convbert"),CZo=o(" \u2014 "),yD=a("a"),MZo=o("TFConvBertForSequenceClassification"),EZo=o(" (ConvBERT model)"),yZo=l(),ZF=a("li"),efe=a("strong"),wZo=o("ctrl"),AZo=o(" \u2014 "),wD=a("a"),LZo=o("TFCTRLForSequenceClassification"),BZo=o(" (CTRL model)"),xZo=l(),eC=a("li"),ofe=a("strong"),kZo=o("deberta"),RZo=o(" \u2014 "),AD=a("a"),SZo=o("TFDebertaForSequenceClassification"),PZo=o(" (DeBERTa model)"),$Zo=l(),oC=a("li"),rfe=a("strong"),IZo=o("deberta-v2"),jZo=o(" \u2014 "),LD=a("a"),NZo=o("TFDebertaV2ForSequenceClassification"),DZo=o(" (DeBERTa-v2 model)"),qZo=l(),rC=a("li"),tfe=a("strong"),GZo=o("distilbert"),OZo=o(" \u2014 "),BD=a("a"),XZo=o("TFDistilBertForSequenceClassification"),zZo=o(" (DistilBERT model)"),VZo=l(),tC=a("li"),afe=a("strong"),WZo=o("electra"),QZo=o(" \u2014 "),xD=a("a"),HZo=o("TFElectraForSequenceClassification"),UZo=o(" (ELECTRA model)"),JZo=l(),aC=a("li"),sfe=a("strong"),YZo=o("flaubert"),KZo=o(" \u2014 "),kD=a("a"),ZZo=o("TFFlaubertForSequenceClassification"),eer=o(" (FlauBERT model)"),oer=l(),sC=a("li"),nfe=a("strong"),rer=o("funnel"),ter=o(" \u2014 "),RD=a("a"),aer=o("TFFunnelForSequenceClassification"),ser=o(" (Funnel Transformer model)"),ner=l(),nC=a("li"),lfe=a("strong"),ler=o("gpt2"),ier=o(" \u2014 "),SD=a("a"),der=o("TFGPT2ForSequenceClassification"),cer=o(" (OpenAI GPT-2 model)"),mer=l(),lC=a("li"),ife=a("strong"),fer=o("layoutlm"),ger=o(" \u2014 "),PD=a("a"),her=o("TFLayoutLMForSequenceClassification"),uer=o(" (LayoutLM model)"),per=l(),iC=a("li"),dfe=a("strong"),_er=o("longformer"),ber=o(" \u2014 "),$D=a("a"),ver=o("TFLongformerForSequenceClassification"),Ter=o(" (Longformer model)"),Fer=l(),dC=a("li"),cfe=a("strong"),Cer=o("mobilebert"),Mer=o(" \u2014 "),ID=a("a"),Eer=o("TFMobileBertForSequenceClassification"),yer=o(" (MobileBERT model)"),wer=l(),cC=a("li"),mfe=a("strong"),Aer=o("mpnet"),Ler=o(" \u2014 "),jD=a("a"),Ber=o("TFMPNetForSequenceClassification"),xer=o(" (MPNet model)"),ker=l(),mC=a("li"),ffe=a("strong"),Rer=o("openai-gpt"),Ser=o(" \u2014 "),ND=a("a"),Per=o("TFOpenAIGPTForSequenceClassification"),$er=o(" (OpenAI GPT model)"),Ier=l(),fC=a("li"),gfe=a("strong"),jer=o("rembert"),Ner=o(" \u2014 "),DD=a("a"),Der=o("TFRemBertForSequenceClassification"),qer=o(" (RemBERT model)"),Ger=l(),gC=a("li"),hfe=a("strong"),Oer=o("roberta"),Xer=o(" \u2014 "),qD=a("a"),zer=o("TFRobertaForSequenceClassification"),Ver=o(" (RoBERTa model)"),Wer=l(),hC=a("li"),ufe=a("strong"),Qer=o("roformer"),Her=o(" \u2014 "),GD=a("a"),Uer=o("TFRoFormerForSequenceClassification"),Jer=o(" (RoFormer model)"),Yer=l(),uC=a("li"),pfe=a("strong"),Ker=o("tapas"),Zer=o(" \u2014 "),OD=a("a"),eor=o("TFTapasForSequenceClassification"),oor=o(" (TAPAS model)"),ror=l(),pC=a("li"),_fe=a("strong"),tor=o("transfo-xl"),aor=o(" \u2014 "),XD=a("a"),sor=o("TFTransfoXLForSequenceClassification"),nor=o(" (Transformer-XL model)"),lor=l(),_C=a("li"),bfe=a("strong"),ior=o("xlm"),dor=o(" \u2014 "),zD=a("a"),cor=o("TFXLMForSequenceClassification"),mor=o(" (XLM model)"),gor=l(),bC=a("li"),vfe=a("strong"),hor=o("xlm-roberta"),uor=o(" \u2014 "),VD=a("a"),por=o("TFXLMRobertaForSequenceClassification"),_or=o(" (XLM-RoBERTa model)"),bor=l(),vC=a("li"),Tfe=a("strong"),vor=o("xlnet"),Tor=o(" \u2014 "),WD=a("a"),For=o("TFXLNetForSequenceClassification"),Cor=o(" (XLNet model)"),Mor=l(),Ffe=a("p"),Eor=o("Examples:"),yor=l(),m(DA.$$.fragment),A7e=l(),dc=a("h2"),TC=a("a"),Cfe=a("span"),m(qA.$$.fragment),wor=l(),Mfe=a("span"),Aor=o("TFAutoModelForMultipleChoice"),L7e=l(),pr=a("div"),m(GA.$$.fragment),Lor=l(),cc=a("p"),Bor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Efe=a("code"),xor=o("from_pretrained()"),kor=o("class method or the "),yfe=a("code"),Ror=o("from_config()"),Sor=o(`class
method.`),Por=l(),OA=a("p"),$or=o("This class cannot be instantiated directly using "),wfe=a("code"),Ior=o("__init__()"),jor=o(" (throws an error)."),Nor=l(),it=a("div"),m(XA.$$.fragment),Dor=l(),Afe=a("p"),qor=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Gor=l(),mc=a("p"),Oor=o(`Note:
Loading a model from its configuration file does `),Lfe=a("strong"),Xor=o("not"),zor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bfe=a("code"),Vor=o("from_pretrained()"),Wor=o("to load the model weights."),Qor=l(),xfe=a("p"),Hor=o("Examples:"),Uor=l(),m(zA.$$.fragment),Jor=l(),bo=a("div"),m(VA.$$.fragment),Yor=l(),kfe=a("p"),Kor=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Zor=l(),ds=a("p"),err=o("The model class to instantiate is selected based on the "),Rfe=a("code"),orr=o("model_type"),rrr=o(` property of the config object (either
passed as an argument or loaded from `),Sfe=a("code"),trr=o("pretrained_model_name_or_path"),arr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pfe=a("code"),srr=o("pretrained_model_name_or_path"),nrr=o(":"),lrr=l(),te=a("ul"),FC=a("li"),$fe=a("strong"),irr=o("albert"),drr=o(" \u2014 "),QD=a("a"),crr=o("TFAlbertForMultipleChoice"),mrr=o(" (ALBERT model)"),frr=l(),CC=a("li"),Ife=a("strong"),grr=o("bert"),hrr=o(" \u2014 "),HD=a("a"),urr=o("TFBertForMultipleChoice"),prr=o(" (BERT model)"),_rr=l(),MC=a("li"),jfe=a("strong"),brr=o("camembert"),vrr=o(" \u2014 "),UD=a("a"),Trr=o("TFCamembertForMultipleChoice"),Frr=o(" (CamemBERT model)"),Crr=l(),EC=a("li"),Nfe=a("strong"),Mrr=o("convbert"),Err=o(" \u2014 "),JD=a("a"),yrr=o("TFConvBertForMultipleChoice"),wrr=o(" (ConvBERT model)"),Arr=l(),yC=a("li"),Dfe=a("strong"),Lrr=o("distilbert"),Brr=o(" \u2014 "),YD=a("a"),xrr=o("TFDistilBertForMultipleChoice"),krr=o(" (DistilBERT model)"),Rrr=l(),wC=a("li"),qfe=a("strong"),Srr=o("electra"),Prr=o(" \u2014 "),KD=a("a"),$rr=o("TFElectraForMultipleChoice"),Irr=o(" (ELECTRA model)"),jrr=l(),AC=a("li"),Gfe=a("strong"),Nrr=o("flaubert"),Drr=o(" \u2014 "),ZD=a("a"),qrr=o("TFFlaubertForMultipleChoice"),Grr=o(" (FlauBERT model)"),Orr=l(),LC=a("li"),Ofe=a("strong"),Xrr=o("funnel"),zrr=o(" \u2014 "),eq=a("a"),Vrr=o("TFFunnelForMultipleChoice"),Wrr=o(" (Funnel Transformer model)"),Qrr=l(),BC=a("li"),Xfe=a("strong"),Hrr=o("longformer"),Urr=o(" \u2014 "),oq=a("a"),Jrr=o("TFLongformerForMultipleChoice"),Yrr=o(" (Longformer model)"),Krr=l(),xC=a("li"),zfe=a("strong"),Zrr=o("mobilebert"),etr=o(" \u2014 "),rq=a("a"),otr=o("TFMobileBertForMultipleChoice"),rtr=o(" (MobileBERT model)"),ttr=l(),kC=a("li"),Vfe=a("strong"),atr=o("mpnet"),str=o(" \u2014 "),tq=a("a"),ntr=o("TFMPNetForMultipleChoice"),ltr=o(" (MPNet model)"),itr=l(),RC=a("li"),Wfe=a("strong"),dtr=o("rembert"),ctr=o(" \u2014 "),aq=a("a"),mtr=o("TFRemBertForMultipleChoice"),ftr=o(" (RemBERT model)"),gtr=l(),SC=a("li"),Qfe=a("strong"),htr=o("roberta"),utr=o(" \u2014 "),sq=a("a"),ptr=o("TFRobertaForMultipleChoice"),_tr=o(" (RoBERTa model)"),btr=l(),PC=a("li"),Hfe=a("strong"),vtr=o("roformer"),Ttr=o(" \u2014 "),nq=a("a"),Ftr=o("TFRoFormerForMultipleChoice"),Ctr=o(" (RoFormer model)"),Mtr=l(),$C=a("li"),Ufe=a("strong"),Etr=o("xlm"),ytr=o(" \u2014 "),lq=a("a"),wtr=o("TFXLMForMultipleChoice"),Atr=o(" (XLM model)"),Ltr=l(),IC=a("li"),Jfe=a("strong"),Btr=o("xlm-roberta"),xtr=o(" \u2014 "),iq=a("a"),ktr=o("TFXLMRobertaForMultipleChoice"),Rtr=o(" (XLM-RoBERTa model)"),Str=l(),jC=a("li"),Yfe=a("strong"),Ptr=o("xlnet"),$tr=o(" \u2014 "),dq=a("a"),Itr=o("TFXLNetForMultipleChoice"),jtr=o(" (XLNet model)"),Ntr=l(),Kfe=a("p"),Dtr=o("Examples:"),qtr=l(),m(WA.$$.fragment),B7e=l(),fc=a("h2"),NC=a("a"),Zfe=a("span"),m(QA.$$.fragment),Gtr=l(),ege=a("span"),Otr=o("TFAutoModelForTableQuestionAnswering"),x7e=l(),_r=a("div"),m(HA.$$.fragment),Xtr=l(),gc=a("p"),ztr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),oge=a("code"),Vtr=o("from_pretrained()"),Wtr=o("class method or the "),rge=a("code"),Qtr=o("from_config()"),Htr=o(`class
method.`),Utr=l(),UA=a("p"),Jtr=o("This class cannot be instantiated directly using "),tge=a("code"),Ytr=o("__init__()"),Ktr=o(" (throws an error)."),Ztr=l(),dt=a("div"),m(JA.$$.fragment),ear=l(),age=a("p"),oar=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),rar=l(),hc=a("p"),tar=o(`Note:
Loading a model from its configuration file does `),sge=a("strong"),aar=o("not"),sar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nge=a("code"),nar=o("from_pretrained()"),lar=o("to load the model weights."),iar=l(),lge=a("p"),dar=o("Examples:"),car=l(),m(YA.$$.fragment),mar=l(),vo=a("div"),m(KA.$$.fragment),far=l(),ige=a("p"),gar=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),har=l(),cs=a("p"),uar=o("The model class to instantiate is selected based on the "),dge=a("code"),par=o("model_type"),_ar=o(` property of the config object (either
passed as an argument or loaded from `),cge=a("code"),bar=o("pretrained_model_name_or_path"),Tar=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mge=a("code"),Far=o("pretrained_model_name_or_path"),Car=o(":"),Mar=l(),fge=a("ul"),DC=a("li"),gge=a("strong"),Ear=o("tapas"),yar=o(" \u2014 "),cq=a("a"),war=o("TFTapasForQuestionAnswering"),Aar=o(" (TAPAS model)"),Lar=l(),hge=a("p"),Bar=o("Examples:"),xar=l(),m(ZA.$$.fragment),k7e=l(),uc=a("h2"),qC=a("a"),uge=a("span"),m(e0.$$.fragment),kar=l(),pge=a("span"),Rar=o("TFAutoModelForTokenClassification"),R7e=l(),br=a("div"),m(o0.$$.fragment),Sar=l(),pc=a("p"),Par=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),_ge=a("code"),$ar=o("from_pretrained()"),Iar=o("class method or the "),bge=a("code"),jar=o("from_config()"),Nar=o(`class
method.`),Dar=l(),r0=a("p"),qar=o("This class cannot be instantiated directly using "),vge=a("code"),Gar=o("__init__()"),Oar=o(" (throws an error)."),Xar=l(),ct=a("div"),m(t0.$$.fragment),zar=l(),Tge=a("p"),Var=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),War=l(),_c=a("p"),Qar=o(`Note:
Loading a model from its configuration file does `),Fge=a("strong"),Har=o("not"),Uar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cge=a("code"),Jar=o("from_pretrained()"),Yar=o("to load the model weights."),Kar=l(),Mge=a("p"),Zar=o("Examples:"),esr=l(),m(a0.$$.fragment),osr=l(),To=a("div"),m(s0.$$.fragment),rsr=l(),Ege=a("p"),tsr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),asr=l(),ms=a("p"),ssr=o("The model class to instantiate is selected based on the "),yge=a("code"),nsr=o("model_type"),lsr=o(` property of the config object (either
passed as an argument or loaded from `),wge=a("code"),isr=o("pretrained_model_name_or_path"),dsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Age=a("code"),csr=o("pretrained_model_name_or_path"),msr=o(":"),fsr=l(),K=a("ul"),GC=a("li"),Lge=a("strong"),gsr=o("albert"),hsr=o(" \u2014 "),mq=a("a"),usr=o("TFAlbertForTokenClassification"),psr=o(" (ALBERT model)"),_sr=l(),OC=a("li"),Bge=a("strong"),bsr=o("bert"),vsr=o(" \u2014 "),fq=a("a"),Tsr=o("TFBertForTokenClassification"),Fsr=o(" (BERT model)"),Csr=l(),XC=a("li"),xge=a("strong"),Msr=o("camembert"),Esr=o(" \u2014 "),gq=a("a"),ysr=o("TFCamembertForTokenClassification"),wsr=o(" (CamemBERT model)"),Asr=l(),zC=a("li"),kge=a("strong"),Lsr=o("convbert"),Bsr=o(" \u2014 "),hq=a("a"),xsr=o("TFConvBertForTokenClassification"),ksr=o(" (ConvBERT model)"),Rsr=l(),VC=a("li"),Rge=a("strong"),Ssr=o("deberta"),Psr=o(" \u2014 "),uq=a("a"),$sr=o("TFDebertaForTokenClassification"),Isr=o(" (DeBERTa model)"),jsr=l(),WC=a("li"),Sge=a("strong"),Nsr=o("deberta-v2"),Dsr=o(" \u2014 "),pq=a("a"),qsr=o("TFDebertaV2ForTokenClassification"),Gsr=o(" (DeBERTa-v2 model)"),Osr=l(),QC=a("li"),Pge=a("strong"),Xsr=o("distilbert"),zsr=o(" \u2014 "),_q=a("a"),Vsr=o("TFDistilBertForTokenClassification"),Wsr=o(" (DistilBERT model)"),Qsr=l(),HC=a("li"),$ge=a("strong"),Hsr=o("electra"),Usr=o(" \u2014 "),bq=a("a"),Jsr=o("TFElectraForTokenClassification"),Ysr=o(" (ELECTRA model)"),Ksr=l(),UC=a("li"),Ige=a("strong"),Zsr=o("flaubert"),enr=o(" \u2014 "),vq=a("a"),onr=o("TFFlaubertForTokenClassification"),rnr=o(" (FlauBERT model)"),tnr=l(),JC=a("li"),jge=a("strong"),anr=o("funnel"),snr=o(" \u2014 "),Tq=a("a"),nnr=o("TFFunnelForTokenClassification"),lnr=o(" (Funnel Transformer model)"),inr=l(),YC=a("li"),Nge=a("strong"),dnr=o("layoutlm"),cnr=o(" \u2014 "),Fq=a("a"),mnr=o("TFLayoutLMForTokenClassification"),fnr=o(" (LayoutLM model)"),gnr=l(),KC=a("li"),Dge=a("strong"),hnr=o("longformer"),unr=o(" \u2014 "),Cq=a("a"),pnr=o("TFLongformerForTokenClassification"),_nr=o(" (Longformer model)"),bnr=l(),ZC=a("li"),qge=a("strong"),vnr=o("mobilebert"),Tnr=o(" \u2014 "),Mq=a("a"),Fnr=o("TFMobileBertForTokenClassification"),Cnr=o(" (MobileBERT model)"),Mnr=l(),e4=a("li"),Gge=a("strong"),Enr=o("mpnet"),ynr=o(" \u2014 "),Eq=a("a"),wnr=o("TFMPNetForTokenClassification"),Anr=o(" (MPNet model)"),Lnr=l(),o4=a("li"),Oge=a("strong"),Bnr=o("rembert"),xnr=o(" \u2014 "),yq=a("a"),knr=o("TFRemBertForTokenClassification"),Rnr=o(" (RemBERT model)"),Snr=l(),r4=a("li"),Xge=a("strong"),Pnr=o("roberta"),$nr=o(" \u2014 "),wq=a("a"),Inr=o("TFRobertaForTokenClassification"),jnr=o(" (RoBERTa model)"),Nnr=l(),t4=a("li"),zge=a("strong"),Dnr=o("roformer"),qnr=o(" \u2014 "),Aq=a("a"),Gnr=o("TFRoFormerForTokenClassification"),Onr=o(" (RoFormer model)"),Xnr=l(),a4=a("li"),Vge=a("strong"),znr=o("xlm"),Vnr=o(" \u2014 "),Lq=a("a"),Wnr=o("TFXLMForTokenClassification"),Qnr=o(" (XLM model)"),Hnr=l(),s4=a("li"),Wge=a("strong"),Unr=o("xlm-roberta"),Jnr=o(" \u2014 "),Bq=a("a"),Ynr=o("TFXLMRobertaForTokenClassification"),Knr=o(" (XLM-RoBERTa model)"),Znr=l(),n4=a("li"),Qge=a("strong"),elr=o("xlnet"),olr=o(" \u2014 "),xq=a("a"),rlr=o("TFXLNetForTokenClassification"),tlr=o(" (XLNet model)"),alr=l(),Hge=a("p"),slr=o("Examples:"),nlr=l(),m(n0.$$.fragment),S7e=l(),bc=a("h2"),l4=a("a"),Uge=a("span"),m(l0.$$.fragment),llr=l(),Jge=a("span"),ilr=o("TFAutoModelForQuestionAnswering"),P7e=l(),vr=a("div"),m(i0.$$.fragment),dlr=l(),vc=a("p"),clr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Yge=a("code"),mlr=o("from_pretrained()"),flr=o("class method or the "),Kge=a("code"),glr=o("from_config()"),hlr=o(`class
method.`),ulr=l(),d0=a("p"),plr=o("This class cannot be instantiated directly using "),Zge=a("code"),_lr=o("__init__()"),blr=o(" (throws an error)."),vlr=l(),mt=a("div"),m(c0.$$.fragment),Tlr=l(),ehe=a("p"),Flr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Clr=l(),Tc=a("p"),Mlr=o(`Note:
Loading a model from its configuration file does `),ohe=a("strong"),Elr=o("not"),ylr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rhe=a("code"),wlr=o("from_pretrained()"),Alr=o("to load the model weights."),Llr=l(),the=a("p"),Blr=o("Examples:"),xlr=l(),m(m0.$$.fragment),klr=l(),Fo=a("div"),m(f0.$$.fragment),Rlr=l(),ahe=a("p"),Slr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Plr=l(),fs=a("p"),$lr=o("The model class to instantiate is selected based on the "),she=a("code"),Ilr=o("model_type"),jlr=o(` property of the config object (either
passed as an argument or loaded from `),nhe=a("code"),Nlr=o("pretrained_model_name_or_path"),Dlr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lhe=a("code"),qlr=o("pretrained_model_name_or_path"),Glr=o(":"),Olr=l(),Z=a("ul"),i4=a("li"),ihe=a("strong"),Xlr=o("albert"),zlr=o(" \u2014 "),kq=a("a"),Vlr=o("TFAlbertForQuestionAnswering"),Wlr=o(" (ALBERT model)"),Qlr=l(),d4=a("li"),dhe=a("strong"),Hlr=o("bert"),Ulr=o(" \u2014 "),Rq=a("a"),Jlr=o("TFBertForQuestionAnswering"),Ylr=o(" (BERT model)"),Klr=l(),c4=a("li"),che=a("strong"),Zlr=o("camembert"),eir=o(" \u2014 "),Sq=a("a"),oir=o("TFCamembertForQuestionAnswering"),rir=o(" (CamemBERT model)"),tir=l(),m4=a("li"),mhe=a("strong"),air=o("convbert"),sir=o(" \u2014 "),Pq=a("a"),nir=o("TFConvBertForQuestionAnswering"),lir=o(" (ConvBERT model)"),iir=l(),f4=a("li"),fhe=a("strong"),dir=o("deberta"),cir=o(" \u2014 "),$q=a("a"),mir=o("TFDebertaForQuestionAnswering"),fir=o(" (DeBERTa model)"),gir=l(),g4=a("li"),ghe=a("strong"),hir=o("deberta-v2"),uir=o(" \u2014 "),Iq=a("a"),pir=o("TFDebertaV2ForQuestionAnswering"),_ir=o(" (DeBERTa-v2 model)"),bir=l(),h4=a("li"),hhe=a("strong"),vir=o("distilbert"),Tir=o(" \u2014 "),jq=a("a"),Fir=o("TFDistilBertForQuestionAnswering"),Cir=o(" (DistilBERT model)"),Mir=l(),u4=a("li"),uhe=a("strong"),Eir=o("electra"),yir=o(" \u2014 "),Nq=a("a"),wir=o("TFElectraForQuestionAnswering"),Air=o(" (ELECTRA model)"),Lir=l(),p4=a("li"),phe=a("strong"),Bir=o("flaubert"),xir=o(" \u2014 "),Dq=a("a"),kir=o("TFFlaubertForQuestionAnsweringSimple"),Rir=o(" (FlauBERT model)"),Sir=l(),_4=a("li"),_he=a("strong"),Pir=o("funnel"),$ir=o(" \u2014 "),qq=a("a"),Iir=o("TFFunnelForQuestionAnswering"),jir=o(" (Funnel Transformer model)"),Nir=l(),b4=a("li"),bhe=a("strong"),Dir=o("longformer"),qir=o(" \u2014 "),Gq=a("a"),Gir=o("TFLongformerForQuestionAnswering"),Oir=o(" (Longformer model)"),Xir=l(),v4=a("li"),vhe=a("strong"),zir=o("mobilebert"),Vir=o(" \u2014 "),Oq=a("a"),Wir=o("TFMobileBertForQuestionAnswering"),Qir=o(" (MobileBERT model)"),Hir=l(),T4=a("li"),The=a("strong"),Uir=o("mpnet"),Jir=o(" \u2014 "),Xq=a("a"),Yir=o("TFMPNetForQuestionAnswering"),Kir=o(" (MPNet model)"),Zir=l(),F4=a("li"),Fhe=a("strong"),edr=o("rembert"),odr=o(" \u2014 "),zq=a("a"),rdr=o("TFRemBertForQuestionAnswering"),tdr=o(" (RemBERT model)"),adr=l(),C4=a("li"),Che=a("strong"),sdr=o("roberta"),ndr=o(" \u2014 "),Vq=a("a"),ldr=o("TFRobertaForQuestionAnswering"),idr=o(" (RoBERTa model)"),ddr=l(),M4=a("li"),Mhe=a("strong"),cdr=o("roformer"),mdr=o(" \u2014 "),Wq=a("a"),fdr=o("TFRoFormerForQuestionAnswering"),gdr=o(" (RoFormer model)"),hdr=l(),E4=a("li"),Ehe=a("strong"),udr=o("xlm"),pdr=o(" \u2014 "),Qq=a("a"),_dr=o("TFXLMForQuestionAnsweringSimple"),bdr=o(" (XLM model)"),vdr=l(),y4=a("li"),yhe=a("strong"),Tdr=o("xlm-roberta"),Fdr=o(" \u2014 "),Hq=a("a"),Cdr=o("TFXLMRobertaForQuestionAnswering"),Mdr=o(" (XLM-RoBERTa model)"),Edr=l(),w4=a("li"),whe=a("strong"),ydr=o("xlnet"),wdr=o(" \u2014 "),Uq=a("a"),Adr=o("TFXLNetForQuestionAnsweringSimple"),Ldr=o(" (XLNet model)"),Bdr=l(),Ahe=a("p"),xdr=o("Examples:"),kdr=l(),m(g0.$$.fragment),$7e=l(),Fc=a("h2"),A4=a("a"),Lhe=a("span"),m(h0.$$.fragment),Rdr=l(),Bhe=a("span"),Sdr=o("TFAutoModelForVision2Seq"),I7e=l(),Tr=a("div"),m(u0.$$.fragment),Pdr=l(),Cc=a("p"),$dr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),xhe=a("code"),Idr=o("from_pretrained()"),jdr=o("class method or the "),khe=a("code"),Ndr=o("from_config()"),Ddr=o(`class
method.`),qdr=l(),p0=a("p"),Gdr=o("This class cannot be instantiated directly using "),Rhe=a("code"),Odr=o("__init__()"),Xdr=o(" (throws an error)."),zdr=l(),ft=a("div"),m(_0.$$.fragment),Vdr=l(),She=a("p"),Wdr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Qdr=l(),Mc=a("p"),Hdr=o(`Note:
Loading a model from its configuration file does `),Phe=a("strong"),Udr=o("not"),Jdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$he=a("code"),Ydr=o("from_pretrained()"),Kdr=o("to load the model weights."),Zdr=l(),Ihe=a("p"),ecr=o("Examples:"),ocr=l(),m(b0.$$.fragment),rcr=l(),Co=a("div"),m(v0.$$.fragment),tcr=l(),jhe=a("p"),acr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),scr=l(),gs=a("p"),ncr=o("The model class to instantiate is selected based on the "),Nhe=a("code"),lcr=o("model_type"),icr=o(` property of the config object (either
passed as an argument or loaded from `),Dhe=a("code"),dcr=o("pretrained_model_name_or_path"),ccr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qhe=a("code"),mcr=o("pretrained_model_name_or_path"),fcr=o(":"),gcr=l(),Ghe=a("ul"),L4=a("li"),Ohe=a("strong"),hcr=o("vision-encoder-decoder"),ucr=o(" \u2014 "),Jq=a("a"),pcr=o("TFVisionEncoderDecoderModel"),_cr=o(" (Vision Encoder decoder model)"),bcr=l(),Xhe=a("p"),vcr=o("Examples:"),Tcr=l(),m(T0.$$.fragment),j7e=l(),Ec=a("h2"),B4=a("a"),zhe=a("span"),m(F0.$$.fragment),Fcr=l(),Vhe=a("span"),Ccr=o("TFAutoModelForSpeechSeq2Seq"),N7e=l(),Fr=a("div"),m(C0.$$.fragment),Mcr=l(),yc=a("p"),Ecr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Whe=a("code"),ycr=o("from_pretrained()"),wcr=o("class method or the "),Qhe=a("code"),Acr=o("from_config()"),Lcr=o(`class
method.`),Bcr=l(),M0=a("p"),xcr=o("This class cannot be instantiated directly using "),Hhe=a("code"),kcr=o("__init__()"),Rcr=o(" (throws an error)."),Scr=l(),gt=a("div"),m(E0.$$.fragment),Pcr=l(),Uhe=a("p"),$cr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Icr=l(),wc=a("p"),jcr=o(`Note:
Loading a model from its configuration file does `),Jhe=a("strong"),Ncr=o("not"),Dcr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yhe=a("code"),qcr=o("from_pretrained()"),Gcr=o("to load the model weights."),Ocr=l(),Khe=a("p"),Xcr=o("Examples:"),zcr=l(),m(y0.$$.fragment),Vcr=l(),Mo=a("div"),m(w0.$$.fragment),Wcr=l(),Zhe=a("p"),Qcr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Hcr=l(),hs=a("p"),Ucr=o("The model class to instantiate is selected based on the "),eue=a("code"),Jcr=o("model_type"),Ycr=o(` property of the config object (either
passed as an argument or loaded from `),oue=a("code"),Kcr=o("pretrained_model_name_or_path"),Zcr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rue=a("code"),emr=o("pretrained_model_name_or_path"),omr=o(":"),rmr=l(),tue=a("ul"),x4=a("li"),aue=a("strong"),tmr=o("speech_to_text"),amr=o(" \u2014 "),Yq=a("a"),smr=o("TFSpeech2TextForConditionalGeneration"),nmr=o(" (Speech2Text model)"),lmr=l(),sue=a("p"),imr=o("Examples:"),dmr=l(),m(A0.$$.fragment),D7e=l(),Ac=a("h2"),k4=a("a"),nue=a("span"),m(L0.$$.fragment),cmr=l(),lue=a("span"),mmr=o("FlaxAutoModel"),q7e=l(),Cr=a("div"),m(B0.$$.fragment),fmr=l(),Lc=a("p"),gmr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),iue=a("code"),hmr=o("from_pretrained()"),umr=o("class method or the "),due=a("code"),pmr=o("from_config()"),_mr=o(`class
method.`),bmr=l(),x0=a("p"),vmr=o("This class cannot be instantiated directly using "),cue=a("code"),Tmr=o("__init__()"),Fmr=o(" (throws an error)."),Cmr=l(),ht=a("div"),m(k0.$$.fragment),Mmr=l(),mue=a("p"),Emr=o("Instantiates one of the base model classes of the library from a configuration."),ymr=l(),Bc=a("p"),wmr=o(`Note:
Loading a model from its configuration file does `),fue=a("strong"),Amr=o("not"),Lmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gue=a("code"),Bmr=o("from_pretrained()"),xmr=o("to load the model weights."),kmr=l(),hue=a("p"),Rmr=o("Examples:"),Smr=l(),m(R0.$$.fragment),Pmr=l(),Eo=a("div"),m(S0.$$.fragment),$mr=l(),uue=a("p"),Imr=o("Instantiate one of the base model classes of the library from a pretrained model."),jmr=l(),us=a("p"),Nmr=o("The model class to instantiate is selected based on the "),pue=a("code"),Dmr=o("model_type"),qmr=o(` property of the config object (either
passed as an argument or loaded from `),_ue=a("code"),Gmr=o("pretrained_model_name_or_path"),Omr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bue=a("code"),Xmr=o("pretrained_model_name_or_path"),zmr=o(":"),Vmr=l(),V=a("ul"),R4=a("li"),vue=a("strong"),Wmr=o("albert"),Qmr=o(" \u2014 "),Kq=a("a"),Hmr=o("FlaxAlbertModel"),Umr=o(" (ALBERT model)"),Jmr=l(),S4=a("li"),Tue=a("strong"),Ymr=o("bart"),Kmr=o(" \u2014 "),Zq=a("a"),Zmr=o("FlaxBartModel"),efr=o(" (BART model)"),ofr=l(),P4=a("li"),Fue=a("strong"),rfr=o("beit"),tfr=o(" \u2014 "),eG=a("a"),afr=o("FlaxBeitModel"),sfr=o(" (BEiT model)"),nfr=l(),$4=a("li"),Cue=a("strong"),lfr=o("bert"),ifr=o(" \u2014 "),oG=a("a"),dfr=o("FlaxBertModel"),cfr=o(" (BERT model)"),mfr=l(),I4=a("li"),Mue=a("strong"),ffr=o("big_bird"),gfr=o(" \u2014 "),rG=a("a"),hfr=o("FlaxBigBirdModel"),ufr=o(" (BigBird model)"),pfr=l(),j4=a("li"),Eue=a("strong"),_fr=o("blenderbot"),bfr=o(" \u2014 "),tG=a("a"),vfr=o("FlaxBlenderbotModel"),Tfr=o(" (Blenderbot model)"),Ffr=l(),N4=a("li"),yue=a("strong"),Cfr=o("blenderbot-small"),Mfr=o(" \u2014 "),aG=a("a"),Efr=o("FlaxBlenderbotSmallModel"),yfr=o(" (BlenderbotSmall model)"),wfr=l(),D4=a("li"),wue=a("strong"),Afr=o("clip"),Lfr=o(" \u2014 "),sG=a("a"),Bfr=o("FlaxCLIPModel"),xfr=o(" (CLIP model)"),kfr=l(),q4=a("li"),Aue=a("strong"),Rfr=o("distilbert"),Sfr=o(" \u2014 "),nG=a("a"),Pfr=o("FlaxDistilBertModel"),$fr=o(" (DistilBERT model)"),Ifr=l(),G4=a("li"),Lue=a("strong"),jfr=o("electra"),Nfr=o(" \u2014 "),lG=a("a"),Dfr=o("FlaxElectraModel"),qfr=o(" (ELECTRA model)"),Gfr=l(),O4=a("li"),Bue=a("strong"),Ofr=o("gpt2"),Xfr=o(" \u2014 "),iG=a("a"),zfr=o("FlaxGPT2Model"),Vfr=o(" (OpenAI GPT-2 model)"),Wfr=l(),X4=a("li"),xue=a("strong"),Qfr=o("gpt_neo"),Hfr=o(" \u2014 "),dG=a("a"),Ufr=o("FlaxGPTNeoModel"),Jfr=o(" (GPT Neo model)"),Yfr=l(),z4=a("li"),kue=a("strong"),Kfr=o("gptj"),Zfr=o(" \u2014 "),cG=a("a"),egr=o("FlaxGPTJModel"),ogr=o(" (GPT-J model)"),rgr=l(),V4=a("li"),Rue=a("strong"),tgr=o("marian"),agr=o(" \u2014 "),mG=a("a"),sgr=o("FlaxMarianModel"),ngr=o(" (Marian model)"),lgr=l(),W4=a("li"),Sue=a("strong"),igr=o("mbart"),dgr=o(" \u2014 "),fG=a("a"),cgr=o("FlaxMBartModel"),mgr=o(" (mBART model)"),fgr=l(),Q4=a("li"),Pue=a("strong"),ggr=o("mt5"),hgr=o(" \u2014 "),gG=a("a"),ugr=o("FlaxMT5Model"),pgr=o(" (mT5 model)"),_gr=l(),H4=a("li"),$ue=a("strong"),bgr=o("pegasus"),vgr=o(" \u2014 "),hG=a("a"),Tgr=o("FlaxPegasusModel"),Fgr=o(" (Pegasus model)"),Cgr=l(),U4=a("li"),Iue=a("strong"),Mgr=o("roberta"),Egr=o(" \u2014 "),uG=a("a"),ygr=o("FlaxRobertaModel"),wgr=o(" (RoBERTa model)"),Agr=l(),J4=a("li"),jue=a("strong"),Lgr=o("roformer"),Bgr=o(" \u2014 "),pG=a("a"),xgr=o("FlaxRoFormerModel"),kgr=o(" (RoFormer model)"),Rgr=l(),Y4=a("li"),Nue=a("strong"),Sgr=o("t5"),Pgr=o(" \u2014 "),_G=a("a"),$gr=o("FlaxT5Model"),Igr=o(" (T5 model)"),jgr=l(),K4=a("li"),Due=a("strong"),Ngr=o("vision-text-dual-encoder"),Dgr=o(" \u2014 "),bG=a("a"),qgr=o("FlaxVisionTextDualEncoderModel"),Ggr=o(" (VisionTextDualEncoder model)"),Ogr=l(),Z4=a("li"),que=a("strong"),Xgr=o("vit"),zgr=o(" \u2014 "),vG=a("a"),Vgr=o("FlaxViTModel"),Wgr=o(" (ViT model)"),Qgr=l(),eM=a("li"),Gue=a("strong"),Hgr=o("wav2vec2"),Ugr=o(" \u2014 "),TG=a("a"),Jgr=o("FlaxWav2Vec2Model"),Ygr=o(" (Wav2Vec2 model)"),Kgr=l(),oM=a("li"),Oue=a("strong"),Zgr=o("xglm"),ehr=o(" \u2014 "),FG=a("a"),ohr=o("FlaxXGLMModel"),rhr=o(" (XGLM model)"),thr=l(),Xue=a("p"),ahr=o("Examples:"),shr=l(),m(P0.$$.fragment),G7e=l(),xc=a("h2"),rM=a("a"),zue=a("span"),m($0.$$.fragment),nhr=l(),Vue=a("span"),lhr=o("FlaxAutoModelForCausalLM"),O7e=l(),Mr=a("div"),m(I0.$$.fragment),ihr=l(),kc=a("p"),dhr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Wue=a("code"),chr=o("from_pretrained()"),mhr=o("class method or the "),Que=a("code"),fhr=o("from_config()"),ghr=o(`class
method.`),hhr=l(),j0=a("p"),uhr=o("This class cannot be instantiated directly using "),Hue=a("code"),phr=o("__init__()"),_hr=o(" (throws an error)."),bhr=l(),ut=a("div"),m(N0.$$.fragment),vhr=l(),Uue=a("p"),Thr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Fhr=l(),Rc=a("p"),Chr=o(`Note:
Loading a model from its configuration file does `),Jue=a("strong"),Mhr=o("not"),Ehr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yue=a("code"),yhr=o("from_pretrained()"),whr=o("to load the model weights."),Ahr=l(),Kue=a("p"),Lhr=o("Examples:"),Bhr=l(),m(D0.$$.fragment),xhr=l(),yo=a("div"),m(q0.$$.fragment),khr=l(),Zue=a("p"),Rhr=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Shr=l(),ps=a("p"),Phr=o("The model class to instantiate is selected based on the "),epe=a("code"),$hr=o("model_type"),Ihr=o(` property of the config object (either
passed as an argument or loaded from `),ope=a("code"),jhr=o("pretrained_model_name_or_path"),Nhr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rpe=a("code"),Dhr=o("pretrained_model_name_or_path"),qhr=o(":"),Ghr=l(),_s=a("ul"),tM=a("li"),tpe=a("strong"),Ohr=o("gpt2"),Xhr=o(" \u2014 "),CG=a("a"),zhr=o("FlaxGPT2LMHeadModel"),Vhr=o(" (OpenAI GPT-2 model)"),Whr=l(),aM=a("li"),ape=a("strong"),Qhr=o("gpt_neo"),Hhr=o(" \u2014 "),MG=a("a"),Uhr=o("FlaxGPTNeoForCausalLM"),Jhr=o(" (GPT Neo model)"),Yhr=l(),sM=a("li"),spe=a("strong"),Khr=o("gptj"),Zhr=o(" \u2014 "),EG=a("a"),eur=o("FlaxGPTJForCausalLM"),our=o(" (GPT-J model)"),rur=l(),nM=a("li"),npe=a("strong"),tur=o("xglm"),aur=o(" \u2014 "),yG=a("a"),sur=o("FlaxXGLMForCausalLM"),nur=o(" (XGLM model)"),lur=l(),lpe=a("p"),iur=o("Examples:"),dur=l(),m(G0.$$.fragment),X7e=l(),Sc=a("h2"),lM=a("a"),ipe=a("span"),m(O0.$$.fragment),cur=l(),dpe=a("span"),mur=o("FlaxAutoModelForPreTraining"),z7e=l(),Er=a("div"),m(X0.$$.fragment),fur=l(),Pc=a("p"),gur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),cpe=a("code"),hur=o("from_pretrained()"),uur=o("class method or the "),mpe=a("code"),pur=o("from_config()"),_ur=o(`class
method.`),bur=l(),z0=a("p"),vur=o("This class cannot be instantiated directly using "),fpe=a("code"),Tur=o("__init__()"),Fur=o(" (throws an error)."),Cur=l(),pt=a("div"),m(V0.$$.fragment),Mur=l(),gpe=a("p"),Eur=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),yur=l(),$c=a("p"),wur=o(`Note:
Loading a model from its configuration file does `),hpe=a("strong"),Aur=o("not"),Lur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),upe=a("code"),Bur=o("from_pretrained()"),xur=o("to load the model weights."),kur=l(),ppe=a("p"),Rur=o("Examples:"),Sur=l(),m(W0.$$.fragment),Pur=l(),wo=a("div"),m(Q0.$$.fragment),$ur=l(),_pe=a("p"),Iur=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),jur=l(),bs=a("p"),Nur=o("The model class to instantiate is selected based on the "),bpe=a("code"),Dur=o("model_type"),qur=o(` property of the config object (either
passed as an argument or loaded from `),vpe=a("code"),Gur=o("pretrained_model_name_or_path"),Our=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tpe=a("code"),Xur=o("pretrained_model_name_or_path"),zur=o(":"),Vur=l(),me=a("ul"),iM=a("li"),Fpe=a("strong"),Wur=o("albert"),Qur=o(" \u2014 "),wG=a("a"),Hur=o("FlaxAlbertForPreTraining"),Uur=o(" (ALBERT model)"),Jur=l(),dM=a("li"),Cpe=a("strong"),Yur=o("bart"),Kur=o(" \u2014 "),AG=a("a"),Zur=o("FlaxBartForConditionalGeneration"),epr=o(" (BART model)"),opr=l(),cM=a("li"),Mpe=a("strong"),rpr=o("bert"),tpr=o(" \u2014 "),LG=a("a"),apr=o("FlaxBertForPreTraining"),spr=o(" (BERT model)"),npr=l(),mM=a("li"),Epe=a("strong"),lpr=o("big_bird"),ipr=o(" \u2014 "),BG=a("a"),dpr=o("FlaxBigBirdForPreTraining"),cpr=o(" (BigBird model)"),mpr=l(),fM=a("li"),ype=a("strong"),fpr=o("electra"),gpr=o(" \u2014 "),xG=a("a"),hpr=o("FlaxElectraForPreTraining"),upr=o(" (ELECTRA model)"),ppr=l(),gM=a("li"),wpe=a("strong"),_pr=o("mbart"),bpr=o(" \u2014 "),kG=a("a"),vpr=o("FlaxMBartForConditionalGeneration"),Tpr=o(" (mBART model)"),Fpr=l(),hM=a("li"),Ape=a("strong"),Cpr=o("mt5"),Mpr=o(" \u2014 "),RG=a("a"),Epr=o("FlaxMT5ForConditionalGeneration"),ypr=o(" (mT5 model)"),wpr=l(),uM=a("li"),Lpe=a("strong"),Apr=o("roberta"),Lpr=o(" \u2014 "),SG=a("a"),Bpr=o("FlaxRobertaForMaskedLM"),xpr=o(" (RoBERTa model)"),kpr=l(),pM=a("li"),Bpe=a("strong"),Rpr=o("roformer"),Spr=o(" \u2014 "),PG=a("a"),Ppr=o("FlaxRoFormerForMaskedLM"),$pr=o(" (RoFormer model)"),Ipr=l(),_M=a("li"),xpe=a("strong"),jpr=o("t5"),Npr=o(" \u2014 "),$G=a("a"),Dpr=o("FlaxT5ForConditionalGeneration"),qpr=o(" (T5 model)"),Gpr=l(),bM=a("li"),kpe=a("strong"),Opr=o("wav2vec2"),Xpr=o(" \u2014 "),IG=a("a"),zpr=o("FlaxWav2Vec2ForPreTraining"),Vpr=o(" (Wav2Vec2 model)"),Wpr=l(),Rpe=a("p"),Qpr=o("Examples:"),Hpr=l(),m(H0.$$.fragment),V7e=l(),Ic=a("h2"),vM=a("a"),Spe=a("span"),m(U0.$$.fragment),Upr=l(),Ppe=a("span"),Jpr=o("FlaxAutoModelForMaskedLM"),W7e=l(),yr=a("div"),m(J0.$$.fragment),Ypr=l(),jc=a("p"),Kpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),$pe=a("code"),Zpr=o("from_pretrained()"),e_r=o("class method or the "),Ipe=a("code"),o_r=o("from_config()"),r_r=o(`class
method.`),t_r=l(),Y0=a("p"),a_r=o("This class cannot be instantiated directly using "),jpe=a("code"),s_r=o("__init__()"),n_r=o(" (throws an error)."),l_r=l(),_t=a("div"),m(K0.$$.fragment),i_r=l(),Npe=a("p"),d_r=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),c_r=l(),Nc=a("p"),m_r=o(`Note:
Loading a model from its configuration file does `),Dpe=a("strong"),f_r=o("not"),g_r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qpe=a("code"),h_r=o("from_pretrained()"),u_r=o("to load the model weights."),p_r=l(),Gpe=a("p"),__r=o("Examples:"),b_r=l(),m(Z0.$$.fragment),v_r=l(),Ao=a("div"),m(e6.$$.fragment),T_r=l(),Ope=a("p"),F_r=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),C_r=l(),vs=a("p"),M_r=o("The model class to instantiate is selected based on the "),Xpe=a("code"),E_r=o("model_type"),y_r=o(` property of the config object (either
passed as an argument or loaded from `),zpe=a("code"),w_r=o("pretrained_model_name_or_path"),A_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vpe=a("code"),L_r=o("pretrained_model_name_or_path"),B_r=o(":"),x_r=l(),be=a("ul"),TM=a("li"),Wpe=a("strong"),k_r=o("albert"),R_r=o(" \u2014 "),jG=a("a"),S_r=o("FlaxAlbertForMaskedLM"),P_r=o(" (ALBERT model)"),$_r=l(),FM=a("li"),Qpe=a("strong"),I_r=o("bart"),j_r=o(" \u2014 "),NG=a("a"),N_r=o("FlaxBartForConditionalGeneration"),D_r=o(" (BART model)"),q_r=l(),CM=a("li"),Hpe=a("strong"),G_r=o("bert"),O_r=o(" \u2014 "),DG=a("a"),X_r=o("FlaxBertForMaskedLM"),z_r=o(" (BERT model)"),V_r=l(),MM=a("li"),Upe=a("strong"),W_r=o("big_bird"),Q_r=o(" \u2014 "),qG=a("a"),H_r=o("FlaxBigBirdForMaskedLM"),U_r=o(" (BigBird model)"),J_r=l(),EM=a("li"),Jpe=a("strong"),Y_r=o("distilbert"),K_r=o(" \u2014 "),GG=a("a"),Z_r=o("FlaxDistilBertForMaskedLM"),ebr=o(" (DistilBERT model)"),obr=l(),yM=a("li"),Ype=a("strong"),rbr=o("electra"),tbr=o(" \u2014 "),OG=a("a"),abr=o("FlaxElectraForMaskedLM"),sbr=o(" (ELECTRA model)"),nbr=l(),wM=a("li"),Kpe=a("strong"),lbr=o("mbart"),ibr=o(" \u2014 "),XG=a("a"),dbr=o("FlaxMBartForConditionalGeneration"),cbr=o(" (mBART model)"),mbr=l(),AM=a("li"),Zpe=a("strong"),fbr=o("roberta"),gbr=o(" \u2014 "),zG=a("a"),hbr=o("FlaxRobertaForMaskedLM"),ubr=o(" (RoBERTa model)"),pbr=l(),LM=a("li"),e_e=a("strong"),_br=o("roformer"),bbr=o(" \u2014 "),VG=a("a"),vbr=o("FlaxRoFormerForMaskedLM"),Tbr=o(" (RoFormer model)"),Fbr=l(),o_e=a("p"),Cbr=o("Examples:"),Mbr=l(),m(o6.$$.fragment),Q7e=l(),Dc=a("h2"),BM=a("a"),r_e=a("span"),m(r6.$$.fragment),Ebr=l(),t_e=a("span"),ybr=o("FlaxAutoModelForSeq2SeqLM"),H7e=l(),wr=a("div"),m(t6.$$.fragment),wbr=l(),qc=a("p"),Abr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),a_e=a("code"),Lbr=o("from_pretrained()"),Bbr=o("class method or the "),s_e=a("code"),xbr=o("from_config()"),kbr=o(`class
method.`),Rbr=l(),a6=a("p"),Sbr=o("This class cannot be instantiated directly using "),n_e=a("code"),Pbr=o("__init__()"),$br=o(" (throws an error)."),Ibr=l(),bt=a("div"),m(s6.$$.fragment),jbr=l(),l_e=a("p"),Nbr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Dbr=l(),Gc=a("p"),qbr=o(`Note:
Loading a model from its configuration file does `),i_e=a("strong"),Gbr=o("not"),Obr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),d_e=a("code"),Xbr=o("from_pretrained()"),zbr=o("to load the model weights."),Vbr=l(),c_e=a("p"),Wbr=o("Examples:"),Qbr=l(),m(n6.$$.fragment),Hbr=l(),Lo=a("div"),m(l6.$$.fragment),Ubr=l(),m_e=a("p"),Jbr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Ybr=l(),Ts=a("p"),Kbr=o("The model class to instantiate is selected based on the "),f_e=a("code"),Zbr=o("model_type"),e2r=o(` property of the config object (either
passed as an argument or loaded from `),g_e=a("code"),o2r=o("pretrained_model_name_or_path"),r2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h_e=a("code"),t2r=o("pretrained_model_name_or_path"),a2r=o(":"),s2r=l(),ve=a("ul"),xM=a("li"),u_e=a("strong"),n2r=o("bart"),l2r=o(" \u2014 "),WG=a("a"),i2r=o("FlaxBartForConditionalGeneration"),d2r=o(" (BART model)"),c2r=l(),kM=a("li"),p_e=a("strong"),m2r=o("blenderbot"),f2r=o(" \u2014 "),QG=a("a"),g2r=o("FlaxBlenderbotForConditionalGeneration"),h2r=o(" (Blenderbot model)"),u2r=l(),RM=a("li"),__e=a("strong"),p2r=o("blenderbot-small"),_2r=o(" \u2014 "),HG=a("a"),b2r=o("FlaxBlenderbotSmallForConditionalGeneration"),v2r=o(" (BlenderbotSmall model)"),T2r=l(),SM=a("li"),b_e=a("strong"),F2r=o("encoder-decoder"),C2r=o(" \u2014 "),UG=a("a"),M2r=o("FlaxEncoderDecoderModel"),E2r=o(" (Encoder decoder model)"),y2r=l(),PM=a("li"),v_e=a("strong"),w2r=o("marian"),A2r=o(" \u2014 "),JG=a("a"),L2r=o("FlaxMarianMTModel"),B2r=o(" (Marian model)"),x2r=l(),$M=a("li"),T_e=a("strong"),k2r=o("mbart"),R2r=o(" \u2014 "),YG=a("a"),S2r=o("FlaxMBartForConditionalGeneration"),P2r=o(" (mBART model)"),$2r=l(),IM=a("li"),F_e=a("strong"),I2r=o("mt5"),j2r=o(" \u2014 "),KG=a("a"),N2r=o("FlaxMT5ForConditionalGeneration"),D2r=o(" (mT5 model)"),q2r=l(),jM=a("li"),C_e=a("strong"),G2r=o("pegasus"),O2r=o(" \u2014 "),ZG=a("a"),X2r=o("FlaxPegasusForConditionalGeneration"),z2r=o(" (Pegasus model)"),V2r=l(),NM=a("li"),M_e=a("strong"),W2r=o("t5"),Q2r=o(" \u2014 "),eO=a("a"),H2r=o("FlaxT5ForConditionalGeneration"),U2r=o(" (T5 model)"),J2r=l(),E_e=a("p"),Y2r=o("Examples:"),K2r=l(),m(i6.$$.fragment),U7e=l(),Oc=a("h2"),DM=a("a"),y_e=a("span"),m(d6.$$.fragment),Z2r=l(),w_e=a("span"),evr=o("FlaxAutoModelForSequenceClassification"),J7e=l(),Ar=a("div"),m(c6.$$.fragment),ovr=l(),Xc=a("p"),rvr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),A_e=a("code"),tvr=o("from_pretrained()"),avr=o("class method or the "),L_e=a("code"),svr=o("from_config()"),nvr=o(`class
method.`),lvr=l(),m6=a("p"),ivr=o("This class cannot be instantiated directly using "),B_e=a("code"),dvr=o("__init__()"),cvr=o(" (throws an error)."),mvr=l(),vt=a("div"),m(f6.$$.fragment),fvr=l(),x_e=a("p"),gvr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),hvr=l(),zc=a("p"),uvr=o(`Note:
Loading a model from its configuration file does `),k_e=a("strong"),pvr=o("not"),_vr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),R_e=a("code"),bvr=o("from_pretrained()"),vvr=o("to load the model weights."),Tvr=l(),S_e=a("p"),Fvr=o("Examples:"),Cvr=l(),m(g6.$$.fragment),Mvr=l(),Bo=a("div"),m(h6.$$.fragment),Evr=l(),P_e=a("p"),yvr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),wvr=l(),Fs=a("p"),Avr=o("The model class to instantiate is selected based on the "),$_e=a("code"),Lvr=o("model_type"),Bvr=o(` property of the config object (either
passed as an argument or loaded from `),I_e=a("code"),xvr=o("pretrained_model_name_or_path"),kvr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j_e=a("code"),Rvr=o("pretrained_model_name_or_path"),Svr=o(":"),Pvr=l(),Te=a("ul"),qM=a("li"),N_e=a("strong"),$vr=o("albert"),Ivr=o(" \u2014 "),oO=a("a"),jvr=o("FlaxAlbertForSequenceClassification"),Nvr=o(" (ALBERT model)"),Dvr=l(),GM=a("li"),D_e=a("strong"),qvr=o("bart"),Gvr=o(" \u2014 "),rO=a("a"),Ovr=o("FlaxBartForSequenceClassification"),Xvr=o(" (BART model)"),zvr=l(),OM=a("li"),q_e=a("strong"),Vvr=o("bert"),Wvr=o(" \u2014 "),tO=a("a"),Qvr=o("FlaxBertForSequenceClassification"),Hvr=o(" (BERT model)"),Uvr=l(),XM=a("li"),G_e=a("strong"),Jvr=o("big_bird"),Yvr=o(" \u2014 "),aO=a("a"),Kvr=o("FlaxBigBirdForSequenceClassification"),Zvr=o(" (BigBird model)"),eTr=l(),zM=a("li"),O_e=a("strong"),oTr=o("distilbert"),rTr=o(" \u2014 "),sO=a("a"),tTr=o("FlaxDistilBertForSequenceClassification"),aTr=o(" (DistilBERT model)"),sTr=l(),VM=a("li"),X_e=a("strong"),nTr=o("electra"),lTr=o(" \u2014 "),nO=a("a"),iTr=o("FlaxElectraForSequenceClassification"),dTr=o(" (ELECTRA model)"),cTr=l(),WM=a("li"),z_e=a("strong"),mTr=o("mbart"),fTr=o(" \u2014 "),lO=a("a"),gTr=o("FlaxMBartForSequenceClassification"),hTr=o(" (mBART model)"),uTr=l(),QM=a("li"),V_e=a("strong"),pTr=o("roberta"),_Tr=o(" \u2014 "),iO=a("a"),bTr=o("FlaxRobertaForSequenceClassification"),vTr=o(" (RoBERTa model)"),TTr=l(),HM=a("li"),W_e=a("strong"),FTr=o("roformer"),CTr=o(" \u2014 "),dO=a("a"),MTr=o("FlaxRoFormerForSequenceClassification"),ETr=o(" (RoFormer model)"),yTr=l(),Q_e=a("p"),wTr=o("Examples:"),ATr=l(),m(u6.$$.fragment),Y7e=l(),Vc=a("h2"),UM=a("a"),H_e=a("span"),m(p6.$$.fragment),LTr=l(),U_e=a("span"),BTr=o("FlaxAutoModelForQuestionAnswering"),K7e=l(),Lr=a("div"),m(_6.$$.fragment),xTr=l(),Wc=a("p"),kTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),J_e=a("code"),RTr=o("from_pretrained()"),STr=o("class method or the "),Y_e=a("code"),PTr=o("from_config()"),$Tr=o(`class
method.`),ITr=l(),b6=a("p"),jTr=o("This class cannot be instantiated directly using "),K_e=a("code"),NTr=o("__init__()"),DTr=o(" (throws an error)."),qTr=l(),Tt=a("div"),m(v6.$$.fragment),GTr=l(),Z_e=a("p"),OTr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),XTr=l(),Qc=a("p"),zTr=o(`Note:
Loading a model from its configuration file does `),ebe=a("strong"),VTr=o("not"),WTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),obe=a("code"),QTr=o("from_pretrained()"),HTr=o("to load the model weights."),UTr=l(),rbe=a("p"),JTr=o("Examples:"),YTr=l(),m(T6.$$.fragment),KTr=l(),xo=a("div"),m(F6.$$.fragment),ZTr=l(),tbe=a("p"),e1r=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),o1r=l(),Cs=a("p"),r1r=o("The model class to instantiate is selected based on the "),abe=a("code"),t1r=o("model_type"),a1r=o(` property of the config object (either
passed as an argument or loaded from `),sbe=a("code"),s1r=o("pretrained_model_name_or_path"),n1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nbe=a("code"),l1r=o("pretrained_model_name_or_path"),i1r=o(":"),d1r=l(),Fe=a("ul"),JM=a("li"),lbe=a("strong"),c1r=o("albert"),m1r=o(" \u2014 "),cO=a("a"),f1r=o("FlaxAlbertForQuestionAnswering"),g1r=o(" (ALBERT model)"),h1r=l(),YM=a("li"),ibe=a("strong"),u1r=o("bart"),p1r=o(" \u2014 "),mO=a("a"),_1r=o("FlaxBartForQuestionAnswering"),b1r=o(" (BART model)"),v1r=l(),KM=a("li"),dbe=a("strong"),T1r=o("bert"),F1r=o(" \u2014 "),fO=a("a"),C1r=o("FlaxBertForQuestionAnswering"),M1r=o(" (BERT model)"),E1r=l(),ZM=a("li"),cbe=a("strong"),y1r=o("big_bird"),w1r=o(" \u2014 "),gO=a("a"),A1r=o("FlaxBigBirdForQuestionAnswering"),L1r=o(" (BigBird model)"),B1r=l(),eE=a("li"),mbe=a("strong"),x1r=o("distilbert"),k1r=o(" \u2014 "),hO=a("a"),R1r=o("FlaxDistilBertForQuestionAnswering"),S1r=o(" (DistilBERT model)"),P1r=l(),oE=a("li"),fbe=a("strong"),$1r=o("electra"),I1r=o(" \u2014 "),uO=a("a"),j1r=o("FlaxElectraForQuestionAnswering"),N1r=o(" (ELECTRA model)"),D1r=l(),rE=a("li"),gbe=a("strong"),q1r=o("mbart"),G1r=o(" \u2014 "),pO=a("a"),O1r=o("FlaxMBartForQuestionAnswering"),X1r=o(" (mBART model)"),z1r=l(),tE=a("li"),hbe=a("strong"),V1r=o("roberta"),W1r=o(" \u2014 "),_O=a("a"),Q1r=o("FlaxRobertaForQuestionAnswering"),H1r=o(" (RoBERTa model)"),U1r=l(),aE=a("li"),ube=a("strong"),J1r=o("roformer"),Y1r=o(" \u2014 "),bO=a("a"),K1r=o("FlaxRoFormerForQuestionAnswering"),Z1r=o(" (RoFormer model)"),eFr=l(),pbe=a("p"),oFr=o("Examples:"),rFr=l(),m(C6.$$.fragment),Z7e=l(),Hc=a("h2"),sE=a("a"),_be=a("span"),m(M6.$$.fragment),tFr=l(),bbe=a("span"),aFr=o("FlaxAutoModelForTokenClassification"),e8e=l(),Br=a("div"),m(E6.$$.fragment),sFr=l(),Uc=a("p"),nFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),vbe=a("code"),lFr=o("from_pretrained()"),iFr=o("class method or the "),Tbe=a("code"),dFr=o("from_config()"),cFr=o(`class
method.`),mFr=l(),y6=a("p"),fFr=o("This class cannot be instantiated directly using "),Fbe=a("code"),gFr=o("__init__()"),hFr=o(" (throws an error)."),uFr=l(),Ft=a("div"),m(w6.$$.fragment),pFr=l(),Cbe=a("p"),_Fr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),bFr=l(),Jc=a("p"),vFr=o(`Note:
Loading a model from its configuration file does `),Mbe=a("strong"),TFr=o("not"),FFr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ebe=a("code"),CFr=o("from_pretrained()"),MFr=o("to load the model weights."),EFr=l(),ybe=a("p"),yFr=o("Examples:"),wFr=l(),m(A6.$$.fragment),AFr=l(),ko=a("div"),m(L6.$$.fragment),LFr=l(),wbe=a("p"),BFr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),xFr=l(),Ms=a("p"),kFr=o("The model class to instantiate is selected based on the "),Abe=a("code"),RFr=o("model_type"),SFr=o(` property of the config object (either
passed as an argument or loaded from `),Lbe=a("code"),PFr=o("pretrained_model_name_or_path"),$Fr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bbe=a("code"),IFr=o("pretrained_model_name_or_path"),jFr=o(":"),NFr=l(),ao=a("ul"),nE=a("li"),xbe=a("strong"),DFr=o("albert"),qFr=o(" \u2014 "),vO=a("a"),GFr=o("FlaxAlbertForTokenClassification"),OFr=o(" (ALBERT model)"),XFr=l(),lE=a("li"),kbe=a("strong"),zFr=o("bert"),VFr=o(" \u2014 "),TO=a("a"),WFr=o("FlaxBertForTokenClassification"),QFr=o(" (BERT model)"),HFr=l(),iE=a("li"),Rbe=a("strong"),UFr=o("big_bird"),JFr=o(" \u2014 "),FO=a("a"),YFr=o("FlaxBigBirdForTokenClassification"),KFr=o(" (BigBird model)"),ZFr=l(),dE=a("li"),Sbe=a("strong"),eCr=o("distilbert"),oCr=o(" \u2014 "),CO=a("a"),rCr=o("FlaxDistilBertForTokenClassification"),tCr=o(" (DistilBERT model)"),aCr=l(),cE=a("li"),Pbe=a("strong"),sCr=o("electra"),nCr=o(" \u2014 "),MO=a("a"),lCr=o("FlaxElectraForTokenClassification"),iCr=o(" (ELECTRA model)"),dCr=l(),mE=a("li"),$be=a("strong"),cCr=o("roberta"),mCr=o(" \u2014 "),EO=a("a"),fCr=o("FlaxRobertaForTokenClassification"),gCr=o(" (RoBERTa model)"),hCr=l(),fE=a("li"),Ibe=a("strong"),uCr=o("roformer"),pCr=o(" \u2014 "),yO=a("a"),_Cr=o("FlaxRoFormerForTokenClassification"),bCr=o(" (RoFormer model)"),vCr=l(),jbe=a("p"),TCr=o("Examples:"),FCr=l(),m(B6.$$.fragment),o8e=l(),Yc=a("h2"),gE=a("a"),Nbe=a("span"),m(x6.$$.fragment),CCr=l(),Dbe=a("span"),MCr=o("FlaxAutoModelForMultipleChoice"),r8e=l(),xr=a("div"),m(k6.$$.fragment),ECr=l(),Kc=a("p"),yCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),qbe=a("code"),wCr=o("from_pretrained()"),ACr=o("class method or the "),Gbe=a("code"),LCr=o("from_config()"),BCr=o(`class
method.`),xCr=l(),R6=a("p"),kCr=o("This class cannot be instantiated directly using "),Obe=a("code"),RCr=o("__init__()"),SCr=o(" (throws an error)."),PCr=l(),Ct=a("div"),m(S6.$$.fragment),$Cr=l(),Xbe=a("p"),ICr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),jCr=l(),Zc=a("p"),NCr=o(`Note:
Loading a model from its configuration file does `),zbe=a("strong"),DCr=o("not"),qCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vbe=a("code"),GCr=o("from_pretrained()"),OCr=o("to load the model weights."),XCr=l(),Wbe=a("p"),zCr=o("Examples:"),VCr=l(),m(P6.$$.fragment),WCr=l(),Ro=a("div"),m($6.$$.fragment),QCr=l(),Qbe=a("p"),HCr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),UCr=l(),Es=a("p"),JCr=o("The model class to instantiate is selected based on the "),Hbe=a("code"),YCr=o("model_type"),KCr=o(` property of the config object (either
passed as an argument or loaded from `),Ube=a("code"),ZCr=o("pretrained_model_name_or_path"),e4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jbe=a("code"),o4r=o("pretrained_model_name_or_path"),r4r=o(":"),t4r=l(),so=a("ul"),hE=a("li"),Ybe=a("strong"),a4r=o("albert"),s4r=o(" \u2014 "),wO=a("a"),n4r=o("FlaxAlbertForMultipleChoice"),l4r=o(" (ALBERT model)"),i4r=l(),uE=a("li"),Kbe=a("strong"),d4r=o("bert"),c4r=o(" \u2014 "),AO=a("a"),m4r=o("FlaxBertForMultipleChoice"),f4r=o(" (BERT model)"),g4r=l(),pE=a("li"),Zbe=a("strong"),h4r=o("big_bird"),u4r=o(" \u2014 "),LO=a("a"),p4r=o("FlaxBigBirdForMultipleChoice"),_4r=o(" (BigBird model)"),b4r=l(),_E=a("li"),e2e=a("strong"),v4r=o("distilbert"),T4r=o(" \u2014 "),BO=a("a"),F4r=o("FlaxDistilBertForMultipleChoice"),C4r=o(" (DistilBERT model)"),M4r=l(),bE=a("li"),o2e=a("strong"),E4r=o("electra"),y4r=o(" \u2014 "),xO=a("a"),w4r=o("FlaxElectraForMultipleChoice"),A4r=o(" (ELECTRA model)"),L4r=l(),vE=a("li"),r2e=a("strong"),B4r=o("roberta"),x4r=o(" \u2014 "),kO=a("a"),k4r=o("FlaxRobertaForMultipleChoice"),R4r=o(" (RoBERTa model)"),S4r=l(),TE=a("li"),t2e=a("strong"),P4r=o("roformer"),$4r=o(" \u2014 "),RO=a("a"),I4r=o("FlaxRoFormerForMultipleChoice"),j4r=o(" (RoFormer model)"),N4r=l(),a2e=a("p"),D4r=o("Examples:"),q4r=l(),m(I6.$$.fragment),t8e=l(),em=a("h2"),FE=a("a"),s2e=a("span"),m(j6.$$.fragment),G4r=l(),n2e=a("span"),O4r=o("FlaxAutoModelForNextSentencePrediction"),a8e=l(),kr=a("div"),m(N6.$$.fragment),X4r=l(),om=a("p"),z4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),l2e=a("code"),V4r=o("from_pretrained()"),W4r=o("class method or the "),i2e=a("code"),Q4r=o("from_config()"),H4r=o(`class
method.`),U4r=l(),D6=a("p"),J4r=o("This class cannot be instantiated directly using "),d2e=a("code"),Y4r=o("__init__()"),K4r=o(" (throws an error)."),Z4r=l(),Mt=a("div"),m(q6.$$.fragment),eMr=l(),c2e=a("p"),oMr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),rMr=l(),rm=a("p"),tMr=o(`Note:
Loading a model from its configuration file does `),m2e=a("strong"),aMr=o("not"),sMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),f2e=a("code"),nMr=o("from_pretrained()"),lMr=o("to load the model weights."),iMr=l(),g2e=a("p"),dMr=o("Examples:"),cMr=l(),m(G6.$$.fragment),mMr=l(),So=a("div"),m(O6.$$.fragment),fMr=l(),h2e=a("p"),gMr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),hMr=l(),ys=a("p"),uMr=o("The model class to instantiate is selected based on the "),u2e=a("code"),pMr=o("model_type"),_Mr=o(` property of the config object (either
passed as an argument or loaded from `),p2e=a("code"),bMr=o("pretrained_model_name_or_path"),vMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_2e=a("code"),TMr=o("pretrained_model_name_or_path"),FMr=o(":"),CMr=l(),b2e=a("ul"),CE=a("li"),v2e=a("strong"),MMr=o("bert"),EMr=o(" \u2014 "),SO=a("a"),yMr=o("FlaxBertForNextSentencePrediction"),wMr=o(" (BERT model)"),AMr=l(),T2e=a("p"),LMr=o("Examples:"),BMr=l(),m(X6.$$.fragment),s8e=l(),tm=a("h2"),ME=a("a"),F2e=a("span"),m(z6.$$.fragment),xMr=l(),C2e=a("span"),kMr=o("FlaxAutoModelForImageClassification"),n8e=l(),Rr=a("div"),m(V6.$$.fragment),RMr=l(),am=a("p"),SMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),M2e=a("code"),PMr=o("from_pretrained()"),$Mr=o("class method or the "),E2e=a("code"),IMr=o("from_config()"),jMr=o(`class
method.`),NMr=l(),W6=a("p"),DMr=o("This class cannot be instantiated directly using "),y2e=a("code"),qMr=o("__init__()"),GMr=o(" (throws an error)."),OMr=l(),Et=a("div"),m(Q6.$$.fragment),XMr=l(),w2e=a("p"),zMr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),VMr=l(),sm=a("p"),WMr=o(`Note:
Loading a model from its configuration file does `),A2e=a("strong"),QMr=o("not"),HMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),L2e=a("code"),UMr=o("from_pretrained()"),JMr=o("to load the model weights."),YMr=l(),B2e=a("p"),KMr=o("Examples:"),ZMr=l(),m(H6.$$.fragment),eEr=l(),Po=a("div"),m(U6.$$.fragment),oEr=l(),x2e=a("p"),rEr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),tEr=l(),ws=a("p"),aEr=o("The model class to instantiate is selected based on the "),k2e=a("code"),sEr=o("model_type"),nEr=o(` property of the config object (either
passed as an argument or loaded from `),R2e=a("code"),lEr=o("pretrained_model_name_or_path"),iEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S2e=a("code"),dEr=o("pretrained_model_name_or_path"),cEr=o(":"),mEr=l(),J6=a("ul"),EE=a("li"),P2e=a("strong"),fEr=o("beit"),gEr=o(" \u2014 "),PO=a("a"),hEr=o("FlaxBeitForImageClassification"),uEr=o(" (BEiT model)"),pEr=l(),yE=a("li"),$2e=a("strong"),_Er=o("vit"),bEr=o(" \u2014 "),$O=a("a"),vEr=o("FlaxViTForImageClassification"),TEr=o(" (ViT model)"),FEr=l(),I2e=a("p"),CEr=o("Examples:"),MEr=l(),m(Y6.$$.fragment),l8e=l(),nm=a("h2"),wE=a("a"),j2e=a("span"),m(K6.$$.fragment),EEr=l(),N2e=a("span"),yEr=o("FlaxAutoModelForVision2Seq"),i8e=l(),Sr=a("div"),m(Z6.$$.fragment),wEr=l(),lm=a("p"),AEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),D2e=a("code"),LEr=o("from_pretrained()"),BEr=o("class method or the "),q2e=a("code"),xEr=o("from_config()"),kEr=o(`class
method.`),REr=l(),eL=a("p"),SEr=o("This class cannot be instantiated directly using "),G2e=a("code"),PEr=o("__init__()"),$Er=o(" (throws an error)."),IEr=l(),yt=a("div"),m(oL.$$.fragment),jEr=l(),O2e=a("p"),NEr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),DEr=l(),im=a("p"),qEr=o(`Note:
Loading a model from its configuration file does `),X2e=a("strong"),GEr=o("not"),OEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),z2e=a("code"),XEr=o("from_pretrained()"),zEr=o("to load the model weights."),VEr=l(),V2e=a("p"),WEr=o("Examples:"),QEr=l(),m(rL.$$.fragment),HEr=l(),$o=a("div"),m(tL.$$.fragment),UEr=l(),W2e=a("p"),JEr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),YEr=l(),As=a("p"),KEr=o("The model class to instantiate is selected based on the "),Q2e=a("code"),ZEr=o("model_type"),e3r=o(` property of the config object (either
passed as an argument or loaded from `),H2e=a("code"),o3r=o("pretrained_model_name_or_path"),r3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U2e=a("code"),t3r=o("pretrained_model_name_or_path"),a3r=o(":"),s3r=l(),J2e=a("ul"),AE=a("li"),Y2e=a("strong"),n3r=o("vision-encoder-decoder"),l3r=o(" \u2014 "),IO=a("a"),i3r=o("FlaxVisionEncoderDecoderModel"),d3r=o(" (Vision Encoder decoder model)"),c3r=l(),K2e=a("p"),m3r=o("Examples:"),f3r=l(),m(aL.$$.fragment),this.h()},l(d){const _=wmt('[data-svelte="svelte-1phssyn"]',document.head);J=s(_,"META",{name:!0,content:!0}),_.forEach(t),Ae=i(d),le=s(d,"H1",{class:!0});var sL=n(le);fe=s(sL,"A",{id:!0,class:!0,href:!0});var Z2e=n(fe);oo=s(Z2e,"SPAN",{});var eve=n(oo);f(ce.$$.fragment,eve),eve.forEach(t),Z2e.forEach(t),_e=i(sL),No=s(sL,"SPAN",{});var h3r=n(No);_i=r(h3r,"Auto Classes"),h3r.forEach(t),sL.forEach(t),cm=i(d),ra=s(d,"P",{});var c8e=n(ra);bi=r(c8e,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),vi=s(c8e,"CODE",{});var u3r=n(vi);w3=r(u3r,"from_pretrained()"),u3r.forEach(t),mm=r(c8e,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),c8e.forEach(t),Ee=i(d),no=s(d,"P",{});var LE=n(no);Ti=r(LE,"Instantiating one of "),Ls=s(LE,"A",{href:!0});var p3r=n(Ls);A3=r(p3r,"AutoConfig"),p3r.forEach(t),Bs=r(LE,", "),xs=s(LE,"A",{href:!0});var _3r=n(xs);L3=r(_3r,"AutoModel"),_3r.forEach(t),Fi=r(LE,`, and
`),ks=s(LE,"A",{href:!0});var b3r=n(ks);B3=r(b3r,"AutoTokenizer"),b3r.forEach(t),Ci=r(LE," will directly create a class of the relevant architecture. For instance"),LE.forEach(t),fm=i(d),f(ka.$$.fragment,d),lo=i(d),ge=s(d,"P",{});var m8e=n(ge);o7=r(m8e,"will create a model that is an instance of "),Mi=s(m8e,"A",{href:!0});var v3r=n(Mi);r7=r(v3r,"BertModel"),v3r.forEach(t),t7=r(m8e,"."),m8e.forEach(t),Do=i(d),Ra=s(d,"P",{});var f8e=n(Ra);a7=r(f8e,"There is one class of "),gm=s(f8e,"CODE",{});var T3r=n(gm);s7=r(T3r,"AutoModel"),T3r.forEach(t),uBe=r(f8e," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),f8e.forEach(t),fLe=i(d),Ei=s(d,"H2",{class:!0});var g8e=n(Ei);hm=s(g8e,"A",{id:!0,class:!0,href:!0});var F3r=n(hm);wz=s(F3r,"SPAN",{});var C3r=n(wz);f(x3.$$.fragment,C3r),C3r.forEach(t),F3r.forEach(t),pBe=i(g8e),Az=s(g8e,"SPAN",{});var M3r=n(Az);_Be=r(M3r,"Extending the Auto Classes"),M3r.forEach(t),g8e.forEach(t),gLe=i(d),Rs=s(d,"P",{});var jO=n(Rs);bBe=r(jO,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),Lz=s(jO,"CODE",{});var E3r=n(Lz);vBe=r(E3r,"NewModel"),E3r.forEach(t),TBe=r(jO,", make sure you have a "),Bz=s(jO,"CODE",{});var y3r=n(Bz);FBe=r(y3r,"NewModelConfig"),y3r.forEach(t),CBe=r(jO,` then you can add those to the auto
classes like this:`),jO.forEach(t),hLe=i(d),f(k3.$$.fragment,d),uLe=i(d),n7=s(d,"P",{});var w3r=n(n7);MBe=r(w3r,"You will then be able to use the auto classes like you would usually do!"),w3r.forEach(t),pLe=i(d),f(um.$$.fragment,d),_Le=i(d),yi=s(d,"H2",{class:!0});var h8e=n(yi);pm=s(h8e,"A",{id:!0,class:!0,href:!0});var A3r=n(pm);xz=s(A3r,"SPAN",{});var L3r=n(xz);f(R3.$$.fragment,L3r),L3r.forEach(t),A3r.forEach(t),EBe=i(h8e),kz=s(h8e,"SPAN",{});var B3r=n(kz);yBe=r(B3r,"AutoConfig"),B3r.forEach(t),h8e.forEach(t),bLe=i(d),qo=s(d,"DIV",{class:!0});var Bn=n(qo);f(S3.$$.fragment,Bn),wBe=i(Bn),P3=s(Bn,"P",{});var u8e=n(P3);ABe=r(u8e,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),l7=s(u8e,"A",{href:!0});var x3r=n(l7);LBe=r(x3r,"from_pretrained()"),x3r.forEach(t),BBe=r(u8e," class method."),u8e.forEach(t),xBe=i(Bn),$3=s(Bn,"P",{});var p8e=n($3);kBe=r(p8e,"This class cannot be instantiated directly using "),Rz=s(p8e,"CODE",{});var k3r=n(Rz);RBe=r(k3r,"__init__()"),k3r.forEach(t),SBe=r(p8e," (throws an error)."),p8e.forEach(t),PBe=i(Bn),io=s(Bn,"DIV",{class:!0});var aa=n(io);f(I3.$$.fragment,aa),$Be=i(aa),Sz=s(aa,"P",{});var R3r=n(Sz);IBe=r(R3r,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),R3r.forEach(t),jBe=i(aa),wi=s(aa,"P",{});var NO=n(wi);NBe=r(NO,"The configuration class to instantiate is selected based on the "),Pz=s(NO,"CODE",{});var S3r=n(Pz);DBe=r(S3r,"model_type"),S3r.forEach(t),qBe=r(NO,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),$z=s(NO,"CODE",{});var P3r=n($z);GBe=r(P3r,"pretrained_model_name_or_path"),P3r.forEach(t),OBe=r(NO,":"),NO.forEach(t),XBe=i(aa),v=s(aa,"UL",{});var T=n(v);_m=s(T,"LI",{});var ove=n(_m);Iz=s(ove,"STRONG",{});var $3r=n(Iz);zBe=r($3r,"albert"),$3r.forEach(t),VBe=r(ove," \u2014 "),i7=s(ove,"A",{href:!0});var I3r=n(i7);WBe=r(I3r,"AlbertConfig"),I3r.forEach(t),QBe=r(ove," (ALBERT model)"),ove.forEach(t),HBe=i(T),bm=s(T,"LI",{});var rve=n(bm);jz=s(rve,"STRONG",{});var j3r=n(jz);UBe=r(j3r,"bart"),j3r.forEach(t),JBe=r(rve," \u2014 "),d7=s(rve,"A",{href:!0});var N3r=n(d7);YBe=r(N3r,"BartConfig"),N3r.forEach(t),KBe=r(rve," (BART model)"),rve.forEach(t),ZBe=i(T),vm=s(T,"LI",{});var tve=n(vm);Nz=s(tve,"STRONG",{});var D3r=n(Nz);exe=r(D3r,"beit"),D3r.forEach(t),oxe=r(tve," \u2014 "),c7=s(tve,"A",{href:!0});var q3r=n(c7);rxe=r(q3r,"BeitConfig"),q3r.forEach(t),txe=r(tve," (BEiT model)"),tve.forEach(t),axe=i(T),Tm=s(T,"LI",{});var ave=n(Tm);Dz=s(ave,"STRONG",{});var G3r=n(Dz);sxe=r(G3r,"bert"),G3r.forEach(t),nxe=r(ave," \u2014 "),m7=s(ave,"A",{href:!0});var O3r=n(m7);lxe=r(O3r,"BertConfig"),O3r.forEach(t),ixe=r(ave," (BERT model)"),ave.forEach(t),dxe=i(T),Fm=s(T,"LI",{});var sve=n(Fm);qz=s(sve,"STRONG",{});var X3r=n(qz);cxe=r(X3r,"bert-generation"),X3r.forEach(t),mxe=r(sve," \u2014 "),f7=s(sve,"A",{href:!0});var z3r=n(f7);fxe=r(z3r,"BertGenerationConfig"),z3r.forEach(t),gxe=r(sve," (Bert Generation model)"),sve.forEach(t),hxe=i(T),Cm=s(T,"LI",{});var nve=n(Cm);Gz=s(nve,"STRONG",{});var V3r=n(Gz);uxe=r(V3r,"big_bird"),V3r.forEach(t),pxe=r(nve," \u2014 "),g7=s(nve,"A",{href:!0});var W3r=n(g7);_xe=r(W3r,"BigBirdConfig"),W3r.forEach(t),bxe=r(nve," (BigBird model)"),nve.forEach(t),vxe=i(T),Mm=s(T,"LI",{});var lve=n(Mm);Oz=s(lve,"STRONG",{});var Q3r=n(Oz);Txe=r(Q3r,"bigbird_pegasus"),Q3r.forEach(t),Fxe=r(lve," \u2014 "),h7=s(lve,"A",{href:!0});var H3r=n(h7);Cxe=r(H3r,"BigBirdPegasusConfig"),H3r.forEach(t),Mxe=r(lve," (BigBirdPegasus model)"),lve.forEach(t),Exe=i(T),Em=s(T,"LI",{});var ive=n(Em);Xz=s(ive,"STRONG",{});var U3r=n(Xz);yxe=r(U3r,"blenderbot"),U3r.forEach(t),wxe=r(ive," \u2014 "),u7=s(ive,"A",{href:!0});var J3r=n(u7);Axe=r(J3r,"BlenderbotConfig"),J3r.forEach(t),Lxe=r(ive," (Blenderbot model)"),ive.forEach(t),Bxe=i(T),ym=s(T,"LI",{});var dve=n(ym);zz=s(dve,"STRONG",{});var Y3r=n(zz);xxe=r(Y3r,"blenderbot-small"),Y3r.forEach(t),kxe=r(dve," \u2014 "),p7=s(dve,"A",{href:!0});var K3r=n(p7);Rxe=r(K3r,"BlenderbotSmallConfig"),K3r.forEach(t),Sxe=r(dve," (BlenderbotSmall model)"),dve.forEach(t),Pxe=i(T),wm=s(T,"LI",{});var cve=n(wm);Vz=s(cve,"STRONG",{});var Z3r=n(Vz);$xe=r(Z3r,"camembert"),Z3r.forEach(t),Ixe=r(cve," \u2014 "),_7=s(cve,"A",{href:!0});var e5r=n(_7);jxe=r(e5r,"CamembertConfig"),e5r.forEach(t),Nxe=r(cve," (CamemBERT model)"),cve.forEach(t),Dxe=i(T),Am=s(T,"LI",{});var mve=n(Am);Wz=s(mve,"STRONG",{});var o5r=n(Wz);qxe=r(o5r,"canine"),o5r.forEach(t),Gxe=r(mve," \u2014 "),b7=s(mve,"A",{href:!0});var r5r=n(b7);Oxe=r(r5r,"CanineConfig"),r5r.forEach(t),Xxe=r(mve," (Canine model)"),mve.forEach(t),zxe=i(T),Lm=s(T,"LI",{});var fve=n(Lm);Qz=s(fve,"STRONG",{});var t5r=n(Qz);Vxe=r(t5r,"clip"),t5r.forEach(t),Wxe=r(fve," \u2014 "),v7=s(fve,"A",{href:!0});var a5r=n(v7);Qxe=r(a5r,"CLIPConfig"),a5r.forEach(t),Hxe=r(fve," (CLIP model)"),fve.forEach(t),Uxe=i(T),Bm=s(T,"LI",{});var gve=n(Bm);Hz=s(gve,"STRONG",{});var s5r=n(Hz);Jxe=r(s5r,"convbert"),s5r.forEach(t),Yxe=r(gve," \u2014 "),T7=s(gve,"A",{href:!0});var n5r=n(T7);Kxe=r(n5r,"ConvBertConfig"),n5r.forEach(t),Zxe=r(gve," (ConvBERT model)"),gve.forEach(t),eke=i(T),xm=s(T,"LI",{});var hve=n(xm);Uz=s(hve,"STRONG",{});var l5r=n(Uz);oke=r(l5r,"convnext"),l5r.forEach(t),rke=r(hve," \u2014 "),F7=s(hve,"A",{href:!0});var i5r=n(F7);tke=r(i5r,"ConvNextConfig"),i5r.forEach(t),ake=r(hve," (ConvNext model)"),hve.forEach(t),ske=i(T),km=s(T,"LI",{});var uve=n(km);Jz=s(uve,"STRONG",{});var d5r=n(Jz);nke=r(d5r,"ctrl"),d5r.forEach(t),lke=r(uve," \u2014 "),C7=s(uve,"A",{href:!0});var c5r=n(C7);ike=r(c5r,"CTRLConfig"),c5r.forEach(t),dke=r(uve," (CTRL model)"),uve.forEach(t),cke=i(T),Rm=s(T,"LI",{});var pve=n(Rm);Yz=s(pve,"STRONG",{});var m5r=n(Yz);mke=r(m5r,"deberta"),m5r.forEach(t),fke=r(pve," \u2014 "),M7=s(pve,"A",{href:!0});var f5r=n(M7);gke=r(f5r,"DebertaConfig"),f5r.forEach(t),hke=r(pve," (DeBERTa model)"),pve.forEach(t),uke=i(T),Sm=s(T,"LI",{});var _ve=n(Sm);Kz=s(_ve,"STRONG",{});var g5r=n(Kz);pke=r(g5r,"deberta-v2"),g5r.forEach(t),_ke=r(_ve," \u2014 "),E7=s(_ve,"A",{href:!0});var h5r=n(E7);bke=r(h5r,"DebertaV2Config"),h5r.forEach(t),vke=r(_ve," (DeBERTa-v2 model)"),_ve.forEach(t),Tke=i(T),Pm=s(T,"LI",{});var bve=n(Pm);Zz=s(bve,"STRONG",{});var u5r=n(Zz);Fke=r(u5r,"deit"),u5r.forEach(t),Cke=r(bve," \u2014 "),y7=s(bve,"A",{href:!0});var p5r=n(y7);Mke=r(p5r,"DeiTConfig"),p5r.forEach(t),Eke=r(bve," (DeiT model)"),bve.forEach(t),yke=i(T),$m=s(T,"LI",{});var vve=n($m);eV=s(vve,"STRONG",{});var _5r=n(eV);wke=r(_5r,"detr"),_5r.forEach(t),Ake=r(vve," \u2014 "),w7=s(vve,"A",{href:!0});var b5r=n(w7);Lke=r(b5r,"DetrConfig"),b5r.forEach(t),Bke=r(vve," (DETR model)"),vve.forEach(t),xke=i(T),Im=s(T,"LI",{});var Tve=n(Im);oV=s(Tve,"STRONG",{});var v5r=n(oV);kke=r(v5r,"distilbert"),v5r.forEach(t),Rke=r(Tve," \u2014 "),A7=s(Tve,"A",{href:!0});var T5r=n(A7);Ske=r(T5r,"DistilBertConfig"),T5r.forEach(t),Pke=r(Tve," (DistilBERT model)"),Tve.forEach(t),$ke=i(T),jm=s(T,"LI",{});var Fve=n(jm);rV=s(Fve,"STRONG",{});var F5r=n(rV);Ike=r(F5r,"dpr"),F5r.forEach(t),jke=r(Fve," \u2014 "),L7=s(Fve,"A",{href:!0});var C5r=n(L7);Nke=r(C5r,"DPRConfig"),C5r.forEach(t),Dke=r(Fve," (DPR model)"),Fve.forEach(t),qke=i(T),Nm=s(T,"LI",{});var Cve=n(Nm);tV=s(Cve,"STRONG",{});var M5r=n(tV);Gke=r(M5r,"electra"),M5r.forEach(t),Oke=r(Cve," \u2014 "),B7=s(Cve,"A",{href:!0});var E5r=n(B7);Xke=r(E5r,"ElectraConfig"),E5r.forEach(t),zke=r(Cve," (ELECTRA model)"),Cve.forEach(t),Vke=i(T),Dm=s(T,"LI",{});var Mve=n(Dm);aV=s(Mve,"STRONG",{});var y5r=n(aV);Wke=r(y5r,"encoder-decoder"),y5r.forEach(t),Qke=r(Mve," \u2014 "),x7=s(Mve,"A",{href:!0});var w5r=n(x7);Hke=r(w5r,"EncoderDecoderConfig"),w5r.forEach(t),Uke=r(Mve," (Encoder decoder model)"),Mve.forEach(t),Jke=i(T),qm=s(T,"LI",{});var Eve=n(qm);sV=s(Eve,"STRONG",{});var A5r=n(sV);Yke=r(A5r,"flaubert"),A5r.forEach(t),Kke=r(Eve," \u2014 "),k7=s(Eve,"A",{href:!0});var L5r=n(k7);Zke=r(L5r,"FlaubertConfig"),L5r.forEach(t),eRe=r(Eve," (FlauBERT model)"),Eve.forEach(t),oRe=i(T),Gm=s(T,"LI",{});var yve=n(Gm);nV=s(yve,"STRONG",{});var B5r=n(nV);rRe=r(B5r,"fnet"),B5r.forEach(t),tRe=r(yve," \u2014 "),R7=s(yve,"A",{href:!0});var x5r=n(R7);aRe=r(x5r,"FNetConfig"),x5r.forEach(t),sRe=r(yve," (FNet model)"),yve.forEach(t),nRe=i(T),Om=s(T,"LI",{});var wve=n(Om);lV=s(wve,"STRONG",{});var k5r=n(lV);lRe=r(k5r,"fsmt"),k5r.forEach(t),iRe=r(wve," \u2014 "),S7=s(wve,"A",{href:!0});var R5r=n(S7);dRe=r(R5r,"FSMTConfig"),R5r.forEach(t),cRe=r(wve," (FairSeq Machine-Translation model)"),wve.forEach(t),mRe=i(T),Xm=s(T,"LI",{});var Ave=n(Xm);iV=s(Ave,"STRONG",{});var S5r=n(iV);fRe=r(S5r,"funnel"),S5r.forEach(t),gRe=r(Ave," \u2014 "),P7=s(Ave,"A",{href:!0});var P5r=n(P7);hRe=r(P5r,"FunnelConfig"),P5r.forEach(t),uRe=r(Ave," (Funnel Transformer model)"),Ave.forEach(t),pRe=i(T),zm=s(T,"LI",{});var Lve=n(zm);dV=s(Lve,"STRONG",{});var $5r=n(dV);_Re=r($5r,"gpt2"),$5r.forEach(t),bRe=r(Lve," \u2014 "),$7=s(Lve,"A",{href:!0});var I5r=n($7);vRe=r(I5r,"GPT2Config"),I5r.forEach(t),TRe=r(Lve," (OpenAI GPT-2 model)"),Lve.forEach(t),FRe=i(T),Vm=s(T,"LI",{});var Bve=n(Vm);cV=s(Bve,"STRONG",{});var j5r=n(cV);CRe=r(j5r,"gpt_neo"),j5r.forEach(t),MRe=r(Bve," \u2014 "),I7=s(Bve,"A",{href:!0});var N5r=n(I7);ERe=r(N5r,"GPTNeoConfig"),N5r.forEach(t),yRe=r(Bve," (GPT Neo model)"),Bve.forEach(t),wRe=i(T),Wm=s(T,"LI",{});var xve=n(Wm);mV=s(xve,"STRONG",{});var D5r=n(mV);ARe=r(D5r,"gptj"),D5r.forEach(t),LRe=r(xve," \u2014 "),j7=s(xve,"A",{href:!0});var q5r=n(j7);BRe=r(q5r,"GPTJConfig"),q5r.forEach(t),xRe=r(xve," (GPT-J model)"),xve.forEach(t),kRe=i(T),Qm=s(T,"LI",{});var kve=n(Qm);fV=s(kve,"STRONG",{});var G5r=n(fV);RRe=r(G5r,"hubert"),G5r.forEach(t),SRe=r(kve," \u2014 "),N7=s(kve,"A",{href:!0});var O5r=n(N7);PRe=r(O5r,"HubertConfig"),O5r.forEach(t),$Re=r(kve," (Hubert model)"),kve.forEach(t),IRe=i(T),Hm=s(T,"LI",{});var Rve=n(Hm);gV=s(Rve,"STRONG",{});var X5r=n(gV);jRe=r(X5r,"ibert"),X5r.forEach(t),NRe=r(Rve," \u2014 "),D7=s(Rve,"A",{href:!0});var z5r=n(D7);DRe=r(z5r,"IBertConfig"),z5r.forEach(t),qRe=r(Rve," (I-BERT model)"),Rve.forEach(t),GRe=i(T),Um=s(T,"LI",{});var Sve=n(Um);hV=s(Sve,"STRONG",{});var V5r=n(hV);ORe=r(V5r,"imagegpt"),V5r.forEach(t),XRe=r(Sve," \u2014 "),q7=s(Sve,"A",{href:!0});var W5r=n(q7);zRe=r(W5r,"ImageGPTConfig"),W5r.forEach(t),VRe=r(Sve," (ImageGPT model)"),Sve.forEach(t),WRe=i(T),Jm=s(T,"LI",{});var Pve=n(Jm);uV=s(Pve,"STRONG",{});var Q5r=n(uV);QRe=r(Q5r,"layoutlm"),Q5r.forEach(t),HRe=r(Pve," \u2014 "),G7=s(Pve,"A",{href:!0});var H5r=n(G7);URe=r(H5r,"LayoutLMConfig"),H5r.forEach(t),JRe=r(Pve," (LayoutLM model)"),Pve.forEach(t),YRe=i(T),Ym=s(T,"LI",{});var $ve=n(Ym);pV=s($ve,"STRONG",{});var U5r=n(pV);KRe=r(U5r,"layoutlmv2"),U5r.forEach(t),ZRe=r($ve," \u2014 "),O7=s($ve,"A",{href:!0});var J5r=n(O7);eSe=r(J5r,"LayoutLMv2Config"),J5r.forEach(t),oSe=r($ve," (LayoutLMv2 model)"),$ve.forEach(t),rSe=i(T),Km=s(T,"LI",{});var Ive=n(Km);_V=s(Ive,"STRONG",{});var Y5r=n(_V);tSe=r(Y5r,"led"),Y5r.forEach(t),aSe=r(Ive," \u2014 "),X7=s(Ive,"A",{href:!0});var K5r=n(X7);sSe=r(K5r,"LEDConfig"),K5r.forEach(t),nSe=r(Ive," (LED model)"),Ive.forEach(t),lSe=i(T),Zm=s(T,"LI",{});var jve=n(Zm);bV=s(jve,"STRONG",{});var Z5r=n(bV);iSe=r(Z5r,"longformer"),Z5r.forEach(t),dSe=r(jve," \u2014 "),z7=s(jve,"A",{href:!0});var eyr=n(z7);cSe=r(eyr,"LongformerConfig"),eyr.forEach(t),mSe=r(jve," (Longformer model)"),jve.forEach(t),fSe=i(T),ef=s(T,"LI",{});var Nve=n(ef);vV=s(Nve,"STRONG",{});var oyr=n(vV);gSe=r(oyr,"luke"),oyr.forEach(t),hSe=r(Nve," \u2014 "),V7=s(Nve,"A",{href:!0});var ryr=n(V7);uSe=r(ryr,"LukeConfig"),ryr.forEach(t),pSe=r(Nve," (LUKE model)"),Nve.forEach(t),_Se=i(T),of=s(T,"LI",{});var Dve=n(of);TV=s(Dve,"STRONG",{});var tyr=n(TV);bSe=r(tyr,"lxmert"),tyr.forEach(t),vSe=r(Dve," \u2014 "),W7=s(Dve,"A",{href:!0});var ayr=n(W7);TSe=r(ayr,"LxmertConfig"),ayr.forEach(t),FSe=r(Dve," (LXMERT model)"),Dve.forEach(t),CSe=i(T),rf=s(T,"LI",{});var qve=n(rf);FV=s(qve,"STRONG",{});var syr=n(FV);MSe=r(syr,"m2m_100"),syr.forEach(t),ESe=r(qve," \u2014 "),Q7=s(qve,"A",{href:!0});var nyr=n(Q7);ySe=r(nyr,"M2M100Config"),nyr.forEach(t),wSe=r(qve," (M2M100 model)"),qve.forEach(t),ASe=i(T),tf=s(T,"LI",{});var Gve=n(tf);CV=s(Gve,"STRONG",{});var lyr=n(CV);LSe=r(lyr,"marian"),lyr.forEach(t),BSe=r(Gve," \u2014 "),H7=s(Gve,"A",{href:!0});var iyr=n(H7);xSe=r(iyr,"MarianConfig"),iyr.forEach(t),kSe=r(Gve," (Marian model)"),Gve.forEach(t),RSe=i(T),af=s(T,"LI",{});var Ove=n(af);MV=s(Ove,"STRONG",{});var dyr=n(MV);SSe=r(dyr,"mbart"),dyr.forEach(t),PSe=r(Ove," \u2014 "),U7=s(Ove,"A",{href:!0});var cyr=n(U7);$Se=r(cyr,"MBartConfig"),cyr.forEach(t),ISe=r(Ove," (mBART model)"),Ove.forEach(t),jSe=i(T),sf=s(T,"LI",{});var Xve=n(sf);EV=s(Xve,"STRONG",{});var myr=n(EV);NSe=r(myr,"megatron-bert"),myr.forEach(t),DSe=r(Xve," \u2014 "),J7=s(Xve,"A",{href:!0});var fyr=n(J7);qSe=r(fyr,"MegatronBertConfig"),fyr.forEach(t),GSe=r(Xve," (MegatronBert model)"),Xve.forEach(t),OSe=i(T),nf=s(T,"LI",{});var zve=n(nf);yV=s(zve,"STRONG",{});var gyr=n(yV);XSe=r(gyr,"mobilebert"),gyr.forEach(t),zSe=r(zve," \u2014 "),Y7=s(zve,"A",{href:!0});var hyr=n(Y7);VSe=r(hyr,"MobileBertConfig"),hyr.forEach(t),WSe=r(zve," (MobileBERT model)"),zve.forEach(t),QSe=i(T),lf=s(T,"LI",{});var Vve=n(lf);wV=s(Vve,"STRONG",{});var uyr=n(wV);HSe=r(uyr,"mpnet"),uyr.forEach(t),USe=r(Vve," \u2014 "),K7=s(Vve,"A",{href:!0});var pyr=n(K7);JSe=r(pyr,"MPNetConfig"),pyr.forEach(t),YSe=r(Vve," (MPNet model)"),Vve.forEach(t),KSe=i(T),df=s(T,"LI",{});var Wve=n(df);AV=s(Wve,"STRONG",{});var _yr=n(AV);ZSe=r(_yr,"mt5"),_yr.forEach(t),ePe=r(Wve," \u2014 "),Z7=s(Wve,"A",{href:!0});var byr=n(Z7);oPe=r(byr,"MT5Config"),byr.forEach(t),rPe=r(Wve," (mT5 model)"),Wve.forEach(t),tPe=i(T),cf=s(T,"LI",{});var Qve=n(cf);LV=s(Qve,"STRONG",{});var vyr=n(LV);aPe=r(vyr,"nystromformer"),vyr.forEach(t),sPe=r(Qve," \u2014 "),e8=s(Qve,"A",{href:!0});var Tyr=n(e8);nPe=r(Tyr,"NystromformerConfig"),Tyr.forEach(t),lPe=r(Qve," (Nystromformer model)"),Qve.forEach(t),iPe=i(T),mf=s(T,"LI",{});var Hve=n(mf);BV=s(Hve,"STRONG",{});var Fyr=n(BV);dPe=r(Fyr,"openai-gpt"),Fyr.forEach(t),cPe=r(Hve," \u2014 "),o8=s(Hve,"A",{href:!0});var Cyr=n(o8);mPe=r(Cyr,"OpenAIGPTConfig"),Cyr.forEach(t),fPe=r(Hve," (OpenAI GPT model)"),Hve.forEach(t),gPe=i(T),ff=s(T,"LI",{});var Uve=n(ff);xV=s(Uve,"STRONG",{});var Myr=n(xV);hPe=r(Myr,"pegasus"),Myr.forEach(t),uPe=r(Uve," \u2014 "),r8=s(Uve,"A",{href:!0});var Eyr=n(r8);pPe=r(Eyr,"PegasusConfig"),Eyr.forEach(t),_Pe=r(Uve," (Pegasus model)"),Uve.forEach(t),bPe=i(T),gf=s(T,"LI",{});var Jve=n(gf);kV=s(Jve,"STRONG",{});var yyr=n(kV);vPe=r(yyr,"perceiver"),yyr.forEach(t),TPe=r(Jve," \u2014 "),t8=s(Jve,"A",{href:!0});var wyr=n(t8);FPe=r(wyr,"PerceiverConfig"),wyr.forEach(t),CPe=r(Jve," (Perceiver model)"),Jve.forEach(t),MPe=i(T),hf=s(T,"LI",{});var Yve=n(hf);RV=s(Yve,"STRONG",{});var Ayr=n(RV);EPe=r(Ayr,"prophetnet"),Ayr.forEach(t),yPe=r(Yve," \u2014 "),a8=s(Yve,"A",{href:!0});var Lyr=n(a8);wPe=r(Lyr,"ProphetNetConfig"),Lyr.forEach(t),APe=r(Yve," (ProphetNet model)"),Yve.forEach(t),LPe=i(T),uf=s(T,"LI",{});var Kve=n(uf);SV=s(Kve,"STRONG",{});var Byr=n(SV);BPe=r(Byr,"qdqbert"),Byr.forEach(t),xPe=r(Kve," \u2014 "),PV=s(Kve,"CODE",{});var xyr=n(PV);kPe=r(xyr,"QDQBertConfig"),xyr.forEach(t),RPe=r(Kve,"(QDQBert model)"),Kve.forEach(t),SPe=i(T),pf=s(T,"LI",{});var Zve=n(pf);$V=s(Zve,"STRONG",{});var kyr=n($V);PPe=r(kyr,"rag"),kyr.forEach(t),$Pe=r(Zve," \u2014 "),s8=s(Zve,"A",{href:!0});var Ryr=n(s8);IPe=r(Ryr,"RagConfig"),Ryr.forEach(t),jPe=r(Zve," (RAG model)"),Zve.forEach(t),NPe=i(T),_f=s(T,"LI",{});var eTe=n(_f);IV=s(eTe,"STRONG",{});var Syr=n(IV);DPe=r(Syr,"realm"),Syr.forEach(t),qPe=r(eTe," \u2014 "),n8=s(eTe,"A",{href:!0});var Pyr=n(n8);GPe=r(Pyr,"RealmConfig"),Pyr.forEach(t),OPe=r(eTe," (Realm model)"),eTe.forEach(t),XPe=i(T),bf=s(T,"LI",{});var oTe=n(bf);jV=s(oTe,"STRONG",{});var $yr=n(jV);zPe=r($yr,"reformer"),$yr.forEach(t),VPe=r(oTe," \u2014 "),l8=s(oTe,"A",{href:!0});var Iyr=n(l8);WPe=r(Iyr,"ReformerConfig"),Iyr.forEach(t),QPe=r(oTe," (Reformer model)"),oTe.forEach(t),HPe=i(T),vf=s(T,"LI",{});var rTe=n(vf);NV=s(rTe,"STRONG",{});var jyr=n(NV);UPe=r(jyr,"rembert"),jyr.forEach(t),JPe=r(rTe," \u2014 "),i8=s(rTe,"A",{href:!0});var Nyr=n(i8);YPe=r(Nyr,"RemBertConfig"),Nyr.forEach(t),KPe=r(rTe," (RemBERT model)"),rTe.forEach(t),ZPe=i(T),Tf=s(T,"LI",{});var tTe=n(Tf);DV=s(tTe,"STRONG",{});var Dyr=n(DV);e$e=r(Dyr,"retribert"),Dyr.forEach(t),o$e=r(tTe," \u2014 "),d8=s(tTe,"A",{href:!0});var qyr=n(d8);r$e=r(qyr,"RetriBertConfig"),qyr.forEach(t),t$e=r(tTe," (RetriBERT model)"),tTe.forEach(t),a$e=i(T),Ff=s(T,"LI",{});var aTe=n(Ff);qV=s(aTe,"STRONG",{});var Gyr=n(qV);s$e=r(Gyr,"roberta"),Gyr.forEach(t),n$e=r(aTe," \u2014 "),c8=s(aTe,"A",{href:!0});var Oyr=n(c8);l$e=r(Oyr,"RobertaConfig"),Oyr.forEach(t),i$e=r(aTe," (RoBERTa model)"),aTe.forEach(t),d$e=i(T),Cf=s(T,"LI",{});var sTe=n(Cf);GV=s(sTe,"STRONG",{});var Xyr=n(GV);c$e=r(Xyr,"roformer"),Xyr.forEach(t),m$e=r(sTe," \u2014 "),m8=s(sTe,"A",{href:!0});var zyr=n(m8);f$e=r(zyr,"RoFormerConfig"),zyr.forEach(t),g$e=r(sTe," (RoFormer model)"),sTe.forEach(t),h$e=i(T),Mf=s(T,"LI",{});var nTe=n(Mf);OV=s(nTe,"STRONG",{});var Vyr=n(OV);u$e=r(Vyr,"segformer"),Vyr.forEach(t),p$e=r(nTe," \u2014 "),f8=s(nTe,"A",{href:!0});var Wyr=n(f8);_$e=r(Wyr,"SegformerConfig"),Wyr.forEach(t),b$e=r(nTe," (SegFormer model)"),nTe.forEach(t),v$e=i(T),Ef=s(T,"LI",{});var lTe=n(Ef);XV=s(lTe,"STRONG",{});var Qyr=n(XV);T$e=r(Qyr,"sew"),Qyr.forEach(t),F$e=r(lTe," \u2014 "),g8=s(lTe,"A",{href:!0});var Hyr=n(g8);C$e=r(Hyr,"SEWConfig"),Hyr.forEach(t),M$e=r(lTe," (SEW model)"),lTe.forEach(t),E$e=i(T),yf=s(T,"LI",{});var iTe=n(yf);zV=s(iTe,"STRONG",{});var Uyr=n(zV);y$e=r(Uyr,"sew-d"),Uyr.forEach(t),w$e=r(iTe," \u2014 "),h8=s(iTe,"A",{href:!0});var Jyr=n(h8);A$e=r(Jyr,"SEWDConfig"),Jyr.forEach(t),L$e=r(iTe," (SEW-D model)"),iTe.forEach(t),B$e=i(T),wf=s(T,"LI",{});var dTe=n(wf);VV=s(dTe,"STRONG",{});var Yyr=n(VV);x$e=r(Yyr,"speech-encoder-decoder"),Yyr.forEach(t),k$e=r(dTe," \u2014 "),u8=s(dTe,"A",{href:!0});var Kyr=n(u8);R$e=r(Kyr,"SpeechEncoderDecoderConfig"),Kyr.forEach(t),S$e=r(dTe," (Speech Encoder decoder model)"),dTe.forEach(t),P$e=i(T),Af=s(T,"LI",{});var cTe=n(Af);WV=s(cTe,"STRONG",{});var Zyr=n(WV);$$e=r(Zyr,"speech_to_text"),Zyr.forEach(t),I$e=r(cTe," \u2014 "),p8=s(cTe,"A",{href:!0});var ewr=n(p8);j$e=r(ewr,"Speech2TextConfig"),ewr.forEach(t),N$e=r(cTe," (Speech2Text model)"),cTe.forEach(t),D$e=i(T),Lf=s(T,"LI",{});var mTe=n(Lf);QV=s(mTe,"STRONG",{});var owr=n(QV);q$e=r(owr,"speech_to_text_2"),owr.forEach(t),G$e=r(mTe," \u2014 "),_8=s(mTe,"A",{href:!0});var rwr=n(_8);O$e=r(rwr,"Speech2Text2Config"),rwr.forEach(t),X$e=r(mTe," (Speech2Text2 model)"),mTe.forEach(t),z$e=i(T),Bf=s(T,"LI",{});var fTe=n(Bf);HV=s(fTe,"STRONG",{});var twr=n(HV);V$e=r(twr,"splinter"),twr.forEach(t),W$e=r(fTe," \u2014 "),b8=s(fTe,"A",{href:!0});var awr=n(b8);Q$e=r(awr,"SplinterConfig"),awr.forEach(t),H$e=r(fTe," (Splinter model)"),fTe.forEach(t),U$e=i(T),xf=s(T,"LI",{});var gTe=n(xf);UV=s(gTe,"STRONG",{});var swr=n(UV);J$e=r(swr,"squeezebert"),swr.forEach(t),Y$e=r(gTe," \u2014 "),v8=s(gTe,"A",{href:!0});var nwr=n(v8);K$e=r(nwr,"SqueezeBertConfig"),nwr.forEach(t),Z$e=r(gTe," (SqueezeBERT model)"),gTe.forEach(t),eIe=i(T),kf=s(T,"LI",{});var hTe=n(kf);JV=s(hTe,"STRONG",{});var lwr=n(JV);oIe=r(lwr,"swin"),lwr.forEach(t),rIe=r(hTe," \u2014 "),T8=s(hTe,"A",{href:!0});var iwr=n(T8);tIe=r(iwr,"SwinConfig"),iwr.forEach(t),aIe=r(hTe," (Swin model)"),hTe.forEach(t),sIe=i(T),Rf=s(T,"LI",{});var uTe=n(Rf);YV=s(uTe,"STRONG",{});var dwr=n(YV);nIe=r(dwr,"t5"),dwr.forEach(t),lIe=r(uTe," \u2014 "),F8=s(uTe,"A",{href:!0});var cwr=n(F8);iIe=r(cwr,"T5Config"),cwr.forEach(t),dIe=r(uTe," (T5 model)"),uTe.forEach(t),cIe=i(T),Sf=s(T,"LI",{});var pTe=n(Sf);KV=s(pTe,"STRONG",{});var mwr=n(KV);mIe=r(mwr,"tapas"),mwr.forEach(t),fIe=r(pTe," \u2014 "),C8=s(pTe,"A",{href:!0});var fwr=n(C8);gIe=r(fwr,"TapasConfig"),fwr.forEach(t),hIe=r(pTe," (TAPAS model)"),pTe.forEach(t),uIe=i(T),Pf=s(T,"LI",{});var _Te=n(Pf);ZV=s(_Te,"STRONG",{});var gwr=n(ZV);pIe=r(gwr,"transfo-xl"),gwr.forEach(t),_Ie=r(_Te," \u2014 "),M8=s(_Te,"A",{href:!0});var hwr=n(M8);bIe=r(hwr,"TransfoXLConfig"),hwr.forEach(t),vIe=r(_Te," (Transformer-XL model)"),_Te.forEach(t),TIe=i(T),$f=s(T,"LI",{});var bTe=n($f);eW=s(bTe,"STRONG",{});var uwr=n(eW);FIe=r(uwr,"trocr"),uwr.forEach(t),CIe=r(bTe," \u2014 "),E8=s(bTe,"A",{href:!0});var pwr=n(E8);MIe=r(pwr,"TrOCRConfig"),pwr.forEach(t),EIe=r(bTe," (TrOCR model)"),bTe.forEach(t),yIe=i(T),If=s(T,"LI",{});var vTe=n(If);oW=s(vTe,"STRONG",{});var _wr=n(oW);wIe=r(_wr,"unispeech"),_wr.forEach(t),AIe=r(vTe," \u2014 "),y8=s(vTe,"A",{href:!0});var bwr=n(y8);LIe=r(bwr,"UniSpeechConfig"),bwr.forEach(t),BIe=r(vTe," (UniSpeech model)"),vTe.forEach(t),xIe=i(T),jf=s(T,"LI",{});var TTe=n(jf);rW=s(TTe,"STRONG",{});var vwr=n(rW);kIe=r(vwr,"unispeech-sat"),vwr.forEach(t),RIe=r(TTe," \u2014 "),w8=s(TTe,"A",{href:!0});var Twr=n(w8);SIe=r(Twr,"UniSpeechSatConfig"),Twr.forEach(t),PIe=r(TTe," (UniSpeechSat model)"),TTe.forEach(t),$Ie=i(T),Nf=s(T,"LI",{});var FTe=n(Nf);tW=s(FTe,"STRONG",{});var Fwr=n(tW);IIe=r(Fwr,"vilt"),Fwr.forEach(t),jIe=r(FTe," \u2014 "),A8=s(FTe,"A",{href:!0});var Cwr=n(A8);NIe=r(Cwr,"ViltConfig"),Cwr.forEach(t),DIe=r(FTe," (ViLT model)"),FTe.forEach(t),qIe=i(T),Df=s(T,"LI",{});var CTe=n(Df);aW=s(CTe,"STRONG",{});var Mwr=n(aW);GIe=r(Mwr,"vision-encoder-decoder"),Mwr.forEach(t),OIe=r(CTe," \u2014 "),L8=s(CTe,"A",{href:!0});var Ewr=n(L8);XIe=r(Ewr,"VisionEncoderDecoderConfig"),Ewr.forEach(t),zIe=r(CTe," (Vision Encoder decoder model)"),CTe.forEach(t),VIe=i(T),qf=s(T,"LI",{});var MTe=n(qf);sW=s(MTe,"STRONG",{});var ywr=n(sW);WIe=r(ywr,"vision-text-dual-encoder"),ywr.forEach(t),QIe=r(MTe," \u2014 "),B8=s(MTe,"A",{href:!0});var wwr=n(B8);HIe=r(wwr,"VisionTextDualEncoderConfig"),wwr.forEach(t),UIe=r(MTe," (VisionTextDualEncoder model)"),MTe.forEach(t),JIe=i(T),Gf=s(T,"LI",{});var ETe=n(Gf);nW=s(ETe,"STRONG",{});var Awr=n(nW);YIe=r(Awr,"visual_bert"),Awr.forEach(t),KIe=r(ETe," \u2014 "),x8=s(ETe,"A",{href:!0});var Lwr=n(x8);ZIe=r(Lwr,"VisualBertConfig"),Lwr.forEach(t),eje=r(ETe," (VisualBert model)"),ETe.forEach(t),oje=i(T),Of=s(T,"LI",{});var yTe=n(Of);lW=s(yTe,"STRONG",{});var Bwr=n(lW);rje=r(Bwr,"vit"),Bwr.forEach(t),tje=r(yTe," \u2014 "),k8=s(yTe,"A",{href:!0});var xwr=n(k8);aje=r(xwr,"ViTConfig"),xwr.forEach(t),sje=r(yTe," (ViT model)"),yTe.forEach(t),nje=i(T),Xf=s(T,"LI",{});var wTe=n(Xf);iW=s(wTe,"STRONG",{});var kwr=n(iW);lje=r(kwr,"vit_mae"),kwr.forEach(t),ije=r(wTe," \u2014 "),R8=s(wTe,"A",{href:!0});var Rwr=n(R8);dje=r(Rwr,"ViTMAEConfig"),Rwr.forEach(t),cje=r(wTe," (ViTMAE model)"),wTe.forEach(t),mje=i(T),zf=s(T,"LI",{});var ATe=n(zf);dW=s(ATe,"STRONG",{});var Swr=n(dW);fje=r(Swr,"wav2vec2"),Swr.forEach(t),gje=r(ATe," \u2014 "),S8=s(ATe,"A",{href:!0});var Pwr=n(S8);hje=r(Pwr,"Wav2Vec2Config"),Pwr.forEach(t),uje=r(ATe," (Wav2Vec2 model)"),ATe.forEach(t),pje=i(T),Vf=s(T,"LI",{});var LTe=n(Vf);cW=s(LTe,"STRONG",{});var $wr=n(cW);_je=r($wr,"wavlm"),$wr.forEach(t),bje=r(LTe," \u2014 "),P8=s(LTe,"A",{href:!0});var Iwr=n(P8);vje=r(Iwr,"WavLMConfig"),Iwr.forEach(t),Tje=r(LTe," (WavLM model)"),LTe.forEach(t),Fje=i(T),Wf=s(T,"LI",{});var BTe=n(Wf);mW=s(BTe,"STRONG",{});var jwr=n(mW);Cje=r(jwr,"xglm"),jwr.forEach(t),Mje=r(BTe," \u2014 "),$8=s(BTe,"A",{href:!0});var Nwr=n($8);Eje=r(Nwr,"XGLMConfig"),Nwr.forEach(t),yje=r(BTe," (XGLM model)"),BTe.forEach(t),wje=i(T),Qf=s(T,"LI",{});var xTe=n(Qf);fW=s(xTe,"STRONG",{});var Dwr=n(fW);Aje=r(Dwr,"xlm"),Dwr.forEach(t),Lje=r(xTe," \u2014 "),I8=s(xTe,"A",{href:!0});var qwr=n(I8);Bje=r(qwr,"XLMConfig"),qwr.forEach(t),xje=r(xTe," (XLM model)"),xTe.forEach(t),kje=i(T),Hf=s(T,"LI",{});var kTe=n(Hf);gW=s(kTe,"STRONG",{});var Gwr=n(gW);Rje=r(Gwr,"xlm-prophetnet"),Gwr.forEach(t),Sje=r(kTe," \u2014 "),j8=s(kTe,"A",{href:!0});var Owr=n(j8);Pje=r(Owr,"XLMProphetNetConfig"),Owr.forEach(t),$je=r(kTe," (XLMProphetNet model)"),kTe.forEach(t),Ije=i(T),Uf=s(T,"LI",{});var RTe=n(Uf);hW=s(RTe,"STRONG",{});var Xwr=n(hW);jje=r(Xwr,"xlm-roberta"),Xwr.forEach(t),Nje=r(RTe," \u2014 "),N8=s(RTe,"A",{href:!0});var zwr=n(N8);Dje=r(zwr,"XLMRobertaConfig"),zwr.forEach(t),qje=r(RTe," (XLM-RoBERTa model)"),RTe.forEach(t),Gje=i(T),Jf=s(T,"LI",{});var STe=n(Jf);uW=s(STe,"STRONG",{});var Vwr=n(uW);Oje=r(Vwr,"xlm-roberta-xl"),Vwr.forEach(t),Xje=r(STe," \u2014 "),D8=s(STe,"A",{href:!0});var Wwr=n(D8);zje=r(Wwr,"XLMRobertaXLConfig"),Wwr.forEach(t),Vje=r(STe," (XLM-RoBERTa-XL model)"),STe.forEach(t),Wje=i(T),Yf=s(T,"LI",{});var PTe=n(Yf);pW=s(PTe,"STRONG",{});var Qwr=n(pW);Qje=r(Qwr,"xlnet"),Qwr.forEach(t),Hje=r(PTe," \u2014 "),q8=s(PTe,"A",{href:!0});var Hwr=n(q8);Uje=r(Hwr,"XLNetConfig"),Hwr.forEach(t),Jje=r(PTe," (XLNet model)"),PTe.forEach(t),Yje=i(T),Kf=s(T,"LI",{});var $Te=n(Kf);_W=s($Te,"STRONG",{});var Uwr=n(_W);Kje=r(Uwr,"yoso"),Uwr.forEach(t),Zje=r($Te," \u2014 "),G8=s($Te,"A",{href:!0});var Jwr=n(G8);eNe=r(Jwr,"YosoConfig"),Jwr.forEach(t),oNe=r($Te," (YOSO model)"),$Te.forEach(t),T.forEach(t),rNe=i(aa),bW=s(aa,"P",{});var Ywr=n(bW);tNe=r(Ywr,"Examples:"),Ywr.forEach(t),aNe=i(aa),f(j3.$$.fragment,aa),aa.forEach(t),sNe=i(Bn),Zf=s(Bn,"DIV",{class:!0});var _8e=n(Zf);f(N3.$$.fragment,_8e),nNe=i(_8e),vW=s(_8e,"P",{});var Kwr=n(vW);lNe=r(Kwr,"Register a new configuration for this class."),Kwr.forEach(t),_8e.forEach(t),Bn.forEach(t),vLe=i(d),Ai=s(d,"H2",{class:!0});var b8e=n(Ai);eg=s(b8e,"A",{id:!0,class:!0,href:!0});var Zwr=n(eg);TW=s(Zwr,"SPAN",{});var eAr=n(TW);f(D3.$$.fragment,eAr),eAr.forEach(t),Zwr.forEach(t),iNe=i(b8e),FW=s(b8e,"SPAN",{});var oAr=n(FW);dNe=r(oAr,"AutoTokenizer"),oAr.forEach(t),b8e.forEach(t),TLe=i(d),Go=s(d,"DIV",{class:!0});var xn=n(Go);f(q3.$$.fragment,xn),cNe=i(xn),G3=s(xn,"P",{});var v8e=n(G3);mNe=r(v8e,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),O8=s(v8e,"A",{href:!0});var rAr=n(O8);fNe=r(rAr,"AutoTokenizer.from_pretrained()"),rAr.forEach(t),gNe=r(v8e," class method."),v8e.forEach(t),hNe=i(xn),O3=s(xn,"P",{});var T8e=n(O3);uNe=r(T8e,"This class cannot be instantiated directly using "),CW=s(T8e,"CODE",{});var tAr=n(CW);pNe=r(tAr,"__init__()"),tAr.forEach(t),_Ne=r(T8e," (throws an error)."),T8e.forEach(t),bNe=i(xn),co=s(xn,"DIV",{class:!0});var sa=n(co);f(X3.$$.fragment,sa),vNe=i(sa),MW=s(sa,"P",{});var aAr=n(MW);TNe=r(aAr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),aAr.forEach(t),FNe=i(sa),Sa=s(sa,"P",{});var BE=n(Sa);CNe=r(BE,"The tokenizer class to instantiate is selected based on the "),EW=s(BE,"CODE",{});var sAr=n(EW);MNe=r(sAr,"model_type"),sAr.forEach(t),ENe=r(BE,` property of the config object (either
passed as an argument or loaded from `),yW=s(BE,"CODE",{});var nAr=n(yW);yNe=r(nAr,"pretrained_model_name_or_path"),nAr.forEach(t),wNe=r(BE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wW=s(BE,"CODE",{});var lAr=n(wW);ANe=r(lAr,"pretrained_model_name_or_path"),lAr.forEach(t),LNe=r(BE,":"),BE.forEach(t),BNe=i(sa),M=s(sa,"UL",{});var E=n(M);Ss=s(E,"LI",{});var nL=n(Ss);AW=s(nL,"STRONG",{});var iAr=n(AW);xNe=r(iAr,"albert"),iAr.forEach(t),kNe=r(nL," \u2014 "),X8=s(nL,"A",{href:!0});var dAr=n(X8);RNe=r(dAr,"AlbertTokenizer"),dAr.forEach(t),SNe=r(nL," or "),z8=s(nL,"A",{href:!0});var cAr=n(z8);PNe=r(cAr,"AlbertTokenizerFast"),cAr.forEach(t),$Ne=r(nL," (ALBERT model)"),nL.forEach(t),INe=i(E),Ps=s(E,"LI",{});var lL=n(Ps);LW=s(lL,"STRONG",{});var mAr=n(LW);jNe=r(mAr,"bart"),mAr.forEach(t),NNe=r(lL," \u2014 "),V8=s(lL,"A",{href:!0});var fAr=n(V8);DNe=r(fAr,"BartTokenizer"),fAr.forEach(t),qNe=r(lL," or "),W8=s(lL,"A",{href:!0});var gAr=n(W8);GNe=r(gAr,"BartTokenizerFast"),gAr.forEach(t),ONe=r(lL," (BART model)"),lL.forEach(t),XNe=i(E),$s=s(E,"LI",{});var iL=n($s);BW=s(iL,"STRONG",{});var hAr=n(BW);zNe=r(hAr,"barthez"),hAr.forEach(t),VNe=r(iL," \u2014 "),Q8=s(iL,"A",{href:!0});var uAr=n(Q8);WNe=r(uAr,"BarthezTokenizer"),uAr.forEach(t),QNe=r(iL," or "),H8=s(iL,"A",{href:!0});var pAr=n(H8);HNe=r(pAr,"BarthezTokenizerFast"),pAr.forEach(t),UNe=r(iL," (BARThez model)"),iL.forEach(t),JNe=i(E),og=s(E,"LI",{});var ITe=n(og);xW=s(ITe,"STRONG",{});var _Ar=n(xW);YNe=r(_Ar,"bartpho"),_Ar.forEach(t),KNe=r(ITe," \u2014 "),U8=s(ITe,"A",{href:!0});var bAr=n(U8);ZNe=r(bAr,"BartphoTokenizer"),bAr.forEach(t),eDe=r(ITe," (BARTpho model)"),ITe.forEach(t),oDe=i(E),Is=s(E,"LI",{});var dL=n(Is);kW=s(dL,"STRONG",{});var vAr=n(kW);rDe=r(vAr,"bert"),vAr.forEach(t),tDe=r(dL," \u2014 "),J8=s(dL,"A",{href:!0});var TAr=n(J8);aDe=r(TAr,"BertTokenizer"),TAr.forEach(t),sDe=r(dL," or "),Y8=s(dL,"A",{href:!0});var FAr=n(Y8);nDe=r(FAr,"BertTokenizerFast"),FAr.forEach(t),lDe=r(dL," (BERT model)"),dL.forEach(t),iDe=i(E),rg=s(E,"LI",{});var jTe=n(rg);RW=s(jTe,"STRONG",{});var CAr=n(RW);dDe=r(CAr,"bert-generation"),CAr.forEach(t),cDe=r(jTe," \u2014 "),K8=s(jTe,"A",{href:!0});var MAr=n(K8);mDe=r(MAr,"BertGenerationTokenizer"),MAr.forEach(t),fDe=r(jTe," (Bert Generation model)"),jTe.forEach(t),gDe=i(E),tg=s(E,"LI",{});var NTe=n(tg);SW=s(NTe,"STRONG",{});var EAr=n(SW);hDe=r(EAr,"bert-japanese"),EAr.forEach(t),uDe=r(NTe," \u2014 "),Z8=s(NTe,"A",{href:!0});var yAr=n(Z8);pDe=r(yAr,"BertJapaneseTokenizer"),yAr.forEach(t),_De=r(NTe," (BertJapanese model)"),NTe.forEach(t),bDe=i(E),ag=s(E,"LI",{});var DTe=n(ag);PW=s(DTe,"STRONG",{});var wAr=n(PW);vDe=r(wAr,"bertweet"),wAr.forEach(t),TDe=r(DTe," \u2014 "),e9=s(DTe,"A",{href:!0});var AAr=n(e9);FDe=r(AAr,"BertweetTokenizer"),AAr.forEach(t),CDe=r(DTe," (Bertweet model)"),DTe.forEach(t),MDe=i(E),js=s(E,"LI",{});var cL=n(js);$W=s(cL,"STRONG",{});var LAr=n($W);EDe=r(LAr,"big_bird"),LAr.forEach(t),yDe=r(cL," \u2014 "),o9=s(cL,"A",{href:!0});var BAr=n(o9);wDe=r(BAr,"BigBirdTokenizer"),BAr.forEach(t),ADe=r(cL," or "),r9=s(cL,"A",{href:!0});var xAr=n(r9);LDe=r(xAr,"BigBirdTokenizerFast"),xAr.forEach(t),BDe=r(cL," (BigBird model)"),cL.forEach(t),xDe=i(E),Ns=s(E,"LI",{});var mL=n(Ns);IW=s(mL,"STRONG",{});var kAr=n(IW);kDe=r(kAr,"bigbird_pegasus"),kAr.forEach(t),RDe=r(mL," \u2014 "),t9=s(mL,"A",{href:!0});var RAr=n(t9);SDe=r(RAr,"PegasusTokenizer"),RAr.forEach(t),PDe=r(mL," or "),a9=s(mL,"A",{href:!0});var SAr=n(a9);$De=r(SAr,"PegasusTokenizerFast"),SAr.forEach(t),IDe=r(mL," (BigBirdPegasus model)"),mL.forEach(t),jDe=i(E),Ds=s(E,"LI",{});var fL=n(Ds);jW=s(fL,"STRONG",{});var PAr=n(jW);NDe=r(PAr,"blenderbot"),PAr.forEach(t),DDe=r(fL," \u2014 "),s9=s(fL,"A",{href:!0});var $Ar=n(s9);qDe=r($Ar,"BlenderbotTokenizer"),$Ar.forEach(t),GDe=r(fL," or "),n9=s(fL,"A",{href:!0});var IAr=n(n9);ODe=r(IAr,"BlenderbotTokenizerFast"),IAr.forEach(t),XDe=r(fL," (Blenderbot model)"),fL.forEach(t),zDe=i(E),sg=s(E,"LI",{});var qTe=n(sg);NW=s(qTe,"STRONG",{});var jAr=n(NW);VDe=r(jAr,"blenderbot-small"),jAr.forEach(t),WDe=r(qTe," \u2014 "),l9=s(qTe,"A",{href:!0});var NAr=n(l9);QDe=r(NAr,"BlenderbotSmallTokenizer"),NAr.forEach(t),HDe=r(qTe," (BlenderbotSmall model)"),qTe.forEach(t),UDe=i(E),ng=s(E,"LI",{});var GTe=n(ng);DW=s(GTe,"STRONG",{});var DAr=n(DW);JDe=r(DAr,"byt5"),DAr.forEach(t),YDe=r(GTe," \u2014 "),i9=s(GTe,"A",{href:!0});var qAr=n(i9);KDe=r(qAr,"ByT5Tokenizer"),qAr.forEach(t),ZDe=r(GTe," (ByT5 model)"),GTe.forEach(t),eqe=i(E),qs=s(E,"LI",{});var gL=n(qs);qW=s(gL,"STRONG",{});var GAr=n(qW);oqe=r(GAr,"camembert"),GAr.forEach(t),rqe=r(gL," \u2014 "),d9=s(gL,"A",{href:!0});var OAr=n(d9);tqe=r(OAr,"CamembertTokenizer"),OAr.forEach(t),aqe=r(gL," or "),c9=s(gL,"A",{href:!0});var XAr=n(c9);sqe=r(XAr,"CamembertTokenizerFast"),XAr.forEach(t),nqe=r(gL," (CamemBERT model)"),gL.forEach(t),lqe=i(E),lg=s(E,"LI",{});var OTe=n(lg);GW=s(OTe,"STRONG",{});var zAr=n(GW);iqe=r(zAr,"canine"),zAr.forEach(t),dqe=r(OTe," \u2014 "),m9=s(OTe,"A",{href:!0});var VAr=n(m9);cqe=r(VAr,"CanineTokenizer"),VAr.forEach(t),mqe=r(OTe," (Canine model)"),OTe.forEach(t),fqe=i(E),Gs=s(E,"LI",{});var hL=n(Gs);OW=s(hL,"STRONG",{});var WAr=n(OW);gqe=r(WAr,"clip"),WAr.forEach(t),hqe=r(hL," \u2014 "),f9=s(hL,"A",{href:!0});var QAr=n(f9);uqe=r(QAr,"CLIPTokenizer"),QAr.forEach(t),pqe=r(hL," or "),g9=s(hL,"A",{href:!0});var HAr=n(g9);_qe=r(HAr,"CLIPTokenizerFast"),HAr.forEach(t),bqe=r(hL," (CLIP model)"),hL.forEach(t),vqe=i(E),Os=s(E,"LI",{});var uL=n(Os);XW=s(uL,"STRONG",{});var UAr=n(XW);Tqe=r(UAr,"convbert"),UAr.forEach(t),Fqe=r(uL," \u2014 "),h9=s(uL,"A",{href:!0});var JAr=n(h9);Cqe=r(JAr,"ConvBertTokenizer"),JAr.forEach(t),Mqe=r(uL," or "),u9=s(uL,"A",{href:!0});var YAr=n(u9);Eqe=r(YAr,"ConvBertTokenizerFast"),YAr.forEach(t),yqe=r(uL," (ConvBERT model)"),uL.forEach(t),wqe=i(E),Xs=s(E,"LI",{});var pL=n(Xs);zW=s(pL,"STRONG",{});var KAr=n(zW);Aqe=r(KAr,"cpm"),KAr.forEach(t),Lqe=r(pL," \u2014 "),p9=s(pL,"A",{href:!0});var ZAr=n(p9);Bqe=r(ZAr,"CpmTokenizer"),ZAr.forEach(t),xqe=r(pL," or "),VW=s(pL,"CODE",{});var e0r=n(VW);kqe=r(e0r,"CpmTokenizerFast"),e0r.forEach(t),Rqe=r(pL," (CPM model)"),pL.forEach(t),Sqe=i(E),ig=s(E,"LI",{});var XTe=n(ig);WW=s(XTe,"STRONG",{});var o0r=n(WW);Pqe=r(o0r,"ctrl"),o0r.forEach(t),$qe=r(XTe," \u2014 "),_9=s(XTe,"A",{href:!0});var r0r=n(_9);Iqe=r(r0r,"CTRLTokenizer"),r0r.forEach(t),jqe=r(XTe," (CTRL model)"),XTe.forEach(t),Nqe=i(E),zs=s(E,"LI",{});var _L=n(zs);QW=s(_L,"STRONG",{});var t0r=n(QW);Dqe=r(t0r,"deberta"),t0r.forEach(t),qqe=r(_L," \u2014 "),b9=s(_L,"A",{href:!0});var a0r=n(b9);Gqe=r(a0r,"DebertaTokenizer"),a0r.forEach(t),Oqe=r(_L," or "),v9=s(_L,"A",{href:!0});var s0r=n(v9);Xqe=r(s0r,"DebertaTokenizerFast"),s0r.forEach(t),zqe=r(_L," (DeBERTa model)"),_L.forEach(t),Vqe=i(E),dg=s(E,"LI",{});var zTe=n(dg);HW=s(zTe,"STRONG",{});var n0r=n(HW);Wqe=r(n0r,"deberta-v2"),n0r.forEach(t),Qqe=r(zTe," \u2014 "),T9=s(zTe,"A",{href:!0});var l0r=n(T9);Hqe=r(l0r,"DebertaV2Tokenizer"),l0r.forEach(t),Uqe=r(zTe," (DeBERTa-v2 model)"),zTe.forEach(t),Jqe=i(E),Vs=s(E,"LI",{});var bL=n(Vs);UW=s(bL,"STRONG",{});var i0r=n(UW);Yqe=r(i0r,"distilbert"),i0r.forEach(t),Kqe=r(bL," \u2014 "),F9=s(bL,"A",{href:!0});var d0r=n(F9);Zqe=r(d0r,"DistilBertTokenizer"),d0r.forEach(t),eGe=r(bL," or "),C9=s(bL,"A",{href:!0});var c0r=n(C9);oGe=r(c0r,"DistilBertTokenizerFast"),c0r.forEach(t),rGe=r(bL," (DistilBERT model)"),bL.forEach(t),tGe=i(E),Ws=s(E,"LI",{});var vL=n(Ws);JW=s(vL,"STRONG",{});var m0r=n(JW);aGe=r(m0r,"dpr"),m0r.forEach(t),sGe=r(vL," \u2014 "),M9=s(vL,"A",{href:!0});var f0r=n(M9);nGe=r(f0r,"DPRQuestionEncoderTokenizer"),f0r.forEach(t),lGe=r(vL," or "),E9=s(vL,"A",{href:!0});var g0r=n(E9);iGe=r(g0r,"DPRQuestionEncoderTokenizerFast"),g0r.forEach(t),dGe=r(vL," (DPR model)"),vL.forEach(t),cGe=i(E),Qs=s(E,"LI",{});var TL=n(Qs);YW=s(TL,"STRONG",{});var h0r=n(YW);mGe=r(h0r,"electra"),h0r.forEach(t),fGe=r(TL," \u2014 "),y9=s(TL,"A",{href:!0});var u0r=n(y9);gGe=r(u0r,"ElectraTokenizer"),u0r.forEach(t),hGe=r(TL," or "),w9=s(TL,"A",{href:!0});var p0r=n(w9);uGe=r(p0r,"ElectraTokenizerFast"),p0r.forEach(t),pGe=r(TL," (ELECTRA model)"),TL.forEach(t),_Ge=i(E),cg=s(E,"LI",{});var VTe=n(cg);KW=s(VTe,"STRONG",{});var _0r=n(KW);bGe=r(_0r,"flaubert"),_0r.forEach(t),vGe=r(VTe," \u2014 "),A9=s(VTe,"A",{href:!0});var b0r=n(A9);TGe=r(b0r,"FlaubertTokenizer"),b0r.forEach(t),FGe=r(VTe," (FlauBERT model)"),VTe.forEach(t),CGe=i(E),Hs=s(E,"LI",{});var FL=n(Hs);ZW=s(FL,"STRONG",{});var v0r=n(ZW);MGe=r(v0r,"fnet"),v0r.forEach(t),EGe=r(FL," \u2014 "),L9=s(FL,"A",{href:!0});var T0r=n(L9);yGe=r(T0r,"FNetTokenizer"),T0r.forEach(t),wGe=r(FL," or "),B9=s(FL,"A",{href:!0});var F0r=n(B9);AGe=r(F0r,"FNetTokenizerFast"),F0r.forEach(t),LGe=r(FL," (FNet model)"),FL.forEach(t),BGe=i(E),mg=s(E,"LI",{});var WTe=n(mg);eQ=s(WTe,"STRONG",{});var C0r=n(eQ);xGe=r(C0r,"fsmt"),C0r.forEach(t),kGe=r(WTe," \u2014 "),x9=s(WTe,"A",{href:!0});var M0r=n(x9);RGe=r(M0r,"FSMTTokenizer"),M0r.forEach(t),SGe=r(WTe," (FairSeq Machine-Translation model)"),WTe.forEach(t),PGe=i(E),Us=s(E,"LI",{});var CL=n(Us);oQ=s(CL,"STRONG",{});var E0r=n(oQ);$Ge=r(E0r,"funnel"),E0r.forEach(t),IGe=r(CL," \u2014 "),k9=s(CL,"A",{href:!0});var y0r=n(k9);jGe=r(y0r,"FunnelTokenizer"),y0r.forEach(t),NGe=r(CL," or "),R9=s(CL,"A",{href:!0});var w0r=n(R9);DGe=r(w0r,"FunnelTokenizerFast"),w0r.forEach(t),qGe=r(CL," (Funnel Transformer model)"),CL.forEach(t),GGe=i(E),Js=s(E,"LI",{});var ML=n(Js);rQ=s(ML,"STRONG",{});var A0r=n(rQ);OGe=r(A0r,"gpt2"),A0r.forEach(t),XGe=r(ML," \u2014 "),S9=s(ML,"A",{href:!0});var L0r=n(S9);zGe=r(L0r,"GPT2Tokenizer"),L0r.forEach(t),VGe=r(ML," or "),P9=s(ML,"A",{href:!0});var B0r=n(P9);WGe=r(B0r,"GPT2TokenizerFast"),B0r.forEach(t),QGe=r(ML," (OpenAI GPT-2 model)"),ML.forEach(t),HGe=i(E),Ys=s(E,"LI",{});var EL=n(Ys);tQ=s(EL,"STRONG",{});var x0r=n(tQ);UGe=r(x0r,"gpt_neo"),x0r.forEach(t),JGe=r(EL," \u2014 "),$9=s(EL,"A",{href:!0});var k0r=n($9);YGe=r(k0r,"GPT2Tokenizer"),k0r.forEach(t),KGe=r(EL," or "),I9=s(EL,"A",{href:!0});var R0r=n(I9);ZGe=r(R0r,"GPT2TokenizerFast"),R0r.forEach(t),eOe=r(EL," (GPT Neo model)"),EL.forEach(t),oOe=i(E),Ks=s(E,"LI",{});var yL=n(Ks);aQ=s(yL,"STRONG",{});var S0r=n(aQ);rOe=r(S0r,"herbert"),S0r.forEach(t),tOe=r(yL," \u2014 "),j9=s(yL,"A",{href:!0});var P0r=n(j9);aOe=r(P0r,"HerbertTokenizer"),P0r.forEach(t),sOe=r(yL," or "),N9=s(yL,"A",{href:!0});var $0r=n(N9);nOe=r($0r,"HerbertTokenizerFast"),$0r.forEach(t),lOe=r(yL," (HerBERT model)"),yL.forEach(t),iOe=i(E),fg=s(E,"LI",{});var QTe=n(fg);sQ=s(QTe,"STRONG",{});var I0r=n(sQ);dOe=r(I0r,"hubert"),I0r.forEach(t),cOe=r(QTe," \u2014 "),D9=s(QTe,"A",{href:!0});var j0r=n(D9);mOe=r(j0r,"Wav2Vec2CTCTokenizer"),j0r.forEach(t),fOe=r(QTe," (Hubert model)"),QTe.forEach(t),gOe=i(E),Zs=s(E,"LI",{});var wL=n(Zs);nQ=s(wL,"STRONG",{});var N0r=n(nQ);hOe=r(N0r,"ibert"),N0r.forEach(t),uOe=r(wL," \u2014 "),q9=s(wL,"A",{href:!0});var D0r=n(q9);pOe=r(D0r,"RobertaTokenizer"),D0r.forEach(t),_Oe=r(wL," or "),G9=s(wL,"A",{href:!0});var q0r=n(G9);bOe=r(q0r,"RobertaTokenizerFast"),q0r.forEach(t),vOe=r(wL," (I-BERT model)"),wL.forEach(t),TOe=i(E),en=s(E,"LI",{});var AL=n(en);lQ=s(AL,"STRONG",{});var G0r=n(lQ);FOe=r(G0r,"layoutlm"),G0r.forEach(t),COe=r(AL," \u2014 "),O9=s(AL,"A",{href:!0});var O0r=n(O9);MOe=r(O0r,"LayoutLMTokenizer"),O0r.forEach(t),EOe=r(AL," or "),X9=s(AL,"A",{href:!0});var X0r=n(X9);yOe=r(X0r,"LayoutLMTokenizerFast"),X0r.forEach(t),wOe=r(AL," (LayoutLM model)"),AL.forEach(t),AOe=i(E),on=s(E,"LI",{});var LL=n(on);iQ=s(LL,"STRONG",{});var z0r=n(iQ);LOe=r(z0r,"layoutlmv2"),z0r.forEach(t),BOe=r(LL," \u2014 "),z9=s(LL,"A",{href:!0});var V0r=n(z9);xOe=r(V0r,"LayoutLMv2Tokenizer"),V0r.forEach(t),kOe=r(LL," or "),V9=s(LL,"A",{href:!0});var W0r=n(V9);ROe=r(W0r,"LayoutLMv2TokenizerFast"),W0r.forEach(t),SOe=r(LL," (LayoutLMv2 model)"),LL.forEach(t),POe=i(E),rn=s(E,"LI",{});var BL=n(rn);dQ=s(BL,"STRONG",{});var Q0r=n(dQ);$Oe=r(Q0r,"layoutxlm"),Q0r.forEach(t),IOe=r(BL," \u2014 "),W9=s(BL,"A",{href:!0});var H0r=n(W9);jOe=r(H0r,"LayoutXLMTokenizer"),H0r.forEach(t),NOe=r(BL," or "),Q9=s(BL,"A",{href:!0});var U0r=n(Q9);DOe=r(U0r,"LayoutXLMTokenizerFast"),U0r.forEach(t),qOe=r(BL," (LayoutXLM model)"),BL.forEach(t),GOe=i(E),tn=s(E,"LI",{});var xL=n(tn);cQ=s(xL,"STRONG",{});var J0r=n(cQ);OOe=r(J0r,"led"),J0r.forEach(t),XOe=r(xL," \u2014 "),H9=s(xL,"A",{href:!0});var Y0r=n(H9);zOe=r(Y0r,"LEDTokenizer"),Y0r.forEach(t),VOe=r(xL," or "),U9=s(xL,"A",{href:!0});var K0r=n(U9);WOe=r(K0r,"LEDTokenizerFast"),K0r.forEach(t),QOe=r(xL," (LED model)"),xL.forEach(t),HOe=i(E),an=s(E,"LI",{});var kL=n(an);mQ=s(kL,"STRONG",{});var Z0r=n(mQ);UOe=r(Z0r,"longformer"),Z0r.forEach(t),JOe=r(kL," \u2014 "),J9=s(kL,"A",{href:!0});var e6r=n(J9);YOe=r(e6r,"LongformerTokenizer"),e6r.forEach(t),KOe=r(kL," or "),Y9=s(kL,"A",{href:!0});var o6r=n(Y9);ZOe=r(o6r,"LongformerTokenizerFast"),o6r.forEach(t),eXe=r(kL," (Longformer model)"),kL.forEach(t),oXe=i(E),gg=s(E,"LI",{});var HTe=n(gg);fQ=s(HTe,"STRONG",{});var r6r=n(fQ);rXe=r(r6r,"luke"),r6r.forEach(t),tXe=r(HTe," \u2014 "),K9=s(HTe,"A",{href:!0});var t6r=n(K9);aXe=r(t6r,"LukeTokenizer"),t6r.forEach(t),sXe=r(HTe," (LUKE model)"),HTe.forEach(t),nXe=i(E),sn=s(E,"LI",{});var RL=n(sn);gQ=s(RL,"STRONG",{});var a6r=n(gQ);lXe=r(a6r,"lxmert"),a6r.forEach(t),iXe=r(RL," \u2014 "),Z9=s(RL,"A",{href:!0});var s6r=n(Z9);dXe=r(s6r,"LxmertTokenizer"),s6r.forEach(t),cXe=r(RL," or "),eB=s(RL,"A",{href:!0});var n6r=n(eB);mXe=r(n6r,"LxmertTokenizerFast"),n6r.forEach(t),fXe=r(RL," (LXMERT model)"),RL.forEach(t),gXe=i(E),hg=s(E,"LI",{});var UTe=n(hg);hQ=s(UTe,"STRONG",{});var l6r=n(hQ);hXe=r(l6r,"m2m_100"),l6r.forEach(t),uXe=r(UTe," \u2014 "),oB=s(UTe,"A",{href:!0});var i6r=n(oB);pXe=r(i6r,"M2M100Tokenizer"),i6r.forEach(t),_Xe=r(UTe," (M2M100 model)"),UTe.forEach(t),bXe=i(E),ug=s(E,"LI",{});var JTe=n(ug);uQ=s(JTe,"STRONG",{});var d6r=n(uQ);vXe=r(d6r,"marian"),d6r.forEach(t),TXe=r(JTe," \u2014 "),rB=s(JTe,"A",{href:!0});var c6r=n(rB);FXe=r(c6r,"MarianTokenizer"),c6r.forEach(t),CXe=r(JTe," (Marian model)"),JTe.forEach(t),MXe=i(E),nn=s(E,"LI",{});var SL=n(nn);pQ=s(SL,"STRONG",{});var m6r=n(pQ);EXe=r(m6r,"mbart"),m6r.forEach(t),yXe=r(SL," \u2014 "),tB=s(SL,"A",{href:!0});var f6r=n(tB);wXe=r(f6r,"MBartTokenizer"),f6r.forEach(t),AXe=r(SL," or "),aB=s(SL,"A",{href:!0});var g6r=n(aB);LXe=r(g6r,"MBartTokenizerFast"),g6r.forEach(t),BXe=r(SL," (mBART model)"),SL.forEach(t),xXe=i(E),ln=s(E,"LI",{});var PL=n(ln);_Q=s(PL,"STRONG",{});var h6r=n(_Q);kXe=r(h6r,"mbart50"),h6r.forEach(t),RXe=r(PL," \u2014 "),sB=s(PL,"A",{href:!0});var u6r=n(sB);SXe=r(u6r,"MBart50Tokenizer"),u6r.forEach(t),PXe=r(PL," or "),nB=s(PL,"A",{href:!0});var p6r=n(nB);$Xe=r(p6r,"MBart50TokenizerFast"),p6r.forEach(t),IXe=r(PL," (mBART-50 model)"),PL.forEach(t),jXe=i(E),pg=s(E,"LI",{});var YTe=n(pg);bQ=s(YTe,"STRONG",{});var _6r=n(bQ);NXe=r(_6r,"mluke"),_6r.forEach(t),DXe=r(YTe," \u2014 "),lB=s(YTe,"A",{href:!0});var b6r=n(lB);qXe=r(b6r,"MLukeTokenizer"),b6r.forEach(t),GXe=r(YTe," (mLUKE model)"),YTe.forEach(t),OXe=i(E),dn=s(E,"LI",{});var $L=n(dn);vQ=s($L,"STRONG",{});var v6r=n(vQ);XXe=r(v6r,"mobilebert"),v6r.forEach(t),zXe=r($L," \u2014 "),iB=s($L,"A",{href:!0});var T6r=n(iB);VXe=r(T6r,"MobileBertTokenizer"),T6r.forEach(t),WXe=r($L," or "),dB=s($L,"A",{href:!0});var F6r=n(dB);QXe=r(F6r,"MobileBertTokenizerFast"),F6r.forEach(t),HXe=r($L," (MobileBERT model)"),$L.forEach(t),UXe=i(E),cn=s(E,"LI",{});var IL=n(cn);TQ=s(IL,"STRONG",{});var C6r=n(TQ);JXe=r(C6r,"mpnet"),C6r.forEach(t),YXe=r(IL," \u2014 "),cB=s(IL,"A",{href:!0});var M6r=n(cB);KXe=r(M6r,"MPNetTokenizer"),M6r.forEach(t),ZXe=r(IL," or "),mB=s(IL,"A",{href:!0});var E6r=n(mB);eze=r(E6r,"MPNetTokenizerFast"),E6r.forEach(t),oze=r(IL," (MPNet model)"),IL.forEach(t),rze=i(E),mn=s(E,"LI",{});var jL=n(mn);FQ=s(jL,"STRONG",{});var y6r=n(FQ);tze=r(y6r,"mt5"),y6r.forEach(t),aze=r(jL," \u2014 "),fB=s(jL,"A",{href:!0});var w6r=n(fB);sze=r(w6r,"MT5Tokenizer"),w6r.forEach(t),nze=r(jL," or "),gB=s(jL,"A",{href:!0});var A6r=n(gB);lze=r(A6r,"MT5TokenizerFast"),A6r.forEach(t),ize=r(jL," (mT5 model)"),jL.forEach(t),dze=i(E),fn=s(E,"LI",{});var NL=n(fn);CQ=s(NL,"STRONG",{});var L6r=n(CQ);cze=r(L6r,"openai-gpt"),L6r.forEach(t),mze=r(NL," \u2014 "),hB=s(NL,"A",{href:!0});var B6r=n(hB);fze=r(B6r,"OpenAIGPTTokenizer"),B6r.forEach(t),gze=r(NL," or "),uB=s(NL,"A",{href:!0});var x6r=n(uB);hze=r(x6r,"OpenAIGPTTokenizerFast"),x6r.forEach(t),uze=r(NL," (OpenAI GPT model)"),NL.forEach(t),pze=i(E),gn=s(E,"LI",{});var DL=n(gn);MQ=s(DL,"STRONG",{});var k6r=n(MQ);_ze=r(k6r,"pegasus"),k6r.forEach(t),bze=r(DL," \u2014 "),pB=s(DL,"A",{href:!0});var R6r=n(pB);vze=r(R6r,"PegasusTokenizer"),R6r.forEach(t),Tze=r(DL," or "),_B=s(DL,"A",{href:!0});var S6r=n(_B);Fze=r(S6r,"PegasusTokenizerFast"),S6r.forEach(t),Cze=r(DL," (Pegasus model)"),DL.forEach(t),Mze=i(E),_g=s(E,"LI",{});var KTe=n(_g);EQ=s(KTe,"STRONG",{});var P6r=n(EQ);Eze=r(P6r,"perceiver"),P6r.forEach(t),yze=r(KTe," \u2014 "),bB=s(KTe,"A",{href:!0});var $6r=n(bB);wze=r($6r,"PerceiverTokenizer"),$6r.forEach(t),Aze=r(KTe," (Perceiver model)"),KTe.forEach(t),Lze=i(E),bg=s(E,"LI",{});var ZTe=n(bg);yQ=s(ZTe,"STRONG",{});var I6r=n(yQ);Bze=r(I6r,"phobert"),I6r.forEach(t),xze=r(ZTe," \u2014 "),vB=s(ZTe,"A",{href:!0});var j6r=n(vB);kze=r(j6r,"PhobertTokenizer"),j6r.forEach(t),Rze=r(ZTe," (PhoBERT model)"),ZTe.forEach(t),Sze=i(E),vg=s(E,"LI",{});var e1e=n(vg);wQ=s(e1e,"STRONG",{});var N6r=n(wQ);Pze=r(N6r,"prophetnet"),N6r.forEach(t),$ze=r(e1e," \u2014 "),TB=s(e1e,"A",{href:!0});var D6r=n(TB);Ize=r(D6r,"ProphetNetTokenizer"),D6r.forEach(t),jze=r(e1e," (ProphetNet model)"),e1e.forEach(t),Nze=i(E),hn=s(E,"LI",{});var qL=n(hn);AQ=s(qL,"STRONG",{});var q6r=n(AQ);Dze=r(q6r,"qdqbert"),q6r.forEach(t),qze=r(qL," \u2014 "),FB=s(qL,"A",{href:!0});var G6r=n(FB);Gze=r(G6r,"BertTokenizer"),G6r.forEach(t),Oze=r(qL," or "),CB=s(qL,"A",{href:!0});var O6r=n(CB);Xze=r(O6r,"BertTokenizerFast"),O6r.forEach(t),zze=r(qL," (QDQBert model)"),qL.forEach(t),Vze=i(E),Tg=s(E,"LI",{});var o1e=n(Tg);LQ=s(o1e,"STRONG",{});var X6r=n(LQ);Wze=r(X6r,"rag"),X6r.forEach(t),Qze=r(o1e," \u2014 "),MB=s(o1e,"A",{href:!0});var z6r=n(MB);Hze=r(z6r,"RagTokenizer"),z6r.forEach(t),Uze=r(o1e," (RAG model)"),o1e.forEach(t),Jze=i(E),un=s(E,"LI",{});var GL=n(un);BQ=s(GL,"STRONG",{});var V6r=n(BQ);Yze=r(V6r,"reformer"),V6r.forEach(t),Kze=r(GL," \u2014 "),EB=s(GL,"A",{href:!0});var W6r=n(EB);Zze=r(W6r,"ReformerTokenizer"),W6r.forEach(t),eVe=r(GL," or "),yB=s(GL,"A",{href:!0});var Q6r=n(yB);oVe=r(Q6r,"ReformerTokenizerFast"),Q6r.forEach(t),rVe=r(GL," (Reformer model)"),GL.forEach(t),tVe=i(E),pn=s(E,"LI",{});var OL=n(pn);xQ=s(OL,"STRONG",{});var H6r=n(xQ);aVe=r(H6r,"rembert"),H6r.forEach(t),sVe=r(OL," \u2014 "),wB=s(OL,"A",{href:!0});var U6r=n(wB);nVe=r(U6r,"RemBertTokenizer"),U6r.forEach(t),lVe=r(OL," or "),AB=s(OL,"A",{href:!0});var J6r=n(AB);iVe=r(J6r,"RemBertTokenizerFast"),J6r.forEach(t),dVe=r(OL," (RemBERT model)"),OL.forEach(t),cVe=i(E),_n=s(E,"LI",{});var XL=n(_n);kQ=s(XL,"STRONG",{});var Y6r=n(kQ);mVe=r(Y6r,"retribert"),Y6r.forEach(t),fVe=r(XL," \u2014 "),LB=s(XL,"A",{href:!0});var K6r=n(LB);gVe=r(K6r,"RetriBertTokenizer"),K6r.forEach(t),hVe=r(XL," or "),BB=s(XL,"A",{href:!0});var Z6r=n(BB);uVe=r(Z6r,"RetriBertTokenizerFast"),Z6r.forEach(t),pVe=r(XL," (RetriBERT model)"),XL.forEach(t),_Ve=i(E),bn=s(E,"LI",{});var zL=n(bn);RQ=s(zL,"STRONG",{});var eLr=n(RQ);bVe=r(eLr,"roberta"),eLr.forEach(t),vVe=r(zL," \u2014 "),xB=s(zL,"A",{href:!0});var oLr=n(xB);TVe=r(oLr,"RobertaTokenizer"),oLr.forEach(t),FVe=r(zL," or "),kB=s(zL,"A",{href:!0});var rLr=n(kB);CVe=r(rLr,"RobertaTokenizerFast"),rLr.forEach(t),MVe=r(zL," (RoBERTa model)"),zL.forEach(t),EVe=i(E),vn=s(E,"LI",{});var VL=n(vn);SQ=s(VL,"STRONG",{});var tLr=n(SQ);yVe=r(tLr,"roformer"),tLr.forEach(t),wVe=r(VL," \u2014 "),RB=s(VL,"A",{href:!0});var aLr=n(RB);AVe=r(aLr,"RoFormerTokenizer"),aLr.forEach(t),LVe=r(VL," or "),SB=s(VL,"A",{href:!0});var sLr=n(SB);BVe=r(sLr,"RoFormerTokenizerFast"),sLr.forEach(t),xVe=r(VL," (RoFormer model)"),VL.forEach(t),kVe=i(E),Fg=s(E,"LI",{});var r1e=n(Fg);PQ=s(r1e,"STRONG",{});var nLr=n(PQ);RVe=r(nLr,"speech_to_text"),nLr.forEach(t),SVe=r(r1e," \u2014 "),PB=s(r1e,"A",{href:!0});var lLr=n(PB);PVe=r(lLr,"Speech2TextTokenizer"),lLr.forEach(t),$Ve=r(r1e," (Speech2Text model)"),r1e.forEach(t),IVe=i(E),Cg=s(E,"LI",{});var t1e=n(Cg);$Q=s(t1e,"STRONG",{});var iLr=n($Q);jVe=r(iLr,"speech_to_text_2"),iLr.forEach(t),NVe=r(t1e," \u2014 "),$B=s(t1e,"A",{href:!0});var dLr=n($B);DVe=r(dLr,"Speech2Text2Tokenizer"),dLr.forEach(t),qVe=r(t1e," (Speech2Text2 model)"),t1e.forEach(t),GVe=i(E),Tn=s(E,"LI",{});var WL=n(Tn);IQ=s(WL,"STRONG",{});var cLr=n(IQ);OVe=r(cLr,"splinter"),cLr.forEach(t),XVe=r(WL," \u2014 "),IB=s(WL,"A",{href:!0});var mLr=n(IB);zVe=r(mLr,"SplinterTokenizer"),mLr.forEach(t),VVe=r(WL," or "),jB=s(WL,"A",{href:!0});var fLr=n(jB);WVe=r(fLr,"SplinterTokenizerFast"),fLr.forEach(t),QVe=r(WL," (Splinter model)"),WL.forEach(t),HVe=i(E),Fn=s(E,"LI",{});var QL=n(Fn);jQ=s(QL,"STRONG",{});var gLr=n(jQ);UVe=r(gLr,"squeezebert"),gLr.forEach(t),JVe=r(QL," \u2014 "),NB=s(QL,"A",{href:!0});var hLr=n(NB);YVe=r(hLr,"SqueezeBertTokenizer"),hLr.forEach(t),KVe=r(QL," or "),DB=s(QL,"A",{href:!0});var uLr=n(DB);ZVe=r(uLr,"SqueezeBertTokenizerFast"),uLr.forEach(t),eWe=r(QL," (SqueezeBERT model)"),QL.forEach(t),oWe=i(E),Cn=s(E,"LI",{});var HL=n(Cn);NQ=s(HL,"STRONG",{});var pLr=n(NQ);rWe=r(pLr,"t5"),pLr.forEach(t),tWe=r(HL," \u2014 "),qB=s(HL,"A",{href:!0});var _Lr=n(qB);aWe=r(_Lr,"T5Tokenizer"),_Lr.forEach(t),sWe=r(HL," or "),GB=s(HL,"A",{href:!0});var bLr=n(GB);nWe=r(bLr,"T5TokenizerFast"),bLr.forEach(t),lWe=r(HL," (T5 model)"),HL.forEach(t),iWe=i(E),Mg=s(E,"LI",{});var a1e=n(Mg);DQ=s(a1e,"STRONG",{});var vLr=n(DQ);dWe=r(vLr,"tapas"),vLr.forEach(t),cWe=r(a1e," \u2014 "),OB=s(a1e,"A",{href:!0});var TLr=n(OB);mWe=r(TLr,"TapasTokenizer"),TLr.forEach(t),fWe=r(a1e," (TAPAS model)"),a1e.forEach(t),gWe=i(E),Eg=s(E,"LI",{});var s1e=n(Eg);qQ=s(s1e,"STRONG",{});var FLr=n(qQ);hWe=r(FLr,"transfo-xl"),FLr.forEach(t),uWe=r(s1e," \u2014 "),XB=s(s1e,"A",{href:!0});var CLr=n(XB);pWe=r(CLr,"TransfoXLTokenizer"),CLr.forEach(t),_We=r(s1e," (Transformer-XL model)"),s1e.forEach(t),bWe=i(E),yg=s(E,"LI",{});var n1e=n(yg);GQ=s(n1e,"STRONG",{});var MLr=n(GQ);vWe=r(MLr,"wav2vec2"),MLr.forEach(t),TWe=r(n1e," \u2014 "),zB=s(n1e,"A",{href:!0});var ELr=n(zB);FWe=r(ELr,"Wav2Vec2CTCTokenizer"),ELr.forEach(t),CWe=r(n1e," (Wav2Vec2 model)"),n1e.forEach(t),MWe=i(E),wg=s(E,"LI",{});var l1e=n(wg);OQ=s(l1e,"STRONG",{});var yLr=n(OQ);EWe=r(yLr,"wav2vec2_phoneme"),yLr.forEach(t),yWe=r(l1e," \u2014 "),VB=s(l1e,"A",{href:!0});var wLr=n(VB);wWe=r(wLr,"Wav2Vec2PhonemeCTCTokenizer"),wLr.forEach(t),AWe=r(l1e," (Wav2Vec2Phoneme model)"),l1e.forEach(t),LWe=i(E),Mn=s(E,"LI",{});var UL=n(Mn);XQ=s(UL,"STRONG",{});var ALr=n(XQ);BWe=r(ALr,"xglm"),ALr.forEach(t),xWe=r(UL," \u2014 "),WB=s(UL,"A",{href:!0});var LLr=n(WB);kWe=r(LLr,"XGLMTokenizer"),LLr.forEach(t),RWe=r(UL," or "),QB=s(UL,"A",{href:!0});var BLr=n(QB);SWe=r(BLr,"XGLMTokenizerFast"),BLr.forEach(t),PWe=r(UL," (XGLM model)"),UL.forEach(t),$We=i(E),Ag=s(E,"LI",{});var i1e=n(Ag);zQ=s(i1e,"STRONG",{});var xLr=n(zQ);IWe=r(xLr,"xlm"),xLr.forEach(t),jWe=r(i1e," \u2014 "),HB=s(i1e,"A",{href:!0});var kLr=n(HB);NWe=r(kLr,"XLMTokenizer"),kLr.forEach(t),DWe=r(i1e," (XLM model)"),i1e.forEach(t),qWe=i(E),Lg=s(E,"LI",{});var d1e=n(Lg);VQ=s(d1e,"STRONG",{});var RLr=n(VQ);GWe=r(RLr,"xlm-prophetnet"),RLr.forEach(t),OWe=r(d1e," \u2014 "),UB=s(d1e,"A",{href:!0});var SLr=n(UB);XWe=r(SLr,"XLMProphetNetTokenizer"),SLr.forEach(t),zWe=r(d1e," (XLMProphetNet model)"),d1e.forEach(t),VWe=i(E),En=s(E,"LI",{});var JL=n(En);WQ=s(JL,"STRONG",{});var PLr=n(WQ);WWe=r(PLr,"xlm-roberta"),PLr.forEach(t),QWe=r(JL," \u2014 "),JB=s(JL,"A",{href:!0});var $Lr=n(JB);HWe=r($Lr,"XLMRobertaTokenizer"),$Lr.forEach(t),UWe=r(JL," or "),YB=s(JL,"A",{href:!0});var ILr=n(YB);JWe=r(ILr,"XLMRobertaTokenizerFast"),ILr.forEach(t),YWe=r(JL," (XLM-RoBERTa model)"),JL.forEach(t),KWe=i(E),yn=s(E,"LI",{});var YL=n(yn);QQ=s(YL,"STRONG",{});var jLr=n(QQ);ZWe=r(jLr,"xlnet"),jLr.forEach(t),eQe=r(YL," \u2014 "),KB=s(YL,"A",{href:!0});var NLr=n(KB);oQe=r(NLr,"XLNetTokenizer"),NLr.forEach(t),rQe=r(YL," or "),ZB=s(YL,"A",{href:!0});var DLr=n(ZB);tQe=r(DLr,"XLNetTokenizerFast"),DLr.forEach(t),aQe=r(YL," (XLNet model)"),YL.forEach(t),E.forEach(t),sQe=i(sa),HQ=s(sa,"P",{});var qLr=n(HQ);nQe=r(qLr,"Examples:"),qLr.forEach(t),lQe=i(sa),f(z3.$$.fragment,sa),sa.forEach(t),iQe=i(xn),Bg=s(xn,"DIV",{class:!0});var F8e=n(Bg);f(V3.$$.fragment,F8e),dQe=i(F8e),UQ=s(F8e,"P",{});var GLr=n(UQ);cQe=r(GLr,"Register a new tokenizer in this mapping."),GLr.forEach(t),F8e.forEach(t),xn.forEach(t),FLe=i(d),Li=s(d,"H2",{class:!0});var C8e=n(Li);xg=s(C8e,"A",{id:!0,class:!0,href:!0});var OLr=n(xg);JQ=s(OLr,"SPAN",{});var XLr=n(JQ);f(W3.$$.fragment,XLr),XLr.forEach(t),OLr.forEach(t),mQe=i(C8e),YQ=s(C8e,"SPAN",{});var zLr=n(YQ);fQe=r(zLr,"AutoFeatureExtractor"),zLr.forEach(t),C8e.forEach(t),CLe=i(d),Zt=s(d,"DIV",{class:!0});var xE=n(Zt);f(Q3.$$.fragment,xE),gQe=i(xE),H3=s(xE,"P",{});var M8e=n(H3);hQe=r(M8e,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),ex=s(M8e,"A",{href:!0});var VLr=n(ex);uQe=r(VLr,"AutoFeatureExtractor.from_pretrained()"),VLr.forEach(t),pQe=r(M8e," class method."),M8e.forEach(t),_Qe=i(xE),U3=s(xE,"P",{});var E8e=n(U3);bQe=r(E8e,"This class cannot be instantiated directly using "),KQ=s(E8e,"CODE",{});var WLr=n(KQ);vQe=r(WLr,"__init__()"),WLr.forEach(t),TQe=r(E8e," (throws an error)."),E8e.forEach(t),FQe=i(xE),Le=s(xE,"DIV",{class:!0});var wt=n(Le);f(J3.$$.fragment,wt),CQe=i(wt),ZQ=s(wt,"P",{});var QLr=n(ZQ);MQe=r(QLr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),QLr.forEach(t),EQe=i(wt),Pa=s(wt,"P",{});var kE=n(Pa);yQe=r(kE,"The feature extractor class to instantiate is selected based on the "),eH=s(kE,"CODE",{});var HLr=n(eH);wQe=r(HLr,"model_type"),HLr.forEach(t),AQe=r(kE,` property of the config object
(either passed as an argument or loaded from `),oH=s(kE,"CODE",{});var ULr=n(oH);LQe=r(ULr,"pretrained_model_name_or_path"),ULr.forEach(t),BQe=r(kE,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),rH=s(kE,"CODE",{});var JLr=n(rH);xQe=r(JLr,"pretrained_model_name_or_path"),JLr.forEach(t),kQe=r(kE,":"),kE.forEach(t),RQe=i(wt),ne=s(wt,"UL",{});var de=n(ne);kg=s(de,"LI",{});var c1e=n(kg);tH=s(c1e,"STRONG",{});var YLr=n(tH);SQe=r(YLr,"beit"),YLr.forEach(t),PQe=r(c1e," \u2014 "),ox=s(c1e,"A",{href:!0});var KLr=n(ox);$Qe=r(KLr,"BeitFeatureExtractor"),KLr.forEach(t),IQe=r(c1e," (BEiT model)"),c1e.forEach(t),jQe=i(de),Rg=s(de,"LI",{});var m1e=n(Rg);aH=s(m1e,"STRONG",{});var ZLr=n(aH);NQe=r(ZLr,"clip"),ZLr.forEach(t),DQe=r(m1e," \u2014 "),rx=s(m1e,"A",{href:!0});var e7r=n(rx);qQe=r(e7r,"CLIPFeatureExtractor"),e7r.forEach(t),GQe=r(m1e," (CLIP model)"),m1e.forEach(t),OQe=i(de),Sg=s(de,"LI",{});var f1e=n(Sg);sH=s(f1e,"STRONG",{});var o7r=n(sH);XQe=r(o7r,"convnext"),o7r.forEach(t),zQe=r(f1e," \u2014 "),tx=s(f1e,"A",{href:!0});var r7r=n(tx);VQe=r(r7r,"ConvNextFeatureExtractor"),r7r.forEach(t),WQe=r(f1e," (ConvNext model)"),f1e.forEach(t),QQe=i(de),Pg=s(de,"LI",{});var g1e=n(Pg);nH=s(g1e,"STRONG",{});var t7r=n(nH);HQe=r(t7r,"deit"),t7r.forEach(t),UQe=r(g1e," \u2014 "),ax=s(g1e,"A",{href:!0});var a7r=n(ax);JQe=r(a7r,"DeiTFeatureExtractor"),a7r.forEach(t),YQe=r(g1e," (DeiT model)"),g1e.forEach(t),KQe=i(de),$g=s(de,"LI",{});var h1e=n($g);lH=s(h1e,"STRONG",{});var s7r=n(lH);ZQe=r(s7r,"detr"),s7r.forEach(t),eHe=r(h1e," \u2014 "),sx=s(h1e,"A",{href:!0});var n7r=n(sx);oHe=r(n7r,"DetrFeatureExtractor"),n7r.forEach(t),rHe=r(h1e," (DETR model)"),h1e.forEach(t),tHe=i(de),Ig=s(de,"LI",{});var u1e=n(Ig);iH=s(u1e,"STRONG",{});var l7r=n(iH);aHe=r(l7r,"hubert"),l7r.forEach(t),sHe=r(u1e," \u2014 "),nx=s(u1e,"A",{href:!0});var i7r=n(nx);nHe=r(i7r,"Wav2Vec2FeatureExtractor"),i7r.forEach(t),lHe=r(u1e," (Hubert model)"),u1e.forEach(t),iHe=i(de),jg=s(de,"LI",{});var p1e=n(jg);dH=s(p1e,"STRONG",{});var d7r=n(dH);dHe=r(d7r,"layoutlmv2"),d7r.forEach(t),cHe=r(p1e," \u2014 "),lx=s(p1e,"A",{href:!0});var c7r=n(lx);mHe=r(c7r,"LayoutLMv2FeatureExtractor"),c7r.forEach(t),fHe=r(p1e," (LayoutLMv2 model)"),p1e.forEach(t),gHe=i(de),Ng=s(de,"LI",{});var _1e=n(Ng);cH=s(_1e,"STRONG",{});var m7r=n(cH);hHe=r(m7r,"perceiver"),m7r.forEach(t),uHe=r(_1e," \u2014 "),ix=s(_1e,"A",{href:!0});var f7r=n(ix);pHe=r(f7r,"PerceiverFeatureExtractor"),f7r.forEach(t),_He=r(_1e," (Perceiver model)"),_1e.forEach(t),bHe=i(de),Dg=s(de,"LI",{});var b1e=n(Dg);mH=s(b1e,"STRONG",{});var g7r=n(mH);vHe=r(g7r,"segformer"),g7r.forEach(t),THe=r(b1e," \u2014 "),dx=s(b1e,"A",{href:!0});var h7r=n(dx);FHe=r(h7r,"SegformerFeatureExtractor"),h7r.forEach(t),CHe=r(b1e," (SegFormer model)"),b1e.forEach(t),MHe=i(de),qg=s(de,"LI",{});var v1e=n(qg);fH=s(v1e,"STRONG",{});var u7r=n(fH);EHe=r(u7r,"speech_to_text"),u7r.forEach(t),yHe=r(v1e," \u2014 "),cx=s(v1e,"A",{href:!0});var p7r=n(cx);wHe=r(p7r,"Speech2TextFeatureExtractor"),p7r.forEach(t),AHe=r(v1e," (Speech2Text model)"),v1e.forEach(t),LHe=i(de),Gg=s(de,"LI",{});var T1e=n(Gg);gH=s(T1e,"STRONG",{});var _7r=n(gH);BHe=r(_7r,"swin"),_7r.forEach(t),xHe=r(T1e," \u2014 "),mx=s(T1e,"A",{href:!0});var b7r=n(mx);kHe=r(b7r,"ViTFeatureExtractor"),b7r.forEach(t),RHe=r(T1e," (Swin model)"),T1e.forEach(t),SHe=i(de),Og=s(de,"LI",{});var F1e=n(Og);hH=s(F1e,"STRONG",{});var v7r=n(hH);PHe=r(v7r,"vit"),v7r.forEach(t),$He=r(F1e," \u2014 "),fx=s(F1e,"A",{href:!0});var T7r=n(fx);IHe=r(T7r,"ViTFeatureExtractor"),T7r.forEach(t),jHe=r(F1e," (ViT model)"),F1e.forEach(t),NHe=i(de),Xg=s(de,"LI",{});var C1e=n(Xg);uH=s(C1e,"STRONG",{});var F7r=n(uH);DHe=r(F7r,"vit_mae"),F7r.forEach(t),qHe=r(C1e," \u2014 "),gx=s(C1e,"A",{href:!0});var C7r=n(gx);GHe=r(C7r,"ViTFeatureExtractor"),C7r.forEach(t),OHe=r(C1e," (ViTMAE model)"),C1e.forEach(t),XHe=i(de),zg=s(de,"LI",{});var M1e=n(zg);pH=s(M1e,"STRONG",{});var M7r=n(pH);zHe=r(M7r,"wav2vec2"),M7r.forEach(t),VHe=r(M1e," \u2014 "),hx=s(M1e,"A",{href:!0});var E7r=n(hx);WHe=r(E7r,"Wav2Vec2FeatureExtractor"),E7r.forEach(t),QHe=r(M1e," (Wav2Vec2 model)"),M1e.forEach(t),de.forEach(t),HHe=i(wt),f(Vg.$$.fragment,wt),UHe=i(wt),_H=s(wt,"P",{});var y7r=n(_H);JHe=r(y7r,"Examples:"),y7r.forEach(t),YHe=i(wt),f(Y3.$$.fragment,wt),wt.forEach(t),xE.forEach(t),MLe=i(d),Bi=s(d,"H2",{class:!0});var y8e=n(Bi);Wg=s(y8e,"A",{id:!0,class:!0,href:!0});var w7r=n(Wg);bH=s(w7r,"SPAN",{});var A7r=n(bH);f(K3.$$.fragment,A7r),A7r.forEach(t),w7r.forEach(t),KHe=i(y8e),vH=s(y8e,"SPAN",{});var L7r=n(vH);ZHe=r(L7r,"AutoProcessor"),L7r.forEach(t),y8e.forEach(t),ELe=i(d),ea=s(d,"DIV",{class:!0});var RE=n(ea);f(Z3.$$.fragment,RE),eUe=i(RE),e5=s(RE,"P",{});var w8e=n(e5);oUe=r(w8e,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ux=s(w8e,"A",{href:!0});var B7r=n(ux);rUe=r(B7r,"AutoProcessor.from_pretrained()"),B7r.forEach(t),tUe=r(w8e," class method."),w8e.forEach(t),aUe=i(RE),o5=s(RE,"P",{});var A8e=n(o5);sUe=r(A8e,"This class cannot be instantiated directly using "),TH=s(A8e,"CODE",{});var x7r=n(TH);nUe=r(x7r,"__init__()"),x7r.forEach(t),lUe=r(A8e," (throws an error)."),A8e.forEach(t),iUe=i(RE),Be=s(RE,"DIV",{class:!0});var At=n(Be);f(r5.$$.fragment,At),dUe=i(At),FH=s(At,"P",{});var k7r=n(FH);cUe=r(k7r,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),k7r.forEach(t),mUe=i(At),xi=s(At,"P",{});var DO=n(xi);fUe=r(DO,"The processor class to instantiate is selected based on the "),CH=s(DO,"CODE",{});var R7r=n(CH);gUe=r(R7r,"model_type"),R7r.forEach(t),hUe=r(DO,` property of the config object (either
passed as an argument or loaded from `),MH=s(DO,"CODE",{});var S7r=n(MH);uUe=r(S7r,"pretrained_model_name_or_path"),S7r.forEach(t),pUe=r(DO," if possible):"),DO.forEach(t),_Ue=i(At),ye=s(At,"UL",{});var Io=n(ye);Qg=s(Io,"LI",{});var E1e=n(Qg);EH=s(E1e,"STRONG",{});var P7r=n(EH);bUe=r(P7r,"clip"),P7r.forEach(t),vUe=r(E1e," \u2014 "),px=s(E1e,"A",{href:!0});var $7r=n(px);TUe=r($7r,"CLIPProcessor"),$7r.forEach(t),FUe=r(E1e," (CLIP model)"),E1e.forEach(t),CUe=i(Io),Hg=s(Io,"LI",{});var y1e=n(Hg);yH=s(y1e,"STRONG",{});var I7r=n(yH);MUe=r(I7r,"layoutlmv2"),I7r.forEach(t),EUe=r(y1e," \u2014 "),_x=s(y1e,"A",{href:!0});var j7r=n(_x);yUe=r(j7r,"LayoutLMv2Processor"),j7r.forEach(t),wUe=r(y1e," (LayoutLMv2 model)"),y1e.forEach(t),AUe=i(Io),Ug=s(Io,"LI",{});var w1e=n(Ug);wH=s(w1e,"STRONG",{});var N7r=n(wH);LUe=r(N7r,"layoutxlm"),N7r.forEach(t),BUe=r(w1e," \u2014 "),bx=s(w1e,"A",{href:!0});var D7r=n(bx);xUe=r(D7r,"LayoutXLMProcessor"),D7r.forEach(t),kUe=r(w1e," (LayoutXLM model)"),w1e.forEach(t),RUe=i(Io),Jg=s(Io,"LI",{});var A1e=n(Jg);AH=s(A1e,"STRONG",{});var q7r=n(AH);SUe=r(q7r,"speech_to_text"),q7r.forEach(t),PUe=r(A1e," \u2014 "),vx=s(A1e,"A",{href:!0});var G7r=n(vx);$Ue=r(G7r,"Speech2TextProcessor"),G7r.forEach(t),IUe=r(A1e," (Speech2Text model)"),A1e.forEach(t),jUe=i(Io),Yg=s(Io,"LI",{});var L1e=n(Yg);LH=s(L1e,"STRONG",{});var O7r=n(LH);NUe=r(O7r,"speech_to_text_2"),O7r.forEach(t),DUe=r(L1e," \u2014 "),Tx=s(L1e,"A",{href:!0});var X7r=n(Tx);qUe=r(X7r,"Speech2Text2Processor"),X7r.forEach(t),GUe=r(L1e," (Speech2Text2 model)"),L1e.forEach(t),OUe=i(Io),Kg=s(Io,"LI",{});var B1e=n(Kg);BH=s(B1e,"STRONG",{});var z7r=n(BH);XUe=r(z7r,"trocr"),z7r.forEach(t),zUe=r(B1e," \u2014 "),Fx=s(B1e,"A",{href:!0});var V7r=n(Fx);VUe=r(V7r,"TrOCRProcessor"),V7r.forEach(t),WUe=r(B1e," (TrOCR model)"),B1e.forEach(t),QUe=i(Io),Zg=s(Io,"LI",{});var x1e=n(Zg);xH=s(x1e,"STRONG",{});var W7r=n(xH);HUe=r(W7r,"vision-text-dual-encoder"),W7r.forEach(t),UUe=r(x1e," \u2014 "),Cx=s(x1e,"A",{href:!0});var Q7r=n(Cx);JUe=r(Q7r,"VisionTextDualEncoderProcessor"),Q7r.forEach(t),YUe=r(x1e," (VisionTextDualEncoder model)"),x1e.forEach(t),KUe=i(Io),eh=s(Io,"LI",{});var k1e=n(eh);kH=s(k1e,"STRONG",{});var H7r=n(kH);ZUe=r(H7r,"wav2vec2"),H7r.forEach(t),eJe=r(k1e," \u2014 "),Mx=s(k1e,"A",{href:!0});var U7r=n(Mx);oJe=r(U7r,"Wav2Vec2Processor"),U7r.forEach(t),rJe=r(k1e," (Wav2Vec2 model)"),k1e.forEach(t),Io.forEach(t),tJe=i(At),f(oh.$$.fragment,At),aJe=i(At),RH=s(At,"P",{});var J7r=n(RH);sJe=r(J7r,"Examples:"),J7r.forEach(t),nJe=i(At),f(t5.$$.fragment,At),At.forEach(t),RE.forEach(t),yLe=i(d),ki=s(d,"H2",{class:!0});var L8e=n(ki);rh=s(L8e,"A",{id:!0,class:!0,href:!0});var Y7r=n(rh);SH=s(Y7r,"SPAN",{});var K7r=n(SH);f(a5.$$.fragment,K7r),K7r.forEach(t),Y7r.forEach(t),lJe=i(L8e),PH=s(L8e,"SPAN",{});var Z7r=n(PH);iJe=r(Z7r,"AutoModel"),Z7r.forEach(t),L8e.forEach(t),wLe=i(d),Oo=s(d,"DIV",{class:!0});var kn=n(Oo);f(s5.$$.fragment,kn),dJe=i(kn),Ri=s(kn,"P",{});var qO=n(Ri);cJe=r(qO,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),$H=s(qO,"CODE",{});var e8r=n($H);mJe=r(e8r,"from_pretrained()"),e8r.forEach(t),fJe=r(qO,"class method or the "),IH=s(qO,"CODE",{});var o8r=n(IH);gJe=r(o8r,"from_config()"),o8r.forEach(t),hJe=r(qO,`class
method.`),qO.forEach(t),uJe=i(kn),n5=s(kn,"P",{});var B8e=n(n5);pJe=r(B8e,"This class cannot be instantiated directly using "),jH=s(B8e,"CODE",{});var r8r=n(jH);_Je=r(r8r,"__init__()"),r8r.forEach(t),bJe=r(B8e," (throws an error)."),B8e.forEach(t),vJe=i(kn),Pr=s(kn,"DIV",{class:!0});var Rn=n(Pr);f(l5.$$.fragment,Rn),TJe=i(Rn),NH=s(Rn,"P",{});var t8r=n(NH);FJe=r(t8r,"Instantiates one of the base model classes of the library from a configuration."),t8r.forEach(t),CJe=i(Rn),Si=s(Rn,"P",{});var GO=n(Si);MJe=r(GO,`Note:
Loading a model from its configuration file does `),DH=s(GO,"STRONG",{});var a8r=n(DH);EJe=r(a8r,"not"),a8r.forEach(t),yJe=r(GO,` load the model weights. It only affects the
model\u2019s configuration. Use `),qH=s(GO,"CODE",{});var s8r=n(qH);wJe=r(s8r,"from_pretrained()"),s8r.forEach(t),AJe=r(GO,"to load the model weights."),GO.forEach(t),LJe=i(Rn),GH=s(Rn,"P",{});var n8r=n(GH);BJe=r(n8r,"Examples:"),n8r.forEach(t),xJe=i(Rn),f(i5.$$.fragment,Rn),Rn.forEach(t),kJe=i(kn),xe=s(kn,"DIV",{class:!0});var Lt=n(xe);f(d5.$$.fragment,Lt),RJe=i(Lt),OH=s(Lt,"P",{});var l8r=n(OH);SJe=r(l8r,"Instantiate one of the base model classes of the library from a pretrained model."),l8r.forEach(t),PJe=i(Lt),$a=s(Lt,"P",{});var SE=n($a);$Je=r(SE,"The model class to instantiate is selected based on the "),XH=s(SE,"CODE",{});var i8r=n(XH);IJe=r(i8r,"model_type"),i8r.forEach(t),jJe=r(SE,` property of the config object (either
passed as an argument or loaded from `),zH=s(SE,"CODE",{});var d8r=n(zH);NJe=r(d8r,"pretrained_model_name_or_path"),d8r.forEach(t),DJe=r(SE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),VH=s(SE,"CODE",{});var c8r=n(VH);qJe=r(c8r,"pretrained_model_name_or_path"),c8r.forEach(t),GJe=r(SE,":"),SE.forEach(t),OJe=i(Lt),F=s(Lt,"UL",{});var C=n(F);th=s(C,"LI",{});var R1e=n(th);WH=s(R1e,"STRONG",{});var m8r=n(WH);XJe=r(m8r,"albert"),m8r.forEach(t),zJe=r(R1e," \u2014 "),Ex=s(R1e,"A",{href:!0});var f8r=n(Ex);VJe=r(f8r,"AlbertModel"),f8r.forEach(t),WJe=r(R1e," (ALBERT model)"),R1e.forEach(t),QJe=i(C),ah=s(C,"LI",{});var S1e=n(ah);QH=s(S1e,"STRONG",{});var g8r=n(QH);HJe=r(g8r,"bart"),g8r.forEach(t),UJe=r(S1e," \u2014 "),yx=s(S1e,"A",{href:!0});var h8r=n(yx);JJe=r(h8r,"BartModel"),h8r.forEach(t),YJe=r(S1e," (BART model)"),S1e.forEach(t),KJe=i(C),sh=s(C,"LI",{});var P1e=n(sh);HH=s(P1e,"STRONG",{});var u8r=n(HH);ZJe=r(u8r,"beit"),u8r.forEach(t),eYe=r(P1e," \u2014 "),wx=s(P1e,"A",{href:!0});var p8r=n(wx);oYe=r(p8r,"BeitModel"),p8r.forEach(t),rYe=r(P1e," (BEiT model)"),P1e.forEach(t),tYe=i(C),nh=s(C,"LI",{});var $1e=n(nh);UH=s($1e,"STRONG",{});var _8r=n(UH);aYe=r(_8r,"bert"),_8r.forEach(t),sYe=r($1e," \u2014 "),Ax=s($1e,"A",{href:!0});var b8r=n(Ax);nYe=r(b8r,"BertModel"),b8r.forEach(t),lYe=r($1e," (BERT model)"),$1e.forEach(t),iYe=i(C),lh=s(C,"LI",{});var I1e=n(lh);JH=s(I1e,"STRONG",{});var v8r=n(JH);dYe=r(v8r,"bert-generation"),v8r.forEach(t),cYe=r(I1e," \u2014 "),Lx=s(I1e,"A",{href:!0});var T8r=n(Lx);mYe=r(T8r,"BertGenerationEncoder"),T8r.forEach(t),fYe=r(I1e," (Bert Generation model)"),I1e.forEach(t),gYe=i(C),ih=s(C,"LI",{});var j1e=n(ih);YH=s(j1e,"STRONG",{});var F8r=n(YH);hYe=r(F8r,"big_bird"),F8r.forEach(t),uYe=r(j1e," \u2014 "),Bx=s(j1e,"A",{href:!0});var C8r=n(Bx);pYe=r(C8r,"BigBirdModel"),C8r.forEach(t),_Ye=r(j1e," (BigBird model)"),j1e.forEach(t),bYe=i(C),dh=s(C,"LI",{});var N1e=n(dh);KH=s(N1e,"STRONG",{});var M8r=n(KH);vYe=r(M8r,"bigbird_pegasus"),M8r.forEach(t),TYe=r(N1e," \u2014 "),xx=s(N1e,"A",{href:!0});var E8r=n(xx);FYe=r(E8r,"BigBirdPegasusModel"),E8r.forEach(t),CYe=r(N1e," (BigBirdPegasus model)"),N1e.forEach(t),MYe=i(C),ch=s(C,"LI",{});var D1e=n(ch);ZH=s(D1e,"STRONG",{});var y8r=n(ZH);EYe=r(y8r,"blenderbot"),y8r.forEach(t),yYe=r(D1e," \u2014 "),kx=s(D1e,"A",{href:!0});var w8r=n(kx);wYe=r(w8r,"BlenderbotModel"),w8r.forEach(t),AYe=r(D1e," (Blenderbot model)"),D1e.forEach(t),LYe=i(C),mh=s(C,"LI",{});var q1e=n(mh);eU=s(q1e,"STRONG",{});var A8r=n(eU);BYe=r(A8r,"blenderbot-small"),A8r.forEach(t),xYe=r(q1e," \u2014 "),Rx=s(q1e,"A",{href:!0});var L8r=n(Rx);kYe=r(L8r,"BlenderbotSmallModel"),L8r.forEach(t),RYe=r(q1e," (BlenderbotSmall model)"),q1e.forEach(t),SYe=i(C),fh=s(C,"LI",{});var G1e=n(fh);oU=s(G1e,"STRONG",{});var B8r=n(oU);PYe=r(B8r,"camembert"),B8r.forEach(t),$Ye=r(G1e," \u2014 "),Sx=s(G1e,"A",{href:!0});var x8r=n(Sx);IYe=r(x8r,"CamembertModel"),x8r.forEach(t),jYe=r(G1e," (CamemBERT model)"),G1e.forEach(t),NYe=i(C),gh=s(C,"LI",{});var O1e=n(gh);rU=s(O1e,"STRONG",{});var k8r=n(rU);DYe=r(k8r,"canine"),k8r.forEach(t),qYe=r(O1e," \u2014 "),Px=s(O1e,"A",{href:!0});var R8r=n(Px);GYe=r(R8r,"CanineModel"),R8r.forEach(t),OYe=r(O1e," (Canine model)"),O1e.forEach(t),XYe=i(C),hh=s(C,"LI",{});var X1e=n(hh);tU=s(X1e,"STRONG",{});var S8r=n(tU);zYe=r(S8r,"clip"),S8r.forEach(t),VYe=r(X1e," \u2014 "),$x=s(X1e,"A",{href:!0});var P8r=n($x);WYe=r(P8r,"CLIPModel"),P8r.forEach(t),QYe=r(X1e," (CLIP model)"),X1e.forEach(t),HYe=i(C),uh=s(C,"LI",{});var z1e=n(uh);aU=s(z1e,"STRONG",{});var $8r=n(aU);UYe=r($8r,"convbert"),$8r.forEach(t),JYe=r(z1e," \u2014 "),Ix=s(z1e,"A",{href:!0});var I8r=n(Ix);YYe=r(I8r,"ConvBertModel"),I8r.forEach(t),KYe=r(z1e," (ConvBERT model)"),z1e.forEach(t),ZYe=i(C),ph=s(C,"LI",{});var V1e=n(ph);sU=s(V1e,"STRONG",{});var j8r=n(sU);eKe=r(j8r,"convnext"),j8r.forEach(t),oKe=r(V1e," \u2014 "),jx=s(V1e,"A",{href:!0});var N8r=n(jx);rKe=r(N8r,"ConvNextModel"),N8r.forEach(t),tKe=r(V1e," (ConvNext model)"),V1e.forEach(t),aKe=i(C),_h=s(C,"LI",{});var W1e=n(_h);nU=s(W1e,"STRONG",{});var D8r=n(nU);sKe=r(D8r,"ctrl"),D8r.forEach(t),nKe=r(W1e," \u2014 "),Nx=s(W1e,"A",{href:!0});var q8r=n(Nx);lKe=r(q8r,"CTRLModel"),q8r.forEach(t),iKe=r(W1e," (CTRL model)"),W1e.forEach(t),dKe=i(C),bh=s(C,"LI",{});var Q1e=n(bh);lU=s(Q1e,"STRONG",{});var G8r=n(lU);cKe=r(G8r,"deberta"),G8r.forEach(t),mKe=r(Q1e," \u2014 "),Dx=s(Q1e,"A",{href:!0});var O8r=n(Dx);fKe=r(O8r,"DebertaModel"),O8r.forEach(t),gKe=r(Q1e," (DeBERTa model)"),Q1e.forEach(t),hKe=i(C),vh=s(C,"LI",{});var H1e=n(vh);iU=s(H1e,"STRONG",{});var X8r=n(iU);uKe=r(X8r,"deberta-v2"),X8r.forEach(t),pKe=r(H1e," \u2014 "),qx=s(H1e,"A",{href:!0});var z8r=n(qx);_Ke=r(z8r,"DebertaV2Model"),z8r.forEach(t),bKe=r(H1e," (DeBERTa-v2 model)"),H1e.forEach(t),vKe=i(C),Th=s(C,"LI",{});var U1e=n(Th);dU=s(U1e,"STRONG",{});var V8r=n(dU);TKe=r(V8r,"deit"),V8r.forEach(t),FKe=r(U1e," \u2014 "),Gx=s(U1e,"A",{href:!0});var W8r=n(Gx);CKe=r(W8r,"DeiTModel"),W8r.forEach(t),MKe=r(U1e," (DeiT model)"),U1e.forEach(t),EKe=i(C),Fh=s(C,"LI",{});var J1e=n(Fh);cU=s(J1e,"STRONG",{});var Q8r=n(cU);yKe=r(Q8r,"detr"),Q8r.forEach(t),wKe=r(J1e," \u2014 "),Ox=s(J1e,"A",{href:!0});var H8r=n(Ox);AKe=r(H8r,"DetrModel"),H8r.forEach(t),LKe=r(J1e," (DETR model)"),J1e.forEach(t),BKe=i(C),Ch=s(C,"LI",{});var Y1e=n(Ch);mU=s(Y1e,"STRONG",{});var U8r=n(mU);xKe=r(U8r,"distilbert"),U8r.forEach(t),kKe=r(Y1e," \u2014 "),Xx=s(Y1e,"A",{href:!0});var J8r=n(Xx);RKe=r(J8r,"DistilBertModel"),J8r.forEach(t),SKe=r(Y1e," (DistilBERT model)"),Y1e.forEach(t),PKe=i(C),Mh=s(C,"LI",{});var K1e=n(Mh);fU=s(K1e,"STRONG",{});var Y8r=n(fU);$Ke=r(Y8r,"dpr"),Y8r.forEach(t),IKe=r(K1e," \u2014 "),zx=s(K1e,"A",{href:!0});var K8r=n(zx);jKe=r(K8r,"DPRQuestionEncoder"),K8r.forEach(t),NKe=r(K1e," (DPR model)"),K1e.forEach(t),DKe=i(C),Eh=s(C,"LI",{});var Z1e=n(Eh);gU=s(Z1e,"STRONG",{});var Z8r=n(gU);qKe=r(Z8r,"electra"),Z8r.forEach(t),GKe=r(Z1e," \u2014 "),Vx=s(Z1e,"A",{href:!0});var e9r=n(Vx);OKe=r(e9r,"ElectraModel"),e9r.forEach(t),XKe=r(Z1e," (ELECTRA model)"),Z1e.forEach(t),zKe=i(C),yh=s(C,"LI",{});var eFe=n(yh);hU=s(eFe,"STRONG",{});var o9r=n(hU);VKe=r(o9r,"flaubert"),o9r.forEach(t),WKe=r(eFe," \u2014 "),Wx=s(eFe,"A",{href:!0});var r9r=n(Wx);QKe=r(r9r,"FlaubertModel"),r9r.forEach(t),HKe=r(eFe," (FlauBERT model)"),eFe.forEach(t),UKe=i(C),wh=s(C,"LI",{});var oFe=n(wh);uU=s(oFe,"STRONG",{});var t9r=n(uU);JKe=r(t9r,"fnet"),t9r.forEach(t),YKe=r(oFe," \u2014 "),Qx=s(oFe,"A",{href:!0});var a9r=n(Qx);KKe=r(a9r,"FNetModel"),a9r.forEach(t),ZKe=r(oFe," (FNet model)"),oFe.forEach(t),eZe=i(C),Ah=s(C,"LI",{});var rFe=n(Ah);pU=s(rFe,"STRONG",{});var s9r=n(pU);oZe=r(s9r,"fsmt"),s9r.forEach(t),rZe=r(rFe," \u2014 "),Hx=s(rFe,"A",{href:!0});var n9r=n(Hx);tZe=r(n9r,"FSMTModel"),n9r.forEach(t),aZe=r(rFe," (FairSeq Machine-Translation model)"),rFe.forEach(t),sZe=i(C),wn=s(C,"LI",{});var KL=n(wn);_U=s(KL,"STRONG",{});var l9r=n(_U);nZe=r(l9r,"funnel"),l9r.forEach(t),lZe=r(KL," \u2014 "),Ux=s(KL,"A",{href:!0});var i9r=n(Ux);iZe=r(i9r,"FunnelModel"),i9r.forEach(t),dZe=r(KL," or "),Jx=s(KL,"A",{href:!0});var d9r=n(Jx);cZe=r(d9r,"FunnelBaseModel"),d9r.forEach(t),mZe=r(KL," (Funnel Transformer model)"),KL.forEach(t),fZe=i(C),Lh=s(C,"LI",{});var tFe=n(Lh);bU=s(tFe,"STRONG",{});var c9r=n(bU);gZe=r(c9r,"gpt2"),c9r.forEach(t),hZe=r(tFe," \u2014 "),Yx=s(tFe,"A",{href:!0});var m9r=n(Yx);uZe=r(m9r,"GPT2Model"),m9r.forEach(t),pZe=r(tFe," (OpenAI GPT-2 model)"),tFe.forEach(t),_Ze=i(C),Bh=s(C,"LI",{});var aFe=n(Bh);vU=s(aFe,"STRONG",{});var f9r=n(vU);bZe=r(f9r,"gpt_neo"),f9r.forEach(t),vZe=r(aFe," \u2014 "),Kx=s(aFe,"A",{href:!0});var g9r=n(Kx);TZe=r(g9r,"GPTNeoModel"),g9r.forEach(t),FZe=r(aFe," (GPT Neo model)"),aFe.forEach(t),CZe=i(C),xh=s(C,"LI",{});var sFe=n(xh);TU=s(sFe,"STRONG",{});var h9r=n(TU);MZe=r(h9r,"gptj"),h9r.forEach(t),EZe=r(sFe," \u2014 "),Zx=s(sFe,"A",{href:!0});var u9r=n(Zx);yZe=r(u9r,"GPTJModel"),u9r.forEach(t),wZe=r(sFe," (GPT-J model)"),sFe.forEach(t),AZe=i(C),kh=s(C,"LI",{});var nFe=n(kh);FU=s(nFe,"STRONG",{});var p9r=n(FU);LZe=r(p9r,"hubert"),p9r.forEach(t),BZe=r(nFe," \u2014 "),ek=s(nFe,"A",{href:!0});var _9r=n(ek);xZe=r(_9r,"HubertModel"),_9r.forEach(t),kZe=r(nFe," (Hubert model)"),nFe.forEach(t),RZe=i(C),Rh=s(C,"LI",{});var lFe=n(Rh);CU=s(lFe,"STRONG",{});var b9r=n(CU);SZe=r(b9r,"ibert"),b9r.forEach(t),PZe=r(lFe," \u2014 "),ok=s(lFe,"A",{href:!0});var v9r=n(ok);$Ze=r(v9r,"IBertModel"),v9r.forEach(t),IZe=r(lFe," (I-BERT model)"),lFe.forEach(t),jZe=i(C),Sh=s(C,"LI",{});var iFe=n(Sh);MU=s(iFe,"STRONG",{});var T9r=n(MU);NZe=r(T9r,"imagegpt"),T9r.forEach(t),DZe=r(iFe," \u2014 "),rk=s(iFe,"A",{href:!0});var F9r=n(rk);qZe=r(F9r,"ImageGPTModel"),F9r.forEach(t),GZe=r(iFe," (ImageGPT model)"),iFe.forEach(t),OZe=i(C),Ph=s(C,"LI",{});var dFe=n(Ph);EU=s(dFe,"STRONG",{});var C9r=n(EU);XZe=r(C9r,"layoutlm"),C9r.forEach(t),zZe=r(dFe," \u2014 "),tk=s(dFe,"A",{href:!0});var M9r=n(tk);VZe=r(M9r,"LayoutLMModel"),M9r.forEach(t),WZe=r(dFe," (LayoutLM model)"),dFe.forEach(t),QZe=i(C),$h=s(C,"LI",{});var cFe=n($h);yU=s(cFe,"STRONG",{});var E9r=n(yU);HZe=r(E9r,"layoutlmv2"),E9r.forEach(t),UZe=r(cFe," \u2014 "),ak=s(cFe,"A",{href:!0});var y9r=n(ak);JZe=r(y9r,"LayoutLMv2Model"),y9r.forEach(t),YZe=r(cFe," (LayoutLMv2 model)"),cFe.forEach(t),KZe=i(C),Ih=s(C,"LI",{});var mFe=n(Ih);wU=s(mFe,"STRONG",{});var w9r=n(wU);ZZe=r(w9r,"led"),w9r.forEach(t),eeo=r(mFe," \u2014 "),sk=s(mFe,"A",{href:!0});var A9r=n(sk);oeo=r(A9r,"LEDModel"),A9r.forEach(t),reo=r(mFe," (LED model)"),mFe.forEach(t),teo=i(C),jh=s(C,"LI",{});var fFe=n(jh);AU=s(fFe,"STRONG",{});var L9r=n(AU);aeo=r(L9r,"longformer"),L9r.forEach(t),seo=r(fFe," \u2014 "),nk=s(fFe,"A",{href:!0});var B9r=n(nk);neo=r(B9r,"LongformerModel"),B9r.forEach(t),leo=r(fFe," (Longformer model)"),fFe.forEach(t),ieo=i(C),Nh=s(C,"LI",{});var gFe=n(Nh);LU=s(gFe,"STRONG",{});var x9r=n(LU);deo=r(x9r,"luke"),x9r.forEach(t),ceo=r(gFe," \u2014 "),lk=s(gFe,"A",{href:!0});var k9r=n(lk);meo=r(k9r,"LukeModel"),k9r.forEach(t),feo=r(gFe," (LUKE model)"),gFe.forEach(t),geo=i(C),Dh=s(C,"LI",{});var hFe=n(Dh);BU=s(hFe,"STRONG",{});var R9r=n(BU);heo=r(R9r,"lxmert"),R9r.forEach(t),ueo=r(hFe," \u2014 "),ik=s(hFe,"A",{href:!0});var S9r=n(ik);peo=r(S9r,"LxmertModel"),S9r.forEach(t),_eo=r(hFe," (LXMERT model)"),hFe.forEach(t),beo=i(C),qh=s(C,"LI",{});var uFe=n(qh);xU=s(uFe,"STRONG",{});var P9r=n(xU);veo=r(P9r,"m2m_100"),P9r.forEach(t),Teo=r(uFe," \u2014 "),dk=s(uFe,"A",{href:!0});var $9r=n(dk);Feo=r($9r,"M2M100Model"),$9r.forEach(t),Ceo=r(uFe," (M2M100 model)"),uFe.forEach(t),Meo=i(C),Gh=s(C,"LI",{});var pFe=n(Gh);kU=s(pFe,"STRONG",{});var I9r=n(kU);Eeo=r(I9r,"marian"),I9r.forEach(t),yeo=r(pFe," \u2014 "),ck=s(pFe,"A",{href:!0});var j9r=n(ck);weo=r(j9r,"MarianModel"),j9r.forEach(t),Aeo=r(pFe," (Marian model)"),pFe.forEach(t),Leo=i(C),Oh=s(C,"LI",{});var _Fe=n(Oh);RU=s(_Fe,"STRONG",{});var N9r=n(RU);Beo=r(N9r,"mbart"),N9r.forEach(t),xeo=r(_Fe," \u2014 "),mk=s(_Fe,"A",{href:!0});var D9r=n(mk);keo=r(D9r,"MBartModel"),D9r.forEach(t),Reo=r(_Fe," (mBART model)"),_Fe.forEach(t),Seo=i(C),Xh=s(C,"LI",{});var bFe=n(Xh);SU=s(bFe,"STRONG",{});var q9r=n(SU);Peo=r(q9r,"megatron-bert"),q9r.forEach(t),$eo=r(bFe," \u2014 "),fk=s(bFe,"A",{href:!0});var G9r=n(fk);Ieo=r(G9r,"MegatronBertModel"),G9r.forEach(t),jeo=r(bFe," (MegatronBert model)"),bFe.forEach(t),Neo=i(C),zh=s(C,"LI",{});var vFe=n(zh);PU=s(vFe,"STRONG",{});var O9r=n(PU);Deo=r(O9r,"mobilebert"),O9r.forEach(t),qeo=r(vFe," \u2014 "),gk=s(vFe,"A",{href:!0});var X9r=n(gk);Geo=r(X9r,"MobileBertModel"),X9r.forEach(t),Oeo=r(vFe," (MobileBERT model)"),vFe.forEach(t),Xeo=i(C),Vh=s(C,"LI",{});var TFe=n(Vh);$U=s(TFe,"STRONG",{});var z9r=n($U);zeo=r(z9r,"mpnet"),z9r.forEach(t),Veo=r(TFe," \u2014 "),hk=s(TFe,"A",{href:!0});var V9r=n(hk);Weo=r(V9r,"MPNetModel"),V9r.forEach(t),Qeo=r(TFe," (MPNet model)"),TFe.forEach(t),Heo=i(C),Wh=s(C,"LI",{});var FFe=n(Wh);IU=s(FFe,"STRONG",{});var W9r=n(IU);Ueo=r(W9r,"mt5"),W9r.forEach(t),Jeo=r(FFe," \u2014 "),uk=s(FFe,"A",{href:!0});var Q9r=n(uk);Yeo=r(Q9r,"MT5Model"),Q9r.forEach(t),Keo=r(FFe," (mT5 model)"),FFe.forEach(t),Zeo=i(C),Qh=s(C,"LI",{});var CFe=n(Qh);jU=s(CFe,"STRONG",{});var H9r=n(jU);eoo=r(H9r,"nystromformer"),H9r.forEach(t),ooo=r(CFe," \u2014 "),pk=s(CFe,"A",{href:!0});var U9r=n(pk);roo=r(U9r,"NystromformerModel"),U9r.forEach(t),too=r(CFe," (Nystromformer model)"),CFe.forEach(t),aoo=i(C),Hh=s(C,"LI",{});var MFe=n(Hh);NU=s(MFe,"STRONG",{});var J9r=n(NU);soo=r(J9r,"openai-gpt"),J9r.forEach(t),noo=r(MFe," \u2014 "),_k=s(MFe,"A",{href:!0});var Y9r=n(_k);loo=r(Y9r,"OpenAIGPTModel"),Y9r.forEach(t),ioo=r(MFe," (OpenAI GPT model)"),MFe.forEach(t),doo=i(C),Uh=s(C,"LI",{});var EFe=n(Uh);DU=s(EFe,"STRONG",{});var K9r=n(DU);coo=r(K9r,"pegasus"),K9r.forEach(t),moo=r(EFe," \u2014 "),bk=s(EFe,"A",{href:!0});var Z9r=n(bk);foo=r(Z9r,"PegasusModel"),Z9r.forEach(t),goo=r(EFe," (Pegasus model)"),EFe.forEach(t),hoo=i(C),Jh=s(C,"LI",{});var yFe=n(Jh);qU=s(yFe,"STRONG",{});var eBr=n(qU);uoo=r(eBr,"perceiver"),eBr.forEach(t),poo=r(yFe," \u2014 "),vk=s(yFe,"A",{href:!0});var oBr=n(vk);_oo=r(oBr,"PerceiverModel"),oBr.forEach(t),boo=r(yFe," (Perceiver model)"),yFe.forEach(t),voo=i(C),Yh=s(C,"LI",{});var wFe=n(Yh);GU=s(wFe,"STRONG",{});var rBr=n(GU);Too=r(rBr,"prophetnet"),rBr.forEach(t),Foo=r(wFe," \u2014 "),Tk=s(wFe,"A",{href:!0});var tBr=n(Tk);Coo=r(tBr,"ProphetNetModel"),tBr.forEach(t),Moo=r(wFe," (ProphetNet model)"),wFe.forEach(t),Eoo=i(C),Kh=s(C,"LI",{});var AFe=n(Kh);OU=s(AFe,"STRONG",{});var aBr=n(OU);yoo=r(aBr,"qdqbert"),aBr.forEach(t),woo=r(AFe," \u2014 "),XU=s(AFe,"CODE",{});var sBr=n(XU);Aoo=r(sBr,"QDQBertModel"),sBr.forEach(t),Loo=r(AFe,"(QDQBert model)"),AFe.forEach(t),Boo=i(C),Zh=s(C,"LI",{});var LFe=n(Zh);zU=s(LFe,"STRONG",{});var nBr=n(zU);xoo=r(nBr,"reformer"),nBr.forEach(t),koo=r(LFe," \u2014 "),Fk=s(LFe,"A",{href:!0});var lBr=n(Fk);Roo=r(lBr,"ReformerModel"),lBr.forEach(t),Soo=r(LFe," (Reformer model)"),LFe.forEach(t),Poo=i(C),eu=s(C,"LI",{});var BFe=n(eu);VU=s(BFe,"STRONG",{});var iBr=n(VU);$oo=r(iBr,"rembert"),iBr.forEach(t),Ioo=r(BFe," \u2014 "),Ck=s(BFe,"A",{href:!0});var dBr=n(Ck);joo=r(dBr,"RemBertModel"),dBr.forEach(t),Noo=r(BFe," (RemBERT model)"),BFe.forEach(t),Doo=i(C),ou=s(C,"LI",{});var xFe=n(ou);WU=s(xFe,"STRONG",{});var cBr=n(WU);qoo=r(cBr,"retribert"),cBr.forEach(t),Goo=r(xFe," \u2014 "),Mk=s(xFe,"A",{href:!0});var mBr=n(Mk);Ooo=r(mBr,"RetriBertModel"),mBr.forEach(t),Xoo=r(xFe," (RetriBERT model)"),xFe.forEach(t),zoo=i(C),ru=s(C,"LI",{});var kFe=n(ru);QU=s(kFe,"STRONG",{});var fBr=n(QU);Voo=r(fBr,"roberta"),fBr.forEach(t),Woo=r(kFe," \u2014 "),Ek=s(kFe,"A",{href:!0});var gBr=n(Ek);Qoo=r(gBr,"RobertaModel"),gBr.forEach(t),Hoo=r(kFe," (RoBERTa model)"),kFe.forEach(t),Uoo=i(C),tu=s(C,"LI",{});var RFe=n(tu);HU=s(RFe,"STRONG",{});var hBr=n(HU);Joo=r(hBr,"roformer"),hBr.forEach(t),Yoo=r(RFe," \u2014 "),yk=s(RFe,"A",{href:!0});var uBr=n(yk);Koo=r(uBr,"RoFormerModel"),uBr.forEach(t),Zoo=r(RFe," (RoFormer model)"),RFe.forEach(t),ero=i(C),au=s(C,"LI",{});var SFe=n(au);UU=s(SFe,"STRONG",{});var pBr=n(UU);oro=r(pBr,"segformer"),pBr.forEach(t),rro=r(SFe," \u2014 "),wk=s(SFe,"A",{href:!0});var _Br=n(wk);tro=r(_Br,"SegformerModel"),_Br.forEach(t),aro=r(SFe," (SegFormer model)"),SFe.forEach(t),sro=i(C),su=s(C,"LI",{});var PFe=n(su);JU=s(PFe,"STRONG",{});var bBr=n(JU);nro=r(bBr,"sew"),bBr.forEach(t),lro=r(PFe," \u2014 "),Ak=s(PFe,"A",{href:!0});var vBr=n(Ak);iro=r(vBr,"SEWModel"),vBr.forEach(t),dro=r(PFe," (SEW model)"),PFe.forEach(t),cro=i(C),nu=s(C,"LI",{});var $Fe=n(nu);YU=s($Fe,"STRONG",{});var TBr=n(YU);mro=r(TBr,"sew-d"),TBr.forEach(t),fro=r($Fe," \u2014 "),Lk=s($Fe,"A",{href:!0});var FBr=n(Lk);gro=r(FBr,"SEWDModel"),FBr.forEach(t),hro=r($Fe," (SEW-D model)"),$Fe.forEach(t),uro=i(C),lu=s(C,"LI",{});var IFe=n(lu);KU=s(IFe,"STRONG",{});var CBr=n(KU);pro=r(CBr,"speech_to_text"),CBr.forEach(t),_ro=r(IFe," \u2014 "),Bk=s(IFe,"A",{href:!0});var MBr=n(Bk);bro=r(MBr,"Speech2TextModel"),MBr.forEach(t),vro=r(IFe," (Speech2Text model)"),IFe.forEach(t),Tro=i(C),iu=s(C,"LI",{});var jFe=n(iu);ZU=s(jFe,"STRONG",{});var EBr=n(ZU);Fro=r(EBr,"splinter"),EBr.forEach(t),Cro=r(jFe," \u2014 "),xk=s(jFe,"A",{href:!0});var yBr=n(xk);Mro=r(yBr,"SplinterModel"),yBr.forEach(t),Ero=r(jFe," (Splinter model)"),jFe.forEach(t),yro=i(C),du=s(C,"LI",{});var NFe=n(du);eJ=s(NFe,"STRONG",{});var wBr=n(eJ);wro=r(wBr,"squeezebert"),wBr.forEach(t),Aro=r(NFe," \u2014 "),kk=s(NFe,"A",{href:!0});var ABr=n(kk);Lro=r(ABr,"SqueezeBertModel"),ABr.forEach(t),Bro=r(NFe," (SqueezeBERT model)"),NFe.forEach(t),xro=i(C),cu=s(C,"LI",{});var DFe=n(cu);oJ=s(DFe,"STRONG",{});var LBr=n(oJ);kro=r(LBr,"swin"),LBr.forEach(t),Rro=r(DFe," \u2014 "),Rk=s(DFe,"A",{href:!0});var BBr=n(Rk);Sro=r(BBr,"SwinModel"),BBr.forEach(t),Pro=r(DFe," (Swin model)"),DFe.forEach(t),$ro=i(C),mu=s(C,"LI",{});var qFe=n(mu);rJ=s(qFe,"STRONG",{});var xBr=n(rJ);Iro=r(xBr,"t5"),xBr.forEach(t),jro=r(qFe," \u2014 "),Sk=s(qFe,"A",{href:!0});var kBr=n(Sk);Nro=r(kBr,"T5Model"),kBr.forEach(t),Dro=r(qFe," (T5 model)"),qFe.forEach(t),qro=i(C),fu=s(C,"LI",{});var GFe=n(fu);tJ=s(GFe,"STRONG",{});var RBr=n(tJ);Gro=r(RBr,"tapas"),RBr.forEach(t),Oro=r(GFe," \u2014 "),Pk=s(GFe,"A",{href:!0});var SBr=n(Pk);Xro=r(SBr,"TapasModel"),SBr.forEach(t),zro=r(GFe," (TAPAS model)"),GFe.forEach(t),Vro=i(C),gu=s(C,"LI",{});var OFe=n(gu);aJ=s(OFe,"STRONG",{});var PBr=n(aJ);Wro=r(PBr,"transfo-xl"),PBr.forEach(t),Qro=r(OFe," \u2014 "),$k=s(OFe,"A",{href:!0});var $Br=n($k);Hro=r($Br,"TransfoXLModel"),$Br.forEach(t),Uro=r(OFe," (Transformer-XL model)"),OFe.forEach(t),Jro=i(C),hu=s(C,"LI",{});var XFe=n(hu);sJ=s(XFe,"STRONG",{});var IBr=n(sJ);Yro=r(IBr,"unispeech"),IBr.forEach(t),Kro=r(XFe," \u2014 "),Ik=s(XFe,"A",{href:!0});var jBr=n(Ik);Zro=r(jBr,"UniSpeechModel"),jBr.forEach(t),eto=r(XFe," (UniSpeech model)"),XFe.forEach(t),oto=i(C),uu=s(C,"LI",{});var zFe=n(uu);nJ=s(zFe,"STRONG",{});var NBr=n(nJ);rto=r(NBr,"unispeech-sat"),NBr.forEach(t),tto=r(zFe," \u2014 "),jk=s(zFe,"A",{href:!0});var DBr=n(jk);ato=r(DBr,"UniSpeechSatModel"),DBr.forEach(t),sto=r(zFe," (UniSpeechSat model)"),zFe.forEach(t),nto=i(C),pu=s(C,"LI",{});var VFe=n(pu);lJ=s(VFe,"STRONG",{});var qBr=n(lJ);lto=r(qBr,"vilt"),qBr.forEach(t),ito=r(VFe," \u2014 "),Nk=s(VFe,"A",{href:!0});var GBr=n(Nk);dto=r(GBr,"ViltModel"),GBr.forEach(t),cto=r(VFe," (ViLT model)"),VFe.forEach(t),mto=i(C),_u=s(C,"LI",{});var WFe=n(_u);iJ=s(WFe,"STRONG",{});var OBr=n(iJ);fto=r(OBr,"vision-text-dual-encoder"),OBr.forEach(t),gto=r(WFe," \u2014 "),Dk=s(WFe,"A",{href:!0});var XBr=n(Dk);hto=r(XBr,"VisionTextDualEncoderModel"),XBr.forEach(t),uto=r(WFe," (VisionTextDualEncoder model)"),WFe.forEach(t),pto=i(C),bu=s(C,"LI",{});var QFe=n(bu);dJ=s(QFe,"STRONG",{});var zBr=n(dJ);_to=r(zBr,"visual_bert"),zBr.forEach(t),bto=r(QFe," \u2014 "),qk=s(QFe,"A",{href:!0});var VBr=n(qk);vto=r(VBr,"VisualBertModel"),VBr.forEach(t),Tto=r(QFe," (VisualBert model)"),QFe.forEach(t),Fto=i(C),vu=s(C,"LI",{});var HFe=n(vu);cJ=s(HFe,"STRONG",{});var WBr=n(cJ);Cto=r(WBr,"vit"),WBr.forEach(t),Mto=r(HFe," \u2014 "),Gk=s(HFe,"A",{href:!0});var QBr=n(Gk);Eto=r(QBr,"ViTModel"),QBr.forEach(t),yto=r(HFe," (ViT model)"),HFe.forEach(t),wto=i(C),Tu=s(C,"LI",{});var UFe=n(Tu);mJ=s(UFe,"STRONG",{});var HBr=n(mJ);Ato=r(HBr,"vit_mae"),HBr.forEach(t),Lto=r(UFe," \u2014 "),Ok=s(UFe,"A",{href:!0});var UBr=n(Ok);Bto=r(UBr,"ViTMAEModel"),UBr.forEach(t),xto=r(UFe," (ViTMAE model)"),UFe.forEach(t),kto=i(C),Fu=s(C,"LI",{});var JFe=n(Fu);fJ=s(JFe,"STRONG",{});var JBr=n(fJ);Rto=r(JBr,"wav2vec2"),JBr.forEach(t),Sto=r(JFe," \u2014 "),Xk=s(JFe,"A",{href:!0});var YBr=n(Xk);Pto=r(YBr,"Wav2Vec2Model"),YBr.forEach(t),$to=r(JFe," (Wav2Vec2 model)"),JFe.forEach(t),Ito=i(C),Cu=s(C,"LI",{});var YFe=n(Cu);gJ=s(YFe,"STRONG",{});var KBr=n(gJ);jto=r(KBr,"wavlm"),KBr.forEach(t),Nto=r(YFe," \u2014 "),zk=s(YFe,"A",{href:!0});var ZBr=n(zk);Dto=r(ZBr,"WavLMModel"),ZBr.forEach(t),qto=r(YFe," (WavLM model)"),YFe.forEach(t),Gto=i(C),Mu=s(C,"LI",{});var KFe=n(Mu);hJ=s(KFe,"STRONG",{});var exr=n(hJ);Oto=r(exr,"xglm"),exr.forEach(t),Xto=r(KFe," \u2014 "),Vk=s(KFe,"A",{href:!0});var oxr=n(Vk);zto=r(oxr,"XGLMModel"),oxr.forEach(t),Vto=r(KFe," (XGLM model)"),KFe.forEach(t),Wto=i(C),Eu=s(C,"LI",{});var ZFe=n(Eu);uJ=s(ZFe,"STRONG",{});var rxr=n(uJ);Qto=r(rxr,"xlm"),rxr.forEach(t),Hto=r(ZFe," \u2014 "),Wk=s(ZFe,"A",{href:!0});var txr=n(Wk);Uto=r(txr,"XLMModel"),txr.forEach(t),Jto=r(ZFe," (XLM model)"),ZFe.forEach(t),Yto=i(C),yu=s(C,"LI",{});var eCe=n(yu);pJ=s(eCe,"STRONG",{});var axr=n(pJ);Kto=r(axr,"xlm-prophetnet"),axr.forEach(t),Zto=r(eCe," \u2014 "),Qk=s(eCe,"A",{href:!0});var sxr=n(Qk);eao=r(sxr,"XLMProphetNetModel"),sxr.forEach(t),oao=r(eCe," (XLMProphetNet model)"),eCe.forEach(t),rao=i(C),wu=s(C,"LI",{});var oCe=n(wu);_J=s(oCe,"STRONG",{});var nxr=n(_J);tao=r(nxr,"xlm-roberta"),nxr.forEach(t),aao=r(oCe," \u2014 "),Hk=s(oCe,"A",{href:!0});var lxr=n(Hk);sao=r(lxr,"XLMRobertaModel"),lxr.forEach(t),nao=r(oCe," (XLM-RoBERTa model)"),oCe.forEach(t),lao=i(C),Au=s(C,"LI",{});var rCe=n(Au);bJ=s(rCe,"STRONG",{});var ixr=n(bJ);iao=r(ixr,"xlm-roberta-xl"),ixr.forEach(t),dao=r(rCe," \u2014 "),Uk=s(rCe,"A",{href:!0});var dxr=n(Uk);cao=r(dxr,"XLMRobertaXLModel"),dxr.forEach(t),mao=r(rCe," (XLM-RoBERTa-XL model)"),rCe.forEach(t),fao=i(C),Lu=s(C,"LI",{});var tCe=n(Lu);vJ=s(tCe,"STRONG",{});var cxr=n(vJ);gao=r(cxr,"xlnet"),cxr.forEach(t),hao=r(tCe," \u2014 "),Jk=s(tCe,"A",{href:!0});var mxr=n(Jk);uao=r(mxr,"XLNetModel"),mxr.forEach(t),pao=r(tCe," (XLNet model)"),tCe.forEach(t),_ao=i(C),Bu=s(C,"LI",{});var aCe=n(Bu);TJ=s(aCe,"STRONG",{});var fxr=n(TJ);bao=r(fxr,"yoso"),fxr.forEach(t),vao=r(aCe," \u2014 "),Yk=s(aCe,"A",{href:!0});var gxr=n(Yk);Tao=r(gxr,"YosoModel"),gxr.forEach(t),Fao=r(aCe," (YOSO model)"),aCe.forEach(t),C.forEach(t),Cao=i(Lt),xu=s(Lt,"P",{});var sCe=n(xu);Mao=r(sCe,"The model is set in evaluation mode by default using "),FJ=s(sCe,"CODE",{});var hxr=n(FJ);Eao=r(hxr,"model.eval()"),hxr.forEach(t),yao=r(sCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),CJ=s(sCe,"CODE",{});var uxr=n(CJ);wao=r(uxr,"model.train()"),uxr.forEach(t),sCe.forEach(t),Aao=i(Lt),MJ=s(Lt,"P",{});var pxr=n(MJ);Lao=r(pxr,"Examples:"),pxr.forEach(t),Bao=i(Lt),f(c5.$$.fragment,Lt),Lt.forEach(t),kn.forEach(t),ALe=i(d),Pi=s(d,"H2",{class:!0});var x8e=n(Pi);ku=s(x8e,"A",{id:!0,class:!0,href:!0});var _xr=n(ku);EJ=s(_xr,"SPAN",{});var bxr=n(EJ);f(m5.$$.fragment,bxr),bxr.forEach(t),_xr.forEach(t),xao=i(x8e),yJ=s(x8e,"SPAN",{});var vxr=n(yJ);kao=r(vxr,"AutoModelForPreTraining"),vxr.forEach(t),x8e.forEach(t),LLe=i(d),Xo=s(d,"DIV",{class:!0});var Sn=n(Xo);f(f5.$$.fragment,Sn),Rao=i(Sn),$i=s(Sn,"P",{});var OO=n($i);Sao=r(OO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),wJ=s(OO,"CODE",{});var Txr=n(wJ);Pao=r(Txr,"from_pretrained()"),Txr.forEach(t),$ao=r(OO,"class method or the "),AJ=s(OO,"CODE",{});var Fxr=n(AJ);Iao=r(Fxr,"from_config()"),Fxr.forEach(t),jao=r(OO,`class
method.`),OO.forEach(t),Nao=i(Sn),g5=s(Sn,"P",{});var k8e=n(g5);Dao=r(k8e,"This class cannot be instantiated directly using "),LJ=s(k8e,"CODE",{});var Cxr=n(LJ);qao=r(Cxr,"__init__()"),Cxr.forEach(t),Gao=r(k8e," (throws an error)."),k8e.forEach(t),Oao=i(Sn),$r=s(Sn,"DIV",{class:!0});var Pn=n($r);f(h5.$$.fragment,Pn),Xao=i(Pn),BJ=s(Pn,"P",{});var Mxr=n(BJ);zao=r(Mxr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Mxr.forEach(t),Vao=i(Pn),Ii=s(Pn,"P",{});var XO=n(Ii);Wao=r(XO,`Note:
Loading a model from its configuration file does `),xJ=s(XO,"STRONG",{});var Exr=n(xJ);Qao=r(Exr,"not"),Exr.forEach(t),Hao=r(XO,` load the model weights. It only affects the
model\u2019s configuration. Use `),kJ=s(XO,"CODE",{});var yxr=n(kJ);Uao=r(yxr,"from_pretrained()"),yxr.forEach(t),Jao=r(XO,"to load the model weights."),XO.forEach(t),Yao=i(Pn),RJ=s(Pn,"P",{});var wxr=n(RJ);Kao=r(wxr,"Examples:"),wxr.forEach(t),Zao=i(Pn),f(u5.$$.fragment,Pn),Pn.forEach(t),eso=i(Sn),ke=s(Sn,"DIV",{class:!0});var Bt=n(ke);f(p5.$$.fragment,Bt),oso=i(Bt),SJ=s(Bt,"P",{});var Axr=n(SJ);rso=r(Axr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Axr.forEach(t),tso=i(Bt),Ia=s(Bt,"P",{});var PE=n(Ia);aso=r(PE,"The model class to instantiate is selected based on the "),PJ=s(PE,"CODE",{});var Lxr=n(PJ);sso=r(Lxr,"model_type"),Lxr.forEach(t),nso=r(PE,` property of the config object (either
passed as an argument or loaded from `),$J=s(PE,"CODE",{});var Bxr=n($J);lso=r(Bxr,"pretrained_model_name_or_path"),Bxr.forEach(t),iso=r(PE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IJ=s(PE,"CODE",{});var xxr=n(IJ);dso=r(xxr,"pretrained_model_name_or_path"),xxr.forEach(t),cso=r(PE,":"),PE.forEach(t),mso=i(Bt),k=s(Bt,"UL",{});var S=n(k);Ru=s(S,"LI",{});var nCe=n(Ru);jJ=s(nCe,"STRONG",{});var kxr=n(jJ);fso=r(kxr,"albert"),kxr.forEach(t),gso=r(nCe," \u2014 "),Kk=s(nCe,"A",{href:!0});var Rxr=n(Kk);hso=r(Rxr,"AlbertForPreTraining"),Rxr.forEach(t),uso=r(nCe," (ALBERT model)"),nCe.forEach(t),pso=i(S),Su=s(S,"LI",{});var lCe=n(Su);NJ=s(lCe,"STRONG",{});var Sxr=n(NJ);_so=r(Sxr,"bart"),Sxr.forEach(t),bso=r(lCe," \u2014 "),Zk=s(lCe,"A",{href:!0});var Pxr=n(Zk);vso=r(Pxr,"BartForConditionalGeneration"),Pxr.forEach(t),Tso=r(lCe," (BART model)"),lCe.forEach(t),Fso=i(S),Pu=s(S,"LI",{});var iCe=n(Pu);DJ=s(iCe,"STRONG",{});var $xr=n(DJ);Cso=r($xr,"bert"),$xr.forEach(t),Mso=r(iCe," \u2014 "),eR=s(iCe,"A",{href:!0});var Ixr=n(eR);Eso=r(Ixr,"BertForPreTraining"),Ixr.forEach(t),yso=r(iCe," (BERT model)"),iCe.forEach(t),wso=i(S),$u=s(S,"LI",{});var dCe=n($u);qJ=s(dCe,"STRONG",{});var jxr=n(qJ);Aso=r(jxr,"big_bird"),jxr.forEach(t),Lso=r(dCe," \u2014 "),oR=s(dCe,"A",{href:!0});var Nxr=n(oR);Bso=r(Nxr,"BigBirdForPreTraining"),Nxr.forEach(t),xso=r(dCe," (BigBird model)"),dCe.forEach(t),kso=i(S),Iu=s(S,"LI",{});var cCe=n(Iu);GJ=s(cCe,"STRONG",{});var Dxr=n(GJ);Rso=r(Dxr,"camembert"),Dxr.forEach(t),Sso=r(cCe," \u2014 "),rR=s(cCe,"A",{href:!0});var qxr=n(rR);Pso=r(qxr,"CamembertForMaskedLM"),qxr.forEach(t),$so=r(cCe," (CamemBERT model)"),cCe.forEach(t),Iso=i(S),ju=s(S,"LI",{});var mCe=n(ju);OJ=s(mCe,"STRONG",{});var Gxr=n(OJ);jso=r(Gxr,"ctrl"),Gxr.forEach(t),Nso=r(mCe," \u2014 "),tR=s(mCe,"A",{href:!0});var Oxr=n(tR);Dso=r(Oxr,"CTRLLMHeadModel"),Oxr.forEach(t),qso=r(mCe," (CTRL model)"),mCe.forEach(t),Gso=i(S),Nu=s(S,"LI",{});var fCe=n(Nu);XJ=s(fCe,"STRONG",{});var Xxr=n(XJ);Oso=r(Xxr,"deberta"),Xxr.forEach(t),Xso=r(fCe," \u2014 "),aR=s(fCe,"A",{href:!0});var zxr=n(aR);zso=r(zxr,"DebertaForMaskedLM"),zxr.forEach(t),Vso=r(fCe," (DeBERTa model)"),fCe.forEach(t),Wso=i(S),Du=s(S,"LI",{});var gCe=n(Du);zJ=s(gCe,"STRONG",{});var Vxr=n(zJ);Qso=r(Vxr,"deberta-v2"),Vxr.forEach(t),Hso=r(gCe," \u2014 "),sR=s(gCe,"A",{href:!0});var Wxr=n(sR);Uso=r(Wxr,"DebertaV2ForMaskedLM"),Wxr.forEach(t),Jso=r(gCe," (DeBERTa-v2 model)"),gCe.forEach(t),Yso=i(S),qu=s(S,"LI",{});var hCe=n(qu);VJ=s(hCe,"STRONG",{});var Qxr=n(VJ);Kso=r(Qxr,"distilbert"),Qxr.forEach(t),Zso=r(hCe," \u2014 "),nR=s(hCe,"A",{href:!0});var Hxr=n(nR);eno=r(Hxr,"DistilBertForMaskedLM"),Hxr.forEach(t),ono=r(hCe," (DistilBERT model)"),hCe.forEach(t),rno=i(S),Gu=s(S,"LI",{});var uCe=n(Gu);WJ=s(uCe,"STRONG",{});var Uxr=n(WJ);tno=r(Uxr,"electra"),Uxr.forEach(t),ano=r(uCe," \u2014 "),lR=s(uCe,"A",{href:!0});var Jxr=n(lR);sno=r(Jxr,"ElectraForPreTraining"),Jxr.forEach(t),nno=r(uCe," (ELECTRA model)"),uCe.forEach(t),lno=i(S),Ou=s(S,"LI",{});var pCe=n(Ou);QJ=s(pCe,"STRONG",{});var Yxr=n(QJ);ino=r(Yxr,"flaubert"),Yxr.forEach(t),dno=r(pCe," \u2014 "),iR=s(pCe,"A",{href:!0});var Kxr=n(iR);cno=r(Kxr,"FlaubertWithLMHeadModel"),Kxr.forEach(t),mno=r(pCe," (FlauBERT model)"),pCe.forEach(t),fno=i(S),Xu=s(S,"LI",{});var _Ce=n(Xu);HJ=s(_Ce,"STRONG",{});var Zxr=n(HJ);gno=r(Zxr,"fnet"),Zxr.forEach(t),hno=r(_Ce," \u2014 "),dR=s(_Ce,"A",{href:!0});var ekr=n(dR);uno=r(ekr,"FNetForPreTraining"),ekr.forEach(t),pno=r(_Ce," (FNet model)"),_Ce.forEach(t),_no=i(S),zu=s(S,"LI",{});var bCe=n(zu);UJ=s(bCe,"STRONG",{});var okr=n(UJ);bno=r(okr,"fsmt"),okr.forEach(t),vno=r(bCe," \u2014 "),cR=s(bCe,"A",{href:!0});var rkr=n(cR);Tno=r(rkr,"FSMTForConditionalGeneration"),rkr.forEach(t),Fno=r(bCe," (FairSeq Machine-Translation model)"),bCe.forEach(t),Cno=i(S),Vu=s(S,"LI",{});var vCe=n(Vu);JJ=s(vCe,"STRONG",{});var tkr=n(JJ);Mno=r(tkr,"funnel"),tkr.forEach(t),Eno=r(vCe," \u2014 "),mR=s(vCe,"A",{href:!0});var akr=n(mR);yno=r(akr,"FunnelForPreTraining"),akr.forEach(t),wno=r(vCe," (Funnel Transformer model)"),vCe.forEach(t),Ano=i(S),Wu=s(S,"LI",{});var TCe=n(Wu);YJ=s(TCe,"STRONG",{});var skr=n(YJ);Lno=r(skr,"gpt2"),skr.forEach(t),Bno=r(TCe," \u2014 "),fR=s(TCe,"A",{href:!0});var nkr=n(fR);xno=r(nkr,"GPT2LMHeadModel"),nkr.forEach(t),kno=r(TCe," (OpenAI GPT-2 model)"),TCe.forEach(t),Rno=i(S),Qu=s(S,"LI",{});var FCe=n(Qu);KJ=s(FCe,"STRONG",{});var lkr=n(KJ);Sno=r(lkr,"ibert"),lkr.forEach(t),Pno=r(FCe," \u2014 "),gR=s(FCe,"A",{href:!0});var ikr=n(gR);$no=r(ikr,"IBertForMaskedLM"),ikr.forEach(t),Ino=r(FCe," (I-BERT model)"),FCe.forEach(t),jno=i(S),Hu=s(S,"LI",{});var CCe=n(Hu);ZJ=s(CCe,"STRONG",{});var dkr=n(ZJ);Nno=r(dkr,"layoutlm"),dkr.forEach(t),Dno=r(CCe," \u2014 "),hR=s(CCe,"A",{href:!0});var ckr=n(hR);qno=r(ckr,"LayoutLMForMaskedLM"),ckr.forEach(t),Gno=r(CCe," (LayoutLM model)"),CCe.forEach(t),Ono=i(S),Uu=s(S,"LI",{});var MCe=n(Uu);eY=s(MCe,"STRONG",{});var mkr=n(eY);Xno=r(mkr,"longformer"),mkr.forEach(t),zno=r(MCe," \u2014 "),uR=s(MCe,"A",{href:!0});var fkr=n(uR);Vno=r(fkr,"LongformerForMaskedLM"),fkr.forEach(t),Wno=r(MCe," (Longformer model)"),MCe.forEach(t),Qno=i(S),Ju=s(S,"LI",{});var ECe=n(Ju);oY=s(ECe,"STRONG",{});var gkr=n(oY);Hno=r(gkr,"lxmert"),gkr.forEach(t),Uno=r(ECe," \u2014 "),pR=s(ECe,"A",{href:!0});var hkr=n(pR);Jno=r(hkr,"LxmertForPreTraining"),hkr.forEach(t),Yno=r(ECe," (LXMERT model)"),ECe.forEach(t),Kno=i(S),Yu=s(S,"LI",{});var yCe=n(Yu);rY=s(yCe,"STRONG",{});var ukr=n(rY);Zno=r(ukr,"megatron-bert"),ukr.forEach(t),elo=r(yCe," \u2014 "),_R=s(yCe,"A",{href:!0});var pkr=n(_R);olo=r(pkr,"MegatronBertForPreTraining"),pkr.forEach(t),rlo=r(yCe," (MegatronBert model)"),yCe.forEach(t),tlo=i(S),Ku=s(S,"LI",{});var wCe=n(Ku);tY=s(wCe,"STRONG",{});var _kr=n(tY);alo=r(_kr,"mobilebert"),_kr.forEach(t),slo=r(wCe," \u2014 "),bR=s(wCe,"A",{href:!0});var bkr=n(bR);nlo=r(bkr,"MobileBertForPreTraining"),bkr.forEach(t),llo=r(wCe," (MobileBERT model)"),wCe.forEach(t),ilo=i(S),Zu=s(S,"LI",{});var ACe=n(Zu);aY=s(ACe,"STRONG",{});var vkr=n(aY);dlo=r(vkr,"mpnet"),vkr.forEach(t),clo=r(ACe," \u2014 "),vR=s(ACe,"A",{href:!0});var Tkr=n(vR);mlo=r(Tkr,"MPNetForMaskedLM"),Tkr.forEach(t),flo=r(ACe," (MPNet model)"),ACe.forEach(t),glo=i(S),ep=s(S,"LI",{});var LCe=n(ep);sY=s(LCe,"STRONG",{});var Fkr=n(sY);hlo=r(Fkr,"openai-gpt"),Fkr.forEach(t),ulo=r(LCe," \u2014 "),TR=s(LCe,"A",{href:!0});var Ckr=n(TR);plo=r(Ckr,"OpenAIGPTLMHeadModel"),Ckr.forEach(t),_lo=r(LCe," (OpenAI GPT model)"),LCe.forEach(t),blo=i(S),op=s(S,"LI",{});var BCe=n(op);nY=s(BCe,"STRONG",{});var Mkr=n(nY);vlo=r(Mkr,"retribert"),Mkr.forEach(t),Tlo=r(BCe," \u2014 "),FR=s(BCe,"A",{href:!0});var Ekr=n(FR);Flo=r(Ekr,"RetriBertModel"),Ekr.forEach(t),Clo=r(BCe," (RetriBERT model)"),BCe.forEach(t),Mlo=i(S),rp=s(S,"LI",{});var xCe=n(rp);lY=s(xCe,"STRONG",{});var ykr=n(lY);Elo=r(ykr,"roberta"),ykr.forEach(t),ylo=r(xCe," \u2014 "),CR=s(xCe,"A",{href:!0});var wkr=n(CR);wlo=r(wkr,"RobertaForMaskedLM"),wkr.forEach(t),Alo=r(xCe," (RoBERTa model)"),xCe.forEach(t),Llo=i(S),tp=s(S,"LI",{});var kCe=n(tp);iY=s(kCe,"STRONG",{});var Akr=n(iY);Blo=r(Akr,"squeezebert"),Akr.forEach(t),xlo=r(kCe," \u2014 "),MR=s(kCe,"A",{href:!0});var Lkr=n(MR);klo=r(Lkr,"SqueezeBertForMaskedLM"),Lkr.forEach(t),Rlo=r(kCe," (SqueezeBERT model)"),kCe.forEach(t),Slo=i(S),ap=s(S,"LI",{});var RCe=n(ap);dY=s(RCe,"STRONG",{});var Bkr=n(dY);Plo=r(Bkr,"t5"),Bkr.forEach(t),$lo=r(RCe," \u2014 "),ER=s(RCe,"A",{href:!0});var xkr=n(ER);Ilo=r(xkr,"T5ForConditionalGeneration"),xkr.forEach(t),jlo=r(RCe," (T5 model)"),RCe.forEach(t),Nlo=i(S),sp=s(S,"LI",{});var SCe=n(sp);cY=s(SCe,"STRONG",{});var kkr=n(cY);Dlo=r(kkr,"tapas"),kkr.forEach(t),qlo=r(SCe," \u2014 "),yR=s(SCe,"A",{href:!0});var Rkr=n(yR);Glo=r(Rkr,"TapasForMaskedLM"),Rkr.forEach(t),Olo=r(SCe," (TAPAS model)"),SCe.forEach(t),Xlo=i(S),np=s(S,"LI",{});var PCe=n(np);mY=s(PCe,"STRONG",{});var Skr=n(mY);zlo=r(Skr,"transfo-xl"),Skr.forEach(t),Vlo=r(PCe," \u2014 "),wR=s(PCe,"A",{href:!0});var Pkr=n(wR);Wlo=r(Pkr,"TransfoXLLMHeadModel"),Pkr.forEach(t),Qlo=r(PCe," (Transformer-XL model)"),PCe.forEach(t),Hlo=i(S),lp=s(S,"LI",{});var $Ce=n(lp);fY=s($Ce,"STRONG",{});var $kr=n(fY);Ulo=r($kr,"unispeech"),$kr.forEach(t),Jlo=r($Ce," \u2014 "),AR=s($Ce,"A",{href:!0});var Ikr=n(AR);Ylo=r(Ikr,"UniSpeechForPreTraining"),Ikr.forEach(t),Klo=r($Ce," (UniSpeech model)"),$Ce.forEach(t),Zlo=i(S),ip=s(S,"LI",{});var ICe=n(ip);gY=s(ICe,"STRONG",{});var jkr=n(gY);eio=r(jkr,"unispeech-sat"),jkr.forEach(t),oio=r(ICe," \u2014 "),LR=s(ICe,"A",{href:!0});var Nkr=n(LR);rio=r(Nkr,"UniSpeechSatForPreTraining"),Nkr.forEach(t),tio=r(ICe," (UniSpeechSat model)"),ICe.forEach(t),aio=i(S),dp=s(S,"LI",{});var jCe=n(dp);hY=s(jCe,"STRONG",{});var Dkr=n(hY);sio=r(Dkr,"visual_bert"),Dkr.forEach(t),nio=r(jCe," \u2014 "),BR=s(jCe,"A",{href:!0});var qkr=n(BR);lio=r(qkr,"VisualBertForPreTraining"),qkr.forEach(t),iio=r(jCe," (VisualBert model)"),jCe.forEach(t),dio=i(S),cp=s(S,"LI",{});var NCe=n(cp);uY=s(NCe,"STRONG",{});var Gkr=n(uY);cio=r(Gkr,"vit_mae"),Gkr.forEach(t),mio=r(NCe," \u2014 "),xR=s(NCe,"A",{href:!0});var Okr=n(xR);fio=r(Okr,"ViTMAEForPreTraining"),Okr.forEach(t),gio=r(NCe," (ViTMAE model)"),NCe.forEach(t),hio=i(S),mp=s(S,"LI",{});var DCe=n(mp);pY=s(DCe,"STRONG",{});var Xkr=n(pY);uio=r(Xkr,"wav2vec2"),Xkr.forEach(t),pio=r(DCe," \u2014 "),kR=s(DCe,"A",{href:!0});var zkr=n(kR);_io=r(zkr,"Wav2Vec2ForPreTraining"),zkr.forEach(t),bio=r(DCe," (Wav2Vec2 model)"),DCe.forEach(t),vio=i(S),fp=s(S,"LI",{});var qCe=n(fp);_Y=s(qCe,"STRONG",{});var Vkr=n(_Y);Tio=r(Vkr,"xlm"),Vkr.forEach(t),Fio=r(qCe," \u2014 "),RR=s(qCe,"A",{href:!0});var Wkr=n(RR);Cio=r(Wkr,"XLMWithLMHeadModel"),Wkr.forEach(t),Mio=r(qCe," (XLM model)"),qCe.forEach(t),Eio=i(S),gp=s(S,"LI",{});var GCe=n(gp);bY=s(GCe,"STRONG",{});var Qkr=n(bY);yio=r(Qkr,"xlm-roberta"),Qkr.forEach(t),wio=r(GCe," \u2014 "),SR=s(GCe,"A",{href:!0});var Hkr=n(SR);Aio=r(Hkr,"XLMRobertaForMaskedLM"),Hkr.forEach(t),Lio=r(GCe," (XLM-RoBERTa model)"),GCe.forEach(t),Bio=i(S),hp=s(S,"LI",{});var OCe=n(hp);vY=s(OCe,"STRONG",{});var Ukr=n(vY);xio=r(Ukr,"xlm-roberta-xl"),Ukr.forEach(t),kio=r(OCe," \u2014 "),PR=s(OCe,"A",{href:!0});var Jkr=n(PR);Rio=r(Jkr,"XLMRobertaXLForMaskedLM"),Jkr.forEach(t),Sio=r(OCe," (XLM-RoBERTa-XL model)"),OCe.forEach(t),Pio=i(S),up=s(S,"LI",{});var XCe=n(up);TY=s(XCe,"STRONG",{});var Ykr=n(TY);$io=r(Ykr,"xlnet"),Ykr.forEach(t),Iio=r(XCe," \u2014 "),$R=s(XCe,"A",{href:!0});var Kkr=n($R);jio=r(Kkr,"XLNetLMHeadModel"),Kkr.forEach(t),Nio=r(XCe," (XLNet model)"),XCe.forEach(t),S.forEach(t),Dio=i(Bt),pp=s(Bt,"P",{});var zCe=n(pp);qio=r(zCe,"The model is set in evaluation mode by default using "),FY=s(zCe,"CODE",{});var Zkr=n(FY);Gio=r(Zkr,"model.eval()"),Zkr.forEach(t),Oio=r(zCe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),CY=s(zCe,"CODE",{});var eRr=n(CY);Xio=r(eRr,"model.train()"),eRr.forEach(t),zCe.forEach(t),zio=i(Bt),MY=s(Bt,"P",{});var oRr=n(MY);Vio=r(oRr,"Examples:"),oRr.forEach(t),Wio=i(Bt),f(_5.$$.fragment,Bt),Bt.forEach(t),Sn.forEach(t),BLe=i(d),ji=s(d,"H2",{class:!0});var R8e=n(ji);_p=s(R8e,"A",{id:!0,class:!0,href:!0});var rRr=n(_p);EY=s(rRr,"SPAN",{});var tRr=n(EY);f(b5.$$.fragment,tRr),tRr.forEach(t),rRr.forEach(t),Qio=i(R8e),yY=s(R8e,"SPAN",{});var aRr=n(yY);Hio=r(aRr,"AutoModelForCausalLM"),aRr.forEach(t),R8e.forEach(t),xLe=i(d),zo=s(d,"DIV",{class:!0});var $n=n(zo);f(v5.$$.fragment,$n),Uio=i($n),Ni=s($n,"P",{});var zO=n(Ni);Jio=r(zO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wY=s(zO,"CODE",{});var sRr=n(wY);Yio=r(sRr,"from_pretrained()"),sRr.forEach(t),Kio=r(zO,"class method or the "),AY=s(zO,"CODE",{});var nRr=n(AY);Zio=r(nRr,"from_config()"),nRr.forEach(t),edo=r(zO,`class
method.`),zO.forEach(t),odo=i($n),T5=s($n,"P",{});var S8e=n(T5);rdo=r(S8e,"This class cannot be instantiated directly using "),LY=s(S8e,"CODE",{});var lRr=n(LY);tdo=r(lRr,"__init__()"),lRr.forEach(t),ado=r(S8e," (throws an error)."),S8e.forEach(t),sdo=i($n),Ir=s($n,"DIV",{class:!0});var In=n(Ir);f(F5.$$.fragment,In),ndo=i(In),BY=s(In,"P",{});var iRr=n(BY);ldo=r(iRr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),iRr.forEach(t),ido=i(In),Di=s(In,"P",{});var VO=n(Di);ddo=r(VO,`Note:
Loading a model from its configuration file does `),xY=s(VO,"STRONG",{});var dRr=n(xY);cdo=r(dRr,"not"),dRr.forEach(t),mdo=r(VO,` load the model weights. It only affects the
model\u2019s configuration. Use `),kY=s(VO,"CODE",{});var cRr=n(kY);fdo=r(cRr,"from_pretrained()"),cRr.forEach(t),gdo=r(VO,"to load the model weights."),VO.forEach(t),hdo=i(In),RY=s(In,"P",{});var mRr=n(RY);udo=r(mRr,"Examples:"),mRr.forEach(t),pdo=i(In),f(C5.$$.fragment,In),In.forEach(t),_do=i($n),Re=s($n,"DIV",{class:!0});var xt=n(Re);f(M5.$$.fragment,xt),bdo=i(xt),SY=s(xt,"P",{});var fRr=n(SY);vdo=r(fRr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),fRr.forEach(t),Tdo=i(xt),ja=s(xt,"P",{});var $E=n(ja);Fdo=r($E,"The model class to instantiate is selected based on the "),PY=s($E,"CODE",{});var gRr=n(PY);Cdo=r(gRr,"model_type"),gRr.forEach(t),Mdo=r($E,` property of the config object (either
passed as an argument or loaded from `),$Y=s($E,"CODE",{});var hRr=n($Y);Edo=r(hRr,"pretrained_model_name_or_path"),hRr.forEach(t),ydo=r($E,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IY=s($E,"CODE",{});var uRr=n(IY);wdo=r(uRr,"pretrained_model_name_or_path"),uRr.forEach(t),Ado=r($E,":"),$E.forEach(t),Ldo=i(xt),I=s(xt,"UL",{});var D=n(I);bp=s(D,"LI",{});var VCe=n(bp);jY=s(VCe,"STRONG",{});var pRr=n(jY);Bdo=r(pRr,"bart"),pRr.forEach(t),xdo=r(VCe," \u2014 "),IR=s(VCe,"A",{href:!0});var _Rr=n(IR);kdo=r(_Rr,"BartForCausalLM"),_Rr.forEach(t),Rdo=r(VCe," (BART model)"),VCe.forEach(t),Sdo=i(D),vp=s(D,"LI",{});var WCe=n(vp);NY=s(WCe,"STRONG",{});var bRr=n(NY);Pdo=r(bRr,"bert"),bRr.forEach(t),$do=r(WCe," \u2014 "),jR=s(WCe,"A",{href:!0});var vRr=n(jR);Ido=r(vRr,"BertLMHeadModel"),vRr.forEach(t),jdo=r(WCe," (BERT model)"),WCe.forEach(t),Ndo=i(D),Tp=s(D,"LI",{});var QCe=n(Tp);DY=s(QCe,"STRONG",{});var TRr=n(DY);Ddo=r(TRr,"bert-generation"),TRr.forEach(t),qdo=r(QCe," \u2014 "),NR=s(QCe,"A",{href:!0});var FRr=n(NR);Gdo=r(FRr,"BertGenerationDecoder"),FRr.forEach(t),Odo=r(QCe," (Bert Generation model)"),QCe.forEach(t),Xdo=i(D),Fp=s(D,"LI",{});var HCe=n(Fp);qY=s(HCe,"STRONG",{});var CRr=n(qY);zdo=r(CRr,"big_bird"),CRr.forEach(t),Vdo=r(HCe," \u2014 "),DR=s(HCe,"A",{href:!0});var MRr=n(DR);Wdo=r(MRr,"BigBirdForCausalLM"),MRr.forEach(t),Qdo=r(HCe," (BigBird model)"),HCe.forEach(t),Hdo=i(D),Cp=s(D,"LI",{});var UCe=n(Cp);GY=s(UCe,"STRONG",{});var ERr=n(GY);Udo=r(ERr,"bigbird_pegasus"),ERr.forEach(t),Jdo=r(UCe," \u2014 "),qR=s(UCe,"A",{href:!0});var yRr=n(qR);Ydo=r(yRr,"BigBirdPegasusForCausalLM"),yRr.forEach(t),Kdo=r(UCe," (BigBirdPegasus model)"),UCe.forEach(t),Zdo=i(D),Mp=s(D,"LI",{});var JCe=n(Mp);OY=s(JCe,"STRONG",{});var wRr=n(OY);eco=r(wRr,"blenderbot"),wRr.forEach(t),oco=r(JCe," \u2014 "),GR=s(JCe,"A",{href:!0});var ARr=n(GR);rco=r(ARr,"BlenderbotForCausalLM"),ARr.forEach(t),tco=r(JCe," (Blenderbot model)"),JCe.forEach(t),aco=i(D),Ep=s(D,"LI",{});var YCe=n(Ep);XY=s(YCe,"STRONG",{});var LRr=n(XY);sco=r(LRr,"blenderbot-small"),LRr.forEach(t),nco=r(YCe," \u2014 "),OR=s(YCe,"A",{href:!0});var BRr=n(OR);lco=r(BRr,"BlenderbotSmallForCausalLM"),BRr.forEach(t),ico=r(YCe," (BlenderbotSmall model)"),YCe.forEach(t),dco=i(D),yp=s(D,"LI",{});var KCe=n(yp);zY=s(KCe,"STRONG",{});var xRr=n(zY);cco=r(xRr,"camembert"),xRr.forEach(t),mco=r(KCe," \u2014 "),XR=s(KCe,"A",{href:!0});var kRr=n(XR);fco=r(kRr,"CamembertForCausalLM"),kRr.forEach(t),gco=r(KCe," (CamemBERT model)"),KCe.forEach(t),hco=i(D),wp=s(D,"LI",{});var ZCe=n(wp);VY=s(ZCe,"STRONG",{});var RRr=n(VY);uco=r(RRr,"ctrl"),RRr.forEach(t),pco=r(ZCe," \u2014 "),zR=s(ZCe,"A",{href:!0});var SRr=n(zR);_co=r(SRr,"CTRLLMHeadModel"),SRr.forEach(t),bco=r(ZCe," (CTRL model)"),ZCe.forEach(t),vco=i(D),Ap=s(D,"LI",{});var e4e=n(Ap);WY=s(e4e,"STRONG",{});var PRr=n(WY);Tco=r(PRr,"electra"),PRr.forEach(t),Fco=r(e4e," \u2014 "),VR=s(e4e,"A",{href:!0});var $Rr=n(VR);Cco=r($Rr,"ElectraForCausalLM"),$Rr.forEach(t),Mco=r(e4e," (ELECTRA model)"),e4e.forEach(t),Eco=i(D),Lp=s(D,"LI",{});var o4e=n(Lp);QY=s(o4e,"STRONG",{});var IRr=n(QY);yco=r(IRr,"gpt2"),IRr.forEach(t),wco=r(o4e," \u2014 "),WR=s(o4e,"A",{href:!0});var jRr=n(WR);Aco=r(jRr,"GPT2LMHeadModel"),jRr.forEach(t),Lco=r(o4e," (OpenAI GPT-2 model)"),o4e.forEach(t),Bco=i(D),Bp=s(D,"LI",{});var r4e=n(Bp);HY=s(r4e,"STRONG",{});var NRr=n(HY);xco=r(NRr,"gpt_neo"),NRr.forEach(t),kco=r(r4e," \u2014 "),QR=s(r4e,"A",{href:!0});var DRr=n(QR);Rco=r(DRr,"GPTNeoForCausalLM"),DRr.forEach(t),Sco=r(r4e," (GPT Neo model)"),r4e.forEach(t),Pco=i(D),xp=s(D,"LI",{});var t4e=n(xp);UY=s(t4e,"STRONG",{});var qRr=n(UY);$co=r(qRr,"gptj"),qRr.forEach(t),Ico=r(t4e," \u2014 "),HR=s(t4e,"A",{href:!0});var GRr=n(HR);jco=r(GRr,"GPTJForCausalLM"),GRr.forEach(t),Nco=r(t4e," (GPT-J model)"),t4e.forEach(t),Dco=i(D),kp=s(D,"LI",{});var a4e=n(kp);JY=s(a4e,"STRONG",{});var ORr=n(JY);qco=r(ORr,"marian"),ORr.forEach(t),Gco=r(a4e," \u2014 "),UR=s(a4e,"A",{href:!0});var XRr=n(UR);Oco=r(XRr,"MarianForCausalLM"),XRr.forEach(t),Xco=r(a4e," (Marian model)"),a4e.forEach(t),zco=i(D),Rp=s(D,"LI",{});var s4e=n(Rp);YY=s(s4e,"STRONG",{});var zRr=n(YY);Vco=r(zRr,"mbart"),zRr.forEach(t),Wco=r(s4e," \u2014 "),JR=s(s4e,"A",{href:!0});var VRr=n(JR);Qco=r(VRr,"MBartForCausalLM"),VRr.forEach(t),Hco=r(s4e," (mBART model)"),s4e.forEach(t),Uco=i(D),Sp=s(D,"LI",{});var n4e=n(Sp);KY=s(n4e,"STRONG",{});var WRr=n(KY);Jco=r(WRr,"megatron-bert"),WRr.forEach(t),Yco=r(n4e," \u2014 "),YR=s(n4e,"A",{href:!0});var QRr=n(YR);Kco=r(QRr,"MegatronBertForCausalLM"),QRr.forEach(t),Zco=r(n4e," (MegatronBert model)"),n4e.forEach(t),emo=i(D),Pp=s(D,"LI",{});var l4e=n(Pp);ZY=s(l4e,"STRONG",{});var HRr=n(ZY);omo=r(HRr,"openai-gpt"),HRr.forEach(t),rmo=r(l4e," \u2014 "),KR=s(l4e,"A",{href:!0});var URr=n(KR);tmo=r(URr,"OpenAIGPTLMHeadModel"),URr.forEach(t),amo=r(l4e," (OpenAI GPT model)"),l4e.forEach(t),smo=i(D),$p=s(D,"LI",{});var i4e=n($p);eK=s(i4e,"STRONG",{});var JRr=n(eK);nmo=r(JRr,"pegasus"),JRr.forEach(t),lmo=r(i4e," \u2014 "),ZR=s(i4e,"A",{href:!0});var YRr=n(ZR);imo=r(YRr,"PegasusForCausalLM"),YRr.forEach(t),dmo=r(i4e," (Pegasus model)"),i4e.forEach(t),cmo=i(D),Ip=s(D,"LI",{});var d4e=n(Ip);oK=s(d4e,"STRONG",{});var KRr=n(oK);mmo=r(KRr,"prophetnet"),KRr.forEach(t),fmo=r(d4e," \u2014 "),eS=s(d4e,"A",{href:!0});var ZRr=n(eS);gmo=r(ZRr,"ProphetNetForCausalLM"),ZRr.forEach(t),hmo=r(d4e," (ProphetNet model)"),d4e.forEach(t),umo=i(D),jp=s(D,"LI",{});var c4e=n(jp);rK=s(c4e,"STRONG",{});var eSr=n(rK);pmo=r(eSr,"qdqbert"),eSr.forEach(t),_mo=r(c4e," \u2014 "),tK=s(c4e,"CODE",{});var oSr=n(tK);bmo=r(oSr,"QDQBertLMHeadModel"),oSr.forEach(t),vmo=r(c4e,"(QDQBert model)"),c4e.forEach(t),Tmo=i(D),Np=s(D,"LI",{});var m4e=n(Np);aK=s(m4e,"STRONG",{});var rSr=n(aK);Fmo=r(rSr,"reformer"),rSr.forEach(t),Cmo=r(m4e," \u2014 "),oS=s(m4e,"A",{href:!0});var tSr=n(oS);Mmo=r(tSr,"ReformerModelWithLMHead"),tSr.forEach(t),Emo=r(m4e," (Reformer model)"),m4e.forEach(t),ymo=i(D),Dp=s(D,"LI",{});var f4e=n(Dp);sK=s(f4e,"STRONG",{});var aSr=n(sK);wmo=r(aSr,"rembert"),aSr.forEach(t),Amo=r(f4e," \u2014 "),rS=s(f4e,"A",{href:!0});var sSr=n(rS);Lmo=r(sSr,"RemBertForCausalLM"),sSr.forEach(t),Bmo=r(f4e," (RemBERT model)"),f4e.forEach(t),xmo=i(D),qp=s(D,"LI",{});var g4e=n(qp);nK=s(g4e,"STRONG",{});var nSr=n(nK);kmo=r(nSr,"roberta"),nSr.forEach(t),Rmo=r(g4e," \u2014 "),tS=s(g4e,"A",{href:!0});var lSr=n(tS);Smo=r(lSr,"RobertaForCausalLM"),lSr.forEach(t),Pmo=r(g4e," (RoBERTa model)"),g4e.forEach(t),$mo=i(D),Gp=s(D,"LI",{});var h4e=n(Gp);lK=s(h4e,"STRONG",{});var iSr=n(lK);Imo=r(iSr,"roformer"),iSr.forEach(t),jmo=r(h4e," \u2014 "),aS=s(h4e,"A",{href:!0});var dSr=n(aS);Nmo=r(dSr,"RoFormerForCausalLM"),dSr.forEach(t),Dmo=r(h4e," (RoFormer model)"),h4e.forEach(t),qmo=i(D),Op=s(D,"LI",{});var u4e=n(Op);iK=s(u4e,"STRONG",{});var cSr=n(iK);Gmo=r(cSr,"speech_to_text_2"),cSr.forEach(t),Omo=r(u4e," \u2014 "),sS=s(u4e,"A",{href:!0});var mSr=n(sS);Xmo=r(mSr,"Speech2Text2ForCausalLM"),mSr.forEach(t),zmo=r(u4e," (Speech2Text2 model)"),u4e.forEach(t),Vmo=i(D),Xp=s(D,"LI",{});var p4e=n(Xp);dK=s(p4e,"STRONG",{});var fSr=n(dK);Wmo=r(fSr,"transfo-xl"),fSr.forEach(t),Qmo=r(p4e," \u2014 "),nS=s(p4e,"A",{href:!0});var gSr=n(nS);Hmo=r(gSr,"TransfoXLLMHeadModel"),gSr.forEach(t),Umo=r(p4e," (Transformer-XL model)"),p4e.forEach(t),Jmo=i(D),zp=s(D,"LI",{});var _4e=n(zp);cK=s(_4e,"STRONG",{});var hSr=n(cK);Ymo=r(hSr,"trocr"),hSr.forEach(t),Kmo=r(_4e," \u2014 "),lS=s(_4e,"A",{href:!0});var uSr=n(lS);Zmo=r(uSr,"TrOCRForCausalLM"),uSr.forEach(t),efo=r(_4e," (TrOCR model)"),_4e.forEach(t),ofo=i(D),Vp=s(D,"LI",{});var b4e=n(Vp);mK=s(b4e,"STRONG",{});var pSr=n(mK);rfo=r(pSr,"xglm"),pSr.forEach(t),tfo=r(b4e," \u2014 "),iS=s(b4e,"A",{href:!0});var _Sr=n(iS);afo=r(_Sr,"XGLMForCausalLM"),_Sr.forEach(t),sfo=r(b4e," (XGLM model)"),b4e.forEach(t),nfo=i(D),Wp=s(D,"LI",{});var v4e=n(Wp);fK=s(v4e,"STRONG",{});var bSr=n(fK);lfo=r(bSr,"xlm"),bSr.forEach(t),ifo=r(v4e," \u2014 "),dS=s(v4e,"A",{href:!0});var vSr=n(dS);dfo=r(vSr,"XLMWithLMHeadModel"),vSr.forEach(t),cfo=r(v4e," (XLM model)"),v4e.forEach(t),mfo=i(D),Qp=s(D,"LI",{});var T4e=n(Qp);gK=s(T4e,"STRONG",{});var TSr=n(gK);ffo=r(TSr,"xlm-prophetnet"),TSr.forEach(t),gfo=r(T4e," \u2014 "),cS=s(T4e,"A",{href:!0});var FSr=n(cS);hfo=r(FSr,"XLMProphetNetForCausalLM"),FSr.forEach(t),ufo=r(T4e," (XLMProphetNet model)"),T4e.forEach(t),pfo=i(D),Hp=s(D,"LI",{});var F4e=n(Hp);hK=s(F4e,"STRONG",{});var CSr=n(hK);_fo=r(CSr,"xlm-roberta"),CSr.forEach(t),bfo=r(F4e," \u2014 "),mS=s(F4e,"A",{href:!0});var MSr=n(mS);vfo=r(MSr,"XLMRobertaForCausalLM"),MSr.forEach(t),Tfo=r(F4e," (XLM-RoBERTa model)"),F4e.forEach(t),Ffo=i(D),Up=s(D,"LI",{});var C4e=n(Up);uK=s(C4e,"STRONG",{});var ESr=n(uK);Cfo=r(ESr,"xlm-roberta-xl"),ESr.forEach(t),Mfo=r(C4e," \u2014 "),fS=s(C4e,"A",{href:!0});var ySr=n(fS);Efo=r(ySr,"XLMRobertaXLForCausalLM"),ySr.forEach(t),yfo=r(C4e," (XLM-RoBERTa-XL model)"),C4e.forEach(t),wfo=i(D),Jp=s(D,"LI",{});var M4e=n(Jp);pK=s(M4e,"STRONG",{});var wSr=n(pK);Afo=r(wSr,"xlnet"),wSr.forEach(t),Lfo=r(M4e," \u2014 "),gS=s(M4e,"A",{href:!0});var ASr=n(gS);Bfo=r(ASr,"XLNetLMHeadModel"),ASr.forEach(t),xfo=r(M4e," (XLNet model)"),M4e.forEach(t),D.forEach(t),kfo=i(xt),Yp=s(xt,"P",{});var E4e=n(Yp);Rfo=r(E4e,"The model is set in evaluation mode by default using "),_K=s(E4e,"CODE",{});var LSr=n(_K);Sfo=r(LSr,"model.eval()"),LSr.forEach(t),Pfo=r(E4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),bK=s(E4e,"CODE",{});var BSr=n(bK);$fo=r(BSr,"model.train()"),BSr.forEach(t),E4e.forEach(t),Ifo=i(xt),vK=s(xt,"P",{});var xSr=n(vK);jfo=r(xSr,"Examples:"),xSr.forEach(t),Nfo=i(xt),f(E5.$$.fragment,xt),xt.forEach(t),$n.forEach(t),kLe=i(d),qi=s(d,"H2",{class:!0});var P8e=n(qi);Kp=s(P8e,"A",{id:!0,class:!0,href:!0});var kSr=n(Kp);TK=s(kSr,"SPAN",{});var RSr=n(TK);f(y5.$$.fragment,RSr),RSr.forEach(t),kSr.forEach(t),Dfo=i(P8e),FK=s(P8e,"SPAN",{});var SSr=n(FK);qfo=r(SSr,"AutoModelForMaskedLM"),SSr.forEach(t),P8e.forEach(t),RLe=i(d),Vo=s(d,"DIV",{class:!0});var jn=n(Vo);f(w5.$$.fragment,jn),Gfo=i(jn),Gi=s(jn,"P",{});var WO=n(Gi);Ofo=r(WO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),CK=s(WO,"CODE",{});var PSr=n(CK);Xfo=r(PSr,"from_pretrained()"),PSr.forEach(t),zfo=r(WO,"class method or the "),MK=s(WO,"CODE",{});var $Sr=n(MK);Vfo=r($Sr,"from_config()"),$Sr.forEach(t),Wfo=r(WO,`class
method.`),WO.forEach(t),Qfo=i(jn),A5=s(jn,"P",{});var $8e=n(A5);Hfo=r($8e,"This class cannot be instantiated directly using "),EK=s($8e,"CODE",{});var ISr=n(EK);Ufo=r(ISr,"__init__()"),ISr.forEach(t),Jfo=r($8e," (throws an error)."),$8e.forEach(t),Yfo=i(jn),jr=s(jn,"DIV",{class:!0});var Nn=n(jr);f(L5.$$.fragment,Nn),Kfo=i(Nn),yK=s(Nn,"P",{});var jSr=n(yK);Zfo=r(jSr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),jSr.forEach(t),ego=i(Nn),Oi=s(Nn,"P",{});var QO=n(Oi);ogo=r(QO,`Note:
Loading a model from its configuration file does `),wK=s(QO,"STRONG",{});var NSr=n(wK);rgo=r(NSr,"not"),NSr.forEach(t),tgo=r(QO,` load the model weights. It only affects the
model\u2019s configuration. Use `),AK=s(QO,"CODE",{});var DSr=n(AK);ago=r(DSr,"from_pretrained()"),DSr.forEach(t),sgo=r(QO,"to load the model weights."),QO.forEach(t),ngo=i(Nn),LK=s(Nn,"P",{});var qSr=n(LK);lgo=r(qSr,"Examples:"),qSr.forEach(t),igo=i(Nn),f(B5.$$.fragment,Nn),Nn.forEach(t),dgo=i(jn),Se=s(jn,"DIV",{class:!0});var kt=n(Se);f(x5.$$.fragment,kt),cgo=i(kt),BK=s(kt,"P",{});var GSr=n(BK);mgo=r(GSr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),GSr.forEach(t),fgo=i(kt),Na=s(kt,"P",{});var IE=n(Na);ggo=r(IE,"The model class to instantiate is selected based on the "),xK=s(IE,"CODE",{});var OSr=n(xK);hgo=r(OSr,"model_type"),OSr.forEach(t),ugo=r(IE,` property of the config object (either
passed as an argument or loaded from `),kK=s(IE,"CODE",{});var XSr=n(kK);pgo=r(XSr,"pretrained_model_name_or_path"),XSr.forEach(t),_go=r(IE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),RK=s(IE,"CODE",{});var zSr=n(RK);bgo=r(zSr,"pretrained_model_name_or_path"),zSr.forEach(t),vgo=r(IE,":"),IE.forEach(t),Tgo=i(kt),$=s(kt,"UL",{});var j=n($);Zp=s(j,"LI",{});var y4e=n(Zp);SK=s(y4e,"STRONG",{});var VSr=n(SK);Fgo=r(VSr,"albert"),VSr.forEach(t),Cgo=r(y4e," \u2014 "),hS=s(y4e,"A",{href:!0});var WSr=n(hS);Mgo=r(WSr,"AlbertForMaskedLM"),WSr.forEach(t),Ego=r(y4e," (ALBERT model)"),y4e.forEach(t),ygo=i(j),e_=s(j,"LI",{});var w4e=n(e_);PK=s(w4e,"STRONG",{});var QSr=n(PK);wgo=r(QSr,"bart"),QSr.forEach(t),Ago=r(w4e," \u2014 "),uS=s(w4e,"A",{href:!0});var HSr=n(uS);Lgo=r(HSr,"BartForConditionalGeneration"),HSr.forEach(t),Bgo=r(w4e," (BART model)"),w4e.forEach(t),xgo=i(j),o_=s(j,"LI",{});var A4e=n(o_);$K=s(A4e,"STRONG",{});var USr=n($K);kgo=r(USr,"bert"),USr.forEach(t),Rgo=r(A4e," \u2014 "),pS=s(A4e,"A",{href:!0});var JSr=n(pS);Sgo=r(JSr,"BertForMaskedLM"),JSr.forEach(t),Pgo=r(A4e," (BERT model)"),A4e.forEach(t),$go=i(j),r_=s(j,"LI",{});var L4e=n(r_);IK=s(L4e,"STRONG",{});var YSr=n(IK);Igo=r(YSr,"big_bird"),YSr.forEach(t),jgo=r(L4e," \u2014 "),_S=s(L4e,"A",{href:!0});var KSr=n(_S);Ngo=r(KSr,"BigBirdForMaskedLM"),KSr.forEach(t),Dgo=r(L4e," (BigBird model)"),L4e.forEach(t),qgo=i(j),t_=s(j,"LI",{});var B4e=n(t_);jK=s(B4e,"STRONG",{});var ZSr=n(jK);Ggo=r(ZSr,"camembert"),ZSr.forEach(t),Ogo=r(B4e," \u2014 "),bS=s(B4e,"A",{href:!0});var ePr=n(bS);Xgo=r(ePr,"CamembertForMaskedLM"),ePr.forEach(t),zgo=r(B4e," (CamemBERT model)"),B4e.forEach(t),Vgo=i(j),a_=s(j,"LI",{});var x4e=n(a_);NK=s(x4e,"STRONG",{});var oPr=n(NK);Wgo=r(oPr,"convbert"),oPr.forEach(t),Qgo=r(x4e," \u2014 "),vS=s(x4e,"A",{href:!0});var rPr=n(vS);Hgo=r(rPr,"ConvBertForMaskedLM"),rPr.forEach(t),Ugo=r(x4e," (ConvBERT model)"),x4e.forEach(t),Jgo=i(j),s_=s(j,"LI",{});var k4e=n(s_);DK=s(k4e,"STRONG",{});var tPr=n(DK);Ygo=r(tPr,"deberta"),tPr.forEach(t),Kgo=r(k4e," \u2014 "),TS=s(k4e,"A",{href:!0});var aPr=n(TS);Zgo=r(aPr,"DebertaForMaskedLM"),aPr.forEach(t),eho=r(k4e," (DeBERTa model)"),k4e.forEach(t),oho=i(j),n_=s(j,"LI",{});var R4e=n(n_);qK=s(R4e,"STRONG",{});var sPr=n(qK);rho=r(sPr,"deberta-v2"),sPr.forEach(t),tho=r(R4e," \u2014 "),FS=s(R4e,"A",{href:!0});var nPr=n(FS);aho=r(nPr,"DebertaV2ForMaskedLM"),nPr.forEach(t),sho=r(R4e," (DeBERTa-v2 model)"),R4e.forEach(t),nho=i(j),l_=s(j,"LI",{});var S4e=n(l_);GK=s(S4e,"STRONG",{});var lPr=n(GK);lho=r(lPr,"distilbert"),lPr.forEach(t),iho=r(S4e," \u2014 "),CS=s(S4e,"A",{href:!0});var iPr=n(CS);dho=r(iPr,"DistilBertForMaskedLM"),iPr.forEach(t),cho=r(S4e," (DistilBERT model)"),S4e.forEach(t),mho=i(j),i_=s(j,"LI",{});var P4e=n(i_);OK=s(P4e,"STRONG",{});var dPr=n(OK);fho=r(dPr,"electra"),dPr.forEach(t),gho=r(P4e," \u2014 "),MS=s(P4e,"A",{href:!0});var cPr=n(MS);hho=r(cPr,"ElectraForMaskedLM"),cPr.forEach(t),uho=r(P4e," (ELECTRA model)"),P4e.forEach(t),pho=i(j),d_=s(j,"LI",{});var $4e=n(d_);XK=s($4e,"STRONG",{});var mPr=n(XK);_ho=r(mPr,"flaubert"),mPr.forEach(t),bho=r($4e," \u2014 "),ES=s($4e,"A",{href:!0});var fPr=n(ES);vho=r(fPr,"FlaubertWithLMHeadModel"),fPr.forEach(t),Tho=r($4e," (FlauBERT model)"),$4e.forEach(t),Fho=i(j),c_=s(j,"LI",{});var I4e=n(c_);zK=s(I4e,"STRONG",{});var gPr=n(zK);Cho=r(gPr,"fnet"),gPr.forEach(t),Mho=r(I4e," \u2014 "),yS=s(I4e,"A",{href:!0});var hPr=n(yS);Eho=r(hPr,"FNetForMaskedLM"),hPr.forEach(t),yho=r(I4e," (FNet model)"),I4e.forEach(t),who=i(j),m_=s(j,"LI",{});var j4e=n(m_);VK=s(j4e,"STRONG",{});var uPr=n(VK);Aho=r(uPr,"funnel"),uPr.forEach(t),Lho=r(j4e," \u2014 "),wS=s(j4e,"A",{href:!0});var pPr=n(wS);Bho=r(pPr,"FunnelForMaskedLM"),pPr.forEach(t),xho=r(j4e," (Funnel Transformer model)"),j4e.forEach(t),kho=i(j),f_=s(j,"LI",{});var N4e=n(f_);WK=s(N4e,"STRONG",{});var _Pr=n(WK);Rho=r(_Pr,"ibert"),_Pr.forEach(t),Sho=r(N4e," \u2014 "),AS=s(N4e,"A",{href:!0});var bPr=n(AS);Pho=r(bPr,"IBertForMaskedLM"),bPr.forEach(t),$ho=r(N4e," (I-BERT model)"),N4e.forEach(t),Iho=i(j),g_=s(j,"LI",{});var D4e=n(g_);QK=s(D4e,"STRONG",{});var vPr=n(QK);jho=r(vPr,"layoutlm"),vPr.forEach(t),Nho=r(D4e," \u2014 "),LS=s(D4e,"A",{href:!0});var TPr=n(LS);Dho=r(TPr,"LayoutLMForMaskedLM"),TPr.forEach(t),qho=r(D4e," (LayoutLM model)"),D4e.forEach(t),Gho=i(j),h_=s(j,"LI",{});var q4e=n(h_);HK=s(q4e,"STRONG",{});var FPr=n(HK);Oho=r(FPr,"longformer"),FPr.forEach(t),Xho=r(q4e," \u2014 "),BS=s(q4e,"A",{href:!0});var CPr=n(BS);zho=r(CPr,"LongformerForMaskedLM"),CPr.forEach(t),Vho=r(q4e," (Longformer model)"),q4e.forEach(t),Who=i(j),u_=s(j,"LI",{});var G4e=n(u_);UK=s(G4e,"STRONG",{});var MPr=n(UK);Qho=r(MPr,"mbart"),MPr.forEach(t),Hho=r(G4e," \u2014 "),xS=s(G4e,"A",{href:!0});var EPr=n(xS);Uho=r(EPr,"MBartForConditionalGeneration"),EPr.forEach(t),Jho=r(G4e," (mBART model)"),G4e.forEach(t),Yho=i(j),p_=s(j,"LI",{});var O4e=n(p_);JK=s(O4e,"STRONG",{});var yPr=n(JK);Kho=r(yPr,"megatron-bert"),yPr.forEach(t),Zho=r(O4e," \u2014 "),kS=s(O4e,"A",{href:!0});var wPr=n(kS);euo=r(wPr,"MegatronBertForMaskedLM"),wPr.forEach(t),ouo=r(O4e," (MegatronBert model)"),O4e.forEach(t),ruo=i(j),__=s(j,"LI",{});var X4e=n(__);YK=s(X4e,"STRONG",{});var APr=n(YK);tuo=r(APr,"mobilebert"),APr.forEach(t),auo=r(X4e," \u2014 "),RS=s(X4e,"A",{href:!0});var LPr=n(RS);suo=r(LPr,"MobileBertForMaskedLM"),LPr.forEach(t),nuo=r(X4e," (MobileBERT model)"),X4e.forEach(t),luo=i(j),b_=s(j,"LI",{});var z4e=n(b_);KK=s(z4e,"STRONG",{});var BPr=n(KK);iuo=r(BPr,"mpnet"),BPr.forEach(t),duo=r(z4e," \u2014 "),SS=s(z4e,"A",{href:!0});var xPr=n(SS);cuo=r(xPr,"MPNetForMaskedLM"),xPr.forEach(t),muo=r(z4e," (MPNet model)"),z4e.forEach(t),fuo=i(j),v_=s(j,"LI",{});var V4e=n(v_);ZK=s(V4e,"STRONG",{});var kPr=n(ZK);guo=r(kPr,"nystromformer"),kPr.forEach(t),huo=r(V4e," \u2014 "),PS=s(V4e,"A",{href:!0});var RPr=n(PS);uuo=r(RPr,"NystromformerForMaskedLM"),RPr.forEach(t),puo=r(V4e," (Nystromformer model)"),V4e.forEach(t),_uo=i(j),T_=s(j,"LI",{});var W4e=n(T_);eZ=s(W4e,"STRONG",{});var SPr=n(eZ);buo=r(SPr,"perceiver"),SPr.forEach(t),vuo=r(W4e," \u2014 "),$S=s(W4e,"A",{href:!0});var PPr=n($S);Tuo=r(PPr,"PerceiverForMaskedLM"),PPr.forEach(t),Fuo=r(W4e," (Perceiver model)"),W4e.forEach(t),Cuo=i(j),F_=s(j,"LI",{});var Q4e=n(F_);oZ=s(Q4e,"STRONG",{});var $Pr=n(oZ);Muo=r($Pr,"qdqbert"),$Pr.forEach(t),Euo=r(Q4e," \u2014 "),rZ=s(Q4e,"CODE",{});var IPr=n(rZ);yuo=r(IPr,"QDQBertForMaskedLM"),IPr.forEach(t),wuo=r(Q4e,"(QDQBert model)"),Q4e.forEach(t),Auo=i(j),C_=s(j,"LI",{});var H4e=n(C_);tZ=s(H4e,"STRONG",{});var jPr=n(tZ);Luo=r(jPr,"reformer"),jPr.forEach(t),Buo=r(H4e," \u2014 "),IS=s(H4e,"A",{href:!0});var NPr=n(IS);xuo=r(NPr,"ReformerForMaskedLM"),NPr.forEach(t),kuo=r(H4e," (Reformer model)"),H4e.forEach(t),Ruo=i(j),M_=s(j,"LI",{});var U4e=n(M_);aZ=s(U4e,"STRONG",{});var DPr=n(aZ);Suo=r(DPr,"rembert"),DPr.forEach(t),Puo=r(U4e," \u2014 "),jS=s(U4e,"A",{href:!0});var qPr=n(jS);$uo=r(qPr,"RemBertForMaskedLM"),qPr.forEach(t),Iuo=r(U4e," (RemBERT model)"),U4e.forEach(t),juo=i(j),E_=s(j,"LI",{});var J4e=n(E_);sZ=s(J4e,"STRONG",{});var GPr=n(sZ);Nuo=r(GPr,"roberta"),GPr.forEach(t),Duo=r(J4e," \u2014 "),NS=s(J4e,"A",{href:!0});var OPr=n(NS);quo=r(OPr,"RobertaForMaskedLM"),OPr.forEach(t),Guo=r(J4e," (RoBERTa model)"),J4e.forEach(t),Ouo=i(j),y_=s(j,"LI",{});var Y4e=n(y_);nZ=s(Y4e,"STRONG",{});var XPr=n(nZ);Xuo=r(XPr,"roformer"),XPr.forEach(t),zuo=r(Y4e," \u2014 "),DS=s(Y4e,"A",{href:!0});var zPr=n(DS);Vuo=r(zPr,"RoFormerForMaskedLM"),zPr.forEach(t),Wuo=r(Y4e," (RoFormer model)"),Y4e.forEach(t),Quo=i(j),w_=s(j,"LI",{});var K4e=n(w_);lZ=s(K4e,"STRONG",{});var VPr=n(lZ);Huo=r(VPr,"squeezebert"),VPr.forEach(t),Uuo=r(K4e," \u2014 "),qS=s(K4e,"A",{href:!0});var WPr=n(qS);Juo=r(WPr,"SqueezeBertForMaskedLM"),WPr.forEach(t),Yuo=r(K4e," (SqueezeBERT model)"),K4e.forEach(t),Kuo=i(j),A_=s(j,"LI",{});var Z4e=n(A_);iZ=s(Z4e,"STRONG",{});var QPr=n(iZ);Zuo=r(QPr,"tapas"),QPr.forEach(t),epo=r(Z4e," \u2014 "),GS=s(Z4e,"A",{href:!0});var HPr=n(GS);opo=r(HPr,"TapasForMaskedLM"),HPr.forEach(t),rpo=r(Z4e," (TAPAS model)"),Z4e.forEach(t),tpo=i(j),L_=s(j,"LI",{});var eMe=n(L_);dZ=s(eMe,"STRONG",{});var UPr=n(dZ);apo=r(UPr,"wav2vec2"),UPr.forEach(t),spo=r(eMe," \u2014 "),cZ=s(eMe,"CODE",{});var JPr=n(cZ);npo=r(JPr,"Wav2Vec2ForMaskedLM"),JPr.forEach(t),lpo=r(eMe,"(Wav2Vec2 model)"),eMe.forEach(t),ipo=i(j),B_=s(j,"LI",{});var oMe=n(B_);mZ=s(oMe,"STRONG",{});var YPr=n(mZ);dpo=r(YPr,"xlm"),YPr.forEach(t),cpo=r(oMe," \u2014 "),OS=s(oMe,"A",{href:!0});var KPr=n(OS);mpo=r(KPr,"XLMWithLMHeadModel"),KPr.forEach(t),fpo=r(oMe," (XLM model)"),oMe.forEach(t),gpo=i(j),x_=s(j,"LI",{});var rMe=n(x_);fZ=s(rMe,"STRONG",{});var ZPr=n(fZ);hpo=r(ZPr,"xlm-roberta"),ZPr.forEach(t),upo=r(rMe," \u2014 "),XS=s(rMe,"A",{href:!0});var e$r=n(XS);ppo=r(e$r,"XLMRobertaForMaskedLM"),e$r.forEach(t),_po=r(rMe," (XLM-RoBERTa model)"),rMe.forEach(t),bpo=i(j),k_=s(j,"LI",{});var tMe=n(k_);gZ=s(tMe,"STRONG",{});var o$r=n(gZ);vpo=r(o$r,"xlm-roberta-xl"),o$r.forEach(t),Tpo=r(tMe," \u2014 "),zS=s(tMe,"A",{href:!0});var r$r=n(zS);Fpo=r(r$r,"XLMRobertaXLForMaskedLM"),r$r.forEach(t),Cpo=r(tMe," (XLM-RoBERTa-XL model)"),tMe.forEach(t),Mpo=i(j),R_=s(j,"LI",{});var aMe=n(R_);hZ=s(aMe,"STRONG",{});var t$r=n(hZ);Epo=r(t$r,"yoso"),t$r.forEach(t),ypo=r(aMe," \u2014 "),VS=s(aMe,"A",{href:!0});var a$r=n(VS);wpo=r(a$r,"YosoForMaskedLM"),a$r.forEach(t),Apo=r(aMe," (YOSO model)"),aMe.forEach(t),j.forEach(t),Lpo=i(kt),S_=s(kt,"P",{});var sMe=n(S_);Bpo=r(sMe,"The model is set in evaluation mode by default using "),uZ=s(sMe,"CODE",{});var s$r=n(uZ);xpo=r(s$r,"model.eval()"),s$r.forEach(t),kpo=r(sMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pZ=s(sMe,"CODE",{});var n$r=n(pZ);Rpo=r(n$r,"model.train()"),n$r.forEach(t),sMe.forEach(t),Spo=i(kt),_Z=s(kt,"P",{});var l$r=n(_Z);Ppo=r(l$r,"Examples:"),l$r.forEach(t),$po=i(kt),f(k5.$$.fragment,kt),kt.forEach(t),jn.forEach(t),SLe=i(d),Xi=s(d,"H2",{class:!0});var I8e=n(Xi);P_=s(I8e,"A",{id:!0,class:!0,href:!0});var i$r=n(P_);bZ=s(i$r,"SPAN",{});var d$r=n(bZ);f(R5.$$.fragment,d$r),d$r.forEach(t),i$r.forEach(t),Ipo=i(I8e),vZ=s(I8e,"SPAN",{});var c$r=n(vZ);jpo=r(c$r,"AutoModelForSeq2SeqLM"),c$r.forEach(t),I8e.forEach(t),PLe=i(d),Wo=s(d,"DIV",{class:!0});var Dn=n(Wo);f(S5.$$.fragment,Dn),Npo=i(Dn),zi=s(Dn,"P",{});var HO=n(zi);Dpo=r(HO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),TZ=s(HO,"CODE",{});var m$r=n(TZ);qpo=r(m$r,"from_pretrained()"),m$r.forEach(t),Gpo=r(HO,"class method or the "),FZ=s(HO,"CODE",{});var f$r=n(FZ);Opo=r(f$r,"from_config()"),f$r.forEach(t),Xpo=r(HO,`class
method.`),HO.forEach(t),zpo=i(Dn),P5=s(Dn,"P",{});var j8e=n(P5);Vpo=r(j8e,"This class cannot be instantiated directly using "),CZ=s(j8e,"CODE",{});var g$r=n(CZ);Wpo=r(g$r,"__init__()"),g$r.forEach(t),Qpo=r(j8e," (throws an error)."),j8e.forEach(t),Hpo=i(Dn),Nr=s(Dn,"DIV",{class:!0});var qn=n(Nr);f($5.$$.fragment,qn),Upo=i(qn),MZ=s(qn,"P",{});var h$r=n(MZ);Jpo=r(h$r,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),h$r.forEach(t),Ypo=i(qn),Vi=s(qn,"P",{});var UO=n(Vi);Kpo=r(UO,`Note:
Loading a model from its configuration file does `),EZ=s(UO,"STRONG",{});var u$r=n(EZ);Zpo=r(u$r,"not"),u$r.forEach(t),e_o=r(UO,` load the model weights. It only affects the
model\u2019s configuration. Use `),yZ=s(UO,"CODE",{});var p$r=n(yZ);o_o=r(p$r,"from_pretrained()"),p$r.forEach(t),r_o=r(UO,"to load the model weights."),UO.forEach(t),t_o=i(qn),wZ=s(qn,"P",{});var _$r=n(wZ);a_o=r(_$r,"Examples:"),_$r.forEach(t),s_o=i(qn),f(I5.$$.fragment,qn),qn.forEach(t),n_o=i(Dn),Pe=s(Dn,"DIV",{class:!0});var Rt=n(Pe);f(j5.$$.fragment,Rt),l_o=i(Rt),AZ=s(Rt,"P",{});var b$r=n(AZ);i_o=r(b$r,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),b$r.forEach(t),d_o=i(Rt),Da=s(Rt,"P",{});var jE=n(Da);c_o=r(jE,"The model class to instantiate is selected based on the "),LZ=s(jE,"CODE",{});var v$r=n(LZ);m_o=r(v$r,"model_type"),v$r.forEach(t),f_o=r(jE,` property of the config object (either
passed as an argument or loaded from `),BZ=s(jE,"CODE",{});var T$r=n(BZ);g_o=r(T$r,"pretrained_model_name_or_path"),T$r.forEach(t),h_o=r(jE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xZ=s(jE,"CODE",{});var F$r=n(xZ);u_o=r(F$r,"pretrained_model_name_or_path"),F$r.forEach(t),p_o=r(jE,":"),jE.forEach(t),__o=i(Rt),se=s(Rt,"UL",{});var ie=n(se);$_=s(ie,"LI",{});var nMe=n($_);kZ=s(nMe,"STRONG",{});var C$r=n(kZ);b_o=r(C$r,"bart"),C$r.forEach(t),v_o=r(nMe," \u2014 "),WS=s(nMe,"A",{href:!0});var M$r=n(WS);T_o=r(M$r,"BartForConditionalGeneration"),M$r.forEach(t),F_o=r(nMe," (BART model)"),nMe.forEach(t),C_o=i(ie),I_=s(ie,"LI",{});var lMe=n(I_);RZ=s(lMe,"STRONG",{});var E$r=n(RZ);M_o=r(E$r,"bigbird_pegasus"),E$r.forEach(t),E_o=r(lMe," \u2014 "),QS=s(lMe,"A",{href:!0});var y$r=n(QS);y_o=r(y$r,"BigBirdPegasusForConditionalGeneration"),y$r.forEach(t),w_o=r(lMe," (BigBirdPegasus model)"),lMe.forEach(t),A_o=i(ie),j_=s(ie,"LI",{});var iMe=n(j_);SZ=s(iMe,"STRONG",{});var w$r=n(SZ);L_o=r(w$r,"blenderbot"),w$r.forEach(t),B_o=r(iMe," \u2014 "),HS=s(iMe,"A",{href:!0});var A$r=n(HS);x_o=r(A$r,"BlenderbotForConditionalGeneration"),A$r.forEach(t),k_o=r(iMe," (Blenderbot model)"),iMe.forEach(t),R_o=i(ie),N_=s(ie,"LI",{});var dMe=n(N_);PZ=s(dMe,"STRONG",{});var L$r=n(PZ);S_o=r(L$r,"blenderbot-small"),L$r.forEach(t),P_o=r(dMe," \u2014 "),US=s(dMe,"A",{href:!0});var B$r=n(US);$_o=r(B$r,"BlenderbotSmallForConditionalGeneration"),B$r.forEach(t),I_o=r(dMe," (BlenderbotSmall model)"),dMe.forEach(t),j_o=i(ie),D_=s(ie,"LI",{});var cMe=n(D_);$Z=s(cMe,"STRONG",{});var x$r=n($Z);N_o=r(x$r,"encoder-decoder"),x$r.forEach(t),D_o=r(cMe," \u2014 "),JS=s(cMe,"A",{href:!0});var k$r=n(JS);q_o=r(k$r,"EncoderDecoderModel"),k$r.forEach(t),G_o=r(cMe," (Encoder decoder model)"),cMe.forEach(t),O_o=i(ie),q_=s(ie,"LI",{});var mMe=n(q_);IZ=s(mMe,"STRONG",{});var R$r=n(IZ);X_o=r(R$r,"fsmt"),R$r.forEach(t),z_o=r(mMe," \u2014 "),YS=s(mMe,"A",{href:!0});var S$r=n(YS);V_o=r(S$r,"FSMTForConditionalGeneration"),S$r.forEach(t),W_o=r(mMe," (FairSeq Machine-Translation model)"),mMe.forEach(t),Q_o=i(ie),G_=s(ie,"LI",{});var fMe=n(G_);jZ=s(fMe,"STRONG",{});var P$r=n(jZ);H_o=r(P$r,"led"),P$r.forEach(t),U_o=r(fMe," \u2014 "),KS=s(fMe,"A",{href:!0});var $$r=n(KS);J_o=r($$r,"LEDForConditionalGeneration"),$$r.forEach(t),Y_o=r(fMe," (LED model)"),fMe.forEach(t),K_o=i(ie),O_=s(ie,"LI",{});var gMe=n(O_);NZ=s(gMe,"STRONG",{});var I$r=n(NZ);Z_o=r(I$r,"m2m_100"),I$r.forEach(t),ebo=r(gMe," \u2014 "),ZS=s(gMe,"A",{href:!0});var j$r=n(ZS);obo=r(j$r,"M2M100ForConditionalGeneration"),j$r.forEach(t),rbo=r(gMe," (M2M100 model)"),gMe.forEach(t),tbo=i(ie),X_=s(ie,"LI",{});var hMe=n(X_);DZ=s(hMe,"STRONG",{});var N$r=n(DZ);abo=r(N$r,"marian"),N$r.forEach(t),sbo=r(hMe," \u2014 "),eP=s(hMe,"A",{href:!0});var D$r=n(eP);nbo=r(D$r,"MarianMTModel"),D$r.forEach(t),lbo=r(hMe," (Marian model)"),hMe.forEach(t),ibo=i(ie),z_=s(ie,"LI",{});var uMe=n(z_);qZ=s(uMe,"STRONG",{});var q$r=n(qZ);dbo=r(q$r,"mbart"),q$r.forEach(t),cbo=r(uMe," \u2014 "),oP=s(uMe,"A",{href:!0});var G$r=n(oP);mbo=r(G$r,"MBartForConditionalGeneration"),G$r.forEach(t),fbo=r(uMe," (mBART model)"),uMe.forEach(t),gbo=i(ie),V_=s(ie,"LI",{});var pMe=n(V_);GZ=s(pMe,"STRONG",{});var O$r=n(GZ);hbo=r(O$r,"mt5"),O$r.forEach(t),ubo=r(pMe," \u2014 "),rP=s(pMe,"A",{href:!0});var X$r=n(rP);pbo=r(X$r,"MT5ForConditionalGeneration"),X$r.forEach(t),_bo=r(pMe," (mT5 model)"),pMe.forEach(t),bbo=i(ie),W_=s(ie,"LI",{});var _Me=n(W_);OZ=s(_Me,"STRONG",{});var z$r=n(OZ);vbo=r(z$r,"pegasus"),z$r.forEach(t),Tbo=r(_Me," \u2014 "),tP=s(_Me,"A",{href:!0});var V$r=n(tP);Fbo=r(V$r,"PegasusForConditionalGeneration"),V$r.forEach(t),Cbo=r(_Me," (Pegasus model)"),_Me.forEach(t),Mbo=i(ie),Q_=s(ie,"LI",{});var bMe=n(Q_);XZ=s(bMe,"STRONG",{});var W$r=n(XZ);Ebo=r(W$r,"prophetnet"),W$r.forEach(t),ybo=r(bMe," \u2014 "),aP=s(bMe,"A",{href:!0});var Q$r=n(aP);wbo=r(Q$r,"ProphetNetForConditionalGeneration"),Q$r.forEach(t),Abo=r(bMe," (ProphetNet model)"),bMe.forEach(t),Lbo=i(ie),H_=s(ie,"LI",{});var vMe=n(H_);zZ=s(vMe,"STRONG",{});var H$r=n(zZ);Bbo=r(H$r,"t5"),H$r.forEach(t),xbo=r(vMe," \u2014 "),sP=s(vMe,"A",{href:!0});var U$r=n(sP);kbo=r(U$r,"T5ForConditionalGeneration"),U$r.forEach(t),Rbo=r(vMe," (T5 model)"),vMe.forEach(t),Sbo=i(ie),U_=s(ie,"LI",{});var TMe=n(U_);VZ=s(TMe,"STRONG",{});var J$r=n(VZ);Pbo=r(J$r,"xlm-prophetnet"),J$r.forEach(t),$bo=r(TMe," \u2014 "),nP=s(TMe,"A",{href:!0});var Y$r=n(nP);Ibo=r(Y$r,"XLMProphetNetForConditionalGeneration"),Y$r.forEach(t),jbo=r(TMe," (XLMProphetNet model)"),TMe.forEach(t),ie.forEach(t),Nbo=i(Rt),J_=s(Rt,"P",{});var FMe=n(J_);Dbo=r(FMe,"The model is set in evaluation mode by default using "),WZ=s(FMe,"CODE",{});var K$r=n(WZ);qbo=r(K$r,"model.eval()"),K$r.forEach(t),Gbo=r(FMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),QZ=s(FMe,"CODE",{});var Z$r=n(QZ);Obo=r(Z$r,"model.train()"),Z$r.forEach(t),FMe.forEach(t),Xbo=i(Rt),HZ=s(Rt,"P",{});var eIr=n(HZ);zbo=r(eIr,"Examples:"),eIr.forEach(t),Vbo=i(Rt),f(N5.$$.fragment,Rt),Rt.forEach(t),Dn.forEach(t),$Le=i(d),Wi=s(d,"H2",{class:!0});var N8e=n(Wi);Y_=s(N8e,"A",{id:!0,class:!0,href:!0});var oIr=n(Y_);UZ=s(oIr,"SPAN",{});var rIr=n(UZ);f(D5.$$.fragment,rIr),rIr.forEach(t),oIr.forEach(t),Wbo=i(N8e),JZ=s(N8e,"SPAN",{});var tIr=n(JZ);Qbo=r(tIr,"AutoModelForSequenceClassification"),tIr.forEach(t),N8e.forEach(t),ILe=i(d),Qo=s(d,"DIV",{class:!0});var Gn=n(Qo);f(q5.$$.fragment,Gn),Hbo=i(Gn),Qi=s(Gn,"P",{});var JO=n(Qi);Ubo=r(JO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),YZ=s(JO,"CODE",{});var aIr=n(YZ);Jbo=r(aIr,"from_pretrained()"),aIr.forEach(t),Ybo=r(JO,"class method or the "),KZ=s(JO,"CODE",{});var sIr=n(KZ);Kbo=r(sIr,"from_config()"),sIr.forEach(t),Zbo=r(JO,`class
method.`),JO.forEach(t),e2o=i(Gn),G5=s(Gn,"P",{});var D8e=n(G5);o2o=r(D8e,"This class cannot be instantiated directly using "),ZZ=s(D8e,"CODE",{});var nIr=n(ZZ);r2o=r(nIr,"__init__()"),nIr.forEach(t),t2o=r(D8e," (throws an error)."),D8e.forEach(t),a2o=i(Gn),Dr=s(Gn,"DIV",{class:!0});var On=n(Dr);f(O5.$$.fragment,On),s2o=i(On),eee=s(On,"P",{});var lIr=n(eee);n2o=r(lIr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),lIr.forEach(t),l2o=i(On),Hi=s(On,"P",{});var YO=n(Hi);i2o=r(YO,`Note:
Loading a model from its configuration file does `),oee=s(YO,"STRONG",{});var iIr=n(oee);d2o=r(iIr,"not"),iIr.forEach(t),c2o=r(YO,` load the model weights. It only affects the
model\u2019s configuration. Use `),ree=s(YO,"CODE",{});var dIr=n(ree);m2o=r(dIr,"from_pretrained()"),dIr.forEach(t),f2o=r(YO,"to load the model weights."),YO.forEach(t),g2o=i(On),tee=s(On,"P",{});var cIr=n(tee);h2o=r(cIr,"Examples:"),cIr.forEach(t),u2o=i(On),f(X5.$$.fragment,On),On.forEach(t),p2o=i(Gn),$e=s(Gn,"DIV",{class:!0});var St=n($e);f(z5.$$.fragment,St),_2o=i(St),aee=s(St,"P",{});var mIr=n(aee);b2o=r(mIr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),mIr.forEach(t),v2o=i(St),qa=s(St,"P",{});var NE=n(qa);T2o=r(NE,"The model class to instantiate is selected based on the "),see=s(NE,"CODE",{});var fIr=n(see);F2o=r(fIr,"model_type"),fIr.forEach(t),C2o=r(NE,` property of the config object (either
passed as an argument or loaded from `),nee=s(NE,"CODE",{});var gIr=n(nee);M2o=r(gIr,"pretrained_model_name_or_path"),gIr.forEach(t),E2o=r(NE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lee=s(NE,"CODE",{});var hIr=n(lee);y2o=r(hIr,"pretrained_model_name_or_path"),hIr.forEach(t),w2o=r(NE,":"),NE.forEach(t),A2o=i(St),A=s(St,"UL",{});var L=n(A);K_=s(L,"LI",{});var CMe=n(K_);iee=s(CMe,"STRONG",{});var uIr=n(iee);L2o=r(uIr,"albert"),uIr.forEach(t),B2o=r(CMe," \u2014 "),lP=s(CMe,"A",{href:!0});var pIr=n(lP);x2o=r(pIr,"AlbertForSequenceClassification"),pIr.forEach(t),k2o=r(CMe," (ALBERT model)"),CMe.forEach(t),R2o=i(L),Z_=s(L,"LI",{});var MMe=n(Z_);dee=s(MMe,"STRONG",{});var _Ir=n(dee);S2o=r(_Ir,"bart"),_Ir.forEach(t),P2o=r(MMe," \u2014 "),iP=s(MMe,"A",{href:!0});var bIr=n(iP);$2o=r(bIr,"BartForSequenceClassification"),bIr.forEach(t),I2o=r(MMe," (BART model)"),MMe.forEach(t),j2o=i(L),eb=s(L,"LI",{});var EMe=n(eb);cee=s(EMe,"STRONG",{});var vIr=n(cee);N2o=r(vIr,"bert"),vIr.forEach(t),D2o=r(EMe," \u2014 "),dP=s(EMe,"A",{href:!0});var TIr=n(dP);q2o=r(TIr,"BertForSequenceClassification"),TIr.forEach(t),G2o=r(EMe," (BERT model)"),EMe.forEach(t),O2o=i(L),ob=s(L,"LI",{});var yMe=n(ob);mee=s(yMe,"STRONG",{});var FIr=n(mee);X2o=r(FIr,"big_bird"),FIr.forEach(t),z2o=r(yMe," \u2014 "),cP=s(yMe,"A",{href:!0});var CIr=n(cP);V2o=r(CIr,"BigBirdForSequenceClassification"),CIr.forEach(t),W2o=r(yMe," (BigBird model)"),yMe.forEach(t),Q2o=i(L),rb=s(L,"LI",{});var wMe=n(rb);fee=s(wMe,"STRONG",{});var MIr=n(fee);H2o=r(MIr,"bigbird_pegasus"),MIr.forEach(t),U2o=r(wMe," \u2014 "),mP=s(wMe,"A",{href:!0});var EIr=n(mP);J2o=r(EIr,"BigBirdPegasusForSequenceClassification"),EIr.forEach(t),Y2o=r(wMe," (BigBirdPegasus model)"),wMe.forEach(t),K2o=i(L),tb=s(L,"LI",{});var AMe=n(tb);gee=s(AMe,"STRONG",{});var yIr=n(gee);Z2o=r(yIr,"camembert"),yIr.forEach(t),evo=r(AMe," \u2014 "),fP=s(AMe,"A",{href:!0});var wIr=n(fP);ovo=r(wIr,"CamembertForSequenceClassification"),wIr.forEach(t),rvo=r(AMe," (CamemBERT model)"),AMe.forEach(t),tvo=i(L),ab=s(L,"LI",{});var LMe=n(ab);hee=s(LMe,"STRONG",{});var AIr=n(hee);avo=r(AIr,"canine"),AIr.forEach(t),svo=r(LMe," \u2014 "),gP=s(LMe,"A",{href:!0});var LIr=n(gP);nvo=r(LIr,"CanineForSequenceClassification"),LIr.forEach(t),lvo=r(LMe," (Canine model)"),LMe.forEach(t),ivo=i(L),sb=s(L,"LI",{});var BMe=n(sb);uee=s(BMe,"STRONG",{});var BIr=n(uee);dvo=r(BIr,"convbert"),BIr.forEach(t),cvo=r(BMe," \u2014 "),hP=s(BMe,"A",{href:!0});var xIr=n(hP);mvo=r(xIr,"ConvBertForSequenceClassification"),xIr.forEach(t),fvo=r(BMe," (ConvBERT model)"),BMe.forEach(t),gvo=i(L),nb=s(L,"LI",{});var xMe=n(nb);pee=s(xMe,"STRONG",{});var kIr=n(pee);hvo=r(kIr,"ctrl"),kIr.forEach(t),uvo=r(xMe," \u2014 "),uP=s(xMe,"A",{href:!0});var RIr=n(uP);pvo=r(RIr,"CTRLForSequenceClassification"),RIr.forEach(t),_vo=r(xMe," (CTRL model)"),xMe.forEach(t),bvo=i(L),lb=s(L,"LI",{});var kMe=n(lb);_ee=s(kMe,"STRONG",{});var SIr=n(_ee);vvo=r(SIr,"deberta"),SIr.forEach(t),Tvo=r(kMe," \u2014 "),pP=s(kMe,"A",{href:!0});var PIr=n(pP);Fvo=r(PIr,"DebertaForSequenceClassification"),PIr.forEach(t),Cvo=r(kMe," (DeBERTa model)"),kMe.forEach(t),Mvo=i(L),ib=s(L,"LI",{});var RMe=n(ib);bee=s(RMe,"STRONG",{});var $Ir=n(bee);Evo=r($Ir,"deberta-v2"),$Ir.forEach(t),yvo=r(RMe," \u2014 "),_P=s(RMe,"A",{href:!0});var IIr=n(_P);wvo=r(IIr,"DebertaV2ForSequenceClassification"),IIr.forEach(t),Avo=r(RMe," (DeBERTa-v2 model)"),RMe.forEach(t),Lvo=i(L),db=s(L,"LI",{});var SMe=n(db);vee=s(SMe,"STRONG",{});var jIr=n(vee);Bvo=r(jIr,"distilbert"),jIr.forEach(t),xvo=r(SMe," \u2014 "),bP=s(SMe,"A",{href:!0});var NIr=n(bP);kvo=r(NIr,"DistilBertForSequenceClassification"),NIr.forEach(t),Rvo=r(SMe," (DistilBERT model)"),SMe.forEach(t),Svo=i(L),cb=s(L,"LI",{});var PMe=n(cb);Tee=s(PMe,"STRONG",{});var DIr=n(Tee);Pvo=r(DIr,"electra"),DIr.forEach(t),$vo=r(PMe," \u2014 "),vP=s(PMe,"A",{href:!0});var qIr=n(vP);Ivo=r(qIr,"ElectraForSequenceClassification"),qIr.forEach(t),jvo=r(PMe," (ELECTRA model)"),PMe.forEach(t),Nvo=i(L),mb=s(L,"LI",{});var $Me=n(mb);Fee=s($Me,"STRONG",{});var GIr=n(Fee);Dvo=r(GIr,"flaubert"),GIr.forEach(t),qvo=r($Me," \u2014 "),TP=s($Me,"A",{href:!0});var OIr=n(TP);Gvo=r(OIr,"FlaubertForSequenceClassification"),OIr.forEach(t),Ovo=r($Me," (FlauBERT model)"),$Me.forEach(t),Xvo=i(L),fb=s(L,"LI",{});var IMe=n(fb);Cee=s(IMe,"STRONG",{});var XIr=n(Cee);zvo=r(XIr,"fnet"),XIr.forEach(t),Vvo=r(IMe," \u2014 "),FP=s(IMe,"A",{href:!0});var zIr=n(FP);Wvo=r(zIr,"FNetForSequenceClassification"),zIr.forEach(t),Qvo=r(IMe," (FNet model)"),IMe.forEach(t),Hvo=i(L),gb=s(L,"LI",{});var jMe=n(gb);Mee=s(jMe,"STRONG",{});var VIr=n(Mee);Uvo=r(VIr,"funnel"),VIr.forEach(t),Jvo=r(jMe," \u2014 "),CP=s(jMe,"A",{href:!0});var WIr=n(CP);Yvo=r(WIr,"FunnelForSequenceClassification"),WIr.forEach(t),Kvo=r(jMe," (Funnel Transformer model)"),jMe.forEach(t),Zvo=i(L),hb=s(L,"LI",{});var NMe=n(hb);Eee=s(NMe,"STRONG",{});var QIr=n(Eee);eTo=r(QIr,"gpt2"),QIr.forEach(t),oTo=r(NMe," \u2014 "),MP=s(NMe,"A",{href:!0});var HIr=n(MP);rTo=r(HIr,"GPT2ForSequenceClassification"),HIr.forEach(t),tTo=r(NMe," (OpenAI GPT-2 model)"),NMe.forEach(t),aTo=i(L),ub=s(L,"LI",{});var DMe=n(ub);yee=s(DMe,"STRONG",{});var UIr=n(yee);sTo=r(UIr,"gpt_neo"),UIr.forEach(t),nTo=r(DMe," \u2014 "),EP=s(DMe,"A",{href:!0});var JIr=n(EP);lTo=r(JIr,"GPTNeoForSequenceClassification"),JIr.forEach(t),iTo=r(DMe," (GPT Neo model)"),DMe.forEach(t),dTo=i(L),pb=s(L,"LI",{});var qMe=n(pb);wee=s(qMe,"STRONG",{});var YIr=n(wee);cTo=r(YIr,"gptj"),YIr.forEach(t),mTo=r(qMe," \u2014 "),yP=s(qMe,"A",{href:!0});var KIr=n(yP);fTo=r(KIr,"GPTJForSequenceClassification"),KIr.forEach(t),gTo=r(qMe," (GPT-J model)"),qMe.forEach(t),hTo=i(L),_b=s(L,"LI",{});var GMe=n(_b);Aee=s(GMe,"STRONG",{});var ZIr=n(Aee);uTo=r(ZIr,"ibert"),ZIr.forEach(t),pTo=r(GMe," \u2014 "),wP=s(GMe,"A",{href:!0});var ejr=n(wP);_To=r(ejr,"IBertForSequenceClassification"),ejr.forEach(t),bTo=r(GMe," (I-BERT model)"),GMe.forEach(t),vTo=i(L),bb=s(L,"LI",{});var OMe=n(bb);Lee=s(OMe,"STRONG",{});var ojr=n(Lee);TTo=r(ojr,"layoutlm"),ojr.forEach(t),FTo=r(OMe," \u2014 "),AP=s(OMe,"A",{href:!0});var rjr=n(AP);CTo=r(rjr,"LayoutLMForSequenceClassification"),rjr.forEach(t),MTo=r(OMe," (LayoutLM model)"),OMe.forEach(t),ETo=i(L),vb=s(L,"LI",{});var XMe=n(vb);Bee=s(XMe,"STRONG",{});var tjr=n(Bee);yTo=r(tjr,"layoutlmv2"),tjr.forEach(t),wTo=r(XMe," \u2014 "),LP=s(XMe,"A",{href:!0});var ajr=n(LP);ATo=r(ajr,"LayoutLMv2ForSequenceClassification"),ajr.forEach(t),LTo=r(XMe," (LayoutLMv2 model)"),XMe.forEach(t),BTo=i(L),Tb=s(L,"LI",{});var zMe=n(Tb);xee=s(zMe,"STRONG",{});var sjr=n(xee);xTo=r(sjr,"led"),sjr.forEach(t),kTo=r(zMe," \u2014 "),BP=s(zMe,"A",{href:!0});var njr=n(BP);RTo=r(njr,"LEDForSequenceClassification"),njr.forEach(t),STo=r(zMe," (LED model)"),zMe.forEach(t),PTo=i(L),Fb=s(L,"LI",{});var VMe=n(Fb);kee=s(VMe,"STRONG",{});var ljr=n(kee);$To=r(ljr,"longformer"),ljr.forEach(t),ITo=r(VMe," \u2014 "),xP=s(VMe,"A",{href:!0});var ijr=n(xP);jTo=r(ijr,"LongformerForSequenceClassification"),ijr.forEach(t),NTo=r(VMe," (Longformer model)"),VMe.forEach(t),DTo=i(L),Cb=s(L,"LI",{});var WMe=n(Cb);Ree=s(WMe,"STRONG",{});var djr=n(Ree);qTo=r(djr,"mbart"),djr.forEach(t),GTo=r(WMe," \u2014 "),kP=s(WMe,"A",{href:!0});var cjr=n(kP);OTo=r(cjr,"MBartForSequenceClassification"),cjr.forEach(t),XTo=r(WMe," (mBART model)"),WMe.forEach(t),zTo=i(L),Mb=s(L,"LI",{});var QMe=n(Mb);See=s(QMe,"STRONG",{});var mjr=n(See);VTo=r(mjr,"megatron-bert"),mjr.forEach(t),WTo=r(QMe," \u2014 "),RP=s(QMe,"A",{href:!0});var fjr=n(RP);QTo=r(fjr,"MegatronBertForSequenceClassification"),fjr.forEach(t),HTo=r(QMe," (MegatronBert model)"),QMe.forEach(t),UTo=i(L),Eb=s(L,"LI",{});var HMe=n(Eb);Pee=s(HMe,"STRONG",{});var gjr=n(Pee);JTo=r(gjr,"mobilebert"),gjr.forEach(t),YTo=r(HMe," \u2014 "),SP=s(HMe,"A",{href:!0});var hjr=n(SP);KTo=r(hjr,"MobileBertForSequenceClassification"),hjr.forEach(t),ZTo=r(HMe," (MobileBERT model)"),HMe.forEach(t),e1o=i(L),yb=s(L,"LI",{});var UMe=n(yb);$ee=s(UMe,"STRONG",{});var ujr=n($ee);o1o=r(ujr,"mpnet"),ujr.forEach(t),r1o=r(UMe," \u2014 "),PP=s(UMe,"A",{href:!0});var pjr=n(PP);t1o=r(pjr,"MPNetForSequenceClassification"),pjr.forEach(t),a1o=r(UMe," (MPNet model)"),UMe.forEach(t),s1o=i(L),wb=s(L,"LI",{});var JMe=n(wb);Iee=s(JMe,"STRONG",{});var _jr=n(Iee);n1o=r(_jr,"nystromformer"),_jr.forEach(t),l1o=r(JMe," \u2014 "),$P=s(JMe,"A",{href:!0});var bjr=n($P);i1o=r(bjr,"NystromformerForSequenceClassification"),bjr.forEach(t),d1o=r(JMe," (Nystromformer model)"),JMe.forEach(t),c1o=i(L),Ab=s(L,"LI",{});var YMe=n(Ab);jee=s(YMe,"STRONG",{});var vjr=n(jee);m1o=r(vjr,"openai-gpt"),vjr.forEach(t),f1o=r(YMe," \u2014 "),IP=s(YMe,"A",{href:!0});var Tjr=n(IP);g1o=r(Tjr,"OpenAIGPTForSequenceClassification"),Tjr.forEach(t),h1o=r(YMe," (OpenAI GPT model)"),YMe.forEach(t),u1o=i(L),Lb=s(L,"LI",{});var KMe=n(Lb);Nee=s(KMe,"STRONG",{});var Fjr=n(Nee);p1o=r(Fjr,"perceiver"),Fjr.forEach(t),_1o=r(KMe," \u2014 "),jP=s(KMe,"A",{href:!0});var Cjr=n(jP);b1o=r(Cjr,"PerceiverForSequenceClassification"),Cjr.forEach(t),v1o=r(KMe," (Perceiver model)"),KMe.forEach(t),T1o=i(L),Bb=s(L,"LI",{});var ZMe=n(Bb);Dee=s(ZMe,"STRONG",{});var Mjr=n(Dee);F1o=r(Mjr,"qdqbert"),Mjr.forEach(t),C1o=r(ZMe," \u2014 "),qee=s(ZMe,"CODE",{});var Ejr=n(qee);M1o=r(Ejr,"QDQBertForSequenceClassification"),Ejr.forEach(t),E1o=r(ZMe,"(QDQBert model)"),ZMe.forEach(t),y1o=i(L),xb=s(L,"LI",{});var eEe=n(xb);Gee=s(eEe,"STRONG",{});var yjr=n(Gee);w1o=r(yjr,"reformer"),yjr.forEach(t),A1o=r(eEe," \u2014 "),NP=s(eEe,"A",{href:!0});var wjr=n(NP);L1o=r(wjr,"ReformerForSequenceClassification"),wjr.forEach(t),B1o=r(eEe," (Reformer model)"),eEe.forEach(t),x1o=i(L),kb=s(L,"LI",{});var oEe=n(kb);Oee=s(oEe,"STRONG",{});var Ajr=n(Oee);k1o=r(Ajr,"rembert"),Ajr.forEach(t),R1o=r(oEe," \u2014 "),DP=s(oEe,"A",{href:!0});var Ljr=n(DP);S1o=r(Ljr,"RemBertForSequenceClassification"),Ljr.forEach(t),P1o=r(oEe," (RemBERT model)"),oEe.forEach(t),$1o=i(L),Rb=s(L,"LI",{});var rEe=n(Rb);Xee=s(rEe,"STRONG",{});var Bjr=n(Xee);I1o=r(Bjr,"roberta"),Bjr.forEach(t),j1o=r(rEe," \u2014 "),qP=s(rEe,"A",{href:!0});var xjr=n(qP);N1o=r(xjr,"RobertaForSequenceClassification"),xjr.forEach(t),D1o=r(rEe," (RoBERTa model)"),rEe.forEach(t),q1o=i(L),Sb=s(L,"LI",{});var tEe=n(Sb);zee=s(tEe,"STRONG",{});var kjr=n(zee);G1o=r(kjr,"roformer"),kjr.forEach(t),O1o=r(tEe," \u2014 "),GP=s(tEe,"A",{href:!0});var Rjr=n(GP);X1o=r(Rjr,"RoFormerForSequenceClassification"),Rjr.forEach(t),z1o=r(tEe," (RoFormer model)"),tEe.forEach(t),V1o=i(L),Pb=s(L,"LI",{});var aEe=n(Pb);Vee=s(aEe,"STRONG",{});var Sjr=n(Vee);W1o=r(Sjr,"squeezebert"),Sjr.forEach(t),Q1o=r(aEe," \u2014 "),OP=s(aEe,"A",{href:!0});var Pjr=n(OP);H1o=r(Pjr,"SqueezeBertForSequenceClassification"),Pjr.forEach(t),U1o=r(aEe," (SqueezeBERT model)"),aEe.forEach(t),J1o=i(L),$b=s(L,"LI",{});var sEe=n($b);Wee=s(sEe,"STRONG",{});var $jr=n(Wee);Y1o=r($jr,"tapas"),$jr.forEach(t),K1o=r(sEe," \u2014 "),XP=s(sEe,"A",{href:!0});var Ijr=n(XP);Z1o=r(Ijr,"TapasForSequenceClassification"),Ijr.forEach(t),eFo=r(sEe," (TAPAS model)"),sEe.forEach(t),oFo=i(L),Ib=s(L,"LI",{});var nEe=n(Ib);Qee=s(nEe,"STRONG",{});var jjr=n(Qee);rFo=r(jjr,"transfo-xl"),jjr.forEach(t),tFo=r(nEe," \u2014 "),zP=s(nEe,"A",{href:!0});var Njr=n(zP);aFo=r(Njr,"TransfoXLForSequenceClassification"),Njr.forEach(t),sFo=r(nEe," (Transformer-XL model)"),nEe.forEach(t),nFo=i(L),jb=s(L,"LI",{});var lEe=n(jb);Hee=s(lEe,"STRONG",{});var Djr=n(Hee);lFo=r(Djr,"xlm"),Djr.forEach(t),iFo=r(lEe," \u2014 "),VP=s(lEe,"A",{href:!0});var qjr=n(VP);dFo=r(qjr,"XLMForSequenceClassification"),qjr.forEach(t),cFo=r(lEe," (XLM model)"),lEe.forEach(t),mFo=i(L),Nb=s(L,"LI",{});var iEe=n(Nb);Uee=s(iEe,"STRONG",{});var Gjr=n(Uee);fFo=r(Gjr,"xlm-roberta"),Gjr.forEach(t),gFo=r(iEe," \u2014 "),WP=s(iEe,"A",{href:!0});var Ojr=n(WP);hFo=r(Ojr,"XLMRobertaForSequenceClassification"),Ojr.forEach(t),uFo=r(iEe," (XLM-RoBERTa model)"),iEe.forEach(t),pFo=i(L),Db=s(L,"LI",{});var dEe=n(Db);Jee=s(dEe,"STRONG",{});var Xjr=n(Jee);_Fo=r(Xjr,"xlm-roberta-xl"),Xjr.forEach(t),bFo=r(dEe," \u2014 "),QP=s(dEe,"A",{href:!0});var zjr=n(QP);vFo=r(zjr,"XLMRobertaXLForSequenceClassification"),zjr.forEach(t),TFo=r(dEe," (XLM-RoBERTa-XL model)"),dEe.forEach(t),FFo=i(L),qb=s(L,"LI",{});var cEe=n(qb);Yee=s(cEe,"STRONG",{});var Vjr=n(Yee);CFo=r(Vjr,"xlnet"),Vjr.forEach(t),MFo=r(cEe," \u2014 "),HP=s(cEe,"A",{href:!0});var Wjr=n(HP);EFo=r(Wjr,"XLNetForSequenceClassification"),Wjr.forEach(t),yFo=r(cEe," (XLNet model)"),cEe.forEach(t),wFo=i(L),Gb=s(L,"LI",{});var mEe=n(Gb);Kee=s(mEe,"STRONG",{});var Qjr=n(Kee);AFo=r(Qjr,"yoso"),Qjr.forEach(t),LFo=r(mEe," \u2014 "),UP=s(mEe,"A",{href:!0});var Hjr=n(UP);BFo=r(Hjr,"YosoForSequenceClassification"),Hjr.forEach(t),xFo=r(mEe," (YOSO model)"),mEe.forEach(t),L.forEach(t),kFo=i(St),Ob=s(St,"P",{});var fEe=n(Ob);RFo=r(fEe,"The model is set in evaluation mode by default using "),Zee=s(fEe,"CODE",{});var Ujr=n(Zee);SFo=r(Ujr,"model.eval()"),Ujr.forEach(t),PFo=r(fEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),eoe=s(fEe,"CODE",{});var Jjr=n(eoe);$Fo=r(Jjr,"model.train()"),Jjr.forEach(t),fEe.forEach(t),IFo=i(St),ooe=s(St,"P",{});var Yjr=n(ooe);jFo=r(Yjr,"Examples:"),Yjr.forEach(t),NFo=i(St),f(V5.$$.fragment,St),St.forEach(t),Gn.forEach(t),jLe=i(d),Ui=s(d,"H2",{class:!0});var q8e=n(Ui);Xb=s(q8e,"A",{id:!0,class:!0,href:!0});var Kjr=n(Xb);roe=s(Kjr,"SPAN",{});var Zjr=n(roe);f(W5.$$.fragment,Zjr),Zjr.forEach(t),Kjr.forEach(t),DFo=i(q8e),toe=s(q8e,"SPAN",{});var eNr=n(toe);qFo=r(eNr,"AutoModelForMultipleChoice"),eNr.forEach(t),q8e.forEach(t),NLe=i(d),Ho=s(d,"DIV",{class:!0});var Xn=n(Ho);f(Q5.$$.fragment,Xn),GFo=i(Xn),Ji=s(Xn,"P",{});var KO=n(Ji);OFo=r(KO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),aoe=s(KO,"CODE",{});var oNr=n(aoe);XFo=r(oNr,"from_pretrained()"),oNr.forEach(t),zFo=r(KO,"class method or the "),soe=s(KO,"CODE",{});var rNr=n(soe);VFo=r(rNr,"from_config()"),rNr.forEach(t),WFo=r(KO,`class
method.`),KO.forEach(t),QFo=i(Xn),H5=s(Xn,"P",{});var G8e=n(H5);HFo=r(G8e,"This class cannot be instantiated directly using "),noe=s(G8e,"CODE",{});var tNr=n(noe);UFo=r(tNr,"__init__()"),tNr.forEach(t),JFo=r(G8e," (throws an error)."),G8e.forEach(t),YFo=i(Xn),qr=s(Xn,"DIV",{class:!0});var zn=n(qr);f(U5.$$.fragment,zn),KFo=i(zn),loe=s(zn,"P",{});var aNr=n(loe);ZFo=r(aNr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),aNr.forEach(t),eCo=i(zn),Yi=s(zn,"P",{});var ZO=n(Yi);oCo=r(ZO,`Note:
Loading a model from its configuration file does `),ioe=s(ZO,"STRONG",{});var sNr=n(ioe);rCo=r(sNr,"not"),sNr.forEach(t),tCo=r(ZO,` load the model weights. It only affects the
model\u2019s configuration. Use `),doe=s(ZO,"CODE",{});var nNr=n(doe);aCo=r(nNr,"from_pretrained()"),nNr.forEach(t),sCo=r(ZO,"to load the model weights."),ZO.forEach(t),nCo=i(zn),coe=s(zn,"P",{});var lNr=n(coe);lCo=r(lNr,"Examples:"),lNr.forEach(t),iCo=i(zn),f(J5.$$.fragment,zn),zn.forEach(t),dCo=i(Xn),Ie=s(Xn,"DIV",{class:!0});var Pt=n(Ie);f(Y5.$$.fragment,Pt),cCo=i(Pt),moe=s(Pt,"P",{});var iNr=n(moe);mCo=r(iNr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),iNr.forEach(t),fCo=i(Pt),Ga=s(Pt,"P",{});var DE=n(Ga);gCo=r(DE,"The model class to instantiate is selected based on the "),foe=s(DE,"CODE",{});var dNr=n(foe);hCo=r(dNr,"model_type"),dNr.forEach(t),uCo=r(DE,` property of the config object (either
passed as an argument or loaded from `),goe=s(DE,"CODE",{});var cNr=n(goe);pCo=r(cNr,"pretrained_model_name_or_path"),cNr.forEach(t),_Co=r(DE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hoe=s(DE,"CODE",{});var mNr=n(hoe);bCo=r(mNr,"pretrained_model_name_or_path"),mNr.forEach(t),vCo=r(DE,":"),DE.forEach(t),TCo=i(Pt),G=s(Pt,"UL",{});var O=n(G);zb=s(O,"LI",{});var gEe=n(zb);uoe=s(gEe,"STRONG",{});var fNr=n(uoe);FCo=r(fNr,"albert"),fNr.forEach(t),CCo=r(gEe," \u2014 "),JP=s(gEe,"A",{href:!0});var gNr=n(JP);MCo=r(gNr,"AlbertForMultipleChoice"),gNr.forEach(t),ECo=r(gEe," (ALBERT model)"),gEe.forEach(t),yCo=i(O),Vb=s(O,"LI",{});var hEe=n(Vb);poe=s(hEe,"STRONG",{});var hNr=n(poe);wCo=r(hNr,"bert"),hNr.forEach(t),ACo=r(hEe," \u2014 "),YP=s(hEe,"A",{href:!0});var uNr=n(YP);LCo=r(uNr,"BertForMultipleChoice"),uNr.forEach(t),BCo=r(hEe," (BERT model)"),hEe.forEach(t),xCo=i(O),Wb=s(O,"LI",{});var uEe=n(Wb);_oe=s(uEe,"STRONG",{});var pNr=n(_oe);kCo=r(pNr,"big_bird"),pNr.forEach(t),RCo=r(uEe," \u2014 "),KP=s(uEe,"A",{href:!0});var _Nr=n(KP);SCo=r(_Nr,"BigBirdForMultipleChoice"),_Nr.forEach(t),PCo=r(uEe," (BigBird model)"),uEe.forEach(t),$Co=i(O),Qb=s(O,"LI",{});var pEe=n(Qb);boe=s(pEe,"STRONG",{});var bNr=n(boe);ICo=r(bNr,"camembert"),bNr.forEach(t),jCo=r(pEe," \u2014 "),ZP=s(pEe,"A",{href:!0});var vNr=n(ZP);NCo=r(vNr,"CamembertForMultipleChoice"),vNr.forEach(t),DCo=r(pEe," (CamemBERT model)"),pEe.forEach(t),qCo=i(O),Hb=s(O,"LI",{});var _Ee=n(Hb);voe=s(_Ee,"STRONG",{});var TNr=n(voe);GCo=r(TNr,"canine"),TNr.forEach(t),OCo=r(_Ee," \u2014 "),e$=s(_Ee,"A",{href:!0});var FNr=n(e$);XCo=r(FNr,"CanineForMultipleChoice"),FNr.forEach(t),zCo=r(_Ee," (Canine model)"),_Ee.forEach(t),VCo=i(O),Ub=s(O,"LI",{});var bEe=n(Ub);Toe=s(bEe,"STRONG",{});var CNr=n(Toe);WCo=r(CNr,"convbert"),CNr.forEach(t),QCo=r(bEe," \u2014 "),o$=s(bEe,"A",{href:!0});var MNr=n(o$);HCo=r(MNr,"ConvBertForMultipleChoice"),MNr.forEach(t),UCo=r(bEe," (ConvBERT model)"),bEe.forEach(t),JCo=i(O),Jb=s(O,"LI",{});var vEe=n(Jb);Foe=s(vEe,"STRONG",{});var ENr=n(Foe);YCo=r(ENr,"distilbert"),ENr.forEach(t),KCo=r(vEe," \u2014 "),r$=s(vEe,"A",{href:!0});var yNr=n(r$);ZCo=r(yNr,"DistilBertForMultipleChoice"),yNr.forEach(t),e4o=r(vEe," (DistilBERT model)"),vEe.forEach(t),o4o=i(O),Yb=s(O,"LI",{});var TEe=n(Yb);Coe=s(TEe,"STRONG",{});var wNr=n(Coe);r4o=r(wNr,"electra"),wNr.forEach(t),t4o=r(TEe," \u2014 "),t$=s(TEe,"A",{href:!0});var ANr=n(t$);a4o=r(ANr,"ElectraForMultipleChoice"),ANr.forEach(t),s4o=r(TEe," (ELECTRA model)"),TEe.forEach(t),n4o=i(O),Kb=s(O,"LI",{});var FEe=n(Kb);Moe=s(FEe,"STRONG",{});var LNr=n(Moe);l4o=r(LNr,"flaubert"),LNr.forEach(t),i4o=r(FEe," \u2014 "),a$=s(FEe,"A",{href:!0});var BNr=n(a$);d4o=r(BNr,"FlaubertForMultipleChoice"),BNr.forEach(t),c4o=r(FEe," (FlauBERT model)"),FEe.forEach(t),m4o=i(O),Zb=s(O,"LI",{});var CEe=n(Zb);Eoe=s(CEe,"STRONG",{});var xNr=n(Eoe);f4o=r(xNr,"fnet"),xNr.forEach(t),g4o=r(CEe," \u2014 "),s$=s(CEe,"A",{href:!0});var kNr=n(s$);h4o=r(kNr,"FNetForMultipleChoice"),kNr.forEach(t),u4o=r(CEe," (FNet model)"),CEe.forEach(t),p4o=i(O),e2=s(O,"LI",{});var MEe=n(e2);yoe=s(MEe,"STRONG",{});var RNr=n(yoe);_4o=r(RNr,"funnel"),RNr.forEach(t),b4o=r(MEe," \u2014 "),n$=s(MEe,"A",{href:!0});var SNr=n(n$);v4o=r(SNr,"FunnelForMultipleChoice"),SNr.forEach(t),T4o=r(MEe," (Funnel Transformer model)"),MEe.forEach(t),F4o=i(O),o2=s(O,"LI",{});var EEe=n(o2);woe=s(EEe,"STRONG",{});var PNr=n(woe);C4o=r(PNr,"ibert"),PNr.forEach(t),M4o=r(EEe," \u2014 "),l$=s(EEe,"A",{href:!0});var $Nr=n(l$);E4o=r($Nr,"IBertForMultipleChoice"),$Nr.forEach(t),y4o=r(EEe," (I-BERT model)"),EEe.forEach(t),w4o=i(O),r2=s(O,"LI",{});var yEe=n(r2);Aoe=s(yEe,"STRONG",{});var INr=n(Aoe);A4o=r(INr,"longformer"),INr.forEach(t),L4o=r(yEe," \u2014 "),i$=s(yEe,"A",{href:!0});var jNr=n(i$);B4o=r(jNr,"LongformerForMultipleChoice"),jNr.forEach(t),x4o=r(yEe," (Longformer model)"),yEe.forEach(t),k4o=i(O),t2=s(O,"LI",{});var wEe=n(t2);Loe=s(wEe,"STRONG",{});var NNr=n(Loe);R4o=r(NNr,"megatron-bert"),NNr.forEach(t),S4o=r(wEe," \u2014 "),d$=s(wEe,"A",{href:!0});var DNr=n(d$);P4o=r(DNr,"MegatronBertForMultipleChoice"),DNr.forEach(t),$4o=r(wEe," (MegatronBert model)"),wEe.forEach(t),I4o=i(O),a2=s(O,"LI",{});var AEe=n(a2);Boe=s(AEe,"STRONG",{});var qNr=n(Boe);j4o=r(qNr,"mobilebert"),qNr.forEach(t),N4o=r(AEe," \u2014 "),c$=s(AEe,"A",{href:!0});var GNr=n(c$);D4o=r(GNr,"MobileBertForMultipleChoice"),GNr.forEach(t),q4o=r(AEe," (MobileBERT model)"),AEe.forEach(t),G4o=i(O),s2=s(O,"LI",{});var LEe=n(s2);xoe=s(LEe,"STRONG",{});var ONr=n(xoe);O4o=r(ONr,"mpnet"),ONr.forEach(t),X4o=r(LEe," \u2014 "),m$=s(LEe,"A",{href:!0});var XNr=n(m$);z4o=r(XNr,"MPNetForMultipleChoice"),XNr.forEach(t),V4o=r(LEe," (MPNet model)"),LEe.forEach(t),W4o=i(O),n2=s(O,"LI",{});var BEe=n(n2);koe=s(BEe,"STRONG",{});var zNr=n(koe);Q4o=r(zNr,"nystromformer"),zNr.forEach(t),H4o=r(BEe," \u2014 "),f$=s(BEe,"A",{href:!0});var VNr=n(f$);U4o=r(VNr,"NystromformerForMultipleChoice"),VNr.forEach(t),J4o=r(BEe," (Nystromformer model)"),BEe.forEach(t),Y4o=i(O),l2=s(O,"LI",{});var xEe=n(l2);Roe=s(xEe,"STRONG",{});var WNr=n(Roe);K4o=r(WNr,"qdqbert"),WNr.forEach(t),Z4o=r(xEe," \u2014 "),Soe=s(xEe,"CODE",{});var QNr=n(Soe);eMo=r(QNr,"QDQBertForMultipleChoice"),QNr.forEach(t),oMo=r(xEe,"(QDQBert model)"),xEe.forEach(t),rMo=i(O),i2=s(O,"LI",{});var kEe=n(i2);Poe=s(kEe,"STRONG",{});var HNr=n(Poe);tMo=r(HNr,"rembert"),HNr.forEach(t),aMo=r(kEe," \u2014 "),g$=s(kEe,"A",{href:!0});var UNr=n(g$);sMo=r(UNr,"RemBertForMultipleChoice"),UNr.forEach(t),nMo=r(kEe," (RemBERT model)"),kEe.forEach(t),lMo=i(O),d2=s(O,"LI",{});var REe=n(d2);$oe=s(REe,"STRONG",{});var JNr=n($oe);iMo=r(JNr,"roberta"),JNr.forEach(t),dMo=r(REe," \u2014 "),h$=s(REe,"A",{href:!0});var YNr=n(h$);cMo=r(YNr,"RobertaForMultipleChoice"),YNr.forEach(t),mMo=r(REe," (RoBERTa model)"),REe.forEach(t),fMo=i(O),c2=s(O,"LI",{});var SEe=n(c2);Ioe=s(SEe,"STRONG",{});var KNr=n(Ioe);gMo=r(KNr,"roformer"),KNr.forEach(t),hMo=r(SEe," \u2014 "),u$=s(SEe,"A",{href:!0});var ZNr=n(u$);uMo=r(ZNr,"RoFormerForMultipleChoice"),ZNr.forEach(t),pMo=r(SEe," (RoFormer model)"),SEe.forEach(t),_Mo=i(O),m2=s(O,"LI",{});var PEe=n(m2);joe=s(PEe,"STRONG",{});var eDr=n(joe);bMo=r(eDr,"squeezebert"),eDr.forEach(t),vMo=r(PEe," \u2014 "),p$=s(PEe,"A",{href:!0});var oDr=n(p$);TMo=r(oDr,"SqueezeBertForMultipleChoice"),oDr.forEach(t),FMo=r(PEe," (SqueezeBERT model)"),PEe.forEach(t),CMo=i(O),f2=s(O,"LI",{});var $Ee=n(f2);Noe=s($Ee,"STRONG",{});var rDr=n(Noe);MMo=r(rDr,"xlm"),rDr.forEach(t),EMo=r($Ee," \u2014 "),_$=s($Ee,"A",{href:!0});var tDr=n(_$);yMo=r(tDr,"XLMForMultipleChoice"),tDr.forEach(t),wMo=r($Ee," (XLM model)"),$Ee.forEach(t),AMo=i(O),g2=s(O,"LI",{});var IEe=n(g2);Doe=s(IEe,"STRONG",{});var aDr=n(Doe);LMo=r(aDr,"xlm-roberta"),aDr.forEach(t),BMo=r(IEe," \u2014 "),b$=s(IEe,"A",{href:!0});var sDr=n(b$);xMo=r(sDr,"XLMRobertaForMultipleChoice"),sDr.forEach(t),kMo=r(IEe," (XLM-RoBERTa model)"),IEe.forEach(t),RMo=i(O),h2=s(O,"LI",{});var jEe=n(h2);qoe=s(jEe,"STRONG",{});var nDr=n(qoe);SMo=r(nDr,"xlm-roberta-xl"),nDr.forEach(t),PMo=r(jEe," \u2014 "),v$=s(jEe,"A",{href:!0});var lDr=n(v$);$Mo=r(lDr,"XLMRobertaXLForMultipleChoice"),lDr.forEach(t),IMo=r(jEe," (XLM-RoBERTa-XL model)"),jEe.forEach(t),jMo=i(O),u2=s(O,"LI",{});var NEe=n(u2);Goe=s(NEe,"STRONG",{});var iDr=n(Goe);NMo=r(iDr,"xlnet"),iDr.forEach(t),DMo=r(NEe," \u2014 "),T$=s(NEe,"A",{href:!0});var dDr=n(T$);qMo=r(dDr,"XLNetForMultipleChoice"),dDr.forEach(t),GMo=r(NEe," (XLNet model)"),NEe.forEach(t),OMo=i(O),p2=s(O,"LI",{});var DEe=n(p2);Ooe=s(DEe,"STRONG",{});var cDr=n(Ooe);XMo=r(cDr,"yoso"),cDr.forEach(t),zMo=r(DEe," \u2014 "),F$=s(DEe,"A",{href:!0});var mDr=n(F$);VMo=r(mDr,"YosoForMultipleChoice"),mDr.forEach(t),WMo=r(DEe," (YOSO model)"),DEe.forEach(t),O.forEach(t),QMo=i(Pt),_2=s(Pt,"P",{});var qEe=n(_2);HMo=r(qEe,"The model is set in evaluation mode by default using "),Xoe=s(qEe,"CODE",{});var fDr=n(Xoe);UMo=r(fDr,"model.eval()"),fDr.forEach(t),JMo=r(qEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zoe=s(qEe,"CODE",{});var gDr=n(zoe);YMo=r(gDr,"model.train()"),gDr.forEach(t),qEe.forEach(t),KMo=i(Pt),Voe=s(Pt,"P",{});var hDr=n(Voe);ZMo=r(hDr,"Examples:"),hDr.forEach(t),eEo=i(Pt),f(K5.$$.fragment,Pt),Pt.forEach(t),Xn.forEach(t),DLe=i(d),Ki=s(d,"H2",{class:!0});var O8e=n(Ki);b2=s(O8e,"A",{id:!0,class:!0,href:!0});var uDr=n(b2);Woe=s(uDr,"SPAN",{});var pDr=n(Woe);f(Z5.$$.fragment,pDr),pDr.forEach(t),uDr.forEach(t),oEo=i(O8e),Qoe=s(O8e,"SPAN",{});var _Dr=n(Qoe);rEo=r(_Dr,"AutoModelForNextSentencePrediction"),_Dr.forEach(t),O8e.forEach(t),qLe=i(d),Uo=s(d,"DIV",{class:!0});var Vn=n(Uo);f(ey.$$.fragment,Vn),tEo=i(Vn),Zi=s(Vn,"P",{});var eX=n(Zi);aEo=r(eX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Hoe=s(eX,"CODE",{});var bDr=n(Hoe);sEo=r(bDr,"from_pretrained()"),bDr.forEach(t),nEo=r(eX,"class method or the "),Uoe=s(eX,"CODE",{});var vDr=n(Uoe);lEo=r(vDr,"from_config()"),vDr.forEach(t),iEo=r(eX,`class
method.`),eX.forEach(t),dEo=i(Vn),oy=s(Vn,"P",{});var X8e=n(oy);cEo=r(X8e,"This class cannot be instantiated directly using "),Joe=s(X8e,"CODE",{});var TDr=n(Joe);mEo=r(TDr,"__init__()"),TDr.forEach(t),fEo=r(X8e," (throws an error)."),X8e.forEach(t),gEo=i(Vn),Gr=s(Vn,"DIV",{class:!0});var Wn=n(Gr);f(ry.$$.fragment,Wn),hEo=i(Wn),Yoe=s(Wn,"P",{});var FDr=n(Yoe);uEo=r(FDr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),FDr.forEach(t),pEo=i(Wn),ed=s(Wn,"P",{});var oX=n(ed);_Eo=r(oX,`Note:
Loading a model from its configuration file does `),Koe=s(oX,"STRONG",{});var CDr=n(Koe);bEo=r(CDr,"not"),CDr.forEach(t),vEo=r(oX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zoe=s(oX,"CODE",{});var MDr=n(Zoe);TEo=r(MDr,"from_pretrained()"),MDr.forEach(t),FEo=r(oX,"to load the model weights."),oX.forEach(t),CEo=i(Wn),ere=s(Wn,"P",{});var EDr=n(ere);MEo=r(EDr,"Examples:"),EDr.forEach(t),EEo=i(Wn),f(ty.$$.fragment,Wn),Wn.forEach(t),yEo=i(Vn),je=s(Vn,"DIV",{class:!0});var $t=n(je);f(ay.$$.fragment,$t),wEo=i($t),ore=s($t,"P",{});var yDr=n(ore);AEo=r(yDr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),yDr.forEach(t),LEo=i($t),Oa=s($t,"P",{});var qE=n(Oa);BEo=r(qE,"The model class to instantiate is selected based on the "),rre=s(qE,"CODE",{});var wDr=n(rre);xEo=r(wDr,"model_type"),wDr.forEach(t),kEo=r(qE,` property of the config object (either
passed as an argument or loaded from `),tre=s(qE,"CODE",{});var ADr=n(tre);REo=r(ADr,"pretrained_model_name_or_path"),ADr.forEach(t),SEo=r(qE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),are=s(qE,"CODE",{});var LDr=n(are);PEo=r(LDr,"pretrained_model_name_or_path"),LDr.forEach(t),$Eo=r(qE,":"),qE.forEach(t),IEo=i($t),oa=s($t,"UL",{});var Qn=n(oa);v2=s(Qn,"LI",{});var GEe=n(v2);sre=s(GEe,"STRONG",{});var BDr=n(sre);jEo=r(BDr,"bert"),BDr.forEach(t),NEo=r(GEe," \u2014 "),C$=s(GEe,"A",{href:!0});var xDr=n(C$);DEo=r(xDr,"BertForNextSentencePrediction"),xDr.forEach(t),qEo=r(GEe," (BERT model)"),GEe.forEach(t),GEo=i(Qn),T2=s(Qn,"LI",{});var OEe=n(T2);nre=s(OEe,"STRONG",{});var kDr=n(nre);OEo=r(kDr,"fnet"),kDr.forEach(t),XEo=r(OEe," \u2014 "),M$=s(OEe,"A",{href:!0});var RDr=n(M$);zEo=r(RDr,"FNetForNextSentencePrediction"),RDr.forEach(t),VEo=r(OEe," (FNet model)"),OEe.forEach(t),WEo=i(Qn),F2=s(Qn,"LI",{});var XEe=n(F2);lre=s(XEe,"STRONG",{});var SDr=n(lre);QEo=r(SDr,"megatron-bert"),SDr.forEach(t),HEo=r(XEe," \u2014 "),E$=s(XEe,"A",{href:!0});var PDr=n(E$);UEo=r(PDr,"MegatronBertForNextSentencePrediction"),PDr.forEach(t),JEo=r(XEe," (MegatronBert model)"),XEe.forEach(t),YEo=i(Qn),C2=s(Qn,"LI",{});var zEe=n(C2);ire=s(zEe,"STRONG",{});var $Dr=n(ire);KEo=r($Dr,"mobilebert"),$Dr.forEach(t),ZEo=r(zEe," \u2014 "),y$=s(zEe,"A",{href:!0});var IDr=n(y$);e3o=r(IDr,"MobileBertForNextSentencePrediction"),IDr.forEach(t),o3o=r(zEe," (MobileBERT model)"),zEe.forEach(t),r3o=i(Qn),M2=s(Qn,"LI",{});var VEe=n(M2);dre=s(VEe,"STRONG",{});var jDr=n(dre);t3o=r(jDr,"qdqbert"),jDr.forEach(t),a3o=r(VEe," \u2014 "),cre=s(VEe,"CODE",{});var NDr=n(cre);s3o=r(NDr,"QDQBertForNextSentencePrediction"),NDr.forEach(t),n3o=r(VEe,"(QDQBert model)"),VEe.forEach(t),Qn.forEach(t),l3o=i($t),E2=s($t,"P",{});var WEe=n(E2);i3o=r(WEe,"The model is set in evaluation mode by default using "),mre=s(WEe,"CODE",{});var DDr=n(mre);d3o=r(DDr,"model.eval()"),DDr.forEach(t),c3o=r(WEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),fre=s(WEe,"CODE",{});var qDr=n(fre);m3o=r(qDr,"model.train()"),qDr.forEach(t),WEe.forEach(t),f3o=i($t),gre=s($t,"P",{});var GDr=n(gre);g3o=r(GDr,"Examples:"),GDr.forEach(t),h3o=i($t),f(sy.$$.fragment,$t),$t.forEach(t),Vn.forEach(t),GLe=i(d),od=s(d,"H2",{class:!0});var z8e=n(od);y2=s(z8e,"A",{id:!0,class:!0,href:!0});var ODr=n(y2);hre=s(ODr,"SPAN",{});var XDr=n(hre);f(ny.$$.fragment,XDr),XDr.forEach(t),ODr.forEach(t),u3o=i(z8e),ure=s(z8e,"SPAN",{});var zDr=n(ure);p3o=r(zDr,"AutoModelForTokenClassification"),zDr.forEach(t),z8e.forEach(t),OLe=i(d),Jo=s(d,"DIV",{class:!0});var Hn=n(Jo);f(ly.$$.fragment,Hn),_3o=i(Hn),rd=s(Hn,"P",{});var rX=n(rd);b3o=r(rX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),pre=s(rX,"CODE",{});var VDr=n(pre);v3o=r(VDr,"from_pretrained()"),VDr.forEach(t),T3o=r(rX,"class method or the "),_re=s(rX,"CODE",{});var WDr=n(_re);F3o=r(WDr,"from_config()"),WDr.forEach(t),C3o=r(rX,`class
method.`),rX.forEach(t),M3o=i(Hn),iy=s(Hn,"P",{});var V8e=n(iy);E3o=r(V8e,"This class cannot be instantiated directly using "),bre=s(V8e,"CODE",{});var QDr=n(bre);y3o=r(QDr,"__init__()"),QDr.forEach(t),w3o=r(V8e," (throws an error)."),V8e.forEach(t),A3o=i(Hn),Or=s(Hn,"DIV",{class:!0});var Un=n(Or);f(dy.$$.fragment,Un),L3o=i(Un),vre=s(Un,"P",{});var HDr=n(vre);B3o=r(HDr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),HDr.forEach(t),x3o=i(Un),td=s(Un,"P",{});var tX=n(td);k3o=r(tX,`Note:
Loading a model from its configuration file does `),Tre=s(tX,"STRONG",{});var UDr=n(Tre);R3o=r(UDr,"not"),UDr.forEach(t),S3o=r(tX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fre=s(tX,"CODE",{});var JDr=n(Fre);P3o=r(JDr,"from_pretrained()"),JDr.forEach(t),$3o=r(tX,"to load the model weights."),tX.forEach(t),I3o=i(Un),Cre=s(Un,"P",{});var YDr=n(Cre);j3o=r(YDr,"Examples:"),YDr.forEach(t),N3o=i(Un),f(cy.$$.fragment,Un),Un.forEach(t),D3o=i(Hn),Ne=s(Hn,"DIV",{class:!0});var It=n(Ne);f(my.$$.fragment,It),q3o=i(It),Mre=s(It,"P",{});var KDr=n(Mre);G3o=r(KDr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),KDr.forEach(t),O3o=i(It),Xa=s(It,"P",{});var GE=n(Xa);X3o=r(GE,"The model class to instantiate is selected based on the "),Ere=s(GE,"CODE",{});var ZDr=n(Ere);z3o=r(ZDr,"model_type"),ZDr.forEach(t),V3o=r(GE,` property of the config object (either
passed as an argument or loaded from `),yre=s(GE,"CODE",{});var eqr=n(yre);W3o=r(eqr,"pretrained_model_name_or_path"),eqr.forEach(t),Q3o=r(GE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wre=s(GE,"CODE",{});var oqr=n(wre);H3o=r(oqr,"pretrained_model_name_or_path"),oqr.forEach(t),U3o=r(GE,":"),GE.forEach(t),J3o=i(It),N=s(It,"UL",{});var q=n(N);w2=s(q,"LI",{});var QEe=n(w2);Are=s(QEe,"STRONG",{});var rqr=n(Are);Y3o=r(rqr,"albert"),rqr.forEach(t),K3o=r(QEe," \u2014 "),w$=s(QEe,"A",{href:!0});var tqr=n(w$);Z3o=r(tqr,"AlbertForTokenClassification"),tqr.forEach(t),e5o=r(QEe," (ALBERT model)"),QEe.forEach(t),o5o=i(q),A2=s(q,"LI",{});var HEe=n(A2);Lre=s(HEe,"STRONG",{});var aqr=n(Lre);r5o=r(aqr,"bert"),aqr.forEach(t),t5o=r(HEe," \u2014 "),A$=s(HEe,"A",{href:!0});var sqr=n(A$);a5o=r(sqr,"BertForTokenClassification"),sqr.forEach(t),s5o=r(HEe," (BERT model)"),HEe.forEach(t),n5o=i(q),L2=s(q,"LI",{});var UEe=n(L2);Bre=s(UEe,"STRONG",{});var nqr=n(Bre);l5o=r(nqr,"big_bird"),nqr.forEach(t),i5o=r(UEe," \u2014 "),L$=s(UEe,"A",{href:!0});var lqr=n(L$);d5o=r(lqr,"BigBirdForTokenClassification"),lqr.forEach(t),c5o=r(UEe," (BigBird model)"),UEe.forEach(t),m5o=i(q),B2=s(q,"LI",{});var JEe=n(B2);xre=s(JEe,"STRONG",{});var iqr=n(xre);f5o=r(iqr,"camembert"),iqr.forEach(t),g5o=r(JEe," \u2014 "),B$=s(JEe,"A",{href:!0});var dqr=n(B$);h5o=r(dqr,"CamembertForTokenClassification"),dqr.forEach(t),u5o=r(JEe," (CamemBERT model)"),JEe.forEach(t),p5o=i(q),x2=s(q,"LI",{});var YEe=n(x2);kre=s(YEe,"STRONG",{});var cqr=n(kre);_5o=r(cqr,"canine"),cqr.forEach(t),b5o=r(YEe," \u2014 "),x$=s(YEe,"A",{href:!0});var mqr=n(x$);v5o=r(mqr,"CanineForTokenClassification"),mqr.forEach(t),T5o=r(YEe," (Canine model)"),YEe.forEach(t),F5o=i(q),k2=s(q,"LI",{});var KEe=n(k2);Rre=s(KEe,"STRONG",{});var fqr=n(Rre);C5o=r(fqr,"convbert"),fqr.forEach(t),M5o=r(KEe," \u2014 "),k$=s(KEe,"A",{href:!0});var gqr=n(k$);E5o=r(gqr,"ConvBertForTokenClassification"),gqr.forEach(t),y5o=r(KEe," (ConvBERT model)"),KEe.forEach(t),w5o=i(q),R2=s(q,"LI",{});var ZEe=n(R2);Sre=s(ZEe,"STRONG",{});var hqr=n(Sre);A5o=r(hqr,"deberta"),hqr.forEach(t),L5o=r(ZEe," \u2014 "),R$=s(ZEe,"A",{href:!0});var uqr=n(R$);B5o=r(uqr,"DebertaForTokenClassification"),uqr.forEach(t),x5o=r(ZEe," (DeBERTa model)"),ZEe.forEach(t),k5o=i(q),S2=s(q,"LI",{});var e3e=n(S2);Pre=s(e3e,"STRONG",{});var pqr=n(Pre);R5o=r(pqr,"deberta-v2"),pqr.forEach(t),S5o=r(e3e," \u2014 "),S$=s(e3e,"A",{href:!0});var _qr=n(S$);P5o=r(_qr,"DebertaV2ForTokenClassification"),_qr.forEach(t),$5o=r(e3e," (DeBERTa-v2 model)"),e3e.forEach(t),I5o=i(q),P2=s(q,"LI",{});var o3e=n(P2);$re=s(o3e,"STRONG",{});var bqr=n($re);j5o=r(bqr,"distilbert"),bqr.forEach(t),N5o=r(o3e," \u2014 "),P$=s(o3e,"A",{href:!0});var vqr=n(P$);D5o=r(vqr,"DistilBertForTokenClassification"),vqr.forEach(t),q5o=r(o3e," (DistilBERT model)"),o3e.forEach(t),G5o=i(q),$2=s(q,"LI",{});var r3e=n($2);Ire=s(r3e,"STRONG",{});var Tqr=n(Ire);O5o=r(Tqr,"electra"),Tqr.forEach(t),X5o=r(r3e," \u2014 "),$$=s(r3e,"A",{href:!0});var Fqr=n($$);z5o=r(Fqr,"ElectraForTokenClassification"),Fqr.forEach(t),V5o=r(r3e," (ELECTRA model)"),r3e.forEach(t),W5o=i(q),I2=s(q,"LI",{});var t3e=n(I2);jre=s(t3e,"STRONG",{});var Cqr=n(jre);Q5o=r(Cqr,"flaubert"),Cqr.forEach(t),H5o=r(t3e," \u2014 "),I$=s(t3e,"A",{href:!0});var Mqr=n(I$);U5o=r(Mqr,"FlaubertForTokenClassification"),Mqr.forEach(t),J5o=r(t3e," (FlauBERT model)"),t3e.forEach(t),Y5o=i(q),j2=s(q,"LI",{});var a3e=n(j2);Nre=s(a3e,"STRONG",{});var Eqr=n(Nre);K5o=r(Eqr,"fnet"),Eqr.forEach(t),Z5o=r(a3e," \u2014 "),j$=s(a3e,"A",{href:!0});var yqr=n(j$);eyo=r(yqr,"FNetForTokenClassification"),yqr.forEach(t),oyo=r(a3e," (FNet model)"),a3e.forEach(t),ryo=i(q),N2=s(q,"LI",{});var s3e=n(N2);Dre=s(s3e,"STRONG",{});var wqr=n(Dre);tyo=r(wqr,"funnel"),wqr.forEach(t),ayo=r(s3e," \u2014 "),N$=s(s3e,"A",{href:!0});var Aqr=n(N$);syo=r(Aqr,"FunnelForTokenClassification"),Aqr.forEach(t),nyo=r(s3e," (Funnel Transformer model)"),s3e.forEach(t),lyo=i(q),D2=s(q,"LI",{});var n3e=n(D2);qre=s(n3e,"STRONG",{});var Lqr=n(qre);iyo=r(Lqr,"gpt2"),Lqr.forEach(t),dyo=r(n3e," \u2014 "),D$=s(n3e,"A",{href:!0});var Bqr=n(D$);cyo=r(Bqr,"GPT2ForTokenClassification"),Bqr.forEach(t),myo=r(n3e," (OpenAI GPT-2 model)"),n3e.forEach(t),fyo=i(q),q2=s(q,"LI",{});var l3e=n(q2);Gre=s(l3e,"STRONG",{});var xqr=n(Gre);gyo=r(xqr,"ibert"),xqr.forEach(t),hyo=r(l3e," \u2014 "),q$=s(l3e,"A",{href:!0});var kqr=n(q$);uyo=r(kqr,"IBertForTokenClassification"),kqr.forEach(t),pyo=r(l3e," (I-BERT model)"),l3e.forEach(t),_yo=i(q),G2=s(q,"LI",{});var i3e=n(G2);Ore=s(i3e,"STRONG",{});var Rqr=n(Ore);byo=r(Rqr,"layoutlm"),Rqr.forEach(t),vyo=r(i3e," \u2014 "),G$=s(i3e,"A",{href:!0});var Sqr=n(G$);Tyo=r(Sqr,"LayoutLMForTokenClassification"),Sqr.forEach(t),Fyo=r(i3e," (LayoutLM model)"),i3e.forEach(t),Cyo=i(q),O2=s(q,"LI",{});var d3e=n(O2);Xre=s(d3e,"STRONG",{});var Pqr=n(Xre);Myo=r(Pqr,"layoutlmv2"),Pqr.forEach(t),Eyo=r(d3e," \u2014 "),O$=s(d3e,"A",{href:!0});var $qr=n(O$);yyo=r($qr,"LayoutLMv2ForTokenClassification"),$qr.forEach(t),wyo=r(d3e," (LayoutLMv2 model)"),d3e.forEach(t),Ayo=i(q),X2=s(q,"LI",{});var c3e=n(X2);zre=s(c3e,"STRONG",{});var Iqr=n(zre);Lyo=r(Iqr,"longformer"),Iqr.forEach(t),Byo=r(c3e," \u2014 "),X$=s(c3e,"A",{href:!0});var jqr=n(X$);xyo=r(jqr,"LongformerForTokenClassification"),jqr.forEach(t),kyo=r(c3e," (Longformer model)"),c3e.forEach(t),Ryo=i(q),z2=s(q,"LI",{});var m3e=n(z2);Vre=s(m3e,"STRONG",{});var Nqr=n(Vre);Syo=r(Nqr,"megatron-bert"),Nqr.forEach(t),Pyo=r(m3e," \u2014 "),z$=s(m3e,"A",{href:!0});var Dqr=n(z$);$yo=r(Dqr,"MegatronBertForTokenClassification"),Dqr.forEach(t),Iyo=r(m3e," (MegatronBert model)"),m3e.forEach(t),jyo=i(q),V2=s(q,"LI",{});var f3e=n(V2);Wre=s(f3e,"STRONG",{});var qqr=n(Wre);Nyo=r(qqr,"mobilebert"),qqr.forEach(t),Dyo=r(f3e," \u2014 "),V$=s(f3e,"A",{href:!0});var Gqr=n(V$);qyo=r(Gqr,"MobileBertForTokenClassification"),Gqr.forEach(t),Gyo=r(f3e," (MobileBERT model)"),f3e.forEach(t),Oyo=i(q),W2=s(q,"LI",{});var g3e=n(W2);Qre=s(g3e,"STRONG",{});var Oqr=n(Qre);Xyo=r(Oqr,"mpnet"),Oqr.forEach(t),zyo=r(g3e," \u2014 "),W$=s(g3e,"A",{href:!0});var Xqr=n(W$);Vyo=r(Xqr,"MPNetForTokenClassification"),Xqr.forEach(t),Wyo=r(g3e," (MPNet model)"),g3e.forEach(t),Qyo=i(q),Q2=s(q,"LI",{});var h3e=n(Q2);Hre=s(h3e,"STRONG",{});var zqr=n(Hre);Hyo=r(zqr,"nystromformer"),zqr.forEach(t),Uyo=r(h3e," \u2014 "),Q$=s(h3e,"A",{href:!0});var Vqr=n(Q$);Jyo=r(Vqr,"NystromformerForTokenClassification"),Vqr.forEach(t),Yyo=r(h3e," (Nystromformer model)"),h3e.forEach(t),Kyo=i(q),H2=s(q,"LI",{});var u3e=n(H2);Ure=s(u3e,"STRONG",{});var Wqr=n(Ure);Zyo=r(Wqr,"qdqbert"),Wqr.forEach(t),ewo=r(u3e," \u2014 "),Jre=s(u3e,"CODE",{});var Qqr=n(Jre);owo=r(Qqr,"QDQBertForTokenClassification"),Qqr.forEach(t),rwo=r(u3e,"(QDQBert model)"),u3e.forEach(t),two=i(q),U2=s(q,"LI",{});var p3e=n(U2);Yre=s(p3e,"STRONG",{});var Hqr=n(Yre);awo=r(Hqr,"rembert"),Hqr.forEach(t),swo=r(p3e," \u2014 "),H$=s(p3e,"A",{href:!0});var Uqr=n(H$);nwo=r(Uqr,"RemBertForTokenClassification"),Uqr.forEach(t),lwo=r(p3e," (RemBERT model)"),p3e.forEach(t),iwo=i(q),J2=s(q,"LI",{});var _3e=n(J2);Kre=s(_3e,"STRONG",{});var Jqr=n(Kre);dwo=r(Jqr,"roberta"),Jqr.forEach(t),cwo=r(_3e," \u2014 "),U$=s(_3e,"A",{href:!0});var Yqr=n(U$);mwo=r(Yqr,"RobertaForTokenClassification"),Yqr.forEach(t),fwo=r(_3e," (RoBERTa model)"),_3e.forEach(t),gwo=i(q),Y2=s(q,"LI",{});var b3e=n(Y2);Zre=s(b3e,"STRONG",{});var Kqr=n(Zre);hwo=r(Kqr,"roformer"),Kqr.forEach(t),uwo=r(b3e," \u2014 "),J$=s(b3e,"A",{href:!0});var Zqr=n(J$);pwo=r(Zqr,"RoFormerForTokenClassification"),Zqr.forEach(t),_wo=r(b3e," (RoFormer model)"),b3e.forEach(t),bwo=i(q),K2=s(q,"LI",{});var v3e=n(K2);ete=s(v3e,"STRONG",{});var eGr=n(ete);vwo=r(eGr,"squeezebert"),eGr.forEach(t),Two=r(v3e," \u2014 "),Y$=s(v3e,"A",{href:!0});var oGr=n(Y$);Fwo=r(oGr,"SqueezeBertForTokenClassification"),oGr.forEach(t),Cwo=r(v3e," (SqueezeBERT model)"),v3e.forEach(t),Mwo=i(q),Z2=s(q,"LI",{});var T3e=n(Z2);ote=s(T3e,"STRONG",{});var rGr=n(ote);Ewo=r(rGr,"xlm"),rGr.forEach(t),ywo=r(T3e," \u2014 "),K$=s(T3e,"A",{href:!0});var tGr=n(K$);wwo=r(tGr,"XLMForTokenClassification"),tGr.forEach(t),Awo=r(T3e," (XLM model)"),T3e.forEach(t),Lwo=i(q),ev=s(q,"LI",{});var F3e=n(ev);rte=s(F3e,"STRONG",{});var aGr=n(rte);Bwo=r(aGr,"xlm-roberta"),aGr.forEach(t),xwo=r(F3e," \u2014 "),Z$=s(F3e,"A",{href:!0});var sGr=n(Z$);kwo=r(sGr,"XLMRobertaForTokenClassification"),sGr.forEach(t),Rwo=r(F3e," (XLM-RoBERTa model)"),F3e.forEach(t),Swo=i(q),ov=s(q,"LI",{});var C3e=n(ov);tte=s(C3e,"STRONG",{});var nGr=n(tte);Pwo=r(nGr,"xlm-roberta-xl"),nGr.forEach(t),$wo=r(C3e," \u2014 "),eI=s(C3e,"A",{href:!0});var lGr=n(eI);Iwo=r(lGr,"XLMRobertaXLForTokenClassification"),lGr.forEach(t),jwo=r(C3e," (XLM-RoBERTa-XL model)"),C3e.forEach(t),Nwo=i(q),rv=s(q,"LI",{});var M3e=n(rv);ate=s(M3e,"STRONG",{});var iGr=n(ate);Dwo=r(iGr,"xlnet"),iGr.forEach(t),qwo=r(M3e," \u2014 "),oI=s(M3e,"A",{href:!0});var dGr=n(oI);Gwo=r(dGr,"XLNetForTokenClassification"),dGr.forEach(t),Owo=r(M3e," (XLNet model)"),M3e.forEach(t),Xwo=i(q),tv=s(q,"LI",{});var E3e=n(tv);ste=s(E3e,"STRONG",{});var cGr=n(ste);zwo=r(cGr,"yoso"),cGr.forEach(t),Vwo=r(E3e," \u2014 "),rI=s(E3e,"A",{href:!0});var mGr=n(rI);Wwo=r(mGr,"YosoForTokenClassification"),mGr.forEach(t),Qwo=r(E3e," (YOSO model)"),E3e.forEach(t),q.forEach(t),Hwo=i(It),av=s(It,"P",{});var y3e=n(av);Uwo=r(y3e,"The model is set in evaluation mode by default using "),nte=s(y3e,"CODE",{});var fGr=n(nte);Jwo=r(fGr,"model.eval()"),fGr.forEach(t),Ywo=r(y3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lte=s(y3e,"CODE",{});var gGr=n(lte);Kwo=r(gGr,"model.train()"),gGr.forEach(t),y3e.forEach(t),Zwo=i(It),ite=s(It,"P",{});var hGr=n(ite);eAo=r(hGr,"Examples:"),hGr.forEach(t),oAo=i(It),f(fy.$$.fragment,It),It.forEach(t),Hn.forEach(t),XLe=i(d),ad=s(d,"H2",{class:!0});var W8e=n(ad);sv=s(W8e,"A",{id:!0,class:!0,href:!0});var uGr=n(sv);dte=s(uGr,"SPAN",{});var pGr=n(dte);f(gy.$$.fragment,pGr),pGr.forEach(t),uGr.forEach(t),rAo=i(W8e),cte=s(W8e,"SPAN",{});var _Gr=n(cte);tAo=r(_Gr,"AutoModelForQuestionAnswering"),_Gr.forEach(t),W8e.forEach(t),zLe=i(d),Yo=s(d,"DIV",{class:!0});var Jn=n(Yo);f(hy.$$.fragment,Jn),aAo=i(Jn),sd=s(Jn,"P",{});var aX=n(sd);sAo=r(aX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),mte=s(aX,"CODE",{});var bGr=n(mte);nAo=r(bGr,"from_pretrained()"),bGr.forEach(t),lAo=r(aX,"class method or the "),fte=s(aX,"CODE",{});var vGr=n(fte);iAo=r(vGr,"from_config()"),vGr.forEach(t),dAo=r(aX,`class
method.`),aX.forEach(t),cAo=i(Jn),uy=s(Jn,"P",{});var Q8e=n(uy);mAo=r(Q8e,"This class cannot be instantiated directly using "),gte=s(Q8e,"CODE",{});var TGr=n(gte);fAo=r(TGr,"__init__()"),TGr.forEach(t),gAo=r(Q8e," (throws an error)."),Q8e.forEach(t),hAo=i(Jn),Xr=s(Jn,"DIV",{class:!0});var Yn=n(Xr);f(py.$$.fragment,Yn),uAo=i(Yn),hte=s(Yn,"P",{});var FGr=n(hte);pAo=r(FGr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),FGr.forEach(t),_Ao=i(Yn),nd=s(Yn,"P",{});var sX=n(nd);bAo=r(sX,`Note:
Loading a model from its configuration file does `),ute=s(sX,"STRONG",{});var CGr=n(ute);vAo=r(CGr,"not"),CGr.forEach(t),TAo=r(sX,` load the model weights. It only affects the
model\u2019s configuration. Use `),pte=s(sX,"CODE",{});var MGr=n(pte);FAo=r(MGr,"from_pretrained()"),MGr.forEach(t),CAo=r(sX,"to load the model weights."),sX.forEach(t),MAo=i(Yn),_te=s(Yn,"P",{});var EGr=n(_te);EAo=r(EGr,"Examples:"),EGr.forEach(t),yAo=i(Yn),f(_y.$$.fragment,Yn),Yn.forEach(t),wAo=i(Jn),De=s(Jn,"DIV",{class:!0});var jt=n(De);f(by.$$.fragment,jt),AAo=i(jt),bte=s(jt,"P",{});var yGr=n(bte);LAo=r(yGr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),yGr.forEach(t),BAo=i(jt),za=s(jt,"P",{});var OE=n(za);xAo=r(OE,"The model class to instantiate is selected based on the "),vte=s(OE,"CODE",{});var wGr=n(vte);kAo=r(wGr,"model_type"),wGr.forEach(t),RAo=r(OE,` property of the config object (either
passed as an argument or loaded from `),Tte=s(OE,"CODE",{});var AGr=n(Tte);SAo=r(AGr,"pretrained_model_name_or_path"),AGr.forEach(t),PAo=r(OE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fte=s(OE,"CODE",{});var LGr=n(Fte);$Ao=r(LGr,"pretrained_model_name_or_path"),LGr.forEach(t),IAo=r(OE,":"),OE.forEach(t),jAo=i(jt),R=s(jt,"UL",{});var P=n(R);nv=s(P,"LI",{});var w3e=n(nv);Cte=s(w3e,"STRONG",{});var BGr=n(Cte);NAo=r(BGr,"albert"),BGr.forEach(t),DAo=r(w3e," \u2014 "),tI=s(w3e,"A",{href:!0});var xGr=n(tI);qAo=r(xGr,"AlbertForQuestionAnswering"),xGr.forEach(t),GAo=r(w3e," (ALBERT model)"),w3e.forEach(t),OAo=i(P),lv=s(P,"LI",{});var A3e=n(lv);Mte=s(A3e,"STRONG",{});var kGr=n(Mte);XAo=r(kGr,"bart"),kGr.forEach(t),zAo=r(A3e," \u2014 "),aI=s(A3e,"A",{href:!0});var RGr=n(aI);VAo=r(RGr,"BartForQuestionAnswering"),RGr.forEach(t),WAo=r(A3e," (BART model)"),A3e.forEach(t),QAo=i(P),iv=s(P,"LI",{});var L3e=n(iv);Ete=s(L3e,"STRONG",{});var SGr=n(Ete);HAo=r(SGr,"bert"),SGr.forEach(t),UAo=r(L3e," \u2014 "),sI=s(L3e,"A",{href:!0});var PGr=n(sI);JAo=r(PGr,"BertForQuestionAnswering"),PGr.forEach(t),YAo=r(L3e," (BERT model)"),L3e.forEach(t),KAo=i(P),dv=s(P,"LI",{});var B3e=n(dv);yte=s(B3e,"STRONG",{});var $Gr=n(yte);ZAo=r($Gr,"big_bird"),$Gr.forEach(t),e0o=r(B3e," \u2014 "),nI=s(B3e,"A",{href:!0});var IGr=n(nI);o0o=r(IGr,"BigBirdForQuestionAnswering"),IGr.forEach(t),r0o=r(B3e," (BigBird model)"),B3e.forEach(t),t0o=i(P),cv=s(P,"LI",{});var x3e=n(cv);wte=s(x3e,"STRONG",{});var jGr=n(wte);a0o=r(jGr,"bigbird_pegasus"),jGr.forEach(t),s0o=r(x3e," \u2014 "),lI=s(x3e,"A",{href:!0});var NGr=n(lI);n0o=r(NGr,"BigBirdPegasusForQuestionAnswering"),NGr.forEach(t),l0o=r(x3e," (BigBirdPegasus model)"),x3e.forEach(t),i0o=i(P),mv=s(P,"LI",{});var k3e=n(mv);Ate=s(k3e,"STRONG",{});var DGr=n(Ate);d0o=r(DGr,"camembert"),DGr.forEach(t),c0o=r(k3e," \u2014 "),iI=s(k3e,"A",{href:!0});var qGr=n(iI);m0o=r(qGr,"CamembertForQuestionAnswering"),qGr.forEach(t),f0o=r(k3e," (CamemBERT model)"),k3e.forEach(t),g0o=i(P),fv=s(P,"LI",{});var R3e=n(fv);Lte=s(R3e,"STRONG",{});var GGr=n(Lte);h0o=r(GGr,"canine"),GGr.forEach(t),u0o=r(R3e," \u2014 "),dI=s(R3e,"A",{href:!0});var OGr=n(dI);p0o=r(OGr,"CanineForQuestionAnswering"),OGr.forEach(t),_0o=r(R3e," (Canine model)"),R3e.forEach(t),b0o=i(P),gv=s(P,"LI",{});var S3e=n(gv);Bte=s(S3e,"STRONG",{});var XGr=n(Bte);v0o=r(XGr,"convbert"),XGr.forEach(t),T0o=r(S3e," \u2014 "),cI=s(S3e,"A",{href:!0});var zGr=n(cI);F0o=r(zGr,"ConvBertForQuestionAnswering"),zGr.forEach(t),C0o=r(S3e," (ConvBERT model)"),S3e.forEach(t),M0o=i(P),hv=s(P,"LI",{});var P3e=n(hv);xte=s(P3e,"STRONG",{});var VGr=n(xte);E0o=r(VGr,"deberta"),VGr.forEach(t),y0o=r(P3e," \u2014 "),mI=s(P3e,"A",{href:!0});var WGr=n(mI);w0o=r(WGr,"DebertaForQuestionAnswering"),WGr.forEach(t),A0o=r(P3e," (DeBERTa model)"),P3e.forEach(t),L0o=i(P),uv=s(P,"LI",{});var $3e=n(uv);kte=s($3e,"STRONG",{});var QGr=n(kte);B0o=r(QGr,"deberta-v2"),QGr.forEach(t),x0o=r($3e," \u2014 "),fI=s($3e,"A",{href:!0});var HGr=n(fI);k0o=r(HGr,"DebertaV2ForQuestionAnswering"),HGr.forEach(t),R0o=r($3e," (DeBERTa-v2 model)"),$3e.forEach(t),S0o=i(P),pv=s(P,"LI",{});var I3e=n(pv);Rte=s(I3e,"STRONG",{});var UGr=n(Rte);P0o=r(UGr,"distilbert"),UGr.forEach(t),$0o=r(I3e," \u2014 "),gI=s(I3e,"A",{href:!0});var JGr=n(gI);I0o=r(JGr,"DistilBertForQuestionAnswering"),JGr.forEach(t),j0o=r(I3e," (DistilBERT model)"),I3e.forEach(t),N0o=i(P),_v=s(P,"LI",{});var j3e=n(_v);Ste=s(j3e,"STRONG",{});var YGr=n(Ste);D0o=r(YGr,"electra"),YGr.forEach(t),q0o=r(j3e," \u2014 "),hI=s(j3e,"A",{href:!0});var KGr=n(hI);G0o=r(KGr,"ElectraForQuestionAnswering"),KGr.forEach(t),O0o=r(j3e," (ELECTRA model)"),j3e.forEach(t),X0o=i(P),bv=s(P,"LI",{});var N3e=n(bv);Pte=s(N3e,"STRONG",{});var ZGr=n(Pte);z0o=r(ZGr,"flaubert"),ZGr.forEach(t),V0o=r(N3e," \u2014 "),uI=s(N3e,"A",{href:!0});var eOr=n(uI);W0o=r(eOr,"FlaubertForQuestionAnsweringSimple"),eOr.forEach(t),Q0o=r(N3e," (FlauBERT model)"),N3e.forEach(t),H0o=i(P),vv=s(P,"LI",{});var D3e=n(vv);$te=s(D3e,"STRONG",{});var oOr=n($te);U0o=r(oOr,"fnet"),oOr.forEach(t),J0o=r(D3e," \u2014 "),pI=s(D3e,"A",{href:!0});var rOr=n(pI);Y0o=r(rOr,"FNetForQuestionAnswering"),rOr.forEach(t),K0o=r(D3e," (FNet model)"),D3e.forEach(t),Z0o=i(P),Tv=s(P,"LI",{});var q3e=n(Tv);Ite=s(q3e,"STRONG",{});var tOr=n(Ite);e6o=r(tOr,"funnel"),tOr.forEach(t),o6o=r(q3e," \u2014 "),_I=s(q3e,"A",{href:!0});var aOr=n(_I);r6o=r(aOr,"FunnelForQuestionAnswering"),aOr.forEach(t),t6o=r(q3e," (Funnel Transformer model)"),q3e.forEach(t),a6o=i(P),Fv=s(P,"LI",{});var G3e=n(Fv);jte=s(G3e,"STRONG",{});var sOr=n(jte);s6o=r(sOr,"gptj"),sOr.forEach(t),n6o=r(G3e," \u2014 "),bI=s(G3e,"A",{href:!0});var nOr=n(bI);l6o=r(nOr,"GPTJForQuestionAnswering"),nOr.forEach(t),i6o=r(G3e," (GPT-J model)"),G3e.forEach(t),d6o=i(P),Cv=s(P,"LI",{});var O3e=n(Cv);Nte=s(O3e,"STRONG",{});var lOr=n(Nte);c6o=r(lOr,"ibert"),lOr.forEach(t),m6o=r(O3e," \u2014 "),vI=s(O3e,"A",{href:!0});var iOr=n(vI);f6o=r(iOr,"IBertForQuestionAnswering"),iOr.forEach(t),g6o=r(O3e," (I-BERT model)"),O3e.forEach(t),h6o=i(P),Mv=s(P,"LI",{});var X3e=n(Mv);Dte=s(X3e,"STRONG",{});var dOr=n(Dte);u6o=r(dOr,"layoutlmv2"),dOr.forEach(t),p6o=r(X3e," \u2014 "),TI=s(X3e,"A",{href:!0});var cOr=n(TI);_6o=r(cOr,"LayoutLMv2ForQuestionAnswering"),cOr.forEach(t),b6o=r(X3e," (LayoutLMv2 model)"),X3e.forEach(t),v6o=i(P),Ev=s(P,"LI",{});var z3e=n(Ev);qte=s(z3e,"STRONG",{});var mOr=n(qte);T6o=r(mOr,"led"),mOr.forEach(t),F6o=r(z3e," \u2014 "),FI=s(z3e,"A",{href:!0});var fOr=n(FI);C6o=r(fOr,"LEDForQuestionAnswering"),fOr.forEach(t),M6o=r(z3e," (LED model)"),z3e.forEach(t),E6o=i(P),yv=s(P,"LI",{});var V3e=n(yv);Gte=s(V3e,"STRONG",{});var gOr=n(Gte);y6o=r(gOr,"longformer"),gOr.forEach(t),w6o=r(V3e," \u2014 "),CI=s(V3e,"A",{href:!0});var hOr=n(CI);A6o=r(hOr,"LongformerForQuestionAnswering"),hOr.forEach(t),L6o=r(V3e," (Longformer model)"),V3e.forEach(t),B6o=i(P),wv=s(P,"LI",{});var W3e=n(wv);Ote=s(W3e,"STRONG",{});var uOr=n(Ote);x6o=r(uOr,"lxmert"),uOr.forEach(t),k6o=r(W3e," \u2014 "),MI=s(W3e,"A",{href:!0});var pOr=n(MI);R6o=r(pOr,"LxmertForQuestionAnswering"),pOr.forEach(t),S6o=r(W3e," (LXMERT model)"),W3e.forEach(t),P6o=i(P),Av=s(P,"LI",{});var Q3e=n(Av);Xte=s(Q3e,"STRONG",{});var _Or=n(Xte);$6o=r(_Or,"mbart"),_Or.forEach(t),I6o=r(Q3e," \u2014 "),EI=s(Q3e,"A",{href:!0});var bOr=n(EI);j6o=r(bOr,"MBartForQuestionAnswering"),bOr.forEach(t),N6o=r(Q3e," (mBART model)"),Q3e.forEach(t),D6o=i(P),Lv=s(P,"LI",{});var H3e=n(Lv);zte=s(H3e,"STRONG",{});var vOr=n(zte);q6o=r(vOr,"megatron-bert"),vOr.forEach(t),G6o=r(H3e," \u2014 "),yI=s(H3e,"A",{href:!0});var TOr=n(yI);O6o=r(TOr,"MegatronBertForQuestionAnswering"),TOr.forEach(t),X6o=r(H3e," (MegatronBert model)"),H3e.forEach(t),z6o=i(P),Bv=s(P,"LI",{});var U3e=n(Bv);Vte=s(U3e,"STRONG",{});var FOr=n(Vte);V6o=r(FOr,"mobilebert"),FOr.forEach(t),W6o=r(U3e," \u2014 "),wI=s(U3e,"A",{href:!0});var COr=n(wI);Q6o=r(COr,"MobileBertForQuestionAnswering"),COr.forEach(t),H6o=r(U3e," (MobileBERT model)"),U3e.forEach(t),U6o=i(P),xv=s(P,"LI",{});var J3e=n(xv);Wte=s(J3e,"STRONG",{});var MOr=n(Wte);J6o=r(MOr,"mpnet"),MOr.forEach(t),Y6o=r(J3e," \u2014 "),AI=s(J3e,"A",{href:!0});var EOr=n(AI);K6o=r(EOr,"MPNetForQuestionAnswering"),EOr.forEach(t),Z6o=r(J3e," (MPNet model)"),J3e.forEach(t),eLo=i(P),kv=s(P,"LI",{});var Y3e=n(kv);Qte=s(Y3e,"STRONG",{});var yOr=n(Qte);oLo=r(yOr,"nystromformer"),yOr.forEach(t),rLo=r(Y3e," \u2014 "),LI=s(Y3e,"A",{href:!0});var wOr=n(LI);tLo=r(wOr,"NystromformerForQuestionAnswering"),wOr.forEach(t),aLo=r(Y3e," (Nystromformer model)"),Y3e.forEach(t),sLo=i(P),Rv=s(P,"LI",{});var K3e=n(Rv);Hte=s(K3e,"STRONG",{});var AOr=n(Hte);nLo=r(AOr,"qdqbert"),AOr.forEach(t),lLo=r(K3e," \u2014 "),Ute=s(K3e,"CODE",{});var LOr=n(Ute);iLo=r(LOr,"QDQBertForQuestionAnswering"),LOr.forEach(t),dLo=r(K3e,"(QDQBert model)"),K3e.forEach(t),cLo=i(P),Sv=s(P,"LI",{});var Z3e=n(Sv);Jte=s(Z3e,"STRONG",{});var BOr=n(Jte);mLo=r(BOr,"reformer"),BOr.forEach(t),fLo=r(Z3e," \u2014 "),BI=s(Z3e,"A",{href:!0});var xOr=n(BI);gLo=r(xOr,"ReformerForQuestionAnswering"),xOr.forEach(t),hLo=r(Z3e," (Reformer model)"),Z3e.forEach(t),uLo=i(P),Pv=s(P,"LI",{});var e5e=n(Pv);Yte=s(e5e,"STRONG",{});var kOr=n(Yte);pLo=r(kOr,"rembert"),kOr.forEach(t),_Lo=r(e5e," \u2014 "),xI=s(e5e,"A",{href:!0});var ROr=n(xI);bLo=r(ROr,"RemBertForQuestionAnswering"),ROr.forEach(t),vLo=r(e5e," (RemBERT model)"),e5e.forEach(t),TLo=i(P),$v=s(P,"LI",{});var o5e=n($v);Kte=s(o5e,"STRONG",{});var SOr=n(Kte);FLo=r(SOr,"roberta"),SOr.forEach(t),CLo=r(o5e," \u2014 "),kI=s(o5e,"A",{href:!0});var POr=n(kI);MLo=r(POr,"RobertaForQuestionAnswering"),POr.forEach(t),ELo=r(o5e," (RoBERTa model)"),o5e.forEach(t),yLo=i(P),Iv=s(P,"LI",{});var r5e=n(Iv);Zte=s(r5e,"STRONG",{});var $Or=n(Zte);wLo=r($Or,"roformer"),$Or.forEach(t),ALo=r(r5e," \u2014 "),RI=s(r5e,"A",{href:!0});var IOr=n(RI);LLo=r(IOr,"RoFormerForQuestionAnswering"),IOr.forEach(t),BLo=r(r5e," (RoFormer model)"),r5e.forEach(t),xLo=i(P),jv=s(P,"LI",{});var t5e=n(jv);eae=s(t5e,"STRONG",{});var jOr=n(eae);kLo=r(jOr,"splinter"),jOr.forEach(t),RLo=r(t5e," \u2014 "),SI=s(t5e,"A",{href:!0});var NOr=n(SI);SLo=r(NOr,"SplinterForQuestionAnswering"),NOr.forEach(t),PLo=r(t5e," (Splinter model)"),t5e.forEach(t),$Lo=i(P),Nv=s(P,"LI",{});var a5e=n(Nv);oae=s(a5e,"STRONG",{});var DOr=n(oae);ILo=r(DOr,"squeezebert"),DOr.forEach(t),jLo=r(a5e," \u2014 "),PI=s(a5e,"A",{href:!0});var qOr=n(PI);NLo=r(qOr,"SqueezeBertForQuestionAnswering"),qOr.forEach(t),DLo=r(a5e," (SqueezeBERT model)"),a5e.forEach(t),qLo=i(P),Dv=s(P,"LI",{});var s5e=n(Dv);rae=s(s5e,"STRONG",{});var GOr=n(rae);GLo=r(GOr,"xlm"),GOr.forEach(t),OLo=r(s5e," \u2014 "),$I=s(s5e,"A",{href:!0});var OOr=n($I);XLo=r(OOr,"XLMForQuestionAnsweringSimple"),OOr.forEach(t),zLo=r(s5e," (XLM model)"),s5e.forEach(t),VLo=i(P),qv=s(P,"LI",{});var n5e=n(qv);tae=s(n5e,"STRONG",{});var XOr=n(tae);WLo=r(XOr,"xlm-roberta"),XOr.forEach(t),QLo=r(n5e," \u2014 "),II=s(n5e,"A",{href:!0});var zOr=n(II);HLo=r(zOr,"XLMRobertaForQuestionAnswering"),zOr.forEach(t),ULo=r(n5e," (XLM-RoBERTa model)"),n5e.forEach(t),JLo=i(P),Gv=s(P,"LI",{});var l5e=n(Gv);aae=s(l5e,"STRONG",{});var VOr=n(aae);YLo=r(VOr,"xlm-roberta-xl"),VOr.forEach(t),KLo=r(l5e," \u2014 "),jI=s(l5e,"A",{href:!0});var WOr=n(jI);ZLo=r(WOr,"XLMRobertaXLForQuestionAnswering"),WOr.forEach(t),e7o=r(l5e," (XLM-RoBERTa-XL model)"),l5e.forEach(t),o7o=i(P),Ov=s(P,"LI",{});var i5e=n(Ov);sae=s(i5e,"STRONG",{});var QOr=n(sae);r7o=r(QOr,"xlnet"),QOr.forEach(t),t7o=r(i5e," \u2014 "),NI=s(i5e,"A",{href:!0});var HOr=n(NI);a7o=r(HOr,"XLNetForQuestionAnsweringSimple"),HOr.forEach(t),s7o=r(i5e," (XLNet model)"),i5e.forEach(t),n7o=i(P),Xv=s(P,"LI",{});var d5e=n(Xv);nae=s(d5e,"STRONG",{});var UOr=n(nae);l7o=r(UOr,"yoso"),UOr.forEach(t),i7o=r(d5e," \u2014 "),DI=s(d5e,"A",{href:!0});var JOr=n(DI);d7o=r(JOr,"YosoForQuestionAnswering"),JOr.forEach(t),c7o=r(d5e," (YOSO model)"),d5e.forEach(t),P.forEach(t),m7o=i(jt),zv=s(jt,"P",{});var c5e=n(zv);f7o=r(c5e,"The model is set in evaluation mode by default using "),lae=s(c5e,"CODE",{});var YOr=n(lae);g7o=r(YOr,"model.eval()"),YOr.forEach(t),h7o=r(c5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iae=s(c5e,"CODE",{});var KOr=n(iae);u7o=r(KOr,"model.train()"),KOr.forEach(t),c5e.forEach(t),p7o=i(jt),dae=s(jt,"P",{});var ZOr=n(dae);_7o=r(ZOr,"Examples:"),ZOr.forEach(t),b7o=i(jt),f(vy.$$.fragment,jt),jt.forEach(t),Jn.forEach(t),VLe=i(d),ld=s(d,"H2",{class:!0});var H8e=n(ld);Vv=s(H8e,"A",{id:!0,class:!0,href:!0});var eXr=n(Vv);cae=s(eXr,"SPAN",{});var oXr=n(cae);f(Ty.$$.fragment,oXr),oXr.forEach(t),eXr.forEach(t),v7o=i(H8e),mae=s(H8e,"SPAN",{});var rXr=n(mae);T7o=r(rXr,"AutoModelForTableQuestionAnswering"),rXr.forEach(t),H8e.forEach(t),WLe=i(d),Ko=s(d,"DIV",{class:!0});var Kn=n(Ko);f(Fy.$$.fragment,Kn),F7o=i(Kn),id=s(Kn,"P",{});var nX=n(id);C7o=r(nX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),fae=s(nX,"CODE",{});var tXr=n(fae);M7o=r(tXr,"from_pretrained()"),tXr.forEach(t),E7o=r(nX,"class method or the "),gae=s(nX,"CODE",{});var aXr=n(gae);y7o=r(aXr,"from_config()"),aXr.forEach(t),w7o=r(nX,`class
method.`),nX.forEach(t),A7o=i(Kn),Cy=s(Kn,"P",{});var U8e=n(Cy);L7o=r(U8e,"This class cannot be instantiated directly using "),hae=s(U8e,"CODE",{});var sXr=n(hae);B7o=r(sXr,"__init__()"),sXr.forEach(t),x7o=r(U8e," (throws an error)."),U8e.forEach(t),k7o=i(Kn),zr=s(Kn,"DIV",{class:!0});var Zn=n(zr);f(My.$$.fragment,Zn),R7o=i(Zn),uae=s(Zn,"P",{});var nXr=n(uae);S7o=r(nXr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),nXr.forEach(t),P7o=i(Zn),dd=s(Zn,"P",{});var lX=n(dd);$7o=r(lX,`Note:
Loading a model from its configuration file does `),pae=s(lX,"STRONG",{});var lXr=n(pae);I7o=r(lXr,"not"),lXr.forEach(t),j7o=r(lX,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ae=s(lX,"CODE",{});var iXr=n(_ae);N7o=r(iXr,"from_pretrained()"),iXr.forEach(t),D7o=r(lX,"to load the model weights."),lX.forEach(t),q7o=i(Zn),bae=s(Zn,"P",{});var dXr=n(bae);G7o=r(dXr,"Examples:"),dXr.forEach(t),O7o=i(Zn),f(Ey.$$.fragment,Zn),Zn.forEach(t),X7o=i(Kn),qe=s(Kn,"DIV",{class:!0});var Nt=n(qe);f(yy.$$.fragment,Nt),z7o=i(Nt),vae=s(Nt,"P",{});var cXr=n(vae);V7o=r(cXr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),cXr.forEach(t),W7o=i(Nt),Va=s(Nt,"P",{});var XE=n(Va);Q7o=r(XE,"The model class to instantiate is selected based on the "),Tae=s(XE,"CODE",{});var mXr=n(Tae);H7o=r(mXr,"model_type"),mXr.forEach(t),U7o=r(XE,` property of the config object (either
passed as an argument or loaded from `),Fae=s(XE,"CODE",{});var fXr=n(Fae);J7o=r(fXr,"pretrained_model_name_or_path"),fXr.forEach(t),Y7o=r(XE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cae=s(XE,"CODE",{});var gXr=n(Cae);K7o=r(gXr,"pretrained_model_name_or_path"),gXr.forEach(t),Z7o=r(XE,":"),XE.forEach(t),e8o=i(Nt),Mae=s(Nt,"UL",{});var hXr=n(Mae);Wv=s(hXr,"LI",{});var m5e=n(Wv);Eae=s(m5e,"STRONG",{});var uXr=n(Eae);o8o=r(uXr,"tapas"),uXr.forEach(t),r8o=r(m5e," \u2014 "),qI=s(m5e,"A",{href:!0});var pXr=n(qI);t8o=r(pXr,"TapasForQuestionAnswering"),pXr.forEach(t),a8o=r(m5e," (TAPAS model)"),m5e.forEach(t),hXr.forEach(t),s8o=i(Nt),Qv=s(Nt,"P",{});var f5e=n(Qv);n8o=r(f5e,"The model is set in evaluation mode by default using "),yae=s(f5e,"CODE",{});var _Xr=n(yae);l8o=r(_Xr,"model.eval()"),_Xr.forEach(t),i8o=r(f5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wae=s(f5e,"CODE",{});var bXr=n(wae);d8o=r(bXr,"model.train()"),bXr.forEach(t),f5e.forEach(t),c8o=i(Nt),Aae=s(Nt,"P",{});var vXr=n(Aae);m8o=r(vXr,"Examples:"),vXr.forEach(t),f8o=i(Nt),f(wy.$$.fragment,Nt),Nt.forEach(t),Kn.forEach(t),QLe=i(d),cd=s(d,"H2",{class:!0});var J8e=n(cd);Hv=s(J8e,"A",{id:!0,class:!0,href:!0});var TXr=n(Hv);Lae=s(TXr,"SPAN",{});var FXr=n(Lae);f(Ay.$$.fragment,FXr),FXr.forEach(t),TXr.forEach(t),g8o=i(J8e),Bae=s(J8e,"SPAN",{});var CXr=n(Bae);h8o=r(CXr,"AutoModelForImageClassification"),CXr.forEach(t),J8e.forEach(t),HLe=i(d),Zo=s(d,"DIV",{class:!0});var el=n(Zo);f(Ly.$$.fragment,el),u8o=i(el),md=s(el,"P",{});var iX=n(md);p8o=r(iX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),xae=s(iX,"CODE",{});var MXr=n(xae);_8o=r(MXr,"from_pretrained()"),MXr.forEach(t),b8o=r(iX,"class method or the "),kae=s(iX,"CODE",{});var EXr=n(kae);v8o=r(EXr,"from_config()"),EXr.forEach(t),T8o=r(iX,`class
method.`),iX.forEach(t),F8o=i(el),By=s(el,"P",{});var Y8e=n(By);C8o=r(Y8e,"This class cannot be instantiated directly using "),Rae=s(Y8e,"CODE",{});var yXr=n(Rae);M8o=r(yXr,"__init__()"),yXr.forEach(t),E8o=r(Y8e," (throws an error)."),Y8e.forEach(t),y8o=i(el),Vr=s(el,"DIV",{class:!0});var ol=n(Vr);f(xy.$$.fragment,ol),w8o=i(ol),Sae=s(ol,"P",{});var wXr=n(Sae);A8o=r(wXr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),wXr.forEach(t),L8o=i(ol),fd=s(ol,"P",{});var dX=n(fd);B8o=r(dX,`Note:
Loading a model from its configuration file does `),Pae=s(dX,"STRONG",{});var AXr=n(Pae);x8o=r(AXr,"not"),AXr.forEach(t),k8o=r(dX,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ae=s(dX,"CODE",{});var LXr=n($ae);R8o=r(LXr,"from_pretrained()"),LXr.forEach(t),S8o=r(dX,"to load the model weights."),dX.forEach(t),P8o=i(ol),Iae=s(ol,"P",{});var BXr=n(Iae);$8o=r(BXr,"Examples:"),BXr.forEach(t),I8o=i(ol),f(ky.$$.fragment,ol),ol.forEach(t),j8o=i(el),Ge=s(el,"DIV",{class:!0});var Dt=n(Ge);f(Ry.$$.fragment,Dt),N8o=i(Dt),jae=s(Dt,"P",{});var xXr=n(jae);D8o=r(xXr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),xXr.forEach(t),q8o=i(Dt),Wa=s(Dt,"P",{});var zE=n(Wa);G8o=r(zE,"The model class to instantiate is selected based on the "),Nae=s(zE,"CODE",{});var kXr=n(Nae);O8o=r(kXr,"model_type"),kXr.forEach(t),X8o=r(zE,` property of the config object (either
passed as an argument or loaded from `),Dae=s(zE,"CODE",{});var RXr=n(Dae);z8o=r(RXr,"pretrained_model_name_or_path"),RXr.forEach(t),V8o=r(zE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qae=s(zE,"CODE",{});var SXr=n(qae);W8o=r(SXr,"pretrained_model_name_or_path"),SXr.forEach(t),Q8o=r(zE,":"),zE.forEach(t),H8o=i(Dt),we=s(Dt,"UL",{});var jo=n(we);Uv=s(jo,"LI",{});var g5e=n(Uv);Gae=s(g5e,"STRONG",{});var PXr=n(Gae);U8o=r(PXr,"beit"),PXr.forEach(t),J8o=r(g5e," \u2014 "),GI=s(g5e,"A",{href:!0});var $Xr=n(GI);Y8o=r($Xr,"BeitForImageClassification"),$Xr.forEach(t),K8o=r(g5e," (BEiT model)"),g5e.forEach(t),Z8o=i(jo),Jv=s(jo,"LI",{});var h5e=n(Jv);Oae=s(h5e,"STRONG",{});var IXr=n(Oae);e9o=r(IXr,"convnext"),IXr.forEach(t),o9o=r(h5e," \u2014 "),OI=s(h5e,"A",{href:!0});var jXr=n(OI);r9o=r(jXr,"ConvNextForImageClassification"),jXr.forEach(t),t9o=r(h5e," (ConvNext model)"),h5e.forEach(t),a9o=i(jo),An=s(jo,"LI",{});var ZL=n(An);Xae=s(ZL,"STRONG",{});var NXr=n(Xae);s9o=r(NXr,"deit"),NXr.forEach(t),n9o=r(ZL," \u2014 "),XI=s(ZL,"A",{href:!0});var DXr=n(XI);l9o=r(DXr,"DeiTForImageClassification"),DXr.forEach(t),i9o=r(ZL," or "),zI=s(ZL,"A",{href:!0});var qXr=n(zI);d9o=r(qXr,"DeiTForImageClassificationWithTeacher"),qXr.forEach(t),c9o=r(ZL," (DeiT model)"),ZL.forEach(t),m9o=i(jo),Yv=s(jo,"LI",{});var u5e=n(Yv);zae=s(u5e,"STRONG",{});var GXr=n(zae);f9o=r(GXr,"imagegpt"),GXr.forEach(t),g9o=r(u5e," \u2014 "),VI=s(u5e,"A",{href:!0});var OXr=n(VI);h9o=r(OXr,"ImageGPTForImageClassification"),OXr.forEach(t),u9o=r(u5e," (ImageGPT model)"),u5e.forEach(t),p9o=i(jo),ta=s(jo,"LI",{});var dm=n(ta);Vae=s(dm,"STRONG",{});var XXr=n(Vae);_9o=r(XXr,"perceiver"),XXr.forEach(t),b9o=r(dm," \u2014 "),WI=s(dm,"A",{href:!0});var zXr=n(WI);v9o=r(zXr,"PerceiverForImageClassificationLearned"),zXr.forEach(t),T9o=r(dm," or "),QI=s(dm,"A",{href:!0});var VXr=n(QI);F9o=r(VXr,"PerceiverForImageClassificationFourier"),VXr.forEach(t),C9o=r(dm," or "),HI=s(dm,"A",{href:!0});var WXr=n(HI);M9o=r(WXr,"PerceiverForImageClassificationConvProcessing"),WXr.forEach(t),E9o=r(dm," (Perceiver model)"),dm.forEach(t),y9o=i(jo),Kv=s(jo,"LI",{});var p5e=n(Kv);Wae=s(p5e,"STRONG",{});var QXr=n(Wae);w9o=r(QXr,"segformer"),QXr.forEach(t),A9o=r(p5e," \u2014 "),UI=s(p5e,"A",{href:!0});var HXr=n(UI);L9o=r(HXr,"SegformerForImageClassification"),HXr.forEach(t),B9o=r(p5e," (SegFormer model)"),p5e.forEach(t),x9o=i(jo),Zv=s(jo,"LI",{});var _5e=n(Zv);Qae=s(_5e,"STRONG",{});var UXr=n(Qae);k9o=r(UXr,"swin"),UXr.forEach(t),R9o=r(_5e," \u2014 "),JI=s(_5e,"A",{href:!0});var JXr=n(JI);S9o=r(JXr,"SwinForImageClassification"),JXr.forEach(t),P9o=r(_5e," (Swin model)"),_5e.forEach(t),$9o=i(jo),eT=s(jo,"LI",{});var b5e=n(eT);Hae=s(b5e,"STRONG",{});var YXr=n(Hae);I9o=r(YXr,"vit"),YXr.forEach(t),j9o=r(b5e," \u2014 "),YI=s(b5e,"A",{href:!0});var KXr=n(YI);N9o=r(KXr,"ViTForImageClassification"),KXr.forEach(t),D9o=r(b5e," (ViT model)"),b5e.forEach(t),jo.forEach(t),q9o=i(Dt),oT=s(Dt,"P",{});var v5e=n(oT);G9o=r(v5e,"The model is set in evaluation mode by default using "),Uae=s(v5e,"CODE",{});var ZXr=n(Uae);O9o=r(ZXr,"model.eval()"),ZXr.forEach(t),X9o=r(v5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jae=s(v5e,"CODE",{});var ezr=n(Jae);z9o=r(ezr,"model.train()"),ezr.forEach(t),v5e.forEach(t),V9o=i(Dt),Yae=s(Dt,"P",{});var ozr=n(Yae);W9o=r(ozr,"Examples:"),ozr.forEach(t),Q9o=i(Dt),f(Sy.$$.fragment,Dt),Dt.forEach(t),el.forEach(t),ULe=i(d),gd=s(d,"H2",{class:!0});var K8e=n(gd);rT=s(K8e,"A",{id:!0,class:!0,href:!0});var rzr=n(rT);Kae=s(rzr,"SPAN",{});var tzr=n(Kae);f(Py.$$.fragment,tzr),tzr.forEach(t),rzr.forEach(t),H9o=i(K8e),Zae=s(K8e,"SPAN",{});var azr=n(Zae);U9o=r(azr,"AutoModelForVision2Seq"),azr.forEach(t),K8e.forEach(t),JLe=i(d),er=s(d,"DIV",{class:!0});var rl=n(er);f($y.$$.fragment,rl),J9o=i(rl),hd=s(rl,"P",{});var cX=n(hd);Y9o=r(cX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ese=s(cX,"CODE",{});var szr=n(ese);K9o=r(szr,"from_pretrained()"),szr.forEach(t),Z9o=r(cX,"class method or the "),ose=s(cX,"CODE",{});var nzr=n(ose);eBo=r(nzr,"from_config()"),nzr.forEach(t),oBo=r(cX,`class
method.`),cX.forEach(t),rBo=i(rl),Iy=s(rl,"P",{});var Z8e=n(Iy);tBo=r(Z8e,"This class cannot be instantiated directly using "),rse=s(Z8e,"CODE",{});var lzr=n(rse);aBo=r(lzr,"__init__()"),lzr.forEach(t),sBo=r(Z8e," (throws an error)."),Z8e.forEach(t),nBo=i(rl),Wr=s(rl,"DIV",{class:!0});var tl=n(Wr);f(jy.$$.fragment,tl),lBo=i(tl),tse=s(tl,"P",{});var izr=n(tse);iBo=r(izr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),izr.forEach(t),dBo=i(tl),ud=s(tl,"P",{});var mX=n(ud);cBo=r(mX,`Note:
Loading a model from its configuration file does `),ase=s(mX,"STRONG",{});var dzr=n(ase);mBo=r(dzr,"not"),dzr.forEach(t),fBo=r(mX,` load the model weights. It only affects the
model\u2019s configuration. Use `),sse=s(mX,"CODE",{});var czr=n(sse);gBo=r(czr,"from_pretrained()"),czr.forEach(t),hBo=r(mX,"to load the model weights."),mX.forEach(t),uBo=i(tl),nse=s(tl,"P",{});var mzr=n(nse);pBo=r(mzr,"Examples:"),mzr.forEach(t),_Bo=i(tl),f(Ny.$$.fragment,tl),tl.forEach(t),bBo=i(rl),Oe=s(rl,"DIV",{class:!0});var qt=n(Oe);f(Dy.$$.fragment,qt),vBo=i(qt),lse=s(qt,"P",{});var fzr=n(lse);TBo=r(fzr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),fzr.forEach(t),FBo=i(qt),Qa=s(qt,"P",{});var VE=n(Qa);CBo=r(VE,"The model class to instantiate is selected based on the "),ise=s(VE,"CODE",{});var gzr=n(ise);MBo=r(gzr,"model_type"),gzr.forEach(t),EBo=r(VE,` property of the config object (either
passed as an argument or loaded from `),dse=s(VE,"CODE",{});var hzr=n(dse);yBo=r(hzr,"pretrained_model_name_or_path"),hzr.forEach(t),wBo=r(VE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cse=s(VE,"CODE",{});var uzr=n(cse);ABo=r(uzr,"pretrained_model_name_or_path"),uzr.forEach(t),LBo=r(VE,":"),VE.forEach(t),BBo=i(qt),mse=s(qt,"UL",{});var pzr=n(mse);tT=s(pzr,"LI",{});var T5e=n(tT);fse=s(T5e,"STRONG",{});var _zr=n(fse);xBo=r(_zr,"vision-encoder-decoder"),_zr.forEach(t),kBo=r(T5e," \u2014 "),KI=s(T5e,"A",{href:!0});var bzr=n(KI);RBo=r(bzr,"VisionEncoderDecoderModel"),bzr.forEach(t),SBo=r(T5e," (Vision Encoder decoder model)"),T5e.forEach(t),pzr.forEach(t),PBo=i(qt),aT=s(qt,"P",{});var F5e=n(aT);$Bo=r(F5e,"The model is set in evaluation mode by default using "),gse=s(F5e,"CODE",{});var vzr=n(gse);IBo=r(vzr,"model.eval()"),vzr.forEach(t),jBo=r(F5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hse=s(F5e,"CODE",{});var Tzr=n(hse);NBo=r(Tzr,"model.train()"),Tzr.forEach(t),F5e.forEach(t),DBo=i(qt),use=s(qt,"P",{});var Fzr=n(use);qBo=r(Fzr,"Examples:"),Fzr.forEach(t),GBo=i(qt),f(qy.$$.fragment,qt),qt.forEach(t),rl.forEach(t),YLe=i(d),pd=s(d,"H2",{class:!0});var e9e=n(pd);sT=s(e9e,"A",{id:!0,class:!0,href:!0});var Czr=n(sT);pse=s(Czr,"SPAN",{});var Mzr=n(pse);f(Gy.$$.fragment,Mzr),Mzr.forEach(t),Czr.forEach(t),OBo=i(e9e),_se=s(e9e,"SPAN",{});var Ezr=n(_se);XBo=r(Ezr,"AutoModelForAudioClassification"),Ezr.forEach(t),e9e.forEach(t),KLe=i(d),or=s(d,"DIV",{class:!0});var al=n(or);f(Oy.$$.fragment,al),zBo=i(al),_d=s(al,"P",{});var fX=n(_d);VBo=r(fX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),bse=s(fX,"CODE",{});var yzr=n(bse);WBo=r(yzr,"from_pretrained()"),yzr.forEach(t),QBo=r(fX,"class method or the "),vse=s(fX,"CODE",{});var wzr=n(vse);HBo=r(wzr,"from_config()"),wzr.forEach(t),UBo=r(fX,`class
method.`),fX.forEach(t),JBo=i(al),Xy=s(al,"P",{});var o9e=n(Xy);YBo=r(o9e,"This class cannot be instantiated directly using "),Tse=s(o9e,"CODE",{});var Azr=n(Tse);KBo=r(Azr,"__init__()"),Azr.forEach(t),ZBo=r(o9e," (throws an error)."),o9e.forEach(t),exo=i(al),Qr=s(al,"DIV",{class:!0});var sl=n(Qr);f(zy.$$.fragment,sl),oxo=i(sl),Fse=s(sl,"P",{});var Lzr=n(Fse);rxo=r(Lzr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),Lzr.forEach(t),txo=i(sl),bd=s(sl,"P",{});var gX=n(bd);axo=r(gX,`Note:
Loading a model from its configuration file does `),Cse=s(gX,"STRONG",{});var Bzr=n(Cse);sxo=r(Bzr,"not"),Bzr.forEach(t),nxo=r(gX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mse=s(gX,"CODE",{});var xzr=n(Mse);lxo=r(xzr,"from_pretrained()"),xzr.forEach(t),ixo=r(gX,"to load the model weights."),gX.forEach(t),dxo=i(sl),Ese=s(sl,"P",{});var kzr=n(Ese);cxo=r(kzr,"Examples:"),kzr.forEach(t),mxo=i(sl),f(Vy.$$.fragment,sl),sl.forEach(t),fxo=i(al),Xe=s(al,"DIV",{class:!0});var Gt=n(Xe);f(Wy.$$.fragment,Gt),gxo=i(Gt),yse=s(Gt,"P",{});var Rzr=n(yse);hxo=r(Rzr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),Rzr.forEach(t),uxo=i(Gt),Ha=s(Gt,"P",{});var WE=n(Ha);pxo=r(WE,"The model class to instantiate is selected based on the "),wse=s(WE,"CODE",{});var Szr=n(wse);_xo=r(Szr,"model_type"),Szr.forEach(t),bxo=r(WE,` property of the config object (either
passed as an argument or loaded from `),Ase=s(WE,"CODE",{});var Pzr=n(Ase);vxo=r(Pzr,"pretrained_model_name_or_path"),Pzr.forEach(t),Txo=r(WE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lse=s(WE,"CODE",{});var $zr=n(Lse);Fxo=r($zr,"pretrained_model_name_or_path"),$zr.forEach(t),Cxo=r(WE,":"),WE.forEach(t),Mxo=i(Gt),ro=s(Gt,"UL",{});var Ot=n(ro);nT=s(Ot,"LI",{});var C5e=n(nT);Bse=s(C5e,"STRONG",{});var Izr=n(Bse);Exo=r(Izr,"hubert"),Izr.forEach(t),yxo=r(C5e," \u2014 "),ZI=s(C5e,"A",{href:!0});var jzr=n(ZI);wxo=r(jzr,"HubertForSequenceClassification"),jzr.forEach(t),Axo=r(C5e," (Hubert model)"),C5e.forEach(t),Lxo=i(Ot),lT=s(Ot,"LI",{});var M5e=n(lT);xse=s(M5e,"STRONG",{});var Nzr=n(xse);Bxo=r(Nzr,"sew"),Nzr.forEach(t),xxo=r(M5e," \u2014 "),ej=s(M5e,"A",{href:!0});var Dzr=n(ej);kxo=r(Dzr,"SEWForSequenceClassification"),Dzr.forEach(t),Rxo=r(M5e," (SEW model)"),M5e.forEach(t),Sxo=i(Ot),iT=s(Ot,"LI",{});var E5e=n(iT);kse=s(E5e,"STRONG",{});var qzr=n(kse);Pxo=r(qzr,"sew-d"),qzr.forEach(t),$xo=r(E5e," \u2014 "),oj=s(E5e,"A",{href:!0});var Gzr=n(oj);Ixo=r(Gzr,"SEWDForSequenceClassification"),Gzr.forEach(t),jxo=r(E5e," (SEW-D model)"),E5e.forEach(t),Nxo=i(Ot),dT=s(Ot,"LI",{});var y5e=n(dT);Rse=s(y5e,"STRONG",{});var Ozr=n(Rse);Dxo=r(Ozr,"unispeech"),Ozr.forEach(t),qxo=r(y5e," \u2014 "),rj=s(y5e,"A",{href:!0});var Xzr=n(rj);Gxo=r(Xzr,"UniSpeechForSequenceClassification"),Xzr.forEach(t),Oxo=r(y5e," (UniSpeech model)"),y5e.forEach(t),Xxo=i(Ot),cT=s(Ot,"LI",{});var w5e=n(cT);Sse=s(w5e,"STRONG",{});var zzr=n(Sse);zxo=r(zzr,"unispeech-sat"),zzr.forEach(t),Vxo=r(w5e," \u2014 "),tj=s(w5e,"A",{href:!0});var Vzr=n(tj);Wxo=r(Vzr,"UniSpeechSatForSequenceClassification"),Vzr.forEach(t),Qxo=r(w5e," (UniSpeechSat model)"),w5e.forEach(t),Hxo=i(Ot),mT=s(Ot,"LI",{});var A5e=n(mT);Pse=s(A5e,"STRONG",{});var Wzr=n(Pse);Uxo=r(Wzr,"wav2vec2"),Wzr.forEach(t),Jxo=r(A5e," \u2014 "),aj=s(A5e,"A",{href:!0});var Qzr=n(aj);Yxo=r(Qzr,"Wav2Vec2ForSequenceClassification"),Qzr.forEach(t),Kxo=r(A5e," (Wav2Vec2 model)"),A5e.forEach(t),Zxo=i(Ot),fT=s(Ot,"LI",{});var L5e=n(fT);$se=s(L5e,"STRONG",{});var Hzr=n($se);eko=r(Hzr,"wavlm"),Hzr.forEach(t),oko=r(L5e," \u2014 "),sj=s(L5e,"A",{href:!0});var Uzr=n(sj);rko=r(Uzr,"WavLMForSequenceClassification"),Uzr.forEach(t),tko=r(L5e," (WavLM model)"),L5e.forEach(t),Ot.forEach(t),ako=i(Gt),gT=s(Gt,"P",{});var B5e=n(gT);sko=r(B5e,"The model is set in evaluation mode by default using "),Ise=s(B5e,"CODE",{});var Jzr=n(Ise);nko=r(Jzr,"model.eval()"),Jzr.forEach(t),lko=r(B5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jse=s(B5e,"CODE",{});var Yzr=n(jse);iko=r(Yzr,"model.train()"),Yzr.forEach(t),B5e.forEach(t),dko=i(Gt),Nse=s(Gt,"P",{});var Kzr=n(Nse);cko=r(Kzr,"Examples:"),Kzr.forEach(t),mko=i(Gt),f(Qy.$$.fragment,Gt),Gt.forEach(t),al.forEach(t),ZLe=i(d),vd=s(d,"H2",{class:!0});var r9e=n(vd);hT=s(r9e,"A",{id:!0,class:!0,href:!0});var Zzr=n(hT);Dse=s(Zzr,"SPAN",{});var eVr=n(Dse);f(Hy.$$.fragment,eVr),eVr.forEach(t),Zzr.forEach(t),fko=i(r9e),qse=s(r9e,"SPAN",{});var oVr=n(qse);gko=r(oVr,"AutoModelForAudioFrameClassification"),oVr.forEach(t),r9e.forEach(t),e7e=i(d),rr=s(d,"DIV",{class:!0});var nl=n(rr);f(Uy.$$.fragment,nl),hko=i(nl),Td=s(nl,"P",{});var hX=n(Td);uko=r(hX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Gse=s(hX,"CODE",{});var rVr=n(Gse);pko=r(rVr,"from_pretrained()"),rVr.forEach(t),_ko=r(hX,"class method or the "),Ose=s(hX,"CODE",{});var tVr=n(Ose);bko=r(tVr,"from_config()"),tVr.forEach(t),vko=r(hX,`class
method.`),hX.forEach(t),Tko=i(nl),Jy=s(nl,"P",{});var t9e=n(Jy);Fko=r(t9e,"This class cannot be instantiated directly using "),Xse=s(t9e,"CODE",{});var aVr=n(Xse);Cko=r(aVr,"__init__()"),aVr.forEach(t),Mko=r(t9e," (throws an error)."),t9e.forEach(t),Eko=i(nl),Hr=s(nl,"DIV",{class:!0});var ll=n(Hr);f(Yy.$$.fragment,ll),yko=i(ll),zse=s(ll,"P",{});var sVr=n(zse);wko=r(sVr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),sVr.forEach(t),Ako=i(ll),Fd=s(ll,"P",{});var uX=n(Fd);Lko=r(uX,`Note:
Loading a model from its configuration file does `),Vse=s(uX,"STRONG",{});var nVr=n(Vse);Bko=r(nVr,"not"),nVr.forEach(t),xko=r(uX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=s(uX,"CODE",{});var lVr=n(Wse);kko=r(lVr,"from_pretrained()"),lVr.forEach(t),Rko=r(uX,"to load the model weights."),uX.forEach(t),Sko=i(ll),Qse=s(ll,"P",{});var iVr=n(Qse);Pko=r(iVr,"Examples:"),iVr.forEach(t),$ko=i(ll),f(Ky.$$.fragment,ll),ll.forEach(t),Iko=i(nl),ze=s(nl,"DIV",{class:!0});var Xt=n(ze);f(Zy.$$.fragment,Xt),jko=i(Xt),Hse=s(Xt,"P",{});var dVr=n(Hse);Nko=r(dVr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),dVr.forEach(t),Dko=i(Xt),Ua=s(Xt,"P",{});var QE=n(Ua);qko=r(QE,"The model class to instantiate is selected based on the "),Use=s(QE,"CODE",{});var cVr=n(Use);Gko=r(cVr,"model_type"),cVr.forEach(t),Oko=r(QE,` property of the config object (either
passed as an argument or loaded from `),Jse=s(QE,"CODE",{});var mVr=n(Jse);Xko=r(mVr,"pretrained_model_name_or_path"),mVr.forEach(t),zko=r(QE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yse=s(QE,"CODE",{});var fVr=n(Yse);Vko=r(fVr,"pretrained_model_name_or_path"),fVr.forEach(t),Wko=r(QE,":"),QE.forEach(t),Qko=i(Xt),Cd=s(Xt,"UL",{});var pX=n(Cd);uT=s(pX,"LI",{});var x5e=n(uT);Kse=s(x5e,"STRONG",{});var gVr=n(Kse);Hko=r(gVr,"unispeech-sat"),gVr.forEach(t),Uko=r(x5e," \u2014 "),nj=s(x5e,"A",{href:!0});var hVr=n(nj);Jko=r(hVr,"UniSpeechSatForAudioFrameClassification"),hVr.forEach(t),Yko=r(x5e," (UniSpeechSat model)"),x5e.forEach(t),Kko=i(pX),pT=s(pX,"LI",{});var k5e=n(pT);Zse=s(k5e,"STRONG",{});var uVr=n(Zse);Zko=r(uVr,"wav2vec2"),uVr.forEach(t),eRo=r(k5e," \u2014 "),lj=s(k5e,"A",{href:!0});var pVr=n(lj);oRo=r(pVr,"Wav2Vec2ForAudioFrameClassification"),pVr.forEach(t),rRo=r(k5e," (Wav2Vec2 model)"),k5e.forEach(t),tRo=i(pX),_T=s(pX,"LI",{});var R5e=n(_T);ene=s(R5e,"STRONG",{});var _Vr=n(ene);aRo=r(_Vr,"wavlm"),_Vr.forEach(t),sRo=r(R5e," \u2014 "),ij=s(R5e,"A",{href:!0});var bVr=n(ij);nRo=r(bVr,"WavLMForAudioFrameClassification"),bVr.forEach(t),lRo=r(R5e," (WavLM model)"),R5e.forEach(t),pX.forEach(t),iRo=i(Xt),bT=s(Xt,"P",{});var S5e=n(bT);dRo=r(S5e,"The model is set in evaluation mode by default using "),one=s(S5e,"CODE",{});var vVr=n(one);cRo=r(vVr,"model.eval()"),vVr.forEach(t),mRo=r(S5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),rne=s(S5e,"CODE",{});var TVr=n(rne);fRo=r(TVr,"model.train()"),TVr.forEach(t),S5e.forEach(t),gRo=i(Xt),tne=s(Xt,"P",{});var FVr=n(tne);hRo=r(FVr,"Examples:"),FVr.forEach(t),uRo=i(Xt),f(ew.$$.fragment,Xt),Xt.forEach(t),nl.forEach(t),o7e=i(d),Md=s(d,"H2",{class:!0});var a9e=n(Md);vT=s(a9e,"A",{id:!0,class:!0,href:!0});var CVr=n(vT);ane=s(CVr,"SPAN",{});var MVr=n(ane);f(ow.$$.fragment,MVr),MVr.forEach(t),CVr.forEach(t),pRo=i(a9e),sne=s(a9e,"SPAN",{});var EVr=n(sne);_Ro=r(EVr,"AutoModelForCTC"),EVr.forEach(t),a9e.forEach(t),r7e=i(d),tr=s(d,"DIV",{class:!0});var il=n(tr);f(rw.$$.fragment,il),bRo=i(il),Ed=s(il,"P",{});var _X=n(Ed);vRo=r(_X,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),nne=s(_X,"CODE",{});var yVr=n(nne);TRo=r(yVr,"from_pretrained()"),yVr.forEach(t),FRo=r(_X,"class method or the "),lne=s(_X,"CODE",{});var wVr=n(lne);CRo=r(wVr,"from_config()"),wVr.forEach(t),MRo=r(_X,`class
method.`),_X.forEach(t),ERo=i(il),tw=s(il,"P",{});var s9e=n(tw);yRo=r(s9e,"This class cannot be instantiated directly using "),ine=s(s9e,"CODE",{});var AVr=n(ine);wRo=r(AVr,"__init__()"),AVr.forEach(t),ARo=r(s9e," (throws an error)."),s9e.forEach(t),LRo=i(il),Ur=s(il,"DIV",{class:!0});var dl=n(Ur);f(aw.$$.fragment,dl),BRo=i(dl),dne=s(dl,"P",{});var LVr=n(dne);xRo=r(LVr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),LVr.forEach(t),kRo=i(dl),yd=s(dl,"P",{});var bX=n(yd);RRo=r(bX,`Note:
Loading a model from its configuration file does `),cne=s(bX,"STRONG",{});var BVr=n(cne);SRo=r(BVr,"not"),BVr.forEach(t),PRo=r(bX,` load the model weights. It only affects the
model\u2019s configuration. Use `),mne=s(bX,"CODE",{});var xVr=n(mne);$Ro=r(xVr,"from_pretrained()"),xVr.forEach(t),IRo=r(bX,"to load the model weights."),bX.forEach(t),jRo=i(dl),fne=s(dl,"P",{});var kVr=n(fne);NRo=r(kVr,"Examples:"),kVr.forEach(t),DRo=i(dl),f(sw.$$.fragment,dl),dl.forEach(t),qRo=i(il),Ve=s(il,"DIV",{class:!0});var zt=n(Ve);f(nw.$$.fragment,zt),GRo=i(zt),gne=s(zt,"P",{});var RVr=n(gne);ORo=r(RVr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),RVr.forEach(t),XRo=i(zt),Ja=s(zt,"P",{});var HE=n(Ja);zRo=r(HE,"The model class to instantiate is selected based on the "),hne=s(HE,"CODE",{});var SVr=n(hne);VRo=r(SVr,"model_type"),SVr.forEach(t),WRo=r(HE,` property of the config object (either
passed as an argument or loaded from `),une=s(HE,"CODE",{});var PVr=n(une);QRo=r(PVr,"pretrained_model_name_or_path"),PVr.forEach(t),HRo=r(HE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),pne=s(HE,"CODE",{});var $Vr=n(pne);URo=r($Vr,"pretrained_model_name_or_path"),$Vr.forEach(t),JRo=r(HE,":"),HE.forEach(t),YRo=i(zt),to=s(zt,"UL",{});var Vt=n(to);TT=s(Vt,"LI",{});var P5e=n(TT);_ne=s(P5e,"STRONG",{});var IVr=n(_ne);KRo=r(IVr,"hubert"),IVr.forEach(t),ZRo=r(P5e," \u2014 "),dj=s(P5e,"A",{href:!0});var jVr=n(dj);eSo=r(jVr,"HubertForCTC"),jVr.forEach(t),oSo=r(P5e," (Hubert model)"),P5e.forEach(t),rSo=i(Vt),FT=s(Vt,"LI",{});var $5e=n(FT);bne=s($5e,"STRONG",{});var NVr=n(bne);tSo=r(NVr,"sew"),NVr.forEach(t),aSo=r($5e," \u2014 "),cj=s($5e,"A",{href:!0});var DVr=n(cj);sSo=r(DVr,"SEWForCTC"),DVr.forEach(t),nSo=r($5e," (SEW model)"),$5e.forEach(t),lSo=i(Vt),CT=s(Vt,"LI",{});var I5e=n(CT);vne=s(I5e,"STRONG",{});var qVr=n(vne);iSo=r(qVr,"sew-d"),qVr.forEach(t),dSo=r(I5e," \u2014 "),mj=s(I5e,"A",{href:!0});var GVr=n(mj);cSo=r(GVr,"SEWDForCTC"),GVr.forEach(t),mSo=r(I5e," (SEW-D model)"),I5e.forEach(t),fSo=i(Vt),MT=s(Vt,"LI",{});var j5e=n(MT);Tne=s(j5e,"STRONG",{});var OVr=n(Tne);gSo=r(OVr,"unispeech"),OVr.forEach(t),hSo=r(j5e," \u2014 "),fj=s(j5e,"A",{href:!0});var XVr=n(fj);uSo=r(XVr,"UniSpeechForCTC"),XVr.forEach(t),pSo=r(j5e," (UniSpeech model)"),j5e.forEach(t),_So=i(Vt),ET=s(Vt,"LI",{});var N5e=n(ET);Fne=s(N5e,"STRONG",{});var zVr=n(Fne);bSo=r(zVr,"unispeech-sat"),zVr.forEach(t),vSo=r(N5e," \u2014 "),gj=s(N5e,"A",{href:!0});var VVr=n(gj);TSo=r(VVr,"UniSpeechSatForCTC"),VVr.forEach(t),FSo=r(N5e," (UniSpeechSat model)"),N5e.forEach(t),CSo=i(Vt),yT=s(Vt,"LI",{});var D5e=n(yT);Cne=s(D5e,"STRONG",{});var WVr=n(Cne);MSo=r(WVr,"wav2vec2"),WVr.forEach(t),ESo=r(D5e," \u2014 "),hj=s(D5e,"A",{href:!0});var QVr=n(hj);ySo=r(QVr,"Wav2Vec2ForCTC"),QVr.forEach(t),wSo=r(D5e," (Wav2Vec2 model)"),D5e.forEach(t),ASo=i(Vt),wT=s(Vt,"LI",{});var q5e=n(wT);Mne=s(q5e,"STRONG",{});var HVr=n(Mne);LSo=r(HVr,"wavlm"),HVr.forEach(t),BSo=r(q5e," \u2014 "),uj=s(q5e,"A",{href:!0});var UVr=n(uj);xSo=r(UVr,"WavLMForCTC"),UVr.forEach(t),kSo=r(q5e," (WavLM model)"),q5e.forEach(t),Vt.forEach(t),RSo=i(zt),AT=s(zt,"P",{});var G5e=n(AT);SSo=r(G5e,"The model is set in evaluation mode by default using "),Ene=s(G5e,"CODE",{});var JVr=n(Ene);PSo=r(JVr,"model.eval()"),JVr.forEach(t),$So=r(G5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yne=s(G5e,"CODE",{});var YVr=n(yne);ISo=r(YVr,"model.train()"),YVr.forEach(t),G5e.forEach(t),jSo=i(zt),wne=s(zt,"P",{});var KVr=n(wne);NSo=r(KVr,"Examples:"),KVr.forEach(t),DSo=i(zt),f(lw.$$.fragment,zt),zt.forEach(t),il.forEach(t),t7e=i(d),wd=s(d,"H2",{class:!0});var n9e=n(wd);LT=s(n9e,"A",{id:!0,class:!0,href:!0});var ZVr=n(LT);Ane=s(ZVr,"SPAN",{});var eWr=n(Ane);f(iw.$$.fragment,eWr),eWr.forEach(t),ZVr.forEach(t),qSo=i(n9e),Lne=s(n9e,"SPAN",{});var oWr=n(Lne);GSo=r(oWr,"AutoModelForSpeechSeq2Seq"),oWr.forEach(t),n9e.forEach(t),a7e=i(d),ar=s(d,"DIV",{class:!0});var cl=n(ar);f(dw.$$.fragment,cl),OSo=i(cl),Ad=s(cl,"P",{});var vX=n(Ad);XSo=r(vX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Bne=s(vX,"CODE",{});var rWr=n(Bne);zSo=r(rWr,"from_pretrained()"),rWr.forEach(t),VSo=r(vX,"class method or the "),xne=s(vX,"CODE",{});var tWr=n(xne);WSo=r(tWr,"from_config()"),tWr.forEach(t),QSo=r(vX,`class
method.`),vX.forEach(t),HSo=i(cl),cw=s(cl,"P",{});var l9e=n(cw);USo=r(l9e,"This class cannot be instantiated directly using "),kne=s(l9e,"CODE",{});var aWr=n(kne);JSo=r(aWr,"__init__()"),aWr.forEach(t),YSo=r(l9e," (throws an error)."),l9e.forEach(t),KSo=i(cl),Jr=s(cl,"DIV",{class:!0});var ml=n(Jr);f(mw.$$.fragment,ml),ZSo=i(ml),Rne=s(ml,"P",{});var sWr=n(Rne);ePo=r(sWr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),sWr.forEach(t),oPo=i(ml),Ld=s(ml,"P",{});var TX=n(Ld);rPo=r(TX,`Note:
Loading a model from its configuration file does `),Sne=s(TX,"STRONG",{});var nWr=n(Sne);tPo=r(nWr,"not"),nWr.forEach(t),aPo=r(TX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pne=s(TX,"CODE",{});var lWr=n(Pne);sPo=r(lWr,"from_pretrained()"),lWr.forEach(t),nPo=r(TX,"to load the model weights."),TX.forEach(t),lPo=i(ml),$ne=s(ml,"P",{});var iWr=n($ne);iPo=r(iWr,"Examples:"),iWr.forEach(t),dPo=i(ml),f(fw.$$.fragment,ml),ml.forEach(t),cPo=i(cl),We=s(cl,"DIV",{class:!0});var Wt=n(We);f(gw.$$.fragment,Wt),mPo=i(Wt),Ine=s(Wt,"P",{});var dWr=n(Ine);fPo=r(dWr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),dWr.forEach(t),gPo=i(Wt),Ya=s(Wt,"P",{});var UE=n(Ya);hPo=r(UE,"The model class to instantiate is selected based on the "),jne=s(UE,"CODE",{});var cWr=n(jne);uPo=r(cWr,"model_type"),cWr.forEach(t),pPo=r(UE,` property of the config object (either
passed as an argument or loaded from `),Nne=s(UE,"CODE",{});var mWr=n(Nne);_Po=r(mWr,"pretrained_model_name_or_path"),mWr.forEach(t),bPo=r(UE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dne=s(UE,"CODE",{});var fWr=n(Dne);vPo=r(fWr,"pretrained_model_name_or_path"),fWr.forEach(t),TPo=r(UE,":"),UE.forEach(t),FPo=i(Wt),hw=s(Wt,"UL",{});var i9e=n(hw);BT=s(i9e,"LI",{});var O5e=n(BT);qne=s(O5e,"STRONG",{});var gWr=n(qne);CPo=r(gWr,"speech-encoder-decoder"),gWr.forEach(t),MPo=r(O5e," \u2014 "),pj=s(O5e,"A",{href:!0});var hWr=n(pj);EPo=r(hWr,"SpeechEncoderDecoderModel"),hWr.forEach(t),yPo=r(O5e," (Speech Encoder decoder model)"),O5e.forEach(t),wPo=i(i9e),xT=s(i9e,"LI",{});var X5e=n(xT);Gne=s(X5e,"STRONG",{});var uWr=n(Gne);APo=r(uWr,"speech_to_text"),uWr.forEach(t),LPo=r(X5e," \u2014 "),_j=s(X5e,"A",{href:!0});var pWr=n(_j);BPo=r(pWr,"Speech2TextForConditionalGeneration"),pWr.forEach(t),xPo=r(X5e," (Speech2Text model)"),X5e.forEach(t),i9e.forEach(t),kPo=i(Wt),kT=s(Wt,"P",{});var z5e=n(kT);RPo=r(z5e,"The model is set in evaluation mode by default using "),One=s(z5e,"CODE",{});var _Wr=n(One);SPo=r(_Wr,"model.eval()"),_Wr.forEach(t),PPo=r(z5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xne=s(z5e,"CODE",{});var bWr=n(Xne);$Po=r(bWr,"model.train()"),bWr.forEach(t),z5e.forEach(t),IPo=i(Wt),zne=s(Wt,"P",{});var vWr=n(zne);jPo=r(vWr,"Examples:"),vWr.forEach(t),NPo=i(Wt),f(uw.$$.fragment,Wt),Wt.forEach(t),cl.forEach(t),s7e=i(d),Bd=s(d,"H2",{class:!0});var d9e=n(Bd);RT=s(d9e,"A",{id:!0,class:!0,href:!0});var TWr=n(RT);Vne=s(TWr,"SPAN",{});var FWr=n(Vne);f(pw.$$.fragment,FWr),FWr.forEach(t),TWr.forEach(t),DPo=i(d9e),Wne=s(d9e,"SPAN",{});var CWr=n(Wne);qPo=r(CWr,"AutoModelForAudioXVector"),CWr.forEach(t),d9e.forEach(t),n7e=i(d),sr=s(d,"DIV",{class:!0});var fl=n(sr);f(_w.$$.fragment,fl),GPo=i(fl),xd=s(fl,"P",{});var FX=n(xd);OPo=r(FX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),Qne=s(FX,"CODE",{});var MWr=n(Qne);XPo=r(MWr,"from_pretrained()"),MWr.forEach(t),zPo=r(FX,"class method or the "),Hne=s(FX,"CODE",{});var EWr=n(Hne);VPo=r(EWr,"from_config()"),EWr.forEach(t),WPo=r(FX,`class
method.`),FX.forEach(t),QPo=i(fl),bw=s(fl,"P",{});var c9e=n(bw);HPo=r(c9e,"This class cannot be instantiated directly using "),Une=s(c9e,"CODE",{});var yWr=n(Une);UPo=r(yWr,"__init__()"),yWr.forEach(t),JPo=r(c9e," (throws an error)."),c9e.forEach(t),YPo=i(fl),Yr=s(fl,"DIV",{class:!0});var gl=n(Yr);f(vw.$$.fragment,gl),KPo=i(gl),Jne=s(gl,"P",{});var wWr=n(Jne);ZPo=r(wWr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),wWr.forEach(t),e$o=i(gl),kd=s(gl,"P",{});var CX=n(kd);o$o=r(CX,`Note:
Loading a model from its configuration file does `),Yne=s(CX,"STRONG",{});var AWr=n(Yne);r$o=r(AWr,"not"),AWr.forEach(t),t$o=r(CX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kne=s(CX,"CODE",{});var LWr=n(Kne);a$o=r(LWr,"from_pretrained()"),LWr.forEach(t),s$o=r(CX,"to load the model weights."),CX.forEach(t),n$o=i(gl),Zne=s(gl,"P",{});var BWr=n(Zne);l$o=r(BWr,"Examples:"),BWr.forEach(t),i$o=i(gl),f(Tw.$$.fragment,gl),gl.forEach(t),d$o=i(fl),Qe=s(fl,"DIV",{class:!0});var Qt=n(Qe);f(Fw.$$.fragment,Qt),c$o=i(Qt),ele=s(Qt,"P",{});var xWr=n(ele);m$o=r(xWr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),xWr.forEach(t),f$o=i(Qt),Ka=s(Qt,"P",{});var JE=n(Ka);g$o=r(JE,"The model class to instantiate is selected based on the "),ole=s(JE,"CODE",{});var kWr=n(ole);h$o=r(kWr,"model_type"),kWr.forEach(t),u$o=r(JE,` property of the config object (either
passed as an argument or loaded from `),rle=s(JE,"CODE",{});var RWr=n(rle);p$o=r(RWr,"pretrained_model_name_or_path"),RWr.forEach(t),_$o=r(JE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tle=s(JE,"CODE",{});var SWr=n(tle);b$o=r(SWr,"pretrained_model_name_or_path"),SWr.forEach(t),v$o=r(JE,":"),JE.forEach(t),T$o=i(Qt),Rd=s(Qt,"UL",{});var MX=n(Rd);ST=s(MX,"LI",{});var V5e=n(ST);ale=s(V5e,"STRONG",{});var PWr=n(ale);F$o=r(PWr,"unispeech-sat"),PWr.forEach(t),C$o=r(V5e," \u2014 "),bj=s(V5e,"A",{href:!0});var $Wr=n(bj);M$o=r($Wr,"UniSpeechSatForXVector"),$Wr.forEach(t),E$o=r(V5e," (UniSpeechSat model)"),V5e.forEach(t),y$o=i(MX),PT=s(MX,"LI",{});var W5e=n(PT);sle=s(W5e,"STRONG",{});var IWr=n(sle);w$o=r(IWr,"wav2vec2"),IWr.forEach(t),A$o=r(W5e," \u2014 "),vj=s(W5e,"A",{href:!0});var jWr=n(vj);L$o=r(jWr,"Wav2Vec2ForXVector"),jWr.forEach(t),B$o=r(W5e," (Wav2Vec2 model)"),W5e.forEach(t),x$o=i(MX),$T=s(MX,"LI",{});var Q5e=n($T);nle=s(Q5e,"STRONG",{});var NWr=n(nle);k$o=r(NWr,"wavlm"),NWr.forEach(t),R$o=r(Q5e," \u2014 "),Tj=s(Q5e,"A",{href:!0});var DWr=n(Tj);S$o=r(DWr,"WavLMForXVector"),DWr.forEach(t),P$o=r(Q5e," (WavLM model)"),Q5e.forEach(t),MX.forEach(t),$$o=i(Qt),IT=s(Qt,"P",{});var H5e=n(IT);I$o=r(H5e,"The model is set in evaluation mode by default using "),lle=s(H5e,"CODE",{});var qWr=n(lle);j$o=r(qWr,"model.eval()"),qWr.forEach(t),N$o=r(H5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ile=s(H5e,"CODE",{});var GWr=n(ile);D$o=r(GWr,"model.train()"),GWr.forEach(t),H5e.forEach(t),q$o=i(Qt),dle=s(Qt,"P",{});var OWr=n(dle);G$o=r(OWr,"Examples:"),OWr.forEach(t),O$o=i(Qt),f(Cw.$$.fragment,Qt),Qt.forEach(t),fl.forEach(t),l7e=i(d),Sd=s(d,"H2",{class:!0});var m9e=n(Sd);jT=s(m9e,"A",{id:!0,class:!0,href:!0});var XWr=n(jT);cle=s(XWr,"SPAN",{});var zWr=n(cle);f(Mw.$$.fragment,zWr),zWr.forEach(t),XWr.forEach(t),X$o=i(m9e),mle=s(m9e,"SPAN",{});var VWr=n(mle);z$o=r(VWr,"AutoModelForObjectDetection"),VWr.forEach(t),m9e.forEach(t),i7e=i(d),nr=s(d,"DIV",{class:!0});var hl=n(nr);f(Ew.$$.fragment,hl),V$o=i(hl),Pd=s(hl,"P",{});var EX=n(Pd);W$o=r(EX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),fle=s(EX,"CODE",{});var WWr=n(fle);Q$o=r(WWr,"from_pretrained()"),WWr.forEach(t),H$o=r(EX,"class method or the "),gle=s(EX,"CODE",{});var QWr=n(gle);U$o=r(QWr,"from_config()"),QWr.forEach(t),J$o=r(EX,`class
method.`),EX.forEach(t),Y$o=i(hl),yw=s(hl,"P",{});var f9e=n(yw);K$o=r(f9e,"This class cannot be instantiated directly using "),hle=s(f9e,"CODE",{});var HWr=n(hle);Z$o=r(HWr,"__init__()"),HWr.forEach(t),eIo=r(f9e," (throws an error)."),f9e.forEach(t),oIo=i(hl),Kr=s(hl,"DIV",{class:!0});var ul=n(Kr);f(ww.$$.fragment,ul),rIo=i(ul),ule=s(ul,"P",{});var UWr=n(ule);tIo=r(UWr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),UWr.forEach(t),aIo=i(ul),$d=s(ul,"P",{});var yX=n($d);sIo=r(yX,`Note:
Loading a model from its configuration file does `),ple=s(yX,"STRONG",{});var JWr=n(ple);nIo=r(JWr,"not"),JWr.forEach(t),lIo=r(yX,` load the model weights. It only affects the
model\u2019s configuration. Use `),_le=s(yX,"CODE",{});var YWr=n(_le);iIo=r(YWr,"from_pretrained()"),YWr.forEach(t),dIo=r(yX,"to load the model weights."),yX.forEach(t),cIo=i(ul),ble=s(ul,"P",{});var KWr=n(ble);mIo=r(KWr,"Examples:"),KWr.forEach(t),fIo=i(ul),f(Aw.$$.fragment,ul),ul.forEach(t),gIo=i(hl),He=s(hl,"DIV",{class:!0});var Ht=n(He);f(Lw.$$.fragment,Ht),hIo=i(Ht),vle=s(Ht,"P",{});var ZWr=n(vle);uIo=r(ZWr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),ZWr.forEach(t),pIo=i(Ht),Za=s(Ht,"P",{});var YE=n(Za);_Io=r(YE,"The model class to instantiate is selected based on the "),Tle=s(YE,"CODE",{});var eQr=n(Tle);bIo=r(eQr,"model_type"),eQr.forEach(t),vIo=r(YE,` property of the config object (either
passed as an argument or loaded from `),Fle=s(YE,"CODE",{});var oQr=n(Fle);TIo=r(oQr,"pretrained_model_name_or_path"),oQr.forEach(t),FIo=r(YE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cle=s(YE,"CODE",{});var rQr=n(Cle);CIo=r(rQr,"pretrained_model_name_or_path"),rQr.forEach(t),MIo=r(YE,":"),YE.forEach(t),EIo=i(Ht),Mle=s(Ht,"UL",{});var tQr=n(Mle);NT=s(tQr,"LI",{});var U5e=n(NT);Ele=s(U5e,"STRONG",{});var aQr=n(Ele);yIo=r(aQr,"detr"),aQr.forEach(t),wIo=r(U5e," \u2014 "),Fj=s(U5e,"A",{href:!0});var sQr=n(Fj);AIo=r(sQr,"DetrForObjectDetection"),sQr.forEach(t),LIo=r(U5e," (DETR model)"),U5e.forEach(t),tQr.forEach(t),BIo=i(Ht),DT=s(Ht,"P",{});var J5e=n(DT);xIo=r(J5e,"The model is set in evaluation mode by default using "),yle=s(J5e,"CODE",{});var nQr=n(yle);kIo=r(nQr,"model.eval()"),nQr.forEach(t),RIo=r(J5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wle=s(J5e,"CODE",{});var lQr=n(wle);SIo=r(lQr,"model.train()"),lQr.forEach(t),J5e.forEach(t),PIo=i(Ht),Ale=s(Ht,"P",{});var iQr=n(Ale);$Io=r(iQr,"Examples:"),iQr.forEach(t),IIo=i(Ht),f(Bw.$$.fragment,Ht),Ht.forEach(t),hl.forEach(t),d7e=i(d),Id=s(d,"H2",{class:!0});var g9e=n(Id);qT=s(g9e,"A",{id:!0,class:!0,href:!0});var dQr=n(qT);Lle=s(dQr,"SPAN",{});var cQr=n(Lle);f(xw.$$.fragment,cQr),cQr.forEach(t),dQr.forEach(t),jIo=i(g9e),Ble=s(g9e,"SPAN",{});var mQr=n(Ble);NIo=r(mQr,"AutoModelForImageSegmentation"),mQr.forEach(t),g9e.forEach(t),c7e=i(d),lr=s(d,"DIV",{class:!0});var pl=n(lr);f(kw.$$.fragment,pl),DIo=i(pl),jd=s(pl,"P",{});var wX=n(jd);qIo=r(wX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),xle=s(wX,"CODE",{});var fQr=n(xle);GIo=r(fQr,"from_pretrained()"),fQr.forEach(t),OIo=r(wX,"class method or the "),kle=s(wX,"CODE",{});var gQr=n(kle);XIo=r(gQr,"from_config()"),gQr.forEach(t),zIo=r(wX,`class
method.`),wX.forEach(t),VIo=i(pl),Rw=s(pl,"P",{});var h9e=n(Rw);WIo=r(h9e,"This class cannot be instantiated directly using "),Rle=s(h9e,"CODE",{});var hQr=n(Rle);QIo=r(hQr,"__init__()"),hQr.forEach(t),HIo=r(h9e," (throws an error)."),h9e.forEach(t),UIo=i(pl),Zr=s(pl,"DIV",{class:!0});var _l=n(Zr);f(Sw.$$.fragment,_l),JIo=i(_l),Sle=s(_l,"P",{});var uQr=n(Sle);YIo=r(uQr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),uQr.forEach(t),KIo=i(_l),Nd=s(_l,"P",{});var AX=n(Nd);ZIo=r(AX,`Note:
Loading a model from its configuration file does `),Ple=s(AX,"STRONG",{});var pQr=n(Ple);ejo=r(pQr,"not"),pQr.forEach(t),ojo=r(AX,` load the model weights. It only affects the
model\u2019s configuration. Use `),$le=s(AX,"CODE",{});var _Qr=n($le);rjo=r(_Qr,"from_pretrained()"),_Qr.forEach(t),tjo=r(AX,"to load the model weights."),AX.forEach(t),ajo=i(_l),Ile=s(_l,"P",{});var bQr=n(Ile);sjo=r(bQr,"Examples:"),bQr.forEach(t),njo=i(_l),f(Pw.$$.fragment,_l),_l.forEach(t),ljo=i(pl),Ue=s(pl,"DIV",{class:!0});var Ut=n(Ue);f($w.$$.fragment,Ut),ijo=i(Ut),jle=s(Ut,"P",{});var vQr=n(jle);djo=r(vQr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),vQr.forEach(t),cjo=i(Ut),es=s(Ut,"P",{});var KE=n(es);mjo=r(KE,"The model class to instantiate is selected based on the "),Nle=s(KE,"CODE",{});var TQr=n(Nle);fjo=r(TQr,"model_type"),TQr.forEach(t),gjo=r(KE,` property of the config object (either
passed as an argument or loaded from `),Dle=s(KE,"CODE",{});var FQr=n(Dle);hjo=r(FQr,"pretrained_model_name_or_path"),FQr.forEach(t),ujo=r(KE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qle=s(KE,"CODE",{});var CQr=n(qle);pjo=r(CQr,"pretrained_model_name_or_path"),CQr.forEach(t),_jo=r(KE,":"),KE.forEach(t),bjo=i(Ut),Gle=s(Ut,"UL",{});var MQr=n(Gle);GT=s(MQr,"LI",{});var Y5e=n(GT);Ole=s(Y5e,"STRONG",{});var EQr=n(Ole);vjo=r(EQr,"detr"),EQr.forEach(t),Tjo=r(Y5e," \u2014 "),Cj=s(Y5e,"A",{href:!0});var yQr=n(Cj);Fjo=r(yQr,"DetrForSegmentation"),yQr.forEach(t),Cjo=r(Y5e," (DETR model)"),Y5e.forEach(t),MQr.forEach(t),Mjo=i(Ut),OT=s(Ut,"P",{});var K5e=n(OT);Ejo=r(K5e,"The model is set in evaluation mode by default using "),Xle=s(K5e,"CODE",{});var wQr=n(Xle);yjo=r(wQr,"model.eval()"),wQr.forEach(t),wjo=r(K5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zle=s(K5e,"CODE",{});var AQr=n(zle);Ajo=r(AQr,"model.train()"),AQr.forEach(t),K5e.forEach(t),Ljo=i(Ut),Vle=s(Ut,"P",{});var LQr=n(Vle);Bjo=r(LQr,"Examples:"),LQr.forEach(t),xjo=i(Ut),f(Iw.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),m7e=i(d),Dd=s(d,"H2",{class:!0});var u9e=n(Dd);XT=s(u9e,"A",{id:!0,class:!0,href:!0});var BQr=n(XT);Wle=s(BQr,"SPAN",{});var xQr=n(Wle);f(jw.$$.fragment,xQr),xQr.forEach(t),BQr.forEach(t),kjo=i(u9e),Qle=s(u9e,"SPAN",{});var kQr=n(Qle);Rjo=r(kQr,"AutoModelForSemanticSegmentation"),kQr.forEach(t),u9e.forEach(t),f7e=i(d),ir=s(d,"DIV",{class:!0});var bl=n(ir);f(Nw.$$.fragment,bl),Sjo=i(bl),qd=s(bl,"P",{});var LX=n(qd);Pjo=r(LX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Hle=s(LX,"CODE",{});var RQr=n(Hle);$jo=r(RQr,"from_pretrained()"),RQr.forEach(t),Ijo=r(LX,"class method or the "),Ule=s(LX,"CODE",{});var SQr=n(Ule);jjo=r(SQr,"from_config()"),SQr.forEach(t),Njo=r(LX,`class
method.`),LX.forEach(t),Djo=i(bl),Dw=s(bl,"P",{});var p9e=n(Dw);qjo=r(p9e,"This class cannot be instantiated directly using "),Jle=s(p9e,"CODE",{});var PQr=n(Jle);Gjo=r(PQr,"__init__()"),PQr.forEach(t),Ojo=r(p9e," (throws an error)."),p9e.forEach(t),Xjo=i(bl),et=s(bl,"DIV",{class:!0});var vl=n(et);f(qw.$$.fragment,vl),zjo=i(vl),Yle=s(vl,"P",{});var $Qr=n(Yle);Vjo=r($Qr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),$Qr.forEach(t),Wjo=i(vl),Gd=s(vl,"P",{});var BX=n(Gd);Qjo=r(BX,`Note:
Loading a model from its configuration file does `),Kle=s(BX,"STRONG",{});var IQr=n(Kle);Hjo=r(IQr,"not"),IQr.forEach(t),Ujo=r(BX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zle=s(BX,"CODE",{});var jQr=n(Zle);Jjo=r(jQr,"from_pretrained()"),jQr.forEach(t),Yjo=r(BX,"to load the model weights."),BX.forEach(t),Kjo=i(vl),eie=s(vl,"P",{});var NQr=n(eie);Zjo=r(NQr,"Examples:"),NQr.forEach(t),eNo=i(vl),f(Gw.$$.fragment,vl),vl.forEach(t),oNo=i(bl),Je=s(bl,"DIV",{class:!0});var Jt=n(Je);f(Ow.$$.fragment,Jt),rNo=i(Jt),oie=s(Jt,"P",{});var DQr=n(oie);tNo=r(DQr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),DQr.forEach(t),aNo=i(Jt),os=s(Jt,"P",{});var ZE=n(os);sNo=r(ZE,"The model class to instantiate is selected based on the "),rie=s(ZE,"CODE",{});var qQr=n(rie);nNo=r(qQr,"model_type"),qQr.forEach(t),lNo=r(ZE,` property of the config object (either
passed as an argument or loaded from `),tie=s(ZE,"CODE",{});var GQr=n(tie);iNo=r(GQr,"pretrained_model_name_or_path"),GQr.forEach(t),dNo=r(ZE,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aie=s(ZE,"CODE",{});var OQr=n(aie);cNo=r(OQr,"pretrained_model_name_or_path"),OQr.forEach(t),mNo=r(ZE,":"),ZE.forEach(t),fNo=i(Jt),Xw=s(Jt,"UL",{});var _9e=n(Xw);zT=s(_9e,"LI",{});var Z5e=n(zT);sie=s(Z5e,"STRONG",{});var XQr=n(sie);gNo=r(XQr,"beit"),XQr.forEach(t),hNo=r(Z5e," \u2014 "),Mj=s(Z5e,"A",{href:!0});var zQr=n(Mj);uNo=r(zQr,"BeitForSemanticSegmentation"),zQr.forEach(t),pNo=r(Z5e," (BEiT model)"),Z5e.forEach(t),_No=i(_9e),VT=s(_9e,"LI",{});var eye=n(VT);nie=s(eye,"STRONG",{});var VQr=n(nie);bNo=r(VQr,"segformer"),VQr.forEach(t),vNo=r(eye," \u2014 "),Ej=s(eye,"A",{href:!0});var WQr=n(Ej);TNo=r(WQr,"SegformerForSemanticSegmentation"),WQr.forEach(t),FNo=r(eye," (SegFormer model)"),eye.forEach(t),_9e.forEach(t),CNo=i(Jt),WT=s(Jt,"P",{});var oye=n(WT);MNo=r(oye,"The model is set in evaluation mode by default using "),lie=s(oye,"CODE",{});var QQr=n(lie);ENo=r(QQr,"model.eval()"),QQr.forEach(t),yNo=r(oye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iie=s(oye,"CODE",{});var HQr=n(iie);wNo=r(HQr,"model.train()"),HQr.forEach(t),oye.forEach(t),ANo=i(Jt),die=s(Jt,"P",{});var UQr=n(die);LNo=r(UQr,"Examples:"),UQr.forEach(t),BNo=i(Jt),f(zw.$$.fragment,Jt),Jt.forEach(t),bl.forEach(t),g7e=i(d),Od=s(d,"H2",{class:!0});var b9e=n(Od);QT=s(b9e,"A",{id:!0,class:!0,href:!0});var JQr=n(QT);cie=s(JQr,"SPAN",{});var YQr=n(cie);f(Vw.$$.fragment,YQr),YQr.forEach(t),JQr.forEach(t),xNo=i(b9e),mie=s(b9e,"SPAN",{});var KQr=n(mie);kNo=r(KQr,"TFAutoModel"),KQr.forEach(t),b9e.forEach(t),h7e=i(d),dr=s(d,"DIV",{class:!0});var Tl=n(dr);f(Ww.$$.fragment,Tl),RNo=i(Tl),Xd=s(Tl,"P",{});var xX=n(Xd);SNo=r(xX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),fie=s(xX,"CODE",{});var ZQr=n(fie);PNo=r(ZQr,"from_pretrained()"),ZQr.forEach(t),$No=r(xX,"class method or the "),gie=s(xX,"CODE",{});var eHr=n(gie);INo=r(eHr,"from_config()"),eHr.forEach(t),jNo=r(xX,`class
method.`),xX.forEach(t),NNo=i(Tl),Qw=s(Tl,"P",{});var v9e=n(Qw);DNo=r(v9e,"This class cannot be instantiated directly using "),hie=s(v9e,"CODE",{});var oHr=n(hie);qNo=r(oHr,"__init__()"),oHr.forEach(t),GNo=r(v9e," (throws an error)."),v9e.forEach(t),ONo=i(Tl),ot=s(Tl,"DIV",{class:!0});var Fl=n(ot);f(Hw.$$.fragment,Fl),XNo=i(Fl),uie=s(Fl,"P",{});var rHr=n(uie);zNo=r(rHr,"Instantiates one of the base model classes of the library from a configuration."),rHr.forEach(t),VNo=i(Fl),zd=s(Fl,"P",{});var kX=n(zd);WNo=r(kX,`Note:
Loading a model from its configuration file does `),pie=s(kX,"STRONG",{});var tHr=n(pie);QNo=r(tHr,"not"),tHr.forEach(t),HNo=r(kX,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ie=s(kX,"CODE",{});var aHr=n(_ie);UNo=r(aHr,"from_pretrained()"),aHr.forEach(t),JNo=r(kX,"to load the model weights."),kX.forEach(t),YNo=i(Fl),bie=s(Fl,"P",{});var sHr=n(bie);KNo=r(sHr,"Examples:"),sHr.forEach(t),ZNo=i(Fl),f(Uw.$$.fragment,Fl),Fl.forEach(t),eDo=i(Tl),mo=s(Tl,"DIV",{class:!0});var na=n(mo);f(Jw.$$.fragment,na),oDo=i(na),vie=s(na,"P",{});var nHr=n(vie);rDo=r(nHr,"Instantiate one of the base model classes of the library from a pretrained model."),nHr.forEach(t),tDo=i(na),rs=s(na,"P",{});var e3=n(rs);aDo=r(e3,"The model class to instantiate is selected based on the "),Tie=s(e3,"CODE",{});var lHr=n(Tie);sDo=r(lHr,"model_type"),lHr.forEach(t),nDo=r(e3,` property of the config object (either
passed as an argument or loaded from `),Fie=s(e3,"CODE",{});var iHr=n(Fie);lDo=r(iHr,"pretrained_model_name_or_path"),iHr.forEach(t),iDo=r(e3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cie=s(e3,"CODE",{});var dHr=n(Cie);dDo=r(dHr,"pretrained_model_name_or_path"),dHr.forEach(t),cDo=r(e3,":"),e3.forEach(t),mDo=i(na),B=s(na,"UL",{});var x=n(B);HT=s(x,"LI",{});var rye=n(HT);Mie=s(rye,"STRONG",{});var cHr=n(Mie);fDo=r(cHr,"albert"),cHr.forEach(t),gDo=r(rye," \u2014 "),yj=s(rye,"A",{href:!0});var mHr=n(yj);hDo=r(mHr,"TFAlbertModel"),mHr.forEach(t),uDo=r(rye," (ALBERT model)"),rye.forEach(t),pDo=i(x),UT=s(x,"LI",{});var tye=n(UT);Eie=s(tye,"STRONG",{});var fHr=n(Eie);_Do=r(fHr,"bart"),fHr.forEach(t),bDo=r(tye," \u2014 "),wj=s(tye,"A",{href:!0});var gHr=n(wj);vDo=r(gHr,"TFBartModel"),gHr.forEach(t),TDo=r(tye," (BART model)"),tye.forEach(t),FDo=i(x),JT=s(x,"LI",{});var aye=n(JT);yie=s(aye,"STRONG",{});var hHr=n(yie);CDo=r(hHr,"bert"),hHr.forEach(t),MDo=r(aye," \u2014 "),Aj=s(aye,"A",{href:!0});var uHr=n(Aj);EDo=r(uHr,"TFBertModel"),uHr.forEach(t),yDo=r(aye," (BERT model)"),aye.forEach(t),wDo=i(x),YT=s(x,"LI",{});var sye=n(YT);wie=s(sye,"STRONG",{});var pHr=n(wie);ADo=r(pHr,"blenderbot"),pHr.forEach(t),LDo=r(sye," \u2014 "),Lj=s(sye,"A",{href:!0});var _Hr=n(Lj);BDo=r(_Hr,"TFBlenderbotModel"),_Hr.forEach(t),xDo=r(sye," (Blenderbot model)"),sye.forEach(t),kDo=i(x),KT=s(x,"LI",{});var nye=n(KT);Aie=s(nye,"STRONG",{});var bHr=n(Aie);RDo=r(bHr,"blenderbot-small"),bHr.forEach(t),SDo=r(nye," \u2014 "),Bj=s(nye,"A",{href:!0});var vHr=n(Bj);PDo=r(vHr,"TFBlenderbotSmallModel"),vHr.forEach(t),$Do=r(nye," (BlenderbotSmall model)"),nye.forEach(t),IDo=i(x),ZT=s(x,"LI",{});var lye=n(ZT);Lie=s(lye,"STRONG",{});var THr=n(Lie);jDo=r(THr,"camembert"),THr.forEach(t),NDo=r(lye," \u2014 "),xj=s(lye,"A",{href:!0});var FHr=n(xj);DDo=r(FHr,"TFCamembertModel"),FHr.forEach(t),qDo=r(lye," (CamemBERT model)"),lye.forEach(t),GDo=i(x),e1=s(x,"LI",{});var iye=n(e1);Bie=s(iye,"STRONG",{});var CHr=n(Bie);ODo=r(CHr,"clip"),CHr.forEach(t),XDo=r(iye," \u2014 "),kj=s(iye,"A",{href:!0});var MHr=n(kj);zDo=r(MHr,"TFCLIPModel"),MHr.forEach(t),VDo=r(iye," (CLIP model)"),iye.forEach(t),WDo=i(x),o1=s(x,"LI",{});var dye=n(o1);xie=s(dye,"STRONG",{});var EHr=n(xie);QDo=r(EHr,"convbert"),EHr.forEach(t),HDo=r(dye," \u2014 "),Rj=s(dye,"A",{href:!0});var yHr=n(Rj);UDo=r(yHr,"TFConvBertModel"),yHr.forEach(t),JDo=r(dye," (ConvBERT model)"),dye.forEach(t),YDo=i(x),r1=s(x,"LI",{});var cye=n(r1);kie=s(cye,"STRONG",{});var wHr=n(kie);KDo=r(wHr,"ctrl"),wHr.forEach(t),ZDo=r(cye," \u2014 "),Sj=s(cye,"A",{href:!0});var AHr=n(Sj);eqo=r(AHr,"TFCTRLModel"),AHr.forEach(t),oqo=r(cye," (CTRL model)"),cye.forEach(t),rqo=i(x),t1=s(x,"LI",{});var mye=n(t1);Rie=s(mye,"STRONG",{});var LHr=n(Rie);tqo=r(LHr,"deberta"),LHr.forEach(t),aqo=r(mye," \u2014 "),Pj=s(mye,"A",{href:!0});var BHr=n(Pj);sqo=r(BHr,"TFDebertaModel"),BHr.forEach(t),nqo=r(mye," (DeBERTa model)"),mye.forEach(t),lqo=i(x),a1=s(x,"LI",{});var fye=n(a1);Sie=s(fye,"STRONG",{});var xHr=n(Sie);iqo=r(xHr,"deberta-v2"),xHr.forEach(t),dqo=r(fye," \u2014 "),$j=s(fye,"A",{href:!0});var kHr=n($j);cqo=r(kHr,"TFDebertaV2Model"),kHr.forEach(t),mqo=r(fye," (DeBERTa-v2 model)"),fye.forEach(t),fqo=i(x),s1=s(x,"LI",{});var gye=n(s1);Pie=s(gye,"STRONG",{});var RHr=n(Pie);gqo=r(RHr,"distilbert"),RHr.forEach(t),hqo=r(gye," \u2014 "),Ij=s(gye,"A",{href:!0});var SHr=n(Ij);uqo=r(SHr,"TFDistilBertModel"),SHr.forEach(t),pqo=r(gye," (DistilBERT model)"),gye.forEach(t),_qo=i(x),n1=s(x,"LI",{});var hye=n(n1);$ie=s(hye,"STRONG",{});var PHr=n($ie);bqo=r(PHr,"dpr"),PHr.forEach(t),vqo=r(hye," \u2014 "),jj=s(hye,"A",{href:!0});var $Hr=n(jj);Tqo=r($Hr,"TFDPRQuestionEncoder"),$Hr.forEach(t),Fqo=r(hye," (DPR model)"),hye.forEach(t),Cqo=i(x),l1=s(x,"LI",{});var uye=n(l1);Iie=s(uye,"STRONG",{});var IHr=n(Iie);Mqo=r(IHr,"electra"),IHr.forEach(t),Eqo=r(uye," \u2014 "),Nj=s(uye,"A",{href:!0});var jHr=n(Nj);yqo=r(jHr,"TFElectraModel"),jHr.forEach(t),wqo=r(uye," (ELECTRA model)"),uye.forEach(t),Aqo=i(x),i1=s(x,"LI",{});var pye=n(i1);jie=s(pye,"STRONG",{});var NHr=n(jie);Lqo=r(NHr,"flaubert"),NHr.forEach(t),Bqo=r(pye," \u2014 "),Dj=s(pye,"A",{href:!0});var DHr=n(Dj);xqo=r(DHr,"TFFlaubertModel"),DHr.forEach(t),kqo=r(pye," (FlauBERT model)"),pye.forEach(t),Rqo=i(x),Ln=s(x,"LI",{});var e7=n(Ln);Nie=s(e7,"STRONG",{});var qHr=n(Nie);Sqo=r(qHr,"funnel"),qHr.forEach(t),Pqo=r(e7," \u2014 "),qj=s(e7,"A",{href:!0});var GHr=n(qj);$qo=r(GHr,"TFFunnelModel"),GHr.forEach(t),Iqo=r(e7," or "),Gj=s(e7,"A",{href:!0});var OHr=n(Gj);jqo=r(OHr,"TFFunnelBaseModel"),OHr.forEach(t),Nqo=r(e7," (Funnel Transformer model)"),e7.forEach(t),Dqo=i(x),d1=s(x,"LI",{});var _ye=n(d1);Die=s(_ye,"STRONG",{});var XHr=n(Die);qqo=r(XHr,"gpt2"),XHr.forEach(t),Gqo=r(_ye," \u2014 "),Oj=s(_ye,"A",{href:!0});var zHr=n(Oj);Oqo=r(zHr,"TFGPT2Model"),zHr.forEach(t),Xqo=r(_ye," (OpenAI GPT-2 model)"),_ye.forEach(t),zqo=i(x),c1=s(x,"LI",{});var bye=n(c1);qie=s(bye,"STRONG",{});var VHr=n(qie);Vqo=r(VHr,"hubert"),VHr.forEach(t),Wqo=r(bye," \u2014 "),Xj=s(bye,"A",{href:!0});var WHr=n(Xj);Qqo=r(WHr,"TFHubertModel"),WHr.forEach(t),Hqo=r(bye," (Hubert model)"),bye.forEach(t),Uqo=i(x),m1=s(x,"LI",{});var vye=n(m1);Gie=s(vye,"STRONG",{});var QHr=n(Gie);Jqo=r(QHr,"layoutlm"),QHr.forEach(t),Yqo=r(vye," \u2014 "),zj=s(vye,"A",{href:!0});var HHr=n(zj);Kqo=r(HHr,"TFLayoutLMModel"),HHr.forEach(t),Zqo=r(vye," (LayoutLM model)"),vye.forEach(t),eGo=i(x),f1=s(x,"LI",{});var Tye=n(f1);Oie=s(Tye,"STRONG",{});var UHr=n(Oie);oGo=r(UHr,"led"),UHr.forEach(t),rGo=r(Tye," \u2014 "),Vj=s(Tye,"A",{href:!0});var JHr=n(Vj);tGo=r(JHr,"TFLEDModel"),JHr.forEach(t),aGo=r(Tye," (LED model)"),Tye.forEach(t),sGo=i(x),g1=s(x,"LI",{});var Fye=n(g1);Xie=s(Fye,"STRONG",{});var YHr=n(Xie);nGo=r(YHr,"longformer"),YHr.forEach(t),lGo=r(Fye," \u2014 "),Wj=s(Fye,"A",{href:!0});var KHr=n(Wj);iGo=r(KHr,"TFLongformerModel"),KHr.forEach(t),dGo=r(Fye," (Longformer model)"),Fye.forEach(t),cGo=i(x),h1=s(x,"LI",{});var Cye=n(h1);zie=s(Cye,"STRONG",{});var ZHr=n(zie);mGo=r(ZHr,"lxmert"),ZHr.forEach(t),fGo=r(Cye," \u2014 "),Qj=s(Cye,"A",{href:!0});var eUr=n(Qj);gGo=r(eUr,"TFLxmertModel"),eUr.forEach(t),hGo=r(Cye," (LXMERT model)"),Cye.forEach(t),uGo=i(x),u1=s(x,"LI",{});var Mye=n(u1);Vie=s(Mye,"STRONG",{});var oUr=n(Vie);pGo=r(oUr,"marian"),oUr.forEach(t),_Go=r(Mye," \u2014 "),Hj=s(Mye,"A",{href:!0});var rUr=n(Hj);bGo=r(rUr,"TFMarianModel"),rUr.forEach(t),vGo=r(Mye," (Marian model)"),Mye.forEach(t),TGo=i(x),p1=s(x,"LI",{});var Eye=n(p1);Wie=s(Eye,"STRONG",{});var tUr=n(Wie);FGo=r(tUr,"mbart"),tUr.forEach(t),CGo=r(Eye," \u2014 "),Uj=s(Eye,"A",{href:!0});var aUr=n(Uj);MGo=r(aUr,"TFMBartModel"),aUr.forEach(t),EGo=r(Eye," (mBART model)"),Eye.forEach(t),yGo=i(x),_1=s(x,"LI",{});var yye=n(_1);Qie=s(yye,"STRONG",{});var sUr=n(Qie);wGo=r(sUr,"mobilebert"),sUr.forEach(t),AGo=r(yye," \u2014 "),Jj=s(yye,"A",{href:!0});var nUr=n(Jj);LGo=r(nUr,"TFMobileBertModel"),nUr.forEach(t),BGo=r(yye," (MobileBERT model)"),yye.forEach(t),xGo=i(x),b1=s(x,"LI",{});var wye=n(b1);Hie=s(wye,"STRONG",{});var lUr=n(Hie);kGo=r(lUr,"mpnet"),lUr.forEach(t),RGo=r(wye," \u2014 "),Yj=s(wye,"A",{href:!0});var iUr=n(Yj);SGo=r(iUr,"TFMPNetModel"),iUr.forEach(t),PGo=r(wye," (MPNet model)"),wye.forEach(t),$Go=i(x),v1=s(x,"LI",{});var Aye=n(v1);Uie=s(Aye,"STRONG",{});var dUr=n(Uie);IGo=r(dUr,"mt5"),dUr.forEach(t),jGo=r(Aye," \u2014 "),Kj=s(Aye,"A",{href:!0});var cUr=n(Kj);NGo=r(cUr,"TFMT5Model"),cUr.forEach(t),DGo=r(Aye," (mT5 model)"),Aye.forEach(t),qGo=i(x),T1=s(x,"LI",{});var Lye=n(T1);Jie=s(Lye,"STRONG",{});var mUr=n(Jie);GGo=r(mUr,"openai-gpt"),mUr.forEach(t),OGo=r(Lye," \u2014 "),Zj=s(Lye,"A",{href:!0});var fUr=n(Zj);XGo=r(fUr,"TFOpenAIGPTModel"),fUr.forEach(t),zGo=r(Lye," (OpenAI GPT model)"),Lye.forEach(t),VGo=i(x),F1=s(x,"LI",{});var Bye=n(F1);Yie=s(Bye,"STRONG",{});var gUr=n(Yie);WGo=r(gUr,"pegasus"),gUr.forEach(t),QGo=r(Bye," \u2014 "),eN=s(Bye,"A",{href:!0});var hUr=n(eN);HGo=r(hUr,"TFPegasusModel"),hUr.forEach(t),UGo=r(Bye," (Pegasus model)"),Bye.forEach(t),JGo=i(x),C1=s(x,"LI",{});var xye=n(C1);Kie=s(xye,"STRONG",{});var uUr=n(Kie);YGo=r(uUr,"rembert"),uUr.forEach(t),KGo=r(xye," \u2014 "),oN=s(xye,"A",{href:!0});var pUr=n(oN);ZGo=r(pUr,"TFRemBertModel"),pUr.forEach(t),eOo=r(xye," (RemBERT model)"),xye.forEach(t),oOo=i(x),M1=s(x,"LI",{});var kye=n(M1);Zie=s(kye,"STRONG",{});var _Ur=n(Zie);rOo=r(_Ur,"roberta"),_Ur.forEach(t),tOo=r(kye," \u2014 "),rN=s(kye,"A",{href:!0});var bUr=n(rN);aOo=r(bUr,"TFRobertaModel"),bUr.forEach(t),sOo=r(kye," (RoBERTa model)"),kye.forEach(t),nOo=i(x),E1=s(x,"LI",{});var Rye=n(E1);ede=s(Rye,"STRONG",{});var vUr=n(ede);lOo=r(vUr,"roformer"),vUr.forEach(t),iOo=r(Rye," \u2014 "),tN=s(Rye,"A",{href:!0});var TUr=n(tN);dOo=r(TUr,"TFRoFormerModel"),TUr.forEach(t),cOo=r(Rye," (RoFormer model)"),Rye.forEach(t),mOo=i(x),y1=s(x,"LI",{});var Sye=n(y1);ode=s(Sye,"STRONG",{});var FUr=n(ode);fOo=r(FUr,"speech_to_text"),FUr.forEach(t),gOo=r(Sye," \u2014 "),aN=s(Sye,"A",{href:!0});var CUr=n(aN);hOo=r(CUr,"TFSpeech2TextModel"),CUr.forEach(t),uOo=r(Sye," (Speech2Text model)"),Sye.forEach(t),pOo=i(x),w1=s(x,"LI",{});var Pye=n(w1);rde=s(Pye,"STRONG",{});var MUr=n(rde);_Oo=r(MUr,"t5"),MUr.forEach(t),bOo=r(Pye," \u2014 "),sN=s(Pye,"A",{href:!0});var EUr=n(sN);vOo=r(EUr,"TFT5Model"),EUr.forEach(t),TOo=r(Pye," (T5 model)"),Pye.forEach(t),FOo=i(x),A1=s(x,"LI",{});var $ye=n(A1);tde=s($ye,"STRONG",{});var yUr=n(tde);COo=r(yUr,"tapas"),yUr.forEach(t),MOo=r($ye," \u2014 "),nN=s($ye,"A",{href:!0});var wUr=n(nN);EOo=r(wUr,"TFTapasModel"),wUr.forEach(t),yOo=r($ye," (TAPAS model)"),$ye.forEach(t),wOo=i(x),L1=s(x,"LI",{});var Iye=n(L1);ade=s(Iye,"STRONG",{});var AUr=n(ade);AOo=r(AUr,"transfo-xl"),AUr.forEach(t),LOo=r(Iye," \u2014 "),lN=s(Iye,"A",{href:!0});var LUr=n(lN);BOo=r(LUr,"TFTransfoXLModel"),LUr.forEach(t),xOo=r(Iye," (Transformer-XL model)"),Iye.forEach(t),kOo=i(x),B1=s(x,"LI",{});var jye=n(B1);sde=s(jye,"STRONG",{});var BUr=n(sde);ROo=r(BUr,"vit"),BUr.forEach(t),SOo=r(jye," \u2014 "),iN=s(jye,"A",{href:!0});var xUr=n(iN);POo=r(xUr,"TFViTModel"),xUr.forEach(t),$Oo=r(jye," (ViT model)"),jye.forEach(t),IOo=i(x),x1=s(x,"LI",{});var Nye=n(x1);nde=s(Nye,"STRONG",{});var kUr=n(nde);jOo=r(kUr,"wav2vec2"),kUr.forEach(t),NOo=r(Nye," \u2014 "),dN=s(Nye,"A",{href:!0});var RUr=n(dN);DOo=r(RUr,"TFWav2Vec2Model"),RUr.forEach(t),qOo=r(Nye," (Wav2Vec2 model)"),Nye.forEach(t),GOo=i(x),k1=s(x,"LI",{});var Dye=n(k1);lde=s(Dye,"STRONG",{});var SUr=n(lde);OOo=r(SUr,"xlm"),SUr.forEach(t),XOo=r(Dye," \u2014 "),cN=s(Dye,"A",{href:!0});var PUr=n(cN);zOo=r(PUr,"TFXLMModel"),PUr.forEach(t),VOo=r(Dye," (XLM model)"),Dye.forEach(t),WOo=i(x),R1=s(x,"LI",{});var qye=n(R1);ide=s(qye,"STRONG",{});var $Ur=n(ide);QOo=r($Ur,"xlm-roberta"),$Ur.forEach(t),HOo=r(qye," \u2014 "),mN=s(qye,"A",{href:!0});var IUr=n(mN);UOo=r(IUr,"TFXLMRobertaModel"),IUr.forEach(t),JOo=r(qye," (XLM-RoBERTa model)"),qye.forEach(t),YOo=i(x),S1=s(x,"LI",{});var Gye=n(S1);dde=s(Gye,"STRONG",{});var jUr=n(dde);KOo=r(jUr,"xlnet"),jUr.forEach(t),ZOo=r(Gye," \u2014 "),fN=s(Gye,"A",{href:!0});var NUr=n(fN);eXo=r(NUr,"TFXLNetModel"),NUr.forEach(t),oXo=r(Gye," (XLNet model)"),Gye.forEach(t),x.forEach(t),rXo=i(na),cde=s(na,"P",{});var DUr=n(cde);tXo=r(DUr,"Examples:"),DUr.forEach(t),aXo=i(na),f(Yw.$$.fragment,na),na.forEach(t),Tl.forEach(t),u7e=i(d),Vd=s(d,"H2",{class:!0});var T9e=n(Vd);P1=s(T9e,"A",{id:!0,class:!0,href:!0});var qUr=n(P1);mde=s(qUr,"SPAN",{});var GUr=n(mde);f(Kw.$$.fragment,GUr),GUr.forEach(t),qUr.forEach(t),sXo=i(T9e),fde=s(T9e,"SPAN",{});var OUr=n(fde);nXo=r(OUr,"TFAutoModelForPreTraining"),OUr.forEach(t),T9e.forEach(t),p7e=i(d),cr=s(d,"DIV",{class:!0});var Cl=n(cr);f(Zw.$$.fragment,Cl),lXo=i(Cl),Wd=s(Cl,"P",{});var RX=n(Wd);iXo=r(RX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),gde=s(RX,"CODE",{});var XUr=n(gde);dXo=r(XUr,"from_pretrained()"),XUr.forEach(t),cXo=r(RX,"class method or the "),hde=s(RX,"CODE",{});var zUr=n(hde);mXo=r(zUr,"from_config()"),zUr.forEach(t),fXo=r(RX,`class
method.`),RX.forEach(t),gXo=i(Cl),eA=s(Cl,"P",{});var F9e=n(eA);hXo=r(F9e,"This class cannot be instantiated directly using "),ude=s(F9e,"CODE",{});var VUr=n(ude);uXo=r(VUr,"__init__()"),VUr.forEach(t),pXo=r(F9e," (throws an error)."),F9e.forEach(t),_Xo=i(Cl),rt=s(Cl,"DIV",{class:!0});var Ml=n(rt);f(oA.$$.fragment,Ml),bXo=i(Ml),pde=s(Ml,"P",{});var WUr=n(pde);vXo=r(WUr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),WUr.forEach(t),TXo=i(Ml),Qd=s(Ml,"P",{});var SX=n(Qd);FXo=r(SX,`Note:
Loading a model from its configuration file does `),_de=s(SX,"STRONG",{});var QUr=n(_de);CXo=r(QUr,"not"),QUr.forEach(t),MXo=r(SX,` load the model weights. It only affects the
model\u2019s configuration. Use `),bde=s(SX,"CODE",{});var HUr=n(bde);EXo=r(HUr,"from_pretrained()"),HUr.forEach(t),yXo=r(SX,"to load the model weights."),SX.forEach(t),wXo=i(Ml),vde=s(Ml,"P",{});var UUr=n(vde);AXo=r(UUr,"Examples:"),UUr.forEach(t),LXo=i(Ml),f(rA.$$.fragment,Ml),Ml.forEach(t),BXo=i(Cl),fo=s(Cl,"DIV",{class:!0});var la=n(fo);f(tA.$$.fragment,la),xXo=i(la),Tde=s(la,"P",{});var JUr=n(Tde);kXo=r(JUr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),JUr.forEach(t),RXo=i(la),ts=s(la,"P",{});var o3=n(ts);SXo=r(o3,"The model class to instantiate is selected based on the "),Fde=s(o3,"CODE",{});var YUr=n(Fde);PXo=r(YUr,"model_type"),YUr.forEach(t),$Xo=r(o3,` property of the config object (either
passed as an argument or loaded from `),Cde=s(o3,"CODE",{});var KUr=n(Cde);IXo=r(KUr,"pretrained_model_name_or_path"),KUr.forEach(t),jXo=r(o3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mde=s(o3,"CODE",{});var ZUr=n(Mde);NXo=r(ZUr,"pretrained_model_name_or_path"),ZUr.forEach(t),DXo=r(o3,":"),o3.forEach(t),qXo=i(la),H=s(la,"UL",{});var U=n(H);$1=s(U,"LI",{});var Oye=n($1);Ede=s(Oye,"STRONG",{});var eJr=n(Ede);GXo=r(eJr,"albert"),eJr.forEach(t),OXo=r(Oye," \u2014 "),gN=s(Oye,"A",{href:!0});var oJr=n(gN);XXo=r(oJr,"TFAlbertForPreTraining"),oJr.forEach(t),zXo=r(Oye," (ALBERT model)"),Oye.forEach(t),VXo=i(U),I1=s(U,"LI",{});var Xye=n(I1);yde=s(Xye,"STRONG",{});var rJr=n(yde);WXo=r(rJr,"bart"),rJr.forEach(t),QXo=r(Xye," \u2014 "),hN=s(Xye,"A",{href:!0});var tJr=n(hN);HXo=r(tJr,"TFBartForConditionalGeneration"),tJr.forEach(t),UXo=r(Xye," (BART model)"),Xye.forEach(t),JXo=i(U),j1=s(U,"LI",{});var zye=n(j1);wde=s(zye,"STRONG",{});var aJr=n(wde);YXo=r(aJr,"bert"),aJr.forEach(t),KXo=r(zye," \u2014 "),uN=s(zye,"A",{href:!0});var sJr=n(uN);ZXo=r(sJr,"TFBertForPreTraining"),sJr.forEach(t),ezo=r(zye," (BERT model)"),zye.forEach(t),ozo=i(U),N1=s(U,"LI",{});var Vye=n(N1);Ade=s(Vye,"STRONG",{});var nJr=n(Ade);rzo=r(nJr,"camembert"),nJr.forEach(t),tzo=r(Vye," \u2014 "),pN=s(Vye,"A",{href:!0});var lJr=n(pN);azo=r(lJr,"TFCamembertForMaskedLM"),lJr.forEach(t),szo=r(Vye," (CamemBERT model)"),Vye.forEach(t),nzo=i(U),D1=s(U,"LI",{});var Wye=n(D1);Lde=s(Wye,"STRONG",{});var iJr=n(Lde);lzo=r(iJr,"ctrl"),iJr.forEach(t),izo=r(Wye," \u2014 "),_N=s(Wye,"A",{href:!0});var dJr=n(_N);dzo=r(dJr,"TFCTRLLMHeadModel"),dJr.forEach(t),czo=r(Wye," (CTRL model)"),Wye.forEach(t),mzo=i(U),q1=s(U,"LI",{});var Qye=n(q1);Bde=s(Qye,"STRONG",{});var cJr=n(Bde);fzo=r(cJr,"distilbert"),cJr.forEach(t),gzo=r(Qye," \u2014 "),bN=s(Qye,"A",{href:!0});var mJr=n(bN);hzo=r(mJr,"TFDistilBertForMaskedLM"),mJr.forEach(t),uzo=r(Qye," (DistilBERT model)"),Qye.forEach(t),pzo=i(U),G1=s(U,"LI",{});var Hye=n(G1);xde=s(Hye,"STRONG",{});var fJr=n(xde);_zo=r(fJr,"electra"),fJr.forEach(t),bzo=r(Hye," \u2014 "),vN=s(Hye,"A",{href:!0});var gJr=n(vN);vzo=r(gJr,"TFElectraForPreTraining"),gJr.forEach(t),Tzo=r(Hye," (ELECTRA model)"),Hye.forEach(t),Fzo=i(U),O1=s(U,"LI",{});var Uye=n(O1);kde=s(Uye,"STRONG",{});var hJr=n(kde);Czo=r(hJr,"flaubert"),hJr.forEach(t),Mzo=r(Uye," \u2014 "),TN=s(Uye,"A",{href:!0});var uJr=n(TN);Ezo=r(uJr,"TFFlaubertWithLMHeadModel"),uJr.forEach(t),yzo=r(Uye," (FlauBERT model)"),Uye.forEach(t),wzo=i(U),X1=s(U,"LI",{});var Jye=n(X1);Rde=s(Jye,"STRONG",{});var pJr=n(Rde);Azo=r(pJr,"funnel"),pJr.forEach(t),Lzo=r(Jye," \u2014 "),FN=s(Jye,"A",{href:!0});var _Jr=n(FN);Bzo=r(_Jr,"TFFunnelForPreTraining"),_Jr.forEach(t),xzo=r(Jye," (Funnel Transformer model)"),Jye.forEach(t),kzo=i(U),z1=s(U,"LI",{});var Yye=n(z1);Sde=s(Yye,"STRONG",{});var bJr=n(Sde);Rzo=r(bJr,"gpt2"),bJr.forEach(t),Szo=r(Yye," \u2014 "),CN=s(Yye,"A",{href:!0});var vJr=n(CN);Pzo=r(vJr,"TFGPT2LMHeadModel"),vJr.forEach(t),$zo=r(Yye," (OpenAI GPT-2 model)"),Yye.forEach(t),Izo=i(U),V1=s(U,"LI",{});var Kye=n(V1);Pde=s(Kye,"STRONG",{});var TJr=n(Pde);jzo=r(TJr,"layoutlm"),TJr.forEach(t),Nzo=r(Kye," \u2014 "),MN=s(Kye,"A",{href:!0});var FJr=n(MN);Dzo=r(FJr,"TFLayoutLMForMaskedLM"),FJr.forEach(t),qzo=r(Kye," (LayoutLM model)"),Kye.forEach(t),Gzo=i(U),W1=s(U,"LI",{});var Zye=n(W1);$de=s(Zye,"STRONG",{});var CJr=n($de);Ozo=r(CJr,"lxmert"),CJr.forEach(t),Xzo=r(Zye," \u2014 "),EN=s(Zye,"A",{href:!0});var MJr=n(EN);zzo=r(MJr,"TFLxmertForPreTraining"),MJr.forEach(t),Vzo=r(Zye," (LXMERT model)"),Zye.forEach(t),Wzo=i(U),Q1=s(U,"LI",{});var ewe=n(Q1);Ide=s(ewe,"STRONG",{});var EJr=n(Ide);Qzo=r(EJr,"mobilebert"),EJr.forEach(t),Hzo=r(ewe," \u2014 "),yN=s(ewe,"A",{href:!0});var yJr=n(yN);Uzo=r(yJr,"TFMobileBertForPreTraining"),yJr.forEach(t),Jzo=r(ewe," (MobileBERT model)"),ewe.forEach(t),Yzo=i(U),H1=s(U,"LI",{});var owe=n(H1);jde=s(owe,"STRONG",{});var wJr=n(jde);Kzo=r(wJr,"mpnet"),wJr.forEach(t),Zzo=r(owe," \u2014 "),wN=s(owe,"A",{href:!0});var AJr=n(wN);eVo=r(AJr,"TFMPNetForMaskedLM"),AJr.forEach(t),oVo=r(owe," (MPNet model)"),owe.forEach(t),rVo=i(U),U1=s(U,"LI",{});var rwe=n(U1);Nde=s(rwe,"STRONG",{});var LJr=n(Nde);tVo=r(LJr,"openai-gpt"),LJr.forEach(t),aVo=r(rwe," \u2014 "),AN=s(rwe,"A",{href:!0});var BJr=n(AN);sVo=r(BJr,"TFOpenAIGPTLMHeadModel"),BJr.forEach(t),nVo=r(rwe," (OpenAI GPT model)"),rwe.forEach(t),lVo=i(U),J1=s(U,"LI",{});var twe=n(J1);Dde=s(twe,"STRONG",{});var xJr=n(Dde);iVo=r(xJr,"roberta"),xJr.forEach(t),dVo=r(twe," \u2014 "),LN=s(twe,"A",{href:!0});var kJr=n(LN);cVo=r(kJr,"TFRobertaForMaskedLM"),kJr.forEach(t),mVo=r(twe," (RoBERTa model)"),twe.forEach(t),fVo=i(U),Y1=s(U,"LI",{});var awe=n(Y1);qde=s(awe,"STRONG",{});var RJr=n(qde);gVo=r(RJr,"t5"),RJr.forEach(t),hVo=r(awe," \u2014 "),BN=s(awe,"A",{href:!0});var SJr=n(BN);uVo=r(SJr,"TFT5ForConditionalGeneration"),SJr.forEach(t),pVo=r(awe," (T5 model)"),awe.forEach(t),_Vo=i(U),K1=s(U,"LI",{});var swe=n(K1);Gde=s(swe,"STRONG",{});var PJr=n(Gde);bVo=r(PJr,"tapas"),PJr.forEach(t),vVo=r(swe," \u2014 "),xN=s(swe,"A",{href:!0});var $Jr=n(xN);TVo=r($Jr,"TFTapasForMaskedLM"),$Jr.forEach(t),FVo=r(swe," (TAPAS model)"),swe.forEach(t),CVo=i(U),Z1=s(U,"LI",{});var nwe=n(Z1);Ode=s(nwe,"STRONG",{});var IJr=n(Ode);MVo=r(IJr,"transfo-xl"),IJr.forEach(t),EVo=r(nwe," \u2014 "),kN=s(nwe,"A",{href:!0});var jJr=n(kN);yVo=r(jJr,"TFTransfoXLLMHeadModel"),jJr.forEach(t),wVo=r(nwe," (Transformer-XL model)"),nwe.forEach(t),AVo=i(U),eF=s(U,"LI",{});var lwe=n(eF);Xde=s(lwe,"STRONG",{});var NJr=n(Xde);LVo=r(NJr,"xlm"),NJr.forEach(t),BVo=r(lwe," \u2014 "),RN=s(lwe,"A",{href:!0});var DJr=n(RN);xVo=r(DJr,"TFXLMWithLMHeadModel"),DJr.forEach(t),kVo=r(lwe," (XLM model)"),lwe.forEach(t),RVo=i(U),oF=s(U,"LI",{});var iwe=n(oF);zde=s(iwe,"STRONG",{});var qJr=n(zde);SVo=r(qJr,"xlm-roberta"),qJr.forEach(t),PVo=r(iwe," \u2014 "),SN=s(iwe,"A",{href:!0});var GJr=n(SN);$Vo=r(GJr,"TFXLMRobertaForMaskedLM"),GJr.forEach(t),IVo=r(iwe," (XLM-RoBERTa model)"),iwe.forEach(t),jVo=i(U),rF=s(U,"LI",{});var dwe=n(rF);Vde=s(dwe,"STRONG",{});var OJr=n(Vde);NVo=r(OJr,"xlnet"),OJr.forEach(t),DVo=r(dwe," \u2014 "),PN=s(dwe,"A",{href:!0});var XJr=n(PN);qVo=r(XJr,"TFXLNetLMHeadModel"),XJr.forEach(t),GVo=r(dwe," (XLNet model)"),dwe.forEach(t),U.forEach(t),OVo=i(la),Wde=s(la,"P",{});var zJr=n(Wde);XVo=r(zJr,"Examples:"),zJr.forEach(t),zVo=i(la),f(aA.$$.fragment,la),la.forEach(t),Cl.forEach(t),_7e=i(d),Hd=s(d,"H2",{class:!0});var C9e=n(Hd);tF=s(C9e,"A",{id:!0,class:!0,href:!0});var VJr=n(tF);Qde=s(VJr,"SPAN",{});var WJr=n(Qde);f(sA.$$.fragment,WJr),WJr.forEach(t),VJr.forEach(t),VVo=i(C9e),Hde=s(C9e,"SPAN",{});var QJr=n(Hde);WVo=r(QJr,"TFAutoModelForCausalLM"),QJr.forEach(t),C9e.forEach(t),b7e=i(d),mr=s(d,"DIV",{class:!0});var El=n(mr);f(nA.$$.fragment,El),QVo=i(El),Ud=s(El,"P",{});var PX=n(Ud);HVo=r(PX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Ude=s(PX,"CODE",{});var HJr=n(Ude);UVo=r(HJr,"from_pretrained()"),HJr.forEach(t),JVo=r(PX,"class method or the "),Jde=s(PX,"CODE",{});var UJr=n(Jde);YVo=r(UJr,"from_config()"),UJr.forEach(t),KVo=r(PX,`class
method.`),PX.forEach(t),ZVo=i(El),lA=s(El,"P",{});var M9e=n(lA);eWo=r(M9e,"This class cannot be instantiated directly using "),Yde=s(M9e,"CODE",{});var JJr=n(Yde);oWo=r(JJr,"__init__()"),JJr.forEach(t),rWo=r(M9e," (throws an error)."),M9e.forEach(t),tWo=i(El),tt=s(El,"DIV",{class:!0});var yl=n(tt);f(iA.$$.fragment,yl),aWo=i(yl),Kde=s(yl,"P",{});var YJr=n(Kde);sWo=r(YJr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),YJr.forEach(t),nWo=i(yl),Jd=s(yl,"P",{});var $X=n(Jd);lWo=r($X,`Note:
Loading a model from its configuration file does `),Zde=s($X,"STRONG",{});var KJr=n(Zde);iWo=r(KJr,"not"),KJr.forEach(t),dWo=r($X,` load the model weights. It only affects the
model\u2019s configuration. Use `),ece=s($X,"CODE",{});var ZJr=n(ece);cWo=r(ZJr,"from_pretrained()"),ZJr.forEach(t),mWo=r($X,"to load the model weights."),$X.forEach(t),fWo=i(yl),oce=s(yl,"P",{});var eYr=n(oce);gWo=r(eYr,"Examples:"),eYr.forEach(t),hWo=i(yl),f(dA.$$.fragment,yl),yl.forEach(t),uWo=i(El),go=s(El,"DIV",{class:!0});var ia=n(go);f(cA.$$.fragment,ia),pWo=i(ia),rce=s(ia,"P",{});var oYr=n(rce);_Wo=r(oYr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),oYr.forEach(t),bWo=i(ia),as=s(ia,"P",{});var r3=n(as);vWo=r(r3,"The model class to instantiate is selected based on the "),tce=s(r3,"CODE",{});var rYr=n(tce);TWo=r(rYr,"model_type"),rYr.forEach(t),FWo=r(r3,` property of the config object (either
passed as an argument or loaded from `),ace=s(r3,"CODE",{});var tYr=n(ace);CWo=r(tYr,"pretrained_model_name_or_path"),tYr.forEach(t),MWo=r(r3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sce=s(r3,"CODE",{});var aYr=n(sce);EWo=r(aYr,"pretrained_model_name_or_path"),aYr.forEach(t),yWo=r(r3,":"),r3.forEach(t),wWo=i(ia),he=s(ia,"UL",{});var Ce=n(he);aF=s(Ce,"LI",{});var cwe=n(aF);nce=s(cwe,"STRONG",{});var sYr=n(nce);AWo=r(sYr,"bert"),sYr.forEach(t),LWo=r(cwe," \u2014 "),$N=s(cwe,"A",{href:!0});var nYr=n($N);BWo=r(nYr,"TFBertLMHeadModel"),nYr.forEach(t),xWo=r(cwe," (BERT model)"),cwe.forEach(t),kWo=i(Ce),sF=s(Ce,"LI",{});var mwe=n(sF);lce=s(mwe,"STRONG",{});var lYr=n(lce);RWo=r(lYr,"ctrl"),lYr.forEach(t),SWo=r(mwe," \u2014 "),IN=s(mwe,"A",{href:!0});var iYr=n(IN);PWo=r(iYr,"TFCTRLLMHeadModel"),iYr.forEach(t),$Wo=r(mwe," (CTRL model)"),mwe.forEach(t),IWo=i(Ce),nF=s(Ce,"LI",{});var fwe=n(nF);ice=s(fwe,"STRONG",{});var dYr=n(ice);jWo=r(dYr,"gpt2"),dYr.forEach(t),NWo=r(fwe," \u2014 "),jN=s(fwe,"A",{href:!0});var cYr=n(jN);DWo=r(cYr,"TFGPT2LMHeadModel"),cYr.forEach(t),qWo=r(fwe," (OpenAI GPT-2 model)"),fwe.forEach(t),GWo=i(Ce),lF=s(Ce,"LI",{});var gwe=n(lF);dce=s(gwe,"STRONG",{});var mYr=n(dce);OWo=r(mYr,"openai-gpt"),mYr.forEach(t),XWo=r(gwe," \u2014 "),NN=s(gwe,"A",{href:!0});var fYr=n(NN);zWo=r(fYr,"TFOpenAIGPTLMHeadModel"),fYr.forEach(t),VWo=r(gwe," (OpenAI GPT model)"),gwe.forEach(t),WWo=i(Ce),iF=s(Ce,"LI",{});var hwe=n(iF);cce=s(hwe,"STRONG",{});var gYr=n(cce);QWo=r(gYr,"rembert"),gYr.forEach(t),HWo=r(hwe," \u2014 "),DN=s(hwe,"A",{href:!0});var hYr=n(DN);UWo=r(hYr,"TFRemBertForCausalLM"),hYr.forEach(t),JWo=r(hwe," (RemBERT model)"),hwe.forEach(t),YWo=i(Ce),dF=s(Ce,"LI",{});var uwe=n(dF);mce=s(uwe,"STRONG",{});var uYr=n(mce);KWo=r(uYr,"roberta"),uYr.forEach(t),ZWo=r(uwe," \u2014 "),qN=s(uwe,"A",{href:!0});var pYr=n(qN);eQo=r(pYr,"TFRobertaForCausalLM"),pYr.forEach(t),oQo=r(uwe," (RoBERTa model)"),uwe.forEach(t),rQo=i(Ce),cF=s(Ce,"LI",{});var pwe=n(cF);fce=s(pwe,"STRONG",{});var _Yr=n(fce);tQo=r(_Yr,"roformer"),_Yr.forEach(t),aQo=r(pwe," \u2014 "),GN=s(pwe,"A",{href:!0});var bYr=n(GN);sQo=r(bYr,"TFRoFormerForCausalLM"),bYr.forEach(t),nQo=r(pwe," (RoFormer model)"),pwe.forEach(t),lQo=i(Ce),mF=s(Ce,"LI",{});var _we=n(mF);gce=s(_we,"STRONG",{});var vYr=n(gce);iQo=r(vYr,"transfo-xl"),vYr.forEach(t),dQo=r(_we," \u2014 "),ON=s(_we,"A",{href:!0});var TYr=n(ON);cQo=r(TYr,"TFTransfoXLLMHeadModel"),TYr.forEach(t),mQo=r(_we," (Transformer-XL model)"),_we.forEach(t),fQo=i(Ce),fF=s(Ce,"LI",{});var bwe=n(fF);hce=s(bwe,"STRONG",{});var FYr=n(hce);gQo=r(FYr,"xlm"),FYr.forEach(t),hQo=r(bwe," \u2014 "),XN=s(bwe,"A",{href:!0});var CYr=n(XN);uQo=r(CYr,"TFXLMWithLMHeadModel"),CYr.forEach(t),pQo=r(bwe," (XLM model)"),bwe.forEach(t),_Qo=i(Ce),gF=s(Ce,"LI",{});var vwe=n(gF);uce=s(vwe,"STRONG",{});var MYr=n(uce);bQo=r(MYr,"xlnet"),MYr.forEach(t),vQo=r(vwe," \u2014 "),zN=s(vwe,"A",{href:!0});var EYr=n(zN);TQo=r(EYr,"TFXLNetLMHeadModel"),EYr.forEach(t),FQo=r(vwe," (XLNet model)"),vwe.forEach(t),Ce.forEach(t),CQo=i(ia),pce=s(ia,"P",{});var yYr=n(pce);MQo=r(yYr,"Examples:"),yYr.forEach(t),EQo=i(ia),f(mA.$$.fragment,ia),ia.forEach(t),El.forEach(t),v7e=i(d),Yd=s(d,"H2",{class:!0});var E9e=n(Yd);hF=s(E9e,"A",{id:!0,class:!0,href:!0});var wYr=n(hF);_ce=s(wYr,"SPAN",{});var AYr=n(_ce);f(fA.$$.fragment,AYr),AYr.forEach(t),wYr.forEach(t),yQo=i(E9e),bce=s(E9e,"SPAN",{});var LYr=n(bce);wQo=r(LYr,"TFAutoModelForImageClassification"),LYr.forEach(t),E9e.forEach(t),T7e=i(d),fr=s(d,"DIV",{class:!0});var wl=n(fr);f(gA.$$.fragment,wl),AQo=i(wl),Kd=s(wl,"P",{});var IX=n(Kd);LQo=r(IX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),vce=s(IX,"CODE",{});var BYr=n(vce);BQo=r(BYr,"from_pretrained()"),BYr.forEach(t),xQo=r(IX,"class method or the "),Tce=s(IX,"CODE",{});var xYr=n(Tce);kQo=r(xYr,"from_config()"),xYr.forEach(t),RQo=r(IX,`class
method.`),IX.forEach(t),SQo=i(wl),hA=s(wl,"P",{});var y9e=n(hA);PQo=r(y9e,"This class cannot be instantiated directly using "),Fce=s(y9e,"CODE",{});var kYr=n(Fce);$Qo=r(kYr,"__init__()"),kYr.forEach(t),IQo=r(y9e," (throws an error)."),y9e.forEach(t),jQo=i(wl),at=s(wl,"DIV",{class:!0});var Al=n(at);f(uA.$$.fragment,Al),NQo=i(Al),Cce=s(Al,"P",{});var RYr=n(Cce);DQo=r(RYr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),RYr.forEach(t),qQo=i(Al),Zd=s(Al,"P",{});var jX=n(Zd);GQo=r(jX,`Note:
Loading a model from its configuration file does `),Mce=s(jX,"STRONG",{});var SYr=n(Mce);OQo=r(SYr,"not"),SYr.forEach(t),XQo=r(jX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ece=s(jX,"CODE",{});var PYr=n(Ece);zQo=r(PYr,"from_pretrained()"),PYr.forEach(t),VQo=r(jX,"to load the model weights."),jX.forEach(t),WQo=i(Al),yce=s(Al,"P",{});var $Yr=n(yce);QQo=r($Yr,"Examples:"),$Yr.forEach(t),HQo=i(Al),f(pA.$$.fragment,Al),Al.forEach(t),UQo=i(wl),ho=s(wl,"DIV",{class:!0});var da=n(ho);f(_A.$$.fragment,da),JQo=i(da),wce=s(da,"P",{});var IYr=n(wce);YQo=r(IYr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),IYr.forEach(t),KQo=i(da),ss=s(da,"P",{});var t3=n(ss);ZQo=r(t3,"The model class to instantiate is selected based on the "),Ace=s(t3,"CODE",{});var jYr=n(Ace);eHo=r(jYr,"model_type"),jYr.forEach(t),oHo=r(t3,` property of the config object (either
passed as an argument or loaded from `),Lce=s(t3,"CODE",{});var NYr=n(Lce);rHo=r(NYr,"pretrained_model_name_or_path"),NYr.forEach(t),tHo=r(t3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bce=s(t3,"CODE",{});var DYr=n(Bce);aHo=r(DYr,"pretrained_model_name_or_path"),DYr.forEach(t),sHo=r(t3,":"),t3.forEach(t),nHo=i(da),xce=s(da,"UL",{});var qYr=n(xce);uF=s(qYr,"LI",{});var Twe=n(uF);kce=s(Twe,"STRONG",{});var GYr=n(kce);lHo=r(GYr,"vit"),GYr.forEach(t),iHo=r(Twe," \u2014 "),VN=s(Twe,"A",{href:!0});var OYr=n(VN);dHo=r(OYr,"TFViTForImageClassification"),OYr.forEach(t),cHo=r(Twe," (ViT model)"),Twe.forEach(t),qYr.forEach(t),mHo=i(da),Rce=s(da,"P",{});var XYr=n(Rce);fHo=r(XYr,"Examples:"),XYr.forEach(t),gHo=i(da),f(bA.$$.fragment,da),da.forEach(t),wl.forEach(t),F7e=i(d),ec=s(d,"H2",{class:!0});var w9e=n(ec);pF=s(w9e,"A",{id:!0,class:!0,href:!0});var zYr=n(pF);Sce=s(zYr,"SPAN",{});var VYr=n(Sce);f(vA.$$.fragment,VYr),VYr.forEach(t),zYr.forEach(t),hHo=i(w9e),Pce=s(w9e,"SPAN",{});var WYr=n(Pce);uHo=r(WYr,"TFAutoModelForMaskedLM"),WYr.forEach(t),w9e.forEach(t),C7e=i(d),gr=s(d,"DIV",{class:!0});var Ll=n(gr);f(TA.$$.fragment,Ll),pHo=i(Ll),oc=s(Ll,"P",{});var NX=n(oc);_Ho=r(NX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),$ce=s(NX,"CODE",{});var QYr=n($ce);bHo=r(QYr,"from_pretrained()"),QYr.forEach(t),vHo=r(NX,"class method or the "),Ice=s(NX,"CODE",{});var HYr=n(Ice);THo=r(HYr,"from_config()"),HYr.forEach(t),FHo=r(NX,`class
method.`),NX.forEach(t),CHo=i(Ll),FA=s(Ll,"P",{});var A9e=n(FA);MHo=r(A9e,"This class cannot be instantiated directly using "),jce=s(A9e,"CODE",{});var UYr=n(jce);EHo=r(UYr,"__init__()"),UYr.forEach(t),yHo=r(A9e," (throws an error)."),A9e.forEach(t),wHo=i(Ll),st=s(Ll,"DIV",{class:!0});var Bl=n(st);f(CA.$$.fragment,Bl),AHo=i(Bl),Nce=s(Bl,"P",{});var JYr=n(Nce);LHo=r(JYr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),JYr.forEach(t),BHo=i(Bl),rc=s(Bl,"P",{});var DX=n(rc);xHo=r(DX,`Note:
Loading a model from its configuration file does `),Dce=s(DX,"STRONG",{});var YYr=n(Dce);kHo=r(YYr,"not"),YYr.forEach(t),RHo=r(DX,` load the model weights. It only affects the
model\u2019s configuration. Use `),qce=s(DX,"CODE",{});var KYr=n(qce);SHo=r(KYr,"from_pretrained()"),KYr.forEach(t),PHo=r(DX,"to load the model weights."),DX.forEach(t),$Ho=i(Bl),Gce=s(Bl,"P",{});var ZYr=n(Gce);IHo=r(ZYr,"Examples:"),ZYr.forEach(t),jHo=i(Bl),f(MA.$$.fragment,Bl),Bl.forEach(t),NHo=i(Ll),uo=s(Ll,"DIV",{class:!0});var ca=n(uo);f(EA.$$.fragment,ca),DHo=i(ca),Oce=s(ca,"P",{});var eKr=n(Oce);qHo=r(eKr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),eKr.forEach(t),GHo=i(ca),ns=s(ca,"P",{});var a3=n(ns);OHo=r(a3,"The model class to instantiate is selected based on the "),Xce=s(a3,"CODE",{});var oKr=n(Xce);XHo=r(oKr,"model_type"),oKr.forEach(t),zHo=r(a3,` property of the config object (either
passed as an argument or loaded from `),zce=s(a3,"CODE",{});var rKr=n(zce);VHo=r(rKr,"pretrained_model_name_or_path"),rKr.forEach(t),WHo=r(a3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vce=s(a3,"CODE",{});var tKr=n(Vce);QHo=r(tKr,"pretrained_model_name_or_path"),tKr.forEach(t),HHo=r(a3,":"),a3.forEach(t),UHo=i(ca),Y=s(ca,"UL",{});var ee=n(Y);_F=s(ee,"LI",{});var Fwe=n(_F);Wce=s(Fwe,"STRONG",{});var aKr=n(Wce);JHo=r(aKr,"albert"),aKr.forEach(t),YHo=r(Fwe," \u2014 "),WN=s(Fwe,"A",{href:!0});var sKr=n(WN);KHo=r(sKr,"TFAlbertForMaskedLM"),sKr.forEach(t),ZHo=r(Fwe," (ALBERT model)"),Fwe.forEach(t),eUo=i(ee),bF=s(ee,"LI",{});var Cwe=n(bF);Qce=s(Cwe,"STRONG",{});var nKr=n(Qce);oUo=r(nKr,"bert"),nKr.forEach(t),rUo=r(Cwe," \u2014 "),QN=s(Cwe,"A",{href:!0});var lKr=n(QN);tUo=r(lKr,"TFBertForMaskedLM"),lKr.forEach(t),aUo=r(Cwe," (BERT model)"),Cwe.forEach(t),sUo=i(ee),vF=s(ee,"LI",{});var Mwe=n(vF);Hce=s(Mwe,"STRONG",{});var iKr=n(Hce);nUo=r(iKr,"camembert"),iKr.forEach(t),lUo=r(Mwe," \u2014 "),HN=s(Mwe,"A",{href:!0});var dKr=n(HN);iUo=r(dKr,"TFCamembertForMaskedLM"),dKr.forEach(t),dUo=r(Mwe," (CamemBERT model)"),Mwe.forEach(t),cUo=i(ee),TF=s(ee,"LI",{});var Ewe=n(TF);Uce=s(Ewe,"STRONG",{});var cKr=n(Uce);mUo=r(cKr,"convbert"),cKr.forEach(t),fUo=r(Ewe," \u2014 "),UN=s(Ewe,"A",{href:!0});var mKr=n(UN);gUo=r(mKr,"TFConvBertForMaskedLM"),mKr.forEach(t),hUo=r(Ewe," (ConvBERT model)"),Ewe.forEach(t),uUo=i(ee),FF=s(ee,"LI",{});var ywe=n(FF);Jce=s(ywe,"STRONG",{});var fKr=n(Jce);pUo=r(fKr,"deberta"),fKr.forEach(t),_Uo=r(ywe," \u2014 "),JN=s(ywe,"A",{href:!0});var gKr=n(JN);bUo=r(gKr,"TFDebertaForMaskedLM"),gKr.forEach(t),vUo=r(ywe," (DeBERTa model)"),ywe.forEach(t),TUo=i(ee),CF=s(ee,"LI",{});var wwe=n(CF);Yce=s(wwe,"STRONG",{});var hKr=n(Yce);FUo=r(hKr,"deberta-v2"),hKr.forEach(t),CUo=r(wwe," \u2014 "),YN=s(wwe,"A",{href:!0});var uKr=n(YN);MUo=r(uKr,"TFDebertaV2ForMaskedLM"),uKr.forEach(t),EUo=r(wwe," (DeBERTa-v2 model)"),wwe.forEach(t),yUo=i(ee),MF=s(ee,"LI",{});var Awe=n(MF);Kce=s(Awe,"STRONG",{});var pKr=n(Kce);wUo=r(pKr,"distilbert"),pKr.forEach(t),AUo=r(Awe," \u2014 "),KN=s(Awe,"A",{href:!0});var _Kr=n(KN);LUo=r(_Kr,"TFDistilBertForMaskedLM"),_Kr.forEach(t),BUo=r(Awe," (DistilBERT model)"),Awe.forEach(t),xUo=i(ee),EF=s(ee,"LI",{});var Lwe=n(EF);Zce=s(Lwe,"STRONG",{});var bKr=n(Zce);kUo=r(bKr,"electra"),bKr.forEach(t),RUo=r(Lwe," \u2014 "),ZN=s(Lwe,"A",{href:!0});var vKr=n(ZN);SUo=r(vKr,"TFElectraForMaskedLM"),vKr.forEach(t),PUo=r(Lwe," (ELECTRA model)"),Lwe.forEach(t),$Uo=i(ee),yF=s(ee,"LI",{});var Bwe=n(yF);eme=s(Bwe,"STRONG",{});var TKr=n(eme);IUo=r(TKr,"flaubert"),TKr.forEach(t),jUo=r(Bwe," \u2014 "),eD=s(Bwe,"A",{href:!0});var FKr=n(eD);NUo=r(FKr,"TFFlaubertWithLMHeadModel"),FKr.forEach(t),DUo=r(Bwe," (FlauBERT model)"),Bwe.forEach(t),qUo=i(ee),wF=s(ee,"LI",{});var xwe=n(wF);ome=s(xwe,"STRONG",{});var CKr=n(ome);GUo=r(CKr,"funnel"),CKr.forEach(t),OUo=r(xwe," \u2014 "),oD=s(xwe,"A",{href:!0});var MKr=n(oD);XUo=r(MKr,"TFFunnelForMaskedLM"),MKr.forEach(t),zUo=r(xwe," (Funnel Transformer model)"),xwe.forEach(t),VUo=i(ee),AF=s(ee,"LI",{});var kwe=n(AF);rme=s(kwe,"STRONG",{});var EKr=n(rme);WUo=r(EKr,"layoutlm"),EKr.forEach(t),QUo=r(kwe," \u2014 "),rD=s(kwe,"A",{href:!0});var yKr=n(rD);HUo=r(yKr,"TFLayoutLMForMaskedLM"),yKr.forEach(t),UUo=r(kwe," (LayoutLM model)"),kwe.forEach(t),JUo=i(ee),LF=s(ee,"LI",{});var Rwe=n(LF);tme=s(Rwe,"STRONG",{});var wKr=n(tme);YUo=r(wKr,"longformer"),wKr.forEach(t),KUo=r(Rwe," \u2014 "),tD=s(Rwe,"A",{href:!0});var AKr=n(tD);ZUo=r(AKr,"TFLongformerForMaskedLM"),AKr.forEach(t),eJo=r(Rwe," (Longformer model)"),Rwe.forEach(t),oJo=i(ee),BF=s(ee,"LI",{});var Swe=n(BF);ame=s(Swe,"STRONG",{});var LKr=n(ame);rJo=r(LKr,"mobilebert"),LKr.forEach(t),tJo=r(Swe," \u2014 "),aD=s(Swe,"A",{href:!0});var BKr=n(aD);aJo=r(BKr,"TFMobileBertForMaskedLM"),BKr.forEach(t),sJo=r(Swe," (MobileBERT model)"),Swe.forEach(t),nJo=i(ee),xF=s(ee,"LI",{});var Pwe=n(xF);sme=s(Pwe,"STRONG",{});var xKr=n(sme);lJo=r(xKr,"mpnet"),xKr.forEach(t),iJo=r(Pwe," \u2014 "),sD=s(Pwe,"A",{href:!0});var kKr=n(sD);dJo=r(kKr,"TFMPNetForMaskedLM"),kKr.forEach(t),cJo=r(Pwe," (MPNet model)"),Pwe.forEach(t),mJo=i(ee),kF=s(ee,"LI",{});var $we=n(kF);nme=s($we,"STRONG",{});var RKr=n(nme);fJo=r(RKr,"rembert"),RKr.forEach(t),gJo=r($we," \u2014 "),nD=s($we,"A",{href:!0});var SKr=n(nD);hJo=r(SKr,"TFRemBertForMaskedLM"),SKr.forEach(t),uJo=r($we," (RemBERT model)"),$we.forEach(t),pJo=i(ee),RF=s(ee,"LI",{});var Iwe=n(RF);lme=s(Iwe,"STRONG",{});var PKr=n(lme);_Jo=r(PKr,"roberta"),PKr.forEach(t),bJo=r(Iwe," \u2014 "),lD=s(Iwe,"A",{href:!0});var $Kr=n(lD);vJo=r($Kr,"TFRobertaForMaskedLM"),$Kr.forEach(t),TJo=r(Iwe," (RoBERTa model)"),Iwe.forEach(t),FJo=i(ee),SF=s(ee,"LI",{});var jwe=n(SF);ime=s(jwe,"STRONG",{});var IKr=n(ime);CJo=r(IKr,"roformer"),IKr.forEach(t),MJo=r(jwe," \u2014 "),iD=s(jwe,"A",{href:!0});var jKr=n(iD);EJo=r(jKr,"TFRoFormerForMaskedLM"),jKr.forEach(t),yJo=r(jwe," (RoFormer model)"),jwe.forEach(t),wJo=i(ee),PF=s(ee,"LI",{});var Nwe=n(PF);dme=s(Nwe,"STRONG",{});var NKr=n(dme);AJo=r(NKr,"tapas"),NKr.forEach(t),LJo=r(Nwe," \u2014 "),dD=s(Nwe,"A",{href:!0});var DKr=n(dD);BJo=r(DKr,"TFTapasForMaskedLM"),DKr.forEach(t),xJo=r(Nwe," (TAPAS model)"),Nwe.forEach(t),kJo=i(ee),$F=s(ee,"LI",{});var Dwe=n($F);cme=s(Dwe,"STRONG",{});var qKr=n(cme);RJo=r(qKr,"xlm"),qKr.forEach(t),SJo=r(Dwe," \u2014 "),cD=s(Dwe,"A",{href:!0});var GKr=n(cD);PJo=r(GKr,"TFXLMWithLMHeadModel"),GKr.forEach(t),$Jo=r(Dwe," (XLM model)"),Dwe.forEach(t),IJo=i(ee),IF=s(ee,"LI",{});var qwe=n(IF);mme=s(qwe,"STRONG",{});var OKr=n(mme);jJo=r(OKr,"xlm-roberta"),OKr.forEach(t),NJo=r(qwe," \u2014 "),mD=s(qwe,"A",{href:!0});var XKr=n(mD);DJo=r(XKr,"TFXLMRobertaForMaskedLM"),XKr.forEach(t),qJo=r(qwe," (XLM-RoBERTa model)"),qwe.forEach(t),ee.forEach(t),GJo=i(ca),fme=s(ca,"P",{});var zKr=n(fme);OJo=r(zKr,"Examples:"),zKr.forEach(t),XJo=i(ca),f(yA.$$.fragment,ca),ca.forEach(t),Ll.forEach(t),M7e=i(d),tc=s(d,"H2",{class:!0});var L9e=n(tc);jF=s(L9e,"A",{id:!0,class:!0,href:!0});var VKr=n(jF);gme=s(VKr,"SPAN",{});var WKr=n(gme);f(wA.$$.fragment,WKr),WKr.forEach(t),VKr.forEach(t),zJo=i(L9e),hme=s(L9e,"SPAN",{});var QKr=n(hme);VJo=r(QKr,"TFAutoModelForSeq2SeqLM"),QKr.forEach(t),L9e.forEach(t),E7e=i(d),hr=s(d,"DIV",{class:!0});var xl=n(hr);f(AA.$$.fragment,xl),WJo=i(xl),ac=s(xl,"P",{});var qX=n(ac);QJo=r(qX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ume=s(qX,"CODE",{});var HKr=n(ume);HJo=r(HKr,"from_pretrained()"),HKr.forEach(t),UJo=r(qX,"class method or the "),pme=s(qX,"CODE",{});var UKr=n(pme);JJo=r(UKr,"from_config()"),UKr.forEach(t),YJo=r(qX,`class
method.`),qX.forEach(t),KJo=i(xl),LA=s(xl,"P",{});var B9e=n(LA);ZJo=r(B9e,"This class cannot be instantiated directly using "),_me=s(B9e,"CODE",{});var JKr=n(_me);eYo=r(JKr,"__init__()"),JKr.forEach(t),oYo=r(B9e," (throws an error)."),B9e.forEach(t),rYo=i(xl),nt=s(xl,"DIV",{class:!0});var kl=n(nt);f(BA.$$.fragment,kl),tYo=i(kl),bme=s(kl,"P",{});var YKr=n(bme);aYo=r(YKr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),YKr.forEach(t),sYo=i(kl),sc=s(kl,"P",{});var GX=n(sc);nYo=r(GX,`Note:
Loading a model from its configuration file does `),vme=s(GX,"STRONG",{});var KKr=n(vme);lYo=r(KKr,"not"),KKr.forEach(t),iYo=r(GX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Tme=s(GX,"CODE",{});var ZKr=n(Tme);dYo=r(ZKr,"from_pretrained()"),ZKr.forEach(t),cYo=r(GX,"to load the model weights."),GX.forEach(t),mYo=i(kl),Fme=s(kl,"P",{});var eZr=n(Fme);fYo=r(eZr,"Examples:"),eZr.forEach(t),gYo=i(kl),f(xA.$$.fragment,kl),kl.forEach(t),hYo=i(xl),po=s(xl,"DIV",{class:!0});var ma=n(po);f(kA.$$.fragment,ma),uYo=i(ma),Cme=s(ma,"P",{});var oZr=n(Cme);pYo=r(oZr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),oZr.forEach(t),_Yo=i(ma),ls=s(ma,"P",{});var s3=n(ls);bYo=r(s3,"The model class to instantiate is selected based on the "),Mme=s(s3,"CODE",{});var rZr=n(Mme);vYo=r(rZr,"model_type"),rZr.forEach(t),TYo=r(s3,` property of the config object (either
passed as an argument or loaded from `),Eme=s(s3,"CODE",{});var tZr=n(Eme);FYo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),CYo=r(s3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yme=s(s3,"CODE",{});var aZr=n(yme);MYo=r(aZr,"pretrained_model_name_or_path"),aZr.forEach(t),EYo=r(s3,":"),s3.forEach(t),yYo=i(ma),ue=s(ma,"UL",{});var Me=n(ue);NF=s(Me,"LI",{});var Gwe=n(NF);wme=s(Gwe,"STRONG",{});var sZr=n(wme);wYo=r(sZr,"bart"),sZr.forEach(t),AYo=r(Gwe," \u2014 "),fD=s(Gwe,"A",{href:!0});var nZr=n(fD);LYo=r(nZr,"TFBartForConditionalGeneration"),nZr.forEach(t),BYo=r(Gwe," (BART model)"),Gwe.forEach(t),xYo=i(Me),DF=s(Me,"LI",{});var Owe=n(DF);Ame=s(Owe,"STRONG",{});var lZr=n(Ame);kYo=r(lZr,"blenderbot"),lZr.forEach(t),RYo=r(Owe," \u2014 "),gD=s(Owe,"A",{href:!0});var iZr=n(gD);SYo=r(iZr,"TFBlenderbotForConditionalGeneration"),iZr.forEach(t),PYo=r(Owe," (Blenderbot model)"),Owe.forEach(t),$Yo=i(Me),qF=s(Me,"LI",{});var Xwe=n(qF);Lme=s(Xwe,"STRONG",{});var dZr=n(Lme);IYo=r(dZr,"blenderbot-small"),dZr.forEach(t),jYo=r(Xwe," \u2014 "),hD=s(Xwe,"A",{href:!0});var cZr=n(hD);NYo=r(cZr,"TFBlenderbotSmallForConditionalGeneration"),cZr.forEach(t),DYo=r(Xwe," (BlenderbotSmall model)"),Xwe.forEach(t),qYo=i(Me),GF=s(Me,"LI",{});var zwe=n(GF);Bme=s(zwe,"STRONG",{});var mZr=n(Bme);GYo=r(mZr,"encoder-decoder"),mZr.forEach(t),OYo=r(zwe," \u2014 "),uD=s(zwe,"A",{href:!0});var fZr=n(uD);XYo=r(fZr,"TFEncoderDecoderModel"),fZr.forEach(t),zYo=r(zwe," (Encoder decoder model)"),zwe.forEach(t),VYo=i(Me),OF=s(Me,"LI",{});var Vwe=n(OF);xme=s(Vwe,"STRONG",{});var gZr=n(xme);WYo=r(gZr,"led"),gZr.forEach(t),QYo=r(Vwe," \u2014 "),pD=s(Vwe,"A",{href:!0});var hZr=n(pD);HYo=r(hZr,"TFLEDForConditionalGeneration"),hZr.forEach(t),UYo=r(Vwe," (LED model)"),Vwe.forEach(t),JYo=i(Me),XF=s(Me,"LI",{});var Wwe=n(XF);kme=s(Wwe,"STRONG",{});var uZr=n(kme);YYo=r(uZr,"marian"),uZr.forEach(t),KYo=r(Wwe," \u2014 "),_D=s(Wwe,"A",{href:!0});var pZr=n(_D);ZYo=r(pZr,"TFMarianMTModel"),pZr.forEach(t),eKo=r(Wwe," (Marian model)"),Wwe.forEach(t),oKo=i(Me),zF=s(Me,"LI",{});var Qwe=n(zF);Rme=s(Qwe,"STRONG",{});var _Zr=n(Rme);rKo=r(_Zr,"mbart"),_Zr.forEach(t),tKo=r(Qwe," \u2014 "),bD=s(Qwe,"A",{href:!0});var bZr=n(bD);aKo=r(bZr,"TFMBartForConditionalGeneration"),bZr.forEach(t),sKo=r(Qwe," (mBART model)"),Qwe.forEach(t),nKo=i(Me),VF=s(Me,"LI",{});var Hwe=n(VF);Sme=s(Hwe,"STRONG",{});var vZr=n(Sme);lKo=r(vZr,"mt5"),vZr.forEach(t),iKo=r(Hwe," \u2014 "),vD=s(Hwe,"A",{href:!0});var TZr=n(vD);dKo=r(TZr,"TFMT5ForConditionalGeneration"),TZr.forEach(t),cKo=r(Hwe," (mT5 model)"),Hwe.forEach(t),mKo=i(Me),WF=s(Me,"LI",{});var Uwe=n(WF);Pme=s(Uwe,"STRONG",{});var FZr=n(Pme);fKo=r(FZr,"pegasus"),FZr.forEach(t),gKo=r(Uwe," \u2014 "),TD=s(Uwe,"A",{href:!0});var CZr=n(TD);hKo=r(CZr,"TFPegasusForConditionalGeneration"),CZr.forEach(t),uKo=r(Uwe," (Pegasus model)"),Uwe.forEach(t),pKo=i(Me),QF=s(Me,"LI",{});var Jwe=n(QF);$me=s(Jwe,"STRONG",{});var MZr=n($me);_Ko=r(MZr,"t5"),MZr.forEach(t),bKo=r(Jwe," \u2014 "),FD=s(Jwe,"A",{href:!0});var EZr=n(FD);vKo=r(EZr,"TFT5ForConditionalGeneration"),EZr.forEach(t),TKo=r(Jwe," (T5 model)"),Jwe.forEach(t),Me.forEach(t),FKo=i(ma),Ime=s(ma,"P",{});var yZr=n(Ime);CKo=r(yZr,"Examples:"),yZr.forEach(t),MKo=i(ma),f(RA.$$.fragment,ma),ma.forEach(t),xl.forEach(t),y7e=i(d),nc=s(d,"H2",{class:!0});var x9e=n(nc);HF=s(x9e,"A",{id:!0,class:!0,href:!0});var wZr=n(HF);jme=s(wZr,"SPAN",{});var AZr=n(jme);f(SA.$$.fragment,AZr),AZr.forEach(t),wZr.forEach(t),EKo=i(x9e),Nme=s(x9e,"SPAN",{});var LZr=n(Nme);yKo=r(LZr,"TFAutoModelForSequenceClassification"),LZr.forEach(t),x9e.forEach(t),w7e=i(d),ur=s(d,"DIV",{class:!0});var Rl=n(ur);f(PA.$$.fragment,Rl),wKo=i(Rl),lc=s(Rl,"P",{});var OX=n(lc);AKo=r(OX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Dme=s(OX,"CODE",{});var BZr=n(Dme);LKo=r(BZr,"from_pretrained()"),BZr.forEach(t),BKo=r(OX,"class method or the "),qme=s(OX,"CODE",{});var xZr=n(qme);xKo=r(xZr,"from_config()"),xZr.forEach(t),kKo=r(OX,`class
method.`),OX.forEach(t),RKo=i(Rl),$A=s(Rl,"P",{});var k9e=n($A);SKo=r(k9e,"This class cannot be instantiated directly using "),Gme=s(k9e,"CODE",{});var kZr=n(Gme);PKo=r(kZr,"__init__()"),kZr.forEach(t),$Ko=r(k9e," (throws an error)."),k9e.forEach(t),IKo=i(Rl),lt=s(Rl,"DIV",{class:!0});var Sl=n(lt);f(IA.$$.fragment,Sl),jKo=i(Sl),Ome=s(Sl,"P",{});var RZr=n(Ome);NKo=r(RZr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),RZr.forEach(t),DKo=i(Sl),ic=s(Sl,"P",{});var XX=n(ic);qKo=r(XX,`Note:
Loading a model from its configuration file does `),Xme=s(XX,"STRONG",{});var SZr=n(Xme);GKo=r(SZr,"not"),SZr.forEach(t),OKo=r(XX,` load the model weights. It only affects the
model\u2019s configuration. Use `),zme=s(XX,"CODE",{});var PZr=n(zme);XKo=r(PZr,"from_pretrained()"),PZr.forEach(t),zKo=r(XX,"to load the model weights."),XX.forEach(t),VKo=i(Sl),Vme=s(Sl,"P",{});var $Zr=n(Vme);WKo=r($Zr,"Examples:"),$Zr.forEach(t),QKo=i(Sl),f(jA.$$.fragment,Sl),Sl.forEach(t),HKo=i(Rl),_o=s(Rl,"DIV",{class:!0});var fa=n(_o);f(NA.$$.fragment,fa),UKo=i(fa),Wme=s(fa,"P",{});var IZr=n(Wme);JKo=r(IZr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),IZr.forEach(t),YKo=i(fa),is=s(fa,"P",{});var n3=n(is);KKo=r(n3,"The model class to instantiate is selected based on the "),Qme=s(n3,"CODE",{});var jZr=n(Qme);ZKo=r(jZr,"model_type"),jZr.forEach(t),eZo=r(n3,` property of the config object (either
passed as an argument or loaded from `),Hme=s(n3,"CODE",{});var NZr=n(Hme);oZo=r(NZr,"pretrained_model_name_or_path"),NZr.forEach(t),rZo=r(n3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ume=s(n3,"CODE",{});var DZr=n(Ume);tZo=r(DZr,"pretrained_model_name_or_path"),DZr.forEach(t),aZo=r(n3,":"),n3.forEach(t),sZo=i(fa),X=s(fa,"UL",{});var W=n(X);UF=s(W,"LI",{});var Ywe=n(UF);Jme=s(Ywe,"STRONG",{});var qZr=n(Jme);nZo=r(qZr,"albert"),qZr.forEach(t),lZo=r(Ywe," \u2014 "),CD=s(Ywe,"A",{href:!0});var GZr=n(CD);iZo=r(GZr,"TFAlbertForSequenceClassification"),GZr.forEach(t),dZo=r(Ywe," (ALBERT model)"),Ywe.forEach(t),cZo=i(W),JF=s(W,"LI",{});var Kwe=n(JF);Yme=s(Kwe,"STRONG",{});var OZr=n(Yme);mZo=r(OZr,"bert"),OZr.forEach(t),fZo=r(Kwe," \u2014 "),MD=s(Kwe,"A",{href:!0});var XZr=n(MD);gZo=r(XZr,"TFBertForSequenceClassification"),XZr.forEach(t),hZo=r(Kwe," (BERT model)"),Kwe.forEach(t),uZo=i(W),YF=s(W,"LI",{});var Zwe=n(YF);Kme=s(Zwe,"STRONG",{});var zZr=n(Kme);pZo=r(zZr,"camembert"),zZr.forEach(t),_Zo=r(Zwe," \u2014 "),ED=s(Zwe,"A",{href:!0});var VZr=n(ED);bZo=r(VZr,"TFCamembertForSequenceClassification"),VZr.forEach(t),vZo=r(Zwe," (CamemBERT model)"),Zwe.forEach(t),TZo=i(W),KF=s(W,"LI",{});var eAe=n(KF);Zme=s(eAe,"STRONG",{});var WZr=n(Zme);FZo=r(WZr,"convbert"),WZr.forEach(t),CZo=r(eAe," \u2014 "),yD=s(eAe,"A",{href:!0});var QZr=n(yD);MZo=r(QZr,"TFConvBertForSequenceClassification"),QZr.forEach(t),EZo=r(eAe," (ConvBERT model)"),eAe.forEach(t),yZo=i(W),ZF=s(W,"LI",{});var oAe=n(ZF);efe=s(oAe,"STRONG",{});var HZr=n(efe);wZo=r(HZr,"ctrl"),HZr.forEach(t),AZo=r(oAe," \u2014 "),wD=s(oAe,"A",{href:!0});var UZr=n(wD);LZo=r(UZr,"TFCTRLForSequenceClassification"),UZr.forEach(t),BZo=r(oAe," (CTRL model)"),oAe.forEach(t),xZo=i(W),eC=s(W,"LI",{});var rAe=n(eC);ofe=s(rAe,"STRONG",{});var JZr=n(ofe);kZo=r(JZr,"deberta"),JZr.forEach(t),RZo=r(rAe," \u2014 "),AD=s(rAe,"A",{href:!0});var YZr=n(AD);SZo=r(YZr,"TFDebertaForSequenceClassification"),YZr.forEach(t),PZo=r(rAe," (DeBERTa model)"),rAe.forEach(t),$Zo=i(W),oC=s(W,"LI",{});var tAe=n(oC);rfe=s(tAe,"STRONG",{});var KZr=n(rfe);IZo=r(KZr,"deberta-v2"),KZr.forEach(t),jZo=r(tAe," \u2014 "),LD=s(tAe,"A",{href:!0});var ZZr=n(LD);NZo=r(ZZr,"TFDebertaV2ForSequenceClassification"),ZZr.forEach(t),DZo=r(tAe," (DeBERTa-v2 model)"),tAe.forEach(t),qZo=i(W),rC=s(W,"LI",{});var aAe=n(rC);tfe=s(aAe,"STRONG",{});var eet=n(tfe);GZo=r(eet,"distilbert"),eet.forEach(t),OZo=r(aAe," \u2014 "),BD=s(aAe,"A",{href:!0});var oet=n(BD);XZo=r(oet,"TFDistilBertForSequenceClassification"),oet.forEach(t),zZo=r(aAe," (DistilBERT model)"),aAe.forEach(t),VZo=i(W),tC=s(W,"LI",{});var sAe=n(tC);afe=s(sAe,"STRONG",{});var ret=n(afe);WZo=r(ret,"electra"),ret.forEach(t),QZo=r(sAe," \u2014 "),xD=s(sAe,"A",{href:!0});var tet=n(xD);HZo=r(tet,"TFElectraForSequenceClassification"),tet.forEach(t),UZo=r(sAe," (ELECTRA model)"),sAe.forEach(t),JZo=i(W),aC=s(W,"LI",{});var nAe=n(aC);sfe=s(nAe,"STRONG",{});var aet=n(sfe);YZo=r(aet,"flaubert"),aet.forEach(t),KZo=r(nAe," \u2014 "),kD=s(nAe,"A",{href:!0});var set=n(kD);ZZo=r(set,"TFFlaubertForSequenceClassification"),set.forEach(t),eer=r(nAe," (FlauBERT model)"),nAe.forEach(t),oer=i(W),sC=s(W,"LI",{});var lAe=n(sC);nfe=s(lAe,"STRONG",{});var net=n(nfe);rer=r(net,"funnel"),net.forEach(t),ter=r(lAe," \u2014 "),RD=s(lAe,"A",{href:!0});var iet=n(RD);aer=r(iet,"TFFunnelForSequenceClassification"),iet.forEach(t),ser=r(lAe," (Funnel Transformer model)"),lAe.forEach(t),ner=i(W),nC=s(W,"LI",{});var iAe=n(nC);lfe=s(iAe,"STRONG",{});var det=n(lfe);ler=r(det,"gpt2"),det.forEach(t),ier=r(iAe," \u2014 "),SD=s(iAe,"A",{href:!0});var cet=n(SD);der=r(cet,"TFGPT2ForSequenceClassification"),cet.forEach(t),cer=r(iAe," (OpenAI GPT-2 model)"),iAe.forEach(t),mer=i(W),lC=s(W,"LI",{});var dAe=n(lC);ife=s(dAe,"STRONG",{});var met=n(ife);fer=r(met,"layoutlm"),met.forEach(t),ger=r(dAe," \u2014 "),PD=s(dAe,"A",{href:!0});var fet=n(PD);her=r(fet,"TFLayoutLMForSequenceClassification"),fet.forEach(t),uer=r(dAe," (LayoutLM model)"),dAe.forEach(t),per=i(W),iC=s(W,"LI",{});var cAe=n(iC);dfe=s(cAe,"STRONG",{});var get=n(dfe);_er=r(get,"longformer"),get.forEach(t),ber=r(cAe," \u2014 "),$D=s(cAe,"A",{href:!0});var het=n($D);ver=r(het,"TFLongformerForSequenceClassification"),het.forEach(t),Ter=r(cAe," (Longformer model)"),cAe.forEach(t),Fer=i(W),dC=s(W,"LI",{});var mAe=n(dC);cfe=s(mAe,"STRONG",{});var uet=n(cfe);Cer=r(uet,"mobilebert"),uet.forEach(t),Mer=r(mAe," \u2014 "),ID=s(mAe,"A",{href:!0});var pet=n(ID);Eer=r(pet,"TFMobileBertForSequenceClassification"),pet.forEach(t),yer=r(mAe," (MobileBERT model)"),mAe.forEach(t),wer=i(W),cC=s(W,"LI",{});var fAe=n(cC);mfe=s(fAe,"STRONG",{});var _et=n(mfe);Aer=r(_et,"mpnet"),_et.forEach(t),Ler=r(fAe," \u2014 "),jD=s(fAe,"A",{href:!0});var bet=n(jD);Ber=r(bet,"TFMPNetForSequenceClassification"),bet.forEach(t),xer=r(fAe," (MPNet model)"),fAe.forEach(t),ker=i(W),mC=s(W,"LI",{});var gAe=n(mC);ffe=s(gAe,"STRONG",{});var vet=n(ffe);Rer=r(vet,"openai-gpt"),vet.forEach(t),Ser=r(gAe," \u2014 "),ND=s(gAe,"A",{href:!0});var Tet=n(ND);Per=r(Tet,"TFOpenAIGPTForSequenceClassification"),Tet.forEach(t),$er=r(gAe," (OpenAI GPT model)"),gAe.forEach(t),Ier=i(W),fC=s(W,"LI",{});var hAe=n(fC);gfe=s(hAe,"STRONG",{});var Fet=n(gfe);jer=r(Fet,"rembert"),Fet.forEach(t),Ner=r(hAe," \u2014 "),DD=s(hAe,"A",{href:!0});var Cet=n(DD);Der=r(Cet,"TFRemBertForSequenceClassification"),Cet.forEach(t),qer=r(hAe," (RemBERT model)"),hAe.forEach(t),Ger=i(W),gC=s(W,"LI",{});var uAe=n(gC);hfe=s(uAe,"STRONG",{});var Met=n(hfe);Oer=r(Met,"roberta"),Met.forEach(t),Xer=r(uAe," \u2014 "),qD=s(uAe,"A",{href:!0});var Eet=n(qD);zer=r(Eet,"TFRobertaForSequenceClassification"),Eet.forEach(t),Ver=r(uAe," (RoBERTa model)"),uAe.forEach(t),Wer=i(W),hC=s(W,"LI",{});var pAe=n(hC);ufe=s(pAe,"STRONG",{});var yet=n(ufe);Qer=r(yet,"roformer"),yet.forEach(t),Her=r(pAe," \u2014 "),GD=s(pAe,"A",{href:!0});var wet=n(GD);Uer=r(wet,"TFRoFormerForSequenceClassification"),wet.forEach(t),Jer=r(pAe," (RoFormer model)"),pAe.forEach(t),Yer=i(W),uC=s(W,"LI",{});var _Ae=n(uC);pfe=s(_Ae,"STRONG",{});var Aet=n(pfe);Ker=r(Aet,"tapas"),Aet.forEach(t),Zer=r(_Ae," \u2014 "),OD=s(_Ae,"A",{href:!0});var Let=n(OD);eor=r(Let,"TFTapasForSequenceClassification"),Let.forEach(t),oor=r(_Ae," (TAPAS model)"),_Ae.forEach(t),ror=i(W),pC=s(W,"LI",{});var bAe=n(pC);_fe=s(bAe,"STRONG",{});var Bet=n(_fe);tor=r(Bet,"transfo-xl"),Bet.forEach(t),aor=r(bAe," \u2014 "),XD=s(bAe,"A",{href:!0});var xet=n(XD);sor=r(xet,"TFTransfoXLForSequenceClassification"),xet.forEach(t),nor=r(bAe," (Transformer-XL model)"),bAe.forEach(t),lor=i(W),_C=s(W,"LI",{});var vAe=n(_C);bfe=s(vAe,"STRONG",{});var ket=n(bfe);ior=r(ket,"xlm"),ket.forEach(t),dor=r(vAe," \u2014 "),zD=s(vAe,"A",{href:!0});var Ret=n(zD);cor=r(Ret,"TFXLMForSequenceClassification"),Ret.forEach(t),mor=r(vAe," (XLM model)"),vAe.forEach(t),gor=i(W),bC=s(W,"LI",{});var TAe=n(bC);vfe=s(TAe,"STRONG",{});var Set=n(vfe);hor=r(Set,"xlm-roberta"),Set.forEach(t),uor=r(TAe," \u2014 "),VD=s(TAe,"A",{href:!0});var Pet=n(VD);por=r(Pet,"TFXLMRobertaForSequenceClassification"),Pet.forEach(t),_or=r(TAe," (XLM-RoBERTa model)"),TAe.forEach(t),bor=i(W),vC=s(W,"LI",{});var FAe=n(vC);Tfe=s(FAe,"STRONG",{});var $et=n(Tfe);vor=r($et,"xlnet"),$et.forEach(t),Tor=r(FAe," \u2014 "),WD=s(FAe,"A",{href:!0});var Iet=n(WD);For=r(Iet,"TFXLNetForSequenceClassification"),Iet.forEach(t),Cor=r(FAe," (XLNet model)"),FAe.forEach(t),W.forEach(t),Mor=i(fa),Ffe=s(fa,"P",{});var jet=n(Ffe);Eor=r(jet,"Examples:"),jet.forEach(t),yor=i(fa),f(DA.$$.fragment,fa),fa.forEach(t),Rl.forEach(t),A7e=i(d),dc=s(d,"H2",{class:!0});var R9e=n(dc);TC=s(R9e,"A",{id:!0,class:!0,href:!0});var Net=n(TC);Cfe=s(Net,"SPAN",{});var Det=n(Cfe);f(qA.$$.fragment,Det),Det.forEach(t),Net.forEach(t),wor=i(R9e),Mfe=s(R9e,"SPAN",{});var qet=n(Mfe);Aor=r(qet,"TFAutoModelForMultipleChoice"),qet.forEach(t),R9e.forEach(t),L7e=i(d),pr=s(d,"DIV",{class:!0});var Pl=n(pr);f(GA.$$.fragment,Pl),Lor=i(Pl),cc=s(Pl,"P",{});var zX=n(cc);Bor=r(zX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Efe=s(zX,"CODE",{});var Get=n(Efe);xor=r(Get,"from_pretrained()"),Get.forEach(t),kor=r(zX,"class method or the "),yfe=s(zX,"CODE",{});var Oet=n(yfe);Ror=r(Oet,"from_config()"),Oet.forEach(t),Sor=r(zX,`class
method.`),zX.forEach(t),Por=i(Pl),OA=s(Pl,"P",{});var S9e=n(OA);$or=r(S9e,"This class cannot be instantiated directly using "),wfe=s(S9e,"CODE",{});var Xet=n(wfe);Ior=r(Xet,"__init__()"),Xet.forEach(t),jor=r(S9e," (throws an error)."),S9e.forEach(t),Nor=i(Pl),it=s(Pl,"DIV",{class:!0});var $l=n(it);f(XA.$$.fragment,$l),Dor=i($l),Afe=s($l,"P",{});var zet=n(Afe);qor=r(zet,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),zet.forEach(t),Gor=i($l),mc=s($l,"P",{});var VX=n(mc);Oor=r(VX,`Note:
Loading a model from its configuration file does `),Lfe=s(VX,"STRONG",{});var Vet=n(Lfe);Xor=r(Vet,"not"),Vet.forEach(t),zor=r(VX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bfe=s(VX,"CODE",{});var Wet=n(Bfe);Vor=r(Wet,"from_pretrained()"),Wet.forEach(t),Wor=r(VX,"to load the model weights."),VX.forEach(t),Qor=i($l),xfe=s($l,"P",{});var Qet=n(xfe);Hor=r(Qet,"Examples:"),Qet.forEach(t),Uor=i($l),f(zA.$$.fragment,$l),$l.forEach(t),Jor=i(Pl),bo=s(Pl,"DIV",{class:!0});var ga=n(bo);f(VA.$$.fragment,ga),Yor=i(ga),kfe=s(ga,"P",{});var Het=n(kfe);Kor=r(Het,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Het.forEach(t),Zor=i(ga),ds=s(ga,"P",{});var l3=n(ds);err=r(l3,"The model class to instantiate is selected based on the "),Rfe=s(l3,"CODE",{});var Uet=n(Rfe);orr=r(Uet,"model_type"),Uet.forEach(t),rrr=r(l3,` property of the config object (either
passed as an argument or loaded from `),Sfe=s(l3,"CODE",{});var Jet=n(Sfe);trr=r(Jet,"pretrained_model_name_or_path"),Jet.forEach(t),arr=r(l3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pfe=s(l3,"CODE",{});var Yet=n(Pfe);srr=r(Yet,"pretrained_model_name_or_path"),Yet.forEach(t),nrr=r(l3,":"),l3.forEach(t),lrr=i(ga),te=s(ga,"UL",{});var ae=n(te);FC=s(ae,"LI",{});var CAe=n(FC);$fe=s(CAe,"STRONG",{});var Ket=n($fe);irr=r(Ket,"albert"),Ket.forEach(t),drr=r(CAe," \u2014 "),QD=s(CAe,"A",{href:!0});var Zet=n(QD);crr=r(Zet,"TFAlbertForMultipleChoice"),Zet.forEach(t),mrr=r(CAe," (ALBERT model)"),CAe.forEach(t),frr=i(ae),CC=s(ae,"LI",{});var MAe=n(CC);Ife=s(MAe,"STRONG",{});var eot=n(Ife);grr=r(eot,"bert"),eot.forEach(t),hrr=r(MAe," \u2014 "),HD=s(MAe,"A",{href:!0});var oot=n(HD);urr=r(oot,"TFBertForMultipleChoice"),oot.forEach(t),prr=r(MAe," (BERT model)"),MAe.forEach(t),_rr=i(ae),MC=s(ae,"LI",{});var EAe=n(MC);jfe=s(EAe,"STRONG",{});var rot=n(jfe);brr=r(rot,"camembert"),rot.forEach(t),vrr=r(EAe," \u2014 "),UD=s(EAe,"A",{href:!0});var tot=n(UD);Trr=r(tot,"TFCamembertForMultipleChoice"),tot.forEach(t),Frr=r(EAe," (CamemBERT model)"),EAe.forEach(t),Crr=i(ae),EC=s(ae,"LI",{});var yAe=n(EC);Nfe=s(yAe,"STRONG",{});var aot=n(Nfe);Mrr=r(aot,"convbert"),aot.forEach(t),Err=r(yAe," \u2014 "),JD=s(yAe,"A",{href:!0});var sot=n(JD);yrr=r(sot,"TFConvBertForMultipleChoice"),sot.forEach(t),wrr=r(yAe," (ConvBERT model)"),yAe.forEach(t),Arr=i(ae),yC=s(ae,"LI",{});var wAe=n(yC);Dfe=s(wAe,"STRONG",{});var not=n(Dfe);Lrr=r(not,"distilbert"),not.forEach(t),Brr=r(wAe," \u2014 "),YD=s(wAe,"A",{href:!0});var lot=n(YD);xrr=r(lot,"TFDistilBertForMultipleChoice"),lot.forEach(t),krr=r(wAe," (DistilBERT model)"),wAe.forEach(t),Rrr=i(ae),wC=s(ae,"LI",{});var AAe=n(wC);qfe=s(AAe,"STRONG",{});var iot=n(qfe);Srr=r(iot,"electra"),iot.forEach(t),Prr=r(AAe," \u2014 "),KD=s(AAe,"A",{href:!0});var dot=n(KD);$rr=r(dot,"TFElectraForMultipleChoice"),dot.forEach(t),Irr=r(AAe," (ELECTRA model)"),AAe.forEach(t),jrr=i(ae),AC=s(ae,"LI",{});var LAe=n(AC);Gfe=s(LAe,"STRONG",{});var cot=n(Gfe);Nrr=r(cot,"flaubert"),cot.forEach(t),Drr=r(LAe," \u2014 "),ZD=s(LAe,"A",{href:!0});var mot=n(ZD);qrr=r(mot,"TFFlaubertForMultipleChoice"),mot.forEach(t),Grr=r(LAe," (FlauBERT model)"),LAe.forEach(t),Orr=i(ae),LC=s(ae,"LI",{});var BAe=n(LC);Ofe=s(BAe,"STRONG",{});var fot=n(Ofe);Xrr=r(fot,"funnel"),fot.forEach(t),zrr=r(BAe," \u2014 "),eq=s(BAe,"A",{href:!0});var got=n(eq);Vrr=r(got,"TFFunnelForMultipleChoice"),got.forEach(t),Wrr=r(BAe," (Funnel Transformer model)"),BAe.forEach(t),Qrr=i(ae),BC=s(ae,"LI",{});var xAe=n(BC);Xfe=s(xAe,"STRONG",{});var hot=n(Xfe);Hrr=r(hot,"longformer"),hot.forEach(t),Urr=r(xAe," \u2014 "),oq=s(xAe,"A",{href:!0});var uot=n(oq);Jrr=r(uot,"TFLongformerForMultipleChoice"),uot.forEach(t),Yrr=r(xAe," (Longformer model)"),xAe.forEach(t),Krr=i(ae),xC=s(ae,"LI",{});var kAe=n(xC);zfe=s(kAe,"STRONG",{});var pot=n(zfe);Zrr=r(pot,"mobilebert"),pot.forEach(t),etr=r(kAe," \u2014 "),rq=s(kAe,"A",{href:!0});var _ot=n(rq);otr=r(_ot,"TFMobileBertForMultipleChoice"),_ot.forEach(t),rtr=r(kAe," (MobileBERT model)"),kAe.forEach(t),ttr=i(ae),kC=s(ae,"LI",{});var RAe=n(kC);Vfe=s(RAe,"STRONG",{});var bot=n(Vfe);atr=r(bot,"mpnet"),bot.forEach(t),str=r(RAe," \u2014 "),tq=s(RAe,"A",{href:!0});var vot=n(tq);ntr=r(vot,"TFMPNetForMultipleChoice"),vot.forEach(t),ltr=r(RAe," (MPNet model)"),RAe.forEach(t),itr=i(ae),RC=s(ae,"LI",{});var SAe=n(RC);Wfe=s(SAe,"STRONG",{});var Tot=n(Wfe);dtr=r(Tot,"rembert"),Tot.forEach(t),ctr=r(SAe," \u2014 "),aq=s(SAe,"A",{href:!0});var Fot=n(aq);mtr=r(Fot,"TFRemBertForMultipleChoice"),Fot.forEach(t),ftr=r(SAe," (RemBERT model)"),SAe.forEach(t),gtr=i(ae),SC=s(ae,"LI",{});var PAe=n(SC);Qfe=s(PAe,"STRONG",{});var Cot=n(Qfe);htr=r(Cot,"roberta"),Cot.forEach(t),utr=r(PAe," \u2014 "),sq=s(PAe,"A",{href:!0});var Mot=n(sq);ptr=r(Mot,"TFRobertaForMultipleChoice"),Mot.forEach(t),_tr=r(PAe," (RoBERTa model)"),PAe.forEach(t),btr=i(ae),PC=s(ae,"LI",{});var $Ae=n(PC);Hfe=s($Ae,"STRONG",{});var Eot=n(Hfe);vtr=r(Eot,"roformer"),Eot.forEach(t),Ttr=r($Ae," \u2014 "),nq=s($Ae,"A",{href:!0});var yot=n(nq);Ftr=r(yot,"TFRoFormerForMultipleChoice"),yot.forEach(t),Ctr=r($Ae," (RoFormer model)"),$Ae.forEach(t),Mtr=i(ae),$C=s(ae,"LI",{});var IAe=n($C);Ufe=s(IAe,"STRONG",{});var wot=n(Ufe);Etr=r(wot,"xlm"),wot.forEach(t),ytr=r(IAe," \u2014 "),lq=s(IAe,"A",{href:!0});var Aot=n(lq);wtr=r(Aot,"TFXLMForMultipleChoice"),Aot.forEach(t),Atr=r(IAe," (XLM model)"),IAe.forEach(t),Ltr=i(ae),IC=s(ae,"LI",{});var jAe=n(IC);Jfe=s(jAe,"STRONG",{});var Lot=n(Jfe);Btr=r(Lot,"xlm-roberta"),Lot.forEach(t),xtr=r(jAe," \u2014 "),iq=s(jAe,"A",{href:!0});var Bot=n(iq);ktr=r(Bot,"TFXLMRobertaForMultipleChoice"),Bot.forEach(t),Rtr=r(jAe," (XLM-RoBERTa model)"),jAe.forEach(t),Str=i(ae),jC=s(ae,"LI",{});var NAe=n(jC);Yfe=s(NAe,"STRONG",{});var xot=n(Yfe);Ptr=r(xot,"xlnet"),xot.forEach(t),$tr=r(NAe," \u2014 "),dq=s(NAe,"A",{href:!0});var kot=n(dq);Itr=r(kot,"TFXLNetForMultipleChoice"),kot.forEach(t),jtr=r(NAe," (XLNet model)"),NAe.forEach(t),ae.forEach(t),Ntr=i(ga),Kfe=s(ga,"P",{});var Rot=n(Kfe);Dtr=r(Rot,"Examples:"),Rot.forEach(t),qtr=i(ga),f(WA.$$.fragment,ga),ga.forEach(t),Pl.forEach(t),B7e=i(d),fc=s(d,"H2",{class:!0});var P9e=n(fc);NC=s(P9e,"A",{id:!0,class:!0,href:!0});var Sot=n(NC);Zfe=s(Sot,"SPAN",{});var Pot=n(Zfe);f(QA.$$.fragment,Pot),Pot.forEach(t),Sot.forEach(t),Gtr=i(P9e),ege=s(P9e,"SPAN",{});var $ot=n(ege);Otr=r($ot,"TFAutoModelForTableQuestionAnswering"),$ot.forEach(t),P9e.forEach(t),x7e=i(d),_r=s(d,"DIV",{class:!0});var Il=n(_r);f(HA.$$.fragment,Il),Xtr=i(Il),gc=s(Il,"P",{});var WX=n(gc);ztr=r(WX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),oge=s(WX,"CODE",{});var Iot=n(oge);Vtr=r(Iot,"from_pretrained()"),Iot.forEach(t),Wtr=r(WX,"class method or the "),rge=s(WX,"CODE",{});var jot=n(rge);Qtr=r(jot,"from_config()"),jot.forEach(t),Htr=r(WX,`class
method.`),WX.forEach(t),Utr=i(Il),UA=s(Il,"P",{});var $9e=n(UA);Jtr=r($9e,"This class cannot be instantiated directly using "),tge=s($9e,"CODE",{});var Not=n(tge);Ytr=r(Not,"__init__()"),Not.forEach(t),Ktr=r($9e," (throws an error)."),$9e.forEach(t),Ztr=i(Il),dt=s(Il,"DIV",{class:!0});var jl=n(dt);f(JA.$$.fragment,jl),ear=i(jl),age=s(jl,"P",{});var Dot=n(age);oar=r(Dot,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Dot.forEach(t),rar=i(jl),hc=s(jl,"P",{});var QX=n(hc);tar=r(QX,`Note:
Loading a model from its configuration file does `),sge=s(QX,"STRONG",{});var qot=n(sge);aar=r(qot,"not"),qot.forEach(t),sar=r(QX,` load the model weights. It only affects the
model\u2019s configuration. Use `),nge=s(QX,"CODE",{});var Got=n(nge);nar=r(Got,"from_pretrained()"),Got.forEach(t),lar=r(QX,"to load the model weights."),QX.forEach(t),iar=i(jl),lge=s(jl,"P",{});var Oot=n(lge);dar=r(Oot,"Examples:"),Oot.forEach(t),car=i(jl),f(YA.$$.fragment,jl),jl.forEach(t),mar=i(Il),vo=s(Il,"DIV",{class:!0});var ha=n(vo);f(KA.$$.fragment,ha),far=i(ha),ige=s(ha,"P",{});var Xot=n(ige);gar=r(Xot,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Xot.forEach(t),har=i(ha),cs=s(ha,"P",{});var i3=n(cs);uar=r(i3,"The model class to instantiate is selected based on the "),dge=s(i3,"CODE",{});var zot=n(dge);par=r(zot,"model_type"),zot.forEach(t),_ar=r(i3,` property of the config object (either
passed as an argument or loaded from `),cge=s(i3,"CODE",{});var Vot=n(cge);bar=r(Vot,"pretrained_model_name_or_path"),Vot.forEach(t),Tar=r(i3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mge=s(i3,"CODE",{});var Wot=n(mge);Far=r(Wot,"pretrained_model_name_or_path"),Wot.forEach(t),Car=r(i3,":"),i3.forEach(t),Mar=i(ha),fge=s(ha,"UL",{});var Qot=n(fge);DC=s(Qot,"LI",{});var DAe=n(DC);gge=s(DAe,"STRONG",{});var Hot=n(gge);Ear=r(Hot,"tapas"),Hot.forEach(t),yar=r(DAe," \u2014 "),cq=s(DAe,"A",{href:!0});var Uot=n(cq);war=r(Uot,"TFTapasForQuestionAnswering"),Uot.forEach(t),Aar=r(DAe," (TAPAS model)"),DAe.forEach(t),Qot.forEach(t),Lar=i(ha),hge=s(ha,"P",{});var Jot=n(hge);Bar=r(Jot,"Examples:"),Jot.forEach(t),xar=i(ha),f(ZA.$$.fragment,ha),ha.forEach(t),Il.forEach(t),k7e=i(d),uc=s(d,"H2",{class:!0});var I9e=n(uc);qC=s(I9e,"A",{id:!0,class:!0,href:!0});var Yot=n(qC);uge=s(Yot,"SPAN",{});var Kot=n(uge);f(e0.$$.fragment,Kot),Kot.forEach(t),Yot.forEach(t),kar=i(I9e),pge=s(I9e,"SPAN",{});var Zot=n(pge);Rar=r(Zot,"TFAutoModelForTokenClassification"),Zot.forEach(t),I9e.forEach(t),R7e=i(d),br=s(d,"DIV",{class:!0});var Nl=n(br);f(o0.$$.fragment,Nl),Sar=i(Nl),pc=s(Nl,"P",{});var HX=n(pc);Par=r(HX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),_ge=s(HX,"CODE",{});var ert=n(_ge);$ar=r(ert,"from_pretrained()"),ert.forEach(t),Iar=r(HX,"class method or the "),bge=s(HX,"CODE",{});var ort=n(bge);jar=r(ort,"from_config()"),ort.forEach(t),Nar=r(HX,`class
method.`),HX.forEach(t),Dar=i(Nl),r0=s(Nl,"P",{});var j9e=n(r0);qar=r(j9e,"This class cannot be instantiated directly using "),vge=s(j9e,"CODE",{});var rrt=n(vge);Gar=r(rrt,"__init__()"),rrt.forEach(t),Oar=r(j9e," (throws an error)."),j9e.forEach(t),Xar=i(Nl),ct=s(Nl,"DIV",{class:!0});var Dl=n(ct);f(t0.$$.fragment,Dl),zar=i(Dl),Tge=s(Dl,"P",{});var trt=n(Tge);Var=r(trt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),trt.forEach(t),War=i(Dl),_c=s(Dl,"P",{});var UX=n(_c);Qar=r(UX,`Note:
Loading a model from its configuration file does `),Fge=s(UX,"STRONG",{});var art=n(Fge);Har=r(art,"not"),art.forEach(t),Uar=r(UX,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cge=s(UX,"CODE",{});var srt=n(Cge);Jar=r(srt,"from_pretrained()"),srt.forEach(t),Yar=r(UX,"to load the model weights."),UX.forEach(t),Kar=i(Dl),Mge=s(Dl,"P",{});var nrt=n(Mge);Zar=r(nrt,"Examples:"),nrt.forEach(t),esr=i(Dl),f(a0.$$.fragment,Dl),Dl.forEach(t),osr=i(Nl),To=s(Nl,"DIV",{class:!0});var ua=n(To);f(s0.$$.fragment,ua),rsr=i(ua),Ege=s(ua,"P",{});var lrt=n(Ege);tsr=r(lrt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),lrt.forEach(t),asr=i(ua),ms=s(ua,"P",{});var d3=n(ms);ssr=r(d3,"The model class to instantiate is selected based on the "),yge=s(d3,"CODE",{});var irt=n(yge);nsr=r(irt,"model_type"),irt.forEach(t),lsr=r(d3,` property of the config object (either
passed as an argument or loaded from `),wge=s(d3,"CODE",{});var drt=n(wge);isr=r(drt,"pretrained_model_name_or_path"),drt.forEach(t),dsr=r(d3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Age=s(d3,"CODE",{});var crt=n(Age);csr=r(crt,"pretrained_model_name_or_path"),crt.forEach(t),msr=r(d3,":"),d3.forEach(t),fsr=i(ua),K=s(ua,"UL",{});var oe=n(K);GC=s(oe,"LI",{});var qAe=n(GC);Lge=s(qAe,"STRONG",{});var mrt=n(Lge);gsr=r(mrt,"albert"),mrt.forEach(t),hsr=r(qAe," \u2014 "),mq=s(qAe,"A",{href:!0});var frt=n(mq);usr=r(frt,"TFAlbertForTokenClassification"),frt.forEach(t),psr=r(qAe," (ALBERT model)"),qAe.forEach(t),_sr=i(oe),OC=s(oe,"LI",{});var GAe=n(OC);Bge=s(GAe,"STRONG",{});var grt=n(Bge);bsr=r(grt,"bert"),grt.forEach(t),vsr=r(GAe," \u2014 "),fq=s(GAe,"A",{href:!0});var hrt=n(fq);Tsr=r(hrt,"TFBertForTokenClassification"),hrt.forEach(t),Fsr=r(GAe," (BERT model)"),GAe.forEach(t),Csr=i(oe),XC=s(oe,"LI",{});var OAe=n(XC);xge=s(OAe,"STRONG",{});var urt=n(xge);Msr=r(urt,"camembert"),urt.forEach(t),Esr=r(OAe," \u2014 "),gq=s(OAe,"A",{href:!0});var prt=n(gq);ysr=r(prt,"TFCamembertForTokenClassification"),prt.forEach(t),wsr=r(OAe," (CamemBERT model)"),OAe.forEach(t),Asr=i(oe),zC=s(oe,"LI",{});var XAe=n(zC);kge=s(XAe,"STRONG",{});var _rt=n(kge);Lsr=r(_rt,"convbert"),_rt.forEach(t),Bsr=r(XAe," \u2014 "),hq=s(XAe,"A",{href:!0});var brt=n(hq);xsr=r(brt,"TFConvBertForTokenClassification"),brt.forEach(t),ksr=r(XAe," (ConvBERT model)"),XAe.forEach(t),Rsr=i(oe),VC=s(oe,"LI",{});var zAe=n(VC);Rge=s(zAe,"STRONG",{});var vrt=n(Rge);Ssr=r(vrt,"deberta"),vrt.forEach(t),Psr=r(zAe," \u2014 "),uq=s(zAe,"A",{href:!0});var Trt=n(uq);$sr=r(Trt,"TFDebertaForTokenClassification"),Trt.forEach(t),Isr=r(zAe," (DeBERTa model)"),zAe.forEach(t),jsr=i(oe),WC=s(oe,"LI",{});var VAe=n(WC);Sge=s(VAe,"STRONG",{});var Frt=n(Sge);Nsr=r(Frt,"deberta-v2"),Frt.forEach(t),Dsr=r(VAe," \u2014 "),pq=s(VAe,"A",{href:!0});var Crt=n(pq);qsr=r(Crt,"TFDebertaV2ForTokenClassification"),Crt.forEach(t),Gsr=r(VAe," (DeBERTa-v2 model)"),VAe.forEach(t),Osr=i(oe),QC=s(oe,"LI",{});var WAe=n(QC);Pge=s(WAe,"STRONG",{});var Mrt=n(Pge);Xsr=r(Mrt,"distilbert"),Mrt.forEach(t),zsr=r(WAe," \u2014 "),_q=s(WAe,"A",{href:!0});var Ert=n(_q);Vsr=r(Ert,"TFDistilBertForTokenClassification"),Ert.forEach(t),Wsr=r(WAe," (DistilBERT model)"),WAe.forEach(t),Qsr=i(oe),HC=s(oe,"LI",{});var QAe=n(HC);$ge=s(QAe,"STRONG",{});var yrt=n($ge);Hsr=r(yrt,"electra"),yrt.forEach(t),Usr=r(QAe," \u2014 "),bq=s(QAe,"A",{href:!0});var wrt=n(bq);Jsr=r(wrt,"TFElectraForTokenClassification"),wrt.forEach(t),Ysr=r(QAe," (ELECTRA model)"),QAe.forEach(t),Ksr=i(oe),UC=s(oe,"LI",{});var HAe=n(UC);Ige=s(HAe,"STRONG",{});var Art=n(Ige);Zsr=r(Art,"flaubert"),Art.forEach(t),enr=r(HAe," \u2014 "),vq=s(HAe,"A",{href:!0});var Lrt=n(vq);onr=r(Lrt,"TFFlaubertForTokenClassification"),Lrt.forEach(t),rnr=r(HAe," (FlauBERT model)"),HAe.forEach(t),tnr=i(oe),JC=s(oe,"LI",{});var UAe=n(JC);jge=s(UAe,"STRONG",{});var Brt=n(jge);anr=r(Brt,"funnel"),Brt.forEach(t),snr=r(UAe," \u2014 "),Tq=s(UAe,"A",{href:!0});var xrt=n(Tq);nnr=r(xrt,"TFFunnelForTokenClassification"),xrt.forEach(t),lnr=r(UAe," (Funnel Transformer model)"),UAe.forEach(t),inr=i(oe),YC=s(oe,"LI",{});var JAe=n(YC);Nge=s(JAe,"STRONG",{});var krt=n(Nge);dnr=r(krt,"layoutlm"),krt.forEach(t),cnr=r(JAe," \u2014 "),Fq=s(JAe,"A",{href:!0});var Rrt=n(Fq);mnr=r(Rrt,"TFLayoutLMForTokenClassification"),Rrt.forEach(t),fnr=r(JAe," (LayoutLM model)"),JAe.forEach(t),gnr=i(oe),KC=s(oe,"LI",{});var YAe=n(KC);Dge=s(YAe,"STRONG",{});var Srt=n(Dge);hnr=r(Srt,"longformer"),Srt.forEach(t),unr=r(YAe," \u2014 "),Cq=s(YAe,"A",{href:!0});var Prt=n(Cq);pnr=r(Prt,"TFLongformerForTokenClassification"),Prt.forEach(t),_nr=r(YAe," (Longformer model)"),YAe.forEach(t),bnr=i(oe),ZC=s(oe,"LI",{});var KAe=n(ZC);qge=s(KAe,"STRONG",{});var $rt=n(qge);vnr=r($rt,"mobilebert"),$rt.forEach(t),Tnr=r(KAe," \u2014 "),Mq=s(KAe,"A",{href:!0});var Irt=n(Mq);Fnr=r(Irt,"TFMobileBertForTokenClassification"),Irt.forEach(t),Cnr=r(KAe," (MobileBERT model)"),KAe.forEach(t),Mnr=i(oe),e4=s(oe,"LI",{});var ZAe=n(e4);Gge=s(ZAe,"STRONG",{});var jrt=n(Gge);Enr=r(jrt,"mpnet"),jrt.forEach(t),ynr=r(ZAe," \u2014 "),Eq=s(ZAe,"A",{href:!0});var Nrt=n(Eq);wnr=r(Nrt,"TFMPNetForTokenClassification"),Nrt.forEach(t),Anr=r(ZAe," (MPNet model)"),ZAe.forEach(t),Lnr=i(oe),o4=s(oe,"LI",{});var e0e=n(o4);Oge=s(e0e,"STRONG",{});var Drt=n(Oge);Bnr=r(Drt,"rembert"),Drt.forEach(t),xnr=r(e0e," \u2014 "),yq=s(e0e,"A",{href:!0});var qrt=n(yq);knr=r(qrt,"TFRemBertForTokenClassification"),qrt.forEach(t),Rnr=r(e0e," (RemBERT model)"),e0e.forEach(t),Snr=i(oe),r4=s(oe,"LI",{});var o0e=n(r4);Xge=s(o0e,"STRONG",{});var Grt=n(Xge);Pnr=r(Grt,"roberta"),Grt.forEach(t),$nr=r(o0e," \u2014 "),wq=s(o0e,"A",{href:!0});var Ort=n(wq);Inr=r(Ort,"TFRobertaForTokenClassification"),Ort.forEach(t),jnr=r(o0e," (RoBERTa model)"),o0e.forEach(t),Nnr=i(oe),t4=s(oe,"LI",{});var r0e=n(t4);zge=s(r0e,"STRONG",{});var Xrt=n(zge);Dnr=r(Xrt,"roformer"),Xrt.forEach(t),qnr=r(r0e," \u2014 "),Aq=s(r0e,"A",{href:!0});var zrt=n(Aq);Gnr=r(zrt,"TFRoFormerForTokenClassification"),zrt.forEach(t),Onr=r(r0e," (RoFormer model)"),r0e.forEach(t),Xnr=i(oe),a4=s(oe,"LI",{});var t0e=n(a4);Vge=s(t0e,"STRONG",{});var Vrt=n(Vge);znr=r(Vrt,"xlm"),Vrt.forEach(t),Vnr=r(t0e," \u2014 "),Lq=s(t0e,"A",{href:!0});var Wrt=n(Lq);Wnr=r(Wrt,"TFXLMForTokenClassification"),Wrt.forEach(t),Qnr=r(t0e," (XLM model)"),t0e.forEach(t),Hnr=i(oe),s4=s(oe,"LI",{});var a0e=n(s4);Wge=s(a0e,"STRONG",{});var Qrt=n(Wge);Unr=r(Qrt,"xlm-roberta"),Qrt.forEach(t),Jnr=r(a0e," \u2014 "),Bq=s(a0e,"A",{href:!0});var Hrt=n(Bq);Ynr=r(Hrt,"TFXLMRobertaForTokenClassification"),Hrt.forEach(t),Knr=r(a0e," (XLM-RoBERTa model)"),a0e.forEach(t),Znr=i(oe),n4=s(oe,"LI",{});var s0e=n(n4);Qge=s(s0e,"STRONG",{});var Urt=n(Qge);elr=r(Urt,"xlnet"),Urt.forEach(t),olr=r(s0e," \u2014 "),xq=s(s0e,"A",{href:!0});var Jrt=n(xq);rlr=r(Jrt,"TFXLNetForTokenClassification"),Jrt.forEach(t),tlr=r(s0e," (XLNet model)"),s0e.forEach(t),oe.forEach(t),alr=i(ua),Hge=s(ua,"P",{});var Yrt=n(Hge);slr=r(Yrt,"Examples:"),Yrt.forEach(t),nlr=i(ua),f(n0.$$.fragment,ua),ua.forEach(t),Nl.forEach(t),S7e=i(d),bc=s(d,"H2",{class:!0});var N9e=n(bc);l4=s(N9e,"A",{id:!0,class:!0,href:!0});var Krt=n(l4);Uge=s(Krt,"SPAN",{});var Zrt=n(Uge);f(l0.$$.fragment,Zrt),Zrt.forEach(t),Krt.forEach(t),llr=i(N9e),Jge=s(N9e,"SPAN",{});var ett=n(Jge);ilr=r(ett,"TFAutoModelForQuestionAnswering"),ett.forEach(t),N9e.forEach(t),P7e=i(d),vr=s(d,"DIV",{class:!0});var ql=n(vr);f(i0.$$.fragment,ql),dlr=i(ql),vc=s(ql,"P",{});var JX=n(vc);clr=r(JX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Yge=s(JX,"CODE",{});var ott=n(Yge);mlr=r(ott,"from_pretrained()"),ott.forEach(t),flr=r(JX,"class method or the "),Kge=s(JX,"CODE",{});var rtt=n(Kge);glr=r(rtt,"from_config()"),rtt.forEach(t),hlr=r(JX,`class
method.`),JX.forEach(t),ulr=i(ql),d0=s(ql,"P",{});var D9e=n(d0);plr=r(D9e,"This class cannot be instantiated directly using "),Zge=s(D9e,"CODE",{});var ttt=n(Zge);_lr=r(ttt,"__init__()"),ttt.forEach(t),blr=r(D9e," (throws an error)."),D9e.forEach(t),vlr=i(ql),mt=s(ql,"DIV",{class:!0});var Gl=n(mt);f(c0.$$.fragment,Gl),Tlr=i(Gl),ehe=s(Gl,"P",{});var att=n(ehe);Flr=r(att,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),att.forEach(t),Clr=i(Gl),Tc=s(Gl,"P",{});var YX=n(Tc);Mlr=r(YX,`Note:
Loading a model from its configuration file does `),ohe=s(YX,"STRONG",{});var stt=n(ohe);Elr=r(stt,"not"),stt.forEach(t),ylr=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),rhe=s(YX,"CODE",{});var ntt=n(rhe);wlr=r(ntt,"from_pretrained()"),ntt.forEach(t),Alr=r(YX,"to load the model weights."),YX.forEach(t),Llr=i(Gl),the=s(Gl,"P",{});var ltt=n(the);Blr=r(ltt,"Examples:"),ltt.forEach(t),xlr=i(Gl),f(m0.$$.fragment,Gl),Gl.forEach(t),klr=i(ql),Fo=s(ql,"DIV",{class:!0});var pa=n(Fo);f(f0.$$.fragment,pa),Rlr=i(pa),ahe=s(pa,"P",{});var itt=n(ahe);Slr=r(itt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),itt.forEach(t),Plr=i(pa),fs=s(pa,"P",{});var c3=n(fs);$lr=r(c3,"The model class to instantiate is selected based on the "),she=s(c3,"CODE",{});var dtt=n(she);Ilr=r(dtt,"model_type"),dtt.forEach(t),jlr=r(c3,` property of the config object (either
passed as an argument or loaded from `),nhe=s(c3,"CODE",{});var ctt=n(nhe);Nlr=r(ctt,"pretrained_model_name_or_path"),ctt.forEach(t),Dlr=r(c3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),lhe=s(c3,"CODE",{});var mtt=n(lhe);qlr=r(mtt,"pretrained_model_name_or_path"),mtt.forEach(t),Glr=r(c3,":"),c3.forEach(t),Olr=i(pa),Z=s(pa,"UL",{});var re=n(Z);i4=s(re,"LI",{});var n0e=n(i4);ihe=s(n0e,"STRONG",{});var ftt=n(ihe);Xlr=r(ftt,"albert"),ftt.forEach(t),zlr=r(n0e," \u2014 "),kq=s(n0e,"A",{href:!0});var gtt=n(kq);Vlr=r(gtt,"TFAlbertForQuestionAnswering"),gtt.forEach(t),Wlr=r(n0e," (ALBERT model)"),n0e.forEach(t),Qlr=i(re),d4=s(re,"LI",{});var l0e=n(d4);dhe=s(l0e,"STRONG",{});var htt=n(dhe);Hlr=r(htt,"bert"),htt.forEach(t),Ulr=r(l0e," \u2014 "),Rq=s(l0e,"A",{href:!0});var utt=n(Rq);Jlr=r(utt,"TFBertForQuestionAnswering"),utt.forEach(t),Ylr=r(l0e," (BERT model)"),l0e.forEach(t),Klr=i(re),c4=s(re,"LI",{});var i0e=n(c4);che=s(i0e,"STRONG",{});var ptt=n(che);Zlr=r(ptt,"camembert"),ptt.forEach(t),eir=r(i0e," \u2014 "),Sq=s(i0e,"A",{href:!0});var _tt=n(Sq);oir=r(_tt,"TFCamembertForQuestionAnswering"),_tt.forEach(t),rir=r(i0e," (CamemBERT model)"),i0e.forEach(t),tir=i(re),m4=s(re,"LI",{});var d0e=n(m4);mhe=s(d0e,"STRONG",{});var btt=n(mhe);air=r(btt,"convbert"),btt.forEach(t),sir=r(d0e," \u2014 "),Pq=s(d0e,"A",{href:!0});var vtt=n(Pq);nir=r(vtt,"TFConvBertForQuestionAnswering"),vtt.forEach(t),lir=r(d0e," (ConvBERT model)"),d0e.forEach(t),iir=i(re),f4=s(re,"LI",{});var c0e=n(f4);fhe=s(c0e,"STRONG",{});var Ttt=n(fhe);dir=r(Ttt,"deberta"),Ttt.forEach(t),cir=r(c0e," \u2014 "),$q=s(c0e,"A",{href:!0});var Ftt=n($q);mir=r(Ftt,"TFDebertaForQuestionAnswering"),Ftt.forEach(t),fir=r(c0e," (DeBERTa model)"),c0e.forEach(t),gir=i(re),g4=s(re,"LI",{});var m0e=n(g4);ghe=s(m0e,"STRONG",{});var Ctt=n(ghe);hir=r(Ctt,"deberta-v2"),Ctt.forEach(t),uir=r(m0e," \u2014 "),Iq=s(m0e,"A",{href:!0});var Mtt=n(Iq);pir=r(Mtt,"TFDebertaV2ForQuestionAnswering"),Mtt.forEach(t),_ir=r(m0e," (DeBERTa-v2 model)"),m0e.forEach(t),bir=i(re),h4=s(re,"LI",{});var f0e=n(h4);hhe=s(f0e,"STRONG",{});var Ett=n(hhe);vir=r(Ett,"distilbert"),Ett.forEach(t),Tir=r(f0e," \u2014 "),jq=s(f0e,"A",{href:!0});var ytt=n(jq);Fir=r(ytt,"TFDistilBertForQuestionAnswering"),ytt.forEach(t),Cir=r(f0e," (DistilBERT model)"),f0e.forEach(t),Mir=i(re),u4=s(re,"LI",{});var g0e=n(u4);uhe=s(g0e,"STRONG",{});var wtt=n(uhe);Eir=r(wtt,"electra"),wtt.forEach(t),yir=r(g0e," \u2014 "),Nq=s(g0e,"A",{href:!0});var Att=n(Nq);wir=r(Att,"TFElectraForQuestionAnswering"),Att.forEach(t),Air=r(g0e," (ELECTRA model)"),g0e.forEach(t),Lir=i(re),p4=s(re,"LI",{});var h0e=n(p4);phe=s(h0e,"STRONG",{});var Ltt=n(phe);Bir=r(Ltt,"flaubert"),Ltt.forEach(t),xir=r(h0e," \u2014 "),Dq=s(h0e,"A",{href:!0});var Btt=n(Dq);kir=r(Btt,"TFFlaubertForQuestionAnsweringSimple"),Btt.forEach(t),Rir=r(h0e," (FlauBERT model)"),h0e.forEach(t),Sir=i(re),_4=s(re,"LI",{});var u0e=n(_4);_he=s(u0e,"STRONG",{});var xtt=n(_he);Pir=r(xtt,"funnel"),xtt.forEach(t),$ir=r(u0e," \u2014 "),qq=s(u0e,"A",{href:!0});var ktt=n(qq);Iir=r(ktt,"TFFunnelForQuestionAnswering"),ktt.forEach(t),jir=r(u0e," (Funnel Transformer model)"),u0e.forEach(t),Nir=i(re),b4=s(re,"LI",{});var p0e=n(b4);bhe=s(p0e,"STRONG",{});var Rtt=n(bhe);Dir=r(Rtt,"longformer"),Rtt.forEach(t),qir=r(p0e," \u2014 "),Gq=s(p0e,"A",{href:!0});var Stt=n(Gq);Gir=r(Stt,"TFLongformerForQuestionAnswering"),Stt.forEach(t),Oir=r(p0e," (Longformer model)"),p0e.forEach(t),Xir=i(re),v4=s(re,"LI",{});var _0e=n(v4);vhe=s(_0e,"STRONG",{});var Ptt=n(vhe);zir=r(Ptt,"mobilebert"),Ptt.forEach(t),Vir=r(_0e," \u2014 "),Oq=s(_0e,"A",{href:!0});var $tt=n(Oq);Wir=r($tt,"TFMobileBertForQuestionAnswering"),$tt.forEach(t),Qir=r(_0e," (MobileBERT model)"),_0e.forEach(t),Hir=i(re),T4=s(re,"LI",{});var b0e=n(T4);The=s(b0e,"STRONG",{});var Itt=n(The);Uir=r(Itt,"mpnet"),Itt.forEach(t),Jir=r(b0e," \u2014 "),Xq=s(b0e,"A",{href:!0});var jtt=n(Xq);Yir=r(jtt,"TFMPNetForQuestionAnswering"),jtt.forEach(t),Kir=r(b0e," (MPNet model)"),b0e.forEach(t),Zir=i(re),F4=s(re,"LI",{});var v0e=n(F4);Fhe=s(v0e,"STRONG",{});var Ntt=n(Fhe);edr=r(Ntt,"rembert"),Ntt.forEach(t),odr=r(v0e," \u2014 "),zq=s(v0e,"A",{href:!0});var Dtt=n(zq);rdr=r(Dtt,"TFRemBertForQuestionAnswering"),Dtt.forEach(t),tdr=r(v0e," (RemBERT model)"),v0e.forEach(t),adr=i(re),C4=s(re,"LI",{});var T0e=n(C4);Che=s(T0e,"STRONG",{});var qtt=n(Che);sdr=r(qtt,"roberta"),qtt.forEach(t),ndr=r(T0e," \u2014 "),Vq=s(T0e,"A",{href:!0});var Gtt=n(Vq);ldr=r(Gtt,"TFRobertaForQuestionAnswering"),Gtt.forEach(t),idr=r(T0e," (RoBERTa model)"),T0e.forEach(t),ddr=i(re),M4=s(re,"LI",{});var F0e=n(M4);Mhe=s(F0e,"STRONG",{});var Ott=n(Mhe);cdr=r(Ott,"roformer"),Ott.forEach(t),mdr=r(F0e," \u2014 "),Wq=s(F0e,"A",{href:!0});var Xtt=n(Wq);fdr=r(Xtt,"TFRoFormerForQuestionAnswering"),Xtt.forEach(t),gdr=r(F0e," (RoFormer model)"),F0e.forEach(t),hdr=i(re),E4=s(re,"LI",{});var C0e=n(E4);Ehe=s(C0e,"STRONG",{});var ztt=n(Ehe);udr=r(ztt,"xlm"),ztt.forEach(t),pdr=r(C0e," \u2014 "),Qq=s(C0e,"A",{href:!0});var Vtt=n(Qq);_dr=r(Vtt,"TFXLMForQuestionAnsweringSimple"),Vtt.forEach(t),bdr=r(C0e," (XLM model)"),C0e.forEach(t),vdr=i(re),y4=s(re,"LI",{});var M0e=n(y4);yhe=s(M0e,"STRONG",{});var Wtt=n(yhe);Tdr=r(Wtt,"xlm-roberta"),Wtt.forEach(t),Fdr=r(M0e," \u2014 "),Hq=s(M0e,"A",{href:!0});var Qtt=n(Hq);Cdr=r(Qtt,"TFXLMRobertaForQuestionAnswering"),Qtt.forEach(t),Mdr=r(M0e," (XLM-RoBERTa model)"),M0e.forEach(t),Edr=i(re),w4=s(re,"LI",{});var E0e=n(w4);whe=s(E0e,"STRONG",{});var Htt=n(whe);ydr=r(Htt,"xlnet"),Htt.forEach(t),wdr=r(E0e," \u2014 "),Uq=s(E0e,"A",{href:!0});var Utt=n(Uq);Adr=r(Utt,"TFXLNetForQuestionAnsweringSimple"),Utt.forEach(t),Ldr=r(E0e," (XLNet model)"),E0e.forEach(t),re.forEach(t),Bdr=i(pa),Ahe=s(pa,"P",{});var Jtt=n(Ahe);xdr=r(Jtt,"Examples:"),Jtt.forEach(t),kdr=i(pa),f(g0.$$.fragment,pa),pa.forEach(t),ql.forEach(t),$7e=i(d),Fc=s(d,"H2",{class:!0});var q9e=n(Fc);A4=s(q9e,"A",{id:!0,class:!0,href:!0});var Ytt=n(A4);Lhe=s(Ytt,"SPAN",{});var Ktt=n(Lhe);f(h0.$$.fragment,Ktt),Ktt.forEach(t),Ytt.forEach(t),Rdr=i(q9e),Bhe=s(q9e,"SPAN",{});var Ztt=n(Bhe);Sdr=r(Ztt,"TFAutoModelForVision2Seq"),Ztt.forEach(t),q9e.forEach(t),I7e=i(d),Tr=s(d,"DIV",{class:!0});var Ol=n(Tr);f(u0.$$.fragment,Ol),Pdr=i(Ol),Cc=s(Ol,"P",{});var KX=n(Cc);$dr=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),xhe=s(KX,"CODE",{});var eat=n(xhe);Idr=r(eat,"from_pretrained()"),eat.forEach(t),jdr=r(KX,"class method or the "),khe=s(KX,"CODE",{});var oat=n(khe);Ndr=r(oat,"from_config()"),oat.forEach(t),Ddr=r(KX,`class
method.`),KX.forEach(t),qdr=i(Ol),p0=s(Ol,"P",{});var G9e=n(p0);Gdr=r(G9e,"This class cannot be instantiated directly using "),Rhe=s(G9e,"CODE",{});var rat=n(Rhe);Odr=r(rat,"__init__()"),rat.forEach(t),Xdr=r(G9e," (throws an error)."),G9e.forEach(t),zdr=i(Ol),ft=s(Ol,"DIV",{class:!0});var Xl=n(ft);f(_0.$$.fragment,Xl),Vdr=i(Xl),She=s(Xl,"P",{});var tat=n(She);Wdr=r(tat,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),tat.forEach(t),Qdr=i(Xl),Mc=s(Xl,"P",{});var ZX=n(Mc);Hdr=r(ZX,`Note:
Loading a model from its configuration file does `),Phe=s(ZX,"STRONG",{});var aat=n(Phe);Udr=r(aat,"not"),aat.forEach(t),Jdr=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),$he=s(ZX,"CODE",{});var sat=n($he);Ydr=r(sat,"from_pretrained()"),sat.forEach(t),Kdr=r(ZX,"to load the model weights."),ZX.forEach(t),Zdr=i(Xl),Ihe=s(Xl,"P",{});var nat=n(Ihe);ecr=r(nat,"Examples:"),nat.forEach(t),ocr=i(Xl),f(b0.$$.fragment,Xl),Xl.forEach(t),rcr=i(Ol),Co=s(Ol,"DIV",{class:!0});var _a=n(Co);f(v0.$$.fragment,_a),tcr=i(_a),jhe=s(_a,"P",{});var lat=n(jhe);acr=r(lat,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),lat.forEach(t),scr=i(_a),gs=s(_a,"P",{});var m3=n(gs);ncr=r(m3,"The model class to instantiate is selected based on the "),Nhe=s(m3,"CODE",{});var iat=n(Nhe);lcr=r(iat,"model_type"),iat.forEach(t),icr=r(m3,` property of the config object (either
passed as an argument or loaded from `),Dhe=s(m3,"CODE",{});var dat=n(Dhe);dcr=r(dat,"pretrained_model_name_or_path"),dat.forEach(t),ccr=r(m3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qhe=s(m3,"CODE",{});var cat=n(qhe);mcr=r(cat,"pretrained_model_name_or_path"),cat.forEach(t),fcr=r(m3,":"),m3.forEach(t),gcr=i(_a),Ghe=s(_a,"UL",{});var mat=n(Ghe);L4=s(mat,"LI",{});var y0e=n(L4);Ohe=s(y0e,"STRONG",{});var fat=n(Ohe);hcr=r(fat,"vision-encoder-decoder"),fat.forEach(t),ucr=r(y0e," \u2014 "),Jq=s(y0e,"A",{href:!0});var gat=n(Jq);pcr=r(gat,"TFVisionEncoderDecoderModel"),gat.forEach(t),_cr=r(y0e," (Vision Encoder decoder model)"),y0e.forEach(t),mat.forEach(t),bcr=i(_a),Xhe=s(_a,"P",{});var hat=n(Xhe);vcr=r(hat,"Examples:"),hat.forEach(t),Tcr=i(_a),f(T0.$$.fragment,_a),_a.forEach(t),Ol.forEach(t),j7e=i(d),Ec=s(d,"H2",{class:!0});var O9e=n(Ec);B4=s(O9e,"A",{id:!0,class:!0,href:!0});var uat=n(B4);zhe=s(uat,"SPAN",{});var pat=n(zhe);f(F0.$$.fragment,pat),pat.forEach(t),uat.forEach(t),Fcr=i(O9e),Vhe=s(O9e,"SPAN",{});var _at=n(Vhe);Ccr=r(_at,"TFAutoModelForSpeechSeq2Seq"),_at.forEach(t),O9e.forEach(t),N7e=i(d),Fr=s(d,"DIV",{class:!0});var zl=n(Fr);f(C0.$$.fragment,zl),Mcr=i(zl),yc=s(zl,"P",{});var ez=n(yc);Ecr=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Whe=s(ez,"CODE",{});var bat=n(Whe);ycr=r(bat,"from_pretrained()"),bat.forEach(t),wcr=r(ez,"class method or the "),Qhe=s(ez,"CODE",{});var vat=n(Qhe);Acr=r(vat,"from_config()"),vat.forEach(t),Lcr=r(ez,`class
method.`),ez.forEach(t),Bcr=i(zl),M0=s(zl,"P",{});var X9e=n(M0);xcr=r(X9e,"This class cannot be instantiated directly using "),Hhe=s(X9e,"CODE",{});var Tat=n(Hhe);kcr=r(Tat,"__init__()"),Tat.forEach(t),Rcr=r(X9e," (throws an error)."),X9e.forEach(t),Scr=i(zl),gt=s(zl,"DIV",{class:!0});var Vl=n(gt);f(E0.$$.fragment,Vl),Pcr=i(Vl),Uhe=s(Vl,"P",{});var Fat=n(Uhe);$cr=r(Fat,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Fat.forEach(t),Icr=i(Vl),wc=s(Vl,"P",{});var oz=n(wc);jcr=r(oz,`Note:
Loading a model from its configuration file does `),Jhe=s(oz,"STRONG",{});var Cat=n(Jhe);Ncr=r(Cat,"not"),Cat.forEach(t),Dcr=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yhe=s(oz,"CODE",{});var Mat=n(Yhe);qcr=r(Mat,"from_pretrained()"),Mat.forEach(t),Gcr=r(oz,"to load the model weights."),oz.forEach(t),Ocr=i(Vl),Khe=s(Vl,"P",{});var Eat=n(Khe);Xcr=r(Eat,"Examples:"),Eat.forEach(t),zcr=i(Vl),f(y0.$$.fragment,Vl),Vl.forEach(t),Vcr=i(zl),Mo=s(zl,"DIV",{class:!0});var ba=n(Mo);f(w0.$$.fragment,ba),Wcr=i(ba),Zhe=s(ba,"P",{});var yat=n(Zhe);Qcr=r(yat,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),yat.forEach(t),Hcr=i(ba),hs=s(ba,"P",{});var f3=n(hs);Ucr=r(f3,"The model class to instantiate is selected based on the "),eue=s(f3,"CODE",{});var wat=n(eue);Jcr=r(wat,"model_type"),wat.forEach(t),Ycr=r(f3,` property of the config object (either
passed as an argument or loaded from `),oue=s(f3,"CODE",{});var Aat=n(oue);Kcr=r(Aat,"pretrained_model_name_or_path"),Aat.forEach(t),Zcr=r(f3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rue=s(f3,"CODE",{});var Lat=n(rue);emr=r(Lat,"pretrained_model_name_or_path"),Lat.forEach(t),omr=r(f3,":"),f3.forEach(t),rmr=i(ba),tue=s(ba,"UL",{});var Bat=n(tue);x4=s(Bat,"LI",{});var w0e=n(x4);aue=s(w0e,"STRONG",{});var xat=n(aue);tmr=r(xat,"speech_to_text"),xat.forEach(t),amr=r(w0e," \u2014 "),Yq=s(w0e,"A",{href:!0});var kat=n(Yq);smr=r(kat,"TFSpeech2TextForConditionalGeneration"),kat.forEach(t),nmr=r(w0e," (Speech2Text model)"),w0e.forEach(t),Bat.forEach(t),lmr=i(ba),sue=s(ba,"P",{});var Rat=n(sue);imr=r(Rat,"Examples:"),Rat.forEach(t),dmr=i(ba),f(A0.$$.fragment,ba),ba.forEach(t),zl.forEach(t),D7e=i(d),Ac=s(d,"H2",{class:!0});var z9e=n(Ac);k4=s(z9e,"A",{id:!0,class:!0,href:!0});var Sat=n(k4);nue=s(Sat,"SPAN",{});var Pat=n(nue);f(L0.$$.fragment,Pat),Pat.forEach(t),Sat.forEach(t),cmr=i(z9e),lue=s(z9e,"SPAN",{});var $at=n(lue);mmr=r($at,"FlaxAutoModel"),$at.forEach(t),z9e.forEach(t),q7e=i(d),Cr=s(d,"DIV",{class:!0});var Wl=n(Cr);f(B0.$$.fragment,Wl),fmr=i(Wl),Lc=s(Wl,"P",{});var rz=n(Lc);gmr=r(rz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),iue=s(rz,"CODE",{});var Iat=n(iue);hmr=r(Iat,"from_pretrained()"),Iat.forEach(t),umr=r(rz,"class method or the "),due=s(rz,"CODE",{});var jat=n(due);pmr=r(jat,"from_config()"),jat.forEach(t),_mr=r(rz,`class
method.`),rz.forEach(t),bmr=i(Wl),x0=s(Wl,"P",{});var V9e=n(x0);vmr=r(V9e,"This class cannot be instantiated directly using "),cue=s(V9e,"CODE",{});var Nat=n(cue);Tmr=r(Nat,"__init__()"),Nat.forEach(t),Fmr=r(V9e," (throws an error)."),V9e.forEach(t),Cmr=i(Wl),ht=s(Wl,"DIV",{class:!0});var Ql=n(ht);f(k0.$$.fragment,Ql),Mmr=i(Ql),mue=s(Ql,"P",{});var Dat=n(mue);Emr=r(Dat,"Instantiates one of the base model classes of the library from a configuration."),Dat.forEach(t),ymr=i(Ql),Bc=s(Ql,"P",{});var tz=n(Bc);wmr=r(tz,`Note:
Loading a model from its configuration file does `),fue=s(tz,"STRONG",{});var qat=n(fue);Amr=r(qat,"not"),qat.forEach(t),Lmr=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),gue=s(tz,"CODE",{});var Gat=n(gue);Bmr=r(Gat,"from_pretrained()"),Gat.forEach(t),xmr=r(tz,"to load the model weights."),tz.forEach(t),kmr=i(Ql),hue=s(Ql,"P",{});var Oat=n(hue);Rmr=r(Oat,"Examples:"),Oat.forEach(t),Smr=i(Ql),f(R0.$$.fragment,Ql),Ql.forEach(t),Pmr=i(Wl),Eo=s(Wl,"DIV",{class:!0});var va=n(Eo);f(S0.$$.fragment,va),$mr=i(va),uue=s(va,"P",{});var Xat=n(uue);Imr=r(Xat,"Instantiate one of the base model classes of the library from a pretrained model."),Xat.forEach(t),jmr=i(va),us=s(va,"P",{});var g3=n(us);Nmr=r(g3,"The model class to instantiate is selected based on the "),pue=s(g3,"CODE",{});var zat=n(pue);Dmr=r(zat,"model_type"),zat.forEach(t),qmr=r(g3,` property of the config object (either
passed as an argument or loaded from `),_ue=s(g3,"CODE",{});var Vat=n(_ue);Gmr=r(Vat,"pretrained_model_name_or_path"),Vat.forEach(t),Omr=r(g3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bue=s(g3,"CODE",{});var Wat=n(bue);Xmr=r(Wat,"pretrained_model_name_or_path"),Wat.forEach(t),zmr=r(g3,":"),g3.forEach(t),Vmr=i(va),V=s(va,"UL",{});var Q=n(V);R4=s(Q,"LI",{});var A0e=n(R4);vue=s(A0e,"STRONG",{});var Qat=n(vue);Wmr=r(Qat,"albert"),Qat.forEach(t),Qmr=r(A0e," \u2014 "),Kq=s(A0e,"A",{href:!0});var Hat=n(Kq);Hmr=r(Hat,"FlaxAlbertModel"),Hat.forEach(t),Umr=r(A0e," (ALBERT model)"),A0e.forEach(t),Jmr=i(Q),S4=s(Q,"LI",{});var L0e=n(S4);Tue=s(L0e,"STRONG",{});var Uat=n(Tue);Ymr=r(Uat,"bart"),Uat.forEach(t),Kmr=r(L0e," \u2014 "),Zq=s(L0e,"A",{href:!0});var Jat=n(Zq);Zmr=r(Jat,"FlaxBartModel"),Jat.forEach(t),efr=r(L0e," (BART model)"),L0e.forEach(t),ofr=i(Q),P4=s(Q,"LI",{});var B0e=n(P4);Fue=s(B0e,"STRONG",{});var Yat=n(Fue);rfr=r(Yat,"beit"),Yat.forEach(t),tfr=r(B0e," \u2014 "),eG=s(B0e,"A",{href:!0});var Kat=n(eG);afr=r(Kat,"FlaxBeitModel"),Kat.forEach(t),sfr=r(B0e," (BEiT model)"),B0e.forEach(t),nfr=i(Q),$4=s(Q,"LI",{});var x0e=n($4);Cue=s(x0e,"STRONG",{});var Zat=n(Cue);lfr=r(Zat,"bert"),Zat.forEach(t),ifr=r(x0e," \u2014 "),oG=s(x0e,"A",{href:!0});var est=n(oG);dfr=r(est,"FlaxBertModel"),est.forEach(t),cfr=r(x0e," (BERT model)"),x0e.forEach(t),mfr=i(Q),I4=s(Q,"LI",{});var k0e=n(I4);Mue=s(k0e,"STRONG",{});var ost=n(Mue);ffr=r(ost,"big_bird"),ost.forEach(t),gfr=r(k0e," \u2014 "),rG=s(k0e,"A",{href:!0});var rst=n(rG);hfr=r(rst,"FlaxBigBirdModel"),rst.forEach(t),ufr=r(k0e," (BigBird model)"),k0e.forEach(t),pfr=i(Q),j4=s(Q,"LI",{});var R0e=n(j4);Eue=s(R0e,"STRONG",{});var tst=n(Eue);_fr=r(tst,"blenderbot"),tst.forEach(t),bfr=r(R0e," \u2014 "),tG=s(R0e,"A",{href:!0});var ast=n(tG);vfr=r(ast,"FlaxBlenderbotModel"),ast.forEach(t),Tfr=r(R0e," (Blenderbot model)"),R0e.forEach(t),Ffr=i(Q),N4=s(Q,"LI",{});var S0e=n(N4);yue=s(S0e,"STRONG",{});var sst=n(yue);Cfr=r(sst,"blenderbot-small"),sst.forEach(t),Mfr=r(S0e," \u2014 "),aG=s(S0e,"A",{href:!0});var nst=n(aG);Efr=r(nst,"FlaxBlenderbotSmallModel"),nst.forEach(t),yfr=r(S0e," (BlenderbotSmall model)"),S0e.forEach(t),wfr=i(Q),D4=s(Q,"LI",{});var P0e=n(D4);wue=s(P0e,"STRONG",{});var lst=n(wue);Afr=r(lst,"clip"),lst.forEach(t),Lfr=r(P0e," \u2014 "),sG=s(P0e,"A",{href:!0});var ist=n(sG);Bfr=r(ist,"FlaxCLIPModel"),ist.forEach(t),xfr=r(P0e," (CLIP model)"),P0e.forEach(t),kfr=i(Q),q4=s(Q,"LI",{});var $0e=n(q4);Aue=s($0e,"STRONG",{});var dst=n(Aue);Rfr=r(dst,"distilbert"),dst.forEach(t),Sfr=r($0e," \u2014 "),nG=s($0e,"A",{href:!0});var cst=n(nG);Pfr=r(cst,"FlaxDistilBertModel"),cst.forEach(t),$fr=r($0e," (DistilBERT model)"),$0e.forEach(t),Ifr=i(Q),G4=s(Q,"LI",{});var I0e=n(G4);Lue=s(I0e,"STRONG",{});var mst=n(Lue);jfr=r(mst,"electra"),mst.forEach(t),Nfr=r(I0e," \u2014 "),lG=s(I0e,"A",{href:!0});var fst=n(lG);Dfr=r(fst,"FlaxElectraModel"),fst.forEach(t),qfr=r(I0e," (ELECTRA model)"),I0e.forEach(t),Gfr=i(Q),O4=s(Q,"LI",{});var j0e=n(O4);Bue=s(j0e,"STRONG",{});var gst=n(Bue);Ofr=r(gst,"gpt2"),gst.forEach(t),Xfr=r(j0e," \u2014 "),iG=s(j0e,"A",{href:!0});var hst=n(iG);zfr=r(hst,"FlaxGPT2Model"),hst.forEach(t),Vfr=r(j0e," (OpenAI GPT-2 model)"),j0e.forEach(t),Wfr=i(Q),X4=s(Q,"LI",{});var N0e=n(X4);xue=s(N0e,"STRONG",{});var ust=n(xue);Qfr=r(ust,"gpt_neo"),ust.forEach(t),Hfr=r(N0e," \u2014 "),dG=s(N0e,"A",{href:!0});var pst=n(dG);Ufr=r(pst,"FlaxGPTNeoModel"),pst.forEach(t),Jfr=r(N0e," (GPT Neo model)"),N0e.forEach(t),Yfr=i(Q),z4=s(Q,"LI",{});var D0e=n(z4);kue=s(D0e,"STRONG",{});var _st=n(kue);Kfr=r(_st,"gptj"),_st.forEach(t),Zfr=r(D0e," \u2014 "),cG=s(D0e,"A",{href:!0});var bst=n(cG);egr=r(bst,"FlaxGPTJModel"),bst.forEach(t),ogr=r(D0e," (GPT-J model)"),D0e.forEach(t),rgr=i(Q),V4=s(Q,"LI",{});var q0e=n(V4);Rue=s(q0e,"STRONG",{});var vst=n(Rue);tgr=r(vst,"marian"),vst.forEach(t),agr=r(q0e," \u2014 "),mG=s(q0e,"A",{href:!0});var Tst=n(mG);sgr=r(Tst,"FlaxMarianModel"),Tst.forEach(t),ngr=r(q0e," (Marian model)"),q0e.forEach(t),lgr=i(Q),W4=s(Q,"LI",{});var G0e=n(W4);Sue=s(G0e,"STRONG",{});var Fst=n(Sue);igr=r(Fst,"mbart"),Fst.forEach(t),dgr=r(G0e," \u2014 "),fG=s(G0e,"A",{href:!0});var Cst=n(fG);cgr=r(Cst,"FlaxMBartModel"),Cst.forEach(t),mgr=r(G0e," (mBART model)"),G0e.forEach(t),fgr=i(Q),Q4=s(Q,"LI",{});var O0e=n(Q4);Pue=s(O0e,"STRONG",{});var Mst=n(Pue);ggr=r(Mst,"mt5"),Mst.forEach(t),hgr=r(O0e," \u2014 "),gG=s(O0e,"A",{href:!0});var Est=n(gG);ugr=r(Est,"FlaxMT5Model"),Est.forEach(t),pgr=r(O0e," (mT5 model)"),O0e.forEach(t),_gr=i(Q),H4=s(Q,"LI",{});var X0e=n(H4);$ue=s(X0e,"STRONG",{});var yst=n($ue);bgr=r(yst,"pegasus"),yst.forEach(t),vgr=r(X0e," \u2014 "),hG=s(X0e,"A",{href:!0});var wst=n(hG);Tgr=r(wst,"FlaxPegasusModel"),wst.forEach(t),Fgr=r(X0e," (Pegasus model)"),X0e.forEach(t),Cgr=i(Q),U4=s(Q,"LI",{});var z0e=n(U4);Iue=s(z0e,"STRONG",{});var Ast=n(Iue);Mgr=r(Ast,"roberta"),Ast.forEach(t),Egr=r(z0e," \u2014 "),uG=s(z0e,"A",{href:!0});var Lst=n(uG);ygr=r(Lst,"FlaxRobertaModel"),Lst.forEach(t),wgr=r(z0e," (RoBERTa model)"),z0e.forEach(t),Agr=i(Q),J4=s(Q,"LI",{});var V0e=n(J4);jue=s(V0e,"STRONG",{});var Bst=n(jue);Lgr=r(Bst,"roformer"),Bst.forEach(t),Bgr=r(V0e," \u2014 "),pG=s(V0e,"A",{href:!0});var xst=n(pG);xgr=r(xst,"FlaxRoFormerModel"),xst.forEach(t),kgr=r(V0e," (RoFormer model)"),V0e.forEach(t),Rgr=i(Q),Y4=s(Q,"LI",{});var W0e=n(Y4);Nue=s(W0e,"STRONG",{});var kst=n(Nue);Sgr=r(kst,"t5"),kst.forEach(t),Pgr=r(W0e," \u2014 "),_G=s(W0e,"A",{href:!0});var Rst=n(_G);$gr=r(Rst,"FlaxT5Model"),Rst.forEach(t),Igr=r(W0e," (T5 model)"),W0e.forEach(t),jgr=i(Q),K4=s(Q,"LI",{});var Q0e=n(K4);Due=s(Q0e,"STRONG",{});var Sst=n(Due);Ngr=r(Sst,"vision-text-dual-encoder"),Sst.forEach(t),Dgr=r(Q0e," \u2014 "),bG=s(Q0e,"A",{href:!0});var Pst=n(bG);qgr=r(Pst,"FlaxVisionTextDualEncoderModel"),Pst.forEach(t),Ggr=r(Q0e," (VisionTextDualEncoder model)"),Q0e.forEach(t),Ogr=i(Q),Z4=s(Q,"LI",{});var H0e=n(Z4);que=s(H0e,"STRONG",{});var $st=n(que);Xgr=r($st,"vit"),$st.forEach(t),zgr=r(H0e," \u2014 "),vG=s(H0e,"A",{href:!0});var Ist=n(vG);Vgr=r(Ist,"FlaxViTModel"),Ist.forEach(t),Wgr=r(H0e," (ViT model)"),H0e.forEach(t),Qgr=i(Q),eM=s(Q,"LI",{});var U0e=n(eM);Gue=s(U0e,"STRONG",{});var jst=n(Gue);Hgr=r(jst,"wav2vec2"),jst.forEach(t),Ugr=r(U0e," \u2014 "),TG=s(U0e,"A",{href:!0});var Nst=n(TG);Jgr=r(Nst,"FlaxWav2Vec2Model"),Nst.forEach(t),Ygr=r(U0e," (Wav2Vec2 model)"),U0e.forEach(t),Kgr=i(Q),oM=s(Q,"LI",{});var J0e=n(oM);Oue=s(J0e,"STRONG",{});var Dst=n(Oue);Zgr=r(Dst,"xglm"),Dst.forEach(t),ehr=r(J0e," \u2014 "),FG=s(J0e,"A",{href:!0});var qst=n(FG);ohr=r(qst,"FlaxXGLMModel"),qst.forEach(t),rhr=r(J0e," (XGLM model)"),J0e.forEach(t),Q.forEach(t),thr=i(va),Xue=s(va,"P",{});var Gst=n(Xue);ahr=r(Gst,"Examples:"),Gst.forEach(t),shr=i(va),f(P0.$$.fragment,va),va.forEach(t),Wl.forEach(t),G7e=i(d),xc=s(d,"H2",{class:!0});var W9e=n(xc);rM=s(W9e,"A",{id:!0,class:!0,href:!0});var Ost=n(rM);zue=s(Ost,"SPAN",{});var Xst=n(zue);f($0.$$.fragment,Xst),Xst.forEach(t),Ost.forEach(t),nhr=i(W9e),Vue=s(W9e,"SPAN",{});var zst=n(Vue);lhr=r(zst,"FlaxAutoModelForCausalLM"),zst.forEach(t),W9e.forEach(t),O7e=i(d),Mr=s(d,"DIV",{class:!0});var Hl=n(Mr);f(I0.$$.fragment,Hl),ihr=i(Hl),kc=s(Hl,"P",{});var az=n(kc);dhr=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Wue=s(az,"CODE",{});var Vst=n(Wue);chr=r(Vst,"from_pretrained()"),Vst.forEach(t),mhr=r(az,"class method or the "),Que=s(az,"CODE",{});var Wst=n(Que);fhr=r(Wst,"from_config()"),Wst.forEach(t),ghr=r(az,`class
method.`),az.forEach(t),hhr=i(Hl),j0=s(Hl,"P",{});var Q9e=n(j0);uhr=r(Q9e,"This class cannot be instantiated directly using "),Hue=s(Q9e,"CODE",{});var Qst=n(Hue);phr=r(Qst,"__init__()"),Qst.forEach(t),_hr=r(Q9e," (throws an error)."),Q9e.forEach(t),bhr=i(Hl),ut=s(Hl,"DIV",{class:!0});var Ul=n(ut);f(N0.$$.fragment,Ul),vhr=i(Ul),Uue=s(Ul,"P",{});var Hst=n(Uue);Thr=r(Hst,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Hst.forEach(t),Fhr=i(Ul),Rc=s(Ul,"P",{});var sz=n(Rc);Chr=r(sz,`Note:
Loading a model from its configuration file does `),Jue=s(sz,"STRONG",{});var Ust=n(Jue);Mhr=r(Ust,"not"),Ust.forEach(t),Ehr=r(sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yue=s(sz,"CODE",{});var Jst=n(Yue);yhr=r(Jst,"from_pretrained()"),Jst.forEach(t),whr=r(sz,"to load the model weights."),sz.forEach(t),Ahr=i(Ul),Kue=s(Ul,"P",{});var Yst=n(Kue);Lhr=r(Yst,"Examples:"),Yst.forEach(t),Bhr=i(Ul),f(D0.$$.fragment,Ul),Ul.forEach(t),xhr=i(Hl),yo=s(Hl,"DIV",{class:!0});var Ta=n(yo);f(q0.$$.fragment,Ta),khr=i(Ta),Zue=s(Ta,"P",{});var Kst=n(Zue);Rhr=r(Kst,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Kst.forEach(t),Shr=i(Ta),ps=s(Ta,"P",{});var h3=n(ps);Phr=r(h3,"The model class to instantiate is selected based on the "),epe=s(h3,"CODE",{});var Zst=n(epe);$hr=r(Zst,"model_type"),Zst.forEach(t),Ihr=r(h3,` property of the config object (either
passed as an argument or loaded from `),ope=s(h3,"CODE",{});var ent=n(ope);jhr=r(ent,"pretrained_model_name_or_path"),ent.forEach(t),Nhr=r(h3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rpe=s(h3,"CODE",{});var ont=n(rpe);Dhr=r(ont,"pretrained_model_name_or_path"),ont.forEach(t),qhr=r(h3,":"),h3.forEach(t),Ghr=i(Ta),_s=s(Ta,"UL",{});var u3=n(_s);tM=s(u3,"LI",{});var Y0e=n(tM);tpe=s(Y0e,"STRONG",{});var rnt=n(tpe);Ohr=r(rnt,"gpt2"),rnt.forEach(t),Xhr=r(Y0e," \u2014 "),CG=s(Y0e,"A",{href:!0});var tnt=n(CG);zhr=r(tnt,"FlaxGPT2LMHeadModel"),tnt.forEach(t),Vhr=r(Y0e," (OpenAI GPT-2 model)"),Y0e.forEach(t),Whr=i(u3),aM=s(u3,"LI",{});var K0e=n(aM);ape=s(K0e,"STRONG",{});var ant=n(ape);Qhr=r(ant,"gpt_neo"),ant.forEach(t),Hhr=r(K0e," \u2014 "),MG=s(K0e,"A",{href:!0});var snt=n(MG);Uhr=r(snt,"FlaxGPTNeoForCausalLM"),snt.forEach(t),Jhr=r(K0e," (GPT Neo model)"),K0e.forEach(t),Yhr=i(u3),sM=s(u3,"LI",{});var Z0e=n(sM);spe=s(Z0e,"STRONG",{});var nnt=n(spe);Khr=r(nnt,"gptj"),nnt.forEach(t),Zhr=r(Z0e," \u2014 "),EG=s(Z0e,"A",{href:!0});var lnt=n(EG);eur=r(lnt,"FlaxGPTJForCausalLM"),lnt.forEach(t),our=r(Z0e," (GPT-J model)"),Z0e.forEach(t),rur=i(u3),nM=s(u3,"LI",{});var e6e=n(nM);npe=s(e6e,"STRONG",{});var int=n(npe);tur=r(int,"xglm"),int.forEach(t),aur=r(e6e," \u2014 "),yG=s(e6e,"A",{href:!0});var dnt=n(yG);sur=r(dnt,"FlaxXGLMForCausalLM"),dnt.forEach(t),nur=r(e6e," (XGLM model)"),e6e.forEach(t),u3.forEach(t),lur=i(Ta),lpe=s(Ta,"P",{});var cnt=n(lpe);iur=r(cnt,"Examples:"),cnt.forEach(t),dur=i(Ta),f(G0.$$.fragment,Ta),Ta.forEach(t),Hl.forEach(t),X7e=i(d),Sc=s(d,"H2",{class:!0});var H9e=n(Sc);lM=s(H9e,"A",{id:!0,class:!0,href:!0});var mnt=n(lM);ipe=s(mnt,"SPAN",{});var fnt=n(ipe);f(O0.$$.fragment,fnt),fnt.forEach(t),mnt.forEach(t),cur=i(H9e),dpe=s(H9e,"SPAN",{});var gnt=n(dpe);mur=r(gnt,"FlaxAutoModelForPreTraining"),gnt.forEach(t),H9e.forEach(t),z7e=i(d),Er=s(d,"DIV",{class:!0});var Jl=n(Er);f(X0.$$.fragment,Jl),fur=i(Jl),Pc=s(Jl,"P",{});var nz=n(Pc);gur=r(nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),cpe=s(nz,"CODE",{});var hnt=n(cpe);hur=r(hnt,"from_pretrained()"),hnt.forEach(t),uur=r(nz,"class method or the "),mpe=s(nz,"CODE",{});var unt=n(mpe);pur=r(unt,"from_config()"),unt.forEach(t),_ur=r(nz,`class
method.`),nz.forEach(t),bur=i(Jl),z0=s(Jl,"P",{});var U9e=n(z0);vur=r(U9e,"This class cannot be instantiated directly using "),fpe=s(U9e,"CODE",{});var pnt=n(fpe);Tur=r(pnt,"__init__()"),pnt.forEach(t),Fur=r(U9e," (throws an error)."),U9e.forEach(t),Cur=i(Jl),pt=s(Jl,"DIV",{class:!0});var Yl=n(pt);f(V0.$$.fragment,Yl),Mur=i(Yl),gpe=s(Yl,"P",{});var _nt=n(gpe);Eur=r(_nt,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),_nt.forEach(t),yur=i(Yl),$c=s(Yl,"P",{});var lz=n($c);wur=r(lz,`Note:
Loading a model from its configuration file does `),hpe=s(lz,"STRONG",{});var bnt=n(hpe);Aur=r(bnt,"not"),bnt.forEach(t),Lur=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),upe=s(lz,"CODE",{});var vnt=n(upe);Bur=r(vnt,"from_pretrained()"),vnt.forEach(t),xur=r(lz,"to load the model weights."),lz.forEach(t),kur=i(Yl),ppe=s(Yl,"P",{});var Tnt=n(ppe);Rur=r(Tnt,"Examples:"),Tnt.forEach(t),Sur=i(Yl),f(W0.$$.fragment,Yl),Yl.forEach(t),Pur=i(Jl),wo=s(Jl,"DIV",{class:!0});var Fa=n(wo);f(Q0.$$.fragment,Fa),$ur=i(Fa),_pe=s(Fa,"P",{});var Fnt=n(_pe);Iur=r(Fnt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Fnt.forEach(t),jur=i(Fa),bs=s(Fa,"P",{});var p3=n(bs);Nur=r(p3,"The model class to instantiate is selected based on the "),bpe=s(p3,"CODE",{});var Cnt=n(bpe);Dur=r(Cnt,"model_type"),Cnt.forEach(t),qur=r(p3,` property of the config object (either
passed as an argument or loaded from `),vpe=s(p3,"CODE",{});var Mnt=n(vpe);Gur=r(Mnt,"pretrained_model_name_or_path"),Mnt.forEach(t),Our=r(p3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Tpe=s(p3,"CODE",{});var Ent=n(Tpe);Xur=r(Ent,"pretrained_model_name_or_path"),Ent.forEach(t),zur=r(p3,":"),p3.forEach(t),Vur=i(Fa),me=s(Fa,"UL",{});var pe=n(me);iM=s(pe,"LI",{});var o6e=n(iM);Fpe=s(o6e,"STRONG",{});var ynt=n(Fpe);Wur=r(ynt,"albert"),ynt.forEach(t),Qur=r(o6e," \u2014 "),wG=s(o6e,"A",{href:!0});var wnt=n(wG);Hur=r(wnt,"FlaxAlbertForPreTraining"),wnt.forEach(t),Uur=r(o6e," (ALBERT model)"),o6e.forEach(t),Jur=i(pe),dM=s(pe,"LI",{});var r6e=n(dM);Cpe=s(r6e,"STRONG",{});var Ant=n(Cpe);Yur=r(Ant,"bart"),Ant.forEach(t),Kur=r(r6e," \u2014 "),AG=s(r6e,"A",{href:!0});var Lnt=n(AG);Zur=r(Lnt,"FlaxBartForConditionalGeneration"),Lnt.forEach(t),epr=r(r6e," (BART model)"),r6e.forEach(t),opr=i(pe),cM=s(pe,"LI",{});var t6e=n(cM);Mpe=s(t6e,"STRONG",{});var Bnt=n(Mpe);rpr=r(Bnt,"bert"),Bnt.forEach(t),tpr=r(t6e," \u2014 "),LG=s(t6e,"A",{href:!0});var xnt=n(LG);apr=r(xnt,"FlaxBertForPreTraining"),xnt.forEach(t),spr=r(t6e," (BERT model)"),t6e.forEach(t),npr=i(pe),mM=s(pe,"LI",{});var a6e=n(mM);Epe=s(a6e,"STRONG",{});var knt=n(Epe);lpr=r(knt,"big_bird"),knt.forEach(t),ipr=r(a6e," \u2014 "),BG=s(a6e,"A",{href:!0});var Rnt=n(BG);dpr=r(Rnt,"FlaxBigBirdForPreTraining"),Rnt.forEach(t),cpr=r(a6e," (BigBird model)"),a6e.forEach(t),mpr=i(pe),fM=s(pe,"LI",{});var s6e=n(fM);ype=s(s6e,"STRONG",{});var Snt=n(ype);fpr=r(Snt,"electra"),Snt.forEach(t),gpr=r(s6e," \u2014 "),xG=s(s6e,"A",{href:!0});var Pnt=n(xG);hpr=r(Pnt,"FlaxElectraForPreTraining"),Pnt.forEach(t),upr=r(s6e," (ELECTRA model)"),s6e.forEach(t),ppr=i(pe),gM=s(pe,"LI",{});var n6e=n(gM);wpe=s(n6e,"STRONG",{});var $nt=n(wpe);_pr=r($nt,"mbart"),$nt.forEach(t),bpr=r(n6e," \u2014 "),kG=s(n6e,"A",{href:!0});var Int=n(kG);vpr=r(Int,"FlaxMBartForConditionalGeneration"),Int.forEach(t),Tpr=r(n6e," (mBART model)"),n6e.forEach(t),Fpr=i(pe),hM=s(pe,"LI",{});var l6e=n(hM);Ape=s(l6e,"STRONG",{});var jnt=n(Ape);Cpr=r(jnt,"mt5"),jnt.forEach(t),Mpr=r(l6e," \u2014 "),RG=s(l6e,"A",{href:!0});var Nnt=n(RG);Epr=r(Nnt,"FlaxMT5ForConditionalGeneration"),Nnt.forEach(t),ypr=r(l6e," (mT5 model)"),l6e.forEach(t),wpr=i(pe),uM=s(pe,"LI",{});var i6e=n(uM);Lpe=s(i6e,"STRONG",{});var Dnt=n(Lpe);Apr=r(Dnt,"roberta"),Dnt.forEach(t),Lpr=r(i6e," \u2014 "),SG=s(i6e,"A",{href:!0});var qnt=n(SG);Bpr=r(qnt,"FlaxRobertaForMaskedLM"),qnt.forEach(t),xpr=r(i6e," (RoBERTa model)"),i6e.forEach(t),kpr=i(pe),pM=s(pe,"LI",{});var d6e=n(pM);Bpe=s(d6e,"STRONG",{});var Gnt=n(Bpe);Rpr=r(Gnt,"roformer"),Gnt.forEach(t),Spr=r(d6e," \u2014 "),PG=s(d6e,"A",{href:!0});var Ont=n(PG);Ppr=r(Ont,"FlaxRoFormerForMaskedLM"),Ont.forEach(t),$pr=r(d6e," (RoFormer model)"),d6e.forEach(t),Ipr=i(pe),_M=s(pe,"LI",{});var c6e=n(_M);xpe=s(c6e,"STRONG",{});var Xnt=n(xpe);jpr=r(Xnt,"t5"),Xnt.forEach(t),Npr=r(c6e," \u2014 "),$G=s(c6e,"A",{href:!0});var znt=n($G);Dpr=r(znt,"FlaxT5ForConditionalGeneration"),znt.forEach(t),qpr=r(c6e," (T5 model)"),c6e.forEach(t),Gpr=i(pe),bM=s(pe,"LI",{});var m6e=n(bM);kpe=s(m6e,"STRONG",{});var Vnt=n(kpe);Opr=r(Vnt,"wav2vec2"),Vnt.forEach(t),Xpr=r(m6e," \u2014 "),IG=s(m6e,"A",{href:!0});var Wnt=n(IG);zpr=r(Wnt,"FlaxWav2Vec2ForPreTraining"),Wnt.forEach(t),Vpr=r(m6e," (Wav2Vec2 model)"),m6e.forEach(t),pe.forEach(t),Wpr=i(Fa),Rpe=s(Fa,"P",{});var Qnt=n(Rpe);Qpr=r(Qnt,"Examples:"),Qnt.forEach(t),Hpr=i(Fa),f(H0.$$.fragment,Fa),Fa.forEach(t),Jl.forEach(t),V7e=i(d),Ic=s(d,"H2",{class:!0});var J9e=n(Ic);vM=s(J9e,"A",{id:!0,class:!0,href:!0});var Hnt=n(vM);Spe=s(Hnt,"SPAN",{});var Unt=n(Spe);f(U0.$$.fragment,Unt),Unt.forEach(t),Hnt.forEach(t),Upr=i(J9e),Ppe=s(J9e,"SPAN",{});var Jnt=n(Ppe);Jpr=r(Jnt,"FlaxAutoModelForMaskedLM"),Jnt.forEach(t),J9e.forEach(t),W7e=i(d),yr=s(d,"DIV",{class:!0});var Kl=n(yr);f(J0.$$.fragment,Kl),Ypr=i(Kl),jc=s(Kl,"P",{});var iz=n(jc);Kpr=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),$pe=s(iz,"CODE",{});var Ynt=n($pe);Zpr=r(Ynt,"from_pretrained()"),Ynt.forEach(t),e_r=r(iz,"class method or the "),Ipe=s(iz,"CODE",{});var Knt=n(Ipe);o_r=r(Knt,"from_config()"),Knt.forEach(t),r_r=r(iz,`class
method.`),iz.forEach(t),t_r=i(Kl),Y0=s(Kl,"P",{});var Y9e=n(Y0);a_r=r(Y9e,"This class cannot be instantiated directly using "),jpe=s(Y9e,"CODE",{});var Znt=n(jpe);s_r=r(Znt,"__init__()"),Znt.forEach(t),n_r=r(Y9e," (throws an error)."),Y9e.forEach(t),l_r=i(Kl),_t=s(Kl,"DIV",{class:!0});var Zl=n(_t);f(K0.$$.fragment,Zl),i_r=i(Zl),Npe=s(Zl,"P",{});var elt=n(Npe);d_r=r(elt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),elt.forEach(t),c_r=i(Zl),Nc=s(Zl,"P",{});var dz=n(Nc);m_r=r(dz,`Note:
Loading a model from its configuration file does `),Dpe=s(dz,"STRONG",{});var olt=n(Dpe);f_r=r(olt,"not"),olt.forEach(t),g_r=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),qpe=s(dz,"CODE",{});var rlt=n(qpe);h_r=r(rlt,"from_pretrained()"),rlt.forEach(t),u_r=r(dz,"to load the model weights."),dz.forEach(t),p_r=i(Zl),Gpe=s(Zl,"P",{});var tlt=n(Gpe);__r=r(tlt,"Examples:"),tlt.forEach(t),b_r=i(Zl),f(Z0.$$.fragment,Zl),Zl.forEach(t),v_r=i(Kl),Ao=s(Kl,"DIV",{class:!0});var Ca=n(Ao);f(e6.$$.fragment,Ca),T_r=i(Ca),Ope=s(Ca,"P",{});var alt=n(Ope);F_r=r(alt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),alt.forEach(t),C_r=i(Ca),vs=s(Ca,"P",{});var _3=n(vs);M_r=r(_3,"The model class to instantiate is selected based on the "),Xpe=s(_3,"CODE",{});var slt=n(Xpe);E_r=r(slt,"model_type"),slt.forEach(t),y_r=r(_3,` property of the config object (either
passed as an argument or loaded from `),zpe=s(_3,"CODE",{});var nlt=n(zpe);w_r=r(nlt,"pretrained_model_name_or_path"),nlt.forEach(t),A_r=r(_3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vpe=s(_3,"CODE",{});var llt=n(Vpe);L_r=r(llt,"pretrained_model_name_or_path"),llt.forEach(t),B_r=r(_3,":"),_3.forEach(t),x_r=i(Ca),be=s(Ca,"UL",{});var Ye=n(be);TM=s(Ye,"LI",{});var f6e=n(TM);Wpe=s(f6e,"STRONG",{});var ilt=n(Wpe);k_r=r(ilt,"albert"),ilt.forEach(t),R_r=r(f6e," \u2014 "),jG=s(f6e,"A",{href:!0});var dlt=n(jG);S_r=r(dlt,"FlaxAlbertForMaskedLM"),dlt.forEach(t),P_r=r(f6e," (ALBERT model)"),f6e.forEach(t),$_r=i(Ye),FM=s(Ye,"LI",{});var g6e=n(FM);Qpe=s(g6e,"STRONG",{});var clt=n(Qpe);I_r=r(clt,"bart"),clt.forEach(t),j_r=r(g6e," \u2014 "),NG=s(g6e,"A",{href:!0});var mlt=n(NG);N_r=r(mlt,"FlaxBartForConditionalGeneration"),mlt.forEach(t),D_r=r(g6e," (BART model)"),g6e.forEach(t),q_r=i(Ye),CM=s(Ye,"LI",{});var h6e=n(CM);Hpe=s(h6e,"STRONG",{});var flt=n(Hpe);G_r=r(flt,"bert"),flt.forEach(t),O_r=r(h6e," \u2014 "),DG=s(h6e,"A",{href:!0});var glt=n(DG);X_r=r(glt,"FlaxBertForMaskedLM"),glt.forEach(t),z_r=r(h6e," (BERT model)"),h6e.forEach(t),V_r=i(Ye),MM=s(Ye,"LI",{});var u6e=n(MM);Upe=s(u6e,"STRONG",{});var hlt=n(Upe);W_r=r(hlt,"big_bird"),hlt.forEach(t),Q_r=r(u6e," \u2014 "),qG=s(u6e,"A",{href:!0});var ult=n(qG);H_r=r(ult,"FlaxBigBirdForMaskedLM"),ult.forEach(t),U_r=r(u6e," (BigBird model)"),u6e.forEach(t),J_r=i(Ye),EM=s(Ye,"LI",{});var p6e=n(EM);Jpe=s(p6e,"STRONG",{});var plt=n(Jpe);Y_r=r(plt,"distilbert"),plt.forEach(t),K_r=r(p6e," \u2014 "),GG=s(p6e,"A",{href:!0});var _lt=n(GG);Z_r=r(_lt,"FlaxDistilBertForMaskedLM"),_lt.forEach(t),ebr=r(p6e," (DistilBERT model)"),p6e.forEach(t),obr=i(Ye),yM=s(Ye,"LI",{});var _6e=n(yM);Ype=s(_6e,"STRONG",{});var blt=n(Ype);rbr=r(blt,"electra"),blt.forEach(t),tbr=r(_6e," \u2014 "),OG=s(_6e,"A",{href:!0});var vlt=n(OG);abr=r(vlt,"FlaxElectraForMaskedLM"),vlt.forEach(t),sbr=r(_6e," (ELECTRA model)"),_6e.forEach(t),nbr=i(Ye),wM=s(Ye,"LI",{});var b6e=n(wM);Kpe=s(b6e,"STRONG",{});var Tlt=n(Kpe);lbr=r(Tlt,"mbart"),Tlt.forEach(t),ibr=r(b6e," \u2014 "),XG=s(b6e,"A",{href:!0});var Flt=n(XG);dbr=r(Flt,"FlaxMBartForConditionalGeneration"),Flt.forEach(t),cbr=r(b6e," (mBART model)"),b6e.forEach(t),mbr=i(Ye),AM=s(Ye,"LI",{});var v6e=n(AM);Zpe=s(v6e,"STRONG",{});var Clt=n(Zpe);fbr=r(Clt,"roberta"),Clt.forEach(t),gbr=r(v6e," \u2014 "),zG=s(v6e,"A",{href:!0});var Mlt=n(zG);hbr=r(Mlt,"FlaxRobertaForMaskedLM"),Mlt.forEach(t),ubr=r(v6e," (RoBERTa model)"),v6e.forEach(t),pbr=i(Ye),LM=s(Ye,"LI",{});var T6e=n(LM);e_e=s(T6e,"STRONG",{});var Elt=n(e_e);_br=r(Elt,"roformer"),Elt.forEach(t),bbr=r(T6e," \u2014 "),VG=s(T6e,"A",{href:!0});var ylt=n(VG);vbr=r(ylt,"FlaxRoFormerForMaskedLM"),ylt.forEach(t),Tbr=r(T6e," (RoFormer model)"),T6e.forEach(t),Ye.forEach(t),Fbr=i(Ca),o_e=s(Ca,"P",{});var wlt=n(o_e);Cbr=r(wlt,"Examples:"),wlt.forEach(t),Mbr=i(Ca),f(o6.$$.fragment,Ca),Ca.forEach(t),Kl.forEach(t),Q7e=i(d),Dc=s(d,"H2",{class:!0});var K9e=n(Dc);BM=s(K9e,"A",{id:!0,class:!0,href:!0});var Alt=n(BM);r_e=s(Alt,"SPAN",{});var Llt=n(r_e);f(r6.$$.fragment,Llt),Llt.forEach(t),Alt.forEach(t),Ebr=i(K9e),t_e=s(K9e,"SPAN",{});var Blt=n(t_e);ybr=r(Blt,"FlaxAutoModelForSeq2SeqLM"),Blt.forEach(t),K9e.forEach(t),H7e=i(d),wr=s(d,"DIV",{class:!0});var ei=n(wr);f(t6.$$.fragment,ei),wbr=i(ei),qc=s(ei,"P",{});var cz=n(qc);Abr=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),a_e=s(cz,"CODE",{});var xlt=n(a_e);Lbr=r(xlt,"from_pretrained()"),xlt.forEach(t),Bbr=r(cz,"class method or the "),s_e=s(cz,"CODE",{});var klt=n(s_e);xbr=r(klt,"from_config()"),klt.forEach(t),kbr=r(cz,`class
method.`),cz.forEach(t),Rbr=i(ei),a6=s(ei,"P",{});var Z9e=n(a6);Sbr=r(Z9e,"This class cannot be instantiated directly using "),n_e=s(Z9e,"CODE",{});var Rlt=n(n_e);Pbr=r(Rlt,"__init__()"),Rlt.forEach(t),$br=r(Z9e," (throws an error)."),Z9e.forEach(t),Ibr=i(ei),bt=s(ei,"DIV",{class:!0});var oi=n(bt);f(s6.$$.fragment,oi),jbr=i(oi),l_e=s(oi,"P",{});var Slt=n(l_e);Nbr=r(Slt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Slt.forEach(t),Dbr=i(oi),Gc=s(oi,"P",{});var mz=n(Gc);qbr=r(mz,`Note:
Loading a model from its configuration file does `),i_e=s(mz,"STRONG",{});var Plt=n(i_e);Gbr=r(Plt,"not"),Plt.forEach(t),Obr=r(mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),d_e=s(mz,"CODE",{});var $lt=n(d_e);Xbr=r($lt,"from_pretrained()"),$lt.forEach(t),zbr=r(mz,"to load the model weights."),mz.forEach(t),Vbr=i(oi),c_e=s(oi,"P",{});var Ilt=n(c_e);Wbr=r(Ilt,"Examples:"),Ilt.forEach(t),Qbr=i(oi),f(n6.$$.fragment,oi),oi.forEach(t),Hbr=i(ei),Lo=s(ei,"DIV",{class:!0});var Ma=n(Lo);f(l6.$$.fragment,Ma),Ubr=i(Ma),m_e=s(Ma,"P",{});var jlt=n(m_e);Jbr=r(jlt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),jlt.forEach(t),Ybr=i(Ma),Ts=s(Ma,"P",{});var b3=n(Ts);Kbr=r(b3,"The model class to instantiate is selected based on the "),f_e=s(b3,"CODE",{});var Nlt=n(f_e);Zbr=r(Nlt,"model_type"),Nlt.forEach(t),e2r=r(b3,` property of the config object (either
passed as an argument or loaded from `),g_e=s(b3,"CODE",{});var Dlt=n(g_e);o2r=r(Dlt,"pretrained_model_name_or_path"),Dlt.forEach(t),r2r=r(b3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h_e=s(b3,"CODE",{});var qlt=n(h_e);t2r=r(qlt,"pretrained_model_name_or_path"),qlt.forEach(t),a2r=r(b3,":"),b3.forEach(t),s2r=i(Ma),ve=s(Ma,"UL",{});var Ke=n(ve);xM=s(Ke,"LI",{});var F6e=n(xM);u_e=s(F6e,"STRONG",{});var Glt=n(u_e);n2r=r(Glt,"bart"),Glt.forEach(t),l2r=r(F6e," \u2014 "),WG=s(F6e,"A",{href:!0});var Olt=n(WG);i2r=r(Olt,"FlaxBartForConditionalGeneration"),Olt.forEach(t),d2r=r(F6e," (BART model)"),F6e.forEach(t),c2r=i(Ke),kM=s(Ke,"LI",{});var C6e=n(kM);p_e=s(C6e,"STRONG",{});var Xlt=n(p_e);m2r=r(Xlt,"blenderbot"),Xlt.forEach(t),f2r=r(C6e," \u2014 "),QG=s(C6e,"A",{href:!0});var zlt=n(QG);g2r=r(zlt,"FlaxBlenderbotForConditionalGeneration"),zlt.forEach(t),h2r=r(C6e," (Blenderbot model)"),C6e.forEach(t),u2r=i(Ke),RM=s(Ke,"LI",{});var M6e=n(RM);__e=s(M6e,"STRONG",{});var Vlt=n(__e);p2r=r(Vlt,"blenderbot-small"),Vlt.forEach(t),_2r=r(M6e," \u2014 "),HG=s(M6e,"A",{href:!0});var Wlt=n(HG);b2r=r(Wlt,"FlaxBlenderbotSmallForConditionalGeneration"),Wlt.forEach(t),v2r=r(M6e," (BlenderbotSmall model)"),M6e.forEach(t),T2r=i(Ke),SM=s(Ke,"LI",{});var E6e=n(SM);b_e=s(E6e,"STRONG",{});var Qlt=n(b_e);F2r=r(Qlt,"encoder-decoder"),Qlt.forEach(t),C2r=r(E6e," \u2014 "),UG=s(E6e,"A",{href:!0});var Hlt=n(UG);M2r=r(Hlt,"FlaxEncoderDecoderModel"),Hlt.forEach(t),E2r=r(E6e," (Encoder decoder model)"),E6e.forEach(t),y2r=i(Ke),PM=s(Ke,"LI",{});var y6e=n(PM);v_e=s(y6e,"STRONG",{});var Ult=n(v_e);w2r=r(Ult,"marian"),Ult.forEach(t),A2r=r(y6e," \u2014 "),JG=s(y6e,"A",{href:!0});var Jlt=n(JG);L2r=r(Jlt,"FlaxMarianMTModel"),Jlt.forEach(t),B2r=r(y6e," (Marian model)"),y6e.forEach(t),x2r=i(Ke),$M=s(Ke,"LI",{});var w6e=n($M);T_e=s(w6e,"STRONG",{});var Ylt=n(T_e);k2r=r(Ylt,"mbart"),Ylt.forEach(t),R2r=r(w6e," \u2014 "),YG=s(w6e,"A",{href:!0});var Klt=n(YG);S2r=r(Klt,"FlaxMBartForConditionalGeneration"),Klt.forEach(t),P2r=r(w6e," (mBART model)"),w6e.forEach(t),$2r=i(Ke),IM=s(Ke,"LI",{});var A6e=n(IM);F_e=s(A6e,"STRONG",{});var Zlt=n(F_e);I2r=r(Zlt,"mt5"),Zlt.forEach(t),j2r=r(A6e," \u2014 "),KG=s(A6e,"A",{href:!0});var eit=n(KG);N2r=r(eit,"FlaxMT5ForConditionalGeneration"),eit.forEach(t),D2r=r(A6e," (mT5 model)"),A6e.forEach(t),q2r=i(Ke),jM=s(Ke,"LI",{});var L6e=n(jM);C_e=s(L6e,"STRONG",{});var oit=n(C_e);G2r=r(oit,"pegasus"),oit.forEach(t),O2r=r(L6e," \u2014 "),ZG=s(L6e,"A",{href:!0});var rit=n(ZG);X2r=r(rit,"FlaxPegasusForConditionalGeneration"),rit.forEach(t),z2r=r(L6e," (Pegasus model)"),L6e.forEach(t),V2r=i(Ke),NM=s(Ke,"LI",{});var B6e=n(NM);M_e=s(B6e,"STRONG",{});var tit=n(M_e);W2r=r(tit,"t5"),tit.forEach(t),Q2r=r(B6e," \u2014 "),eO=s(B6e,"A",{href:!0});var ait=n(eO);H2r=r(ait,"FlaxT5ForConditionalGeneration"),ait.forEach(t),U2r=r(B6e," (T5 model)"),B6e.forEach(t),Ke.forEach(t),J2r=i(Ma),E_e=s(Ma,"P",{});var sit=n(E_e);Y2r=r(sit,"Examples:"),sit.forEach(t),K2r=i(Ma),f(i6.$$.fragment,Ma),Ma.forEach(t),ei.forEach(t),U7e=i(d),Oc=s(d,"H2",{class:!0});var eBe=n(Oc);DM=s(eBe,"A",{id:!0,class:!0,href:!0});var nit=n(DM);y_e=s(nit,"SPAN",{});var lit=n(y_e);f(d6.$$.fragment,lit),lit.forEach(t),nit.forEach(t),Z2r=i(eBe),w_e=s(eBe,"SPAN",{});var iit=n(w_e);evr=r(iit,"FlaxAutoModelForSequenceClassification"),iit.forEach(t),eBe.forEach(t),J7e=i(d),Ar=s(d,"DIV",{class:!0});var ri=n(Ar);f(c6.$$.fragment,ri),ovr=i(ri),Xc=s(ri,"P",{});var fz=n(Xc);rvr=r(fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),A_e=s(fz,"CODE",{});var dit=n(A_e);tvr=r(dit,"from_pretrained()"),dit.forEach(t),avr=r(fz,"class method or the "),L_e=s(fz,"CODE",{});var cit=n(L_e);svr=r(cit,"from_config()"),cit.forEach(t),nvr=r(fz,`class
method.`),fz.forEach(t),lvr=i(ri),m6=s(ri,"P",{});var oBe=n(m6);ivr=r(oBe,"This class cannot be instantiated directly using "),B_e=s(oBe,"CODE",{});var mit=n(B_e);dvr=r(mit,"__init__()"),mit.forEach(t),cvr=r(oBe," (throws an error)."),oBe.forEach(t),mvr=i(ri),vt=s(ri,"DIV",{class:!0});var ti=n(vt);f(f6.$$.fragment,ti),fvr=i(ti),x_e=s(ti,"P",{});var fit=n(x_e);gvr=r(fit,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),fit.forEach(t),hvr=i(ti),zc=s(ti,"P",{});var gz=n(zc);uvr=r(gz,`Note:
Loading a model from its configuration file does `),k_e=s(gz,"STRONG",{});var git=n(k_e);pvr=r(git,"not"),git.forEach(t),_vr=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),R_e=s(gz,"CODE",{});var hit=n(R_e);bvr=r(hit,"from_pretrained()"),hit.forEach(t),vvr=r(gz,"to load the model weights."),gz.forEach(t),Tvr=i(ti),S_e=s(ti,"P",{});var uit=n(S_e);Fvr=r(uit,"Examples:"),uit.forEach(t),Cvr=i(ti),f(g6.$$.fragment,ti),ti.forEach(t),Mvr=i(ri),Bo=s(ri,"DIV",{class:!0});var Ea=n(Bo);f(h6.$$.fragment,Ea),Evr=i(Ea),P_e=s(Ea,"P",{});var pit=n(P_e);yvr=r(pit,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),pit.forEach(t),wvr=i(Ea),Fs=s(Ea,"P",{});var v3=n(Fs);Avr=r(v3,"The model class to instantiate is selected based on the "),$_e=s(v3,"CODE",{});var _it=n($_e);Lvr=r(_it,"model_type"),_it.forEach(t),Bvr=r(v3,` property of the config object (either
passed as an argument or loaded from `),I_e=s(v3,"CODE",{});var bit=n(I_e);xvr=r(bit,"pretrained_model_name_or_path"),bit.forEach(t),kvr=r(v3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j_e=s(v3,"CODE",{});var vit=n(j_e);Rvr=r(vit,"pretrained_model_name_or_path"),vit.forEach(t),Svr=r(v3,":"),v3.forEach(t),Pvr=i(Ea),Te=s(Ea,"UL",{});var Ze=n(Te);qM=s(Ze,"LI",{});var x6e=n(qM);N_e=s(x6e,"STRONG",{});var Tit=n(N_e);$vr=r(Tit,"albert"),Tit.forEach(t),Ivr=r(x6e," \u2014 "),oO=s(x6e,"A",{href:!0});var Fit=n(oO);jvr=r(Fit,"FlaxAlbertForSequenceClassification"),Fit.forEach(t),Nvr=r(x6e," (ALBERT model)"),x6e.forEach(t),Dvr=i(Ze),GM=s(Ze,"LI",{});var k6e=n(GM);D_e=s(k6e,"STRONG",{});var Cit=n(D_e);qvr=r(Cit,"bart"),Cit.forEach(t),Gvr=r(k6e," \u2014 "),rO=s(k6e,"A",{href:!0});var Mit=n(rO);Ovr=r(Mit,"FlaxBartForSequenceClassification"),Mit.forEach(t),Xvr=r(k6e," (BART model)"),k6e.forEach(t),zvr=i(Ze),OM=s(Ze,"LI",{});var R6e=n(OM);q_e=s(R6e,"STRONG",{});var Eit=n(q_e);Vvr=r(Eit,"bert"),Eit.forEach(t),Wvr=r(R6e," \u2014 "),tO=s(R6e,"A",{href:!0});var yit=n(tO);Qvr=r(yit,"FlaxBertForSequenceClassification"),yit.forEach(t),Hvr=r(R6e," (BERT model)"),R6e.forEach(t),Uvr=i(Ze),XM=s(Ze,"LI",{});var S6e=n(XM);G_e=s(S6e,"STRONG",{});var wit=n(G_e);Jvr=r(wit,"big_bird"),wit.forEach(t),Yvr=r(S6e," \u2014 "),aO=s(S6e,"A",{href:!0});var Ait=n(aO);Kvr=r(Ait,"FlaxBigBirdForSequenceClassification"),Ait.forEach(t),Zvr=r(S6e," (BigBird model)"),S6e.forEach(t),eTr=i(Ze),zM=s(Ze,"LI",{});var P6e=n(zM);O_e=s(P6e,"STRONG",{});var Lit=n(O_e);oTr=r(Lit,"distilbert"),Lit.forEach(t),rTr=r(P6e," \u2014 "),sO=s(P6e,"A",{href:!0});var Bit=n(sO);tTr=r(Bit,"FlaxDistilBertForSequenceClassification"),Bit.forEach(t),aTr=r(P6e," (DistilBERT model)"),P6e.forEach(t),sTr=i(Ze),VM=s(Ze,"LI",{});var $6e=n(VM);X_e=s($6e,"STRONG",{});var xit=n(X_e);nTr=r(xit,"electra"),xit.forEach(t),lTr=r($6e," \u2014 "),nO=s($6e,"A",{href:!0});var kit=n(nO);iTr=r(kit,"FlaxElectraForSequenceClassification"),kit.forEach(t),dTr=r($6e," (ELECTRA model)"),$6e.forEach(t),cTr=i(Ze),WM=s(Ze,"LI",{});var I6e=n(WM);z_e=s(I6e,"STRONG",{});var Rit=n(z_e);mTr=r(Rit,"mbart"),Rit.forEach(t),fTr=r(I6e," \u2014 "),lO=s(I6e,"A",{href:!0});var Sit=n(lO);gTr=r(Sit,"FlaxMBartForSequenceClassification"),Sit.forEach(t),hTr=r(I6e," (mBART model)"),I6e.forEach(t),uTr=i(Ze),QM=s(Ze,"LI",{});var j6e=n(QM);V_e=s(j6e,"STRONG",{});var Pit=n(V_e);pTr=r(Pit,"roberta"),Pit.forEach(t),_Tr=r(j6e," \u2014 "),iO=s(j6e,"A",{href:!0});var $it=n(iO);bTr=r($it,"FlaxRobertaForSequenceClassification"),$it.forEach(t),vTr=r(j6e," (RoBERTa model)"),j6e.forEach(t),TTr=i(Ze),HM=s(Ze,"LI",{});var N6e=n(HM);W_e=s(N6e,"STRONG",{});var Iit=n(W_e);FTr=r(Iit,"roformer"),Iit.forEach(t),CTr=r(N6e," \u2014 "),dO=s(N6e,"A",{href:!0});var jit=n(dO);MTr=r(jit,"FlaxRoFormerForSequenceClassification"),jit.forEach(t),ETr=r(N6e," (RoFormer model)"),N6e.forEach(t),Ze.forEach(t),yTr=i(Ea),Q_e=s(Ea,"P",{});var Nit=n(Q_e);wTr=r(Nit,"Examples:"),Nit.forEach(t),ATr=i(Ea),f(u6.$$.fragment,Ea),Ea.forEach(t),ri.forEach(t),Y7e=i(d),Vc=s(d,"H2",{class:!0});var rBe=n(Vc);UM=s(rBe,"A",{id:!0,class:!0,href:!0});var Dit=n(UM);H_e=s(Dit,"SPAN",{});var qit=n(H_e);f(p6.$$.fragment,qit),qit.forEach(t),Dit.forEach(t),LTr=i(rBe),U_e=s(rBe,"SPAN",{});var Git=n(U_e);BTr=r(Git,"FlaxAutoModelForQuestionAnswering"),Git.forEach(t),rBe.forEach(t),K7e=i(d),Lr=s(d,"DIV",{class:!0});var ai=n(Lr);f(_6.$$.fragment,ai),xTr=i(ai),Wc=s(ai,"P",{});var hz=n(Wc);kTr=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),J_e=s(hz,"CODE",{});var Oit=n(J_e);RTr=r(Oit,"from_pretrained()"),Oit.forEach(t),STr=r(hz,"class method or the "),Y_e=s(hz,"CODE",{});var Xit=n(Y_e);PTr=r(Xit,"from_config()"),Xit.forEach(t),$Tr=r(hz,`class
method.`),hz.forEach(t),ITr=i(ai),b6=s(ai,"P",{});var tBe=n(b6);jTr=r(tBe,"This class cannot be instantiated directly using "),K_e=s(tBe,"CODE",{});var zit=n(K_e);NTr=r(zit,"__init__()"),zit.forEach(t),DTr=r(tBe," (throws an error)."),tBe.forEach(t),qTr=i(ai),Tt=s(ai,"DIV",{class:!0});var si=n(Tt);f(v6.$$.fragment,si),GTr=i(si),Z_e=s(si,"P",{});var Vit=n(Z_e);OTr=r(Vit,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Vit.forEach(t),XTr=i(si),Qc=s(si,"P",{});var uz=n(Qc);zTr=r(uz,`Note:
Loading a model from its configuration file does `),ebe=s(uz,"STRONG",{});var Wit=n(ebe);VTr=r(Wit,"not"),Wit.forEach(t),WTr=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),obe=s(uz,"CODE",{});var Qit=n(obe);QTr=r(Qit,"from_pretrained()"),Qit.forEach(t),HTr=r(uz,"to load the model weights."),uz.forEach(t),UTr=i(si),rbe=s(si,"P",{});var Hit=n(rbe);JTr=r(Hit,"Examples:"),Hit.forEach(t),YTr=i(si),f(T6.$$.fragment,si),si.forEach(t),KTr=i(ai),xo=s(ai,"DIV",{class:!0});var ya=n(xo);f(F6.$$.fragment,ya),ZTr=i(ya),tbe=s(ya,"P",{});var Uit=n(tbe);e1r=r(Uit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Uit.forEach(t),o1r=i(ya),Cs=s(ya,"P",{});var T3=n(Cs);r1r=r(T3,"The model class to instantiate is selected based on the "),abe=s(T3,"CODE",{});var Jit=n(abe);t1r=r(Jit,"model_type"),Jit.forEach(t),a1r=r(T3,` property of the config object (either
passed as an argument or loaded from `),sbe=s(T3,"CODE",{});var Yit=n(sbe);s1r=r(Yit,"pretrained_model_name_or_path"),Yit.forEach(t),n1r=r(T3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nbe=s(T3,"CODE",{});var Kit=n(nbe);l1r=r(Kit,"pretrained_model_name_or_path"),Kit.forEach(t),i1r=r(T3,":"),T3.forEach(t),d1r=i(ya),Fe=s(ya,"UL",{});var eo=n(Fe);JM=s(eo,"LI",{});var D6e=n(JM);lbe=s(D6e,"STRONG",{});var Zit=n(lbe);c1r=r(Zit,"albert"),Zit.forEach(t),m1r=r(D6e," \u2014 "),cO=s(D6e,"A",{href:!0});var edt=n(cO);f1r=r(edt,"FlaxAlbertForQuestionAnswering"),edt.forEach(t),g1r=r(D6e," (ALBERT model)"),D6e.forEach(t),h1r=i(eo),YM=s(eo,"LI",{});var q6e=n(YM);ibe=s(q6e,"STRONG",{});var odt=n(ibe);u1r=r(odt,"bart"),odt.forEach(t),p1r=r(q6e," \u2014 "),mO=s(q6e,"A",{href:!0});var rdt=n(mO);_1r=r(rdt,"FlaxBartForQuestionAnswering"),rdt.forEach(t),b1r=r(q6e," (BART model)"),q6e.forEach(t),v1r=i(eo),KM=s(eo,"LI",{});var G6e=n(KM);dbe=s(G6e,"STRONG",{});var tdt=n(dbe);T1r=r(tdt,"bert"),tdt.forEach(t),F1r=r(G6e," \u2014 "),fO=s(G6e,"A",{href:!0});var adt=n(fO);C1r=r(adt,"FlaxBertForQuestionAnswering"),adt.forEach(t),M1r=r(G6e," (BERT model)"),G6e.forEach(t),E1r=i(eo),ZM=s(eo,"LI",{});var O6e=n(ZM);cbe=s(O6e,"STRONG",{});var sdt=n(cbe);y1r=r(sdt,"big_bird"),sdt.forEach(t),w1r=r(O6e," \u2014 "),gO=s(O6e,"A",{href:!0});var ndt=n(gO);A1r=r(ndt,"FlaxBigBirdForQuestionAnswering"),ndt.forEach(t),L1r=r(O6e," (BigBird model)"),O6e.forEach(t),B1r=i(eo),eE=s(eo,"LI",{});var X6e=n(eE);mbe=s(X6e,"STRONG",{});var ldt=n(mbe);x1r=r(ldt,"distilbert"),ldt.forEach(t),k1r=r(X6e," \u2014 "),hO=s(X6e,"A",{href:!0});var idt=n(hO);R1r=r(idt,"FlaxDistilBertForQuestionAnswering"),idt.forEach(t),S1r=r(X6e," (DistilBERT model)"),X6e.forEach(t),P1r=i(eo),oE=s(eo,"LI",{});var z6e=n(oE);fbe=s(z6e,"STRONG",{});var ddt=n(fbe);$1r=r(ddt,"electra"),ddt.forEach(t),I1r=r(z6e," \u2014 "),uO=s(z6e,"A",{href:!0});var cdt=n(uO);j1r=r(cdt,"FlaxElectraForQuestionAnswering"),cdt.forEach(t),N1r=r(z6e," (ELECTRA model)"),z6e.forEach(t),D1r=i(eo),rE=s(eo,"LI",{});var V6e=n(rE);gbe=s(V6e,"STRONG",{});var mdt=n(gbe);q1r=r(mdt,"mbart"),mdt.forEach(t),G1r=r(V6e," \u2014 "),pO=s(V6e,"A",{href:!0});var fdt=n(pO);O1r=r(fdt,"FlaxMBartForQuestionAnswering"),fdt.forEach(t),X1r=r(V6e," (mBART model)"),V6e.forEach(t),z1r=i(eo),tE=s(eo,"LI",{});var W6e=n(tE);hbe=s(W6e,"STRONG",{});var gdt=n(hbe);V1r=r(gdt,"roberta"),gdt.forEach(t),W1r=r(W6e," \u2014 "),_O=s(W6e,"A",{href:!0});var hdt=n(_O);Q1r=r(hdt,"FlaxRobertaForQuestionAnswering"),hdt.forEach(t),H1r=r(W6e," (RoBERTa model)"),W6e.forEach(t),U1r=i(eo),aE=s(eo,"LI",{});var Q6e=n(aE);ube=s(Q6e,"STRONG",{});var udt=n(ube);J1r=r(udt,"roformer"),udt.forEach(t),Y1r=r(Q6e," \u2014 "),bO=s(Q6e,"A",{href:!0});var pdt=n(bO);K1r=r(pdt,"FlaxRoFormerForQuestionAnswering"),pdt.forEach(t),Z1r=r(Q6e," (RoFormer model)"),Q6e.forEach(t),eo.forEach(t),eFr=i(ya),pbe=s(ya,"P",{});var _dt=n(pbe);oFr=r(_dt,"Examples:"),_dt.forEach(t),rFr=i(ya),f(C6.$$.fragment,ya),ya.forEach(t),ai.forEach(t),Z7e=i(d),Hc=s(d,"H2",{class:!0});var aBe=n(Hc);sE=s(aBe,"A",{id:!0,class:!0,href:!0});var bdt=n(sE);_be=s(bdt,"SPAN",{});var vdt=n(_be);f(M6.$$.fragment,vdt),vdt.forEach(t),bdt.forEach(t),tFr=i(aBe),bbe=s(aBe,"SPAN",{});var Tdt=n(bbe);aFr=r(Tdt,"FlaxAutoModelForTokenClassification"),Tdt.forEach(t),aBe.forEach(t),e8e=i(d),Br=s(d,"DIV",{class:!0});var ni=n(Br);f(E6.$$.fragment,ni),sFr=i(ni),Uc=s(ni,"P",{});var pz=n(Uc);nFr=r(pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),vbe=s(pz,"CODE",{});var Fdt=n(vbe);lFr=r(Fdt,"from_pretrained()"),Fdt.forEach(t),iFr=r(pz,"class method or the "),Tbe=s(pz,"CODE",{});var Cdt=n(Tbe);dFr=r(Cdt,"from_config()"),Cdt.forEach(t),cFr=r(pz,`class
method.`),pz.forEach(t),mFr=i(ni),y6=s(ni,"P",{});var sBe=n(y6);fFr=r(sBe,"This class cannot be instantiated directly using "),Fbe=s(sBe,"CODE",{});var Mdt=n(Fbe);gFr=r(Mdt,"__init__()"),Mdt.forEach(t),hFr=r(sBe," (throws an error)."),sBe.forEach(t),uFr=i(ni),Ft=s(ni,"DIV",{class:!0});var li=n(Ft);f(w6.$$.fragment,li),pFr=i(li),Cbe=s(li,"P",{});var Edt=n(Cbe);_Fr=r(Edt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Edt.forEach(t),bFr=i(li),Jc=s(li,"P",{});var _z=n(Jc);vFr=r(_z,`Note:
Loading a model from its configuration file does `),Mbe=s(_z,"STRONG",{});var ydt=n(Mbe);TFr=r(ydt,"not"),ydt.forEach(t),FFr=r(_z,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ebe=s(_z,"CODE",{});var wdt=n(Ebe);CFr=r(wdt,"from_pretrained()"),wdt.forEach(t),MFr=r(_z,"to load the model weights."),_z.forEach(t),EFr=i(li),ybe=s(li,"P",{});var Adt=n(ybe);yFr=r(Adt,"Examples:"),Adt.forEach(t),wFr=i(li),f(A6.$$.fragment,li),li.forEach(t),AFr=i(ni),ko=s(ni,"DIV",{class:!0});var wa=n(ko);f(L6.$$.fragment,wa),LFr=i(wa),wbe=s(wa,"P",{});var Ldt=n(wbe);BFr=r(Ldt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Ldt.forEach(t),xFr=i(wa),Ms=s(wa,"P",{});var F3=n(Ms);kFr=r(F3,"The model class to instantiate is selected based on the "),Abe=s(F3,"CODE",{});var Bdt=n(Abe);RFr=r(Bdt,"model_type"),Bdt.forEach(t),SFr=r(F3,` property of the config object (either
passed as an argument or loaded from `),Lbe=s(F3,"CODE",{});var xdt=n(Lbe);PFr=r(xdt,"pretrained_model_name_or_path"),xdt.forEach(t),$Fr=r(F3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bbe=s(F3,"CODE",{});var kdt=n(Bbe);IFr=r(kdt,"pretrained_model_name_or_path"),kdt.forEach(t),jFr=r(F3,":"),F3.forEach(t),NFr=i(wa),ao=s(wa,"UL",{});var Yt=n(ao);nE=s(Yt,"LI",{});var H6e=n(nE);xbe=s(H6e,"STRONG",{});var Rdt=n(xbe);DFr=r(Rdt,"albert"),Rdt.forEach(t),qFr=r(H6e," \u2014 "),vO=s(H6e,"A",{href:!0});var Sdt=n(vO);GFr=r(Sdt,"FlaxAlbertForTokenClassification"),Sdt.forEach(t),OFr=r(H6e," (ALBERT model)"),H6e.forEach(t),XFr=i(Yt),lE=s(Yt,"LI",{});var U6e=n(lE);kbe=s(U6e,"STRONG",{});var Pdt=n(kbe);zFr=r(Pdt,"bert"),Pdt.forEach(t),VFr=r(U6e," \u2014 "),TO=s(U6e,"A",{href:!0});var $dt=n(TO);WFr=r($dt,"FlaxBertForTokenClassification"),$dt.forEach(t),QFr=r(U6e," (BERT model)"),U6e.forEach(t),HFr=i(Yt),iE=s(Yt,"LI",{});var J6e=n(iE);Rbe=s(J6e,"STRONG",{});var Idt=n(Rbe);UFr=r(Idt,"big_bird"),Idt.forEach(t),JFr=r(J6e," \u2014 "),FO=s(J6e,"A",{href:!0});var jdt=n(FO);YFr=r(jdt,"FlaxBigBirdForTokenClassification"),jdt.forEach(t),KFr=r(J6e," (BigBird model)"),J6e.forEach(t),ZFr=i(Yt),dE=s(Yt,"LI",{});var Y6e=n(dE);Sbe=s(Y6e,"STRONG",{});var Ndt=n(Sbe);eCr=r(Ndt,"distilbert"),Ndt.forEach(t),oCr=r(Y6e," \u2014 "),CO=s(Y6e,"A",{href:!0});var Ddt=n(CO);rCr=r(Ddt,"FlaxDistilBertForTokenClassification"),Ddt.forEach(t),tCr=r(Y6e," (DistilBERT model)"),Y6e.forEach(t),aCr=i(Yt),cE=s(Yt,"LI",{});var K6e=n(cE);Pbe=s(K6e,"STRONG",{});var qdt=n(Pbe);sCr=r(qdt,"electra"),qdt.forEach(t),nCr=r(K6e," \u2014 "),MO=s(K6e,"A",{href:!0});var Gdt=n(MO);lCr=r(Gdt,"FlaxElectraForTokenClassification"),Gdt.forEach(t),iCr=r(K6e," (ELECTRA model)"),K6e.forEach(t),dCr=i(Yt),mE=s(Yt,"LI",{});var Z6e=n(mE);$be=s(Z6e,"STRONG",{});var Odt=n($be);cCr=r(Odt,"roberta"),Odt.forEach(t),mCr=r(Z6e," \u2014 "),EO=s(Z6e,"A",{href:!0});var Xdt=n(EO);fCr=r(Xdt,"FlaxRobertaForTokenClassification"),Xdt.forEach(t),gCr=r(Z6e," (RoBERTa model)"),Z6e.forEach(t),hCr=i(Yt),fE=s(Yt,"LI",{});var eLe=n(fE);Ibe=s(eLe,"STRONG",{});var zdt=n(Ibe);uCr=r(zdt,"roformer"),zdt.forEach(t),pCr=r(eLe," \u2014 "),yO=s(eLe,"A",{href:!0});var Vdt=n(yO);_Cr=r(Vdt,"FlaxRoFormerForTokenClassification"),Vdt.forEach(t),bCr=r(eLe," (RoFormer model)"),eLe.forEach(t),Yt.forEach(t),vCr=i(wa),jbe=s(wa,"P",{});var Wdt=n(jbe);TCr=r(Wdt,"Examples:"),Wdt.forEach(t),FCr=i(wa),f(B6.$$.fragment,wa),wa.forEach(t),ni.forEach(t),o8e=i(d),Yc=s(d,"H2",{class:!0});var nBe=n(Yc);gE=s(nBe,"A",{id:!0,class:!0,href:!0});var Qdt=n(gE);Nbe=s(Qdt,"SPAN",{});var Hdt=n(Nbe);f(x6.$$.fragment,Hdt),Hdt.forEach(t),Qdt.forEach(t),CCr=i(nBe),Dbe=s(nBe,"SPAN",{});var Udt=n(Dbe);MCr=r(Udt,"FlaxAutoModelForMultipleChoice"),Udt.forEach(t),nBe.forEach(t),r8e=i(d),xr=s(d,"DIV",{class:!0});var ii=n(xr);f(k6.$$.fragment,ii),ECr=i(ii),Kc=s(ii,"P",{});var bz=n(Kc);yCr=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),qbe=s(bz,"CODE",{});var Jdt=n(qbe);wCr=r(Jdt,"from_pretrained()"),Jdt.forEach(t),ACr=r(bz,"class method or the "),Gbe=s(bz,"CODE",{});var Ydt=n(Gbe);LCr=r(Ydt,"from_config()"),Ydt.forEach(t),BCr=r(bz,`class
method.`),bz.forEach(t),xCr=i(ii),R6=s(ii,"P",{});var lBe=n(R6);kCr=r(lBe,"This class cannot be instantiated directly using "),Obe=s(lBe,"CODE",{});var Kdt=n(Obe);RCr=r(Kdt,"__init__()"),Kdt.forEach(t),SCr=r(lBe," (throws an error)."),lBe.forEach(t),PCr=i(ii),Ct=s(ii,"DIV",{class:!0});var di=n(Ct);f(S6.$$.fragment,di),$Cr=i(di),Xbe=s(di,"P",{});var Zdt=n(Xbe);ICr=r(Zdt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Zdt.forEach(t),jCr=i(di),Zc=s(di,"P",{});var vz=n(Zc);NCr=r(vz,`Note:
Loading a model from its configuration file does `),zbe=s(vz,"STRONG",{});var ect=n(zbe);DCr=r(ect,"not"),ect.forEach(t),qCr=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vbe=s(vz,"CODE",{});var oct=n(Vbe);GCr=r(oct,"from_pretrained()"),oct.forEach(t),OCr=r(vz,"to load the model weights."),vz.forEach(t),XCr=i(di),Wbe=s(di,"P",{});var rct=n(Wbe);zCr=r(rct,"Examples:"),rct.forEach(t),VCr=i(di),f(P6.$$.fragment,di),di.forEach(t),WCr=i(ii),Ro=s(ii,"DIV",{class:!0});var Aa=n(Ro);f($6.$$.fragment,Aa),QCr=i(Aa),Qbe=s(Aa,"P",{});var tct=n(Qbe);HCr=r(tct,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tct.forEach(t),UCr=i(Aa),Es=s(Aa,"P",{});var C3=n(Es);JCr=r(C3,"The model class to instantiate is selected based on the "),Hbe=s(C3,"CODE",{});var act=n(Hbe);YCr=r(act,"model_type"),act.forEach(t),KCr=r(C3,` property of the config object (either
passed as an argument or loaded from `),Ube=s(C3,"CODE",{});var sct=n(Ube);ZCr=r(sct,"pretrained_model_name_or_path"),sct.forEach(t),e4r=r(C3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jbe=s(C3,"CODE",{});var nct=n(Jbe);o4r=r(nct,"pretrained_model_name_or_path"),nct.forEach(t),r4r=r(C3,":"),C3.forEach(t),t4r=i(Aa),so=s(Aa,"UL",{});var Kt=n(so);hE=s(Kt,"LI",{});var oLe=n(hE);Ybe=s(oLe,"STRONG",{});var lct=n(Ybe);a4r=r(lct,"albert"),lct.forEach(t),s4r=r(oLe," \u2014 "),wO=s(oLe,"A",{href:!0});var ict=n(wO);n4r=r(ict,"FlaxAlbertForMultipleChoice"),ict.forEach(t),l4r=r(oLe," (ALBERT model)"),oLe.forEach(t),i4r=i(Kt),uE=s(Kt,"LI",{});var rLe=n(uE);Kbe=s(rLe,"STRONG",{});var dct=n(Kbe);d4r=r(dct,"bert"),dct.forEach(t),c4r=r(rLe," \u2014 "),AO=s(rLe,"A",{href:!0});var cct=n(AO);m4r=r(cct,"FlaxBertForMultipleChoice"),cct.forEach(t),f4r=r(rLe," (BERT model)"),rLe.forEach(t),g4r=i(Kt),pE=s(Kt,"LI",{});var tLe=n(pE);Zbe=s(tLe,"STRONG",{});var mct=n(Zbe);h4r=r(mct,"big_bird"),mct.forEach(t),u4r=r(tLe," \u2014 "),LO=s(tLe,"A",{href:!0});var fct=n(LO);p4r=r(fct,"FlaxBigBirdForMultipleChoice"),fct.forEach(t),_4r=r(tLe," (BigBird model)"),tLe.forEach(t),b4r=i(Kt),_E=s(Kt,"LI",{});var aLe=n(_E);e2e=s(aLe,"STRONG",{});var gct=n(e2e);v4r=r(gct,"distilbert"),gct.forEach(t),T4r=r(aLe," \u2014 "),BO=s(aLe,"A",{href:!0});var hct=n(BO);F4r=r(hct,"FlaxDistilBertForMultipleChoice"),hct.forEach(t),C4r=r(aLe," (DistilBERT model)"),aLe.forEach(t),M4r=i(Kt),bE=s(Kt,"LI",{});var sLe=n(bE);o2e=s(sLe,"STRONG",{});var uct=n(o2e);E4r=r(uct,"electra"),uct.forEach(t),y4r=r(sLe," \u2014 "),xO=s(sLe,"A",{href:!0});var pct=n(xO);w4r=r(pct,"FlaxElectraForMultipleChoice"),pct.forEach(t),A4r=r(sLe," (ELECTRA model)"),sLe.forEach(t),L4r=i(Kt),vE=s(Kt,"LI",{});var nLe=n(vE);r2e=s(nLe,"STRONG",{});var _ct=n(r2e);B4r=r(_ct,"roberta"),_ct.forEach(t),x4r=r(nLe," \u2014 "),kO=s(nLe,"A",{href:!0});var bct=n(kO);k4r=r(bct,"FlaxRobertaForMultipleChoice"),bct.forEach(t),R4r=r(nLe," (RoBERTa model)"),nLe.forEach(t),S4r=i(Kt),TE=s(Kt,"LI",{});var lLe=n(TE);t2e=s(lLe,"STRONG",{});var vct=n(t2e);P4r=r(vct,"roformer"),vct.forEach(t),$4r=r(lLe," \u2014 "),RO=s(lLe,"A",{href:!0});var Tct=n(RO);I4r=r(Tct,"FlaxRoFormerForMultipleChoice"),Tct.forEach(t),j4r=r(lLe," (RoFormer model)"),lLe.forEach(t),Kt.forEach(t),N4r=i(Aa),a2e=s(Aa,"P",{});var Fct=n(a2e);D4r=r(Fct,"Examples:"),Fct.forEach(t),q4r=i(Aa),f(I6.$$.fragment,Aa),Aa.forEach(t),ii.forEach(t),t8e=i(d),em=s(d,"H2",{class:!0});var iBe=n(em);FE=s(iBe,"A",{id:!0,class:!0,href:!0});var Cct=n(FE);s2e=s(Cct,"SPAN",{});var Mct=n(s2e);f(j6.$$.fragment,Mct),Mct.forEach(t),Cct.forEach(t),G4r=i(iBe),n2e=s(iBe,"SPAN",{});var Ect=n(n2e);O4r=r(Ect,"FlaxAutoModelForNextSentencePrediction"),Ect.forEach(t),iBe.forEach(t),a8e=i(d),kr=s(d,"DIV",{class:!0});var ci=n(kr);f(N6.$$.fragment,ci),X4r=i(ci),om=s(ci,"P",{});var Tz=n(om);z4r=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),l2e=s(Tz,"CODE",{});var yct=n(l2e);V4r=r(yct,"from_pretrained()"),yct.forEach(t),W4r=r(Tz,"class method or the "),i2e=s(Tz,"CODE",{});var wct=n(i2e);Q4r=r(wct,"from_config()"),wct.forEach(t),H4r=r(Tz,`class
method.`),Tz.forEach(t),U4r=i(ci),D6=s(ci,"P",{});var dBe=n(D6);J4r=r(dBe,"This class cannot be instantiated directly using "),d2e=s(dBe,"CODE",{});var Act=n(d2e);Y4r=r(Act,"__init__()"),Act.forEach(t),K4r=r(dBe," (throws an error)."),dBe.forEach(t),Z4r=i(ci),Mt=s(ci,"DIV",{class:!0});var mi=n(Mt);f(q6.$$.fragment,mi),eMr=i(mi),c2e=s(mi,"P",{});var Lct=n(c2e);oMr=r(Lct,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Lct.forEach(t),rMr=i(mi),rm=s(mi,"P",{});var Fz=n(rm);tMr=r(Fz,`Note:
Loading a model from its configuration file does `),m2e=s(Fz,"STRONG",{});var Bct=n(m2e);aMr=r(Bct,"not"),Bct.forEach(t),sMr=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),f2e=s(Fz,"CODE",{});var xct=n(f2e);nMr=r(xct,"from_pretrained()"),xct.forEach(t),lMr=r(Fz,"to load the model weights."),Fz.forEach(t),iMr=i(mi),g2e=s(mi,"P",{});var kct=n(g2e);dMr=r(kct,"Examples:"),kct.forEach(t),cMr=i(mi),f(G6.$$.fragment,mi),mi.forEach(t),mMr=i(ci),So=s(ci,"DIV",{class:!0});var La=n(So);f(O6.$$.fragment,La),fMr=i(La),h2e=s(La,"P",{});var Rct=n(h2e);gMr=r(Rct,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Rct.forEach(t),hMr=i(La),ys=s(La,"P",{});var M3=n(ys);uMr=r(M3,"The model class to instantiate is selected based on the "),u2e=s(M3,"CODE",{});var Sct=n(u2e);pMr=r(Sct,"model_type"),Sct.forEach(t),_Mr=r(M3,` property of the config object (either
passed as an argument or loaded from `),p2e=s(M3,"CODE",{});var Pct=n(p2e);bMr=r(Pct,"pretrained_model_name_or_path"),Pct.forEach(t),vMr=r(M3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_2e=s(M3,"CODE",{});var $ct=n(_2e);TMr=r($ct,"pretrained_model_name_or_path"),$ct.forEach(t),FMr=r(M3,":"),M3.forEach(t),CMr=i(La),b2e=s(La,"UL",{});var Ict=n(b2e);CE=s(Ict,"LI",{});var iLe=n(CE);v2e=s(iLe,"STRONG",{});var jct=n(v2e);MMr=r(jct,"bert"),jct.forEach(t),EMr=r(iLe," \u2014 "),SO=s(iLe,"A",{href:!0});var Nct=n(SO);yMr=r(Nct,"FlaxBertForNextSentencePrediction"),Nct.forEach(t),wMr=r(iLe," (BERT model)"),iLe.forEach(t),Ict.forEach(t),AMr=i(La),T2e=s(La,"P",{});var Dct=n(T2e);LMr=r(Dct,"Examples:"),Dct.forEach(t),BMr=i(La),f(X6.$$.fragment,La),La.forEach(t),ci.forEach(t),s8e=i(d),tm=s(d,"H2",{class:!0});var cBe=n(tm);ME=s(cBe,"A",{id:!0,class:!0,href:!0});var qct=n(ME);F2e=s(qct,"SPAN",{});var Gct=n(F2e);f(z6.$$.fragment,Gct),Gct.forEach(t),qct.forEach(t),xMr=i(cBe),C2e=s(cBe,"SPAN",{});var Oct=n(C2e);kMr=r(Oct,"FlaxAutoModelForImageClassification"),Oct.forEach(t),cBe.forEach(t),n8e=i(d),Rr=s(d,"DIV",{class:!0});var fi=n(Rr);f(V6.$$.fragment,fi),RMr=i(fi),am=s(fi,"P",{});var Cz=n(am);SMr=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),M2e=s(Cz,"CODE",{});var Xct=n(M2e);PMr=r(Xct,"from_pretrained()"),Xct.forEach(t),$Mr=r(Cz,"class method or the "),E2e=s(Cz,"CODE",{});var zct=n(E2e);IMr=r(zct,"from_config()"),zct.forEach(t),jMr=r(Cz,`class
method.`),Cz.forEach(t),NMr=i(fi),W6=s(fi,"P",{});var mBe=n(W6);DMr=r(mBe,"This class cannot be instantiated directly using "),y2e=s(mBe,"CODE",{});var Vct=n(y2e);qMr=r(Vct,"__init__()"),Vct.forEach(t),GMr=r(mBe," (throws an error)."),mBe.forEach(t),OMr=i(fi),Et=s(fi,"DIV",{class:!0});var gi=n(Et);f(Q6.$$.fragment,gi),XMr=i(gi),w2e=s(gi,"P",{});var Wct=n(w2e);zMr=r(Wct,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Wct.forEach(t),VMr=i(gi),sm=s(gi,"P",{});var Mz=n(sm);WMr=r(Mz,`Note:
Loading a model from its configuration file does `),A2e=s(Mz,"STRONG",{});var Qct=n(A2e);QMr=r(Qct,"not"),Qct.forEach(t),HMr=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),L2e=s(Mz,"CODE",{});var Hct=n(L2e);UMr=r(Hct,"from_pretrained()"),Hct.forEach(t),JMr=r(Mz,"to load the model weights."),Mz.forEach(t),YMr=i(gi),B2e=s(gi,"P",{});var Uct=n(B2e);KMr=r(Uct,"Examples:"),Uct.forEach(t),ZMr=i(gi),f(H6.$$.fragment,gi),gi.forEach(t),eEr=i(fi),Po=s(fi,"DIV",{class:!0});var Ba=n(Po);f(U6.$$.fragment,Ba),oEr=i(Ba),x2e=s(Ba,"P",{});var Jct=n(x2e);rEr=r(Jct,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Jct.forEach(t),tEr=i(Ba),ws=s(Ba,"P",{});var E3=n(ws);aEr=r(E3,"The model class to instantiate is selected based on the "),k2e=s(E3,"CODE",{});var Yct=n(k2e);sEr=r(Yct,"model_type"),Yct.forEach(t),nEr=r(E3,` property of the config object (either
passed as an argument or loaded from `),R2e=s(E3,"CODE",{});var Kct=n(R2e);lEr=r(Kct,"pretrained_model_name_or_path"),Kct.forEach(t),iEr=r(E3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S2e=s(E3,"CODE",{});var Zct=n(S2e);dEr=r(Zct,"pretrained_model_name_or_path"),Zct.forEach(t),cEr=r(E3,":"),E3.forEach(t),mEr=i(Ba),J6=s(Ba,"UL",{});var fBe=n(J6);EE=s(fBe,"LI",{});var dLe=n(EE);P2e=s(dLe,"STRONG",{});var emt=n(P2e);fEr=r(emt,"beit"),emt.forEach(t),gEr=r(dLe," \u2014 "),PO=s(dLe,"A",{href:!0});var omt=n(PO);hEr=r(omt,"FlaxBeitForImageClassification"),omt.forEach(t),uEr=r(dLe," (BEiT model)"),dLe.forEach(t),pEr=i(fBe),yE=s(fBe,"LI",{});var cLe=n(yE);$2e=s(cLe,"STRONG",{});var rmt=n($2e);_Er=r(rmt,"vit"),rmt.forEach(t),bEr=r(cLe," \u2014 "),$O=s(cLe,"A",{href:!0});var tmt=n($O);vEr=r(tmt,"FlaxViTForImageClassification"),tmt.forEach(t),TEr=r(cLe," (ViT model)"),cLe.forEach(t),fBe.forEach(t),FEr=i(Ba),I2e=s(Ba,"P",{});var amt=n(I2e);CEr=r(amt,"Examples:"),amt.forEach(t),MEr=i(Ba),f(Y6.$$.fragment,Ba),Ba.forEach(t),fi.forEach(t),l8e=i(d),nm=s(d,"H2",{class:!0});var gBe=n(nm);wE=s(gBe,"A",{id:!0,class:!0,href:!0});var smt=n(wE);j2e=s(smt,"SPAN",{});var nmt=n(j2e);f(K6.$$.fragment,nmt),nmt.forEach(t),smt.forEach(t),EEr=i(gBe),N2e=s(gBe,"SPAN",{});var lmt=n(N2e);yEr=r(lmt,"FlaxAutoModelForVision2Seq"),lmt.forEach(t),gBe.forEach(t),i8e=i(d),Sr=s(d,"DIV",{class:!0});var hi=n(Sr);f(Z6.$$.fragment,hi),wEr=i(hi),lm=s(hi,"P",{});var Ez=n(lm);AEr=r(Ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),D2e=s(Ez,"CODE",{});var imt=n(D2e);LEr=r(imt,"from_pretrained()"),imt.forEach(t),BEr=r(Ez,"class method or the "),q2e=s(Ez,"CODE",{});var dmt=n(q2e);xEr=r(dmt,"from_config()"),dmt.forEach(t),kEr=r(Ez,`class
method.`),Ez.forEach(t),REr=i(hi),eL=s(hi,"P",{});var hBe=n(eL);SEr=r(hBe,"This class cannot be instantiated directly using "),G2e=s(hBe,"CODE",{});var cmt=n(G2e);PEr=r(cmt,"__init__()"),cmt.forEach(t),$Er=r(hBe," (throws an error)."),hBe.forEach(t),IEr=i(hi),yt=s(hi,"DIV",{class:!0});var ui=n(yt);f(oL.$$.fragment,ui),jEr=i(ui),O2e=s(ui,"P",{});var mmt=n(O2e);NEr=r(mmt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),mmt.forEach(t),DEr=i(ui),im=s(ui,"P",{});var yz=n(im);qEr=r(yz,`Note:
Loading a model from its configuration file does `),X2e=s(yz,"STRONG",{});var fmt=n(X2e);GEr=r(fmt,"not"),fmt.forEach(t),OEr=r(yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),z2e=s(yz,"CODE",{});var gmt=n(z2e);XEr=r(gmt,"from_pretrained()"),gmt.forEach(t),zEr=r(yz,"to load the model weights."),yz.forEach(t),VEr=i(ui),V2e=s(ui,"P",{});var hmt=n(V2e);WEr=r(hmt,"Examples:"),hmt.forEach(t),QEr=i(ui),f(rL.$$.fragment,ui),ui.forEach(t),HEr=i(hi),$o=s(hi,"DIV",{class:!0});var xa=n($o);f(tL.$$.fragment,xa),UEr=i(xa),W2e=s(xa,"P",{});var umt=n(W2e);JEr=r(umt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),umt.forEach(t),YEr=i(xa),As=s(xa,"P",{});var y3=n(As);KEr=r(y3,"The model class to instantiate is selected based on the "),Q2e=s(y3,"CODE",{});var pmt=n(Q2e);ZEr=r(pmt,"model_type"),pmt.forEach(t),e3r=r(y3,` property of the config object (either
passed as an argument or loaded from `),H2e=s(y3,"CODE",{});var _mt=n(H2e);o3r=r(_mt,"pretrained_model_name_or_path"),_mt.forEach(t),r3r=r(y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),U2e=s(y3,"CODE",{});var bmt=n(U2e);t3r=r(bmt,"pretrained_model_name_or_path"),bmt.forEach(t),a3r=r(y3,":"),y3.forEach(t),s3r=i(xa),J2e=s(xa,"UL",{});var vmt=n(J2e);AE=s(vmt,"LI",{});var mLe=n(AE);Y2e=s(mLe,"STRONG",{});var Tmt=n(Y2e);n3r=r(Tmt,"vision-encoder-decoder"),Tmt.forEach(t),l3r=r(mLe," \u2014 "),IO=s(mLe,"A",{href:!0});var Fmt=n(IO);i3r=r(Fmt,"FlaxVisionEncoderDecoderModel"),Fmt.forEach(t),d3r=r(mLe," (Vision Encoder decoder model)"),mLe.forEach(t),vmt.forEach(t),c3r=i(xa),K2e=s(xa,"P",{});var Cmt=n(K2e);m3r=r(Cmt,"Examples:"),Cmt.forEach(t),f3r=i(xa),f(aL.$$.fragment,xa),xa.forEach(t),hi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(kmt)),c(fe,"id","auto-classes"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#auto-classes"),c(le,"class","relative group"),c(Ls,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),c(xs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),c(ks,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),c(Mi,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(hm,"id","extending-the-auto-classes"),c(hm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hm,"href","#extending-the-auto-classes"),c(Ei,"class","relative group"),c(pm,"id","transformers.AutoConfig"),c(pm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pm,"href","#transformers.AutoConfig"),c(yi,"class","relative group"),c(l7,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(i7,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),c(d7,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),c(c7,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),c(m7,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),c(f7,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(g7,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig"),c(h7,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(u7,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(p7,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(_7,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),c(b7,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),c(v7,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),c(T7,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),c(F7,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig"),c(C7,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),c(M7,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),c(E7,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(y7,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),c(w7,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),c(A7,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),c(L7,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),c(B7,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),c(x7,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(k7,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),c(R7,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),c(S7,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),c(P7,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),c($7,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),c(I7,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(j7,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),c(N7,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),c(D7,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),c(q7,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(G7,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(O7,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(X7,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),c(z7,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),c(V7,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),c(W7,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),c(Q7,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),c(H7,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),c(U7,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),c(J7,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c(Y7,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(K7,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),c(Z7,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),c(e8,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(o8,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(r8,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),c(t8,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),c(a8,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(s8,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),c(n8,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmConfig"),c(l8,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),c(i8,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),c(d8,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),c(c8,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),c(m8,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),c(f8,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),c(g8,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),c(h8,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig"),c(u8,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(p8,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(_8,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(b8,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),c(v8,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(T8,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig"),c(F8,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),c(C8,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),c(M8,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(E8,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),c(y8,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(w8,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(A8,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig"),c(L8,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(B8,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(x8,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(k8,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),c(R8,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(S8,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(P8,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig"),c($8,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig"),c(I8,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),c(j8,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(N8,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(D8,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(q8,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),c(G8,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig"),c(io,"class","docstring"),c(Zf,"class","docstring"),c(qo,"class","docstring"),c(eg,"id","transformers.AutoTokenizer"),c(eg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eg,"href","#transformers.AutoTokenizer"),c(Ai,"class","relative group"),c(O8,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(X8,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),c(z8,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(V8,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),c(W8,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),c(Q8,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),c(H8,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(U8,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(J8,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(Y8,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(K8,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(Z8,"href","/docs/transformers/master/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(e9,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(o9,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(r9,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(t9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(a9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(s9,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(n9,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(l9,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(i9,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(d9,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),c(c9,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(m9,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),c(f9,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),c(g9,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(h9,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(u9,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(p9,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),c(_9,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(b9,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),c(v9,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(T9,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(F9,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(C9,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(M9,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(E9,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(y9,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),c(w9,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(A9,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(L9,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),c(B9,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(x9,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(k9,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),c(R9,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(S9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(P9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c($9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(I9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(j9,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizer"),c(N9,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(D9,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(q9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(G9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(O9,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(X9,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(z9,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(V9,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(W9,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(Q9,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c(H9,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),c(U9,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),c(J9,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),c(Y9,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(K9,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),c(Z9,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(eB,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(oB,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(rB,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),c(tB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),c(aB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(sB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(nB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(lB,"href","/docs/transformers/master/en/model_doc/mluke#transformers.MLukeTokenizer"),c(iB,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(dB,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(cB,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(mB,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(fB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Tokenizer"),c(gB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5TokenizerFast"),c(hB,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(uB,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(pB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(_B,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(bB,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(vB,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),c(TB,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(FB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(CB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(MB,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),c(EB,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),c(yB,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(wB,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),c(AB,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(LB,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(BB,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(xB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(kB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(RB,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(SB,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(PB,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c($B,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(IB,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),c(jB,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(NB,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(DB,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(qB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Tokenizer"),c(GB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5TokenizerFast"),c(OB,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),c(XB,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(zB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(VB,"href","/docs/transformers/master/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(WB,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizer"),c(QB,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(HB,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),c(UB,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(JB,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(YB,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(KB,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(ZB,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(co,"class","docstring"),c(Bg,"class","docstring"),c(Go,"class","docstring"),c(xg,"id","transformers.AutoFeatureExtractor"),c(xg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xg,"href","#transformers.AutoFeatureExtractor"),c(Li,"class","relative group"),c(ex,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(ox,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(rx,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(tx,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(ax,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(sx,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(nx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(lx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(ix,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(dx,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(cx,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(mx,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(fx,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(gx,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(hx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(Zt,"class","docstring"),c(Wg,"id","transformers.AutoProcessor"),c(Wg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wg,"href","#transformers.AutoProcessor"),c(Bi,"class","relative group"),c(ux,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(px,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),c(_x,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(bx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(vx,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(Tx,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(Fx,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),c(Cx,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(Mx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(ea,"class","docstring"),c(rh,"id","transformers.AutoModel"),c(rh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rh,"href","#transformers.AutoModel"),c(ki,"class","relative group"),c(Pr,"class","docstring"),c(Ex,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),c(yx,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),c(wx,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),c(Ax,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(Lx,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(Bx,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel"),c(xx,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(kx,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Rx,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Sx,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),c(Px,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),c($x,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),c(Ix,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),c(jx,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel"),c(Nx,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),c(Dx,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),c(qx,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(Gx,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),c(Ox,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),c(Xx,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),c(zx,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Vx,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),c(Wx,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),c(Qx,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),c(Hx,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),c(Ux,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(Jx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Yx,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),c(Kx,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Zx,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),c(ek,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),c(ok,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),c(rk,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(tk,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(ak,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(sk,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),c(nk,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),c(lk,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),c(ik,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),c(dk,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),c(ck,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),c(mk,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),c(fk,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(gk,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),c(hk,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),c(uk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),c(pk,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel"),c(_k,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(bk,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),c(vk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),c(Tk,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(Fk,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),c(Ck,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),c(Mk,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(Ek,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),c(yk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),c(wk,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),c(Ak,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),c(Lk,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel"),c(Bk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(xk,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),c(kk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(Rk,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinModel"),c(Sk,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),c(Pk,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),c($k,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(Ik,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),c(jk,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(Nk,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel"),c(Dk,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(qk,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),c(Gk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),c(Ok,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel"),c(Xk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(zk,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel"),c(Vk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel"),c(Wk,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),c(Qk,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(Hk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(Uk,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(Jk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),c(Yk,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel"),c(xe,"class","docstring"),c(Oo,"class","docstring"),c(ku,"id","transformers.AutoModelForPreTraining"),c(ku,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ku,"href","#transformers.AutoModelForPreTraining"),c(Pi,"class","relative group"),c($r,"class","docstring"),c(Kk,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),c(Zk,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(eR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),c(oR,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(rR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(tR,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(aR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(sR,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(nR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(lR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),c(iR,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(dR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),c(cR,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(mR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(fR,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(gR,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(hR,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(uR,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(pR,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(_R,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(bR,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(vR,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(TR,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(FR,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(CR,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(MR,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(ER,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(yR,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(wR,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(AR,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(LR,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(BR,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(xR,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(kR,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(RR,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(SR,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(PR,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c($R,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(ke,"class","docstring"),c(Xo,"class","docstring"),c(_p,"id","transformers.AutoModelForCausalLM"),c(_p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_p,"href","#transformers.AutoModelForCausalLM"),c(ji,"class","relative group"),c(Ir,"class","docstring"),c(IR,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),c(jR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),c(NR,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(DR,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(qR,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(GR,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(OR,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c(XR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(zR,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(VR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM"),c(WR,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(QR,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(HR,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(UR,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),c(JR,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),c(YR,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(KR,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(ZR,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(eS,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(oS,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(rS,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(tS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(aS,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(sS,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(nS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(lS,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(iS,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(dS,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(cS,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(mS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(fS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(gS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(zo,"class","docstring"),c(Kp,"id","transformers.AutoModelForMaskedLM"),c(Kp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kp,"href","#transformers.AutoModelForMaskedLM"),c(qi,"class","relative group"),c(jr,"class","docstring"),c(hS,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(uS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(pS,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),c(_S,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(bS,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(vS,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(TS,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(FS,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(CS,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(MS,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(ES,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(yS,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(wS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(AS,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(LS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(BS,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(xS,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(kS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(RS,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(SS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(PS,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c($S,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(IS,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(jS,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(NS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(DS,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(qS,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(GS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(OS,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(XS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(zS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(VS,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Vo,"class","docstring"),c(P_,"id","transformers.AutoModelForSeq2SeqLM"),c(P_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P_,"href","#transformers.AutoModelForSeq2SeqLM"),c(Xi,"class","relative group"),c(Nr,"class","docstring"),c(WS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(QS,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(HS,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(US,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(JS,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(YS,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(KS,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(ZS,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(eP,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),c(oP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(rP,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(tP,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(aP,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(sP,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(nP,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Wo,"class","docstring"),c(Y_,"id","transformers.AutoModelForSequenceClassification"),c(Y_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Y_,"href","#transformers.AutoModelForSequenceClassification"),c(Wi,"class","relative group"),c(Dr,"class","docstring"),c(lP,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(iP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),c(dP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),c(cP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(mP,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(fP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(gP,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(hP,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(uP,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(pP,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(_P,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(bP,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(vP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(TP,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(FP,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(CP,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(MP,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(EP,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(yP,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(wP,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(AP,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(LP,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(BP,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),c(xP,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(kP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(RP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(SP,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(PP,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c($P,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(IP,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(jP,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(NP,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(DP,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(qP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(GP,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(OP,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(XP,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(zP,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(VP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(WP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(QP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(HP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(UP,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Qo,"class","docstring"),c(Xb,"id","transformers.AutoModelForMultipleChoice"),c(Xb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xb,"href","#transformers.AutoModelForMultipleChoice"),c(Ui,"class","relative group"),c(qr,"class","docstring"),c(JP,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(YP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),c(KP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(ZP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(e$,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(o$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(r$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(t$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(a$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(s$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(n$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(l$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(i$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(d$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(c$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(m$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(f$,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(g$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(h$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(u$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(p$,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(_$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(b$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(v$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(T$,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(F$,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Ho,"class","docstring"),c(b2,"id","transformers.AutoModelForNextSentencePrediction"),c(b2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b2,"href","#transformers.AutoModelForNextSentencePrediction"),c(Ki,"class","relative group"),c(Gr,"class","docstring"),c(C$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(M$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(E$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(y$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(je,"class","docstring"),c(Uo,"class","docstring"),c(y2,"id","transformers.AutoModelForTokenClassification"),c(y2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(y2,"href","#transformers.AutoModelForTokenClassification"),c(od,"class","relative group"),c(Or,"class","docstring"),c(w$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(A$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),c(L$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(B$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(x$,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),c(k$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(R$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(S$,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(P$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c($$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(I$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(j$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(N$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(D$,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(q$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(G$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(O$,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(X$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(z$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(V$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(W$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(Q$,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(H$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(U$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(J$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(Y$,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(K$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(Z$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(eI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(oI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(rI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Jo,"class","docstring"),c(sv,"id","transformers.AutoModelForQuestionAnswering"),c(sv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sv,"href","#transformers.AutoModelForQuestionAnswering"),c(ad,"class","relative group"),c(Xr,"class","docstring"),c(tI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(aI,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(sI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(nI,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(lI,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(iI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(dI,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(cI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(mI,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(fI,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(gI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(hI,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(uI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(pI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(_I,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(bI,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(vI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(TI,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(FI,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(CI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(MI,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(EI,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(yI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(wI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(AI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(LI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(BI,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(xI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(kI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(RI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(SI,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(PI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c($I,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(II,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(jI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(NI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(DI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(Yo,"class","docstring"),c(Vv,"id","transformers.AutoModelForTableQuestionAnswering"),c(Vv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vv,"href","#transformers.AutoModelForTableQuestionAnswering"),c(ld,"class","relative group"),c(zr,"class","docstring"),c(qI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(Ko,"class","docstring"),c(Hv,"id","transformers.AutoModelForImageClassification"),c(Hv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Hv,"href","#transformers.AutoModelForImageClassification"),c(cd,"class","relative group"),c(Vr,"class","docstring"),c(GI,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),c(OI,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(XI,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),c(zI,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(VI,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(WI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(QI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(HI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(UI,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(JI,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification"),c(YI,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(Zo,"class","docstring"),c(rT,"id","transformers.AutoModelForVision2Seq"),c(rT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rT,"href","#transformers.AutoModelForVision2Seq"),c(gd,"class","relative group"),c(Wr,"class","docstring"),c(KI,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(er,"class","docstring"),c(sT,"id","transformers.AutoModelForAudioClassification"),c(sT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sT,"href","#transformers.AutoModelForAudioClassification"),c(pd,"class","relative group"),c(Qr,"class","docstring"),c(ZI,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(ej,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(oj,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(rj,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(tj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(aj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(sj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(or,"class","docstring"),c(hT,"id","transformers.AutoModelForAudioFrameClassification"),c(hT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hT,"href","#transformers.AutoModelForAudioFrameClassification"),c(vd,"class","relative group"),c(Hr,"class","docstring"),c(nj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(lj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(ij,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(rr,"class","docstring"),c(vT,"id","transformers.AutoModelForCTC"),c(vT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vT,"href","#transformers.AutoModelForCTC"),c(Md,"class","relative group"),c(Ur,"class","docstring"),c(dj,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),c(cj,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),c(mj,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC"),c(fj,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(gj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(hj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(uj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(tr,"class","docstring"),c(LT,"id","transformers.AutoModelForSpeechSeq2Seq"),c(LT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(LT,"href","#transformers.AutoModelForSpeechSeq2Seq"),c(wd,"class","relative group"),c(Jr,"class","docstring"),c(pj,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(_j,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(ar,"class","docstring"),c(RT,"id","transformers.AutoModelForAudioXVector"),c(RT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(RT,"href","#transformers.AutoModelForAudioXVector"),c(Bd,"class","relative group"),c(Yr,"class","docstring"),c(bj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(vj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(Tj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(sr,"class","docstring"),c(jT,"id","transformers.AutoModelForObjectDetection"),c(jT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jT,"href","#transformers.AutoModelForObjectDetection"),c(Sd,"class","relative group"),c(Kr,"class","docstring"),c(Fj,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),c(He,"class","docstring"),c(nr,"class","docstring"),c(qT,"id","transformers.AutoModelForImageSegmentation"),c(qT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qT,"href","#transformers.AutoModelForImageSegmentation"),c(Id,"class","relative group"),c(Zr,"class","docstring"),c(Cj,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),c(Ue,"class","docstring"),c(lr,"class","docstring"),c(XT,"id","transformers.AutoModelForSemanticSegmentation"),c(XT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(XT,"href","#transformers.AutoModelForSemanticSegmentation"),c(Dd,"class","relative group"),c(et,"class","docstring"),c(Mj,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(Ej,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Je,"class","docstring"),c(ir,"class","docstring"),c(QT,"id","transformers.TFAutoModel"),c(QT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(QT,"href","#transformers.TFAutoModel"),c(Od,"class","relative group"),c(ot,"class","docstring"),c(yj,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),c(wj,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),c(Aj,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),c(Lj,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(Bj,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(xj,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),c(kj,"href","/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel"),c(Rj,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),c(Sj,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),c(Pj,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),c($j,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(Ij,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(jj,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(Nj,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),c(Dj,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(qj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),c(Gj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(Oj,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),c(Xj,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),c(zj,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(Vj,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),c(Wj,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),c(Qj,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),c(Hj,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),c(Uj,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),c(Jj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(Yj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),c(Kj,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),c(Zj,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(eN,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),c(oN,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),c(rN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),c(tN,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),c(aN,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(sN,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),c(nN,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),c(lN,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(iN,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),c(dN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(cN,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),c(mN,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(fN,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),c(mo,"class","docstring"),c(dr,"class","docstring"),c(P1,"id","transformers.TFAutoModelForPreTraining"),c(P1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P1,"href","#transformers.TFAutoModelForPreTraining"),c(Vd,"class","relative group"),c(rt,"class","docstring"),c(gN,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(hN,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(uN,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),c(pN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(_N,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(bN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(vN,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(TN,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(FN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(CN,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(MN,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(EN,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(yN,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(wN,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(AN,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(LN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(BN,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(xN,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(kN,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(RN,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(SN,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(PN,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(fo,"class","docstring"),c(cr,"class","docstring"),c(tF,"id","transformers.TFAutoModelForCausalLM"),c(tF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tF,"href","#transformers.TFAutoModelForCausalLM"),c(Hd,"class","relative group"),c(tt,"class","docstring"),c($N,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(IN,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(jN,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(NN,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(DN,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(qN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(GN,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(ON,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(XN,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(zN,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(go,"class","docstring"),c(mr,"class","docstring"),c(hF,"id","transformers.TFAutoModelForImageClassification"),c(hF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hF,"href","#transformers.TFAutoModelForImageClassification"),c(Yd,"class","relative group"),c(at,"class","docstring"),c(VN,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),c(ho,"class","docstring"),c(fr,"class","docstring"),c(pF,"id","transformers.TFAutoModelForMaskedLM"),c(pF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pF,"href","#transformers.TFAutoModelForMaskedLM"),c(ec,"class","relative group"),c(st,"class","docstring"),c(WN,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(QN,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(HN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(UN,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(JN,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(YN,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(KN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(ZN,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(eD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(oD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(rD,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(tD,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(aD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(sD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(nD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(lD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(iD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(dD,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(cD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(mD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(uo,"class","docstring"),c(gr,"class","docstring"),c(jF,"id","transformers.TFAutoModelForSeq2SeqLM"),c(jF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jF,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(tc,"class","relative group"),c(nt,"class","docstring"),c(fD,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(gD,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(hD,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(uD,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(pD,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(_D,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),c(bD,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(vD,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(TD,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(FD,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(po,"class","docstring"),c(hr,"class","docstring"),c(HF,"id","transformers.TFAutoModelForSequenceClassification"),c(HF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(HF,"href","#transformers.TFAutoModelForSequenceClassification"),c(nc,"class","relative group"),c(lt,"class","docstring"),c(CD,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(MD,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(ED,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(yD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(wD,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(AD,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(LD,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c(BD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(xD,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(kD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(RD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(SD,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(PD,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c($D,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(ID,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(jD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(ND,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(DD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(qD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(GD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(OD,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(XD,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(zD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(VD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(WD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(_o,"class","docstring"),c(ur,"class","docstring"),c(TC,"id","transformers.TFAutoModelForMultipleChoice"),c(TC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(TC,"href","#transformers.TFAutoModelForMultipleChoice"),c(dc,"class","relative group"),c(it,"class","docstring"),c(QD,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(HD,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(UD,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(JD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(YD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(KD,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(ZD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(eq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(oq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(rq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(tq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(aq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(sq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(nq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(lq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(iq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(dq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(bo,"class","docstring"),c(pr,"class","docstring"),c(NC,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(NC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(NC,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(fc,"class","relative group"),c(dt,"class","docstring"),c(cq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(vo,"class","docstring"),c(_r,"class","docstring"),c(qC,"id","transformers.TFAutoModelForTokenClassification"),c(qC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qC,"href","#transformers.TFAutoModelForTokenClassification"),c(uc,"class","relative group"),c(ct,"class","docstring"),c(mq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(fq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(gq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(hq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(uq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(pq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(_q,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(bq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(vq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(Tq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(Fq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(Cq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(Mq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(Eq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(yq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(wq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(Aq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(Lq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(Bq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(xq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(To,"class","docstring"),c(br,"class","docstring"),c(l4,"id","transformers.TFAutoModelForQuestionAnswering"),c(l4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(l4,"href","#transformers.TFAutoModelForQuestionAnswering"),c(bc,"class","relative group"),c(mt,"class","docstring"),c(kq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(Rq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(Sq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(Pq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c($q,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(Iq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(jq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(Nq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(Dq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(qq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(Gq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(Oq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(Xq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(zq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(Vq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(Wq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(Qq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(Hq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(Uq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Fo,"class","docstring"),c(vr,"class","docstring"),c(A4,"id","transformers.TFAutoModelForVision2Seq"),c(A4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A4,"href","#transformers.TFAutoModelForVision2Seq"),c(Fc,"class","relative group"),c(ft,"class","docstring"),c(Jq,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Co,"class","docstring"),c(Tr,"class","docstring"),c(B4,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(B4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(B4,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(Ec,"class","relative group"),c(gt,"class","docstring"),c(Yq,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(Mo,"class","docstring"),c(Fr,"class","docstring"),c(k4,"id","transformers.FlaxAutoModel"),c(k4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k4,"href","#transformers.FlaxAutoModel"),c(Ac,"class","relative group"),c(ht,"class","docstring"),c(Kq,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),c(Zq,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),c(eG,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),c(oG,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),c(rG,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(tG,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(aG,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(sG,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),c(nG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(lG,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),c(iG,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(dG,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(cG,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(mG,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),c(fG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),c(gG,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),c(hG,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(uG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(pG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(_G,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),c(bG,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(vG,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),c(TG,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(FG,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(Eo,"class","docstring"),c(Cr,"class","docstring"),c(rM,"id","transformers.FlaxAutoModelForCausalLM"),c(rM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rM,"href","#transformers.FlaxAutoModelForCausalLM"),c(xc,"class","relative group"),c(ut,"class","docstring"),c(CG,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(MG,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(EG,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(yG,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(yo,"class","docstring"),c(Mr,"class","docstring"),c(lM,"id","transformers.FlaxAutoModelForPreTraining"),c(lM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lM,"href","#transformers.FlaxAutoModelForPreTraining"),c(Sc,"class","relative group"),c(pt,"class","docstring"),c(wG,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(AG,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(LG,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(BG,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(xG,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(kG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(RG,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(SG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(PG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c($G,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(IG,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(wo,"class","docstring"),c(Er,"class","docstring"),c(vM,"id","transformers.FlaxAutoModelForMaskedLM"),c(vM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vM,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Ic,"class","relative group"),c(_t,"class","docstring"),c(jG,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(NG,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(DG,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(qG,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(GG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(OG,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(XG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(zG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(VG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Ao,"class","docstring"),c(yr,"class","docstring"),c(BM,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(BM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(BM,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Dc,"class","relative group"),c(bt,"class","docstring"),c(WG,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(QG,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(HG,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(UG,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(JG,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(YG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(KG,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(ZG,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(eO,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(Lo,"class","docstring"),c(wr,"class","docstring"),c(DM,"id","transformers.FlaxAutoModelForSequenceClassification"),c(DM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(DM,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(Oc,"class","relative group"),c(vt,"class","docstring"),c(oO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(rO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(tO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(aO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(sO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(nO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(lO,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(iO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(dO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(Bo,"class","docstring"),c(Ar,"class","docstring"),c(UM,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(UM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(UM,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(Vc,"class","relative group"),c(Tt,"class","docstring"),c(cO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(mO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(fO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(gO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(hO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(uO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(pO,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(_O,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(bO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(xo,"class","docstring"),c(Lr,"class","docstring"),c(sE,"id","transformers.FlaxAutoModelForTokenClassification"),c(sE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sE,"href","#transformers.FlaxAutoModelForTokenClassification"),c(Hc,"class","relative group"),c(Ft,"class","docstring"),c(vO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(TO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(FO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(CO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(MO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(EO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(yO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(ko,"class","docstring"),c(Br,"class","docstring"),c(gE,"id","transformers.FlaxAutoModelForMultipleChoice"),c(gE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gE,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(Yc,"class","relative group"),c(Ct,"class","docstring"),c(wO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(AO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(LO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c(BO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(xO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(kO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(RO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Ro,"class","docstring"),c(xr,"class","docstring"),c(FE,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(FE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(FE,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(em,"class","relative group"),c(Mt,"class","docstring"),c(SO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c(So,"class","docstring"),c(kr,"class","docstring"),c(ME,"id","transformers.FlaxAutoModelForImageClassification"),c(ME,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ME,"href","#transformers.FlaxAutoModelForImageClassification"),c(tm,"class","relative group"),c(Et,"class","docstring"),c(PO,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c($O,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Po,"class","docstring"),c(Rr,"class","docstring"),c(wE,"id","transformers.FlaxAutoModelForVision2Seq"),c(wE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wE,"href","#transformers.FlaxAutoModelForVision2Seq"),c(nm,"class","relative group"),c(yt,"class","docstring"),c(IO,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c($o,"class","docstring"),c(Sr,"class","docstring")},m(d,_){e(document.head,J),b(d,Ae,_),b(d,le,_),e(le,fe),e(fe,oo),g(ce,oo,null),e(le,_e),e(le,No),e(No,_i),b(d,cm,_),b(d,ra,_),e(ra,bi),e(ra,vi),e(vi,w3),e(ra,mm),b(d,Ee,_),b(d,no,_),e(no,Ti),e(no,Ls),e(Ls,A3),e(no,Bs),e(no,xs),e(xs,L3),e(no,Fi),e(no,ks),e(ks,B3),e(no,Ci),b(d,fm,_),g(ka,d,_),b(d,lo,_),b(d,ge,_),e(ge,o7),e(ge,Mi),e(Mi,r7),e(ge,t7),b(d,Do,_),b(d,Ra,_),e(Ra,a7),e(Ra,gm),e(gm,s7),e(Ra,uBe),b(d,fLe,_),b(d,Ei,_),e(Ei,hm),e(hm,wz),g(x3,wz,null),e(Ei,pBe),e(Ei,Az),e(Az,_Be),b(d,gLe,_),b(d,Rs,_),e(Rs,bBe),e(Rs,Lz),e(Lz,vBe),e(Rs,TBe),e(Rs,Bz),e(Bz,FBe),e(Rs,CBe),b(d,hLe,_),g(k3,d,_),b(d,uLe,_),b(d,n7,_),e(n7,MBe),b(d,pLe,_),g(um,d,_),b(d,_Le,_),b(d,yi,_),e(yi,pm),e(pm,xz),g(R3,xz,null),e(yi,EBe),e(yi,kz),e(kz,yBe),b(d,bLe,_),b(d,qo,_),g(S3,qo,null),e(qo,wBe),e(qo,P3),e(P3,ABe),e(P3,l7),e(l7,LBe),e(P3,BBe),e(qo,xBe),e(qo,$3),e($3,kBe),e($3,Rz),e(Rz,RBe),e($3,SBe),e(qo,PBe),e(qo,io),g(I3,io,null),e(io,$Be),e(io,Sz),e(Sz,IBe),e(io,jBe),e(io,wi),e(wi,NBe),e(wi,Pz),e(Pz,DBe),e(wi,qBe),e(wi,$z),e($z,GBe),e(wi,OBe),e(io,XBe),e(io,v),e(v,_m),e(_m,Iz),e(Iz,zBe),e(_m,VBe),e(_m,i7),e(i7,WBe),e(_m,QBe),e(v,HBe),e(v,bm),e(bm,jz),e(jz,UBe),e(bm,JBe),e(bm,d7),e(d7,YBe),e(bm,KBe),e(v,ZBe),e(v,vm),e(vm,Nz),e(Nz,exe),e(vm,oxe),e(vm,c7),e(c7,rxe),e(vm,txe),e(v,axe),e(v,Tm),e(Tm,Dz),e(Dz,sxe),e(Tm,nxe),e(Tm,m7),e(m7,lxe),e(Tm,ixe),e(v,dxe),e(v,Fm),e(Fm,qz),e(qz,cxe),e(Fm,mxe),e(Fm,f7),e(f7,fxe),e(Fm,gxe),e(v,hxe),e(v,Cm),e(Cm,Gz),e(Gz,uxe),e(Cm,pxe),e(Cm,g7),e(g7,_xe),e(Cm,bxe),e(v,vxe),e(v,Mm),e(Mm,Oz),e(Oz,Txe),e(Mm,Fxe),e(Mm,h7),e(h7,Cxe),e(Mm,Mxe),e(v,Exe),e(v,Em),e(Em,Xz),e(Xz,yxe),e(Em,wxe),e(Em,u7),e(u7,Axe),e(Em,Lxe),e(v,Bxe),e(v,ym),e(ym,zz),e(zz,xxe),e(ym,kxe),e(ym,p7),e(p7,Rxe),e(ym,Sxe),e(v,Pxe),e(v,wm),e(wm,Vz),e(Vz,$xe),e(wm,Ixe),e(wm,_7),e(_7,jxe),e(wm,Nxe),e(v,Dxe),e(v,Am),e(Am,Wz),e(Wz,qxe),e(Am,Gxe),e(Am,b7),e(b7,Oxe),e(Am,Xxe),e(v,zxe),e(v,Lm),e(Lm,Qz),e(Qz,Vxe),e(Lm,Wxe),e(Lm,v7),e(v7,Qxe),e(Lm,Hxe),e(v,Uxe),e(v,Bm),e(Bm,Hz),e(Hz,Jxe),e(Bm,Yxe),e(Bm,T7),e(T7,Kxe),e(Bm,Zxe),e(v,eke),e(v,xm),e(xm,Uz),e(Uz,oke),e(xm,rke),e(xm,F7),e(F7,tke),e(xm,ake),e(v,ske),e(v,km),e(km,Jz),e(Jz,nke),e(km,lke),e(km,C7),e(C7,ike),e(km,dke),e(v,cke),e(v,Rm),e(Rm,Yz),e(Yz,mke),e(Rm,fke),e(Rm,M7),e(M7,gke),e(Rm,hke),e(v,uke),e(v,Sm),e(Sm,Kz),e(Kz,pke),e(Sm,_ke),e(Sm,E7),e(E7,bke),e(Sm,vke),e(v,Tke),e(v,Pm),e(Pm,Zz),e(Zz,Fke),e(Pm,Cke),e(Pm,y7),e(y7,Mke),e(Pm,Eke),e(v,yke),e(v,$m),e($m,eV),e(eV,wke),e($m,Ake),e($m,w7),e(w7,Lke),e($m,Bke),e(v,xke),e(v,Im),e(Im,oV),e(oV,kke),e(Im,Rke),e(Im,A7),e(A7,Ske),e(Im,Pke),e(v,$ke),e(v,jm),e(jm,rV),e(rV,Ike),e(jm,jke),e(jm,L7),e(L7,Nke),e(jm,Dke),e(v,qke),e(v,Nm),e(Nm,tV),e(tV,Gke),e(Nm,Oke),e(Nm,B7),e(B7,Xke),e(Nm,zke),e(v,Vke),e(v,Dm),e(Dm,aV),e(aV,Wke),e(Dm,Qke),e(Dm,x7),e(x7,Hke),e(Dm,Uke),e(v,Jke),e(v,qm),e(qm,sV),e(sV,Yke),e(qm,Kke),e(qm,k7),e(k7,Zke),e(qm,eRe),e(v,oRe),e(v,Gm),e(Gm,nV),e(nV,rRe),e(Gm,tRe),e(Gm,R7),e(R7,aRe),e(Gm,sRe),e(v,nRe),e(v,Om),e(Om,lV),e(lV,lRe),e(Om,iRe),e(Om,S7),e(S7,dRe),e(Om,cRe),e(v,mRe),e(v,Xm),e(Xm,iV),e(iV,fRe),e(Xm,gRe),e(Xm,P7),e(P7,hRe),e(Xm,uRe),e(v,pRe),e(v,zm),e(zm,dV),e(dV,_Re),e(zm,bRe),e(zm,$7),e($7,vRe),e(zm,TRe),e(v,FRe),e(v,Vm),e(Vm,cV),e(cV,CRe),e(Vm,MRe),e(Vm,I7),e(I7,ERe),e(Vm,yRe),e(v,wRe),e(v,Wm),e(Wm,mV),e(mV,ARe),e(Wm,LRe),e(Wm,j7),e(j7,BRe),e(Wm,xRe),e(v,kRe),e(v,Qm),e(Qm,fV),e(fV,RRe),e(Qm,SRe),e(Qm,N7),e(N7,PRe),e(Qm,$Re),e(v,IRe),e(v,Hm),e(Hm,gV),e(gV,jRe),e(Hm,NRe),e(Hm,D7),e(D7,DRe),e(Hm,qRe),e(v,GRe),e(v,Um),e(Um,hV),e(hV,ORe),e(Um,XRe),e(Um,q7),e(q7,zRe),e(Um,VRe),e(v,WRe),e(v,Jm),e(Jm,uV),e(uV,QRe),e(Jm,HRe),e(Jm,G7),e(G7,URe),e(Jm,JRe),e(v,YRe),e(v,Ym),e(Ym,pV),e(pV,KRe),e(Ym,ZRe),e(Ym,O7),e(O7,eSe),e(Ym,oSe),e(v,rSe),e(v,Km),e(Km,_V),e(_V,tSe),e(Km,aSe),e(Km,X7),e(X7,sSe),e(Km,nSe),e(v,lSe),e(v,Zm),e(Zm,bV),e(bV,iSe),e(Zm,dSe),e(Zm,z7),e(z7,cSe),e(Zm,mSe),e(v,fSe),e(v,ef),e(ef,vV),e(vV,gSe),e(ef,hSe),e(ef,V7),e(V7,uSe),e(ef,pSe),e(v,_Se),e(v,of),e(of,TV),e(TV,bSe),e(of,vSe),e(of,W7),e(W7,TSe),e(of,FSe),e(v,CSe),e(v,rf),e(rf,FV),e(FV,MSe),e(rf,ESe),e(rf,Q7),e(Q7,ySe),e(rf,wSe),e(v,ASe),e(v,tf),e(tf,CV),e(CV,LSe),e(tf,BSe),e(tf,H7),e(H7,xSe),e(tf,kSe),e(v,RSe),e(v,af),e(af,MV),e(MV,SSe),e(af,PSe),e(af,U7),e(U7,$Se),e(af,ISe),e(v,jSe),e(v,sf),e(sf,EV),e(EV,NSe),e(sf,DSe),e(sf,J7),e(J7,qSe),e(sf,GSe),e(v,OSe),e(v,nf),e(nf,yV),e(yV,XSe),e(nf,zSe),e(nf,Y7),e(Y7,VSe),e(nf,WSe),e(v,QSe),e(v,lf),e(lf,wV),e(wV,HSe),e(lf,USe),e(lf,K7),e(K7,JSe),e(lf,YSe),e(v,KSe),e(v,df),e(df,AV),e(AV,ZSe),e(df,ePe),e(df,Z7),e(Z7,oPe),e(df,rPe),e(v,tPe),e(v,cf),e(cf,LV),e(LV,aPe),e(cf,sPe),e(cf,e8),e(e8,nPe),e(cf,lPe),e(v,iPe),e(v,mf),e(mf,BV),e(BV,dPe),e(mf,cPe),e(mf,o8),e(o8,mPe),e(mf,fPe),e(v,gPe),e(v,ff),e(ff,xV),e(xV,hPe),e(ff,uPe),e(ff,r8),e(r8,pPe),e(ff,_Pe),e(v,bPe),e(v,gf),e(gf,kV),e(kV,vPe),e(gf,TPe),e(gf,t8),e(t8,FPe),e(gf,CPe),e(v,MPe),e(v,hf),e(hf,RV),e(RV,EPe),e(hf,yPe),e(hf,a8),e(a8,wPe),e(hf,APe),e(v,LPe),e(v,uf),e(uf,SV),e(SV,BPe),e(uf,xPe),e(uf,PV),e(PV,kPe),e(uf,RPe),e(v,SPe),e(v,pf),e(pf,$V),e($V,PPe),e(pf,$Pe),e(pf,s8),e(s8,IPe),e(pf,jPe),e(v,NPe),e(v,_f),e(_f,IV),e(IV,DPe),e(_f,qPe),e(_f,n8),e(n8,GPe),e(_f,OPe),e(v,XPe),e(v,bf),e(bf,jV),e(jV,zPe),e(bf,VPe),e(bf,l8),e(l8,WPe),e(bf,QPe),e(v,HPe),e(v,vf),e(vf,NV),e(NV,UPe),e(vf,JPe),e(vf,i8),e(i8,YPe),e(vf,KPe),e(v,ZPe),e(v,Tf),e(Tf,DV),e(DV,e$e),e(Tf,o$e),e(Tf,d8),e(d8,r$e),e(Tf,t$e),e(v,a$e),e(v,Ff),e(Ff,qV),e(qV,s$e),e(Ff,n$e),e(Ff,c8),e(c8,l$e),e(Ff,i$e),e(v,d$e),e(v,Cf),e(Cf,GV),e(GV,c$e),e(Cf,m$e),e(Cf,m8),e(m8,f$e),e(Cf,g$e),e(v,h$e),e(v,Mf),e(Mf,OV),e(OV,u$e),e(Mf,p$e),e(Mf,f8),e(f8,_$e),e(Mf,b$e),e(v,v$e),e(v,Ef),e(Ef,XV),e(XV,T$e),e(Ef,F$e),e(Ef,g8),e(g8,C$e),e(Ef,M$e),e(v,E$e),e(v,yf),e(yf,zV),e(zV,y$e),e(yf,w$e),e(yf,h8),e(h8,A$e),e(yf,L$e),e(v,B$e),e(v,wf),e(wf,VV),e(VV,x$e),e(wf,k$e),e(wf,u8),e(u8,R$e),e(wf,S$e),e(v,P$e),e(v,Af),e(Af,WV),e(WV,$$e),e(Af,I$e),e(Af,p8),e(p8,j$e),e(Af,N$e),e(v,D$e),e(v,Lf),e(Lf,QV),e(QV,q$e),e(Lf,G$e),e(Lf,_8),e(_8,O$e),e(Lf,X$e),e(v,z$e),e(v,Bf),e(Bf,HV),e(HV,V$e),e(Bf,W$e),e(Bf,b8),e(b8,Q$e),e(Bf,H$e),e(v,U$e),e(v,xf),e(xf,UV),e(UV,J$e),e(xf,Y$e),e(xf,v8),e(v8,K$e),e(xf,Z$e),e(v,eIe),e(v,kf),e(kf,JV),e(JV,oIe),e(kf,rIe),e(kf,T8),e(T8,tIe),e(kf,aIe),e(v,sIe),e(v,Rf),e(Rf,YV),e(YV,nIe),e(Rf,lIe),e(Rf,F8),e(F8,iIe),e(Rf,dIe),e(v,cIe),e(v,Sf),e(Sf,KV),e(KV,mIe),e(Sf,fIe),e(Sf,C8),e(C8,gIe),e(Sf,hIe),e(v,uIe),e(v,Pf),e(Pf,ZV),e(ZV,pIe),e(Pf,_Ie),e(Pf,M8),e(M8,bIe),e(Pf,vIe),e(v,TIe),e(v,$f),e($f,eW),e(eW,FIe),e($f,CIe),e($f,E8),e(E8,MIe),e($f,EIe),e(v,yIe),e(v,If),e(If,oW),e(oW,wIe),e(If,AIe),e(If,y8),e(y8,LIe),e(If,BIe),e(v,xIe),e(v,jf),e(jf,rW),e(rW,kIe),e(jf,RIe),e(jf,w8),e(w8,SIe),e(jf,PIe),e(v,$Ie),e(v,Nf),e(Nf,tW),e(tW,IIe),e(Nf,jIe),e(Nf,A8),e(A8,NIe),e(Nf,DIe),e(v,qIe),e(v,Df),e(Df,aW),e(aW,GIe),e(Df,OIe),e(Df,L8),e(L8,XIe),e(Df,zIe),e(v,VIe),e(v,qf),e(qf,sW),e(sW,WIe),e(qf,QIe),e(qf,B8),e(B8,HIe),e(qf,UIe),e(v,JIe),e(v,Gf),e(Gf,nW),e(nW,YIe),e(Gf,KIe),e(Gf,x8),e(x8,ZIe),e(Gf,eje),e(v,oje),e(v,Of),e(Of,lW),e(lW,rje),e(Of,tje),e(Of,k8),e(k8,aje),e(Of,sje),e(v,nje),e(v,Xf),e(Xf,iW),e(iW,lje),e(Xf,ije),e(Xf,R8),e(R8,dje),e(Xf,cje),e(v,mje),e(v,zf),e(zf,dW),e(dW,fje),e(zf,gje),e(zf,S8),e(S8,hje),e(zf,uje),e(v,pje),e(v,Vf),e(Vf,cW),e(cW,_je),e(Vf,bje),e(Vf,P8),e(P8,vje),e(Vf,Tje),e(v,Fje),e(v,Wf),e(Wf,mW),e(mW,Cje),e(Wf,Mje),e(Wf,$8),e($8,Eje),e(Wf,yje),e(v,wje),e(v,Qf),e(Qf,fW),e(fW,Aje),e(Qf,Lje),e(Qf,I8),e(I8,Bje),e(Qf,xje),e(v,kje),e(v,Hf),e(Hf,gW),e(gW,Rje),e(Hf,Sje),e(Hf,j8),e(j8,Pje),e(Hf,$je),e(v,Ije),e(v,Uf),e(Uf,hW),e(hW,jje),e(Uf,Nje),e(Uf,N8),e(N8,Dje),e(Uf,qje),e(v,Gje),e(v,Jf),e(Jf,uW),e(uW,Oje),e(Jf,Xje),e(Jf,D8),e(D8,zje),e(Jf,Vje),e(v,Wje),e(v,Yf),e(Yf,pW),e(pW,Qje),e(Yf,Hje),e(Yf,q8),e(q8,Uje),e(Yf,Jje),e(v,Yje),e(v,Kf),e(Kf,_W),e(_W,Kje),e(Kf,Zje),e(Kf,G8),e(G8,eNe),e(Kf,oNe),e(io,rNe),e(io,bW),e(bW,tNe),e(io,aNe),g(j3,io,null),e(qo,sNe),e(qo,Zf),g(N3,Zf,null),e(Zf,nNe),e(Zf,vW),e(vW,lNe),b(d,vLe,_),b(d,Ai,_),e(Ai,eg),e(eg,TW),g(D3,TW,null),e(Ai,iNe),e(Ai,FW),e(FW,dNe),b(d,TLe,_),b(d,Go,_),g(q3,Go,null),e(Go,cNe),e(Go,G3),e(G3,mNe),e(G3,O8),e(O8,fNe),e(G3,gNe),e(Go,hNe),e(Go,O3),e(O3,uNe),e(O3,CW),e(CW,pNe),e(O3,_Ne),e(Go,bNe),e(Go,co),g(X3,co,null),e(co,vNe),e(co,MW),e(MW,TNe),e(co,FNe),e(co,Sa),e(Sa,CNe),e(Sa,EW),e(EW,MNe),e(Sa,ENe),e(Sa,yW),e(yW,yNe),e(Sa,wNe),e(Sa,wW),e(wW,ANe),e(Sa,LNe),e(co,BNe),e(co,M),e(M,Ss),e(Ss,AW),e(AW,xNe),e(Ss,kNe),e(Ss,X8),e(X8,RNe),e(Ss,SNe),e(Ss,z8),e(z8,PNe),e(Ss,$Ne),e(M,INe),e(M,Ps),e(Ps,LW),e(LW,jNe),e(Ps,NNe),e(Ps,V8),e(V8,DNe),e(Ps,qNe),e(Ps,W8),e(W8,GNe),e(Ps,ONe),e(M,XNe),e(M,$s),e($s,BW),e(BW,zNe),e($s,VNe),e($s,Q8),e(Q8,WNe),e($s,QNe),e($s,H8),e(H8,HNe),e($s,UNe),e(M,JNe),e(M,og),e(og,xW),e(xW,YNe),e(og,KNe),e(og,U8),e(U8,ZNe),e(og,eDe),e(M,oDe),e(M,Is),e(Is,kW),e(kW,rDe),e(Is,tDe),e(Is,J8),e(J8,aDe),e(Is,sDe),e(Is,Y8),e(Y8,nDe),e(Is,lDe),e(M,iDe),e(M,rg),e(rg,RW),e(RW,dDe),e(rg,cDe),e(rg,K8),e(K8,mDe),e(rg,fDe),e(M,gDe),e(M,tg),e(tg,SW),e(SW,hDe),e(tg,uDe),e(tg,Z8),e(Z8,pDe),e(tg,_De),e(M,bDe),e(M,ag),e(ag,PW),e(PW,vDe),e(ag,TDe),e(ag,e9),e(e9,FDe),e(ag,CDe),e(M,MDe),e(M,js),e(js,$W),e($W,EDe),e(js,yDe),e(js,o9),e(o9,wDe),e(js,ADe),e(js,r9),e(r9,LDe),e(js,BDe),e(M,xDe),e(M,Ns),e(Ns,IW),e(IW,kDe),e(Ns,RDe),e(Ns,t9),e(t9,SDe),e(Ns,PDe),e(Ns,a9),e(a9,$De),e(Ns,IDe),e(M,jDe),e(M,Ds),e(Ds,jW),e(jW,NDe),e(Ds,DDe),e(Ds,s9),e(s9,qDe),e(Ds,GDe),e(Ds,n9),e(n9,ODe),e(Ds,XDe),e(M,zDe),e(M,sg),e(sg,NW),e(NW,VDe),e(sg,WDe),e(sg,l9),e(l9,QDe),e(sg,HDe),e(M,UDe),e(M,ng),e(ng,DW),e(DW,JDe),e(ng,YDe),e(ng,i9),e(i9,KDe),e(ng,ZDe),e(M,eqe),e(M,qs),e(qs,qW),e(qW,oqe),e(qs,rqe),e(qs,d9),e(d9,tqe),e(qs,aqe),e(qs,c9),e(c9,sqe),e(qs,nqe),e(M,lqe),e(M,lg),e(lg,GW),e(GW,iqe),e(lg,dqe),e(lg,m9),e(m9,cqe),e(lg,mqe),e(M,fqe),e(M,Gs),e(Gs,OW),e(OW,gqe),e(Gs,hqe),e(Gs,f9),e(f9,uqe),e(Gs,pqe),e(Gs,g9),e(g9,_qe),e(Gs,bqe),e(M,vqe),e(M,Os),e(Os,XW),e(XW,Tqe),e(Os,Fqe),e(Os,h9),e(h9,Cqe),e(Os,Mqe),e(Os,u9),e(u9,Eqe),e(Os,yqe),e(M,wqe),e(M,Xs),e(Xs,zW),e(zW,Aqe),e(Xs,Lqe),e(Xs,p9),e(p9,Bqe),e(Xs,xqe),e(Xs,VW),e(VW,kqe),e(Xs,Rqe),e(M,Sqe),e(M,ig),e(ig,WW),e(WW,Pqe),e(ig,$qe),e(ig,_9),e(_9,Iqe),e(ig,jqe),e(M,Nqe),e(M,zs),e(zs,QW),e(QW,Dqe),e(zs,qqe),e(zs,b9),e(b9,Gqe),e(zs,Oqe),e(zs,v9),e(v9,Xqe),e(zs,zqe),e(M,Vqe),e(M,dg),e(dg,HW),e(HW,Wqe),e(dg,Qqe),e(dg,T9),e(T9,Hqe),e(dg,Uqe),e(M,Jqe),e(M,Vs),e(Vs,UW),e(UW,Yqe),e(Vs,Kqe),e(Vs,F9),e(F9,Zqe),e(Vs,eGe),e(Vs,C9),e(C9,oGe),e(Vs,rGe),e(M,tGe),e(M,Ws),e(Ws,JW),e(JW,aGe),e(Ws,sGe),e(Ws,M9),e(M9,nGe),e(Ws,lGe),e(Ws,E9),e(E9,iGe),e(Ws,dGe),e(M,cGe),e(M,Qs),e(Qs,YW),e(YW,mGe),e(Qs,fGe),e(Qs,y9),e(y9,gGe),e(Qs,hGe),e(Qs,w9),e(w9,uGe),e(Qs,pGe),e(M,_Ge),e(M,cg),e(cg,KW),e(KW,bGe),e(cg,vGe),e(cg,A9),e(A9,TGe),e(cg,FGe),e(M,CGe),e(M,Hs),e(Hs,ZW),e(ZW,MGe),e(Hs,EGe),e(Hs,L9),e(L9,yGe),e(Hs,wGe),e(Hs,B9),e(B9,AGe),e(Hs,LGe),e(M,BGe),e(M,mg),e(mg,eQ),e(eQ,xGe),e(mg,kGe),e(mg,x9),e(x9,RGe),e(mg,SGe),e(M,PGe),e(M,Us),e(Us,oQ),e(oQ,$Ge),e(Us,IGe),e(Us,k9),e(k9,jGe),e(Us,NGe),e(Us,R9),e(R9,DGe),e(Us,qGe),e(M,GGe),e(M,Js),e(Js,rQ),e(rQ,OGe),e(Js,XGe),e(Js,S9),e(S9,zGe),e(Js,VGe),e(Js,P9),e(P9,WGe),e(Js,QGe),e(M,HGe),e(M,Ys),e(Ys,tQ),e(tQ,UGe),e(Ys,JGe),e(Ys,$9),e($9,YGe),e(Ys,KGe),e(Ys,I9),e(I9,ZGe),e(Ys,eOe),e(M,oOe),e(M,Ks),e(Ks,aQ),e(aQ,rOe),e(Ks,tOe),e(Ks,j9),e(j9,aOe),e(Ks,sOe),e(Ks,N9),e(N9,nOe),e(Ks,lOe),e(M,iOe),e(M,fg),e(fg,sQ),e(sQ,dOe),e(fg,cOe),e(fg,D9),e(D9,mOe),e(fg,fOe),e(M,gOe),e(M,Zs),e(Zs,nQ),e(nQ,hOe),e(Zs,uOe),e(Zs,q9),e(q9,pOe),e(Zs,_Oe),e(Zs,G9),e(G9,bOe),e(Zs,vOe),e(M,TOe),e(M,en),e(en,lQ),e(lQ,FOe),e(en,COe),e(en,O9),e(O9,MOe),e(en,EOe),e(en,X9),e(X9,yOe),e(en,wOe),e(M,AOe),e(M,on),e(on,iQ),e(iQ,LOe),e(on,BOe),e(on,z9),e(z9,xOe),e(on,kOe),e(on,V9),e(V9,ROe),e(on,SOe),e(M,POe),e(M,rn),e(rn,dQ),e(dQ,$Oe),e(rn,IOe),e(rn,W9),e(W9,jOe),e(rn,NOe),e(rn,Q9),e(Q9,DOe),e(rn,qOe),e(M,GOe),e(M,tn),e(tn,cQ),e(cQ,OOe),e(tn,XOe),e(tn,H9),e(H9,zOe),e(tn,VOe),e(tn,U9),e(U9,WOe),e(tn,QOe),e(M,HOe),e(M,an),e(an,mQ),e(mQ,UOe),e(an,JOe),e(an,J9),e(J9,YOe),e(an,KOe),e(an,Y9),e(Y9,ZOe),e(an,eXe),e(M,oXe),e(M,gg),e(gg,fQ),e(fQ,rXe),e(gg,tXe),e(gg,K9),e(K9,aXe),e(gg,sXe),e(M,nXe),e(M,sn),e(sn,gQ),e(gQ,lXe),e(sn,iXe),e(sn,Z9),e(Z9,dXe),e(sn,cXe),e(sn,eB),e(eB,mXe),e(sn,fXe),e(M,gXe),e(M,hg),e(hg,hQ),e(hQ,hXe),e(hg,uXe),e(hg,oB),e(oB,pXe),e(hg,_Xe),e(M,bXe),e(M,ug),e(ug,uQ),e(uQ,vXe),e(ug,TXe),e(ug,rB),e(rB,FXe),e(ug,CXe),e(M,MXe),e(M,nn),e(nn,pQ),e(pQ,EXe),e(nn,yXe),e(nn,tB),e(tB,wXe),e(nn,AXe),e(nn,aB),e(aB,LXe),e(nn,BXe),e(M,xXe),e(M,ln),e(ln,_Q),e(_Q,kXe),e(ln,RXe),e(ln,sB),e(sB,SXe),e(ln,PXe),e(ln,nB),e(nB,$Xe),e(ln,IXe),e(M,jXe),e(M,pg),e(pg,bQ),e(bQ,NXe),e(pg,DXe),e(pg,lB),e(lB,qXe),e(pg,GXe),e(M,OXe),e(M,dn),e(dn,vQ),e(vQ,XXe),e(dn,zXe),e(dn,iB),e(iB,VXe),e(dn,WXe),e(dn,dB),e(dB,QXe),e(dn,HXe),e(M,UXe),e(M,cn),e(cn,TQ),e(TQ,JXe),e(cn,YXe),e(cn,cB),e(cB,KXe),e(cn,ZXe),e(cn,mB),e(mB,eze),e(cn,oze),e(M,rze),e(M,mn),e(mn,FQ),e(FQ,tze),e(mn,aze),e(mn,fB),e(fB,sze),e(mn,nze),e(mn,gB),e(gB,lze),e(mn,ize),e(M,dze),e(M,fn),e(fn,CQ),e(CQ,cze),e(fn,mze),e(fn,hB),e(hB,fze),e(fn,gze),e(fn,uB),e(uB,hze),e(fn,uze),e(M,pze),e(M,gn),e(gn,MQ),e(MQ,_ze),e(gn,bze),e(gn,pB),e(pB,vze),e(gn,Tze),e(gn,_B),e(_B,Fze),e(gn,Cze),e(M,Mze),e(M,_g),e(_g,EQ),e(EQ,Eze),e(_g,yze),e(_g,bB),e(bB,wze),e(_g,Aze),e(M,Lze),e(M,bg),e(bg,yQ),e(yQ,Bze),e(bg,xze),e(bg,vB),e(vB,kze),e(bg,Rze),e(M,Sze),e(M,vg),e(vg,wQ),e(wQ,Pze),e(vg,$ze),e(vg,TB),e(TB,Ize),e(vg,jze),e(M,Nze),e(M,hn),e(hn,AQ),e(AQ,Dze),e(hn,qze),e(hn,FB),e(FB,Gze),e(hn,Oze),e(hn,CB),e(CB,Xze),e(hn,zze),e(M,Vze),e(M,Tg),e(Tg,LQ),e(LQ,Wze),e(Tg,Qze),e(Tg,MB),e(MB,Hze),e(Tg,Uze),e(M,Jze),e(M,un),e(un,BQ),e(BQ,Yze),e(un,Kze),e(un,EB),e(EB,Zze),e(un,eVe),e(un,yB),e(yB,oVe),e(un,rVe),e(M,tVe),e(M,pn),e(pn,xQ),e(xQ,aVe),e(pn,sVe),e(pn,wB),e(wB,nVe),e(pn,lVe),e(pn,AB),e(AB,iVe),e(pn,dVe),e(M,cVe),e(M,_n),e(_n,kQ),e(kQ,mVe),e(_n,fVe),e(_n,LB),e(LB,gVe),e(_n,hVe),e(_n,BB),e(BB,uVe),e(_n,pVe),e(M,_Ve),e(M,bn),e(bn,RQ),e(RQ,bVe),e(bn,vVe),e(bn,xB),e(xB,TVe),e(bn,FVe),e(bn,kB),e(kB,CVe),e(bn,MVe),e(M,EVe),e(M,vn),e(vn,SQ),e(SQ,yVe),e(vn,wVe),e(vn,RB),e(RB,AVe),e(vn,LVe),e(vn,SB),e(SB,BVe),e(vn,xVe),e(M,kVe),e(M,Fg),e(Fg,PQ),e(PQ,RVe),e(Fg,SVe),e(Fg,PB),e(PB,PVe),e(Fg,$Ve),e(M,IVe),e(M,Cg),e(Cg,$Q),e($Q,jVe),e(Cg,NVe),e(Cg,$B),e($B,DVe),e(Cg,qVe),e(M,GVe),e(M,Tn),e(Tn,IQ),e(IQ,OVe),e(Tn,XVe),e(Tn,IB),e(IB,zVe),e(Tn,VVe),e(Tn,jB),e(jB,WVe),e(Tn,QVe),e(M,HVe),e(M,Fn),e(Fn,jQ),e(jQ,UVe),e(Fn,JVe),e(Fn,NB),e(NB,YVe),e(Fn,KVe),e(Fn,DB),e(DB,ZVe),e(Fn,eWe),e(M,oWe),e(M,Cn),e(Cn,NQ),e(NQ,rWe),e(Cn,tWe),e(Cn,qB),e(qB,aWe),e(Cn,sWe),e(Cn,GB),e(GB,nWe),e(Cn,lWe),e(M,iWe),e(M,Mg),e(Mg,DQ),e(DQ,dWe),e(Mg,cWe),e(Mg,OB),e(OB,mWe),e(Mg,fWe),e(M,gWe),e(M,Eg),e(Eg,qQ),e(qQ,hWe),e(Eg,uWe),e(Eg,XB),e(XB,pWe),e(Eg,_We),e(M,bWe),e(M,yg),e(yg,GQ),e(GQ,vWe),e(yg,TWe),e(yg,zB),e(zB,FWe),e(yg,CWe),e(M,MWe),e(M,wg),e(wg,OQ),e(OQ,EWe),e(wg,yWe),e(wg,VB),e(VB,wWe),e(wg,AWe),e(M,LWe),e(M,Mn),e(Mn,XQ),e(XQ,BWe),e(Mn,xWe),e(Mn,WB),e(WB,kWe),e(Mn,RWe),e(Mn,QB),e(QB,SWe),e(Mn,PWe),e(M,$We),e(M,Ag),e(Ag,zQ),e(zQ,IWe),e(Ag,jWe),e(Ag,HB),e(HB,NWe),e(Ag,DWe),e(M,qWe),e(M,Lg),e(Lg,VQ),e(VQ,GWe),e(Lg,OWe),e(Lg,UB),e(UB,XWe),e(Lg,zWe),e(M,VWe),e(M,En),e(En,WQ),e(WQ,WWe),e(En,QWe),e(En,JB),e(JB,HWe),e(En,UWe),e(En,YB),e(YB,JWe),e(En,YWe),e(M,KWe),e(M,yn),e(yn,QQ),e(QQ,ZWe),e(yn,eQe),e(yn,KB),e(KB,oQe),e(yn,rQe),e(yn,ZB),e(ZB,tQe),e(yn,aQe),e(co,sQe),e(co,HQ),e(HQ,nQe),e(co,lQe),g(z3,co,null),e(Go,iQe),e(Go,Bg),g(V3,Bg,null),e(Bg,dQe),e(Bg,UQ),e(UQ,cQe),b(d,FLe,_),b(d,Li,_),e(Li,xg),e(xg,JQ),g(W3,JQ,null),e(Li,mQe),e(Li,YQ),e(YQ,fQe),b(d,CLe,_),b(d,Zt,_),g(Q3,Zt,null),e(Zt,gQe),e(Zt,H3),e(H3,hQe),e(H3,ex),e(ex,uQe),e(H3,pQe),e(Zt,_Qe),e(Zt,U3),e(U3,bQe),e(U3,KQ),e(KQ,vQe),e(U3,TQe),e(Zt,FQe),e(Zt,Le),g(J3,Le,null),e(Le,CQe),e(Le,ZQ),e(ZQ,MQe),e(Le,EQe),e(Le,Pa),e(Pa,yQe),e(Pa,eH),e(eH,wQe),e(Pa,AQe),e(Pa,oH),e(oH,LQe),e(Pa,BQe),e(Pa,rH),e(rH,xQe),e(Pa,kQe),e(Le,RQe),e(Le,ne),e(ne,kg),e(kg,tH),e(tH,SQe),e(kg,PQe),e(kg,ox),e(ox,$Qe),e(kg,IQe),e(ne,jQe),e(ne,Rg),e(Rg,aH),e(aH,NQe),e(Rg,DQe),e(Rg,rx),e(rx,qQe),e(Rg,GQe),e(ne,OQe),e(ne,Sg),e(Sg,sH),e(sH,XQe),e(Sg,zQe),e(Sg,tx),e(tx,VQe),e(Sg,WQe),e(ne,QQe),e(ne,Pg),e(Pg,nH),e(nH,HQe),e(Pg,UQe),e(Pg,ax),e(ax,JQe),e(Pg,YQe),e(ne,KQe),e(ne,$g),e($g,lH),e(lH,ZQe),e($g,eHe),e($g,sx),e(sx,oHe),e($g,rHe),e(ne,tHe),e(ne,Ig),e(Ig,iH),e(iH,aHe),e(Ig,sHe),e(Ig,nx),e(nx,nHe),e(Ig,lHe),e(ne,iHe),e(ne,jg),e(jg,dH),e(dH,dHe),e(jg,cHe),e(jg,lx),e(lx,mHe),e(jg,fHe),e(ne,gHe),e(ne,Ng),e(Ng,cH),e(cH,hHe),e(Ng,uHe),e(Ng,ix),e(ix,pHe),e(Ng,_He),e(ne,bHe),e(ne,Dg),e(Dg,mH),e(mH,vHe),e(Dg,THe),e(Dg,dx),e(dx,FHe),e(Dg,CHe),e(ne,MHe),e(ne,qg),e(qg,fH),e(fH,EHe),e(qg,yHe),e(qg,cx),e(cx,wHe),e(qg,AHe),e(ne,LHe),e(ne,Gg),e(Gg,gH),e(gH,BHe),e(Gg,xHe),e(Gg,mx),e(mx,kHe),e(Gg,RHe),e(ne,SHe),e(ne,Og),e(Og,hH),e(hH,PHe),e(Og,$He),e(Og,fx),e(fx,IHe),e(Og,jHe),e(ne,NHe),e(ne,Xg),e(Xg,uH),e(uH,DHe),e(Xg,qHe),e(Xg,gx),e(gx,GHe),e(Xg,OHe),e(ne,XHe),e(ne,zg),e(zg,pH),e(pH,zHe),e(zg,VHe),e(zg,hx),e(hx,WHe),e(zg,QHe),e(Le,HHe),g(Vg,Le,null),e(Le,UHe),e(Le,_H),e(_H,JHe),e(Le,YHe),g(Y3,Le,null),b(d,MLe,_),b(d,Bi,_),e(Bi,Wg),e(Wg,bH),g(K3,bH,null),e(Bi,KHe),e(Bi,vH),e(vH,ZHe),b(d,ELe,_),b(d,ea,_),g(Z3,ea,null),e(ea,eUe),e(ea,e5),e(e5,oUe),e(e5,ux),e(ux,rUe),e(e5,tUe),e(ea,aUe),e(ea,o5),e(o5,sUe),e(o5,TH),e(TH,nUe),e(o5,lUe),e(ea,iUe),e(ea,Be),g(r5,Be,null),e(Be,dUe),e(Be,FH),e(FH,cUe),e(Be,mUe),e(Be,xi),e(xi,fUe),e(xi,CH),e(CH,gUe),e(xi,hUe),e(xi,MH),e(MH,uUe),e(xi,pUe),e(Be,_Ue),e(Be,ye),e(ye,Qg),e(Qg,EH),e(EH,bUe),e(Qg,vUe),e(Qg,px),e(px,TUe),e(Qg,FUe),e(ye,CUe),e(ye,Hg),e(Hg,yH),e(yH,MUe),e(Hg,EUe),e(Hg,_x),e(_x,yUe),e(Hg,wUe),e(ye,AUe),e(ye,Ug),e(Ug,wH),e(wH,LUe),e(Ug,BUe),e(Ug,bx),e(bx,xUe),e(Ug,kUe),e(ye,RUe),e(ye,Jg),e(Jg,AH),e(AH,SUe),e(Jg,PUe),e(Jg,vx),e(vx,$Ue),e(Jg,IUe),e(ye,jUe),e(ye,Yg),e(Yg,LH),e(LH,NUe),e(Yg,DUe),e(Yg,Tx),e(Tx,qUe),e(Yg,GUe),e(ye,OUe),e(ye,Kg),e(Kg,BH),e(BH,XUe),e(Kg,zUe),e(Kg,Fx),e(Fx,VUe),e(Kg,WUe),e(ye,QUe),e(ye,Zg),e(Zg,xH),e(xH,HUe),e(Zg,UUe),e(Zg,Cx),e(Cx,JUe),e(Zg,YUe),e(ye,KUe),e(ye,eh),e(eh,kH),e(kH,ZUe),e(eh,eJe),e(eh,Mx),e(Mx,oJe),e(eh,rJe),e(Be,tJe),g(oh,Be,null),e(Be,aJe),e(Be,RH),e(RH,sJe),e(Be,nJe),g(t5,Be,null),b(d,yLe,_),b(d,ki,_),e(ki,rh),e(rh,SH),g(a5,SH,null),e(ki,lJe),e(ki,PH),e(PH,iJe),b(d,wLe,_),b(d,Oo,_),g(s5,Oo,null),e(Oo,dJe),e(Oo,Ri),e(Ri,cJe),e(Ri,$H),e($H,mJe),e(Ri,fJe),e(Ri,IH),e(IH,gJe),e(Ri,hJe),e(Oo,uJe),e(Oo,n5),e(n5,pJe),e(n5,jH),e(jH,_Je),e(n5,bJe),e(Oo,vJe),e(Oo,Pr),g(l5,Pr,null),e(Pr,TJe),e(Pr,NH),e(NH,FJe),e(Pr,CJe),e(Pr,Si),e(Si,MJe),e(Si,DH),e(DH,EJe),e(Si,yJe),e(Si,qH),e(qH,wJe),e(Si,AJe),e(Pr,LJe),e(Pr,GH),e(GH,BJe),e(Pr,xJe),g(i5,Pr,null),e(Oo,kJe),e(Oo,xe),g(d5,xe,null),e(xe,RJe),e(xe,OH),e(OH,SJe),e(xe,PJe),e(xe,$a),e($a,$Je),e($a,XH),e(XH,IJe),e($a,jJe),e($a,zH),e(zH,NJe),e($a,DJe),e($a,VH),e(VH,qJe),e($a,GJe),e(xe,OJe),e(xe,F),e(F,th),e(th,WH),e(WH,XJe),e(th,zJe),e(th,Ex),e(Ex,VJe),e(th,WJe),e(F,QJe),e(F,ah),e(ah,QH),e(QH,HJe),e(ah,UJe),e(ah,yx),e(yx,JJe),e(ah,YJe),e(F,KJe),e(F,sh),e(sh,HH),e(HH,ZJe),e(sh,eYe),e(sh,wx),e(wx,oYe),e(sh,rYe),e(F,tYe),e(F,nh),e(nh,UH),e(UH,aYe),e(nh,sYe),e(nh,Ax),e(Ax,nYe),e(nh,lYe),e(F,iYe),e(F,lh),e(lh,JH),e(JH,dYe),e(lh,cYe),e(lh,Lx),e(Lx,mYe),e(lh,fYe),e(F,gYe),e(F,ih),e(ih,YH),e(YH,hYe),e(ih,uYe),e(ih,Bx),e(Bx,pYe),e(ih,_Ye),e(F,bYe),e(F,dh),e(dh,KH),e(KH,vYe),e(dh,TYe),e(dh,xx),e(xx,FYe),e(dh,CYe),e(F,MYe),e(F,ch),e(ch,ZH),e(ZH,EYe),e(ch,yYe),e(ch,kx),e(kx,wYe),e(ch,AYe),e(F,LYe),e(F,mh),e(mh,eU),e(eU,BYe),e(mh,xYe),e(mh,Rx),e(Rx,kYe),e(mh,RYe),e(F,SYe),e(F,fh),e(fh,oU),e(oU,PYe),e(fh,$Ye),e(fh,Sx),e(Sx,IYe),e(fh,jYe),e(F,NYe),e(F,gh),e(gh,rU),e(rU,DYe),e(gh,qYe),e(gh,Px),e(Px,GYe),e(gh,OYe),e(F,XYe),e(F,hh),e(hh,tU),e(tU,zYe),e(hh,VYe),e(hh,$x),e($x,WYe),e(hh,QYe),e(F,HYe),e(F,uh),e(uh,aU),e(aU,UYe),e(uh,JYe),e(uh,Ix),e(Ix,YYe),e(uh,KYe),e(F,ZYe),e(F,ph),e(ph,sU),e(sU,eKe),e(ph,oKe),e(ph,jx),e(jx,rKe),e(ph,tKe),e(F,aKe),e(F,_h),e(_h,nU),e(nU,sKe),e(_h,nKe),e(_h,Nx),e(Nx,lKe),e(_h,iKe),e(F,dKe),e(F,bh),e(bh,lU),e(lU,cKe),e(bh,mKe),e(bh,Dx),e(Dx,fKe),e(bh,gKe),e(F,hKe),e(F,vh),e(vh,iU),e(iU,uKe),e(vh,pKe),e(vh,qx),e(qx,_Ke),e(vh,bKe),e(F,vKe),e(F,Th),e(Th,dU),e(dU,TKe),e(Th,FKe),e(Th,Gx),e(Gx,CKe),e(Th,MKe),e(F,EKe),e(F,Fh),e(Fh,cU),e(cU,yKe),e(Fh,wKe),e(Fh,Ox),e(Ox,AKe),e(Fh,LKe),e(F,BKe),e(F,Ch),e(Ch,mU),e(mU,xKe),e(Ch,kKe),e(Ch,Xx),e(Xx,RKe),e(Ch,SKe),e(F,PKe),e(F,Mh),e(Mh,fU),e(fU,$Ke),e(Mh,IKe),e(Mh,zx),e(zx,jKe),e(Mh,NKe),e(F,DKe),e(F,Eh),e(Eh,gU),e(gU,qKe),e(Eh,GKe),e(Eh,Vx),e(Vx,OKe),e(Eh,XKe),e(F,zKe),e(F,yh),e(yh,hU),e(hU,VKe),e(yh,WKe),e(yh,Wx),e(Wx,QKe),e(yh,HKe),e(F,UKe),e(F,wh),e(wh,uU),e(uU,JKe),e(wh,YKe),e(wh,Qx),e(Qx,KKe),e(wh,ZKe),e(F,eZe),e(F,Ah),e(Ah,pU),e(pU,oZe),e(Ah,rZe),e(Ah,Hx),e(Hx,tZe),e(Ah,aZe),e(F,sZe),e(F,wn),e(wn,_U),e(_U,nZe),e(wn,lZe),e(wn,Ux),e(Ux,iZe),e(wn,dZe),e(wn,Jx),e(Jx,cZe),e(wn,mZe),e(F,fZe),e(F,Lh),e(Lh,bU),e(bU,gZe),e(Lh,hZe),e(Lh,Yx),e(Yx,uZe),e(Lh,pZe),e(F,_Ze),e(F,Bh),e(Bh,vU),e(vU,bZe),e(Bh,vZe),e(Bh,Kx),e(Kx,TZe),e(Bh,FZe),e(F,CZe),e(F,xh),e(xh,TU),e(TU,MZe),e(xh,EZe),e(xh,Zx),e(Zx,yZe),e(xh,wZe),e(F,AZe),e(F,kh),e(kh,FU),e(FU,LZe),e(kh,BZe),e(kh,ek),e(ek,xZe),e(kh,kZe),e(F,RZe),e(F,Rh),e(Rh,CU),e(CU,SZe),e(Rh,PZe),e(Rh,ok),e(ok,$Ze),e(Rh,IZe),e(F,jZe),e(F,Sh),e(Sh,MU),e(MU,NZe),e(Sh,DZe),e(Sh,rk),e(rk,qZe),e(Sh,GZe),e(F,OZe),e(F,Ph),e(Ph,EU),e(EU,XZe),e(Ph,zZe),e(Ph,tk),e(tk,VZe),e(Ph,WZe),e(F,QZe),e(F,$h),e($h,yU),e(yU,HZe),e($h,UZe),e($h,ak),e(ak,JZe),e($h,YZe),e(F,KZe),e(F,Ih),e(Ih,wU),e(wU,ZZe),e(Ih,eeo),e(Ih,sk),e(sk,oeo),e(Ih,reo),e(F,teo),e(F,jh),e(jh,AU),e(AU,aeo),e(jh,seo),e(jh,nk),e(nk,neo),e(jh,leo),e(F,ieo),e(F,Nh),e(Nh,LU),e(LU,deo),e(Nh,ceo),e(Nh,lk),e(lk,meo),e(Nh,feo),e(F,geo),e(F,Dh),e(Dh,BU),e(BU,heo),e(Dh,ueo),e(Dh,ik),e(ik,peo),e(Dh,_eo),e(F,beo),e(F,qh),e(qh,xU),e(xU,veo),e(qh,Teo),e(qh,dk),e(dk,Feo),e(qh,Ceo),e(F,Meo),e(F,Gh),e(Gh,kU),e(kU,Eeo),e(Gh,yeo),e(Gh,ck),e(ck,weo),e(Gh,Aeo),e(F,Leo),e(F,Oh),e(Oh,RU),e(RU,Beo),e(Oh,xeo),e(Oh,mk),e(mk,keo),e(Oh,Reo),e(F,Seo),e(F,Xh),e(Xh,SU),e(SU,Peo),e(Xh,$eo),e(Xh,fk),e(fk,Ieo),e(Xh,jeo),e(F,Neo),e(F,zh),e(zh,PU),e(PU,Deo),e(zh,qeo),e(zh,gk),e(gk,Geo),e(zh,Oeo),e(F,Xeo),e(F,Vh),e(Vh,$U),e($U,zeo),e(Vh,Veo),e(Vh,hk),e(hk,Weo),e(Vh,Qeo),e(F,Heo),e(F,Wh),e(Wh,IU),e(IU,Ueo),e(Wh,Jeo),e(Wh,uk),e(uk,Yeo),e(Wh,Keo),e(F,Zeo),e(F,Qh),e(Qh,jU),e(jU,eoo),e(Qh,ooo),e(Qh,pk),e(pk,roo),e(Qh,too),e(F,aoo),e(F,Hh),e(Hh,NU),e(NU,soo),e(Hh,noo),e(Hh,_k),e(_k,loo),e(Hh,ioo),e(F,doo),e(F,Uh),e(Uh,DU),e(DU,coo),e(Uh,moo),e(Uh,bk),e(bk,foo),e(Uh,goo),e(F,hoo),e(F,Jh),e(Jh,qU),e(qU,uoo),e(Jh,poo),e(Jh,vk),e(vk,_oo),e(Jh,boo),e(F,voo),e(F,Yh),e(Yh,GU),e(GU,Too),e(Yh,Foo),e(Yh,Tk),e(Tk,Coo),e(Yh,Moo),e(F,Eoo),e(F,Kh),e(Kh,OU),e(OU,yoo),e(Kh,woo),e(Kh,XU),e(XU,Aoo),e(Kh,Loo),e(F,Boo),e(F,Zh),e(Zh,zU),e(zU,xoo),e(Zh,koo),e(Zh,Fk),e(Fk,Roo),e(Zh,Soo),e(F,Poo),e(F,eu),e(eu,VU),e(VU,$oo),e(eu,Ioo),e(eu,Ck),e(Ck,joo),e(eu,Noo),e(F,Doo),e(F,ou),e(ou,WU),e(WU,qoo),e(ou,Goo),e(ou,Mk),e(Mk,Ooo),e(ou,Xoo),e(F,zoo),e(F,ru),e(ru,QU),e(QU,Voo),e(ru,Woo),e(ru,Ek),e(Ek,Qoo),e(ru,Hoo),e(F,Uoo),e(F,tu),e(tu,HU),e(HU,Joo),e(tu,Yoo),e(tu,yk),e(yk,Koo),e(tu,Zoo),e(F,ero),e(F,au),e(au,UU),e(UU,oro),e(au,rro),e(au,wk),e(wk,tro),e(au,aro),e(F,sro),e(F,su),e(su,JU),e(JU,nro),e(su,lro),e(su,Ak),e(Ak,iro),e(su,dro),e(F,cro),e(F,nu),e(nu,YU),e(YU,mro),e(nu,fro),e(nu,Lk),e(Lk,gro),e(nu,hro),e(F,uro),e(F,lu),e(lu,KU),e(KU,pro),e(lu,_ro),e(lu,Bk),e(Bk,bro),e(lu,vro),e(F,Tro),e(F,iu),e(iu,ZU),e(ZU,Fro),e(iu,Cro),e(iu,xk),e(xk,Mro),e(iu,Ero),e(F,yro),e(F,du),e(du,eJ),e(eJ,wro),e(du,Aro),e(du,kk),e(kk,Lro),e(du,Bro),e(F,xro),e(F,cu),e(cu,oJ),e(oJ,kro),e(cu,Rro),e(cu,Rk),e(Rk,Sro),e(cu,Pro),e(F,$ro),e(F,mu),e(mu,rJ),e(rJ,Iro),e(mu,jro),e(mu,Sk),e(Sk,Nro),e(mu,Dro),e(F,qro),e(F,fu),e(fu,tJ),e(tJ,Gro),e(fu,Oro),e(fu,Pk),e(Pk,Xro),e(fu,zro),e(F,Vro),e(F,gu),e(gu,aJ),e(aJ,Wro),e(gu,Qro),e(gu,$k),e($k,Hro),e(gu,Uro),e(F,Jro),e(F,hu),e(hu,sJ),e(sJ,Yro),e(hu,Kro),e(hu,Ik),e(Ik,Zro),e(hu,eto),e(F,oto),e(F,uu),e(uu,nJ),e(nJ,rto),e(uu,tto),e(uu,jk),e(jk,ato),e(uu,sto),e(F,nto),e(F,pu),e(pu,lJ),e(lJ,lto),e(pu,ito),e(pu,Nk),e(Nk,dto),e(pu,cto),e(F,mto),e(F,_u),e(_u,iJ),e(iJ,fto),e(_u,gto),e(_u,Dk),e(Dk,hto),e(_u,uto),e(F,pto),e(F,bu),e(bu,dJ),e(dJ,_to),e(bu,bto),e(bu,qk),e(qk,vto),e(bu,Tto),e(F,Fto),e(F,vu),e(vu,cJ),e(cJ,Cto),e(vu,Mto),e(vu,Gk),e(Gk,Eto),e(vu,yto),e(F,wto),e(F,Tu),e(Tu,mJ),e(mJ,Ato),e(Tu,Lto),e(Tu,Ok),e(Ok,Bto),e(Tu,xto),e(F,kto),e(F,Fu),e(Fu,fJ),e(fJ,Rto),e(Fu,Sto),e(Fu,Xk),e(Xk,Pto),e(Fu,$to),e(F,Ito),e(F,Cu),e(Cu,gJ),e(gJ,jto),e(Cu,Nto),e(Cu,zk),e(zk,Dto),e(Cu,qto),e(F,Gto),e(F,Mu),e(Mu,hJ),e(hJ,Oto),e(Mu,Xto),e(Mu,Vk),e(Vk,zto),e(Mu,Vto),e(F,Wto),e(F,Eu),e(Eu,uJ),e(uJ,Qto),e(Eu,Hto),e(Eu,Wk),e(Wk,Uto),e(Eu,Jto),e(F,Yto),e(F,yu),e(yu,pJ),e(pJ,Kto),e(yu,Zto),e(yu,Qk),e(Qk,eao),e(yu,oao),e(F,rao),e(F,wu),e(wu,_J),e(_J,tao),e(wu,aao),e(wu,Hk),e(Hk,sao),e(wu,nao),e(F,lao),e(F,Au),e(Au,bJ),e(bJ,iao),e(Au,dao),e(Au,Uk),e(Uk,cao),e(Au,mao),e(F,fao),e(F,Lu),e(Lu,vJ),e(vJ,gao),e(Lu,hao),e(Lu,Jk),e(Jk,uao),e(Lu,pao),e(F,_ao),e(F,Bu),e(Bu,TJ),e(TJ,bao),e(Bu,vao),e(Bu,Yk),e(Yk,Tao),e(Bu,Fao),e(xe,Cao),e(xe,xu),e(xu,Mao),e(xu,FJ),e(FJ,Eao),e(xu,yao),e(xu,CJ),e(CJ,wao),e(xe,Aao),e(xe,MJ),e(MJ,Lao),e(xe,Bao),g(c5,xe,null),b(d,ALe,_),b(d,Pi,_),e(Pi,ku),e(ku,EJ),g(m5,EJ,null),e(Pi,xao),e(Pi,yJ),e(yJ,kao),b(d,LLe,_),b(d,Xo,_),g(f5,Xo,null),e(Xo,Rao),e(Xo,$i),e($i,Sao),e($i,wJ),e(wJ,Pao),e($i,$ao),e($i,AJ),e(AJ,Iao),e($i,jao),e(Xo,Nao),e(Xo,g5),e(g5,Dao),e(g5,LJ),e(LJ,qao),e(g5,Gao),e(Xo,Oao),e(Xo,$r),g(h5,$r,null),e($r,Xao),e($r,BJ),e(BJ,zao),e($r,Vao),e($r,Ii),e(Ii,Wao),e(Ii,xJ),e(xJ,Qao),e(Ii,Hao),e(Ii,kJ),e(kJ,Uao),e(Ii,Jao),e($r,Yao),e($r,RJ),e(RJ,Kao),e($r,Zao),g(u5,$r,null),e(Xo,eso),e(Xo,ke),g(p5,ke,null),e(ke,oso),e(ke,SJ),e(SJ,rso),e(ke,tso),e(ke,Ia),e(Ia,aso),e(Ia,PJ),e(PJ,sso),e(Ia,nso),e(Ia,$J),e($J,lso),e(Ia,iso),e(Ia,IJ),e(IJ,dso),e(Ia,cso),e(ke,mso),e(ke,k),e(k,Ru),e(Ru,jJ),e(jJ,fso),e(Ru,gso),e(Ru,Kk),e(Kk,hso),e(Ru,uso),e(k,pso),e(k,Su),e(Su,NJ),e(NJ,_so),e(Su,bso),e(Su,Zk),e(Zk,vso),e(Su,Tso),e(k,Fso),e(k,Pu),e(Pu,DJ),e(DJ,Cso),e(Pu,Mso),e(Pu,eR),e(eR,Eso),e(Pu,yso),e(k,wso),e(k,$u),e($u,qJ),e(qJ,Aso),e($u,Lso),e($u,oR),e(oR,Bso),e($u,xso),e(k,kso),e(k,Iu),e(Iu,GJ),e(GJ,Rso),e(Iu,Sso),e(Iu,rR),e(rR,Pso),e(Iu,$so),e(k,Iso),e(k,ju),e(ju,OJ),e(OJ,jso),e(ju,Nso),e(ju,tR),e(tR,Dso),e(ju,qso),e(k,Gso),e(k,Nu),e(Nu,XJ),e(XJ,Oso),e(Nu,Xso),e(Nu,aR),e(aR,zso),e(Nu,Vso),e(k,Wso),e(k,Du),e(Du,zJ),e(zJ,Qso),e(Du,Hso),e(Du,sR),e(sR,Uso),e(Du,Jso),e(k,Yso),e(k,qu),e(qu,VJ),e(VJ,Kso),e(qu,Zso),e(qu,nR),e(nR,eno),e(qu,ono),e(k,rno),e(k,Gu),e(Gu,WJ),e(WJ,tno),e(Gu,ano),e(Gu,lR),e(lR,sno),e(Gu,nno),e(k,lno),e(k,Ou),e(Ou,QJ),e(QJ,ino),e(Ou,dno),e(Ou,iR),e(iR,cno),e(Ou,mno),e(k,fno),e(k,Xu),e(Xu,HJ),e(HJ,gno),e(Xu,hno),e(Xu,dR),e(dR,uno),e(Xu,pno),e(k,_no),e(k,zu),e(zu,UJ),e(UJ,bno),e(zu,vno),e(zu,cR),e(cR,Tno),e(zu,Fno),e(k,Cno),e(k,Vu),e(Vu,JJ),e(JJ,Mno),e(Vu,Eno),e(Vu,mR),e(mR,yno),e(Vu,wno),e(k,Ano),e(k,Wu),e(Wu,YJ),e(YJ,Lno),e(Wu,Bno),e(Wu,fR),e(fR,xno),e(Wu,kno),e(k,Rno),e(k,Qu),e(Qu,KJ),e(KJ,Sno),e(Qu,Pno),e(Qu,gR),e(gR,$no),e(Qu,Ino),e(k,jno),e(k,Hu),e(Hu,ZJ),e(ZJ,Nno),e(Hu,Dno),e(Hu,hR),e(hR,qno),e(Hu,Gno),e(k,Ono),e(k,Uu),e(Uu,eY),e(eY,Xno),e(Uu,zno),e(Uu,uR),e(uR,Vno),e(Uu,Wno),e(k,Qno),e(k,Ju),e(Ju,oY),e(oY,Hno),e(Ju,Uno),e(Ju,pR),e(pR,Jno),e(Ju,Yno),e(k,Kno),e(k,Yu),e(Yu,rY),e(rY,Zno),e(Yu,elo),e(Yu,_R),e(_R,olo),e(Yu,rlo),e(k,tlo),e(k,Ku),e(Ku,tY),e(tY,alo),e(Ku,slo),e(Ku,bR),e(bR,nlo),e(Ku,llo),e(k,ilo),e(k,Zu),e(Zu,aY),e(aY,dlo),e(Zu,clo),e(Zu,vR),e(vR,mlo),e(Zu,flo),e(k,glo),e(k,ep),e(ep,sY),e(sY,hlo),e(ep,ulo),e(ep,TR),e(TR,plo),e(ep,_lo),e(k,blo),e(k,op),e(op,nY),e(nY,vlo),e(op,Tlo),e(op,FR),e(FR,Flo),e(op,Clo),e(k,Mlo),e(k,rp),e(rp,lY),e(lY,Elo),e(rp,ylo),e(rp,CR),e(CR,wlo),e(rp,Alo),e(k,Llo),e(k,tp),e(tp,iY),e(iY,Blo),e(tp,xlo),e(tp,MR),e(MR,klo),e(tp,Rlo),e(k,Slo),e(k,ap),e(ap,dY),e(dY,Plo),e(ap,$lo),e(ap,ER),e(ER,Ilo),e(ap,jlo),e(k,Nlo),e(k,sp),e(sp,cY),e(cY,Dlo),e(sp,qlo),e(sp,yR),e(yR,Glo),e(sp,Olo),e(k,Xlo),e(k,np),e(np,mY),e(mY,zlo),e(np,Vlo),e(np,wR),e(wR,Wlo),e(np,Qlo),e(k,Hlo),e(k,lp),e(lp,fY),e(fY,Ulo),e(lp,Jlo),e(lp,AR),e(AR,Ylo),e(lp,Klo),e(k,Zlo),e(k,ip),e(ip,gY),e(gY,eio),e(ip,oio),e(ip,LR),e(LR,rio),e(ip,tio),e(k,aio),e(k,dp),e(dp,hY),e(hY,sio),e(dp,nio),e(dp,BR),e(BR,lio),e(dp,iio),e(k,dio),e(k,cp),e(cp,uY),e(uY,cio),e(cp,mio),e(cp,xR),e(xR,fio),e(cp,gio),e(k,hio),e(k,mp),e(mp,pY),e(pY,uio),e(mp,pio),e(mp,kR),e(kR,_io),e(mp,bio),e(k,vio),e(k,fp),e(fp,_Y),e(_Y,Tio),e(fp,Fio),e(fp,RR),e(RR,Cio),e(fp,Mio),e(k,Eio),e(k,gp),e(gp,bY),e(bY,yio),e(gp,wio),e(gp,SR),e(SR,Aio),e(gp,Lio),e(k,Bio),e(k,hp),e(hp,vY),e(vY,xio),e(hp,kio),e(hp,PR),e(PR,Rio),e(hp,Sio),e(k,Pio),e(k,up),e(up,TY),e(TY,$io),e(up,Iio),e(up,$R),e($R,jio),e(up,Nio),e(ke,Dio),e(ke,pp),e(pp,qio),e(pp,FY),e(FY,Gio),e(pp,Oio),e(pp,CY),e(CY,Xio),e(ke,zio),e(ke,MY),e(MY,Vio),e(ke,Wio),g(_5,ke,null),b(d,BLe,_),b(d,ji,_),e(ji,_p),e(_p,EY),g(b5,EY,null),e(ji,Qio),e(ji,yY),e(yY,Hio),b(d,xLe,_),b(d,zo,_),g(v5,zo,null),e(zo,Uio),e(zo,Ni),e(Ni,Jio),e(Ni,wY),e(wY,Yio),e(Ni,Kio),e(Ni,AY),e(AY,Zio),e(Ni,edo),e(zo,odo),e(zo,T5),e(T5,rdo),e(T5,LY),e(LY,tdo),e(T5,ado),e(zo,sdo),e(zo,Ir),g(F5,Ir,null),e(Ir,ndo),e(Ir,BY),e(BY,ldo),e(Ir,ido),e(Ir,Di),e(Di,ddo),e(Di,xY),e(xY,cdo),e(Di,mdo),e(Di,kY),e(kY,fdo),e(Di,gdo),e(Ir,hdo),e(Ir,RY),e(RY,udo),e(Ir,pdo),g(C5,Ir,null),e(zo,_do),e(zo,Re),g(M5,Re,null),e(Re,bdo),e(Re,SY),e(SY,vdo),e(Re,Tdo),e(Re,ja),e(ja,Fdo),e(ja,PY),e(PY,Cdo),e(ja,Mdo),e(ja,$Y),e($Y,Edo),e(ja,ydo),e(ja,IY),e(IY,wdo),e(ja,Ado),e(Re,Ldo),e(Re,I),e(I,bp),e(bp,jY),e(jY,Bdo),e(bp,xdo),e(bp,IR),e(IR,kdo),e(bp,Rdo),e(I,Sdo),e(I,vp),e(vp,NY),e(NY,Pdo),e(vp,$do),e(vp,jR),e(jR,Ido),e(vp,jdo),e(I,Ndo),e(I,Tp),e(Tp,DY),e(DY,Ddo),e(Tp,qdo),e(Tp,NR),e(NR,Gdo),e(Tp,Odo),e(I,Xdo),e(I,Fp),e(Fp,qY),e(qY,zdo),e(Fp,Vdo),e(Fp,DR),e(DR,Wdo),e(Fp,Qdo),e(I,Hdo),e(I,Cp),e(Cp,GY),e(GY,Udo),e(Cp,Jdo),e(Cp,qR),e(qR,Ydo),e(Cp,Kdo),e(I,Zdo),e(I,Mp),e(Mp,OY),e(OY,eco),e(Mp,oco),e(Mp,GR),e(GR,rco),e(Mp,tco),e(I,aco),e(I,Ep),e(Ep,XY),e(XY,sco),e(Ep,nco),e(Ep,OR),e(OR,lco),e(Ep,ico),e(I,dco),e(I,yp),e(yp,zY),e(zY,cco),e(yp,mco),e(yp,XR),e(XR,fco),e(yp,gco),e(I,hco),e(I,wp),e(wp,VY),e(VY,uco),e(wp,pco),e(wp,zR),e(zR,_co),e(wp,bco),e(I,vco),e(I,Ap),e(Ap,WY),e(WY,Tco),e(Ap,Fco),e(Ap,VR),e(VR,Cco),e(Ap,Mco),e(I,Eco),e(I,Lp),e(Lp,QY),e(QY,yco),e(Lp,wco),e(Lp,WR),e(WR,Aco),e(Lp,Lco),e(I,Bco),e(I,Bp),e(Bp,HY),e(HY,xco),e(Bp,kco),e(Bp,QR),e(QR,Rco),e(Bp,Sco),e(I,Pco),e(I,xp),e(xp,UY),e(UY,$co),e(xp,Ico),e(xp,HR),e(HR,jco),e(xp,Nco),e(I,Dco),e(I,kp),e(kp,JY),e(JY,qco),e(kp,Gco),e(kp,UR),e(UR,Oco),e(kp,Xco),e(I,zco),e(I,Rp),e(Rp,YY),e(YY,Vco),e(Rp,Wco),e(Rp,JR),e(JR,Qco),e(Rp,Hco),e(I,Uco),e(I,Sp),e(Sp,KY),e(KY,Jco),e(Sp,Yco),e(Sp,YR),e(YR,Kco),e(Sp,Zco),e(I,emo),e(I,Pp),e(Pp,ZY),e(ZY,omo),e(Pp,rmo),e(Pp,KR),e(KR,tmo),e(Pp,amo),e(I,smo),e(I,$p),e($p,eK),e(eK,nmo),e($p,lmo),e($p,ZR),e(ZR,imo),e($p,dmo),e(I,cmo),e(I,Ip),e(Ip,oK),e(oK,mmo),e(Ip,fmo),e(Ip,eS),e(eS,gmo),e(Ip,hmo),e(I,umo),e(I,jp),e(jp,rK),e(rK,pmo),e(jp,_mo),e(jp,tK),e(tK,bmo),e(jp,vmo),e(I,Tmo),e(I,Np),e(Np,aK),e(aK,Fmo),e(Np,Cmo),e(Np,oS),e(oS,Mmo),e(Np,Emo),e(I,ymo),e(I,Dp),e(Dp,sK),e(sK,wmo),e(Dp,Amo),e(Dp,rS),e(rS,Lmo),e(Dp,Bmo),e(I,xmo),e(I,qp),e(qp,nK),e(nK,kmo),e(qp,Rmo),e(qp,tS),e(tS,Smo),e(qp,Pmo),e(I,$mo),e(I,Gp),e(Gp,lK),e(lK,Imo),e(Gp,jmo),e(Gp,aS),e(aS,Nmo),e(Gp,Dmo),e(I,qmo),e(I,Op),e(Op,iK),e(iK,Gmo),e(Op,Omo),e(Op,sS),e(sS,Xmo),e(Op,zmo),e(I,Vmo),e(I,Xp),e(Xp,dK),e(dK,Wmo),e(Xp,Qmo),e(Xp,nS),e(nS,Hmo),e(Xp,Umo),e(I,Jmo),e(I,zp),e(zp,cK),e(cK,Ymo),e(zp,Kmo),e(zp,lS),e(lS,Zmo),e(zp,efo),e(I,ofo),e(I,Vp),e(Vp,mK),e(mK,rfo),e(Vp,tfo),e(Vp,iS),e(iS,afo),e(Vp,sfo),e(I,nfo),e(I,Wp),e(Wp,fK),e(fK,lfo),e(Wp,ifo),e(Wp,dS),e(dS,dfo),e(Wp,cfo),e(I,mfo),e(I,Qp),e(Qp,gK),e(gK,ffo),e(Qp,gfo),e(Qp,cS),e(cS,hfo),e(Qp,ufo),e(I,pfo),e(I,Hp),e(Hp,hK),e(hK,_fo),e(Hp,bfo),e(Hp,mS),e(mS,vfo),e(Hp,Tfo),e(I,Ffo),e(I,Up),e(Up,uK),e(uK,Cfo),e(Up,Mfo),e(Up,fS),e(fS,Efo),e(Up,yfo),e(I,wfo),e(I,Jp),e(Jp,pK),e(pK,Afo),e(Jp,Lfo),e(Jp,gS),e(gS,Bfo),e(Jp,xfo),e(Re,kfo),e(Re,Yp),e(Yp,Rfo),e(Yp,_K),e(_K,Sfo),e(Yp,Pfo),e(Yp,bK),e(bK,$fo),e(Re,Ifo),e(Re,vK),e(vK,jfo),e(Re,Nfo),g(E5,Re,null),b(d,kLe,_),b(d,qi,_),e(qi,Kp),e(Kp,TK),g(y5,TK,null),e(qi,Dfo),e(qi,FK),e(FK,qfo),b(d,RLe,_),b(d,Vo,_),g(w5,Vo,null),e(Vo,Gfo),e(Vo,Gi),e(Gi,Ofo),e(Gi,CK),e(CK,Xfo),e(Gi,zfo),e(Gi,MK),e(MK,Vfo),e(Gi,Wfo),e(Vo,Qfo),e(Vo,A5),e(A5,Hfo),e(A5,EK),e(EK,Ufo),e(A5,Jfo),e(Vo,Yfo),e(Vo,jr),g(L5,jr,null),e(jr,Kfo),e(jr,yK),e(yK,Zfo),e(jr,ego),e(jr,Oi),e(Oi,ogo),e(Oi,wK),e(wK,rgo),e(Oi,tgo),e(Oi,AK),e(AK,ago),e(Oi,sgo),e(jr,ngo),e(jr,LK),e(LK,lgo),e(jr,igo),g(B5,jr,null),e(Vo,dgo),e(Vo,Se),g(x5,Se,null),e(Se,cgo),e(Se,BK),e(BK,mgo),e(Se,fgo),e(Se,Na),e(Na,ggo),e(Na,xK),e(xK,hgo),e(Na,ugo),e(Na,kK),e(kK,pgo),e(Na,_go),e(Na,RK),e(RK,bgo),e(Na,vgo),e(Se,Tgo),e(Se,$),e($,Zp),e(Zp,SK),e(SK,Fgo),e(Zp,Cgo),e(Zp,hS),e(hS,Mgo),e(Zp,Ego),e($,ygo),e($,e_),e(e_,PK),e(PK,wgo),e(e_,Ago),e(e_,uS),e(uS,Lgo),e(e_,Bgo),e($,xgo),e($,o_),e(o_,$K),e($K,kgo),e(o_,Rgo),e(o_,pS),e(pS,Sgo),e(o_,Pgo),e($,$go),e($,r_),e(r_,IK),e(IK,Igo),e(r_,jgo),e(r_,_S),e(_S,Ngo),e(r_,Dgo),e($,qgo),e($,t_),e(t_,jK),e(jK,Ggo),e(t_,Ogo),e(t_,bS),e(bS,Xgo),e(t_,zgo),e($,Vgo),e($,a_),e(a_,NK),e(NK,Wgo),e(a_,Qgo),e(a_,vS),e(vS,Hgo),e(a_,Ugo),e($,Jgo),e($,s_),e(s_,DK),e(DK,Ygo),e(s_,Kgo),e(s_,TS),e(TS,Zgo),e(s_,eho),e($,oho),e($,n_),e(n_,qK),e(qK,rho),e(n_,tho),e(n_,FS),e(FS,aho),e(n_,sho),e($,nho),e($,l_),e(l_,GK),e(GK,lho),e(l_,iho),e(l_,CS),e(CS,dho),e(l_,cho),e($,mho),e($,i_),e(i_,OK),e(OK,fho),e(i_,gho),e(i_,MS),e(MS,hho),e(i_,uho),e($,pho),e($,d_),e(d_,XK),e(XK,_ho),e(d_,bho),e(d_,ES),e(ES,vho),e(d_,Tho),e($,Fho),e($,c_),e(c_,zK),e(zK,Cho),e(c_,Mho),e(c_,yS),e(yS,Eho),e(c_,yho),e($,who),e($,m_),e(m_,VK),e(VK,Aho),e(m_,Lho),e(m_,wS),e(wS,Bho),e(m_,xho),e($,kho),e($,f_),e(f_,WK),e(WK,Rho),e(f_,Sho),e(f_,AS),e(AS,Pho),e(f_,$ho),e($,Iho),e($,g_),e(g_,QK),e(QK,jho),e(g_,Nho),e(g_,LS),e(LS,Dho),e(g_,qho),e($,Gho),e($,h_),e(h_,HK),e(HK,Oho),e(h_,Xho),e(h_,BS),e(BS,zho),e(h_,Vho),e($,Who),e($,u_),e(u_,UK),e(UK,Qho),e(u_,Hho),e(u_,xS),e(xS,Uho),e(u_,Jho),e($,Yho),e($,p_),e(p_,JK),e(JK,Kho),e(p_,Zho),e(p_,kS),e(kS,euo),e(p_,ouo),e($,ruo),e($,__),e(__,YK),e(YK,tuo),e(__,auo),e(__,RS),e(RS,suo),e(__,nuo),e($,luo),e($,b_),e(b_,KK),e(KK,iuo),e(b_,duo),e(b_,SS),e(SS,cuo),e(b_,muo),e($,fuo),e($,v_),e(v_,ZK),e(ZK,guo),e(v_,huo),e(v_,PS),e(PS,uuo),e(v_,puo),e($,_uo),e($,T_),e(T_,eZ),e(eZ,buo),e(T_,vuo),e(T_,$S),e($S,Tuo),e(T_,Fuo),e($,Cuo),e($,F_),e(F_,oZ),e(oZ,Muo),e(F_,Euo),e(F_,rZ),e(rZ,yuo),e(F_,wuo),e($,Auo),e($,C_),e(C_,tZ),e(tZ,Luo),e(C_,Buo),e(C_,IS),e(IS,xuo),e(C_,kuo),e($,Ruo),e($,M_),e(M_,aZ),e(aZ,Suo),e(M_,Puo),e(M_,jS),e(jS,$uo),e(M_,Iuo),e($,juo),e($,E_),e(E_,sZ),e(sZ,Nuo),e(E_,Duo),e(E_,NS),e(NS,quo),e(E_,Guo),e($,Ouo),e($,y_),e(y_,nZ),e(nZ,Xuo),e(y_,zuo),e(y_,DS),e(DS,Vuo),e(y_,Wuo),e($,Quo),e($,w_),e(w_,lZ),e(lZ,Huo),e(w_,Uuo),e(w_,qS),e(qS,Juo),e(w_,Yuo),e($,Kuo),e($,A_),e(A_,iZ),e(iZ,Zuo),e(A_,epo),e(A_,GS),e(GS,opo),e(A_,rpo),e($,tpo),e($,L_),e(L_,dZ),e(dZ,apo),e(L_,spo),e(L_,cZ),e(cZ,npo),e(L_,lpo),e($,ipo),e($,B_),e(B_,mZ),e(mZ,dpo),e(B_,cpo),e(B_,OS),e(OS,mpo),e(B_,fpo),e($,gpo),e($,x_),e(x_,fZ),e(fZ,hpo),e(x_,upo),e(x_,XS),e(XS,ppo),e(x_,_po),e($,bpo),e($,k_),e(k_,gZ),e(gZ,vpo),e(k_,Tpo),e(k_,zS),e(zS,Fpo),e(k_,Cpo),e($,Mpo),e($,R_),e(R_,hZ),e(hZ,Epo),e(R_,ypo),e(R_,VS),e(VS,wpo),e(R_,Apo),e(Se,Lpo),e(Se,S_),e(S_,Bpo),e(S_,uZ),e(uZ,xpo),e(S_,kpo),e(S_,pZ),e(pZ,Rpo),e(Se,Spo),e(Se,_Z),e(_Z,Ppo),e(Se,$po),g(k5,Se,null),b(d,SLe,_),b(d,Xi,_),e(Xi,P_),e(P_,bZ),g(R5,bZ,null),e(Xi,Ipo),e(Xi,vZ),e(vZ,jpo),b(d,PLe,_),b(d,Wo,_),g(S5,Wo,null),e(Wo,Npo),e(Wo,zi),e(zi,Dpo),e(zi,TZ),e(TZ,qpo),e(zi,Gpo),e(zi,FZ),e(FZ,Opo),e(zi,Xpo),e(Wo,zpo),e(Wo,P5),e(P5,Vpo),e(P5,CZ),e(CZ,Wpo),e(P5,Qpo),e(Wo,Hpo),e(Wo,Nr),g($5,Nr,null),e(Nr,Upo),e(Nr,MZ),e(MZ,Jpo),e(Nr,Ypo),e(Nr,Vi),e(Vi,Kpo),e(Vi,EZ),e(EZ,Zpo),e(Vi,e_o),e(Vi,yZ),e(yZ,o_o),e(Vi,r_o),e(Nr,t_o),e(Nr,wZ),e(wZ,a_o),e(Nr,s_o),g(I5,Nr,null),e(Wo,n_o),e(Wo,Pe),g(j5,Pe,null),e(Pe,l_o),e(Pe,AZ),e(AZ,i_o),e(Pe,d_o),e(Pe,Da),e(Da,c_o),e(Da,LZ),e(LZ,m_o),e(Da,f_o),e(Da,BZ),e(BZ,g_o),e(Da,h_o),e(Da,xZ),e(xZ,u_o),e(Da,p_o),e(Pe,__o),e(Pe,se),e(se,$_),e($_,kZ),e(kZ,b_o),e($_,v_o),e($_,WS),e(WS,T_o),e($_,F_o),e(se,C_o),e(se,I_),e(I_,RZ),e(RZ,M_o),e(I_,E_o),e(I_,QS),e(QS,y_o),e(I_,w_o),e(se,A_o),e(se,j_),e(j_,SZ),e(SZ,L_o),e(j_,B_o),e(j_,HS),e(HS,x_o),e(j_,k_o),e(se,R_o),e(se,N_),e(N_,PZ),e(PZ,S_o),e(N_,P_o),e(N_,US),e(US,$_o),e(N_,I_o),e(se,j_o),e(se,D_),e(D_,$Z),e($Z,N_o),e(D_,D_o),e(D_,JS),e(JS,q_o),e(D_,G_o),e(se,O_o),e(se,q_),e(q_,IZ),e(IZ,X_o),e(q_,z_o),e(q_,YS),e(YS,V_o),e(q_,W_o),e(se,Q_o),e(se,G_),e(G_,jZ),e(jZ,H_o),e(G_,U_o),e(G_,KS),e(KS,J_o),e(G_,Y_o),e(se,K_o),e(se,O_),e(O_,NZ),e(NZ,Z_o),e(O_,ebo),e(O_,ZS),e(ZS,obo),e(O_,rbo),e(se,tbo),e(se,X_),e(X_,DZ),e(DZ,abo),e(X_,sbo),e(X_,eP),e(eP,nbo),e(X_,lbo),e(se,ibo),e(se,z_),e(z_,qZ),e(qZ,dbo),e(z_,cbo),e(z_,oP),e(oP,mbo),e(z_,fbo),e(se,gbo),e(se,V_),e(V_,GZ),e(GZ,hbo),e(V_,ubo),e(V_,rP),e(rP,pbo),e(V_,_bo),e(se,bbo),e(se,W_),e(W_,OZ),e(OZ,vbo),e(W_,Tbo),e(W_,tP),e(tP,Fbo),e(W_,Cbo),e(se,Mbo),e(se,Q_),e(Q_,XZ),e(XZ,Ebo),e(Q_,ybo),e(Q_,aP),e(aP,wbo),e(Q_,Abo),e(se,Lbo),e(se,H_),e(H_,zZ),e(zZ,Bbo),e(H_,xbo),e(H_,sP),e(sP,kbo),e(H_,Rbo),e(se,Sbo),e(se,U_),e(U_,VZ),e(VZ,Pbo),e(U_,$bo),e(U_,nP),e(nP,Ibo),e(U_,jbo),e(Pe,Nbo),e(Pe,J_),e(J_,Dbo),e(J_,WZ),e(WZ,qbo),e(J_,Gbo),e(J_,QZ),e(QZ,Obo),e(Pe,Xbo),e(Pe,HZ),e(HZ,zbo),e(Pe,Vbo),g(N5,Pe,null),b(d,$Le,_),b(d,Wi,_),e(Wi,Y_),e(Y_,UZ),g(D5,UZ,null),e(Wi,Wbo),e(Wi,JZ),e(JZ,Qbo),b(d,ILe,_),b(d,Qo,_),g(q5,Qo,null),e(Qo,Hbo),e(Qo,Qi),e(Qi,Ubo),e(Qi,YZ),e(YZ,Jbo),e(Qi,Ybo),e(Qi,KZ),e(KZ,Kbo),e(Qi,Zbo),e(Qo,e2o),e(Qo,G5),e(G5,o2o),e(G5,ZZ),e(ZZ,r2o),e(G5,t2o),e(Qo,a2o),e(Qo,Dr),g(O5,Dr,null),e(Dr,s2o),e(Dr,eee),e(eee,n2o),e(Dr,l2o),e(Dr,Hi),e(Hi,i2o),e(Hi,oee),e(oee,d2o),e(Hi,c2o),e(Hi,ree),e(ree,m2o),e(Hi,f2o),e(Dr,g2o),e(Dr,tee),e(tee,h2o),e(Dr,u2o),g(X5,Dr,null),e(Qo,p2o),e(Qo,$e),g(z5,$e,null),e($e,_2o),e($e,aee),e(aee,b2o),e($e,v2o),e($e,qa),e(qa,T2o),e(qa,see),e(see,F2o),e(qa,C2o),e(qa,nee),e(nee,M2o),e(qa,E2o),e(qa,lee),e(lee,y2o),e(qa,w2o),e($e,A2o),e($e,A),e(A,K_),e(K_,iee),e(iee,L2o),e(K_,B2o),e(K_,lP),e(lP,x2o),e(K_,k2o),e(A,R2o),e(A,Z_),e(Z_,dee),e(dee,S2o),e(Z_,P2o),e(Z_,iP),e(iP,$2o),e(Z_,I2o),e(A,j2o),e(A,eb),e(eb,cee),e(cee,N2o),e(eb,D2o),e(eb,dP),e(dP,q2o),e(eb,G2o),e(A,O2o),e(A,ob),e(ob,mee),e(mee,X2o),e(ob,z2o),e(ob,cP),e(cP,V2o),e(ob,W2o),e(A,Q2o),e(A,rb),e(rb,fee),e(fee,H2o),e(rb,U2o),e(rb,mP),e(mP,J2o),e(rb,Y2o),e(A,K2o),e(A,tb),e(tb,gee),e(gee,Z2o),e(tb,evo),e(tb,fP),e(fP,ovo),e(tb,rvo),e(A,tvo),e(A,ab),e(ab,hee),e(hee,avo),e(ab,svo),e(ab,gP),e(gP,nvo),e(ab,lvo),e(A,ivo),e(A,sb),e(sb,uee),e(uee,dvo),e(sb,cvo),e(sb,hP),e(hP,mvo),e(sb,fvo),e(A,gvo),e(A,nb),e(nb,pee),e(pee,hvo),e(nb,uvo),e(nb,uP),e(uP,pvo),e(nb,_vo),e(A,bvo),e(A,lb),e(lb,_ee),e(_ee,vvo),e(lb,Tvo),e(lb,pP),e(pP,Fvo),e(lb,Cvo),e(A,Mvo),e(A,ib),e(ib,bee),e(bee,Evo),e(ib,yvo),e(ib,_P),e(_P,wvo),e(ib,Avo),e(A,Lvo),e(A,db),e(db,vee),e(vee,Bvo),e(db,xvo),e(db,bP),e(bP,kvo),e(db,Rvo),e(A,Svo),e(A,cb),e(cb,Tee),e(Tee,Pvo),e(cb,$vo),e(cb,vP),e(vP,Ivo),e(cb,jvo),e(A,Nvo),e(A,mb),e(mb,Fee),e(Fee,Dvo),e(mb,qvo),e(mb,TP),e(TP,Gvo),e(mb,Ovo),e(A,Xvo),e(A,fb),e(fb,Cee),e(Cee,zvo),e(fb,Vvo),e(fb,FP),e(FP,Wvo),e(fb,Qvo),e(A,Hvo),e(A,gb),e(gb,Mee),e(Mee,Uvo),e(gb,Jvo),e(gb,CP),e(CP,Yvo),e(gb,Kvo),e(A,Zvo),e(A,hb),e(hb,Eee),e(Eee,eTo),e(hb,oTo),e(hb,MP),e(MP,rTo),e(hb,tTo),e(A,aTo),e(A,ub),e(ub,yee),e(yee,sTo),e(ub,nTo),e(ub,EP),e(EP,lTo),e(ub,iTo),e(A,dTo),e(A,pb),e(pb,wee),e(wee,cTo),e(pb,mTo),e(pb,yP),e(yP,fTo),e(pb,gTo),e(A,hTo),e(A,_b),e(_b,Aee),e(Aee,uTo),e(_b,pTo),e(_b,wP),e(wP,_To),e(_b,bTo),e(A,vTo),e(A,bb),e(bb,Lee),e(Lee,TTo),e(bb,FTo),e(bb,AP),e(AP,CTo),e(bb,MTo),e(A,ETo),e(A,vb),e(vb,Bee),e(Bee,yTo),e(vb,wTo),e(vb,LP),e(LP,ATo),e(vb,LTo),e(A,BTo),e(A,Tb),e(Tb,xee),e(xee,xTo),e(Tb,kTo),e(Tb,BP),e(BP,RTo),e(Tb,STo),e(A,PTo),e(A,Fb),e(Fb,kee),e(kee,$To),e(Fb,ITo),e(Fb,xP),e(xP,jTo),e(Fb,NTo),e(A,DTo),e(A,Cb),e(Cb,Ree),e(Ree,qTo),e(Cb,GTo),e(Cb,kP),e(kP,OTo),e(Cb,XTo),e(A,zTo),e(A,Mb),e(Mb,See),e(See,VTo),e(Mb,WTo),e(Mb,RP),e(RP,QTo),e(Mb,HTo),e(A,UTo),e(A,Eb),e(Eb,Pee),e(Pee,JTo),e(Eb,YTo),e(Eb,SP),e(SP,KTo),e(Eb,ZTo),e(A,e1o),e(A,yb),e(yb,$ee),e($ee,o1o),e(yb,r1o),e(yb,PP),e(PP,t1o),e(yb,a1o),e(A,s1o),e(A,wb),e(wb,Iee),e(Iee,n1o),e(wb,l1o),e(wb,$P),e($P,i1o),e(wb,d1o),e(A,c1o),e(A,Ab),e(Ab,jee),e(jee,m1o),e(Ab,f1o),e(Ab,IP),e(IP,g1o),e(Ab,h1o),e(A,u1o),e(A,Lb),e(Lb,Nee),e(Nee,p1o),e(Lb,_1o),e(Lb,jP),e(jP,b1o),e(Lb,v1o),e(A,T1o),e(A,Bb),e(Bb,Dee),e(Dee,F1o),e(Bb,C1o),e(Bb,qee),e(qee,M1o),e(Bb,E1o),e(A,y1o),e(A,xb),e(xb,Gee),e(Gee,w1o),e(xb,A1o),e(xb,NP),e(NP,L1o),e(xb,B1o),e(A,x1o),e(A,kb),e(kb,Oee),e(Oee,k1o),e(kb,R1o),e(kb,DP),e(DP,S1o),e(kb,P1o),e(A,$1o),e(A,Rb),e(Rb,Xee),e(Xee,I1o),e(Rb,j1o),e(Rb,qP),e(qP,N1o),e(Rb,D1o),e(A,q1o),e(A,Sb),e(Sb,zee),e(zee,G1o),e(Sb,O1o),e(Sb,GP),e(GP,X1o),e(Sb,z1o),e(A,V1o),e(A,Pb),e(Pb,Vee),e(Vee,W1o),e(Pb,Q1o),e(Pb,OP),e(OP,H1o),e(Pb,U1o),e(A,J1o),e(A,$b),e($b,Wee),e(Wee,Y1o),e($b,K1o),e($b,XP),e(XP,Z1o),e($b,eFo),e(A,oFo),e(A,Ib),e(Ib,Qee),e(Qee,rFo),e(Ib,tFo),e(Ib,zP),e(zP,aFo),e(Ib,sFo),e(A,nFo),e(A,jb),e(jb,Hee),e(Hee,lFo),e(jb,iFo),e(jb,VP),e(VP,dFo),e(jb,cFo),e(A,mFo),e(A,Nb),e(Nb,Uee),e(Uee,fFo),e(Nb,gFo),e(Nb,WP),e(WP,hFo),e(Nb,uFo),e(A,pFo),e(A,Db),e(Db,Jee),e(Jee,_Fo),e(Db,bFo),e(Db,QP),e(QP,vFo),e(Db,TFo),e(A,FFo),e(A,qb),e(qb,Yee),e(Yee,CFo),e(qb,MFo),e(qb,HP),e(HP,EFo),e(qb,yFo),e(A,wFo),e(A,Gb),e(Gb,Kee),e(Kee,AFo),e(Gb,LFo),e(Gb,UP),e(UP,BFo),e(Gb,xFo),e($e,kFo),e($e,Ob),e(Ob,RFo),e(Ob,Zee),e(Zee,SFo),e(Ob,PFo),e(Ob,eoe),e(eoe,$Fo),e($e,IFo),e($e,ooe),e(ooe,jFo),e($e,NFo),g(V5,$e,null),b(d,jLe,_),b(d,Ui,_),e(Ui,Xb),e(Xb,roe),g(W5,roe,null),e(Ui,DFo),e(Ui,toe),e(toe,qFo),b(d,NLe,_),b(d,Ho,_),g(Q5,Ho,null),e(Ho,GFo),e(Ho,Ji),e(Ji,OFo),e(Ji,aoe),e(aoe,XFo),e(Ji,zFo),e(Ji,soe),e(soe,VFo),e(Ji,WFo),e(Ho,QFo),e(Ho,H5),e(H5,HFo),e(H5,noe),e(noe,UFo),e(H5,JFo),e(Ho,YFo),e(Ho,qr),g(U5,qr,null),e(qr,KFo),e(qr,loe),e(loe,ZFo),e(qr,eCo),e(qr,Yi),e(Yi,oCo),e(Yi,ioe),e(ioe,rCo),e(Yi,tCo),e(Yi,doe),e(doe,aCo),e(Yi,sCo),e(qr,nCo),e(qr,coe),e(coe,lCo),e(qr,iCo),g(J5,qr,null),e(Ho,dCo),e(Ho,Ie),g(Y5,Ie,null),e(Ie,cCo),e(Ie,moe),e(moe,mCo),e(Ie,fCo),e(Ie,Ga),e(Ga,gCo),e(Ga,foe),e(foe,hCo),e(Ga,uCo),e(Ga,goe),e(goe,pCo),e(Ga,_Co),e(Ga,hoe),e(hoe,bCo),e(Ga,vCo),e(Ie,TCo),e(Ie,G),e(G,zb),e(zb,uoe),e(uoe,FCo),e(zb,CCo),e(zb,JP),e(JP,MCo),e(zb,ECo),e(G,yCo),e(G,Vb),e(Vb,poe),e(poe,wCo),e(Vb,ACo),e(Vb,YP),e(YP,LCo),e(Vb,BCo),e(G,xCo),e(G,Wb),e(Wb,_oe),e(_oe,kCo),e(Wb,RCo),e(Wb,KP),e(KP,SCo),e(Wb,PCo),e(G,$Co),e(G,Qb),e(Qb,boe),e(boe,ICo),e(Qb,jCo),e(Qb,ZP),e(ZP,NCo),e(Qb,DCo),e(G,qCo),e(G,Hb),e(Hb,voe),e(voe,GCo),e(Hb,OCo),e(Hb,e$),e(e$,XCo),e(Hb,zCo),e(G,VCo),e(G,Ub),e(Ub,Toe),e(Toe,WCo),e(Ub,QCo),e(Ub,o$),e(o$,HCo),e(Ub,UCo),e(G,JCo),e(G,Jb),e(Jb,Foe),e(Foe,YCo),e(Jb,KCo),e(Jb,r$),e(r$,ZCo),e(Jb,e4o),e(G,o4o),e(G,Yb),e(Yb,Coe),e(Coe,r4o),e(Yb,t4o),e(Yb,t$),e(t$,a4o),e(Yb,s4o),e(G,n4o),e(G,Kb),e(Kb,Moe),e(Moe,l4o),e(Kb,i4o),e(Kb,a$),e(a$,d4o),e(Kb,c4o),e(G,m4o),e(G,Zb),e(Zb,Eoe),e(Eoe,f4o),e(Zb,g4o),e(Zb,s$),e(s$,h4o),e(Zb,u4o),e(G,p4o),e(G,e2),e(e2,yoe),e(yoe,_4o),e(e2,b4o),e(e2,n$),e(n$,v4o),e(e2,T4o),e(G,F4o),e(G,o2),e(o2,woe),e(woe,C4o),e(o2,M4o),e(o2,l$),e(l$,E4o),e(o2,y4o),e(G,w4o),e(G,r2),e(r2,Aoe),e(Aoe,A4o),e(r2,L4o),e(r2,i$),e(i$,B4o),e(r2,x4o),e(G,k4o),e(G,t2),e(t2,Loe),e(Loe,R4o),e(t2,S4o),e(t2,d$),e(d$,P4o),e(t2,$4o),e(G,I4o),e(G,a2),e(a2,Boe),e(Boe,j4o),e(a2,N4o),e(a2,c$),e(c$,D4o),e(a2,q4o),e(G,G4o),e(G,s2),e(s2,xoe),e(xoe,O4o),e(s2,X4o),e(s2,m$),e(m$,z4o),e(s2,V4o),e(G,W4o),e(G,n2),e(n2,koe),e(koe,Q4o),e(n2,H4o),e(n2,f$),e(f$,U4o),e(n2,J4o),e(G,Y4o),e(G,l2),e(l2,Roe),e(Roe,K4o),e(l2,Z4o),e(l2,Soe),e(Soe,eMo),e(l2,oMo),e(G,rMo),e(G,i2),e(i2,Poe),e(Poe,tMo),e(i2,aMo),e(i2,g$),e(g$,sMo),e(i2,nMo),e(G,lMo),e(G,d2),e(d2,$oe),e($oe,iMo),e(d2,dMo),e(d2,h$),e(h$,cMo),e(d2,mMo),e(G,fMo),e(G,c2),e(c2,Ioe),e(Ioe,gMo),e(c2,hMo),e(c2,u$),e(u$,uMo),e(c2,pMo),e(G,_Mo),e(G,m2),e(m2,joe),e(joe,bMo),e(m2,vMo),e(m2,p$),e(p$,TMo),e(m2,FMo),e(G,CMo),e(G,f2),e(f2,Noe),e(Noe,MMo),e(f2,EMo),e(f2,_$),e(_$,yMo),e(f2,wMo),e(G,AMo),e(G,g2),e(g2,Doe),e(Doe,LMo),e(g2,BMo),e(g2,b$),e(b$,xMo),e(g2,kMo),e(G,RMo),e(G,h2),e(h2,qoe),e(qoe,SMo),e(h2,PMo),e(h2,v$),e(v$,$Mo),e(h2,IMo),e(G,jMo),e(G,u2),e(u2,Goe),e(Goe,NMo),e(u2,DMo),e(u2,T$),e(T$,qMo),e(u2,GMo),e(G,OMo),e(G,p2),e(p2,Ooe),e(Ooe,XMo),e(p2,zMo),e(p2,F$),e(F$,VMo),e(p2,WMo),e(Ie,QMo),e(Ie,_2),e(_2,HMo),e(_2,Xoe),e(Xoe,UMo),e(_2,JMo),e(_2,zoe),e(zoe,YMo),e(Ie,KMo),e(Ie,Voe),e(Voe,ZMo),e(Ie,eEo),g(K5,Ie,null),b(d,DLe,_),b(d,Ki,_),e(Ki,b2),e(b2,Woe),g(Z5,Woe,null),e(Ki,oEo),e(Ki,Qoe),e(Qoe,rEo),b(d,qLe,_),b(d,Uo,_),g(ey,Uo,null),e(Uo,tEo),e(Uo,Zi),e(Zi,aEo),e(Zi,Hoe),e(Hoe,sEo),e(Zi,nEo),e(Zi,Uoe),e(Uoe,lEo),e(Zi,iEo),e(Uo,dEo),e(Uo,oy),e(oy,cEo),e(oy,Joe),e(Joe,mEo),e(oy,fEo),e(Uo,gEo),e(Uo,Gr),g(ry,Gr,null),e(Gr,hEo),e(Gr,Yoe),e(Yoe,uEo),e(Gr,pEo),e(Gr,ed),e(ed,_Eo),e(ed,Koe),e(Koe,bEo),e(ed,vEo),e(ed,Zoe),e(Zoe,TEo),e(ed,FEo),e(Gr,CEo),e(Gr,ere),e(ere,MEo),e(Gr,EEo),g(ty,Gr,null),e(Uo,yEo),e(Uo,je),g(ay,je,null),e(je,wEo),e(je,ore),e(ore,AEo),e(je,LEo),e(je,Oa),e(Oa,BEo),e(Oa,rre),e(rre,xEo),e(Oa,kEo),e(Oa,tre),e(tre,REo),e(Oa,SEo),e(Oa,are),e(are,PEo),e(Oa,$Eo),e(je,IEo),e(je,oa),e(oa,v2),e(v2,sre),e(sre,jEo),e(v2,NEo),e(v2,C$),e(C$,DEo),e(v2,qEo),e(oa,GEo),e(oa,T2),e(T2,nre),e(nre,OEo),e(T2,XEo),e(T2,M$),e(M$,zEo),e(T2,VEo),e(oa,WEo),e(oa,F2),e(F2,lre),e(lre,QEo),e(F2,HEo),e(F2,E$),e(E$,UEo),e(F2,JEo),e(oa,YEo),e(oa,C2),e(C2,ire),e(ire,KEo),e(C2,ZEo),e(C2,y$),e(y$,e3o),e(C2,o3o),e(oa,r3o),e(oa,M2),e(M2,dre),e(dre,t3o),e(M2,a3o),e(M2,cre),e(cre,s3o),e(M2,n3o),e(je,l3o),e(je,E2),e(E2,i3o),e(E2,mre),e(mre,d3o),e(E2,c3o),e(E2,fre),e(fre,m3o),e(je,f3o),e(je,gre),e(gre,g3o),e(je,h3o),g(sy,je,null),b(d,GLe,_),b(d,od,_),e(od,y2),e(y2,hre),g(ny,hre,null),e(od,u3o),e(od,ure),e(ure,p3o),b(d,OLe,_),b(d,Jo,_),g(ly,Jo,null),e(Jo,_3o),e(Jo,rd),e(rd,b3o),e(rd,pre),e(pre,v3o),e(rd,T3o),e(rd,_re),e(_re,F3o),e(rd,C3o),e(Jo,M3o),e(Jo,iy),e(iy,E3o),e(iy,bre),e(bre,y3o),e(iy,w3o),e(Jo,A3o),e(Jo,Or),g(dy,Or,null),e(Or,L3o),e(Or,vre),e(vre,B3o),e(Or,x3o),e(Or,td),e(td,k3o),e(td,Tre),e(Tre,R3o),e(td,S3o),e(td,Fre),e(Fre,P3o),e(td,$3o),e(Or,I3o),e(Or,Cre),e(Cre,j3o),e(Or,N3o),g(cy,Or,null),e(Jo,D3o),e(Jo,Ne),g(my,Ne,null),e(Ne,q3o),e(Ne,Mre),e(Mre,G3o),e(Ne,O3o),e(Ne,Xa),e(Xa,X3o),e(Xa,Ere),e(Ere,z3o),e(Xa,V3o),e(Xa,yre),e(yre,W3o),e(Xa,Q3o),e(Xa,wre),e(wre,H3o),e(Xa,U3o),e(Ne,J3o),e(Ne,N),e(N,w2),e(w2,Are),e(Are,Y3o),e(w2,K3o),e(w2,w$),e(w$,Z3o),e(w2,e5o),e(N,o5o),e(N,A2),e(A2,Lre),e(Lre,r5o),e(A2,t5o),e(A2,A$),e(A$,a5o),e(A2,s5o),e(N,n5o),e(N,L2),e(L2,Bre),e(Bre,l5o),e(L2,i5o),e(L2,L$),e(L$,d5o),e(L2,c5o),e(N,m5o),e(N,B2),e(B2,xre),e(xre,f5o),e(B2,g5o),e(B2,B$),e(B$,h5o),e(B2,u5o),e(N,p5o),e(N,x2),e(x2,kre),e(kre,_5o),e(x2,b5o),e(x2,x$),e(x$,v5o),e(x2,T5o),e(N,F5o),e(N,k2),e(k2,Rre),e(Rre,C5o),e(k2,M5o),e(k2,k$),e(k$,E5o),e(k2,y5o),e(N,w5o),e(N,R2),e(R2,Sre),e(Sre,A5o),e(R2,L5o),e(R2,R$),e(R$,B5o),e(R2,x5o),e(N,k5o),e(N,S2),e(S2,Pre),e(Pre,R5o),e(S2,S5o),e(S2,S$),e(S$,P5o),e(S2,$5o),e(N,I5o),e(N,P2),e(P2,$re),e($re,j5o),e(P2,N5o),e(P2,P$),e(P$,D5o),e(P2,q5o),e(N,G5o),e(N,$2),e($2,Ire),e(Ire,O5o),e($2,X5o),e($2,$$),e($$,z5o),e($2,V5o),e(N,W5o),e(N,I2),e(I2,jre),e(jre,Q5o),e(I2,H5o),e(I2,I$),e(I$,U5o),e(I2,J5o),e(N,Y5o),e(N,j2),e(j2,Nre),e(Nre,K5o),e(j2,Z5o),e(j2,j$),e(j$,eyo),e(j2,oyo),e(N,ryo),e(N,N2),e(N2,Dre),e(Dre,tyo),e(N2,ayo),e(N2,N$),e(N$,syo),e(N2,nyo),e(N,lyo),e(N,D2),e(D2,qre),e(qre,iyo),e(D2,dyo),e(D2,D$),e(D$,cyo),e(D2,myo),e(N,fyo),e(N,q2),e(q2,Gre),e(Gre,gyo),e(q2,hyo),e(q2,q$),e(q$,uyo),e(q2,pyo),e(N,_yo),e(N,G2),e(G2,Ore),e(Ore,byo),e(G2,vyo),e(G2,G$),e(G$,Tyo),e(G2,Fyo),e(N,Cyo),e(N,O2),e(O2,Xre),e(Xre,Myo),e(O2,Eyo),e(O2,O$),e(O$,yyo),e(O2,wyo),e(N,Ayo),e(N,X2),e(X2,zre),e(zre,Lyo),e(X2,Byo),e(X2,X$),e(X$,xyo),e(X2,kyo),e(N,Ryo),e(N,z2),e(z2,Vre),e(Vre,Syo),e(z2,Pyo),e(z2,z$),e(z$,$yo),e(z2,Iyo),e(N,jyo),e(N,V2),e(V2,Wre),e(Wre,Nyo),e(V2,Dyo),e(V2,V$),e(V$,qyo),e(V2,Gyo),e(N,Oyo),e(N,W2),e(W2,Qre),e(Qre,Xyo),e(W2,zyo),e(W2,W$),e(W$,Vyo),e(W2,Wyo),e(N,Qyo),e(N,Q2),e(Q2,Hre),e(Hre,Hyo),e(Q2,Uyo),e(Q2,Q$),e(Q$,Jyo),e(Q2,Yyo),e(N,Kyo),e(N,H2),e(H2,Ure),e(Ure,Zyo),e(H2,ewo),e(H2,Jre),e(Jre,owo),e(H2,rwo),e(N,two),e(N,U2),e(U2,Yre),e(Yre,awo),e(U2,swo),e(U2,H$),e(H$,nwo),e(U2,lwo),e(N,iwo),e(N,J2),e(J2,Kre),e(Kre,dwo),e(J2,cwo),e(J2,U$),e(U$,mwo),e(J2,fwo),e(N,gwo),e(N,Y2),e(Y2,Zre),e(Zre,hwo),e(Y2,uwo),e(Y2,J$),e(J$,pwo),e(Y2,_wo),e(N,bwo),e(N,K2),e(K2,ete),e(ete,vwo),e(K2,Two),e(K2,Y$),e(Y$,Fwo),e(K2,Cwo),e(N,Mwo),e(N,Z2),e(Z2,ote),e(ote,Ewo),e(Z2,ywo),e(Z2,K$),e(K$,wwo),e(Z2,Awo),e(N,Lwo),e(N,ev),e(ev,rte),e(rte,Bwo),e(ev,xwo),e(ev,Z$),e(Z$,kwo),e(ev,Rwo),e(N,Swo),e(N,ov),e(ov,tte),e(tte,Pwo),e(ov,$wo),e(ov,eI),e(eI,Iwo),e(ov,jwo),e(N,Nwo),e(N,rv),e(rv,ate),e(ate,Dwo),e(rv,qwo),e(rv,oI),e(oI,Gwo),e(rv,Owo),e(N,Xwo),e(N,tv),e(tv,ste),e(ste,zwo),e(tv,Vwo),e(tv,rI),e(rI,Wwo),e(tv,Qwo),e(Ne,Hwo),e(Ne,av),e(av,Uwo),e(av,nte),e(nte,Jwo),e(av,Ywo),e(av,lte),e(lte,Kwo),e(Ne,Zwo),e(Ne,ite),e(ite,eAo),e(Ne,oAo),g(fy,Ne,null),b(d,XLe,_),b(d,ad,_),e(ad,sv),e(sv,dte),g(gy,dte,null),e(ad,rAo),e(ad,cte),e(cte,tAo),b(d,zLe,_),b(d,Yo,_),g(hy,Yo,null),e(Yo,aAo),e(Yo,sd),e(sd,sAo),e(sd,mte),e(mte,nAo),e(sd,lAo),e(sd,fte),e(fte,iAo),e(sd,dAo),e(Yo,cAo),e(Yo,uy),e(uy,mAo),e(uy,gte),e(gte,fAo),e(uy,gAo),e(Yo,hAo),e(Yo,Xr),g(py,Xr,null),e(Xr,uAo),e(Xr,hte),e(hte,pAo),e(Xr,_Ao),e(Xr,nd),e(nd,bAo),e(nd,ute),e(ute,vAo),e(nd,TAo),e(nd,pte),e(pte,FAo),e(nd,CAo),e(Xr,MAo),e(Xr,_te),e(_te,EAo),e(Xr,yAo),g(_y,Xr,null),e(Yo,wAo),e(Yo,De),g(by,De,null),e(De,AAo),e(De,bte),e(bte,LAo),e(De,BAo),e(De,za),e(za,xAo),e(za,vte),e(vte,kAo),e(za,RAo),e(za,Tte),e(Tte,SAo),e(za,PAo),e(za,Fte),e(Fte,$Ao),e(za,IAo),e(De,jAo),e(De,R),e(R,nv),e(nv,Cte),e(Cte,NAo),e(nv,DAo),e(nv,tI),e(tI,qAo),e(nv,GAo),e(R,OAo),e(R,lv),e(lv,Mte),e(Mte,XAo),e(lv,zAo),e(lv,aI),e(aI,VAo),e(lv,WAo),e(R,QAo),e(R,iv),e(iv,Ete),e(Ete,HAo),e(iv,UAo),e(iv,sI),e(sI,JAo),e(iv,YAo),e(R,KAo),e(R,dv),e(dv,yte),e(yte,ZAo),e(dv,e0o),e(dv,nI),e(nI,o0o),e(dv,r0o),e(R,t0o),e(R,cv),e(cv,wte),e(wte,a0o),e(cv,s0o),e(cv,lI),e(lI,n0o),e(cv,l0o),e(R,i0o),e(R,mv),e(mv,Ate),e(Ate,d0o),e(mv,c0o),e(mv,iI),e(iI,m0o),e(mv,f0o),e(R,g0o),e(R,fv),e(fv,Lte),e(Lte,h0o),e(fv,u0o),e(fv,dI),e(dI,p0o),e(fv,_0o),e(R,b0o),e(R,gv),e(gv,Bte),e(Bte,v0o),e(gv,T0o),e(gv,cI),e(cI,F0o),e(gv,C0o),e(R,M0o),e(R,hv),e(hv,xte),e(xte,E0o),e(hv,y0o),e(hv,mI),e(mI,w0o),e(hv,A0o),e(R,L0o),e(R,uv),e(uv,kte),e(kte,B0o),e(uv,x0o),e(uv,fI),e(fI,k0o),e(uv,R0o),e(R,S0o),e(R,pv),e(pv,Rte),e(Rte,P0o),e(pv,$0o),e(pv,gI),e(gI,I0o),e(pv,j0o),e(R,N0o),e(R,_v),e(_v,Ste),e(Ste,D0o),e(_v,q0o),e(_v,hI),e(hI,G0o),e(_v,O0o),e(R,X0o),e(R,bv),e(bv,Pte),e(Pte,z0o),e(bv,V0o),e(bv,uI),e(uI,W0o),e(bv,Q0o),e(R,H0o),e(R,vv),e(vv,$te),e($te,U0o),e(vv,J0o),e(vv,pI),e(pI,Y0o),e(vv,K0o),e(R,Z0o),e(R,Tv),e(Tv,Ite),e(Ite,e6o),e(Tv,o6o),e(Tv,_I),e(_I,r6o),e(Tv,t6o),e(R,a6o),e(R,Fv),e(Fv,jte),e(jte,s6o),e(Fv,n6o),e(Fv,bI),e(bI,l6o),e(Fv,i6o),e(R,d6o),e(R,Cv),e(Cv,Nte),e(Nte,c6o),e(Cv,m6o),e(Cv,vI),e(vI,f6o),e(Cv,g6o),e(R,h6o),e(R,Mv),e(Mv,Dte),e(Dte,u6o),e(Mv,p6o),e(Mv,TI),e(TI,_6o),e(Mv,b6o),e(R,v6o),e(R,Ev),e(Ev,qte),e(qte,T6o),e(Ev,F6o),e(Ev,FI),e(FI,C6o),e(Ev,M6o),e(R,E6o),e(R,yv),e(yv,Gte),e(Gte,y6o),e(yv,w6o),e(yv,CI),e(CI,A6o),e(yv,L6o),e(R,B6o),e(R,wv),e(wv,Ote),e(Ote,x6o),e(wv,k6o),e(wv,MI),e(MI,R6o),e(wv,S6o),e(R,P6o),e(R,Av),e(Av,Xte),e(Xte,$6o),e(Av,I6o),e(Av,EI),e(EI,j6o),e(Av,N6o),e(R,D6o),e(R,Lv),e(Lv,zte),e(zte,q6o),e(Lv,G6o),e(Lv,yI),e(yI,O6o),e(Lv,X6o),e(R,z6o),e(R,Bv),e(Bv,Vte),e(Vte,V6o),e(Bv,W6o),e(Bv,wI),e(wI,Q6o),e(Bv,H6o),e(R,U6o),e(R,xv),e(xv,Wte),e(Wte,J6o),e(xv,Y6o),e(xv,AI),e(AI,K6o),e(xv,Z6o),e(R,eLo),e(R,kv),e(kv,Qte),e(Qte,oLo),e(kv,rLo),e(kv,LI),e(LI,tLo),e(kv,aLo),e(R,sLo),e(R,Rv),e(Rv,Hte),e(Hte,nLo),e(Rv,lLo),e(Rv,Ute),e(Ute,iLo),e(Rv,dLo),e(R,cLo),e(R,Sv),e(Sv,Jte),e(Jte,mLo),e(Sv,fLo),e(Sv,BI),e(BI,gLo),e(Sv,hLo),e(R,uLo),e(R,Pv),e(Pv,Yte),e(Yte,pLo),e(Pv,_Lo),e(Pv,xI),e(xI,bLo),e(Pv,vLo),e(R,TLo),e(R,$v),e($v,Kte),e(Kte,FLo),e($v,CLo),e($v,kI),e(kI,MLo),e($v,ELo),e(R,yLo),e(R,Iv),e(Iv,Zte),e(Zte,wLo),e(Iv,ALo),e(Iv,RI),e(RI,LLo),e(Iv,BLo),e(R,xLo),e(R,jv),e(jv,eae),e(eae,kLo),e(jv,RLo),e(jv,SI),e(SI,SLo),e(jv,PLo),e(R,$Lo),e(R,Nv),e(Nv,oae),e(oae,ILo),e(Nv,jLo),e(Nv,PI),e(PI,NLo),e(Nv,DLo),e(R,qLo),e(R,Dv),e(Dv,rae),e(rae,GLo),e(Dv,OLo),e(Dv,$I),e($I,XLo),e(Dv,zLo),e(R,VLo),e(R,qv),e(qv,tae),e(tae,WLo),e(qv,QLo),e(qv,II),e(II,HLo),e(qv,ULo),e(R,JLo),e(R,Gv),e(Gv,aae),e(aae,YLo),e(Gv,KLo),e(Gv,jI),e(jI,ZLo),e(Gv,e7o),e(R,o7o),e(R,Ov),e(Ov,sae),e(sae,r7o),e(Ov,t7o),e(Ov,NI),e(NI,a7o),e(Ov,s7o),e(R,n7o),e(R,Xv),e(Xv,nae),e(nae,l7o),e(Xv,i7o),e(Xv,DI),e(DI,d7o),e(Xv,c7o),e(De,m7o),e(De,zv),e(zv,f7o),e(zv,lae),e(lae,g7o),e(zv,h7o),e(zv,iae),e(iae,u7o),e(De,p7o),e(De,dae),e(dae,_7o),e(De,b7o),g(vy,De,null),b(d,VLe,_),b(d,ld,_),e(ld,Vv),e(Vv,cae),g(Ty,cae,null),e(ld,v7o),e(ld,mae),e(mae,T7o),b(d,WLe,_),b(d,Ko,_),g(Fy,Ko,null),e(Ko,F7o),e(Ko,id),e(id,C7o),e(id,fae),e(fae,M7o),e(id,E7o),e(id,gae),e(gae,y7o),e(id,w7o),e(Ko,A7o),e(Ko,Cy),e(Cy,L7o),e(Cy,hae),e(hae,B7o),e(Cy,x7o),e(Ko,k7o),e(Ko,zr),g(My,zr,null),e(zr,R7o),e(zr,uae),e(uae,S7o),e(zr,P7o),e(zr,dd),e(dd,$7o),e(dd,pae),e(pae,I7o),e(dd,j7o),e(dd,_ae),e(_ae,N7o),e(dd,D7o),e(zr,q7o),e(zr,bae),e(bae,G7o),e(zr,O7o),g(Ey,zr,null),e(Ko,X7o),e(Ko,qe),g(yy,qe,null),e(qe,z7o),e(qe,vae),e(vae,V7o),e(qe,W7o),e(qe,Va),e(Va,Q7o),e(Va,Tae),e(Tae,H7o),e(Va,U7o),e(Va,Fae),e(Fae,J7o),e(Va,Y7o),e(Va,Cae),e(Cae,K7o),e(Va,Z7o),e(qe,e8o),e(qe,Mae),e(Mae,Wv),e(Wv,Eae),e(Eae,o8o),e(Wv,r8o),e(Wv,qI),e(qI,t8o),e(Wv,a8o),e(qe,s8o),e(qe,Qv),e(Qv,n8o),e(Qv,yae),e(yae,l8o),e(Qv,i8o),e(Qv,wae),e(wae,d8o),e(qe,c8o),e(qe,Aae),e(Aae,m8o),e(qe,f8o),g(wy,qe,null),b(d,QLe,_),b(d,cd,_),e(cd,Hv),e(Hv,Lae),g(Ay,Lae,null),e(cd,g8o),e(cd,Bae),e(Bae,h8o),b(d,HLe,_),b(d,Zo,_),g(Ly,Zo,null),e(Zo,u8o),e(Zo,md),e(md,p8o),e(md,xae),e(xae,_8o),e(md,b8o),e(md,kae),e(kae,v8o),e(md,T8o),e(Zo,F8o),e(Zo,By),e(By,C8o),e(By,Rae),e(Rae,M8o),e(By,E8o),e(Zo,y8o),e(Zo,Vr),g(xy,Vr,null),e(Vr,w8o),e(Vr,Sae),e(Sae,A8o),e(Vr,L8o),e(Vr,fd),e(fd,B8o),e(fd,Pae),e(Pae,x8o),e(fd,k8o),e(fd,$ae),e($ae,R8o),e(fd,S8o),e(Vr,P8o),e(Vr,Iae),e(Iae,$8o),e(Vr,I8o),g(ky,Vr,null),e(Zo,j8o),e(Zo,Ge),g(Ry,Ge,null),e(Ge,N8o),e(Ge,jae),e(jae,D8o),e(Ge,q8o),e(Ge,Wa),e(Wa,G8o),e(Wa,Nae),e(Nae,O8o),e(Wa,X8o),e(Wa,Dae),e(Dae,z8o),e(Wa,V8o),e(Wa,qae),e(qae,W8o),e(Wa,Q8o),e(Ge,H8o),e(Ge,we),e(we,Uv),e(Uv,Gae),e(Gae,U8o),e(Uv,J8o),e(Uv,GI),e(GI,Y8o),e(Uv,K8o),e(we,Z8o),e(we,Jv),e(Jv,Oae),e(Oae,e9o),e(Jv,o9o),e(Jv,OI),e(OI,r9o),e(Jv,t9o),e(we,a9o),e(we,An),e(An,Xae),e(Xae,s9o),e(An,n9o),e(An,XI),e(XI,l9o),e(An,i9o),e(An,zI),e(zI,d9o),e(An,c9o),e(we,m9o),e(we,Yv),e(Yv,zae),e(zae,f9o),e(Yv,g9o),e(Yv,VI),e(VI,h9o),e(Yv,u9o),e(we,p9o),e(we,ta),e(ta,Vae),e(Vae,_9o),e(ta,b9o),e(ta,WI),e(WI,v9o),e(ta,T9o),e(ta,QI),e(QI,F9o),e(ta,C9o),e(ta,HI),e(HI,M9o),e(ta,E9o),e(we,y9o),e(we,Kv),e(Kv,Wae),e(Wae,w9o),e(Kv,A9o),e(Kv,UI),e(UI,L9o),e(Kv,B9o),e(we,x9o),e(we,Zv),e(Zv,Qae),e(Qae,k9o),e(Zv,R9o),e(Zv,JI),e(JI,S9o),e(Zv,P9o),e(we,$9o),e(we,eT),e(eT,Hae),e(Hae,I9o),e(eT,j9o),e(eT,YI),e(YI,N9o),e(eT,D9o),e(Ge,q9o),e(Ge,oT),e(oT,G9o),e(oT,Uae),e(Uae,O9o),e(oT,X9o),e(oT,Jae),e(Jae,z9o),e(Ge,V9o),e(Ge,Yae),e(Yae,W9o),e(Ge,Q9o),g(Sy,Ge,null),b(d,ULe,_),b(d,gd,_),e(gd,rT),e(rT,Kae),g(Py,Kae,null),e(gd,H9o),e(gd,Zae),e(Zae,U9o),b(d,JLe,_),b(d,er,_),g($y,er,null),e(er,J9o),e(er,hd),e(hd,Y9o),e(hd,ese),e(ese,K9o),e(hd,Z9o),e(hd,ose),e(ose,eBo),e(hd,oBo),e(er,rBo),e(er,Iy),e(Iy,tBo),e(Iy,rse),e(rse,aBo),e(Iy,sBo),e(er,nBo),e(er,Wr),g(jy,Wr,null),e(Wr,lBo),e(Wr,tse),e(tse,iBo),e(Wr,dBo),e(Wr,ud),e(ud,cBo),e(ud,ase),e(ase,mBo),e(ud,fBo),e(ud,sse),e(sse,gBo),e(ud,hBo),e(Wr,uBo),e(Wr,nse),e(nse,pBo),e(Wr,_Bo),g(Ny,Wr,null),e(er,bBo),e(er,Oe),g(Dy,Oe,null),e(Oe,vBo),e(Oe,lse),e(lse,TBo),e(Oe,FBo),e(Oe,Qa),e(Qa,CBo),e(Qa,ise),e(ise,MBo),e(Qa,EBo),e(Qa,dse),e(dse,yBo),e(Qa,wBo),e(Qa,cse),e(cse,ABo),e(Qa,LBo),e(Oe,BBo),e(Oe,mse),e(mse,tT),e(tT,fse),e(fse,xBo),e(tT,kBo),e(tT,KI),e(KI,RBo),e(tT,SBo),e(Oe,PBo),e(Oe,aT),e(aT,$Bo),e(aT,gse),e(gse,IBo),e(aT,jBo),e(aT,hse),e(hse,NBo),e(Oe,DBo),e(Oe,use),e(use,qBo),e(Oe,GBo),g(qy,Oe,null),b(d,YLe,_),b(d,pd,_),e(pd,sT),e(sT,pse),g(Gy,pse,null),e(pd,OBo),e(pd,_se),e(_se,XBo),b(d,KLe,_),b(d,or,_),g(Oy,or,null),e(or,zBo),e(or,_d),e(_d,VBo),e(_d,bse),e(bse,WBo),e(_d,QBo),e(_d,vse),e(vse,HBo),e(_d,UBo),e(or,JBo),e(or,Xy),e(Xy,YBo),e(Xy,Tse),e(Tse,KBo),e(Xy,ZBo),e(or,exo),e(or,Qr),g(zy,Qr,null),e(Qr,oxo),e(Qr,Fse),e(Fse,rxo),e(Qr,txo),e(Qr,bd),e(bd,axo),e(bd,Cse),e(Cse,sxo),e(bd,nxo),e(bd,Mse),e(Mse,lxo),e(bd,ixo),e(Qr,dxo),e(Qr,Ese),e(Ese,cxo),e(Qr,mxo),g(Vy,Qr,null),e(or,fxo),e(or,Xe),g(Wy,Xe,null),e(Xe,gxo),e(Xe,yse),e(yse,hxo),e(Xe,uxo),e(Xe,Ha),e(Ha,pxo),e(Ha,wse),e(wse,_xo),e(Ha,bxo),e(Ha,Ase),e(Ase,vxo),e(Ha,Txo),e(Ha,Lse),e(Lse,Fxo),e(Ha,Cxo),e(Xe,Mxo),e(Xe,ro),e(ro,nT),e(nT,Bse),e(Bse,Exo),e(nT,yxo),e(nT,ZI),e(ZI,wxo),e(nT,Axo),e(ro,Lxo),e(ro,lT),e(lT,xse),e(xse,Bxo),e(lT,xxo),e(lT,ej),e(ej,kxo),e(lT,Rxo),e(ro,Sxo),e(ro,iT),e(iT,kse),e(kse,Pxo),e(iT,$xo),e(iT,oj),e(oj,Ixo),e(iT,jxo),e(ro,Nxo),e(ro,dT),e(dT,Rse),e(Rse,Dxo),e(dT,qxo),e(dT,rj),e(rj,Gxo),e(dT,Oxo),e(ro,Xxo),e(ro,cT),e(cT,Sse),e(Sse,zxo),e(cT,Vxo),e(cT,tj),e(tj,Wxo),e(cT,Qxo),e(ro,Hxo),e(ro,mT),e(mT,Pse),e(Pse,Uxo),e(mT,Jxo),e(mT,aj),e(aj,Yxo),e(mT,Kxo),e(ro,Zxo),e(ro,fT),e(fT,$se),e($se,eko),e(fT,oko),e(fT,sj),e(sj,rko),e(fT,tko),e(Xe,ako),e(Xe,gT),e(gT,sko),e(gT,Ise),e(Ise,nko),e(gT,lko),e(gT,jse),e(jse,iko),e(Xe,dko),e(Xe,Nse),e(Nse,cko),e(Xe,mko),g(Qy,Xe,null),b(d,ZLe,_),b(d,vd,_),e(vd,hT),e(hT,Dse),g(Hy,Dse,null),e(vd,fko),e(vd,qse),e(qse,gko),b(d,e7e,_),b(d,rr,_),g(Uy,rr,null),e(rr,hko),e(rr,Td),e(Td,uko),e(Td,Gse),e(Gse,pko),e(Td,_ko),e(Td,Ose),e(Ose,bko),e(Td,vko),e(rr,Tko),e(rr,Jy),e(Jy,Fko),e(Jy,Xse),e(Xse,Cko),e(Jy,Mko),e(rr,Eko),e(rr,Hr),g(Yy,Hr,null),e(Hr,yko),e(Hr,zse),e(zse,wko),e(Hr,Ako),e(Hr,Fd),e(Fd,Lko),e(Fd,Vse),e(Vse,Bko),e(Fd,xko),e(Fd,Wse),e(Wse,kko),e(Fd,Rko),e(Hr,Sko),e(Hr,Qse),e(Qse,Pko),e(Hr,$ko),g(Ky,Hr,null),e(rr,Iko),e(rr,ze),g(Zy,ze,null),e(ze,jko),e(ze,Hse),e(Hse,Nko),e(ze,Dko),e(ze,Ua),e(Ua,qko),e(Ua,Use),e(Use,Gko),e(Ua,Oko),e(Ua,Jse),e(Jse,Xko),e(Ua,zko),e(Ua,Yse),e(Yse,Vko),e(Ua,Wko),e(ze,Qko),e(ze,Cd),e(Cd,uT),e(uT,Kse),e(Kse,Hko),e(uT,Uko),e(uT,nj),e(nj,Jko),e(uT,Yko),e(Cd,Kko),e(Cd,pT),e(pT,Zse),e(Zse,Zko),e(pT,eRo),e(pT,lj),e(lj,oRo),e(pT,rRo),e(Cd,tRo),e(Cd,_T),e(_T,ene),e(ene,aRo),e(_T,sRo),e(_T,ij),e(ij,nRo),e(_T,lRo),e(ze,iRo),e(ze,bT),e(bT,dRo),e(bT,one),e(one,cRo),e(bT,mRo),e(bT,rne),e(rne,fRo),e(ze,gRo),e(ze,tne),e(tne,hRo),e(ze,uRo),g(ew,ze,null),b(d,o7e,_),b(d,Md,_),e(Md,vT),e(vT,ane),g(ow,ane,null),e(Md,pRo),e(Md,sne),e(sne,_Ro),b(d,r7e,_),b(d,tr,_),g(rw,tr,null),e(tr,bRo),e(tr,Ed),e(Ed,vRo),e(Ed,nne),e(nne,TRo),e(Ed,FRo),e(Ed,lne),e(lne,CRo),e(Ed,MRo),e(tr,ERo),e(tr,tw),e(tw,yRo),e(tw,ine),e(ine,wRo),e(tw,ARo),e(tr,LRo),e(tr,Ur),g(aw,Ur,null),e(Ur,BRo),e(Ur,dne),e(dne,xRo),e(Ur,kRo),e(Ur,yd),e(yd,RRo),e(yd,cne),e(cne,SRo),e(yd,PRo),e(yd,mne),e(mne,$Ro),e(yd,IRo),e(Ur,jRo),e(Ur,fne),e(fne,NRo),e(Ur,DRo),g(sw,Ur,null),e(tr,qRo),e(tr,Ve),g(nw,Ve,null),e(Ve,GRo),e(Ve,gne),e(gne,ORo),e(Ve,XRo),e(Ve,Ja),e(Ja,zRo),e(Ja,hne),e(hne,VRo),e(Ja,WRo),e(Ja,une),e(une,QRo),e(Ja,HRo),e(Ja,pne),e(pne,URo),e(Ja,JRo),e(Ve,YRo),e(Ve,to),e(to,TT),e(TT,_ne),e(_ne,KRo),e(TT,ZRo),e(TT,dj),e(dj,eSo),e(TT,oSo),e(to,rSo),e(to,FT),e(FT,bne),e(bne,tSo),e(FT,aSo),e(FT,cj),e(cj,sSo),e(FT,nSo),e(to,lSo),e(to,CT),e(CT,vne),e(vne,iSo),e(CT,dSo),e(CT,mj),e(mj,cSo),e(CT,mSo),e(to,fSo),e(to,MT),e(MT,Tne),e(Tne,gSo),e(MT,hSo),e(MT,fj),e(fj,uSo),e(MT,pSo),e(to,_So),e(to,ET),e(ET,Fne),e(Fne,bSo),e(ET,vSo),e(ET,gj),e(gj,TSo),e(ET,FSo),e(to,CSo),e(to,yT),e(yT,Cne),e(Cne,MSo),e(yT,ESo),e(yT,hj),e(hj,ySo),e(yT,wSo),e(to,ASo),e(to,wT),e(wT,Mne),e(Mne,LSo),e(wT,BSo),e(wT,uj),e(uj,xSo),e(wT,kSo),e(Ve,RSo),e(Ve,AT),e(AT,SSo),e(AT,Ene),e(Ene,PSo),e(AT,$So),e(AT,yne),e(yne,ISo),e(Ve,jSo),e(Ve,wne),e(wne,NSo),e(Ve,DSo),g(lw,Ve,null),b(d,t7e,_),b(d,wd,_),e(wd,LT),e(LT,Ane),g(iw,Ane,null),e(wd,qSo),e(wd,Lne),e(Lne,GSo),b(d,a7e,_),b(d,ar,_),g(dw,ar,null),e(ar,OSo),e(ar,Ad),e(Ad,XSo),e(Ad,Bne),e(Bne,zSo),e(Ad,VSo),e(Ad,xne),e(xne,WSo),e(Ad,QSo),e(ar,HSo),e(ar,cw),e(cw,USo),e(cw,kne),e(kne,JSo),e(cw,YSo),e(ar,KSo),e(ar,Jr),g(mw,Jr,null),e(Jr,ZSo),e(Jr,Rne),e(Rne,ePo),e(Jr,oPo),e(Jr,Ld),e(Ld,rPo),e(Ld,Sne),e(Sne,tPo),e(Ld,aPo),e(Ld,Pne),e(Pne,sPo),e(Ld,nPo),e(Jr,lPo),e(Jr,$ne),e($ne,iPo),e(Jr,dPo),g(fw,Jr,null),e(ar,cPo),e(ar,We),g(gw,We,null),e(We,mPo),e(We,Ine),e(Ine,fPo),e(We,gPo),e(We,Ya),e(Ya,hPo),e(Ya,jne),e(jne,uPo),e(Ya,pPo),e(Ya,Nne),e(Nne,_Po),e(Ya,bPo),e(Ya,Dne),e(Dne,vPo),e(Ya,TPo),e(We,FPo),e(We,hw),e(hw,BT),e(BT,qne),e(qne,CPo),e(BT,MPo),e(BT,pj),e(pj,EPo),e(BT,yPo),e(hw,wPo),e(hw,xT),e(xT,Gne),e(Gne,APo),e(xT,LPo),e(xT,_j),e(_j,BPo),e(xT,xPo),e(We,kPo),e(We,kT),e(kT,RPo),e(kT,One),e(One,SPo),e(kT,PPo),e(kT,Xne),e(Xne,$Po),e(We,IPo),e(We,zne),e(zne,jPo),e(We,NPo),g(uw,We,null),b(d,s7e,_),b(d,Bd,_),e(Bd,RT),e(RT,Vne),g(pw,Vne,null),e(Bd,DPo),e(Bd,Wne),e(Wne,qPo),b(d,n7e,_),b(d,sr,_),g(_w,sr,null),e(sr,GPo),e(sr,xd),e(xd,OPo),e(xd,Qne),e(Qne,XPo),e(xd,zPo),e(xd,Hne),e(Hne,VPo),e(xd,WPo),e(sr,QPo),e(sr,bw),e(bw,HPo),e(bw,Une),e(Une,UPo),e(bw,JPo),e(sr,YPo),e(sr,Yr),g(vw,Yr,null),e(Yr,KPo),e(Yr,Jne),e(Jne,ZPo),e(Yr,e$o),e(Yr,kd),e(kd,o$o),e(kd,Yne),e(Yne,r$o),e(kd,t$o),e(kd,Kne),e(Kne,a$o),e(kd,s$o),e(Yr,n$o),e(Yr,Zne),e(Zne,l$o),e(Yr,i$o),g(Tw,Yr,null),e(sr,d$o),e(sr,Qe),g(Fw,Qe,null),e(Qe,c$o),e(Qe,ele),e(ele,m$o),e(Qe,f$o),e(Qe,Ka),e(Ka,g$o),e(Ka,ole),e(ole,h$o),e(Ka,u$o),e(Ka,rle),e(rle,p$o),e(Ka,_$o),e(Ka,tle),e(tle,b$o),e(Ka,v$o),e(Qe,T$o),e(Qe,Rd),e(Rd,ST),e(ST,ale),e(ale,F$o),e(ST,C$o),e(ST,bj),e(bj,M$o),e(ST,E$o),e(Rd,y$o),e(Rd,PT),e(PT,sle),e(sle,w$o),e(PT,A$o),e(PT,vj),e(vj,L$o),e(PT,B$o),e(Rd,x$o),e(Rd,$T),e($T,nle),e(nle,k$o),e($T,R$o),e($T,Tj),e(Tj,S$o),e($T,P$o),e(Qe,$$o),e(Qe,IT),e(IT,I$o),e(IT,lle),e(lle,j$o),e(IT,N$o),e(IT,ile),e(ile,D$o),e(Qe,q$o),e(Qe,dle),e(dle,G$o),e(Qe,O$o),g(Cw,Qe,null),b(d,l7e,_),b(d,Sd,_),e(Sd,jT),e(jT,cle),g(Mw,cle,null),e(Sd,X$o),e(Sd,mle),e(mle,z$o),b(d,i7e,_),b(d,nr,_),g(Ew,nr,null),e(nr,V$o),e(nr,Pd),e(Pd,W$o),e(Pd,fle),e(fle,Q$o),e(Pd,H$o),e(Pd,gle),e(gle,U$o),e(Pd,J$o),e(nr,Y$o),e(nr,yw),e(yw,K$o),e(yw,hle),e(hle,Z$o),e(yw,eIo),e(nr,oIo),e(nr,Kr),g(ww,Kr,null),e(Kr,rIo),e(Kr,ule),e(ule,tIo),e(Kr,aIo),e(Kr,$d),e($d,sIo),e($d,ple),e(ple,nIo),e($d,lIo),e($d,_le),e(_le,iIo),e($d,dIo),e(Kr,cIo),e(Kr,ble),e(ble,mIo),e(Kr,fIo),g(Aw,Kr,null),e(nr,gIo),e(nr,He),g(Lw,He,null),e(He,hIo),e(He,vle),e(vle,uIo),e(He,pIo),e(He,Za),e(Za,_Io),e(Za,Tle),e(Tle,bIo),e(Za,vIo),e(Za,Fle),e(Fle,TIo),e(Za,FIo),e(Za,Cle),e(Cle,CIo),e(Za,MIo),e(He,EIo),e(He,Mle),e(Mle,NT),e(NT,Ele),e(Ele,yIo),e(NT,wIo),e(NT,Fj),e(Fj,AIo),e(NT,LIo),e(He,BIo),e(He,DT),e(DT,xIo),e(DT,yle),e(yle,kIo),e(DT,RIo),e(DT,wle),e(wle,SIo),e(He,PIo),e(He,Ale),e(Ale,$Io),e(He,IIo),g(Bw,He,null),b(d,d7e,_),b(d,Id,_),e(Id,qT),e(qT,Lle),g(xw,Lle,null),e(Id,jIo),e(Id,Ble),e(Ble,NIo),b(d,c7e,_),b(d,lr,_),g(kw,lr,null),e(lr,DIo),e(lr,jd),e(jd,qIo),e(jd,xle),e(xle,GIo),e(jd,OIo),e(jd,kle),e(kle,XIo),e(jd,zIo),e(lr,VIo),e(lr,Rw),e(Rw,WIo),e(Rw,Rle),e(Rle,QIo),e(Rw,HIo),e(lr,UIo),e(lr,Zr),g(Sw,Zr,null),e(Zr,JIo),e(Zr,Sle),e(Sle,YIo),e(Zr,KIo),e(Zr,Nd),e(Nd,ZIo),e(Nd,Ple),e(Ple,ejo),e(Nd,ojo),e(Nd,$le),e($le,rjo),e(Nd,tjo),e(Zr,ajo),e(Zr,Ile),e(Ile,sjo),e(Zr,njo),g(Pw,Zr,null),e(lr,ljo),e(lr,Ue),g($w,Ue,null),e(Ue,ijo),e(Ue,jle),e(jle,djo),e(Ue,cjo),e(Ue,es),e(es,mjo),e(es,Nle),e(Nle,fjo),e(es,gjo),e(es,Dle),e(Dle,hjo),e(es,ujo),e(es,qle),e(qle,pjo),e(es,_jo),e(Ue,bjo),e(Ue,Gle),e(Gle,GT),e(GT,Ole),e(Ole,vjo),e(GT,Tjo),e(GT,Cj),e(Cj,Fjo),e(GT,Cjo),e(Ue,Mjo),e(Ue,OT),e(OT,Ejo),e(OT,Xle),e(Xle,yjo),e(OT,wjo),e(OT,zle),e(zle,Ajo),e(Ue,Ljo),e(Ue,Vle),e(Vle,Bjo),e(Ue,xjo),g(Iw,Ue,null),b(d,m7e,_),b(d,Dd,_),e(Dd,XT),e(XT,Wle),g(jw,Wle,null),e(Dd,kjo),e(Dd,Qle),e(Qle,Rjo),b(d,f7e,_),b(d,ir,_),g(Nw,ir,null),e(ir,Sjo),e(ir,qd),e(qd,Pjo),e(qd,Hle),e(Hle,$jo),e(qd,Ijo),e(qd,Ule),e(Ule,jjo),e(qd,Njo),e(ir,Djo),e(ir,Dw),e(Dw,qjo),e(Dw,Jle),e(Jle,Gjo),e(Dw,Ojo),e(ir,Xjo),e(ir,et),g(qw,et,null),e(et,zjo),e(et,Yle),e(Yle,Vjo),e(et,Wjo),e(et,Gd),e(Gd,Qjo),e(Gd,Kle),e(Kle,Hjo),e(Gd,Ujo),e(Gd,Zle),e(Zle,Jjo),e(Gd,Yjo),e(et,Kjo),e(et,eie),e(eie,Zjo),e(et,eNo),g(Gw,et,null),e(ir,oNo),e(ir,Je),g(Ow,Je,null),e(Je,rNo),e(Je,oie),e(oie,tNo),e(Je,aNo),e(Je,os),e(os,sNo),e(os,rie),e(rie,nNo),e(os,lNo),e(os,tie),e(tie,iNo),e(os,dNo),e(os,aie),e(aie,cNo),e(os,mNo),e(Je,fNo),e(Je,Xw),e(Xw,zT),e(zT,sie),e(sie,gNo),e(zT,hNo),e(zT,Mj),e(Mj,uNo),e(zT,pNo),e(Xw,_No),e(Xw,VT),e(VT,nie),e(nie,bNo),e(VT,vNo),e(VT,Ej),e(Ej,TNo),e(VT,FNo),e(Je,CNo),e(Je,WT),e(WT,MNo),e(WT,lie),e(lie,ENo),e(WT,yNo),e(WT,iie),e(iie,wNo),e(Je,ANo),e(Je,die),e(die,LNo),e(Je,BNo),g(zw,Je,null),b(d,g7e,_),b(d,Od,_),e(Od,QT),e(QT,cie),g(Vw,cie,null),e(Od,xNo),e(Od,mie),e(mie,kNo),b(d,h7e,_),b(d,dr,_),g(Ww,dr,null),e(dr,RNo),e(dr,Xd),e(Xd,SNo),e(Xd,fie),e(fie,PNo),e(Xd,$No),e(Xd,gie),e(gie,INo),e(Xd,jNo),e(dr,NNo),e(dr,Qw),e(Qw,DNo),e(Qw,hie),e(hie,qNo),e(Qw,GNo),e(dr,ONo),e(dr,ot),g(Hw,ot,null),e(ot,XNo),e(ot,uie),e(uie,zNo),e(ot,VNo),e(ot,zd),e(zd,WNo),e(zd,pie),e(pie,QNo),e(zd,HNo),e(zd,_ie),e(_ie,UNo),e(zd,JNo),e(ot,YNo),e(ot,bie),e(bie,KNo),e(ot,ZNo),g(Uw,ot,null),e(dr,eDo),e(dr,mo),g(Jw,mo,null),e(mo,oDo),e(mo,vie),e(vie,rDo),e(mo,tDo),e(mo,rs),e(rs,aDo),e(rs,Tie),e(Tie,sDo),e(rs,nDo),e(rs,Fie),e(Fie,lDo),e(rs,iDo),e(rs,Cie),e(Cie,dDo),e(rs,cDo),e(mo,mDo),e(mo,B),e(B,HT),e(HT,Mie),e(Mie,fDo),e(HT,gDo),e(HT,yj),e(yj,hDo),e(HT,uDo),e(B,pDo),e(B,UT),e(UT,Eie),e(Eie,_Do),e(UT,bDo),e(UT,wj),e(wj,vDo),e(UT,TDo),e(B,FDo),e(B,JT),e(JT,yie),e(yie,CDo),e(JT,MDo),e(JT,Aj),e(Aj,EDo),e(JT,yDo),e(B,wDo),e(B,YT),e(YT,wie),e(wie,ADo),e(YT,LDo),e(YT,Lj),e(Lj,BDo),e(YT,xDo),e(B,kDo),e(B,KT),e(KT,Aie),e(Aie,RDo),e(KT,SDo),e(KT,Bj),e(Bj,PDo),e(KT,$Do),e(B,IDo),e(B,ZT),e(ZT,Lie),e(Lie,jDo),e(ZT,NDo),e(ZT,xj),e(xj,DDo),e(ZT,qDo),e(B,GDo),e(B,e1),e(e1,Bie),e(Bie,ODo),e(e1,XDo),e(e1,kj),e(kj,zDo),e(e1,VDo),e(B,WDo),e(B,o1),e(o1,xie),e(xie,QDo),e(o1,HDo),e(o1,Rj),e(Rj,UDo),e(o1,JDo),e(B,YDo),e(B,r1),e(r1,kie),e(kie,KDo),e(r1,ZDo),e(r1,Sj),e(Sj,eqo),e(r1,oqo),e(B,rqo),e(B,t1),e(t1,Rie),e(Rie,tqo),e(t1,aqo),e(t1,Pj),e(Pj,sqo),e(t1,nqo),e(B,lqo),e(B,a1),e(a1,Sie),e(Sie,iqo),e(a1,dqo),e(a1,$j),e($j,cqo),e(a1,mqo),e(B,fqo),e(B,s1),e(s1,Pie),e(Pie,gqo),e(s1,hqo),e(s1,Ij),e(Ij,uqo),e(s1,pqo),e(B,_qo),e(B,n1),e(n1,$ie),e($ie,bqo),e(n1,vqo),e(n1,jj),e(jj,Tqo),e(n1,Fqo),e(B,Cqo),e(B,l1),e(l1,Iie),e(Iie,Mqo),e(l1,Eqo),e(l1,Nj),e(Nj,yqo),e(l1,wqo),e(B,Aqo),e(B,i1),e(i1,jie),e(jie,Lqo),e(i1,Bqo),e(i1,Dj),e(Dj,xqo),e(i1,kqo),e(B,Rqo),e(B,Ln),e(Ln,Nie),e(Nie,Sqo),e(Ln,Pqo),e(Ln,qj),e(qj,$qo),e(Ln,Iqo),e(Ln,Gj),e(Gj,jqo),e(Ln,Nqo),e(B,Dqo),e(B,d1),e(d1,Die),e(Die,qqo),e(d1,Gqo),e(d1,Oj),e(Oj,Oqo),e(d1,Xqo),e(B,zqo),e(B,c1),e(c1,qie),e(qie,Vqo),e(c1,Wqo),e(c1,Xj),e(Xj,Qqo),e(c1,Hqo),e(B,Uqo),e(B,m1),e(m1,Gie),e(Gie,Jqo),e(m1,Yqo),e(m1,zj),e(zj,Kqo),e(m1,Zqo),e(B,eGo),e(B,f1),e(f1,Oie),e(Oie,oGo),e(f1,rGo),e(f1,Vj),e(Vj,tGo),e(f1,aGo),e(B,sGo),e(B,g1),e(g1,Xie),e(Xie,nGo),e(g1,lGo),e(g1,Wj),e(Wj,iGo),e(g1,dGo),e(B,cGo),e(B,h1),e(h1,zie),e(zie,mGo),e(h1,fGo),e(h1,Qj),e(Qj,gGo),e(h1,hGo),e(B,uGo),e(B,u1),e(u1,Vie),e(Vie,pGo),e(u1,_Go),e(u1,Hj),e(Hj,bGo),e(u1,vGo),e(B,TGo),e(B,p1),e(p1,Wie),e(Wie,FGo),e(p1,CGo),e(p1,Uj),e(Uj,MGo),e(p1,EGo),e(B,yGo),e(B,_1),e(_1,Qie),e(Qie,wGo),e(_1,AGo),e(_1,Jj),e(Jj,LGo),e(_1,BGo),e(B,xGo),e(B,b1),e(b1,Hie),e(Hie,kGo),e(b1,RGo),e(b1,Yj),e(Yj,SGo),e(b1,PGo),e(B,$Go),e(B,v1),e(v1,Uie),e(Uie,IGo),e(v1,jGo),e(v1,Kj),e(Kj,NGo),e(v1,DGo),e(B,qGo),e(B,T1),e(T1,Jie),e(Jie,GGo),e(T1,OGo),e(T1,Zj),e(Zj,XGo),e(T1,zGo),e(B,VGo),e(B,F1),e(F1,Yie),e(Yie,WGo),e(F1,QGo),e(F1,eN),e(eN,HGo),e(F1,UGo),e(B,JGo),e(B,C1),e(C1,Kie),e(Kie,YGo),e(C1,KGo),e(C1,oN),e(oN,ZGo),e(C1,eOo),e(B,oOo),e(B,M1),e(M1,Zie),e(Zie,rOo),e(M1,tOo),e(M1,rN),e(rN,aOo),e(M1,sOo),e(B,nOo),e(B,E1),e(E1,ede),e(ede,lOo),e(E1,iOo),e(E1,tN),e(tN,dOo),e(E1,cOo),e(B,mOo),e(B,y1),e(y1,ode),e(ode,fOo),e(y1,gOo),e(y1,aN),e(aN,hOo),e(y1,uOo),e(B,pOo),e(B,w1),e(w1,rde),e(rde,_Oo),e(w1,bOo),e(w1,sN),e(sN,vOo),e(w1,TOo),e(B,FOo),e(B,A1),e(A1,tde),e(tde,COo),e(A1,MOo),e(A1,nN),e(nN,EOo),e(A1,yOo),e(B,wOo),e(B,L1),e(L1,ade),e(ade,AOo),e(L1,LOo),e(L1,lN),e(lN,BOo),e(L1,xOo),e(B,kOo),e(B,B1),e(B1,sde),e(sde,ROo),e(B1,SOo),e(B1,iN),e(iN,POo),e(B1,$Oo),e(B,IOo),e(B,x1),e(x1,nde),e(nde,jOo),e(x1,NOo),e(x1,dN),e(dN,DOo),e(x1,qOo),e(B,GOo),e(B,k1),e(k1,lde),e(lde,OOo),e(k1,XOo),e(k1,cN),e(cN,zOo),e(k1,VOo),e(B,WOo),e(B,R1),e(R1,ide),e(ide,QOo),e(R1,HOo),e(R1,mN),e(mN,UOo),e(R1,JOo),e(B,YOo),e(B,S1),e(S1,dde),e(dde,KOo),e(S1,ZOo),e(S1,fN),e(fN,eXo),e(S1,oXo),e(mo,rXo),e(mo,cde),e(cde,tXo),e(mo,aXo),g(Yw,mo,null),b(d,u7e,_),b(d,Vd,_),e(Vd,P1),e(P1,mde),g(Kw,mde,null),e(Vd,sXo),e(Vd,fde),e(fde,nXo),b(d,p7e,_),b(d,cr,_),g(Zw,cr,null),e(cr,lXo),e(cr,Wd),e(Wd,iXo),e(Wd,gde),e(gde,dXo),e(Wd,cXo),e(Wd,hde),e(hde,mXo),e(Wd,fXo),e(cr,gXo),e(cr,eA),e(eA,hXo),e(eA,ude),e(ude,uXo),e(eA,pXo),e(cr,_Xo),e(cr,rt),g(oA,rt,null),e(rt,bXo),e(rt,pde),e(pde,vXo),e(rt,TXo),e(rt,Qd),e(Qd,FXo),e(Qd,_de),e(_de,CXo),e(Qd,MXo),e(Qd,bde),e(bde,EXo),e(Qd,yXo),e(rt,wXo),e(rt,vde),e(vde,AXo),e(rt,LXo),g(rA,rt,null),e(cr,BXo),e(cr,fo),g(tA,fo,null),e(fo,xXo),e(fo,Tde),e(Tde,kXo),e(fo,RXo),e(fo,ts),e(ts,SXo),e(ts,Fde),e(Fde,PXo),e(ts,$Xo),e(ts,Cde),e(Cde,IXo),e(ts,jXo),e(ts,Mde),e(Mde,NXo),e(ts,DXo),e(fo,qXo),e(fo,H),e(H,$1),e($1,Ede),e(Ede,GXo),e($1,OXo),e($1,gN),e(gN,XXo),e($1,zXo),e(H,VXo),e(H,I1),e(I1,yde),e(yde,WXo),e(I1,QXo),e(I1,hN),e(hN,HXo),e(I1,UXo),e(H,JXo),e(H,j1),e(j1,wde),e(wde,YXo),e(j1,KXo),e(j1,uN),e(uN,ZXo),e(j1,ezo),e(H,ozo),e(H,N1),e(N1,Ade),e(Ade,rzo),e(N1,tzo),e(N1,pN),e(pN,azo),e(N1,szo),e(H,nzo),e(H,D1),e(D1,Lde),e(Lde,lzo),e(D1,izo),e(D1,_N),e(_N,dzo),e(D1,czo),e(H,mzo),e(H,q1),e(q1,Bde),e(Bde,fzo),e(q1,gzo),e(q1,bN),e(bN,hzo),e(q1,uzo),e(H,pzo),e(H,G1),e(G1,xde),e(xde,_zo),e(G1,bzo),e(G1,vN),e(vN,vzo),e(G1,Tzo),e(H,Fzo),e(H,O1),e(O1,kde),e(kde,Czo),e(O1,Mzo),e(O1,TN),e(TN,Ezo),e(O1,yzo),e(H,wzo),e(H,X1),e(X1,Rde),e(Rde,Azo),e(X1,Lzo),e(X1,FN),e(FN,Bzo),e(X1,xzo),e(H,kzo),e(H,z1),e(z1,Sde),e(Sde,Rzo),e(z1,Szo),e(z1,CN),e(CN,Pzo),e(z1,$zo),e(H,Izo),e(H,V1),e(V1,Pde),e(Pde,jzo),e(V1,Nzo),e(V1,MN),e(MN,Dzo),e(V1,qzo),e(H,Gzo),e(H,W1),e(W1,$de),e($de,Ozo),e(W1,Xzo),e(W1,EN),e(EN,zzo),e(W1,Vzo),e(H,Wzo),e(H,Q1),e(Q1,Ide),e(Ide,Qzo),e(Q1,Hzo),e(Q1,yN),e(yN,Uzo),e(Q1,Jzo),e(H,Yzo),e(H,H1),e(H1,jde),e(jde,Kzo),e(H1,Zzo),e(H1,wN),e(wN,eVo),e(H1,oVo),e(H,rVo),e(H,U1),e(U1,Nde),e(Nde,tVo),e(U1,aVo),e(U1,AN),e(AN,sVo),e(U1,nVo),e(H,lVo),e(H,J1),e(J1,Dde),e(Dde,iVo),e(J1,dVo),e(J1,LN),e(LN,cVo),e(J1,mVo),e(H,fVo),e(H,Y1),e(Y1,qde),e(qde,gVo),e(Y1,hVo),e(Y1,BN),e(BN,uVo),e(Y1,pVo),e(H,_Vo),e(H,K1),e(K1,Gde),e(Gde,bVo),e(K1,vVo),e(K1,xN),e(xN,TVo),e(K1,FVo),e(H,CVo),e(H,Z1),e(Z1,Ode),e(Ode,MVo),e(Z1,EVo),e(Z1,kN),e(kN,yVo),e(Z1,wVo),e(H,AVo),e(H,eF),e(eF,Xde),e(Xde,LVo),e(eF,BVo),e(eF,RN),e(RN,xVo),e(eF,kVo),e(H,RVo),e(H,oF),e(oF,zde),e(zde,SVo),e(oF,PVo),e(oF,SN),e(SN,$Vo),e(oF,IVo),e(H,jVo),e(H,rF),e(rF,Vde),e(Vde,NVo),e(rF,DVo),e(rF,PN),e(PN,qVo),e(rF,GVo),e(fo,OVo),e(fo,Wde),e(Wde,XVo),e(fo,zVo),g(aA,fo,null),b(d,_7e,_),b(d,Hd,_),e(Hd,tF),e(tF,Qde),g(sA,Qde,null),e(Hd,VVo),e(Hd,Hde),e(Hde,WVo),b(d,b7e,_),b(d,mr,_),g(nA,mr,null),e(mr,QVo),e(mr,Ud),e(Ud,HVo),e(Ud,Ude),e(Ude,UVo),e(Ud,JVo),e(Ud,Jde),e(Jde,YVo),e(Ud,KVo),e(mr,ZVo),e(mr,lA),e(lA,eWo),e(lA,Yde),e(Yde,oWo),e(lA,rWo),e(mr,tWo),e(mr,tt),g(iA,tt,null),e(tt,aWo),e(tt,Kde),e(Kde,sWo),e(tt,nWo),e(tt,Jd),e(Jd,lWo),e(Jd,Zde),e(Zde,iWo),e(Jd,dWo),e(Jd,ece),e(ece,cWo),e(Jd,mWo),e(tt,fWo),e(tt,oce),e(oce,gWo),e(tt,hWo),g(dA,tt,null),e(mr,uWo),e(mr,go),g(cA,go,null),e(go,pWo),e(go,rce),e(rce,_Wo),e(go,bWo),e(go,as),e(as,vWo),e(as,tce),e(tce,TWo),e(as,FWo),e(as,ace),e(ace,CWo),e(as,MWo),e(as,sce),e(sce,EWo),e(as,yWo),e(go,wWo),e(go,he),e(he,aF),e(aF,nce),e(nce,AWo),e(aF,LWo),e(aF,$N),e($N,BWo),e(aF,xWo),e(he,kWo),e(he,sF),e(sF,lce),e(lce,RWo),e(sF,SWo),e(sF,IN),e(IN,PWo),e(sF,$Wo),e(he,IWo),e(he,nF),e(nF,ice),e(ice,jWo),e(nF,NWo),e(nF,jN),e(jN,DWo),e(nF,qWo),e(he,GWo),e(he,lF),e(lF,dce),e(dce,OWo),e(lF,XWo),e(lF,NN),e(NN,zWo),e(lF,VWo),e(he,WWo),e(he,iF),e(iF,cce),e(cce,QWo),e(iF,HWo),e(iF,DN),e(DN,UWo),e(iF,JWo),e(he,YWo),e(he,dF),e(dF,mce),e(mce,KWo),e(dF,ZWo),e(dF,qN),e(qN,eQo),e(dF,oQo),e(he,rQo),e(he,cF),e(cF,fce),e(fce,tQo),e(cF,aQo),e(cF,GN),e(GN,sQo),e(cF,nQo),e(he,lQo),e(he,mF),e(mF,gce),e(gce,iQo),e(mF,dQo),e(mF,ON),e(ON,cQo),e(mF,mQo),e(he,fQo),e(he,fF),e(fF,hce),e(hce,gQo),e(fF,hQo),e(fF,XN),e(XN,uQo),e(fF,pQo),e(he,_Qo),e(he,gF),e(gF,uce),e(uce,bQo),e(gF,vQo),e(gF,zN),e(zN,TQo),e(gF,FQo),e(go,CQo),e(go,pce),e(pce,MQo),e(go,EQo),g(mA,go,null),b(d,v7e,_),b(d,Yd,_),e(Yd,hF),e(hF,_ce),g(fA,_ce,null),e(Yd,yQo),e(Yd,bce),e(bce,wQo),b(d,T7e,_),b(d,fr,_),g(gA,fr,null),e(fr,AQo),e(fr,Kd),e(Kd,LQo),e(Kd,vce),e(vce,BQo),e(Kd,xQo),e(Kd,Tce),e(Tce,kQo),e(Kd,RQo),e(fr,SQo),e(fr,hA),e(hA,PQo),e(hA,Fce),e(Fce,$Qo),e(hA,IQo),e(fr,jQo),e(fr,at),g(uA,at,null),e(at,NQo),e(at,Cce),e(Cce,DQo),e(at,qQo),e(at,Zd),e(Zd,GQo),e(Zd,Mce),e(Mce,OQo),e(Zd,XQo),e(Zd,Ece),e(Ece,zQo),e(Zd,VQo),e(at,WQo),e(at,yce),e(yce,QQo),e(at,HQo),g(pA,at,null),e(fr,UQo),e(fr,ho),g(_A,ho,null),e(ho,JQo),e(ho,wce),e(wce,YQo),e(ho,KQo),e(ho,ss),e(ss,ZQo),e(ss,Ace),e(Ace,eHo),e(ss,oHo),e(ss,Lce),e(Lce,rHo),e(ss,tHo),e(ss,Bce),e(Bce,aHo),e(ss,sHo),e(ho,nHo),e(ho,xce),e(xce,uF),e(uF,kce),e(kce,lHo),e(uF,iHo),e(uF,VN),e(VN,dHo),e(uF,cHo),e(ho,mHo),e(ho,Rce),e(Rce,fHo),e(ho,gHo),g(bA,ho,null),b(d,F7e,_),b(d,ec,_),e(ec,pF),e(pF,Sce),g(vA,Sce,null),e(ec,hHo),e(ec,Pce),e(Pce,uHo),b(d,C7e,_),b(d,gr,_),g(TA,gr,null),e(gr,pHo),e(gr,oc),e(oc,_Ho),e(oc,$ce),e($ce,bHo),e(oc,vHo),e(oc,Ice),e(Ice,THo),e(oc,FHo),e(gr,CHo),e(gr,FA),e(FA,MHo),e(FA,jce),e(jce,EHo),e(FA,yHo),e(gr,wHo),e(gr,st),g(CA,st,null),e(st,AHo),e(st,Nce),e(Nce,LHo),e(st,BHo),e(st,rc),e(rc,xHo),e(rc,Dce),e(Dce,kHo),e(rc,RHo),e(rc,qce),e(qce,SHo),e(rc,PHo),e(st,$Ho),e(st,Gce),e(Gce,IHo),e(st,jHo),g(MA,st,null),e(gr,NHo),e(gr,uo),g(EA,uo,null),e(uo,DHo),e(uo,Oce),e(Oce,qHo),e(uo,GHo),e(uo,ns),e(ns,OHo),e(ns,Xce),e(Xce,XHo),e(ns,zHo),e(ns,zce),e(zce,VHo),e(ns,WHo),e(ns,Vce),e(Vce,QHo),e(ns,HHo),e(uo,UHo),e(uo,Y),e(Y,_F),e(_F,Wce),e(Wce,JHo),e(_F,YHo),e(_F,WN),e(WN,KHo),e(_F,ZHo),e(Y,eUo),e(Y,bF),e(bF,Qce),e(Qce,oUo),e(bF,rUo),e(bF,QN),e(QN,tUo),e(bF,aUo),e(Y,sUo),e(Y,vF),e(vF,Hce),e(Hce,nUo),e(vF,lUo),e(vF,HN),e(HN,iUo),e(vF,dUo),e(Y,cUo),e(Y,TF),e(TF,Uce),e(Uce,mUo),e(TF,fUo),e(TF,UN),e(UN,gUo),e(TF,hUo),e(Y,uUo),e(Y,FF),e(FF,Jce),e(Jce,pUo),e(FF,_Uo),e(FF,JN),e(JN,bUo),e(FF,vUo),e(Y,TUo),e(Y,CF),e(CF,Yce),e(Yce,FUo),e(CF,CUo),e(CF,YN),e(YN,MUo),e(CF,EUo),e(Y,yUo),e(Y,MF),e(MF,Kce),e(Kce,wUo),e(MF,AUo),e(MF,KN),e(KN,LUo),e(MF,BUo),e(Y,xUo),e(Y,EF),e(EF,Zce),e(Zce,kUo),e(EF,RUo),e(EF,ZN),e(ZN,SUo),e(EF,PUo),e(Y,$Uo),e(Y,yF),e(yF,eme),e(eme,IUo),e(yF,jUo),e(yF,eD),e(eD,NUo),e(yF,DUo),e(Y,qUo),e(Y,wF),e(wF,ome),e(ome,GUo),e(wF,OUo),e(wF,oD),e(oD,XUo),e(wF,zUo),e(Y,VUo),e(Y,AF),e(AF,rme),e(rme,WUo),e(AF,QUo),e(AF,rD),e(rD,HUo),e(AF,UUo),e(Y,JUo),e(Y,LF),e(LF,tme),e(tme,YUo),e(LF,KUo),e(LF,tD),e(tD,ZUo),e(LF,eJo),e(Y,oJo),e(Y,BF),e(BF,ame),e(ame,rJo),e(BF,tJo),e(BF,aD),e(aD,aJo),e(BF,sJo),e(Y,nJo),e(Y,xF),e(xF,sme),e(sme,lJo),e(xF,iJo),e(xF,sD),e(sD,dJo),e(xF,cJo),e(Y,mJo),e(Y,kF),e(kF,nme),e(nme,fJo),e(kF,gJo),e(kF,nD),e(nD,hJo),e(kF,uJo),e(Y,pJo),e(Y,RF),e(RF,lme),e(lme,_Jo),e(RF,bJo),e(RF,lD),e(lD,vJo),e(RF,TJo),e(Y,FJo),e(Y,SF),e(SF,ime),e(ime,CJo),e(SF,MJo),e(SF,iD),e(iD,EJo),e(SF,yJo),e(Y,wJo),e(Y,PF),e(PF,dme),e(dme,AJo),e(PF,LJo),e(PF,dD),e(dD,BJo),e(PF,xJo),e(Y,kJo),e(Y,$F),e($F,cme),e(cme,RJo),e($F,SJo),e($F,cD),e(cD,PJo),e($F,$Jo),e(Y,IJo),e(Y,IF),e(IF,mme),e(mme,jJo),e(IF,NJo),e(IF,mD),e(mD,DJo),e(IF,qJo),e(uo,GJo),e(uo,fme),e(fme,OJo),e(uo,XJo),g(yA,uo,null),b(d,M7e,_),b(d,tc,_),e(tc,jF),e(jF,gme),g(wA,gme,null),e(tc,zJo),e(tc,hme),e(hme,VJo),b(d,E7e,_),b(d,hr,_),g(AA,hr,null),e(hr,WJo),e(hr,ac),e(ac,QJo),e(ac,ume),e(ume,HJo),e(ac,UJo),e(ac,pme),e(pme,JJo),e(ac,YJo),e(hr,KJo),e(hr,LA),e(LA,ZJo),e(LA,_me),e(_me,eYo),e(LA,oYo),e(hr,rYo),e(hr,nt),g(BA,nt,null),e(nt,tYo),e(nt,bme),e(bme,aYo),e(nt,sYo),e(nt,sc),e(sc,nYo),e(sc,vme),e(vme,lYo),e(sc,iYo),e(sc,Tme),e(Tme,dYo),e(sc,cYo),e(nt,mYo),e(nt,Fme),e(Fme,fYo),e(nt,gYo),g(xA,nt,null),e(hr,hYo),e(hr,po),g(kA,po,null),e(po,uYo),e(po,Cme),e(Cme,pYo),e(po,_Yo),e(po,ls),e(ls,bYo),e(ls,Mme),e(Mme,vYo),e(ls,TYo),e(ls,Eme),e(Eme,FYo),e(ls,CYo),e(ls,yme),e(yme,MYo),e(ls,EYo),e(po,yYo),e(po,ue),e(ue,NF),e(NF,wme),e(wme,wYo),e(NF,AYo),e(NF,fD),e(fD,LYo),e(NF,BYo),e(ue,xYo),e(ue,DF),e(DF,Ame),e(Ame,kYo),e(DF,RYo),e(DF,gD),e(gD,SYo),e(DF,PYo),e(ue,$Yo),e(ue,qF),e(qF,Lme),e(Lme,IYo),e(qF,jYo),e(qF,hD),e(hD,NYo),e(qF,DYo),e(ue,qYo),e(ue,GF),e(GF,Bme),e(Bme,GYo),e(GF,OYo),e(GF,uD),e(uD,XYo),e(GF,zYo),e(ue,VYo),e(ue,OF),e(OF,xme),e(xme,WYo),e(OF,QYo),e(OF,pD),e(pD,HYo),e(OF,UYo),e(ue,JYo),e(ue,XF),e(XF,kme),e(kme,YYo),e(XF,KYo),e(XF,_D),e(_D,ZYo),e(XF,eKo),e(ue,oKo),e(ue,zF),e(zF,Rme),e(Rme,rKo),e(zF,tKo),e(zF,bD),e(bD,aKo),e(zF,sKo),e(ue,nKo),e(ue,VF),e(VF,Sme),e(Sme,lKo),e(VF,iKo),e(VF,vD),e(vD,dKo),e(VF,cKo),e(ue,mKo),e(ue,WF),e(WF,Pme),e(Pme,fKo),e(WF,gKo),e(WF,TD),e(TD,hKo),e(WF,uKo),e(ue,pKo),e(ue,QF),e(QF,$me),e($me,_Ko),e(QF,bKo),e(QF,FD),e(FD,vKo),e(QF,TKo),e(po,FKo),e(po,Ime),e(Ime,CKo),e(po,MKo),g(RA,po,null),b(d,y7e,_),b(d,nc,_),e(nc,HF),e(HF,jme),g(SA,jme,null),e(nc,EKo),e(nc,Nme),e(Nme,yKo),b(d,w7e,_),b(d,ur,_),g(PA,ur,null),e(ur,wKo),e(ur,lc),e(lc,AKo),e(lc,Dme),e(Dme,LKo),e(lc,BKo),e(lc,qme),e(qme,xKo),e(lc,kKo),e(ur,RKo),e(ur,$A),e($A,SKo),e($A,Gme),e(Gme,PKo),e($A,$Ko),e(ur,IKo),e(ur,lt),g(IA,lt,null),e(lt,jKo),e(lt,Ome),e(Ome,NKo),e(lt,DKo),e(lt,ic),e(ic,qKo),e(ic,Xme),e(Xme,GKo),e(ic,OKo),e(ic,zme),e(zme,XKo),e(ic,zKo),e(lt,VKo),e(lt,Vme),e(Vme,WKo),e(lt,QKo),g(jA,lt,null),e(ur,HKo),e(ur,_o),g(NA,_o,null),e(_o,UKo),e(_o,Wme),e(Wme,JKo),e(_o,YKo),e(_o,is),e(is,KKo),e(is,Qme),e(Qme,ZKo),e(is,eZo),e(is,Hme),e(Hme,oZo),e(is,rZo),e(is,Ume),e(Ume,tZo),e(is,aZo),e(_o,sZo),e(_o,X),e(X,UF),e(UF,Jme),e(Jme,nZo),e(UF,lZo),e(UF,CD),e(CD,iZo),e(UF,dZo),e(X,cZo),e(X,JF),e(JF,Yme),e(Yme,mZo),e(JF,fZo),e(JF,MD),e(MD,gZo),e(JF,hZo),e(X,uZo),e(X,YF),e(YF,Kme),e(Kme,pZo),e(YF,_Zo),e(YF,ED),e(ED,bZo),e(YF,vZo),e(X,TZo),e(X,KF),e(KF,Zme),e(Zme,FZo),e(KF,CZo),e(KF,yD),e(yD,MZo),e(KF,EZo),e(X,yZo),e(X,ZF),e(ZF,efe),e(efe,wZo),e(ZF,AZo),e(ZF,wD),e(wD,LZo),e(ZF,BZo),e(X,xZo),e(X,eC),e(eC,ofe),e(ofe,kZo),e(eC,RZo),e(eC,AD),e(AD,SZo),e(eC,PZo),e(X,$Zo),e(X,oC),e(oC,rfe),e(rfe,IZo),e(oC,jZo),e(oC,LD),e(LD,NZo),e(oC,DZo),e(X,qZo),e(X,rC),e(rC,tfe),e(tfe,GZo),e(rC,OZo),e(rC,BD),e(BD,XZo),e(rC,zZo),e(X,VZo),e(X,tC),e(tC,afe),e(afe,WZo),e(tC,QZo),e(tC,xD),e(xD,HZo),e(tC,UZo),e(X,JZo),e(X,aC),e(aC,sfe),e(sfe,YZo),e(aC,KZo),e(aC,kD),e(kD,ZZo),e(aC,eer),e(X,oer),e(X,sC),e(sC,nfe),e(nfe,rer),e(sC,ter),e(sC,RD),e(RD,aer),e(sC,ser),e(X,ner),e(X,nC),e(nC,lfe),e(lfe,ler),e(nC,ier),e(nC,SD),e(SD,der),e(nC,cer),e(X,mer),e(X,lC),e(lC,ife),e(ife,fer),e(lC,ger),e(lC,PD),e(PD,her),e(lC,uer),e(X,per),e(X,iC),e(iC,dfe),e(dfe,_er),e(iC,ber),e(iC,$D),e($D,ver),e(iC,Ter),e(X,Fer),e(X,dC),e(dC,cfe),e(cfe,Cer),e(dC,Mer),e(dC,ID),e(ID,Eer),e(dC,yer),e(X,wer),e(X,cC),e(cC,mfe),e(mfe,Aer),e(cC,Ler),e(cC,jD),e(jD,Ber),e(cC,xer),e(X,ker),e(X,mC),e(mC,ffe),e(ffe,Rer),e(mC,Ser),e(mC,ND),e(ND,Per),e(mC,$er),e(X,Ier),e(X,fC),e(fC,gfe),e(gfe,jer),e(fC,Ner),e(fC,DD),e(DD,Der),e(fC,qer),e(X,Ger),e(X,gC),e(gC,hfe),e(hfe,Oer),e(gC,Xer),e(gC,qD),e(qD,zer),e(gC,Ver),e(X,Wer),e(X,hC),e(hC,ufe),e(ufe,Qer),e(hC,Her),e(hC,GD),e(GD,Uer),e(hC,Jer),e(X,Yer),e(X,uC),e(uC,pfe),e(pfe,Ker),e(uC,Zer),e(uC,OD),e(OD,eor),e(uC,oor),e(X,ror),e(X,pC),e(pC,_fe),e(_fe,tor),e(pC,aor),e(pC,XD),e(XD,sor),e(pC,nor),e(X,lor),e(X,_C),e(_C,bfe),e(bfe,ior),e(_C,dor),e(_C,zD),e(zD,cor),e(_C,mor),e(X,gor),e(X,bC),e(bC,vfe),e(vfe,hor),e(bC,uor),e(bC,VD),e(VD,por),e(bC,_or),e(X,bor),e(X,vC),e(vC,Tfe),e(Tfe,vor),e(vC,Tor),e(vC,WD),e(WD,For),e(vC,Cor),e(_o,Mor),e(_o,Ffe),e(Ffe,Eor),e(_o,yor),g(DA,_o,null),b(d,A7e,_),b(d,dc,_),e(dc,TC),e(TC,Cfe),g(qA,Cfe,null),e(dc,wor),e(dc,Mfe),e(Mfe,Aor),b(d,L7e,_),b(d,pr,_),g(GA,pr,null),e(pr,Lor),e(pr,cc),e(cc,Bor),e(cc,Efe),e(Efe,xor),e(cc,kor),e(cc,yfe),e(yfe,Ror),e(cc,Sor),e(pr,Por),e(pr,OA),e(OA,$or),e(OA,wfe),e(wfe,Ior),e(OA,jor),e(pr,Nor),e(pr,it),g(XA,it,null),e(it,Dor),e(it,Afe),e(Afe,qor),e(it,Gor),e(it,mc),e(mc,Oor),e(mc,Lfe),e(Lfe,Xor),e(mc,zor),e(mc,Bfe),e(Bfe,Vor),e(mc,Wor),e(it,Qor),e(it,xfe),e(xfe,Hor),e(it,Uor),g(zA,it,null),e(pr,Jor),e(pr,bo),g(VA,bo,null),e(bo,Yor),e(bo,kfe),e(kfe,Kor),e(bo,Zor),e(bo,ds),e(ds,err),e(ds,Rfe),e(Rfe,orr),e(ds,rrr),e(ds,Sfe),e(Sfe,trr),e(ds,arr),e(ds,Pfe),e(Pfe,srr),e(ds,nrr),e(bo,lrr),e(bo,te),e(te,FC),e(FC,$fe),e($fe,irr),e(FC,drr),e(FC,QD),e(QD,crr),e(FC,mrr),e(te,frr),e(te,CC),e(CC,Ife),e(Ife,grr),e(CC,hrr),e(CC,HD),e(HD,urr),e(CC,prr),e(te,_rr),e(te,MC),e(MC,jfe),e(jfe,brr),e(MC,vrr),e(MC,UD),e(UD,Trr),e(MC,Frr),e(te,Crr),e(te,EC),e(EC,Nfe),e(Nfe,Mrr),e(EC,Err),e(EC,JD),e(JD,yrr),e(EC,wrr),e(te,Arr),e(te,yC),e(yC,Dfe),e(Dfe,Lrr),e(yC,Brr),e(yC,YD),e(YD,xrr),e(yC,krr),e(te,Rrr),e(te,wC),e(wC,qfe),e(qfe,Srr),e(wC,Prr),e(wC,KD),e(KD,$rr),e(wC,Irr),e(te,jrr),e(te,AC),e(AC,Gfe),e(Gfe,Nrr),e(AC,Drr),e(AC,ZD),e(ZD,qrr),e(AC,Grr),e(te,Orr),e(te,LC),e(LC,Ofe),e(Ofe,Xrr),e(LC,zrr),e(LC,eq),e(eq,Vrr),e(LC,Wrr),e(te,Qrr),e(te,BC),e(BC,Xfe),e(Xfe,Hrr),e(BC,Urr),e(BC,oq),e(oq,Jrr),e(BC,Yrr),e(te,Krr),e(te,xC),e(xC,zfe),e(zfe,Zrr),e(xC,etr),e(xC,rq),e(rq,otr),e(xC,rtr),e(te,ttr),e(te,kC),e(kC,Vfe),e(Vfe,atr),e(kC,str),e(kC,tq),e(tq,ntr),e(kC,ltr),e(te,itr),e(te,RC),e(RC,Wfe),e(Wfe,dtr),e(RC,ctr),e(RC,aq),e(aq,mtr),e(RC,ftr),e(te,gtr),e(te,SC),e(SC,Qfe),e(Qfe,htr),e(SC,utr),e(SC,sq),e(sq,ptr),e(SC,_tr),e(te,btr),e(te,PC),e(PC,Hfe),e(Hfe,vtr),e(PC,Ttr),e(PC,nq),e(nq,Ftr),e(PC,Ctr),e(te,Mtr),e(te,$C),e($C,Ufe),e(Ufe,Etr),e($C,ytr),e($C,lq),e(lq,wtr),e($C,Atr),e(te,Ltr),e(te,IC),e(IC,Jfe),e(Jfe,Btr),e(IC,xtr),e(IC,iq),e(iq,ktr),e(IC,Rtr),e(te,Str),e(te,jC),e(jC,Yfe),e(Yfe,Ptr),e(jC,$tr),e(jC,dq),e(dq,Itr),e(jC,jtr),e(bo,Ntr),e(bo,Kfe),e(Kfe,Dtr),e(bo,qtr),g(WA,bo,null),b(d,B7e,_),b(d,fc,_),e(fc,NC),e(NC,Zfe),g(QA,Zfe,null),e(fc,Gtr),e(fc,ege),e(ege,Otr),b(d,x7e,_),b(d,_r,_),g(HA,_r,null),e(_r,Xtr),e(_r,gc),e(gc,ztr),e(gc,oge),e(oge,Vtr),e(gc,Wtr),e(gc,rge),e(rge,Qtr),e(gc,Htr),e(_r,Utr),e(_r,UA),e(UA,Jtr),e(UA,tge),e(tge,Ytr),e(UA,Ktr),e(_r,Ztr),e(_r,dt),g(JA,dt,null),e(dt,ear),e(dt,age),e(age,oar),e(dt,rar),e(dt,hc),e(hc,tar),e(hc,sge),e(sge,aar),e(hc,sar),e(hc,nge),e(nge,nar),e(hc,lar),e(dt,iar),e(dt,lge),e(lge,dar),e(dt,car),g(YA,dt,null),e(_r,mar),e(_r,vo),g(KA,vo,null),e(vo,far),e(vo,ige),e(ige,gar),e(vo,har),e(vo,cs),e(cs,uar),e(cs,dge),e(dge,par),e(cs,_ar),e(cs,cge),e(cge,bar),e(cs,Tar),e(cs,mge),e(mge,Far),e(cs,Car),e(vo,Mar),e(vo,fge),e(fge,DC),e(DC,gge),e(gge,Ear),e(DC,yar),e(DC,cq),e(cq,war),e(DC,Aar),e(vo,Lar),e(vo,hge),e(hge,Bar),e(vo,xar),g(ZA,vo,null),b(d,k7e,_),b(d,uc,_),e(uc,qC),e(qC,uge),g(e0,uge,null),e(uc,kar),e(uc,pge),e(pge,Rar),b(d,R7e,_),b(d,br,_),g(o0,br,null),e(br,Sar),e(br,pc),e(pc,Par),e(pc,_ge),e(_ge,$ar),e(pc,Iar),e(pc,bge),e(bge,jar),e(pc,Nar),e(br,Dar),e(br,r0),e(r0,qar),e(r0,vge),e(vge,Gar),e(r0,Oar),e(br,Xar),e(br,ct),g(t0,ct,null),e(ct,zar),e(ct,Tge),e(Tge,Var),e(ct,War),e(ct,_c),e(_c,Qar),e(_c,Fge),e(Fge,Har),e(_c,Uar),e(_c,Cge),e(Cge,Jar),e(_c,Yar),e(ct,Kar),e(ct,Mge),e(Mge,Zar),e(ct,esr),g(a0,ct,null),e(br,osr),e(br,To),g(s0,To,null),e(To,rsr),e(To,Ege),e(Ege,tsr),e(To,asr),e(To,ms),e(ms,ssr),e(ms,yge),e(yge,nsr),e(ms,lsr),e(ms,wge),e(wge,isr),e(ms,dsr),e(ms,Age),e(Age,csr),e(ms,msr),e(To,fsr),e(To,K),e(K,GC),e(GC,Lge),e(Lge,gsr),e(GC,hsr),e(GC,mq),e(mq,usr),e(GC,psr),e(K,_sr),e(K,OC),e(OC,Bge),e(Bge,bsr),e(OC,vsr),e(OC,fq),e(fq,Tsr),e(OC,Fsr),e(K,Csr),e(K,XC),e(XC,xge),e(xge,Msr),e(XC,Esr),e(XC,gq),e(gq,ysr),e(XC,wsr),e(K,Asr),e(K,zC),e(zC,kge),e(kge,Lsr),e(zC,Bsr),e(zC,hq),e(hq,xsr),e(zC,ksr),e(K,Rsr),e(K,VC),e(VC,Rge),e(Rge,Ssr),e(VC,Psr),e(VC,uq),e(uq,$sr),e(VC,Isr),e(K,jsr),e(K,WC),e(WC,Sge),e(Sge,Nsr),e(WC,Dsr),e(WC,pq),e(pq,qsr),e(WC,Gsr),e(K,Osr),e(K,QC),e(QC,Pge),e(Pge,Xsr),e(QC,zsr),e(QC,_q),e(_q,Vsr),e(QC,Wsr),e(K,Qsr),e(K,HC),e(HC,$ge),e($ge,Hsr),e(HC,Usr),e(HC,bq),e(bq,Jsr),e(HC,Ysr),e(K,Ksr),e(K,UC),e(UC,Ige),e(Ige,Zsr),e(UC,enr),e(UC,vq),e(vq,onr),e(UC,rnr),e(K,tnr),e(K,JC),e(JC,jge),e(jge,anr),e(JC,snr),e(JC,Tq),e(Tq,nnr),e(JC,lnr),e(K,inr),e(K,YC),e(YC,Nge),e(Nge,dnr),e(YC,cnr),e(YC,Fq),e(Fq,mnr),e(YC,fnr),e(K,gnr),e(K,KC),e(KC,Dge),e(Dge,hnr),e(KC,unr),e(KC,Cq),e(Cq,pnr),e(KC,_nr),e(K,bnr),e(K,ZC),e(ZC,qge),e(qge,vnr),e(ZC,Tnr),e(ZC,Mq),e(Mq,Fnr),e(ZC,Cnr),e(K,Mnr),e(K,e4),e(e4,Gge),e(Gge,Enr),e(e4,ynr),e(e4,Eq),e(Eq,wnr),e(e4,Anr),e(K,Lnr),e(K,o4),e(o4,Oge),e(Oge,Bnr),e(o4,xnr),e(o4,yq),e(yq,knr),e(o4,Rnr),e(K,Snr),e(K,r4),e(r4,Xge),e(Xge,Pnr),e(r4,$nr),e(r4,wq),e(wq,Inr),e(r4,jnr),e(K,Nnr),e(K,t4),e(t4,zge),e(zge,Dnr),e(t4,qnr),e(t4,Aq),e(Aq,Gnr),e(t4,Onr),e(K,Xnr),e(K,a4),e(a4,Vge),e(Vge,znr),e(a4,Vnr),e(a4,Lq),e(Lq,Wnr),e(a4,Qnr),e(K,Hnr),e(K,s4),e(s4,Wge),e(Wge,Unr),e(s4,Jnr),e(s4,Bq),e(Bq,Ynr),e(s4,Knr),e(K,Znr),e(K,n4),e(n4,Qge),e(Qge,elr),e(n4,olr),e(n4,xq),e(xq,rlr),e(n4,tlr),e(To,alr),e(To,Hge),e(Hge,slr),e(To,nlr),g(n0,To,null),b(d,S7e,_),b(d,bc,_),e(bc,l4),e(l4,Uge),g(l0,Uge,null),e(bc,llr),e(bc,Jge),e(Jge,ilr),b(d,P7e,_),b(d,vr,_),g(i0,vr,null),e(vr,dlr),e(vr,vc),e(vc,clr),e(vc,Yge),e(Yge,mlr),e(vc,flr),e(vc,Kge),e(Kge,glr),e(vc,hlr),e(vr,ulr),e(vr,d0),e(d0,plr),e(d0,Zge),e(Zge,_lr),e(d0,blr),e(vr,vlr),e(vr,mt),g(c0,mt,null),e(mt,Tlr),e(mt,ehe),e(ehe,Flr),e(mt,Clr),e(mt,Tc),e(Tc,Mlr),e(Tc,ohe),e(ohe,Elr),e(Tc,ylr),e(Tc,rhe),e(rhe,wlr),e(Tc,Alr),e(mt,Llr),e(mt,the),e(the,Blr),e(mt,xlr),g(m0,mt,null),e(vr,klr),e(vr,Fo),g(f0,Fo,null),e(Fo,Rlr),e(Fo,ahe),e(ahe,Slr),e(Fo,Plr),e(Fo,fs),e(fs,$lr),e(fs,she),e(she,Ilr),e(fs,jlr),e(fs,nhe),e(nhe,Nlr),e(fs,Dlr),e(fs,lhe),e(lhe,qlr),e(fs,Glr),e(Fo,Olr),e(Fo,Z),e(Z,i4),e(i4,ihe),e(ihe,Xlr),e(i4,zlr),e(i4,kq),e(kq,Vlr),e(i4,Wlr),e(Z,Qlr),e(Z,d4),e(d4,dhe),e(dhe,Hlr),e(d4,Ulr),e(d4,Rq),e(Rq,Jlr),e(d4,Ylr),e(Z,Klr),e(Z,c4),e(c4,che),e(che,Zlr),e(c4,eir),e(c4,Sq),e(Sq,oir),e(c4,rir),e(Z,tir),e(Z,m4),e(m4,mhe),e(mhe,air),e(m4,sir),e(m4,Pq),e(Pq,nir),e(m4,lir),e(Z,iir),e(Z,f4),e(f4,fhe),e(fhe,dir),e(f4,cir),e(f4,$q),e($q,mir),e(f4,fir),e(Z,gir),e(Z,g4),e(g4,ghe),e(ghe,hir),e(g4,uir),e(g4,Iq),e(Iq,pir),e(g4,_ir),e(Z,bir),e(Z,h4),e(h4,hhe),e(hhe,vir),e(h4,Tir),e(h4,jq),e(jq,Fir),e(h4,Cir),e(Z,Mir),e(Z,u4),e(u4,uhe),e(uhe,Eir),e(u4,yir),e(u4,Nq),e(Nq,wir),e(u4,Air),e(Z,Lir),e(Z,p4),e(p4,phe),e(phe,Bir),e(p4,xir),e(p4,Dq),e(Dq,kir),e(p4,Rir),e(Z,Sir),e(Z,_4),e(_4,_he),e(_he,Pir),e(_4,$ir),e(_4,qq),e(qq,Iir),e(_4,jir),e(Z,Nir),e(Z,b4),e(b4,bhe),e(bhe,Dir),e(b4,qir),e(b4,Gq),e(Gq,Gir),e(b4,Oir),e(Z,Xir),e(Z,v4),e(v4,vhe),e(vhe,zir),e(v4,Vir),e(v4,Oq),e(Oq,Wir),e(v4,Qir),e(Z,Hir),e(Z,T4),e(T4,The),e(The,Uir),e(T4,Jir),e(T4,Xq),e(Xq,Yir),e(T4,Kir),e(Z,Zir),e(Z,F4),e(F4,Fhe),e(Fhe,edr),e(F4,odr),e(F4,zq),e(zq,rdr),e(F4,tdr),e(Z,adr),e(Z,C4),e(C4,Che),e(Che,sdr),e(C4,ndr),e(C4,Vq),e(Vq,ldr),e(C4,idr),e(Z,ddr),e(Z,M4),e(M4,Mhe),e(Mhe,cdr),e(M4,mdr),e(M4,Wq),e(Wq,fdr),e(M4,gdr),e(Z,hdr),e(Z,E4),e(E4,Ehe),e(Ehe,udr),e(E4,pdr),e(E4,Qq),e(Qq,_dr),e(E4,bdr),e(Z,vdr),e(Z,y4),e(y4,yhe),e(yhe,Tdr),e(y4,Fdr),e(y4,Hq),e(Hq,Cdr),e(y4,Mdr),e(Z,Edr),e(Z,w4),e(w4,whe),e(whe,ydr),e(w4,wdr),e(w4,Uq),e(Uq,Adr),e(w4,Ldr),e(Fo,Bdr),e(Fo,Ahe),e(Ahe,xdr),e(Fo,kdr),g(g0,Fo,null),b(d,$7e,_),b(d,Fc,_),e(Fc,A4),e(A4,Lhe),g(h0,Lhe,null),e(Fc,Rdr),e(Fc,Bhe),e(Bhe,Sdr),b(d,I7e,_),b(d,Tr,_),g(u0,Tr,null),e(Tr,Pdr),e(Tr,Cc),e(Cc,$dr),e(Cc,xhe),e(xhe,Idr),e(Cc,jdr),e(Cc,khe),e(khe,Ndr),e(Cc,Ddr),e(Tr,qdr),e(Tr,p0),e(p0,Gdr),e(p0,Rhe),e(Rhe,Odr),e(p0,Xdr),e(Tr,zdr),e(Tr,ft),g(_0,ft,null),e(ft,Vdr),e(ft,She),e(She,Wdr),e(ft,Qdr),e(ft,Mc),e(Mc,Hdr),e(Mc,Phe),e(Phe,Udr),e(Mc,Jdr),e(Mc,$he),e($he,Ydr),e(Mc,Kdr),e(ft,Zdr),e(ft,Ihe),e(Ihe,ecr),e(ft,ocr),g(b0,ft,null),e(Tr,rcr),e(Tr,Co),g(v0,Co,null),e(Co,tcr),e(Co,jhe),e(jhe,acr),e(Co,scr),e(Co,gs),e(gs,ncr),e(gs,Nhe),e(Nhe,lcr),e(gs,icr),e(gs,Dhe),e(Dhe,dcr),e(gs,ccr),e(gs,qhe),e(qhe,mcr),e(gs,fcr),e(Co,gcr),e(Co,Ghe),e(Ghe,L4),e(L4,Ohe),e(Ohe,hcr),e(L4,ucr),e(L4,Jq),e(Jq,pcr),e(L4,_cr),e(Co,bcr),e(Co,Xhe),e(Xhe,vcr),e(Co,Tcr),g(T0,Co,null),b(d,j7e,_),b(d,Ec,_),e(Ec,B4),e(B4,zhe),g(F0,zhe,null),e(Ec,Fcr),e(Ec,Vhe),e(Vhe,Ccr),b(d,N7e,_),b(d,Fr,_),g(C0,Fr,null),e(Fr,Mcr),e(Fr,yc),e(yc,Ecr),e(yc,Whe),e(Whe,ycr),e(yc,wcr),e(yc,Qhe),e(Qhe,Acr),e(yc,Lcr),e(Fr,Bcr),e(Fr,M0),e(M0,xcr),e(M0,Hhe),e(Hhe,kcr),e(M0,Rcr),e(Fr,Scr),e(Fr,gt),g(E0,gt,null),e(gt,Pcr),e(gt,Uhe),e(Uhe,$cr),e(gt,Icr),e(gt,wc),e(wc,jcr),e(wc,Jhe),e(Jhe,Ncr),e(wc,Dcr),e(wc,Yhe),e(Yhe,qcr),e(wc,Gcr),e(gt,Ocr),e(gt,Khe),e(Khe,Xcr),e(gt,zcr),g(y0,gt,null),e(Fr,Vcr),e(Fr,Mo),g(w0,Mo,null),e(Mo,Wcr),e(Mo,Zhe),e(Zhe,Qcr),e(Mo,Hcr),e(Mo,hs),e(hs,Ucr),e(hs,eue),e(eue,Jcr),e(hs,Ycr),e(hs,oue),e(oue,Kcr),e(hs,Zcr),e(hs,rue),e(rue,emr),e(hs,omr),e(Mo,rmr),e(Mo,tue),e(tue,x4),e(x4,aue),e(aue,tmr),e(x4,amr),e(x4,Yq),e(Yq,smr),e(x4,nmr),e(Mo,lmr),e(Mo,sue),e(sue,imr),e(Mo,dmr),g(A0,Mo,null),b(d,D7e,_),b(d,Ac,_),e(Ac,k4),e(k4,nue),g(L0,nue,null),e(Ac,cmr),e(Ac,lue),e(lue,mmr),b(d,q7e,_),b(d,Cr,_),g(B0,Cr,null),e(Cr,fmr),e(Cr,Lc),e(Lc,gmr),e(Lc,iue),e(iue,hmr),e(Lc,umr),e(Lc,due),e(due,pmr),e(Lc,_mr),e(Cr,bmr),e(Cr,x0),e(x0,vmr),e(x0,cue),e(cue,Tmr),e(x0,Fmr),e(Cr,Cmr),e(Cr,ht),g(k0,ht,null),e(ht,Mmr),e(ht,mue),e(mue,Emr),e(ht,ymr),e(ht,Bc),e(Bc,wmr),e(Bc,fue),e(fue,Amr),e(Bc,Lmr),e(Bc,gue),e(gue,Bmr),e(Bc,xmr),e(ht,kmr),e(ht,hue),e(hue,Rmr),e(ht,Smr),g(R0,ht,null),e(Cr,Pmr),e(Cr,Eo),g(S0,Eo,null),e(Eo,$mr),e(Eo,uue),e(uue,Imr),e(Eo,jmr),e(Eo,us),e(us,Nmr),e(us,pue),e(pue,Dmr),e(us,qmr),e(us,_ue),e(_ue,Gmr),e(us,Omr),e(us,bue),e(bue,Xmr),e(us,zmr),e(Eo,Vmr),e(Eo,V),e(V,R4),e(R4,vue),e(vue,Wmr),e(R4,Qmr),e(R4,Kq),e(Kq,Hmr),e(R4,Umr),e(V,Jmr),e(V,S4),e(S4,Tue),e(Tue,Ymr),e(S4,Kmr),e(S4,Zq),e(Zq,Zmr),e(S4,efr),e(V,ofr),e(V,P4),e(P4,Fue),e(Fue,rfr),e(P4,tfr),e(P4,eG),e(eG,afr),e(P4,sfr),e(V,nfr),e(V,$4),e($4,Cue),e(Cue,lfr),e($4,ifr),e($4,oG),e(oG,dfr),e($4,cfr),e(V,mfr),e(V,I4),e(I4,Mue),e(Mue,ffr),e(I4,gfr),e(I4,rG),e(rG,hfr),e(I4,ufr),e(V,pfr),e(V,j4),e(j4,Eue),e(Eue,_fr),e(j4,bfr),e(j4,tG),e(tG,vfr),e(j4,Tfr),e(V,Ffr),e(V,N4),e(N4,yue),e(yue,Cfr),e(N4,Mfr),e(N4,aG),e(aG,Efr),e(N4,yfr),e(V,wfr),e(V,D4),e(D4,wue),e(wue,Afr),e(D4,Lfr),e(D4,sG),e(sG,Bfr),e(D4,xfr),e(V,kfr),e(V,q4),e(q4,Aue),e(Aue,Rfr),e(q4,Sfr),e(q4,nG),e(nG,Pfr),e(q4,$fr),e(V,Ifr),e(V,G4),e(G4,Lue),e(Lue,jfr),e(G4,Nfr),e(G4,lG),e(lG,Dfr),e(G4,qfr),e(V,Gfr),e(V,O4),e(O4,Bue),e(Bue,Ofr),e(O4,Xfr),e(O4,iG),e(iG,zfr),e(O4,Vfr),e(V,Wfr),e(V,X4),e(X4,xue),e(xue,Qfr),e(X4,Hfr),e(X4,dG),e(dG,Ufr),e(X4,Jfr),e(V,Yfr),e(V,z4),e(z4,kue),e(kue,Kfr),e(z4,Zfr),e(z4,cG),e(cG,egr),e(z4,ogr),e(V,rgr),e(V,V4),e(V4,Rue),e(Rue,tgr),e(V4,agr),e(V4,mG),e(mG,sgr),e(V4,ngr),e(V,lgr),e(V,W4),e(W4,Sue),e(Sue,igr),e(W4,dgr),e(W4,fG),e(fG,cgr),e(W4,mgr),e(V,fgr),e(V,Q4),e(Q4,Pue),e(Pue,ggr),e(Q4,hgr),e(Q4,gG),e(gG,ugr),e(Q4,pgr),e(V,_gr),e(V,H4),e(H4,$ue),e($ue,bgr),e(H4,vgr),e(H4,hG),e(hG,Tgr),e(H4,Fgr),e(V,Cgr),e(V,U4),e(U4,Iue),e(Iue,Mgr),e(U4,Egr),e(U4,uG),e(uG,ygr),e(U4,wgr),e(V,Agr),e(V,J4),e(J4,jue),e(jue,Lgr),e(J4,Bgr),e(J4,pG),e(pG,xgr),e(J4,kgr),e(V,Rgr),e(V,Y4),e(Y4,Nue),e(Nue,Sgr),e(Y4,Pgr),e(Y4,_G),e(_G,$gr),e(Y4,Igr),e(V,jgr),e(V,K4),e(K4,Due),e(Due,Ngr),e(K4,Dgr),e(K4,bG),e(bG,qgr),e(K4,Ggr),e(V,Ogr),e(V,Z4),e(Z4,que),e(que,Xgr),e(Z4,zgr),e(Z4,vG),e(vG,Vgr),e(Z4,Wgr),e(V,Qgr),e(V,eM),e(eM,Gue),e(Gue,Hgr),e(eM,Ugr),e(eM,TG),e(TG,Jgr),e(eM,Ygr),e(V,Kgr),e(V,oM),e(oM,Oue),e(Oue,Zgr),e(oM,ehr),e(oM,FG),e(FG,ohr),e(oM,rhr),e(Eo,thr),e(Eo,Xue),e(Xue,ahr),e(Eo,shr),g(P0,Eo,null),b(d,G7e,_),b(d,xc,_),e(xc,rM),e(rM,zue),g($0,zue,null),e(xc,nhr),e(xc,Vue),e(Vue,lhr),b(d,O7e,_),b(d,Mr,_),g(I0,Mr,null),e(Mr,ihr),e(Mr,kc),e(kc,dhr),e(kc,Wue),e(Wue,chr),e(kc,mhr),e(kc,Que),e(Que,fhr),e(kc,ghr),e(Mr,hhr),e(Mr,j0),e(j0,uhr),e(j0,Hue),e(Hue,phr),e(j0,_hr),e(Mr,bhr),e(Mr,ut),g(N0,ut,null),e(ut,vhr),e(ut,Uue),e(Uue,Thr),e(ut,Fhr),e(ut,Rc),e(Rc,Chr),e(Rc,Jue),e(Jue,Mhr),e(Rc,Ehr),e(Rc,Yue),e(Yue,yhr),e(Rc,whr),e(ut,Ahr),e(ut,Kue),e(Kue,Lhr),e(ut,Bhr),g(D0,ut,null),e(Mr,xhr),e(Mr,yo),g(q0,yo,null),e(yo,khr),e(yo,Zue),e(Zue,Rhr),e(yo,Shr),e(yo,ps),e(ps,Phr),e(ps,epe),e(epe,$hr),e(ps,Ihr),e(ps,ope),e(ope,jhr),e(ps,Nhr),e(ps,rpe),e(rpe,Dhr),e(ps,qhr),e(yo,Ghr),e(yo,_s),e(_s,tM),e(tM,tpe),e(tpe,Ohr),e(tM,Xhr),e(tM,CG),e(CG,zhr),e(tM,Vhr),e(_s,Whr),e(_s,aM),e(aM,ape),e(ape,Qhr),e(aM,Hhr),e(aM,MG),e(MG,Uhr),e(aM,Jhr),e(_s,Yhr),e(_s,sM),e(sM,spe),e(spe,Khr),e(sM,Zhr),e(sM,EG),e(EG,eur),e(sM,our),e(_s,rur),e(_s,nM),e(nM,npe),e(npe,tur),e(nM,aur),e(nM,yG),e(yG,sur),e(nM,nur),e(yo,lur),e(yo,lpe),e(lpe,iur),e(yo,dur),g(G0,yo,null),b(d,X7e,_),b(d,Sc,_),e(Sc,lM),e(lM,ipe),g(O0,ipe,null),e(Sc,cur),e(Sc,dpe),e(dpe,mur),b(d,z7e,_),b(d,Er,_),g(X0,Er,null),e(Er,fur),e(Er,Pc),e(Pc,gur),e(Pc,cpe),e(cpe,hur),e(Pc,uur),e(Pc,mpe),e(mpe,pur),e(Pc,_ur),e(Er,bur),e(Er,z0),e(z0,vur),e(z0,fpe),e(fpe,Tur),e(z0,Fur),e(Er,Cur),e(Er,pt),g(V0,pt,null),e(pt,Mur),e(pt,gpe),e(gpe,Eur),e(pt,yur),e(pt,$c),e($c,wur),e($c,hpe),e(hpe,Aur),e($c,Lur),e($c,upe),e(upe,Bur),e($c,xur),e(pt,kur),e(pt,ppe),e(ppe,Rur),e(pt,Sur),g(W0,pt,null),e(Er,Pur),e(Er,wo),g(Q0,wo,null),e(wo,$ur),e(wo,_pe),e(_pe,Iur),e(wo,jur),e(wo,bs),e(bs,Nur),e(bs,bpe),e(bpe,Dur),e(bs,qur),e(bs,vpe),e(vpe,Gur),e(bs,Our),e(bs,Tpe),e(Tpe,Xur),e(bs,zur),e(wo,Vur),e(wo,me),e(me,iM),e(iM,Fpe),e(Fpe,Wur),e(iM,Qur),e(iM,wG),e(wG,Hur),e(iM,Uur),e(me,Jur),e(me,dM),e(dM,Cpe),e(Cpe,Yur),e(dM,Kur),e(dM,AG),e(AG,Zur),e(dM,epr),e(me,opr),e(me,cM),e(cM,Mpe),e(Mpe,rpr),e(cM,tpr),e(cM,LG),e(LG,apr),e(cM,spr),e(me,npr),e(me,mM),e(mM,Epe),e(Epe,lpr),e(mM,ipr),e(mM,BG),e(BG,dpr),e(mM,cpr),e(me,mpr),e(me,fM),e(fM,ype),e(ype,fpr),e(fM,gpr),e(fM,xG),e(xG,hpr),e(fM,upr),e(me,ppr),e(me,gM),e(gM,wpe),e(wpe,_pr),e(gM,bpr),e(gM,kG),e(kG,vpr),e(gM,Tpr),e(me,Fpr),e(me,hM),e(hM,Ape),e(Ape,Cpr),e(hM,Mpr),e(hM,RG),e(RG,Epr),e(hM,ypr),e(me,wpr),e(me,uM),e(uM,Lpe),e(Lpe,Apr),e(uM,Lpr),e(uM,SG),e(SG,Bpr),e(uM,xpr),e(me,kpr),e(me,pM),e(pM,Bpe),e(Bpe,Rpr),e(pM,Spr),e(pM,PG),e(PG,Ppr),e(pM,$pr),e(me,Ipr),e(me,_M),e(_M,xpe),e(xpe,jpr),e(_M,Npr),e(_M,$G),e($G,Dpr),e(_M,qpr),e(me,Gpr),e(me,bM),e(bM,kpe),e(kpe,Opr),e(bM,Xpr),e(bM,IG),e(IG,zpr),e(bM,Vpr),e(wo,Wpr),e(wo,Rpe),e(Rpe,Qpr),e(wo,Hpr),g(H0,wo,null),b(d,V7e,_),b(d,Ic,_),e(Ic,vM),e(vM,Spe),g(U0,Spe,null),e(Ic,Upr),e(Ic,Ppe),e(Ppe,Jpr),b(d,W7e,_),b(d,yr,_),g(J0,yr,null),e(yr,Ypr),e(yr,jc),e(jc,Kpr),e(jc,$pe),e($pe,Zpr),e(jc,e_r),e(jc,Ipe),e(Ipe,o_r),e(jc,r_r),e(yr,t_r),e(yr,Y0),e(Y0,a_r),e(Y0,jpe),e(jpe,s_r),e(Y0,n_r),e(yr,l_r),e(yr,_t),g(K0,_t,null),e(_t,i_r),e(_t,Npe),e(Npe,d_r),e(_t,c_r),e(_t,Nc),e(Nc,m_r),e(Nc,Dpe),e(Dpe,f_r),e(Nc,g_r),e(Nc,qpe),e(qpe,h_r),e(Nc,u_r),e(_t,p_r),e(_t,Gpe),e(Gpe,__r),e(_t,b_r),g(Z0,_t,null),e(yr,v_r),e(yr,Ao),g(e6,Ao,null),e(Ao,T_r),e(Ao,Ope),e(Ope,F_r),e(Ao,C_r),e(Ao,vs),e(vs,M_r),e(vs,Xpe),e(Xpe,E_r),e(vs,y_r),e(vs,zpe),e(zpe,w_r),e(vs,A_r),e(vs,Vpe),e(Vpe,L_r),e(vs,B_r),e(Ao,x_r),e(Ao,be),e(be,TM),e(TM,Wpe),e(Wpe,k_r),e(TM,R_r),e(TM,jG),e(jG,S_r),e(TM,P_r),e(be,$_r),e(be,FM),e(FM,Qpe),e(Qpe,I_r),e(FM,j_r),e(FM,NG),e(NG,N_r),e(FM,D_r),e(be,q_r),e(be,CM),e(CM,Hpe),e(Hpe,G_r),e(CM,O_r),e(CM,DG),e(DG,X_r),e(CM,z_r),e(be,V_r),e(be,MM),e(MM,Upe),e(Upe,W_r),e(MM,Q_r),e(MM,qG),e(qG,H_r),e(MM,U_r),e(be,J_r),e(be,EM),e(EM,Jpe),e(Jpe,Y_r),e(EM,K_r),e(EM,GG),e(GG,Z_r),e(EM,ebr),e(be,obr),e(be,yM),e(yM,Ype),e(Ype,rbr),e(yM,tbr),e(yM,OG),e(OG,abr),e(yM,sbr),e(be,nbr),e(be,wM),e(wM,Kpe),e(Kpe,lbr),e(wM,ibr),e(wM,XG),e(XG,dbr),e(wM,cbr),e(be,mbr),e(be,AM),e(AM,Zpe),e(Zpe,fbr),e(AM,gbr),e(AM,zG),e(zG,hbr),e(AM,ubr),e(be,pbr),e(be,LM),e(LM,e_e),e(e_e,_br),e(LM,bbr),e(LM,VG),e(VG,vbr),e(LM,Tbr),e(Ao,Fbr),e(Ao,o_e),e(o_e,Cbr),e(Ao,Mbr),g(o6,Ao,null),b(d,Q7e,_),b(d,Dc,_),e(Dc,BM),e(BM,r_e),g(r6,r_e,null),e(Dc,Ebr),e(Dc,t_e),e(t_e,ybr),b(d,H7e,_),b(d,wr,_),g(t6,wr,null),e(wr,wbr),e(wr,qc),e(qc,Abr),e(qc,a_e),e(a_e,Lbr),e(qc,Bbr),e(qc,s_e),e(s_e,xbr),e(qc,kbr),e(wr,Rbr),e(wr,a6),e(a6,Sbr),e(a6,n_e),e(n_e,Pbr),e(a6,$br),e(wr,Ibr),e(wr,bt),g(s6,bt,null),e(bt,jbr),e(bt,l_e),e(l_e,Nbr),e(bt,Dbr),e(bt,Gc),e(Gc,qbr),e(Gc,i_e),e(i_e,Gbr),e(Gc,Obr),e(Gc,d_e),e(d_e,Xbr),e(Gc,zbr),e(bt,Vbr),e(bt,c_e),e(c_e,Wbr),e(bt,Qbr),g(n6,bt,null),e(wr,Hbr),e(wr,Lo),g(l6,Lo,null),e(Lo,Ubr),e(Lo,m_e),e(m_e,Jbr),e(Lo,Ybr),e(Lo,Ts),e(Ts,Kbr),e(Ts,f_e),e(f_e,Zbr),e(Ts,e2r),e(Ts,g_e),e(g_e,o2r),e(Ts,r2r),e(Ts,h_e),e(h_e,t2r),e(Ts,a2r),e(Lo,s2r),e(Lo,ve),e(ve,xM),e(xM,u_e),e(u_e,n2r),e(xM,l2r),e(xM,WG),e(WG,i2r),e(xM,d2r),e(ve,c2r),e(ve,kM),e(kM,p_e),e(p_e,m2r),e(kM,f2r),e(kM,QG),e(QG,g2r),e(kM,h2r),e(ve,u2r),e(ve,RM),e(RM,__e),e(__e,p2r),e(RM,_2r),e(RM,HG),e(HG,b2r),e(RM,v2r),e(ve,T2r),e(ve,SM),e(SM,b_e),e(b_e,F2r),e(SM,C2r),e(SM,UG),e(UG,M2r),e(SM,E2r),e(ve,y2r),e(ve,PM),e(PM,v_e),e(v_e,w2r),e(PM,A2r),e(PM,JG),e(JG,L2r),e(PM,B2r),e(ve,x2r),e(ve,$M),e($M,T_e),e(T_e,k2r),e($M,R2r),e($M,YG),e(YG,S2r),e($M,P2r),e(ve,$2r),e(ve,IM),e(IM,F_e),e(F_e,I2r),e(IM,j2r),e(IM,KG),e(KG,N2r),e(IM,D2r),e(ve,q2r),e(ve,jM),e(jM,C_e),e(C_e,G2r),e(jM,O2r),e(jM,ZG),e(ZG,X2r),e(jM,z2r),e(ve,V2r),e(ve,NM),e(NM,M_e),e(M_e,W2r),e(NM,Q2r),e(NM,eO),e(eO,H2r),e(NM,U2r),e(Lo,J2r),e(Lo,E_e),e(E_e,Y2r),e(Lo,K2r),g(i6,Lo,null),b(d,U7e,_),b(d,Oc,_),e(Oc,DM),e(DM,y_e),g(d6,y_e,null),e(Oc,Z2r),e(Oc,w_e),e(w_e,evr),b(d,J7e,_),b(d,Ar,_),g(c6,Ar,null),e(Ar,ovr),e(Ar,Xc),e(Xc,rvr),e(Xc,A_e),e(A_e,tvr),e(Xc,avr),e(Xc,L_e),e(L_e,svr),e(Xc,nvr),e(Ar,lvr),e(Ar,m6),e(m6,ivr),e(m6,B_e),e(B_e,dvr),e(m6,cvr),e(Ar,mvr),e(Ar,vt),g(f6,vt,null),e(vt,fvr),e(vt,x_e),e(x_e,gvr),e(vt,hvr),e(vt,zc),e(zc,uvr),e(zc,k_e),e(k_e,pvr),e(zc,_vr),e(zc,R_e),e(R_e,bvr),e(zc,vvr),e(vt,Tvr),e(vt,S_e),e(S_e,Fvr),e(vt,Cvr),g(g6,vt,null),e(Ar,Mvr),e(Ar,Bo),g(h6,Bo,null),e(Bo,Evr),e(Bo,P_e),e(P_e,yvr),e(Bo,wvr),e(Bo,Fs),e(Fs,Avr),e(Fs,$_e),e($_e,Lvr),e(Fs,Bvr),e(Fs,I_e),e(I_e,xvr),e(Fs,kvr),e(Fs,j_e),e(j_e,Rvr),e(Fs,Svr),e(Bo,Pvr),e(Bo,Te),e(Te,qM),e(qM,N_e),e(N_e,$vr),e(qM,Ivr),e(qM,oO),e(oO,jvr),e(qM,Nvr),e(Te,Dvr),e(Te,GM),e(GM,D_e),e(D_e,qvr),e(GM,Gvr),e(GM,rO),e(rO,Ovr),e(GM,Xvr),e(Te,zvr),e(Te,OM),e(OM,q_e),e(q_e,Vvr),e(OM,Wvr),e(OM,tO),e(tO,Qvr),e(OM,Hvr),e(Te,Uvr),e(Te,XM),e(XM,G_e),e(G_e,Jvr),e(XM,Yvr),e(XM,aO),e(aO,Kvr),e(XM,Zvr),e(Te,eTr),e(Te,zM),e(zM,O_e),e(O_e,oTr),e(zM,rTr),e(zM,sO),e(sO,tTr),e(zM,aTr),e(Te,sTr),e(Te,VM),e(VM,X_e),e(X_e,nTr),e(VM,lTr),e(VM,nO),e(nO,iTr),e(VM,dTr),e(Te,cTr),e(Te,WM),e(WM,z_e),e(z_e,mTr),e(WM,fTr),e(WM,lO),e(lO,gTr),e(WM,hTr),e(Te,uTr),e(Te,QM),e(QM,V_e),e(V_e,pTr),e(QM,_Tr),e(QM,iO),e(iO,bTr),e(QM,vTr),e(Te,TTr),e(Te,HM),e(HM,W_e),e(W_e,FTr),e(HM,CTr),e(HM,dO),e(dO,MTr),e(HM,ETr),e(Bo,yTr),e(Bo,Q_e),e(Q_e,wTr),e(Bo,ATr),g(u6,Bo,null),b(d,Y7e,_),b(d,Vc,_),e(Vc,UM),e(UM,H_e),g(p6,H_e,null),e(Vc,LTr),e(Vc,U_e),e(U_e,BTr),b(d,K7e,_),b(d,Lr,_),g(_6,Lr,null),e(Lr,xTr),e(Lr,Wc),e(Wc,kTr),e(Wc,J_e),e(J_e,RTr),e(Wc,STr),e(Wc,Y_e),e(Y_e,PTr),e(Wc,$Tr),e(Lr,ITr),e(Lr,b6),e(b6,jTr),e(b6,K_e),e(K_e,NTr),e(b6,DTr),e(Lr,qTr),e(Lr,Tt),g(v6,Tt,null),e(Tt,GTr),e(Tt,Z_e),e(Z_e,OTr),e(Tt,XTr),e(Tt,Qc),e(Qc,zTr),e(Qc,ebe),e(ebe,VTr),e(Qc,WTr),e(Qc,obe),e(obe,QTr),e(Qc,HTr),e(Tt,UTr),e(Tt,rbe),e(rbe,JTr),e(Tt,YTr),g(T6,Tt,null),e(Lr,KTr),e(Lr,xo),g(F6,xo,null),e(xo,ZTr),e(xo,tbe),e(tbe,e1r),e(xo,o1r),e(xo,Cs),e(Cs,r1r),e(Cs,abe),e(abe,t1r),e(Cs,a1r),e(Cs,sbe),e(sbe,s1r),e(Cs,n1r),e(Cs,nbe),e(nbe,l1r),e(Cs,i1r),e(xo,d1r),e(xo,Fe),e(Fe,JM),e(JM,lbe),e(lbe,c1r),e(JM,m1r),e(JM,cO),e(cO,f1r),e(JM,g1r),e(Fe,h1r),e(Fe,YM),e(YM,ibe),e(ibe,u1r),e(YM,p1r),e(YM,mO),e(mO,_1r),e(YM,b1r),e(Fe,v1r),e(Fe,KM),e(KM,dbe),e(dbe,T1r),e(KM,F1r),e(KM,fO),e(fO,C1r),e(KM,M1r),e(Fe,E1r),e(Fe,ZM),e(ZM,cbe),e(cbe,y1r),e(ZM,w1r),e(ZM,gO),e(gO,A1r),e(ZM,L1r),e(Fe,B1r),e(Fe,eE),e(eE,mbe),e(mbe,x1r),e(eE,k1r),e(eE,hO),e(hO,R1r),e(eE,S1r),e(Fe,P1r),e(Fe,oE),e(oE,fbe),e(fbe,$1r),e(oE,I1r),e(oE,uO),e(uO,j1r),e(oE,N1r),e(Fe,D1r),e(Fe,rE),e(rE,gbe),e(gbe,q1r),e(rE,G1r),e(rE,pO),e(pO,O1r),e(rE,X1r),e(Fe,z1r),e(Fe,tE),e(tE,hbe),e(hbe,V1r),e(tE,W1r),e(tE,_O),e(_O,Q1r),e(tE,H1r),e(Fe,U1r),e(Fe,aE),e(aE,ube),e(ube,J1r),e(aE,Y1r),e(aE,bO),e(bO,K1r),e(aE,Z1r),e(xo,eFr),e(xo,pbe),e(pbe,oFr),e(xo,rFr),g(C6,xo,null),b(d,Z7e,_),b(d,Hc,_),e(Hc,sE),e(sE,_be),g(M6,_be,null),e(Hc,tFr),e(Hc,bbe),e(bbe,aFr),b(d,e8e,_),b(d,Br,_),g(E6,Br,null),e(Br,sFr),e(Br,Uc),e(Uc,nFr),e(Uc,vbe),e(vbe,lFr),e(Uc,iFr),e(Uc,Tbe),e(Tbe,dFr),e(Uc,cFr),e(Br,mFr),e(Br,y6),e(y6,fFr),e(y6,Fbe),e(Fbe,gFr),e(y6,hFr),e(Br,uFr),e(Br,Ft),g(w6,Ft,null),e(Ft,pFr),e(Ft,Cbe),e(Cbe,_Fr),e(Ft,bFr),e(Ft,Jc),e(Jc,vFr),e(Jc,Mbe),e(Mbe,TFr),e(Jc,FFr),e(Jc,Ebe),e(Ebe,CFr),e(Jc,MFr),e(Ft,EFr),e(Ft,ybe),e(ybe,yFr),e(Ft,wFr),g(A6,Ft,null),e(Br,AFr),e(Br,ko),g(L6,ko,null),e(ko,LFr),e(ko,wbe),e(wbe,BFr),e(ko,xFr),e(ko,Ms),e(Ms,kFr),e(Ms,Abe),e(Abe,RFr),e(Ms,SFr),e(Ms,Lbe),e(Lbe,PFr),e(Ms,$Fr),e(Ms,Bbe),e(Bbe,IFr),e(Ms,jFr),e(ko,NFr),e(ko,ao),e(ao,nE),e(nE,xbe),e(xbe,DFr),e(nE,qFr),e(nE,vO),e(vO,GFr),e(nE,OFr),e(ao,XFr),e(ao,lE),e(lE,kbe),e(kbe,zFr),e(lE,VFr),e(lE,TO),e(TO,WFr),e(lE,QFr),e(ao,HFr),e(ao,iE),e(iE,Rbe),e(Rbe,UFr),e(iE,JFr),e(iE,FO),e(FO,YFr),e(iE,KFr),e(ao,ZFr),e(ao,dE),e(dE,Sbe),e(Sbe,eCr),e(dE,oCr),e(dE,CO),e(CO,rCr),e(dE,tCr),e(ao,aCr),e(ao,cE),e(cE,Pbe),e(Pbe,sCr),e(cE,nCr),e(cE,MO),e(MO,lCr),e(cE,iCr),e(ao,dCr),e(ao,mE),e(mE,$be),e($be,cCr),e(mE,mCr),e(mE,EO),e(EO,fCr),e(mE,gCr),e(ao,hCr),e(ao,fE),e(fE,Ibe),e(Ibe,uCr),e(fE,pCr),e(fE,yO),e(yO,_Cr),e(fE,bCr),e(ko,vCr),e(ko,jbe),e(jbe,TCr),e(ko,FCr),g(B6,ko,null),b(d,o8e,_),b(d,Yc,_),e(Yc,gE),e(gE,Nbe),g(x6,Nbe,null),e(Yc,CCr),e(Yc,Dbe),e(Dbe,MCr),b(d,r8e,_),b(d,xr,_),g(k6,xr,null),e(xr,ECr),e(xr,Kc),e(Kc,yCr),e(Kc,qbe),e(qbe,wCr),e(Kc,ACr),e(Kc,Gbe),e(Gbe,LCr),e(Kc,BCr),e(xr,xCr),e(xr,R6),e(R6,kCr),e(R6,Obe),e(Obe,RCr),e(R6,SCr),e(xr,PCr),e(xr,Ct),g(S6,Ct,null),e(Ct,$Cr),e(Ct,Xbe),e(Xbe,ICr),e(Ct,jCr),e(Ct,Zc),e(Zc,NCr),e(Zc,zbe),e(zbe,DCr),e(Zc,qCr),e(Zc,Vbe),e(Vbe,GCr),e(Zc,OCr),e(Ct,XCr),e(Ct,Wbe),e(Wbe,zCr),e(Ct,VCr),g(P6,Ct,null),e(xr,WCr),e(xr,Ro),g($6,Ro,null),e(Ro,QCr),e(Ro,Qbe),e(Qbe,HCr),e(Ro,UCr),e(Ro,Es),e(Es,JCr),e(Es,Hbe),e(Hbe,YCr),e(Es,KCr),e(Es,Ube),e(Ube,ZCr),e(Es,e4r),e(Es,Jbe),e(Jbe,o4r),e(Es,r4r),e(Ro,t4r),e(Ro,so),e(so,hE),e(hE,Ybe),e(Ybe,a4r),e(hE,s4r),e(hE,wO),e(wO,n4r),e(hE,l4r),e(so,i4r),e(so,uE),e(uE,Kbe),e(Kbe,d4r),e(uE,c4r),e(uE,AO),e(AO,m4r),e(uE,f4r),e(so,g4r),e(so,pE),e(pE,Zbe),e(Zbe,h4r),e(pE,u4r),e(pE,LO),e(LO,p4r),e(pE,_4r),e(so,b4r),e(so,_E),e(_E,e2e),e(e2e,v4r),e(_E,T4r),e(_E,BO),e(BO,F4r),e(_E,C4r),e(so,M4r),e(so,bE),e(bE,o2e),e(o2e,E4r),e(bE,y4r),e(bE,xO),e(xO,w4r),e(bE,A4r),e(so,L4r),e(so,vE),e(vE,r2e),e(r2e,B4r),e(vE,x4r),e(vE,kO),e(kO,k4r),e(vE,R4r),e(so,S4r),e(so,TE),e(TE,t2e),e(t2e,P4r),e(TE,$4r),e(TE,RO),e(RO,I4r),e(TE,j4r),e(Ro,N4r),e(Ro,a2e),e(a2e,D4r),e(Ro,q4r),g(I6,Ro,null),b(d,t8e,_),b(d,em,_),e(em,FE),e(FE,s2e),g(j6,s2e,null),e(em,G4r),e(em,n2e),e(n2e,O4r),b(d,a8e,_),b(d,kr,_),g(N6,kr,null),e(kr,X4r),e(kr,om),e(om,z4r),e(om,l2e),e(l2e,V4r),e(om,W4r),e(om,i2e),e(i2e,Q4r),e(om,H4r),e(kr,U4r),e(kr,D6),e(D6,J4r),e(D6,d2e),e(d2e,Y4r),e(D6,K4r),e(kr,Z4r),e(kr,Mt),g(q6,Mt,null),e(Mt,eMr),e(Mt,c2e),e(c2e,oMr),e(Mt,rMr),e(Mt,rm),e(rm,tMr),e(rm,m2e),e(m2e,aMr),e(rm,sMr),e(rm,f2e),e(f2e,nMr),e(rm,lMr),e(Mt,iMr),e(Mt,g2e),e(g2e,dMr),e(Mt,cMr),g(G6,Mt,null),e(kr,mMr),e(kr,So),g(O6,So,null),e(So,fMr),e(So,h2e),e(h2e,gMr),e(So,hMr),e(So,ys),e(ys,uMr),e(ys,u2e),e(u2e,pMr),e(ys,_Mr),e(ys,p2e),e(p2e,bMr),e(ys,vMr),e(ys,_2e),e(_2e,TMr),e(ys,FMr),e(So,CMr),e(So,b2e),e(b2e,CE),e(CE,v2e),e(v2e,MMr),e(CE,EMr),e(CE,SO),e(SO,yMr),e(CE,wMr),e(So,AMr),e(So,T2e),e(T2e,LMr),e(So,BMr),g(X6,So,null),b(d,s8e,_),b(d,tm,_),e(tm,ME),e(ME,F2e),g(z6,F2e,null),e(tm,xMr),e(tm,C2e),e(C2e,kMr),b(d,n8e,_),b(d,Rr,_),g(V6,Rr,null),e(Rr,RMr),e(Rr,am),e(am,SMr),e(am,M2e),e(M2e,PMr),e(am,$Mr),e(am,E2e),e(E2e,IMr),e(am,jMr),e(Rr,NMr),e(Rr,W6),e(W6,DMr),e(W6,y2e),e(y2e,qMr),e(W6,GMr),e(Rr,OMr),e(Rr,Et),g(Q6,Et,null),e(Et,XMr),e(Et,w2e),e(w2e,zMr),e(Et,VMr),e(Et,sm),e(sm,WMr),e(sm,A2e),e(A2e,QMr),e(sm,HMr),e(sm,L2e),e(L2e,UMr),e(sm,JMr),e(Et,YMr),e(Et,B2e),e(B2e,KMr),e(Et,ZMr),g(H6,Et,null),e(Rr,eEr),e(Rr,Po),g(U6,Po,null),e(Po,oEr),e(Po,x2e),e(x2e,rEr),e(Po,tEr),e(Po,ws),e(ws,aEr),e(ws,k2e),e(k2e,sEr),e(ws,nEr),e(ws,R2e),e(R2e,lEr),e(ws,iEr),e(ws,S2e),e(S2e,dEr),e(ws,cEr),e(Po,mEr),e(Po,J6),e(J6,EE),e(EE,P2e),e(P2e,fEr),e(EE,gEr),e(EE,PO),e(PO,hEr),e(EE,uEr),e(J6,pEr),e(J6,yE),e(yE,$2e),e($2e,_Er),e(yE,bEr),e(yE,$O),e($O,vEr),e(yE,TEr),e(Po,FEr),e(Po,I2e),e(I2e,CEr),e(Po,MEr),g(Y6,Po,null),b(d,l8e,_),b(d,nm,_),e(nm,wE),e(wE,j2e),g(K6,j2e,null),e(nm,EEr),e(nm,N2e),e(N2e,yEr),b(d,i8e,_),b(d,Sr,_),g(Z6,Sr,null),e(Sr,wEr),e(Sr,lm),e(lm,AEr),e(lm,D2e),e(D2e,LEr),e(lm,BEr),e(lm,q2e),e(q2e,xEr),e(lm,kEr),e(Sr,REr),e(Sr,eL),e(eL,SEr),e(eL,G2e),e(G2e,PEr),e(eL,$Er),e(Sr,IEr),e(Sr,yt),g(oL,yt,null),e(yt,jEr),e(yt,O2e),e(O2e,NEr),e(yt,DEr),e(yt,im),e(im,qEr),e(im,X2e),e(X2e,GEr),e(im,OEr),e(im,z2e),e(z2e,XEr),e(im,zEr),e(yt,VEr),e(yt,V2e),e(V2e,WEr),e(yt,QEr),g(rL,yt,null),e(Sr,HEr),e(Sr,$o),g(tL,$o,null),e($o,UEr),e($o,W2e),e(W2e,JEr),e($o,YEr),e($o,As),e(As,KEr),e(As,Q2e),e(Q2e,ZEr),e(As,e3r),e(As,H2e),e(H2e,o3r),e(As,r3r),e(As,U2e),e(U2e,t3r),e(As,a3r),e($o,s3r),e($o,J2e),e(J2e,AE),e(AE,Y2e),e(Y2e,n3r),e(AE,l3r),e(AE,IO),e(IO,i3r),e(AE,d3r),e($o,c3r),e($o,K2e),e(K2e,m3r),e($o,f3r),g(aL,$o,null),d8e=!0},p(d,[_]){const sL={};_&2&&(sL.$$scope={dirty:_,ctx:d}),um.$set(sL);const Z2e={};_&2&&(Z2e.$$scope={dirty:_,ctx:d}),Vg.$set(Z2e);const eve={};_&2&&(eve.$$scope={dirty:_,ctx:d}),oh.$set(eve)},i(d){d8e||(h(ce.$$.fragment,d),h(ka.$$.fragment,d),h(x3.$$.fragment,d),h(k3.$$.fragment,d),h(um.$$.fragment,d),h(R3.$$.fragment,d),h(S3.$$.fragment,d),h(I3.$$.fragment,d),h(j3.$$.fragment,d),h(N3.$$.fragment,d),h(D3.$$.fragment,d),h(q3.$$.fragment,d),h(X3.$$.fragment,d),h(z3.$$.fragment,d),h(V3.$$.fragment,d),h(W3.$$.fragment,d),h(Q3.$$.fragment,d),h(J3.$$.fragment,d),h(Vg.$$.fragment,d),h(Y3.$$.fragment,d),h(K3.$$.fragment,d),h(Z3.$$.fragment,d),h(r5.$$.fragment,d),h(oh.$$.fragment,d),h(t5.$$.fragment,d),h(a5.$$.fragment,d),h(s5.$$.fragment,d),h(l5.$$.fragment,d),h(i5.$$.fragment,d),h(d5.$$.fragment,d),h(c5.$$.fragment,d),h(m5.$$.fragment,d),h(f5.$$.fragment,d),h(h5.$$.fragment,d),h(u5.$$.fragment,d),h(p5.$$.fragment,d),h(_5.$$.fragment,d),h(b5.$$.fragment,d),h(v5.$$.fragment,d),h(F5.$$.fragment,d),h(C5.$$.fragment,d),h(M5.$$.fragment,d),h(E5.$$.fragment,d),h(y5.$$.fragment,d),h(w5.$$.fragment,d),h(L5.$$.fragment,d),h(B5.$$.fragment,d),h(x5.$$.fragment,d),h(k5.$$.fragment,d),h(R5.$$.fragment,d),h(S5.$$.fragment,d),h($5.$$.fragment,d),h(I5.$$.fragment,d),h(j5.$$.fragment,d),h(N5.$$.fragment,d),h(D5.$$.fragment,d),h(q5.$$.fragment,d),h(O5.$$.fragment,d),h(X5.$$.fragment,d),h(z5.$$.fragment,d),h(V5.$$.fragment,d),h(W5.$$.fragment,d),h(Q5.$$.fragment,d),h(U5.$$.fragment,d),h(J5.$$.fragment,d),h(Y5.$$.fragment,d),h(K5.$$.fragment,d),h(Z5.$$.fragment,d),h(ey.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(ay.$$.fragment,d),h(sy.$$.fragment,d),h(ny.$$.fragment,d),h(ly.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(my.$$.fragment,d),h(fy.$$.fragment,d),h(gy.$$.fragment,d),h(hy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(by.$$.fragment,d),h(vy.$$.fragment,d),h(Ty.$$.fragment,d),h(Fy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(yy.$$.fragment,d),h(wy.$$.fragment,d),h(Ay.$$.fragment,d),h(Ly.$$.fragment,d),h(xy.$$.fragment,d),h(ky.$$.fragment,d),h(Ry.$$.fragment,d),h(Sy.$$.fragment,d),h(Py.$$.fragment,d),h($y.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(Dy.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Oy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Wy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Uy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(Zy.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(rw.$$.fragment,d),h(aw.$$.fragment,d),h(sw.$$.fragment,d),h(nw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(mw.$$.fragment,d),h(fw.$$.fragment,d),h(gw.$$.fragment,d),h(uw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(vw.$$.fragment,d),h(Tw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(ww.$$.fragment,d),h(Aw.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(xw.$$.fragment,d),h(kw.$$.fragment,d),h(Sw.$$.fragment,d),h(Pw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(qw.$$.fragment,d),h(Gw.$$.fragment,d),h(Ow.$$.fragment,d),h(zw.$$.fragment,d),h(Vw.$$.fragment,d),h(Ww.$$.fragment,d),h(Hw.$$.fragment,d),h(Uw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(oA.$$.fragment,d),h(rA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(sA.$$.fragment,d),h(nA.$$.fragment,d),h(iA.$$.fragment,d),h(dA.$$.fragment,d),h(cA.$$.fragment,d),h(mA.$$.fragment,d),h(fA.$$.fragment,d),h(gA.$$.fragment,d),h(uA.$$.fragment,d),h(pA.$$.fragment,d),h(_A.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(CA.$$.fragment,d),h(MA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(AA.$$.fragment,d),h(BA.$$.fragment,d),h(xA.$$.fragment,d),h(kA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h(PA.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(NA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(XA.$$.fragment,d),h(zA.$$.fragment,d),h(VA.$$.fragment,d),h(WA.$$.fragment,d),h(QA.$$.fragment,d),h(HA.$$.fragment,d),h(JA.$$.fragment,d),h(YA.$$.fragment,d),h(KA.$$.fragment,d),h(ZA.$$.fragment,d),h(e0.$$.fragment,d),h(o0.$$.fragment,d),h(t0.$$.fragment,d),h(a0.$$.fragment,d),h(s0.$$.fragment,d),h(n0.$$.fragment,d),h(l0.$$.fragment,d),h(i0.$$.fragment,d),h(c0.$$.fragment,d),h(m0.$$.fragment,d),h(f0.$$.fragment,d),h(g0.$$.fragment,d),h(h0.$$.fragment,d),h(u0.$$.fragment,d),h(_0.$$.fragment,d),h(b0.$$.fragment,d),h(v0.$$.fragment,d),h(T0.$$.fragment,d),h(F0.$$.fragment,d),h(C0.$$.fragment,d),h(E0.$$.fragment,d),h(y0.$$.fragment,d),h(w0.$$.fragment,d),h(A0.$$.fragment,d),h(L0.$$.fragment,d),h(B0.$$.fragment,d),h(k0.$$.fragment,d),h(R0.$$.fragment,d),h(S0.$$.fragment,d),h(P0.$$.fragment,d),h($0.$$.fragment,d),h(I0.$$.fragment,d),h(N0.$$.fragment,d),h(D0.$$.fragment,d),h(q0.$$.fragment,d),h(G0.$$.fragment,d),h(O0.$$.fragment,d),h(X0.$$.fragment,d),h(V0.$$.fragment,d),h(W0.$$.fragment,d),h(Q0.$$.fragment,d),h(H0.$$.fragment,d),h(U0.$$.fragment,d),h(J0.$$.fragment,d),h(K0.$$.fragment,d),h(Z0.$$.fragment,d),h(e6.$$.fragment,d),h(o6.$$.fragment,d),h(r6.$$.fragment,d),h(t6.$$.fragment,d),h(s6.$$.fragment,d),h(n6.$$.fragment,d),h(l6.$$.fragment,d),h(i6.$$.fragment,d),h(d6.$$.fragment,d),h(c6.$$.fragment,d),h(f6.$$.fragment,d),h(g6.$$.fragment,d),h(h6.$$.fragment,d),h(u6.$$.fragment,d),h(p6.$$.fragment,d),h(_6.$$.fragment,d),h(v6.$$.fragment,d),h(T6.$$.fragment,d),h(F6.$$.fragment,d),h(C6.$$.fragment,d),h(M6.$$.fragment,d),h(E6.$$.fragment,d),h(w6.$$.fragment,d),h(A6.$$.fragment,d),h(L6.$$.fragment,d),h(B6.$$.fragment,d),h(x6.$$.fragment,d),h(k6.$$.fragment,d),h(S6.$$.fragment,d),h(P6.$$.fragment,d),h($6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(N6.$$.fragment,d),h(q6.$$.fragment,d),h(G6.$$.fragment,d),h(O6.$$.fragment,d),h(X6.$$.fragment,d),h(z6.$$.fragment,d),h(V6.$$.fragment,d),h(Q6.$$.fragment,d),h(H6.$$.fragment,d),h(U6.$$.fragment,d),h(Y6.$$.fragment,d),h(K6.$$.fragment,d),h(Z6.$$.fragment,d),h(oL.$$.fragment,d),h(rL.$$.fragment,d),h(tL.$$.fragment,d),h(aL.$$.fragment,d),d8e=!0)},o(d){u(ce.$$.fragment,d),u(ka.$$.fragment,d),u(x3.$$.fragment,d),u(k3.$$.fragment,d),u(um.$$.fragment,d),u(R3.$$.fragment,d),u(S3.$$.fragment,d),u(I3.$$.fragment,d),u(j3.$$.fragment,d),u(N3.$$.fragment,d),u(D3.$$.fragment,d),u(q3.$$.fragment,d),u(X3.$$.fragment,d),u(z3.$$.fragment,d),u(V3.$$.fragment,d),u(W3.$$.fragment,d),u(Q3.$$.fragment,d),u(J3.$$.fragment,d),u(Vg.$$.fragment,d),u(Y3.$$.fragment,d),u(K3.$$.fragment,d),u(Z3.$$.fragment,d),u(r5.$$.fragment,d),u(oh.$$.fragment,d),u(t5.$$.fragment,d),u(a5.$$.fragment,d),u(s5.$$.fragment,d),u(l5.$$.fragment,d),u(i5.$$.fragment,d),u(d5.$$.fragment,d),u(c5.$$.fragment,d),u(m5.$$.fragment,d),u(f5.$$.fragment,d),u(h5.$$.fragment,d),u(u5.$$.fragment,d),u(p5.$$.fragment,d),u(_5.$$.fragment,d),u(b5.$$.fragment,d),u(v5.$$.fragment,d),u(F5.$$.fragment,d),u(C5.$$.fragment,d),u(M5.$$.fragment,d),u(E5.$$.fragment,d),u(y5.$$.fragment,d),u(w5.$$.fragment,d),u(L5.$$.fragment,d),u(B5.$$.fragment,d),u(x5.$$.fragment,d),u(k5.$$.fragment,d),u(R5.$$.fragment,d),u(S5.$$.fragment,d),u($5.$$.fragment,d),u(I5.$$.fragment,d),u(j5.$$.fragment,d),u(N5.$$.fragment,d),u(D5.$$.fragment,d),u(q5.$$.fragment,d),u(O5.$$.fragment,d),u(X5.$$.fragment,d),u(z5.$$.fragment,d),u(V5.$$.fragment,d),u(W5.$$.fragment,d),u(Q5.$$.fragment,d),u(U5.$$.fragment,d),u(J5.$$.fragment,d),u(Y5.$$.fragment,d),u(K5.$$.fragment,d),u(Z5.$$.fragment,d),u(ey.$$.fragment,d),u(ry.$$.fragment,d),u(ty.$$.fragment,d),u(ay.$$.fragment,d),u(sy.$$.fragment,d),u(ny.$$.fragment,d),u(ly.$$.fragment,d),u(dy.$$.fragment,d),u(cy.$$.fragment,d),u(my.$$.fragment,d),u(fy.$$.fragment,d),u(gy.$$.fragment,d),u(hy.$$.fragment,d),u(py.$$.fragment,d),u(_y.$$.fragment,d),u(by.$$.fragment,d),u(vy.$$.fragment,d),u(Ty.$$.fragment,d),u(Fy.$$.fragment,d),u(My.$$.fragment,d),u(Ey.$$.fragment,d),u(yy.$$.fragment,d),u(wy.$$.fragment,d),u(Ay.$$.fragment,d),u(Ly.$$.fragment,d),u(xy.$$.fragment,d),u(ky.$$.fragment,d),u(Ry.$$.fragment,d),u(Sy.$$.fragment,d),u(Py.$$.fragment,d),u($y.$$.fragment,d),u(jy.$$.fragment,d),u(Ny.$$.fragment,d),u(Dy.$$.fragment,d),u(qy.$$.fragment,d),u(Gy.$$.fragment,d),u(Oy.$$.fragment,d),u(zy.$$.fragment,d),u(Vy.$$.fragment,d),u(Wy.$$.fragment,d),u(Qy.$$.fragment,d),u(Hy.$$.fragment,d),u(Uy.$$.fragment,d),u(Yy.$$.fragment,d),u(Ky.$$.fragment,d),u(Zy.$$.fragment,d),u(ew.$$.fragment,d),u(ow.$$.fragment,d),u(rw.$$.fragment,d),u(aw.$$.fragment,d),u(sw.$$.fragment,d),u(nw.$$.fragment,d),u(lw.$$.fragment,d),u(iw.$$.fragment,d),u(dw.$$.fragment,d),u(mw.$$.fragment,d),u(fw.$$.fragment,d),u(gw.$$.fragment,d),u(uw.$$.fragment,d),u(pw.$$.fragment,d),u(_w.$$.fragment,d),u(vw.$$.fragment,d),u(Tw.$$.fragment,d),u(Fw.$$.fragment,d),u(Cw.$$.fragment,d),u(Mw.$$.fragment,d),u(Ew.$$.fragment,d),u(ww.$$.fragment,d),u(Aw.$$.fragment,d),u(Lw.$$.fragment,d),u(Bw.$$.fragment,d),u(xw.$$.fragment,d),u(kw.$$.fragment,d),u(Sw.$$.fragment,d),u(Pw.$$.fragment,d),u($w.$$.fragment,d),u(Iw.$$.fragment,d),u(jw.$$.fragment,d),u(Nw.$$.fragment,d),u(qw.$$.fragment,d),u(Gw.$$.fragment,d),u(Ow.$$.fragment,d),u(zw.$$.fragment,d),u(Vw.$$.fragment,d),u(Ww.$$.fragment,d),u(Hw.$$.fragment,d),u(Uw.$$.fragment,d),u(Jw.$$.fragment,d),u(Yw.$$.fragment,d),u(Kw.$$.fragment,d),u(Zw.$$.fragment,d),u(oA.$$.fragment,d),u(rA.$$.fragment,d),u(tA.$$.fragment,d),u(aA.$$.fragment,d),u(sA.$$.fragment,d),u(nA.$$.fragment,d),u(iA.$$.fragment,d),u(dA.$$.fragment,d),u(cA.$$.fragment,d),u(mA.$$.fragment,d),u(fA.$$.fragment,d),u(gA.$$.fragment,d),u(uA.$$.fragment,d),u(pA.$$.fragment,d),u(_A.$$.fragment,d),u(bA.$$.fragment,d),u(vA.$$.fragment,d),u(TA.$$.fragment,d),u(CA.$$.fragment,d),u(MA.$$.fragment,d),u(EA.$$.fragment,d),u(yA.$$.fragment,d),u(wA.$$.fragment,d),u(AA.$$.fragment,d),u(BA.$$.fragment,d),u(xA.$$.fragment,d),u(kA.$$.fragment,d),u(RA.$$.fragment,d),u(SA.$$.fragment,d),u(PA.$$.fragment,d),u(IA.$$.fragment,d),u(jA.$$.fragment,d),u(NA.$$.fragment,d),u(DA.$$.fragment,d),u(qA.$$.fragment,d),u(GA.$$.fragment,d),u(XA.$$.fragment,d),u(zA.$$.fragment,d),u(VA.$$.fragment,d),u(WA.$$.fragment,d),u(QA.$$.fragment,d),u(HA.$$.fragment,d),u(JA.$$.fragment,d),u(YA.$$.fragment,d),u(KA.$$.fragment,d),u(ZA.$$.fragment,d),u(e0.$$.fragment,d),u(o0.$$.fragment,d),u(t0.$$.fragment,d),u(a0.$$.fragment,d),u(s0.$$.fragment,d),u(n0.$$.fragment,d),u(l0.$$.fragment,d),u(i0.$$.fragment,d),u(c0.$$.fragment,d),u(m0.$$.fragment,d),u(f0.$$.fragment,d),u(g0.$$.fragment,d),u(h0.$$.fragment,d),u(u0.$$.fragment,d),u(_0.$$.fragment,d),u(b0.$$.fragment,d),u(v0.$$.fragment,d),u(T0.$$.fragment,d),u(F0.$$.fragment,d),u(C0.$$.fragment,d),u(E0.$$.fragment,d),u(y0.$$.fragment,d),u(w0.$$.fragment,d),u(A0.$$.fragment,d),u(L0.$$.fragment,d),u(B0.$$.fragment,d),u(k0.$$.fragment,d),u(R0.$$.fragment,d),u(S0.$$.fragment,d),u(P0.$$.fragment,d),u($0.$$.fragment,d),u(I0.$$.fragment,d),u(N0.$$.fragment,d),u(D0.$$.fragment,d),u(q0.$$.fragment,d),u(G0.$$.fragment,d),u(O0.$$.fragment,d),u(X0.$$.fragment,d),u(V0.$$.fragment,d),u(W0.$$.fragment,d),u(Q0.$$.fragment,d),u(H0.$$.fragment,d),u(U0.$$.fragment,d),u(J0.$$.fragment,d),u(K0.$$.fragment,d),u(Z0.$$.fragment,d),u(e6.$$.fragment,d),u(o6.$$.fragment,d),u(r6.$$.fragment,d),u(t6.$$.fragment,d),u(s6.$$.fragment,d),u(n6.$$.fragment,d),u(l6.$$.fragment,d),u(i6.$$.fragment,d),u(d6.$$.fragment,d),u(c6.$$.fragment,d),u(f6.$$.fragment,d),u(g6.$$.fragment,d),u(h6.$$.fragment,d),u(u6.$$.fragment,d),u(p6.$$.fragment,d),u(_6.$$.fragment,d),u(v6.$$.fragment,d),u(T6.$$.fragment,d),u(F6.$$.fragment,d),u(C6.$$.fragment,d),u(M6.$$.fragment,d),u(E6.$$.fragment,d),u(w6.$$.fragment,d),u(A6.$$.fragment,d),u(L6.$$.fragment,d),u(B6.$$.fragment,d),u(x6.$$.fragment,d),u(k6.$$.fragment,d),u(S6.$$.fragment,d),u(P6.$$.fragment,d),u($6.$$.fragment,d),u(I6.$$.fragment,d),u(j6.$$.fragment,d),u(N6.$$.fragment,d),u(q6.$$.fragment,d),u(G6.$$.fragment,d),u(O6.$$.fragment,d),u(X6.$$.fragment,d),u(z6.$$.fragment,d),u(V6.$$.fragment,d),u(Q6.$$.fragment,d),u(H6.$$.fragment,d),u(U6.$$.fragment,d),u(Y6.$$.fragment,d),u(K6.$$.fragment,d),u(Z6.$$.fragment,d),u(oL.$$.fragment,d),u(rL.$$.fragment,d),u(tL.$$.fragment,d),u(aL.$$.fragment,d),d8e=!1},d(d){t(J),d&&t(Ae),d&&t(le),p(ce),d&&t(cm),d&&t(ra),d&&t(Ee),d&&t(no),d&&t(fm),p(ka,d),d&&t(lo),d&&t(ge),d&&t(Do),d&&t(Ra),d&&t(fLe),d&&t(Ei),p(x3),d&&t(gLe),d&&t(Rs),d&&t(hLe),p(k3,d),d&&t(uLe),d&&t(n7),d&&t(pLe),p(um,d),d&&t(_Le),d&&t(yi),p(R3),d&&t(bLe),d&&t(qo),p(S3),p(I3),p(j3),p(N3),d&&t(vLe),d&&t(Ai),p(D3),d&&t(TLe),d&&t(Go),p(q3),p(X3),p(z3),p(V3),d&&t(FLe),d&&t(Li),p(W3),d&&t(CLe),d&&t(Zt),p(Q3),p(J3),p(Vg),p(Y3),d&&t(MLe),d&&t(Bi),p(K3),d&&t(ELe),d&&t(ea),p(Z3),p(r5),p(oh),p(t5),d&&t(yLe),d&&t(ki),p(a5),d&&t(wLe),d&&t(Oo),p(s5),p(l5),p(i5),p(d5),p(c5),d&&t(ALe),d&&t(Pi),p(m5),d&&t(LLe),d&&t(Xo),p(f5),p(h5),p(u5),p(p5),p(_5),d&&t(BLe),d&&t(ji),p(b5),d&&t(xLe),d&&t(zo),p(v5),p(F5),p(C5),p(M5),p(E5),d&&t(kLe),d&&t(qi),p(y5),d&&t(RLe),d&&t(Vo),p(w5),p(L5),p(B5),p(x5),p(k5),d&&t(SLe),d&&t(Xi),p(R5),d&&t(PLe),d&&t(Wo),p(S5),p($5),p(I5),p(j5),p(N5),d&&t($Le),d&&t(Wi),p(D5),d&&t(ILe),d&&t(Qo),p(q5),p(O5),p(X5),p(z5),p(V5),d&&t(jLe),d&&t(Ui),p(W5),d&&t(NLe),d&&t(Ho),p(Q5),p(U5),p(J5),p(Y5),p(K5),d&&t(DLe),d&&t(Ki),p(Z5),d&&t(qLe),d&&t(Uo),p(ey),p(ry),p(ty),p(ay),p(sy),d&&t(GLe),d&&t(od),p(ny),d&&t(OLe),d&&t(Jo),p(ly),p(dy),p(cy),p(my),p(fy),d&&t(XLe),d&&t(ad),p(gy),d&&t(zLe),d&&t(Yo),p(hy),p(py),p(_y),p(by),p(vy),d&&t(VLe),d&&t(ld),p(Ty),d&&t(WLe),d&&t(Ko),p(Fy),p(My),p(Ey),p(yy),p(wy),d&&t(QLe),d&&t(cd),p(Ay),d&&t(HLe),d&&t(Zo),p(Ly),p(xy),p(ky),p(Ry),p(Sy),d&&t(ULe),d&&t(gd),p(Py),d&&t(JLe),d&&t(er),p($y),p(jy),p(Ny),p(Dy),p(qy),d&&t(YLe),d&&t(pd),p(Gy),d&&t(KLe),d&&t(or),p(Oy),p(zy),p(Vy),p(Wy),p(Qy),d&&t(ZLe),d&&t(vd),p(Hy),d&&t(e7e),d&&t(rr),p(Uy),p(Yy),p(Ky),p(Zy),p(ew),d&&t(o7e),d&&t(Md),p(ow),d&&t(r7e),d&&t(tr),p(rw),p(aw),p(sw),p(nw),p(lw),d&&t(t7e),d&&t(wd),p(iw),d&&t(a7e),d&&t(ar),p(dw),p(mw),p(fw),p(gw),p(uw),d&&t(s7e),d&&t(Bd),p(pw),d&&t(n7e),d&&t(sr),p(_w),p(vw),p(Tw),p(Fw),p(Cw),d&&t(l7e),d&&t(Sd),p(Mw),d&&t(i7e),d&&t(nr),p(Ew),p(ww),p(Aw),p(Lw),p(Bw),d&&t(d7e),d&&t(Id),p(xw),d&&t(c7e),d&&t(lr),p(kw),p(Sw),p(Pw),p($w),p(Iw),d&&t(m7e),d&&t(Dd),p(jw),d&&t(f7e),d&&t(ir),p(Nw),p(qw),p(Gw),p(Ow),p(zw),d&&t(g7e),d&&t(Od),p(Vw),d&&t(h7e),d&&t(dr),p(Ww),p(Hw),p(Uw),p(Jw),p(Yw),d&&t(u7e),d&&t(Vd),p(Kw),d&&t(p7e),d&&t(cr),p(Zw),p(oA),p(rA),p(tA),p(aA),d&&t(_7e),d&&t(Hd),p(sA),d&&t(b7e),d&&t(mr),p(nA),p(iA),p(dA),p(cA),p(mA),d&&t(v7e),d&&t(Yd),p(fA),d&&t(T7e),d&&t(fr),p(gA),p(uA),p(pA),p(_A),p(bA),d&&t(F7e),d&&t(ec),p(vA),d&&t(C7e),d&&t(gr),p(TA),p(CA),p(MA),p(EA),p(yA),d&&t(M7e),d&&t(tc),p(wA),d&&t(E7e),d&&t(hr),p(AA),p(BA),p(xA),p(kA),p(RA),d&&t(y7e),d&&t(nc),p(SA),d&&t(w7e),d&&t(ur),p(PA),p(IA),p(jA),p(NA),p(DA),d&&t(A7e),d&&t(dc),p(qA),d&&t(L7e),d&&t(pr),p(GA),p(XA),p(zA),p(VA),p(WA),d&&t(B7e),d&&t(fc),p(QA),d&&t(x7e),d&&t(_r),p(HA),p(JA),p(YA),p(KA),p(ZA),d&&t(k7e),d&&t(uc),p(e0),d&&t(R7e),d&&t(br),p(o0),p(t0),p(a0),p(s0),p(n0),d&&t(S7e),d&&t(bc),p(l0),d&&t(P7e),d&&t(vr),p(i0),p(c0),p(m0),p(f0),p(g0),d&&t($7e),d&&t(Fc),p(h0),d&&t(I7e),d&&t(Tr),p(u0),p(_0),p(b0),p(v0),p(T0),d&&t(j7e),d&&t(Ec),p(F0),d&&t(N7e),d&&t(Fr),p(C0),p(E0),p(y0),p(w0),p(A0),d&&t(D7e),d&&t(Ac),p(L0),d&&t(q7e),d&&t(Cr),p(B0),p(k0),p(R0),p(S0),p(P0),d&&t(G7e),d&&t(xc),p($0),d&&t(O7e),d&&t(Mr),p(I0),p(N0),p(D0),p(q0),p(G0),d&&t(X7e),d&&t(Sc),p(O0),d&&t(z7e),d&&t(Er),p(X0),p(V0),p(W0),p(Q0),p(H0),d&&t(V7e),d&&t(Ic),p(U0),d&&t(W7e),d&&t(yr),p(J0),p(K0),p(Z0),p(e6),p(o6),d&&t(Q7e),d&&t(Dc),p(r6),d&&t(H7e),d&&t(wr),p(t6),p(s6),p(n6),p(l6),p(i6),d&&t(U7e),d&&t(Oc),p(d6),d&&t(J7e),d&&t(Ar),p(c6),p(f6),p(g6),p(h6),p(u6),d&&t(Y7e),d&&t(Vc),p(p6),d&&t(K7e),d&&t(Lr),p(_6),p(v6),p(T6),p(F6),p(C6),d&&t(Z7e),d&&t(Hc),p(M6),d&&t(e8e),d&&t(Br),p(E6),p(w6),p(A6),p(L6),p(B6),d&&t(o8e),d&&t(Yc),p(x6),d&&t(r8e),d&&t(xr),p(k6),p(S6),p(P6),p($6),p(I6),d&&t(t8e),d&&t(em),p(j6),d&&t(a8e),d&&t(kr),p(N6),p(q6),p(G6),p(O6),p(X6),d&&t(s8e),d&&t(tm),p(z6),d&&t(n8e),d&&t(Rr),p(V6),p(Q6),p(H6),p(U6),p(Y6),d&&t(l8e),d&&t(nm),p(K6),d&&t(i8e),d&&t(Sr),p(Z6),p(oL),p(rL),p(tL),p(aL)}}}const kmt={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function Rmt(pi,J,Ae){let{fw:le}=J;return pi.$$set=fe=>{"fw"in fe&&Ae(0,le=fe.fw)},[le]}class Dmt extends Mmt{constructor(J){super();Emt(this,J,Rmt,xmt,ymt,{fw:0})}}export{Dmt as default,kmt as metadata};
