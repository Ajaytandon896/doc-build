import{S as yvt,i as wvt,s as Avt,e as a,k as l,w as m,t as o,M as Lvt,c as s,d as t,m as i,a as n,x as f,h as r,b as d,F as e,g as b,y as g,q as h,o as u,B as p}from"../../chunks/vendor-4833417e.js";import{T as jLr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-7b52c3d4.js";import{C as w}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as V}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function Bvt(Li){let J,Be,ie,fe,so,ce,_e,Go,Bi,Em,na,xi,ki,M5,ym,ye,io,Ri,$s,E5,Is,Ds,y5,Si,js,w5,Pi,wm,$a;return{c(){J=a("p"),Be=o("If your "),ie=a("code"),fe=o("NewModelConfig"),so=o(" is a subclass of "),ce=a("code"),_e=o("PretrainedConfig"),Go=o(`, make sure its
`),Bi=a("code"),Em=o("model_type"),na=o(" attribute is set to the same key you use when registering the config (here "),xi=a("code"),ki=o('"new-model"'),M5=o(")."),ym=l(),ye=a("p"),io=o("Likewise, if your "),Ri=a("code"),$s=o("NewModel"),E5=o(" is a subclass of "),Is=a("a"),Ds=o("PreTrainedModel"),y5=o(`, make sure its
`),Si=a("code"),js=o("config_class"),w5=o(` attribute is set to the same class you use when registering the model (here
`),Pi=a("code"),wm=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=s(co,"P",{});var ge=n(J);Be=r(ge,"If your "),ie=s(ge,"CODE",{});var d7=n(ie);fe=r(d7,"NewModelConfig"),d7.forEach(t),so=r(ge," is a subclass of "),ce=s(ge,"CODE",{});var $i=n(ce);_e=r($i,"PretrainedConfig"),$i.forEach(t),Go=r(ge,`, make sure its
`),Bi=s(ge,"CODE",{});var c7=n(Bi);Em=r(c7,"model_type"),c7.forEach(t),na=r(ge," attribute is set to the same key you use when registering the config (here "),xi=s(ge,"CODE",{});var m7=n(xi);ki=r(m7,'"new-model"'),m7.forEach(t),M5=r(ge,")."),ge.forEach(t),ym=i(co),ye=s(co,"P",{});var Oo=n(ye);io=r(Oo,"Likewise, if your "),Ri=s(Oo,"CODE",{});var Ia=n(Ri);$s=r(Ia,"NewModel"),Ia.forEach(t),E5=r(Oo," is a subclass of "),Is=s(Oo,"A",{href:!0});var f7=n(Is);Ds=r(f7,"PreTrainedModel"),f7.forEach(t),y5=r(Oo,`, make sure its
`),Si=s(Oo,"CODE",{});var Am=n(Si);js=r(Am,"config_class"),Am.forEach(t),w5=r(Oo,` attribute is set to the same class you use when registering the model (here
`),Pi=s(Oo,"CODE",{});var g7=n(Pi);wm=r(g7,"NewModelConfig"),g7.forEach(t),$a=r(Oo,")."),Oo.forEach(t),this.h()},h(){d(Is,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Be),e(J,ie),e(ie,fe),e(J,so),e(J,ce),e(ce,_e),e(J,Go),e(J,Bi),e(Bi,Em),e(J,na),e(J,xi),e(xi,ki),e(J,M5),b(co,ym,ge),b(co,ye,ge),e(ye,io),e(ye,Ri),e(Ri,$s),e(ye,E5),e(ye,Is),e(Is,Ds),e(ye,y5),e(ye,Si),e(Si,js),e(ye,w5),e(ye,Pi),e(Pi,wm),e(ye,$a)},d(co){co&&t(J),co&&t(ym),co&&t(ye)}}}function xvt(Li){let J,Be,ie,fe,so;return{c(){J=a("p"),Be=o("Passing "),ie=a("code"),fe=o("use_auth_token=True"),so=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Be=r(_e,"Passing "),ie=s(_e,"CODE",{});var Go=n(ie);fe=r(Go,"use_auth_token=True"),Go.forEach(t),so=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Be),e(J,ie),e(ie,fe),e(J,so)},d(ce){ce&&t(J)}}}function kvt(Li){let J,Be,ie,fe,so;return{c(){J=a("p"),Be=o("Passing "),ie=a("code"),fe=o("use_auth_token=True"),so=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Be=r(_e,"Passing "),ie=s(_e,"CODE",{});var Go=n(ie);fe=r(Go,"use_auth_token=True"),Go.forEach(t),so=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Be),e(J,ie),e(ie,fe),e(J,so)},d(ce){ce&&t(J)}}}function Rvt(Li){let J,Be,ie,fe,so,ce,_e,Go,Bi,Em,na,xi,ki,M5,ym,ye,io,Ri,$s,E5,Is,Ds,y5,Si,js,w5,Pi,wm,$a,co,ge,d7,$i,c7,m7,Oo,Ia,f7,Am,g7,qRe,x9e,Ii,Lm,EW,A5,GRe,yW,ORe,k9e,Ns,XRe,wW,VRe,zRe,AW,WRe,QRe,R9e,L5,S9e,h7,HRe,P9e,Bm,$9e,Di,xm,LW,B5,URe,BW,JRe,I9e,Xo,x5,YRe,k5,KRe,u7,ZRe,eSe,oSe,R5,rSe,xW,tSe,aSe,sSe,mo,S5,nSe,kW,lSe,iSe,ji,dSe,RW,cSe,mSe,SW,fSe,gSe,hSe,v,km,PW,uSe,pSe,p7,_Se,bSe,vSe,Rm,$W,TSe,FSe,_7,CSe,MSe,ESe,Sm,IW,ySe,wSe,b7,ASe,LSe,BSe,Pm,DW,xSe,kSe,v7,RSe,SSe,PSe,$m,jW,$Se,ISe,T7,DSe,jSe,NSe,Im,NW,qSe,GSe,F7,OSe,XSe,VSe,Dm,qW,zSe,WSe,C7,QSe,HSe,USe,jm,GW,JSe,YSe,M7,KSe,ZSe,ePe,Nm,OW,oPe,rPe,E7,tPe,aPe,sPe,qm,XW,nPe,lPe,y7,iPe,dPe,cPe,Gm,VW,mPe,fPe,w7,gPe,hPe,uPe,Om,zW,pPe,_Pe,A7,bPe,vPe,TPe,Xm,WW,FPe,CPe,L7,MPe,EPe,yPe,Vm,QW,wPe,APe,B7,LPe,BPe,xPe,zm,HW,kPe,RPe,x7,SPe,PPe,$Pe,Wm,UW,IPe,DPe,k7,jPe,NPe,qPe,Qm,JW,GPe,OPe,R7,XPe,VPe,zPe,Hm,YW,WPe,QPe,S7,HPe,UPe,JPe,Um,KW,YPe,KPe,P7,ZPe,e$e,o$e,Jm,ZW,r$e,t$e,$7,a$e,s$e,n$e,Ym,eQ,l$e,i$e,I7,d$e,c$e,m$e,Km,oQ,f$e,g$e,D7,h$e,u$e,p$e,Zm,rQ,_$e,b$e,j7,v$e,T$e,F$e,ef,tQ,C$e,M$e,N7,E$e,y$e,w$e,of,aQ,A$e,L$e,q7,B$e,x$e,k$e,rf,sQ,R$e,S$e,G7,P$e,$$e,I$e,tf,nQ,D$e,j$e,O7,N$e,q$e,G$e,af,lQ,O$e,X$e,X7,V$e,z$e,W$e,sf,iQ,Q$e,H$e,V7,U$e,J$e,Y$e,nf,dQ,K$e,Z$e,z7,eIe,oIe,rIe,lf,cQ,tIe,aIe,W7,sIe,nIe,lIe,df,mQ,iIe,dIe,Q7,cIe,mIe,fIe,cf,fQ,gIe,hIe,H7,uIe,pIe,_Ie,mf,gQ,bIe,vIe,U7,TIe,FIe,CIe,ff,hQ,MIe,EIe,J7,yIe,wIe,AIe,gf,uQ,LIe,BIe,Y7,xIe,kIe,RIe,hf,pQ,SIe,PIe,K7,$Ie,IIe,DIe,uf,_Q,jIe,NIe,Z7,qIe,GIe,OIe,pf,bQ,XIe,VIe,e9,zIe,WIe,QIe,_f,vQ,HIe,UIe,o9,JIe,YIe,KIe,bf,TQ,ZIe,eDe,r9,oDe,rDe,tDe,vf,FQ,aDe,sDe,t9,nDe,lDe,iDe,Tf,CQ,dDe,cDe,a9,mDe,fDe,gDe,Ff,MQ,hDe,uDe,s9,pDe,_De,bDe,Cf,EQ,vDe,TDe,n9,FDe,CDe,MDe,Mf,yQ,EDe,yDe,l9,wDe,ADe,LDe,Ef,wQ,BDe,xDe,i9,kDe,RDe,SDe,yf,AQ,PDe,$De,d9,IDe,DDe,jDe,wf,LQ,NDe,qDe,c9,GDe,ODe,XDe,Af,BQ,VDe,zDe,m9,WDe,QDe,HDe,Lf,xQ,UDe,JDe,f9,YDe,KDe,ZDe,Bf,kQ,eje,oje,g9,rje,tje,aje,xf,RQ,sje,nje,h9,lje,ije,dje,kf,SQ,cje,mje,u9,fje,gje,hje,Rf,PQ,uje,pje,p9,_je,bje,vje,Sf,$Q,Tje,Fje,_9,Cje,Mje,Eje,Pf,IQ,yje,wje,b9,Aje,Lje,Bje,$f,DQ,xje,kje,v9,Rje,Sje,Pje,If,jQ,$je,Ije,T9,Dje,jje,Nje,Df,NQ,qje,Gje,F9,Oje,Xje,Vje,jf,qQ,zje,Wje,C9,Qje,Hje,Uje,Nf,GQ,Jje,Yje,M9,Kje,Zje,eNe,qf,OQ,oNe,rNe,E9,tNe,aNe,sNe,Gf,XQ,nNe,lNe,y9,iNe,dNe,cNe,Of,VQ,mNe,fNe,w9,gNe,hNe,uNe,Xf,zQ,pNe,_Ne,A9,bNe,vNe,TNe,Vf,WQ,FNe,CNe,L9,MNe,ENe,yNe,zf,QQ,wNe,ANe,B9,LNe,BNe,xNe,Wf,HQ,kNe,RNe,x9,SNe,PNe,$Ne,Qf,UQ,INe,DNe,k9,jNe,NNe,qNe,Hf,JQ,GNe,ONe,R9,XNe,VNe,zNe,Uf,YQ,WNe,QNe,S9,HNe,UNe,JNe,Jf,KQ,YNe,KNe,P9,ZNe,eqe,oqe,Yf,ZQ,rqe,tqe,$9,aqe,sqe,nqe,Kf,eH,lqe,iqe,I9,dqe,cqe,mqe,Zf,oH,fqe,gqe,D9,hqe,uqe,pqe,eg,rH,_qe,bqe,j9,vqe,Tqe,Fqe,og,tH,Cqe,Mqe,N9,Eqe,yqe,wqe,rg,aH,Aqe,Lqe,q9,Bqe,xqe,kqe,tg,sH,Rqe,Sqe,G9,Pqe,$qe,Iqe,ag,nH,Dqe,jqe,O9,Nqe,qqe,Gqe,sg,lH,Oqe,Xqe,X9,Vqe,zqe,Wqe,ng,iH,Qqe,Hqe,V9,Uqe,Jqe,Yqe,lg,dH,Kqe,Zqe,z9,eGe,oGe,rGe,ig,cH,tGe,aGe,W9,sGe,nGe,lGe,dg,mH,iGe,dGe,Q9,cGe,mGe,fGe,cg,fH,gGe,hGe,H9,uGe,pGe,_Ge,mg,gH,bGe,vGe,U9,TGe,FGe,CGe,fg,hH,MGe,EGe,J9,yGe,wGe,AGe,gg,uH,LGe,BGe,Y9,xGe,kGe,RGe,hg,pH,SGe,PGe,K9,$Ge,IGe,DGe,ug,_H,jGe,NGe,Z9,qGe,GGe,OGe,pg,bH,XGe,VGe,eB,zGe,WGe,QGe,_g,vH,HGe,UGe,oB,JGe,YGe,KGe,TH,ZGe,eOe,P5,oOe,bg,$5,rOe,FH,tOe,D9e,Ni,vg,CH,I5,aOe,MH,sOe,j9e,Vo,D5,nOe,j5,lOe,rB,iOe,dOe,cOe,N5,mOe,EH,fOe,gOe,hOe,fo,q5,uOe,yH,pOe,_Oe,Da,bOe,wH,vOe,TOe,AH,FOe,COe,LH,MOe,EOe,yOe,M,qs,BH,wOe,AOe,tB,LOe,BOe,aB,xOe,kOe,ROe,Gs,xH,SOe,POe,sB,$Oe,IOe,nB,DOe,jOe,NOe,Os,kH,qOe,GOe,lB,OOe,XOe,iB,VOe,zOe,WOe,Tg,RH,QOe,HOe,dB,UOe,JOe,YOe,Xs,SH,KOe,ZOe,cB,eXe,oXe,mB,rXe,tXe,aXe,Fg,PH,sXe,nXe,fB,lXe,iXe,dXe,Cg,$H,cXe,mXe,gB,fXe,gXe,hXe,Mg,IH,uXe,pXe,hB,_Xe,bXe,vXe,Vs,DH,TXe,FXe,uB,CXe,MXe,pB,EXe,yXe,wXe,zs,jH,AXe,LXe,_B,BXe,xXe,bB,kXe,RXe,SXe,Ws,NH,PXe,$Xe,vB,IXe,DXe,TB,jXe,NXe,qXe,Eg,qH,GXe,OXe,FB,XXe,VXe,zXe,yg,GH,WXe,QXe,CB,HXe,UXe,JXe,Qs,OH,YXe,KXe,MB,ZXe,eVe,EB,oVe,rVe,tVe,wg,XH,aVe,sVe,yB,nVe,lVe,iVe,Hs,VH,dVe,cVe,wB,mVe,fVe,AB,gVe,hVe,uVe,Us,zH,pVe,_Ve,LB,bVe,vVe,BB,TVe,FVe,CVe,Js,WH,MVe,EVe,xB,yVe,wVe,QH,AVe,LVe,BVe,Ag,HH,xVe,kVe,kB,RVe,SVe,PVe,Ys,UH,$Ve,IVe,RB,DVe,jVe,SB,NVe,qVe,GVe,Lg,JH,OVe,XVe,PB,VVe,zVe,WVe,Ks,YH,QVe,HVe,$B,UVe,JVe,IB,YVe,KVe,ZVe,Zs,KH,eze,oze,DB,rze,tze,jB,aze,sze,nze,en,ZH,lze,ize,NB,dze,cze,qB,mze,fze,gze,Bg,eU,hze,uze,GB,pze,_ze,bze,on,oU,vze,Tze,OB,Fze,Cze,XB,Mze,Eze,yze,xg,rU,wze,Aze,VB,Lze,Bze,xze,rn,tU,kze,Rze,zB,Sze,Pze,WB,$ze,Ize,Dze,tn,aU,jze,Nze,QB,qze,Gze,HB,Oze,Xze,Vze,an,sU,zze,Wze,UB,Qze,Hze,JB,Uze,Jze,Yze,sn,nU,Kze,Zze,YB,eWe,oWe,KB,rWe,tWe,aWe,kg,lU,sWe,nWe,ZB,lWe,iWe,dWe,nn,iU,cWe,mWe,ex,fWe,gWe,ox,hWe,uWe,pWe,ln,dU,_We,bWe,rx,vWe,TWe,tx,FWe,CWe,MWe,dn,cU,EWe,yWe,ax,wWe,AWe,sx,LWe,BWe,xWe,cn,mU,kWe,RWe,nx,SWe,PWe,lx,$We,IWe,DWe,mn,fU,jWe,NWe,ix,qWe,GWe,dx,OWe,XWe,VWe,fn,gU,zWe,WWe,cx,QWe,HWe,mx,UWe,JWe,YWe,Rg,hU,KWe,ZWe,fx,eQe,oQe,rQe,gn,uU,tQe,aQe,gx,sQe,nQe,hx,lQe,iQe,dQe,Sg,pU,cQe,mQe,ux,fQe,gQe,hQe,Pg,_U,uQe,pQe,px,_Qe,bQe,vQe,hn,bU,TQe,FQe,_x,CQe,MQe,bx,EQe,yQe,wQe,un,vU,AQe,LQe,vx,BQe,xQe,Tx,kQe,RQe,SQe,$g,TU,PQe,$Qe,Fx,IQe,DQe,jQe,pn,FU,NQe,qQe,Cx,GQe,OQe,Mx,XQe,VQe,zQe,_n,CU,WQe,QQe,Ex,HQe,UQe,yx,JQe,YQe,KQe,bn,MU,ZQe,eHe,wx,oHe,rHe,Ax,tHe,aHe,sHe,vn,EU,nHe,lHe,Lx,iHe,dHe,Bx,cHe,mHe,fHe,Tn,yU,gHe,hHe,xx,uHe,pHe,kx,_He,bHe,vHe,Ig,wU,THe,FHe,Rx,CHe,MHe,EHe,Dg,AU,yHe,wHe,Sx,AHe,LHe,BHe,jg,LU,xHe,kHe,Px,RHe,SHe,PHe,Ng,BU,$He,IHe,$x,DHe,jHe,NHe,Fn,xU,qHe,GHe,Ix,OHe,XHe,Dx,VHe,zHe,WHe,qg,kU,QHe,HHe,jx,UHe,JHe,YHe,Cn,RU,KHe,ZHe,Nx,eUe,oUe,qx,rUe,tUe,aUe,Mn,SU,sUe,nUe,Gx,lUe,iUe,Ox,dUe,cUe,mUe,En,PU,fUe,gUe,Xx,hUe,uUe,Vx,pUe,_Ue,bUe,yn,$U,vUe,TUe,zx,FUe,CUe,Wx,MUe,EUe,yUe,wn,IU,wUe,AUe,Qx,LUe,BUe,Hx,xUe,kUe,RUe,An,DU,SUe,PUe,Ux,$Ue,IUe,Jx,DUe,jUe,NUe,Gg,jU,qUe,GUe,Yx,OUe,XUe,VUe,Og,NU,zUe,WUe,Kx,QUe,HUe,UUe,Ln,qU,JUe,YUe,Zx,KUe,ZUe,ek,eJe,oJe,rJe,Bn,GU,tJe,aJe,ok,sJe,nJe,rk,lJe,iJe,dJe,xn,OU,cJe,mJe,tk,fJe,gJe,ak,hJe,uJe,pJe,Xg,XU,_Je,bJe,sk,vJe,TJe,FJe,Vg,VU,CJe,MJe,nk,EJe,yJe,wJe,zg,zU,AJe,LJe,lk,BJe,xJe,kJe,Wg,WU,RJe,SJe,ik,PJe,$Je,IJe,kn,QU,DJe,jJe,dk,NJe,qJe,ck,GJe,OJe,XJe,Qg,HU,VJe,zJe,mk,WJe,QJe,HJe,Hg,UU,UJe,JJe,fk,YJe,KJe,ZJe,Rn,JU,eYe,oYe,gk,rYe,tYe,hk,aYe,sYe,nYe,Sn,YU,lYe,iYe,uk,dYe,cYe,pk,mYe,fYe,gYe,KU,hYe,uYe,G5,pYe,Ug,O5,_Ye,ZU,bYe,N9e,qi,Jg,eJ,X5,vYe,oJ,TYe,q9e,zo,V5,FYe,z5,CYe,_k,MYe,EYe,yYe,W5,wYe,rJ,AYe,LYe,BYe,xe,Q5,xYe,tJ,kYe,RYe,ja,SYe,aJ,PYe,$Ye,sJ,IYe,DYe,nJ,jYe,NYe,qYe,ne,Yg,lJ,GYe,OYe,bk,XYe,VYe,zYe,Kg,iJ,WYe,QYe,vk,HYe,UYe,JYe,Zg,dJ,YYe,KYe,Tk,ZYe,eKe,oKe,eh,cJ,rKe,tKe,Fk,aKe,sKe,nKe,oh,mJ,lKe,iKe,Ck,dKe,cKe,mKe,rh,fJ,fKe,gKe,Mk,hKe,uKe,pKe,th,gJ,_Ke,bKe,Ek,vKe,TKe,FKe,ah,hJ,CKe,MKe,yk,EKe,yKe,wKe,sh,uJ,AKe,LKe,wk,BKe,xKe,kKe,nh,pJ,RKe,SKe,Ak,PKe,$Ke,IKe,lh,_J,DKe,jKe,Lk,NKe,qKe,GKe,ih,bJ,OKe,XKe,Bk,VKe,zKe,WKe,dh,vJ,QKe,HKe,xk,UKe,JKe,YKe,ch,TJ,KKe,ZKe,kk,eZe,oZe,rZe,mh,FJ,tZe,aZe,Rk,sZe,nZe,lZe,fh,iZe,CJ,dZe,cZe,H5,mZe,gh,U5,fZe,MJ,gZe,G9e,Gi,hh,EJ,J5,hZe,yJ,uZe,O9e,Wo,Y5,pZe,K5,_Ze,Sk,bZe,vZe,TZe,Z5,FZe,wJ,CZe,MZe,EZe,ke,ey,yZe,AJ,wZe,AZe,Oi,LZe,LJ,BZe,xZe,BJ,kZe,RZe,SZe,we,uh,xJ,PZe,$Ze,Pk,IZe,DZe,jZe,ph,kJ,NZe,qZe,$k,GZe,OZe,XZe,_h,RJ,VZe,zZe,Ik,WZe,QZe,HZe,bh,SJ,UZe,JZe,Dk,YZe,KZe,ZZe,vh,PJ,eeo,oeo,jk,reo,teo,aeo,Th,$J,seo,neo,Nk,leo,ieo,deo,Fh,IJ,ceo,meo,qk,feo,geo,heo,Ch,DJ,ueo,peo,Gk,_eo,beo,veo,Mh,Teo,jJ,Feo,Ceo,oy,Meo,Eh,ry,Eeo,NJ,yeo,X9e,Xi,yh,qJ,ty,weo,GJ,Aeo,V9e,Qo,ay,Leo,Vi,Beo,OJ,xeo,keo,XJ,Reo,Seo,Peo,sy,$eo,VJ,Ieo,Deo,jeo,qr,ny,Neo,zJ,qeo,Geo,zi,Oeo,WJ,Xeo,Veo,QJ,zeo,Weo,Qeo,HJ,Heo,Ueo,ly,Jeo,Re,iy,Yeo,UJ,Keo,Zeo,Na,eoo,JJ,ooo,roo,YJ,too,aoo,KJ,soo,noo,loo,F,wh,ZJ,ioo,doo,Ok,coo,moo,foo,Ah,eY,goo,hoo,Xk,uoo,poo,_oo,Lh,oY,boo,voo,Vk,Too,Foo,Coo,Bh,rY,Moo,Eoo,zk,yoo,woo,Aoo,xh,tY,Loo,Boo,Wk,xoo,koo,Roo,kh,aY,Soo,Poo,Qk,$oo,Ioo,Doo,Rh,sY,joo,Noo,Hk,qoo,Goo,Ooo,Sh,nY,Xoo,Voo,Uk,zoo,Woo,Qoo,Ph,lY,Hoo,Uoo,Jk,Joo,Yoo,Koo,$h,iY,Zoo,ero,Yk,oro,rro,tro,Ih,dY,aro,sro,Kk,nro,lro,iro,Dh,cY,dro,cro,Zk,mro,fro,gro,jh,mY,hro,uro,eR,pro,_ro,bro,Nh,fY,vro,Tro,oR,Fro,Cro,Mro,qh,gY,Ero,yro,rR,wro,Aro,Lro,Gh,hY,Bro,xro,tR,kro,Rro,Sro,Oh,uY,Pro,$ro,aR,Iro,Dro,jro,Xh,pY,Nro,qro,sR,Gro,Oro,Xro,Vh,_Y,Vro,zro,nR,Wro,Qro,Hro,zh,bY,Uro,Jro,lR,Yro,Kro,Zro,Wh,vY,eto,oto,iR,rto,tto,ato,Qh,TY,sto,nto,dR,lto,ito,dto,Hh,FY,cto,mto,cR,fto,gto,hto,Uh,CY,uto,pto,mR,_to,bto,vto,Jh,MY,Tto,Fto,fR,Cto,Mto,Eto,Yh,EY,yto,wto,gR,Ato,Lto,Bto,Kh,yY,xto,kto,hR,Rto,Sto,Pto,Pn,wY,$to,Ito,uR,Dto,jto,pR,Nto,qto,Gto,Zh,AY,Oto,Xto,_R,Vto,zto,Wto,eu,LY,Qto,Hto,bR,Uto,Jto,Yto,ou,BY,Kto,Zto,vR,eao,oao,rao,ru,xY,tao,aao,TR,sao,nao,lao,tu,kY,iao,dao,FR,cao,mao,fao,au,RY,gao,hao,CR,uao,pao,_ao,su,SY,bao,vao,MR,Tao,Fao,Cao,nu,PY,Mao,Eao,ER,yao,wao,Aao,lu,$Y,Lao,Bao,yR,xao,kao,Rao,iu,IY,Sao,Pao,wR,$ao,Iao,Dao,du,DY,jao,Nao,AR,qao,Gao,Oao,cu,jY,Xao,Vao,LR,zao,Wao,Qao,mu,NY,Hao,Uao,BR,Jao,Yao,Kao,fu,qY,Zao,eso,xR,oso,rso,tso,gu,GY,aso,sso,kR,nso,lso,iso,hu,OY,dso,cso,RR,mso,fso,gso,uu,XY,hso,uso,SR,pso,_so,bso,pu,VY,vso,Tso,PR,Fso,Cso,Mso,_u,zY,Eso,yso,$R,wso,Aso,Lso,bu,WY,Bso,xso,IR,kso,Rso,Sso,vu,QY,Pso,$so,DR,Iso,Dso,jso,Tu,HY,Nso,qso,jR,Gso,Oso,Xso,Fu,UY,Vso,zso,NR,Wso,Qso,Hso,Cu,JY,Uso,Jso,qR,Yso,Kso,Zso,Mu,YY,eno,ono,GR,rno,tno,ano,Eu,KY,sno,nno,OR,lno,ino,dno,yu,ZY,cno,mno,XR,fno,gno,hno,wu,eK,uno,pno,VR,_no,bno,vno,Au,oK,Tno,Fno,zR,Cno,Mno,Eno,Lu,rK,yno,wno,WR,Ano,Lno,Bno,Bu,tK,xno,kno,QR,Rno,Sno,Pno,xu,aK,$no,Ino,HR,Dno,jno,Nno,ku,sK,qno,Gno,UR,Ono,Xno,Vno,Ru,nK,zno,Wno,JR,Qno,Hno,Uno,Su,lK,Jno,Yno,YR,Kno,Zno,elo,Pu,iK,olo,rlo,KR,tlo,alo,slo,$u,dK,nlo,llo,ZR,ilo,dlo,clo,Iu,cK,mlo,flo,eS,glo,hlo,ulo,Du,mK,plo,_lo,oS,blo,vlo,Tlo,ju,fK,Flo,Clo,rS,Mlo,Elo,ylo,Nu,gK,wlo,Alo,tS,Llo,Blo,xlo,qu,hK,klo,Rlo,aS,Slo,Plo,$lo,Gu,uK,Ilo,Dlo,sS,jlo,Nlo,qlo,Ou,pK,Glo,Olo,nS,Xlo,Vlo,zlo,Xu,_K,Wlo,Qlo,lS,Hlo,Ulo,Jlo,Vu,bK,Ylo,Klo,iS,Zlo,eio,oio,zu,vK,rio,tio,dS,aio,sio,nio,Wu,TK,lio,iio,cS,dio,cio,mio,Qu,FK,fio,gio,mS,hio,uio,pio,Hu,CK,_io,bio,fS,vio,Tio,Fio,Uu,MK,Cio,Mio,gS,Eio,yio,wio,Ju,EK,Aio,Lio,hS,Bio,xio,kio,Yu,yK,Rio,Sio,uS,Pio,$io,Iio,Ku,wK,Dio,jio,pS,Nio,qio,Gio,Zu,AK,Oio,Xio,_S,Vio,zio,Wio,ep,LK,Qio,Hio,bS,Uio,Jio,Yio,op,BK,Kio,Zio,vS,edo,odo,rdo,rp,xK,tdo,ado,TS,sdo,ndo,ldo,tp,kK,ido,ddo,FS,cdo,mdo,fdo,ap,gdo,RK,hdo,udo,SK,pdo,_do,PK,bdo,vdo,dy,z9e,Wi,sp,$K,cy,Tdo,IK,Fdo,W9e,Ho,my,Cdo,Qi,Mdo,DK,Edo,ydo,jK,wdo,Ado,Ldo,fy,Bdo,NK,xdo,kdo,Rdo,Gr,gy,Sdo,qK,Pdo,$do,Hi,Ido,GK,Ddo,jdo,OK,Ndo,qdo,Gdo,XK,Odo,Xdo,hy,Vdo,Se,uy,zdo,VK,Wdo,Qdo,qa,Hdo,zK,Udo,Jdo,WK,Ydo,Kdo,QK,Zdo,eco,oco,k,np,HK,rco,tco,CS,aco,sco,nco,lp,UK,lco,ico,MS,dco,cco,mco,ip,JK,fco,gco,ES,hco,uco,pco,dp,YK,_co,bco,yS,vco,Tco,Fco,cp,KK,Cco,Mco,wS,Eco,yco,wco,mp,ZK,Aco,Lco,AS,Bco,xco,kco,fp,eZ,Rco,Sco,LS,Pco,$co,Ico,gp,oZ,Dco,jco,BS,Nco,qco,Gco,hp,rZ,Oco,Xco,xS,Vco,zco,Wco,up,tZ,Qco,Hco,kS,Uco,Jco,Yco,pp,aZ,Kco,Zco,RS,emo,omo,rmo,_p,sZ,tmo,amo,SS,smo,nmo,lmo,bp,nZ,imo,dmo,PS,cmo,mmo,fmo,vp,lZ,gmo,hmo,$S,umo,pmo,_mo,Tp,iZ,bmo,vmo,IS,Tmo,Fmo,Cmo,Fp,dZ,Mmo,Emo,DS,ymo,wmo,Amo,Cp,cZ,Lmo,Bmo,jS,xmo,kmo,Rmo,Mp,mZ,Smo,Pmo,NS,$mo,Imo,Dmo,Ep,fZ,jmo,Nmo,qS,qmo,Gmo,Omo,yp,gZ,Xmo,Vmo,GS,zmo,Wmo,Qmo,wp,hZ,Hmo,Umo,OS,Jmo,Ymo,Kmo,Ap,uZ,Zmo,efo,XS,ofo,rfo,tfo,Lp,pZ,afo,sfo,VS,nfo,lfo,ifo,Bp,_Z,dfo,cfo,zS,mfo,ffo,gfo,xp,bZ,hfo,ufo,WS,pfo,_fo,bfo,kp,vZ,vfo,Tfo,QS,Ffo,Cfo,Mfo,Rp,TZ,Efo,yfo,HS,wfo,Afo,Lfo,Sp,FZ,Bfo,xfo,US,kfo,Rfo,Sfo,Pp,CZ,Pfo,$fo,JS,Ifo,Dfo,jfo,$p,MZ,Nfo,qfo,YS,Gfo,Ofo,Xfo,Ip,EZ,Vfo,zfo,KS,Wfo,Qfo,Hfo,Dp,yZ,Ufo,Jfo,ZS,Yfo,Kfo,Zfo,jp,wZ,ego,ogo,eP,rgo,tgo,ago,Np,AZ,sgo,ngo,oP,lgo,igo,dgo,qp,LZ,cgo,mgo,rP,fgo,ggo,hgo,Gp,BZ,ugo,pgo,tP,_go,bgo,vgo,Op,xZ,Tgo,Fgo,aP,Cgo,Mgo,Ego,Xp,kZ,ygo,wgo,sP,Ago,Lgo,Bgo,Vp,RZ,xgo,kgo,nP,Rgo,Sgo,Pgo,zp,$go,SZ,Igo,Dgo,PZ,jgo,Ngo,$Z,qgo,Ggo,py,Q9e,Ui,Wp,IZ,_y,Ogo,DZ,Xgo,H9e,Uo,by,Vgo,Ji,zgo,jZ,Wgo,Qgo,NZ,Hgo,Ugo,Jgo,vy,Ygo,qZ,Kgo,Zgo,eho,Or,Ty,oho,GZ,rho,tho,Yi,aho,OZ,sho,nho,XZ,lho,iho,dho,VZ,cho,mho,Fy,fho,Pe,Cy,gho,zZ,hho,uho,Ga,pho,WZ,_ho,bho,QZ,vho,Tho,HZ,Fho,Cho,Mho,$,Qp,UZ,Eho,yho,lP,who,Aho,Lho,Hp,JZ,Bho,xho,iP,kho,Rho,Sho,Up,YZ,Pho,$ho,dP,Iho,Dho,jho,Jp,KZ,Nho,qho,cP,Gho,Oho,Xho,Yp,ZZ,Vho,zho,mP,Who,Qho,Hho,Kp,eee,Uho,Jho,fP,Yho,Kho,Zho,Zp,oee,euo,ouo,gP,ruo,tuo,auo,e_,ree,suo,nuo,hP,luo,iuo,duo,o_,tee,cuo,muo,uP,fuo,guo,huo,r_,aee,uuo,puo,pP,_uo,buo,vuo,t_,see,Tuo,Fuo,_P,Cuo,Muo,Euo,a_,nee,yuo,wuo,bP,Auo,Luo,Buo,s_,lee,xuo,kuo,vP,Ruo,Suo,Puo,n_,iee,$uo,Iuo,TP,Duo,juo,Nuo,l_,dee,quo,Guo,FP,Ouo,Xuo,Vuo,i_,cee,zuo,Wuo,CP,Quo,Huo,Uuo,d_,mee,Juo,Yuo,MP,Kuo,Zuo,epo,c_,fee,opo,rpo,EP,tpo,apo,spo,m_,gee,npo,lpo,yP,ipo,dpo,cpo,f_,hee,mpo,fpo,wP,gpo,hpo,upo,g_,uee,ppo,_po,AP,bpo,vpo,Tpo,h_,pee,Fpo,Cpo,LP,Mpo,Epo,ypo,u_,_ee,wpo,Apo,BP,Lpo,Bpo,xpo,p_,bee,kpo,Rpo,xP,Spo,Ppo,$po,__,vee,Ipo,Dpo,kP,jpo,Npo,qpo,b_,Tee,Gpo,Opo,RP,Xpo,Vpo,zpo,v_,Fee,Wpo,Qpo,SP,Hpo,Upo,Jpo,T_,Cee,Ypo,Kpo,PP,Zpo,e_o,o_o,F_,Mee,r_o,t_o,$P,a_o,s_o,n_o,C_,Eee,l_o,i_o,IP,d_o,c_o,m_o,M_,yee,f_o,g_o,DP,h_o,u_o,p_o,E_,wee,__o,b_o,jP,v_o,T_o,F_o,y_,Aee,C_o,M_o,NP,E_o,y_o,w_o,w_,Lee,A_o,L_o,qP,B_o,x_o,k_o,A_,Bee,R_o,S_o,GP,P_o,$_o,I_o,L_,D_o,xee,j_o,N_o,kee,q_o,G_o,Ree,O_o,X_o,My,U9e,Ki,B_,See,Ey,V_o,Pee,z_o,J9e,Jo,yy,W_o,Zi,Q_o,$ee,H_o,U_o,Iee,J_o,Y_o,K_o,wy,Z_o,Dee,ebo,obo,rbo,Xr,Ay,tbo,jee,abo,sbo,ed,nbo,Nee,lbo,ibo,qee,dbo,cbo,mbo,Gee,fbo,gbo,Ly,hbo,$e,By,ubo,Oee,pbo,_bo,Oa,bbo,Xee,vbo,Tbo,Vee,Fbo,Cbo,zee,Mbo,Ebo,ybo,I,x_,Wee,wbo,Abo,OP,Lbo,Bbo,xbo,k_,Qee,kbo,Rbo,XP,Sbo,Pbo,$bo,R_,Hee,Ibo,Dbo,VP,jbo,Nbo,qbo,S_,Uee,Gbo,Obo,zP,Xbo,Vbo,zbo,P_,Jee,Wbo,Qbo,WP,Hbo,Ubo,Jbo,$_,Yee,Ybo,Kbo,QP,Zbo,e2o,o2o,I_,Kee,r2o,t2o,HP,a2o,s2o,n2o,D_,Zee,l2o,i2o,UP,d2o,c2o,m2o,j_,eoe,f2o,g2o,JP,h2o,u2o,p2o,N_,ooe,_2o,b2o,YP,v2o,T2o,F2o,q_,roe,C2o,M2o,KP,E2o,y2o,w2o,G_,toe,A2o,L2o,ZP,B2o,x2o,k2o,O_,aoe,R2o,S2o,e$,P2o,$2o,I2o,X_,soe,D2o,j2o,o$,N2o,q2o,G2o,V_,noe,O2o,X2o,r$,V2o,z2o,W2o,z_,loe,Q2o,H2o,t$,U2o,J2o,Y2o,W_,ioe,K2o,Z2o,a$,evo,ovo,rvo,Q_,doe,tvo,avo,s$,svo,nvo,lvo,H_,coe,ivo,dvo,n$,cvo,mvo,fvo,U_,moe,gvo,hvo,l$,uvo,pvo,_vo,J_,foe,bvo,vvo,i$,Tvo,Fvo,Cvo,Y_,goe,Mvo,Evo,d$,yvo,wvo,Avo,K_,hoe,Lvo,Bvo,c$,xvo,kvo,Rvo,Z_,uoe,Svo,Pvo,m$,$vo,Ivo,Dvo,eb,poe,jvo,Nvo,f$,qvo,Gvo,Ovo,ob,_oe,Xvo,Vvo,g$,zvo,Wvo,Qvo,rb,boe,Hvo,Uvo,h$,Jvo,Yvo,Kvo,tb,voe,Zvo,eTo,u$,oTo,rTo,tTo,ab,Toe,aTo,sTo,p$,nTo,lTo,iTo,sb,Foe,dTo,cTo,_$,mTo,fTo,gTo,nb,Coe,hTo,uTo,Moe,pTo,_To,bTo,lb,Eoe,vTo,TTo,b$,FTo,CTo,MTo,ib,yoe,ETo,yTo,v$,wTo,ATo,LTo,db,woe,BTo,xTo,T$,kTo,RTo,STo,cb,Aoe,PTo,$To,F$,ITo,DTo,jTo,mb,NTo,Loe,qTo,GTo,Boe,OTo,XTo,xoe,VTo,zTo,xy,Y9e,od,fb,koe,ky,WTo,Roe,QTo,K9e,Yo,Ry,HTo,rd,UTo,Soe,JTo,YTo,Poe,KTo,ZTo,e1o,Sy,o1o,$oe,r1o,t1o,a1o,Vr,Py,s1o,Ioe,n1o,l1o,td,i1o,Doe,d1o,c1o,joe,m1o,f1o,g1o,Noe,h1o,u1o,$y,p1o,Ie,Iy,_1o,qoe,b1o,v1o,Xa,T1o,Goe,F1o,C1o,Ooe,M1o,E1o,Xoe,y1o,w1o,A1o,ae,gb,Voe,L1o,B1o,C$,x1o,k1o,R1o,hb,zoe,S1o,P1o,M$,$1o,I1o,D1o,ub,Woe,j1o,N1o,E$,q1o,G1o,O1o,pb,Qoe,X1o,V1o,y$,z1o,W1o,Q1o,_b,Hoe,H1o,U1o,w$,J1o,Y1o,K1o,bb,Uoe,Z1o,eFo,A$,oFo,rFo,tFo,vb,Joe,aFo,sFo,L$,nFo,lFo,iFo,Tb,Yoe,dFo,cFo,B$,mFo,fFo,gFo,Fb,Koe,hFo,uFo,x$,pFo,_Fo,bFo,Cb,Zoe,vFo,TFo,k$,FFo,CFo,MFo,Mb,ere,EFo,yFo,R$,wFo,AFo,LFo,Eb,ore,BFo,xFo,S$,kFo,RFo,SFo,yb,rre,PFo,$Fo,P$,IFo,DFo,jFo,wb,tre,NFo,qFo,$$,GFo,OFo,XFo,Ab,are,VFo,zFo,I$,WFo,QFo,HFo,Lb,sre,UFo,JFo,D$,YFo,KFo,ZFo,Bb,eCo,nre,oCo,rCo,lre,tCo,aCo,ire,sCo,nCo,Dy,Z9e,ad,xb,dre,jy,lCo,cre,iCo,eBe,Ko,Ny,dCo,sd,cCo,mre,mCo,fCo,fre,gCo,hCo,uCo,qy,pCo,gre,_Co,bCo,vCo,zr,Gy,TCo,hre,FCo,CCo,nd,MCo,ure,ECo,yCo,pre,wCo,ACo,LCo,_re,BCo,xCo,Oy,kCo,De,Xy,RCo,bre,SCo,PCo,Va,$Co,vre,ICo,DCo,Tre,jCo,NCo,Fre,qCo,GCo,OCo,A,kb,Cre,XCo,VCo,j$,zCo,WCo,QCo,Rb,Mre,HCo,UCo,N$,JCo,YCo,KCo,Sb,Ere,ZCo,e4o,q$,o4o,r4o,t4o,Pb,yre,a4o,s4o,G$,n4o,l4o,i4o,$b,wre,d4o,c4o,O$,m4o,f4o,g4o,Ib,Are,h4o,u4o,X$,p4o,_4o,b4o,Db,Lre,v4o,T4o,V$,F4o,C4o,M4o,jb,Bre,E4o,y4o,z$,w4o,A4o,L4o,Nb,xre,B4o,x4o,W$,k4o,R4o,S4o,qb,kre,P4o,$4o,Q$,I4o,D4o,j4o,Gb,Rre,N4o,q4o,H$,G4o,O4o,X4o,Ob,Sre,V4o,z4o,U$,W4o,Q4o,H4o,Xb,Pre,U4o,J4o,J$,Y4o,K4o,Z4o,Vb,$re,eMo,oMo,Y$,rMo,tMo,aMo,zb,Ire,sMo,nMo,K$,lMo,iMo,dMo,Wb,Dre,cMo,mMo,Z$,fMo,gMo,hMo,Qb,jre,uMo,pMo,eI,_Mo,bMo,vMo,Hb,Nre,TMo,FMo,oI,CMo,MMo,EMo,Ub,qre,yMo,wMo,rI,AMo,LMo,BMo,Jb,Gre,xMo,kMo,tI,RMo,SMo,PMo,Yb,Ore,$Mo,IMo,aI,DMo,jMo,NMo,Kb,Xre,qMo,GMo,sI,OMo,XMo,VMo,Zb,Vre,zMo,WMo,nI,QMo,HMo,UMo,e2,zre,JMo,YMo,lI,KMo,ZMo,eEo,o2,Wre,oEo,rEo,iI,tEo,aEo,sEo,r2,Qre,nEo,lEo,dI,iEo,dEo,cEo,t2,Hre,mEo,fEo,cI,gEo,hEo,uEo,a2,Ure,pEo,_Eo,mI,bEo,vEo,TEo,s2,Jre,FEo,CEo,fI,MEo,EEo,yEo,n2,Yre,wEo,AEo,gI,LEo,BEo,xEo,l2,Kre,kEo,REo,hI,SEo,PEo,$Eo,i2,Zre,IEo,DEo,uI,jEo,NEo,qEo,d2,ete,GEo,OEo,pI,XEo,VEo,zEo,c2,ote,WEo,QEo,_I,HEo,UEo,JEo,m2,rte,YEo,KEo,bI,ZEo,e3o,o3o,f2,tte,r3o,t3o,vI,a3o,s3o,n3o,g2,ate,l3o,i3o,TI,d3o,c3o,m3o,h2,ste,f3o,g3o,FI,h3o,u3o,p3o,u2,nte,_3o,b3o,CI,v3o,T3o,F3o,p2,lte,C3o,M3o,MI,E3o,y3o,w3o,_2,ite,A3o,L3o,EI,B3o,x3o,k3o,b2,dte,R3o,S3o,yI,P3o,$3o,I3o,v2,cte,D3o,j3o,wI,N3o,q3o,G3o,T2,mte,O3o,X3o,AI,V3o,z3o,W3o,F2,fte,Q3o,H3o,LI,U3o,J3o,Y3o,C2,gte,K3o,Z3o,BI,e5o,o5o,r5o,M2,t5o,hte,a5o,s5o,ute,n5o,l5o,pte,i5o,d5o,Vy,oBe,ld,E2,_te,zy,c5o,bte,m5o,rBe,Zo,Wy,f5o,id,g5o,vte,h5o,u5o,Tte,p5o,_5o,b5o,Qy,v5o,Fte,T5o,F5o,C5o,Wr,Hy,M5o,Cte,E5o,y5o,dd,w5o,Mte,A5o,L5o,Ete,B5o,x5o,k5o,yte,R5o,S5o,Uy,P5o,je,Jy,$5o,wte,I5o,D5o,za,j5o,Ate,N5o,q5o,Lte,G5o,O5o,Bte,X5o,V5o,z5o,G,y2,xte,W5o,Q5o,xI,H5o,U5o,J5o,w2,kte,Y5o,K5o,kI,Z5o,eyo,oyo,A2,Rte,ryo,tyo,RI,ayo,syo,nyo,L2,Ste,lyo,iyo,SI,dyo,cyo,myo,B2,Pte,fyo,gyo,PI,hyo,uyo,pyo,x2,$te,_yo,byo,$I,vyo,Tyo,Fyo,k2,Ite,Cyo,Myo,II,Eyo,yyo,wyo,R2,Dte,Ayo,Lyo,DI,Byo,xyo,kyo,S2,jte,Ryo,Syo,jI,Pyo,$yo,Iyo,P2,Nte,Dyo,jyo,NI,Nyo,qyo,Gyo,$2,qte,Oyo,Xyo,qI,Vyo,zyo,Wyo,I2,Gte,Qyo,Hyo,GI,Uyo,Jyo,Yyo,D2,Ote,Kyo,Zyo,OI,ewo,owo,rwo,j2,Xte,two,awo,XI,swo,nwo,lwo,N2,Vte,iwo,dwo,VI,cwo,mwo,fwo,q2,zte,gwo,hwo,zI,uwo,pwo,_wo,G2,Wte,bwo,vwo,WI,Two,Fwo,Cwo,O2,Qte,Mwo,Ewo,QI,ywo,wwo,Awo,X2,Hte,Lwo,Bwo,HI,xwo,kwo,Rwo,V2,Ute,Swo,Pwo,UI,$wo,Iwo,Dwo,z2,Jte,jwo,Nwo,JI,qwo,Gwo,Owo,W2,Yte,Xwo,Vwo,YI,zwo,Wwo,Qwo,Q2,Kte,Hwo,Uwo,KI,Jwo,Ywo,Kwo,H2,Zte,Zwo,e6o,ZI,o6o,r6o,t6o,U2,eae,a6o,s6o,eD,n6o,l6o,i6o,J2,oae,d6o,c6o,oD,m6o,f6o,g6o,Y2,rae,h6o,u6o,rD,p6o,_6o,b6o,K2,tae,v6o,T6o,tD,F6o,C6o,M6o,Z2,E6o,aae,y6o,w6o,sae,A6o,L6o,nae,B6o,x6o,Yy,tBe,cd,ev,lae,Ky,k6o,iae,R6o,aBe,er,Zy,S6o,md,P6o,dae,$6o,I6o,cae,D6o,j6o,N6o,ew,q6o,mae,G6o,O6o,X6o,Qr,ow,V6o,fae,z6o,W6o,fd,Q6o,gae,H6o,U6o,hae,J6o,Y6o,K6o,uae,Z6o,eAo,rw,oAo,Ne,tw,rAo,pae,tAo,aAo,Wa,sAo,_ae,nAo,lAo,bae,iAo,dAo,vae,cAo,mAo,fAo,sa,ov,Tae,gAo,hAo,aD,uAo,pAo,_Ao,rv,Fae,bAo,vAo,sD,TAo,FAo,CAo,tv,Cae,MAo,EAo,nD,yAo,wAo,AAo,av,Mae,LAo,BAo,lD,xAo,kAo,RAo,sv,Eae,SAo,PAo,iD,$Ao,IAo,DAo,nv,jAo,yae,NAo,qAo,wae,GAo,OAo,Aae,XAo,VAo,aw,sBe,gd,lv,Lae,sw,zAo,Bae,WAo,nBe,or,nw,QAo,hd,HAo,xae,UAo,JAo,kae,YAo,KAo,ZAo,lw,e0o,Rae,o0o,r0o,t0o,Hr,iw,a0o,Sae,s0o,n0o,ud,l0o,Pae,i0o,d0o,$ae,c0o,m0o,f0o,Iae,g0o,h0o,dw,u0o,qe,cw,p0o,Dae,_0o,b0o,Qa,v0o,jae,T0o,F0o,Nae,C0o,M0o,qae,E0o,y0o,w0o,N,iv,Gae,A0o,L0o,dD,B0o,x0o,k0o,dv,Oae,R0o,S0o,cD,P0o,$0o,I0o,cv,Xae,D0o,j0o,mD,N0o,q0o,G0o,mv,Vae,O0o,X0o,fD,V0o,z0o,W0o,fv,zae,Q0o,H0o,gD,U0o,J0o,Y0o,gv,Wae,K0o,Z0o,hD,eLo,oLo,rLo,hv,Qae,tLo,aLo,uD,sLo,nLo,lLo,uv,Hae,iLo,dLo,pD,cLo,mLo,fLo,pv,Uae,gLo,hLo,_D,uLo,pLo,_Lo,_v,Jae,bLo,vLo,bD,TLo,FLo,CLo,bv,Yae,MLo,ELo,vD,yLo,wLo,ALo,vv,Kae,LLo,BLo,TD,xLo,kLo,RLo,Tv,Zae,SLo,PLo,FD,$Lo,ILo,DLo,Fv,ese,jLo,NLo,CD,qLo,GLo,OLo,Cv,ose,XLo,VLo,MD,zLo,WLo,QLo,Mv,rse,HLo,ULo,ED,JLo,YLo,KLo,Ev,tse,ZLo,e8o,yD,o8o,r8o,t8o,yv,ase,a8o,s8o,wD,n8o,l8o,i8o,wv,sse,d8o,c8o,AD,m8o,f8o,g8o,Av,nse,h8o,u8o,LD,p8o,_8o,b8o,Lv,lse,v8o,T8o,BD,F8o,C8o,M8o,Bv,ise,E8o,y8o,xD,w8o,A8o,L8o,xv,dse,B8o,x8o,kD,k8o,R8o,S8o,kv,cse,P8o,$8o,RD,I8o,D8o,j8o,Rv,mse,N8o,q8o,SD,G8o,O8o,X8o,Sv,fse,V8o,z8o,PD,W8o,Q8o,H8o,Pv,gse,U8o,J8o,$D,Y8o,K8o,Z8o,$v,hse,e7o,o7o,ID,r7o,t7o,a7o,Iv,use,s7o,n7o,DD,l7o,i7o,d7o,Dv,pse,c7o,m7o,jD,f7o,g7o,h7o,jv,_se,u7o,p7o,ND,_7o,b7o,v7o,Nv,bse,T7o,F7o,qD,C7o,M7o,E7o,qv,vse,y7o,w7o,GD,A7o,L7o,B7o,Gv,x7o,Tse,k7o,R7o,Fse,S7o,P7o,Cse,$7o,I7o,mw,lBe,pd,Ov,Mse,fw,D7o,Ese,j7o,iBe,rr,gw,N7o,_d,q7o,yse,G7o,O7o,wse,X7o,V7o,z7o,hw,W7o,Ase,Q7o,H7o,U7o,Ur,uw,J7o,Lse,Y7o,K7o,bd,Z7o,Bse,e9o,o9o,xse,r9o,t9o,a9o,kse,s9o,n9o,pw,l9o,Ge,_w,i9o,Rse,d9o,c9o,Ha,m9o,Sse,f9o,g9o,Pse,h9o,u9o,$se,p9o,_9o,b9o,R,Xv,Ise,v9o,T9o,OD,F9o,C9o,M9o,Vv,Dse,E9o,y9o,XD,w9o,A9o,L9o,zv,jse,B9o,x9o,VD,k9o,R9o,S9o,Wv,Nse,P9o,$9o,zD,I9o,D9o,j9o,Qv,qse,N9o,q9o,WD,G9o,O9o,X9o,Hv,Gse,V9o,z9o,QD,W9o,Q9o,H9o,Uv,Ose,U9o,J9o,HD,Y9o,K9o,Z9o,Jv,Xse,eBo,oBo,UD,rBo,tBo,aBo,Yv,Vse,sBo,nBo,JD,lBo,iBo,dBo,Kv,zse,cBo,mBo,YD,fBo,gBo,hBo,Zv,Wse,uBo,pBo,KD,_Bo,bBo,vBo,eT,Qse,TBo,FBo,ZD,CBo,MBo,EBo,oT,Hse,yBo,wBo,ej,ABo,LBo,BBo,rT,Use,xBo,kBo,oj,RBo,SBo,PBo,tT,Jse,$Bo,IBo,rj,DBo,jBo,NBo,aT,Yse,qBo,GBo,tj,OBo,XBo,VBo,sT,Kse,zBo,WBo,aj,QBo,HBo,UBo,nT,Zse,JBo,YBo,sj,KBo,ZBo,exo,lT,ene,oxo,rxo,nj,txo,axo,sxo,iT,one,nxo,lxo,lj,ixo,dxo,cxo,dT,rne,mxo,fxo,ij,gxo,hxo,uxo,cT,tne,pxo,_xo,dj,bxo,vxo,Txo,mT,ane,Fxo,Cxo,cj,Mxo,Exo,yxo,fT,sne,wxo,Axo,mj,Lxo,Bxo,xxo,gT,nne,kxo,Rxo,fj,Sxo,Pxo,$xo,hT,lne,Ixo,Dxo,gj,jxo,Nxo,qxo,uT,ine,Gxo,Oxo,hj,Xxo,Vxo,zxo,pT,dne,Wxo,Qxo,uj,Hxo,Uxo,Jxo,_T,cne,Yxo,Kxo,pj,Zxo,eko,oko,bT,mne,rko,tko,_j,ako,sko,nko,vT,fne,lko,iko,bj,dko,cko,mko,TT,gne,fko,gko,vj,hko,uko,pko,FT,hne,_ko,bko,Tj,vko,Tko,Fko,CT,une,Cko,Mko,Fj,Eko,yko,wko,MT,pne,Ako,Lko,Cj,Bko,xko,kko,ET,_ne,Rko,Sko,Mj,Pko,$ko,Iko,yT,bne,Dko,jko,Ej,Nko,qko,Gko,wT,vne,Oko,Xko,yj,Vko,zko,Wko,AT,Tne,Qko,Hko,wj,Uko,Jko,Yko,LT,Kko,Fne,Zko,eRo,Cne,oRo,rRo,Mne,tRo,aRo,bw,dBe,vd,BT,Ene,vw,sRo,yne,nRo,cBe,tr,Tw,lRo,Td,iRo,wne,dRo,cRo,Ane,mRo,fRo,gRo,Fw,hRo,Lne,uRo,pRo,_Ro,Jr,Cw,bRo,Bne,vRo,TRo,Fd,FRo,xne,CRo,MRo,kne,ERo,yRo,wRo,Rne,ARo,LRo,Mw,BRo,Oe,Ew,xRo,Sne,kRo,RRo,Ua,SRo,Pne,PRo,$Ro,$ne,IRo,DRo,Ine,jRo,NRo,qRo,Dne,xT,jne,GRo,ORo,Aj,XRo,VRo,zRo,kT,WRo,Nne,QRo,HRo,qne,URo,JRo,Gne,YRo,KRo,yw,mBe,Cd,RT,One,ww,ZRo,Xne,eSo,fBe,ar,Aw,oSo,Md,rSo,Vne,tSo,aSo,zne,sSo,nSo,lSo,Lw,iSo,Wne,dSo,cSo,mSo,Yr,Bw,fSo,Qne,gSo,hSo,Ed,uSo,Hne,pSo,_So,Une,bSo,vSo,TSo,Jne,FSo,CSo,xw,MSo,Xe,kw,ESo,Yne,ySo,wSo,Ja,ASo,Kne,LSo,BSo,Zne,xSo,kSo,ele,RSo,SSo,PSo,be,ST,ole,$So,ISo,Lj,DSo,jSo,NSo,PT,rle,qSo,GSo,Bj,OSo,XSo,VSo,$n,tle,zSo,WSo,xj,QSo,HSo,kj,USo,JSo,YSo,$T,ale,KSo,ZSo,Rj,ePo,oPo,rPo,la,sle,tPo,aPo,Sj,sPo,nPo,Pj,lPo,iPo,$j,dPo,cPo,mPo,IT,nle,fPo,gPo,Ij,hPo,uPo,pPo,DT,lle,_Po,bPo,Dj,vPo,TPo,FPo,jT,ile,CPo,MPo,jj,EPo,yPo,wPo,NT,dle,APo,LPo,Nj,BPo,xPo,kPo,qT,RPo,cle,SPo,PPo,mle,$Po,IPo,fle,DPo,jPo,Rw,gBe,yd,GT,gle,Sw,NPo,hle,qPo,hBe,sr,Pw,GPo,wd,OPo,ule,XPo,VPo,ple,zPo,WPo,QPo,$w,HPo,_le,UPo,JPo,YPo,Kr,Iw,KPo,ble,ZPo,e$o,Ad,o$o,vle,r$o,t$o,Tle,a$o,s$o,n$o,Fle,l$o,i$o,Dw,d$o,Ve,jw,c$o,Cle,m$o,f$o,Ya,g$o,Mle,h$o,u$o,Ele,p$o,_$o,yle,b$o,v$o,T$o,wle,OT,Ale,F$o,C$o,qj,M$o,E$o,y$o,XT,w$o,Lle,A$o,L$o,Ble,B$o,x$o,xle,k$o,R$o,Nw,uBe,Ld,VT,kle,qw,S$o,Rle,P$o,pBe,nr,Gw,$$o,Bd,I$o,Sle,D$o,j$o,Ple,N$o,q$o,G$o,Ow,O$o,$le,X$o,V$o,z$o,Zr,Xw,W$o,Ile,Q$o,H$o,xd,U$o,Dle,J$o,Y$o,jle,K$o,Z$o,eIo,Nle,oIo,rIo,Vw,tIo,ze,zw,aIo,qle,sIo,nIo,Ka,lIo,Gle,iIo,dIo,Ole,cIo,mIo,Xle,fIo,gIo,hIo,Ae,zT,Vle,uIo,pIo,Gj,_Io,bIo,vIo,WT,zle,TIo,FIo,Oj,CIo,MIo,EIo,QT,Wle,yIo,wIo,Xj,AIo,LIo,BIo,HT,Qle,xIo,kIo,Vj,RIo,SIo,PIo,UT,Hle,$Io,IIo,zj,DIo,jIo,NIo,JT,Ule,qIo,GIo,Wj,OIo,XIo,VIo,YT,Jle,zIo,WIo,Qj,QIo,HIo,UIo,KT,Yle,JIo,YIo,Hj,KIo,ZIo,eDo,ZT,oDo,Kle,rDo,tDo,Zle,aDo,sDo,eie,nDo,lDo,Ww,_Be,kd,e1,oie,Qw,iDo,rie,dDo,bBe,lr,Hw,cDo,Rd,mDo,tie,fDo,gDo,aie,hDo,uDo,pDo,Uw,_Do,sie,bDo,vDo,TDo,et,Jw,FDo,nie,CDo,MDo,Sd,EDo,lie,yDo,wDo,iie,ADo,LDo,BDo,die,xDo,kDo,Yw,RDo,We,Kw,SDo,cie,PDo,$Do,Za,IDo,mie,DDo,jDo,fie,NDo,qDo,gie,GDo,ODo,XDo,es,o1,hie,VDo,zDo,Uj,WDo,QDo,HDo,r1,uie,UDo,JDo,Jj,YDo,KDo,ZDo,t1,pie,ejo,ojo,Yj,rjo,tjo,ajo,a1,_ie,sjo,njo,Kj,ljo,ijo,djo,s1,cjo,bie,mjo,fjo,vie,gjo,hjo,Tie,ujo,pjo,Zw,vBe,Pd,n1,Fie,e6,_jo,Cie,bjo,TBe,ir,o6,vjo,$d,Tjo,Mie,Fjo,Cjo,Eie,Mjo,Ejo,yjo,r6,wjo,yie,Ajo,Ljo,Bjo,ot,t6,xjo,wie,kjo,Rjo,Id,Sjo,Aie,Pjo,$jo,Lie,Ijo,Djo,jjo,Bie,Njo,qjo,a6,Gjo,Qe,s6,Ojo,xie,Xjo,Vjo,os,zjo,kie,Wjo,Qjo,Rie,Hjo,Ujo,Sie,Jjo,Yjo,Kjo,Le,l1,Pie,Zjo,eNo,Zj,oNo,rNo,tNo,i1,$ie,aNo,sNo,eN,nNo,lNo,iNo,d1,Iie,dNo,cNo,oN,mNo,fNo,gNo,c1,Die,hNo,uNo,rN,pNo,_No,bNo,m1,jie,vNo,TNo,tN,FNo,CNo,MNo,f1,Nie,ENo,yNo,aN,wNo,ANo,LNo,g1,qie,BNo,xNo,sN,kNo,RNo,SNo,h1,Gie,PNo,$No,nN,INo,DNo,jNo,u1,NNo,Oie,qNo,GNo,Xie,ONo,XNo,Vie,VNo,zNo,n6,FBe,Dd,p1,zie,l6,WNo,Wie,QNo,CBe,dr,i6,HNo,jd,UNo,Qie,JNo,YNo,Hie,KNo,ZNo,eqo,d6,oqo,Uie,rqo,tqo,aqo,rt,c6,sqo,Jie,nqo,lqo,Nd,iqo,Yie,dqo,cqo,Kie,mqo,fqo,gqo,Zie,hqo,uqo,m6,pqo,He,f6,_qo,ede,bqo,vqo,rs,Tqo,ode,Fqo,Cqo,rde,Mqo,Eqo,tde,yqo,wqo,Aqo,g6,_1,ade,Lqo,Bqo,lN,xqo,kqo,Rqo,b1,sde,Sqo,Pqo,iN,$qo,Iqo,Dqo,v1,jqo,nde,Nqo,qqo,lde,Gqo,Oqo,ide,Xqo,Vqo,h6,MBe,qd,T1,dde,u6,zqo,cde,Wqo,EBe,cr,p6,Qqo,Gd,Hqo,mde,Uqo,Jqo,fde,Yqo,Kqo,Zqo,_6,eGo,gde,oGo,rGo,tGo,tt,b6,aGo,hde,sGo,nGo,Od,lGo,ude,iGo,dGo,pde,cGo,mGo,fGo,_de,gGo,hGo,v6,uGo,Ue,T6,pGo,bde,_Go,bGo,ts,vGo,vde,TGo,FGo,Tde,CGo,MGo,Fde,EGo,yGo,wGo,as,F1,Cde,AGo,LGo,dN,BGo,xGo,kGo,C1,Mde,RGo,SGo,cN,PGo,$Go,IGo,M1,Ede,DGo,jGo,mN,NGo,qGo,GGo,E1,yde,OGo,XGo,fN,VGo,zGo,WGo,y1,QGo,wde,HGo,UGo,Ade,JGo,YGo,Lde,KGo,ZGo,F6,yBe,Xd,w1,Bde,C6,eOo,xde,oOo,wBe,mr,M6,rOo,Vd,tOo,kde,aOo,sOo,Rde,nOo,lOo,iOo,E6,dOo,Sde,cOo,mOo,fOo,at,y6,gOo,Pde,hOo,uOo,zd,pOo,$de,_Oo,bOo,Ide,vOo,TOo,FOo,Dde,COo,MOo,w6,EOo,Je,A6,yOo,jde,wOo,AOo,ss,LOo,Nde,BOo,xOo,qde,kOo,ROo,Gde,SOo,POo,$Oo,Wd,A1,Ode,IOo,DOo,gN,jOo,NOo,qOo,L1,Xde,GOo,OOo,hN,XOo,VOo,zOo,B1,Vde,WOo,QOo,uN,HOo,UOo,JOo,x1,YOo,zde,KOo,ZOo,Wde,eXo,oXo,Qde,rXo,tXo,L6,ABe,Qd,k1,Hde,B6,aXo,Ude,sXo,LBe,fr,x6,nXo,Hd,lXo,Jde,iXo,dXo,Yde,cXo,mXo,fXo,k6,gXo,Kde,hXo,uXo,pXo,st,R6,_Xo,Zde,bXo,vXo,Ud,TXo,ece,FXo,CXo,oce,MXo,EXo,yXo,rce,wXo,AXo,S6,LXo,Ye,P6,BXo,tce,xXo,kXo,ns,RXo,ace,SXo,PXo,sce,$Xo,IXo,nce,DXo,jXo,NXo,lce,R1,ice,qXo,GXo,pN,OXo,XXo,VXo,S1,zXo,dce,WXo,QXo,cce,HXo,UXo,mce,JXo,YXo,$6,BBe,Jd,P1,fce,I6,KXo,gce,ZXo,xBe,gr,D6,eVo,Yd,oVo,hce,rVo,tVo,uce,aVo,sVo,nVo,j6,lVo,pce,iVo,dVo,cVo,nt,N6,mVo,_ce,fVo,gVo,Kd,hVo,bce,uVo,pVo,vce,_Vo,bVo,vVo,Tce,TVo,FVo,q6,CVo,Ke,G6,MVo,Fce,EVo,yVo,ls,wVo,Cce,AVo,LVo,Mce,BVo,xVo,Ece,kVo,RVo,SVo,yce,$1,wce,PVo,$Vo,_N,IVo,DVo,jVo,I1,NVo,Ace,qVo,GVo,Lce,OVo,XVo,Bce,VVo,zVo,O6,kBe,Zd,D1,xce,X6,WVo,kce,QVo,RBe,hr,V6,HVo,ec,UVo,Rce,JVo,YVo,Sce,KVo,ZVo,ezo,z6,ozo,Pce,rzo,tzo,azo,lt,W6,szo,$ce,nzo,lzo,oc,izo,Ice,dzo,czo,Dce,mzo,fzo,gzo,jce,hzo,uzo,Q6,pzo,Ze,H6,_zo,Nce,bzo,vzo,is,Tzo,qce,Fzo,Czo,Gce,Mzo,Ezo,Oce,yzo,wzo,Azo,U6,j1,Xce,Lzo,Bzo,bN,xzo,kzo,Rzo,N1,Vce,Szo,Pzo,vN,$zo,Izo,Dzo,q1,jzo,zce,Nzo,qzo,Wce,Gzo,Ozo,Qce,Xzo,Vzo,J6,SBe,rc,G1,Hce,Y6,zzo,Uce,Wzo,PBe,ur,K6,Qzo,tc,Hzo,Jce,Uzo,Jzo,Yce,Yzo,Kzo,Zzo,Z6,eWo,Kce,oWo,rWo,tWo,it,eA,aWo,Zce,sWo,nWo,ac,lWo,eme,iWo,dWo,ome,cWo,mWo,fWo,rme,gWo,hWo,oA,uWo,go,rA,pWo,tme,_Wo,bWo,ds,vWo,ame,TWo,FWo,sme,CWo,MWo,nme,EWo,yWo,wWo,B,O1,lme,AWo,LWo,TN,BWo,xWo,kWo,X1,ime,RWo,SWo,FN,PWo,$Wo,IWo,V1,dme,DWo,jWo,CN,NWo,qWo,GWo,z1,cme,OWo,XWo,MN,VWo,zWo,WWo,W1,mme,QWo,HWo,EN,UWo,JWo,YWo,Q1,fme,KWo,ZWo,yN,eQo,oQo,rQo,H1,gme,tQo,aQo,wN,sQo,nQo,lQo,U1,hme,iQo,dQo,AN,cQo,mQo,fQo,J1,ume,gQo,hQo,LN,uQo,pQo,_Qo,Y1,pme,bQo,vQo,BN,TQo,FQo,CQo,K1,_me,MQo,EQo,xN,yQo,wQo,AQo,Z1,bme,LQo,BQo,kN,xQo,kQo,RQo,eF,vme,SQo,PQo,RN,$Qo,IQo,DQo,oF,Tme,jQo,NQo,SN,qQo,GQo,OQo,rF,Fme,XQo,VQo,PN,zQo,WQo,QQo,tF,Cme,HQo,UQo,$N,JQo,YQo,KQo,In,Mme,ZQo,eHo,IN,oHo,rHo,DN,tHo,aHo,sHo,aF,Eme,nHo,lHo,jN,iHo,dHo,cHo,sF,yme,mHo,fHo,NN,gHo,hHo,uHo,nF,wme,pHo,_Ho,qN,bHo,vHo,THo,lF,Ame,FHo,CHo,GN,MHo,EHo,yHo,iF,Lme,wHo,AHo,ON,LHo,BHo,xHo,dF,Bme,kHo,RHo,XN,SHo,PHo,$Ho,cF,xme,IHo,DHo,VN,jHo,NHo,qHo,mF,kme,GHo,OHo,zN,XHo,VHo,zHo,fF,Rme,WHo,QHo,WN,HHo,UHo,JHo,gF,Sme,YHo,KHo,QN,ZHo,eUo,oUo,hF,Pme,rUo,tUo,HN,aUo,sUo,nUo,uF,$me,lUo,iUo,UN,dUo,cUo,mUo,pF,Ime,fUo,gUo,JN,hUo,uUo,pUo,_F,Dme,_Uo,bUo,YN,vUo,TUo,FUo,bF,jme,CUo,MUo,KN,EUo,yUo,wUo,vF,Nme,AUo,LUo,ZN,BUo,xUo,kUo,TF,qme,RUo,SUo,eq,PUo,$Uo,IUo,FF,Gme,DUo,jUo,oq,NUo,qUo,GUo,CF,Ome,OUo,XUo,rq,VUo,zUo,WUo,MF,Xme,QUo,HUo,tq,UUo,JUo,YUo,EF,Vme,KUo,ZUo,aq,eJo,oJo,rJo,yF,zme,tJo,aJo,sq,sJo,nJo,lJo,wF,Wme,iJo,dJo,nq,cJo,mJo,fJo,AF,Qme,gJo,hJo,lq,uJo,pJo,_Jo,LF,Hme,bJo,vJo,iq,TJo,FJo,CJo,Ume,MJo,EJo,tA,$Be,sc,BF,Jme,aA,yJo,Yme,wJo,IBe,pr,sA,AJo,nc,LJo,Kme,BJo,xJo,Zme,kJo,RJo,SJo,nA,PJo,efe,$Jo,IJo,DJo,dt,lA,jJo,ofe,NJo,qJo,lc,GJo,rfe,OJo,XJo,tfe,VJo,zJo,WJo,afe,QJo,HJo,iA,UJo,ho,dA,JJo,sfe,YJo,KJo,cs,ZJo,nfe,eYo,oYo,lfe,rYo,tYo,ife,aYo,sYo,nYo,H,xF,dfe,lYo,iYo,dq,dYo,cYo,mYo,kF,cfe,fYo,gYo,cq,hYo,uYo,pYo,RF,mfe,_Yo,bYo,mq,vYo,TYo,FYo,SF,ffe,CYo,MYo,fq,EYo,yYo,wYo,PF,gfe,AYo,LYo,gq,BYo,xYo,kYo,$F,hfe,RYo,SYo,hq,PYo,$Yo,IYo,IF,ufe,DYo,jYo,uq,NYo,qYo,GYo,DF,pfe,OYo,XYo,pq,VYo,zYo,WYo,jF,_fe,QYo,HYo,_q,UYo,JYo,YYo,NF,bfe,KYo,ZYo,bq,eKo,oKo,rKo,qF,vfe,tKo,aKo,vq,sKo,nKo,lKo,GF,Tfe,iKo,dKo,Tq,cKo,mKo,fKo,OF,Ffe,gKo,hKo,Fq,uKo,pKo,_Ko,XF,Cfe,bKo,vKo,Cq,TKo,FKo,CKo,VF,Mfe,MKo,EKo,Mq,yKo,wKo,AKo,zF,Efe,LKo,BKo,Eq,xKo,kKo,RKo,WF,yfe,SKo,PKo,yq,$Ko,IKo,DKo,QF,wfe,jKo,NKo,wq,qKo,GKo,OKo,HF,Afe,XKo,VKo,Aq,zKo,WKo,QKo,UF,Lfe,HKo,UKo,Lq,JKo,YKo,KKo,JF,Bfe,ZKo,eZo,Bq,oZo,rZo,tZo,YF,xfe,aZo,sZo,xq,nZo,lZo,iZo,kfe,dZo,cZo,cA,DBe,ic,KF,Rfe,mA,mZo,Sfe,fZo,jBe,_r,fA,gZo,dc,hZo,Pfe,uZo,pZo,$fe,_Zo,bZo,vZo,gA,TZo,Ife,FZo,CZo,MZo,ct,hA,EZo,Dfe,yZo,wZo,cc,AZo,jfe,LZo,BZo,Nfe,xZo,kZo,RZo,qfe,SZo,PZo,uA,$Zo,uo,pA,IZo,Gfe,DZo,jZo,ms,NZo,Ofe,qZo,GZo,Xfe,OZo,XZo,Vfe,VZo,zZo,WZo,he,ZF,zfe,QZo,HZo,kq,UZo,JZo,YZo,eC,Wfe,KZo,ZZo,Rq,eer,oer,rer,oC,Qfe,ter,aer,Sq,ser,ner,ler,rC,Hfe,ier,der,Pq,cer,mer,fer,tC,Ufe,ger,her,$q,uer,per,_er,aC,Jfe,ber,ver,Iq,Ter,Fer,Cer,sC,Yfe,Mer,Eer,Dq,yer,wer,Aer,nC,Kfe,Ler,Ber,jq,xer,ker,Rer,lC,Zfe,Ser,Per,Nq,$er,Ier,Der,iC,ege,jer,Ner,qq,qer,Ger,Oer,oge,Xer,Ver,_A,NBe,mc,dC,rge,bA,zer,tge,Wer,qBe,br,vA,Qer,fc,Her,age,Uer,Jer,sge,Yer,Ker,Zer,TA,eor,nge,oor,ror,tor,mt,FA,aor,lge,sor,nor,gc,lor,ige,ior,dor,dge,cor,mor,gor,cge,hor,uor,CA,por,po,MA,_or,mge,bor,vor,fs,Tor,fge,For,Cor,gge,Mor,Eor,hge,yor,wor,Aor,EA,cC,uge,Lor,Bor,Gq,xor,kor,Ror,mC,pge,Sor,Por,Oq,$or,Ior,Dor,_ge,jor,Nor,yA,GBe,hc,fC,bge,wA,qor,vge,Gor,OBe,vr,AA,Oor,uc,Xor,Tge,Vor,zor,Fge,Wor,Qor,Hor,LA,Uor,Cge,Jor,Yor,Kor,ft,BA,Zor,Mge,err,orr,pc,rrr,Ege,trr,arr,yge,srr,nrr,lrr,wge,irr,drr,xA,crr,_o,kA,mrr,Age,frr,grr,gs,hrr,Lge,urr,prr,Bge,_rr,brr,xge,vrr,Trr,Frr,Y,gC,kge,Crr,Mrr,Xq,Err,yrr,wrr,hC,Rge,Arr,Lrr,Vq,Brr,xrr,krr,uC,Sge,Rrr,Srr,zq,Prr,$rr,Irr,pC,Pge,Drr,jrr,Wq,Nrr,qrr,Grr,_C,$ge,Orr,Xrr,Qq,Vrr,zrr,Wrr,bC,Ige,Qrr,Hrr,Hq,Urr,Jrr,Yrr,vC,Dge,Krr,Zrr,Uq,etr,otr,rtr,TC,jge,ttr,atr,Jq,str,ntr,ltr,FC,Nge,itr,dtr,Yq,ctr,mtr,ftr,CC,qge,gtr,htr,Kq,utr,ptr,_tr,MC,Gge,btr,vtr,Zq,Ttr,Ftr,Ctr,EC,Oge,Mtr,Etr,eG,ytr,wtr,Atr,yC,Xge,Ltr,Btr,oG,xtr,ktr,Rtr,wC,Vge,Str,Ptr,rG,$tr,Itr,Dtr,AC,zge,jtr,Ntr,tG,qtr,Gtr,Otr,LC,Wge,Xtr,Vtr,aG,ztr,Wtr,Qtr,BC,Qge,Htr,Utr,sG,Jtr,Ytr,Ktr,xC,Hge,Ztr,ear,nG,oar,rar,tar,kC,Uge,aar,sar,lG,nar,lar,iar,RC,Jge,dar,car,iG,mar,far,gar,Yge,har,uar,RA,XBe,_c,SC,Kge,SA,par,Zge,_ar,VBe,Tr,PA,bar,bc,Tar,ehe,Far,Car,ohe,Mar,Ear,yar,$A,war,rhe,Aar,Lar,Bar,gt,IA,xar,the,kar,Rar,vc,Sar,ahe,Par,$ar,she,Iar,Dar,jar,nhe,Nar,qar,DA,Gar,bo,jA,Oar,lhe,Xar,Var,hs,zar,ihe,War,Qar,dhe,Har,Uar,che,Jar,Yar,Kar,ue,PC,mhe,Zar,esr,dG,osr,rsr,tsr,$C,fhe,asr,ssr,cG,nsr,lsr,isr,IC,ghe,dsr,csr,mG,msr,fsr,gsr,DC,hhe,hsr,usr,fG,psr,_sr,bsr,jC,uhe,vsr,Tsr,gG,Fsr,Csr,Msr,NC,phe,Esr,ysr,hG,wsr,Asr,Lsr,qC,_he,Bsr,xsr,uG,ksr,Rsr,Ssr,GC,bhe,Psr,$sr,pG,Isr,Dsr,jsr,OC,vhe,Nsr,qsr,_G,Gsr,Osr,Xsr,XC,The,Vsr,zsr,bG,Wsr,Qsr,Hsr,Fhe,Usr,Jsr,NA,zBe,Tc,VC,Che,qA,Ysr,Mhe,Ksr,WBe,Fr,GA,Zsr,Fc,enr,Ehe,onr,rnr,yhe,tnr,anr,snr,OA,nnr,whe,lnr,inr,dnr,ht,XA,cnr,Ahe,mnr,fnr,Cc,gnr,Lhe,hnr,unr,Bhe,pnr,_nr,bnr,xhe,vnr,Tnr,VA,Fnr,vo,zA,Cnr,khe,Mnr,Enr,us,ynr,Rhe,wnr,Anr,She,Lnr,Bnr,Phe,xnr,knr,Rnr,X,zC,$he,Snr,Pnr,vG,$nr,Inr,Dnr,WC,Ihe,jnr,Nnr,TG,qnr,Gnr,Onr,QC,Dhe,Xnr,Vnr,FG,znr,Wnr,Qnr,HC,jhe,Hnr,Unr,CG,Jnr,Ynr,Knr,UC,Nhe,Znr,elr,MG,olr,rlr,tlr,JC,qhe,alr,slr,EG,nlr,llr,ilr,YC,Ghe,dlr,clr,yG,mlr,flr,glr,KC,Ohe,hlr,ulr,wG,plr,_lr,blr,ZC,Xhe,vlr,Tlr,AG,Flr,Clr,Mlr,e4,Vhe,Elr,ylr,LG,wlr,Alr,Llr,o4,zhe,Blr,xlr,BG,klr,Rlr,Slr,r4,Whe,Plr,$lr,xG,Ilr,Dlr,jlr,t4,Qhe,Nlr,qlr,kG,Glr,Olr,Xlr,a4,Hhe,Vlr,zlr,RG,Wlr,Qlr,Hlr,s4,Uhe,Ulr,Jlr,SG,Ylr,Klr,Zlr,n4,Jhe,eir,oir,PG,rir,tir,air,l4,Yhe,sir,nir,$G,lir,iir,dir,i4,Khe,cir,mir,IG,fir,gir,hir,d4,Zhe,uir,pir,DG,_ir,bir,vir,c4,eue,Tir,Fir,jG,Cir,Mir,Eir,m4,oue,yir,wir,NG,Air,Lir,Bir,f4,rue,xir,kir,qG,Rir,Sir,Pir,g4,tue,$ir,Iir,GG,Dir,jir,Nir,h4,aue,qir,Gir,OG,Oir,Xir,Vir,u4,sue,zir,Wir,XG,Qir,Hir,Uir,nue,Jir,Yir,WA,QBe,Mc,p4,lue,QA,Kir,iue,Zir,HBe,Cr,HA,edr,Ec,odr,due,rdr,tdr,cue,adr,sdr,ndr,UA,ldr,mue,idr,ddr,cdr,ut,JA,mdr,fue,fdr,gdr,yc,hdr,gue,udr,pdr,hue,_dr,bdr,vdr,uue,Tdr,Fdr,YA,Cdr,To,KA,Mdr,pue,Edr,ydr,ps,wdr,_ue,Adr,Ldr,bue,Bdr,xdr,vue,kdr,Rdr,Sdr,te,_4,Tue,Pdr,$dr,VG,Idr,Ddr,jdr,b4,Fue,Ndr,qdr,zG,Gdr,Odr,Xdr,v4,Cue,Vdr,zdr,WG,Wdr,Qdr,Hdr,T4,Mue,Udr,Jdr,QG,Ydr,Kdr,Zdr,F4,Eue,ecr,ocr,HG,rcr,tcr,acr,C4,yue,scr,ncr,UG,lcr,icr,dcr,M4,wue,ccr,mcr,JG,fcr,gcr,hcr,E4,Aue,ucr,pcr,YG,_cr,bcr,vcr,y4,Lue,Tcr,Fcr,KG,Ccr,Mcr,Ecr,w4,Bue,ycr,wcr,ZG,Acr,Lcr,Bcr,A4,xue,xcr,kcr,eO,Rcr,Scr,Pcr,L4,kue,$cr,Icr,oO,Dcr,jcr,Ncr,B4,Rue,qcr,Gcr,rO,Ocr,Xcr,Vcr,x4,Sue,zcr,Wcr,tO,Qcr,Hcr,Ucr,k4,Pue,Jcr,Ycr,aO,Kcr,Zcr,emr,R4,$ue,omr,rmr,sO,tmr,amr,smr,S4,Iue,nmr,lmr,nO,imr,dmr,cmr,Due,mmr,fmr,ZA,UBe,wc,P4,jue,e0,gmr,Nue,hmr,JBe,Mr,o0,umr,Ac,pmr,que,_mr,bmr,Gue,vmr,Tmr,Fmr,r0,Cmr,Oue,Mmr,Emr,ymr,pt,t0,wmr,Xue,Amr,Lmr,Lc,Bmr,Vue,xmr,kmr,zue,Rmr,Smr,Pmr,Wue,$mr,Imr,a0,Dmr,Fo,s0,jmr,Que,Nmr,qmr,_s,Gmr,Hue,Omr,Xmr,Uue,Vmr,zmr,Jue,Wmr,Qmr,Hmr,Yue,$4,Kue,Umr,Jmr,lO,Ymr,Kmr,Zmr,Zue,efr,ofr,n0,YBe,Bc,I4,epe,l0,rfr,ope,tfr,KBe,Er,i0,afr,xc,sfr,rpe,nfr,lfr,tpe,ifr,dfr,cfr,d0,mfr,ape,ffr,gfr,hfr,_t,c0,ufr,spe,pfr,_fr,kc,bfr,npe,vfr,Tfr,lpe,Ffr,Cfr,Mfr,ipe,Efr,yfr,m0,wfr,Co,f0,Afr,dpe,Lfr,Bfr,bs,xfr,cpe,kfr,Rfr,mpe,Sfr,Pfr,fpe,$fr,Ifr,Dfr,K,D4,gpe,jfr,Nfr,iO,qfr,Gfr,Ofr,j4,hpe,Xfr,Vfr,dO,zfr,Wfr,Qfr,N4,upe,Hfr,Ufr,cO,Jfr,Yfr,Kfr,q4,ppe,Zfr,egr,mO,ogr,rgr,tgr,G4,_pe,agr,sgr,fO,ngr,lgr,igr,O4,bpe,dgr,cgr,gO,mgr,fgr,ggr,X4,vpe,hgr,ugr,hO,pgr,_gr,bgr,V4,Tpe,vgr,Tgr,uO,Fgr,Cgr,Mgr,z4,Fpe,Egr,ygr,pO,wgr,Agr,Lgr,W4,Cpe,Bgr,xgr,_O,kgr,Rgr,Sgr,Q4,Mpe,Pgr,$gr,bO,Igr,Dgr,jgr,H4,Epe,Ngr,qgr,vO,Ggr,Ogr,Xgr,U4,ype,Vgr,zgr,TO,Wgr,Qgr,Hgr,J4,wpe,Ugr,Jgr,FO,Ygr,Kgr,Zgr,Y4,Ape,ehr,ohr,CO,rhr,thr,ahr,K4,Lpe,shr,nhr,MO,lhr,ihr,dhr,Z4,Bpe,chr,mhr,EO,fhr,ghr,hhr,eM,xpe,uhr,phr,yO,_hr,bhr,vhr,oM,kpe,Thr,Fhr,wO,Chr,Mhr,Ehr,rM,Rpe,yhr,whr,AO,Ahr,Lhr,Bhr,Spe,xhr,khr,g0,ZBe,Rc,tM,Ppe,h0,Rhr,$pe,Shr,exe,yr,u0,Phr,Sc,$hr,Ipe,Ihr,Dhr,Dpe,jhr,Nhr,qhr,p0,Ghr,jpe,Ohr,Xhr,Vhr,bt,_0,zhr,Npe,Whr,Qhr,Pc,Hhr,qpe,Uhr,Jhr,Gpe,Yhr,Khr,Zhr,Ope,eur,our,b0,rur,Mo,v0,tur,Xpe,aur,sur,vs,nur,Vpe,lur,iur,zpe,dur,cur,Wpe,mur,fur,gur,Z,aM,Qpe,hur,uur,LO,pur,_ur,bur,sM,Hpe,vur,Tur,BO,Fur,Cur,Mur,nM,Upe,Eur,yur,xO,wur,Aur,Lur,lM,Jpe,Bur,xur,kO,kur,Rur,Sur,iM,Ype,Pur,$ur,RO,Iur,Dur,jur,dM,Kpe,Nur,qur,SO,Gur,Our,Xur,cM,Zpe,Vur,zur,PO,Wur,Qur,Hur,mM,e_e,Uur,Jur,$O,Yur,Kur,Zur,fM,o_e,epr,opr,IO,rpr,tpr,apr,gM,r_e,spr,npr,DO,lpr,ipr,dpr,hM,t_e,cpr,mpr,jO,fpr,gpr,hpr,uM,a_e,upr,ppr,NO,_pr,bpr,vpr,pM,s_e,Tpr,Fpr,qO,Cpr,Mpr,Epr,_M,n_e,ypr,wpr,GO,Apr,Lpr,Bpr,bM,l_e,xpr,kpr,OO,Rpr,Spr,Ppr,vM,i_e,$pr,Ipr,XO,Dpr,jpr,Npr,TM,d_e,qpr,Gpr,VO,Opr,Xpr,Vpr,FM,c_e,zpr,Wpr,zO,Qpr,Hpr,Upr,CM,m_e,Jpr,Ypr,WO,Kpr,Zpr,e_r,f_e,o_r,r_r,T0,oxe,$c,MM,g_e,F0,t_r,h_e,a_r,rxe,wr,C0,s_r,Ic,n_r,u_e,l_r,i_r,p_e,d_r,c_r,m_r,M0,f_r,__e,g_r,h_r,u_r,vt,E0,p_r,b_e,__r,b_r,Dc,v_r,v_e,T_r,F_r,T_e,C_r,M_r,E_r,F_e,y_r,w_r,y0,A_r,Eo,w0,L_r,C_e,B_r,x_r,Ts,k_r,M_e,R_r,S_r,E_e,P_r,$_r,y_e,I_r,D_r,j_r,w_e,EM,A_e,N_r,q_r,QO,G_r,O_r,X_r,L_e,V_r,z_r,A0,txe,jc,yM,B_e,L0,W_r,x_e,Q_r,axe,Ar,B0,H_r,Nc,U_r,k_e,J_r,Y_r,R_e,K_r,Z_r,ebr,x0,obr,S_e,rbr,tbr,abr,Tt,k0,sbr,P_e,nbr,lbr,qc,ibr,$_e,dbr,cbr,I_e,mbr,fbr,gbr,D_e,hbr,ubr,R0,pbr,yo,S0,_br,j_e,bbr,vbr,Fs,Tbr,N_e,Fbr,Cbr,q_e,Mbr,Ebr,G_e,ybr,wbr,Abr,O_e,wM,X_e,Lbr,Bbr,HO,xbr,kbr,Rbr,V_e,Sbr,Pbr,P0,sxe,Gc,AM,z_e,$0,$br,W_e,Ibr,nxe,Lr,I0,Dbr,Oc,jbr,Q_e,Nbr,qbr,H_e,Gbr,Obr,Xbr,D0,Vbr,U_e,zbr,Wbr,Qbr,Ft,j0,Hbr,J_e,Ubr,Jbr,Xc,Ybr,Y_e,Kbr,Zbr,K_e,e2r,o2r,r2r,Z_e,t2r,a2r,N0,s2r,wo,q0,n2r,ebe,l2r,i2r,Cs,d2r,obe,c2r,m2r,rbe,f2r,g2r,tbe,h2r,u2r,p2r,z,LM,abe,_2r,b2r,UO,v2r,T2r,F2r,BM,sbe,C2r,M2r,JO,E2r,y2r,w2r,xM,nbe,A2r,L2r,YO,B2r,x2r,k2r,kM,lbe,R2r,S2r,KO,P2r,$2r,I2r,RM,ibe,D2r,j2r,ZO,N2r,q2r,G2r,SM,dbe,O2r,X2r,eX,V2r,z2r,W2r,PM,cbe,Q2r,H2r,oX,U2r,J2r,Y2r,$M,mbe,K2r,Z2r,rX,evr,ovr,rvr,IM,fbe,tvr,avr,tX,svr,nvr,lvr,DM,gbe,ivr,dvr,aX,cvr,mvr,fvr,jM,hbe,gvr,hvr,sX,uvr,pvr,_vr,NM,ube,bvr,vvr,nX,Tvr,Fvr,Cvr,qM,pbe,Mvr,Evr,lX,yvr,wvr,Avr,GM,_be,Lvr,Bvr,iX,xvr,kvr,Rvr,OM,bbe,Svr,Pvr,dX,$vr,Ivr,Dvr,XM,vbe,jvr,Nvr,cX,qvr,Gvr,Ovr,VM,Tbe,Xvr,Vvr,mX,zvr,Wvr,Qvr,zM,Fbe,Hvr,Uvr,fX,Jvr,Yvr,Kvr,WM,Cbe,Zvr,eTr,gX,oTr,rTr,tTr,QM,Mbe,aTr,sTr,hX,nTr,lTr,iTr,HM,Ebe,dTr,cTr,uX,mTr,fTr,gTr,UM,ybe,hTr,uTr,pX,pTr,_Tr,bTr,JM,wbe,vTr,TTr,_X,FTr,CTr,MTr,YM,Abe,ETr,yTr,bX,wTr,ATr,LTr,Lbe,BTr,xTr,G0,lxe,Vc,KM,Bbe,O0,kTr,xbe,RTr,ixe,Br,X0,STr,zc,PTr,kbe,$Tr,ITr,Rbe,DTr,jTr,NTr,V0,qTr,Sbe,GTr,OTr,XTr,Ct,z0,VTr,Pbe,zTr,WTr,Wc,QTr,$be,HTr,UTr,Ibe,JTr,YTr,KTr,Dbe,ZTr,e1r,W0,o1r,Ao,Q0,r1r,jbe,t1r,a1r,Ms,s1r,Nbe,n1r,l1r,qbe,i1r,d1r,Gbe,c1r,m1r,f1r,Es,ZM,Obe,g1r,h1r,vX,u1r,p1r,_1r,eE,Xbe,b1r,v1r,TX,T1r,F1r,C1r,oE,Vbe,M1r,E1r,FX,y1r,w1r,A1r,rE,zbe,L1r,B1r,CX,x1r,k1r,R1r,Wbe,S1r,P1r,H0,dxe,Qc,tE,Qbe,U0,$1r,Hbe,I1r,cxe,xr,J0,D1r,Hc,j1r,Ube,N1r,q1r,Jbe,G1r,O1r,X1r,Y0,V1r,Ybe,z1r,W1r,Q1r,Mt,K0,H1r,Kbe,U1r,J1r,Uc,Y1r,Zbe,K1r,Z1r,e2e,eFr,oFr,rFr,o2e,tFr,aFr,Z0,sFr,Lo,eL,nFr,r2e,lFr,iFr,ys,dFr,t2e,cFr,mFr,a2e,fFr,gFr,s2e,hFr,uFr,pFr,me,aE,n2e,_Fr,bFr,MX,vFr,TFr,FFr,sE,l2e,CFr,MFr,EX,EFr,yFr,wFr,nE,i2e,AFr,LFr,yX,BFr,xFr,kFr,lE,d2e,RFr,SFr,wX,PFr,$Fr,IFr,iE,c2e,DFr,jFr,AX,NFr,qFr,GFr,dE,m2e,OFr,XFr,LX,VFr,zFr,WFr,cE,f2e,QFr,HFr,BX,UFr,JFr,YFr,mE,g2e,KFr,ZFr,xX,eCr,oCr,rCr,fE,h2e,tCr,aCr,kX,sCr,nCr,lCr,gE,u2e,iCr,dCr,RX,cCr,mCr,fCr,hE,p2e,gCr,hCr,SX,uCr,pCr,_Cr,_2e,bCr,vCr,oL,mxe,Jc,uE,b2e,rL,TCr,v2e,FCr,fxe,kr,tL,CCr,Yc,MCr,T2e,ECr,yCr,F2e,wCr,ACr,LCr,aL,BCr,C2e,xCr,kCr,RCr,Et,sL,SCr,M2e,PCr,$Cr,Kc,ICr,E2e,DCr,jCr,y2e,NCr,qCr,GCr,w2e,OCr,XCr,nL,VCr,Bo,lL,zCr,A2e,WCr,QCr,ws,HCr,L2e,UCr,JCr,B2e,YCr,KCr,x2e,ZCr,e4r,o4r,ve,pE,k2e,r4r,t4r,PX,a4r,s4r,n4r,_E,R2e,l4r,i4r,$X,d4r,c4r,m4r,bE,S2e,f4r,g4r,IX,h4r,u4r,p4r,vE,P2e,_4r,b4r,DX,v4r,T4r,F4r,TE,$2e,C4r,M4r,jX,E4r,y4r,w4r,FE,I2e,A4r,L4r,NX,B4r,x4r,k4r,CE,D2e,R4r,S4r,qX,P4r,$4r,I4r,ME,j2e,D4r,j4r,GX,N4r,q4r,G4r,EE,N2e,O4r,X4r,OX,V4r,z4r,W4r,q2e,Q4r,H4r,iL,gxe,Zc,yE,G2e,dL,U4r,O2e,J4r,hxe,Rr,cL,Y4r,em,K4r,X2e,Z4r,eMr,V2e,oMr,rMr,tMr,mL,aMr,z2e,sMr,nMr,lMr,yt,fL,iMr,W2e,dMr,cMr,om,mMr,Q2e,fMr,gMr,H2e,hMr,uMr,pMr,U2e,_Mr,bMr,gL,vMr,xo,hL,TMr,J2e,FMr,CMr,As,MMr,Y2e,EMr,yMr,K2e,wMr,AMr,Z2e,LMr,BMr,xMr,Te,wE,eve,kMr,RMr,XX,SMr,PMr,$Mr,AE,ove,IMr,DMr,VX,jMr,NMr,qMr,LE,rve,GMr,OMr,zX,XMr,VMr,zMr,BE,tve,WMr,QMr,WX,HMr,UMr,JMr,xE,ave,YMr,KMr,QX,ZMr,eEr,oEr,kE,sve,rEr,tEr,HX,aEr,sEr,nEr,RE,nve,lEr,iEr,UX,dEr,cEr,mEr,SE,lve,fEr,gEr,JX,hEr,uEr,pEr,PE,ive,_Er,bEr,YX,vEr,TEr,FEr,dve,CEr,MEr,uL,uxe,rm,$E,cve,pL,EEr,mve,yEr,pxe,Sr,_L,wEr,tm,AEr,fve,LEr,BEr,gve,xEr,kEr,REr,bL,SEr,hve,PEr,$Er,IEr,wt,vL,DEr,uve,jEr,NEr,am,qEr,pve,GEr,OEr,_ve,XEr,VEr,zEr,bve,WEr,QEr,TL,HEr,ko,FL,UEr,vve,JEr,YEr,Ls,KEr,Tve,ZEr,e3r,Fve,o3r,r3r,Cve,t3r,a3r,s3r,Fe,IE,Mve,n3r,l3r,KX,i3r,d3r,c3r,DE,Eve,m3r,f3r,ZX,g3r,h3r,u3r,jE,yve,p3r,_3r,eV,b3r,v3r,T3r,NE,wve,F3r,C3r,oV,M3r,E3r,y3r,qE,Ave,w3r,A3r,rV,L3r,B3r,x3r,GE,Lve,k3r,R3r,tV,S3r,P3r,$3r,OE,Bve,I3r,D3r,aV,j3r,N3r,q3r,XE,xve,G3r,O3r,sV,X3r,V3r,z3r,VE,kve,W3r,Q3r,nV,H3r,U3r,J3r,Rve,Y3r,K3r,CL,_xe,sm,zE,Sve,ML,Z3r,Pve,e5r,bxe,Pr,EL,o5r,nm,r5r,$ve,t5r,a5r,Ive,s5r,n5r,l5r,yL,i5r,Dve,d5r,c5r,m5r,At,wL,f5r,jve,g5r,h5r,lm,u5r,Nve,p5r,_5r,qve,b5r,v5r,T5r,Gve,F5r,C5r,AL,M5r,Ro,LL,E5r,Ove,y5r,w5r,Bs,A5r,Xve,L5r,B5r,Vve,x5r,k5r,zve,R5r,S5r,P5r,Ce,WE,Wve,$5r,I5r,lV,D5r,j5r,N5r,QE,Qve,q5r,G5r,iV,O5r,X5r,V5r,HE,Hve,z5r,W5r,dV,Q5r,H5r,U5r,UE,Uve,J5r,Y5r,cV,K5r,Z5r,eyr,JE,Jve,oyr,ryr,mV,tyr,ayr,syr,YE,Yve,nyr,lyr,fV,iyr,dyr,cyr,KE,Kve,myr,fyr,gV,gyr,hyr,uyr,ZE,Zve,pyr,_yr,hV,byr,vyr,Tyr,e3,eTe,Fyr,Cyr,uV,Myr,Eyr,yyr,oTe,wyr,Ayr,BL,vxe,im,o3,rTe,xL,Lyr,tTe,Byr,Txe,$r,kL,xyr,dm,kyr,aTe,Ryr,Syr,sTe,Pyr,$yr,Iyr,RL,Dyr,nTe,jyr,Nyr,qyr,Lt,SL,Gyr,lTe,Oyr,Xyr,cm,Vyr,iTe,zyr,Wyr,dTe,Qyr,Hyr,Uyr,cTe,Jyr,Yyr,PL,Kyr,So,$L,Zyr,mTe,ewr,owr,xs,rwr,fTe,twr,awr,gTe,swr,nwr,hTe,lwr,iwr,dwr,no,r3,uTe,cwr,mwr,pV,fwr,gwr,hwr,t3,pTe,uwr,pwr,_V,_wr,bwr,vwr,a3,_Te,Twr,Fwr,bV,Cwr,Mwr,Ewr,s3,bTe,ywr,wwr,vV,Awr,Lwr,Bwr,n3,vTe,xwr,kwr,TV,Rwr,Swr,Pwr,l3,TTe,$wr,Iwr,FV,Dwr,jwr,Nwr,i3,FTe,qwr,Gwr,CV,Owr,Xwr,Vwr,CTe,zwr,Wwr,IL,Fxe,mm,d3,MTe,DL,Qwr,ETe,Hwr,Cxe,Ir,jL,Uwr,fm,Jwr,yTe,Ywr,Kwr,wTe,Zwr,e6r,o6r,NL,r6r,ATe,t6r,a6r,s6r,Bt,qL,n6r,LTe,l6r,i6r,gm,d6r,BTe,c6r,m6r,xTe,f6r,g6r,h6r,kTe,u6r,p6r,GL,_6r,Po,OL,b6r,RTe,v6r,T6r,ks,F6r,STe,C6r,M6r,PTe,E6r,y6r,$Te,w6r,A6r,L6r,lo,c3,ITe,B6r,x6r,MV,k6r,R6r,S6r,m3,DTe,P6r,$6r,EV,I6r,D6r,j6r,f3,jTe,N6r,q6r,yV,G6r,O6r,X6r,g3,NTe,V6r,z6r,wV,W6r,Q6r,H6r,h3,qTe,U6r,J6r,AV,Y6r,K6r,Z6r,u3,GTe,eAr,oAr,LV,rAr,tAr,aAr,p3,OTe,sAr,nAr,BV,lAr,iAr,dAr,XTe,cAr,mAr,XL,Mxe,hm,_3,VTe,VL,fAr,zTe,gAr,Exe,Dr,zL,hAr,um,uAr,WTe,pAr,_Ar,QTe,bAr,vAr,TAr,WL,FAr,HTe,CAr,MAr,EAr,xt,QL,yAr,UTe,wAr,AAr,pm,LAr,JTe,BAr,xAr,YTe,kAr,RAr,SAr,KTe,PAr,$Ar,HL,IAr,$o,UL,DAr,ZTe,jAr,NAr,Rs,qAr,e1e,GAr,OAr,o1e,XAr,VAr,r1e,zAr,WAr,QAr,t1e,b3,a1e,HAr,UAr,xV,JAr,YAr,KAr,s1e,ZAr,e0r,JL,yxe,_m,v3,n1e,YL,o0r,l1e,r0r,wxe,jr,KL,t0r,bm,a0r,i1e,s0r,n0r,d1e,l0r,i0r,d0r,ZL,c0r,c1e,m0r,f0r,g0r,kt,e8,h0r,m1e,u0r,p0r,vm,_0r,f1e,b0r,v0r,g1e,T0r,F0r,C0r,h1e,M0r,E0r,o8,y0r,Io,r8,w0r,u1e,A0r,L0r,Ss,B0r,p1e,x0r,k0r,_1e,R0r,S0r,b1e,P0r,$0r,I0r,t8,T3,v1e,D0r,j0r,kV,N0r,q0r,G0r,F3,T1e,O0r,X0r,RV,V0r,z0r,W0r,F1e,Q0r,H0r,a8,Axe,Tm,C3,C1e,s8,U0r,M1e,J0r,Lxe,Nr,n8,Y0r,Fm,K0r,E1e,Z0r,eLr,y1e,oLr,rLr,tLr,l8,aLr,w1e,sLr,nLr,lLr,Rt,i8,iLr,A1e,dLr,cLr,Cm,mLr,L1e,fLr,gLr,B1e,hLr,uLr,pLr,x1e,_Lr,bLr,d8,vLr,Do,c8,TLr,k1e,FLr,CLr,Ps,MLr,R1e,ELr,yLr,S1e,wLr,ALr,P1e,LLr,BLr,xLr,$1e,M3,I1e,kLr,RLr,SV,SLr,PLr,$Lr,D1e,ILr,DLr,m8,Bxe;return ce=new V({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased")',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),A5=new V({}),L5=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Bm=new jLr({props:{warning:"&lcub;true}",$$slots:{default:[Bvt]},$$scope:{ctx:Li}}}),B5=new V({}),x5=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L526"}}),S5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L549",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),P5=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),$5=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L671",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),I5=new V({}),D5=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L352"}}),q5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L366",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),G5=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),O5=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L562",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),X5=new V({}),V5=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),Q5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),fh=new jLr({props:{$$slots:{default:[xvt]},$$scope:{ctx:Li}}}),H5=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),U5=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),J5=new V({}),Y5=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L71"}}),ey=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Mh=new jLr({props:{$$slots:{default:[kvt]},$$scope:{ctx:Li}}}),oy=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),ry=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),ty=new V({}),ay=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L687"}}),ny=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),ly=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),iy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dy=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),cy=new V({}),my=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L694"}}),gy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),hy=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),uy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),py=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_y=new V({}),by=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L709"}}),Ty=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Fy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),Cy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),My=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ey=new V({}),yy=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L716"}}),Ay=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Ly=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),By=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ky=new V({}),Ry=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L723"}}),Py=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),Iy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jy=new V({}),Ny=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L732"}}),Gy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),Xy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Vy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zy=new V({}),Wy=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L766"}}),Hy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),Jy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Yy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ky=new V({}),Zy=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L773"}}),ow=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),tw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),aw=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),sw=new V({}),nw=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L759"}}),iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),dw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),cw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),mw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),fw=new V({}),gw=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L741"}}),uw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),pw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),_w=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),bw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),vw=new V({}),Tw=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L748"}}),Cw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Mw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),Ew=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ww=new V({}),Aw=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L782"}}),Bw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),xw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Rw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Sw=new V({}),Pw=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L812"}}),Iw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),jw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Nw=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qw=new V({}),Gw=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L819"}}),Xw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Vw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),zw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ww=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qw=new V({}),Hw=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L842"}}),Jw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),Kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e6=new V({}),o6=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L826"}}),t6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),a6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),s6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),n6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l6=new V({}),i6=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L833"}}),c6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),m6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),f6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),h6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),u6=new V({}),p6=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L851"}}),b6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),v6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),T6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),C6=new V({}),M6=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L858"}}),y6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),w6=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),A6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L6=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),B6=new V({}),x6=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L805"}}),R6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),S6=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),P6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$6=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),I6=new V({}),D6=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L789"}}),N6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),q6=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),G6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O6=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),X6=new V({}),V6=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L796"}}),W6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),Q6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),H6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),J6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Y6=new V({}),K6=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L373"}}),eA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),oA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),rA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aA=new V({}),sA=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L380"}}),lA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),iA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),dA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mA=new V({}),fA=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L395"}}),hA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),uA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),pA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_A=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bA=new V({}),vA=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L402"}}),FA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),CA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),MA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),wA=new V({}),AA=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L416"}}),BA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),xA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),kA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),RA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),SA=new V({}),PA=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L423"}}),IA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),DA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),jA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),NA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),qA=new V({}),GA=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),XA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),VA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),zA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),WA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),QA=new V({}),HA=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L468"}}),JA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),YA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),KA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ZA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),e0=new V({}),o0=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L448"}}),t0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),a0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),s0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),n0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),l0=new V({}),i0=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L459"}}),c0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),m0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),f0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),h0=new V({}),u0=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),_0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),b0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),v0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),T0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),F0=new V({}),C0=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L409"}}),E0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),y0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),w0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),A0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),L0=new V({}),B0=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L484"}}),k0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),R0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),S0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),P0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$0=new V({}),I0=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L229"}}),j0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),N0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),q0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),G0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),O0=new V({}),X0=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L243"}}),z0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),W0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),Q0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),H0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),U0=new V({}),J0=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L236"}}),K0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),Z0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),eL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rL=new V({}),tL=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L250"}}),sL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),nL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),lL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dL=new V({}),cL=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),fL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),gL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),hL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),pL=new V({}),_L=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),vL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),TL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),FL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ML=new V({}),EL=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L275"}}),wL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),AL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),LL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),BL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xL=new V({}),kL=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),SL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),PL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),$L=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),IL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DL=new V({}),jL=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L291"}}),qL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),GL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),OL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VL=new V({}),zL=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),QL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),HL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),UL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YL=new V({}),KL=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),e8=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),o8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),r8=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),s8=new V({}),n8=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L316"}}),i8=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),d8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),c8=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Be=l(),ie=a("h1"),fe=a("a"),so=a("span"),m(ce.$$.fragment),_e=l(),Go=a("span"),Bi=o("Auto Classes"),Em=l(),na=a("p"),xi=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),ki=a("code"),M5=o("from_pretrained()"),ym=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Ri=o("Instantiating one of "),$s=a("a"),E5=o("AutoConfig"),Is=o(", "),Ds=a("a"),y5=o("AutoModel"),Si=o(`, and
`),js=a("a"),w5=o("AutoTokenizer"),Pi=o(" will directly create a class of the relevant architecture. For instance"),wm=l(),m($a.$$.fragment),co=l(),ge=a("p"),d7=o("will create a model that is an instance of "),$i=a("a"),c7=o("BertModel"),m7=o("."),Oo=l(),Ia=a("p"),f7=o("There is one class of "),Am=a("code"),g7=o("AutoModel"),qRe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),x9e=l(),Ii=a("h2"),Lm=a("a"),EW=a("span"),m(A5.$$.fragment),GRe=l(),yW=a("span"),ORe=o("Extending the Auto Classes"),k9e=l(),Ns=a("p"),XRe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),wW=a("code"),VRe=o("NewModel"),zRe=o(", make sure you have a "),AW=a("code"),WRe=o("NewModelConfig"),QRe=o(` then you can add those to the auto
classes like this:`),R9e=l(),m(L5.$$.fragment),S9e=l(),h7=a("p"),HRe=o("You will then be able to use the auto classes like you would usually do!"),P9e=l(),m(Bm.$$.fragment),$9e=l(),Di=a("h2"),xm=a("a"),LW=a("span"),m(B5.$$.fragment),URe=l(),BW=a("span"),JRe=o("AutoConfig"),I9e=l(),Xo=a("div"),m(x5.$$.fragment),YRe=l(),k5=a("p"),KRe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),u7=a("a"),ZRe=o("from_pretrained()"),eSe=o(" class method."),oSe=l(),R5=a("p"),rSe=o("This class cannot be instantiated directly using "),xW=a("code"),tSe=o("__init__()"),aSe=o(" (throws an error)."),sSe=l(),mo=a("div"),m(S5.$$.fragment),nSe=l(),kW=a("p"),lSe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),iSe=l(),ji=a("p"),dSe=o("The configuration class to instantiate is selected based on the "),RW=a("code"),cSe=o("model_type"),mSe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),SW=a("code"),fSe=o("pretrained_model_name_or_path"),gSe=o(":"),hSe=l(),v=a("ul"),km=a("li"),PW=a("strong"),uSe=o("albert"),pSe=o(" \u2014 "),p7=a("a"),_Se=o("AlbertConfig"),bSe=o(" (ALBERT model)"),vSe=l(),Rm=a("li"),$W=a("strong"),TSe=o("bart"),FSe=o(" \u2014 "),_7=a("a"),CSe=o("BartConfig"),MSe=o(" (BART model)"),ESe=l(),Sm=a("li"),IW=a("strong"),ySe=o("beit"),wSe=o(" \u2014 "),b7=a("a"),ASe=o("BeitConfig"),LSe=o(" (BEiT model)"),BSe=l(),Pm=a("li"),DW=a("strong"),xSe=o("bert"),kSe=o(" \u2014 "),v7=a("a"),RSe=o("BertConfig"),SSe=o(" (BERT model)"),PSe=l(),$m=a("li"),jW=a("strong"),$Se=o("bert-generation"),ISe=o(" \u2014 "),T7=a("a"),DSe=o("BertGenerationConfig"),jSe=o(" (Bert Generation model)"),NSe=l(),Im=a("li"),NW=a("strong"),qSe=o("big_bird"),GSe=o(" \u2014 "),F7=a("a"),OSe=o("BigBirdConfig"),XSe=o(" (BigBird model)"),VSe=l(),Dm=a("li"),qW=a("strong"),zSe=o("bigbird_pegasus"),WSe=o(" \u2014 "),C7=a("a"),QSe=o("BigBirdPegasusConfig"),HSe=o(" (BigBirdPegasus model)"),USe=l(),jm=a("li"),GW=a("strong"),JSe=o("blenderbot"),YSe=o(" \u2014 "),M7=a("a"),KSe=o("BlenderbotConfig"),ZSe=o(" (Blenderbot model)"),ePe=l(),Nm=a("li"),OW=a("strong"),oPe=o("blenderbot-small"),rPe=o(" \u2014 "),E7=a("a"),tPe=o("BlenderbotSmallConfig"),aPe=o(" (BlenderbotSmall model)"),sPe=l(),qm=a("li"),XW=a("strong"),nPe=o("camembert"),lPe=o(" \u2014 "),y7=a("a"),iPe=o("CamembertConfig"),dPe=o(" (CamemBERT model)"),cPe=l(),Gm=a("li"),VW=a("strong"),mPe=o("canine"),fPe=o(" \u2014 "),w7=a("a"),gPe=o("CanineConfig"),hPe=o(" (Canine model)"),uPe=l(),Om=a("li"),zW=a("strong"),pPe=o("clip"),_Pe=o(" \u2014 "),A7=a("a"),bPe=o("CLIPConfig"),vPe=o(" (CLIP model)"),TPe=l(),Xm=a("li"),WW=a("strong"),FPe=o("convbert"),CPe=o(" \u2014 "),L7=a("a"),MPe=o("ConvBertConfig"),EPe=o(" (ConvBERT model)"),yPe=l(),Vm=a("li"),QW=a("strong"),wPe=o("convnext"),APe=o(" \u2014 "),B7=a("a"),LPe=o("ConvNextConfig"),BPe=o(" (ConvNext model)"),xPe=l(),zm=a("li"),HW=a("strong"),kPe=o("ctrl"),RPe=o(" \u2014 "),x7=a("a"),SPe=o("CTRLConfig"),PPe=o(" (CTRL model)"),$Pe=l(),Wm=a("li"),UW=a("strong"),IPe=o("data2vec-audio"),DPe=o(" \u2014 "),k7=a("a"),jPe=o("Data2VecAudioConfig"),NPe=o(" (Data2VecAudio model)"),qPe=l(),Qm=a("li"),JW=a("strong"),GPe=o("data2vec-text"),OPe=o(" \u2014 "),R7=a("a"),XPe=o("Data2VecTextConfig"),VPe=o(" (Data2VecText model)"),zPe=l(),Hm=a("li"),YW=a("strong"),WPe=o("deberta"),QPe=o(" \u2014 "),S7=a("a"),HPe=o("DebertaConfig"),UPe=o(" (DeBERTa model)"),JPe=l(),Um=a("li"),KW=a("strong"),YPe=o("deberta-v2"),KPe=o(" \u2014 "),P7=a("a"),ZPe=o("DebertaV2Config"),e$e=o(" (DeBERTa-v2 model)"),o$e=l(),Jm=a("li"),ZW=a("strong"),r$e=o("deit"),t$e=o(" \u2014 "),$7=a("a"),a$e=o("DeiTConfig"),s$e=o(" (DeiT model)"),n$e=l(),Ym=a("li"),eQ=a("strong"),l$e=o("detr"),i$e=o(" \u2014 "),I7=a("a"),d$e=o("DetrConfig"),c$e=o(" (DETR model)"),m$e=l(),Km=a("li"),oQ=a("strong"),f$e=o("distilbert"),g$e=o(" \u2014 "),D7=a("a"),h$e=o("DistilBertConfig"),u$e=o(" (DistilBERT model)"),p$e=l(),Zm=a("li"),rQ=a("strong"),_$e=o("dpr"),b$e=o(" \u2014 "),j7=a("a"),v$e=o("DPRConfig"),T$e=o(" (DPR model)"),F$e=l(),ef=a("li"),tQ=a("strong"),C$e=o("electra"),M$e=o(" \u2014 "),N7=a("a"),E$e=o("ElectraConfig"),y$e=o(" (ELECTRA model)"),w$e=l(),of=a("li"),aQ=a("strong"),A$e=o("encoder-decoder"),L$e=o(" \u2014 "),q7=a("a"),B$e=o("EncoderDecoderConfig"),x$e=o(" (Encoder decoder model)"),k$e=l(),rf=a("li"),sQ=a("strong"),R$e=o("flaubert"),S$e=o(" \u2014 "),G7=a("a"),P$e=o("FlaubertConfig"),$$e=o(" (FlauBERT model)"),I$e=l(),tf=a("li"),nQ=a("strong"),D$e=o("fnet"),j$e=o(" \u2014 "),O7=a("a"),N$e=o("FNetConfig"),q$e=o(" (FNet model)"),G$e=l(),af=a("li"),lQ=a("strong"),O$e=o("fsmt"),X$e=o(" \u2014 "),X7=a("a"),V$e=o("FSMTConfig"),z$e=o(" (FairSeq Machine-Translation model)"),W$e=l(),sf=a("li"),iQ=a("strong"),Q$e=o("funnel"),H$e=o(" \u2014 "),V7=a("a"),U$e=o("FunnelConfig"),J$e=o(" (Funnel Transformer model)"),Y$e=l(),nf=a("li"),dQ=a("strong"),K$e=o("gpt2"),Z$e=o(" \u2014 "),z7=a("a"),eIe=o("GPT2Config"),oIe=o(" (OpenAI GPT-2 model)"),rIe=l(),lf=a("li"),cQ=a("strong"),tIe=o("gpt_neo"),aIe=o(" \u2014 "),W7=a("a"),sIe=o("GPTNeoConfig"),nIe=o(" (GPT Neo model)"),lIe=l(),df=a("li"),mQ=a("strong"),iIe=o("gptj"),dIe=o(" \u2014 "),Q7=a("a"),cIe=o("GPTJConfig"),mIe=o(" (GPT-J model)"),fIe=l(),cf=a("li"),fQ=a("strong"),gIe=o("hubert"),hIe=o(" \u2014 "),H7=a("a"),uIe=o("HubertConfig"),pIe=o(" (Hubert model)"),_Ie=l(),mf=a("li"),gQ=a("strong"),bIe=o("ibert"),vIe=o(" \u2014 "),U7=a("a"),TIe=o("IBertConfig"),FIe=o(" (I-BERT model)"),CIe=l(),ff=a("li"),hQ=a("strong"),MIe=o("imagegpt"),EIe=o(" \u2014 "),J7=a("a"),yIe=o("ImageGPTConfig"),wIe=o(" (ImageGPT model)"),AIe=l(),gf=a("li"),uQ=a("strong"),LIe=o("layoutlm"),BIe=o(" \u2014 "),Y7=a("a"),xIe=o("LayoutLMConfig"),kIe=o(" (LayoutLM model)"),RIe=l(),hf=a("li"),pQ=a("strong"),SIe=o("layoutlmv2"),PIe=o(" \u2014 "),K7=a("a"),$Ie=o("LayoutLMv2Config"),IIe=o(" (LayoutLMv2 model)"),DIe=l(),uf=a("li"),_Q=a("strong"),jIe=o("led"),NIe=o(" \u2014 "),Z7=a("a"),qIe=o("LEDConfig"),GIe=o(" (LED model)"),OIe=l(),pf=a("li"),bQ=a("strong"),XIe=o("longformer"),VIe=o(" \u2014 "),e9=a("a"),zIe=o("LongformerConfig"),WIe=o(" (Longformer model)"),QIe=l(),_f=a("li"),vQ=a("strong"),HIe=o("luke"),UIe=o(" \u2014 "),o9=a("a"),JIe=o("LukeConfig"),YIe=o(" (LUKE model)"),KIe=l(),bf=a("li"),TQ=a("strong"),ZIe=o("lxmert"),eDe=o(" \u2014 "),r9=a("a"),oDe=o("LxmertConfig"),rDe=o(" (LXMERT model)"),tDe=l(),vf=a("li"),FQ=a("strong"),aDe=o("m2m_100"),sDe=o(" \u2014 "),t9=a("a"),nDe=o("M2M100Config"),lDe=o(" (M2M100 model)"),iDe=l(),Tf=a("li"),CQ=a("strong"),dDe=o("marian"),cDe=o(" \u2014 "),a9=a("a"),mDe=o("MarianConfig"),fDe=o(" (Marian model)"),gDe=l(),Ff=a("li"),MQ=a("strong"),hDe=o("maskformer"),uDe=o(" \u2014 "),s9=a("a"),pDe=o("MaskFormerConfig"),_De=o(" (MaskFormer model)"),bDe=l(),Cf=a("li"),EQ=a("strong"),vDe=o("mbart"),TDe=o(" \u2014 "),n9=a("a"),FDe=o("MBartConfig"),CDe=o(" (mBART model)"),MDe=l(),Mf=a("li"),yQ=a("strong"),EDe=o("megatron-bert"),yDe=o(" \u2014 "),l9=a("a"),wDe=o("MegatronBertConfig"),ADe=o(" (MegatronBert model)"),LDe=l(),Ef=a("li"),wQ=a("strong"),BDe=o("mobilebert"),xDe=o(" \u2014 "),i9=a("a"),kDe=o("MobileBertConfig"),RDe=o(" (MobileBERT model)"),SDe=l(),yf=a("li"),AQ=a("strong"),PDe=o("mpnet"),$De=o(" \u2014 "),d9=a("a"),IDe=o("MPNetConfig"),DDe=o(" (MPNet model)"),jDe=l(),wf=a("li"),LQ=a("strong"),NDe=o("mt5"),qDe=o(" \u2014 "),c9=a("a"),GDe=o("MT5Config"),ODe=o(" (mT5 model)"),XDe=l(),Af=a("li"),BQ=a("strong"),VDe=o("nystromformer"),zDe=o(" \u2014 "),m9=a("a"),WDe=o("NystromformerConfig"),QDe=o(" (Nystromformer model)"),HDe=l(),Lf=a("li"),xQ=a("strong"),UDe=o("openai-gpt"),JDe=o(" \u2014 "),f9=a("a"),YDe=o("OpenAIGPTConfig"),KDe=o(" (OpenAI GPT model)"),ZDe=l(),Bf=a("li"),kQ=a("strong"),eje=o("pegasus"),oje=o(" \u2014 "),g9=a("a"),rje=o("PegasusConfig"),tje=o(" (Pegasus model)"),aje=l(),xf=a("li"),RQ=a("strong"),sje=o("perceiver"),nje=o(" \u2014 "),h9=a("a"),lje=o("PerceiverConfig"),ije=o(" (Perceiver model)"),dje=l(),kf=a("li"),SQ=a("strong"),cje=o("plbart"),mje=o(" \u2014 "),u9=a("a"),fje=o("PLBartConfig"),gje=o(" (PLBart model)"),hje=l(),Rf=a("li"),PQ=a("strong"),uje=o("poolformer"),pje=o(" \u2014 "),p9=a("a"),_je=o("PoolFormerConfig"),bje=o(" (PoolFormer model)"),vje=l(),Sf=a("li"),$Q=a("strong"),Tje=o("prophetnet"),Fje=o(" \u2014 "),_9=a("a"),Cje=o("ProphetNetConfig"),Mje=o(" (ProphetNet model)"),Eje=l(),Pf=a("li"),IQ=a("strong"),yje=o("qdqbert"),wje=o(" \u2014 "),b9=a("a"),Aje=o("QDQBertConfig"),Lje=o(" (QDQBert model)"),Bje=l(),$f=a("li"),DQ=a("strong"),xje=o("rag"),kje=o(" \u2014 "),v9=a("a"),Rje=o("RagConfig"),Sje=o(" (RAG model)"),Pje=l(),If=a("li"),jQ=a("strong"),$je=o("realm"),Ije=o(" \u2014 "),T9=a("a"),Dje=o("RealmConfig"),jje=o(" (Realm model)"),Nje=l(),Df=a("li"),NQ=a("strong"),qje=o("reformer"),Gje=o(" \u2014 "),F9=a("a"),Oje=o("ReformerConfig"),Xje=o(" (Reformer model)"),Vje=l(),jf=a("li"),qQ=a("strong"),zje=o("rembert"),Wje=o(" \u2014 "),C9=a("a"),Qje=o("RemBertConfig"),Hje=o(" (RemBERT model)"),Uje=l(),Nf=a("li"),GQ=a("strong"),Jje=o("retribert"),Yje=o(" \u2014 "),M9=a("a"),Kje=o("RetriBertConfig"),Zje=o(" (RetriBERT model)"),eNe=l(),qf=a("li"),OQ=a("strong"),oNe=o("roberta"),rNe=o(" \u2014 "),E9=a("a"),tNe=o("RobertaConfig"),aNe=o(" (RoBERTa model)"),sNe=l(),Gf=a("li"),XQ=a("strong"),nNe=o("roformer"),lNe=o(" \u2014 "),y9=a("a"),iNe=o("RoFormerConfig"),dNe=o(" (RoFormer model)"),cNe=l(),Of=a("li"),VQ=a("strong"),mNe=o("segformer"),fNe=o(" \u2014 "),w9=a("a"),gNe=o("SegformerConfig"),hNe=o(" (SegFormer model)"),uNe=l(),Xf=a("li"),zQ=a("strong"),pNe=o("sew"),_Ne=o(" \u2014 "),A9=a("a"),bNe=o("SEWConfig"),vNe=o(" (SEW model)"),TNe=l(),Vf=a("li"),WQ=a("strong"),FNe=o("sew-d"),CNe=o(" \u2014 "),L9=a("a"),MNe=o("SEWDConfig"),ENe=o(" (SEW-D model)"),yNe=l(),zf=a("li"),QQ=a("strong"),wNe=o("speech-encoder-decoder"),ANe=o(" \u2014 "),B9=a("a"),LNe=o("SpeechEncoderDecoderConfig"),BNe=o(" (Speech Encoder decoder model)"),xNe=l(),Wf=a("li"),HQ=a("strong"),kNe=o("speech_to_text"),RNe=o(" \u2014 "),x9=a("a"),SNe=o("Speech2TextConfig"),PNe=o(" (Speech2Text model)"),$Ne=l(),Qf=a("li"),UQ=a("strong"),INe=o("speech_to_text_2"),DNe=o(" \u2014 "),k9=a("a"),jNe=o("Speech2Text2Config"),NNe=o(" (Speech2Text2 model)"),qNe=l(),Hf=a("li"),JQ=a("strong"),GNe=o("splinter"),ONe=o(" \u2014 "),R9=a("a"),XNe=o("SplinterConfig"),VNe=o(" (Splinter model)"),zNe=l(),Uf=a("li"),YQ=a("strong"),WNe=o("squeezebert"),QNe=o(" \u2014 "),S9=a("a"),HNe=o("SqueezeBertConfig"),UNe=o(" (SqueezeBERT model)"),JNe=l(),Jf=a("li"),KQ=a("strong"),YNe=o("swin"),KNe=o(" \u2014 "),P9=a("a"),ZNe=o("SwinConfig"),eqe=o(" (Swin model)"),oqe=l(),Yf=a("li"),ZQ=a("strong"),rqe=o("t5"),tqe=o(" \u2014 "),$9=a("a"),aqe=o("T5Config"),sqe=o(" (T5 model)"),nqe=l(),Kf=a("li"),eH=a("strong"),lqe=o("tapas"),iqe=o(" \u2014 "),I9=a("a"),dqe=o("TapasConfig"),cqe=o(" (TAPAS model)"),mqe=l(),Zf=a("li"),oH=a("strong"),fqe=o("transfo-xl"),gqe=o(" \u2014 "),D9=a("a"),hqe=o("TransfoXLConfig"),uqe=o(" (Transformer-XL model)"),pqe=l(),eg=a("li"),rH=a("strong"),_qe=o("trocr"),bqe=o(" \u2014 "),j9=a("a"),vqe=o("TrOCRConfig"),Tqe=o(" (TrOCR model)"),Fqe=l(),og=a("li"),tH=a("strong"),Cqe=o("unispeech"),Mqe=o(" \u2014 "),N9=a("a"),Eqe=o("UniSpeechConfig"),yqe=o(" (UniSpeech model)"),wqe=l(),rg=a("li"),aH=a("strong"),Aqe=o("unispeech-sat"),Lqe=o(" \u2014 "),q9=a("a"),Bqe=o("UniSpeechSatConfig"),xqe=o(" (UniSpeechSat model)"),kqe=l(),tg=a("li"),sH=a("strong"),Rqe=o("vilt"),Sqe=o(" \u2014 "),G9=a("a"),Pqe=o("ViltConfig"),$qe=o(" (ViLT model)"),Iqe=l(),ag=a("li"),nH=a("strong"),Dqe=o("vision-encoder-decoder"),jqe=o(" \u2014 "),O9=a("a"),Nqe=o("VisionEncoderDecoderConfig"),qqe=o(" (Vision Encoder decoder model)"),Gqe=l(),sg=a("li"),lH=a("strong"),Oqe=o("vision-text-dual-encoder"),Xqe=o(" \u2014 "),X9=a("a"),Vqe=o("VisionTextDualEncoderConfig"),zqe=o(" (VisionTextDualEncoder model)"),Wqe=l(),ng=a("li"),iH=a("strong"),Qqe=o("visual_bert"),Hqe=o(" \u2014 "),V9=a("a"),Uqe=o("VisualBertConfig"),Jqe=o(" (VisualBert model)"),Yqe=l(),lg=a("li"),dH=a("strong"),Kqe=o("vit"),Zqe=o(" \u2014 "),z9=a("a"),eGe=o("ViTConfig"),oGe=o(" (ViT model)"),rGe=l(),ig=a("li"),cH=a("strong"),tGe=o("vit_mae"),aGe=o(" \u2014 "),W9=a("a"),sGe=o("ViTMAEConfig"),nGe=o(" (ViTMAE model)"),lGe=l(),dg=a("li"),mH=a("strong"),iGe=o("wav2vec2"),dGe=o(" \u2014 "),Q9=a("a"),cGe=o("Wav2Vec2Config"),mGe=o(" (Wav2Vec2 model)"),fGe=l(),cg=a("li"),fH=a("strong"),gGe=o("wavlm"),hGe=o(" \u2014 "),H9=a("a"),uGe=o("WavLMConfig"),pGe=o(" (WavLM model)"),_Ge=l(),mg=a("li"),gH=a("strong"),bGe=o("xglm"),vGe=o(" \u2014 "),U9=a("a"),TGe=o("XGLMConfig"),FGe=o(" (XGLM model)"),CGe=l(),fg=a("li"),hH=a("strong"),MGe=o("xlm"),EGe=o(" \u2014 "),J9=a("a"),yGe=o("XLMConfig"),wGe=o(" (XLM model)"),AGe=l(),gg=a("li"),uH=a("strong"),LGe=o("xlm-prophetnet"),BGe=o(" \u2014 "),Y9=a("a"),xGe=o("XLMProphetNetConfig"),kGe=o(" (XLMProphetNet model)"),RGe=l(),hg=a("li"),pH=a("strong"),SGe=o("xlm-roberta"),PGe=o(" \u2014 "),K9=a("a"),$Ge=o("XLMRobertaConfig"),IGe=o(" (XLM-RoBERTa model)"),DGe=l(),ug=a("li"),_H=a("strong"),jGe=o("xlm-roberta-xl"),NGe=o(" \u2014 "),Z9=a("a"),qGe=o("XLMRobertaXLConfig"),GGe=o(" (XLM-RoBERTa-XL model)"),OGe=l(),pg=a("li"),bH=a("strong"),XGe=o("xlnet"),VGe=o(" \u2014 "),eB=a("a"),zGe=o("XLNetConfig"),WGe=o(" (XLNet model)"),QGe=l(),_g=a("li"),vH=a("strong"),HGe=o("yoso"),UGe=o(" \u2014 "),oB=a("a"),JGe=o("YosoConfig"),YGe=o(" (YOSO model)"),KGe=l(),TH=a("p"),ZGe=o("Examples:"),eOe=l(),m(P5.$$.fragment),oOe=l(),bg=a("div"),m($5.$$.fragment),rOe=l(),FH=a("p"),tOe=o("Register a new configuration for this class."),D9e=l(),Ni=a("h2"),vg=a("a"),CH=a("span"),m(I5.$$.fragment),aOe=l(),MH=a("span"),sOe=o("AutoTokenizer"),j9e=l(),Vo=a("div"),m(D5.$$.fragment),nOe=l(),j5=a("p"),lOe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),rB=a("a"),iOe=o("AutoTokenizer.from_pretrained()"),dOe=o(" class method."),cOe=l(),N5=a("p"),mOe=o("This class cannot be instantiated directly using "),EH=a("code"),fOe=o("__init__()"),gOe=o(" (throws an error)."),hOe=l(),fo=a("div"),m(q5.$$.fragment),uOe=l(),yH=a("p"),pOe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),_Oe=l(),Da=a("p"),bOe=o("The tokenizer class to instantiate is selected based on the "),wH=a("code"),vOe=o("model_type"),TOe=o(` property of the config object (either
passed as an argument or loaded from `),AH=a("code"),FOe=o("pretrained_model_name_or_path"),COe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),LH=a("code"),MOe=o("pretrained_model_name_or_path"),EOe=o(":"),yOe=l(),M=a("ul"),qs=a("li"),BH=a("strong"),wOe=o("albert"),AOe=o(" \u2014 "),tB=a("a"),LOe=o("AlbertTokenizer"),BOe=o(" or "),aB=a("a"),xOe=o("AlbertTokenizerFast"),kOe=o(" (ALBERT model)"),ROe=l(),Gs=a("li"),xH=a("strong"),SOe=o("bart"),POe=o(" \u2014 "),sB=a("a"),$Oe=o("BartTokenizer"),IOe=o(" or "),nB=a("a"),DOe=o("BartTokenizerFast"),jOe=o(" (BART model)"),NOe=l(),Os=a("li"),kH=a("strong"),qOe=o("barthez"),GOe=o(" \u2014 "),lB=a("a"),OOe=o("BarthezTokenizer"),XOe=o(" or "),iB=a("a"),VOe=o("BarthezTokenizerFast"),zOe=o(" (BARThez model)"),WOe=l(),Tg=a("li"),RH=a("strong"),QOe=o("bartpho"),HOe=o(" \u2014 "),dB=a("a"),UOe=o("BartphoTokenizer"),JOe=o(" (BARTpho model)"),YOe=l(),Xs=a("li"),SH=a("strong"),KOe=o("bert"),ZOe=o(" \u2014 "),cB=a("a"),eXe=o("BertTokenizer"),oXe=o(" or "),mB=a("a"),rXe=o("BertTokenizerFast"),tXe=o(" (BERT model)"),aXe=l(),Fg=a("li"),PH=a("strong"),sXe=o("bert-generation"),nXe=o(" \u2014 "),fB=a("a"),lXe=o("BertGenerationTokenizer"),iXe=o(" (Bert Generation model)"),dXe=l(),Cg=a("li"),$H=a("strong"),cXe=o("bert-japanese"),mXe=o(" \u2014 "),gB=a("a"),fXe=o("BertJapaneseTokenizer"),gXe=o(" (BertJapanese model)"),hXe=l(),Mg=a("li"),IH=a("strong"),uXe=o("bertweet"),pXe=o(" \u2014 "),hB=a("a"),_Xe=o("BertweetTokenizer"),bXe=o(" (Bertweet model)"),vXe=l(),Vs=a("li"),DH=a("strong"),TXe=o("big_bird"),FXe=o(" \u2014 "),uB=a("a"),CXe=o("BigBirdTokenizer"),MXe=o(" or "),pB=a("a"),EXe=o("BigBirdTokenizerFast"),yXe=o(" (BigBird model)"),wXe=l(),zs=a("li"),jH=a("strong"),AXe=o("bigbird_pegasus"),LXe=o(" \u2014 "),_B=a("a"),BXe=o("PegasusTokenizer"),xXe=o(" or "),bB=a("a"),kXe=o("PegasusTokenizerFast"),RXe=o(" (BigBirdPegasus model)"),SXe=l(),Ws=a("li"),NH=a("strong"),PXe=o("blenderbot"),$Xe=o(" \u2014 "),vB=a("a"),IXe=o("BlenderbotTokenizer"),DXe=o(" or "),TB=a("a"),jXe=o("BlenderbotTokenizerFast"),NXe=o(" (Blenderbot model)"),qXe=l(),Eg=a("li"),qH=a("strong"),GXe=o("blenderbot-small"),OXe=o(" \u2014 "),FB=a("a"),XXe=o("BlenderbotSmallTokenizer"),VXe=o(" (BlenderbotSmall model)"),zXe=l(),yg=a("li"),GH=a("strong"),WXe=o("byt5"),QXe=o(" \u2014 "),CB=a("a"),HXe=o("ByT5Tokenizer"),UXe=o(" (ByT5 model)"),JXe=l(),Qs=a("li"),OH=a("strong"),YXe=o("camembert"),KXe=o(" \u2014 "),MB=a("a"),ZXe=o("CamembertTokenizer"),eVe=o(" or "),EB=a("a"),oVe=o("CamembertTokenizerFast"),rVe=o(" (CamemBERT model)"),tVe=l(),wg=a("li"),XH=a("strong"),aVe=o("canine"),sVe=o(" \u2014 "),yB=a("a"),nVe=o("CanineTokenizer"),lVe=o(" (Canine model)"),iVe=l(),Hs=a("li"),VH=a("strong"),dVe=o("clip"),cVe=o(" \u2014 "),wB=a("a"),mVe=o("CLIPTokenizer"),fVe=o(" or "),AB=a("a"),gVe=o("CLIPTokenizerFast"),hVe=o(" (CLIP model)"),uVe=l(),Us=a("li"),zH=a("strong"),pVe=o("convbert"),_Ve=o(" \u2014 "),LB=a("a"),bVe=o("ConvBertTokenizer"),vVe=o(" or "),BB=a("a"),TVe=o("ConvBertTokenizerFast"),FVe=o(" (ConvBERT model)"),CVe=l(),Js=a("li"),WH=a("strong"),MVe=o("cpm"),EVe=o(" \u2014 "),xB=a("a"),yVe=o("CpmTokenizer"),wVe=o(" or "),QH=a("code"),AVe=o("CpmTokenizerFast"),LVe=o(" (CPM model)"),BVe=l(),Ag=a("li"),HH=a("strong"),xVe=o("ctrl"),kVe=o(" \u2014 "),kB=a("a"),RVe=o("CTRLTokenizer"),SVe=o(" (CTRL model)"),PVe=l(),Ys=a("li"),UH=a("strong"),$Ve=o("deberta"),IVe=o(" \u2014 "),RB=a("a"),DVe=o("DebertaTokenizer"),jVe=o(" or "),SB=a("a"),NVe=o("DebertaTokenizerFast"),qVe=o(" (DeBERTa model)"),GVe=l(),Lg=a("li"),JH=a("strong"),OVe=o("deberta-v2"),XVe=o(" \u2014 "),PB=a("a"),VVe=o("DebertaV2Tokenizer"),zVe=o(" (DeBERTa-v2 model)"),WVe=l(),Ks=a("li"),YH=a("strong"),QVe=o("distilbert"),HVe=o(" \u2014 "),$B=a("a"),UVe=o("DistilBertTokenizer"),JVe=o(" or "),IB=a("a"),YVe=o("DistilBertTokenizerFast"),KVe=o(" (DistilBERT model)"),ZVe=l(),Zs=a("li"),KH=a("strong"),eze=o("dpr"),oze=o(" \u2014 "),DB=a("a"),rze=o("DPRQuestionEncoderTokenizer"),tze=o(" or "),jB=a("a"),aze=o("DPRQuestionEncoderTokenizerFast"),sze=o(" (DPR model)"),nze=l(),en=a("li"),ZH=a("strong"),lze=o("electra"),ize=o(" \u2014 "),NB=a("a"),dze=o("ElectraTokenizer"),cze=o(" or "),qB=a("a"),mze=o("ElectraTokenizerFast"),fze=o(" (ELECTRA model)"),gze=l(),Bg=a("li"),eU=a("strong"),hze=o("flaubert"),uze=o(" \u2014 "),GB=a("a"),pze=o("FlaubertTokenizer"),_ze=o(" (FlauBERT model)"),bze=l(),on=a("li"),oU=a("strong"),vze=o("fnet"),Tze=o(" \u2014 "),OB=a("a"),Fze=o("FNetTokenizer"),Cze=o(" or "),XB=a("a"),Mze=o("FNetTokenizerFast"),Eze=o(" (FNet model)"),yze=l(),xg=a("li"),rU=a("strong"),wze=o("fsmt"),Aze=o(" \u2014 "),VB=a("a"),Lze=o("FSMTTokenizer"),Bze=o(" (FairSeq Machine-Translation model)"),xze=l(),rn=a("li"),tU=a("strong"),kze=o("funnel"),Rze=o(" \u2014 "),zB=a("a"),Sze=o("FunnelTokenizer"),Pze=o(" or "),WB=a("a"),$ze=o("FunnelTokenizerFast"),Ize=o(" (Funnel Transformer model)"),Dze=l(),tn=a("li"),aU=a("strong"),jze=o("gpt2"),Nze=o(" \u2014 "),QB=a("a"),qze=o("GPT2Tokenizer"),Gze=o(" or "),HB=a("a"),Oze=o("GPT2TokenizerFast"),Xze=o(" (OpenAI GPT-2 model)"),Vze=l(),an=a("li"),sU=a("strong"),zze=o("gpt_neo"),Wze=o(" \u2014 "),UB=a("a"),Qze=o("GPT2Tokenizer"),Hze=o(" or "),JB=a("a"),Uze=o("GPT2TokenizerFast"),Jze=o(" (GPT Neo model)"),Yze=l(),sn=a("li"),nU=a("strong"),Kze=o("herbert"),Zze=o(" \u2014 "),YB=a("a"),eWe=o("HerbertTokenizer"),oWe=o(" or "),KB=a("a"),rWe=o("HerbertTokenizerFast"),tWe=o(" (HerBERT model)"),aWe=l(),kg=a("li"),lU=a("strong"),sWe=o("hubert"),nWe=o(" \u2014 "),ZB=a("a"),lWe=o("Wav2Vec2CTCTokenizer"),iWe=o(" (Hubert model)"),dWe=l(),nn=a("li"),iU=a("strong"),cWe=o("ibert"),mWe=o(" \u2014 "),ex=a("a"),fWe=o("RobertaTokenizer"),gWe=o(" or "),ox=a("a"),hWe=o("RobertaTokenizerFast"),uWe=o(" (I-BERT model)"),pWe=l(),ln=a("li"),dU=a("strong"),_We=o("layoutlm"),bWe=o(" \u2014 "),rx=a("a"),vWe=o("LayoutLMTokenizer"),TWe=o(" or "),tx=a("a"),FWe=o("LayoutLMTokenizerFast"),CWe=o(" (LayoutLM model)"),MWe=l(),dn=a("li"),cU=a("strong"),EWe=o("layoutlmv2"),yWe=o(" \u2014 "),ax=a("a"),wWe=o("LayoutLMv2Tokenizer"),AWe=o(" or "),sx=a("a"),LWe=o("LayoutLMv2TokenizerFast"),BWe=o(" (LayoutLMv2 model)"),xWe=l(),cn=a("li"),mU=a("strong"),kWe=o("layoutxlm"),RWe=o(" \u2014 "),nx=a("a"),SWe=o("LayoutXLMTokenizer"),PWe=o(" or "),lx=a("a"),$We=o("LayoutXLMTokenizerFast"),IWe=o(" (LayoutXLM model)"),DWe=l(),mn=a("li"),fU=a("strong"),jWe=o("led"),NWe=o(" \u2014 "),ix=a("a"),qWe=o("LEDTokenizer"),GWe=o(" or "),dx=a("a"),OWe=o("LEDTokenizerFast"),XWe=o(" (LED model)"),VWe=l(),fn=a("li"),gU=a("strong"),zWe=o("longformer"),WWe=o(" \u2014 "),cx=a("a"),QWe=o("LongformerTokenizer"),HWe=o(" or "),mx=a("a"),UWe=o("LongformerTokenizerFast"),JWe=o(" (Longformer model)"),YWe=l(),Rg=a("li"),hU=a("strong"),KWe=o("luke"),ZWe=o(" \u2014 "),fx=a("a"),eQe=o("LukeTokenizer"),oQe=o(" (LUKE model)"),rQe=l(),gn=a("li"),uU=a("strong"),tQe=o("lxmert"),aQe=o(" \u2014 "),gx=a("a"),sQe=o("LxmertTokenizer"),nQe=o(" or "),hx=a("a"),lQe=o("LxmertTokenizerFast"),iQe=o(" (LXMERT model)"),dQe=l(),Sg=a("li"),pU=a("strong"),cQe=o("m2m_100"),mQe=o(" \u2014 "),ux=a("a"),fQe=o("M2M100Tokenizer"),gQe=o(" (M2M100 model)"),hQe=l(),Pg=a("li"),_U=a("strong"),uQe=o("marian"),pQe=o(" \u2014 "),px=a("a"),_Qe=o("MarianTokenizer"),bQe=o(" (Marian model)"),vQe=l(),hn=a("li"),bU=a("strong"),TQe=o("mbart"),FQe=o(" \u2014 "),_x=a("a"),CQe=o("MBartTokenizer"),MQe=o(" or "),bx=a("a"),EQe=o("MBartTokenizerFast"),yQe=o(" (mBART model)"),wQe=l(),un=a("li"),vU=a("strong"),AQe=o("mbart50"),LQe=o(" \u2014 "),vx=a("a"),BQe=o("MBart50Tokenizer"),xQe=o(" or "),Tx=a("a"),kQe=o("MBart50TokenizerFast"),RQe=o(" (mBART-50 model)"),SQe=l(),$g=a("li"),TU=a("strong"),PQe=o("mluke"),$Qe=o(" \u2014 "),Fx=a("a"),IQe=o("MLukeTokenizer"),DQe=o(" (mLUKE model)"),jQe=l(),pn=a("li"),FU=a("strong"),NQe=o("mobilebert"),qQe=o(" \u2014 "),Cx=a("a"),GQe=o("MobileBertTokenizer"),OQe=o(" or "),Mx=a("a"),XQe=o("MobileBertTokenizerFast"),VQe=o(" (MobileBERT model)"),zQe=l(),_n=a("li"),CU=a("strong"),WQe=o("mpnet"),QQe=o(" \u2014 "),Ex=a("a"),HQe=o("MPNetTokenizer"),UQe=o(" or "),yx=a("a"),JQe=o("MPNetTokenizerFast"),YQe=o(" (MPNet model)"),KQe=l(),bn=a("li"),MU=a("strong"),ZQe=o("mt5"),eHe=o(" \u2014 "),wx=a("a"),oHe=o("MT5Tokenizer"),rHe=o(" or "),Ax=a("a"),tHe=o("MT5TokenizerFast"),aHe=o(" (mT5 model)"),sHe=l(),vn=a("li"),EU=a("strong"),nHe=o("openai-gpt"),lHe=o(" \u2014 "),Lx=a("a"),iHe=o("OpenAIGPTTokenizer"),dHe=o(" or "),Bx=a("a"),cHe=o("OpenAIGPTTokenizerFast"),mHe=o(" (OpenAI GPT model)"),fHe=l(),Tn=a("li"),yU=a("strong"),gHe=o("pegasus"),hHe=o(" \u2014 "),xx=a("a"),uHe=o("PegasusTokenizer"),pHe=o(" or "),kx=a("a"),_He=o("PegasusTokenizerFast"),bHe=o(" (Pegasus model)"),vHe=l(),Ig=a("li"),wU=a("strong"),THe=o("perceiver"),FHe=o(" \u2014 "),Rx=a("a"),CHe=o("PerceiverTokenizer"),MHe=o(" (Perceiver model)"),EHe=l(),Dg=a("li"),AU=a("strong"),yHe=o("phobert"),wHe=o(" \u2014 "),Sx=a("a"),AHe=o("PhobertTokenizer"),LHe=o(" (PhoBERT model)"),BHe=l(),jg=a("li"),LU=a("strong"),xHe=o("plbart"),kHe=o(" \u2014 "),Px=a("a"),RHe=o("PLBartTokenizer"),SHe=o(" (PLBart model)"),PHe=l(),Ng=a("li"),BU=a("strong"),$He=o("prophetnet"),IHe=o(" \u2014 "),$x=a("a"),DHe=o("ProphetNetTokenizer"),jHe=o(" (ProphetNet model)"),NHe=l(),Fn=a("li"),xU=a("strong"),qHe=o("qdqbert"),GHe=o(" \u2014 "),Ix=a("a"),OHe=o("BertTokenizer"),XHe=o(" or "),Dx=a("a"),VHe=o("BertTokenizerFast"),zHe=o(" (QDQBert model)"),WHe=l(),qg=a("li"),kU=a("strong"),QHe=o("rag"),HHe=o(" \u2014 "),jx=a("a"),UHe=o("RagTokenizer"),JHe=o(" (RAG model)"),YHe=l(),Cn=a("li"),RU=a("strong"),KHe=o("realm"),ZHe=o(" \u2014 "),Nx=a("a"),eUe=o("RealmTokenizer"),oUe=o(" or "),qx=a("a"),rUe=o("RealmTokenizerFast"),tUe=o(" (Realm model)"),aUe=l(),Mn=a("li"),SU=a("strong"),sUe=o("reformer"),nUe=o(" \u2014 "),Gx=a("a"),lUe=o("ReformerTokenizer"),iUe=o(" or "),Ox=a("a"),dUe=o("ReformerTokenizerFast"),cUe=o(" (Reformer model)"),mUe=l(),En=a("li"),PU=a("strong"),fUe=o("rembert"),gUe=o(" \u2014 "),Xx=a("a"),hUe=o("RemBertTokenizer"),uUe=o(" or "),Vx=a("a"),pUe=o("RemBertTokenizerFast"),_Ue=o(" (RemBERT model)"),bUe=l(),yn=a("li"),$U=a("strong"),vUe=o("retribert"),TUe=o(" \u2014 "),zx=a("a"),FUe=o("RetriBertTokenizer"),CUe=o(" or "),Wx=a("a"),MUe=o("RetriBertTokenizerFast"),EUe=o(" (RetriBERT model)"),yUe=l(),wn=a("li"),IU=a("strong"),wUe=o("roberta"),AUe=o(" \u2014 "),Qx=a("a"),LUe=o("RobertaTokenizer"),BUe=o(" or "),Hx=a("a"),xUe=o("RobertaTokenizerFast"),kUe=o(" (RoBERTa model)"),RUe=l(),An=a("li"),DU=a("strong"),SUe=o("roformer"),PUe=o(" \u2014 "),Ux=a("a"),$Ue=o("RoFormerTokenizer"),IUe=o(" or "),Jx=a("a"),DUe=o("RoFormerTokenizerFast"),jUe=o(" (RoFormer model)"),NUe=l(),Gg=a("li"),jU=a("strong"),qUe=o("speech_to_text"),GUe=o(" \u2014 "),Yx=a("a"),OUe=o("Speech2TextTokenizer"),XUe=o(" (Speech2Text model)"),VUe=l(),Og=a("li"),NU=a("strong"),zUe=o("speech_to_text_2"),WUe=o(" \u2014 "),Kx=a("a"),QUe=o("Speech2Text2Tokenizer"),HUe=o(" (Speech2Text2 model)"),UUe=l(),Ln=a("li"),qU=a("strong"),JUe=o("splinter"),YUe=o(" \u2014 "),Zx=a("a"),KUe=o("SplinterTokenizer"),ZUe=o(" or "),ek=a("a"),eJe=o("SplinterTokenizerFast"),oJe=o(" (Splinter model)"),rJe=l(),Bn=a("li"),GU=a("strong"),tJe=o("squeezebert"),aJe=o(" \u2014 "),ok=a("a"),sJe=o("SqueezeBertTokenizer"),nJe=o(" or "),rk=a("a"),lJe=o("SqueezeBertTokenizerFast"),iJe=o(" (SqueezeBERT model)"),dJe=l(),xn=a("li"),OU=a("strong"),cJe=o("t5"),mJe=o(" \u2014 "),tk=a("a"),fJe=o("T5Tokenizer"),gJe=o(" or "),ak=a("a"),hJe=o("T5TokenizerFast"),uJe=o(" (T5 model)"),pJe=l(),Xg=a("li"),XU=a("strong"),_Je=o("tapas"),bJe=o(" \u2014 "),sk=a("a"),vJe=o("TapasTokenizer"),TJe=o(" (TAPAS model)"),FJe=l(),Vg=a("li"),VU=a("strong"),CJe=o("transfo-xl"),MJe=o(" \u2014 "),nk=a("a"),EJe=o("TransfoXLTokenizer"),yJe=o(" (Transformer-XL model)"),wJe=l(),zg=a("li"),zU=a("strong"),AJe=o("wav2vec2"),LJe=o(" \u2014 "),lk=a("a"),BJe=o("Wav2Vec2CTCTokenizer"),xJe=o(" (Wav2Vec2 model)"),kJe=l(),Wg=a("li"),WU=a("strong"),RJe=o("wav2vec2_phoneme"),SJe=o(" \u2014 "),ik=a("a"),PJe=o("Wav2Vec2PhonemeCTCTokenizer"),$Je=o(" (Wav2Vec2Phoneme model)"),IJe=l(),kn=a("li"),QU=a("strong"),DJe=o("xglm"),jJe=o(" \u2014 "),dk=a("a"),NJe=o("XGLMTokenizer"),qJe=o(" or "),ck=a("a"),GJe=o("XGLMTokenizerFast"),OJe=o(" (XGLM model)"),XJe=l(),Qg=a("li"),HU=a("strong"),VJe=o("xlm"),zJe=o(" \u2014 "),mk=a("a"),WJe=o("XLMTokenizer"),QJe=o(" (XLM model)"),HJe=l(),Hg=a("li"),UU=a("strong"),UJe=o("xlm-prophetnet"),JJe=o(" \u2014 "),fk=a("a"),YJe=o("XLMProphetNetTokenizer"),KJe=o(" (XLMProphetNet model)"),ZJe=l(),Rn=a("li"),JU=a("strong"),eYe=o("xlm-roberta"),oYe=o(" \u2014 "),gk=a("a"),rYe=o("XLMRobertaTokenizer"),tYe=o(" or "),hk=a("a"),aYe=o("XLMRobertaTokenizerFast"),sYe=o(" (XLM-RoBERTa model)"),nYe=l(),Sn=a("li"),YU=a("strong"),lYe=o("xlnet"),iYe=o(" \u2014 "),uk=a("a"),dYe=o("XLNetTokenizer"),cYe=o(" or "),pk=a("a"),mYe=o("XLNetTokenizerFast"),fYe=o(" (XLNet model)"),gYe=l(),KU=a("p"),hYe=o("Examples:"),uYe=l(),m(G5.$$.fragment),pYe=l(),Ug=a("div"),m(O5.$$.fragment),_Ye=l(),ZU=a("p"),bYe=o("Register a new tokenizer in this mapping."),N9e=l(),qi=a("h2"),Jg=a("a"),eJ=a("span"),m(X5.$$.fragment),vYe=l(),oJ=a("span"),TYe=o("AutoFeatureExtractor"),q9e=l(),zo=a("div"),m(V5.$$.fragment),FYe=l(),z5=a("p"),CYe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),_k=a("a"),MYe=o("AutoFeatureExtractor.from_pretrained()"),EYe=o(" class method."),yYe=l(),W5=a("p"),wYe=o("This class cannot be instantiated directly using "),rJ=a("code"),AYe=o("__init__()"),LYe=o(" (throws an error)."),BYe=l(),xe=a("div"),m(Q5.$$.fragment),xYe=l(),tJ=a("p"),kYe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),RYe=l(),ja=a("p"),SYe=o("The feature extractor class to instantiate is selected based on the "),aJ=a("code"),PYe=o("model_type"),$Ye=o(` property of the config object
(either passed as an argument or loaded from `),sJ=a("code"),IYe=o("pretrained_model_name_or_path"),DYe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),nJ=a("code"),jYe=o("pretrained_model_name_or_path"),NYe=o(":"),qYe=l(),ne=a("ul"),Yg=a("li"),lJ=a("strong"),GYe=o("beit"),OYe=o(" \u2014 "),bk=a("a"),XYe=o("BeitFeatureExtractor"),VYe=o(" (BEiT model)"),zYe=l(),Kg=a("li"),iJ=a("strong"),WYe=o("clip"),QYe=o(" \u2014 "),vk=a("a"),HYe=o("CLIPFeatureExtractor"),UYe=o(" (CLIP model)"),JYe=l(),Zg=a("li"),dJ=a("strong"),YYe=o("convnext"),KYe=o(" \u2014 "),Tk=a("a"),ZYe=o("ConvNextFeatureExtractor"),eKe=o(" (ConvNext model)"),oKe=l(),eh=a("li"),cJ=a("strong"),rKe=o("deit"),tKe=o(" \u2014 "),Fk=a("a"),aKe=o("DeiTFeatureExtractor"),sKe=o(" (DeiT model)"),nKe=l(),oh=a("li"),mJ=a("strong"),lKe=o("detr"),iKe=o(" \u2014 "),Ck=a("a"),dKe=o("DetrFeatureExtractor"),cKe=o(" (DETR model)"),mKe=l(),rh=a("li"),fJ=a("strong"),fKe=o("hubert"),gKe=o(" \u2014 "),Mk=a("a"),hKe=o("Wav2Vec2FeatureExtractor"),uKe=o(" (Hubert model)"),pKe=l(),th=a("li"),gJ=a("strong"),_Ke=o("layoutlmv2"),bKe=o(" \u2014 "),Ek=a("a"),vKe=o("LayoutLMv2FeatureExtractor"),TKe=o(" (LayoutLMv2 model)"),FKe=l(),ah=a("li"),hJ=a("strong"),CKe=o("perceiver"),MKe=o(" \u2014 "),yk=a("a"),EKe=o("PerceiverFeatureExtractor"),yKe=o(" (Perceiver model)"),wKe=l(),sh=a("li"),uJ=a("strong"),AKe=o("poolformer"),LKe=o(" \u2014 "),wk=a("a"),BKe=o("PoolFormerFeatureExtractor"),xKe=o(" (PoolFormer model)"),kKe=l(),nh=a("li"),pJ=a("strong"),RKe=o("segformer"),SKe=o(" \u2014 "),Ak=a("a"),PKe=o("SegformerFeatureExtractor"),$Ke=o(" (SegFormer model)"),IKe=l(),lh=a("li"),_J=a("strong"),DKe=o("speech_to_text"),jKe=o(" \u2014 "),Lk=a("a"),NKe=o("Speech2TextFeatureExtractor"),qKe=o(" (Speech2Text model)"),GKe=l(),ih=a("li"),bJ=a("strong"),OKe=o("swin"),XKe=o(" \u2014 "),Bk=a("a"),VKe=o("ViTFeatureExtractor"),zKe=o(" (Swin model)"),WKe=l(),dh=a("li"),vJ=a("strong"),QKe=o("vit"),HKe=o(" \u2014 "),xk=a("a"),UKe=o("ViTFeatureExtractor"),JKe=o(" (ViT model)"),YKe=l(),ch=a("li"),TJ=a("strong"),KKe=o("vit_mae"),ZKe=o(" \u2014 "),kk=a("a"),eZe=o("ViTFeatureExtractor"),oZe=o(" (ViTMAE model)"),rZe=l(),mh=a("li"),FJ=a("strong"),tZe=o("wav2vec2"),aZe=o(" \u2014 "),Rk=a("a"),sZe=o("Wav2Vec2FeatureExtractor"),nZe=o(" (Wav2Vec2 model)"),lZe=l(),m(fh.$$.fragment),iZe=l(),CJ=a("p"),dZe=o("Examples:"),cZe=l(),m(H5.$$.fragment),mZe=l(),gh=a("div"),m(U5.$$.fragment),fZe=l(),MJ=a("p"),gZe=o("Register a new feature extractor for this class."),G9e=l(),Gi=a("h2"),hh=a("a"),EJ=a("span"),m(J5.$$.fragment),hZe=l(),yJ=a("span"),uZe=o("AutoProcessor"),O9e=l(),Wo=a("div"),m(Y5.$$.fragment),pZe=l(),K5=a("p"),_Ze=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),Sk=a("a"),bZe=o("AutoProcessor.from_pretrained()"),vZe=o(" class method."),TZe=l(),Z5=a("p"),FZe=o("This class cannot be instantiated directly using "),wJ=a("code"),CZe=o("__init__()"),MZe=o(" (throws an error)."),EZe=l(),ke=a("div"),m(ey.$$.fragment),yZe=l(),AJ=a("p"),wZe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),AZe=l(),Oi=a("p"),LZe=o("The processor class to instantiate is selected based on the "),LJ=a("code"),BZe=o("model_type"),xZe=o(` property of the config object (either
passed as an argument or loaded from `),BJ=a("code"),kZe=o("pretrained_model_name_or_path"),RZe=o(" if possible):"),SZe=l(),we=a("ul"),uh=a("li"),xJ=a("strong"),PZe=o("clip"),$Ze=o(" \u2014 "),Pk=a("a"),IZe=o("CLIPProcessor"),DZe=o(" (CLIP model)"),jZe=l(),ph=a("li"),kJ=a("strong"),NZe=o("layoutlmv2"),qZe=o(" \u2014 "),$k=a("a"),GZe=o("LayoutLMv2Processor"),OZe=o(" (LayoutLMv2 model)"),XZe=l(),_h=a("li"),RJ=a("strong"),VZe=o("layoutxlm"),zZe=o(" \u2014 "),Ik=a("a"),WZe=o("LayoutXLMProcessor"),QZe=o(" (LayoutXLM model)"),HZe=l(),bh=a("li"),SJ=a("strong"),UZe=o("speech_to_text"),JZe=o(" \u2014 "),Dk=a("a"),YZe=o("Speech2TextProcessor"),KZe=o(" (Speech2Text model)"),ZZe=l(),vh=a("li"),PJ=a("strong"),eeo=o("speech_to_text_2"),oeo=o(" \u2014 "),jk=a("a"),reo=o("Speech2Text2Processor"),teo=o(" (Speech2Text2 model)"),aeo=l(),Th=a("li"),$J=a("strong"),seo=o("trocr"),neo=o(" \u2014 "),Nk=a("a"),leo=o("TrOCRProcessor"),ieo=o(" (TrOCR model)"),deo=l(),Fh=a("li"),IJ=a("strong"),ceo=o("vision-text-dual-encoder"),meo=o(" \u2014 "),qk=a("a"),feo=o("VisionTextDualEncoderProcessor"),geo=o(" (VisionTextDualEncoder model)"),heo=l(),Ch=a("li"),DJ=a("strong"),ueo=o("wav2vec2"),peo=o(" \u2014 "),Gk=a("a"),_eo=o("Wav2Vec2Processor"),beo=o(" (Wav2Vec2 model)"),veo=l(),m(Mh.$$.fragment),Teo=l(),jJ=a("p"),Feo=o("Examples:"),Ceo=l(),m(oy.$$.fragment),Meo=l(),Eh=a("div"),m(ry.$$.fragment),Eeo=l(),NJ=a("p"),yeo=o("Register a new processor for this class."),X9e=l(),Xi=a("h2"),yh=a("a"),qJ=a("span"),m(ty.$$.fragment),weo=l(),GJ=a("span"),Aeo=o("AutoModel"),V9e=l(),Qo=a("div"),m(ay.$$.fragment),Leo=l(),Vi=a("p"),Beo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),OJ=a("code"),xeo=o("from_pretrained()"),keo=o("class method or the "),XJ=a("code"),Reo=o("from_config()"),Seo=o(`class
method.`),Peo=l(),sy=a("p"),$eo=o("This class cannot be instantiated directly using "),VJ=a("code"),Ieo=o("__init__()"),Deo=o(" (throws an error)."),jeo=l(),qr=a("div"),m(ny.$$.fragment),Neo=l(),zJ=a("p"),qeo=o("Instantiates one of the base model classes of the library from a configuration."),Geo=l(),zi=a("p"),Oeo=o(`Note:
Loading a model from its configuration file does `),WJ=a("strong"),Xeo=o("not"),Veo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),QJ=a("code"),zeo=o("from_pretrained()"),Weo=o("to load the model weights."),Qeo=l(),HJ=a("p"),Heo=o("Examples:"),Ueo=l(),m(ly.$$.fragment),Jeo=l(),Re=a("div"),m(iy.$$.fragment),Yeo=l(),UJ=a("p"),Keo=o("Instantiate one of the base model classes of the library from a pretrained model."),Zeo=l(),Na=a("p"),eoo=o("The model class to instantiate is selected based on the "),JJ=a("code"),ooo=o("model_type"),roo=o(` property of the config object (either
passed as an argument or loaded from `),YJ=a("code"),too=o("pretrained_model_name_or_path"),aoo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),KJ=a("code"),soo=o("pretrained_model_name_or_path"),noo=o(":"),loo=l(),F=a("ul"),wh=a("li"),ZJ=a("strong"),ioo=o("albert"),doo=o(" \u2014 "),Ok=a("a"),coo=o("AlbertModel"),moo=o(" (ALBERT model)"),foo=l(),Ah=a("li"),eY=a("strong"),goo=o("bart"),hoo=o(" \u2014 "),Xk=a("a"),uoo=o("BartModel"),poo=o(" (BART model)"),_oo=l(),Lh=a("li"),oY=a("strong"),boo=o("beit"),voo=o(" \u2014 "),Vk=a("a"),Too=o("BeitModel"),Foo=o(" (BEiT model)"),Coo=l(),Bh=a("li"),rY=a("strong"),Moo=o("bert"),Eoo=o(" \u2014 "),zk=a("a"),yoo=o("BertModel"),woo=o(" (BERT model)"),Aoo=l(),xh=a("li"),tY=a("strong"),Loo=o("bert-generation"),Boo=o(" \u2014 "),Wk=a("a"),xoo=o("BertGenerationEncoder"),koo=o(" (Bert Generation model)"),Roo=l(),kh=a("li"),aY=a("strong"),Soo=o("big_bird"),Poo=o(" \u2014 "),Qk=a("a"),$oo=o("BigBirdModel"),Ioo=o(" (BigBird model)"),Doo=l(),Rh=a("li"),sY=a("strong"),joo=o("bigbird_pegasus"),Noo=o(" \u2014 "),Hk=a("a"),qoo=o("BigBirdPegasusModel"),Goo=o(" (BigBirdPegasus model)"),Ooo=l(),Sh=a("li"),nY=a("strong"),Xoo=o("blenderbot"),Voo=o(" \u2014 "),Uk=a("a"),zoo=o("BlenderbotModel"),Woo=o(" (Blenderbot model)"),Qoo=l(),Ph=a("li"),lY=a("strong"),Hoo=o("blenderbot-small"),Uoo=o(" \u2014 "),Jk=a("a"),Joo=o("BlenderbotSmallModel"),Yoo=o(" (BlenderbotSmall model)"),Koo=l(),$h=a("li"),iY=a("strong"),Zoo=o("camembert"),ero=o(" \u2014 "),Yk=a("a"),oro=o("CamembertModel"),rro=o(" (CamemBERT model)"),tro=l(),Ih=a("li"),dY=a("strong"),aro=o("canine"),sro=o(" \u2014 "),Kk=a("a"),nro=o("CanineModel"),lro=o(" (Canine model)"),iro=l(),Dh=a("li"),cY=a("strong"),dro=o("clip"),cro=o(" \u2014 "),Zk=a("a"),mro=o("CLIPModel"),fro=o(" (CLIP model)"),gro=l(),jh=a("li"),mY=a("strong"),hro=o("convbert"),uro=o(" \u2014 "),eR=a("a"),pro=o("ConvBertModel"),_ro=o(" (ConvBERT model)"),bro=l(),Nh=a("li"),fY=a("strong"),vro=o("convnext"),Tro=o(" \u2014 "),oR=a("a"),Fro=o("ConvNextModel"),Cro=o(" (ConvNext model)"),Mro=l(),qh=a("li"),gY=a("strong"),Ero=o("ctrl"),yro=o(" \u2014 "),rR=a("a"),wro=o("CTRLModel"),Aro=o(" (CTRL model)"),Lro=l(),Gh=a("li"),hY=a("strong"),Bro=o("data2vec-audio"),xro=o(" \u2014 "),tR=a("a"),kro=o("Data2VecAudioModel"),Rro=o(" (Data2VecAudio model)"),Sro=l(),Oh=a("li"),uY=a("strong"),Pro=o("data2vec-text"),$ro=o(" \u2014 "),aR=a("a"),Iro=o("Data2VecTextModel"),Dro=o(" (Data2VecText model)"),jro=l(),Xh=a("li"),pY=a("strong"),Nro=o("deberta"),qro=o(" \u2014 "),sR=a("a"),Gro=o("DebertaModel"),Oro=o(" (DeBERTa model)"),Xro=l(),Vh=a("li"),_Y=a("strong"),Vro=o("deberta-v2"),zro=o(" \u2014 "),nR=a("a"),Wro=o("DebertaV2Model"),Qro=o(" (DeBERTa-v2 model)"),Hro=l(),zh=a("li"),bY=a("strong"),Uro=o("deit"),Jro=o(" \u2014 "),lR=a("a"),Yro=o("DeiTModel"),Kro=o(" (DeiT model)"),Zro=l(),Wh=a("li"),vY=a("strong"),eto=o("detr"),oto=o(" \u2014 "),iR=a("a"),rto=o("DetrModel"),tto=o(" (DETR model)"),ato=l(),Qh=a("li"),TY=a("strong"),sto=o("distilbert"),nto=o(" \u2014 "),dR=a("a"),lto=o("DistilBertModel"),ito=o(" (DistilBERT model)"),dto=l(),Hh=a("li"),FY=a("strong"),cto=o("dpr"),mto=o(" \u2014 "),cR=a("a"),fto=o("DPRQuestionEncoder"),gto=o(" (DPR model)"),hto=l(),Uh=a("li"),CY=a("strong"),uto=o("electra"),pto=o(" \u2014 "),mR=a("a"),_to=o("ElectraModel"),bto=o(" (ELECTRA model)"),vto=l(),Jh=a("li"),MY=a("strong"),Tto=o("flaubert"),Fto=o(" \u2014 "),fR=a("a"),Cto=o("FlaubertModel"),Mto=o(" (FlauBERT model)"),Eto=l(),Yh=a("li"),EY=a("strong"),yto=o("fnet"),wto=o(" \u2014 "),gR=a("a"),Ato=o("FNetModel"),Lto=o(" (FNet model)"),Bto=l(),Kh=a("li"),yY=a("strong"),xto=o("fsmt"),kto=o(" \u2014 "),hR=a("a"),Rto=o("FSMTModel"),Sto=o(" (FairSeq Machine-Translation model)"),Pto=l(),Pn=a("li"),wY=a("strong"),$to=o("funnel"),Ito=o(" \u2014 "),uR=a("a"),Dto=o("FunnelModel"),jto=o(" or "),pR=a("a"),Nto=o("FunnelBaseModel"),qto=o(" (Funnel Transformer model)"),Gto=l(),Zh=a("li"),AY=a("strong"),Oto=o("gpt2"),Xto=o(" \u2014 "),_R=a("a"),Vto=o("GPT2Model"),zto=o(" (OpenAI GPT-2 model)"),Wto=l(),eu=a("li"),LY=a("strong"),Qto=o("gpt_neo"),Hto=o(" \u2014 "),bR=a("a"),Uto=o("GPTNeoModel"),Jto=o(" (GPT Neo model)"),Yto=l(),ou=a("li"),BY=a("strong"),Kto=o("gptj"),Zto=o(" \u2014 "),vR=a("a"),eao=o("GPTJModel"),oao=o(" (GPT-J model)"),rao=l(),ru=a("li"),xY=a("strong"),tao=o("hubert"),aao=o(" \u2014 "),TR=a("a"),sao=o("HubertModel"),nao=o(" (Hubert model)"),lao=l(),tu=a("li"),kY=a("strong"),iao=o("ibert"),dao=o(" \u2014 "),FR=a("a"),cao=o("IBertModel"),mao=o(" (I-BERT model)"),fao=l(),au=a("li"),RY=a("strong"),gao=o("imagegpt"),hao=o(" \u2014 "),CR=a("a"),uao=o("ImageGPTModel"),pao=o(" (ImageGPT model)"),_ao=l(),su=a("li"),SY=a("strong"),bao=o("layoutlm"),vao=o(" \u2014 "),MR=a("a"),Tao=o("LayoutLMModel"),Fao=o(" (LayoutLM model)"),Cao=l(),nu=a("li"),PY=a("strong"),Mao=o("layoutlmv2"),Eao=o(" \u2014 "),ER=a("a"),yao=o("LayoutLMv2Model"),wao=o(" (LayoutLMv2 model)"),Aao=l(),lu=a("li"),$Y=a("strong"),Lao=o("led"),Bao=o(" \u2014 "),yR=a("a"),xao=o("LEDModel"),kao=o(" (LED model)"),Rao=l(),iu=a("li"),IY=a("strong"),Sao=o("longformer"),Pao=o(" \u2014 "),wR=a("a"),$ao=o("LongformerModel"),Iao=o(" (Longformer model)"),Dao=l(),du=a("li"),DY=a("strong"),jao=o("luke"),Nao=o(" \u2014 "),AR=a("a"),qao=o("LukeModel"),Gao=o(" (LUKE model)"),Oao=l(),cu=a("li"),jY=a("strong"),Xao=o("lxmert"),Vao=o(" \u2014 "),LR=a("a"),zao=o("LxmertModel"),Wao=o(" (LXMERT model)"),Qao=l(),mu=a("li"),NY=a("strong"),Hao=o("m2m_100"),Uao=o(" \u2014 "),BR=a("a"),Jao=o("M2M100Model"),Yao=o(" (M2M100 model)"),Kao=l(),fu=a("li"),qY=a("strong"),Zao=o("marian"),eso=o(" \u2014 "),xR=a("a"),oso=o("MarianModel"),rso=o(" (Marian model)"),tso=l(),gu=a("li"),GY=a("strong"),aso=o("maskformer"),sso=o(" \u2014 "),kR=a("a"),nso=o("MaskFormerModel"),lso=o(" (MaskFormer model)"),iso=l(),hu=a("li"),OY=a("strong"),dso=o("mbart"),cso=o(" \u2014 "),RR=a("a"),mso=o("MBartModel"),fso=o(" (mBART model)"),gso=l(),uu=a("li"),XY=a("strong"),hso=o("megatron-bert"),uso=o(" \u2014 "),SR=a("a"),pso=o("MegatronBertModel"),_so=o(" (MegatronBert model)"),bso=l(),pu=a("li"),VY=a("strong"),vso=o("mobilebert"),Tso=o(" \u2014 "),PR=a("a"),Fso=o("MobileBertModel"),Cso=o(" (MobileBERT model)"),Mso=l(),_u=a("li"),zY=a("strong"),Eso=o("mpnet"),yso=o(" \u2014 "),$R=a("a"),wso=o("MPNetModel"),Aso=o(" (MPNet model)"),Lso=l(),bu=a("li"),WY=a("strong"),Bso=o("mt5"),xso=o(" \u2014 "),IR=a("a"),kso=o("MT5Model"),Rso=o(" (mT5 model)"),Sso=l(),vu=a("li"),QY=a("strong"),Pso=o("nystromformer"),$so=o(" \u2014 "),DR=a("a"),Iso=o("NystromformerModel"),Dso=o(" (Nystromformer model)"),jso=l(),Tu=a("li"),HY=a("strong"),Nso=o("openai-gpt"),qso=o(" \u2014 "),jR=a("a"),Gso=o("OpenAIGPTModel"),Oso=o(" (OpenAI GPT model)"),Xso=l(),Fu=a("li"),UY=a("strong"),Vso=o("pegasus"),zso=o(" \u2014 "),NR=a("a"),Wso=o("PegasusModel"),Qso=o(" (Pegasus model)"),Hso=l(),Cu=a("li"),JY=a("strong"),Uso=o("perceiver"),Jso=o(" \u2014 "),qR=a("a"),Yso=o("PerceiverModel"),Kso=o(" (Perceiver model)"),Zso=l(),Mu=a("li"),YY=a("strong"),eno=o("plbart"),ono=o(" \u2014 "),GR=a("a"),rno=o("PLBartModel"),tno=o(" (PLBart model)"),ano=l(),Eu=a("li"),KY=a("strong"),sno=o("poolformer"),nno=o(" \u2014 "),OR=a("a"),lno=o("PoolFormerModel"),ino=o(" (PoolFormer model)"),dno=l(),yu=a("li"),ZY=a("strong"),cno=o("prophetnet"),mno=o(" \u2014 "),XR=a("a"),fno=o("ProphetNetModel"),gno=o(" (ProphetNet model)"),hno=l(),wu=a("li"),eK=a("strong"),uno=o("qdqbert"),pno=o(" \u2014 "),VR=a("a"),_no=o("QDQBertModel"),bno=o(" (QDQBert model)"),vno=l(),Au=a("li"),oK=a("strong"),Tno=o("reformer"),Fno=o(" \u2014 "),zR=a("a"),Cno=o("ReformerModel"),Mno=o(" (Reformer model)"),Eno=l(),Lu=a("li"),rK=a("strong"),yno=o("rembert"),wno=o(" \u2014 "),WR=a("a"),Ano=o("RemBertModel"),Lno=o(" (RemBERT model)"),Bno=l(),Bu=a("li"),tK=a("strong"),xno=o("retribert"),kno=o(" \u2014 "),QR=a("a"),Rno=o("RetriBertModel"),Sno=o(" (RetriBERT model)"),Pno=l(),xu=a("li"),aK=a("strong"),$no=o("roberta"),Ino=o(" \u2014 "),HR=a("a"),Dno=o("RobertaModel"),jno=o(" (RoBERTa model)"),Nno=l(),ku=a("li"),sK=a("strong"),qno=o("roformer"),Gno=o(" \u2014 "),UR=a("a"),Ono=o("RoFormerModel"),Xno=o(" (RoFormer model)"),Vno=l(),Ru=a("li"),nK=a("strong"),zno=o("segformer"),Wno=o(" \u2014 "),JR=a("a"),Qno=o("SegformerModel"),Hno=o(" (SegFormer model)"),Uno=l(),Su=a("li"),lK=a("strong"),Jno=o("sew"),Yno=o(" \u2014 "),YR=a("a"),Kno=o("SEWModel"),Zno=o(" (SEW model)"),elo=l(),Pu=a("li"),iK=a("strong"),olo=o("sew-d"),rlo=o(" \u2014 "),KR=a("a"),tlo=o("SEWDModel"),alo=o(" (SEW-D model)"),slo=l(),$u=a("li"),dK=a("strong"),nlo=o("speech_to_text"),llo=o(" \u2014 "),ZR=a("a"),ilo=o("Speech2TextModel"),dlo=o(" (Speech2Text model)"),clo=l(),Iu=a("li"),cK=a("strong"),mlo=o("splinter"),flo=o(" \u2014 "),eS=a("a"),glo=o("SplinterModel"),hlo=o(" (Splinter model)"),ulo=l(),Du=a("li"),mK=a("strong"),plo=o("squeezebert"),_lo=o(" \u2014 "),oS=a("a"),blo=o("SqueezeBertModel"),vlo=o(" (SqueezeBERT model)"),Tlo=l(),ju=a("li"),fK=a("strong"),Flo=o("swin"),Clo=o(" \u2014 "),rS=a("a"),Mlo=o("SwinModel"),Elo=o(" (Swin model)"),ylo=l(),Nu=a("li"),gK=a("strong"),wlo=o("t5"),Alo=o(" \u2014 "),tS=a("a"),Llo=o("T5Model"),Blo=o(" (T5 model)"),xlo=l(),qu=a("li"),hK=a("strong"),klo=o("tapas"),Rlo=o(" \u2014 "),aS=a("a"),Slo=o("TapasModel"),Plo=o(" (TAPAS model)"),$lo=l(),Gu=a("li"),uK=a("strong"),Ilo=o("transfo-xl"),Dlo=o(" \u2014 "),sS=a("a"),jlo=o("TransfoXLModel"),Nlo=o(" (Transformer-XL model)"),qlo=l(),Ou=a("li"),pK=a("strong"),Glo=o("unispeech"),Olo=o(" \u2014 "),nS=a("a"),Xlo=o("UniSpeechModel"),Vlo=o(" (UniSpeech model)"),zlo=l(),Xu=a("li"),_K=a("strong"),Wlo=o("unispeech-sat"),Qlo=o(" \u2014 "),lS=a("a"),Hlo=o("UniSpeechSatModel"),Ulo=o(" (UniSpeechSat model)"),Jlo=l(),Vu=a("li"),bK=a("strong"),Ylo=o("vilt"),Klo=o(" \u2014 "),iS=a("a"),Zlo=o("ViltModel"),eio=o(" (ViLT model)"),oio=l(),zu=a("li"),vK=a("strong"),rio=o("vision-text-dual-encoder"),tio=o(" \u2014 "),dS=a("a"),aio=o("VisionTextDualEncoderModel"),sio=o(" (VisionTextDualEncoder model)"),nio=l(),Wu=a("li"),TK=a("strong"),lio=o("visual_bert"),iio=o(" \u2014 "),cS=a("a"),dio=o("VisualBertModel"),cio=o(" (VisualBert model)"),mio=l(),Qu=a("li"),FK=a("strong"),fio=o("vit"),gio=o(" \u2014 "),mS=a("a"),hio=o("ViTModel"),uio=o(" (ViT model)"),pio=l(),Hu=a("li"),CK=a("strong"),_io=o("vit_mae"),bio=o(" \u2014 "),fS=a("a"),vio=o("ViTMAEModel"),Tio=o(" (ViTMAE model)"),Fio=l(),Uu=a("li"),MK=a("strong"),Cio=o("wav2vec2"),Mio=o(" \u2014 "),gS=a("a"),Eio=o("Wav2Vec2Model"),yio=o(" (Wav2Vec2 model)"),wio=l(),Ju=a("li"),EK=a("strong"),Aio=o("wavlm"),Lio=o(" \u2014 "),hS=a("a"),Bio=o("WavLMModel"),xio=o(" (WavLM model)"),kio=l(),Yu=a("li"),yK=a("strong"),Rio=o("xglm"),Sio=o(" \u2014 "),uS=a("a"),Pio=o("XGLMModel"),$io=o(" (XGLM model)"),Iio=l(),Ku=a("li"),wK=a("strong"),Dio=o("xlm"),jio=o(" \u2014 "),pS=a("a"),Nio=o("XLMModel"),qio=o(" (XLM model)"),Gio=l(),Zu=a("li"),AK=a("strong"),Oio=o("xlm-prophetnet"),Xio=o(" \u2014 "),_S=a("a"),Vio=o("XLMProphetNetModel"),zio=o(" (XLMProphetNet model)"),Wio=l(),ep=a("li"),LK=a("strong"),Qio=o("xlm-roberta"),Hio=o(" \u2014 "),bS=a("a"),Uio=o("XLMRobertaModel"),Jio=o(" (XLM-RoBERTa model)"),Yio=l(),op=a("li"),BK=a("strong"),Kio=o("xlm-roberta-xl"),Zio=o(" \u2014 "),vS=a("a"),edo=o("XLMRobertaXLModel"),odo=o(" (XLM-RoBERTa-XL model)"),rdo=l(),rp=a("li"),xK=a("strong"),tdo=o("xlnet"),ado=o(" \u2014 "),TS=a("a"),sdo=o("XLNetModel"),ndo=o(" (XLNet model)"),ldo=l(),tp=a("li"),kK=a("strong"),ido=o("yoso"),ddo=o(" \u2014 "),FS=a("a"),cdo=o("YosoModel"),mdo=o(" (YOSO model)"),fdo=l(),ap=a("p"),gdo=o("The model is set in evaluation mode by default using "),RK=a("code"),hdo=o("model.eval()"),udo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SK=a("code"),pdo=o("model.train()"),_do=l(),PK=a("p"),bdo=o("Examples:"),vdo=l(),m(dy.$$.fragment),z9e=l(),Wi=a("h2"),sp=a("a"),$K=a("span"),m(cy.$$.fragment),Tdo=l(),IK=a("span"),Fdo=o("AutoModelForPreTraining"),W9e=l(),Ho=a("div"),m(my.$$.fragment),Cdo=l(),Qi=a("p"),Mdo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),DK=a("code"),Edo=o("from_pretrained()"),ydo=o("class method or the "),jK=a("code"),wdo=o("from_config()"),Ado=o(`class
method.`),Ldo=l(),fy=a("p"),Bdo=o("This class cannot be instantiated directly using "),NK=a("code"),xdo=o("__init__()"),kdo=o(" (throws an error)."),Rdo=l(),Gr=a("div"),m(gy.$$.fragment),Sdo=l(),qK=a("p"),Pdo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),$do=l(),Hi=a("p"),Ido=o(`Note:
Loading a model from its configuration file does `),GK=a("strong"),Ddo=o("not"),jdo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),OK=a("code"),Ndo=o("from_pretrained()"),qdo=o("to load the model weights."),Gdo=l(),XK=a("p"),Odo=o("Examples:"),Xdo=l(),m(hy.$$.fragment),Vdo=l(),Se=a("div"),m(uy.$$.fragment),zdo=l(),VK=a("p"),Wdo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Qdo=l(),qa=a("p"),Hdo=o("The model class to instantiate is selected based on the "),zK=a("code"),Udo=o("model_type"),Jdo=o(` property of the config object (either
passed as an argument or loaded from `),WK=a("code"),Ydo=o("pretrained_model_name_or_path"),Kdo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QK=a("code"),Zdo=o("pretrained_model_name_or_path"),eco=o(":"),oco=l(),k=a("ul"),np=a("li"),HK=a("strong"),rco=o("albert"),tco=o(" \u2014 "),CS=a("a"),aco=o("AlbertForPreTraining"),sco=o(" (ALBERT model)"),nco=l(),lp=a("li"),UK=a("strong"),lco=o("bart"),ico=o(" \u2014 "),MS=a("a"),dco=o("BartForConditionalGeneration"),cco=o(" (BART model)"),mco=l(),ip=a("li"),JK=a("strong"),fco=o("bert"),gco=o(" \u2014 "),ES=a("a"),hco=o("BertForPreTraining"),uco=o(" (BERT model)"),pco=l(),dp=a("li"),YK=a("strong"),_co=o("big_bird"),bco=o(" \u2014 "),yS=a("a"),vco=o("BigBirdForPreTraining"),Tco=o(" (BigBird model)"),Fco=l(),cp=a("li"),KK=a("strong"),Cco=o("camembert"),Mco=o(" \u2014 "),wS=a("a"),Eco=o("CamembertForMaskedLM"),yco=o(" (CamemBERT model)"),wco=l(),mp=a("li"),ZK=a("strong"),Aco=o("ctrl"),Lco=o(" \u2014 "),AS=a("a"),Bco=o("CTRLLMHeadModel"),xco=o(" (CTRL model)"),kco=l(),fp=a("li"),eZ=a("strong"),Rco=o("data2vec-text"),Sco=o(" \u2014 "),LS=a("a"),Pco=o("Data2VecTextForMaskedLM"),$co=o(" (Data2VecText model)"),Ico=l(),gp=a("li"),oZ=a("strong"),Dco=o("deberta"),jco=o(" \u2014 "),BS=a("a"),Nco=o("DebertaForMaskedLM"),qco=o(" (DeBERTa model)"),Gco=l(),hp=a("li"),rZ=a("strong"),Oco=o("deberta-v2"),Xco=o(" \u2014 "),xS=a("a"),Vco=o("DebertaV2ForMaskedLM"),zco=o(" (DeBERTa-v2 model)"),Wco=l(),up=a("li"),tZ=a("strong"),Qco=o("distilbert"),Hco=o(" \u2014 "),kS=a("a"),Uco=o("DistilBertForMaskedLM"),Jco=o(" (DistilBERT model)"),Yco=l(),pp=a("li"),aZ=a("strong"),Kco=o("electra"),Zco=o(" \u2014 "),RS=a("a"),emo=o("ElectraForPreTraining"),omo=o(" (ELECTRA model)"),rmo=l(),_p=a("li"),sZ=a("strong"),tmo=o("flaubert"),amo=o(" \u2014 "),SS=a("a"),smo=o("FlaubertWithLMHeadModel"),nmo=o(" (FlauBERT model)"),lmo=l(),bp=a("li"),nZ=a("strong"),imo=o("fnet"),dmo=o(" \u2014 "),PS=a("a"),cmo=o("FNetForPreTraining"),mmo=o(" (FNet model)"),fmo=l(),vp=a("li"),lZ=a("strong"),gmo=o("fsmt"),hmo=o(" \u2014 "),$S=a("a"),umo=o("FSMTForConditionalGeneration"),pmo=o(" (FairSeq Machine-Translation model)"),_mo=l(),Tp=a("li"),iZ=a("strong"),bmo=o("funnel"),vmo=o(" \u2014 "),IS=a("a"),Tmo=o("FunnelForPreTraining"),Fmo=o(" (Funnel Transformer model)"),Cmo=l(),Fp=a("li"),dZ=a("strong"),Mmo=o("gpt2"),Emo=o(" \u2014 "),DS=a("a"),ymo=o("GPT2LMHeadModel"),wmo=o(" (OpenAI GPT-2 model)"),Amo=l(),Cp=a("li"),cZ=a("strong"),Lmo=o("ibert"),Bmo=o(" \u2014 "),jS=a("a"),xmo=o("IBertForMaskedLM"),kmo=o(" (I-BERT model)"),Rmo=l(),Mp=a("li"),mZ=a("strong"),Smo=o("layoutlm"),Pmo=o(" \u2014 "),NS=a("a"),$mo=o("LayoutLMForMaskedLM"),Imo=o(" (LayoutLM model)"),Dmo=l(),Ep=a("li"),fZ=a("strong"),jmo=o("longformer"),Nmo=o(" \u2014 "),qS=a("a"),qmo=o("LongformerForMaskedLM"),Gmo=o(" (Longformer model)"),Omo=l(),yp=a("li"),gZ=a("strong"),Xmo=o("lxmert"),Vmo=o(" \u2014 "),GS=a("a"),zmo=o("LxmertForPreTraining"),Wmo=o(" (LXMERT model)"),Qmo=l(),wp=a("li"),hZ=a("strong"),Hmo=o("megatron-bert"),Umo=o(" \u2014 "),OS=a("a"),Jmo=o("MegatronBertForPreTraining"),Ymo=o(" (MegatronBert model)"),Kmo=l(),Ap=a("li"),uZ=a("strong"),Zmo=o("mobilebert"),efo=o(" \u2014 "),XS=a("a"),ofo=o("MobileBertForPreTraining"),rfo=o(" (MobileBERT model)"),tfo=l(),Lp=a("li"),pZ=a("strong"),afo=o("mpnet"),sfo=o(" \u2014 "),VS=a("a"),nfo=o("MPNetForMaskedLM"),lfo=o(" (MPNet model)"),ifo=l(),Bp=a("li"),_Z=a("strong"),dfo=o("openai-gpt"),cfo=o(" \u2014 "),zS=a("a"),mfo=o("OpenAIGPTLMHeadModel"),ffo=o(" (OpenAI GPT model)"),gfo=l(),xp=a("li"),bZ=a("strong"),hfo=o("retribert"),ufo=o(" \u2014 "),WS=a("a"),pfo=o("RetriBertModel"),_fo=o(" (RetriBERT model)"),bfo=l(),kp=a("li"),vZ=a("strong"),vfo=o("roberta"),Tfo=o(" \u2014 "),QS=a("a"),Ffo=o("RobertaForMaskedLM"),Cfo=o(" (RoBERTa model)"),Mfo=l(),Rp=a("li"),TZ=a("strong"),Efo=o("squeezebert"),yfo=o(" \u2014 "),HS=a("a"),wfo=o("SqueezeBertForMaskedLM"),Afo=o(" (SqueezeBERT model)"),Lfo=l(),Sp=a("li"),FZ=a("strong"),Bfo=o("t5"),xfo=o(" \u2014 "),US=a("a"),kfo=o("T5ForConditionalGeneration"),Rfo=o(" (T5 model)"),Sfo=l(),Pp=a("li"),CZ=a("strong"),Pfo=o("tapas"),$fo=o(" \u2014 "),JS=a("a"),Ifo=o("TapasForMaskedLM"),Dfo=o(" (TAPAS model)"),jfo=l(),$p=a("li"),MZ=a("strong"),Nfo=o("transfo-xl"),qfo=o(" \u2014 "),YS=a("a"),Gfo=o("TransfoXLLMHeadModel"),Ofo=o(" (Transformer-XL model)"),Xfo=l(),Ip=a("li"),EZ=a("strong"),Vfo=o("unispeech"),zfo=o(" \u2014 "),KS=a("a"),Wfo=o("UniSpeechForPreTraining"),Qfo=o(" (UniSpeech model)"),Hfo=l(),Dp=a("li"),yZ=a("strong"),Ufo=o("unispeech-sat"),Jfo=o(" \u2014 "),ZS=a("a"),Yfo=o("UniSpeechSatForPreTraining"),Kfo=o(" (UniSpeechSat model)"),Zfo=l(),jp=a("li"),wZ=a("strong"),ego=o("visual_bert"),ogo=o(" \u2014 "),eP=a("a"),rgo=o("VisualBertForPreTraining"),tgo=o(" (VisualBert model)"),ago=l(),Np=a("li"),AZ=a("strong"),sgo=o("vit_mae"),ngo=o(" \u2014 "),oP=a("a"),lgo=o("ViTMAEForPreTraining"),igo=o(" (ViTMAE model)"),dgo=l(),qp=a("li"),LZ=a("strong"),cgo=o("wav2vec2"),mgo=o(" \u2014 "),rP=a("a"),fgo=o("Wav2Vec2ForPreTraining"),ggo=o(" (Wav2Vec2 model)"),hgo=l(),Gp=a("li"),BZ=a("strong"),ugo=o("xlm"),pgo=o(" \u2014 "),tP=a("a"),_go=o("XLMWithLMHeadModel"),bgo=o(" (XLM model)"),vgo=l(),Op=a("li"),xZ=a("strong"),Tgo=o("xlm-roberta"),Fgo=o(" \u2014 "),aP=a("a"),Cgo=o("XLMRobertaForMaskedLM"),Mgo=o(" (XLM-RoBERTa model)"),Ego=l(),Xp=a("li"),kZ=a("strong"),ygo=o("xlm-roberta-xl"),wgo=o(" \u2014 "),sP=a("a"),Ago=o("XLMRobertaXLForMaskedLM"),Lgo=o(" (XLM-RoBERTa-XL model)"),Bgo=l(),Vp=a("li"),RZ=a("strong"),xgo=o("xlnet"),kgo=o(" \u2014 "),nP=a("a"),Rgo=o("XLNetLMHeadModel"),Sgo=o(" (XLNet model)"),Pgo=l(),zp=a("p"),$go=o("The model is set in evaluation mode by default using "),SZ=a("code"),Igo=o("model.eval()"),Dgo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),PZ=a("code"),jgo=o("model.train()"),Ngo=l(),$Z=a("p"),qgo=o("Examples:"),Ggo=l(),m(py.$$.fragment),Q9e=l(),Ui=a("h2"),Wp=a("a"),IZ=a("span"),m(_y.$$.fragment),Ogo=l(),DZ=a("span"),Xgo=o("AutoModelForCausalLM"),H9e=l(),Uo=a("div"),m(by.$$.fragment),Vgo=l(),Ji=a("p"),zgo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),jZ=a("code"),Wgo=o("from_pretrained()"),Qgo=o("class method or the "),NZ=a("code"),Hgo=o("from_config()"),Ugo=o(`class
method.`),Jgo=l(),vy=a("p"),Ygo=o("This class cannot be instantiated directly using "),qZ=a("code"),Kgo=o("__init__()"),Zgo=o(" (throws an error)."),eho=l(),Or=a("div"),m(Ty.$$.fragment),oho=l(),GZ=a("p"),rho=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),tho=l(),Yi=a("p"),aho=o(`Note:
Loading a model from its configuration file does `),OZ=a("strong"),sho=o("not"),nho=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),XZ=a("code"),lho=o("from_pretrained()"),iho=o("to load the model weights."),dho=l(),VZ=a("p"),cho=o("Examples:"),mho=l(),m(Fy.$$.fragment),fho=l(),Pe=a("div"),m(Cy.$$.fragment),gho=l(),zZ=a("p"),hho=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),uho=l(),Ga=a("p"),pho=o("The model class to instantiate is selected based on the "),WZ=a("code"),_ho=o("model_type"),bho=o(` property of the config object (either
passed as an argument or loaded from `),QZ=a("code"),vho=o("pretrained_model_name_or_path"),Tho=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),HZ=a("code"),Fho=o("pretrained_model_name_or_path"),Cho=o(":"),Mho=l(),$=a("ul"),Qp=a("li"),UZ=a("strong"),Eho=o("bart"),yho=o(" \u2014 "),lP=a("a"),who=o("BartForCausalLM"),Aho=o(" (BART model)"),Lho=l(),Hp=a("li"),JZ=a("strong"),Bho=o("bert"),xho=o(" \u2014 "),iP=a("a"),kho=o("BertLMHeadModel"),Rho=o(" (BERT model)"),Sho=l(),Up=a("li"),YZ=a("strong"),Pho=o("bert-generation"),$ho=o(" \u2014 "),dP=a("a"),Iho=o("BertGenerationDecoder"),Dho=o(" (Bert Generation model)"),jho=l(),Jp=a("li"),KZ=a("strong"),Nho=o("big_bird"),qho=o(" \u2014 "),cP=a("a"),Gho=o("BigBirdForCausalLM"),Oho=o(" (BigBird model)"),Xho=l(),Yp=a("li"),ZZ=a("strong"),Vho=o("bigbird_pegasus"),zho=o(" \u2014 "),mP=a("a"),Who=o("BigBirdPegasusForCausalLM"),Qho=o(" (BigBirdPegasus model)"),Hho=l(),Kp=a("li"),eee=a("strong"),Uho=o("blenderbot"),Jho=o(" \u2014 "),fP=a("a"),Yho=o("BlenderbotForCausalLM"),Kho=o(" (Blenderbot model)"),Zho=l(),Zp=a("li"),oee=a("strong"),euo=o("blenderbot-small"),ouo=o(" \u2014 "),gP=a("a"),ruo=o("BlenderbotSmallForCausalLM"),tuo=o(" (BlenderbotSmall model)"),auo=l(),e_=a("li"),ree=a("strong"),suo=o("camembert"),nuo=o(" \u2014 "),hP=a("a"),luo=o("CamembertForCausalLM"),iuo=o(" (CamemBERT model)"),duo=l(),o_=a("li"),tee=a("strong"),cuo=o("ctrl"),muo=o(" \u2014 "),uP=a("a"),fuo=o("CTRLLMHeadModel"),guo=o(" (CTRL model)"),huo=l(),r_=a("li"),aee=a("strong"),uuo=o("data2vec-text"),puo=o(" \u2014 "),pP=a("a"),_uo=o("Data2VecTextForCausalLM"),buo=o(" (Data2VecText model)"),vuo=l(),t_=a("li"),see=a("strong"),Tuo=o("electra"),Fuo=o(" \u2014 "),_P=a("a"),Cuo=o("ElectraForCausalLM"),Muo=o(" (ELECTRA model)"),Euo=l(),a_=a("li"),nee=a("strong"),yuo=o("gpt2"),wuo=o(" \u2014 "),bP=a("a"),Auo=o("GPT2LMHeadModel"),Luo=o(" (OpenAI GPT-2 model)"),Buo=l(),s_=a("li"),lee=a("strong"),xuo=o("gpt_neo"),kuo=o(" \u2014 "),vP=a("a"),Ruo=o("GPTNeoForCausalLM"),Suo=o(" (GPT Neo model)"),Puo=l(),n_=a("li"),iee=a("strong"),$uo=o("gptj"),Iuo=o(" \u2014 "),TP=a("a"),Duo=o("GPTJForCausalLM"),juo=o(" (GPT-J model)"),Nuo=l(),l_=a("li"),dee=a("strong"),quo=o("marian"),Guo=o(" \u2014 "),FP=a("a"),Ouo=o("MarianForCausalLM"),Xuo=o(" (Marian model)"),Vuo=l(),i_=a("li"),cee=a("strong"),zuo=o("mbart"),Wuo=o(" \u2014 "),CP=a("a"),Quo=o("MBartForCausalLM"),Huo=o(" (mBART model)"),Uuo=l(),d_=a("li"),mee=a("strong"),Juo=o("megatron-bert"),Yuo=o(" \u2014 "),MP=a("a"),Kuo=o("MegatronBertForCausalLM"),Zuo=o(" (MegatronBert model)"),epo=l(),c_=a("li"),fee=a("strong"),opo=o("openai-gpt"),rpo=o(" \u2014 "),EP=a("a"),tpo=o("OpenAIGPTLMHeadModel"),apo=o(" (OpenAI GPT model)"),spo=l(),m_=a("li"),gee=a("strong"),npo=o("pegasus"),lpo=o(" \u2014 "),yP=a("a"),ipo=o("PegasusForCausalLM"),dpo=o(" (Pegasus model)"),cpo=l(),f_=a("li"),hee=a("strong"),mpo=o("plbart"),fpo=o(" \u2014 "),wP=a("a"),gpo=o("PLBartForCausalLM"),hpo=o(" (PLBart model)"),upo=l(),g_=a("li"),uee=a("strong"),ppo=o("prophetnet"),_po=o(" \u2014 "),AP=a("a"),bpo=o("ProphetNetForCausalLM"),vpo=o(" (ProphetNet model)"),Tpo=l(),h_=a("li"),pee=a("strong"),Fpo=o("qdqbert"),Cpo=o(" \u2014 "),LP=a("a"),Mpo=o("QDQBertLMHeadModel"),Epo=o(" (QDQBert model)"),ypo=l(),u_=a("li"),_ee=a("strong"),wpo=o("reformer"),Apo=o(" \u2014 "),BP=a("a"),Lpo=o("ReformerModelWithLMHead"),Bpo=o(" (Reformer model)"),xpo=l(),p_=a("li"),bee=a("strong"),kpo=o("rembert"),Rpo=o(" \u2014 "),xP=a("a"),Spo=o("RemBertForCausalLM"),Ppo=o(" (RemBERT model)"),$po=l(),__=a("li"),vee=a("strong"),Ipo=o("roberta"),Dpo=o(" \u2014 "),kP=a("a"),jpo=o("RobertaForCausalLM"),Npo=o(" (RoBERTa model)"),qpo=l(),b_=a("li"),Tee=a("strong"),Gpo=o("roformer"),Opo=o(" \u2014 "),RP=a("a"),Xpo=o("RoFormerForCausalLM"),Vpo=o(" (RoFormer model)"),zpo=l(),v_=a("li"),Fee=a("strong"),Wpo=o("speech_to_text_2"),Qpo=o(" \u2014 "),SP=a("a"),Hpo=o("Speech2Text2ForCausalLM"),Upo=o(" (Speech2Text2 model)"),Jpo=l(),T_=a("li"),Cee=a("strong"),Ypo=o("transfo-xl"),Kpo=o(" \u2014 "),PP=a("a"),Zpo=o("TransfoXLLMHeadModel"),e_o=o(" (Transformer-XL model)"),o_o=l(),F_=a("li"),Mee=a("strong"),r_o=o("trocr"),t_o=o(" \u2014 "),$P=a("a"),a_o=o("TrOCRForCausalLM"),s_o=o(" (TrOCR model)"),n_o=l(),C_=a("li"),Eee=a("strong"),l_o=o("xglm"),i_o=o(" \u2014 "),IP=a("a"),d_o=o("XGLMForCausalLM"),c_o=o(" (XGLM model)"),m_o=l(),M_=a("li"),yee=a("strong"),f_o=o("xlm"),g_o=o(" \u2014 "),DP=a("a"),h_o=o("XLMWithLMHeadModel"),u_o=o(" (XLM model)"),p_o=l(),E_=a("li"),wee=a("strong"),__o=o("xlm-prophetnet"),b_o=o(" \u2014 "),jP=a("a"),v_o=o("XLMProphetNetForCausalLM"),T_o=o(" (XLMProphetNet model)"),F_o=l(),y_=a("li"),Aee=a("strong"),C_o=o("xlm-roberta"),M_o=o(" \u2014 "),NP=a("a"),E_o=o("XLMRobertaForCausalLM"),y_o=o(" (XLM-RoBERTa model)"),w_o=l(),w_=a("li"),Lee=a("strong"),A_o=o("xlm-roberta-xl"),L_o=o(" \u2014 "),qP=a("a"),B_o=o("XLMRobertaXLForCausalLM"),x_o=o(" (XLM-RoBERTa-XL model)"),k_o=l(),A_=a("li"),Bee=a("strong"),R_o=o("xlnet"),S_o=o(" \u2014 "),GP=a("a"),P_o=o("XLNetLMHeadModel"),$_o=o(" (XLNet model)"),I_o=l(),L_=a("p"),D_o=o("The model is set in evaluation mode by default using "),xee=a("code"),j_o=o("model.eval()"),N_o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=a("code"),q_o=o("model.train()"),G_o=l(),Ree=a("p"),O_o=o("Examples:"),X_o=l(),m(My.$$.fragment),U9e=l(),Ki=a("h2"),B_=a("a"),See=a("span"),m(Ey.$$.fragment),V_o=l(),Pee=a("span"),z_o=o("AutoModelForMaskedLM"),J9e=l(),Jo=a("div"),m(yy.$$.fragment),W_o=l(),Zi=a("p"),Q_o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),$ee=a("code"),H_o=o("from_pretrained()"),U_o=o("class method or the "),Iee=a("code"),J_o=o("from_config()"),Y_o=o(`class
method.`),K_o=l(),wy=a("p"),Z_o=o("This class cannot be instantiated directly using "),Dee=a("code"),ebo=o("__init__()"),obo=o(" (throws an error)."),rbo=l(),Xr=a("div"),m(Ay.$$.fragment),tbo=l(),jee=a("p"),abo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),sbo=l(),ed=a("p"),nbo=o(`Note:
Loading a model from its configuration file does `),Nee=a("strong"),lbo=o("not"),ibo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qee=a("code"),dbo=o("from_pretrained()"),cbo=o("to load the model weights."),mbo=l(),Gee=a("p"),fbo=o("Examples:"),gbo=l(),m(Ly.$$.fragment),hbo=l(),$e=a("div"),m(By.$$.fragment),ubo=l(),Oee=a("p"),pbo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),_bo=l(),Oa=a("p"),bbo=o("The model class to instantiate is selected based on the "),Xee=a("code"),vbo=o("model_type"),Tbo=o(` property of the config object (either
passed as an argument or loaded from `),Vee=a("code"),Fbo=o("pretrained_model_name_or_path"),Cbo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=a("code"),Mbo=o("pretrained_model_name_or_path"),Ebo=o(":"),ybo=l(),I=a("ul"),x_=a("li"),Wee=a("strong"),wbo=o("albert"),Abo=o(" \u2014 "),OP=a("a"),Lbo=o("AlbertForMaskedLM"),Bbo=o(" (ALBERT model)"),xbo=l(),k_=a("li"),Qee=a("strong"),kbo=o("bart"),Rbo=o(" \u2014 "),XP=a("a"),Sbo=o("BartForConditionalGeneration"),Pbo=o(" (BART model)"),$bo=l(),R_=a("li"),Hee=a("strong"),Ibo=o("bert"),Dbo=o(" \u2014 "),VP=a("a"),jbo=o("BertForMaskedLM"),Nbo=o(" (BERT model)"),qbo=l(),S_=a("li"),Uee=a("strong"),Gbo=o("big_bird"),Obo=o(" \u2014 "),zP=a("a"),Xbo=o("BigBirdForMaskedLM"),Vbo=o(" (BigBird model)"),zbo=l(),P_=a("li"),Jee=a("strong"),Wbo=o("camembert"),Qbo=o(" \u2014 "),WP=a("a"),Hbo=o("CamembertForMaskedLM"),Ubo=o(" (CamemBERT model)"),Jbo=l(),$_=a("li"),Yee=a("strong"),Ybo=o("convbert"),Kbo=o(" \u2014 "),QP=a("a"),Zbo=o("ConvBertForMaskedLM"),e2o=o(" (ConvBERT model)"),o2o=l(),I_=a("li"),Kee=a("strong"),r2o=o("data2vec-text"),t2o=o(" \u2014 "),HP=a("a"),a2o=o("Data2VecTextForMaskedLM"),s2o=o(" (Data2VecText model)"),n2o=l(),D_=a("li"),Zee=a("strong"),l2o=o("deberta"),i2o=o(" \u2014 "),UP=a("a"),d2o=o("DebertaForMaskedLM"),c2o=o(" (DeBERTa model)"),m2o=l(),j_=a("li"),eoe=a("strong"),f2o=o("deberta-v2"),g2o=o(" \u2014 "),JP=a("a"),h2o=o("DebertaV2ForMaskedLM"),u2o=o(" (DeBERTa-v2 model)"),p2o=l(),N_=a("li"),ooe=a("strong"),_2o=o("distilbert"),b2o=o(" \u2014 "),YP=a("a"),v2o=o("DistilBertForMaskedLM"),T2o=o(" (DistilBERT model)"),F2o=l(),q_=a("li"),roe=a("strong"),C2o=o("electra"),M2o=o(" \u2014 "),KP=a("a"),E2o=o("ElectraForMaskedLM"),y2o=o(" (ELECTRA model)"),w2o=l(),G_=a("li"),toe=a("strong"),A2o=o("flaubert"),L2o=o(" \u2014 "),ZP=a("a"),B2o=o("FlaubertWithLMHeadModel"),x2o=o(" (FlauBERT model)"),k2o=l(),O_=a("li"),aoe=a("strong"),R2o=o("fnet"),S2o=o(" \u2014 "),e$=a("a"),P2o=o("FNetForMaskedLM"),$2o=o(" (FNet model)"),I2o=l(),X_=a("li"),soe=a("strong"),D2o=o("funnel"),j2o=o(" \u2014 "),o$=a("a"),N2o=o("FunnelForMaskedLM"),q2o=o(" (Funnel Transformer model)"),G2o=l(),V_=a("li"),noe=a("strong"),O2o=o("ibert"),X2o=o(" \u2014 "),r$=a("a"),V2o=o("IBertForMaskedLM"),z2o=o(" (I-BERT model)"),W2o=l(),z_=a("li"),loe=a("strong"),Q2o=o("layoutlm"),H2o=o(" \u2014 "),t$=a("a"),U2o=o("LayoutLMForMaskedLM"),J2o=o(" (LayoutLM model)"),Y2o=l(),W_=a("li"),ioe=a("strong"),K2o=o("longformer"),Z2o=o(" \u2014 "),a$=a("a"),evo=o("LongformerForMaskedLM"),ovo=o(" (Longformer model)"),rvo=l(),Q_=a("li"),doe=a("strong"),tvo=o("mbart"),avo=o(" \u2014 "),s$=a("a"),svo=o("MBartForConditionalGeneration"),nvo=o(" (mBART model)"),lvo=l(),H_=a("li"),coe=a("strong"),ivo=o("megatron-bert"),dvo=o(" \u2014 "),n$=a("a"),cvo=o("MegatronBertForMaskedLM"),mvo=o(" (MegatronBert model)"),fvo=l(),U_=a("li"),moe=a("strong"),gvo=o("mobilebert"),hvo=o(" \u2014 "),l$=a("a"),uvo=o("MobileBertForMaskedLM"),pvo=o(" (MobileBERT model)"),_vo=l(),J_=a("li"),foe=a("strong"),bvo=o("mpnet"),vvo=o(" \u2014 "),i$=a("a"),Tvo=o("MPNetForMaskedLM"),Fvo=o(" (MPNet model)"),Cvo=l(),Y_=a("li"),goe=a("strong"),Mvo=o("nystromformer"),Evo=o(" \u2014 "),d$=a("a"),yvo=o("NystromformerForMaskedLM"),wvo=o(" (Nystromformer model)"),Avo=l(),K_=a("li"),hoe=a("strong"),Lvo=o("perceiver"),Bvo=o(" \u2014 "),c$=a("a"),xvo=o("PerceiverForMaskedLM"),kvo=o(" (Perceiver model)"),Rvo=l(),Z_=a("li"),uoe=a("strong"),Svo=o("qdqbert"),Pvo=o(" \u2014 "),m$=a("a"),$vo=o("QDQBertForMaskedLM"),Ivo=o(" (QDQBert model)"),Dvo=l(),eb=a("li"),poe=a("strong"),jvo=o("reformer"),Nvo=o(" \u2014 "),f$=a("a"),qvo=o("ReformerForMaskedLM"),Gvo=o(" (Reformer model)"),Ovo=l(),ob=a("li"),_oe=a("strong"),Xvo=o("rembert"),Vvo=o(" \u2014 "),g$=a("a"),zvo=o("RemBertForMaskedLM"),Wvo=o(" (RemBERT model)"),Qvo=l(),rb=a("li"),boe=a("strong"),Hvo=o("roberta"),Uvo=o(" \u2014 "),h$=a("a"),Jvo=o("RobertaForMaskedLM"),Yvo=o(" (RoBERTa model)"),Kvo=l(),tb=a("li"),voe=a("strong"),Zvo=o("roformer"),eTo=o(" \u2014 "),u$=a("a"),oTo=o("RoFormerForMaskedLM"),rTo=o(" (RoFormer model)"),tTo=l(),ab=a("li"),Toe=a("strong"),aTo=o("squeezebert"),sTo=o(" \u2014 "),p$=a("a"),nTo=o("SqueezeBertForMaskedLM"),lTo=o(" (SqueezeBERT model)"),iTo=l(),sb=a("li"),Foe=a("strong"),dTo=o("tapas"),cTo=o(" \u2014 "),_$=a("a"),mTo=o("TapasForMaskedLM"),fTo=o(" (TAPAS model)"),gTo=l(),nb=a("li"),Coe=a("strong"),hTo=o("wav2vec2"),uTo=o(" \u2014 "),Moe=a("code"),pTo=o("Wav2Vec2ForMaskedLM"),_To=o("(Wav2Vec2 model)"),bTo=l(),lb=a("li"),Eoe=a("strong"),vTo=o("xlm"),TTo=o(" \u2014 "),b$=a("a"),FTo=o("XLMWithLMHeadModel"),CTo=o(" (XLM model)"),MTo=l(),ib=a("li"),yoe=a("strong"),ETo=o("xlm-roberta"),yTo=o(" \u2014 "),v$=a("a"),wTo=o("XLMRobertaForMaskedLM"),ATo=o(" (XLM-RoBERTa model)"),LTo=l(),db=a("li"),woe=a("strong"),BTo=o("xlm-roberta-xl"),xTo=o(" \u2014 "),T$=a("a"),kTo=o("XLMRobertaXLForMaskedLM"),RTo=o(" (XLM-RoBERTa-XL model)"),STo=l(),cb=a("li"),Aoe=a("strong"),PTo=o("yoso"),$To=o(" \u2014 "),F$=a("a"),ITo=o("YosoForMaskedLM"),DTo=o(" (YOSO model)"),jTo=l(),mb=a("p"),NTo=o("The model is set in evaluation mode by default using "),Loe=a("code"),qTo=o("model.eval()"),GTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Boe=a("code"),OTo=o("model.train()"),XTo=l(),xoe=a("p"),VTo=o("Examples:"),zTo=l(),m(xy.$$.fragment),Y9e=l(),od=a("h2"),fb=a("a"),koe=a("span"),m(ky.$$.fragment),WTo=l(),Roe=a("span"),QTo=o("AutoModelForSeq2SeqLM"),K9e=l(),Yo=a("div"),m(Ry.$$.fragment),HTo=l(),rd=a("p"),UTo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Soe=a("code"),JTo=o("from_pretrained()"),YTo=o("class method or the "),Poe=a("code"),KTo=o("from_config()"),ZTo=o(`class
method.`),e1o=l(),Sy=a("p"),o1o=o("This class cannot be instantiated directly using "),$oe=a("code"),r1o=o("__init__()"),t1o=o(" (throws an error)."),a1o=l(),Vr=a("div"),m(Py.$$.fragment),s1o=l(),Ioe=a("p"),n1o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),l1o=l(),td=a("p"),i1o=o(`Note:
Loading a model from its configuration file does `),Doe=a("strong"),d1o=o("not"),c1o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),joe=a("code"),m1o=o("from_pretrained()"),f1o=o("to load the model weights."),g1o=l(),Noe=a("p"),h1o=o("Examples:"),u1o=l(),m($y.$$.fragment),p1o=l(),Ie=a("div"),m(Iy.$$.fragment),_1o=l(),qoe=a("p"),b1o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),v1o=l(),Xa=a("p"),T1o=o("The model class to instantiate is selected based on the "),Goe=a("code"),F1o=o("model_type"),C1o=o(` property of the config object (either
passed as an argument or loaded from `),Ooe=a("code"),M1o=o("pretrained_model_name_or_path"),E1o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xoe=a("code"),y1o=o("pretrained_model_name_or_path"),w1o=o(":"),A1o=l(),ae=a("ul"),gb=a("li"),Voe=a("strong"),L1o=o("bart"),B1o=o(" \u2014 "),C$=a("a"),x1o=o("BartForConditionalGeneration"),k1o=o(" (BART model)"),R1o=l(),hb=a("li"),zoe=a("strong"),S1o=o("bigbird_pegasus"),P1o=o(" \u2014 "),M$=a("a"),$1o=o("BigBirdPegasusForConditionalGeneration"),I1o=o(" (BigBirdPegasus model)"),D1o=l(),ub=a("li"),Woe=a("strong"),j1o=o("blenderbot"),N1o=o(" \u2014 "),E$=a("a"),q1o=o("BlenderbotForConditionalGeneration"),G1o=o(" (Blenderbot model)"),O1o=l(),pb=a("li"),Qoe=a("strong"),X1o=o("blenderbot-small"),V1o=o(" \u2014 "),y$=a("a"),z1o=o("BlenderbotSmallForConditionalGeneration"),W1o=o(" (BlenderbotSmall model)"),Q1o=l(),_b=a("li"),Hoe=a("strong"),H1o=o("encoder-decoder"),U1o=o(" \u2014 "),w$=a("a"),J1o=o("EncoderDecoderModel"),Y1o=o(" (Encoder decoder model)"),K1o=l(),bb=a("li"),Uoe=a("strong"),Z1o=o("fsmt"),eFo=o(" \u2014 "),A$=a("a"),oFo=o("FSMTForConditionalGeneration"),rFo=o(" (FairSeq Machine-Translation model)"),tFo=l(),vb=a("li"),Joe=a("strong"),aFo=o("led"),sFo=o(" \u2014 "),L$=a("a"),nFo=o("LEDForConditionalGeneration"),lFo=o(" (LED model)"),iFo=l(),Tb=a("li"),Yoe=a("strong"),dFo=o("m2m_100"),cFo=o(" \u2014 "),B$=a("a"),mFo=o("M2M100ForConditionalGeneration"),fFo=o(" (M2M100 model)"),gFo=l(),Fb=a("li"),Koe=a("strong"),hFo=o("marian"),uFo=o(" \u2014 "),x$=a("a"),pFo=o("MarianMTModel"),_Fo=o(" (Marian model)"),bFo=l(),Cb=a("li"),Zoe=a("strong"),vFo=o("mbart"),TFo=o(" \u2014 "),k$=a("a"),FFo=o("MBartForConditionalGeneration"),CFo=o(" (mBART model)"),MFo=l(),Mb=a("li"),ere=a("strong"),EFo=o("mt5"),yFo=o(" \u2014 "),R$=a("a"),wFo=o("MT5ForConditionalGeneration"),AFo=o(" (mT5 model)"),LFo=l(),Eb=a("li"),ore=a("strong"),BFo=o("pegasus"),xFo=o(" \u2014 "),S$=a("a"),kFo=o("PegasusForConditionalGeneration"),RFo=o(" (Pegasus model)"),SFo=l(),yb=a("li"),rre=a("strong"),PFo=o("plbart"),$Fo=o(" \u2014 "),P$=a("a"),IFo=o("PLBartForConditionalGeneration"),DFo=o(" (PLBart model)"),jFo=l(),wb=a("li"),tre=a("strong"),NFo=o("prophetnet"),qFo=o(" \u2014 "),$$=a("a"),GFo=o("ProphetNetForConditionalGeneration"),OFo=o(" (ProphetNet model)"),XFo=l(),Ab=a("li"),are=a("strong"),VFo=o("t5"),zFo=o(" \u2014 "),I$=a("a"),WFo=o("T5ForConditionalGeneration"),QFo=o(" (T5 model)"),HFo=l(),Lb=a("li"),sre=a("strong"),UFo=o("xlm-prophetnet"),JFo=o(" \u2014 "),D$=a("a"),YFo=o("XLMProphetNetForConditionalGeneration"),KFo=o(" (XLMProphetNet model)"),ZFo=l(),Bb=a("p"),eCo=o("The model is set in evaluation mode by default using "),nre=a("code"),oCo=o("model.eval()"),rCo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lre=a("code"),tCo=o("model.train()"),aCo=l(),ire=a("p"),sCo=o("Examples:"),nCo=l(),m(Dy.$$.fragment),Z9e=l(),ad=a("h2"),xb=a("a"),dre=a("span"),m(jy.$$.fragment),lCo=l(),cre=a("span"),iCo=o("AutoModelForSequenceClassification"),eBe=l(),Ko=a("div"),m(Ny.$$.fragment),dCo=l(),sd=a("p"),cCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),mre=a("code"),mCo=o("from_pretrained()"),fCo=o("class method or the "),fre=a("code"),gCo=o("from_config()"),hCo=o(`class
method.`),uCo=l(),qy=a("p"),pCo=o("This class cannot be instantiated directly using "),gre=a("code"),_Co=o("__init__()"),bCo=o(" (throws an error)."),vCo=l(),zr=a("div"),m(Gy.$$.fragment),TCo=l(),hre=a("p"),FCo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),CCo=l(),nd=a("p"),MCo=o(`Note:
Loading a model from its configuration file does `),ure=a("strong"),ECo=o("not"),yCo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pre=a("code"),wCo=o("from_pretrained()"),ACo=o("to load the model weights."),LCo=l(),_re=a("p"),BCo=o("Examples:"),xCo=l(),m(Oy.$$.fragment),kCo=l(),De=a("div"),m(Xy.$$.fragment),RCo=l(),bre=a("p"),SCo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),PCo=l(),Va=a("p"),$Co=o("The model class to instantiate is selected based on the "),vre=a("code"),ICo=o("model_type"),DCo=o(` property of the config object (either
passed as an argument or loaded from `),Tre=a("code"),jCo=o("pretrained_model_name_or_path"),NCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fre=a("code"),qCo=o("pretrained_model_name_or_path"),GCo=o(":"),OCo=l(),A=a("ul"),kb=a("li"),Cre=a("strong"),XCo=o("albert"),VCo=o(" \u2014 "),j$=a("a"),zCo=o("AlbertForSequenceClassification"),WCo=o(" (ALBERT model)"),QCo=l(),Rb=a("li"),Mre=a("strong"),HCo=o("bart"),UCo=o(" \u2014 "),N$=a("a"),JCo=o("BartForSequenceClassification"),YCo=o(" (BART model)"),KCo=l(),Sb=a("li"),Ere=a("strong"),ZCo=o("bert"),e4o=o(" \u2014 "),q$=a("a"),o4o=o("BertForSequenceClassification"),r4o=o(" (BERT model)"),t4o=l(),Pb=a("li"),yre=a("strong"),a4o=o("big_bird"),s4o=o(" \u2014 "),G$=a("a"),n4o=o("BigBirdForSequenceClassification"),l4o=o(" (BigBird model)"),i4o=l(),$b=a("li"),wre=a("strong"),d4o=o("bigbird_pegasus"),c4o=o(" \u2014 "),O$=a("a"),m4o=o("BigBirdPegasusForSequenceClassification"),f4o=o(" (BigBirdPegasus model)"),g4o=l(),Ib=a("li"),Are=a("strong"),h4o=o("camembert"),u4o=o(" \u2014 "),X$=a("a"),p4o=o("CamembertForSequenceClassification"),_4o=o(" (CamemBERT model)"),b4o=l(),Db=a("li"),Lre=a("strong"),v4o=o("canine"),T4o=o(" \u2014 "),V$=a("a"),F4o=o("CanineForSequenceClassification"),C4o=o(" (Canine model)"),M4o=l(),jb=a("li"),Bre=a("strong"),E4o=o("convbert"),y4o=o(" \u2014 "),z$=a("a"),w4o=o("ConvBertForSequenceClassification"),A4o=o(" (ConvBERT model)"),L4o=l(),Nb=a("li"),xre=a("strong"),B4o=o("ctrl"),x4o=o(" \u2014 "),W$=a("a"),k4o=o("CTRLForSequenceClassification"),R4o=o(" (CTRL model)"),S4o=l(),qb=a("li"),kre=a("strong"),P4o=o("data2vec-text"),$4o=o(" \u2014 "),Q$=a("a"),I4o=o("Data2VecTextForSequenceClassification"),D4o=o(" (Data2VecText model)"),j4o=l(),Gb=a("li"),Rre=a("strong"),N4o=o("deberta"),q4o=o(" \u2014 "),H$=a("a"),G4o=o("DebertaForSequenceClassification"),O4o=o(" (DeBERTa model)"),X4o=l(),Ob=a("li"),Sre=a("strong"),V4o=o("deberta-v2"),z4o=o(" \u2014 "),U$=a("a"),W4o=o("DebertaV2ForSequenceClassification"),Q4o=o(" (DeBERTa-v2 model)"),H4o=l(),Xb=a("li"),Pre=a("strong"),U4o=o("distilbert"),J4o=o(" \u2014 "),J$=a("a"),Y4o=o("DistilBertForSequenceClassification"),K4o=o(" (DistilBERT model)"),Z4o=l(),Vb=a("li"),$re=a("strong"),eMo=o("electra"),oMo=o(" \u2014 "),Y$=a("a"),rMo=o("ElectraForSequenceClassification"),tMo=o(" (ELECTRA model)"),aMo=l(),zb=a("li"),Ire=a("strong"),sMo=o("flaubert"),nMo=o(" \u2014 "),K$=a("a"),lMo=o("FlaubertForSequenceClassification"),iMo=o(" (FlauBERT model)"),dMo=l(),Wb=a("li"),Dre=a("strong"),cMo=o("fnet"),mMo=o(" \u2014 "),Z$=a("a"),fMo=o("FNetForSequenceClassification"),gMo=o(" (FNet model)"),hMo=l(),Qb=a("li"),jre=a("strong"),uMo=o("funnel"),pMo=o(" \u2014 "),eI=a("a"),_Mo=o("FunnelForSequenceClassification"),bMo=o(" (Funnel Transformer model)"),vMo=l(),Hb=a("li"),Nre=a("strong"),TMo=o("gpt2"),FMo=o(" \u2014 "),oI=a("a"),CMo=o("GPT2ForSequenceClassification"),MMo=o(" (OpenAI GPT-2 model)"),EMo=l(),Ub=a("li"),qre=a("strong"),yMo=o("gpt_neo"),wMo=o(" \u2014 "),rI=a("a"),AMo=o("GPTNeoForSequenceClassification"),LMo=o(" (GPT Neo model)"),BMo=l(),Jb=a("li"),Gre=a("strong"),xMo=o("gptj"),kMo=o(" \u2014 "),tI=a("a"),RMo=o("GPTJForSequenceClassification"),SMo=o(" (GPT-J model)"),PMo=l(),Yb=a("li"),Ore=a("strong"),$Mo=o("ibert"),IMo=o(" \u2014 "),aI=a("a"),DMo=o("IBertForSequenceClassification"),jMo=o(" (I-BERT model)"),NMo=l(),Kb=a("li"),Xre=a("strong"),qMo=o("layoutlm"),GMo=o(" \u2014 "),sI=a("a"),OMo=o("LayoutLMForSequenceClassification"),XMo=o(" (LayoutLM model)"),VMo=l(),Zb=a("li"),Vre=a("strong"),zMo=o("layoutlmv2"),WMo=o(" \u2014 "),nI=a("a"),QMo=o("LayoutLMv2ForSequenceClassification"),HMo=o(" (LayoutLMv2 model)"),UMo=l(),e2=a("li"),zre=a("strong"),JMo=o("led"),YMo=o(" \u2014 "),lI=a("a"),KMo=o("LEDForSequenceClassification"),ZMo=o(" (LED model)"),eEo=l(),o2=a("li"),Wre=a("strong"),oEo=o("longformer"),rEo=o(" \u2014 "),iI=a("a"),tEo=o("LongformerForSequenceClassification"),aEo=o(" (Longformer model)"),sEo=l(),r2=a("li"),Qre=a("strong"),nEo=o("mbart"),lEo=o(" \u2014 "),dI=a("a"),iEo=o("MBartForSequenceClassification"),dEo=o(" (mBART model)"),cEo=l(),t2=a("li"),Hre=a("strong"),mEo=o("megatron-bert"),fEo=o(" \u2014 "),cI=a("a"),gEo=o("MegatronBertForSequenceClassification"),hEo=o(" (MegatronBert model)"),uEo=l(),a2=a("li"),Ure=a("strong"),pEo=o("mobilebert"),_Eo=o(" \u2014 "),mI=a("a"),bEo=o("MobileBertForSequenceClassification"),vEo=o(" (MobileBERT model)"),TEo=l(),s2=a("li"),Jre=a("strong"),FEo=o("mpnet"),CEo=o(" \u2014 "),fI=a("a"),MEo=o("MPNetForSequenceClassification"),EEo=o(" (MPNet model)"),yEo=l(),n2=a("li"),Yre=a("strong"),wEo=o("nystromformer"),AEo=o(" \u2014 "),gI=a("a"),LEo=o("NystromformerForSequenceClassification"),BEo=o(" (Nystromformer model)"),xEo=l(),l2=a("li"),Kre=a("strong"),kEo=o("openai-gpt"),REo=o(" \u2014 "),hI=a("a"),SEo=o("OpenAIGPTForSequenceClassification"),PEo=o(" (OpenAI GPT model)"),$Eo=l(),i2=a("li"),Zre=a("strong"),IEo=o("perceiver"),DEo=o(" \u2014 "),uI=a("a"),jEo=o("PerceiverForSequenceClassification"),NEo=o(" (Perceiver model)"),qEo=l(),d2=a("li"),ete=a("strong"),GEo=o("plbart"),OEo=o(" \u2014 "),pI=a("a"),XEo=o("PLBartForSequenceClassification"),VEo=o(" (PLBart model)"),zEo=l(),c2=a("li"),ote=a("strong"),WEo=o("qdqbert"),QEo=o(" \u2014 "),_I=a("a"),HEo=o("QDQBertForSequenceClassification"),UEo=o(" (QDQBert model)"),JEo=l(),m2=a("li"),rte=a("strong"),YEo=o("reformer"),KEo=o(" \u2014 "),bI=a("a"),ZEo=o("ReformerForSequenceClassification"),e3o=o(" (Reformer model)"),o3o=l(),f2=a("li"),tte=a("strong"),r3o=o("rembert"),t3o=o(" \u2014 "),vI=a("a"),a3o=o("RemBertForSequenceClassification"),s3o=o(" (RemBERT model)"),n3o=l(),g2=a("li"),ate=a("strong"),l3o=o("roberta"),i3o=o(" \u2014 "),TI=a("a"),d3o=o("RobertaForSequenceClassification"),c3o=o(" (RoBERTa model)"),m3o=l(),h2=a("li"),ste=a("strong"),f3o=o("roformer"),g3o=o(" \u2014 "),FI=a("a"),h3o=o("RoFormerForSequenceClassification"),u3o=o(" (RoFormer model)"),p3o=l(),u2=a("li"),nte=a("strong"),_3o=o("squeezebert"),b3o=o(" \u2014 "),CI=a("a"),v3o=o("SqueezeBertForSequenceClassification"),T3o=o(" (SqueezeBERT model)"),F3o=l(),p2=a("li"),lte=a("strong"),C3o=o("tapas"),M3o=o(" \u2014 "),MI=a("a"),E3o=o("TapasForSequenceClassification"),y3o=o(" (TAPAS model)"),w3o=l(),_2=a("li"),ite=a("strong"),A3o=o("transfo-xl"),L3o=o(" \u2014 "),EI=a("a"),B3o=o("TransfoXLForSequenceClassification"),x3o=o(" (Transformer-XL model)"),k3o=l(),b2=a("li"),dte=a("strong"),R3o=o("xlm"),S3o=o(" \u2014 "),yI=a("a"),P3o=o("XLMForSequenceClassification"),$3o=o(" (XLM model)"),I3o=l(),v2=a("li"),cte=a("strong"),D3o=o("xlm-roberta"),j3o=o(" \u2014 "),wI=a("a"),N3o=o("XLMRobertaForSequenceClassification"),q3o=o(" (XLM-RoBERTa model)"),G3o=l(),T2=a("li"),mte=a("strong"),O3o=o("xlm-roberta-xl"),X3o=o(" \u2014 "),AI=a("a"),V3o=o("XLMRobertaXLForSequenceClassification"),z3o=o(" (XLM-RoBERTa-XL model)"),W3o=l(),F2=a("li"),fte=a("strong"),Q3o=o("xlnet"),H3o=o(" \u2014 "),LI=a("a"),U3o=o("XLNetForSequenceClassification"),J3o=o(" (XLNet model)"),Y3o=l(),C2=a("li"),gte=a("strong"),K3o=o("yoso"),Z3o=o(" \u2014 "),BI=a("a"),e5o=o("YosoForSequenceClassification"),o5o=o(" (YOSO model)"),r5o=l(),M2=a("p"),t5o=o("The model is set in evaluation mode by default using "),hte=a("code"),a5o=o("model.eval()"),s5o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ute=a("code"),n5o=o("model.train()"),l5o=l(),pte=a("p"),i5o=o("Examples:"),d5o=l(),m(Vy.$$.fragment),oBe=l(),ld=a("h2"),E2=a("a"),_te=a("span"),m(zy.$$.fragment),c5o=l(),bte=a("span"),m5o=o("AutoModelForMultipleChoice"),rBe=l(),Zo=a("div"),m(Wy.$$.fragment),f5o=l(),id=a("p"),g5o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vte=a("code"),h5o=o("from_pretrained()"),u5o=o("class method or the "),Tte=a("code"),p5o=o("from_config()"),_5o=o(`class
method.`),b5o=l(),Qy=a("p"),v5o=o("This class cannot be instantiated directly using "),Fte=a("code"),T5o=o("__init__()"),F5o=o(" (throws an error)."),C5o=l(),Wr=a("div"),m(Hy.$$.fragment),M5o=l(),Cte=a("p"),E5o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),y5o=l(),dd=a("p"),w5o=o(`Note:
Loading a model from its configuration file does `),Mte=a("strong"),A5o=o("not"),L5o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ete=a("code"),B5o=o("from_pretrained()"),x5o=o("to load the model weights."),k5o=l(),yte=a("p"),R5o=o("Examples:"),S5o=l(),m(Uy.$$.fragment),P5o=l(),je=a("div"),m(Jy.$$.fragment),$5o=l(),wte=a("p"),I5o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),D5o=l(),za=a("p"),j5o=o("The model class to instantiate is selected based on the "),Ate=a("code"),N5o=o("model_type"),q5o=o(` property of the config object (either
passed as an argument or loaded from `),Lte=a("code"),G5o=o("pretrained_model_name_or_path"),O5o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bte=a("code"),X5o=o("pretrained_model_name_or_path"),V5o=o(":"),z5o=l(),G=a("ul"),y2=a("li"),xte=a("strong"),W5o=o("albert"),Q5o=o(" \u2014 "),xI=a("a"),H5o=o("AlbertForMultipleChoice"),U5o=o(" (ALBERT model)"),J5o=l(),w2=a("li"),kte=a("strong"),Y5o=o("bert"),K5o=o(" \u2014 "),kI=a("a"),Z5o=o("BertForMultipleChoice"),eyo=o(" (BERT model)"),oyo=l(),A2=a("li"),Rte=a("strong"),ryo=o("big_bird"),tyo=o(" \u2014 "),RI=a("a"),ayo=o("BigBirdForMultipleChoice"),syo=o(" (BigBird model)"),nyo=l(),L2=a("li"),Ste=a("strong"),lyo=o("camembert"),iyo=o(" \u2014 "),SI=a("a"),dyo=o("CamembertForMultipleChoice"),cyo=o(" (CamemBERT model)"),myo=l(),B2=a("li"),Pte=a("strong"),fyo=o("canine"),gyo=o(" \u2014 "),PI=a("a"),hyo=o("CanineForMultipleChoice"),uyo=o(" (Canine model)"),pyo=l(),x2=a("li"),$te=a("strong"),_yo=o("convbert"),byo=o(" \u2014 "),$I=a("a"),vyo=o("ConvBertForMultipleChoice"),Tyo=o(" (ConvBERT model)"),Fyo=l(),k2=a("li"),Ite=a("strong"),Cyo=o("data2vec-text"),Myo=o(" \u2014 "),II=a("a"),Eyo=o("Data2VecTextForMultipleChoice"),yyo=o(" (Data2VecText model)"),wyo=l(),R2=a("li"),Dte=a("strong"),Ayo=o("distilbert"),Lyo=o(" \u2014 "),DI=a("a"),Byo=o("DistilBertForMultipleChoice"),xyo=o(" (DistilBERT model)"),kyo=l(),S2=a("li"),jte=a("strong"),Ryo=o("electra"),Syo=o(" \u2014 "),jI=a("a"),Pyo=o("ElectraForMultipleChoice"),$yo=o(" (ELECTRA model)"),Iyo=l(),P2=a("li"),Nte=a("strong"),Dyo=o("flaubert"),jyo=o(" \u2014 "),NI=a("a"),Nyo=o("FlaubertForMultipleChoice"),qyo=o(" (FlauBERT model)"),Gyo=l(),$2=a("li"),qte=a("strong"),Oyo=o("fnet"),Xyo=o(" \u2014 "),qI=a("a"),Vyo=o("FNetForMultipleChoice"),zyo=o(" (FNet model)"),Wyo=l(),I2=a("li"),Gte=a("strong"),Qyo=o("funnel"),Hyo=o(" \u2014 "),GI=a("a"),Uyo=o("FunnelForMultipleChoice"),Jyo=o(" (Funnel Transformer model)"),Yyo=l(),D2=a("li"),Ote=a("strong"),Kyo=o("ibert"),Zyo=o(" \u2014 "),OI=a("a"),ewo=o("IBertForMultipleChoice"),owo=o(" (I-BERT model)"),rwo=l(),j2=a("li"),Xte=a("strong"),two=o("longformer"),awo=o(" \u2014 "),XI=a("a"),swo=o("LongformerForMultipleChoice"),nwo=o(" (Longformer model)"),lwo=l(),N2=a("li"),Vte=a("strong"),iwo=o("megatron-bert"),dwo=o(" \u2014 "),VI=a("a"),cwo=o("MegatronBertForMultipleChoice"),mwo=o(" (MegatronBert model)"),fwo=l(),q2=a("li"),zte=a("strong"),gwo=o("mobilebert"),hwo=o(" \u2014 "),zI=a("a"),uwo=o("MobileBertForMultipleChoice"),pwo=o(" (MobileBERT model)"),_wo=l(),G2=a("li"),Wte=a("strong"),bwo=o("mpnet"),vwo=o(" \u2014 "),WI=a("a"),Two=o("MPNetForMultipleChoice"),Fwo=o(" (MPNet model)"),Cwo=l(),O2=a("li"),Qte=a("strong"),Mwo=o("nystromformer"),Ewo=o(" \u2014 "),QI=a("a"),ywo=o("NystromformerForMultipleChoice"),wwo=o(" (Nystromformer model)"),Awo=l(),X2=a("li"),Hte=a("strong"),Lwo=o("qdqbert"),Bwo=o(" \u2014 "),HI=a("a"),xwo=o("QDQBertForMultipleChoice"),kwo=o(" (QDQBert model)"),Rwo=l(),V2=a("li"),Ute=a("strong"),Swo=o("rembert"),Pwo=o(" \u2014 "),UI=a("a"),$wo=o("RemBertForMultipleChoice"),Iwo=o(" (RemBERT model)"),Dwo=l(),z2=a("li"),Jte=a("strong"),jwo=o("roberta"),Nwo=o(" \u2014 "),JI=a("a"),qwo=o("RobertaForMultipleChoice"),Gwo=o(" (RoBERTa model)"),Owo=l(),W2=a("li"),Yte=a("strong"),Xwo=o("roformer"),Vwo=o(" \u2014 "),YI=a("a"),zwo=o("RoFormerForMultipleChoice"),Wwo=o(" (RoFormer model)"),Qwo=l(),Q2=a("li"),Kte=a("strong"),Hwo=o("squeezebert"),Uwo=o(" \u2014 "),KI=a("a"),Jwo=o("SqueezeBertForMultipleChoice"),Ywo=o(" (SqueezeBERT model)"),Kwo=l(),H2=a("li"),Zte=a("strong"),Zwo=o("xlm"),e6o=o(" \u2014 "),ZI=a("a"),o6o=o("XLMForMultipleChoice"),r6o=o(" (XLM model)"),t6o=l(),U2=a("li"),eae=a("strong"),a6o=o("xlm-roberta"),s6o=o(" \u2014 "),eD=a("a"),n6o=o("XLMRobertaForMultipleChoice"),l6o=o(" (XLM-RoBERTa model)"),i6o=l(),J2=a("li"),oae=a("strong"),d6o=o("xlm-roberta-xl"),c6o=o(" \u2014 "),oD=a("a"),m6o=o("XLMRobertaXLForMultipleChoice"),f6o=o(" (XLM-RoBERTa-XL model)"),g6o=l(),Y2=a("li"),rae=a("strong"),h6o=o("xlnet"),u6o=o(" \u2014 "),rD=a("a"),p6o=o("XLNetForMultipleChoice"),_6o=o(" (XLNet model)"),b6o=l(),K2=a("li"),tae=a("strong"),v6o=o("yoso"),T6o=o(" \u2014 "),tD=a("a"),F6o=o("YosoForMultipleChoice"),C6o=o(" (YOSO model)"),M6o=l(),Z2=a("p"),E6o=o("The model is set in evaluation mode by default using "),aae=a("code"),y6o=o("model.eval()"),w6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sae=a("code"),A6o=o("model.train()"),L6o=l(),nae=a("p"),B6o=o("Examples:"),x6o=l(),m(Yy.$$.fragment),tBe=l(),cd=a("h2"),ev=a("a"),lae=a("span"),m(Ky.$$.fragment),k6o=l(),iae=a("span"),R6o=o("AutoModelForNextSentencePrediction"),aBe=l(),er=a("div"),m(Zy.$$.fragment),S6o=l(),md=a("p"),P6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),dae=a("code"),$6o=o("from_pretrained()"),I6o=o("class method or the "),cae=a("code"),D6o=o("from_config()"),j6o=o(`class
method.`),N6o=l(),ew=a("p"),q6o=o("This class cannot be instantiated directly using "),mae=a("code"),G6o=o("__init__()"),O6o=o(" (throws an error)."),X6o=l(),Qr=a("div"),m(ow.$$.fragment),V6o=l(),fae=a("p"),z6o=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),W6o=l(),fd=a("p"),Q6o=o(`Note:
Loading a model from its configuration file does `),gae=a("strong"),H6o=o("not"),U6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),hae=a("code"),J6o=o("from_pretrained()"),Y6o=o("to load the model weights."),K6o=l(),uae=a("p"),Z6o=o("Examples:"),eAo=l(),m(rw.$$.fragment),oAo=l(),Ne=a("div"),m(tw.$$.fragment),rAo=l(),pae=a("p"),tAo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),aAo=l(),Wa=a("p"),sAo=o("The model class to instantiate is selected based on the "),_ae=a("code"),nAo=o("model_type"),lAo=o(` property of the config object (either
passed as an argument or loaded from `),bae=a("code"),iAo=o("pretrained_model_name_or_path"),dAo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vae=a("code"),cAo=o("pretrained_model_name_or_path"),mAo=o(":"),fAo=l(),sa=a("ul"),ov=a("li"),Tae=a("strong"),gAo=o("bert"),hAo=o(" \u2014 "),aD=a("a"),uAo=o("BertForNextSentencePrediction"),pAo=o(" (BERT model)"),_Ao=l(),rv=a("li"),Fae=a("strong"),bAo=o("fnet"),vAo=o(" \u2014 "),sD=a("a"),TAo=o("FNetForNextSentencePrediction"),FAo=o(" (FNet model)"),CAo=l(),tv=a("li"),Cae=a("strong"),MAo=o("megatron-bert"),EAo=o(" \u2014 "),nD=a("a"),yAo=o("MegatronBertForNextSentencePrediction"),wAo=o(" (MegatronBert model)"),AAo=l(),av=a("li"),Mae=a("strong"),LAo=o("mobilebert"),BAo=o(" \u2014 "),lD=a("a"),xAo=o("MobileBertForNextSentencePrediction"),kAo=o(" (MobileBERT model)"),RAo=l(),sv=a("li"),Eae=a("strong"),SAo=o("qdqbert"),PAo=o(" \u2014 "),iD=a("a"),$Ao=o("QDQBertForNextSentencePrediction"),IAo=o(" (QDQBert model)"),DAo=l(),nv=a("p"),jAo=o("The model is set in evaluation mode by default using "),yae=a("code"),NAo=o("model.eval()"),qAo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wae=a("code"),GAo=o("model.train()"),OAo=l(),Aae=a("p"),XAo=o("Examples:"),VAo=l(),m(aw.$$.fragment),sBe=l(),gd=a("h2"),lv=a("a"),Lae=a("span"),m(sw.$$.fragment),zAo=l(),Bae=a("span"),WAo=o("AutoModelForTokenClassification"),nBe=l(),or=a("div"),m(nw.$$.fragment),QAo=l(),hd=a("p"),HAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),xae=a("code"),UAo=o("from_pretrained()"),JAo=o("class method or the "),kae=a("code"),YAo=o("from_config()"),KAo=o(`class
method.`),ZAo=l(),lw=a("p"),e0o=o("This class cannot be instantiated directly using "),Rae=a("code"),o0o=o("__init__()"),r0o=o(" (throws an error)."),t0o=l(),Hr=a("div"),m(iw.$$.fragment),a0o=l(),Sae=a("p"),s0o=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),n0o=l(),ud=a("p"),l0o=o(`Note:
Loading a model from its configuration file does `),Pae=a("strong"),i0o=o("not"),d0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ae=a("code"),c0o=o("from_pretrained()"),m0o=o("to load the model weights."),f0o=l(),Iae=a("p"),g0o=o("Examples:"),h0o=l(),m(dw.$$.fragment),u0o=l(),qe=a("div"),m(cw.$$.fragment),p0o=l(),Dae=a("p"),_0o=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),b0o=l(),Qa=a("p"),v0o=o("The model class to instantiate is selected based on the "),jae=a("code"),T0o=o("model_type"),F0o=o(` property of the config object (either
passed as an argument or loaded from `),Nae=a("code"),C0o=o("pretrained_model_name_or_path"),M0o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qae=a("code"),E0o=o("pretrained_model_name_or_path"),y0o=o(":"),w0o=l(),N=a("ul"),iv=a("li"),Gae=a("strong"),A0o=o("albert"),L0o=o(" \u2014 "),dD=a("a"),B0o=o("AlbertForTokenClassification"),x0o=o(" (ALBERT model)"),k0o=l(),dv=a("li"),Oae=a("strong"),R0o=o("bert"),S0o=o(" \u2014 "),cD=a("a"),P0o=o("BertForTokenClassification"),$0o=o(" (BERT model)"),I0o=l(),cv=a("li"),Xae=a("strong"),D0o=o("big_bird"),j0o=o(" \u2014 "),mD=a("a"),N0o=o("BigBirdForTokenClassification"),q0o=o(" (BigBird model)"),G0o=l(),mv=a("li"),Vae=a("strong"),O0o=o("camembert"),X0o=o(" \u2014 "),fD=a("a"),V0o=o("CamembertForTokenClassification"),z0o=o(" (CamemBERT model)"),W0o=l(),fv=a("li"),zae=a("strong"),Q0o=o("canine"),H0o=o(" \u2014 "),gD=a("a"),U0o=o("CanineForTokenClassification"),J0o=o(" (Canine model)"),Y0o=l(),gv=a("li"),Wae=a("strong"),K0o=o("convbert"),Z0o=o(" \u2014 "),hD=a("a"),eLo=o("ConvBertForTokenClassification"),oLo=o(" (ConvBERT model)"),rLo=l(),hv=a("li"),Qae=a("strong"),tLo=o("data2vec-text"),aLo=o(" \u2014 "),uD=a("a"),sLo=o("Data2VecTextForTokenClassification"),nLo=o(" (Data2VecText model)"),lLo=l(),uv=a("li"),Hae=a("strong"),iLo=o("deberta"),dLo=o(" \u2014 "),pD=a("a"),cLo=o("DebertaForTokenClassification"),mLo=o(" (DeBERTa model)"),fLo=l(),pv=a("li"),Uae=a("strong"),gLo=o("deberta-v2"),hLo=o(" \u2014 "),_D=a("a"),uLo=o("DebertaV2ForTokenClassification"),pLo=o(" (DeBERTa-v2 model)"),_Lo=l(),_v=a("li"),Jae=a("strong"),bLo=o("distilbert"),vLo=o(" \u2014 "),bD=a("a"),TLo=o("DistilBertForTokenClassification"),FLo=o(" (DistilBERT model)"),CLo=l(),bv=a("li"),Yae=a("strong"),MLo=o("electra"),ELo=o(" \u2014 "),vD=a("a"),yLo=o("ElectraForTokenClassification"),wLo=o(" (ELECTRA model)"),ALo=l(),vv=a("li"),Kae=a("strong"),LLo=o("flaubert"),BLo=o(" \u2014 "),TD=a("a"),xLo=o("FlaubertForTokenClassification"),kLo=o(" (FlauBERT model)"),RLo=l(),Tv=a("li"),Zae=a("strong"),SLo=o("fnet"),PLo=o(" \u2014 "),FD=a("a"),$Lo=o("FNetForTokenClassification"),ILo=o(" (FNet model)"),DLo=l(),Fv=a("li"),ese=a("strong"),jLo=o("funnel"),NLo=o(" \u2014 "),CD=a("a"),qLo=o("FunnelForTokenClassification"),GLo=o(" (Funnel Transformer model)"),OLo=l(),Cv=a("li"),ose=a("strong"),XLo=o("gpt2"),VLo=o(" \u2014 "),MD=a("a"),zLo=o("GPT2ForTokenClassification"),WLo=o(" (OpenAI GPT-2 model)"),QLo=l(),Mv=a("li"),rse=a("strong"),HLo=o("ibert"),ULo=o(" \u2014 "),ED=a("a"),JLo=o("IBertForTokenClassification"),YLo=o(" (I-BERT model)"),KLo=l(),Ev=a("li"),tse=a("strong"),ZLo=o("layoutlm"),e8o=o(" \u2014 "),yD=a("a"),o8o=o("LayoutLMForTokenClassification"),r8o=o(" (LayoutLM model)"),t8o=l(),yv=a("li"),ase=a("strong"),a8o=o("layoutlmv2"),s8o=o(" \u2014 "),wD=a("a"),n8o=o("LayoutLMv2ForTokenClassification"),l8o=o(" (LayoutLMv2 model)"),i8o=l(),wv=a("li"),sse=a("strong"),d8o=o("longformer"),c8o=o(" \u2014 "),AD=a("a"),m8o=o("LongformerForTokenClassification"),f8o=o(" (Longformer model)"),g8o=l(),Av=a("li"),nse=a("strong"),h8o=o("megatron-bert"),u8o=o(" \u2014 "),LD=a("a"),p8o=o("MegatronBertForTokenClassification"),_8o=o(" (MegatronBert model)"),b8o=l(),Lv=a("li"),lse=a("strong"),v8o=o("mobilebert"),T8o=o(" \u2014 "),BD=a("a"),F8o=o("MobileBertForTokenClassification"),C8o=o(" (MobileBERT model)"),M8o=l(),Bv=a("li"),ise=a("strong"),E8o=o("mpnet"),y8o=o(" \u2014 "),xD=a("a"),w8o=o("MPNetForTokenClassification"),A8o=o(" (MPNet model)"),L8o=l(),xv=a("li"),dse=a("strong"),B8o=o("nystromformer"),x8o=o(" \u2014 "),kD=a("a"),k8o=o("NystromformerForTokenClassification"),R8o=o(" (Nystromformer model)"),S8o=l(),kv=a("li"),cse=a("strong"),P8o=o("qdqbert"),$8o=o(" \u2014 "),RD=a("a"),I8o=o("QDQBertForTokenClassification"),D8o=o(" (QDQBert model)"),j8o=l(),Rv=a("li"),mse=a("strong"),N8o=o("rembert"),q8o=o(" \u2014 "),SD=a("a"),G8o=o("RemBertForTokenClassification"),O8o=o(" (RemBERT model)"),X8o=l(),Sv=a("li"),fse=a("strong"),V8o=o("roberta"),z8o=o(" \u2014 "),PD=a("a"),W8o=o("RobertaForTokenClassification"),Q8o=o(" (RoBERTa model)"),H8o=l(),Pv=a("li"),gse=a("strong"),U8o=o("roformer"),J8o=o(" \u2014 "),$D=a("a"),Y8o=o("RoFormerForTokenClassification"),K8o=o(" (RoFormer model)"),Z8o=l(),$v=a("li"),hse=a("strong"),e7o=o("squeezebert"),o7o=o(" \u2014 "),ID=a("a"),r7o=o("SqueezeBertForTokenClassification"),t7o=o(" (SqueezeBERT model)"),a7o=l(),Iv=a("li"),use=a("strong"),s7o=o("xlm"),n7o=o(" \u2014 "),DD=a("a"),l7o=o("XLMForTokenClassification"),i7o=o(" (XLM model)"),d7o=l(),Dv=a("li"),pse=a("strong"),c7o=o("xlm-roberta"),m7o=o(" \u2014 "),jD=a("a"),f7o=o("XLMRobertaForTokenClassification"),g7o=o(" (XLM-RoBERTa model)"),h7o=l(),jv=a("li"),_se=a("strong"),u7o=o("xlm-roberta-xl"),p7o=o(" \u2014 "),ND=a("a"),_7o=o("XLMRobertaXLForTokenClassification"),b7o=o(" (XLM-RoBERTa-XL model)"),v7o=l(),Nv=a("li"),bse=a("strong"),T7o=o("xlnet"),F7o=o(" \u2014 "),qD=a("a"),C7o=o("XLNetForTokenClassification"),M7o=o(" (XLNet model)"),E7o=l(),qv=a("li"),vse=a("strong"),y7o=o("yoso"),w7o=o(" \u2014 "),GD=a("a"),A7o=o("YosoForTokenClassification"),L7o=o(" (YOSO model)"),B7o=l(),Gv=a("p"),x7o=o("The model is set in evaluation mode by default using "),Tse=a("code"),k7o=o("model.eval()"),R7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fse=a("code"),S7o=o("model.train()"),P7o=l(),Cse=a("p"),$7o=o("Examples:"),I7o=l(),m(mw.$$.fragment),lBe=l(),pd=a("h2"),Ov=a("a"),Mse=a("span"),m(fw.$$.fragment),D7o=l(),Ese=a("span"),j7o=o("AutoModelForQuestionAnswering"),iBe=l(),rr=a("div"),m(gw.$$.fragment),N7o=l(),_d=a("p"),q7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),yse=a("code"),G7o=o("from_pretrained()"),O7o=o("class method or the "),wse=a("code"),X7o=o("from_config()"),V7o=o(`class
method.`),z7o=l(),hw=a("p"),W7o=o("This class cannot be instantiated directly using "),Ase=a("code"),Q7o=o("__init__()"),H7o=o(" (throws an error)."),U7o=l(),Ur=a("div"),m(uw.$$.fragment),J7o=l(),Lse=a("p"),Y7o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),K7o=l(),bd=a("p"),Z7o=o(`Note:
Loading a model from its configuration file does `),Bse=a("strong"),e9o=o("not"),o9o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xse=a("code"),r9o=o("from_pretrained()"),t9o=o("to load the model weights."),a9o=l(),kse=a("p"),s9o=o("Examples:"),n9o=l(),m(pw.$$.fragment),l9o=l(),Ge=a("div"),m(_w.$$.fragment),i9o=l(),Rse=a("p"),d9o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),c9o=l(),Ha=a("p"),m9o=o("The model class to instantiate is selected based on the "),Sse=a("code"),f9o=o("model_type"),g9o=o(` property of the config object (either
passed as an argument or loaded from `),Pse=a("code"),h9o=o("pretrained_model_name_or_path"),u9o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$se=a("code"),p9o=o("pretrained_model_name_or_path"),_9o=o(":"),b9o=l(),R=a("ul"),Xv=a("li"),Ise=a("strong"),v9o=o("albert"),T9o=o(" \u2014 "),OD=a("a"),F9o=o("AlbertForQuestionAnswering"),C9o=o(" (ALBERT model)"),M9o=l(),Vv=a("li"),Dse=a("strong"),E9o=o("bart"),y9o=o(" \u2014 "),XD=a("a"),w9o=o("BartForQuestionAnswering"),A9o=o(" (BART model)"),L9o=l(),zv=a("li"),jse=a("strong"),B9o=o("bert"),x9o=o(" \u2014 "),VD=a("a"),k9o=o("BertForQuestionAnswering"),R9o=o(" (BERT model)"),S9o=l(),Wv=a("li"),Nse=a("strong"),P9o=o("big_bird"),$9o=o(" \u2014 "),zD=a("a"),I9o=o("BigBirdForQuestionAnswering"),D9o=o(" (BigBird model)"),j9o=l(),Qv=a("li"),qse=a("strong"),N9o=o("bigbird_pegasus"),q9o=o(" \u2014 "),WD=a("a"),G9o=o("BigBirdPegasusForQuestionAnswering"),O9o=o(" (BigBirdPegasus model)"),X9o=l(),Hv=a("li"),Gse=a("strong"),V9o=o("camembert"),z9o=o(" \u2014 "),QD=a("a"),W9o=o("CamembertForQuestionAnswering"),Q9o=o(" (CamemBERT model)"),H9o=l(),Uv=a("li"),Ose=a("strong"),U9o=o("canine"),J9o=o(" \u2014 "),HD=a("a"),Y9o=o("CanineForQuestionAnswering"),K9o=o(" (Canine model)"),Z9o=l(),Jv=a("li"),Xse=a("strong"),eBo=o("convbert"),oBo=o(" \u2014 "),UD=a("a"),rBo=o("ConvBertForQuestionAnswering"),tBo=o(" (ConvBERT model)"),aBo=l(),Yv=a("li"),Vse=a("strong"),sBo=o("data2vec-text"),nBo=o(" \u2014 "),JD=a("a"),lBo=o("Data2VecTextForQuestionAnswering"),iBo=o(" (Data2VecText model)"),dBo=l(),Kv=a("li"),zse=a("strong"),cBo=o("deberta"),mBo=o(" \u2014 "),YD=a("a"),fBo=o("DebertaForQuestionAnswering"),gBo=o(" (DeBERTa model)"),hBo=l(),Zv=a("li"),Wse=a("strong"),uBo=o("deberta-v2"),pBo=o(" \u2014 "),KD=a("a"),_Bo=o("DebertaV2ForQuestionAnswering"),bBo=o(" (DeBERTa-v2 model)"),vBo=l(),eT=a("li"),Qse=a("strong"),TBo=o("distilbert"),FBo=o(" \u2014 "),ZD=a("a"),CBo=o("DistilBertForQuestionAnswering"),MBo=o(" (DistilBERT model)"),EBo=l(),oT=a("li"),Hse=a("strong"),yBo=o("electra"),wBo=o(" \u2014 "),ej=a("a"),ABo=o("ElectraForQuestionAnswering"),LBo=o(" (ELECTRA model)"),BBo=l(),rT=a("li"),Use=a("strong"),xBo=o("flaubert"),kBo=o(" \u2014 "),oj=a("a"),RBo=o("FlaubertForQuestionAnsweringSimple"),SBo=o(" (FlauBERT model)"),PBo=l(),tT=a("li"),Jse=a("strong"),$Bo=o("fnet"),IBo=o(" \u2014 "),rj=a("a"),DBo=o("FNetForQuestionAnswering"),jBo=o(" (FNet model)"),NBo=l(),aT=a("li"),Yse=a("strong"),qBo=o("funnel"),GBo=o(" \u2014 "),tj=a("a"),OBo=o("FunnelForQuestionAnswering"),XBo=o(" (Funnel Transformer model)"),VBo=l(),sT=a("li"),Kse=a("strong"),zBo=o("gptj"),WBo=o(" \u2014 "),aj=a("a"),QBo=o("GPTJForQuestionAnswering"),HBo=o(" (GPT-J model)"),UBo=l(),nT=a("li"),Zse=a("strong"),JBo=o("ibert"),YBo=o(" \u2014 "),sj=a("a"),KBo=o("IBertForQuestionAnswering"),ZBo=o(" (I-BERT model)"),exo=l(),lT=a("li"),ene=a("strong"),oxo=o("layoutlmv2"),rxo=o(" \u2014 "),nj=a("a"),txo=o("LayoutLMv2ForQuestionAnswering"),axo=o(" (LayoutLMv2 model)"),sxo=l(),iT=a("li"),one=a("strong"),nxo=o("led"),lxo=o(" \u2014 "),lj=a("a"),ixo=o("LEDForQuestionAnswering"),dxo=o(" (LED model)"),cxo=l(),dT=a("li"),rne=a("strong"),mxo=o("longformer"),fxo=o(" \u2014 "),ij=a("a"),gxo=o("LongformerForQuestionAnswering"),hxo=o(" (Longformer model)"),uxo=l(),cT=a("li"),tne=a("strong"),pxo=o("lxmert"),_xo=o(" \u2014 "),dj=a("a"),bxo=o("LxmertForQuestionAnswering"),vxo=o(" (LXMERT model)"),Txo=l(),mT=a("li"),ane=a("strong"),Fxo=o("mbart"),Cxo=o(" \u2014 "),cj=a("a"),Mxo=o("MBartForQuestionAnswering"),Exo=o(" (mBART model)"),yxo=l(),fT=a("li"),sne=a("strong"),wxo=o("megatron-bert"),Axo=o(" \u2014 "),mj=a("a"),Lxo=o("MegatronBertForQuestionAnswering"),Bxo=o(" (MegatronBert model)"),xxo=l(),gT=a("li"),nne=a("strong"),kxo=o("mobilebert"),Rxo=o(" \u2014 "),fj=a("a"),Sxo=o("MobileBertForQuestionAnswering"),Pxo=o(" (MobileBERT model)"),$xo=l(),hT=a("li"),lne=a("strong"),Ixo=o("mpnet"),Dxo=o(" \u2014 "),gj=a("a"),jxo=o("MPNetForQuestionAnswering"),Nxo=o(" (MPNet model)"),qxo=l(),uT=a("li"),ine=a("strong"),Gxo=o("nystromformer"),Oxo=o(" \u2014 "),hj=a("a"),Xxo=o("NystromformerForQuestionAnswering"),Vxo=o(" (Nystromformer model)"),zxo=l(),pT=a("li"),dne=a("strong"),Wxo=o("qdqbert"),Qxo=o(" \u2014 "),uj=a("a"),Hxo=o("QDQBertForQuestionAnswering"),Uxo=o(" (QDQBert model)"),Jxo=l(),_T=a("li"),cne=a("strong"),Yxo=o("reformer"),Kxo=o(" \u2014 "),pj=a("a"),Zxo=o("ReformerForQuestionAnswering"),eko=o(" (Reformer model)"),oko=l(),bT=a("li"),mne=a("strong"),rko=o("rembert"),tko=o(" \u2014 "),_j=a("a"),ako=o("RemBertForQuestionAnswering"),sko=o(" (RemBERT model)"),nko=l(),vT=a("li"),fne=a("strong"),lko=o("roberta"),iko=o(" \u2014 "),bj=a("a"),dko=o("RobertaForQuestionAnswering"),cko=o(" (RoBERTa model)"),mko=l(),TT=a("li"),gne=a("strong"),fko=o("roformer"),gko=o(" \u2014 "),vj=a("a"),hko=o("RoFormerForQuestionAnswering"),uko=o(" (RoFormer model)"),pko=l(),FT=a("li"),hne=a("strong"),_ko=o("splinter"),bko=o(" \u2014 "),Tj=a("a"),vko=o("SplinterForQuestionAnswering"),Tko=o(" (Splinter model)"),Fko=l(),CT=a("li"),une=a("strong"),Cko=o("squeezebert"),Mko=o(" \u2014 "),Fj=a("a"),Eko=o("SqueezeBertForQuestionAnswering"),yko=o(" (SqueezeBERT model)"),wko=l(),MT=a("li"),pne=a("strong"),Ako=o("xlm"),Lko=o(" \u2014 "),Cj=a("a"),Bko=o("XLMForQuestionAnsweringSimple"),xko=o(" (XLM model)"),kko=l(),ET=a("li"),_ne=a("strong"),Rko=o("xlm-roberta"),Sko=o(" \u2014 "),Mj=a("a"),Pko=o("XLMRobertaForQuestionAnswering"),$ko=o(" (XLM-RoBERTa model)"),Iko=l(),yT=a("li"),bne=a("strong"),Dko=o("xlm-roberta-xl"),jko=o(" \u2014 "),Ej=a("a"),Nko=o("XLMRobertaXLForQuestionAnswering"),qko=o(" (XLM-RoBERTa-XL model)"),Gko=l(),wT=a("li"),vne=a("strong"),Oko=o("xlnet"),Xko=o(" \u2014 "),yj=a("a"),Vko=o("XLNetForQuestionAnsweringSimple"),zko=o(" (XLNet model)"),Wko=l(),AT=a("li"),Tne=a("strong"),Qko=o("yoso"),Hko=o(" \u2014 "),wj=a("a"),Uko=o("YosoForQuestionAnswering"),Jko=o(" (YOSO model)"),Yko=l(),LT=a("p"),Kko=o("The model is set in evaluation mode by default using "),Fne=a("code"),Zko=o("model.eval()"),eRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Cne=a("code"),oRo=o("model.train()"),rRo=l(),Mne=a("p"),tRo=o("Examples:"),aRo=l(),m(bw.$$.fragment),dBe=l(),vd=a("h2"),BT=a("a"),Ene=a("span"),m(vw.$$.fragment),sRo=l(),yne=a("span"),nRo=o("AutoModelForTableQuestionAnswering"),cBe=l(),tr=a("div"),m(Tw.$$.fragment),lRo=l(),Td=a("p"),iRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),wne=a("code"),dRo=o("from_pretrained()"),cRo=o("class method or the "),Ane=a("code"),mRo=o("from_config()"),fRo=o(`class
method.`),gRo=l(),Fw=a("p"),hRo=o("This class cannot be instantiated directly using "),Lne=a("code"),uRo=o("__init__()"),pRo=o(" (throws an error)."),_Ro=l(),Jr=a("div"),m(Cw.$$.fragment),bRo=l(),Bne=a("p"),vRo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),TRo=l(),Fd=a("p"),FRo=o(`Note:
Loading a model from its configuration file does `),xne=a("strong"),CRo=o("not"),MRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kne=a("code"),ERo=o("from_pretrained()"),yRo=o("to load the model weights."),wRo=l(),Rne=a("p"),ARo=o("Examples:"),LRo=l(),m(Mw.$$.fragment),BRo=l(),Oe=a("div"),m(Ew.$$.fragment),xRo=l(),Sne=a("p"),kRo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),RRo=l(),Ua=a("p"),SRo=o("The model class to instantiate is selected based on the "),Pne=a("code"),PRo=o("model_type"),$Ro=o(` property of the config object (either
passed as an argument or loaded from `),$ne=a("code"),IRo=o("pretrained_model_name_or_path"),DRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ine=a("code"),jRo=o("pretrained_model_name_or_path"),NRo=o(":"),qRo=l(),Dne=a("ul"),xT=a("li"),jne=a("strong"),GRo=o("tapas"),ORo=o(" \u2014 "),Aj=a("a"),XRo=o("TapasForQuestionAnswering"),VRo=o(" (TAPAS model)"),zRo=l(),kT=a("p"),WRo=o("The model is set in evaluation mode by default using "),Nne=a("code"),QRo=o("model.eval()"),HRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),qne=a("code"),URo=o("model.train()"),JRo=l(),Gne=a("p"),YRo=o("Examples:"),KRo=l(),m(yw.$$.fragment),mBe=l(),Cd=a("h2"),RT=a("a"),One=a("span"),m(ww.$$.fragment),ZRo=l(),Xne=a("span"),eSo=o("AutoModelForImageClassification"),fBe=l(),ar=a("div"),m(Aw.$$.fragment),oSo=l(),Md=a("p"),rSo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Vne=a("code"),tSo=o("from_pretrained()"),aSo=o("class method or the "),zne=a("code"),sSo=o("from_config()"),nSo=o(`class
method.`),lSo=l(),Lw=a("p"),iSo=o("This class cannot be instantiated directly using "),Wne=a("code"),dSo=o("__init__()"),cSo=o(" (throws an error)."),mSo=l(),Yr=a("div"),m(Bw.$$.fragment),fSo=l(),Qne=a("p"),gSo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),hSo=l(),Ed=a("p"),uSo=o(`Note:
Loading a model from its configuration file does `),Hne=a("strong"),pSo=o("not"),_So=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Une=a("code"),bSo=o("from_pretrained()"),vSo=o("to load the model weights."),TSo=l(),Jne=a("p"),FSo=o("Examples:"),CSo=l(),m(xw.$$.fragment),MSo=l(),Xe=a("div"),m(kw.$$.fragment),ESo=l(),Yne=a("p"),ySo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),wSo=l(),Ja=a("p"),ASo=o("The model class to instantiate is selected based on the "),Kne=a("code"),LSo=o("model_type"),BSo=o(` property of the config object (either
passed as an argument or loaded from `),Zne=a("code"),xSo=o("pretrained_model_name_or_path"),kSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ele=a("code"),RSo=o("pretrained_model_name_or_path"),SSo=o(":"),PSo=l(),be=a("ul"),ST=a("li"),ole=a("strong"),$So=o("beit"),ISo=o(" \u2014 "),Lj=a("a"),DSo=o("BeitForImageClassification"),jSo=o(" (BEiT model)"),NSo=l(),PT=a("li"),rle=a("strong"),qSo=o("convnext"),GSo=o(" \u2014 "),Bj=a("a"),OSo=o("ConvNextForImageClassification"),XSo=o(" (ConvNext model)"),VSo=l(),$n=a("li"),tle=a("strong"),zSo=o("deit"),WSo=o(" \u2014 "),xj=a("a"),QSo=o("DeiTForImageClassification"),HSo=o(" or "),kj=a("a"),USo=o("DeiTForImageClassificationWithTeacher"),JSo=o(" (DeiT model)"),YSo=l(),$T=a("li"),ale=a("strong"),KSo=o("imagegpt"),ZSo=o(" \u2014 "),Rj=a("a"),ePo=o("ImageGPTForImageClassification"),oPo=o(" (ImageGPT model)"),rPo=l(),la=a("li"),sle=a("strong"),tPo=o("perceiver"),aPo=o(" \u2014 "),Sj=a("a"),sPo=o("PerceiverForImageClassificationLearned"),nPo=o(" or "),Pj=a("a"),lPo=o("PerceiverForImageClassificationFourier"),iPo=o(" or "),$j=a("a"),dPo=o("PerceiverForImageClassificationConvProcessing"),cPo=o(" (Perceiver model)"),mPo=l(),IT=a("li"),nle=a("strong"),fPo=o("poolformer"),gPo=o(" \u2014 "),Ij=a("a"),hPo=o("PoolFormerForImageClassification"),uPo=o(" (PoolFormer model)"),pPo=l(),DT=a("li"),lle=a("strong"),_Po=o("segformer"),bPo=o(" \u2014 "),Dj=a("a"),vPo=o("SegformerForImageClassification"),TPo=o(" (SegFormer model)"),FPo=l(),jT=a("li"),ile=a("strong"),CPo=o("swin"),MPo=o(" \u2014 "),jj=a("a"),EPo=o("SwinForImageClassification"),yPo=o(" (Swin model)"),wPo=l(),NT=a("li"),dle=a("strong"),APo=o("vit"),LPo=o(" \u2014 "),Nj=a("a"),BPo=o("ViTForImageClassification"),xPo=o(" (ViT model)"),kPo=l(),qT=a("p"),RPo=o("The model is set in evaluation mode by default using "),cle=a("code"),SPo=o("model.eval()"),PPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mle=a("code"),$Po=o("model.train()"),IPo=l(),fle=a("p"),DPo=o("Examples:"),jPo=l(),m(Rw.$$.fragment),gBe=l(),yd=a("h2"),GT=a("a"),gle=a("span"),m(Sw.$$.fragment),NPo=l(),hle=a("span"),qPo=o("AutoModelForVision2Seq"),hBe=l(),sr=a("div"),m(Pw.$$.fragment),GPo=l(),wd=a("p"),OPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ule=a("code"),XPo=o("from_pretrained()"),VPo=o("class method or the "),ple=a("code"),zPo=o("from_config()"),WPo=o(`class
method.`),QPo=l(),$w=a("p"),HPo=o("This class cannot be instantiated directly using "),_le=a("code"),UPo=o("__init__()"),JPo=o(" (throws an error)."),YPo=l(),Kr=a("div"),m(Iw.$$.fragment),KPo=l(),ble=a("p"),ZPo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),e$o=l(),Ad=a("p"),o$o=o(`Note:
Loading a model from its configuration file does `),vle=a("strong"),r$o=o("not"),t$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Tle=a("code"),a$o=o("from_pretrained()"),s$o=o("to load the model weights."),n$o=l(),Fle=a("p"),l$o=o("Examples:"),i$o=l(),m(Dw.$$.fragment),d$o=l(),Ve=a("div"),m(jw.$$.fragment),c$o=l(),Cle=a("p"),m$o=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),f$o=l(),Ya=a("p"),g$o=o("The model class to instantiate is selected based on the "),Mle=a("code"),h$o=o("model_type"),u$o=o(` property of the config object (either
passed as an argument or loaded from `),Ele=a("code"),p$o=o("pretrained_model_name_or_path"),_$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yle=a("code"),b$o=o("pretrained_model_name_or_path"),v$o=o(":"),T$o=l(),wle=a("ul"),OT=a("li"),Ale=a("strong"),F$o=o("vision-encoder-decoder"),C$o=o(" \u2014 "),qj=a("a"),M$o=o("VisionEncoderDecoderModel"),E$o=o(" (Vision Encoder decoder model)"),y$o=l(),XT=a("p"),w$o=o("The model is set in evaluation mode by default using "),Lle=a("code"),A$o=o("model.eval()"),L$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ble=a("code"),B$o=o("model.train()"),x$o=l(),xle=a("p"),k$o=o("Examples:"),R$o=l(),m(Nw.$$.fragment),uBe=l(),Ld=a("h2"),VT=a("a"),kle=a("span"),m(qw.$$.fragment),S$o=l(),Rle=a("span"),P$o=o("AutoModelForAudioClassification"),pBe=l(),nr=a("div"),m(Gw.$$.fragment),$$o=l(),Bd=a("p"),I$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Sle=a("code"),D$o=o("from_pretrained()"),j$o=o("class method or the "),Ple=a("code"),N$o=o("from_config()"),q$o=o(`class
method.`),G$o=l(),Ow=a("p"),O$o=o("This class cannot be instantiated directly using "),$le=a("code"),X$o=o("__init__()"),V$o=o(" (throws an error)."),z$o=l(),Zr=a("div"),m(Xw.$$.fragment),W$o=l(),Ile=a("p"),Q$o=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),H$o=l(),xd=a("p"),U$o=o(`Note:
Loading a model from its configuration file does `),Dle=a("strong"),J$o=o("not"),Y$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jle=a("code"),K$o=o("from_pretrained()"),Z$o=o("to load the model weights."),eIo=l(),Nle=a("p"),oIo=o("Examples:"),rIo=l(),m(Vw.$$.fragment),tIo=l(),ze=a("div"),m(zw.$$.fragment),aIo=l(),qle=a("p"),sIo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),nIo=l(),Ka=a("p"),lIo=o("The model class to instantiate is selected based on the "),Gle=a("code"),iIo=o("model_type"),dIo=o(` property of the config object (either
passed as an argument or loaded from `),Ole=a("code"),cIo=o("pretrained_model_name_or_path"),mIo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xle=a("code"),fIo=o("pretrained_model_name_or_path"),gIo=o(":"),hIo=l(),Ae=a("ul"),zT=a("li"),Vle=a("strong"),uIo=o("data2vec-audio"),pIo=o(" \u2014 "),Gj=a("a"),_Io=o("Data2VecAudioForSequenceClassification"),bIo=o(" (Data2VecAudio model)"),vIo=l(),WT=a("li"),zle=a("strong"),TIo=o("hubert"),FIo=o(" \u2014 "),Oj=a("a"),CIo=o("HubertForSequenceClassification"),MIo=o(" (Hubert model)"),EIo=l(),QT=a("li"),Wle=a("strong"),yIo=o("sew"),wIo=o(" \u2014 "),Xj=a("a"),AIo=o("SEWForSequenceClassification"),LIo=o(" (SEW model)"),BIo=l(),HT=a("li"),Qle=a("strong"),xIo=o("sew-d"),kIo=o(" \u2014 "),Vj=a("a"),RIo=o("SEWDForSequenceClassification"),SIo=o(" (SEW-D model)"),PIo=l(),UT=a("li"),Hle=a("strong"),$Io=o("unispeech"),IIo=o(" \u2014 "),zj=a("a"),DIo=o("UniSpeechForSequenceClassification"),jIo=o(" (UniSpeech model)"),NIo=l(),JT=a("li"),Ule=a("strong"),qIo=o("unispeech-sat"),GIo=o(" \u2014 "),Wj=a("a"),OIo=o("UniSpeechSatForSequenceClassification"),XIo=o(" (UniSpeechSat model)"),VIo=l(),YT=a("li"),Jle=a("strong"),zIo=o("wav2vec2"),WIo=o(" \u2014 "),Qj=a("a"),QIo=o("Wav2Vec2ForSequenceClassification"),HIo=o(" (Wav2Vec2 model)"),UIo=l(),KT=a("li"),Yle=a("strong"),JIo=o("wavlm"),YIo=o(" \u2014 "),Hj=a("a"),KIo=o("WavLMForSequenceClassification"),ZIo=o(" (WavLM model)"),eDo=l(),ZT=a("p"),oDo=o("The model is set in evaluation mode by default using "),Kle=a("code"),rDo=o("model.eval()"),tDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zle=a("code"),aDo=o("model.train()"),sDo=l(),eie=a("p"),nDo=o("Examples:"),lDo=l(),m(Ww.$$.fragment),_Be=l(),kd=a("h2"),e1=a("a"),oie=a("span"),m(Qw.$$.fragment),iDo=l(),rie=a("span"),dDo=o("AutoModelForAudioFrameClassification"),bBe=l(),lr=a("div"),m(Hw.$$.fragment),cDo=l(),Rd=a("p"),mDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),tie=a("code"),fDo=o("from_pretrained()"),gDo=o("class method or the "),aie=a("code"),hDo=o("from_config()"),uDo=o(`class
method.`),pDo=l(),Uw=a("p"),_Do=o("This class cannot be instantiated directly using "),sie=a("code"),bDo=o("__init__()"),vDo=o(" (throws an error)."),TDo=l(),et=a("div"),m(Jw.$$.fragment),FDo=l(),nie=a("p"),CDo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),MDo=l(),Sd=a("p"),EDo=o(`Note:
Loading a model from its configuration file does `),lie=a("strong"),yDo=o("not"),wDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),iie=a("code"),ADo=o("from_pretrained()"),LDo=o("to load the model weights."),BDo=l(),die=a("p"),xDo=o("Examples:"),kDo=l(),m(Yw.$$.fragment),RDo=l(),We=a("div"),m(Kw.$$.fragment),SDo=l(),cie=a("p"),PDo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),$Do=l(),Za=a("p"),IDo=o("The model class to instantiate is selected based on the "),mie=a("code"),DDo=o("model_type"),jDo=o(` property of the config object (either
passed as an argument or loaded from `),fie=a("code"),NDo=o("pretrained_model_name_or_path"),qDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gie=a("code"),GDo=o("pretrained_model_name_or_path"),ODo=o(":"),XDo=l(),es=a("ul"),o1=a("li"),hie=a("strong"),VDo=o("data2vec-audio"),zDo=o(" \u2014 "),Uj=a("a"),WDo=o("Data2VecAudioForAudioFrameClassification"),QDo=o(" (Data2VecAudio model)"),HDo=l(),r1=a("li"),uie=a("strong"),UDo=o("unispeech-sat"),JDo=o(" \u2014 "),Jj=a("a"),YDo=o("UniSpeechSatForAudioFrameClassification"),KDo=o(" (UniSpeechSat model)"),ZDo=l(),t1=a("li"),pie=a("strong"),ejo=o("wav2vec2"),ojo=o(" \u2014 "),Yj=a("a"),rjo=o("Wav2Vec2ForAudioFrameClassification"),tjo=o(" (Wav2Vec2 model)"),ajo=l(),a1=a("li"),_ie=a("strong"),sjo=o("wavlm"),njo=o(" \u2014 "),Kj=a("a"),ljo=o("WavLMForAudioFrameClassification"),ijo=o(" (WavLM model)"),djo=l(),s1=a("p"),cjo=o("The model is set in evaluation mode by default using "),bie=a("code"),mjo=o("model.eval()"),fjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vie=a("code"),gjo=o("model.train()"),hjo=l(),Tie=a("p"),ujo=o("Examples:"),pjo=l(),m(Zw.$$.fragment),vBe=l(),Pd=a("h2"),n1=a("a"),Fie=a("span"),m(e6.$$.fragment),_jo=l(),Cie=a("span"),bjo=o("AutoModelForCTC"),TBe=l(),ir=a("div"),m(o6.$$.fragment),vjo=l(),$d=a("p"),Tjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Mie=a("code"),Fjo=o("from_pretrained()"),Cjo=o("class method or the "),Eie=a("code"),Mjo=o("from_config()"),Ejo=o(`class
method.`),yjo=l(),r6=a("p"),wjo=o("This class cannot be instantiated directly using "),yie=a("code"),Ajo=o("__init__()"),Ljo=o(" (throws an error)."),Bjo=l(),ot=a("div"),m(t6.$$.fragment),xjo=l(),wie=a("p"),kjo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),Rjo=l(),Id=a("p"),Sjo=o(`Note:
Loading a model from its configuration file does `),Aie=a("strong"),Pjo=o("not"),$jo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lie=a("code"),Ijo=o("from_pretrained()"),Djo=o("to load the model weights."),jjo=l(),Bie=a("p"),Njo=o("Examples:"),qjo=l(),m(a6.$$.fragment),Gjo=l(),Qe=a("div"),m(s6.$$.fragment),Ojo=l(),xie=a("p"),Xjo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),Vjo=l(),os=a("p"),zjo=o("The model class to instantiate is selected based on the "),kie=a("code"),Wjo=o("model_type"),Qjo=o(` property of the config object (either
passed as an argument or loaded from `),Rie=a("code"),Hjo=o("pretrained_model_name_or_path"),Ujo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sie=a("code"),Jjo=o("pretrained_model_name_or_path"),Yjo=o(":"),Kjo=l(),Le=a("ul"),l1=a("li"),Pie=a("strong"),Zjo=o("data2vec-audio"),eNo=o(" \u2014 "),Zj=a("a"),oNo=o("Data2VecAudioForCTC"),rNo=o(" (Data2VecAudio model)"),tNo=l(),i1=a("li"),$ie=a("strong"),aNo=o("hubert"),sNo=o(" \u2014 "),eN=a("a"),nNo=o("HubertForCTC"),lNo=o(" (Hubert model)"),iNo=l(),d1=a("li"),Iie=a("strong"),dNo=o("sew"),cNo=o(" \u2014 "),oN=a("a"),mNo=o("SEWForCTC"),fNo=o(" (SEW model)"),gNo=l(),c1=a("li"),Die=a("strong"),hNo=o("sew-d"),uNo=o(" \u2014 "),rN=a("a"),pNo=o("SEWDForCTC"),_No=o(" (SEW-D model)"),bNo=l(),m1=a("li"),jie=a("strong"),vNo=o("unispeech"),TNo=o(" \u2014 "),tN=a("a"),FNo=o("UniSpeechForCTC"),CNo=o(" (UniSpeech model)"),MNo=l(),f1=a("li"),Nie=a("strong"),ENo=o("unispeech-sat"),yNo=o(" \u2014 "),aN=a("a"),wNo=o("UniSpeechSatForCTC"),ANo=o(" (UniSpeechSat model)"),LNo=l(),g1=a("li"),qie=a("strong"),BNo=o("wav2vec2"),xNo=o(" \u2014 "),sN=a("a"),kNo=o("Wav2Vec2ForCTC"),RNo=o(" (Wav2Vec2 model)"),SNo=l(),h1=a("li"),Gie=a("strong"),PNo=o("wavlm"),$No=o(" \u2014 "),nN=a("a"),INo=o("WavLMForCTC"),DNo=o(" (WavLM model)"),jNo=l(),u1=a("p"),NNo=o("The model is set in evaluation mode by default using "),Oie=a("code"),qNo=o("model.eval()"),GNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xie=a("code"),ONo=o("model.train()"),XNo=l(),Vie=a("p"),VNo=o("Examples:"),zNo=l(),m(n6.$$.fragment),FBe=l(),Dd=a("h2"),p1=a("a"),zie=a("span"),m(l6.$$.fragment),WNo=l(),Wie=a("span"),QNo=o("AutoModelForSpeechSeq2Seq"),CBe=l(),dr=a("div"),m(i6.$$.fragment),HNo=l(),jd=a("p"),UNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Qie=a("code"),JNo=o("from_pretrained()"),YNo=o("class method or the "),Hie=a("code"),KNo=o("from_config()"),ZNo=o(`class
method.`),eqo=l(),d6=a("p"),oqo=o("This class cannot be instantiated directly using "),Uie=a("code"),rqo=o("__init__()"),tqo=o(" (throws an error)."),aqo=l(),rt=a("div"),m(c6.$$.fragment),sqo=l(),Jie=a("p"),nqo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),lqo=l(),Nd=a("p"),iqo=o(`Note:
Loading a model from its configuration file does `),Yie=a("strong"),dqo=o("not"),cqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kie=a("code"),mqo=o("from_pretrained()"),fqo=o("to load the model weights."),gqo=l(),Zie=a("p"),hqo=o("Examples:"),uqo=l(),m(m6.$$.fragment),pqo=l(),He=a("div"),m(f6.$$.fragment),_qo=l(),ede=a("p"),bqo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),vqo=l(),rs=a("p"),Tqo=o("The model class to instantiate is selected based on the "),ode=a("code"),Fqo=o("model_type"),Cqo=o(` property of the config object (either
passed as an argument or loaded from `),rde=a("code"),Mqo=o("pretrained_model_name_or_path"),Eqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tde=a("code"),yqo=o("pretrained_model_name_or_path"),wqo=o(":"),Aqo=l(),g6=a("ul"),_1=a("li"),ade=a("strong"),Lqo=o("speech-encoder-decoder"),Bqo=o(" \u2014 "),lN=a("a"),xqo=o("SpeechEncoderDecoderModel"),kqo=o(" (Speech Encoder decoder model)"),Rqo=l(),b1=a("li"),sde=a("strong"),Sqo=o("speech_to_text"),Pqo=o(" \u2014 "),iN=a("a"),$qo=o("Speech2TextForConditionalGeneration"),Iqo=o(" (Speech2Text model)"),Dqo=l(),v1=a("p"),jqo=o("The model is set in evaluation mode by default using "),nde=a("code"),Nqo=o("model.eval()"),qqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lde=a("code"),Gqo=o("model.train()"),Oqo=l(),ide=a("p"),Xqo=o("Examples:"),Vqo=l(),m(h6.$$.fragment),MBe=l(),qd=a("h2"),T1=a("a"),dde=a("span"),m(u6.$$.fragment),zqo=l(),cde=a("span"),Wqo=o("AutoModelForAudioXVector"),EBe=l(),cr=a("div"),m(p6.$$.fragment),Qqo=l(),Gd=a("p"),Hqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),mde=a("code"),Uqo=o("from_pretrained()"),Jqo=o("class method or the "),fde=a("code"),Yqo=o("from_config()"),Kqo=o(`class
method.`),Zqo=l(),_6=a("p"),eGo=o("This class cannot be instantiated directly using "),gde=a("code"),oGo=o("__init__()"),rGo=o(" (throws an error)."),tGo=l(),tt=a("div"),m(b6.$$.fragment),aGo=l(),hde=a("p"),sGo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),nGo=l(),Od=a("p"),lGo=o(`Note:
Loading a model from its configuration file does `),ude=a("strong"),iGo=o("not"),dGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),pde=a("code"),cGo=o("from_pretrained()"),mGo=o("to load the model weights."),fGo=l(),_de=a("p"),gGo=o("Examples:"),hGo=l(),m(v6.$$.fragment),uGo=l(),Ue=a("div"),m(T6.$$.fragment),pGo=l(),bde=a("p"),_Go=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),bGo=l(),ts=a("p"),vGo=o("The model class to instantiate is selected based on the "),vde=a("code"),TGo=o("model_type"),FGo=o(` property of the config object (either
passed as an argument or loaded from `),Tde=a("code"),CGo=o("pretrained_model_name_or_path"),MGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fde=a("code"),EGo=o("pretrained_model_name_or_path"),yGo=o(":"),wGo=l(),as=a("ul"),F1=a("li"),Cde=a("strong"),AGo=o("data2vec-audio"),LGo=o(" \u2014 "),dN=a("a"),BGo=o("Data2VecAudioForXVector"),xGo=o(" (Data2VecAudio model)"),kGo=l(),C1=a("li"),Mde=a("strong"),RGo=o("unispeech-sat"),SGo=o(" \u2014 "),cN=a("a"),PGo=o("UniSpeechSatForXVector"),$Go=o(" (UniSpeechSat model)"),IGo=l(),M1=a("li"),Ede=a("strong"),DGo=o("wav2vec2"),jGo=o(" \u2014 "),mN=a("a"),NGo=o("Wav2Vec2ForXVector"),qGo=o(" (Wav2Vec2 model)"),GGo=l(),E1=a("li"),yde=a("strong"),OGo=o("wavlm"),XGo=o(" \u2014 "),fN=a("a"),VGo=o("WavLMForXVector"),zGo=o(" (WavLM model)"),WGo=l(),y1=a("p"),QGo=o("The model is set in evaluation mode by default using "),wde=a("code"),HGo=o("model.eval()"),UGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ade=a("code"),JGo=o("model.train()"),YGo=l(),Lde=a("p"),KGo=o("Examples:"),ZGo=l(),m(F6.$$.fragment),yBe=l(),Xd=a("h2"),w1=a("a"),Bde=a("span"),m(C6.$$.fragment),eOo=l(),xde=a("span"),oOo=o("AutoModelForMaskedImageModeling"),wBe=l(),mr=a("div"),m(M6.$$.fragment),rOo=l(),Vd=a("p"),tOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),kde=a("code"),aOo=o("from_pretrained()"),sOo=o("class method or the "),Rde=a("code"),nOo=o("from_config()"),lOo=o(`class
method.`),iOo=l(),E6=a("p"),dOo=o("This class cannot be instantiated directly using "),Sde=a("code"),cOo=o("__init__()"),mOo=o(" (throws an error)."),fOo=l(),at=a("div"),m(y6.$$.fragment),gOo=l(),Pde=a("p"),hOo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),uOo=l(),zd=a("p"),pOo=o(`Note:
Loading a model from its configuration file does `),$de=a("strong"),_Oo=o("not"),bOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ide=a("code"),vOo=o("from_pretrained()"),TOo=o("to load the model weights."),FOo=l(),Dde=a("p"),COo=o("Examples:"),MOo=l(),m(w6.$$.fragment),EOo=l(),Je=a("div"),m(A6.$$.fragment),yOo=l(),jde=a("p"),wOo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),AOo=l(),ss=a("p"),LOo=o("The model class to instantiate is selected based on the "),Nde=a("code"),BOo=o("model_type"),xOo=o(` property of the config object (either
passed as an argument or loaded from `),qde=a("code"),kOo=o("pretrained_model_name_or_path"),ROo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gde=a("code"),SOo=o("pretrained_model_name_or_path"),POo=o(":"),$Oo=l(),Wd=a("ul"),A1=a("li"),Ode=a("strong"),IOo=o("deit"),DOo=o(" \u2014 "),gN=a("a"),jOo=o("DeiTForMaskedImageModeling"),NOo=o(" (DeiT model)"),qOo=l(),L1=a("li"),Xde=a("strong"),GOo=o("swin"),OOo=o(" \u2014 "),hN=a("a"),XOo=o("SwinForMaskedImageModeling"),VOo=o(" (Swin model)"),zOo=l(),B1=a("li"),Vde=a("strong"),WOo=o("vit"),QOo=o(" \u2014 "),uN=a("a"),HOo=o("ViTForMaskedImageModeling"),UOo=o(" (ViT model)"),JOo=l(),x1=a("p"),YOo=o("The model is set in evaluation mode by default using "),zde=a("code"),KOo=o("model.eval()"),ZOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wde=a("code"),eXo=o("model.train()"),oXo=l(),Qde=a("p"),rXo=o("Examples:"),tXo=l(),m(L6.$$.fragment),ABe=l(),Qd=a("h2"),k1=a("a"),Hde=a("span"),m(B6.$$.fragment),aXo=l(),Ude=a("span"),sXo=o("AutoModelForObjectDetection"),LBe=l(),fr=a("div"),m(x6.$$.fragment),nXo=l(),Hd=a("p"),lXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Jde=a("code"),iXo=o("from_pretrained()"),dXo=o("class method or the "),Yde=a("code"),cXo=o("from_config()"),mXo=o(`class
method.`),fXo=l(),k6=a("p"),gXo=o("This class cannot be instantiated directly using "),Kde=a("code"),hXo=o("__init__()"),uXo=o(" (throws an error)."),pXo=l(),st=a("div"),m(R6.$$.fragment),_Xo=l(),Zde=a("p"),bXo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),vXo=l(),Ud=a("p"),TXo=o(`Note:
Loading a model from its configuration file does `),ece=a("strong"),FXo=o("not"),CXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),oce=a("code"),MXo=o("from_pretrained()"),EXo=o("to load the model weights."),yXo=l(),rce=a("p"),wXo=o("Examples:"),AXo=l(),m(S6.$$.fragment),LXo=l(),Ye=a("div"),m(P6.$$.fragment),BXo=l(),tce=a("p"),xXo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),kXo=l(),ns=a("p"),RXo=o("The model class to instantiate is selected based on the "),ace=a("code"),SXo=o("model_type"),PXo=o(` property of the config object (either
passed as an argument or loaded from `),sce=a("code"),$Xo=o("pretrained_model_name_or_path"),IXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nce=a("code"),DXo=o("pretrained_model_name_or_path"),jXo=o(":"),NXo=l(),lce=a("ul"),R1=a("li"),ice=a("strong"),qXo=o("detr"),GXo=o(" \u2014 "),pN=a("a"),OXo=o("DetrForObjectDetection"),XXo=o(" (DETR model)"),VXo=l(),S1=a("p"),zXo=o("The model is set in evaluation mode by default using "),dce=a("code"),WXo=o("model.eval()"),QXo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cce=a("code"),HXo=o("model.train()"),UXo=l(),mce=a("p"),JXo=o("Examples:"),YXo=l(),m($6.$$.fragment),BBe=l(),Jd=a("h2"),P1=a("a"),fce=a("span"),m(I6.$$.fragment),KXo=l(),gce=a("span"),ZXo=o("AutoModelForImageSegmentation"),xBe=l(),gr=a("div"),m(D6.$$.fragment),eVo=l(),Yd=a("p"),oVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),hce=a("code"),rVo=o("from_pretrained()"),tVo=o("class method or the "),uce=a("code"),aVo=o("from_config()"),sVo=o(`class
method.`),nVo=l(),j6=a("p"),lVo=o("This class cannot be instantiated directly using "),pce=a("code"),iVo=o("__init__()"),dVo=o(" (throws an error)."),cVo=l(),nt=a("div"),m(N6.$$.fragment),mVo=l(),_ce=a("p"),fVo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),gVo=l(),Kd=a("p"),hVo=o(`Note:
Loading a model from its configuration file does `),bce=a("strong"),uVo=o("not"),pVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vce=a("code"),_Vo=o("from_pretrained()"),bVo=o("to load the model weights."),vVo=l(),Tce=a("p"),TVo=o("Examples:"),FVo=l(),m(q6.$$.fragment),CVo=l(),Ke=a("div"),m(G6.$$.fragment),MVo=l(),Fce=a("p"),EVo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),yVo=l(),ls=a("p"),wVo=o("The model class to instantiate is selected based on the "),Cce=a("code"),AVo=o("model_type"),LVo=o(` property of the config object (either
passed as an argument or loaded from `),Mce=a("code"),BVo=o("pretrained_model_name_or_path"),xVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ece=a("code"),kVo=o("pretrained_model_name_or_path"),RVo=o(":"),SVo=l(),yce=a("ul"),$1=a("li"),wce=a("strong"),PVo=o("detr"),$Vo=o(" \u2014 "),_N=a("a"),IVo=o("DetrForSegmentation"),DVo=o(" (DETR model)"),jVo=l(),I1=a("p"),NVo=o("The model is set in evaluation mode by default using "),Ace=a("code"),qVo=o("model.eval()"),GVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Lce=a("code"),OVo=o("model.train()"),XVo=l(),Bce=a("p"),VVo=o("Examples:"),zVo=l(),m(O6.$$.fragment),kBe=l(),Zd=a("h2"),D1=a("a"),xce=a("span"),m(X6.$$.fragment),WVo=l(),kce=a("span"),QVo=o("AutoModelForSemanticSegmentation"),RBe=l(),hr=a("div"),m(V6.$$.fragment),HVo=l(),ec=a("p"),UVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Rce=a("code"),JVo=o("from_pretrained()"),YVo=o("class method or the "),Sce=a("code"),KVo=o("from_config()"),ZVo=o(`class
method.`),ezo=l(),z6=a("p"),ozo=o("This class cannot be instantiated directly using "),Pce=a("code"),rzo=o("__init__()"),tzo=o(" (throws an error)."),azo=l(),lt=a("div"),m(W6.$$.fragment),szo=l(),$ce=a("p"),nzo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),lzo=l(),oc=a("p"),izo=o(`Note:
Loading a model from its configuration file does `),Ice=a("strong"),dzo=o("not"),czo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dce=a("code"),mzo=o("from_pretrained()"),fzo=o("to load the model weights."),gzo=l(),jce=a("p"),hzo=o("Examples:"),uzo=l(),m(Q6.$$.fragment),pzo=l(),Ze=a("div"),m(H6.$$.fragment),_zo=l(),Nce=a("p"),bzo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),vzo=l(),is=a("p"),Tzo=o("The model class to instantiate is selected based on the "),qce=a("code"),Fzo=o("model_type"),Czo=o(` property of the config object (either
passed as an argument or loaded from `),Gce=a("code"),Mzo=o("pretrained_model_name_or_path"),Ezo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Oce=a("code"),yzo=o("pretrained_model_name_or_path"),wzo=o(":"),Azo=l(),U6=a("ul"),j1=a("li"),Xce=a("strong"),Lzo=o("beit"),Bzo=o(" \u2014 "),bN=a("a"),xzo=o("BeitForSemanticSegmentation"),kzo=o(" (BEiT model)"),Rzo=l(),N1=a("li"),Vce=a("strong"),Szo=o("segformer"),Pzo=o(" \u2014 "),vN=a("a"),$zo=o("SegformerForSemanticSegmentation"),Izo=o(" (SegFormer model)"),Dzo=l(),q1=a("p"),jzo=o("The model is set in evaluation mode by default using "),zce=a("code"),Nzo=o("model.eval()"),qzo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wce=a("code"),Gzo=o("model.train()"),Ozo=l(),Qce=a("p"),Xzo=o("Examples:"),Vzo=l(),m(J6.$$.fragment),SBe=l(),rc=a("h2"),G1=a("a"),Hce=a("span"),m(Y6.$$.fragment),zzo=l(),Uce=a("span"),Wzo=o("TFAutoModel"),PBe=l(),ur=a("div"),m(K6.$$.fragment),Qzo=l(),tc=a("p"),Hzo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Jce=a("code"),Uzo=o("from_pretrained()"),Jzo=o("class method or the "),Yce=a("code"),Yzo=o("from_config()"),Kzo=o(`class
method.`),Zzo=l(),Z6=a("p"),eWo=o("This class cannot be instantiated directly using "),Kce=a("code"),oWo=o("__init__()"),rWo=o(" (throws an error)."),tWo=l(),it=a("div"),m(eA.$$.fragment),aWo=l(),Zce=a("p"),sWo=o("Instantiates one of the base model classes of the library from a configuration."),nWo=l(),ac=a("p"),lWo=o(`Note:
Loading a model from its configuration file does `),eme=a("strong"),iWo=o("not"),dWo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=a("code"),cWo=o("from_pretrained()"),mWo=o("to load the model weights."),fWo=l(),rme=a("p"),gWo=o("Examples:"),hWo=l(),m(oA.$$.fragment),uWo=l(),go=a("div"),m(rA.$$.fragment),pWo=l(),tme=a("p"),_Wo=o("Instantiate one of the base model classes of the library from a pretrained model."),bWo=l(),ds=a("p"),vWo=o("The model class to instantiate is selected based on the "),ame=a("code"),TWo=o("model_type"),FWo=o(` property of the config object (either
passed as an argument or loaded from `),sme=a("code"),CWo=o("pretrained_model_name_or_path"),MWo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nme=a("code"),EWo=o("pretrained_model_name_or_path"),yWo=o(":"),wWo=l(),B=a("ul"),O1=a("li"),lme=a("strong"),AWo=o("albert"),LWo=o(" \u2014 "),TN=a("a"),BWo=o("TFAlbertModel"),xWo=o(" (ALBERT model)"),kWo=l(),X1=a("li"),ime=a("strong"),RWo=o("bart"),SWo=o(" \u2014 "),FN=a("a"),PWo=o("TFBartModel"),$Wo=o(" (BART model)"),IWo=l(),V1=a("li"),dme=a("strong"),DWo=o("bert"),jWo=o(" \u2014 "),CN=a("a"),NWo=o("TFBertModel"),qWo=o(" (BERT model)"),GWo=l(),z1=a("li"),cme=a("strong"),OWo=o("blenderbot"),XWo=o(" \u2014 "),MN=a("a"),VWo=o("TFBlenderbotModel"),zWo=o(" (Blenderbot model)"),WWo=l(),W1=a("li"),mme=a("strong"),QWo=o("blenderbot-small"),HWo=o(" \u2014 "),EN=a("a"),UWo=o("TFBlenderbotSmallModel"),JWo=o(" (BlenderbotSmall model)"),YWo=l(),Q1=a("li"),fme=a("strong"),KWo=o("camembert"),ZWo=o(" \u2014 "),yN=a("a"),eQo=o("TFCamembertModel"),oQo=o(" (CamemBERT model)"),rQo=l(),H1=a("li"),gme=a("strong"),tQo=o("clip"),aQo=o(" \u2014 "),wN=a("a"),sQo=o("TFCLIPModel"),nQo=o(" (CLIP model)"),lQo=l(),U1=a("li"),hme=a("strong"),iQo=o("convbert"),dQo=o(" \u2014 "),AN=a("a"),cQo=o("TFConvBertModel"),mQo=o(" (ConvBERT model)"),fQo=l(),J1=a("li"),ume=a("strong"),gQo=o("convnext"),hQo=o(" \u2014 "),LN=a("a"),uQo=o("TFConvNextModel"),pQo=o(" (ConvNext model)"),_Qo=l(),Y1=a("li"),pme=a("strong"),bQo=o("ctrl"),vQo=o(" \u2014 "),BN=a("a"),TQo=o("TFCTRLModel"),FQo=o(" (CTRL model)"),CQo=l(),K1=a("li"),_me=a("strong"),MQo=o("deberta"),EQo=o(" \u2014 "),xN=a("a"),yQo=o("TFDebertaModel"),wQo=o(" (DeBERTa model)"),AQo=l(),Z1=a("li"),bme=a("strong"),LQo=o("deberta-v2"),BQo=o(" \u2014 "),kN=a("a"),xQo=o("TFDebertaV2Model"),kQo=o(" (DeBERTa-v2 model)"),RQo=l(),eF=a("li"),vme=a("strong"),SQo=o("distilbert"),PQo=o(" \u2014 "),RN=a("a"),$Qo=o("TFDistilBertModel"),IQo=o(" (DistilBERT model)"),DQo=l(),oF=a("li"),Tme=a("strong"),jQo=o("dpr"),NQo=o(" \u2014 "),SN=a("a"),qQo=o("TFDPRQuestionEncoder"),GQo=o(" (DPR model)"),OQo=l(),rF=a("li"),Fme=a("strong"),XQo=o("electra"),VQo=o(" \u2014 "),PN=a("a"),zQo=o("TFElectraModel"),WQo=o(" (ELECTRA model)"),QQo=l(),tF=a("li"),Cme=a("strong"),HQo=o("flaubert"),UQo=o(" \u2014 "),$N=a("a"),JQo=o("TFFlaubertModel"),YQo=o(" (FlauBERT model)"),KQo=l(),In=a("li"),Mme=a("strong"),ZQo=o("funnel"),eHo=o(" \u2014 "),IN=a("a"),oHo=o("TFFunnelModel"),rHo=o(" or "),DN=a("a"),tHo=o("TFFunnelBaseModel"),aHo=o(" (Funnel Transformer model)"),sHo=l(),aF=a("li"),Eme=a("strong"),nHo=o("gpt2"),lHo=o(" \u2014 "),jN=a("a"),iHo=o("TFGPT2Model"),dHo=o(" (OpenAI GPT-2 model)"),cHo=l(),sF=a("li"),yme=a("strong"),mHo=o("hubert"),fHo=o(" \u2014 "),NN=a("a"),gHo=o("TFHubertModel"),hHo=o(" (Hubert model)"),uHo=l(),nF=a("li"),wme=a("strong"),pHo=o("layoutlm"),_Ho=o(" \u2014 "),qN=a("a"),bHo=o("TFLayoutLMModel"),vHo=o(" (LayoutLM model)"),THo=l(),lF=a("li"),Ame=a("strong"),FHo=o("led"),CHo=o(" \u2014 "),GN=a("a"),MHo=o("TFLEDModel"),EHo=o(" (LED model)"),yHo=l(),iF=a("li"),Lme=a("strong"),wHo=o("longformer"),AHo=o(" \u2014 "),ON=a("a"),LHo=o("TFLongformerModel"),BHo=o(" (Longformer model)"),xHo=l(),dF=a("li"),Bme=a("strong"),kHo=o("lxmert"),RHo=o(" \u2014 "),XN=a("a"),SHo=o("TFLxmertModel"),PHo=o(" (LXMERT model)"),$Ho=l(),cF=a("li"),xme=a("strong"),IHo=o("marian"),DHo=o(" \u2014 "),VN=a("a"),jHo=o("TFMarianModel"),NHo=o(" (Marian model)"),qHo=l(),mF=a("li"),kme=a("strong"),GHo=o("mbart"),OHo=o(" \u2014 "),zN=a("a"),XHo=o("TFMBartModel"),VHo=o(" (mBART model)"),zHo=l(),fF=a("li"),Rme=a("strong"),WHo=o("mobilebert"),QHo=o(" \u2014 "),WN=a("a"),HHo=o("TFMobileBertModel"),UHo=o(" (MobileBERT model)"),JHo=l(),gF=a("li"),Sme=a("strong"),YHo=o("mpnet"),KHo=o(" \u2014 "),QN=a("a"),ZHo=o("TFMPNetModel"),eUo=o(" (MPNet model)"),oUo=l(),hF=a("li"),Pme=a("strong"),rUo=o("mt5"),tUo=o(" \u2014 "),HN=a("a"),aUo=o("TFMT5Model"),sUo=o(" (mT5 model)"),nUo=l(),uF=a("li"),$me=a("strong"),lUo=o("openai-gpt"),iUo=o(" \u2014 "),UN=a("a"),dUo=o("TFOpenAIGPTModel"),cUo=o(" (OpenAI GPT model)"),mUo=l(),pF=a("li"),Ime=a("strong"),fUo=o("pegasus"),gUo=o(" \u2014 "),JN=a("a"),hUo=o("TFPegasusModel"),uUo=o(" (Pegasus model)"),pUo=l(),_F=a("li"),Dme=a("strong"),_Uo=o("rembert"),bUo=o(" \u2014 "),YN=a("a"),vUo=o("TFRemBertModel"),TUo=o(" (RemBERT model)"),FUo=l(),bF=a("li"),jme=a("strong"),CUo=o("roberta"),MUo=o(" \u2014 "),KN=a("a"),EUo=o("TFRobertaModel"),yUo=o(" (RoBERTa model)"),wUo=l(),vF=a("li"),Nme=a("strong"),AUo=o("roformer"),LUo=o(" \u2014 "),ZN=a("a"),BUo=o("TFRoFormerModel"),xUo=o(" (RoFormer model)"),kUo=l(),TF=a("li"),qme=a("strong"),RUo=o("speech_to_text"),SUo=o(" \u2014 "),eq=a("a"),PUo=o("TFSpeech2TextModel"),$Uo=o(" (Speech2Text model)"),IUo=l(),FF=a("li"),Gme=a("strong"),DUo=o("t5"),jUo=o(" \u2014 "),oq=a("a"),NUo=o("TFT5Model"),qUo=o(" (T5 model)"),GUo=l(),CF=a("li"),Ome=a("strong"),OUo=o("tapas"),XUo=o(" \u2014 "),rq=a("a"),VUo=o("TFTapasModel"),zUo=o(" (TAPAS model)"),WUo=l(),MF=a("li"),Xme=a("strong"),QUo=o("transfo-xl"),HUo=o(" \u2014 "),tq=a("a"),UUo=o("TFTransfoXLModel"),JUo=o(" (Transformer-XL model)"),YUo=l(),EF=a("li"),Vme=a("strong"),KUo=o("vit"),ZUo=o(" \u2014 "),aq=a("a"),eJo=o("TFViTModel"),oJo=o(" (ViT model)"),rJo=l(),yF=a("li"),zme=a("strong"),tJo=o("wav2vec2"),aJo=o(" \u2014 "),sq=a("a"),sJo=o("TFWav2Vec2Model"),nJo=o(" (Wav2Vec2 model)"),lJo=l(),wF=a("li"),Wme=a("strong"),iJo=o("xlm"),dJo=o(" \u2014 "),nq=a("a"),cJo=o("TFXLMModel"),mJo=o(" (XLM model)"),fJo=l(),AF=a("li"),Qme=a("strong"),gJo=o("xlm-roberta"),hJo=o(" \u2014 "),lq=a("a"),uJo=o("TFXLMRobertaModel"),pJo=o(" (XLM-RoBERTa model)"),_Jo=l(),LF=a("li"),Hme=a("strong"),bJo=o("xlnet"),vJo=o(" \u2014 "),iq=a("a"),TJo=o("TFXLNetModel"),FJo=o(" (XLNet model)"),CJo=l(),Ume=a("p"),MJo=o("Examples:"),EJo=l(),m(tA.$$.fragment),$Be=l(),sc=a("h2"),BF=a("a"),Jme=a("span"),m(aA.$$.fragment),yJo=l(),Yme=a("span"),wJo=o("TFAutoModelForPreTraining"),IBe=l(),pr=a("div"),m(sA.$$.fragment),AJo=l(),nc=a("p"),LJo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Kme=a("code"),BJo=o("from_pretrained()"),xJo=o("class method or the "),Zme=a("code"),kJo=o("from_config()"),RJo=o(`class
method.`),SJo=l(),nA=a("p"),PJo=o("This class cannot be instantiated directly using "),efe=a("code"),$Jo=o("__init__()"),IJo=o(" (throws an error)."),DJo=l(),dt=a("div"),m(lA.$$.fragment),jJo=l(),ofe=a("p"),NJo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),qJo=l(),lc=a("p"),GJo=o(`Note:
Loading a model from its configuration file does `),rfe=a("strong"),OJo=o("not"),XJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tfe=a("code"),VJo=o("from_pretrained()"),zJo=o("to load the model weights."),WJo=l(),afe=a("p"),QJo=o("Examples:"),HJo=l(),m(iA.$$.fragment),UJo=l(),ho=a("div"),m(dA.$$.fragment),JJo=l(),sfe=a("p"),YJo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),KJo=l(),cs=a("p"),ZJo=o("The model class to instantiate is selected based on the "),nfe=a("code"),eYo=o("model_type"),oYo=o(` property of the config object (either
passed as an argument or loaded from `),lfe=a("code"),rYo=o("pretrained_model_name_or_path"),tYo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ife=a("code"),aYo=o("pretrained_model_name_or_path"),sYo=o(":"),nYo=l(),H=a("ul"),xF=a("li"),dfe=a("strong"),lYo=o("albert"),iYo=o(" \u2014 "),dq=a("a"),dYo=o("TFAlbertForPreTraining"),cYo=o(" (ALBERT model)"),mYo=l(),kF=a("li"),cfe=a("strong"),fYo=o("bart"),gYo=o(" \u2014 "),cq=a("a"),hYo=o("TFBartForConditionalGeneration"),uYo=o(" (BART model)"),pYo=l(),RF=a("li"),mfe=a("strong"),_Yo=o("bert"),bYo=o(" \u2014 "),mq=a("a"),vYo=o("TFBertForPreTraining"),TYo=o(" (BERT model)"),FYo=l(),SF=a("li"),ffe=a("strong"),CYo=o("camembert"),MYo=o(" \u2014 "),fq=a("a"),EYo=o("TFCamembertForMaskedLM"),yYo=o(" (CamemBERT model)"),wYo=l(),PF=a("li"),gfe=a("strong"),AYo=o("ctrl"),LYo=o(" \u2014 "),gq=a("a"),BYo=o("TFCTRLLMHeadModel"),xYo=o(" (CTRL model)"),kYo=l(),$F=a("li"),hfe=a("strong"),RYo=o("distilbert"),SYo=o(" \u2014 "),hq=a("a"),PYo=o("TFDistilBertForMaskedLM"),$Yo=o(" (DistilBERT model)"),IYo=l(),IF=a("li"),ufe=a("strong"),DYo=o("electra"),jYo=o(" \u2014 "),uq=a("a"),NYo=o("TFElectraForPreTraining"),qYo=o(" (ELECTRA model)"),GYo=l(),DF=a("li"),pfe=a("strong"),OYo=o("flaubert"),XYo=o(" \u2014 "),pq=a("a"),VYo=o("TFFlaubertWithLMHeadModel"),zYo=o(" (FlauBERT model)"),WYo=l(),jF=a("li"),_fe=a("strong"),QYo=o("funnel"),HYo=o(" \u2014 "),_q=a("a"),UYo=o("TFFunnelForPreTraining"),JYo=o(" (Funnel Transformer model)"),YYo=l(),NF=a("li"),bfe=a("strong"),KYo=o("gpt2"),ZYo=o(" \u2014 "),bq=a("a"),eKo=o("TFGPT2LMHeadModel"),oKo=o(" (OpenAI GPT-2 model)"),rKo=l(),qF=a("li"),vfe=a("strong"),tKo=o("layoutlm"),aKo=o(" \u2014 "),vq=a("a"),sKo=o("TFLayoutLMForMaskedLM"),nKo=o(" (LayoutLM model)"),lKo=l(),GF=a("li"),Tfe=a("strong"),iKo=o("lxmert"),dKo=o(" \u2014 "),Tq=a("a"),cKo=o("TFLxmertForPreTraining"),mKo=o(" (LXMERT model)"),fKo=l(),OF=a("li"),Ffe=a("strong"),gKo=o("mobilebert"),hKo=o(" \u2014 "),Fq=a("a"),uKo=o("TFMobileBertForPreTraining"),pKo=o(" (MobileBERT model)"),_Ko=l(),XF=a("li"),Cfe=a("strong"),bKo=o("mpnet"),vKo=o(" \u2014 "),Cq=a("a"),TKo=o("TFMPNetForMaskedLM"),FKo=o(" (MPNet model)"),CKo=l(),VF=a("li"),Mfe=a("strong"),MKo=o("openai-gpt"),EKo=o(" \u2014 "),Mq=a("a"),yKo=o("TFOpenAIGPTLMHeadModel"),wKo=o(" (OpenAI GPT model)"),AKo=l(),zF=a("li"),Efe=a("strong"),LKo=o("roberta"),BKo=o(" \u2014 "),Eq=a("a"),xKo=o("TFRobertaForMaskedLM"),kKo=o(" (RoBERTa model)"),RKo=l(),WF=a("li"),yfe=a("strong"),SKo=o("t5"),PKo=o(" \u2014 "),yq=a("a"),$Ko=o("TFT5ForConditionalGeneration"),IKo=o(" (T5 model)"),DKo=l(),QF=a("li"),wfe=a("strong"),jKo=o("tapas"),NKo=o(" \u2014 "),wq=a("a"),qKo=o("TFTapasForMaskedLM"),GKo=o(" (TAPAS model)"),OKo=l(),HF=a("li"),Afe=a("strong"),XKo=o("transfo-xl"),VKo=o(" \u2014 "),Aq=a("a"),zKo=o("TFTransfoXLLMHeadModel"),WKo=o(" (Transformer-XL model)"),QKo=l(),UF=a("li"),Lfe=a("strong"),HKo=o("xlm"),UKo=o(" \u2014 "),Lq=a("a"),JKo=o("TFXLMWithLMHeadModel"),YKo=o(" (XLM model)"),KKo=l(),JF=a("li"),Bfe=a("strong"),ZKo=o("xlm-roberta"),eZo=o(" \u2014 "),Bq=a("a"),oZo=o("TFXLMRobertaForMaskedLM"),rZo=o(" (XLM-RoBERTa model)"),tZo=l(),YF=a("li"),xfe=a("strong"),aZo=o("xlnet"),sZo=o(" \u2014 "),xq=a("a"),nZo=o("TFXLNetLMHeadModel"),lZo=o(" (XLNet model)"),iZo=l(),kfe=a("p"),dZo=o("Examples:"),cZo=l(),m(cA.$$.fragment),DBe=l(),ic=a("h2"),KF=a("a"),Rfe=a("span"),m(mA.$$.fragment),mZo=l(),Sfe=a("span"),fZo=o("TFAutoModelForCausalLM"),jBe=l(),_r=a("div"),m(fA.$$.fragment),gZo=l(),dc=a("p"),hZo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Pfe=a("code"),uZo=o("from_pretrained()"),pZo=o("class method or the "),$fe=a("code"),_Zo=o("from_config()"),bZo=o(`class
method.`),vZo=l(),gA=a("p"),TZo=o("This class cannot be instantiated directly using "),Ife=a("code"),FZo=o("__init__()"),CZo=o(" (throws an error)."),MZo=l(),ct=a("div"),m(hA.$$.fragment),EZo=l(),Dfe=a("p"),yZo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),wZo=l(),cc=a("p"),AZo=o(`Note:
Loading a model from its configuration file does `),jfe=a("strong"),LZo=o("not"),BZo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Nfe=a("code"),xZo=o("from_pretrained()"),kZo=o("to load the model weights."),RZo=l(),qfe=a("p"),SZo=o("Examples:"),PZo=l(),m(uA.$$.fragment),$Zo=l(),uo=a("div"),m(pA.$$.fragment),IZo=l(),Gfe=a("p"),DZo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),jZo=l(),ms=a("p"),NZo=o("The model class to instantiate is selected based on the "),Ofe=a("code"),qZo=o("model_type"),GZo=o(` property of the config object (either
passed as an argument or loaded from `),Xfe=a("code"),OZo=o("pretrained_model_name_or_path"),XZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vfe=a("code"),VZo=o("pretrained_model_name_or_path"),zZo=o(":"),WZo=l(),he=a("ul"),ZF=a("li"),zfe=a("strong"),QZo=o("bert"),HZo=o(" \u2014 "),kq=a("a"),UZo=o("TFBertLMHeadModel"),JZo=o(" (BERT model)"),YZo=l(),eC=a("li"),Wfe=a("strong"),KZo=o("ctrl"),ZZo=o(" \u2014 "),Rq=a("a"),eer=o("TFCTRLLMHeadModel"),oer=o(" (CTRL model)"),rer=l(),oC=a("li"),Qfe=a("strong"),ter=o("gpt2"),aer=o(" \u2014 "),Sq=a("a"),ser=o("TFGPT2LMHeadModel"),ner=o(" (OpenAI GPT-2 model)"),ler=l(),rC=a("li"),Hfe=a("strong"),ier=o("openai-gpt"),der=o(" \u2014 "),Pq=a("a"),cer=o("TFOpenAIGPTLMHeadModel"),mer=o(" (OpenAI GPT model)"),fer=l(),tC=a("li"),Ufe=a("strong"),ger=o("rembert"),her=o(" \u2014 "),$q=a("a"),uer=o("TFRemBertForCausalLM"),per=o(" (RemBERT model)"),_er=l(),aC=a("li"),Jfe=a("strong"),ber=o("roberta"),ver=o(" \u2014 "),Iq=a("a"),Ter=o("TFRobertaForCausalLM"),Fer=o(" (RoBERTa model)"),Cer=l(),sC=a("li"),Yfe=a("strong"),Mer=o("roformer"),Eer=o(" \u2014 "),Dq=a("a"),yer=o("TFRoFormerForCausalLM"),wer=o(" (RoFormer model)"),Aer=l(),nC=a("li"),Kfe=a("strong"),Ler=o("transfo-xl"),Ber=o(" \u2014 "),jq=a("a"),xer=o("TFTransfoXLLMHeadModel"),ker=o(" (Transformer-XL model)"),Rer=l(),lC=a("li"),Zfe=a("strong"),Ser=o("xlm"),Per=o(" \u2014 "),Nq=a("a"),$er=o("TFXLMWithLMHeadModel"),Ier=o(" (XLM model)"),Der=l(),iC=a("li"),ege=a("strong"),jer=o("xlnet"),Ner=o(" \u2014 "),qq=a("a"),qer=o("TFXLNetLMHeadModel"),Ger=o(" (XLNet model)"),Oer=l(),oge=a("p"),Xer=o("Examples:"),Ver=l(),m(_A.$$.fragment),NBe=l(),mc=a("h2"),dC=a("a"),rge=a("span"),m(bA.$$.fragment),zer=l(),tge=a("span"),Wer=o("TFAutoModelForImageClassification"),qBe=l(),br=a("div"),m(vA.$$.fragment),Qer=l(),fc=a("p"),Her=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),age=a("code"),Uer=o("from_pretrained()"),Jer=o("class method or the "),sge=a("code"),Yer=o("from_config()"),Ker=o(`class
method.`),Zer=l(),TA=a("p"),eor=o("This class cannot be instantiated directly using "),nge=a("code"),oor=o("__init__()"),ror=o(" (throws an error)."),tor=l(),mt=a("div"),m(FA.$$.fragment),aor=l(),lge=a("p"),sor=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),nor=l(),gc=a("p"),lor=o(`Note:
Loading a model from its configuration file does `),ige=a("strong"),ior=o("not"),dor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dge=a("code"),cor=o("from_pretrained()"),mor=o("to load the model weights."),gor=l(),cge=a("p"),hor=o("Examples:"),uor=l(),m(CA.$$.fragment),por=l(),po=a("div"),m(MA.$$.fragment),_or=l(),mge=a("p"),bor=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),vor=l(),fs=a("p"),Tor=o("The model class to instantiate is selected based on the "),fge=a("code"),For=o("model_type"),Cor=o(` property of the config object (either
passed as an argument or loaded from `),gge=a("code"),Mor=o("pretrained_model_name_or_path"),Eor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hge=a("code"),yor=o("pretrained_model_name_or_path"),wor=o(":"),Aor=l(),EA=a("ul"),cC=a("li"),uge=a("strong"),Lor=o("convnext"),Bor=o(" \u2014 "),Gq=a("a"),xor=o("TFConvNextForImageClassification"),kor=o(" (ConvNext model)"),Ror=l(),mC=a("li"),pge=a("strong"),Sor=o("vit"),Por=o(" \u2014 "),Oq=a("a"),$or=o("TFViTForImageClassification"),Ior=o(" (ViT model)"),Dor=l(),_ge=a("p"),jor=o("Examples:"),Nor=l(),m(yA.$$.fragment),GBe=l(),hc=a("h2"),fC=a("a"),bge=a("span"),m(wA.$$.fragment),qor=l(),vge=a("span"),Gor=o("TFAutoModelForMaskedLM"),OBe=l(),vr=a("div"),m(AA.$$.fragment),Oor=l(),uc=a("p"),Xor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Tge=a("code"),Vor=o("from_pretrained()"),zor=o("class method or the "),Fge=a("code"),Wor=o("from_config()"),Qor=o(`class
method.`),Hor=l(),LA=a("p"),Uor=o("This class cannot be instantiated directly using "),Cge=a("code"),Jor=o("__init__()"),Yor=o(" (throws an error)."),Kor=l(),ft=a("div"),m(BA.$$.fragment),Zor=l(),Mge=a("p"),err=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),orr=l(),pc=a("p"),rrr=o(`Note:
Loading a model from its configuration file does `),Ege=a("strong"),trr=o("not"),arr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),yge=a("code"),srr=o("from_pretrained()"),nrr=o("to load the model weights."),lrr=l(),wge=a("p"),irr=o("Examples:"),drr=l(),m(xA.$$.fragment),crr=l(),_o=a("div"),m(kA.$$.fragment),mrr=l(),Age=a("p"),frr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),grr=l(),gs=a("p"),hrr=o("The model class to instantiate is selected based on the "),Lge=a("code"),urr=o("model_type"),prr=o(` property of the config object (either
passed as an argument or loaded from `),Bge=a("code"),_rr=o("pretrained_model_name_or_path"),brr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xge=a("code"),vrr=o("pretrained_model_name_or_path"),Trr=o(":"),Frr=l(),Y=a("ul"),gC=a("li"),kge=a("strong"),Crr=o("albert"),Mrr=o(" \u2014 "),Xq=a("a"),Err=o("TFAlbertForMaskedLM"),yrr=o(" (ALBERT model)"),wrr=l(),hC=a("li"),Rge=a("strong"),Arr=o("bert"),Lrr=o(" \u2014 "),Vq=a("a"),Brr=o("TFBertForMaskedLM"),xrr=o(" (BERT model)"),krr=l(),uC=a("li"),Sge=a("strong"),Rrr=o("camembert"),Srr=o(" \u2014 "),zq=a("a"),Prr=o("TFCamembertForMaskedLM"),$rr=o(" (CamemBERT model)"),Irr=l(),pC=a("li"),Pge=a("strong"),Drr=o("convbert"),jrr=o(" \u2014 "),Wq=a("a"),Nrr=o("TFConvBertForMaskedLM"),qrr=o(" (ConvBERT model)"),Grr=l(),_C=a("li"),$ge=a("strong"),Orr=o("deberta"),Xrr=o(" \u2014 "),Qq=a("a"),Vrr=o("TFDebertaForMaskedLM"),zrr=o(" (DeBERTa model)"),Wrr=l(),bC=a("li"),Ige=a("strong"),Qrr=o("deberta-v2"),Hrr=o(" \u2014 "),Hq=a("a"),Urr=o("TFDebertaV2ForMaskedLM"),Jrr=o(" (DeBERTa-v2 model)"),Yrr=l(),vC=a("li"),Dge=a("strong"),Krr=o("distilbert"),Zrr=o(" \u2014 "),Uq=a("a"),etr=o("TFDistilBertForMaskedLM"),otr=o(" (DistilBERT model)"),rtr=l(),TC=a("li"),jge=a("strong"),ttr=o("electra"),atr=o(" \u2014 "),Jq=a("a"),str=o("TFElectraForMaskedLM"),ntr=o(" (ELECTRA model)"),ltr=l(),FC=a("li"),Nge=a("strong"),itr=o("flaubert"),dtr=o(" \u2014 "),Yq=a("a"),ctr=o("TFFlaubertWithLMHeadModel"),mtr=o(" (FlauBERT model)"),ftr=l(),CC=a("li"),qge=a("strong"),gtr=o("funnel"),htr=o(" \u2014 "),Kq=a("a"),utr=o("TFFunnelForMaskedLM"),ptr=o(" (Funnel Transformer model)"),_tr=l(),MC=a("li"),Gge=a("strong"),btr=o("layoutlm"),vtr=o(" \u2014 "),Zq=a("a"),Ttr=o("TFLayoutLMForMaskedLM"),Ftr=o(" (LayoutLM model)"),Ctr=l(),EC=a("li"),Oge=a("strong"),Mtr=o("longformer"),Etr=o(" \u2014 "),eG=a("a"),ytr=o("TFLongformerForMaskedLM"),wtr=o(" (Longformer model)"),Atr=l(),yC=a("li"),Xge=a("strong"),Ltr=o("mobilebert"),Btr=o(" \u2014 "),oG=a("a"),xtr=o("TFMobileBertForMaskedLM"),ktr=o(" (MobileBERT model)"),Rtr=l(),wC=a("li"),Vge=a("strong"),Str=o("mpnet"),Ptr=o(" \u2014 "),rG=a("a"),$tr=o("TFMPNetForMaskedLM"),Itr=o(" (MPNet model)"),Dtr=l(),AC=a("li"),zge=a("strong"),jtr=o("rembert"),Ntr=o(" \u2014 "),tG=a("a"),qtr=o("TFRemBertForMaskedLM"),Gtr=o(" (RemBERT model)"),Otr=l(),LC=a("li"),Wge=a("strong"),Xtr=o("roberta"),Vtr=o(" \u2014 "),aG=a("a"),ztr=o("TFRobertaForMaskedLM"),Wtr=o(" (RoBERTa model)"),Qtr=l(),BC=a("li"),Qge=a("strong"),Htr=o("roformer"),Utr=o(" \u2014 "),sG=a("a"),Jtr=o("TFRoFormerForMaskedLM"),Ytr=o(" (RoFormer model)"),Ktr=l(),xC=a("li"),Hge=a("strong"),Ztr=o("tapas"),ear=o(" \u2014 "),nG=a("a"),oar=o("TFTapasForMaskedLM"),rar=o(" (TAPAS model)"),tar=l(),kC=a("li"),Uge=a("strong"),aar=o("xlm"),sar=o(" \u2014 "),lG=a("a"),nar=o("TFXLMWithLMHeadModel"),lar=o(" (XLM model)"),iar=l(),RC=a("li"),Jge=a("strong"),dar=o("xlm-roberta"),car=o(" \u2014 "),iG=a("a"),mar=o("TFXLMRobertaForMaskedLM"),far=o(" (XLM-RoBERTa model)"),gar=l(),Yge=a("p"),har=o("Examples:"),uar=l(),m(RA.$$.fragment),XBe=l(),_c=a("h2"),SC=a("a"),Kge=a("span"),m(SA.$$.fragment),par=l(),Zge=a("span"),_ar=o("TFAutoModelForSeq2SeqLM"),VBe=l(),Tr=a("div"),m(PA.$$.fragment),bar=l(),bc=a("p"),Tar=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ehe=a("code"),Far=o("from_pretrained()"),Car=o("class method or the "),ohe=a("code"),Mar=o("from_config()"),Ear=o(`class
method.`),yar=l(),$A=a("p"),war=o("This class cannot be instantiated directly using "),rhe=a("code"),Aar=o("__init__()"),Lar=o(" (throws an error)."),Bar=l(),gt=a("div"),m(IA.$$.fragment),xar=l(),the=a("p"),kar=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Rar=l(),vc=a("p"),Sar=o(`Note:
Loading a model from its configuration file does `),ahe=a("strong"),Par=o("not"),$ar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),she=a("code"),Iar=o("from_pretrained()"),Dar=o("to load the model weights."),jar=l(),nhe=a("p"),Nar=o("Examples:"),qar=l(),m(DA.$$.fragment),Gar=l(),bo=a("div"),m(jA.$$.fragment),Oar=l(),lhe=a("p"),Xar=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Var=l(),hs=a("p"),zar=o("The model class to instantiate is selected based on the "),ihe=a("code"),War=o("model_type"),Qar=o(` property of the config object (either
passed as an argument or loaded from `),dhe=a("code"),Har=o("pretrained_model_name_or_path"),Uar=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),che=a("code"),Jar=o("pretrained_model_name_or_path"),Yar=o(":"),Kar=l(),ue=a("ul"),PC=a("li"),mhe=a("strong"),Zar=o("bart"),esr=o(" \u2014 "),dG=a("a"),osr=o("TFBartForConditionalGeneration"),rsr=o(" (BART model)"),tsr=l(),$C=a("li"),fhe=a("strong"),asr=o("blenderbot"),ssr=o(" \u2014 "),cG=a("a"),nsr=o("TFBlenderbotForConditionalGeneration"),lsr=o(" (Blenderbot model)"),isr=l(),IC=a("li"),ghe=a("strong"),dsr=o("blenderbot-small"),csr=o(" \u2014 "),mG=a("a"),msr=o("TFBlenderbotSmallForConditionalGeneration"),fsr=o(" (BlenderbotSmall model)"),gsr=l(),DC=a("li"),hhe=a("strong"),hsr=o("encoder-decoder"),usr=o(" \u2014 "),fG=a("a"),psr=o("TFEncoderDecoderModel"),_sr=o(" (Encoder decoder model)"),bsr=l(),jC=a("li"),uhe=a("strong"),vsr=o("led"),Tsr=o(" \u2014 "),gG=a("a"),Fsr=o("TFLEDForConditionalGeneration"),Csr=o(" (LED model)"),Msr=l(),NC=a("li"),phe=a("strong"),Esr=o("marian"),ysr=o(" \u2014 "),hG=a("a"),wsr=o("TFMarianMTModel"),Asr=o(" (Marian model)"),Lsr=l(),qC=a("li"),_he=a("strong"),Bsr=o("mbart"),xsr=o(" \u2014 "),uG=a("a"),ksr=o("TFMBartForConditionalGeneration"),Rsr=o(" (mBART model)"),Ssr=l(),GC=a("li"),bhe=a("strong"),Psr=o("mt5"),$sr=o(" \u2014 "),pG=a("a"),Isr=o("TFMT5ForConditionalGeneration"),Dsr=o(" (mT5 model)"),jsr=l(),OC=a("li"),vhe=a("strong"),Nsr=o("pegasus"),qsr=o(" \u2014 "),_G=a("a"),Gsr=o("TFPegasusForConditionalGeneration"),Osr=o(" (Pegasus model)"),Xsr=l(),XC=a("li"),The=a("strong"),Vsr=o("t5"),zsr=o(" \u2014 "),bG=a("a"),Wsr=o("TFT5ForConditionalGeneration"),Qsr=o(" (T5 model)"),Hsr=l(),Fhe=a("p"),Usr=o("Examples:"),Jsr=l(),m(NA.$$.fragment),zBe=l(),Tc=a("h2"),VC=a("a"),Che=a("span"),m(qA.$$.fragment),Ysr=l(),Mhe=a("span"),Ksr=o("TFAutoModelForSequenceClassification"),WBe=l(),Fr=a("div"),m(GA.$$.fragment),Zsr=l(),Fc=a("p"),enr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Ehe=a("code"),onr=o("from_pretrained()"),rnr=o("class method or the "),yhe=a("code"),tnr=o("from_config()"),anr=o(`class
method.`),snr=l(),OA=a("p"),nnr=o("This class cannot be instantiated directly using "),whe=a("code"),lnr=o("__init__()"),inr=o(" (throws an error)."),dnr=l(),ht=a("div"),m(XA.$$.fragment),cnr=l(),Ahe=a("p"),mnr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),fnr=l(),Cc=a("p"),gnr=o(`Note:
Loading a model from its configuration file does `),Lhe=a("strong"),hnr=o("not"),unr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bhe=a("code"),pnr=o("from_pretrained()"),_nr=o("to load the model weights."),bnr=l(),xhe=a("p"),vnr=o("Examples:"),Tnr=l(),m(VA.$$.fragment),Fnr=l(),vo=a("div"),m(zA.$$.fragment),Cnr=l(),khe=a("p"),Mnr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Enr=l(),us=a("p"),ynr=o("The model class to instantiate is selected based on the "),Rhe=a("code"),wnr=o("model_type"),Anr=o(` property of the config object (either
passed as an argument or loaded from `),She=a("code"),Lnr=o("pretrained_model_name_or_path"),Bnr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Phe=a("code"),xnr=o("pretrained_model_name_or_path"),knr=o(":"),Rnr=l(),X=a("ul"),zC=a("li"),$he=a("strong"),Snr=o("albert"),Pnr=o(" \u2014 "),vG=a("a"),$nr=o("TFAlbertForSequenceClassification"),Inr=o(" (ALBERT model)"),Dnr=l(),WC=a("li"),Ihe=a("strong"),jnr=o("bert"),Nnr=o(" \u2014 "),TG=a("a"),qnr=o("TFBertForSequenceClassification"),Gnr=o(" (BERT model)"),Onr=l(),QC=a("li"),Dhe=a("strong"),Xnr=o("camembert"),Vnr=o(" \u2014 "),FG=a("a"),znr=o("TFCamembertForSequenceClassification"),Wnr=o(" (CamemBERT model)"),Qnr=l(),HC=a("li"),jhe=a("strong"),Hnr=o("convbert"),Unr=o(" \u2014 "),CG=a("a"),Jnr=o("TFConvBertForSequenceClassification"),Ynr=o(" (ConvBERT model)"),Knr=l(),UC=a("li"),Nhe=a("strong"),Znr=o("ctrl"),elr=o(" \u2014 "),MG=a("a"),olr=o("TFCTRLForSequenceClassification"),rlr=o(" (CTRL model)"),tlr=l(),JC=a("li"),qhe=a("strong"),alr=o("deberta"),slr=o(" \u2014 "),EG=a("a"),nlr=o("TFDebertaForSequenceClassification"),llr=o(" (DeBERTa model)"),ilr=l(),YC=a("li"),Ghe=a("strong"),dlr=o("deberta-v2"),clr=o(" \u2014 "),yG=a("a"),mlr=o("TFDebertaV2ForSequenceClassification"),flr=o(" (DeBERTa-v2 model)"),glr=l(),KC=a("li"),Ohe=a("strong"),hlr=o("distilbert"),ulr=o(" \u2014 "),wG=a("a"),plr=o("TFDistilBertForSequenceClassification"),_lr=o(" (DistilBERT model)"),blr=l(),ZC=a("li"),Xhe=a("strong"),vlr=o("electra"),Tlr=o(" \u2014 "),AG=a("a"),Flr=o("TFElectraForSequenceClassification"),Clr=o(" (ELECTRA model)"),Mlr=l(),e4=a("li"),Vhe=a("strong"),Elr=o("flaubert"),ylr=o(" \u2014 "),LG=a("a"),wlr=o("TFFlaubertForSequenceClassification"),Alr=o(" (FlauBERT model)"),Llr=l(),o4=a("li"),zhe=a("strong"),Blr=o("funnel"),xlr=o(" \u2014 "),BG=a("a"),klr=o("TFFunnelForSequenceClassification"),Rlr=o(" (Funnel Transformer model)"),Slr=l(),r4=a("li"),Whe=a("strong"),Plr=o("gpt2"),$lr=o(" \u2014 "),xG=a("a"),Ilr=o("TFGPT2ForSequenceClassification"),Dlr=o(" (OpenAI GPT-2 model)"),jlr=l(),t4=a("li"),Qhe=a("strong"),Nlr=o("layoutlm"),qlr=o(" \u2014 "),kG=a("a"),Glr=o("TFLayoutLMForSequenceClassification"),Olr=o(" (LayoutLM model)"),Xlr=l(),a4=a("li"),Hhe=a("strong"),Vlr=o("longformer"),zlr=o(" \u2014 "),RG=a("a"),Wlr=o("TFLongformerForSequenceClassification"),Qlr=o(" (Longformer model)"),Hlr=l(),s4=a("li"),Uhe=a("strong"),Ulr=o("mobilebert"),Jlr=o(" \u2014 "),SG=a("a"),Ylr=o("TFMobileBertForSequenceClassification"),Klr=o(" (MobileBERT model)"),Zlr=l(),n4=a("li"),Jhe=a("strong"),eir=o("mpnet"),oir=o(" \u2014 "),PG=a("a"),rir=o("TFMPNetForSequenceClassification"),tir=o(" (MPNet model)"),air=l(),l4=a("li"),Yhe=a("strong"),sir=o("openai-gpt"),nir=o(" \u2014 "),$G=a("a"),lir=o("TFOpenAIGPTForSequenceClassification"),iir=o(" (OpenAI GPT model)"),dir=l(),i4=a("li"),Khe=a("strong"),cir=o("rembert"),mir=o(" \u2014 "),IG=a("a"),fir=o("TFRemBertForSequenceClassification"),gir=o(" (RemBERT model)"),hir=l(),d4=a("li"),Zhe=a("strong"),uir=o("roberta"),pir=o(" \u2014 "),DG=a("a"),_ir=o("TFRobertaForSequenceClassification"),bir=o(" (RoBERTa model)"),vir=l(),c4=a("li"),eue=a("strong"),Tir=o("roformer"),Fir=o(" \u2014 "),jG=a("a"),Cir=o("TFRoFormerForSequenceClassification"),Mir=o(" (RoFormer model)"),Eir=l(),m4=a("li"),oue=a("strong"),yir=o("tapas"),wir=o(" \u2014 "),NG=a("a"),Air=o("TFTapasForSequenceClassification"),Lir=o(" (TAPAS model)"),Bir=l(),f4=a("li"),rue=a("strong"),xir=o("transfo-xl"),kir=o(" \u2014 "),qG=a("a"),Rir=o("TFTransfoXLForSequenceClassification"),Sir=o(" (Transformer-XL model)"),Pir=l(),g4=a("li"),tue=a("strong"),$ir=o("xlm"),Iir=o(" \u2014 "),GG=a("a"),Dir=o("TFXLMForSequenceClassification"),jir=o(" (XLM model)"),Nir=l(),h4=a("li"),aue=a("strong"),qir=o("xlm-roberta"),Gir=o(" \u2014 "),OG=a("a"),Oir=o("TFXLMRobertaForSequenceClassification"),Xir=o(" (XLM-RoBERTa model)"),Vir=l(),u4=a("li"),sue=a("strong"),zir=o("xlnet"),Wir=o(" \u2014 "),XG=a("a"),Qir=o("TFXLNetForSequenceClassification"),Hir=o(" (XLNet model)"),Uir=l(),nue=a("p"),Jir=o("Examples:"),Yir=l(),m(WA.$$.fragment),QBe=l(),Mc=a("h2"),p4=a("a"),lue=a("span"),m(QA.$$.fragment),Kir=l(),iue=a("span"),Zir=o("TFAutoModelForMultipleChoice"),HBe=l(),Cr=a("div"),m(HA.$$.fragment),edr=l(),Ec=a("p"),odr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),due=a("code"),rdr=o("from_pretrained()"),tdr=o("class method or the "),cue=a("code"),adr=o("from_config()"),sdr=o(`class
method.`),ndr=l(),UA=a("p"),ldr=o("This class cannot be instantiated directly using "),mue=a("code"),idr=o("__init__()"),ddr=o(" (throws an error)."),cdr=l(),ut=a("div"),m(JA.$$.fragment),mdr=l(),fue=a("p"),fdr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),gdr=l(),yc=a("p"),hdr=o(`Note:
Loading a model from its configuration file does `),gue=a("strong"),udr=o("not"),pdr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),hue=a("code"),_dr=o("from_pretrained()"),bdr=o("to load the model weights."),vdr=l(),uue=a("p"),Tdr=o("Examples:"),Fdr=l(),m(YA.$$.fragment),Cdr=l(),To=a("div"),m(KA.$$.fragment),Mdr=l(),pue=a("p"),Edr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),ydr=l(),ps=a("p"),wdr=o("The model class to instantiate is selected based on the "),_ue=a("code"),Adr=o("model_type"),Ldr=o(` property of the config object (either
passed as an argument or loaded from `),bue=a("code"),Bdr=o("pretrained_model_name_or_path"),xdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vue=a("code"),kdr=o("pretrained_model_name_or_path"),Rdr=o(":"),Sdr=l(),te=a("ul"),_4=a("li"),Tue=a("strong"),Pdr=o("albert"),$dr=o(" \u2014 "),VG=a("a"),Idr=o("TFAlbertForMultipleChoice"),Ddr=o(" (ALBERT model)"),jdr=l(),b4=a("li"),Fue=a("strong"),Ndr=o("bert"),qdr=o(" \u2014 "),zG=a("a"),Gdr=o("TFBertForMultipleChoice"),Odr=o(" (BERT model)"),Xdr=l(),v4=a("li"),Cue=a("strong"),Vdr=o("camembert"),zdr=o(" \u2014 "),WG=a("a"),Wdr=o("TFCamembertForMultipleChoice"),Qdr=o(" (CamemBERT model)"),Hdr=l(),T4=a("li"),Mue=a("strong"),Udr=o("convbert"),Jdr=o(" \u2014 "),QG=a("a"),Ydr=o("TFConvBertForMultipleChoice"),Kdr=o(" (ConvBERT model)"),Zdr=l(),F4=a("li"),Eue=a("strong"),ecr=o("distilbert"),ocr=o(" \u2014 "),HG=a("a"),rcr=o("TFDistilBertForMultipleChoice"),tcr=o(" (DistilBERT model)"),acr=l(),C4=a("li"),yue=a("strong"),scr=o("electra"),ncr=o(" \u2014 "),UG=a("a"),lcr=o("TFElectraForMultipleChoice"),icr=o(" (ELECTRA model)"),dcr=l(),M4=a("li"),wue=a("strong"),ccr=o("flaubert"),mcr=o(" \u2014 "),JG=a("a"),fcr=o("TFFlaubertForMultipleChoice"),gcr=o(" (FlauBERT model)"),hcr=l(),E4=a("li"),Aue=a("strong"),ucr=o("funnel"),pcr=o(" \u2014 "),YG=a("a"),_cr=o("TFFunnelForMultipleChoice"),bcr=o(" (Funnel Transformer model)"),vcr=l(),y4=a("li"),Lue=a("strong"),Tcr=o("longformer"),Fcr=o(" \u2014 "),KG=a("a"),Ccr=o("TFLongformerForMultipleChoice"),Mcr=o(" (Longformer model)"),Ecr=l(),w4=a("li"),Bue=a("strong"),ycr=o("mobilebert"),wcr=o(" \u2014 "),ZG=a("a"),Acr=o("TFMobileBertForMultipleChoice"),Lcr=o(" (MobileBERT model)"),Bcr=l(),A4=a("li"),xue=a("strong"),xcr=o("mpnet"),kcr=o(" \u2014 "),eO=a("a"),Rcr=o("TFMPNetForMultipleChoice"),Scr=o(" (MPNet model)"),Pcr=l(),L4=a("li"),kue=a("strong"),$cr=o("rembert"),Icr=o(" \u2014 "),oO=a("a"),Dcr=o("TFRemBertForMultipleChoice"),jcr=o(" (RemBERT model)"),Ncr=l(),B4=a("li"),Rue=a("strong"),qcr=o("roberta"),Gcr=o(" \u2014 "),rO=a("a"),Ocr=o("TFRobertaForMultipleChoice"),Xcr=o(" (RoBERTa model)"),Vcr=l(),x4=a("li"),Sue=a("strong"),zcr=o("roformer"),Wcr=o(" \u2014 "),tO=a("a"),Qcr=o("TFRoFormerForMultipleChoice"),Hcr=o(" (RoFormer model)"),Ucr=l(),k4=a("li"),Pue=a("strong"),Jcr=o("xlm"),Ycr=o(" \u2014 "),aO=a("a"),Kcr=o("TFXLMForMultipleChoice"),Zcr=o(" (XLM model)"),emr=l(),R4=a("li"),$ue=a("strong"),omr=o("xlm-roberta"),rmr=o(" \u2014 "),sO=a("a"),tmr=o("TFXLMRobertaForMultipleChoice"),amr=o(" (XLM-RoBERTa model)"),smr=l(),S4=a("li"),Iue=a("strong"),nmr=o("xlnet"),lmr=o(" \u2014 "),nO=a("a"),imr=o("TFXLNetForMultipleChoice"),dmr=o(" (XLNet model)"),cmr=l(),Due=a("p"),mmr=o("Examples:"),fmr=l(),m(ZA.$$.fragment),UBe=l(),wc=a("h2"),P4=a("a"),jue=a("span"),m(e0.$$.fragment),gmr=l(),Nue=a("span"),hmr=o("TFAutoModelForTableQuestionAnswering"),JBe=l(),Mr=a("div"),m(o0.$$.fragment),umr=l(),Ac=a("p"),pmr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),que=a("code"),_mr=o("from_pretrained()"),bmr=o("class method or the "),Gue=a("code"),vmr=o("from_config()"),Tmr=o(`class
method.`),Fmr=l(),r0=a("p"),Cmr=o("This class cannot be instantiated directly using "),Oue=a("code"),Mmr=o("__init__()"),Emr=o(" (throws an error)."),ymr=l(),pt=a("div"),m(t0.$$.fragment),wmr=l(),Xue=a("p"),Amr=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Lmr=l(),Lc=a("p"),Bmr=o(`Note:
Loading a model from its configuration file does `),Vue=a("strong"),xmr=o("not"),kmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zue=a("code"),Rmr=o("from_pretrained()"),Smr=o("to load the model weights."),Pmr=l(),Wue=a("p"),$mr=o("Examples:"),Imr=l(),m(a0.$$.fragment),Dmr=l(),Fo=a("div"),m(s0.$$.fragment),jmr=l(),Que=a("p"),Nmr=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),qmr=l(),_s=a("p"),Gmr=o("The model class to instantiate is selected based on the "),Hue=a("code"),Omr=o("model_type"),Xmr=o(` property of the config object (either
passed as an argument or loaded from `),Uue=a("code"),Vmr=o("pretrained_model_name_or_path"),zmr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jue=a("code"),Wmr=o("pretrained_model_name_or_path"),Qmr=o(":"),Hmr=l(),Yue=a("ul"),$4=a("li"),Kue=a("strong"),Umr=o("tapas"),Jmr=o(" \u2014 "),lO=a("a"),Ymr=o("TFTapasForQuestionAnswering"),Kmr=o(" (TAPAS model)"),Zmr=l(),Zue=a("p"),efr=o("Examples:"),ofr=l(),m(n0.$$.fragment),YBe=l(),Bc=a("h2"),I4=a("a"),epe=a("span"),m(l0.$$.fragment),rfr=l(),ope=a("span"),tfr=o("TFAutoModelForTokenClassification"),KBe=l(),Er=a("div"),m(i0.$$.fragment),afr=l(),xc=a("p"),sfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),rpe=a("code"),nfr=o("from_pretrained()"),lfr=o("class method or the "),tpe=a("code"),ifr=o("from_config()"),dfr=o(`class
method.`),cfr=l(),d0=a("p"),mfr=o("This class cannot be instantiated directly using "),ape=a("code"),ffr=o("__init__()"),gfr=o(" (throws an error)."),hfr=l(),_t=a("div"),m(c0.$$.fragment),ufr=l(),spe=a("p"),pfr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),_fr=l(),kc=a("p"),bfr=o(`Note:
Loading a model from its configuration file does `),npe=a("strong"),vfr=o("not"),Tfr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lpe=a("code"),Ffr=o("from_pretrained()"),Cfr=o("to load the model weights."),Mfr=l(),ipe=a("p"),Efr=o("Examples:"),yfr=l(),m(m0.$$.fragment),wfr=l(),Co=a("div"),m(f0.$$.fragment),Afr=l(),dpe=a("p"),Lfr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Bfr=l(),bs=a("p"),xfr=o("The model class to instantiate is selected based on the "),cpe=a("code"),kfr=o("model_type"),Rfr=o(` property of the config object (either
passed as an argument or loaded from `),mpe=a("code"),Sfr=o("pretrained_model_name_or_path"),Pfr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fpe=a("code"),$fr=o("pretrained_model_name_or_path"),Ifr=o(":"),Dfr=l(),K=a("ul"),D4=a("li"),gpe=a("strong"),jfr=o("albert"),Nfr=o(" \u2014 "),iO=a("a"),qfr=o("TFAlbertForTokenClassification"),Gfr=o(" (ALBERT model)"),Ofr=l(),j4=a("li"),hpe=a("strong"),Xfr=o("bert"),Vfr=o(" \u2014 "),dO=a("a"),zfr=o("TFBertForTokenClassification"),Wfr=o(" (BERT model)"),Qfr=l(),N4=a("li"),upe=a("strong"),Hfr=o("camembert"),Ufr=o(" \u2014 "),cO=a("a"),Jfr=o("TFCamembertForTokenClassification"),Yfr=o(" (CamemBERT model)"),Kfr=l(),q4=a("li"),ppe=a("strong"),Zfr=o("convbert"),egr=o(" \u2014 "),mO=a("a"),ogr=o("TFConvBertForTokenClassification"),rgr=o(" (ConvBERT model)"),tgr=l(),G4=a("li"),_pe=a("strong"),agr=o("deberta"),sgr=o(" \u2014 "),fO=a("a"),ngr=o("TFDebertaForTokenClassification"),lgr=o(" (DeBERTa model)"),igr=l(),O4=a("li"),bpe=a("strong"),dgr=o("deberta-v2"),cgr=o(" \u2014 "),gO=a("a"),mgr=o("TFDebertaV2ForTokenClassification"),fgr=o(" (DeBERTa-v2 model)"),ggr=l(),X4=a("li"),vpe=a("strong"),hgr=o("distilbert"),ugr=o(" \u2014 "),hO=a("a"),pgr=o("TFDistilBertForTokenClassification"),_gr=o(" (DistilBERT model)"),bgr=l(),V4=a("li"),Tpe=a("strong"),vgr=o("electra"),Tgr=o(" \u2014 "),uO=a("a"),Fgr=o("TFElectraForTokenClassification"),Cgr=o(" (ELECTRA model)"),Mgr=l(),z4=a("li"),Fpe=a("strong"),Egr=o("flaubert"),ygr=o(" \u2014 "),pO=a("a"),wgr=o("TFFlaubertForTokenClassification"),Agr=o(" (FlauBERT model)"),Lgr=l(),W4=a("li"),Cpe=a("strong"),Bgr=o("funnel"),xgr=o(" \u2014 "),_O=a("a"),kgr=o("TFFunnelForTokenClassification"),Rgr=o(" (Funnel Transformer model)"),Sgr=l(),Q4=a("li"),Mpe=a("strong"),Pgr=o("layoutlm"),$gr=o(" \u2014 "),bO=a("a"),Igr=o("TFLayoutLMForTokenClassification"),Dgr=o(" (LayoutLM model)"),jgr=l(),H4=a("li"),Epe=a("strong"),Ngr=o("longformer"),qgr=o(" \u2014 "),vO=a("a"),Ggr=o("TFLongformerForTokenClassification"),Ogr=o(" (Longformer model)"),Xgr=l(),U4=a("li"),ype=a("strong"),Vgr=o("mobilebert"),zgr=o(" \u2014 "),TO=a("a"),Wgr=o("TFMobileBertForTokenClassification"),Qgr=o(" (MobileBERT model)"),Hgr=l(),J4=a("li"),wpe=a("strong"),Ugr=o("mpnet"),Jgr=o(" \u2014 "),FO=a("a"),Ygr=o("TFMPNetForTokenClassification"),Kgr=o(" (MPNet model)"),Zgr=l(),Y4=a("li"),Ape=a("strong"),ehr=o("rembert"),ohr=o(" \u2014 "),CO=a("a"),rhr=o("TFRemBertForTokenClassification"),thr=o(" (RemBERT model)"),ahr=l(),K4=a("li"),Lpe=a("strong"),shr=o("roberta"),nhr=o(" \u2014 "),MO=a("a"),lhr=o("TFRobertaForTokenClassification"),ihr=o(" (RoBERTa model)"),dhr=l(),Z4=a("li"),Bpe=a("strong"),chr=o("roformer"),mhr=o(" \u2014 "),EO=a("a"),fhr=o("TFRoFormerForTokenClassification"),ghr=o(" (RoFormer model)"),hhr=l(),eM=a("li"),xpe=a("strong"),uhr=o("xlm"),phr=o(" \u2014 "),yO=a("a"),_hr=o("TFXLMForTokenClassification"),bhr=o(" (XLM model)"),vhr=l(),oM=a("li"),kpe=a("strong"),Thr=o("xlm-roberta"),Fhr=o(" \u2014 "),wO=a("a"),Chr=o("TFXLMRobertaForTokenClassification"),Mhr=o(" (XLM-RoBERTa model)"),Ehr=l(),rM=a("li"),Rpe=a("strong"),yhr=o("xlnet"),whr=o(" \u2014 "),AO=a("a"),Ahr=o("TFXLNetForTokenClassification"),Lhr=o(" (XLNet model)"),Bhr=l(),Spe=a("p"),xhr=o("Examples:"),khr=l(),m(g0.$$.fragment),ZBe=l(),Rc=a("h2"),tM=a("a"),Ppe=a("span"),m(h0.$$.fragment),Rhr=l(),$pe=a("span"),Shr=o("TFAutoModelForQuestionAnswering"),exe=l(),yr=a("div"),m(u0.$$.fragment),Phr=l(),Sc=a("p"),$hr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Ipe=a("code"),Ihr=o("from_pretrained()"),Dhr=o("class method or the "),Dpe=a("code"),jhr=o("from_config()"),Nhr=o(`class
method.`),qhr=l(),p0=a("p"),Ghr=o("This class cannot be instantiated directly using "),jpe=a("code"),Ohr=o("__init__()"),Xhr=o(" (throws an error)."),Vhr=l(),bt=a("div"),m(_0.$$.fragment),zhr=l(),Npe=a("p"),Whr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Qhr=l(),Pc=a("p"),Hhr=o(`Note:
Loading a model from its configuration file does `),qpe=a("strong"),Uhr=o("not"),Jhr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gpe=a("code"),Yhr=o("from_pretrained()"),Khr=o("to load the model weights."),Zhr=l(),Ope=a("p"),eur=o("Examples:"),our=l(),m(b0.$$.fragment),rur=l(),Mo=a("div"),m(v0.$$.fragment),tur=l(),Xpe=a("p"),aur=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),sur=l(),vs=a("p"),nur=o("The model class to instantiate is selected based on the "),Vpe=a("code"),lur=o("model_type"),iur=o(` property of the config object (either
passed as an argument or loaded from `),zpe=a("code"),dur=o("pretrained_model_name_or_path"),cur=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wpe=a("code"),mur=o("pretrained_model_name_or_path"),fur=o(":"),gur=l(),Z=a("ul"),aM=a("li"),Qpe=a("strong"),hur=o("albert"),uur=o(" \u2014 "),LO=a("a"),pur=o("TFAlbertForQuestionAnswering"),_ur=o(" (ALBERT model)"),bur=l(),sM=a("li"),Hpe=a("strong"),vur=o("bert"),Tur=o(" \u2014 "),BO=a("a"),Fur=o("TFBertForQuestionAnswering"),Cur=o(" (BERT model)"),Mur=l(),nM=a("li"),Upe=a("strong"),Eur=o("camembert"),yur=o(" \u2014 "),xO=a("a"),wur=o("TFCamembertForQuestionAnswering"),Aur=o(" (CamemBERT model)"),Lur=l(),lM=a("li"),Jpe=a("strong"),Bur=o("convbert"),xur=o(" \u2014 "),kO=a("a"),kur=o("TFConvBertForQuestionAnswering"),Rur=o(" (ConvBERT model)"),Sur=l(),iM=a("li"),Ype=a("strong"),Pur=o("deberta"),$ur=o(" \u2014 "),RO=a("a"),Iur=o("TFDebertaForQuestionAnswering"),Dur=o(" (DeBERTa model)"),jur=l(),dM=a("li"),Kpe=a("strong"),Nur=o("deberta-v2"),qur=o(" \u2014 "),SO=a("a"),Gur=o("TFDebertaV2ForQuestionAnswering"),Our=o(" (DeBERTa-v2 model)"),Xur=l(),cM=a("li"),Zpe=a("strong"),Vur=o("distilbert"),zur=o(" \u2014 "),PO=a("a"),Wur=o("TFDistilBertForQuestionAnswering"),Qur=o(" (DistilBERT model)"),Hur=l(),mM=a("li"),e_e=a("strong"),Uur=o("electra"),Jur=o(" \u2014 "),$O=a("a"),Yur=o("TFElectraForQuestionAnswering"),Kur=o(" (ELECTRA model)"),Zur=l(),fM=a("li"),o_e=a("strong"),epr=o("flaubert"),opr=o(" \u2014 "),IO=a("a"),rpr=o("TFFlaubertForQuestionAnsweringSimple"),tpr=o(" (FlauBERT model)"),apr=l(),gM=a("li"),r_e=a("strong"),spr=o("funnel"),npr=o(" \u2014 "),DO=a("a"),lpr=o("TFFunnelForQuestionAnswering"),ipr=o(" (Funnel Transformer model)"),dpr=l(),hM=a("li"),t_e=a("strong"),cpr=o("longformer"),mpr=o(" \u2014 "),jO=a("a"),fpr=o("TFLongformerForQuestionAnswering"),gpr=o(" (Longformer model)"),hpr=l(),uM=a("li"),a_e=a("strong"),upr=o("mobilebert"),ppr=o(" \u2014 "),NO=a("a"),_pr=o("TFMobileBertForQuestionAnswering"),bpr=o(" (MobileBERT model)"),vpr=l(),pM=a("li"),s_e=a("strong"),Tpr=o("mpnet"),Fpr=o(" \u2014 "),qO=a("a"),Cpr=o("TFMPNetForQuestionAnswering"),Mpr=o(" (MPNet model)"),Epr=l(),_M=a("li"),n_e=a("strong"),ypr=o("rembert"),wpr=o(" \u2014 "),GO=a("a"),Apr=o("TFRemBertForQuestionAnswering"),Lpr=o(" (RemBERT model)"),Bpr=l(),bM=a("li"),l_e=a("strong"),xpr=o("roberta"),kpr=o(" \u2014 "),OO=a("a"),Rpr=o("TFRobertaForQuestionAnswering"),Spr=o(" (RoBERTa model)"),Ppr=l(),vM=a("li"),i_e=a("strong"),$pr=o("roformer"),Ipr=o(" \u2014 "),XO=a("a"),Dpr=o("TFRoFormerForQuestionAnswering"),jpr=o(" (RoFormer model)"),Npr=l(),TM=a("li"),d_e=a("strong"),qpr=o("xlm"),Gpr=o(" \u2014 "),VO=a("a"),Opr=o("TFXLMForQuestionAnsweringSimple"),Xpr=o(" (XLM model)"),Vpr=l(),FM=a("li"),c_e=a("strong"),zpr=o("xlm-roberta"),Wpr=o(" \u2014 "),zO=a("a"),Qpr=o("TFXLMRobertaForQuestionAnswering"),Hpr=o(" (XLM-RoBERTa model)"),Upr=l(),CM=a("li"),m_e=a("strong"),Jpr=o("xlnet"),Ypr=o(" \u2014 "),WO=a("a"),Kpr=o("TFXLNetForQuestionAnsweringSimple"),Zpr=o(" (XLNet model)"),e_r=l(),f_e=a("p"),o_r=o("Examples:"),r_r=l(),m(T0.$$.fragment),oxe=l(),$c=a("h2"),MM=a("a"),g_e=a("span"),m(F0.$$.fragment),t_r=l(),h_e=a("span"),a_r=o("TFAutoModelForVision2Seq"),rxe=l(),wr=a("div"),m(C0.$$.fragment),s_r=l(),Ic=a("p"),n_r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),u_e=a("code"),l_r=o("from_pretrained()"),i_r=o("class method or the "),p_e=a("code"),d_r=o("from_config()"),c_r=o(`class
method.`),m_r=l(),M0=a("p"),f_r=o("This class cannot be instantiated directly using "),__e=a("code"),g_r=o("__init__()"),h_r=o(" (throws an error)."),u_r=l(),vt=a("div"),m(E0.$$.fragment),p_r=l(),b_e=a("p"),__r=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),b_r=l(),Dc=a("p"),v_r=o(`Note:
Loading a model from its configuration file does `),v_e=a("strong"),T_r=o("not"),F_r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),T_e=a("code"),C_r=o("from_pretrained()"),M_r=o("to load the model weights."),E_r=l(),F_e=a("p"),y_r=o("Examples:"),w_r=l(),m(y0.$$.fragment),A_r=l(),Eo=a("div"),m(w0.$$.fragment),L_r=l(),C_e=a("p"),B_r=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),x_r=l(),Ts=a("p"),k_r=o("The model class to instantiate is selected based on the "),M_e=a("code"),R_r=o("model_type"),S_r=o(` property of the config object (either
passed as an argument or loaded from `),E_e=a("code"),P_r=o("pretrained_model_name_or_path"),$_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),y_e=a("code"),I_r=o("pretrained_model_name_or_path"),D_r=o(":"),j_r=l(),w_e=a("ul"),EM=a("li"),A_e=a("strong"),N_r=o("vision-encoder-decoder"),q_r=o(" \u2014 "),QO=a("a"),G_r=o("TFVisionEncoderDecoderModel"),O_r=o(" (Vision Encoder decoder model)"),X_r=l(),L_e=a("p"),V_r=o("Examples:"),z_r=l(),m(A0.$$.fragment),txe=l(),jc=a("h2"),yM=a("a"),B_e=a("span"),m(L0.$$.fragment),W_r=l(),x_e=a("span"),Q_r=o("TFAutoModelForSpeechSeq2Seq"),axe=l(),Ar=a("div"),m(B0.$$.fragment),H_r=l(),Nc=a("p"),U_r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),k_e=a("code"),J_r=o("from_pretrained()"),Y_r=o("class method or the "),R_e=a("code"),K_r=o("from_config()"),Z_r=o(`class
method.`),ebr=l(),x0=a("p"),obr=o("This class cannot be instantiated directly using "),S_e=a("code"),rbr=o("__init__()"),tbr=o(" (throws an error)."),abr=l(),Tt=a("div"),m(k0.$$.fragment),sbr=l(),P_e=a("p"),nbr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),lbr=l(),qc=a("p"),ibr=o(`Note:
Loading a model from its configuration file does `),$_e=a("strong"),dbr=o("not"),cbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),I_e=a("code"),mbr=o("from_pretrained()"),fbr=o("to load the model weights."),gbr=l(),D_e=a("p"),hbr=o("Examples:"),ubr=l(),m(R0.$$.fragment),pbr=l(),yo=a("div"),m(S0.$$.fragment),_br=l(),j_e=a("p"),bbr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),vbr=l(),Fs=a("p"),Tbr=o("The model class to instantiate is selected based on the "),N_e=a("code"),Fbr=o("model_type"),Cbr=o(` property of the config object (either
passed as an argument or loaded from `),q_e=a("code"),Mbr=o("pretrained_model_name_or_path"),Ebr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),G_e=a("code"),ybr=o("pretrained_model_name_or_path"),wbr=o(":"),Abr=l(),O_e=a("ul"),wM=a("li"),X_e=a("strong"),Lbr=o("speech_to_text"),Bbr=o(" \u2014 "),HO=a("a"),xbr=o("TFSpeech2TextForConditionalGeneration"),kbr=o(" (Speech2Text model)"),Rbr=l(),V_e=a("p"),Sbr=o("Examples:"),Pbr=l(),m(P0.$$.fragment),sxe=l(),Gc=a("h2"),AM=a("a"),z_e=a("span"),m($0.$$.fragment),$br=l(),W_e=a("span"),Ibr=o("FlaxAutoModel"),nxe=l(),Lr=a("div"),m(I0.$$.fragment),Dbr=l(),Oc=a("p"),jbr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Q_e=a("code"),Nbr=o("from_pretrained()"),qbr=o("class method or the "),H_e=a("code"),Gbr=o("from_config()"),Obr=o(`class
method.`),Xbr=l(),D0=a("p"),Vbr=o("This class cannot be instantiated directly using "),U_e=a("code"),zbr=o("__init__()"),Wbr=o(" (throws an error)."),Qbr=l(),Ft=a("div"),m(j0.$$.fragment),Hbr=l(),J_e=a("p"),Ubr=o("Instantiates one of the base model classes of the library from a configuration."),Jbr=l(),Xc=a("p"),Ybr=o(`Note:
Loading a model from its configuration file does `),Y_e=a("strong"),Kbr=o("not"),Zbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),K_e=a("code"),e2r=o("from_pretrained()"),o2r=o("to load the model weights."),r2r=l(),Z_e=a("p"),t2r=o("Examples:"),a2r=l(),m(N0.$$.fragment),s2r=l(),wo=a("div"),m(q0.$$.fragment),n2r=l(),ebe=a("p"),l2r=o("Instantiate one of the base model classes of the library from a pretrained model."),i2r=l(),Cs=a("p"),d2r=o("The model class to instantiate is selected based on the "),obe=a("code"),c2r=o("model_type"),m2r=o(` property of the config object (either
passed as an argument or loaded from `),rbe=a("code"),f2r=o("pretrained_model_name_or_path"),g2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tbe=a("code"),h2r=o("pretrained_model_name_or_path"),u2r=o(":"),p2r=l(),z=a("ul"),LM=a("li"),abe=a("strong"),_2r=o("albert"),b2r=o(" \u2014 "),UO=a("a"),v2r=o("FlaxAlbertModel"),T2r=o(" (ALBERT model)"),F2r=l(),BM=a("li"),sbe=a("strong"),C2r=o("bart"),M2r=o(" \u2014 "),JO=a("a"),E2r=o("FlaxBartModel"),y2r=o(" (BART model)"),w2r=l(),xM=a("li"),nbe=a("strong"),A2r=o("beit"),L2r=o(" \u2014 "),YO=a("a"),B2r=o("FlaxBeitModel"),x2r=o(" (BEiT model)"),k2r=l(),kM=a("li"),lbe=a("strong"),R2r=o("bert"),S2r=o(" \u2014 "),KO=a("a"),P2r=o("FlaxBertModel"),$2r=o(" (BERT model)"),I2r=l(),RM=a("li"),ibe=a("strong"),D2r=o("big_bird"),j2r=o(" \u2014 "),ZO=a("a"),N2r=o("FlaxBigBirdModel"),q2r=o(" (BigBird model)"),G2r=l(),SM=a("li"),dbe=a("strong"),O2r=o("blenderbot"),X2r=o(" \u2014 "),eX=a("a"),V2r=o("FlaxBlenderbotModel"),z2r=o(" (Blenderbot model)"),W2r=l(),PM=a("li"),cbe=a("strong"),Q2r=o("blenderbot-small"),H2r=o(" \u2014 "),oX=a("a"),U2r=o("FlaxBlenderbotSmallModel"),J2r=o(" (BlenderbotSmall model)"),Y2r=l(),$M=a("li"),mbe=a("strong"),K2r=o("clip"),Z2r=o(" \u2014 "),rX=a("a"),evr=o("FlaxCLIPModel"),ovr=o(" (CLIP model)"),rvr=l(),IM=a("li"),fbe=a("strong"),tvr=o("distilbert"),avr=o(" \u2014 "),tX=a("a"),svr=o("FlaxDistilBertModel"),nvr=o(" (DistilBERT model)"),lvr=l(),DM=a("li"),gbe=a("strong"),ivr=o("electra"),dvr=o(" \u2014 "),aX=a("a"),cvr=o("FlaxElectraModel"),mvr=o(" (ELECTRA model)"),fvr=l(),jM=a("li"),hbe=a("strong"),gvr=o("gpt2"),hvr=o(" \u2014 "),sX=a("a"),uvr=o("FlaxGPT2Model"),pvr=o(" (OpenAI GPT-2 model)"),_vr=l(),NM=a("li"),ube=a("strong"),bvr=o("gpt_neo"),vvr=o(" \u2014 "),nX=a("a"),Tvr=o("FlaxGPTNeoModel"),Fvr=o(" (GPT Neo model)"),Cvr=l(),qM=a("li"),pbe=a("strong"),Mvr=o("gptj"),Evr=o(" \u2014 "),lX=a("a"),yvr=o("FlaxGPTJModel"),wvr=o(" (GPT-J model)"),Avr=l(),GM=a("li"),_be=a("strong"),Lvr=o("marian"),Bvr=o(" \u2014 "),iX=a("a"),xvr=o("FlaxMarianModel"),kvr=o(" (Marian model)"),Rvr=l(),OM=a("li"),bbe=a("strong"),Svr=o("mbart"),Pvr=o(" \u2014 "),dX=a("a"),$vr=o("FlaxMBartModel"),Ivr=o(" (mBART model)"),Dvr=l(),XM=a("li"),vbe=a("strong"),jvr=o("mt5"),Nvr=o(" \u2014 "),cX=a("a"),qvr=o("FlaxMT5Model"),Gvr=o(" (mT5 model)"),Ovr=l(),VM=a("li"),Tbe=a("strong"),Xvr=o("pegasus"),Vvr=o(" \u2014 "),mX=a("a"),zvr=o("FlaxPegasusModel"),Wvr=o(" (Pegasus model)"),Qvr=l(),zM=a("li"),Fbe=a("strong"),Hvr=o("roberta"),Uvr=o(" \u2014 "),fX=a("a"),Jvr=o("FlaxRobertaModel"),Yvr=o(" (RoBERTa model)"),Kvr=l(),WM=a("li"),Cbe=a("strong"),Zvr=o("roformer"),eTr=o(" \u2014 "),gX=a("a"),oTr=o("FlaxRoFormerModel"),rTr=o(" (RoFormer model)"),tTr=l(),QM=a("li"),Mbe=a("strong"),aTr=o("t5"),sTr=o(" \u2014 "),hX=a("a"),nTr=o("FlaxT5Model"),lTr=o(" (T5 model)"),iTr=l(),HM=a("li"),Ebe=a("strong"),dTr=o("vision-text-dual-encoder"),cTr=o(" \u2014 "),uX=a("a"),mTr=o("FlaxVisionTextDualEncoderModel"),fTr=o(" (VisionTextDualEncoder model)"),gTr=l(),UM=a("li"),ybe=a("strong"),hTr=o("vit"),uTr=o(" \u2014 "),pX=a("a"),pTr=o("FlaxViTModel"),_Tr=o(" (ViT model)"),bTr=l(),JM=a("li"),wbe=a("strong"),vTr=o("wav2vec2"),TTr=o(" \u2014 "),_X=a("a"),FTr=o("FlaxWav2Vec2Model"),CTr=o(" (Wav2Vec2 model)"),MTr=l(),YM=a("li"),Abe=a("strong"),ETr=o("xglm"),yTr=o(" \u2014 "),bX=a("a"),wTr=o("FlaxXGLMModel"),ATr=o(" (XGLM model)"),LTr=l(),Lbe=a("p"),BTr=o("Examples:"),xTr=l(),m(G0.$$.fragment),lxe=l(),Vc=a("h2"),KM=a("a"),Bbe=a("span"),m(O0.$$.fragment),kTr=l(),xbe=a("span"),RTr=o("FlaxAutoModelForCausalLM"),ixe=l(),Br=a("div"),m(X0.$$.fragment),STr=l(),zc=a("p"),PTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),kbe=a("code"),$Tr=o("from_pretrained()"),ITr=o("class method or the "),Rbe=a("code"),DTr=o("from_config()"),jTr=o(`class
method.`),NTr=l(),V0=a("p"),qTr=o("This class cannot be instantiated directly using "),Sbe=a("code"),GTr=o("__init__()"),OTr=o(" (throws an error)."),XTr=l(),Ct=a("div"),m(z0.$$.fragment),VTr=l(),Pbe=a("p"),zTr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),WTr=l(),Wc=a("p"),QTr=o(`Note:
Loading a model from its configuration file does `),$be=a("strong"),HTr=o("not"),UTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ibe=a("code"),JTr=o("from_pretrained()"),YTr=o("to load the model weights."),KTr=l(),Dbe=a("p"),ZTr=o("Examples:"),e1r=l(),m(W0.$$.fragment),o1r=l(),Ao=a("div"),m(Q0.$$.fragment),r1r=l(),jbe=a("p"),t1r=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),a1r=l(),Ms=a("p"),s1r=o("The model class to instantiate is selected based on the "),Nbe=a("code"),n1r=o("model_type"),l1r=o(` property of the config object (either
passed as an argument or loaded from `),qbe=a("code"),i1r=o("pretrained_model_name_or_path"),d1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gbe=a("code"),c1r=o("pretrained_model_name_or_path"),m1r=o(":"),f1r=l(),Es=a("ul"),ZM=a("li"),Obe=a("strong"),g1r=o("gpt2"),h1r=o(" \u2014 "),vX=a("a"),u1r=o("FlaxGPT2LMHeadModel"),p1r=o(" (OpenAI GPT-2 model)"),_1r=l(),eE=a("li"),Xbe=a("strong"),b1r=o("gpt_neo"),v1r=o(" \u2014 "),TX=a("a"),T1r=o("FlaxGPTNeoForCausalLM"),F1r=o(" (GPT Neo model)"),C1r=l(),oE=a("li"),Vbe=a("strong"),M1r=o("gptj"),E1r=o(" \u2014 "),FX=a("a"),y1r=o("FlaxGPTJForCausalLM"),w1r=o(" (GPT-J model)"),A1r=l(),rE=a("li"),zbe=a("strong"),L1r=o("xglm"),B1r=o(" \u2014 "),CX=a("a"),x1r=o("FlaxXGLMForCausalLM"),k1r=o(" (XGLM model)"),R1r=l(),Wbe=a("p"),S1r=o("Examples:"),P1r=l(),m(H0.$$.fragment),dxe=l(),Qc=a("h2"),tE=a("a"),Qbe=a("span"),m(U0.$$.fragment),$1r=l(),Hbe=a("span"),I1r=o("FlaxAutoModelForPreTraining"),cxe=l(),xr=a("div"),m(J0.$$.fragment),D1r=l(),Hc=a("p"),j1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Ube=a("code"),N1r=o("from_pretrained()"),q1r=o("class method or the "),Jbe=a("code"),G1r=o("from_config()"),O1r=o(`class
method.`),X1r=l(),Y0=a("p"),V1r=o("This class cannot be instantiated directly using "),Ybe=a("code"),z1r=o("__init__()"),W1r=o(" (throws an error)."),Q1r=l(),Mt=a("div"),m(K0.$$.fragment),H1r=l(),Kbe=a("p"),U1r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),J1r=l(),Uc=a("p"),Y1r=o(`Note:
Loading a model from its configuration file does `),Zbe=a("strong"),K1r=o("not"),Z1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),e2e=a("code"),eFr=o("from_pretrained()"),oFr=o("to load the model weights."),rFr=l(),o2e=a("p"),tFr=o("Examples:"),aFr=l(),m(Z0.$$.fragment),sFr=l(),Lo=a("div"),m(eL.$$.fragment),nFr=l(),r2e=a("p"),lFr=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),iFr=l(),ys=a("p"),dFr=o("The model class to instantiate is selected based on the "),t2e=a("code"),cFr=o("model_type"),mFr=o(` property of the config object (either
passed as an argument or loaded from `),a2e=a("code"),fFr=o("pretrained_model_name_or_path"),gFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s2e=a("code"),hFr=o("pretrained_model_name_or_path"),uFr=o(":"),pFr=l(),me=a("ul"),aE=a("li"),n2e=a("strong"),_Fr=o("albert"),bFr=o(" \u2014 "),MX=a("a"),vFr=o("FlaxAlbertForPreTraining"),TFr=o(" (ALBERT model)"),FFr=l(),sE=a("li"),l2e=a("strong"),CFr=o("bart"),MFr=o(" \u2014 "),EX=a("a"),EFr=o("FlaxBartForConditionalGeneration"),yFr=o(" (BART model)"),wFr=l(),nE=a("li"),i2e=a("strong"),AFr=o("bert"),LFr=o(" \u2014 "),yX=a("a"),BFr=o("FlaxBertForPreTraining"),xFr=o(" (BERT model)"),kFr=l(),lE=a("li"),d2e=a("strong"),RFr=o("big_bird"),SFr=o(" \u2014 "),wX=a("a"),PFr=o("FlaxBigBirdForPreTraining"),$Fr=o(" (BigBird model)"),IFr=l(),iE=a("li"),c2e=a("strong"),DFr=o("electra"),jFr=o(" \u2014 "),AX=a("a"),NFr=o("FlaxElectraForPreTraining"),qFr=o(" (ELECTRA model)"),GFr=l(),dE=a("li"),m2e=a("strong"),OFr=o("mbart"),XFr=o(" \u2014 "),LX=a("a"),VFr=o("FlaxMBartForConditionalGeneration"),zFr=o(" (mBART model)"),WFr=l(),cE=a("li"),f2e=a("strong"),QFr=o("mt5"),HFr=o(" \u2014 "),BX=a("a"),UFr=o("FlaxMT5ForConditionalGeneration"),JFr=o(" (mT5 model)"),YFr=l(),mE=a("li"),g2e=a("strong"),KFr=o("roberta"),ZFr=o(" \u2014 "),xX=a("a"),eCr=o("FlaxRobertaForMaskedLM"),oCr=o(" (RoBERTa model)"),rCr=l(),fE=a("li"),h2e=a("strong"),tCr=o("roformer"),aCr=o(" \u2014 "),kX=a("a"),sCr=o("FlaxRoFormerForMaskedLM"),nCr=o(" (RoFormer model)"),lCr=l(),gE=a("li"),u2e=a("strong"),iCr=o("t5"),dCr=o(" \u2014 "),RX=a("a"),cCr=o("FlaxT5ForConditionalGeneration"),mCr=o(" (T5 model)"),fCr=l(),hE=a("li"),p2e=a("strong"),gCr=o("wav2vec2"),hCr=o(" \u2014 "),SX=a("a"),uCr=o("FlaxWav2Vec2ForPreTraining"),pCr=o(" (Wav2Vec2 model)"),_Cr=l(),_2e=a("p"),bCr=o("Examples:"),vCr=l(),m(oL.$$.fragment),mxe=l(),Jc=a("h2"),uE=a("a"),b2e=a("span"),m(rL.$$.fragment),TCr=l(),v2e=a("span"),FCr=o("FlaxAutoModelForMaskedLM"),fxe=l(),kr=a("div"),m(tL.$$.fragment),CCr=l(),Yc=a("p"),MCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),T2e=a("code"),ECr=o("from_pretrained()"),yCr=o("class method or the "),F2e=a("code"),wCr=o("from_config()"),ACr=o(`class
method.`),LCr=l(),aL=a("p"),BCr=o("This class cannot be instantiated directly using "),C2e=a("code"),xCr=o("__init__()"),kCr=o(" (throws an error)."),RCr=l(),Et=a("div"),m(sL.$$.fragment),SCr=l(),M2e=a("p"),PCr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),$Cr=l(),Kc=a("p"),ICr=o(`Note:
Loading a model from its configuration file does `),E2e=a("strong"),DCr=o("not"),jCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),y2e=a("code"),NCr=o("from_pretrained()"),qCr=o("to load the model weights."),GCr=l(),w2e=a("p"),OCr=o("Examples:"),XCr=l(),m(nL.$$.fragment),VCr=l(),Bo=a("div"),m(lL.$$.fragment),zCr=l(),A2e=a("p"),WCr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),QCr=l(),ws=a("p"),HCr=o("The model class to instantiate is selected based on the "),L2e=a("code"),UCr=o("model_type"),JCr=o(` property of the config object (either
passed as an argument or loaded from `),B2e=a("code"),YCr=o("pretrained_model_name_or_path"),KCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x2e=a("code"),ZCr=o("pretrained_model_name_or_path"),e4r=o(":"),o4r=l(),ve=a("ul"),pE=a("li"),k2e=a("strong"),r4r=o("albert"),t4r=o(" \u2014 "),PX=a("a"),a4r=o("FlaxAlbertForMaskedLM"),s4r=o(" (ALBERT model)"),n4r=l(),_E=a("li"),R2e=a("strong"),l4r=o("bart"),i4r=o(" \u2014 "),$X=a("a"),d4r=o("FlaxBartForConditionalGeneration"),c4r=o(" (BART model)"),m4r=l(),bE=a("li"),S2e=a("strong"),f4r=o("bert"),g4r=o(" \u2014 "),IX=a("a"),h4r=o("FlaxBertForMaskedLM"),u4r=o(" (BERT model)"),p4r=l(),vE=a("li"),P2e=a("strong"),_4r=o("big_bird"),b4r=o(" \u2014 "),DX=a("a"),v4r=o("FlaxBigBirdForMaskedLM"),T4r=o(" (BigBird model)"),F4r=l(),TE=a("li"),$2e=a("strong"),C4r=o("distilbert"),M4r=o(" \u2014 "),jX=a("a"),E4r=o("FlaxDistilBertForMaskedLM"),y4r=o(" (DistilBERT model)"),w4r=l(),FE=a("li"),I2e=a("strong"),A4r=o("electra"),L4r=o(" \u2014 "),NX=a("a"),B4r=o("FlaxElectraForMaskedLM"),x4r=o(" (ELECTRA model)"),k4r=l(),CE=a("li"),D2e=a("strong"),R4r=o("mbart"),S4r=o(" \u2014 "),qX=a("a"),P4r=o("FlaxMBartForConditionalGeneration"),$4r=o(" (mBART model)"),I4r=l(),ME=a("li"),j2e=a("strong"),D4r=o("roberta"),j4r=o(" \u2014 "),GX=a("a"),N4r=o("FlaxRobertaForMaskedLM"),q4r=o(" (RoBERTa model)"),G4r=l(),EE=a("li"),N2e=a("strong"),O4r=o("roformer"),X4r=o(" \u2014 "),OX=a("a"),V4r=o("FlaxRoFormerForMaskedLM"),z4r=o(" (RoFormer model)"),W4r=l(),q2e=a("p"),Q4r=o("Examples:"),H4r=l(),m(iL.$$.fragment),gxe=l(),Zc=a("h2"),yE=a("a"),G2e=a("span"),m(dL.$$.fragment),U4r=l(),O2e=a("span"),J4r=o("FlaxAutoModelForSeq2SeqLM"),hxe=l(),Rr=a("div"),m(cL.$$.fragment),Y4r=l(),em=a("p"),K4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),X2e=a("code"),Z4r=o("from_pretrained()"),eMr=o("class method or the "),V2e=a("code"),oMr=o("from_config()"),rMr=o(`class
method.`),tMr=l(),mL=a("p"),aMr=o("This class cannot be instantiated directly using "),z2e=a("code"),sMr=o("__init__()"),nMr=o(" (throws an error)."),lMr=l(),yt=a("div"),m(fL.$$.fragment),iMr=l(),W2e=a("p"),dMr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),cMr=l(),om=a("p"),mMr=o(`Note:
Loading a model from its configuration file does `),Q2e=a("strong"),fMr=o("not"),gMr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),H2e=a("code"),hMr=o("from_pretrained()"),uMr=o("to load the model weights."),pMr=l(),U2e=a("p"),_Mr=o("Examples:"),bMr=l(),m(gL.$$.fragment),vMr=l(),xo=a("div"),m(hL.$$.fragment),TMr=l(),J2e=a("p"),FMr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),CMr=l(),As=a("p"),MMr=o("The model class to instantiate is selected based on the "),Y2e=a("code"),EMr=o("model_type"),yMr=o(` property of the config object (either
passed as an argument or loaded from `),K2e=a("code"),wMr=o("pretrained_model_name_or_path"),AMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Z2e=a("code"),LMr=o("pretrained_model_name_or_path"),BMr=o(":"),xMr=l(),Te=a("ul"),wE=a("li"),eve=a("strong"),kMr=o("bart"),RMr=o(" \u2014 "),XX=a("a"),SMr=o("FlaxBartForConditionalGeneration"),PMr=o(" (BART model)"),$Mr=l(),AE=a("li"),ove=a("strong"),IMr=o("blenderbot"),DMr=o(" \u2014 "),VX=a("a"),jMr=o("FlaxBlenderbotForConditionalGeneration"),NMr=o(" (Blenderbot model)"),qMr=l(),LE=a("li"),rve=a("strong"),GMr=o("blenderbot-small"),OMr=o(" \u2014 "),zX=a("a"),XMr=o("FlaxBlenderbotSmallForConditionalGeneration"),VMr=o(" (BlenderbotSmall model)"),zMr=l(),BE=a("li"),tve=a("strong"),WMr=o("encoder-decoder"),QMr=o(" \u2014 "),WX=a("a"),HMr=o("FlaxEncoderDecoderModel"),UMr=o(" (Encoder decoder model)"),JMr=l(),xE=a("li"),ave=a("strong"),YMr=o("marian"),KMr=o(" \u2014 "),QX=a("a"),ZMr=o("FlaxMarianMTModel"),eEr=o(" (Marian model)"),oEr=l(),kE=a("li"),sve=a("strong"),rEr=o("mbart"),tEr=o(" \u2014 "),HX=a("a"),aEr=o("FlaxMBartForConditionalGeneration"),sEr=o(" (mBART model)"),nEr=l(),RE=a("li"),nve=a("strong"),lEr=o("mt5"),iEr=o(" \u2014 "),UX=a("a"),dEr=o("FlaxMT5ForConditionalGeneration"),cEr=o(" (mT5 model)"),mEr=l(),SE=a("li"),lve=a("strong"),fEr=o("pegasus"),gEr=o(" \u2014 "),JX=a("a"),hEr=o("FlaxPegasusForConditionalGeneration"),uEr=o(" (Pegasus model)"),pEr=l(),PE=a("li"),ive=a("strong"),_Er=o("t5"),bEr=o(" \u2014 "),YX=a("a"),vEr=o("FlaxT5ForConditionalGeneration"),TEr=o(" (T5 model)"),FEr=l(),dve=a("p"),CEr=o("Examples:"),MEr=l(),m(uL.$$.fragment),uxe=l(),rm=a("h2"),$E=a("a"),cve=a("span"),m(pL.$$.fragment),EEr=l(),mve=a("span"),yEr=o("FlaxAutoModelForSequenceClassification"),pxe=l(),Sr=a("div"),m(_L.$$.fragment),wEr=l(),tm=a("p"),AEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),fve=a("code"),LEr=o("from_pretrained()"),BEr=o("class method or the "),gve=a("code"),xEr=o("from_config()"),kEr=o(`class
method.`),REr=l(),bL=a("p"),SEr=o("This class cannot be instantiated directly using "),hve=a("code"),PEr=o("__init__()"),$Er=o(" (throws an error)."),IEr=l(),wt=a("div"),m(vL.$$.fragment),DEr=l(),uve=a("p"),jEr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),NEr=l(),am=a("p"),qEr=o(`Note:
Loading a model from its configuration file does `),pve=a("strong"),GEr=o("not"),OEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ve=a("code"),XEr=o("from_pretrained()"),VEr=o("to load the model weights."),zEr=l(),bve=a("p"),WEr=o("Examples:"),QEr=l(),m(TL.$$.fragment),HEr=l(),ko=a("div"),m(FL.$$.fragment),UEr=l(),vve=a("p"),JEr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),YEr=l(),Ls=a("p"),KEr=o("The model class to instantiate is selected based on the "),Tve=a("code"),ZEr=o("model_type"),e3r=o(` property of the config object (either
passed as an argument or loaded from `),Fve=a("code"),o3r=o("pretrained_model_name_or_path"),r3r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cve=a("code"),t3r=o("pretrained_model_name_or_path"),a3r=o(":"),s3r=l(),Fe=a("ul"),IE=a("li"),Mve=a("strong"),n3r=o("albert"),l3r=o(" \u2014 "),KX=a("a"),i3r=o("FlaxAlbertForSequenceClassification"),d3r=o(" (ALBERT model)"),c3r=l(),DE=a("li"),Eve=a("strong"),m3r=o("bart"),f3r=o(" \u2014 "),ZX=a("a"),g3r=o("FlaxBartForSequenceClassification"),h3r=o(" (BART model)"),u3r=l(),jE=a("li"),yve=a("strong"),p3r=o("bert"),_3r=o(" \u2014 "),eV=a("a"),b3r=o("FlaxBertForSequenceClassification"),v3r=o(" (BERT model)"),T3r=l(),NE=a("li"),wve=a("strong"),F3r=o("big_bird"),C3r=o(" \u2014 "),oV=a("a"),M3r=o("FlaxBigBirdForSequenceClassification"),E3r=o(" (BigBird model)"),y3r=l(),qE=a("li"),Ave=a("strong"),w3r=o("distilbert"),A3r=o(" \u2014 "),rV=a("a"),L3r=o("FlaxDistilBertForSequenceClassification"),B3r=o(" (DistilBERT model)"),x3r=l(),GE=a("li"),Lve=a("strong"),k3r=o("electra"),R3r=o(" \u2014 "),tV=a("a"),S3r=o("FlaxElectraForSequenceClassification"),P3r=o(" (ELECTRA model)"),$3r=l(),OE=a("li"),Bve=a("strong"),I3r=o("mbart"),D3r=o(" \u2014 "),aV=a("a"),j3r=o("FlaxMBartForSequenceClassification"),N3r=o(" (mBART model)"),q3r=l(),XE=a("li"),xve=a("strong"),G3r=o("roberta"),O3r=o(" \u2014 "),sV=a("a"),X3r=o("FlaxRobertaForSequenceClassification"),V3r=o(" (RoBERTa model)"),z3r=l(),VE=a("li"),kve=a("strong"),W3r=o("roformer"),Q3r=o(" \u2014 "),nV=a("a"),H3r=o("FlaxRoFormerForSequenceClassification"),U3r=o(" (RoFormer model)"),J3r=l(),Rve=a("p"),Y3r=o("Examples:"),K3r=l(),m(CL.$$.fragment),_xe=l(),sm=a("h2"),zE=a("a"),Sve=a("span"),m(ML.$$.fragment),Z3r=l(),Pve=a("span"),e5r=o("FlaxAutoModelForQuestionAnswering"),bxe=l(),Pr=a("div"),m(EL.$$.fragment),o5r=l(),nm=a("p"),r5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),$ve=a("code"),t5r=o("from_pretrained()"),a5r=o("class method or the "),Ive=a("code"),s5r=o("from_config()"),n5r=o(`class
method.`),l5r=l(),yL=a("p"),i5r=o("This class cannot be instantiated directly using "),Dve=a("code"),d5r=o("__init__()"),c5r=o(" (throws an error)."),m5r=l(),At=a("div"),m(wL.$$.fragment),f5r=l(),jve=a("p"),g5r=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),h5r=l(),lm=a("p"),u5r=o(`Note:
Loading a model from its configuration file does `),Nve=a("strong"),p5r=o("not"),_5r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qve=a("code"),b5r=o("from_pretrained()"),v5r=o("to load the model weights."),T5r=l(),Gve=a("p"),F5r=o("Examples:"),C5r=l(),m(AL.$$.fragment),M5r=l(),Ro=a("div"),m(LL.$$.fragment),E5r=l(),Ove=a("p"),y5r=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),w5r=l(),Bs=a("p"),A5r=o("The model class to instantiate is selected based on the "),Xve=a("code"),L5r=o("model_type"),B5r=o(` property of the config object (either
passed as an argument or loaded from `),Vve=a("code"),x5r=o("pretrained_model_name_or_path"),k5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zve=a("code"),R5r=o("pretrained_model_name_or_path"),S5r=o(":"),P5r=l(),Ce=a("ul"),WE=a("li"),Wve=a("strong"),$5r=o("albert"),I5r=o(" \u2014 "),lV=a("a"),D5r=o("FlaxAlbertForQuestionAnswering"),j5r=o(" (ALBERT model)"),N5r=l(),QE=a("li"),Qve=a("strong"),q5r=o("bart"),G5r=o(" \u2014 "),iV=a("a"),O5r=o("FlaxBartForQuestionAnswering"),X5r=o(" (BART model)"),V5r=l(),HE=a("li"),Hve=a("strong"),z5r=o("bert"),W5r=o(" \u2014 "),dV=a("a"),Q5r=o("FlaxBertForQuestionAnswering"),H5r=o(" (BERT model)"),U5r=l(),UE=a("li"),Uve=a("strong"),J5r=o("big_bird"),Y5r=o(" \u2014 "),cV=a("a"),K5r=o("FlaxBigBirdForQuestionAnswering"),Z5r=o(" (BigBird model)"),eyr=l(),JE=a("li"),Jve=a("strong"),oyr=o("distilbert"),ryr=o(" \u2014 "),mV=a("a"),tyr=o("FlaxDistilBertForQuestionAnswering"),ayr=o(" (DistilBERT model)"),syr=l(),YE=a("li"),Yve=a("strong"),nyr=o("electra"),lyr=o(" \u2014 "),fV=a("a"),iyr=o("FlaxElectraForQuestionAnswering"),dyr=o(" (ELECTRA model)"),cyr=l(),KE=a("li"),Kve=a("strong"),myr=o("mbart"),fyr=o(" \u2014 "),gV=a("a"),gyr=o("FlaxMBartForQuestionAnswering"),hyr=o(" (mBART model)"),uyr=l(),ZE=a("li"),Zve=a("strong"),pyr=o("roberta"),_yr=o(" \u2014 "),hV=a("a"),byr=o("FlaxRobertaForQuestionAnswering"),vyr=o(" (RoBERTa model)"),Tyr=l(),e3=a("li"),eTe=a("strong"),Fyr=o("roformer"),Cyr=o(" \u2014 "),uV=a("a"),Myr=o("FlaxRoFormerForQuestionAnswering"),Eyr=o(" (RoFormer model)"),yyr=l(),oTe=a("p"),wyr=o("Examples:"),Ayr=l(),m(BL.$$.fragment),vxe=l(),im=a("h2"),o3=a("a"),rTe=a("span"),m(xL.$$.fragment),Lyr=l(),tTe=a("span"),Byr=o("FlaxAutoModelForTokenClassification"),Txe=l(),$r=a("div"),m(kL.$$.fragment),xyr=l(),dm=a("p"),kyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),aTe=a("code"),Ryr=o("from_pretrained()"),Syr=o("class method or the "),sTe=a("code"),Pyr=o("from_config()"),$yr=o(`class
method.`),Iyr=l(),RL=a("p"),Dyr=o("This class cannot be instantiated directly using "),nTe=a("code"),jyr=o("__init__()"),Nyr=o(" (throws an error)."),qyr=l(),Lt=a("div"),m(SL.$$.fragment),Gyr=l(),lTe=a("p"),Oyr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Xyr=l(),cm=a("p"),Vyr=o(`Note:
Loading a model from its configuration file does `),iTe=a("strong"),zyr=o("not"),Wyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dTe=a("code"),Qyr=o("from_pretrained()"),Hyr=o("to load the model weights."),Uyr=l(),cTe=a("p"),Jyr=o("Examples:"),Yyr=l(),m(PL.$$.fragment),Kyr=l(),So=a("div"),m($L.$$.fragment),Zyr=l(),mTe=a("p"),ewr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),owr=l(),xs=a("p"),rwr=o("The model class to instantiate is selected based on the "),fTe=a("code"),twr=o("model_type"),awr=o(` property of the config object (either
passed as an argument or loaded from `),gTe=a("code"),swr=o("pretrained_model_name_or_path"),nwr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hTe=a("code"),lwr=o("pretrained_model_name_or_path"),iwr=o(":"),dwr=l(),no=a("ul"),r3=a("li"),uTe=a("strong"),cwr=o("albert"),mwr=o(" \u2014 "),pV=a("a"),fwr=o("FlaxAlbertForTokenClassification"),gwr=o(" (ALBERT model)"),hwr=l(),t3=a("li"),pTe=a("strong"),uwr=o("bert"),pwr=o(" \u2014 "),_V=a("a"),_wr=o("FlaxBertForTokenClassification"),bwr=o(" (BERT model)"),vwr=l(),a3=a("li"),_Te=a("strong"),Twr=o("big_bird"),Fwr=o(" \u2014 "),bV=a("a"),Cwr=o("FlaxBigBirdForTokenClassification"),Mwr=o(" (BigBird model)"),Ewr=l(),s3=a("li"),bTe=a("strong"),ywr=o("distilbert"),wwr=o(" \u2014 "),vV=a("a"),Awr=o("FlaxDistilBertForTokenClassification"),Lwr=o(" (DistilBERT model)"),Bwr=l(),n3=a("li"),vTe=a("strong"),xwr=o("electra"),kwr=o(" \u2014 "),TV=a("a"),Rwr=o("FlaxElectraForTokenClassification"),Swr=o(" (ELECTRA model)"),Pwr=l(),l3=a("li"),TTe=a("strong"),$wr=o("roberta"),Iwr=o(" \u2014 "),FV=a("a"),Dwr=o("FlaxRobertaForTokenClassification"),jwr=o(" (RoBERTa model)"),Nwr=l(),i3=a("li"),FTe=a("strong"),qwr=o("roformer"),Gwr=o(" \u2014 "),CV=a("a"),Owr=o("FlaxRoFormerForTokenClassification"),Xwr=o(" (RoFormer model)"),Vwr=l(),CTe=a("p"),zwr=o("Examples:"),Wwr=l(),m(IL.$$.fragment),Fxe=l(),mm=a("h2"),d3=a("a"),MTe=a("span"),m(DL.$$.fragment),Qwr=l(),ETe=a("span"),Hwr=o("FlaxAutoModelForMultipleChoice"),Cxe=l(),Ir=a("div"),m(jL.$$.fragment),Uwr=l(),fm=a("p"),Jwr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),yTe=a("code"),Ywr=o("from_pretrained()"),Kwr=o("class method or the "),wTe=a("code"),Zwr=o("from_config()"),e6r=o(`class
method.`),o6r=l(),NL=a("p"),r6r=o("This class cannot be instantiated directly using "),ATe=a("code"),t6r=o("__init__()"),a6r=o(" (throws an error)."),s6r=l(),Bt=a("div"),m(qL.$$.fragment),n6r=l(),LTe=a("p"),l6r=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),i6r=l(),gm=a("p"),d6r=o(`Note:
Loading a model from its configuration file does `),BTe=a("strong"),c6r=o("not"),m6r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xTe=a("code"),f6r=o("from_pretrained()"),g6r=o("to load the model weights."),h6r=l(),kTe=a("p"),u6r=o("Examples:"),p6r=l(),m(GL.$$.fragment),_6r=l(),Po=a("div"),m(OL.$$.fragment),b6r=l(),RTe=a("p"),v6r=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),T6r=l(),ks=a("p"),F6r=o("The model class to instantiate is selected based on the "),STe=a("code"),C6r=o("model_type"),M6r=o(` property of the config object (either
passed as an argument or loaded from `),PTe=a("code"),E6r=o("pretrained_model_name_or_path"),y6r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$Te=a("code"),w6r=o("pretrained_model_name_or_path"),A6r=o(":"),L6r=l(),lo=a("ul"),c3=a("li"),ITe=a("strong"),B6r=o("albert"),x6r=o(" \u2014 "),MV=a("a"),k6r=o("FlaxAlbertForMultipleChoice"),R6r=o(" (ALBERT model)"),S6r=l(),m3=a("li"),DTe=a("strong"),P6r=o("bert"),$6r=o(" \u2014 "),EV=a("a"),I6r=o("FlaxBertForMultipleChoice"),D6r=o(" (BERT model)"),j6r=l(),f3=a("li"),jTe=a("strong"),N6r=o("big_bird"),q6r=o(" \u2014 "),yV=a("a"),G6r=o("FlaxBigBirdForMultipleChoice"),O6r=o(" (BigBird model)"),X6r=l(),g3=a("li"),NTe=a("strong"),V6r=o("distilbert"),z6r=o(" \u2014 "),wV=a("a"),W6r=o("FlaxDistilBertForMultipleChoice"),Q6r=o(" (DistilBERT model)"),H6r=l(),h3=a("li"),qTe=a("strong"),U6r=o("electra"),J6r=o(" \u2014 "),AV=a("a"),Y6r=o("FlaxElectraForMultipleChoice"),K6r=o(" (ELECTRA model)"),Z6r=l(),u3=a("li"),GTe=a("strong"),eAr=o("roberta"),oAr=o(" \u2014 "),LV=a("a"),rAr=o("FlaxRobertaForMultipleChoice"),tAr=o(" (RoBERTa model)"),aAr=l(),p3=a("li"),OTe=a("strong"),sAr=o("roformer"),nAr=o(" \u2014 "),BV=a("a"),lAr=o("FlaxRoFormerForMultipleChoice"),iAr=o(" (RoFormer model)"),dAr=l(),XTe=a("p"),cAr=o("Examples:"),mAr=l(),m(XL.$$.fragment),Mxe=l(),hm=a("h2"),_3=a("a"),VTe=a("span"),m(VL.$$.fragment),fAr=l(),zTe=a("span"),gAr=o("FlaxAutoModelForNextSentencePrediction"),Exe=l(),Dr=a("div"),m(zL.$$.fragment),hAr=l(),um=a("p"),uAr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),WTe=a("code"),pAr=o("from_pretrained()"),_Ar=o("class method or the "),QTe=a("code"),bAr=o("from_config()"),vAr=o(`class
method.`),TAr=l(),WL=a("p"),FAr=o("This class cannot be instantiated directly using "),HTe=a("code"),CAr=o("__init__()"),MAr=o(" (throws an error)."),EAr=l(),xt=a("div"),m(QL.$$.fragment),yAr=l(),UTe=a("p"),wAr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),AAr=l(),pm=a("p"),LAr=o(`Note:
Loading a model from its configuration file does `),JTe=a("strong"),BAr=o("not"),xAr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),YTe=a("code"),kAr=o("from_pretrained()"),RAr=o("to load the model weights."),SAr=l(),KTe=a("p"),PAr=o("Examples:"),$Ar=l(),m(HL.$$.fragment),IAr=l(),$o=a("div"),m(UL.$$.fragment),DAr=l(),ZTe=a("p"),jAr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),NAr=l(),Rs=a("p"),qAr=o("The model class to instantiate is selected based on the "),e1e=a("code"),GAr=o("model_type"),OAr=o(` property of the config object (either
passed as an argument or loaded from `),o1e=a("code"),XAr=o("pretrained_model_name_or_path"),VAr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r1e=a("code"),zAr=o("pretrained_model_name_or_path"),WAr=o(":"),QAr=l(),t1e=a("ul"),b3=a("li"),a1e=a("strong"),HAr=o("bert"),UAr=o(" \u2014 "),xV=a("a"),JAr=o("FlaxBertForNextSentencePrediction"),YAr=o(" (BERT model)"),KAr=l(),s1e=a("p"),ZAr=o("Examples:"),e0r=l(),m(JL.$$.fragment),yxe=l(),_m=a("h2"),v3=a("a"),n1e=a("span"),m(YL.$$.fragment),o0r=l(),l1e=a("span"),r0r=o("FlaxAutoModelForImageClassification"),wxe=l(),jr=a("div"),m(KL.$$.fragment),t0r=l(),bm=a("p"),a0r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),i1e=a("code"),s0r=o("from_pretrained()"),n0r=o("class method or the "),d1e=a("code"),l0r=o("from_config()"),i0r=o(`class
method.`),d0r=l(),ZL=a("p"),c0r=o("This class cannot be instantiated directly using "),c1e=a("code"),m0r=o("__init__()"),f0r=o(" (throws an error)."),g0r=l(),kt=a("div"),m(e8.$$.fragment),h0r=l(),m1e=a("p"),u0r=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),p0r=l(),vm=a("p"),_0r=o(`Note:
Loading a model from its configuration file does `),f1e=a("strong"),b0r=o("not"),v0r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),g1e=a("code"),T0r=o("from_pretrained()"),F0r=o("to load the model weights."),C0r=l(),h1e=a("p"),M0r=o("Examples:"),E0r=l(),m(o8.$$.fragment),y0r=l(),Io=a("div"),m(r8.$$.fragment),w0r=l(),u1e=a("p"),A0r=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),L0r=l(),Ss=a("p"),B0r=o("The model class to instantiate is selected based on the "),p1e=a("code"),x0r=o("model_type"),k0r=o(` property of the config object (either
passed as an argument or loaded from `),_1e=a("code"),R0r=o("pretrained_model_name_or_path"),S0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),b1e=a("code"),P0r=o("pretrained_model_name_or_path"),$0r=o(":"),I0r=l(),t8=a("ul"),T3=a("li"),v1e=a("strong"),D0r=o("beit"),j0r=o(" \u2014 "),kV=a("a"),N0r=o("FlaxBeitForImageClassification"),q0r=o(" (BEiT model)"),G0r=l(),F3=a("li"),T1e=a("strong"),O0r=o("vit"),X0r=o(" \u2014 "),RV=a("a"),V0r=o("FlaxViTForImageClassification"),z0r=o(" (ViT model)"),W0r=l(),F1e=a("p"),Q0r=o("Examples:"),H0r=l(),m(a8.$$.fragment),Axe=l(),Tm=a("h2"),C3=a("a"),C1e=a("span"),m(s8.$$.fragment),U0r=l(),M1e=a("span"),J0r=o("FlaxAutoModelForVision2Seq"),Lxe=l(),Nr=a("div"),m(n8.$$.fragment),Y0r=l(),Fm=a("p"),K0r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),E1e=a("code"),Z0r=o("from_pretrained()"),eLr=o("class method or the "),y1e=a("code"),oLr=o("from_config()"),rLr=o(`class
method.`),tLr=l(),l8=a("p"),aLr=o("This class cannot be instantiated directly using "),w1e=a("code"),sLr=o("__init__()"),nLr=o(" (throws an error)."),lLr=l(),Rt=a("div"),m(i8.$$.fragment),iLr=l(),A1e=a("p"),dLr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),cLr=l(),Cm=a("p"),mLr=o(`Note:
Loading a model from its configuration file does `),L1e=a("strong"),fLr=o("not"),gLr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),B1e=a("code"),hLr=o("from_pretrained()"),uLr=o("to load the model weights."),pLr=l(),x1e=a("p"),_Lr=o("Examples:"),bLr=l(),m(d8.$$.fragment),vLr=l(),Do=a("div"),m(c8.$$.fragment),TLr=l(),k1e=a("p"),FLr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),CLr=l(),Ps=a("p"),MLr=o("The model class to instantiate is selected based on the "),R1e=a("code"),ELr=o("model_type"),yLr=o(` property of the config object (either
passed as an argument or loaded from `),S1e=a("code"),wLr=o("pretrained_model_name_or_path"),ALr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P1e=a("code"),LLr=o("pretrained_model_name_or_path"),BLr=o(":"),xLr=l(),$1e=a("ul"),M3=a("li"),I1e=a("strong"),kLr=o("vision-encoder-decoder"),RLr=o(" \u2014 "),SV=a("a"),SLr=o("FlaxVisionEncoderDecoderModel"),PLr=o(" (Vision Encoder decoder model)"),$Lr=l(),D1e=a("p"),ILr=o("Examples:"),DLr=l(),m(m8.$$.fragment),this.h()},l(c){const _=Lvt('[data-svelte="svelte-1phssyn"]',document.head);J=s(_,"META",{name:!0,content:!0}),_.forEach(t),Be=i(c),ie=s(c,"H1",{class:!0});var f8=n(ie);fe=s(f8,"A",{id:!0,class:!0,href:!0});var j1e=n(fe);so=s(j1e,"SPAN",{});var N1e=n(so);f(ce.$$.fragment,N1e),N1e.forEach(t),j1e.forEach(t),_e=i(f8),Go=s(f8,"SPAN",{});var NLr=n(Go);Bi=r(NLr,"Auto Classes"),NLr.forEach(t),f8.forEach(t),Em=i(c),na=s(c,"P",{});var xxe=n(na);xi=r(xxe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),ki=s(xxe,"CODE",{});var qLr=n(ki);M5=r(qLr,"from_pretrained()"),qLr.forEach(t),ym=r(xxe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),xxe.forEach(t),ye=i(c),io=s(c,"P",{});var E3=n(io);Ri=r(E3,"Instantiating one of "),$s=s(E3,"A",{href:!0});var GLr=n($s);E5=r(GLr,"AutoConfig"),GLr.forEach(t),Is=r(E3,", "),Ds=s(E3,"A",{href:!0});var OLr=n(Ds);y5=r(OLr,"AutoModel"),OLr.forEach(t),Si=r(E3,`, and
`),js=s(E3,"A",{href:!0});var XLr=n(js);w5=r(XLr,"AutoTokenizer"),XLr.forEach(t),Pi=r(E3," will directly create a class of the relevant architecture. For instance"),E3.forEach(t),wm=i(c),f($a.$$.fragment,c),co=i(c),ge=s(c,"P",{});var kxe=n(ge);d7=r(kxe,"will create a model that is an instance of "),$i=s(kxe,"A",{href:!0});var VLr=n($i);c7=r(VLr,"BertModel"),VLr.forEach(t),m7=r(kxe,"."),kxe.forEach(t),Oo=i(c),Ia=s(c,"P",{});var Rxe=n(Ia);f7=r(Rxe,"There is one class of "),Am=s(Rxe,"CODE",{});var zLr=n(Am);g7=r(zLr,"AutoModel"),zLr.forEach(t),qRe=r(Rxe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),Rxe.forEach(t),x9e=i(c),Ii=s(c,"H2",{class:!0});var Sxe=n(Ii);Lm=s(Sxe,"A",{id:!0,class:!0,href:!0});var WLr=n(Lm);EW=s(WLr,"SPAN",{});var QLr=n(EW);f(A5.$$.fragment,QLr),QLr.forEach(t),WLr.forEach(t),GRe=i(Sxe),yW=s(Sxe,"SPAN",{});var HLr=n(yW);ORe=r(HLr,"Extending the Auto Classes"),HLr.forEach(t),Sxe.forEach(t),k9e=i(c),Ns=s(c,"P",{});var PV=n(Ns);XRe=r(PV,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),wW=s(PV,"CODE",{});var ULr=n(wW);VRe=r(ULr,"NewModel"),ULr.forEach(t),zRe=r(PV,", make sure you have a "),AW=s(PV,"CODE",{});var JLr=n(AW);WRe=r(JLr,"NewModelConfig"),JLr.forEach(t),QRe=r(PV,` then you can add those to the auto
classes like this:`),PV.forEach(t),R9e=i(c),f(L5.$$.fragment,c),S9e=i(c),h7=s(c,"P",{});var YLr=n(h7);HRe=r(YLr,"You will then be able to use the auto classes like you would usually do!"),YLr.forEach(t),P9e=i(c),f(Bm.$$.fragment,c),$9e=i(c),Di=s(c,"H2",{class:!0});var Pxe=n(Di);xm=s(Pxe,"A",{id:!0,class:!0,href:!0});var KLr=n(xm);LW=s(KLr,"SPAN",{});var ZLr=n(LW);f(B5.$$.fragment,ZLr),ZLr.forEach(t),KLr.forEach(t),URe=i(Pxe),BW=s(Pxe,"SPAN",{});var e8r=n(BW);JRe=r(e8r,"AutoConfig"),e8r.forEach(t),Pxe.forEach(t),I9e=i(c),Xo=s(c,"DIV",{class:!0});var Dn=n(Xo);f(x5.$$.fragment,Dn),YRe=i(Dn),k5=s(Dn,"P",{});var $xe=n(k5);KRe=r($xe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),u7=s($xe,"A",{href:!0});var o8r=n(u7);ZRe=r(o8r,"from_pretrained()"),o8r.forEach(t),eSe=r($xe," class method."),$xe.forEach(t),oSe=i(Dn),R5=s(Dn,"P",{});var Ixe=n(R5);rSe=r(Ixe,"This class cannot be instantiated directly using "),xW=s(Ixe,"CODE",{});var r8r=n(xW);tSe=r(r8r,"__init__()"),r8r.forEach(t),aSe=r(Ixe," (throws an error)."),Ixe.forEach(t),sSe=i(Dn),mo=s(Dn,"DIV",{class:!0});var ia=n(mo);f(S5.$$.fragment,ia),nSe=i(ia),kW=s(ia,"P",{});var t8r=n(kW);lSe=r(t8r,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),t8r.forEach(t),iSe=i(ia),ji=s(ia,"P",{});var $V=n(ji);dSe=r($V,"The configuration class to instantiate is selected based on the "),RW=s($V,"CODE",{});var a8r=n(RW);cSe=r(a8r,"model_type"),a8r.forEach(t),mSe=r($V,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),SW=s($V,"CODE",{});var s8r=n(SW);fSe=r(s8r,"pretrained_model_name_or_path"),s8r.forEach(t),gSe=r($V,":"),$V.forEach(t),hSe=i(ia),v=s(ia,"UL",{});var T=n(v);km=s(T,"LI",{});var q1e=n(km);PW=s(q1e,"STRONG",{});var n8r=n(PW);uSe=r(n8r,"albert"),n8r.forEach(t),pSe=r(q1e," \u2014 "),p7=s(q1e,"A",{href:!0});var l8r=n(p7);_Se=r(l8r,"AlbertConfig"),l8r.forEach(t),bSe=r(q1e," (ALBERT model)"),q1e.forEach(t),vSe=i(T),Rm=s(T,"LI",{});var G1e=n(Rm);$W=s(G1e,"STRONG",{});var i8r=n($W);TSe=r(i8r,"bart"),i8r.forEach(t),FSe=r(G1e," \u2014 "),_7=s(G1e,"A",{href:!0});var d8r=n(_7);CSe=r(d8r,"BartConfig"),d8r.forEach(t),MSe=r(G1e," (BART model)"),G1e.forEach(t),ESe=i(T),Sm=s(T,"LI",{});var O1e=n(Sm);IW=s(O1e,"STRONG",{});var c8r=n(IW);ySe=r(c8r,"beit"),c8r.forEach(t),wSe=r(O1e," \u2014 "),b7=s(O1e,"A",{href:!0});var m8r=n(b7);ASe=r(m8r,"BeitConfig"),m8r.forEach(t),LSe=r(O1e," (BEiT model)"),O1e.forEach(t),BSe=i(T),Pm=s(T,"LI",{});var X1e=n(Pm);DW=s(X1e,"STRONG",{});var f8r=n(DW);xSe=r(f8r,"bert"),f8r.forEach(t),kSe=r(X1e," \u2014 "),v7=s(X1e,"A",{href:!0});var g8r=n(v7);RSe=r(g8r,"BertConfig"),g8r.forEach(t),SSe=r(X1e," (BERT model)"),X1e.forEach(t),PSe=i(T),$m=s(T,"LI",{});var V1e=n($m);jW=s(V1e,"STRONG",{});var h8r=n(jW);$Se=r(h8r,"bert-generation"),h8r.forEach(t),ISe=r(V1e," \u2014 "),T7=s(V1e,"A",{href:!0});var u8r=n(T7);DSe=r(u8r,"BertGenerationConfig"),u8r.forEach(t),jSe=r(V1e," (Bert Generation model)"),V1e.forEach(t),NSe=i(T),Im=s(T,"LI",{});var z1e=n(Im);NW=s(z1e,"STRONG",{});var p8r=n(NW);qSe=r(p8r,"big_bird"),p8r.forEach(t),GSe=r(z1e," \u2014 "),F7=s(z1e,"A",{href:!0});var _8r=n(F7);OSe=r(_8r,"BigBirdConfig"),_8r.forEach(t),XSe=r(z1e," (BigBird model)"),z1e.forEach(t),VSe=i(T),Dm=s(T,"LI",{});var W1e=n(Dm);qW=s(W1e,"STRONG",{});var b8r=n(qW);zSe=r(b8r,"bigbird_pegasus"),b8r.forEach(t),WSe=r(W1e," \u2014 "),C7=s(W1e,"A",{href:!0});var v8r=n(C7);QSe=r(v8r,"BigBirdPegasusConfig"),v8r.forEach(t),HSe=r(W1e," (BigBirdPegasus model)"),W1e.forEach(t),USe=i(T),jm=s(T,"LI",{});var Q1e=n(jm);GW=s(Q1e,"STRONG",{});var T8r=n(GW);JSe=r(T8r,"blenderbot"),T8r.forEach(t),YSe=r(Q1e," \u2014 "),M7=s(Q1e,"A",{href:!0});var F8r=n(M7);KSe=r(F8r,"BlenderbotConfig"),F8r.forEach(t),ZSe=r(Q1e," (Blenderbot model)"),Q1e.forEach(t),ePe=i(T),Nm=s(T,"LI",{});var H1e=n(Nm);OW=s(H1e,"STRONG",{});var C8r=n(OW);oPe=r(C8r,"blenderbot-small"),C8r.forEach(t),rPe=r(H1e," \u2014 "),E7=s(H1e,"A",{href:!0});var M8r=n(E7);tPe=r(M8r,"BlenderbotSmallConfig"),M8r.forEach(t),aPe=r(H1e," (BlenderbotSmall model)"),H1e.forEach(t),sPe=i(T),qm=s(T,"LI",{});var U1e=n(qm);XW=s(U1e,"STRONG",{});var E8r=n(XW);nPe=r(E8r,"camembert"),E8r.forEach(t),lPe=r(U1e," \u2014 "),y7=s(U1e,"A",{href:!0});var y8r=n(y7);iPe=r(y8r,"CamembertConfig"),y8r.forEach(t),dPe=r(U1e," (CamemBERT model)"),U1e.forEach(t),cPe=i(T),Gm=s(T,"LI",{});var J1e=n(Gm);VW=s(J1e,"STRONG",{});var w8r=n(VW);mPe=r(w8r,"canine"),w8r.forEach(t),fPe=r(J1e," \u2014 "),w7=s(J1e,"A",{href:!0});var A8r=n(w7);gPe=r(A8r,"CanineConfig"),A8r.forEach(t),hPe=r(J1e," (Canine model)"),J1e.forEach(t),uPe=i(T),Om=s(T,"LI",{});var Y1e=n(Om);zW=s(Y1e,"STRONG",{});var L8r=n(zW);pPe=r(L8r,"clip"),L8r.forEach(t),_Pe=r(Y1e," \u2014 "),A7=s(Y1e,"A",{href:!0});var B8r=n(A7);bPe=r(B8r,"CLIPConfig"),B8r.forEach(t),vPe=r(Y1e," (CLIP model)"),Y1e.forEach(t),TPe=i(T),Xm=s(T,"LI",{});var K1e=n(Xm);WW=s(K1e,"STRONG",{});var x8r=n(WW);FPe=r(x8r,"convbert"),x8r.forEach(t),CPe=r(K1e," \u2014 "),L7=s(K1e,"A",{href:!0});var k8r=n(L7);MPe=r(k8r,"ConvBertConfig"),k8r.forEach(t),EPe=r(K1e," (ConvBERT model)"),K1e.forEach(t),yPe=i(T),Vm=s(T,"LI",{});var Z1e=n(Vm);QW=s(Z1e,"STRONG",{});var R8r=n(QW);wPe=r(R8r,"convnext"),R8r.forEach(t),APe=r(Z1e," \u2014 "),B7=s(Z1e,"A",{href:!0});var S8r=n(B7);LPe=r(S8r,"ConvNextConfig"),S8r.forEach(t),BPe=r(Z1e," (ConvNext model)"),Z1e.forEach(t),xPe=i(T),zm=s(T,"LI",{});var eFe=n(zm);HW=s(eFe,"STRONG",{});var P8r=n(HW);kPe=r(P8r,"ctrl"),P8r.forEach(t),RPe=r(eFe," \u2014 "),x7=s(eFe,"A",{href:!0});var $8r=n(x7);SPe=r($8r,"CTRLConfig"),$8r.forEach(t),PPe=r(eFe," (CTRL model)"),eFe.forEach(t),$Pe=i(T),Wm=s(T,"LI",{});var oFe=n(Wm);UW=s(oFe,"STRONG",{});var I8r=n(UW);IPe=r(I8r,"data2vec-audio"),I8r.forEach(t),DPe=r(oFe," \u2014 "),k7=s(oFe,"A",{href:!0});var D8r=n(k7);jPe=r(D8r,"Data2VecAudioConfig"),D8r.forEach(t),NPe=r(oFe," (Data2VecAudio model)"),oFe.forEach(t),qPe=i(T),Qm=s(T,"LI",{});var rFe=n(Qm);JW=s(rFe,"STRONG",{});var j8r=n(JW);GPe=r(j8r,"data2vec-text"),j8r.forEach(t),OPe=r(rFe," \u2014 "),R7=s(rFe,"A",{href:!0});var N8r=n(R7);XPe=r(N8r,"Data2VecTextConfig"),N8r.forEach(t),VPe=r(rFe," (Data2VecText model)"),rFe.forEach(t),zPe=i(T),Hm=s(T,"LI",{});var tFe=n(Hm);YW=s(tFe,"STRONG",{});var q8r=n(YW);WPe=r(q8r,"deberta"),q8r.forEach(t),QPe=r(tFe," \u2014 "),S7=s(tFe,"A",{href:!0});var G8r=n(S7);HPe=r(G8r,"DebertaConfig"),G8r.forEach(t),UPe=r(tFe," (DeBERTa model)"),tFe.forEach(t),JPe=i(T),Um=s(T,"LI",{});var aFe=n(Um);KW=s(aFe,"STRONG",{});var O8r=n(KW);YPe=r(O8r,"deberta-v2"),O8r.forEach(t),KPe=r(aFe," \u2014 "),P7=s(aFe,"A",{href:!0});var X8r=n(P7);ZPe=r(X8r,"DebertaV2Config"),X8r.forEach(t),e$e=r(aFe," (DeBERTa-v2 model)"),aFe.forEach(t),o$e=i(T),Jm=s(T,"LI",{});var sFe=n(Jm);ZW=s(sFe,"STRONG",{});var V8r=n(ZW);r$e=r(V8r,"deit"),V8r.forEach(t),t$e=r(sFe," \u2014 "),$7=s(sFe,"A",{href:!0});var z8r=n($7);a$e=r(z8r,"DeiTConfig"),z8r.forEach(t),s$e=r(sFe," (DeiT model)"),sFe.forEach(t),n$e=i(T),Ym=s(T,"LI",{});var nFe=n(Ym);eQ=s(nFe,"STRONG",{});var W8r=n(eQ);l$e=r(W8r,"detr"),W8r.forEach(t),i$e=r(nFe," \u2014 "),I7=s(nFe,"A",{href:!0});var Q8r=n(I7);d$e=r(Q8r,"DetrConfig"),Q8r.forEach(t),c$e=r(nFe," (DETR model)"),nFe.forEach(t),m$e=i(T),Km=s(T,"LI",{});var lFe=n(Km);oQ=s(lFe,"STRONG",{});var H8r=n(oQ);f$e=r(H8r,"distilbert"),H8r.forEach(t),g$e=r(lFe," \u2014 "),D7=s(lFe,"A",{href:!0});var U8r=n(D7);h$e=r(U8r,"DistilBertConfig"),U8r.forEach(t),u$e=r(lFe," (DistilBERT model)"),lFe.forEach(t),p$e=i(T),Zm=s(T,"LI",{});var iFe=n(Zm);rQ=s(iFe,"STRONG",{});var J8r=n(rQ);_$e=r(J8r,"dpr"),J8r.forEach(t),b$e=r(iFe," \u2014 "),j7=s(iFe,"A",{href:!0});var Y8r=n(j7);v$e=r(Y8r,"DPRConfig"),Y8r.forEach(t),T$e=r(iFe," (DPR model)"),iFe.forEach(t),F$e=i(T),ef=s(T,"LI",{});var dFe=n(ef);tQ=s(dFe,"STRONG",{});var K8r=n(tQ);C$e=r(K8r,"electra"),K8r.forEach(t),M$e=r(dFe," \u2014 "),N7=s(dFe,"A",{href:!0});var Z8r=n(N7);E$e=r(Z8r,"ElectraConfig"),Z8r.forEach(t),y$e=r(dFe," (ELECTRA model)"),dFe.forEach(t),w$e=i(T),of=s(T,"LI",{});var cFe=n(of);aQ=s(cFe,"STRONG",{});var e7r=n(aQ);A$e=r(e7r,"encoder-decoder"),e7r.forEach(t),L$e=r(cFe," \u2014 "),q7=s(cFe,"A",{href:!0});var o7r=n(q7);B$e=r(o7r,"EncoderDecoderConfig"),o7r.forEach(t),x$e=r(cFe," (Encoder decoder model)"),cFe.forEach(t),k$e=i(T),rf=s(T,"LI",{});var mFe=n(rf);sQ=s(mFe,"STRONG",{});var r7r=n(sQ);R$e=r(r7r,"flaubert"),r7r.forEach(t),S$e=r(mFe," \u2014 "),G7=s(mFe,"A",{href:!0});var t7r=n(G7);P$e=r(t7r,"FlaubertConfig"),t7r.forEach(t),$$e=r(mFe," (FlauBERT model)"),mFe.forEach(t),I$e=i(T),tf=s(T,"LI",{});var fFe=n(tf);nQ=s(fFe,"STRONG",{});var a7r=n(nQ);D$e=r(a7r,"fnet"),a7r.forEach(t),j$e=r(fFe," \u2014 "),O7=s(fFe,"A",{href:!0});var s7r=n(O7);N$e=r(s7r,"FNetConfig"),s7r.forEach(t),q$e=r(fFe," (FNet model)"),fFe.forEach(t),G$e=i(T),af=s(T,"LI",{});var gFe=n(af);lQ=s(gFe,"STRONG",{});var n7r=n(lQ);O$e=r(n7r,"fsmt"),n7r.forEach(t),X$e=r(gFe," \u2014 "),X7=s(gFe,"A",{href:!0});var l7r=n(X7);V$e=r(l7r,"FSMTConfig"),l7r.forEach(t),z$e=r(gFe," (FairSeq Machine-Translation model)"),gFe.forEach(t),W$e=i(T),sf=s(T,"LI",{});var hFe=n(sf);iQ=s(hFe,"STRONG",{});var i7r=n(iQ);Q$e=r(i7r,"funnel"),i7r.forEach(t),H$e=r(hFe," \u2014 "),V7=s(hFe,"A",{href:!0});var d7r=n(V7);U$e=r(d7r,"FunnelConfig"),d7r.forEach(t),J$e=r(hFe," (Funnel Transformer model)"),hFe.forEach(t),Y$e=i(T),nf=s(T,"LI",{});var uFe=n(nf);dQ=s(uFe,"STRONG",{});var c7r=n(dQ);K$e=r(c7r,"gpt2"),c7r.forEach(t),Z$e=r(uFe," \u2014 "),z7=s(uFe,"A",{href:!0});var m7r=n(z7);eIe=r(m7r,"GPT2Config"),m7r.forEach(t),oIe=r(uFe," (OpenAI GPT-2 model)"),uFe.forEach(t),rIe=i(T),lf=s(T,"LI",{});var pFe=n(lf);cQ=s(pFe,"STRONG",{});var f7r=n(cQ);tIe=r(f7r,"gpt_neo"),f7r.forEach(t),aIe=r(pFe," \u2014 "),W7=s(pFe,"A",{href:!0});var g7r=n(W7);sIe=r(g7r,"GPTNeoConfig"),g7r.forEach(t),nIe=r(pFe," (GPT Neo model)"),pFe.forEach(t),lIe=i(T),df=s(T,"LI",{});var _Fe=n(df);mQ=s(_Fe,"STRONG",{});var h7r=n(mQ);iIe=r(h7r,"gptj"),h7r.forEach(t),dIe=r(_Fe," \u2014 "),Q7=s(_Fe,"A",{href:!0});var u7r=n(Q7);cIe=r(u7r,"GPTJConfig"),u7r.forEach(t),mIe=r(_Fe," (GPT-J model)"),_Fe.forEach(t),fIe=i(T),cf=s(T,"LI",{});var bFe=n(cf);fQ=s(bFe,"STRONG",{});var p7r=n(fQ);gIe=r(p7r,"hubert"),p7r.forEach(t),hIe=r(bFe," \u2014 "),H7=s(bFe,"A",{href:!0});var _7r=n(H7);uIe=r(_7r,"HubertConfig"),_7r.forEach(t),pIe=r(bFe," (Hubert model)"),bFe.forEach(t),_Ie=i(T),mf=s(T,"LI",{});var vFe=n(mf);gQ=s(vFe,"STRONG",{});var b7r=n(gQ);bIe=r(b7r,"ibert"),b7r.forEach(t),vIe=r(vFe," \u2014 "),U7=s(vFe,"A",{href:!0});var v7r=n(U7);TIe=r(v7r,"IBertConfig"),v7r.forEach(t),FIe=r(vFe," (I-BERT model)"),vFe.forEach(t),CIe=i(T),ff=s(T,"LI",{});var TFe=n(ff);hQ=s(TFe,"STRONG",{});var T7r=n(hQ);MIe=r(T7r,"imagegpt"),T7r.forEach(t),EIe=r(TFe," \u2014 "),J7=s(TFe,"A",{href:!0});var F7r=n(J7);yIe=r(F7r,"ImageGPTConfig"),F7r.forEach(t),wIe=r(TFe," (ImageGPT model)"),TFe.forEach(t),AIe=i(T),gf=s(T,"LI",{});var FFe=n(gf);uQ=s(FFe,"STRONG",{});var C7r=n(uQ);LIe=r(C7r,"layoutlm"),C7r.forEach(t),BIe=r(FFe," \u2014 "),Y7=s(FFe,"A",{href:!0});var M7r=n(Y7);xIe=r(M7r,"LayoutLMConfig"),M7r.forEach(t),kIe=r(FFe," (LayoutLM model)"),FFe.forEach(t),RIe=i(T),hf=s(T,"LI",{});var CFe=n(hf);pQ=s(CFe,"STRONG",{});var E7r=n(pQ);SIe=r(E7r,"layoutlmv2"),E7r.forEach(t),PIe=r(CFe," \u2014 "),K7=s(CFe,"A",{href:!0});var y7r=n(K7);$Ie=r(y7r,"LayoutLMv2Config"),y7r.forEach(t),IIe=r(CFe," (LayoutLMv2 model)"),CFe.forEach(t),DIe=i(T),uf=s(T,"LI",{});var MFe=n(uf);_Q=s(MFe,"STRONG",{});var w7r=n(_Q);jIe=r(w7r,"led"),w7r.forEach(t),NIe=r(MFe," \u2014 "),Z7=s(MFe,"A",{href:!0});var A7r=n(Z7);qIe=r(A7r,"LEDConfig"),A7r.forEach(t),GIe=r(MFe," (LED model)"),MFe.forEach(t),OIe=i(T),pf=s(T,"LI",{});var EFe=n(pf);bQ=s(EFe,"STRONG",{});var L7r=n(bQ);XIe=r(L7r,"longformer"),L7r.forEach(t),VIe=r(EFe," \u2014 "),e9=s(EFe,"A",{href:!0});var B7r=n(e9);zIe=r(B7r,"LongformerConfig"),B7r.forEach(t),WIe=r(EFe," (Longformer model)"),EFe.forEach(t),QIe=i(T),_f=s(T,"LI",{});var yFe=n(_f);vQ=s(yFe,"STRONG",{});var x7r=n(vQ);HIe=r(x7r,"luke"),x7r.forEach(t),UIe=r(yFe," \u2014 "),o9=s(yFe,"A",{href:!0});var k7r=n(o9);JIe=r(k7r,"LukeConfig"),k7r.forEach(t),YIe=r(yFe," (LUKE model)"),yFe.forEach(t),KIe=i(T),bf=s(T,"LI",{});var wFe=n(bf);TQ=s(wFe,"STRONG",{});var R7r=n(TQ);ZIe=r(R7r,"lxmert"),R7r.forEach(t),eDe=r(wFe," \u2014 "),r9=s(wFe,"A",{href:!0});var S7r=n(r9);oDe=r(S7r,"LxmertConfig"),S7r.forEach(t),rDe=r(wFe," (LXMERT model)"),wFe.forEach(t),tDe=i(T),vf=s(T,"LI",{});var AFe=n(vf);FQ=s(AFe,"STRONG",{});var P7r=n(FQ);aDe=r(P7r,"m2m_100"),P7r.forEach(t),sDe=r(AFe," \u2014 "),t9=s(AFe,"A",{href:!0});var $7r=n(t9);nDe=r($7r,"M2M100Config"),$7r.forEach(t),lDe=r(AFe," (M2M100 model)"),AFe.forEach(t),iDe=i(T),Tf=s(T,"LI",{});var LFe=n(Tf);CQ=s(LFe,"STRONG",{});var I7r=n(CQ);dDe=r(I7r,"marian"),I7r.forEach(t),cDe=r(LFe," \u2014 "),a9=s(LFe,"A",{href:!0});var D7r=n(a9);mDe=r(D7r,"MarianConfig"),D7r.forEach(t),fDe=r(LFe," (Marian model)"),LFe.forEach(t),gDe=i(T),Ff=s(T,"LI",{});var BFe=n(Ff);MQ=s(BFe,"STRONG",{});var j7r=n(MQ);hDe=r(j7r,"maskformer"),j7r.forEach(t),uDe=r(BFe," \u2014 "),s9=s(BFe,"A",{href:!0});var N7r=n(s9);pDe=r(N7r,"MaskFormerConfig"),N7r.forEach(t),_De=r(BFe," (MaskFormer model)"),BFe.forEach(t),bDe=i(T),Cf=s(T,"LI",{});var xFe=n(Cf);EQ=s(xFe,"STRONG",{});var q7r=n(EQ);vDe=r(q7r,"mbart"),q7r.forEach(t),TDe=r(xFe," \u2014 "),n9=s(xFe,"A",{href:!0});var G7r=n(n9);FDe=r(G7r,"MBartConfig"),G7r.forEach(t),CDe=r(xFe," (mBART model)"),xFe.forEach(t),MDe=i(T),Mf=s(T,"LI",{});var kFe=n(Mf);yQ=s(kFe,"STRONG",{});var O7r=n(yQ);EDe=r(O7r,"megatron-bert"),O7r.forEach(t),yDe=r(kFe," \u2014 "),l9=s(kFe,"A",{href:!0});var X7r=n(l9);wDe=r(X7r,"MegatronBertConfig"),X7r.forEach(t),ADe=r(kFe," (MegatronBert model)"),kFe.forEach(t),LDe=i(T),Ef=s(T,"LI",{});var RFe=n(Ef);wQ=s(RFe,"STRONG",{});var V7r=n(wQ);BDe=r(V7r,"mobilebert"),V7r.forEach(t),xDe=r(RFe," \u2014 "),i9=s(RFe,"A",{href:!0});var z7r=n(i9);kDe=r(z7r,"MobileBertConfig"),z7r.forEach(t),RDe=r(RFe," (MobileBERT model)"),RFe.forEach(t),SDe=i(T),yf=s(T,"LI",{});var SFe=n(yf);AQ=s(SFe,"STRONG",{});var W7r=n(AQ);PDe=r(W7r,"mpnet"),W7r.forEach(t),$De=r(SFe," \u2014 "),d9=s(SFe,"A",{href:!0});var Q7r=n(d9);IDe=r(Q7r,"MPNetConfig"),Q7r.forEach(t),DDe=r(SFe," (MPNet model)"),SFe.forEach(t),jDe=i(T),wf=s(T,"LI",{});var PFe=n(wf);LQ=s(PFe,"STRONG",{});var H7r=n(LQ);NDe=r(H7r,"mt5"),H7r.forEach(t),qDe=r(PFe," \u2014 "),c9=s(PFe,"A",{href:!0});var U7r=n(c9);GDe=r(U7r,"MT5Config"),U7r.forEach(t),ODe=r(PFe," (mT5 model)"),PFe.forEach(t),XDe=i(T),Af=s(T,"LI",{});var $Fe=n(Af);BQ=s($Fe,"STRONG",{});var J7r=n(BQ);VDe=r(J7r,"nystromformer"),J7r.forEach(t),zDe=r($Fe," \u2014 "),m9=s($Fe,"A",{href:!0});var Y7r=n(m9);WDe=r(Y7r,"NystromformerConfig"),Y7r.forEach(t),QDe=r($Fe," (Nystromformer model)"),$Fe.forEach(t),HDe=i(T),Lf=s(T,"LI",{});var IFe=n(Lf);xQ=s(IFe,"STRONG",{});var K7r=n(xQ);UDe=r(K7r,"openai-gpt"),K7r.forEach(t),JDe=r(IFe," \u2014 "),f9=s(IFe,"A",{href:!0});var Z7r=n(f9);YDe=r(Z7r,"OpenAIGPTConfig"),Z7r.forEach(t),KDe=r(IFe," (OpenAI GPT model)"),IFe.forEach(t),ZDe=i(T),Bf=s(T,"LI",{});var DFe=n(Bf);kQ=s(DFe,"STRONG",{});var e9r=n(kQ);eje=r(e9r,"pegasus"),e9r.forEach(t),oje=r(DFe," \u2014 "),g9=s(DFe,"A",{href:!0});var o9r=n(g9);rje=r(o9r,"PegasusConfig"),o9r.forEach(t),tje=r(DFe," (Pegasus model)"),DFe.forEach(t),aje=i(T),xf=s(T,"LI",{});var jFe=n(xf);RQ=s(jFe,"STRONG",{});var r9r=n(RQ);sje=r(r9r,"perceiver"),r9r.forEach(t),nje=r(jFe," \u2014 "),h9=s(jFe,"A",{href:!0});var t9r=n(h9);lje=r(t9r,"PerceiverConfig"),t9r.forEach(t),ije=r(jFe," (Perceiver model)"),jFe.forEach(t),dje=i(T),kf=s(T,"LI",{});var NFe=n(kf);SQ=s(NFe,"STRONG",{});var a9r=n(SQ);cje=r(a9r,"plbart"),a9r.forEach(t),mje=r(NFe," \u2014 "),u9=s(NFe,"A",{href:!0});var s9r=n(u9);fje=r(s9r,"PLBartConfig"),s9r.forEach(t),gje=r(NFe," (PLBart model)"),NFe.forEach(t),hje=i(T),Rf=s(T,"LI",{});var qFe=n(Rf);PQ=s(qFe,"STRONG",{});var n9r=n(PQ);uje=r(n9r,"poolformer"),n9r.forEach(t),pje=r(qFe," \u2014 "),p9=s(qFe,"A",{href:!0});var l9r=n(p9);_je=r(l9r,"PoolFormerConfig"),l9r.forEach(t),bje=r(qFe," (PoolFormer model)"),qFe.forEach(t),vje=i(T),Sf=s(T,"LI",{});var GFe=n(Sf);$Q=s(GFe,"STRONG",{});var i9r=n($Q);Tje=r(i9r,"prophetnet"),i9r.forEach(t),Fje=r(GFe," \u2014 "),_9=s(GFe,"A",{href:!0});var d9r=n(_9);Cje=r(d9r,"ProphetNetConfig"),d9r.forEach(t),Mje=r(GFe," (ProphetNet model)"),GFe.forEach(t),Eje=i(T),Pf=s(T,"LI",{});var OFe=n(Pf);IQ=s(OFe,"STRONG",{});var c9r=n(IQ);yje=r(c9r,"qdqbert"),c9r.forEach(t),wje=r(OFe," \u2014 "),b9=s(OFe,"A",{href:!0});var m9r=n(b9);Aje=r(m9r,"QDQBertConfig"),m9r.forEach(t),Lje=r(OFe," (QDQBert model)"),OFe.forEach(t),Bje=i(T),$f=s(T,"LI",{});var XFe=n($f);DQ=s(XFe,"STRONG",{});var f9r=n(DQ);xje=r(f9r,"rag"),f9r.forEach(t),kje=r(XFe," \u2014 "),v9=s(XFe,"A",{href:!0});var g9r=n(v9);Rje=r(g9r,"RagConfig"),g9r.forEach(t),Sje=r(XFe," (RAG model)"),XFe.forEach(t),Pje=i(T),If=s(T,"LI",{});var VFe=n(If);jQ=s(VFe,"STRONG",{});var h9r=n(jQ);$je=r(h9r,"realm"),h9r.forEach(t),Ije=r(VFe," \u2014 "),T9=s(VFe,"A",{href:!0});var u9r=n(T9);Dje=r(u9r,"RealmConfig"),u9r.forEach(t),jje=r(VFe," (Realm model)"),VFe.forEach(t),Nje=i(T),Df=s(T,"LI",{});var zFe=n(Df);NQ=s(zFe,"STRONG",{});var p9r=n(NQ);qje=r(p9r,"reformer"),p9r.forEach(t),Gje=r(zFe," \u2014 "),F9=s(zFe,"A",{href:!0});var _9r=n(F9);Oje=r(_9r,"ReformerConfig"),_9r.forEach(t),Xje=r(zFe," (Reformer model)"),zFe.forEach(t),Vje=i(T),jf=s(T,"LI",{});var WFe=n(jf);qQ=s(WFe,"STRONG",{});var b9r=n(qQ);zje=r(b9r,"rembert"),b9r.forEach(t),Wje=r(WFe," \u2014 "),C9=s(WFe,"A",{href:!0});var v9r=n(C9);Qje=r(v9r,"RemBertConfig"),v9r.forEach(t),Hje=r(WFe," (RemBERT model)"),WFe.forEach(t),Uje=i(T),Nf=s(T,"LI",{});var QFe=n(Nf);GQ=s(QFe,"STRONG",{});var T9r=n(GQ);Jje=r(T9r,"retribert"),T9r.forEach(t),Yje=r(QFe," \u2014 "),M9=s(QFe,"A",{href:!0});var F9r=n(M9);Kje=r(F9r,"RetriBertConfig"),F9r.forEach(t),Zje=r(QFe," (RetriBERT model)"),QFe.forEach(t),eNe=i(T),qf=s(T,"LI",{});var HFe=n(qf);OQ=s(HFe,"STRONG",{});var C9r=n(OQ);oNe=r(C9r,"roberta"),C9r.forEach(t),rNe=r(HFe," \u2014 "),E9=s(HFe,"A",{href:!0});var M9r=n(E9);tNe=r(M9r,"RobertaConfig"),M9r.forEach(t),aNe=r(HFe," (RoBERTa model)"),HFe.forEach(t),sNe=i(T),Gf=s(T,"LI",{});var UFe=n(Gf);XQ=s(UFe,"STRONG",{});var E9r=n(XQ);nNe=r(E9r,"roformer"),E9r.forEach(t),lNe=r(UFe," \u2014 "),y9=s(UFe,"A",{href:!0});var y9r=n(y9);iNe=r(y9r,"RoFormerConfig"),y9r.forEach(t),dNe=r(UFe," (RoFormer model)"),UFe.forEach(t),cNe=i(T),Of=s(T,"LI",{});var JFe=n(Of);VQ=s(JFe,"STRONG",{});var w9r=n(VQ);mNe=r(w9r,"segformer"),w9r.forEach(t),fNe=r(JFe," \u2014 "),w9=s(JFe,"A",{href:!0});var A9r=n(w9);gNe=r(A9r,"SegformerConfig"),A9r.forEach(t),hNe=r(JFe," (SegFormer model)"),JFe.forEach(t),uNe=i(T),Xf=s(T,"LI",{});var YFe=n(Xf);zQ=s(YFe,"STRONG",{});var L9r=n(zQ);pNe=r(L9r,"sew"),L9r.forEach(t),_Ne=r(YFe," \u2014 "),A9=s(YFe,"A",{href:!0});var B9r=n(A9);bNe=r(B9r,"SEWConfig"),B9r.forEach(t),vNe=r(YFe," (SEW model)"),YFe.forEach(t),TNe=i(T),Vf=s(T,"LI",{});var KFe=n(Vf);WQ=s(KFe,"STRONG",{});var x9r=n(WQ);FNe=r(x9r,"sew-d"),x9r.forEach(t),CNe=r(KFe," \u2014 "),L9=s(KFe,"A",{href:!0});var k9r=n(L9);MNe=r(k9r,"SEWDConfig"),k9r.forEach(t),ENe=r(KFe," (SEW-D model)"),KFe.forEach(t),yNe=i(T),zf=s(T,"LI",{});var ZFe=n(zf);QQ=s(ZFe,"STRONG",{});var R9r=n(QQ);wNe=r(R9r,"speech-encoder-decoder"),R9r.forEach(t),ANe=r(ZFe," \u2014 "),B9=s(ZFe,"A",{href:!0});var S9r=n(B9);LNe=r(S9r,"SpeechEncoderDecoderConfig"),S9r.forEach(t),BNe=r(ZFe," (Speech Encoder decoder model)"),ZFe.forEach(t),xNe=i(T),Wf=s(T,"LI",{});var eCe=n(Wf);HQ=s(eCe,"STRONG",{});var P9r=n(HQ);kNe=r(P9r,"speech_to_text"),P9r.forEach(t),RNe=r(eCe," \u2014 "),x9=s(eCe,"A",{href:!0});var $9r=n(x9);SNe=r($9r,"Speech2TextConfig"),$9r.forEach(t),PNe=r(eCe," (Speech2Text model)"),eCe.forEach(t),$Ne=i(T),Qf=s(T,"LI",{});var oCe=n(Qf);UQ=s(oCe,"STRONG",{});var I9r=n(UQ);INe=r(I9r,"speech_to_text_2"),I9r.forEach(t),DNe=r(oCe," \u2014 "),k9=s(oCe,"A",{href:!0});var D9r=n(k9);jNe=r(D9r,"Speech2Text2Config"),D9r.forEach(t),NNe=r(oCe," (Speech2Text2 model)"),oCe.forEach(t),qNe=i(T),Hf=s(T,"LI",{});var rCe=n(Hf);JQ=s(rCe,"STRONG",{});var j9r=n(JQ);GNe=r(j9r,"splinter"),j9r.forEach(t),ONe=r(rCe," \u2014 "),R9=s(rCe,"A",{href:!0});var N9r=n(R9);XNe=r(N9r,"SplinterConfig"),N9r.forEach(t),VNe=r(rCe," (Splinter model)"),rCe.forEach(t),zNe=i(T),Uf=s(T,"LI",{});var tCe=n(Uf);YQ=s(tCe,"STRONG",{});var q9r=n(YQ);WNe=r(q9r,"squeezebert"),q9r.forEach(t),QNe=r(tCe," \u2014 "),S9=s(tCe,"A",{href:!0});var G9r=n(S9);HNe=r(G9r,"SqueezeBertConfig"),G9r.forEach(t),UNe=r(tCe," (SqueezeBERT model)"),tCe.forEach(t),JNe=i(T),Jf=s(T,"LI",{});var aCe=n(Jf);KQ=s(aCe,"STRONG",{});var O9r=n(KQ);YNe=r(O9r,"swin"),O9r.forEach(t),KNe=r(aCe," \u2014 "),P9=s(aCe,"A",{href:!0});var X9r=n(P9);ZNe=r(X9r,"SwinConfig"),X9r.forEach(t),eqe=r(aCe," (Swin model)"),aCe.forEach(t),oqe=i(T),Yf=s(T,"LI",{});var sCe=n(Yf);ZQ=s(sCe,"STRONG",{});var V9r=n(ZQ);rqe=r(V9r,"t5"),V9r.forEach(t),tqe=r(sCe," \u2014 "),$9=s(sCe,"A",{href:!0});var z9r=n($9);aqe=r(z9r,"T5Config"),z9r.forEach(t),sqe=r(sCe," (T5 model)"),sCe.forEach(t),nqe=i(T),Kf=s(T,"LI",{});var nCe=n(Kf);eH=s(nCe,"STRONG",{});var W9r=n(eH);lqe=r(W9r,"tapas"),W9r.forEach(t),iqe=r(nCe," \u2014 "),I9=s(nCe,"A",{href:!0});var Q9r=n(I9);dqe=r(Q9r,"TapasConfig"),Q9r.forEach(t),cqe=r(nCe," (TAPAS model)"),nCe.forEach(t),mqe=i(T),Zf=s(T,"LI",{});var lCe=n(Zf);oH=s(lCe,"STRONG",{});var H9r=n(oH);fqe=r(H9r,"transfo-xl"),H9r.forEach(t),gqe=r(lCe," \u2014 "),D9=s(lCe,"A",{href:!0});var U9r=n(D9);hqe=r(U9r,"TransfoXLConfig"),U9r.forEach(t),uqe=r(lCe," (Transformer-XL model)"),lCe.forEach(t),pqe=i(T),eg=s(T,"LI",{});var iCe=n(eg);rH=s(iCe,"STRONG",{});var J9r=n(rH);_qe=r(J9r,"trocr"),J9r.forEach(t),bqe=r(iCe," \u2014 "),j9=s(iCe,"A",{href:!0});var Y9r=n(j9);vqe=r(Y9r,"TrOCRConfig"),Y9r.forEach(t),Tqe=r(iCe," (TrOCR model)"),iCe.forEach(t),Fqe=i(T),og=s(T,"LI",{});var dCe=n(og);tH=s(dCe,"STRONG",{});var K9r=n(tH);Cqe=r(K9r,"unispeech"),K9r.forEach(t),Mqe=r(dCe," \u2014 "),N9=s(dCe,"A",{href:!0});var Z9r=n(N9);Eqe=r(Z9r,"UniSpeechConfig"),Z9r.forEach(t),yqe=r(dCe," (UniSpeech model)"),dCe.forEach(t),wqe=i(T),rg=s(T,"LI",{});var cCe=n(rg);aH=s(cCe,"STRONG",{});var eBr=n(aH);Aqe=r(eBr,"unispeech-sat"),eBr.forEach(t),Lqe=r(cCe," \u2014 "),q9=s(cCe,"A",{href:!0});var oBr=n(q9);Bqe=r(oBr,"UniSpeechSatConfig"),oBr.forEach(t),xqe=r(cCe," (UniSpeechSat model)"),cCe.forEach(t),kqe=i(T),tg=s(T,"LI",{});var mCe=n(tg);sH=s(mCe,"STRONG",{});var rBr=n(sH);Rqe=r(rBr,"vilt"),rBr.forEach(t),Sqe=r(mCe," \u2014 "),G9=s(mCe,"A",{href:!0});var tBr=n(G9);Pqe=r(tBr,"ViltConfig"),tBr.forEach(t),$qe=r(mCe," (ViLT model)"),mCe.forEach(t),Iqe=i(T),ag=s(T,"LI",{});var fCe=n(ag);nH=s(fCe,"STRONG",{});var aBr=n(nH);Dqe=r(aBr,"vision-encoder-decoder"),aBr.forEach(t),jqe=r(fCe," \u2014 "),O9=s(fCe,"A",{href:!0});var sBr=n(O9);Nqe=r(sBr,"VisionEncoderDecoderConfig"),sBr.forEach(t),qqe=r(fCe," (Vision Encoder decoder model)"),fCe.forEach(t),Gqe=i(T),sg=s(T,"LI",{});var gCe=n(sg);lH=s(gCe,"STRONG",{});var nBr=n(lH);Oqe=r(nBr,"vision-text-dual-encoder"),nBr.forEach(t),Xqe=r(gCe," \u2014 "),X9=s(gCe,"A",{href:!0});var lBr=n(X9);Vqe=r(lBr,"VisionTextDualEncoderConfig"),lBr.forEach(t),zqe=r(gCe," (VisionTextDualEncoder model)"),gCe.forEach(t),Wqe=i(T),ng=s(T,"LI",{});var hCe=n(ng);iH=s(hCe,"STRONG",{});var iBr=n(iH);Qqe=r(iBr,"visual_bert"),iBr.forEach(t),Hqe=r(hCe," \u2014 "),V9=s(hCe,"A",{href:!0});var dBr=n(V9);Uqe=r(dBr,"VisualBertConfig"),dBr.forEach(t),Jqe=r(hCe," (VisualBert model)"),hCe.forEach(t),Yqe=i(T),lg=s(T,"LI",{});var uCe=n(lg);dH=s(uCe,"STRONG",{});var cBr=n(dH);Kqe=r(cBr,"vit"),cBr.forEach(t),Zqe=r(uCe," \u2014 "),z9=s(uCe,"A",{href:!0});var mBr=n(z9);eGe=r(mBr,"ViTConfig"),mBr.forEach(t),oGe=r(uCe," (ViT model)"),uCe.forEach(t),rGe=i(T),ig=s(T,"LI",{});var pCe=n(ig);cH=s(pCe,"STRONG",{});var fBr=n(cH);tGe=r(fBr,"vit_mae"),fBr.forEach(t),aGe=r(pCe," \u2014 "),W9=s(pCe,"A",{href:!0});var gBr=n(W9);sGe=r(gBr,"ViTMAEConfig"),gBr.forEach(t),nGe=r(pCe," (ViTMAE model)"),pCe.forEach(t),lGe=i(T),dg=s(T,"LI",{});var _Ce=n(dg);mH=s(_Ce,"STRONG",{});var hBr=n(mH);iGe=r(hBr,"wav2vec2"),hBr.forEach(t),dGe=r(_Ce," \u2014 "),Q9=s(_Ce,"A",{href:!0});var uBr=n(Q9);cGe=r(uBr,"Wav2Vec2Config"),uBr.forEach(t),mGe=r(_Ce," (Wav2Vec2 model)"),_Ce.forEach(t),fGe=i(T),cg=s(T,"LI",{});var bCe=n(cg);fH=s(bCe,"STRONG",{});var pBr=n(fH);gGe=r(pBr,"wavlm"),pBr.forEach(t),hGe=r(bCe," \u2014 "),H9=s(bCe,"A",{href:!0});var _Br=n(H9);uGe=r(_Br,"WavLMConfig"),_Br.forEach(t),pGe=r(bCe," (WavLM model)"),bCe.forEach(t),_Ge=i(T),mg=s(T,"LI",{});var vCe=n(mg);gH=s(vCe,"STRONG",{});var bBr=n(gH);bGe=r(bBr,"xglm"),bBr.forEach(t),vGe=r(vCe," \u2014 "),U9=s(vCe,"A",{href:!0});var vBr=n(U9);TGe=r(vBr,"XGLMConfig"),vBr.forEach(t),FGe=r(vCe," (XGLM model)"),vCe.forEach(t),CGe=i(T),fg=s(T,"LI",{});var TCe=n(fg);hH=s(TCe,"STRONG",{});var TBr=n(hH);MGe=r(TBr,"xlm"),TBr.forEach(t),EGe=r(TCe," \u2014 "),J9=s(TCe,"A",{href:!0});var FBr=n(J9);yGe=r(FBr,"XLMConfig"),FBr.forEach(t),wGe=r(TCe," (XLM model)"),TCe.forEach(t),AGe=i(T),gg=s(T,"LI",{});var FCe=n(gg);uH=s(FCe,"STRONG",{});var CBr=n(uH);LGe=r(CBr,"xlm-prophetnet"),CBr.forEach(t),BGe=r(FCe," \u2014 "),Y9=s(FCe,"A",{href:!0});var MBr=n(Y9);xGe=r(MBr,"XLMProphetNetConfig"),MBr.forEach(t),kGe=r(FCe," (XLMProphetNet model)"),FCe.forEach(t),RGe=i(T),hg=s(T,"LI",{});var CCe=n(hg);pH=s(CCe,"STRONG",{});var EBr=n(pH);SGe=r(EBr,"xlm-roberta"),EBr.forEach(t),PGe=r(CCe," \u2014 "),K9=s(CCe,"A",{href:!0});var yBr=n(K9);$Ge=r(yBr,"XLMRobertaConfig"),yBr.forEach(t),IGe=r(CCe," (XLM-RoBERTa model)"),CCe.forEach(t),DGe=i(T),ug=s(T,"LI",{});var MCe=n(ug);_H=s(MCe,"STRONG",{});var wBr=n(_H);jGe=r(wBr,"xlm-roberta-xl"),wBr.forEach(t),NGe=r(MCe," \u2014 "),Z9=s(MCe,"A",{href:!0});var ABr=n(Z9);qGe=r(ABr,"XLMRobertaXLConfig"),ABr.forEach(t),GGe=r(MCe," (XLM-RoBERTa-XL model)"),MCe.forEach(t),OGe=i(T),pg=s(T,"LI",{});var ECe=n(pg);bH=s(ECe,"STRONG",{});var LBr=n(bH);XGe=r(LBr,"xlnet"),LBr.forEach(t),VGe=r(ECe," \u2014 "),eB=s(ECe,"A",{href:!0});var BBr=n(eB);zGe=r(BBr,"XLNetConfig"),BBr.forEach(t),WGe=r(ECe," (XLNet model)"),ECe.forEach(t),QGe=i(T),_g=s(T,"LI",{});var yCe=n(_g);vH=s(yCe,"STRONG",{});var xBr=n(vH);HGe=r(xBr,"yoso"),xBr.forEach(t),UGe=r(yCe," \u2014 "),oB=s(yCe,"A",{href:!0});var kBr=n(oB);JGe=r(kBr,"YosoConfig"),kBr.forEach(t),YGe=r(yCe," (YOSO model)"),yCe.forEach(t),T.forEach(t),KGe=i(ia),TH=s(ia,"P",{});var RBr=n(TH);ZGe=r(RBr,"Examples:"),RBr.forEach(t),eOe=i(ia),f(P5.$$.fragment,ia),ia.forEach(t),oOe=i(Dn),bg=s(Dn,"DIV",{class:!0});var Dxe=n(bg);f($5.$$.fragment,Dxe),rOe=i(Dxe),FH=s(Dxe,"P",{});var SBr=n(FH);tOe=r(SBr,"Register a new configuration for this class."),SBr.forEach(t),Dxe.forEach(t),Dn.forEach(t),D9e=i(c),Ni=s(c,"H2",{class:!0});var jxe=n(Ni);vg=s(jxe,"A",{id:!0,class:!0,href:!0});var PBr=n(vg);CH=s(PBr,"SPAN",{});var $Br=n(CH);f(I5.$$.fragment,$Br),$Br.forEach(t),PBr.forEach(t),aOe=i(jxe),MH=s(jxe,"SPAN",{});var IBr=n(MH);sOe=r(IBr,"AutoTokenizer"),IBr.forEach(t),jxe.forEach(t),j9e=i(c),Vo=s(c,"DIV",{class:!0});var jn=n(Vo);f(D5.$$.fragment,jn),nOe=i(jn),j5=s(jn,"P",{});var Nxe=n(j5);lOe=r(Nxe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),rB=s(Nxe,"A",{href:!0});var DBr=n(rB);iOe=r(DBr,"AutoTokenizer.from_pretrained()"),DBr.forEach(t),dOe=r(Nxe," class method."),Nxe.forEach(t),cOe=i(jn),N5=s(jn,"P",{});var qxe=n(N5);mOe=r(qxe,"This class cannot be instantiated directly using "),EH=s(qxe,"CODE",{});var jBr=n(EH);fOe=r(jBr,"__init__()"),jBr.forEach(t),gOe=r(qxe," (throws an error)."),qxe.forEach(t),hOe=i(jn),fo=s(jn,"DIV",{class:!0});var da=n(fo);f(q5.$$.fragment,da),uOe=i(da),yH=s(da,"P",{});var NBr=n(yH);pOe=r(NBr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),NBr.forEach(t),_Oe=i(da),Da=s(da,"P",{});var y3=n(Da);bOe=r(y3,"The tokenizer class to instantiate is selected based on the "),wH=s(y3,"CODE",{});var qBr=n(wH);vOe=r(qBr,"model_type"),qBr.forEach(t),TOe=r(y3,` property of the config object (either
passed as an argument or loaded from `),AH=s(y3,"CODE",{});var GBr=n(AH);FOe=r(GBr,"pretrained_model_name_or_path"),GBr.forEach(t),COe=r(y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),LH=s(y3,"CODE",{});var OBr=n(LH);MOe=r(OBr,"pretrained_model_name_or_path"),OBr.forEach(t),EOe=r(y3,":"),y3.forEach(t),yOe=i(da),M=s(da,"UL",{});var y=n(M);qs=s(y,"LI",{});var g8=n(qs);BH=s(g8,"STRONG",{});var XBr=n(BH);wOe=r(XBr,"albert"),XBr.forEach(t),AOe=r(g8," \u2014 "),tB=s(g8,"A",{href:!0});var VBr=n(tB);LOe=r(VBr,"AlbertTokenizer"),VBr.forEach(t),BOe=r(g8," or "),aB=s(g8,"A",{href:!0});var zBr=n(aB);xOe=r(zBr,"AlbertTokenizerFast"),zBr.forEach(t),kOe=r(g8," (ALBERT model)"),g8.forEach(t),ROe=i(y),Gs=s(y,"LI",{});var h8=n(Gs);xH=s(h8,"STRONG",{});var WBr=n(xH);SOe=r(WBr,"bart"),WBr.forEach(t),POe=r(h8," \u2014 "),sB=s(h8,"A",{href:!0});var QBr=n(sB);$Oe=r(QBr,"BartTokenizer"),QBr.forEach(t),IOe=r(h8," or "),nB=s(h8,"A",{href:!0});var HBr=n(nB);DOe=r(HBr,"BartTokenizerFast"),HBr.forEach(t),jOe=r(h8," (BART model)"),h8.forEach(t),NOe=i(y),Os=s(y,"LI",{});var u8=n(Os);kH=s(u8,"STRONG",{});var UBr=n(kH);qOe=r(UBr,"barthez"),UBr.forEach(t),GOe=r(u8," \u2014 "),lB=s(u8,"A",{href:!0});var JBr=n(lB);OOe=r(JBr,"BarthezTokenizer"),JBr.forEach(t),XOe=r(u8," or "),iB=s(u8,"A",{href:!0});var YBr=n(iB);VOe=r(YBr,"BarthezTokenizerFast"),YBr.forEach(t),zOe=r(u8," (BARThez model)"),u8.forEach(t),WOe=i(y),Tg=s(y,"LI",{});var wCe=n(Tg);RH=s(wCe,"STRONG",{});var KBr=n(RH);QOe=r(KBr,"bartpho"),KBr.forEach(t),HOe=r(wCe," \u2014 "),dB=s(wCe,"A",{href:!0});var ZBr=n(dB);UOe=r(ZBr,"BartphoTokenizer"),ZBr.forEach(t),JOe=r(wCe," (BARTpho model)"),wCe.forEach(t),YOe=i(y),Xs=s(y,"LI",{});var p8=n(Xs);SH=s(p8,"STRONG",{});var exr=n(SH);KOe=r(exr,"bert"),exr.forEach(t),ZOe=r(p8," \u2014 "),cB=s(p8,"A",{href:!0});var oxr=n(cB);eXe=r(oxr,"BertTokenizer"),oxr.forEach(t),oXe=r(p8," or "),mB=s(p8,"A",{href:!0});var rxr=n(mB);rXe=r(rxr,"BertTokenizerFast"),rxr.forEach(t),tXe=r(p8," (BERT model)"),p8.forEach(t),aXe=i(y),Fg=s(y,"LI",{});var ACe=n(Fg);PH=s(ACe,"STRONG",{});var txr=n(PH);sXe=r(txr,"bert-generation"),txr.forEach(t),nXe=r(ACe," \u2014 "),fB=s(ACe,"A",{href:!0});var axr=n(fB);lXe=r(axr,"BertGenerationTokenizer"),axr.forEach(t),iXe=r(ACe," (Bert Generation model)"),ACe.forEach(t),dXe=i(y),Cg=s(y,"LI",{});var LCe=n(Cg);$H=s(LCe,"STRONG",{});var sxr=n($H);cXe=r(sxr,"bert-japanese"),sxr.forEach(t),mXe=r(LCe," \u2014 "),gB=s(LCe,"A",{href:!0});var nxr=n(gB);fXe=r(nxr,"BertJapaneseTokenizer"),nxr.forEach(t),gXe=r(LCe," (BertJapanese model)"),LCe.forEach(t),hXe=i(y),Mg=s(y,"LI",{});var BCe=n(Mg);IH=s(BCe,"STRONG",{});var lxr=n(IH);uXe=r(lxr,"bertweet"),lxr.forEach(t),pXe=r(BCe," \u2014 "),hB=s(BCe,"A",{href:!0});var ixr=n(hB);_Xe=r(ixr,"BertweetTokenizer"),ixr.forEach(t),bXe=r(BCe," (Bertweet model)"),BCe.forEach(t),vXe=i(y),Vs=s(y,"LI",{});var _8=n(Vs);DH=s(_8,"STRONG",{});var dxr=n(DH);TXe=r(dxr,"big_bird"),dxr.forEach(t),FXe=r(_8," \u2014 "),uB=s(_8,"A",{href:!0});var cxr=n(uB);CXe=r(cxr,"BigBirdTokenizer"),cxr.forEach(t),MXe=r(_8," or "),pB=s(_8,"A",{href:!0});var mxr=n(pB);EXe=r(mxr,"BigBirdTokenizerFast"),mxr.forEach(t),yXe=r(_8," (BigBird model)"),_8.forEach(t),wXe=i(y),zs=s(y,"LI",{});var b8=n(zs);jH=s(b8,"STRONG",{});var fxr=n(jH);AXe=r(fxr,"bigbird_pegasus"),fxr.forEach(t),LXe=r(b8," \u2014 "),_B=s(b8,"A",{href:!0});var gxr=n(_B);BXe=r(gxr,"PegasusTokenizer"),gxr.forEach(t),xXe=r(b8," or "),bB=s(b8,"A",{href:!0});var hxr=n(bB);kXe=r(hxr,"PegasusTokenizerFast"),hxr.forEach(t),RXe=r(b8," (BigBirdPegasus model)"),b8.forEach(t),SXe=i(y),Ws=s(y,"LI",{});var v8=n(Ws);NH=s(v8,"STRONG",{});var uxr=n(NH);PXe=r(uxr,"blenderbot"),uxr.forEach(t),$Xe=r(v8," \u2014 "),vB=s(v8,"A",{href:!0});var pxr=n(vB);IXe=r(pxr,"BlenderbotTokenizer"),pxr.forEach(t),DXe=r(v8," or "),TB=s(v8,"A",{href:!0});var _xr=n(TB);jXe=r(_xr,"BlenderbotTokenizerFast"),_xr.forEach(t),NXe=r(v8," (Blenderbot model)"),v8.forEach(t),qXe=i(y),Eg=s(y,"LI",{});var xCe=n(Eg);qH=s(xCe,"STRONG",{});var bxr=n(qH);GXe=r(bxr,"blenderbot-small"),bxr.forEach(t),OXe=r(xCe," \u2014 "),FB=s(xCe,"A",{href:!0});var vxr=n(FB);XXe=r(vxr,"BlenderbotSmallTokenizer"),vxr.forEach(t),VXe=r(xCe," (BlenderbotSmall model)"),xCe.forEach(t),zXe=i(y),yg=s(y,"LI",{});var kCe=n(yg);GH=s(kCe,"STRONG",{});var Txr=n(GH);WXe=r(Txr,"byt5"),Txr.forEach(t),QXe=r(kCe," \u2014 "),CB=s(kCe,"A",{href:!0});var Fxr=n(CB);HXe=r(Fxr,"ByT5Tokenizer"),Fxr.forEach(t),UXe=r(kCe," (ByT5 model)"),kCe.forEach(t),JXe=i(y),Qs=s(y,"LI",{});var T8=n(Qs);OH=s(T8,"STRONG",{});var Cxr=n(OH);YXe=r(Cxr,"camembert"),Cxr.forEach(t),KXe=r(T8," \u2014 "),MB=s(T8,"A",{href:!0});var Mxr=n(MB);ZXe=r(Mxr,"CamembertTokenizer"),Mxr.forEach(t),eVe=r(T8," or "),EB=s(T8,"A",{href:!0});var Exr=n(EB);oVe=r(Exr,"CamembertTokenizerFast"),Exr.forEach(t),rVe=r(T8," (CamemBERT model)"),T8.forEach(t),tVe=i(y),wg=s(y,"LI",{});var RCe=n(wg);XH=s(RCe,"STRONG",{});var yxr=n(XH);aVe=r(yxr,"canine"),yxr.forEach(t),sVe=r(RCe," \u2014 "),yB=s(RCe,"A",{href:!0});var wxr=n(yB);nVe=r(wxr,"CanineTokenizer"),wxr.forEach(t),lVe=r(RCe," (Canine model)"),RCe.forEach(t),iVe=i(y),Hs=s(y,"LI",{});var F8=n(Hs);VH=s(F8,"STRONG",{});var Axr=n(VH);dVe=r(Axr,"clip"),Axr.forEach(t),cVe=r(F8," \u2014 "),wB=s(F8,"A",{href:!0});var Lxr=n(wB);mVe=r(Lxr,"CLIPTokenizer"),Lxr.forEach(t),fVe=r(F8," or "),AB=s(F8,"A",{href:!0});var Bxr=n(AB);gVe=r(Bxr,"CLIPTokenizerFast"),Bxr.forEach(t),hVe=r(F8," (CLIP model)"),F8.forEach(t),uVe=i(y),Us=s(y,"LI",{});var C8=n(Us);zH=s(C8,"STRONG",{});var xxr=n(zH);pVe=r(xxr,"convbert"),xxr.forEach(t),_Ve=r(C8," \u2014 "),LB=s(C8,"A",{href:!0});var kxr=n(LB);bVe=r(kxr,"ConvBertTokenizer"),kxr.forEach(t),vVe=r(C8," or "),BB=s(C8,"A",{href:!0});var Rxr=n(BB);TVe=r(Rxr,"ConvBertTokenizerFast"),Rxr.forEach(t),FVe=r(C8," (ConvBERT model)"),C8.forEach(t),CVe=i(y),Js=s(y,"LI",{});var M8=n(Js);WH=s(M8,"STRONG",{});var Sxr=n(WH);MVe=r(Sxr,"cpm"),Sxr.forEach(t),EVe=r(M8," \u2014 "),xB=s(M8,"A",{href:!0});var Pxr=n(xB);yVe=r(Pxr,"CpmTokenizer"),Pxr.forEach(t),wVe=r(M8," or "),QH=s(M8,"CODE",{});var $xr=n(QH);AVe=r($xr,"CpmTokenizerFast"),$xr.forEach(t),LVe=r(M8," (CPM model)"),M8.forEach(t),BVe=i(y),Ag=s(y,"LI",{});var SCe=n(Ag);HH=s(SCe,"STRONG",{});var Ixr=n(HH);xVe=r(Ixr,"ctrl"),Ixr.forEach(t),kVe=r(SCe," \u2014 "),kB=s(SCe,"A",{href:!0});var Dxr=n(kB);RVe=r(Dxr,"CTRLTokenizer"),Dxr.forEach(t),SVe=r(SCe," (CTRL model)"),SCe.forEach(t),PVe=i(y),Ys=s(y,"LI",{});var E8=n(Ys);UH=s(E8,"STRONG",{});var jxr=n(UH);$Ve=r(jxr,"deberta"),jxr.forEach(t),IVe=r(E8," \u2014 "),RB=s(E8,"A",{href:!0});var Nxr=n(RB);DVe=r(Nxr,"DebertaTokenizer"),Nxr.forEach(t),jVe=r(E8," or "),SB=s(E8,"A",{href:!0});var qxr=n(SB);NVe=r(qxr,"DebertaTokenizerFast"),qxr.forEach(t),qVe=r(E8," (DeBERTa model)"),E8.forEach(t),GVe=i(y),Lg=s(y,"LI",{});var PCe=n(Lg);JH=s(PCe,"STRONG",{});var Gxr=n(JH);OVe=r(Gxr,"deberta-v2"),Gxr.forEach(t),XVe=r(PCe," \u2014 "),PB=s(PCe,"A",{href:!0});var Oxr=n(PB);VVe=r(Oxr,"DebertaV2Tokenizer"),Oxr.forEach(t),zVe=r(PCe," (DeBERTa-v2 model)"),PCe.forEach(t),WVe=i(y),Ks=s(y,"LI",{});var y8=n(Ks);YH=s(y8,"STRONG",{});var Xxr=n(YH);QVe=r(Xxr,"distilbert"),Xxr.forEach(t),HVe=r(y8," \u2014 "),$B=s(y8,"A",{href:!0});var Vxr=n($B);UVe=r(Vxr,"DistilBertTokenizer"),Vxr.forEach(t),JVe=r(y8," or "),IB=s(y8,"A",{href:!0});var zxr=n(IB);YVe=r(zxr,"DistilBertTokenizerFast"),zxr.forEach(t),KVe=r(y8," (DistilBERT model)"),y8.forEach(t),ZVe=i(y),Zs=s(y,"LI",{});var w8=n(Zs);KH=s(w8,"STRONG",{});var Wxr=n(KH);eze=r(Wxr,"dpr"),Wxr.forEach(t),oze=r(w8," \u2014 "),DB=s(w8,"A",{href:!0});var Qxr=n(DB);rze=r(Qxr,"DPRQuestionEncoderTokenizer"),Qxr.forEach(t),tze=r(w8," or "),jB=s(w8,"A",{href:!0});var Hxr=n(jB);aze=r(Hxr,"DPRQuestionEncoderTokenizerFast"),Hxr.forEach(t),sze=r(w8," (DPR model)"),w8.forEach(t),nze=i(y),en=s(y,"LI",{});var A8=n(en);ZH=s(A8,"STRONG",{});var Uxr=n(ZH);lze=r(Uxr,"electra"),Uxr.forEach(t),ize=r(A8," \u2014 "),NB=s(A8,"A",{href:!0});var Jxr=n(NB);dze=r(Jxr,"ElectraTokenizer"),Jxr.forEach(t),cze=r(A8," or "),qB=s(A8,"A",{href:!0});var Yxr=n(qB);mze=r(Yxr,"ElectraTokenizerFast"),Yxr.forEach(t),fze=r(A8," (ELECTRA model)"),A8.forEach(t),gze=i(y),Bg=s(y,"LI",{});var $Ce=n(Bg);eU=s($Ce,"STRONG",{});var Kxr=n(eU);hze=r(Kxr,"flaubert"),Kxr.forEach(t),uze=r($Ce," \u2014 "),GB=s($Ce,"A",{href:!0});var Zxr=n(GB);pze=r(Zxr,"FlaubertTokenizer"),Zxr.forEach(t),_ze=r($Ce," (FlauBERT model)"),$Ce.forEach(t),bze=i(y),on=s(y,"LI",{});var L8=n(on);oU=s(L8,"STRONG",{});var ekr=n(oU);vze=r(ekr,"fnet"),ekr.forEach(t),Tze=r(L8," \u2014 "),OB=s(L8,"A",{href:!0});var okr=n(OB);Fze=r(okr,"FNetTokenizer"),okr.forEach(t),Cze=r(L8," or "),XB=s(L8,"A",{href:!0});var rkr=n(XB);Mze=r(rkr,"FNetTokenizerFast"),rkr.forEach(t),Eze=r(L8," (FNet model)"),L8.forEach(t),yze=i(y),xg=s(y,"LI",{});var ICe=n(xg);rU=s(ICe,"STRONG",{});var tkr=n(rU);wze=r(tkr,"fsmt"),tkr.forEach(t),Aze=r(ICe," \u2014 "),VB=s(ICe,"A",{href:!0});var akr=n(VB);Lze=r(akr,"FSMTTokenizer"),akr.forEach(t),Bze=r(ICe," (FairSeq Machine-Translation model)"),ICe.forEach(t),xze=i(y),rn=s(y,"LI",{});var B8=n(rn);tU=s(B8,"STRONG",{});var skr=n(tU);kze=r(skr,"funnel"),skr.forEach(t),Rze=r(B8," \u2014 "),zB=s(B8,"A",{href:!0});var nkr=n(zB);Sze=r(nkr,"FunnelTokenizer"),nkr.forEach(t),Pze=r(B8," or "),WB=s(B8,"A",{href:!0});var lkr=n(WB);$ze=r(lkr,"FunnelTokenizerFast"),lkr.forEach(t),Ize=r(B8," (Funnel Transformer model)"),B8.forEach(t),Dze=i(y),tn=s(y,"LI",{});var x8=n(tn);aU=s(x8,"STRONG",{});var ikr=n(aU);jze=r(ikr,"gpt2"),ikr.forEach(t),Nze=r(x8," \u2014 "),QB=s(x8,"A",{href:!0});var dkr=n(QB);qze=r(dkr,"GPT2Tokenizer"),dkr.forEach(t),Gze=r(x8," or "),HB=s(x8,"A",{href:!0});var ckr=n(HB);Oze=r(ckr,"GPT2TokenizerFast"),ckr.forEach(t),Xze=r(x8," (OpenAI GPT-2 model)"),x8.forEach(t),Vze=i(y),an=s(y,"LI",{});var k8=n(an);sU=s(k8,"STRONG",{});var mkr=n(sU);zze=r(mkr,"gpt_neo"),mkr.forEach(t),Wze=r(k8," \u2014 "),UB=s(k8,"A",{href:!0});var fkr=n(UB);Qze=r(fkr,"GPT2Tokenizer"),fkr.forEach(t),Hze=r(k8," or "),JB=s(k8,"A",{href:!0});var gkr=n(JB);Uze=r(gkr,"GPT2TokenizerFast"),gkr.forEach(t),Jze=r(k8," (GPT Neo model)"),k8.forEach(t),Yze=i(y),sn=s(y,"LI",{});var R8=n(sn);nU=s(R8,"STRONG",{});var hkr=n(nU);Kze=r(hkr,"herbert"),hkr.forEach(t),Zze=r(R8," \u2014 "),YB=s(R8,"A",{href:!0});var ukr=n(YB);eWe=r(ukr,"HerbertTokenizer"),ukr.forEach(t),oWe=r(R8," or "),KB=s(R8,"A",{href:!0});var pkr=n(KB);rWe=r(pkr,"HerbertTokenizerFast"),pkr.forEach(t),tWe=r(R8," (HerBERT model)"),R8.forEach(t),aWe=i(y),kg=s(y,"LI",{});var DCe=n(kg);lU=s(DCe,"STRONG",{});var _kr=n(lU);sWe=r(_kr,"hubert"),_kr.forEach(t),nWe=r(DCe," \u2014 "),ZB=s(DCe,"A",{href:!0});var bkr=n(ZB);lWe=r(bkr,"Wav2Vec2CTCTokenizer"),bkr.forEach(t),iWe=r(DCe," (Hubert model)"),DCe.forEach(t),dWe=i(y),nn=s(y,"LI",{});var S8=n(nn);iU=s(S8,"STRONG",{});var vkr=n(iU);cWe=r(vkr,"ibert"),vkr.forEach(t),mWe=r(S8," \u2014 "),ex=s(S8,"A",{href:!0});var Tkr=n(ex);fWe=r(Tkr,"RobertaTokenizer"),Tkr.forEach(t),gWe=r(S8," or "),ox=s(S8,"A",{href:!0});var Fkr=n(ox);hWe=r(Fkr,"RobertaTokenizerFast"),Fkr.forEach(t),uWe=r(S8," (I-BERT model)"),S8.forEach(t),pWe=i(y),ln=s(y,"LI",{});var P8=n(ln);dU=s(P8,"STRONG",{});var Ckr=n(dU);_We=r(Ckr,"layoutlm"),Ckr.forEach(t),bWe=r(P8," \u2014 "),rx=s(P8,"A",{href:!0});var Mkr=n(rx);vWe=r(Mkr,"LayoutLMTokenizer"),Mkr.forEach(t),TWe=r(P8," or "),tx=s(P8,"A",{href:!0});var Ekr=n(tx);FWe=r(Ekr,"LayoutLMTokenizerFast"),Ekr.forEach(t),CWe=r(P8," (LayoutLM model)"),P8.forEach(t),MWe=i(y),dn=s(y,"LI",{});var $8=n(dn);cU=s($8,"STRONG",{});var ykr=n(cU);EWe=r(ykr,"layoutlmv2"),ykr.forEach(t),yWe=r($8," \u2014 "),ax=s($8,"A",{href:!0});var wkr=n(ax);wWe=r(wkr,"LayoutLMv2Tokenizer"),wkr.forEach(t),AWe=r($8," or "),sx=s($8,"A",{href:!0});var Akr=n(sx);LWe=r(Akr,"LayoutLMv2TokenizerFast"),Akr.forEach(t),BWe=r($8," (LayoutLMv2 model)"),$8.forEach(t),xWe=i(y),cn=s(y,"LI",{});var I8=n(cn);mU=s(I8,"STRONG",{});var Lkr=n(mU);kWe=r(Lkr,"layoutxlm"),Lkr.forEach(t),RWe=r(I8," \u2014 "),nx=s(I8,"A",{href:!0});var Bkr=n(nx);SWe=r(Bkr,"LayoutXLMTokenizer"),Bkr.forEach(t),PWe=r(I8," or "),lx=s(I8,"A",{href:!0});var xkr=n(lx);$We=r(xkr,"LayoutXLMTokenizerFast"),xkr.forEach(t),IWe=r(I8," (LayoutXLM model)"),I8.forEach(t),DWe=i(y),mn=s(y,"LI",{});var D8=n(mn);fU=s(D8,"STRONG",{});var kkr=n(fU);jWe=r(kkr,"led"),kkr.forEach(t),NWe=r(D8," \u2014 "),ix=s(D8,"A",{href:!0});var Rkr=n(ix);qWe=r(Rkr,"LEDTokenizer"),Rkr.forEach(t),GWe=r(D8," or "),dx=s(D8,"A",{href:!0});var Skr=n(dx);OWe=r(Skr,"LEDTokenizerFast"),Skr.forEach(t),XWe=r(D8," (LED model)"),D8.forEach(t),VWe=i(y),fn=s(y,"LI",{});var j8=n(fn);gU=s(j8,"STRONG",{});var Pkr=n(gU);zWe=r(Pkr,"longformer"),Pkr.forEach(t),WWe=r(j8," \u2014 "),cx=s(j8,"A",{href:!0});var $kr=n(cx);QWe=r($kr,"LongformerTokenizer"),$kr.forEach(t),HWe=r(j8," or "),mx=s(j8,"A",{href:!0});var Ikr=n(mx);UWe=r(Ikr,"LongformerTokenizerFast"),Ikr.forEach(t),JWe=r(j8," (Longformer model)"),j8.forEach(t),YWe=i(y),Rg=s(y,"LI",{});var jCe=n(Rg);hU=s(jCe,"STRONG",{});var Dkr=n(hU);KWe=r(Dkr,"luke"),Dkr.forEach(t),ZWe=r(jCe," \u2014 "),fx=s(jCe,"A",{href:!0});var jkr=n(fx);eQe=r(jkr,"LukeTokenizer"),jkr.forEach(t),oQe=r(jCe," (LUKE model)"),jCe.forEach(t),rQe=i(y),gn=s(y,"LI",{});var N8=n(gn);uU=s(N8,"STRONG",{});var Nkr=n(uU);tQe=r(Nkr,"lxmert"),Nkr.forEach(t),aQe=r(N8," \u2014 "),gx=s(N8,"A",{href:!0});var qkr=n(gx);sQe=r(qkr,"LxmertTokenizer"),qkr.forEach(t),nQe=r(N8," or "),hx=s(N8,"A",{href:!0});var Gkr=n(hx);lQe=r(Gkr,"LxmertTokenizerFast"),Gkr.forEach(t),iQe=r(N8," (LXMERT model)"),N8.forEach(t),dQe=i(y),Sg=s(y,"LI",{});var NCe=n(Sg);pU=s(NCe,"STRONG",{});var Okr=n(pU);cQe=r(Okr,"m2m_100"),Okr.forEach(t),mQe=r(NCe," \u2014 "),ux=s(NCe,"A",{href:!0});var Xkr=n(ux);fQe=r(Xkr,"M2M100Tokenizer"),Xkr.forEach(t),gQe=r(NCe," (M2M100 model)"),NCe.forEach(t),hQe=i(y),Pg=s(y,"LI",{});var qCe=n(Pg);_U=s(qCe,"STRONG",{});var Vkr=n(_U);uQe=r(Vkr,"marian"),Vkr.forEach(t),pQe=r(qCe," \u2014 "),px=s(qCe,"A",{href:!0});var zkr=n(px);_Qe=r(zkr,"MarianTokenizer"),zkr.forEach(t),bQe=r(qCe," (Marian model)"),qCe.forEach(t),vQe=i(y),hn=s(y,"LI",{});var q8=n(hn);bU=s(q8,"STRONG",{});var Wkr=n(bU);TQe=r(Wkr,"mbart"),Wkr.forEach(t),FQe=r(q8," \u2014 "),_x=s(q8,"A",{href:!0});var Qkr=n(_x);CQe=r(Qkr,"MBartTokenizer"),Qkr.forEach(t),MQe=r(q8," or "),bx=s(q8,"A",{href:!0});var Hkr=n(bx);EQe=r(Hkr,"MBartTokenizerFast"),Hkr.forEach(t),yQe=r(q8," (mBART model)"),q8.forEach(t),wQe=i(y),un=s(y,"LI",{});var G8=n(un);vU=s(G8,"STRONG",{});var Ukr=n(vU);AQe=r(Ukr,"mbart50"),Ukr.forEach(t),LQe=r(G8," \u2014 "),vx=s(G8,"A",{href:!0});var Jkr=n(vx);BQe=r(Jkr,"MBart50Tokenizer"),Jkr.forEach(t),xQe=r(G8," or "),Tx=s(G8,"A",{href:!0});var Ykr=n(Tx);kQe=r(Ykr,"MBart50TokenizerFast"),Ykr.forEach(t),RQe=r(G8," (mBART-50 model)"),G8.forEach(t),SQe=i(y),$g=s(y,"LI",{});var GCe=n($g);TU=s(GCe,"STRONG",{});var Kkr=n(TU);PQe=r(Kkr,"mluke"),Kkr.forEach(t),$Qe=r(GCe," \u2014 "),Fx=s(GCe,"A",{href:!0});var Zkr=n(Fx);IQe=r(Zkr,"MLukeTokenizer"),Zkr.forEach(t),DQe=r(GCe," (mLUKE model)"),GCe.forEach(t),jQe=i(y),pn=s(y,"LI",{});var O8=n(pn);FU=s(O8,"STRONG",{});var eRr=n(FU);NQe=r(eRr,"mobilebert"),eRr.forEach(t),qQe=r(O8," \u2014 "),Cx=s(O8,"A",{href:!0});var oRr=n(Cx);GQe=r(oRr,"MobileBertTokenizer"),oRr.forEach(t),OQe=r(O8," or "),Mx=s(O8,"A",{href:!0});var rRr=n(Mx);XQe=r(rRr,"MobileBertTokenizerFast"),rRr.forEach(t),VQe=r(O8," (MobileBERT model)"),O8.forEach(t),zQe=i(y),_n=s(y,"LI",{});var X8=n(_n);CU=s(X8,"STRONG",{});var tRr=n(CU);WQe=r(tRr,"mpnet"),tRr.forEach(t),QQe=r(X8," \u2014 "),Ex=s(X8,"A",{href:!0});var aRr=n(Ex);HQe=r(aRr,"MPNetTokenizer"),aRr.forEach(t),UQe=r(X8," or "),yx=s(X8,"A",{href:!0});var sRr=n(yx);JQe=r(sRr,"MPNetTokenizerFast"),sRr.forEach(t),YQe=r(X8," (MPNet model)"),X8.forEach(t),KQe=i(y),bn=s(y,"LI",{});var V8=n(bn);MU=s(V8,"STRONG",{});var nRr=n(MU);ZQe=r(nRr,"mt5"),nRr.forEach(t),eHe=r(V8," \u2014 "),wx=s(V8,"A",{href:!0});var lRr=n(wx);oHe=r(lRr,"MT5Tokenizer"),lRr.forEach(t),rHe=r(V8," or "),Ax=s(V8,"A",{href:!0});var iRr=n(Ax);tHe=r(iRr,"MT5TokenizerFast"),iRr.forEach(t),aHe=r(V8," (mT5 model)"),V8.forEach(t),sHe=i(y),vn=s(y,"LI",{});var z8=n(vn);EU=s(z8,"STRONG",{});var dRr=n(EU);nHe=r(dRr,"openai-gpt"),dRr.forEach(t),lHe=r(z8," \u2014 "),Lx=s(z8,"A",{href:!0});var cRr=n(Lx);iHe=r(cRr,"OpenAIGPTTokenizer"),cRr.forEach(t),dHe=r(z8," or "),Bx=s(z8,"A",{href:!0});var mRr=n(Bx);cHe=r(mRr,"OpenAIGPTTokenizerFast"),mRr.forEach(t),mHe=r(z8," (OpenAI GPT model)"),z8.forEach(t),fHe=i(y),Tn=s(y,"LI",{});var W8=n(Tn);yU=s(W8,"STRONG",{});var fRr=n(yU);gHe=r(fRr,"pegasus"),fRr.forEach(t),hHe=r(W8," \u2014 "),xx=s(W8,"A",{href:!0});var gRr=n(xx);uHe=r(gRr,"PegasusTokenizer"),gRr.forEach(t),pHe=r(W8," or "),kx=s(W8,"A",{href:!0});var hRr=n(kx);_He=r(hRr,"PegasusTokenizerFast"),hRr.forEach(t),bHe=r(W8," (Pegasus model)"),W8.forEach(t),vHe=i(y),Ig=s(y,"LI",{});var OCe=n(Ig);wU=s(OCe,"STRONG",{});var uRr=n(wU);THe=r(uRr,"perceiver"),uRr.forEach(t),FHe=r(OCe," \u2014 "),Rx=s(OCe,"A",{href:!0});var pRr=n(Rx);CHe=r(pRr,"PerceiverTokenizer"),pRr.forEach(t),MHe=r(OCe," (Perceiver model)"),OCe.forEach(t),EHe=i(y),Dg=s(y,"LI",{});var XCe=n(Dg);AU=s(XCe,"STRONG",{});var _Rr=n(AU);yHe=r(_Rr,"phobert"),_Rr.forEach(t),wHe=r(XCe," \u2014 "),Sx=s(XCe,"A",{href:!0});var bRr=n(Sx);AHe=r(bRr,"PhobertTokenizer"),bRr.forEach(t),LHe=r(XCe," (PhoBERT model)"),XCe.forEach(t),BHe=i(y),jg=s(y,"LI",{});var VCe=n(jg);LU=s(VCe,"STRONG",{});var vRr=n(LU);xHe=r(vRr,"plbart"),vRr.forEach(t),kHe=r(VCe," \u2014 "),Px=s(VCe,"A",{href:!0});var TRr=n(Px);RHe=r(TRr,"PLBartTokenizer"),TRr.forEach(t),SHe=r(VCe," (PLBart model)"),VCe.forEach(t),PHe=i(y),Ng=s(y,"LI",{});var zCe=n(Ng);BU=s(zCe,"STRONG",{});var FRr=n(BU);$He=r(FRr,"prophetnet"),FRr.forEach(t),IHe=r(zCe," \u2014 "),$x=s(zCe,"A",{href:!0});var CRr=n($x);DHe=r(CRr,"ProphetNetTokenizer"),CRr.forEach(t),jHe=r(zCe," (ProphetNet model)"),zCe.forEach(t),NHe=i(y),Fn=s(y,"LI",{});var Q8=n(Fn);xU=s(Q8,"STRONG",{});var MRr=n(xU);qHe=r(MRr,"qdqbert"),MRr.forEach(t),GHe=r(Q8," \u2014 "),Ix=s(Q8,"A",{href:!0});var ERr=n(Ix);OHe=r(ERr,"BertTokenizer"),ERr.forEach(t),XHe=r(Q8," or "),Dx=s(Q8,"A",{href:!0});var yRr=n(Dx);VHe=r(yRr,"BertTokenizerFast"),yRr.forEach(t),zHe=r(Q8," (QDQBert model)"),Q8.forEach(t),WHe=i(y),qg=s(y,"LI",{});var WCe=n(qg);kU=s(WCe,"STRONG",{});var wRr=n(kU);QHe=r(wRr,"rag"),wRr.forEach(t),HHe=r(WCe," \u2014 "),jx=s(WCe,"A",{href:!0});var ARr=n(jx);UHe=r(ARr,"RagTokenizer"),ARr.forEach(t),JHe=r(WCe," (RAG model)"),WCe.forEach(t),YHe=i(y),Cn=s(y,"LI",{});var H8=n(Cn);RU=s(H8,"STRONG",{});var LRr=n(RU);KHe=r(LRr,"realm"),LRr.forEach(t),ZHe=r(H8," \u2014 "),Nx=s(H8,"A",{href:!0});var BRr=n(Nx);eUe=r(BRr,"RealmTokenizer"),BRr.forEach(t),oUe=r(H8," or "),qx=s(H8,"A",{href:!0});var xRr=n(qx);rUe=r(xRr,"RealmTokenizerFast"),xRr.forEach(t),tUe=r(H8," (Realm model)"),H8.forEach(t),aUe=i(y),Mn=s(y,"LI",{});var U8=n(Mn);SU=s(U8,"STRONG",{});var kRr=n(SU);sUe=r(kRr,"reformer"),kRr.forEach(t),nUe=r(U8," \u2014 "),Gx=s(U8,"A",{href:!0});var RRr=n(Gx);lUe=r(RRr,"ReformerTokenizer"),RRr.forEach(t),iUe=r(U8," or "),Ox=s(U8,"A",{href:!0});var SRr=n(Ox);dUe=r(SRr,"ReformerTokenizerFast"),SRr.forEach(t),cUe=r(U8," (Reformer model)"),U8.forEach(t),mUe=i(y),En=s(y,"LI",{});var J8=n(En);PU=s(J8,"STRONG",{});var PRr=n(PU);fUe=r(PRr,"rembert"),PRr.forEach(t),gUe=r(J8," \u2014 "),Xx=s(J8,"A",{href:!0});var $Rr=n(Xx);hUe=r($Rr,"RemBertTokenizer"),$Rr.forEach(t),uUe=r(J8," or "),Vx=s(J8,"A",{href:!0});var IRr=n(Vx);pUe=r(IRr,"RemBertTokenizerFast"),IRr.forEach(t),_Ue=r(J8," (RemBERT model)"),J8.forEach(t),bUe=i(y),yn=s(y,"LI",{});var Y8=n(yn);$U=s(Y8,"STRONG",{});var DRr=n($U);vUe=r(DRr,"retribert"),DRr.forEach(t),TUe=r(Y8," \u2014 "),zx=s(Y8,"A",{href:!0});var jRr=n(zx);FUe=r(jRr,"RetriBertTokenizer"),jRr.forEach(t),CUe=r(Y8," or "),Wx=s(Y8,"A",{href:!0});var NRr=n(Wx);MUe=r(NRr,"RetriBertTokenizerFast"),NRr.forEach(t),EUe=r(Y8," (RetriBERT model)"),Y8.forEach(t),yUe=i(y),wn=s(y,"LI",{});var K8=n(wn);IU=s(K8,"STRONG",{});var qRr=n(IU);wUe=r(qRr,"roberta"),qRr.forEach(t),AUe=r(K8," \u2014 "),Qx=s(K8,"A",{href:!0});var GRr=n(Qx);LUe=r(GRr,"RobertaTokenizer"),GRr.forEach(t),BUe=r(K8," or "),Hx=s(K8,"A",{href:!0});var ORr=n(Hx);xUe=r(ORr,"RobertaTokenizerFast"),ORr.forEach(t),kUe=r(K8," (RoBERTa model)"),K8.forEach(t),RUe=i(y),An=s(y,"LI",{});var Z8=n(An);DU=s(Z8,"STRONG",{});var XRr=n(DU);SUe=r(XRr,"roformer"),XRr.forEach(t),PUe=r(Z8," \u2014 "),Ux=s(Z8,"A",{href:!0});var VRr=n(Ux);$Ue=r(VRr,"RoFormerTokenizer"),VRr.forEach(t),IUe=r(Z8," or "),Jx=s(Z8,"A",{href:!0});var zRr=n(Jx);DUe=r(zRr,"RoFormerTokenizerFast"),zRr.forEach(t),jUe=r(Z8," (RoFormer model)"),Z8.forEach(t),NUe=i(y),Gg=s(y,"LI",{});var QCe=n(Gg);jU=s(QCe,"STRONG",{});var WRr=n(jU);qUe=r(WRr,"speech_to_text"),WRr.forEach(t),GUe=r(QCe," \u2014 "),Yx=s(QCe,"A",{href:!0});var QRr=n(Yx);OUe=r(QRr,"Speech2TextTokenizer"),QRr.forEach(t),XUe=r(QCe," (Speech2Text model)"),QCe.forEach(t),VUe=i(y),Og=s(y,"LI",{});var HCe=n(Og);NU=s(HCe,"STRONG",{});var HRr=n(NU);zUe=r(HRr,"speech_to_text_2"),HRr.forEach(t),WUe=r(HCe," \u2014 "),Kx=s(HCe,"A",{href:!0});var URr=n(Kx);QUe=r(URr,"Speech2Text2Tokenizer"),URr.forEach(t),HUe=r(HCe," (Speech2Text2 model)"),HCe.forEach(t),UUe=i(y),Ln=s(y,"LI",{});var e7=n(Ln);qU=s(e7,"STRONG",{});var JRr=n(qU);JUe=r(JRr,"splinter"),JRr.forEach(t),YUe=r(e7," \u2014 "),Zx=s(e7,"A",{href:!0});var YRr=n(Zx);KUe=r(YRr,"SplinterTokenizer"),YRr.forEach(t),ZUe=r(e7," or "),ek=s(e7,"A",{href:!0});var KRr=n(ek);eJe=r(KRr,"SplinterTokenizerFast"),KRr.forEach(t),oJe=r(e7," (Splinter model)"),e7.forEach(t),rJe=i(y),Bn=s(y,"LI",{});var o7=n(Bn);GU=s(o7,"STRONG",{});var ZRr=n(GU);tJe=r(ZRr,"squeezebert"),ZRr.forEach(t),aJe=r(o7," \u2014 "),ok=s(o7,"A",{href:!0});var eSr=n(ok);sJe=r(eSr,"SqueezeBertTokenizer"),eSr.forEach(t),nJe=r(o7," or "),rk=s(o7,"A",{href:!0});var oSr=n(rk);lJe=r(oSr,"SqueezeBertTokenizerFast"),oSr.forEach(t),iJe=r(o7," (SqueezeBERT model)"),o7.forEach(t),dJe=i(y),xn=s(y,"LI",{});var r7=n(xn);OU=s(r7,"STRONG",{});var rSr=n(OU);cJe=r(rSr,"t5"),rSr.forEach(t),mJe=r(r7," \u2014 "),tk=s(r7,"A",{href:!0});var tSr=n(tk);fJe=r(tSr,"T5Tokenizer"),tSr.forEach(t),gJe=r(r7," or "),ak=s(r7,"A",{href:!0});var aSr=n(ak);hJe=r(aSr,"T5TokenizerFast"),aSr.forEach(t),uJe=r(r7," (T5 model)"),r7.forEach(t),pJe=i(y),Xg=s(y,"LI",{});var UCe=n(Xg);XU=s(UCe,"STRONG",{});var sSr=n(XU);_Je=r(sSr,"tapas"),sSr.forEach(t),bJe=r(UCe," \u2014 "),sk=s(UCe,"A",{href:!0});var nSr=n(sk);vJe=r(nSr,"TapasTokenizer"),nSr.forEach(t),TJe=r(UCe," (TAPAS model)"),UCe.forEach(t),FJe=i(y),Vg=s(y,"LI",{});var JCe=n(Vg);VU=s(JCe,"STRONG",{});var lSr=n(VU);CJe=r(lSr,"transfo-xl"),lSr.forEach(t),MJe=r(JCe," \u2014 "),nk=s(JCe,"A",{href:!0});var iSr=n(nk);EJe=r(iSr,"TransfoXLTokenizer"),iSr.forEach(t),yJe=r(JCe," (Transformer-XL model)"),JCe.forEach(t),wJe=i(y),zg=s(y,"LI",{});var YCe=n(zg);zU=s(YCe,"STRONG",{});var dSr=n(zU);AJe=r(dSr,"wav2vec2"),dSr.forEach(t),LJe=r(YCe," \u2014 "),lk=s(YCe,"A",{href:!0});var cSr=n(lk);BJe=r(cSr,"Wav2Vec2CTCTokenizer"),cSr.forEach(t),xJe=r(YCe," (Wav2Vec2 model)"),YCe.forEach(t),kJe=i(y),Wg=s(y,"LI",{});var KCe=n(Wg);WU=s(KCe,"STRONG",{});var mSr=n(WU);RJe=r(mSr,"wav2vec2_phoneme"),mSr.forEach(t),SJe=r(KCe," \u2014 "),ik=s(KCe,"A",{href:!0});var fSr=n(ik);PJe=r(fSr,"Wav2Vec2PhonemeCTCTokenizer"),fSr.forEach(t),$Je=r(KCe," (Wav2Vec2Phoneme model)"),KCe.forEach(t),IJe=i(y),kn=s(y,"LI",{});var t7=n(kn);QU=s(t7,"STRONG",{});var gSr=n(QU);DJe=r(gSr,"xglm"),gSr.forEach(t),jJe=r(t7," \u2014 "),dk=s(t7,"A",{href:!0});var hSr=n(dk);NJe=r(hSr,"XGLMTokenizer"),hSr.forEach(t),qJe=r(t7," or "),ck=s(t7,"A",{href:!0});var uSr=n(ck);GJe=r(uSr,"XGLMTokenizerFast"),uSr.forEach(t),OJe=r(t7," (XGLM model)"),t7.forEach(t),XJe=i(y),Qg=s(y,"LI",{});var ZCe=n(Qg);HU=s(ZCe,"STRONG",{});var pSr=n(HU);VJe=r(pSr,"xlm"),pSr.forEach(t),zJe=r(ZCe," \u2014 "),mk=s(ZCe,"A",{href:!0});var _Sr=n(mk);WJe=r(_Sr,"XLMTokenizer"),_Sr.forEach(t),QJe=r(ZCe," (XLM model)"),ZCe.forEach(t),HJe=i(y),Hg=s(y,"LI",{});var e4e=n(Hg);UU=s(e4e,"STRONG",{});var bSr=n(UU);UJe=r(bSr,"xlm-prophetnet"),bSr.forEach(t),JJe=r(e4e," \u2014 "),fk=s(e4e,"A",{href:!0});var vSr=n(fk);YJe=r(vSr,"XLMProphetNetTokenizer"),vSr.forEach(t),KJe=r(e4e," (XLMProphetNet model)"),e4e.forEach(t),ZJe=i(y),Rn=s(y,"LI",{});var a7=n(Rn);JU=s(a7,"STRONG",{});var TSr=n(JU);eYe=r(TSr,"xlm-roberta"),TSr.forEach(t),oYe=r(a7," \u2014 "),gk=s(a7,"A",{href:!0});var FSr=n(gk);rYe=r(FSr,"XLMRobertaTokenizer"),FSr.forEach(t),tYe=r(a7," or "),hk=s(a7,"A",{href:!0});var CSr=n(hk);aYe=r(CSr,"XLMRobertaTokenizerFast"),CSr.forEach(t),sYe=r(a7," (XLM-RoBERTa model)"),a7.forEach(t),nYe=i(y),Sn=s(y,"LI",{});var s7=n(Sn);YU=s(s7,"STRONG",{});var MSr=n(YU);lYe=r(MSr,"xlnet"),MSr.forEach(t),iYe=r(s7," \u2014 "),uk=s(s7,"A",{href:!0});var ESr=n(uk);dYe=r(ESr,"XLNetTokenizer"),ESr.forEach(t),cYe=r(s7," or "),pk=s(s7,"A",{href:!0});var ySr=n(pk);mYe=r(ySr,"XLNetTokenizerFast"),ySr.forEach(t),fYe=r(s7," (XLNet model)"),s7.forEach(t),y.forEach(t),gYe=i(da),KU=s(da,"P",{});var wSr=n(KU);hYe=r(wSr,"Examples:"),wSr.forEach(t),uYe=i(da),f(G5.$$.fragment,da),da.forEach(t),pYe=i(jn),Ug=s(jn,"DIV",{class:!0});var Gxe=n(Ug);f(O5.$$.fragment,Gxe),_Ye=i(Gxe),ZU=s(Gxe,"P",{});var ASr=n(ZU);bYe=r(ASr,"Register a new tokenizer in this mapping."),ASr.forEach(t),Gxe.forEach(t),jn.forEach(t),N9e=i(c),qi=s(c,"H2",{class:!0});var Oxe=n(qi);Jg=s(Oxe,"A",{id:!0,class:!0,href:!0});var LSr=n(Jg);eJ=s(LSr,"SPAN",{});var BSr=n(eJ);f(X5.$$.fragment,BSr),BSr.forEach(t),LSr.forEach(t),vYe=i(Oxe),oJ=s(Oxe,"SPAN",{});var xSr=n(oJ);TYe=r(xSr,"AutoFeatureExtractor"),xSr.forEach(t),Oxe.forEach(t),q9e=i(c),zo=s(c,"DIV",{class:!0});var Nn=n(zo);f(V5.$$.fragment,Nn),FYe=i(Nn),z5=s(Nn,"P",{});var Xxe=n(z5);CYe=r(Xxe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),_k=s(Xxe,"A",{href:!0});var kSr=n(_k);MYe=r(kSr,"AutoFeatureExtractor.from_pretrained()"),kSr.forEach(t),EYe=r(Xxe," class method."),Xxe.forEach(t),yYe=i(Nn),W5=s(Nn,"P",{});var Vxe=n(W5);wYe=r(Vxe,"This class cannot be instantiated directly using "),rJ=s(Vxe,"CODE",{});var RSr=n(rJ);AYe=r(RSr,"__init__()"),RSr.forEach(t),LYe=r(Vxe," (throws an error)."),Vxe.forEach(t),BYe=i(Nn),xe=s(Nn,"DIV",{class:!0});var St=n(xe);f(Q5.$$.fragment,St),xYe=i(St),tJ=s(St,"P",{});var SSr=n(tJ);kYe=r(SSr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),SSr.forEach(t),RYe=i(St),ja=s(St,"P",{});var w3=n(ja);SYe=r(w3,"The feature extractor class to instantiate is selected based on the "),aJ=s(w3,"CODE",{});var PSr=n(aJ);PYe=r(PSr,"model_type"),PSr.forEach(t),$Ye=r(w3,` property of the config object
(either passed as an argument or loaded from `),sJ=s(w3,"CODE",{});var $Sr=n(sJ);IYe=r($Sr,"pretrained_model_name_or_path"),$Sr.forEach(t),DYe=r(w3,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),nJ=s(w3,"CODE",{});var ISr=n(nJ);jYe=r(ISr,"pretrained_model_name_or_path"),ISr.forEach(t),NYe=r(w3,":"),w3.forEach(t),qYe=i(St),ne=s(St,"UL",{});var de=n(ne);Yg=s(de,"LI",{});var o4e=n(Yg);lJ=s(o4e,"STRONG",{});var DSr=n(lJ);GYe=r(DSr,"beit"),DSr.forEach(t),OYe=r(o4e," \u2014 "),bk=s(o4e,"A",{href:!0});var jSr=n(bk);XYe=r(jSr,"BeitFeatureExtractor"),jSr.forEach(t),VYe=r(o4e," (BEiT model)"),o4e.forEach(t),zYe=i(de),Kg=s(de,"LI",{});var r4e=n(Kg);iJ=s(r4e,"STRONG",{});var NSr=n(iJ);WYe=r(NSr,"clip"),NSr.forEach(t),QYe=r(r4e," \u2014 "),vk=s(r4e,"A",{href:!0});var qSr=n(vk);HYe=r(qSr,"CLIPFeatureExtractor"),qSr.forEach(t),UYe=r(r4e," (CLIP model)"),r4e.forEach(t),JYe=i(de),Zg=s(de,"LI",{});var t4e=n(Zg);dJ=s(t4e,"STRONG",{});var GSr=n(dJ);YYe=r(GSr,"convnext"),GSr.forEach(t),KYe=r(t4e," \u2014 "),Tk=s(t4e,"A",{href:!0});var OSr=n(Tk);ZYe=r(OSr,"ConvNextFeatureExtractor"),OSr.forEach(t),eKe=r(t4e," (ConvNext model)"),t4e.forEach(t),oKe=i(de),eh=s(de,"LI",{});var a4e=n(eh);cJ=s(a4e,"STRONG",{});var XSr=n(cJ);rKe=r(XSr,"deit"),XSr.forEach(t),tKe=r(a4e," \u2014 "),Fk=s(a4e,"A",{href:!0});var VSr=n(Fk);aKe=r(VSr,"DeiTFeatureExtractor"),VSr.forEach(t),sKe=r(a4e," (DeiT model)"),a4e.forEach(t),nKe=i(de),oh=s(de,"LI",{});var s4e=n(oh);mJ=s(s4e,"STRONG",{});var zSr=n(mJ);lKe=r(zSr,"detr"),zSr.forEach(t),iKe=r(s4e," \u2014 "),Ck=s(s4e,"A",{href:!0});var WSr=n(Ck);dKe=r(WSr,"DetrFeatureExtractor"),WSr.forEach(t),cKe=r(s4e," (DETR model)"),s4e.forEach(t),mKe=i(de),rh=s(de,"LI",{});var n4e=n(rh);fJ=s(n4e,"STRONG",{});var QSr=n(fJ);fKe=r(QSr,"hubert"),QSr.forEach(t),gKe=r(n4e," \u2014 "),Mk=s(n4e,"A",{href:!0});var HSr=n(Mk);hKe=r(HSr,"Wav2Vec2FeatureExtractor"),HSr.forEach(t),uKe=r(n4e," (Hubert model)"),n4e.forEach(t),pKe=i(de),th=s(de,"LI",{});var l4e=n(th);gJ=s(l4e,"STRONG",{});var USr=n(gJ);_Ke=r(USr,"layoutlmv2"),USr.forEach(t),bKe=r(l4e," \u2014 "),Ek=s(l4e,"A",{href:!0});var JSr=n(Ek);vKe=r(JSr,"LayoutLMv2FeatureExtractor"),JSr.forEach(t),TKe=r(l4e," (LayoutLMv2 model)"),l4e.forEach(t),FKe=i(de),ah=s(de,"LI",{});var i4e=n(ah);hJ=s(i4e,"STRONG",{});var YSr=n(hJ);CKe=r(YSr,"perceiver"),YSr.forEach(t),MKe=r(i4e," \u2014 "),yk=s(i4e,"A",{href:!0});var KSr=n(yk);EKe=r(KSr,"PerceiverFeatureExtractor"),KSr.forEach(t),yKe=r(i4e," (Perceiver model)"),i4e.forEach(t),wKe=i(de),sh=s(de,"LI",{});var d4e=n(sh);uJ=s(d4e,"STRONG",{});var ZSr=n(uJ);AKe=r(ZSr,"poolformer"),ZSr.forEach(t),LKe=r(d4e," \u2014 "),wk=s(d4e,"A",{href:!0});var ePr=n(wk);BKe=r(ePr,"PoolFormerFeatureExtractor"),ePr.forEach(t),xKe=r(d4e," (PoolFormer model)"),d4e.forEach(t),kKe=i(de),nh=s(de,"LI",{});var c4e=n(nh);pJ=s(c4e,"STRONG",{});var oPr=n(pJ);RKe=r(oPr,"segformer"),oPr.forEach(t),SKe=r(c4e," \u2014 "),Ak=s(c4e,"A",{href:!0});var rPr=n(Ak);PKe=r(rPr,"SegformerFeatureExtractor"),rPr.forEach(t),$Ke=r(c4e," (SegFormer model)"),c4e.forEach(t),IKe=i(de),lh=s(de,"LI",{});var m4e=n(lh);_J=s(m4e,"STRONG",{});var tPr=n(_J);DKe=r(tPr,"speech_to_text"),tPr.forEach(t),jKe=r(m4e," \u2014 "),Lk=s(m4e,"A",{href:!0});var aPr=n(Lk);NKe=r(aPr,"Speech2TextFeatureExtractor"),aPr.forEach(t),qKe=r(m4e," (Speech2Text model)"),m4e.forEach(t),GKe=i(de),ih=s(de,"LI",{});var f4e=n(ih);bJ=s(f4e,"STRONG",{});var sPr=n(bJ);OKe=r(sPr,"swin"),sPr.forEach(t),XKe=r(f4e," \u2014 "),Bk=s(f4e,"A",{href:!0});var nPr=n(Bk);VKe=r(nPr,"ViTFeatureExtractor"),nPr.forEach(t),zKe=r(f4e," (Swin model)"),f4e.forEach(t),WKe=i(de),dh=s(de,"LI",{});var g4e=n(dh);vJ=s(g4e,"STRONG",{});var lPr=n(vJ);QKe=r(lPr,"vit"),lPr.forEach(t),HKe=r(g4e," \u2014 "),xk=s(g4e,"A",{href:!0});var iPr=n(xk);UKe=r(iPr,"ViTFeatureExtractor"),iPr.forEach(t),JKe=r(g4e," (ViT model)"),g4e.forEach(t),YKe=i(de),ch=s(de,"LI",{});var h4e=n(ch);TJ=s(h4e,"STRONG",{});var dPr=n(TJ);KKe=r(dPr,"vit_mae"),dPr.forEach(t),ZKe=r(h4e," \u2014 "),kk=s(h4e,"A",{href:!0});var cPr=n(kk);eZe=r(cPr,"ViTFeatureExtractor"),cPr.forEach(t),oZe=r(h4e," (ViTMAE model)"),h4e.forEach(t),rZe=i(de),mh=s(de,"LI",{});var u4e=n(mh);FJ=s(u4e,"STRONG",{});var mPr=n(FJ);tZe=r(mPr,"wav2vec2"),mPr.forEach(t),aZe=r(u4e," \u2014 "),Rk=s(u4e,"A",{href:!0});var fPr=n(Rk);sZe=r(fPr,"Wav2Vec2FeatureExtractor"),fPr.forEach(t),nZe=r(u4e," (Wav2Vec2 model)"),u4e.forEach(t),de.forEach(t),lZe=i(St),f(fh.$$.fragment,St),iZe=i(St),CJ=s(St,"P",{});var gPr=n(CJ);dZe=r(gPr,"Examples:"),gPr.forEach(t),cZe=i(St),f(H5.$$.fragment,St),St.forEach(t),mZe=i(Nn),gh=s(Nn,"DIV",{class:!0});var zxe=n(gh);f(U5.$$.fragment,zxe),fZe=i(zxe),MJ=s(zxe,"P",{});var hPr=n(MJ);gZe=r(hPr,"Register a new feature extractor for this class."),hPr.forEach(t),zxe.forEach(t),Nn.forEach(t),G9e=i(c),Gi=s(c,"H2",{class:!0});var Wxe=n(Gi);hh=s(Wxe,"A",{id:!0,class:!0,href:!0});var uPr=n(hh);EJ=s(uPr,"SPAN",{});var pPr=n(EJ);f(J5.$$.fragment,pPr),pPr.forEach(t),uPr.forEach(t),hZe=i(Wxe),yJ=s(Wxe,"SPAN",{});var _Pr=n(yJ);uZe=r(_Pr,"AutoProcessor"),_Pr.forEach(t),Wxe.forEach(t),O9e=i(c),Wo=s(c,"DIV",{class:!0});var qn=n(Wo);f(Y5.$$.fragment,qn),pZe=i(qn),K5=s(qn,"P",{});var Qxe=n(K5);_Ze=r(Qxe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),Sk=s(Qxe,"A",{href:!0});var bPr=n(Sk);bZe=r(bPr,"AutoProcessor.from_pretrained()"),bPr.forEach(t),vZe=r(Qxe," class method."),Qxe.forEach(t),TZe=i(qn),Z5=s(qn,"P",{});var Hxe=n(Z5);FZe=r(Hxe,"This class cannot be instantiated directly using "),wJ=s(Hxe,"CODE",{});var vPr=n(wJ);CZe=r(vPr,"__init__()"),vPr.forEach(t),MZe=r(Hxe," (throws an error)."),Hxe.forEach(t),EZe=i(qn),ke=s(qn,"DIV",{class:!0});var Pt=n(ke);f(ey.$$.fragment,Pt),yZe=i(Pt),AJ=s(Pt,"P",{});var TPr=n(AJ);wZe=r(TPr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),TPr.forEach(t),AZe=i(Pt),Oi=s(Pt,"P",{});var IV=n(Oi);LZe=r(IV,"The processor class to instantiate is selected based on the "),LJ=s(IV,"CODE",{});var FPr=n(LJ);BZe=r(FPr,"model_type"),FPr.forEach(t),xZe=r(IV,` property of the config object (either
passed as an argument or loaded from `),BJ=s(IV,"CODE",{});var CPr=n(BJ);kZe=r(CPr,"pretrained_model_name_or_path"),CPr.forEach(t),RZe=r(IV," if possible):"),IV.forEach(t),SZe=i(Pt),we=s(Pt,"UL",{});var jo=n(we);uh=s(jo,"LI",{});var p4e=n(uh);xJ=s(p4e,"STRONG",{});var MPr=n(xJ);PZe=r(MPr,"clip"),MPr.forEach(t),$Ze=r(p4e," \u2014 "),Pk=s(p4e,"A",{href:!0});var EPr=n(Pk);IZe=r(EPr,"CLIPProcessor"),EPr.forEach(t),DZe=r(p4e," (CLIP model)"),p4e.forEach(t),jZe=i(jo),ph=s(jo,"LI",{});var _4e=n(ph);kJ=s(_4e,"STRONG",{});var yPr=n(kJ);NZe=r(yPr,"layoutlmv2"),yPr.forEach(t),qZe=r(_4e," \u2014 "),$k=s(_4e,"A",{href:!0});var wPr=n($k);GZe=r(wPr,"LayoutLMv2Processor"),wPr.forEach(t),OZe=r(_4e," (LayoutLMv2 model)"),_4e.forEach(t),XZe=i(jo),_h=s(jo,"LI",{});var b4e=n(_h);RJ=s(b4e,"STRONG",{});var APr=n(RJ);VZe=r(APr,"layoutxlm"),APr.forEach(t),zZe=r(b4e," \u2014 "),Ik=s(b4e,"A",{href:!0});var LPr=n(Ik);WZe=r(LPr,"LayoutXLMProcessor"),LPr.forEach(t),QZe=r(b4e," (LayoutXLM model)"),b4e.forEach(t),HZe=i(jo),bh=s(jo,"LI",{});var v4e=n(bh);SJ=s(v4e,"STRONG",{});var BPr=n(SJ);UZe=r(BPr,"speech_to_text"),BPr.forEach(t),JZe=r(v4e," \u2014 "),Dk=s(v4e,"A",{href:!0});var xPr=n(Dk);YZe=r(xPr,"Speech2TextProcessor"),xPr.forEach(t),KZe=r(v4e," (Speech2Text model)"),v4e.forEach(t),ZZe=i(jo),vh=s(jo,"LI",{});var T4e=n(vh);PJ=s(T4e,"STRONG",{});var kPr=n(PJ);eeo=r(kPr,"speech_to_text_2"),kPr.forEach(t),oeo=r(T4e," \u2014 "),jk=s(T4e,"A",{href:!0});var RPr=n(jk);reo=r(RPr,"Speech2Text2Processor"),RPr.forEach(t),teo=r(T4e," (Speech2Text2 model)"),T4e.forEach(t),aeo=i(jo),Th=s(jo,"LI",{});var F4e=n(Th);$J=s(F4e,"STRONG",{});var SPr=n($J);seo=r(SPr,"trocr"),SPr.forEach(t),neo=r(F4e," \u2014 "),Nk=s(F4e,"A",{href:!0});var PPr=n(Nk);leo=r(PPr,"TrOCRProcessor"),PPr.forEach(t),ieo=r(F4e," (TrOCR model)"),F4e.forEach(t),deo=i(jo),Fh=s(jo,"LI",{});var C4e=n(Fh);IJ=s(C4e,"STRONG",{});var $Pr=n(IJ);ceo=r($Pr,"vision-text-dual-encoder"),$Pr.forEach(t),meo=r(C4e," \u2014 "),qk=s(C4e,"A",{href:!0});var IPr=n(qk);feo=r(IPr,"VisionTextDualEncoderProcessor"),IPr.forEach(t),geo=r(C4e," (VisionTextDualEncoder model)"),C4e.forEach(t),heo=i(jo),Ch=s(jo,"LI",{});var M4e=n(Ch);DJ=s(M4e,"STRONG",{});var DPr=n(DJ);ueo=r(DPr,"wav2vec2"),DPr.forEach(t),peo=r(M4e," \u2014 "),Gk=s(M4e,"A",{href:!0});var jPr=n(Gk);_eo=r(jPr,"Wav2Vec2Processor"),jPr.forEach(t),beo=r(M4e," (Wav2Vec2 model)"),M4e.forEach(t),jo.forEach(t),veo=i(Pt),f(Mh.$$.fragment,Pt),Teo=i(Pt),jJ=s(Pt,"P",{});var NPr=n(jJ);Feo=r(NPr,"Examples:"),NPr.forEach(t),Ceo=i(Pt),f(oy.$$.fragment,Pt),Pt.forEach(t),Meo=i(qn),Eh=s(qn,"DIV",{class:!0});var Uxe=n(Eh);f(ry.$$.fragment,Uxe),Eeo=i(Uxe),NJ=s(Uxe,"P",{});var qPr=n(NJ);yeo=r(qPr,"Register a new processor for this class."),qPr.forEach(t),Uxe.forEach(t),qn.forEach(t),X9e=i(c),Xi=s(c,"H2",{class:!0});var Jxe=n(Xi);yh=s(Jxe,"A",{id:!0,class:!0,href:!0});var GPr=n(yh);qJ=s(GPr,"SPAN",{});var OPr=n(qJ);f(ty.$$.fragment,OPr),OPr.forEach(t),GPr.forEach(t),weo=i(Jxe),GJ=s(Jxe,"SPAN",{});var XPr=n(GJ);Aeo=r(XPr,"AutoModel"),XPr.forEach(t),Jxe.forEach(t),V9e=i(c),Qo=s(c,"DIV",{class:!0});var Gn=n(Qo);f(ay.$$.fragment,Gn),Leo=i(Gn),Vi=s(Gn,"P",{});var DV=n(Vi);Beo=r(DV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),OJ=s(DV,"CODE",{});var VPr=n(OJ);xeo=r(VPr,"from_pretrained()"),VPr.forEach(t),keo=r(DV,"class method or the "),XJ=s(DV,"CODE",{});var zPr=n(XJ);Reo=r(zPr,"from_config()"),zPr.forEach(t),Seo=r(DV,`class
method.`),DV.forEach(t),Peo=i(Gn),sy=s(Gn,"P",{});var Yxe=n(sy);$eo=r(Yxe,"This class cannot be instantiated directly using "),VJ=s(Yxe,"CODE",{});var WPr=n(VJ);Ieo=r(WPr,"__init__()"),WPr.forEach(t),Deo=r(Yxe," (throws an error)."),Yxe.forEach(t),jeo=i(Gn),qr=s(Gn,"DIV",{class:!0});var On=n(qr);f(ny.$$.fragment,On),Neo=i(On),zJ=s(On,"P",{});var QPr=n(zJ);qeo=r(QPr,"Instantiates one of the base model classes of the library from a configuration."),QPr.forEach(t),Geo=i(On),zi=s(On,"P",{});var jV=n(zi);Oeo=r(jV,`Note:
Loading a model from its configuration file does `),WJ=s(jV,"STRONG",{});var HPr=n(WJ);Xeo=r(HPr,"not"),HPr.forEach(t),Veo=r(jV,` load the model weights. It only affects the
model\u2019s configuration. Use `),QJ=s(jV,"CODE",{});var UPr=n(QJ);zeo=r(UPr,"from_pretrained()"),UPr.forEach(t),Weo=r(jV,"to load the model weights."),jV.forEach(t),Qeo=i(On),HJ=s(On,"P",{});var JPr=n(HJ);Heo=r(JPr,"Examples:"),JPr.forEach(t),Ueo=i(On),f(ly.$$.fragment,On),On.forEach(t),Jeo=i(Gn),Re=s(Gn,"DIV",{class:!0});var $t=n(Re);f(iy.$$.fragment,$t),Yeo=i($t),UJ=s($t,"P",{});var YPr=n(UJ);Keo=r(YPr,"Instantiate one of the base model classes of the library from a pretrained model."),YPr.forEach(t),Zeo=i($t),Na=s($t,"P",{});var A3=n(Na);eoo=r(A3,"The model class to instantiate is selected based on the "),JJ=s(A3,"CODE",{});var KPr=n(JJ);ooo=r(KPr,"model_type"),KPr.forEach(t),roo=r(A3,` property of the config object (either
passed as an argument or loaded from `),YJ=s(A3,"CODE",{});var ZPr=n(YJ);too=r(ZPr,"pretrained_model_name_or_path"),ZPr.forEach(t),aoo=r(A3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),KJ=s(A3,"CODE",{});var e$r=n(KJ);soo=r(e$r,"pretrained_model_name_or_path"),e$r.forEach(t),noo=r(A3,":"),A3.forEach(t),loo=i($t),F=s($t,"UL",{});var C=n(F);wh=s(C,"LI",{});var E4e=n(wh);ZJ=s(E4e,"STRONG",{});var o$r=n(ZJ);ioo=r(o$r,"albert"),o$r.forEach(t),doo=r(E4e," \u2014 "),Ok=s(E4e,"A",{href:!0});var r$r=n(Ok);coo=r(r$r,"AlbertModel"),r$r.forEach(t),moo=r(E4e," (ALBERT model)"),E4e.forEach(t),foo=i(C),Ah=s(C,"LI",{});var y4e=n(Ah);eY=s(y4e,"STRONG",{});var t$r=n(eY);goo=r(t$r,"bart"),t$r.forEach(t),hoo=r(y4e," \u2014 "),Xk=s(y4e,"A",{href:!0});var a$r=n(Xk);uoo=r(a$r,"BartModel"),a$r.forEach(t),poo=r(y4e," (BART model)"),y4e.forEach(t),_oo=i(C),Lh=s(C,"LI",{});var w4e=n(Lh);oY=s(w4e,"STRONG",{});var s$r=n(oY);boo=r(s$r,"beit"),s$r.forEach(t),voo=r(w4e," \u2014 "),Vk=s(w4e,"A",{href:!0});var n$r=n(Vk);Too=r(n$r,"BeitModel"),n$r.forEach(t),Foo=r(w4e," (BEiT model)"),w4e.forEach(t),Coo=i(C),Bh=s(C,"LI",{});var A4e=n(Bh);rY=s(A4e,"STRONG",{});var l$r=n(rY);Moo=r(l$r,"bert"),l$r.forEach(t),Eoo=r(A4e," \u2014 "),zk=s(A4e,"A",{href:!0});var i$r=n(zk);yoo=r(i$r,"BertModel"),i$r.forEach(t),woo=r(A4e," (BERT model)"),A4e.forEach(t),Aoo=i(C),xh=s(C,"LI",{});var L4e=n(xh);tY=s(L4e,"STRONG",{});var d$r=n(tY);Loo=r(d$r,"bert-generation"),d$r.forEach(t),Boo=r(L4e," \u2014 "),Wk=s(L4e,"A",{href:!0});var c$r=n(Wk);xoo=r(c$r,"BertGenerationEncoder"),c$r.forEach(t),koo=r(L4e," (Bert Generation model)"),L4e.forEach(t),Roo=i(C),kh=s(C,"LI",{});var B4e=n(kh);aY=s(B4e,"STRONG",{});var m$r=n(aY);Soo=r(m$r,"big_bird"),m$r.forEach(t),Poo=r(B4e," \u2014 "),Qk=s(B4e,"A",{href:!0});var f$r=n(Qk);$oo=r(f$r,"BigBirdModel"),f$r.forEach(t),Ioo=r(B4e," (BigBird model)"),B4e.forEach(t),Doo=i(C),Rh=s(C,"LI",{});var x4e=n(Rh);sY=s(x4e,"STRONG",{});var g$r=n(sY);joo=r(g$r,"bigbird_pegasus"),g$r.forEach(t),Noo=r(x4e," \u2014 "),Hk=s(x4e,"A",{href:!0});var h$r=n(Hk);qoo=r(h$r,"BigBirdPegasusModel"),h$r.forEach(t),Goo=r(x4e," (BigBirdPegasus model)"),x4e.forEach(t),Ooo=i(C),Sh=s(C,"LI",{});var k4e=n(Sh);nY=s(k4e,"STRONG",{});var u$r=n(nY);Xoo=r(u$r,"blenderbot"),u$r.forEach(t),Voo=r(k4e," \u2014 "),Uk=s(k4e,"A",{href:!0});var p$r=n(Uk);zoo=r(p$r,"BlenderbotModel"),p$r.forEach(t),Woo=r(k4e," (Blenderbot model)"),k4e.forEach(t),Qoo=i(C),Ph=s(C,"LI",{});var R4e=n(Ph);lY=s(R4e,"STRONG",{});var _$r=n(lY);Hoo=r(_$r,"blenderbot-small"),_$r.forEach(t),Uoo=r(R4e," \u2014 "),Jk=s(R4e,"A",{href:!0});var b$r=n(Jk);Joo=r(b$r,"BlenderbotSmallModel"),b$r.forEach(t),Yoo=r(R4e," (BlenderbotSmall model)"),R4e.forEach(t),Koo=i(C),$h=s(C,"LI",{});var S4e=n($h);iY=s(S4e,"STRONG",{});var v$r=n(iY);Zoo=r(v$r,"camembert"),v$r.forEach(t),ero=r(S4e," \u2014 "),Yk=s(S4e,"A",{href:!0});var T$r=n(Yk);oro=r(T$r,"CamembertModel"),T$r.forEach(t),rro=r(S4e," (CamemBERT model)"),S4e.forEach(t),tro=i(C),Ih=s(C,"LI",{});var P4e=n(Ih);dY=s(P4e,"STRONG",{});var F$r=n(dY);aro=r(F$r,"canine"),F$r.forEach(t),sro=r(P4e," \u2014 "),Kk=s(P4e,"A",{href:!0});var C$r=n(Kk);nro=r(C$r,"CanineModel"),C$r.forEach(t),lro=r(P4e," (Canine model)"),P4e.forEach(t),iro=i(C),Dh=s(C,"LI",{});var $4e=n(Dh);cY=s($4e,"STRONG",{});var M$r=n(cY);dro=r(M$r,"clip"),M$r.forEach(t),cro=r($4e," \u2014 "),Zk=s($4e,"A",{href:!0});var E$r=n(Zk);mro=r(E$r,"CLIPModel"),E$r.forEach(t),fro=r($4e," (CLIP model)"),$4e.forEach(t),gro=i(C),jh=s(C,"LI",{});var I4e=n(jh);mY=s(I4e,"STRONG",{});var y$r=n(mY);hro=r(y$r,"convbert"),y$r.forEach(t),uro=r(I4e," \u2014 "),eR=s(I4e,"A",{href:!0});var w$r=n(eR);pro=r(w$r,"ConvBertModel"),w$r.forEach(t),_ro=r(I4e," (ConvBERT model)"),I4e.forEach(t),bro=i(C),Nh=s(C,"LI",{});var D4e=n(Nh);fY=s(D4e,"STRONG",{});var A$r=n(fY);vro=r(A$r,"convnext"),A$r.forEach(t),Tro=r(D4e," \u2014 "),oR=s(D4e,"A",{href:!0});var L$r=n(oR);Fro=r(L$r,"ConvNextModel"),L$r.forEach(t),Cro=r(D4e," (ConvNext model)"),D4e.forEach(t),Mro=i(C),qh=s(C,"LI",{});var j4e=n(qh);gY=s(j4e,"STRONG",{});var B$r=n(gY);Ero=r(B$r,"ctrl"),B$r.forEach(t),yro=r(j4e," \u2014 "),rR=s(j4e,"A",{href:!0});var x$r=n(rR);wro=r(x$r,"CTRLModel"),x$r.forEach(t),Aro=r(j4e," (CTRL model)"),j4e.forEach(t),Lro=i(C),Gh=s(C,"LI",{});var N4e=n(Gh);hY=s(N4e,"STRONG",{});var k$r=n(hY);Bro=r(k$r,"data2vec-audio"),k$r.forEach(t),xro=r(N4e," \u2014 "),tR=s(N4e,"A",{href:!0});var R$r=n(tR);kro=r(R$r,"Data2VecAudioModel"),R$r.forEach(t),Rro=r(N4e," (Data2VecAudio model)"),N4e.forEach(t),Sro=i(C),Oh=s(C,"LI",{});var q4e=n(Oh);uY=s(q4e,"STRONG",{});var S$r=n(uY);Pro=r(S$r,"data2vec-text"),S$r.forEach(t),$ro=r(q4e," \u2014 "),aR=s(q4e,"A",{href:!0});var P$r=n(aR);Iro=r(P$r,"Data2VecTextModel"),P$r.forEach(t),Dro=r(q4e," (Data2VecText model)"),q4e.forEach(t),jro=i(C),Xh=s(C,"LI",{});var G4e=n(Xh);pY=s(G4e,"STRONG",{});var $$r=n(pY);Nro=r($$r,"deberta"),$$r.forEach(t),qro=r(G4e," \u2014 "),sR=s(G4e,"A",{href:!0});var I$r=n(sR);Gro=r(I$r,"DebertaModel"),I$r.forEach(t),Oro=r(G4e," (DeBERTa model)"),G4e.forEach(t),Xro=i(C),Vh=s(C,"LI",{});var O4e=n(Vh);_Y=s(O4e,"STRONG",{});var D$r=n(_Y);Vro=r(D$r,"deberta-v2"),D$r.forEach(t),zro=r(O4e," \u2014 "),nR=s(O4e,"A",{href:!0});var j$r=n(nR);Wro=r(j$r,"DebertaV2Model"),j$r.forEach(t),Qro=r(O4e," (DeBERTa-v2 model)"),O4e.forEach(t),Hro=i(C),zh=s(C,"LI",{});var X4e=n(zh);bY=s(X4e,"STRONG",{});var N$r=n(bY);Uro=r(N$r,"deit"),N$r.forEach(t),Jro=r(X4e," \u2014 "),lR=s(X4e,"A",{href:!0});var q$r=n(lR);Yro=r(q$r,"DeiTModel"),q$r.forEach(t),Kro=r(X4e," (DeiT model)"),X4e.forEach(t),Zro=i(C),Wh=s(C,"LI",{});var V4e=n(Wh);vY=s(V4e,"STRONG",{});var G$r=n(vY);eto=r(G$r,"detr"),G$r.forEach(t),oto=r(V4e," \u2014 "),iR=s(V4e,"A",{href:!0});var O$r=n(iR);rto=r(O$r,"DetrModel"),O$r.forEach(t),tto=r(V4e," (DETR model)"),V4e.forEach(t),ato=i(C),Qh=s(C,"LI",{});var z4e=n(Qh);TY=s(z4e,"STRONG",{});var X$r=n(TY);sto=r(X$r,"distilbert"),X$r.forEach(t),nto=r(z4e," \u2014 "),dR=s(z4e,"A",{href:!0});var V$r=n(dR);lto=r(V$r,"DistilBertModel"),V$r.forEach(t),ito=r(z4e," (DistilBERT model)"),z4e.forEach(t),dto=i(C),Hh=s(C,"LI",{});var W4e=n(Hh);FY=s(W4e,"STRONG",{});var z$r=n(FY);cto=r(z$r,"dpr"),z$r.forEach(t),mto=r(W4e," \u2014 "),cR=s(W4e,"A",{href:!0});var W$r=n(cR);fto=r(W$r,"DPRQuestionEncoder"),W$r.forEach(t),gto=r(W4e," (DPR model)"),W4e.forEach(t),hto=i(C),Uh=s(C,"LI",{});var Q4e=n(Uh);CY=s(Q4e,"STRONG",{});var Q$r=n(CY);uto=r(Q$r,"electra"),Q$r.forEach(t),pto=r(Q4e," \u2014 "),mR=s(Q4e,"A",{href:!0});var H$r=n(mR);_to=r(H$r,"ElectraModel"),H$r.forEach(t),bto=r(Q4e," (ELECTRA model)"),Q4e.forEach(t),vto=i(C),Jh=s(C,"LI",{});var H4e=n(Jh);MY=s(H4e,"STRONG",{});var U$r=n(MY);Tto=r(U$r,"flaubert"),U$r.forEach(t),Fto=r(H4e," \u2014 "),fR=s(H4e,"A",{href:!0});var J$r=n(fR);Cto=r(J$r,"FlaubertModel"),J$r.forEach(t),Mto=r(H4e," (FlauBERT model)"),H4e.forEach(t),Eto=i(C),Yh=s(C,"LI",{});var U4e=n(Yh);EY=s(U4e,"STRONG",{});var Y$r=n(EY);yto=r(Y$r,"fnet"),Y$r.forEach(t),wto=r(U4e," \u2014 "),gR=s(U4e,"A",{href:!0});var K$r=n(gR);Ato=r(K$r,"FNetModel"),K$r.forEach(t),Lto=r(U4e," (FNet model)"),U4e.forEach(t),Bto=i(C),Kh=s(C,"LI",{});var J4e=n(Kh);yY=s(J4e,"STRONG",{});var Z$r=n(yY);xto=r(Z$r,"fsmt"),Z$r.forEach(t),kto=r(J4e," \u2014 "),hR=s(J4e,"A",{href:!0});var eIr=n(hR);Rto=r(eIr,"FSMTModel"),eIr.forEach(t),Sto=r(J4e," (FairSeq Machine-Translation model)"),J4e.forEach(t),Pto=i(C),Pn=s(C,"LI",{});var n7=n(Pn);wY=s(n7,"STRONG",{});var oIr=n(wY);$to=r(oIr,"funnel"),oIr.forEach(t),Ito=r(n7," \u2014 "),uR=s(n7,"A",{href:!0});var rIr=n(uR);Dto=r(rIr,"FunnelModel"),rIr.forEach(t),jto=r(n7," or "),pR=s(n7,"A",{href:!0});var tIr=n(pR);Nto=r(tIr,"FunnelBaseModel"),tIr.forEach(t),qto=r(n7," (Funnel Transformer model)"),n7.forEach(t),Gto=i(C),Zh=s(C,"LI",{});var Y4e=n(Zh);AY=s(Y4e,"STRONG",{});var aIr=n(AY);Oto=r(aIr,"gpt2"),aIr.forEach(t),Xto=r(Y4e," \u2014 "),_R=s(Y4e,"A",{href:!0});var sIr=n(_R);Vto=r(sIr,"GPT2Model"),sIr.forEach(t),zto=r(Y4e," (OpenAI GPT-2 model)"),Y4e.forEach(t),Wto=i(C),eu=s(C,"LI",{});var K4e=n(eu);LY=s(K4e,"STRONG",{});var nIr=n(LY);Qto=r(nIr,"gpt_neo"),nIr.forEach(t),Hto=r(K4e," \u2014 "),bR=s(K4e,"A",{href:!0});var lIr=n(bR);Uto=r(lIr,"GPTNeoModel"),lIr.forEach(t),Jto=r(K4e," (GPT Neo model)"),K4e.forEach(t),Yto=i(C),ou=s(C,"LI",{});var Z4e=n(ou);BY=s(Z4e,"STRONG",{});var iIr=n(BY);Kto=r(iIr,"gptj"),iIr.forEach(t),Zto=r(Z4e," \u2014 "),vR=s(Z4e,"A",{href:!0});var dIr=n(vR);eao=r(dIr,"GPTJModel"),dIr.forEach(t),oao=r(Z4e," (GPT-J model)"),Z4e.forEach(t),rao=i(C),ru=s(C,"LI",{});var eMe=n(ru);xY=s(eMe,"STRONG",{});var cIr=n(xY);tao=r(cIr,"hubert"),cIr.forEach(t),aao=r(eMe," \u2014 "),TR=s(eMe,"A",{href:!0});var mIr=n(TR);sao=r(mIr,"HubertModel"),mIr.forEach(t),nao=r(eMe," (Hubert model)"),eMe.forEach(t),lao=i(C),tu=s(C,"LI",{});var oMe=n(tu);kY=s(oMe,"STRONG",{});var fIr=n(kY);iao=r(fIr,"ibert"),fIr.forEach(t),dao=r(oMe," \u2014 "),FR=s(oMe,"A",{href:!0});var gIr=n(FR);cao=r(gIr,"IBertModel"),gIr.forEach(t),mao=r(oMe," (I-BERT model)"),oMe.forEach(t),fao=i(C),au=s(C,"LI",{});var rMe=n(au);RY=s(rMe,"STRONG",{});var hIr=n(RY);gao=r(hIr,"imagegpt"),hIr.forEach(t),hao=r(rMe," \u2014 "),CR=s(rMe,"A",{href:!0});var uIr=n(CR);uao=r(uIr,"ImageGPTModel"),uIr.forEach(t),pao=r(rMe," (ImageGPT model)"),rMe.forEach(t),_ao=i(C),su=s(C,"LI",{});var tMe=n(su);SY=s(tMe,"STRONG",{});var pIr=n(SY);bao=r(pIr,"layoutlm"),pIr.forEach(t),vao=r(tMe," \u2014 "),MR=s(tMe,"A",{href:!0});var _Ir=n(MR);Tao=r(_Ir,"LayoutLMModel"),_Ir.forEach(t),Fao=r(tMe," (LayoutLM model)"),tMe.forEach(t),Cao=i(C),nu=s(C,"LI",{});var aMe=n(nu);PY=s(aMe,"STRONG",{});var bIr=n(PY);Mao=r(bIr,"layoutlmv2"),bIr.forEach(t),Eao=r(aMe," \u2014 "),ER=s(aMe,"A",{href:!0});var vIr=n(ER);yao=r(vIr,"LayoutLMv2Model"),vIr.forEach(t),wao=r(aMe," (LayoutLMv2 model)"),aMe.forEach(t),Aao=i(C),lu=s(C,"LI",{});var sMe=n(lu);$Y=s(sMe,"STRONG",{});var TIr=n($Y);Lao=r(TIr,"led"),TIr.forEach(t),Bao=r(sMe," \u2014 "),yR=s(sMe,"A",{href:!0});var FIr=n(yR);xao=r(FIr,"LEDModel"),FIr.forEach(t),kao=r(sMe," (LED model)"),sMe.forEach(t),Rao=i(C),iu=s(C,"LI",{});var nMe=n(iu);IY=s(nMe,"STRONG",{});var CIr=n(IY);Sao=r(CIr,"longformer"),CIr.forEach(t),Pao=r(nMe," \u2014 "),wR=s(nMe,"A",{href:!0});var MIr=n(wR);$ao=r(MIr,"LongformerModel"),MIr.forEach(t),Iao=r(nMe," (Longformer model)"),nMe.forEach(t),Dao=i(C),du=s(C,"LI",{});var lMe=n(du);DY=s(lMe,"STRONG",{});var EIr=n(DY);jao=r(EIr,"luke"),EIr.forEach(t),Nao=r(lMe," \u2014 "),AR=s(lMe,"A",{href:!0});var yIr=n(AR);qao=r(yIr,"LukeModel"),yIr.forEach(t),Gao=r(lMe," (LUKE model)"),lMe.forEach(t),Oao=i(C),cu=s(C,"LI",{});var iMe=n(cu);jY=s(iMe,"STRONG",{});var wIr=n(jY);Xao=r(wIr,"lxmert"),wIr.forEach(t),Vao=r(iMe," \u2014 "),LR=s(iMe,"A",{href:!0});var AIr=n(LR);zao=r(AIr,"LxmertModel"),AIr.forEach(t),Wao=r(iMe," (LXMERT model)"),iMe.forEach(t),Qao=i(C),mu=s(C,"LI",{});var dMe=n(mu);NY=s(dMe,"STRONG",{});var LIr=n(NY);Hao=r(LIr,"m2m_100"),LIr.forEach(t),Uao=r(dMe," \u2014 "),BR=s(dMe,"A",{href:!0});var BIr=n(BR);Jao=r(BIr,"M2M100Model"),BIr.forEach(t),Yao=r(dMe," (M2M100 model)"),dMe.forEach(t),Kao=i(C),fu=s(C,"LI",{});var cMe=n(fu);qY=s(cMe,"STRONG",{});var xIr=n(qY);Zao=r(xIr,"marian"),xIr.forEach(t),eso=r(cMe," \u2014 "),xR=s(cMe,"A",{href:!0});var kIr=n(xR);oso=r(kIr,"MarianModel"),kIr.forEach(t),rso=r(cMe," (Marian model)"),cMe.forEach(t),tso=i(C),gu=s(C,"LI",{});var mMe=n(gu);GY=s(mMe,"STRONG",{});var RIr=n(GY);aso=r(RIr,"maskformer"),RIr.forEach(t),sso=r(mMe," \u2014 "),kR=s(mMe,"A",{href:!0});var SIr=n(kR);nso=r(SIr,"MaskFormerModel"),SIr.forEach(t),lso=r(mMe," (MaskFormer model)"),mMe.forEach(t),iso=i(C),hu=s(C,"LI",{});var fMe=n(hu);OY=s(fMe,"STRONG",{});var PIr=n(OY);dso=r(PIr,"mbart"),PIr.forEach(t),cso=r(fMe," \u2014 "),RR=s(fMe,"A",{href:!0});var $Ir=n(RR);mso=r($Ir,"MBartModel"),$Ir.forEach(t),fso=r(fMe," (mBART model)"),fMe.forEach(t),gso=i(C),uu=s(C,"LI",{});var gMe=n(uu);XY=s(gMe,"STRONG",{});var IIr=n(XY);hso=r(IIr,"megatron-bert"),IIr.forEach(t),uso=r(gMe," \u2014 "),SR=s(gMe,"A",{href:!0});var DIr=n(SR);pso=r(DIr,"MegatronBertModel"),DIr.forEach(t),_so=r(gMe," (MegatronBert model)"),gMe.forEach(t),bso=i(C),pu=s(C,"LI",{});var hMe=n(pu);VY=s(hMe,"STRONG",{});var jIr=n(VY);vso=r(jIr,"mobilebert"),jIr.forEach(t),Tso=r(hMe," \u2014 "),PR=s(hMe,"A",{href:!0});var NIr=n(PR);Fso=r(NIr,"MobileBertModel"),NIr.forEach(t),Cso=r(hMe," (MobileBERT model)"),hMe.forEach(t),Mso=i(C),_u=s(C,"LI",{});var uMe=n(_u);zY=s(uMe,"STRONG",{});var qIr=n(zY);Eso=r(qIr,"mpnet"),qIr.forEach(t),yso=r(uMe," \u2014 "),$R=s(uMe,"A",{href:!0});var GIr=n($R);wso=r(GIr,"MPNetModel"),GIr.forEach(t),Aso=r(uMe," (MPNet model)"),uMe.forEach(t),Lso=i(C),bu=s(C,"LI",{});var pMe=n(bu);WY=s(pMe,"STRONG",{});var OIr=n(WY);Bso=r(OIr,"mt5"),OIr.forEach(t),xso=r(pMe," \u2014 "),IR=s(pMe,"A",{href:!0});var XIr=n(IR);kso=r(XIr,"MT5Model"),XIr.forEach(t),Rso=r(pMe," (mT5 model)"),pMe.forEach(t),Sso=i(C),vu=s(C,"LI",{});var _Me=n(vu);QY=s(_Me,"STRONG",{});var VIr=n(QY);Pso=r(VIr,"nystromformer"),VIr.forEach(t),$so=r(_Me," \u2014 "),DR=s(_Me,"A",{href:!0});var zIr=n(DR);Iso=r(zIr,"NystromformerModel"),zIr.forEach(t),Dso=r(_Me," (Nystromformer model)"),_Me.forEach(t),jso=i(C),Tu=s(C,"LI",{});var bMe=n(Tu);HY=s(bMe,"STRONG",{});var WIr=n(HY);Nso=r(WIr,"openai-gpt"),WIr.forEach(t),qso=r(bMe," \u2014 "),jR=s(bMe,"A",{href:!0});var QIr=n(jR);Gso=r(QIr,"OpenAIGPTModel"),QIr.forEach(t),Oso=r(bMe," (OpenAI GPT model)"),bMe.forEach(t),Xso=i(C),Fu=s(C,"LI",{});var vMe=n(Fu);UY=s(vMe,"STRONG",{});var HIr=n(UY);Vso=r(HIr,"pegasus"),HIr.forEach(t),zso=r(vMe," \u2014 "),NR=s(vMe,"A",{href:!0});var UIr=n(NR);Wso=r(UIr,"PegasusModel"),UIr.forEach(t),Qso=r(vMe," (Pegasus model)"),vMe.forEach(t),Hso=i(C),Cu=s(C,"LI",{});var TMe=n(Cu);JY=s(TMe,"STRONG",{});var JIr=n(JY);Uso=r(JIr,"perceiver"),JIr.forEach(t),Jso=r(TMe," \u2014 "),qR=s(TMe,"A",{href:!0});var YIr=n(qR);Yso=r(YIr,"PerceiverModel"),YIr.forEach(t),Kso=r(TMe," (Perceiver model)"),TMe.forEach(t),Zso=i(C),Mu=s(C,"LI",{});var FMe=n(Mu);YY=s(FMe,"STRONG",{});var KIr=n(YY);eno=r(KIr,"plbart"),KIr.forEach(t),ono=r(FMe," \u2014 "),GR=s(FMe,"A",{href:!0});var ZIr=n(GR);rno=r(ZIr,"PLBartModel"),ZIr.forEach(t),tno=r(FMe," (PLBart model)"),FMe.forEach(t),ano=i(C),Eu=s(C,"LI",{});var CMe=n(Eu);KY=s(CMe,"STRONG",{});var eDr=n(KY);sno=r(eDr,"poolformer"),eDr.forEach(t),nno=r(CMe," \u2014 "),OR=s(CMe,"A",{href:!0});var oDr=n(OR);lno=r(oDr,"PoolFormerModel"),oDr.forEach(t),ino=r(CMe," (PoolFormer model)"),CMe.forEach(t),dno=i(C),yu=s(C,"LI",{});var MMe=n(yu);ZY=s(MMe,"STRONG",{});var rDr=n(ZY);cno=r(rDr,"prophetnet"),rDr.forEach(t),mno=r(MMe," \u2014 "),XR=s(MMe,"A",{href:!0});var tDr=n(XR);fno=r(tDr,"ProphetNetModel"),tDr.forEach(t),gno=r(MMe," (ProphetNet model)"),MMe.forEach(t),hno=i(C),wu=s(C,"LI",{});var EMe=n(wu);eK=s(EMe,"STRONG",{});var aDr=n(eK);uno=r(aDr,"qdqbert"),aDr.forEach(t),pno=r(EMe," \u2014 "),VR=s(EMe,"A",{href:!0});var sDr=n(VR);_no=r(sDr,"QDQBertModel"),sDr.forEach(t),bno=r(EMe," (QDQBert model)"),EMe.forEach(t),vno=i(C),Au=s(C,"LI",{});var yMe=n(Au);oK=s(yMe,"STRONG",{});var nDr=n(oK);Tno=r(nDr,"reformer"),nDr.forEach(t),Fno=r(yMe," \u2014 "),zR=s(yMe,"A",{href:!0});var lDr=n(zR);Cno=r(lDr,"ReformerModel"),lDr.forEach(t),Mno=r(yMe," (Reformer model)"),yMe.forEach(t),Eno=i(C),Lu=s(C,"LI",{});var wMe=n(Lu);rK=s(wMe,"STRONG",{});var iDr=n(rK);yno=r(iDr,"rembert"),iDr.forEach(t),wno=r(wMe," \u2014 "),WR=s(wMe,"A",{href:!0});var dDr=n(WR);Ano=r(dDr,"RemBertModel"),dDr.forEach(t),Lno=r(wMe," (RemBERT model)"),wMe.forEach(t),Bno=i(C),Bu=s(C,"LI",{});var AMe=n(Bu);tK=s(AMe,"STRONG",{});var cDr=n(tK);xno=r(cDr,"retribert"),cDr.forEach(t),kno=r(AMe," \u2014 "),QR=s(AMe,"A",{href:!0});var mDr=n(QR);Rno=r(mDr,"RetriBertModel"),mDr.forEach(t),Sno=r(AMe," (RetriBERT model)"),AMe.forEach(t),Pno=i(C),xu=s(C,"LI",{});var LMe=n(xu);aK=s(LMe,"STRONG",{});var fDr=n(aK);$no=r(fDr,"roberta"),fDr.forEach(t),Ino=r(LMe," \u2014 "),HR=s(LMe,"A",{href:!0});var gDr=n(HR);Dno=r(gDr,"RobertaModel"),gDr.forEach(t),jno=r(LMe," (RoBERTa model)"),LMe.forEach(t),Nno=i(C),ku=s(C,"LI",{});var BMe=n(ku);sK=s(BMe,"STRONG",{});var hDr=n(sK);qno=r(hDr,"roformer"),hDr.forEach(t),Gno=r(BMe," \u2014 "),UR=s(BMe,"A",{href:!0});var uDr=n(UR);Ono=r(uDr,"RoFormerModel"),uDr.forEach(t),Xno=r(BMe," (RoFormer model)"),BMe.forEach(t),Vno=i(C),Ru=s(C,"LI",{});var xMe=n(Ru);nK=s(xMe,"STRONG",{});var pDr=n(nK);zno=r(pDr,"segformer"),pDr.forEach(t),Wno=r(xMe," \u2014 "),JR=s(xMe,"A",{href:!0});var _Dr=n(JR);Qno=r(_Dr,"SegformerModel"),_Dr.forEach(t),Hno=r(xMe," (SegFormer model)"),xMe.forEach(t),Uno=i(C),Su=s(C,"LI",{});var kMe=n(Su);lK=s(kMe,"STRONG",{});var bDr=n(lK);Jno=r(bDr,"sew"),bDr.forEach(t),Yno=r(kMe," \u2014 "),YR=s(kMe,"A",{href:!0});var vDr=n(YR);Kno=r(vDr,"SEWModel"),vDr.forEach(t),Zno=r(kMe," (SEW model)"),kMe.forEach(t),elo=i(C),Pu=s(C,"LI",{});var RMe=n(Pu);iK=s(RMe,"STRONG",{});var TDr=n(iK);olo=r(TDr,"sew-d"),TDr.forEach(t),rlo=r(RMe," \u2014 "),KR=s(RMe,"A",{href:!0});var FDr=n(KR);tlo=r(FDr,"SEWDModel"),FDr.forEach(t),alo=r(RMe," (SEW-D model)"),RMe.forEach(t),slo=i(C),$u=s(C,"LI",{});var SMe=n($u);dK=s(SMe,"STRONG",{});var CDr=n(dK);nlo=r(CDr,"speech_to_text"),CDr.forEach(t),llo=r(SMe," \u2014 "),ZR=s(SMe,"A",{href:!0});var MDr=n(ZR);ilo=r(MDr,"Speech2TextModel"),MDr.forEach(t),dlo=r(SMe," (Speech2Text model)"),SMe.forEach(t),clo=i(C),Iu=s(C,"LI",{});var PMe=n(Iu);cK=s(PMe,"STRONG",{});var EDr=n(cK);mlo=r(EDr,"splinter"),EDr.forEach(t),flo=r(PMe," \u2014 "),eS=s(PMe,"A",{href:!0});var yDr=n(eS);glo=r(yDr,"SplinterModel"),yDr.forEach(t),hlo=r(PMe," (Splinter model)"),PMe.forEach(t),ulo=i(C),Du=s(C,"LI",{});var $Me=n(Du);mK=s($Me,"STRONG",{});var wDr=n(mK);plo=r(wDr,"squeezebert"),wDr.forEach(t),_lo=r($Me," \u2014 "),oS=s($Me,"A",{href:!0});var ADr=n(oS);blo=r(ADr,"SqueezeBertModel"),ADr.forEach(t),vlo=r($Me," (SqueezeBERT model)"),$Me.forEach(t),Tlo=i(C),ju=s(C,"LI",{});var IMe=n(ju);fK=s(IMe,"STRONG",{});var LDr=n(fK);Flo=r(LDr,"swin"),LDr.forEach(t),Clo=r(IMe," \u2014 "),rS=s(IMe,"A",{href:!0});var BDr=n(rS);Mlo=r(BDr,"SwinModel"),BDr.forEach(t),Elo=r(IMe," (Swin model)"),IMe.forEach(t),ylo=i(C),Nu=s(C,"LI",{});var DMe=n(Nu);gK=s(DMe,"STRONG",{});var xDr=n(gK);wlo=r(xDr,"t5"),xDr.forEach(t),Alo=r(DMe," \u2014 "),tS=s(DMe,"A",{href:!0});var kDr=n(tS);Llo=r(kDr,"T5Model"),kDr.forEach(t),Blo=r(DMe," (T5 model)"),DMe.forEach(t),xlo=i(C),qu=s(C,"LI",{});var jMe=n(qu);hK=s(jMe,"STRONG",{});var RDr=n(hK);klo=r(RDr,"tapas"),RDr.forEach(t),Rlo=r(jMe," \u2014 "),aS=s(jMe,"A",{href:!0});var SDr=n(aS);Slo=r(SDr,"TapasModel"),SDr.forEach(t),Plo=r(jMe," (TAPAS model)"),jMe.forEach(t),$lo=i(C),Gu=s(C,"LI",{});var NMe=n(Gu);uK=s(NMe,"STRONG",{});var PDr=n(uK);Ilo=r(PDr,"transfo-xl"),PDr.forEach(t),Dlo=r(NMe," \u2014 "),sS=s(NMe,"A",{href:!0});var $Dr=n(sS);jlo=r($Dr,"TransfoXLModel"),$Dr.forEach(t),Nlo=r(NMe," (Transformer-XL model)"),NMe.forEach(t),qlo=i(C),Ou=s(C,"LI",{});var qMe=n(Ou);pK=s(qMe,"STRONG",{});var IDr=n(pK);Glo=r(IDr,"unispeech"),IDr.forEach(t),Olo=r(qMe," \u2014 "),nS=s(qMe,"A",{href:!0});var DDr=n(nS);Xlo=r(DDr,"UniSpeechModel"),DDr.forEach(t),Vlo=r(qMe," (UniSpeech model)"),qMe.forEach(t),zlo=i(C),Xu=s(C,"LI",{});var GMe=n(Xu);_K=s(GMe,"STRONG",{});var jDr=n(_K);Wlo=r(jDr,"unispeech-sat"),jDr.forEach(t),Qlo=r(GMe," \u2014 "),lS=s(GMe,"A",{href:!0});var NDr=n(lS);Hlo=r(NDr,"UniSpeechSatModel"),NDr.forEach(t),Ulo=r(GMe," (UniSpeechSat model)"),GMe.forEach(t),Jlo=i(C),Vu=s(C,"LI",{});var OMe=n(Vu);bK=s(OMe,"STRONG",{});var qDr=n(bK);Ylo=r(qDr,"vilt"),qDr.forEach(t),Klo=r(OMe," \u2014 "),iS=s(OMe,"A",{href:!0});var GDr=n(iS);Zlo=r(GDr,"ViltModel"),GDr.forEach(t),eio=r(OMe," (ViLT model)"),OMe.forEach(t),oio=i(C),zu=s(C,"LI",{});var XMe=n(zu);vK=s(XMe,"STRONG",{});var ODr=n(vK);rio=r(ODr,"vision-text-dual-encoder"),ODr.forEach(t),tio=r(XMe," \u2014 "),dS=s(XMe,"A",{href:!0});var XDr=n(dS);aio=r(XDr,"VisionTextDualEncoderModel"),XDr.forEach(t),sio=r(XMe," (VisionTextDualEncoder model)"),XMe.forEach(t),nio=i(C),Wu=s(C,"LI",{});var VMe=n(Wu);TK=s(VMe,"STRONG",{});var VDr=n(TK);lio=r(VDr,"visual_bert"),VDr.forEach(t),iio=r(VMe," \u2014 "),cS=s(VMe,"A",{href:!0});var zDr=n(cS);dio=r(zDr,"VisualBertModel"),zDr.forEach(t),cio=r(VMe," (VisualBert model)"),VMe.forEach(t),mio=i(C),Qu=s(C,"LI",{});var zMe=n(Qu);FK=s(zMe,"STRONG",{});var WDr=n(FK);fio=r(WDr,"vit"),WDr.forEach(t),gio=r(zMe," \u2014 "),mS=s(zMe,"A",{href:!0});var QDr=n(mS);hio=r(QDr,"ViTModel"),QDr.forEach(t),uio=r(zMe," (ViT model)"),zMe.forEach(t),pio=i(C),Hu=s(C,"LI",{});var WMe=n(Hu);CK=s(WMe,"STRONG",{});var HDr=n(CK);_io=r(HDr,"vit_mae"),HDr.forEach(t),bio=r(WMe," \u2014 "),fS=s(WMe,"A",{href:!0});var UDr=n(fS);vio=r(UDr,"ViTMAEModel"),UDr.forEach(t),Tio=r(WMe," (ViTMAE model)"),WMe.forEach(t),Fio=i(C),Uu=s(C,"LI",{});var QMe=n(Uu);MK=s(QMe,"STRONG",{});var JDr=n(MK);Cio=r(JDr,"wav2vec2"),JDr.forEach(t),Mio=r(QMe," \u2014 "),gS=s(QMe,"A",{href:!0});var YDr=n(gS);Eio=r(YDr,"Wav2Vec2Model"),YDr.forEach(t),yio=r(QMe," (Wav2Vec2 model)"),QMe.forEach(t),wio=i(C),Ju=s(C,"LI",{});var HMe=n(Ju);EK=s(HMe,"STRONG",{});var KDr=n(EK);Aio=r(KDr,"wavlm"),KDr.forEach(t),Lio=r(HMe," \u2014 "),hS=s(HMe,"A",{href:!0});var ZDr=n(hS);Bio=r(ZDr,"WavLMModel"),ZDr.forEach(t),xio=r(HMe," (WavLM model)"),HMe.forEach(t),kio=i(C),Yu=s(C,"LI",{});var UMe=n(Yu);yK=s(UMe,"STRONG",{});var ejr=n(yK);Rio=r(ejr,"xglm"),ejr.forEach(t),Sio=r(UMe," \u2014 "),uS=s(UMe,"A",{href:!0});var ojr=n(uS);Pio=r(ojr,"XGLMModel"),ojr.forEach(t),$io=r(UMe," (XGLM model)"),UMe.forEach(t),Iio=i(C),Ku=s(C,"LI",{});var JMe=n(Ku);wK=s(JMe,"STRONG",{});var rjr=n(wK);Dio=r(rjr,"xlm"),rjr.forEach(t),jio=r(JMe," \u2014 "),pS=s(JMe,"A",{href:!0});var tjr=n(pS);Nio=r(tjr,"XLMModel"),tjr.forEach(t),qio=r(JMe," (XLM model)"),JMe.forEach(t),Gio=i(C),Zu=s(C,"LI",{});var YMe=n(Zu);AK=s(YMe,"STRONG",{});var ajr=n(AK);Oio=r(ajr,"xlm-prophetnet"),ajr.forEach(t),Xio=r(YMe," \u2014 "),_S=s(YMe,"A",{href:!0});var sjr=n(_S);Vio=r(sjr,"XLMProphetNetModel"),sjr.forEach(t),zio=r(YMe," (XLMProphetNet model)"),YMe.forEach(t),Wio=i(C),ep=s(C,"LI",{});var KMe=n(ep);LK=s(KMe,"STRONG",{});var njr=n(LK);Qio=r(njr,"xlm-roberta"),njr.forEach(t),Hio=r(KMe," \u2014 "),bS=s(KMe,"A",{href:!0});var ljr=n(bS);Uio=r(ljr,"XLMRobertaModel"),ljr.forEach(t),Jio=r(KMe," (XLM-RoBERTa model)"),KMe.forEach(t),Yio=i(C),op=s(C,"LI",{});var ZMe=n(op);BK=s(ZMe,"STRONG",{});var ijr=n(BK);Kio=r(ijr,"xlm-roberta-xl"),ijr.forEach(t),Zio=r(ZMe," \u2014 "),vS=s(ZMe,"A",{href:!0});var djr=n(vS);edo=r(djr,"XLMRobertaXLModel"),djr.forEach(t),odo=r(ZMe," (XLM-RoBERTa-XL model)"),ZMe.forEach(t),rdo=i(C),rp=s(C,"LI",{});var eEe=n(rp);xK=s(eEe,"STRONG",{});var cjr=n(xK);tdo=r(cjr,"xlnet"),cjr.forEach(t),ado=r(eEe," \u2014 "),TS=s(eEe,"A",{href:!0});var mjr=n(TS);sdo=r(mjr,"XLNetModel"),mjr.forEach(t),ndo=r(eEe," (XLNet model)"),eEe.forEach(t),ldo=i(C),tp=s(C,"LI",{});var oEe=n(tp);kK=s(oEe,"STRONG",{});var fjr=n(kK);ido=r(fjr,"yoso"),fjr.forEach(t),ddo=r(oEe," \u2014 "),FS=s(oEe,"A",{href:!0});var gjr=n(FS);cdo=r(gjr,"YosoModel"),gjr.forEach(t),mdo=r(oEe," (YOSO model)"),oEe.forEach(t),C.forEach(t),fdo=i($t),ap=s($t,"P",{});var rEe=n(ap);gdo=r(rEe,"The model is set in evaluation mode by default using "),RK=s(rEe,"CODE",{});var hjr=n(RK);hdo=r(hjr,"model.eval()"),hjr.forEach(t),udo=r(rEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SK=s(rEe,"CODE",{});var ujr=n(SK);pdo=r(ujr,"model.train()"),ujr.forEach(t),rEe.forEach(t),_do=i($t),PK=s($t,"P",{});var pjr=n(PK);bdo=r(pjr,"Examples:"),pjr.forEach(t),vdo=i($t),f(dy.$$.fragment,$t),$t.forEach(t),Gn.forEach(t),z9e=i(c),Wi=s(c,"H2",{class:!0});var Kxe=n(Wi);sp=s(Kxe,"A",{id:!0,class:!0,href:!0});var _jr=n(sp);$K=s(_jr,"SPAN",{});var bjr=n($K);f(cy.$$.fragment,bjr),bjr.forEach(t),_jr.forEach(t),Tdo=i(Kxe),IK=s(Kxe,"SPAN",{});var vjr=n(IK);Fdo=r(vjr,"AutoModelForPreTraining"),vjr.forEach(t),Kxe.forEach(t),W9e=i(c),Ho=s(c,"DIV",{class:!0});var Xn=n(Ho);f(my.$$.fragment,Xn),Cdo=i(Xn),Qi=s(Xn,"P",{});var NV=n(Qi);Mdo=r(NV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),DK=s(NV,"CODE",{});var Tjr=n(DK);Edo=r(Tjr,"from_pretrained()"),Tjr.forEach(t),ydo=r(NV,"class method or the "),jK=s(NV,"CODE",{});var Fjr=n(jK);wdo=r(Fjr,"from_config()"),Fjr.forEach(t),Ado=r(NV,`class
method.`),NV.forEach(t),Ldo=i(Xn),fy=s(Xn,"P",{});var Zxe=n(fy);Bdo=r(Zxe,"This class cannot be instantiated directly using "),NK=s(Zxe,"CODE",{});var Cjr=n(NK);xdo=r(Cjr,"__init__()"),Cjr.forEach(t),kdo=r(Zxe," (throws an error)."),Zxe.forEach(t),Rdo=i(Xn),Gr=s(Xn,"DIV",{class:!0});var Vn=n(Gr);f(gy.$$.fragment,Vn),Sdo=i(Vn),qK=s(Vn,"P",{});var Mjr=n(qK);Pdo=r(Mjr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Mjr.forEach(t),$do=i(Vn),Hi=s(Vn,"P",{});var qV=n(Hi);Ido=r(qV,`Note:
Loading a model from its configuration file does `),GK=s(qV,"STRONG",{});var Ejr=n(GK);Ddo=r(Ejr,"not"),Ejr.forEach(t),jdo=r(qV,` load the model weights. It only affects the
model\u2019s configuration. Use `),OK=s(qV,"CODE",{});var yjr=n(OK);Ndo=r(yjr,"from_pretrained()"),yjr.forEach(t),qdo=r(qV,"to load the model weights."),qV.forEach(t),Gdo=i(Vn),XK=s(Vn,"P",{});var wjr=n(XK);Odo=r(wjr,"Examples:"),wjr.forEach(t),Xdo=i(Vn),f(hy.$$.fragment,Vn),Vn.forEach(t),Vdo=i(Xn),Se=s(Xn,"DIV",{class:!0});var It=n(Se);f(uy.$$.fragment,It),zdo=i(It),VK=s(It,"P",{});var Ajr=n(VK);Wdo=r(Ajr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ajr.forEach(t),Qdo=i(It),qa=s(It,"P",{});var L3=n(qa);Hdo=r(L3,"The model class to instantiate is selected based on the "),zK=s(L3,"CODE",{});var Ljr=n(zK);Udo=r(Ljr,"model_type"),Ljr.forEach(t),Jdo=r(L3,` property of the config object (either
passed as an argument or loaded from `),WK=s(L3,"CODE",{});var Bjr=n(WK);Ydo=r(Bjr,"pretrained_model_name_or_path"),Bjr.forEach(t),Kdo=r(L3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QK=s(L3,"CODE",{});var xjr=n(QK);Zdo=r(xjr,"pretrained_model_name_or_path"),xjr.forEach(t),eco=r(L3,":"),L3.forEach(t),oco=i(It),k=s(It,"UL",{});var S=n(k);np=s(S,"LI",{});var tEe=n(np);HK=s(tEe,"STRONG",{});var kjr=n(HK);rco=r(kjr,"albert"),kjr.forEach(t),tco=r(tEe," \u2014 "),CS=s(tEe,"A",{href:!0});var Rjr=n(CS);aco=r(Rjr,"AlbertForPreTraining"),Rjr.forEach(t),sco=r(tEe," (ALBERT model)"),tEe.forEach(t),nco=i(S),lp=s(S,"LI",{});var aEe=n(lp);UK=s(aEe,"STRONG",{});var Sjr=n(UK);lco=r(Sjr,"bart"),Sjr.forEach(t),ico=r(aEe," \u2014 "),MS=s(aEe,"A",{href:!0});var Pjr=n(MS);dco=r(Pjr,"BartForConditionalGeneration"),Pjr.forEach(t),cco=r(aEe," (BART model)"),aEe.forEach(t),mco=i(S),ip=s(S,"LI",{});var sEe=n(ip);JK=s(sEe,"STRONG",{});var $jr=n(JK);fco=r($jr,"bert"),$jr.forEach(t),gco=r(sEe," \u2014 "),ES=s(sEe,"A",{href:!0});var Ijr=n(ES);hco=r(Ijr,"BertForPreTraining"),Ijr.forEach(t),uco=r(sEe," (BERT model)"),sEe.forEach(t),pco=i(S),dp=s(S,"LI",{});var nEe=n(dp);YK=s(nEe,"STRONG",{});var Djr=n(YK);_co=r(Djr,"big_bird"),Djr.forEach(t),bco=r(nEe," \u2014 "),yS=s(nEe,"A",{href:!0});var jjr=n(yS);vco=r(jjr,"BigBirdForPreTraining"),jjr.forEach(t),Tco=r(nEe," (BigBird model)"),nEe.forEach(t),Fco=i(S),cp=s(S,"LI",{});var lEe=n(cp);KK=s(lEe,"STRONG",{});var Njr=n(KK);Cco=r(Njr,"camembert"),Njr.forEach(t),Mco=r(lEe," \u2014 "),wS=s(lEe,"A",{href:!0});var qjr=n(wS);Eco=r(qjr,"CamembertForMaskedLM"),qjr.forEach(t),yco=r(lEe," (CamemBERT model)"),lEe.forEach(t),wco=i(S),mp=s(S,"LI",{});var iEe=n(mp);ZK=s(iEe,"STRONG",{});var Gjr=n(ZK);Aco=r(Gjr,"ctrl"),Gjr.forEach(t),Lco=r(iEe," \u2014 "),AS=s(iEe,"A",{href:!0});var Ojr=n(AS);Bco=r(Ojr,"CTRLLMHeadModel"),Ojr.forEach(t),xco=r(iEe," (CTRL model)"),iEe.forEach(t),kco=i(S),fp=s(S,"LI",{});var dEe=n(fp);eZ=s(dEe,"STRONG",{});var Xjr=n(eZ);Rco=r(Xjr,"data2vec-text"),Xjr.forEach(t),Sco=r(dEe," \u2014 "),LS=s(dEe,"A",{href:!0});var Vjr=n(LS);Pco=r(Vjr,"Data2VecTextForMaskedLM"),Vjr.forEach(t),$co=r(dEe," (Data2VecText model)"),dEe.forEach(t),Ico=i(S),gp=s(S,"LI",{});var cEe=n(gp);oZ=s(cEe,"STRONG",{});var zjr=n(oZ);Dco=r(zjr,"deberta"),zjr.forEach(t),jco=r(cEe," \u2014 "),BS=s(cEe,"A",{href:!0});var Wjr=n(BS);Nco=r(Wjr,"DebertaForMaskedLM"),Wjr.forEach(t),qco=r(cEe," (DeBERTa model)"),cEe.forEach(t),Gco=i(S),hp=s(S,"LI",{});var mEe=n(hp);rZ=s(mEe,"STRONG",{});var Qjr=n(rZ);Oco=r(Qjr,"deberta-v2"),Qjr.forEach(t),Xco=r(mEe," \u2014 "),xS=s(mEe,"A",{href:!0});var Hjr=n(xS);Vco=r(Hjr,"DebertaV2ForMaskedLM"),Hjr.forEach(t),zco=r(mEe," (DeBERTa-v2 model)"),mEe.forEach(t),Wco=i(S),up=s(S,"LI",{});var fEe=n(up);tZ=s(fEe,"STRONG",{});var Ujr=n(tZ);Qco=r(Ujr,"distilbert"),Ujr.forEach(t),Hco=r(fEe," \u2014 "),kS=s(fEe,"A",{href:!0});var Jjr=n(kS);Uco=r(Jjr,"DistilBertForMaskedLM"),Jjr.forEach(t),Jco=r(fEe," (DistilBERT model)"),fEe.forEach(t),Yco=i(S),pp=s(S,"LI",{});var gEe=n(pp);aZ=s(gEe,"STRONG",{});var Yjr=n(aZ);Kco=r(Yjr,"electra"),Yjr.forEach(t),Zco=r(gEe," \u2014 "),RS=s(gEe,"A",{href:!0});var Kjr=n(RS);emo=r(Kjr,"ElectraForPreTraining"),Kjr.forEach(t),omo=r(gEe," (ELECTRA model)"),gEe.forEach(t),rmo=i(S),_p=s(S,"LI",{});var hEe=n(_p);sZ=s(hEe,"STRONG",{});var Zjr=n(sZ);tmo=r(Zjr,"flaubert"),Zjr.forEach(t),amo=r(hEe," \u2014 "),SS=s(hEe,"A",{href:!0});var eNr=n(SS);smo=r(eNr,"FlaubertWithLMHeadModel"),eNr.forEach(t),nmo=r(hEe," (FlauBERT model)"),hEe.forEach(t),lmo=i(S),bp=s(S,"LI",{});var uEe=n(bp);nZ=s(uEe,"STRONG",{});var oNr=n(nZ);imo=r(oNr,"fnet"),oNr.forEach(t),dmo=r(uEe," \u2014 "),PS=s(uEe,"A",{href:!0});var rNr=n(PS);cmo=r(rNr,"FNetForPreTraining"),rNr.forEach(t),mmo=r(uEe," (FNet model)"),uEe.forEach(t),fmo=i(S),vp=s(S,"LI",{});var pEe=n(vp);lZ=s(pEe,"STRONG",{});var tNr=n(lZ);gmo=r(tNr,"fsmt"),tNr.forEach(t),hmo=r(pEe," \u2014 "),$S=s(pEe,"A",{href:!0});var aNr=n($S);umo=r(aNr,"FSMTForConditionalGeneration"),aNr.forEach(t),pmo=r(pEe," (FairSeq Machine-Translation model)"),pEe.forEach(t),_mo=i(S),Tp=s(S,"LI",{});var _Ee=n(Tp);iZ=s(_Ee,"STRONG",{});var sNr=n(iZ);bmo=r(sNr,"funnel"),sNr.forEach(t),vmo=r(_Ee," \u2014 "),IS=s(_Ee,"A",{href:!0});var nNr=n(IS);Tmo=r(nNr,"FunnelForPreTraining"),nNr.forEach(t),Fmo=r(_Ee," (Funnel Transformer model)"),_Ee.forEach(t),Cmo=i(S),Fp=s(S,"LI",{});var bEe=n(Fp);dZ=s(bEe,"STRONG",{});var lNr=n(dZ);Mmo=r(lNr,"gpt2"),lNr.forEach(t),Emo=r(bEe," \u2014 "),DS=s(bEe,"A",{href:!0});var iNr=n(DS);ymo=r(iNr,"GPT2LMHeadModel"),iNr.forEach(t),wmo=r(bEe," (OpenAI GPT-2 model)"),bEe.forEach(t),Amo=i(S),Cp=s(S,"LI",{});var vEe=n(Cp);cZ=s(vEe,"STRONG",{});var dNr=n(cZ);Lmo=r(dNr,"ibert"),dNr.forEach(t),Bmo=r(vEe," \u2014 "),jS=s(vEe,"A",{href:!0});var cNr=n(jS);xmo=r(cNr,"IBertForMaskedLM"),cNr.forEach(t),kmo=r(vEe," (I-BERT model)"),vEe.forEach(t),Rmo=i(S),Mp=s(S,"LI",{});var TEe=n(Mp);mZ=s(TEe,"STRONG",{});var mNr=n(mZ);Smo=r(mNr,"layoutlm"),mNr.forEach(t),Pmo=r(TEe," \u2014 "),NS=s(TEe,"A",{href:!0});var fNr=n(NS);$mo=r(fNr,"LayoutLMForMaskedLM"),fNr.forEach(t),Imo=r(TEe," (LayoutLM model)"),TEe.forEach(t),Dmo=i(S),Ep=s(S,"LI",{});var FEe=n(Ep);fZ=s(FEe,"STRONG",{});var gNr=n(fZ);jmo=r(gNr,"longformer"),gNr.forEach(t),Nmo=r(FEe," \u2014 "),qS=s(FEe,"A",{href:!0});var hNr=n(qS);qmo=r(hNr,"LongformerForMaskedLM"),hNr.forEach(t),Gmo=r(FEe," (Longformer model)"),FEe.forEach(t),Omo=i(S),yp=s(S,"LI",{});var CEe=n(yp);gZ=s(CEe,"STRONG",{});var uNr=n(gZ);Xmo=r(uNr,"lxmert"),uNr.forEach(t),Vmo=r(CEe," \u2014 "),GS=s(CEe,"A",{href:!0});var pNr=n(GS);zmo=r(pNr,"LxmertForPreTraining"),pNr.forEach(t),Wmo=r(CEe," (LXMERT model)"),CEe.forEach(t),Qmo=i(S),wp=s(S,"LI",{});var MEe=n(wp);hZ=s(MEe,"STRONG",{});var _Nr=n(hZ);Hmo=r(_Nr,"megatron-bert"),_Nr.forEach(t),Umo=r(MEe," \u2014 "),OS=s(MEe,"A",{href:!0});var bNr=n(OS);Jmo=r(bNr,"MegatronBertForPreTraining"),bNr.forEach(t),Ymo=r(MEe," (MegatronBert model)"),MEe.forEach(t),Kmo=i(S),Ap=s(S,"LI",{});var EEe=n(Ap);uZ=s(EEe,"STRONG",{});var vNr=n(uZ);Zmo=r(vNr,"mobilebert"),vNr.forEach(t),efo=r(EEe," \u2014 "),XS=s(EEe,"A",{href:!0});var TNr=n(XS);ofo=r(TNr,"MobileBertForPreTraining"),TNr.forEach(t),rfo=r(EEe," (MobileBERT model)"),EEe.forEach(t),tfo=i(S),Lp=s(S,"LI",{});var yEe=n(Lp);pZ=s(yEe,"STRONG",{});var FNr=n(pZ);afo=r(FNr,"mpnet"),FNr.forEach(t),sfo=r(yEe," \u2014 "),VS=s(yEe,"A",{href:!0});var CNr=n(VS);nfo=r(CNr,"MPNetForMaskedLM"),CNr.forEach(t),lfo=r(yEe," (MPNet model)"),yEe.forEach(t),ifo=i(S),Bp=s(S,"LI",{});var wEe=n(Bp);_Z=s(wEe,"STRONG",{});var MNr=n(_Z);dfo=r(MNr,"openai-gpt"),MNr.forEach(t),cfo=r(wEe," \u2014 "),zS=s(wEe,"A",{href:!0});var ENr=n(zS);mfo=r(ENr,"OpenAIGPTLMHeadModel"),ENr.forEach(t),ffo=r(wEe," (OpenAI GPT model)"),wEe.forEach(t),gfo=i(S),xp=s(S,"LI",{});var AEe=n(xp);bZ=s(AEe,"STRONG",{});var yNr=n(bZ);hfo=r(yNr,"retribert"),yNr.forEach(t),ufo=r(AEe," \u2014 "),WS=s(AEe,"A",{href:!0});var wNr=n(WS);pfo=r(wNr,"RetriBertModel"),wNr.forEach(t),_fo=r(AEe," (RetriBERT model)"),AEe.forEach(t),bfo=i(S),kp=s(S,"LI",{});var LEe=n(kp);vZ=s(LEe,"STRONG",{});var ANr=n(vZ);vfo=r(ANr,"roberta"),ANr.forEach(t),Tfo=r(LEe," \u2014 "),QS=s(LEe,"A",{href:!0});var LNr=n(QS);Ffo=r(LNr,"RobertaForMaskedLM"),LNr.forEach(t),Cfo=r(LEe," (RoBERTa model)"),LEe.forEach(t),Mfo=i(S),Rp=s(S,"LI",{});var BEe=n(Rp);TZ=s(BEe,"STRONG",{});var BNr=n(TZ);Efo=r(BNr,"squeezebert"),BNr.forEach(t),yfo=r(BEe," \u2014 "),HS=s(BEe,"A",{href:!0});var xNr=n(HS);wfo=r(xNr,"SqueezeBertForMaskedLM"),xNr.forEach(t),Afo=r(BEe," (SqueezeBERT model)"),BEe.forEach(t),Lfo=i(S),Sp=s(S,"LI",{});var xEe=n(Sp);FZ=s(xEe,"STRONG",{});var kNr=n(FZ);Bfo=r(kNr,"t5"),kNr.forEach(t),xfo=r(xEe," \u2014 "),US=s(xEe,"A",{href:!0});var RNr=n(US);kfo=r(RNr,"T5ForConditionalGeneration"),RNr.forEach(t),Rfo=r(xEe," (T5 model)"),xEe.forEach(t),Sfo=i(S),Pp=s(S,"LI",{});var kEe=n(Pp);CZ=s(kEe,"STRONG",{});var SNr=n(CZ);Pfo=r(SNr,"tapas"),SNr.forEach(t),$fo=r(kEe," \u2014 "),JS=s(kEe,"A",{href:!0});var PNr=n(JS);Ifo=r(PNr,"TapasForMaskedLM"),PNr.forEach(t),Dfo=r(kEe," (TAPAS model)"),kEe.forEach(t),jfo=i(S),$p=s(S,"LI",{});var REe=n($p);MZ=s(REe,"STRONG",{});var $Nr=n(MZ);Nfo=r($Nr,"transfo-xl"),$Nr.forEach(t),qfo=r(REe," \u2014 "),YS=s(REe,"A",{href:!0});var INr=n(YS);Gfo=r(INr,"TransfoXLLMHeadModel"),INr.forEach(t),Ofo=r(REe," (Transformer-XL model)"),REe.forEach(t),Xfo=i(S),Ip=s(S,"LI",{});var SEe=n(Ip);EZ=s(SEe,"STRONG",{});var DNr=n(EZ);Vfo=r(DNr,"unispeech"),DNr.forEach(t),zfo=r(SEe," \u2014 "),KS=s(SEe,"A",{href:!0});var jNr=n(KS);Wfo=r(jNr,"UniSpeechForPreTraining"),jNr.forEach(t),Qfo=r(SEe," (UniSpeech model)"),SEe.forEach(t),Hfo=i(S),Dp=s(S,"LI",{});var PEe=n(Dp);yZ=s(PEe,"STRONG",{});var NNr=n(yZ);Ufo=r(NNr,"unispeech-sat"),NNr.forEach(t),Jfo=r(PEe," \u2014 "),ZS=s(PEe,"A",{href:!0});var qNr=n(ZS);Yfo=r(qNr,"UniSpeechSatForPreTraining"),qNr.forEach(t),Kfo=r(PEe," (UniSpeechSat model)"),PEe.forEach(t),Zfo=i(S),jp=s(S,"LI",{});var $Ee=n(jp);wZ=s($Ee,"STRONG",{});var GNr=n(wZ);ego=r(GNr,"visual_bert"),GNr.forEach(t),ogo=r($Ee," \u2014 "),eP=s($Ee,"A",{href:!0});var ONr=n(eP);rgo=r(ONr,"VisualBertForPreTraining"),ONr.forEach(t),tgo=r($Ee," (VisualBert model)"),$Ee.forEach(t),ago=i(S),Np=s(S,"LI",{});var IEe=n(Np);AZ=s(IEe,"STRONG",{});var XNr=n(AZ);sgo=r(XNr,"vit_mae"),XNr.forEach(t),ngo=r(IEe," \u2014 "),oP=s(IEe,"A",{href:!0});var VNr=n(oP);lgo=r(VNr,"ViTMAEForPreTraining"),VNr.forEach(t),igo=r(IEe," (ViTMAE model)"),IEe.forEach(t),dgo=i(S),qp=s(S,"LI",{});var DEe=n(qp);LZ=s(DEe,"STRONG",{});var zNr=n(LZ);cgo=r(zNr,"wav2vec2"),zNr.forEach(t),mgo=r(DEe," \u2014 "),rP=s(DEe,"A",{href:!0});var WNr=n(rP);fgo=r(WNr,"Wav2Vec2ForPreTraining"),WNr.forEach(t),ggo=r(DEe," (Wav2Vec2 model)"),DEe.forEach(t),hgo=i(S),Gp=s(S,"LI",{});var jEe=n(Gp);BZ=s(jEe,"STRONG",{});var QNr=n(BZ);ugo=r(QNr,"xlm"),QNr.forEach(t),pgo=r(jEe," \u2014 "),tP=s(jEe,"A",{href:!0});var HNr=n(tP);_go=r(HNr,"XLMWithLMHeadModel"),HNr.forEach(t),bgo=r(jEe," (XLM model)"),jEe.forEach(t),vgo=i(S),Op=s(S,"LI",{});var NEe=n(Op);xZ=s(NEe,"STRONG",{});var UNr=n(xZ);Tgo=r(UNr,"xlm-roberta"),UNr.forEach(t),Fgo=r(NEe," \u2014 "),aP=s(NEe,"A",{href:!0});var JNr=n(aP);Cgo=r(JNr,"XLMRobertaForMaskedLM"),JNr.forEach(t),Mgo=r(NEe," (XLM-RoBERTa model)"),NEe.forEach(t),Ego=i(S),Xp=s(S,"LI",{});var qEe=n(Xp);kZ=s(qEe,"STRONG",{});var YNr=n(kZ);ygo=r(YNr,"xlm-roberta-xl"),YNr.forEach(t),wgo=r(qEe," \u2014 "),sP=s(qEe,"A",{href:!0});var KNr=n(sP);Ago=r(KNr,"XLMRobertaXLForMaskedLM"),KNr.forEach(t),Lgo=r(qEe," (XLM-RoBERTa-XL model)"),qEe.forEach(t),Bgo=i(S),Vp=s(S,"LI",{});var GEe=n(Vp);RZ=s(GEe,"STRONG",{});var ZNr=n(RZ);xgo=r(ZNr,"xlnet"),ZNr.forEach(t),kgo=r(GEe," \u2014 "),nP=s(GEe,"A",{href:!0});var eqr=n(nP);Rgo=r(eqr,"XLNetLMHeadModel"),eqr.forEach(t),Sgo=r(GEe," (XLNet model)"),GEe.forEach(t),S.forEach(t),Pgo=i(It),zp=s(It,"P",{});var OEe=n(zp);$go=r(OEe,"The model is set in evaluation mode by default using "),SZ=s(OEe,"CODE",{});var oqr=n(SZ);Igo=r(oqr,"model.eval()"),oqr.forEach(t),Dgo=r(OEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),PZ=s(OEe,"CODE",{});var rqr=n(PZ);jgo=r(rqr,"model.train()"),rqr.forEach(t),OEe.forEach(t),Ngo=i(It),$Z=s(It,"P",{});var tqr=n($Z);qgo=r(tqr,"Examples:"),tqr.forEach(t),Ggo=i(It),f(py.$$.fragment,It),It.forEach(t),Xn.forEach(t),Q9e=i(c),Ui=s(c,"H2",{class:!0});var eke=n(Ui);Wp=s(eke,"A",{id:!0,class:!0,href:!0});var aqr=n(Wp);IZ=s(aqr,"SPAN",{});var sqr=n(IZ);f(_y.$$.fragment,sqr),sqr.forEach(t),aqr.forEach(t),Ogo=i(eke),DZ=s(eke,"SPAN",{});var nqr=n(DZ);Xgo=r(nqr,"AutoModelForCausalLM"),nqr.forEach(t),eke.forEach(t),H9e=i(c),Uo=s(c,"DIV",{class:!0});var zn=n(Uo);f(by.$$.fragment,zn),Vgo=i(zn),Ji=s(zn,"P",{});var GV=n(Ji);zgo=r(GV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),jZ=s(GV,"CODE",{});var lqr=n(jZ);Wgo=r(lqr,"from_pretrained()"),lqr.forEach(t),Qgo=r(GV,"class method or the "),NZ=s(GV,"CODE",{});var iqr=n(NZ);Hgo=r(iqr,"from_config()"),iqr.forEach(t),Ugo=r(GV,`class
method.`),GV.forEach(t),Jgo=i(zn),vy=s(zn,"P",{});var oke=n(vy);Ygo=r(oke,"This class cannot be instantiated directly using "),qZ=s(oke,"CODE",{});var dqr=n(qZ);Kgo=r(dqr,"__init__()"),dqr.forEach(t),Zgo=r(oke," (throws an error)."),oke.forEach(t),eho=i(zn),Or=s(zn,"DIV",{class:!0});var Wn=n(Or);f(Ty.$$.fragment,Wn),oho=i(Wn),GZ=s(Wn,"P",{});var cqr=n(GZ);rho=r(cqr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),cqr.forEach(t),tho=i(Wn),Yi=s(Wn,"P",{});var OV=n(Yi);aho=r(OV,`Note:
Loading a model from its configuration file does `),OZ=s(OV,"STRONG",{});var mqr=n(OZ);sho=r(mqr,"not"),mqr.forEach(t),nho=r(OV,` load the model weights. It only affects the
model\u2019s configuration. Use `),XZ=s(OV,"CODE",{});var fqr=n(XZ);lho=r(fqr,"from_pretrained()"),fqr.forEach(t),iho=r(OV,"to load the model weights."),OV.forEach(t),dho=i(Wn),VZ=s(Wn,"P",{});var gqr=n(VZ);cho=r(gqr,"Examples:"),gqr.forEach(t),mho=i(Wn),f(Fy.$$.fragment,Wn),Wn.forEach(t),fho=i(zn),Pe=s(zn,"DIV",{class:!0});var Dt=n(Pe);f(Cy.$$.fragment,Dt),gho=i(Dt),zZ=s(Dt,"P",{});var hqr=n(zZ);hho=r(hqr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),hqr.forEach(t),uho=i(Dt),Ga=s(Dt,"P",{});var B3=n(Ga);pho=r(B3,"The model class to instantiate is selected based on the "),WZ=s(B3,"CODE",{});var uqr=n(WZ);_ho=r(uqr,"model_type"),uqr.forEach(t),bho=r(B3,` property of the config object (either
passed as an argument or loaded from `),QZ=s(B3,"CODE",{});var pqr=n(QZ);vho=r(pqr,"pretrained_model_name_or_path"),pqr.forEach(t),Tho=r(B3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),HZ=s(B3,"CODE",{});var _qr=n(HZ);Fho=r(_qr,"pretrained_model_name_or_path"),_qr.forEach(t),Cho=r(B3,":"),B3.forEach(t),Mho=i(Dt),$=s(Dt,"UL",{});var D=n($);Qp=s(D,"LI",{});var XEe=n(Qp);UZ=s(XEe,"STRONG",{});var bqr=n(UZ);Eho=r(bqr,"bart"),bqr.forEach(t),yho=r(XEe," \u2014 "),lP=s(XEe,"A",{href:!0});var vqr=n(lP);who=r(vqr,"BartForCausalLM"),vqr.forEach(t),Aho=r(XEe," (BART model)"),XEe.forEach(t),Lho=i(D),Hp=s(D,"LI",{});var VEe=n(Hp);JZ=s(VEe,"STRONG",{});var Tqr=n(JZ);Bho=r(Tqr,"bert"),Tqr.forEach(t),xho=r(VEe," \u2014 "),iP=s(VEe,"A",{href:!0});var Fqr=n(iP);kho=r(Fqr,"BertLMHeadModel"),Fqr.forEach(t),Rho=r(VEe," (BERT model)"),VEe.forEach(t),Sho=i(D),Up=s(D,"LI",{});var zEe=n(Up);YZ=s(zEe,"STRONG",{});var Cqr=n(YZ);Pho=r(Cqr,"bert-generation"),Cqr.forEach(t),$ho=r(zEe," \u2014 "),dP=s(zEe,"A",{href:!0});var Mqr=n(dP);Iho=r(Mqr,"BertGenerationDecoder"),Mqr.forEach(t),Dho=r(zEe," (Bert Generation model)"),zEe.forEach(t),jho=i(D),Jp=s(D,"LI",{});var WEe=n(Jp);KZ=s(WEe,"STRONG",{});var Eqr=n(KZ);Nho=r(Eqr,"big_bird"),Eqr.forEach(t),qho=r(WEe," \u2014 "),cP=s(WEe,"A",{href:!0});var yqr=n(cP);Gho=r(yqr,"BigBirdForCausalLM"),yqr.forEach(t),Oho=r(WEe," (BigBird model)"),WEe.forEach(t),Xho=i(D),Yp=s(D,"LI",{});var QEe=n(Yp);ZZ=s(QEe,"STRONG",{});var wqr=n(ZZ);Vho=r(wqr,"bigbird_pegasus"),wqr.forEach(t),zho=r(QEe," \u2014 "),mP=s(QEe,"A",{href:!0});var Aqr=n(mP);Who=r(Aqr,"BigBirdPegasusForCausalLM"),Aqr.forEach(t),Qho=r(QEe," (BigBirdPegasus model)"),QEe.forEach(t),Hho=i(D),Kp=s(D,"LI",{});var HEe=n(Kp);eee=s(HEe,"STRONG",{});var Lqr=n(eee);Uho=r(Lqr,"blenderbot"),Lqr.forEach(t),Jho=r(HEe," \u2014 "),fP=s(HEe,"A",{href:!0});var Bqr=n(fP);Yho=r(Bqr,"BlenderbotForCausalLM"),Bqr.forEach(t),Kho=r(HEe," (Blenderbot model)"),HEe.forEach(t),Zho=i(D),Zp=s(D,"LI",{});var UEe=n(Zp);oee=s(UEe,"STRONG",{});var xqr=n(oee);euo=r(xqr,"blenderbot-small"),xqr.forEach(t),ouo=r(UEe," \u2014 "),gP=s(UEe,"A",{href:!0});var kqr=n(gP);ruo=r(kqr,"BlenderbotSmallForCausalLM"),kqr.forEach(t),tuo=r(UEe," (BlenderbotSmall model)"),UEe.forEach(t),auo=i(D),e_=s(D,"LI",{});var JEe=n(e_);ree=s(JEe,"STRONG",{});var Rqr=n(ree);suo=r(Rqr,"camembert"),Rqr.forEach(t),nuo=r(JEe," \u2014 "),hP=s(JEe,"A",{href:!0});var Sqr=n(hP);luo=r(Sqr,"CamembertForCausalLM"),Sqr.forEach(t),iuo=r(JEe," (CamemBERT model)"),JEe.forEach(t),duo=i(D),o_=s(D,"LI",{});var YEe=n(o_);tee=s(YEe,"STRONG",{});var Pqr=n(tee);cuo=r(Pqr,"ctrl"),Pqr.forEach(t),muo=r(YEe," \u2014 "),uP=s(YEe,"A",{href:!0});var $qr=n(uP);fuo=r($qr,"CTRLLMHeadModel"),$qr.forEach(t),guo=r(YEe," (CTRL model)"),YEe.forEach(t),huo=i(D),r_=s(D,"LI",{});var KEe=n(r_);aee=s(KEe,"STRONG",{});var Iqr=n(aee);uuo=r(Iqr,"data2vec-text"),Iqr.forEach(t),puo=r(KEe," \u2014 "),pP=s(KEe,"A",{href:!0});var Dqr=n(pP);_uo=r(Dqr,"Data2VecTextForCausalLM"),Dqr.forEach(t),buo=r(KEe," (Data2VecText model)"),KEe.forEach(t),vuo=i(D),t_=s(D,"LI",{});var ZEe=n(t_);see=s(ZEe,"STRONG",{});var jqr=n(see);Tuo=r(jqr,"electra"),jqr.forEach(t),Fuo=r(ZEe," \u2014 "),_P=s(ZEe,"A",{href:!0});var Nqr=n(_P);Cuo=r(Nqr,"ElectraForCausalLM"),Nqr.forEach(t),Muo=r(ZEe," (ELECTRA model)"),ZEe.forEach(t),Euo=i(D),a_=s(D,"LI",{});var e3e=n(a_);nee=s(e3e,"STRONG",{});var qqr=n(nee);yuo=r(qqr,"gpt2"),qqr.forEach(t),wuo=r(e3e," \u2014 "),bP=s(e3e,"A",{href:!0});var Gqr=n(bP);Auo=r(Gqr,"GPT2LMHeadModel"),Gqr.forEach(t),Luo=r(e3e," (OpenAI GPT-2 model)"),e3e.forEach(t),Buo=i(D),s_=s(D,"LI",{});var o3e=n(s_);lee=s(o3e,"STRONG",{});var Oqr=n(lee);xuo=r(Oqr,"gpt_neo"),Oqr.forEach(t),kuo=r(o3e," \u2014 "),vP=s(o3e,"A",{href:!0});var Xqr=n(vP);Ruo=r(Xqr,"GPTNeoForCausalLM"),Xqr.forEach(t),Suo=r(o3e," (GPT Neo model)"),o3e.forEach(t),Puo=i(D),n_=s(D,"LI",{});var r3e=n(n_);iee=s(r3e,"STRONG",{});var Vqr=n(iee);$uo=r(Vqr,"gptj"),Vqr.forEach(t),Iuo=r(r3e," \u2014 "),TP=s(r3e,"A",{href:!0});var zqr=n(TP);Duo=r(zqr,"GPTJForCausalLM"),zqr.forEach(t),juo=r(r3e," (GPT-J model)"),r3e.forEach(t),Nuo=i(D),l_=s(D,"LI",{});var t3e=n(l_);dee=s(t3e,"STRONG",{});var Wqr=n(dee);quo=r(Wqr,"marian"),Wqr.forEach(t),Guo=r(t3e," \u2014 "),FP=s(t3e,"A",{href:!0});var Qqr=n(FP);Ouo=r(Qqr,"MarianForCausalLM"),Qqr.forEach(t),Xuo=r(t3e," (Marian model)"),t3e.forEach(t),Vuo=i(D),i_=s(D,"LI",{});var a3e=n(i_);cee=s(a3e,"STRONG",{});var Hqr=n(cee);zuo=r(Hqr,"mbart"),Hqr.forEach(t),Wuo=r(a3e," \u2014 "),CP=s(a3e,"A",{href:!0});var Uqr=n(CP);Quo=r(Uqr,"MBartForCausalLM"),Uqr.forEach(t),Huo=r(a3e," (mBART model)"),a3e.forEach(t),Uuo=i(D),d_=s(D,"LI",{});var s3e=n(d_);mee=s(s3e,"STRONG",{});var Jqr=n(mee);Juo=r(Jqr,"megatron-bert"),Jqr.forEach(t),Yuo=r(s3e," \u2014 "),MP=s(s3e,"A",{href:!0});var Yqr=n(MP);Kuo=r(Yqr,"MegatronBertForCausalLM"),Yqr.forEach(t),Zuo=r(s3e," (MegatronBert model)"),s3e.forEach(t),epo=i(D),c_=s(D,"LI",{});var n3e=n(c_);fee=s(n3e,"STRONG",{});var Kqr=n(fee);opo=r(Kqr,"openai-gpt"),Kqr.forEach(t),rpo=r(n3e," \u2014 "),EP=s(n3e,"A",{href:!0});var Zqr=n(EP);tpo=r(Zqr,"OpenAIGPTLMHeadModel"),Zqr.forEach(t),apo=r(n3e," (OpenAI GPT model)"),n3e.forEach(t),spo=i(D),m_=s(D,"LI",{});var l3e=n(m_);gee=s(l3e,"STRONG",{});var eGr=n(gee);npo=r(eGr,"pegasus"),eGr.forEach(t),lpo=r(l3e," \u2014 "),yP=s(l3e,"A",{href:!0});var oGr=n(yP);ipo=r(oGr,"PegasusForCausalLM"),oGr.forEach(t),dpo=r(l3e," (Pegasus model)"),l3e.forEach(t),cpo=i(D),f_=s(D,"LI",{});var i3e=n(f_);hee=s(i3e,"STRONG",{});var rGr=n(hee);mpo=r(rGr,"plbart"),rGr.forEach(t),fpo=r(i3e," \u2014 "),wP=s(i3e,"A",{href:!0});var tGr=n(wP);gpo=r(tGr,"PLBartForCausalLM"),tGr.forEach(t),hpo=r(i3e," (PLBart model)"),i3e.forEach(t),upo=i(D),g_=s(D,"LI",{});var d3e=n(g_);uee=s(d3e,"STRONG",{});var aGr=n(uee);ppo=r(aGr,"prophetnet"),aGr.forEach(t),_po=r(d3e," \u2014 "),AP=s(d3e,"A",{href:!0});var sGr=n(AP);bpo=r(sGr,"ProphetNetForCausalLM"),sGr.forEach(t),vpo=r(d3e," (ProphetNet model)"),d3e.forEach(t),Tpo=i(D),h_=s(D,"LI",{});var c3e=n(h_);pee=s(c3e,"STRONG",{});var nGr=n(pee);Fpo=r(nGr,"qdqbert"),nGr.forEach(t),Cpo=r(c3e," \u2014 "),LP=s(c3e,"A",{href:!0});var lGr=n(LP);Mpo=r(lGr,"QDQBertLMHeadModel"),lGr.forEach(t),Epo=r(c3e," (QDQBert model)"),c3e.forEach(t),ypo=i(D),u_=s(D,"LI",{});var m3e=n(u_);_ee=s(m3e,"STRONG",{});var iGr=n(_ee);wpo=r(iGr,"reformer"),iGr.forEach(t),Apo=r(m3e," \u2014 "),BP=s(m3e,"A",{href:!0});var dGr=n(BP);Lpo=r(dGr,"ReformerModelWithLMHead"),dGr.forEach(t),Bpo=r(m3e," (Reformer model)"),m3e.forEach(t),xpo=i(D),p_=s(D,"LI",{});var f3e=n(p_);bee=s(f3e,"STRONG",{});var cGr=n(bee);kpo=r(cGr,"rembert"),cGr.forEach(t),Rpo=r(f3e," \u2014 "),xP=s(f3e,"A",{href:!0});var mGr=n(xP);Spo=r(mGr,"RemBertForCausalLM"),mGr.forEach(t),Ppo=r(f3e," (RemBERT model)"),f3e.forEach(t),$po=i(D),__=s(D,"LI",{});var g3e=n(__);vee=s(g3e,"STRONG",{});var fGr=n(vee);Ipo=r(fGr,"roberta"),fGr.forEach(t),Dpo=r(g3e," \u2014 "),kP=s(g3e,"A",{href:!0});var gGr=n(kP);jpo=r(gGr,"RobertaForCausalLM"),gGr.forEach(t),Npo=r(g3e," (RoBERTa model)"),g3e.forEach(t),qpo=i(D),b_=s(D,"LI",{});var h3e=n(b_);Tee=s(h3e,"STRONG",{});var hGr=n(Tee);Gpo=r(hGr,"roformer"),hGr.forEach(t),Opo=r(h3e," \u2014 "),RP=s(h3e,"A",{href:!0});var uGr=n(RP);Xpo=r(uGr,"RoFormerForCausalLM"),uGr.forEach(t),Vpo=r(h3e," (RoFormer model)"),h3e.forEach(t),zpo=i(D),v_=s(D,"LI",{});var u3e=n(v_);Fee=s(u3e,"STRONG",{});var pGr=n(Fee);Wpo=r(pGr,"speech_to_text_2"),pGr.forEach(t),Qpo=r(u3e," \u2014 "),SP=s(u3e,"A",{href:!0});var _Gr=n(SP);Hpo=r(_Gr,"Speech2Text2ForCausalLM"),_Gr.forEach(t),Upo=r(u3e," (Speech2Text2 model)"),u3e.forEach(t),Jpo=i(D),T_=s(D,"LI",{});var p3e=n(T_);Cee=s(p3e,"STRONG",{});var bGr=n(Cee);Ypo=r(bGr,"transfo-xl"),bGr.forEach(t),Kpo=r(p3e," \u2014 "),PP=s(p3e,"A",{href:!0});var vGr=n(PP);Zpo=r(vGr,"TransfoXLLMHeadModel"),vGr.forEach(t),e_o=r(p3e," (Transformer-XL model)"),p3e.forEach(t),o_o=i(D),F_=s(D,"LI",{});var _3e=n(F_);Mee=s(_3e,"STRONG",{});var TGr=n(Mee);r_o=r(TGr,"trocr"),TGr.forEach(t),t_o=r(_3e," \u2014 "),$P=s(_3e,"A",{href:!0});var FGr=n($P);a_o=r(FGr,"TrOCRForCausalLM"),FGr.forEach(t),s_o=r(_3e," (TrOCR model)"),_3e.forEach(t),n_o=i(D),C_=s(D,"LI",{});var b3e=n(C_);Eee=s(b3e,"STRONG",{});var CGr=n(Eee);l_o=r(CGr,"xglm"),CGr.forEach(t),i_o=r(b3e," \u2014 "),IP=s(b3e,"A",{href:!0});var MGr=n(IP);d_o=r(MGr,"XGLMForCausalLM"),MGr.forEach(t),c_o=r(b3e," (XGLM model)"),b3e.forEach(t),m_o=i(D),M_=s(D,"LI",{});var v3e=n(M_);yee=s(v3e,"STRONG",{});var EGr=n(yee);f_o=r(EGr,"xlm"),EGr.forEach(t),g_o=r(v3e," \u2014 "),DP=s(v3e,"A",{href:!0});var yGr=n(DP);h_o=r(yGr,"XLMWithLMHeadModel"),yGr.forEach(t),u_o=r(v3e," (XLM model)"),v3e.forEach(t),p_o=i(D),E_=s(D,"LI",{});var T3e=n(E_);wee=s(T3e,"STRONG",{});var wGr=n(wee);__o=r(wGr,"xlm-prophetnet"),wGr.forEach(t),b_o=r(T3e," \u2014 "),jP=s(T3e,"A",{href:!0});var AGr=n(jP);v_o=r(AGr,"XLMProphetNetForCausalLM"),AGr.forEach(t),T_o=r(T3e," (XLMProphetNet model)"),T3e.forEach(t),F_o=i(D),y_=s(D,"LI",{});var F3e=n(y_);Aee=s(F3e,"STRONG",{});var LGr=n(Aee);C_o=r(LGr,"xlm-roberta"),LGr.forEach(t),M_o=r(F3e," \u2014 "),NP=s(F3e,"A",{href:!0});var BGr=n(NP);E_o=r(BGr,"XLMRobertaForCausalLM"),BGr.forEach(t),y_o=r(F3e," (XLM-RoBERTa model)"),F3e.forEach(t),w_o=i(D),w_=s(D,"LI",{});var C3e=n(w_);Lee=s(C3e,"STRONG",{});var xGr=n(Lee);A_o=r(xGr,"xlm-roberta-xl"),xGr.forEach(t),L_o=r(C3e," \u2014 "),qP=s(C3e,"A",{href:!0});var kGr=n(qP);B_o=r(kGr,"XLMRobertaXLForCausalLM"),kGr.forEach(t),x_o=r(C3e," (XLM-RoBERTa-XL model)"),C3e.forEach(t),k_o=i(D),A_=s(D,"LI",{});var M3e=n(A_);Bee=s(M3e,"STRONG",{});var RGr=n(Bee);R_o=r(RGr,"xlnet"),RGr.forEach(t),S_o=r(M3e," \u2014 "),GP=s(M3e,"A",{href:!0});var SGr=n(GP);P_o=r(SGr,"XLNetLMHeadModel"),SGr.forEach(t),$_o=r(M3e," (XLNet model)"),M3e.forEach(t),D.forEach(t),I_o=i(Dt),L_=s(Dt,"P",{});var E3e=n(L_);D_o=r(E3e,"The model is set in evaluation mode by default using "),xee=s(E3e,"CODE",{});var PGr=n(xee);j_o=r(PGr,"model.eval()"),PGr.forEach(t),N_o=r(E3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=s(E3e,"CODE",{});var $Gr=n(kee);q_o=r($Gr,"model.train()"),$Gr.forEach(t),E3e.forEach(t),G_o=i(Dt),Ree=s(Dt,"P",{});var IGr=n(Ree);O_o=r(IGr,"Examples:"),IGr.forEach(t),X_o=i(Dt),f(My.$$.fragment,Dt),Dt.forEach(t),zn.forEach(t),U9e=i(c),Ki=s(c,"H2",{class:!0});var rke=n(Ki);B_=s(rke,"A",{id:!0,class:!0,href:!0});var DGr=n(B_);See=s(DGr,"SPAN",{});var jGr=n(See);f(Ey.$$.fragment,jGr),jGr.forEach(t),DGr.forEach(t),V_o=i(rke),Pee=s(rke,"SPAN",{});var NGr=n(Pee);z_o=r(NGr,"AutoModelForMaskedLM"),NGr.forEach(t),rke.forEach(t),J9e=i(c),Jo=s(c,"DIV",{class:!0});var Qn=n(Jo);f(yy.$$.fragment,Qn),W_o=i(Qn),Zi=s(Qn,"P",{});var XV=n(Zi);Q_o=r(XV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),$ee=s(XV,"CODE",{});var qGr=n($ee);H_o=r(qGr,"from_pretrained()"),qGr.forEach(t),U_o=r(XV,"class method or the "),Iee=s(XV,"CODE",{});var GGr=n(Iee);J_o=r(GGr,"from_config()"),GGr.forEach(t),Y_o=r(XV,`class
method.`),XV.forEach(t),K_o=i(Qn),wy=s(Qn,"P",{});var tke=n(wy);Z_o=r(tke,"This class cannot be instantiated directly using "),Dee=s(tke,"CODE",{});var OGr=n(Dee);ebo=r(OGr,"__init__()"),OGr.forEach(t),obo=r(tke," (throws an error)."),tke.forEach(t),rbo=i(Qn),Xr=s(Qn,"DIV",{class:!0});var Hn=n(Xr);f(Ay.$$.fragment,Hn),tbo=i(Hn),jee=s(Hn,"P",{});var XGr=n(jee);abo=r(XGr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),XGr.forEach(t),sbo=i(Hn),ed=s(Hn,"P",{});var VV=n(ed);nbo=r(VV,`Note:
Loading a model from its configuration file does `),Nee=s(VV,"STRONG",{});var VGr=n(Nee);lbo=r(VGr,"not"),VGr.forEach(t),ibo=r(VV,` load the model weights. It only affects the
model\u2019s configuration. Use `),qee=s(VV,"CODE",{});var zGr=n(qee);dbo=r(zGr,"from_pretrained()"),zGr.forEach(t),cbo=r(VV,"to load the model weights."),VV.forEach(t),mbo=i(Hn),Gee=s(Hn,"P",{});var WGr=n(Gee);fbo=r(WGr,"Examples:"),WGr.forEach(t),gbo=i(Hn),f(Ly.$$.fragment,Hn),Hn.forEach(t),hbo=i(Qn),$e=s(Qn,"DIV",{class:!0});var jt=n($e);f(By.$$.fragment,jt),ubo=i(jt),Oee=s(jt,"P",{});var QGr=n(Oee);pbo=r(QGr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),QGr.forEach(t),_bo=i(jt),Oa=s(jt,"P",{});var x3=n(Oa);bbo=r(x3,"The model class to instantiate is selected based on the "),Xee=s(x3,"CODE",{});var HGr=n(Xee);vbo=r(HGr,"model_type"),HGr.forEach(t),Tbo=r(x3,` property of the config object (either
passed as an argument or loaded from `),Vee=s(x3,"CODE",{});var UGr=n(Vee);Fbo=r(UGr,"pretrained_model_name_or_path"),UGr.forEach(t),Cbo=r(x3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=s(x3,"CODE",{});var JGr=n(zee);Mbo=r(JGr,"pretrained_model_name_or_path"),JGr.forEach(t),Ebo=r(x3,":"),x3.forEach(t),ybo=i(jt),I=s(jt,"UL",{});var j=n(I);x_=s(j,"LI",{});var y3e=n(x_);Wee=s(y3e,"STRONG",{});var YGr=n(Wee);wbo=r(YGr,"albert"),YGr.forEach(t),Abo=r(y3e," \u2014 "),OP=s(y3e,"A",{href:!0});var KGr=n(OP);Lbo=r(KGr,"AlbertForMaskedLM"),KGr.forEach(t),Bbo=r(y3e," (ALBERT model)"),y3e.forEach(t),xbo=i(j),k_=s(j,"LI",{});var w3e=n(k_);Qee=s(w3e,"STRONG",{});var ZGr=n(Qee);kbo=r(ZGr,"bart"),ZGr.forEach(t),Rbo=r(w3e," \u2014 "),XP=s(w3e,"A",{href:!0});var eOr=n(XP);Sbo=r(eOr,"BartForConditionalGeneration"),eOr.forEach(t),Pbo=r(w3e," (BART model)"),w3e.forEach(t),$bo=i(j),R_=s(j,"LI",{});var A3e=n(R_);Hee=s(A3e,"STRONG",{});var oOr=n(Hee);Ibo=r(oOr,"bert"),oOr.forEach(t),Dbo=r(A3e," \u2014 "),VP=s(A3e,"A",{href:!0});var rOr=n(VP);jbo=r(rOr,"BertForMaskedLM"),rOr.forEach(t),Nbo=r(A3e," (BERT model)"),A3e.forEach(t),qbo=i(j),S_=s(j,"LI",{});var L3e=n(S_);Uee=s(L3e,"STRONG",{});var tOr=n(Uee);Gbo=r(tOr,"big_bird"),tOr.forEach(t),Obo=r(L3e," \u2014 "),zP=s(L3e,"A",{href:!0});var aOr=n(zP);Xbo=r(aOr,"BigBirdForMaskedLM"),aOr.forEach(t),Vbo=r(L3e," (BigBird model)"),L3e.forEach(t),zbo=i(j),P_=s(j,"LI",{});var B3e=n(P_);Jee=s(B3e,"STRONG",{});var sOr=n(Jee);Wbo=r(sOr,"camembert"),sOr.forEach(t),Qbo=r(B3e," \u2014 "),WP=s(B3e,"A",{href:!0});var nOr=n(WP);Hbo=r(nOr,"CamembertForMaskedLM"),nOr.forEach(t),Ubo=r(B3e," (CamemBERT model)"),B3e.forEach(t),Jbo=i(j),$_=s(j,"LI",{});var x3e=n($_);Yee=s(x3e,"STRONG",{});var lOr=n(Yee);Ybo=r(lOr,"convbert"),lOr.forEach(t),Kbo=r(x3e," \u2014 "),QP=s(x3e,"A",{href:!0});var iOr=n(QP);Zbo=r(iOr,"ConvBertForMaskedLM"),iOr.forEach(t),e2o=r(x3e," (ConvBERT model)"),x3e.forEach(t),o2o=i(j),I_=s(j,"LI",{});var k3e=n(I_);Kee=s(k3e,"STRONG",{});var dOr=n(Kee);r2o=r(dOr,"data2vec-text"),dOr.forEach(t),t2o=r(k3e," \u2014 "),HP=s(k3e,"A",{href:!0});var cOr=n(HP);a2o=r(cOr,"Data2VecTextForMaskedLM"),cOr.forEach(t),s2o=r(k3e," (Data2VecText model)"),k3e.forEach(t),n2o=i(j),D_=s(j,"LI",{});var R3e=n(D_);Zee=s(R3e,"STRONG",{});var mOr=n(Zee);l2o=r(mOr,"deberta"),mOr.forEach(t),i2o=r(R3e," \u2014 "),UP=s(R3e,"A",{href:!0});var fOr=n(UP);d2o=r(fOr,"DebertaForMaskedLM"),fOr.forEach(t),c2o=r(R3e," (DeBERTa model)"),R3e.forEach(t),m2o=i(j),j_=s(j,"LI",{});var S3e=n(j_);eoe=s(S3e,"STRONG",{});var gOr=n(eoe);f2o=r(gOr,"deberta-v2"),gOr.forEach(t),g2o=r(S3e," \u2014 "),JP=s(S3e,"A",{href:!0});var hOr=n(JP);h2o=r(hOr,"DebertaV2ForMaskedLM"),hOr.forEach(t),u2o=r(S3e," (DeBERTa-v2 model)"),S3e.forEach(t),p2o=i(j),N_=s(j,"LI",{});var P3e=n(N_);ooe=s(P3e,"STRONG",{});var uOr=n(ooe);_2o=r(uOr,"distilbert"),uOr.forEach(t),b2o=r(P3e," \u2014 "),YP=s(P3e,"A",{href:!0});var pOr=n(YP);v2o=r(pOr,"DistilBertForMaskedLM"),pOr.forEach(t),T2o=r(P3e," (DistilBERT model)"),P3e.forEach(t),F2o=i(j),q_=s(j,"LI",{});var $3e=n(q_);roe=s($3e,"STRONG",{});var _Or=n(roe);C2o=r(_Or,"electra"),_Or.forEach(t),M2o=r($3e," \u2014 "),KP=s($3e,"A",{href:!0});var bOr=n(KP);E2o=r(bOr,"ElectraForMaskedLM"),bOr.forEach(t),y2o=r($3e," (ELECTRA model)"),$3e.forEach(t),w2o=i(j),G_=s(j,"LI",{});var I3e=n(G_);toe=s(I3e,"STRONG",{});var vOr=n(toe);A2o=r(vOr,"flaubert"),vOr.forEach(t),L2o=r(I3e," \u2014 "),ZP=s(I3e,"A",{href:!0});var TOr=n(ZP);B2o=r(TOr,"FlaubertWithLMHeadModel"),TOr.forEach(t),x2o=r(I3e," (FlauBERT model)"),I3e.forEach(t),k2o=i(j),O_=s(j,"LI",{});var D3e=n(O_);aoe=s(D3e,"STRONG",{});var FOr=n(aoe);R2o=r(FOr,"fnet"),FOr.forEach(t),S2o=r(D3e," \u2014 "),e$=s(D3e,"A",{href:!0});var COr=n(e$);P2o=r(COr,"FNetForMaskedLM"),COr.forEach(t),$2o=r(D3e," (FNet model)"),D3e.forEach(t),I2o=i(j),X_=s(j,"LI",{});var j3e=n(X_);soe=s(j3e,"STRONG",{});var MOr=n(soe);D2o=r(MOr,"funnel"),MOr.forEach(t),j2o=r(j3e," \u2014 "),o$=s(j3e,"A",{href:!0});var EOr=n(o$);N2o=r(EOr,"FunnelForMaskedLM"),EOr.forEach(t),q2o=r(j3e," (Funnel Transformer model)"),j3e.forEach(t),G2o=i(j),V_=s(j,"LI",{});var N3e=n(V_);noe=s(N3e,"STRONG",{});var yOr=n(noe);O2o=r(yOr,"ibert"),yOr.forEach(t),X2o=r(N3e," \u2014 "),r$=s(N3e,"A",{href:!0});var wOr=n(r$);V2o=r(wOr,"IBertForMaskedLM"),wOr.forEach(t),z2o=r(N3e," (I-BERT model)"),N3e.forEach(t),W2o=i(j),z_=s(j,"LI",{});var q3e=n(z_);loe=s(q3e,"STRONG",{});var AOr=n(loe);Q2o=r(AOr,"layoutlm"),AOr.forEach(t),H2o=r(q3e," \u2014 "),t$=s(q3e,"A",{href:!0});var LOr=n(t$);U2o=r(LOr,"LayoutLMForMaskedLM"),LOr.forEach(t),J2o=r(q3e," (LayoutLM model)"),q3e.forEach(t),Y2o=i(j),W_=s(j,"LI",{});var G3e=n(W_);ioe=s(G3e,"STRONG",{});var BOr=n(ioe);K2o=r(BOr,"longformer"),BOr.forEach(t),Z2o=r(G3e," \u2014 "),a$=s(G3e,"A",{href:!0});var xOr=n(a$);evo=r(xOr,"LongformerForMaskedLM"),xOr.forEach(t),ovo=r(G3e," (Longformer model)"),G3e.forEach(t),rvo=i(j),Q_=s(j,"LI",{});var O3e=n(Q_);doe=s(O3e,"STRONG",{});var kOr=n(doe);tvo=r(kOr,"mbart"),kOr.forEach(t),avo=r(O3e," \u2014 "),s$=s(O3e,"A",{href:!0});var ROr=n(s$);svo=r(ROr,"MBartForConditionalGeneration"),ROr.forEach(t),nvo=r(O3e," (mBART model)"),O3e.forEach(t),lvo=i(j),H_=s(j,"LI",{});var X3e=n(H_);coe=s(X3e,"STRONG",{});var SOr=n(coe);ivo=r(SOr,"megatron-bert"),SOr.forEach(t),dvo=r(X3e," \u2014 "),n$=s(X3e,"A",{href:!0});var POr=n(n$);cvo=r(POr,"MegatronBertForMaskedLM"),POr.forEach(t),mvo=r(X3e," (MegatronBert model)"),X3e.forEach(t),fvo=i(j),U_=s(j,"LI",{});var V3e=n(U_);moe=s(V3e,"STRONG",{});var $Or=n(moe);gvo=r($Or,"mobilebert"),$Or.forEach(t),hvo=r(V3e," \u2014 "),l$=s(V3e,"A",{href:!0});var IOr=n(l$);uvo=r(IOr,"MobileBertForMaskedLM"),IOr.forEach(t),pvo=r(V3e," (MobileBERT model)"),V3e.forEach(t),_vo=i(j),J_=s(j,"LI",{});var z3e=n(J_);foe=s(z3e,"STRONG",{});var DOr=n(foe);bvo=r(DOr,"mpnet"),DOr.forEach(t),vvo=r(z3e," \u2014 "),i$=s(z3e,"A",{href:!0});var jOr=n(i$);Tvo=r(jOr,"MPNetForMaskedLM"),jOr.forEach(t),Fvo=r(z3e," (MPNet model)"),z3e.forEach(t),Cvo=i(j),Y_=s(j,"LI",{});var W3e=n(Y_);goe=s(W3e,"STRONG",{});var NOr=n(goe);Mvo=r(NOr,"nystromformer"),NOr.forEach(t),Evo=r(W3e," \u2014 "),d$=s(W3e,"A",{href:!0});var qOr=n(d$);yvo=r(qOr,"NystromformerForMaskedLM"),qOr.forEach(t),wvo=r(W3e," (Nystromformer model)"),W3e.forEach(t),Avo=i(j),K_=s(j,"LI",{});var Q3e=n(K_);hoe=s(Q3e,"STRONG",{});var GOr=n(hoe);Lvo=r(GOr,"perceiver"),GOr.forEach(t),Bvo=r(Q3e," \u2014 "),c$=s(Q3e,"A",{href:!0});var OOr=n(c$);xvo=r(OOr,"PerceiverForMaskedLM"),OOr.forEach(t),kvo=r(Q3e," (Perceiver model)"),Q3e.forEach(t),Rvo=i(j),Z_=s(j,"LI",{});var H3e=n(Z_);uoe=s(H3e,"STRONG",{});var XOr=n(uoe);Svo=r(XOr,"qdqbert"),XOr.forEach(t),Pvo=r(H3e," \u2014 "),m$=s(H3e,"A",{href:!0});var VOr=n(m$);$vo=r(VOr,"QDQBertForMaskedLM"),VOr.forEach(t),Ivo=r(H3e," (QDQBert model)"),H3e.forEach(t),Dvo=i(j),eb=s(j,"LI",{});var U3e=n(eb);poe=s(U3e,"STRONG",{});var zOr=n(poe);jvo=r(zOr,"reformer"),zOr.forEach(t),Nvo=r(U3e," \u2014 "),f$=s(U3e,"A",{href:!0});var WOr=n(f$);qvo=r(WOr,"ReformerForMaskedLM"),WOr.forEach(t),Gvo=r(U3e," (Reformer model)"),U3e.forEach(t),Ovo=i(j),ob=s(j,"LI",{});var J3e=n(ob);_oe=s(J3e,"STRONG",{});var QOr=n(_oe);Xvo=r(QOr,"rembert"),QOr.forEach(t),Vvo=r(J3e," \u2014 "),g$=s(J3e,"A",{href:!0});var HOr=n(g$);zvo=r(HOr,"RemBertForMaskedLM"),HOr.forEach(t),Wvo=r(J3e," (RemBERT model)"),J3e.forEach(t),Qvo=i(j),rb=s(j,"LI",{});var Y3e=n(rb);boe=s(Y3e,"STRONG",{});var UOr=n(boe);Hvo=r(UOr,"roberta"),UOr.forEach(t),Uvo=r(Y3e," \u2014 "),h$=s(Y3e,"A",{href:!0});var JOr=n(h$);Jvo=r(JOr,"RobertaForMaskedLM"),JOr.forEach(t),Yvo=r(Y3e," (RoBERTa model)"),Y3e.forEach(t),Kvo=i(j),tb=s(j,"LI",{});var K3e=n(tb);voe=s(K3e,"STRONG",{});var YOr=n(voe);Zvo=r(YOr,"roformer"),YOr.forEach(t),eTo=r(K3e," \u2014 "),u$=s(K3e,"A",{href:!0});var KOr=n(u$);oTo=r(KOr,"RoFormerForMaskedLM"),KOr.forEach(t),rTo=r(K3e," (RoFormer model)"),K3e.forEach(t),tTo=i(j),ab=s(j,"LI",{});var Z3e=n(ab);Toe=s(Z3e,"STRONG",{});var ZOr=n(Toe);aTo=r(ZOr,"squeezebert"),ZOr.forEach(t),sTo=r(Z3e," \u2014 "),p$=s(Z3e,"A",{href:!0});var eXr=n(p$);nTo=r(eXr,"SqueezeBertForMaskedLM"),eXr.forEach(t),lTo=r(Z3e," (SqueezeBERT model)"),Z3e.forEach(t),iTo=i(j),sb=s(j,"LI",{});var e5e=n(sb);Foe=s(e5e,"STRONG",{});var oXr=n(Foe);dTo=r(oXr,"tapas"),oXr.forEach(t),cTo=r(e5e," \u2014 "),_$=s(e5e,"A",{href:!0});var rXr=n(_$);mTo=r(rXr,"TapasForMaskedLM"),rXr.forEach(t),fTo=r(e5e," (TAPAS model)"),e5e.forEach(t),gTo=i(j),nb=s(j,"LI",{});var o5e=n(nb);Coe=s(o5e,"STRONG",{});var tXr=n(Coe);hTo=r(tXr,"wav2vec2"),tXr.forEach(t),uTo=r(o5e," \u2014 "),Moe=s(o5e,"CODE",{});var aXr=n(Moe);pTo=r(aXr,"Wav2Vec2ForMaskedLM"),aXr.forEach(t),_To=r(o5e,"(Wav2Vec2 model)"),o5e.forEach(t),bTo=i(j),lb=s(j,"LI",{});var r5e=n(lb);Eoe=s(r5e,"STRONG",{});var sXr=n(Eoe);vTo=r(sXr,"xlm"),sXr.forEach(t),TTo=r(r5e," \u2014 "),b$=s(r5e,"A",{href:!0});var nXr=n(b$);FTo=r(nXr,"XLMWithLMHeadModel"),nXr.forEach(t),CTo=r(r5e," (XLM model)"),r5e.forEach(t),MTo=i(j),ib=s(j,"LI",{});var t5e=n(ib);yoe=s(t5e,"STRONG",{});var lXr=n(yoe);ETo=r(lXr,"xlm-roberta"),lXr.forEach(t),yTo=r(t5e," \u2014 "),v$=s(t5e,"A",{href:!0});var iXr=n(v$);wTo=r(iXr,"XLMRobertaForMaskedLM"),iXr.forEach(t),ATo=r(t5e," (XLM-RoBERTa model)"),t5e.forEach(t),LTo=i(j),db=s(j,"LI",{});var a5e=n(db);woe=s(a5e,"STRONG",{});var dXr=n(woe);BTo=r(dXr,"xlm-roberta-xl"),dXr.forEach(t),xTo=r(a5e," \u2014 "),T$=s(a5e,"A",{href:!0});var cXr=n(T$);kTo=r(cXr,"XLMRobertaXLForMaskedLM"),cXr.forEach(t),RTo=r(a5e," (XLM-RoBERTa-XL model)"),a5e.forEach(t),STo=i(j),cb=s(j,"LI",{});var s5e=n(cb);Aoe=s(s5e,"STRONG",{});var mXr=n(Aoe);PTo=r(mXr,"yoso"),mXr.forEach(t),$To=r(s5e," \u2014 "),F$=s(s5e,"A",{href:!0});var fXr=n(F$);ITo=r(fXr,"YosoForMaskedLM"),fXr.forEach(t),DTo=r(s5e," (YOSO model)"),s5e.forEach(t),j.forEach(t),jTo=i(jt),mb=s(jt,"P",{});var n5e=n(mb);NTo=r(n5e,"The model is set in evaluation mode by default using "),Loe=s(n5e,"CODE",{});var gXr=n(Loe);qTo=r(gXr,"model.eval()"),gXr.forEach(t),GTo=r(n5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Boe=s(n5e,"CODE",{});var hXr=n(Boe);OTo=r(hXr,"model.train()"),hXr.forEach(t),n5e.forEach(t),XTo=i(jt),xoe=s(jt,"P",{});var uXr=n(xoe);VTo=r(uXr,"Examples:"),uXr.forEach(t),zTo=i(jt),f(xy.$$.fragment,jt),jt.forEach(t),Qn.forEach(t),Y9e=i(c),od=s(c,"H2",{class:!0});var ake=n(od);fb=s(ake,"A",{id:!0,class:!0,href:!0});var pXr=n(fb);koe=s(pXr,"SPAN",{});var _Xr=n(koe);f(ky.$$.fragment,_Xr),_Xr.forEach(t),pXr.forEach(t),WTo=i(ake),Roe=s(ake,"SPAN",{});var bXr=n(Roe);QTo=r(bXr,"AutoModelForSeq2SeqLM"),bXr.forEach(t),ake.forEach(t),K9e=i(c),Yo=s(c,"DIV",{class:!0});var Un=n(Yo);f(Ry.$$.fragment,Un),HTo=i(Un),rd=s(Un,"P",{});var zV=n(rd);UTo=r(zV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Soe=s(zV,"CODE",{});var vXr=n(Soe);JTo=r(vXr,"from_pretrained()"),vXr.forEach(t),YTo=r(zV,"class method or the "),Poe=s(zV,"CODE",{});var TXr=n(Poe);KTo=r(TXr,"from_config()"),TXr.forEach(t),ZTo=r(zV,`class
method.`),zV.forEach(t),e1o=i(Un),Sy=s(Un,"P",{});var ske=n(Sy);o1o=r(ske,"This class cannot be instantiated directly using "),$oe=s(ske,"CODE",{});var FXr=n($oe);r1o=r(FXr,"__init__()"),FXr.forEach(t),t1o=r(ske," (throws an error)."),ske.forEach(t),a1o=i(Un),Vr=s(Un,"DIV",{class:!0});var Jn=n(Vr);f(Py.$$.fragment,Jn),s1o=i(Jn),Ioe=s(Jn,"P",{});var CXr=n(Ioe);n1o=r(CXr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),CXr.forEach(t),l1o=i(Jn),td=s(Jn,"P",{});var WV=n(td);i1o=r(WV,`Note:
Loading a model from its configuration file does `),Doe=s(WV,"STRONG",{});var MXr=n(Doe);d1o=r(MXr,"not"),MXr.forEach(t),c1o=r(WV,` load the model weights. It only affects the
model\u2019s configuration. Use `),joe=s(WV,"CODE",{});var EXr=n(joe);m1o=r(EXr,"from_pretrained()"),EXr.forEach(t),f1o=r(WV,"to load the model weights."),WV.forEach(t),g1o=i(Jn),Noe=s(Jn,"P",{});var yXr=n(Noe);h1o=r(yXr,"Examples:"),yXr.forEach(t),u1o=i(Jn),f($y.$$.fragment,Jn),Jn.forEach(t),p1o=i(Un),Ie=s(Un,"DIV",{class:!0});var Nt=n(Ie);f(Iy.$$.fragment,Nt),_1o=i(Nt),qoe=s(Nt,"P",{});var wXr=n(qoe);b1o=r(wXr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),wXr.forEach(t),v1o=i(Nt),Xa=s(Nt,"P",{});var k3=n(Xa);T1o=r(k3,"The model class to instantiate is selected based on the "),Goe=s(k3,"CODE",{});var AXr=n(Goe);F1o=r(AXr,"model_type"),AXr.forEach(t),C1o=r(k3,` property of the config object (either
passed as an argument or loaded from `),Ooe=s(k3,"CODE",{});var LXr=n(Ooe);M1o=r(LXr,"pretrained_model_name_or_path"),LXr.forEach(t),E1o=r(k3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xoe=s(k3,"CODE",{});var BXr=n(Xoe);y1o=r(BXr,"pretrained_model_name_or_path"),BXr.forEach(t),w1o=r(k3,":"),k3.forEach(t),A1o=i(Nt),ae=s(Nt,"UL",{});var le=n(ae);gb=s(le,"LI",{});var l5e=n(gb);Voe=s(l5e,"STRONG",{});var xXr=n(Voe);L1o=r(xXr,"bart"),xXr.forEach(t),B1o=r(l5e," \u2014 "),C$=s(l5e,"A",{href:!0});var kXr=n(C$);x1o=r(kXr,"BartForConditionalGeneration"),kXr.forEach(t),k1o=r(l5e," (BART model)"),l5e.forEach(t),R1o=i(le),hb=s(le,"LI",{});var i5e=n(hb);zoe=s(i5e,"STRONG",{});var RXr=n(zoe);S1o=r(RXr,"bigbird_pegasus"),RXr.forEach(t),P1o=r(i5e," \u2014 "),M$=s(i5e,"A",{href:!0});var SXr=n(M$);$1o=r(SXr,"BigBirdPegasusForConditionalGeneration"),SXr.forEach(t),I1o=r(i5e," (BigBirdPegasus model)"),i5e.forEach(t),D1o=i(le),ub=s(le,"LI",{});var d5e=n(ub);Woe=s(d5e,"STRONG",{});var PXr=n(Woe);j1o=r(PXr,"blenderbot"),PXr.forEach(t),N1o=r(d5e," \u2014 "),E$=s(d5e,"A",{href:!0});var $Xr=n(E$);q1o=r($Xr,"BlenderbotForConditionalGeneration"),$Xr.forEach(t),G1o=r(d5e," (Blenderbot model)"),d5e.forEach(t),O1o=i(le),pb=s(le,"LI",{});var c5e=n(pb);Qoe=s(c5e,"STRONG",{});var IXr=n(Qoe);X1o=r(IXr,"blenderbot-small"),IXr.forEach(t),V1o=r(c5e," \u2014 "),y$=s(c5e,"A",{href:!0});var DXr=n(y$);z1o=r(DXr,"BlenderbotSmallForConditionalGeneration"),DXr.forEach(t),W1o=r(c5e," (BlenderbotSmall model)"),c5e.forEach(t),Q1o=i(le),_b=s(le,"LI",{});var m5e=n(_b);Hoe=s(m5e,"STRONG",{});var jXr=n(Hoe);H1o=r(jXr,"encoder-decoder"),jXr.forEach(t),U1o=r(m5e," \u2014 "),w$=s(m5e,"A",{href:!0});var NXr=n(w$);J1o=r(NXr,"EncoderDecoderModel"),NXr.forEach(t),Y1o=r(m5e," (Encoder decoder model)"),m5e.forEach(t),K1o=i(le),bb=s(le,"LI",{});var f5e=n(bb);Uoe=s(f5e,"STRONG",{});var qXr=n(Uoe);Z1o=r(qXr,"fsmt"),qXr.forEach(t),eFo=r(f5e," \u2014 "),A$=s(f5e,"A",{href:!0});var GXr=n(A$);oFo=r(GXr,"FSMTForConditionalGeneration"),GXr.forEach(t),rFo=r(f5e," (FairSeq Machine-Translation model)"),f5e.forEach(t),tFo=i(le),vb=s(le,"LI",{});var g5e=n(vb);Joe=s(g5e,"STRONG",{});var OXr=n(Joe);aFo=r(OXr,"led"),OXr.forEach(t),sFo=r(g5e," \u2014 "),L$=s(g5e,"A",{href:!0});var XXr=n(L$);nFo=r(XXr,"LEDForConditionalGeneration"),XXr.forEach(t),lFo=r(g5e," (LED model)"),g5e.forEach(t),iFo=i(le),Tb=s(le,"LI",{});var h5e=n(Tb);Yoe=s(h5e,"STRONG",{});var VXr=n(Yoe);dFo=r(VXr,"m2m_100"),VXr.forEach(t),cFo=r(h5e," \u2014 "),B$=s(h5e,"A",{href:!0});var zXr=n(B$);mFo=r(zXr,"M2M100ForConditionalGeneration"),zXr.forEach(t),fFo=r(h5e," (M2M100 model)"),h5e.forEach(t),gFo=i(le),Fb=s(le,"LI",{});var u5e=n(Fb);Koe=s(u5e,"STRONG",{});var WXr=n(Koe);hFo=r(WXr,"marian"),WXr.forEach(t),uFo=r(u5e," \u2014 "),x$=s(u5e,"A",{href:!0});var QXr=n(x$);pFo=r(QXr,"MarianMTModel"),QXr.forEach(t),_Fo=r(u5e," (Marian model)"),u5e.forEach(t),bFo=i(le),Cb=s(le,"LI",{});var p5e=n(Cb);Zoe=s(p5e,"STRONG",{});var HXr=n(Zoe);vFo=r(HXr,"mbart"),HXr.forEach(t),TFo=r(p5e," \u2014 "),k$=s(p5e,"A",{href:!0});var UXr=n(k$);FFo=r(UXr,"MBartForConditionalGeneration"),UXr.forEach(t),CFo=r(p5e," (mBART model)"),p5e.forEach(t),MFo=i(le),Mb=s(le,"LI",{});var _5e=n(Mb);ere=s(_5e,"STRONG",{});var JXr=n(ere);EFo=r(JXr,"mt5"),JXr.forEach(t),yFo=r(_5e," \u2014 "),R$=s(_5e,"A",{href:!0});var YXr=n(R$);wFo=r(YXr,"MT5ForConditionalGeneration"),YXr.forEach(t),AFo=r(_5e," (mT5 model)"),_5e.forEach(t),LFo=i(le),Eb=s(le,"LI",{});var b5e=n(Eb);ore=s(b5e,"STRONG",{});var KXr=n(ore);BFo=r(KXr,"pegasus"),KXr.forEach(t),xFo=r(b5e," \u2014 "),S$=s(b5e,"A",{href:!0});var ZXr=n(S$);kFo=r(ZXr,"PegasusForConditionalGeneration"),ZXr.forEach(t),RFo=r(b5e," (Pegasus model)"),b5e.forEach(t),SFo=i(le),yb=s(le,"LI",{});var v5e=n(yb);rre=s(v5e,"STRONG",{});var eVr=n(rre);PFo=r(eVr,"plbart"),eVr.forEach(t),$Fo=r(v5e," \u2014 "),P$=s(v5e,"A",{href:!0});var oVr=n(P$);IFo=r(oVr,"PLBartForConditionalGeneration"),oVr.forEach(t),DFo=r(v5e," (PLBart model)"),v5e.forEach(t),jFo=i(le),wb=s(le,"LI",{});var T5e=n(wb);tre=s(T5e,"STRONG",{});var rVr=n(tre);NFo=r(rVr,"prophetnet"),rVr.forEach(t),qFo=r(T5e," \u2014 "),$$=s(T5e,"A",{href:!0});var tVr=n($$);GFo=r(tVr,"ProphetNetForConditionalGeneration"),tVr.forEach(t),OFo=r(T5e," (ProphetNet model)"),T5e.forEach(t),XFo=i(le),Ab=s(le,"LI",{});var F5e=n(Ab);are=s(F5e,"STRONG",{});var aVr=n(are);VFo=r(aVr,"t5"),aVr.forEach(t),zFo=r(F5e," \u2014 "),I$=s(F5e,"A",{href:!0});var sVr=n(I$);WFo=r(sVr,"T5ForConditionalGeneration"),sVr.forEach(t),QFo=r(F5e," (T5 model)"),F5e.forEach(t),HFo=i(le),Lb=s(le,"LI",{});var C5e=n(Lb);sre=s(C5e,"STRONG",{});var nVr=n(sre);UFo=r(nVr,"xlm-prophetnet"),nVr.forEach(t),JFo=r(C5e," \u2014 "),D$=s(C5e,"A",{href:!0});var lVr=n(D$);YFo=r(lVr,"XLMProphetNetForConditionalGeneration"),lVr.forEach(t),KFo=r(C5e," (XLMProphetNet model)"),C5e.forEach(t),le.forEach(t),ZFo=i(Nt),Bb=s(Nt,"P",{});var M5e=n(Bb);eCo=r(M5e,"The model is set in evaluation mode by default using "),nre=s(M5e,"CODE",{});var iVr=n(nre);oCo=r(iVr,"model.eval()"),iVr.forEach(t),rCo=r(M5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lre=s(M5e,"CODE",{});var dVr=n(lre);tCo=r(dVr,"model.train()"),dVr.forEach(t),M5e.forEach(t),aCo=i(Nt),ire=s(Nt,"P",{});var cVr=n(ire);sCo=r(cVr,"Examples:"),cVr.forEach(t),nCo=i(Nt),f(Dy.$$.fragment,Nt),Nt.forEach(t),Un.forEach(t),Z9e=i(c),ad=s(c,"H2",{class:!0});var nke=n(ad);xb=s(nke,"A",{id:!0,class:!0,href:!0});var mVr=n(xb);dre=s(mVr,"SPAN",{});var fVr=n(dre);f(jy.$$.fragment,fVr),fVr.forEach(t),mVr.forEach(t),lCo=i(nke),cre=s(nke,"SPAN",{});var gVr=n(cre);iCo=r(gVr,"AutoModelForSequenceClassification"),gVr.forEach(t),nke.forEach(t),eBe=i(c),Ko=s(c,"DIV",{class:!0});var Yn=n(Ko);f(Ny.$$.fragment,Yn),dCo=i(Yn),sd=s(Yn,"P",{});var QV=n(sd);cCo=r(QV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),mre=s(QV,"CODE",{});var hVr=n(mre);mCo=r(hVr,"from_pretrained()"),hVr.forEach(t),fCo=r(QV,"class method or the "),fre=s(QV,"CODE",{});var uVr=n(fre);gCo=r(uVr,"from_config()"),uVr.forEach(t),hCo=r(QV,`class
method.`),QV.forEach(t),uCo=i(Yn),qy=s(Yn,"P",{});var lke=n(qy);pCo=r(lke,"This class cannot be instantiated directly using "),gre=s(lke,"CODE",{});var pVr=n(gre);_Co=r(pVr,"__init__()"),pVr.forEach(t),bCo=r(lke," (throws an error)."),lke.forEach(t),vCo=i(Yn),zr=s(Yn,"DIV",{class:!0});var Kn=n(zr);f(Gy.$$.fragment,Kn),TCo=i(Kn),hre=s(Kn,"P",{});var _Vr=n(hre);FCo=r(_Vr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),_Vr.forEach(t),CCo=i(Kn),nd=s(Kn,"P",{});var HV=n(nd);MCo=r(HV,`Note:
Loading a model from its configuration file does `),ure=s(HV,"STRONG",{});var bVr=n(ure);ECo=r(bVr,"not"),bVr.forEach(t),yCo=r(HV,` load the model weights. It only affects the
model\u2019s configuration. Use `),pre=s(HV,"CODE",{});var vVr=n(pre);wCo=r(vVr,"from_pretrained()"),vVr.forEach(t),ACo=r(HV,"to load the model weights."),HV.forEach(t),LCo=i(Kn),_re=s(Kn,"P",{});var TVr=n(_re);BCo=r(TVr,"Examples:"),TVr.forEach(t),xCo=i(Kn),f(Oy.$$.fragment,Kn),Kn.forEach(t),kCo=i(Yn),De=s(Yn,"DIV",{class:!0});var qt=n(De);f(Xy.$$.fragment,qt),RCo=i(qt),bre=s(qt,"P",{});var FVr=n(bre);SCo=r(FVr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),FVr.forEach(t),PCo=i(qt),Va=s(qt,"P",{});var R3=n(Va);$Co=r(R3,"The model class to instantiate is selected based on the "),vre=s(R3,"CODE",{});var CVr=n(vre);ICo=r(CVr,"model_type"),CVr.forEach(t),DCo=r(R3,` property of the config object (either
passed as an argument or loaded from `),Tre=s(R3,"CODE",{});var MVr=n(Tre);jCo=r(MVr,"pretrained_model_name_or_path"),MVr.forEach(t),NCo=r(R3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fre=s(R3,"CODE",{});var EVr=n(Fre);qCo=r(EVr,"pretrained_model_name_or_path"),EVr.forEach(t),GCo=r(R3,":"),R3.forEach(t),OCo=i(qt),A=s(qt,"UL",{});var L=n(A);kb=s(L,"LI",{});var E5e=n(kb);Cre=s(E5e,"STRONG",{});var yVr=n(Cre);XCo=r(yVr,"albert"),yVr.forEach(t),VCo=r(E5e," \u2014 "),j$=s(E5e,"A",{href:!0});var wVr=n(j$);zCo=r(wVr,"AlbertForSequenceClassification"),wVr.forEach(t),WCo=r(E5e," (ALBERT model)"),E5e.forEach(t),QCo=i(L),Rb=s(L,"LI",{});var y5e=n(Rb);Mre=s(y5e,"STRONG",{});var AVr=n(Mre);HCo=r(AVr,"bart"),AVr.forEach(t),UCo=r(y5e," \u2014 "),N$=s(y5e,"A",{href:!0});var LVr=n(N$);JCo=r(LVr,"BartForSequenceClassification"),LVr.forEach(t),YCo=r(y5e," (BART model)"),y5e.forEach(t),KCo=i(L),Sb=s(L,"LI",{});var w5e=n(Sb);Ere=s(w5e,"STRONG",{});var BVr=n(Ere);ZCo=r(BVr,"bert"),BVr.forEach(t),e4o=r(w5e," \u2014 "),q$=s(w5e,"A",{href:!0});var xVr=n(q$);o4o=r(xVr,"BertForSequenceClassification"),xVr.forEach(t),r4o=r(w5e," (BERT model)"),w5e.forEach(t),t4o=i(L),Pb=s(L,"LI",{});var A5e=n(Pb);yre=s(A5e,"STRONG",{});var kVr=n(yre);a4o=r(kVr,"big_bird"),kVr.forEach(t),s4o=r(A5e," \u2014 "),G$=s(A5e,"A",{href:!0});var RVr=n(G$);n4o=r(RVr,"BigBirdForSequenceClassification"),RVr.forEach(t),l4o=r(A5e," (BigBird model)"),A5e.forEach(t),i4o=i(L),$b=s(L,"LI",{});var L5e=n($b);wre=s(L5e,"STRONG",{});var SVr=n(wre);d4o=r(SVr,"bigbird_pegasus"),SVr.forEach(t),c4o=r(L5e," \u2014 "),O$=s(L5e,"A",{href:!0});var PVr=n(O$);m4o=r(PVr,"BigBirdPegasusForSequenceClassification"),PVr.forEach(t),f4o=r(L5e," (BigBirdPegasus model)"),L5e.forEach(t),g4o=i(L),Ib=s(L,"LI",{});var B5e=n(Ib);Are=s(B5e,"STRONG",{});var $Vr=n(Are);h4o=r($Vr,"camembert"),$Vr.forEach(t),u4o=r(B5e," \u2014 "),X$=s(B5e,"A",{href:!0});var IVr=n(X$);p4o=r(IVr,"CamembertForSequenceClassification"),IVr.forEach(t),_4o=r(B5e," (CamemBERT model)"),B5e.forEach(t),b4o=i(L),Db=s(L,"LI",{});var x5e=n(Db);Lre=s(x5e,"STRONG",{});var DVr=n(Lre);v4o=r(DVr,"canine"),DVr.forEach(t),T4o=r(x5e," \u2014 "),V$=s(x5e,"A",{href:!0});var jVr=n(V$);F4o=r(jVr,"CanineForSequenceClassification"),jVr.forEach(t),C4o=r(x5e," (Canine model)"),x5e.forEach(t),M4o=i(L),jb=s(L,"LI",{});var k5e=n(jb);Bre=s(k5e,"STRONG",{});var NVr=n(Bre);E4o=r(NVr,"convbert"),NVr.forEach(t),y4o=r(k5e," \u2014 "),z$=s(k5e,"A",{href:!0});var qVr=n(z$);w4o=r(qVr,"ConvBertForSequenceClassification"),qVr.forEach(t),A4o=r(k5e," (ConvBERT model)"),k5e.forEach(t),L4o=i(L),Nb=s(L,"LI",{});var R5e=n(Nb);xre=s(R5e,"STRONG",{});var GVr=n(xre);B4o=r(GVr,"ctrl"),GVr.forEach(t),x4o=r(R5e," \u2014 "),W$=s(R5e,"A",{href:!0});var OVr=n(W$);k4o=r(OVr,"CTRLForSequenceClassification"),OVr.forEach(t),R4o=r(R5e," (CTRL model)"),R5e.forEach(t),S4o=i(L),qb=s(L,"LI",{});var S5e=n(qb);kre=s(S5e,"STRONG",{});var XVr=n(kre);P4o=r(XVr,"data2vec-text"),XVr.forEach(t),$4o=r(S5e," \u2014 "),Q$=s(S5e,"A",{href:!0});var VVr=n(Q$);I4o=r(VVr,"Data2VecTextForSequenceClassification"),VVr.forEach(t),D4o=r(S5e," (Data2VecText model)"),S5e.forEach(t),j4o=i(L),Gb=s(L,"LI",{});var P5e=n(Gb);Rre=s(P5e,"STRONG",{});var zVr=n(Rre);N4o=r(zVr,"deberta"),zVr.forEach(t),q4o=r(P5e," \u2014 "),H$=s(P5e,"A",{href:!0});var WVr=n(H$);G4o=r(WVr,"DebertaForSequenceClassification"),WVr.forEach(t),O4o=r(P5e," (DeBERTa model)"),P5e.forEach(t),X4o=i(L),Ob=s(L,"LI",{});var $5e=n(Ob);Sre=s($5e,"STRONG",{});var QVr=n(Sre);V4o=r(QVr,"deberta-v2"),QVr.forEach(t),z4o=r($5e," \u2014 "),U$=s($5e,"A",{href:!0});var HVr=n(U$);W4o=r(HVr,"DebertaV2ForSequenceClassification"),HVr.forEach(t),Q4o=r($5e," (DeBERTa-v2 model)"),$5e.forEach(t),H4o=i(L),Xb=s(L,"LI",{});var I5e=n(Xb);Pre=s(I5e,"STRONG",{});var UVr=n(Pre);U4o=r(UVr,"distilbert"),UVr.forEach(t),J4o=r(I5e," \u2014 "),J$=s(I5e,"A",{href:!0});var JVr=n(J$);Y4o=r(JVr,"DistilBertForSequenceClassification"),JVr.forEach(t),K4o=r(I5e," (DistilBERT model)"),I5e.forEach(t),Z4o=i(L),Vb=s(L,"LI",{});var D5e=n(Vb);$re=s(D5e,"STRONG",{});var YVr=n($re);eMo=r(YVr,"electra"),YVr.forEach(t),oMo=r(D5e," \u2014 "),Y$=s(D5e,"A",{href:!0});var KVr=n(Y$);rMo=r(KVr,"ElectraForSequenceClassification"),KVr.forEach(t),tMo=r(D5e," (ELECTRA model)"),D5e.forEach(t),aMo=i(L),zb=s(L,"LI",{});var j5e=n(zb);Ire=s(j5e,"STRONG",{});var ZVr=n(Ire);sMo=r(ZVr,"flaubert"),ZVr.forEach(t),nMo=r(j5e," \u2014 "),K$=s(j5e,"A",{href:!0});var ezr=n(K$);lMo=r(ezr,"FlaubertForSequenceClassification"),ezr.forEach(t),iMo=r(j5e," (FlauBERT model)"),j5e.forEach(t),dMo=i(L),Wb=s(L,"LI",{});var N5e=n(Wb);Dre=s(N5e,"STRONG",{});var ozr=n(Dre);cMo=r(ozr,"fnet"),ozr.forEach(t),mMo=r(N5e," \u2014 "),Z$=s(N5e,"A",{href:!0});var rzr=n(Z$);fMo=r(rzr,"FNetForSequenceClassification"),rzr.forEach(t),gMo=r(N5e," (FNet model)"),N5e.forEach(t),hMo=i(L),Qb=s(L,"LI",{});var q5e=n(Qb);jre=s(q5e,"STRONG",{});var tzr=n(jre);uMo=r(tzr,"funnel"),tzr.forEach(t),pMo=r(q5e," \u2014 "),eI=s(q5e,"A",{href:!0});var azr=n(eI);_Mo=r(azr,"FunnelForSequenceClassification"),azr.forEach(t),bMo=r(q5e," (Funnel Transformer model)"),q5e.forEach(t),vMo=i(L),Hb=s(L,"LI",{});var G5e=n(Hb);Nre=s(G5e,"STRONG",{});var szr=n(Nre);TMo=r(szr,"gpt2"),szr.forEach(t),FMo=r(G5e," \u2014 "),oI=s(G5e,"A",{href:!0});var nzr=n(oI);CMo=r(nzr,"GPT2ForSequenceClassification"),nzr.forEach(t),MMo=r(G5e," (OpenAI GPT-2 model)"),G5e.forEach(t),EMo=i(L),Ub=s(L,"LI",{});var O5e=n(Ub);qre=s(O5e,"STRONG",{});var lzr=n(qre);yMo=r(lzr,"gpt_neo"),lzr.forEach(t),wMo=r(O5e," \u2014 "),rI=s(O5e,"A",{href:!0});var izr=n(rI);AMo=r(izr,"GPTNeoForSequenceClassification"),izr.forEach(t),LMo=r(O5e," (GPT Neo model)"),O5e.forEach(t),BMo=i(L),Jb=s(L,"LI",{});var X5e=n(Jb);Gre=s(X5e,"STRONG",{});var dzr=n(Gre);xMo=r(dzr,"gptj"),dzr.forEach(t),kMo=r(X5e," \u2014 "),tI=s(X5e,"A",{href:!0});var czr=n(tI);RMo=r(czr,"GPTJForSequenceClassification"),czr.forEach(t),SMo=r(X5e," (GPT-J model)"),X5e.forEach(t),PMo=i(L),Yb=s(L,"LI",{});var V5e=n(Yb);Ore=s(V5e,"STRONG",{});var mzr=n(Ore);$Mo=r(mzr,"ibert"),mzr.forEach(t),IMo=r(V5e," \u2014 "),aI=s(V5e,"A",{href:!0});var fzr=n(aI);DMo=r(fzr,"IBertForSequenceClassification"),fzr.forEach(t),jMo=r(V5e," (I-BERT model)"),V5e.forEach(t),NMo=i(L),Kb=s(L,"LI",{});var z5e=n(Kb);Xre=s(z5e,"STRONG",{});var gzr=n(Xre);qMo=r(gzr,"layoutlm"),gzr.forEach(t),GMo=r(z5e," \u2014 "),sI=s(z5e,"A",{href:!0});var hzr=n(sI);OMo=r(hzr,"LayoutLMForSequenceClassification"),hzr.forEach(t),XMo=r(z5e," (LayoutLM model)"),z5e.forEach(t),VMo=i(L),Zb=s(L,"LI",{});var W5e=n(Zb);Vre=s(W5e,"STRONG",{});var uzr=n(Vre);zMo=r(uzr,"layoutlmv2"),uzr.forEach(t),WMo=r(W5e," \u2014 "),nI=s(W5e,"A",{href:!0});var pzr=n(nI);QMo=r(pzr,"LayoutLMv2ForSequenceClassification"),pzr.forEach(t),HMo=r(W5e," (LayoutLMv2 model)"),W5e.forEach(t),UMo=i(L),e2=s(L,"LI",{});var Q5e=n(e2);zre=s(Q5e,"STRONG",{});var _zr=n(zre);JMo=r(_zr,"led"),_zr.forEach(t),YMo=r(Q5e," \u2014 "),lI=s(Q5e,"A",{href:!0});var bzr=n(lI);KMo=r(bzr,"LEDForSequenceClassification"),bzr.forEach(t),ZMo=r(Q5e," (LED model)"),Q5e.forEach(t),eEo=i(L),o2=s(L,"LI",{});var H5e=n(o2);Wre=s(H5e,"STRONG",{});var vzr=n(Wre);oEo=r(vzr,"longformer"),vzr.forEach(t),rEo=r(H5e," \u2014 "),iI=s(H5e,"A",{href:!0});var Tzr=n(iI);tEo=r(Tzr,"LongformerForSequenceClassification"),Tzr.forEach(t),aEo=r(H5e," (Longformer model)"),H5e.forEach(t),sEo=i(L),r2=s(L,"LI",{});var U5e=n(r2);Qre=s(U5e,"STRONG",{});var Fzr=n(Qre);nEo=r(Fzr,"mbart"),Fzr.forEach(t),lEo=r(U5e," \u2014 "),dI=s(U5e,"A",{href:!0});var Czr=n(dI);iEo=r(Czr,"MBartForSequenceClassification"),Czr.forEach(t),dEo=r(U5e," (mBART model)"),U5e.forEach(t),cEo=i(L),t2=s(L,"LI",{});var J5e=n(t2);Hre=s(J5e,"STRONG",{});var Mzr=n(Hre);mEo=r(Mzr,"megatron-bert"),Mzr.forEach(t),fEo=r(J5e," \u2014 "),cI=s(J5e,"A",{href:!0});var Ezr=n(cI);gEo=r(Ezr,"MegatronBertForSequenceClassification"),Ezr.forEach(t),hEo=r(J5e," (MegatronBert model)"),J5e.forEach(t),uEo=i(L),a2=s(L,"LI",{});var Y5e=n(a2);Ure=s(Y5e,"STRONG",{});var yzr=n(Ure);pEo=r(yzr,"mobilebert"),yzr.forEach(t),_Eo=r(Y5e," \u2014 "),mI=s(Y5e,"A",{href:!0});var wzr=n(mI);bEo=r(wzr,"MobileBertForSequenceClassification"),wzr.forEach(t),vEo=r(Y5e," (MobileBERT model)"),Y5e.forEach(t),TEo=i(L),s2=s(L,"LI",{});var K5e=n(s2);Jre=s(K5e,"STRONG",{});var Azr=n(Jre);FEo=r(Azr,"mpnet"),Azr.forEach(t),CEo=r(K5e," \u2014 "),fI=s(K5e,"A",{href:!0});var Lzr=n(fI);MEo=r(Lzr,"MPNetForSequenceClassification"),Lzr.forEach(t),EEo=r(K5e," (MPNet model)"),K5e.forEach(t),yEo=i(L),n2=s(L,"LI",{});var Z5e=n(n2);Yre=s(Z5e,"STRONG",{});var Bzr=n(Yre);wEo=r(Bzr,"nystromformer"),Bzr.forEach(t),AEo=r(Z5e," \u2014 "),gI=s(Z5e,"A",{href:!0});var xzr=n(gI);LEo=r(xzr,"NystromformerForSequenceClassification"),xzr.forEach(t),BEo=r(Z5e," (Nystromformer model)"),Z5e.forEach(t),xEo=i(L),l2=s(L,"LI",{});var eye=n(l2);Kre=s(eye,"STRONG",{});var kzr=n(Kre);kEo=r(kzr,"openai-gpt"),kzr.forEach(t),REo=r(eye," \u2014 "),hI=s(eye,"A",{href:!0});var Rzr=n(hI);SEo=r(Rzr,"OpenAIGPTForSequenceClassification"),Rzr.forEach(t),PEo=r(eye," (OpenAI GPT model)"),eye.forEach(t),$Eo=i(L),i2=s(L,"LI",{});var oye=n(i2);Zre=s(oye,"STRONG",{});var Szr=n(Zre);IEo=r(Szr,"perceiver"),Szr.forEach(t),DEo=r(oye," \u2014 "),uI=s(oye,"A",{href:!0});var Pzr=n(uI);jEo=r(Pzr,"PerceiverForSequenceClassification"),Pzr.forEach(t),NEo=r(oye," (Perceiver model)"),oye.forEach(t),qEo=i(L),d2=s(L,"LI",{});var rye=n(d2);ete=s(rye,"STRONG",{});var $zr=n(ete);GEo=r($zr,"plbart"),$zr.forEach(t),OEo=r(rye," \u2014 "),pI=s(rye,"A",{href:!0});var Izr=n(pI);XEo=r(Izr,"PLBartForSequenceClassification"),Izr.forEach(t),VEo=r(rye," (PLBart model)"),rye.forEach(t),zEo=i(L),c2=s(L,"LI",{});var tye=n(c2);ote=s(tye,"STRONG",{});var Dzr=n(ote);WEo=r(Dzr,"qdqbert"),Dzr.forEach(t),QEo=r(tye," \u2014 "),_I=s(tye,"A",{href:!0});var jzr=n(_I);HEo=r(jzr,"QDQBertForSequenceClassification"),jzr.forEach(t),UEo=r(tye," (QDQBert model)"),tye.forEach(t),JEo=i(L),m2=s(L,"LI",{});var aye=n(m2);rte=s(aye,"STRONG",{});var Nzr=n(rte);YEo=r(Nzr,"reformer"),Nzr.forEach(t),KEo=r(aye," \u2014 "),bI=s(aye,"A",{href:!0});var qzr=n(bI);ZEo=r(qzr,"ReformerForSequenceClassification"),qzr.forEach(t),e3o=r(aye," (Reformer model)"),aye.forEach(t),o3o=i(L),f2=s(L,"LI",{});var sye=n(f2);tte=s(sye,"STRONG",{});var Gzr=n(tte);r3o=r(Gzr,"rembert"),Gzr.forEach(t),t3o=r(sye," \u2014 "),vI=s(sye,"A",{href:!0});var Ozr=n(vI);a3o=r(Ozr,"RemBertForSequenceClassification"),Ozr.forEach(t),s3o=r(sye," (RemBERT model)"),sye.forEach(t),n3o=i(L),g2=s(L,"LI",{});var nye=n(g2);ate=s(nye,"STRONG",{});var Xzr=n(ate);l3o=r(Xzr,"roberta"),Xzr.forEach(t),i3o=r(nye," \u2014 "),TI=s(nye,"A",{href:!0});var Vzr=n(TI);d3o=r(Vzr,"RobertaForSequenceClassification"),Vzr.forEach(t),c3o=r(nye," (RoBERTa model)"),nye.forEach(t),m3o=i(L),h2=s(L,"LI",{});var lye=n(h2);ste=s(lye,"STRONG",{});var zzr=n(ste);f3o=r(zzr,"roformer"),zzr.forEach(t),g3o=r(lye," \u2014 "),FI=s(lye,"A",{href:!0});var Wzr=n(FI);h3o=r(Wzr,"RoFormerForSequenceClassification"),Wzr.forEach(t),u3o=r(lye," (RoFormer model)"),lye.forEach(t),p3o=i(L),u2=s(L,"LI",{});var iye=n(u2);nte=s(iye,"STRONG",{});var Qzr=n(nte);_3o=r(Qzr,"squeezebert"),Qzr.forEach(t),b3o=r(iye," \u2014 "),CI=s(iye,"A",{href:!0});var Hzr=n(CI);v3o=r(Hzr,"SqueezeBertForSequenceClassification"),Hzr.forEach(t),T3o=r(iye," (SqueezeBERT model)"),iye.forEach(t),F3o=i(L),p2=s(L,"LI",{});var dye=n(p2);lte=s(dye,"STRONG",{});var Uzr=n(lte);C3o=r(Uzr,"tapas"),Uzr.forEach(t),M3o=r(dye," \u2014 "),MI=s(dye,"A",{href:!0});var Jzr=n(MI);E3o=r(Jzr,"TapasForSequenceClassification"),Jzr.forEach(t),y3o=r(dye," (TAPAS model)"),dye.forEach(t),w3o=i(L),_2=s(L,"LI",{});var cye=n(_2);ite=s(cye,"STRONG",{});var Yzr=n(ite);A3o=r(Yzr,"transfo-xl"),Yzr.forEach(t),L3o=r(cye," \u2014 "),EI=s(cye,"A",{href:!0});var Kzr=n(EI);B3o=r(Kzr,"TransfoXLForSequenceClassification"),Kzr.forEach(t),x3o=r(cye," (Transformer-XL model)"),cye.forEach(t),k3o=i(L),b2=s(L,"LI",{});var mye=n(b2);dte=s(mye,"STRONG",{});var Zzr=n(dte);R3o=r(Zzr,"xlm"),Zzr.forEach(t),S3o=r(mye," \u2014 "),yI=s(mye,"A",{href:!0});var eWr=n(yI);P3o=r(eWr,"XLMForSequenceClassification"),eWr.forEach(t),$3o=r(mye," (XLM model)"),mye.forEach(t),I3o=i(L),v2=s(L,"LI",{});var fye=n(v2);cte=s(fye,"STRONG",{});var oWr=n(cte);D3o=r(oWr,"xlm-roberta"),oWr.forEach(t),j3o=r(fye," \u2014 "),wI=s(fye,"A",{href:!0});var rWr=n(wI);N3o=r(rWr,"XLMRobertaForSequenceClassification"),rWr.forEach(t),q3o=r(fye," (XLM-RoBERTa model)"),fye.forEach(t),G3o=i(L),T2=s(L,"LI",{});var gye=n(T2);mte=s(gye,"STRONG",{});var tWr=n(mte);O3o=r(tWr,"xlm-roberta-xl"),tWr.forEach(t),X3o=r(gye," \u2014 "),AI=s(gye,"A",{href:!0});var aWr=n(AI);V3o=r(aWr,"XLMRobertaXLForSequenceClassification"),aWr.forEach(t),z3o=r(gye," (XLM-RoBERTa-XL model)"),gye.forEach(t),W3o=i(L),F2=s(L,"LI",{});var hye=n(F2);fte=s(hye,"STRONG",{});var sWr=n(fte);Q3o=r(sWr,"xlnet"),sWr.forEach(t),H3o=r(hye," \u2014 "),LI=s(hye,"A",{href:!0});var nWr=n(LI);U3o=r(nWr,"XLNetForSequenceClassification"),nWr.forEach(t),J3o=r(hye," (XLNet model)"),hye.forEach(t),Y3o=i(L),C2=s(L,"LI",{});var uye=n(C2);gte=s(uye,"STRONG",{});var lWr=n(gte);K3o=r(lWr,"yoso"),lWr.forEach(t),Z3o=r(uye," \u2014 "),BI=s(uye,"A",{href:!0});var iWr=n(BI);e5o=r(iWr,"YosoForSequenceClassification"),iWr.forEach(t),o5o=r(uye," (YOSO model)"),uye.forEach(t),L.forEach(t),r5o=i(qt),M2=s(qt,"P",{});var pye=n(M2);t5o=r(pye,"The model is set in evaluation mode by default using "),hte=s(pye,"CODE",{});var dWr=n(hte);a5o=r(dWr,"model.eval()"),dWr.forEach(t),s5o=r(pye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ute=s(pye,"CODE",{});var cWr=n(ute);n5o=r(cWr,"model.train()"),cWr.forEach(t),pye.forEach(t),l5o=i(qt),pte=s(qt,"P",{});var mWr=n(pte);i5o=r(mWr,"Examples:"),mWr.forEach(t),d5o=i(qt),f(Vy.$$.fragment,qt),qt.forEach(t),Yn.forEach(t),oBe=i(c),ld=s(c,"H2",{class:!0});var ike=n(ld);E2=s(ike,"A",{id:!0,class:!0,href:!0});var fWr=n(E2);_te=s(fWr,"SPAN",{});var gWr=n(_te);f(zy.$$.fragment,gWr),gWr.forEach(t),fWr.forEach(t),c5o=i(ike),bte=s(ike,"SPAN",{});var hWr=n(bte);m5o=r(hWr,"AutoModelForMultipleChoice"),hWr.forEach(t),ike.forEach(t),rBe=i(c),Zo=s(c,"DIV",{class:!0});var Zn=n(Zo);f(Wy.$$.fragment,Zn),f5o=i(Zn),id=s(Zn,"P",{});var UV=n(id);g5o=r(UV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vte=s(UV,"CODE",{});var uWr=n(vte);h5o=r(uWr,"from_pretrained()"),uWr.forEach(t),u5o=r(UV,"class method or the "),Tte=s(UV,"CODE",{});var pWr=n(Tte);p5o=r(pWr,"from_config()"),pWr.forEach(t),_5o=r(UV,`class
method.`),UV.forEach(t),b5o=i(Zn),Qy=s(Zn,"P",{});var dke=n(Qy);v5o=r(dke,"This class cannot be instantiated directly using "),Fte=s(dke,"CODE",{});var _Wr=n(Fte);T5o=r(_Wr,"__init__()"),_Wr.forEach(t),F5o=r(dke," (throws an error)."),dke.forEach(t),C5o=i(Zn),Wr=s(Zn,"DIV",{class:!0});var el=n(Wr);f(Hy.$$.fragment,el),M5o=i(el),Cte=s(el,"P",{});var bWr=n(Cte);E5o=r(bWr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),bWr.forEach(t),y5o=i(el),dd=s(el,"P",{});var JV=n(dd);w5o=r(JV,`Note:
Loading a model from its configuration file does `),Mte=s(JV,"STRONG",{});var vWr=n(Mte);A5o=r(vWr,"not"),vWr.forEach(t),L5o=r(JV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ete=s(JV,"CODE",{});var TWr=n(Ete);B5o=r(TWr,"from_pretrained()"),TWr.forEach(t),x5o=r(JV,"to load the model weights."),JV.forEach(t),k5o=i(el),yte=s(el,"P",{});var FWr=n(yte);R5o=r(FWr,"Examples:"),FWr.forEach(t),S5o=i(el),f(Uy.$$.fragment,el),el.forEach(t),P5o=i(Zn),je=s(Zn,"DIV",{class:!0});var Gt=n(je);f(Jy.$$.fragment,Gt),$5o=i(Gt),wte=s(Gt,"P",{});var CWr=n(wte);I5o=r(CWr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),CWr.forEach(t),D5o=i(Gt),za=s(Gt,"P",{});var S3=n(za);j5o=r(S3,"The model class to instantiate is selected based on the "),Ate=s(S3,"CODE",{});var MWr=n(Ate);N5o=r(MWr,"model_type"),MWr.forEach(t),q5o=r(S3,` property of the config object (either
passed as an argument or loaded from `),Lte=s(S3,"CODE",{});var EWr=n(Lte);G5o=r(EWr,"pretrained_model_name_or_path"),EWr.forEach(t),O5o=r(S3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bte=s(S3,"CODE",{});var yWr=n(Bte);X5o=r(yWr,"pretrained_model_name_or_path"),yWr.forEach(t),V5o=r(S3,":"),S3.forEach(t),z5o=i(Gt),G=s(Gt,"UL",{});var O=n(G);y2=s(O,"LI",{});var _ye=n(y2);xte=s(_ye,"STRONG",{});var wWr=n(xte);W5o=r(wWr,"albert"),wWr.forEach(t),Q5o=r(_ye," \u2014 "),xI=s(_ye,"A",{href:!0});var AWr=n(xI);H5o=r(AWr,"AlbertForMultipleChoice"),AWr.forEach(t),U5o=r(_ye," (ALBERT model)"),_ye.forEach(t),J5o=i(O),w2=s(O,"LI",{});var bye=n(w2);kte=s(bye,"STRONG",{});var LWr=n(kte);Y5o=r(LWr,"bert"),LWr.forEach(t),K5o=r(bye," \u2014 "),kI=s(bye,"A",{href:!0});var BWr=n(kI);Z5o=r(BWr,"BertForMultipleChoice"),BWr.forEach(t),eyo=r(bye," (BERT model)"),bye.forEach(t),oyo=i(O),A2=s(O,"LI",{});var vye=n(A2);Rte=s(vye,"STRONG",{});var xWr=n(Rte);ryo=r(xWr,"big_bird"),xWr.forEach(t),tyo=r(vye," \u2014 "),RI=s(vye,"A",{href:!0});var kWr=n(RI);ayo=r(kWr,"BigBirdForMultipleChoice"),kWr.forEach(t),syo=r(vye," (BigBird model)"),vye.forEach(t),nyo=i(O),L2=s(O,"LI",{});var Tye=n(L2);Ste=s(Tye,"STRONG",{});var RWr=n(Ste);lyo=r(RWr,"camembert"),RWr.forEach(t),iyo=r(Tye," \u2014 "),SI=s(Tye,"A",{href:!0});var SWr=n(SI);dyo=r(SWr,"CamembertForMultipleChoice"),SWr.forEach(t),cyo=r(Tye," (CamemBERT model)"),Tye.forEach(t),myo=i(O),B2=s(O,"LI",{});var Fye=n(B2);Pte=s(Fye,"STRONG",{});var PWr=n(Pte);fyo=r(PWr,"canine"),PWr.forEach(t),gyo=r(Fye," \u2014 "),PI=s(Fye,"A",{href:!0});var $Wr=n(PI);hyo=r($Wr,"CanineForMultipleChoice"),$Wr.forEach(t),uyo=r(Fye," (Canine model)"),Fye.forEach(t),pyo=i(O),x2=s(O,"LI",{});var Cye=n(x2);$te=s(Cye,"STRONG",{});var IWr=n($te);_yo=r(IWr,"convbert"),IWr.forEach(t),byo=r(Cye," \u2014 "),$I=s(Cye,"A",{href:!0});var DWr=n($I);vyo=r(DWr,"ConvBertForMultipleChoice"),DWr.forEach(t),Tyo=r(Cye," (ConvBERT model)"),Cye.forEach(t),Fyo=i(O),k2=s(O,"LI",{});var Mye=n(k2);Ite=s(Mye,"STRONG",{});var jWr=n(Ite);Cyo=r(jWr,"data2vec-text"),jWr.forEach(t),Myo=r(Mye," \u2014 "),II=s(Mye,"A",{href:!0});var NWr=n(II);Eyo=r(NWr,"Data2VecTextForMultipleChoice"),NWr.forEach(t),yyo=r(Mye," (Data2VecText model)"),Mye.forEach(t),wyo=i(O),R2=s(O,"LI",{});var Eye=n(R2);Dte=s(Eye,"STRONG",{});var qWr=n(Dte);Ayo=r(qWr,"distilbert"),qWr.forEach(t),Lyo=r(Eye," \u2014 "),DI=s(Eye,"A",{href:!0});var GWr=n(DI);Byo=r(GWr,"DistilBertForMultipleChoice"),GWr.forEach(t),xyo=r(Eye," (DistilBERT model)"),Eye.forEach(t),kyo=i(O),S2=s(O,"LI",{});var yye=n(S2);jte=s(yye,"STRONG",{});var OWr=n(jte);Ryo=r(OWr,"electra"),OWr.forEach(t),Syo=r(yye," \u2014 "),jI=s(yye,"A",{href:!0});var XWr=n(jI);Pyo=r(XWr,"ElectraForMultipleChoice"),XWr.forEach(t),$yo=r(yye," (ELECTRA model)"),yye.forEach(t),Iyo=i(O),P2=s(O,"LI",{});var wye=n(P2);Nte=s(wye,"STRONG",{});var VWr=n(Nte);Dyo=r(VWr,"flaubert"),VWr.forEach(t),jyo=r(wye," \u2014 "),NI=s(wye,"A",{href:!0});var zWr=n(NI);Nyo=r(zWr,"FlaubertForMultipleChoice"),zWr.forEach(t),qyo=r(wye," (FlauBERT model)"),wye.forEach(t),Gyo=i(O),$2=s(O,"LI",{});var Aye=n($2);qte=s(Aye,"STRONG",{});var WWr=n(qte);Oyo=r(WWr,"fnet"),WWr.forEach(t),Xyo=r(Aye," \u2014 "),qI=s(Aye,"A",{href:!0});var QWr=n(qI);Vyo=r(QWr,"FNetForMultipleChoice"),QWr.forEach(t),zyo=r(Aye," (FNet model)"),Aye.forEach(t),Wyo=i(O),I2=s(O,"LI",{});var Lye=n(I2);Gte=s(Lye,"STRONG",{});var HWr=n(Gte);Qyo=r(HWr,"funnel"),HWr.forEach(t),Hyo=r(Lye," \u2014 "),GI=s(Lye,"A",{href:!0});var UWr=n(GI);Uyo=r(UWr,"FunnelForMultipleChoice"),UWr.forEach(t),Jyo=r(Lye," (Funnel Transformer model)"),Lye.forEach(t),Yyo=i(O),D2=s(O,"LI",{});var Bye=n(D2);Ote=s(Bye,"STRONG",{});var JWr=n(Ote);Kyo=r(JWr,"ibert"),JWr.forEach(t),Zyo=r(Bye," \u2014 "),OI=s(Bye,"A",{href:!0});var YWr=n(OI);ewo=r(YWr,"IBertForMultipleChoice"),YWr.forEach(t),owo=r(Bye," (I-BERT model)"),Bye.forEach(t),rwo=i(O),j2=s(O,"LI",{});var xye=n(j2);Xte=s(xye,"STRONG",{});var KWr=n(Xte);two=r(KWr,"longformer"),KWr.forEach(t),awo=r(xye," \u2014 "),XI=s(xye,"A",{href:!0});var ZWr=n(XI);swo=r(ZWr,"LongformerForMultipleChoice"),ZWr.forEach(t),nwo=r(xye," (Longformer model)"),xye.forEach(t),lwo=i(O),N2=s(O,"LI",{});var kye=n(N2);Vte=s(kye,"STRONG",{});var eQr=n(Vte);iwo=r(eQr,"megatron-bert"),eQr.forEach(t),dwo=r(kye," \u2014 "),VI=s(kye,"A",{href:!0});var oQr=n(VI);cwo=r(oQr,"MegatronBertForMultipleChoice"),oQr.forEach(t),mwo=r(kye," (MegatronBert model)"),kye.forEach(t),fwo=i(O),q2=s(O,"LI",{});var Rye=n(q2);zte=s(Rye,"STRONG",{});var rQr=n(zte);gwo=r(rQr,"mobilebert"),rQr.forEach(t),hwo=r(Rye," \u2014 "),zI=s(Rye,"A",{href:!0});var tQr=n(zI);uwo=r(tQr,"MobileBertForMultipleChoice"),tQr.forEach(t),pwo=r(Rye," (MobileBERT model)"),Rye.forEach(t),_wo=i(O),G2=s(O,"LI",{});var Sye=n(G2);Wte=s(Sye,"STRONG",{});var aQr=n(Wte);bwo=r(aQr,"mpnet"),aQr.forEach(t),vwo=r(Sye," \u2014 "),WI=s(Sye,"A",{href:!0});var sQr=n(WI);Two=r(sQr,"MPNetForMultipleChoice"),sQr.forEach(t),Fwo=r(Sye," (MPNet model)"),Sye.forEach(t),Cwo=i(O),O2=s(O,"LI",{});var Pye=n(O2);Qte=s(Pye,"STRONG",{});var nQr=n(Qte);Mwo=r(nQr,"nystromformer"),nQr.forEach(t),Ewo=r(Pye," \u2014 "),QI=s(Pye,"A",{href:!0});var lQr=n(QI);ywo=r(lQr,"NystromformerForMultipleChoice"),lQr.forEach(t),wwo=r(Pye," (Nystromformer model)"),Pye.forEach(t),Awo=i(O),X2=s(O,"LI",{});var $ye=n(X2);Hte=s($ye,"STRONG",{});var iQr=n(Hte);Lwo=r(iQr,"qdqbert"),iQr.forEach(t),Bwo=r($ye," \u2014 "),HI=s($ye,"A",{href:!0});var dQr=n(HI);xwo=r(dQr,"QDQBertForMultipleChoice"),dQr.forEach(t),kwo=r($ye," (QDQBert model)"),$ye.forEach(t),Rwo=i(O),V2=s(O,"LI",{});var Iye=n(V2);Ute=s(Iye,"STRONG",{});var cQr=n(Ute);Swo=r(cQr,"rembert"),cQr.forEach(t),Pwo=r(Iye," \u2014 "),UI=s(Iye,"A",{href:!0});var mQr=n(UI);$wo=r(mQr,"RemBertForMultipleChoice"),mQr.forEach(t),Iwo=r(Iye," (RemBERT model)"),Iye.forEach(t),Dwo=i(O),z2=s(O,"LI",{});var Dye=n(z2);Jte=s(Dye,"STRONG",{});var fQr=n(Jte);jwo=r(fQr,"roberta"),fQr.forEach(t),Nwo=r(Dye," \u2014 "),JI=s(Dye,"A",{href:!0});var gQr=n(JI);qwo=r(gQr,"RobertaForMultipleChoice"),gQr.forEach(t),Gwo=r(Dye," (RoBERTa model)"),Dye.forEach(t),Owo=i(O),W2=s(O,"LI",{});var jye=n(W2);Yte=s(jye,"STRONG",{});var hQr=n(Yte);Xwo=r(hQr,"roformer"),hQr.forEach(t),Vwo=r(jye," \u2014 "),YI=s(jye,"A",{href:!0});var uQr=n(YI);zwo=r(uQr,"RoFormerForMultipleChoice"),uQr.forEach(t),Wwo=r(jye," (RoFormer model)"),jye.forEach(t),Qwo=i(O),Q2=s(O,"LI",{});var Nye=n(Q2);Kte=s(Nye,"STRONG",{});var pQr=n(Kte);Hwo=r(pQr,"squeezebert"),pQr.forEach(t),Uwo=r(Nye," \u2014 "),KI=s(Nye,"A",{href:!0});var _Qr=n(KI);Jwo=r(_Qr,"SqueezeBertForMultipleChoice"),_Qr.forEach(t),Ywo=r(Nye," (SqueezeBERT model)"),Nye.forEach(t),Kwo=i(O),H2=s(O,"LI",{});var qye=n(H2);Zte=s(qye,"STRONG",{});var bQr=n(Zte);Zwo=r(bQr,"xlm"),bQr.forEach(t),e6o=r(qye," \u2014 "),ZI=s(qye,"A",{href:!0});var vQr=n(ZI);o6o=r(vQr,"XLMForMultipleChoice"),vQr.forEach(t),r6o=r(qye," (XLM model)"),qye.forEach(t),t6o=i(O),U2=s(O,"LI",{});var Gye=n(U2);eae=s(Gye,"STRONG",{});var TQr=n(eae);a6o=r(TQr,"xlm-roberta"),TQr.forEach(t),s6o=r(Gye," \u2014 "),eD=s(Gye,"A",{href:!0});var FQr=n(eD);n6o=r(FQr,"XLMRobertaForMultipleChoice"),FQr.forEach(t),l6o=r(Gye," (XLM-RoBERTa model)"),Gye.forEach(t),i6o=i(O),J2=s(O,"LI",{});var Oye=n(J2);oae=s(Oye,"STRONG",{});var CQr=n(oae);d6o=r(CQr,"xlm-roberta-xl"),CQr.forEach(t),c6o=r(Oye," \u2014 "),oD=s(Oye,"A",{href:!0});var MQr=n(oD);m6o=r(MQr,"XLMRobertaXLForMultipleChoice"),MQr.forEach(t),f6o=r(Oye," (XLM-RoBERTa-XL model)"),Oye.forEach(t),g6o=i(O),Y2=s(O,"LI",{});var Xye=n(Y2);rae=s(Xye,"STRONG",{});var EQr=n(rae);h6o=r(EQr,"xlnet"),EQr.forEach(t),u6o=r(Xye," \u2014 "),rD=s(Xye,"A",{href:!0});var yQr=n(rD);p6o=r(yQr,"XLNetForMultipleChoice"),yQr.forEach(t),_6o=r(Xye," (XLNet model)"),Xye.forEach(t),b6o=i(O),K2=s(O,"LI",{});var Vye=n(K2);tae=s(Vye,"STRONG",{});var wQr=n(tae);v6o=r(wQr,"yoso"),wQr.forEach(t),T6o=r(Vye," \u2014 "),tD=s(Vye,"A",{href:!0});var AQr=n(tD);F6o=r(AQr,"YosoForMultipleChoice"),AQr.forEach(t),C6o=r(Vye," (YOSO model)"),Vye.forEach(t),O.forEach(t),M6o=i(Gt),Z2=s(Gt,"P",{});var zye=n(Z2);E6o=r(zye,"The model is set in evaluation mode by default using "),aae=s(zye,"CODE",{});var LQr=n(aae);y6o=r(LQr,"model.eval()"),LQr.forEach(t),w6o=r(zye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),sae=s(zye,"CODE",{});var BQr=n(sae);A6o=r(BQr,"model.train()"),BQr.forEach(t),zye.forEach(t),L6o=i(Gt),nae=s(Gt,"P",{});var xQr=n(nae);B6o=r(xQr,"Examples:"),xQr.forEach(t),x6o=i(Gt),f(Yy.$$.fragment,Gt),Gt.forEach(t),Zn.forEach(t),tBe=i(c),cd=s(c,"H2",{class:!0});var cke=n(cd);ev=s(cke,"A",{id:!0,class:!0,href:!0});var kQr=n(ev);lae=s(kQr,"SPAN",{});var RQr=n(lae);f(Ky.$$.fragment,RQr),RQr.forEach(t),kQr.forEach(t),k6o=i(cke),iae=s(cke,"SPAN",{});var SQr=n(iae);R6o=r(SQr,"AutoModelForNextSentencePrediction"),SQr.forEach(t),cke.forEach(t),aBe=i(c),er=s(c,"DIV",{class:!0});var ol=n(er);f(Zy.$$.fragment,ol),S6o=i(ol),md=s(ol,"P",{});var YV=n(md);P6o=r(YV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),dae=s(YV,"CODE",{});var PQr=n(dae);$6o=r(PQr,"from_pretrained()"),PQr.forEach(t),I6o=r(YV,"class method or the "),cae=s(YV,"CODE",{});var $Qr=n(cae);D6o=r($Qr,"from_config()"),$Qr.forEach(t),j6o=r(YV,`class
method.`),YV.forEach(t),N6o=i(ol),ew=s(ol,"P",{});var mke=n(ew);q6o=r(mke,"This class cannot be instantiated directly using "),mae=s(mke,"CODE",{});var IQr=n(mae);G6o=r(IQr,"__init__()"),IQr.forEach(t),O6o=r(mke," (throws an error)."),mke.forEach(t),X6o=i(ol),Qr=s(ol,"DIV",{class:!0});var rl=n(Qr);f(ow.$$.fragment,rl),V6o=i(rl),fae=s(rl,"P",{});var DQr=n(fae);z6o=r(DQr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),DQr.forEach(t),W6o=i(rl),fd=s(rl,"P",{});var KV=n(fd);Q6o=r(KV,`Note:
Loading a model from its configuration file does `),gae=s(KV,"STRONG",{});var jQr=n(gae);H6o=r(jQr,"not"),jQr.forEach(t),U6o=r(KV,` load the model weights. It only affects the
model\u2019s configuration. Use `),hae=s(KV,"CODE",{});var NQr=n(hae);J6o=r(NQr,"from_pretrained()"),NQr.forEach(t),Y6o=r(KV,"to load the model weights."),KV.forEach(t),K6o=i(rl),uae=s(rl,"P",{});var qQr=n(uae);Z6o=r(qQr,"Examples:"),qQr.forEach(t),eAo=i(rl),f(rw.$$.fragment,rl),rl.forEach(t),oAo=i(ol),Ne=s(ol,"DIV",{class:!0});var Ot=n(Ne);f(tw.$$.fragment,Ot),rAo=i(Ot),pae=s(Ot,"P",{});var GQr=n(pae);tAo=r(GQr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),GQr.forEach(t),aAo=i(Ot),Wa=s(Ot,"P",{});var P3=n(Wa);sAo=r(P3,"The model class to instantiate is selected based on the "),_ae=s(P3,"CODE",{});var OQr=n(_ae);nAo=r(OQr,"model_type"),OQr.forEach(t),lAo=r(P3,` property of the config object (either
passed as an argument or loaded from `),bae=s(P3,"CODE",{});var XQr=n(bae);iAo=r(XQr,"pretrained_model_name_or_path"),XQr.forEach(t),dAo=r(P3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vae=s(P3,"CODE",{});var VQr=n(vae);cAo=r(VQr,"pretrained_model_name_or_path"),VQr.forEach(t),mAo=r(P3,":"),P3.forEach(t),fAo=i(Ot),sa=s(Ot,"UL",{});var tl=n(sa);ov=s(tl,"LI",{});var Wye=n(ov);Tae=s(Wye,"STRONG",{});var zQr=n(Tae);gAo=r(zQr,"bert"),zQr.forEach(t),hAo=r(Wye," \u2014 "),aD=s(Wye,"A",{href:!0});var WQr=n(aD);uAo=r(WQr,"BertForNextSentencePrediction"),WQr.forEach(t),pAo=r(Wye," (BERT model)"),Wye.forEach(t),_Ao=i(tl),rv=s(tl,"LI",{});var Qye=n(rv);Fae=s(Qye,"STRONG",{});var QQr=n(Fae);bAo=r(QQr,"fnet"),QQr.forEach(t),vAo=r(Qye," \u2014 "),sD=s(Qye,"A",{href:!0});var HQr=n(sD);TAo=r(HQr,"FNetForNextSentencePrediction"),HQr.forEach(t),FAo=r(Qye," (FNet model)"),Qye.forEach(t),CAo=i(tl),tv=s(tl,"LI",{});var Hye=n(tv);Cae=s(Hye,"STRONG",{});var UQr=n(Cae);MAo=r(UQr,"megatron-bert"),UQr.forEach(t),EAo=r(Hye," \u2014 "),nD=s(Hye,"A",{href:!0});var JQr=n(nD);yAo=r(JQr,"MegatronBertForNextSentencePrediction"),JQr.forEach(t),wAo=r(Hye," (MegatronBert model)"),Hye.forEach(t),AAo=i(tl),av=s(tl,"LI",{});var Uye=n(av);Mae=s(Uye,"STRONG",{});var YQr=n(Mae);LAo=r(YQr,"mobilebert"),YQr.forEach(t),BAo=r(Uye," \u2014 "),lD=s(Uye,"A",{href:!0});var KQr=n(lD);xAo=r(KQr,"MobileBertForNextSentencePrediction"),KQr.forEach(t),kAo=r(Uye," (MobileBERT model)"),Uye.forEach(t),RAo=i(tl),sv=s(tl,"LI",{});var Jye=n(sv);Eae=s(Jye,"STRONG",{});var ZQr=n(Eae);SAo=r(ZQr,"qdqbert"),ZQr.forEach(t),PAo=r(Jye," \u2014 "),iD=s(Jye,"A",{href:!0});var eHr=n(iD);$Ao=r(eHr,"QDQBertForNextSentencePrediction"),eHr.forEach(t),IAo=r(Jye," (QDQBert model)"),Jye.forEach(t),tl.forEach(t),DAo=i(Ot),nv=s(Ot,"P",{});var Yye=n(nv);jAo=r(Yye,"The model is set in evaluation mode by default using "),yae=s(Yye,"CODE",{});var oHr=n(yae);NAo=r(oHr,"model.eval()"),oHr.forEach(t),qAo=r(Yye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wae=s(Yye,"CODE",{});var rHr=n(wae);GAo=r(rHr,"model.train()"),rHr.forEach(t),Yye.forEach(t),OAo=i(Ot),Aae=s(Ot,"P",{});var tHr=n(Aae);XAo=r(tHr,"Examples:"),tHr.forEach(t),VAo=i(Ot),f(aw.$$.fragment,Ot),Ot.forEach(t),ol.forEach(t),sBe=i(c),gd=s(c,"H2",{class:!0});var fke=n(gd);lv=s(fke,"A",{id:!0,class:!0,href:!0});var aHr=n(lv);Lae=s(aHr,"SPAN",{});var sHr=n(Lae);f(sw.$$.fragment,sHr),sHr.forEach(t),aHr.forEach(t),zAo=i(fke),Bae=s(fke,"SPAN",{});var nHr=n(Bae);WAo=r(nHr,"AutoModelForTokenClassification"),nHr.forEach(t),fke.forEach(t),nBe=i(c),or=s(c,"DIV",{class:!0});var al=n(or);f(nw.$$.fragment,al),QAo=i(al),hd=s(al,"P",{});var ZV=n(hd);HAo=r(ZV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),xae=s(ZV,"CODE",{});var lHr=n(xae);UAo=r(lHr,"from_pretrained()"),lHr.forEach(t),JAo=r(ZV,"class method or the "),kae=s(ZV,"CODE",{});var iHr=n(kae);YAo=r(iHr,"from_config()"),iHr.forEach(t),KAo=r(ZV,`class
method.`),ZV.forEach(t),ZAo=i(al),lw=s(al,"P",{});var gke=n(lw);e0o=r(gke,"This class cannot be instantiated directly using "),Rae=s(gke,"CODE",{});var dHr=n(Rae);o0o=r(dHr,"__init__()"),dHr.forEach(t),r0o=r(gke," (throws an error)."),gke.forEach(t),t0o=i(al),Hr=s(al,"DIV",{class:!0});var sl=n(Hr);f(iw.$$.fragment,sl),a0o=i(sl),Sae=s(sl,"P",{});var cHr=n(Sae);s0o=r(cHr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),cHr.forEach(t),n0o=i(sl),ud=s(sl,"P",{});var ez=n(ud);l0o=r(ez,`Note:
Loading a model from its configuration file does `),Pae=s(ez,"STRONG",{});var mHr=n(Pae);i0o=r(mHr,"not"),mHr.forEach(t),d0o=r(ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ae=s(ez,"CODE",{});var fHr=n($ae);c0o=r(fHr,"from_pretrained()"),fHr.forEach(t),m0o=r(ez,"to load the model weights."),ez.forEach(t),f0o=i(sl),Iae=s(sl,"P",{});var gHr=n(Iae);g0o=r(gHr,"Examples:"),gHr.forEach(t),h0o=i(sl),f(dw.$$.fragment,sl),sl.forEach(t),u0o=i(al),qe=s(al,"DIV",{class:!0});var Xt=n(qe);f(cw.$$.fragment,Xt),p0o=i(Xt),Dae=s(Xt,"P",{});var hHr=n(Dae);_0o=r(hHr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),hHr.forEach(t),b0o=i(Xt),Qa=s(Xt,"P",{});var $3=n(Qa);v0o=r($3,"The model class to instantiate is selected based on the "),jae=s($3,"CODE",{});var uHr=n(jae);T0o=r(uHr,"model_type"),uHr.forEach(t),F0o=r($3,` property of the config object (either
passed as an argument or loaded from `),Nae=s($3,"CODE",{});var pHr=n(Nae);C0o=r(pHr,"pretrained_model_name_or_path"),pHr.forEach(t),M0o=r($3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qae=s($3,"CODE",{});var _Hr=n(qae);E0o=r(_Hr,"pretrained_model_name_or_path"),_Hr.forEach(t),y0o=r($3,":"),$3.forEach(t),w0o=i(Xt),N=s(Xt,"UL",{});var q=n(N);iv=s(q,"LI",{});var Kye=n(iv);Gae=s(Kye,"STRONG",{});var bHr=n(Gae);A0o=r(bHr,"albert"),bHr.forEach(t),L0o=r(Kye," \u2014 "),dD=s(Kye,"A",{href:!0});var vHr=n(dD);B0o=r(vHr,"AlbertForTokenClassification"),vHr.forEach(t),x0o=r(Kye," (ALBERT model)"),Kye.forEach(t),k0o=i(q),dv=s(q,"LI",{});var Zye=n(dv);Oae=s(Zye,"STRONG",{});var THr=n(Oae);R0o=r(THr,"bert"),THr.forEach(t),S0o=r(Zye," \u2014 "),cD=s(Zye,"A",{href:!0});var FHr=n(cD);P0o=r(FHr,"BertForTokenClassification"),FHr.forEach(t),$0o=r(Zye," (BERT model)"),Zye.forEach(t),I0o=i(q),cv=s(q,"LI",{});var ewe=n(cv);Xae=s(ewe,"STRONG",{});var CHr=n(Xae);D0o=r(CHr,"big_bird"),CHr.forEach(t),j0o=r(ewe," \u2014 "),mD=s(ewe,"A",{href:!0});var MHr=n(mD);N0o=r(MHr,"BigBirdForTokenClassification"),MHr.forEach(t),q0o=r(ewe," (BigBird model)"),ewe.forEach(t),G0o=i(q),mv=s(q,"LI",{});var owe=n(mv);Vae=s(owe,"STRONG",{});var EHr=n(Vae);O0o=r(EHr,"camembert"),EHr.forEach(t),X0o=r(owe," \u2014 "),fD=s(owe,"A",{href:!0});var yHr=n(fD);V0o=r(yHr,"CamembertForTokenClassification"),yHr.forEach(t),z0o=r(owe," (CamemBERT model)"),owe.forEach(t),W0o=i(q),fv=s(q,"LI",{});var rwe=n(fv);zae=s(rwe,"STRONG",{});var wHr=n(zae);Q0o=r(wHr,"canine"),wHr.forEach(t),H0o=r(rwe," \u2014 "),gD=s(rwe,"A",{href:!0});var AHr=n(gD);U0o=r(AHr,"CanineForTokenClassification"),AHr.forEach(t),J0o=r(rwe," (Canine model)"),rwe.forEach(t),Y0o=i(q),gv=s(q,"LI",{});var twe=n(gv);Wae=s(twe,"STRONG",{});var LHr=n(Wae);K0o=r(LHr,"convbert"),LHr.forEach(t),Z0o=r(twe," \u2014 "),hD=s(twe,"A",{href:!0});var BHr=n(hD);eLo=r(BHr,"ConvBertForTokenClassification"),BHr.forEach(t),oLo=r(twe," (ConvBERT model)"),twe.forEach(t),rLo=i(q),hv=s(q,"LI",{});var awe=n(hv);Qae=s(awe,"STRONG",{});var xHr=n(Qae);tLo=r(xHr,"data2vec-text"),xHr.forEach(t),aLo=r(awe," \u2014 "),uD=s(awe,"A",{href:!0});var kHr=n(uD);sLo=r(kHr,"Data2VecTextForTokenClassification"),kHr.forEach(t),nLo=r(awe," (Data2VecText model)"),awe.forEach(t),lLo=i(q),uv=s(q,"LI",{});var swe=n(uv);Hae=s(swe,"STRONG",{});var RHr=n(Hae);iLo=r(RHr,"deberta"),RHr.forEach(t),dLo=r(swe," \u2014 "),pD=s(swe,"A",{href:!0});var SHr=n(pD);cLo=r(SHr,"DebertaForTokenClassification"),SHr.forEach(t),mLo=r(swe," (DeBERTa model)"),swe.forEach(t),fLo=i(q),pv=s(q,"LI",{});var nwe=n(pv);Uae=s(nwe,"STRONG",{});var PHr=n(Uae);gLo=r(PHr,"deberta-v2"),PHr.forEach(t),hLo=r(nwe," \u2014 "),_D=s(nwe,"A",{href:!0});var $Hr=n(_D);uLo=r($Hr,"DebertaV2ForTokenClassification"),$Hr.forEach(t),pLo=r(nwe," (DeBERTa-v2 model)"),nwe.forEach(t),_Lo=i(q),_v=s(q,"LI",{});var lwe=n(_v);Jae=s(lwe,"STRONG",{});var IHr=n(Jae);bLo=r(IHr,"distilbert"),IHr.forEach(t),vLo=r(lwe," \u2014 "),bD=s(lwe,"A",{href:!0});var DHr=n(bD);TLo=r(DHr,"DistilBertForTokenClassification"),DHr.forEach(t),FLo=r(lwe," (DistilBERT model)"),lwe.forEach(t),CLo=i(q),bv=s(q,"LI",{});var iwe=n(bv);Yae=s(iwe,"STRONG",{});var jHr=n(Yae);MLo=r(jHr,"electra"),jHr.forEach(t),ELo=r(iwe," \u2014 "),vD=s(iwe,"A",{href:!0});var NHr=n(vD);yLo=r(NHr,"ElectraForTokenClassification"),NHr.forEach(t),wLo=r(iwe," (ELECTRA model)"),iwe.forEach(t),ALo=i(q),vv=s(q,"LI",{});var dwe=n(vv);Kae=s(dwe,"STRONG",{});var qHr=n(Kae);LLo=r(qHr,"flaubert"),qHr.forEach(t),BLo=r(dwe," \u2014 "),TD=s(dwe,"A",{href:!0});var GHr=n(TD);xLo=r(GHr,"FlaubertForTokenClassification"),GHr.forEach(t),kLo=r(dwe," (FlauBERT model)"),dwe.forEach(t),RLo=i(q),Tv=s(q,"LI",{});var cwe=n(Tv);Zae=s(cwe,"STRONG",{});var OHr=n(Zae);SLo=r(OHr,"fnet"),OHr.forEach(t),PLo=r(cwe," \u2014 "),FD=s(cwe,"A",{href:!0});var XHr=n(FD);$Lo=r(XHr,"FNetForTokenClassification"),XHr.forEach(t),ILo=r(cwe," (FNet model)"),cwe.forEach(t),DLo=i(q),Fv=s(q,"LI",{});var mwe=n(Fv);ese=s(mwe,"STRONG",{});var VHr=n(ese);jLo=r(VHr,"funnel"),VHr.forEach(t),NLo=r(mwe," \u2014 "),CD=s(mwe,"A",{href:!0});var zHr=n(CD);qLo=r(zHr,"FunnelForTokenClassification"),zHr.forEach(t),GLo=r(mwe," (Funnel Transformer model)"),mwe.forEach(t),OLo=i(q),Cv=s(q,"LI",{});var fwe=n(Cv);ose=s(fwe,"STRONG",{});var WHr=n(ose);XLo=r(WHr,"gpt2"),WHr.forEach(t),VLo=r(fwe," \u2014 "),MD=s(fwe,"A",{href:!0});var QHr=n(MD);zLo=r(QHr,"GPT2ForTokenClassification"),QHr.forEach(t),WLo=r(fwe," (OpenAI GPT-2 model)"),fwe.forEach(t),QLo=i(q),Mv=s(q,"LI",{});var gwe=n(Mv);rse=s(gwe,"STRONG",{});var HHr=n(rse);HLo=r(HHr,"ibert"),HHr.forEach(t),ULo=r(gwe," \u2014 "),ED=s(gwe,"A",{href:!0});var UHr=n(ED);JLo=r(UHr,"IBertForTokenClassification"),UHr.forEach(t),YLo=r(gwe," (I-BERT model)"),gwe.forEach(t),KLo=i(q),Ev=s(q,"LI",{});var hwe=n(Ev);tse=s(hwe,"STRONG",{});var JHr=n(tse);ZLo=r(JHr,"layoutlm"),JHr.forEach(t),e8o=r(hwe," \u2014 "),yD=s(hwe,"A",{href:!0});var YHr=n(yD);o8o=r(YHr,"LayoutLMForTokenClassification"),YHr.forEach(t),r8o=r(hwe," (LayoutLM model)"),hwe.forEach(t),t8o=i(q),yv=s(q,"LI",{});var uwe=n(yv);ase=s(uwe,"STRONG",{});var KHr=n(ase);a8o=r(KHr,"layoutlmv2"),KHr.forEach(t),s8o=r(uwe," \u2014 "),wD=s(uwe,"A",{href:!0});var ZHr=n(wD);n8o=r(ZHr,"LayoutLMv2ForTokenClassification"),ZHr.forEach(t),l8o=r(uwe," (LayoutLMv2 model)"),uwe.forEach(t),i8o=i(q),wv=s(q,"LI",{});var pwe=n(wv);sse=s(pwe,"STRONG",{});var eUr=n(sse);d8o=r(eUr,"longformer"),eUr.forEach(t),c8o=r(pwe," \u2014 "),AD=s(pwe,"A",{href:!0});var oUr=n(AD);m8o=r(oUr,"LongformerForTokenClassification"),oUr.forEach(t),f8o=r(pwe," (Longformer model)"),pwe.forEach(t),g8o=i(q),Av=s(q,"LI",{});var _we=n(Av);nse=s(_we,"STRONG",{});var rUr=n(nse);h8o=r(rUr,"megatron-bert"),rUr.forEach(t),u8o=r(_we," \u2014 "),LD=s(_we,"A",{href:!0});var tUr=n(LD);p8o=r(tUr,"MegatronBertForTokenClassification"),tUr.forEach(t),_8o=r(_we," (MegatronBert model)"),_we.forEach(t),b8o=i(q),Lv=s(q,"LI",{});var bwe=n(Lv);lse=s(bwe,"STRONG",{});var aUr=n(lse);v8o=r(aUr,"mobilebert"),aUr.forEach(t),T8o=r(bwe," \u2014 "),BD=s(bwe,"A",{href:!0});var sUr=n(BD);F8o=r(sUr,"MobileBertForTokenClassification"),sUr.forEach(t),C8o=r(bwe," (MobileBERT model)"),bwe.forEach(t),M8o=i(q),Bv=s(q,"LI",{});var vwe=n(Bv);ise=s(vwe,"STRONG",{});var nUr=n(ise);E8o=r(nUr,"mpnet"),nUr.forEach(t),y8o=r(vwe," \u2014 "),xD=s(vwe,"A",{href:!0});var lUr=n(xD);w8o=r(lUr,"MPNetForTokenClassification"),lUr.forEach(t),A8o=r(vwe," (MPNet model)"),vwe.forEach(t),L8o=i(q),xv=s(q,"LI",{});var Twe=n(xv);dse=s(Twe,"STRONG",{});var iUr=n(dse);B8o=r(iUr,"nystromformer"),iUr.forEach(t),x8o=r(Twe," \u2014 "),kD=s(Twe,"A",{href:!0});var dUr=n(kD);k8o=r(dUr,"NystromformerForTokenClassification"),dUr.forEach(t),R8o=r(Twe," (Nystromformer model)"),Twe.forEach(t),S8o=i(q),kv=s(q,"LI",{});var Fwe=n(kv);cse=s(Fwe,"STRONG",{});var cUr=n(cse);P8o=r(cUr,"qdqbert"),cUr.forEach(t),$8o=r(Fwe," \u2014 "),RD=s(Fwe,"A",{href:!0});var mUr=n(RD);I8o=r(mUr,"QDQBertForTokenClassification"),mUr.forEach(t),D8o=r(Fwe," (QDQBert model)"),Fwe.forEach(t),j8o=i(q),Rv=s(q,"LI",{});var Cwe=n(Rv);mse=s(Cwe,"STRONG",{});var fUr=n(mse);N8o=r(fUr,"rembert"),fUr.forEach(t),q8o=r(Cwe," \u2014 "),SD=s(Cwe,"A",{href:!0});var gUr=n(SD);G8o=r(gUr,"RemBertForTokenClassification"),gUr.forEach(t),O8o=r(Cwe," (RemBERT model)"),Cwe.forEach(t),X8o=i(q),Sv=s(q,"LI",{});var Mwe=n(Sv);fse=s(Mwe,"STRONG",{});var hUr=n(fse);V8o=r(hUr,"roberta"),hUr.forEach(t),z8o=r(Mwe," \u2014 "),PD=s(Mwe,"A",{href:!0});var uUr=n(PD);W8o=r(uUr,"RobertaForTokenClassification"),uUr.forEach(t),Q8o=r(Mwe," (RoBERTa model)"),Mwe.forEach(t),H8o=i(q),Pv=s(q,"LI",{});var Ewe=n(Pv);gse=s(Ewe,"STRONG",{});var pUr=n(gse);U8o=r(pUr,"roformer"),pUr.forEach(t),J8o=r(Ewe," \u2014 "),$D=s(Ewe,"A",{href:!0});var _Ur=n($D);Y8o=r(_Ur,"RoFormerForTokenClassification"),_Ur.forEach(t),K8o=r(Ewe," (RoFormer model)"),Ewe.forEach(t),Z8o=i(q),$v=s(q,"LI",{});var ywe=n($v);hse=s(ywe,"STRONG",{});var bUr=n(hse);e7o=r(bUr,"squeezebert"),bUr.forEach(t),o7o=r(ywe," \u2014 "),ID=s(ywe,"A",{href:!0});var vUr=n(ID);r7o=r(vUr,"SqueezeBertForTokenClassification"),vUr.forEach(t),t7o=r(ywe," (SqueezeBERT model)"),ywe.forEach(t),a7o=i(q),Iv=s(q,"LI",{});var wwe=n(Iv);use=s(wwe,"STRONG",{});var TUr=n(use);s7o=r(TUr,"xlm"),TUr.forEach(t),n7o=r(wwe," \u2014 "),DD=s(wwe,"A",{href:!0});var FUr=n(DD);l7o=r(FUr,"XLMForTokenClassification"),FUr.forEach(t),i7o=r(wwe," (XLM model)"),wwe.forEach(t),d7o=i(q),Dv=s(q,"LI",{});var Awe=n(Dv);pse=s(Awe,"STRONG",{});var CUr=n(pse);c7o=r(CUr,"xlm-roberta"),CUr.forEach(t),m7o=r(Awe," \u2014 "),jD=s(Awe,"A",{href:!0});var MUr=n(jD);f7o=r(MUr,"XLMRobertaForTokenClassification"),MUr.forEach(t),g7o=r(Awe," (XLM-RoBERTa model)"),Awe.forEach(t),h7o=i(q),jv=s(q,"LI",{});var Lwe=n(jv);_se=s(Lwe,"STRONG",{});var EUr=n(_se);u7o=r(EUr,"xlm-roberta-xl"),EUr.forEach(t),p7o=r(Lwe," \u2014 "),ND=s(Lwe,"A",{href:!0});var yUr=n(ND);_7o=r(yUr,"XLMRobertaXLForTokenClassification"),yUr.forEach(t),b7o=r(Lwe," (XLM-RoBERTa-XL model)"),Lwe.forEach(t),v7o=i(q),Nv=s(q,"LI",{});var Bwe=n(Nv);bse=s(Bwe,"STRONG",{});var wUr=n(bse);T7o=r(wUr,"xlnet"),wUr.forEach(t),F7o=r(Bwe," \u2014 "),qD=s(Bwe,"A",{href:!0});var AUr=n(qD);C7o=r(AUr,"XLNetForTokenClassification"),AUr.forEach(t),M7o=r(Bwe," (XLNet model)"),Bwe.forEach(t),E7o=i(q),qv=s(q,"LI",{});var xwe=n(qv);vse=s(xwe,"STRONG",{});var LUr=n(vse);y7o=r(LUr,"yoso"),LUr.forEach(t),w7o=r(xwe," \u2014 "),GD=s(xwe,"A",{href:!0});var BUr=n(GD);A7o=r(BUr,"YosoForTokenClassification"),BUr.forEach(t),L7o=r(xwe," (YOSO model)"),xwe.forEach(t),q.forEach(t),B7o=i(Xt),Gv=s(Xt,"P",{});var kwe=n(Gv);x7o=r(kwe,"The model is set in evaluation mode by default using "),Tse=s(kwe,"CODE",{});var xUr=n(Tse);k7o=r(xUr,"model.eval()"),xUr.forEach(t),R7o=r(kwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fse=s(kwe,"CODE",{});var kUr=n(Fse);S7o=r(kUr,"model.train()"),kUr.forEach(t),kwe.forEach(t),P7o=i(Xt),Cse=s(Xt,"P",{});var RUr=n(Cse);$7o=r(RUr,"Examples:"),RUr.forEach(t),I7o=i(Xt),f(mw.$$.fragment,Xt),Xt.forEach(t),al.forEach(t),lBe=i(c),pd=s(c,"H2",{class:!0});var hke=n(pd);Ov=s(hke,"A",{id:!0,class:!0,href:!0});var SUr=n(Ov);Mse=s(SUr,"SPAN",{});var PUr=n(Mse);f(fw.$$.fragment,PUr),PUr.forEach(t),SUr.forEach(t),D7o=i(hke),Ese=s(hke,"SPAN",{});var $Ur=n(Ese);j7o=r($Ur,"AutoModelForQuestionAnswering"),$Ur.forEach(t),hke.forEach(t),iBe=i(c),rr=s(c,"DIV",{class:!0});var nl=n(rr);f(gw.$$.fragment,nl),N7o=i(nl),_d=s(nl,"P",{});var oz=n(_d);q7o=r(oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),yse=s(oz,"CODE",{});var IUr=n(yse);G7o=r(IUr,"from_pretrained()"),IUr.forEach(t),O7o=r(oz,"class method or the "),wse=s(oz,"CODE",{});var DUr=n(wse);X7o=r(DUr,"from_config()"),DUr.forEach(t),V7o=r(oz,`class
method.`),oz.forEach(t),z7o=i(nl),hw=s(nl,"P",{});var uke=n(hw);W7o=r(uke,"This class cannot be instantiated directly using "),Ase=s(uke,"CODE",{});var jUr=n(Ase);Q7o=r(jUr,"__init__()"),jUr.forEach(t),H7o=r(uke," (throws an error)."),uke.forEach(t),U7o=i(nl),Ur=s(nl,"DIV",{class:!0});var ll=n(Ur);f(uw.$$.fragment,ll),J7o=i(ll),Lse=s(ll,"P",{});var NUr=n(Lse);Y7o=r(NUr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),NUr.forEach(t),K7o=i(ll),bd=s(ll,"P",{});var rz=n(bd);Z7o=r(rz,`Note:
Loading a model from its configuration file does `),Bse=s(rz,"STRONG",{});var qUr=n(Bse);e9o=r(qUr,"not"),qUr.forEach(t),o9o=r(rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xse=s(rz,"CODE",{});var GUr=n(xse);r9o=r(GUr,"from_pretrained()"),GUr.forEach(t),t9o=r(rz,"to load the model weights."),rz.forEach(t),a9o=i(ll),kse=s(ll,"P",{});var OUr=n(kse);s9o=r(OUr,"Examples:"),OUr.forEach(t),n9o=i(ll),f(pw.$$.fragment,ll),ll.forEach(t),l9o=i(nl),Ge=s(nl,"DIV",{class:!0});var Vt=n(Ge);f(_w.$$.fragment,Vt),i9o=i(Vt),Rse=s(Vt,"P",{});var XUr=n(Rse);d9o=r(XUr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),XUr.forEach(t),c9o=i(Vt),Ha=s(Vt,"P",{});var I3=n(Ha);m9o=r(I3,"The model class to instantiate is selected based on the "),Sse=s(I3,"CODE",{});var VUr=n(Sse);f9o=r(VUr,"model_type"),VUr.forEach(t),g9o=r(I3,` property of the config object (either
passed as an argument or loaded from `),Pse=s(I3,"CODE",{});var zUr=n(Pse);h9o=r(zUr,"pretrained_model_name_or_path"),zUr.forEach(t),u9o=r(I3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$se=s(I3,"CODE",{});var WUr=n($se);p9o=r(WUr,"pretrained_model_name_or_path"),WUr.forEach(t),_9o=r(I3,":"),I3.forEach(t),b9o=i(Vt),R=s(Vt,"UL",{});var P=n(R);Xv=s(P,"LI",{});var Rwe=n(Xv);Ise=s(Rwe,"STRONG",{});var QUr=n(Ise);v9o=r(QUr,"albert"),QUr.forEach(t),T9o=r(Rwe," \u2014 "),OD=s(Rwe,"A",{href:!0});var HUr=n(OD);F9o=r(HUr,"AlbertForQuestionAnswering"),HUr.forEach(t),C9o=r(Rwe," (ALBERT model)"),Rwe.forEach(t),M9o=i(P),Vv=s(P,"LI",{});var Swe=n(Vv);Dse=s(Swe,"STRONG",{});var UUr=n(Dse);E9o=r(UUr,"bart"),UUr.forEach(t),y9o=r(Swe," \u2014 "),XD=s(Swe,"A",{href:!0});var JUr=n(XD);w9o=r(JUr,"BartForQuestionAnswering"),JUr.forEach(t),A9o=r(Swe," (BART model)"),Swe.forEach(t),L9o=i(P),zv=s(P,"LI",{});var Pwe=n(zv);jse=s(Pwe,"STRONG",{});var YUr=n(jse);B9o=r(YUr,"bert"),YUr.forEach(t),x9o=r(Pwe," \u2014 "),VD=s(Pwe,"A",{href:!0});var KUr=n(VD);k9o=r(KUr,"BertForQuestionAnswering"),KUr.forEach(t),R9o=r(Pwe," (BERT model)"),Pwe.forEach(t),S9o=i(P),Wv=s(P,"LI",{});var $we=n(Wv);Nse=s($we,"STRONG",{});var ZUr=n(Nse);P9o=r(ZUr,"big_bird"),ZUr.forEach(t),$9o=r($we," \u2014 "),zD=s($we,"A",{href:!0});var eJr=n(zD);I9o=r(eJr,"BigBirdForQuestionAnswering"),eJr.forEach(t),D9o=r($we," (BigBird model)"),$we.forEach(t),j9o=i(P),Qv=s(P,"LI",{});var Iwe=n(Qv);qse=s(Iwe,"STRONG",{});var oJr=n(qse);N9o=r(oJr,"bigbird_pegasus"),oJr.forEach(t),q9o=r(Iwe," \u2014 "),WD=s(Iwe,"A",{href:!0});var rJr=n(WD);G9o=r(rJr,"BigBirdPegasusForQuestionAnswering"),rJr.forEach(t),O9o=r(Iwe," (BigBirdPegasus model)"),Iwe.forEach(t),X9o=i(P),Hv=s(P,"LI",{});var Dwe=n(Hv);Gse=s(Dwe,"STRONG",{});var tJr=n(Gse);V9o=r(tJr,"camembert"),tJr.forEach(t),z9o=r(Dwe," \u2014 "),QD=s(Dwe,"A",{href:!0});var aJr=n(QD);W9o=r(aJr,"CamembertForQuestionAnswering"),aJr.forEach(t),Q9o=r(Dwe," (CamemBERT model)"),Dwe.forEach(t),H9o=i(P),Uv=s(P,"LI",{});var jwe=n(Uv);Ose=s(jwe,"STRONG",{});var sJr=n(Ose);U9o=r(sJr,"canine"),sJr.forEach(t),J9o=r(jwe," \u2014 "),HD=s(jwe,"A",{href:!0});var nJr=n(HD);Y9o=r(nJr,"CanineForQuestionAnswering"),nJr.forEach(t),K9o=r(jwe," (Canine model)"),jwe.forEach(t),Z9o=i(P),Jv=s(P,"LI",{});var Nwe=n(Jv);Xse=s(Nwe,"STRONG",{});var lJr=n(Xse);eBo=r(lJr,"convbert"),lJr.forEach(t),oBo=r(Nwe," \u2014 "),UD=s(Nwe,"A",{href:!0});var iJr=n(UD);rBo=r(iJr,"ConvBertForQuestionAnswering"),iJr.forEach(t),tBo=r(Nwe," (ConvBERT model)"),Nwe.forEach(t),aBo=i(P),Yv=s(P,"LI",{});var qwe=n(Yv);Vse=s(qwe,"STRONG",{});var dJr=n(Vse);sBo=r(dJr,"data2vec-text"),dJr.forEach(t),nBo=r(qwe," \u2014 "),JD=s(qwe,"A",{href:!0});var cJr=n(JD);lBo=r(cJr,"Data2VecTextForQuestionAnswering"),cJr.forEach(t),iBo=r(qwe," (Data2VecText model)"),qwe.forEach(t),dBo=i(P),Kv=s(P,"LI",{});var Gwe=n(Kv);zse=s(Gwe,"STRONG",{});var mJr=n(zse);cBo=r(mJr,"deberta"),mJr.forEach(t),mBo=r(Gwe," \u2014 "),YD=s(Gwe,"A",{href:!0});var fJr=n(YD);fBo=r(fJr,"DebertaForQuestionAnswering"),fJr.forEach(t),gBo=r(Gwe," (DeBERTa model)"),Gwe.forEach(t),hBo=i(P),Zv=s(P,"LI",{});var Owe=n(Zv);Wse=s(Owe,"STRONG",{});var gJr=n(Wse);uBo=r(gJr,"deberta-v2"),gJr.forEach(t),pBo=r(Owe," \u2014 "),KD=s(Owe,"A",{href:!0});var hJr=n(KD);_Bo=r(hJr,"DebertaV2ForQuestionAnswering"),hJr.forEach(t),bBo=r(Owe," (DeBERTa-v2 model)"),Owe.forEach(t),vBo=i(P),eT=s(P,"LI",{});var Xwe=n(eT);Qse=s(Xwe,"STRONG",{});var uJr=n(Qse);TBo=r(uJr,"distilbert"),uJr.forEach(t),FBo=r(Xwe," \u2014 "),ZD=s(Xwe,"A",{href:!0});var pJr=n(ZD);CBo=r(pJr,"DistilBertForQuestionAnswering"),pJr.forEach(t),MBo=r(Xwe," (DistilBERT model)"),Xwe.forEach(t),EBo=i(P),oT=s(P,"LI",{});var Vwe=n(oT);Hse=s(Vwe,"STRONG",{});var _Jr=n(Hse);yBo=r(_Jr,"electra"),_Jr.forEach(t),wBo=r(Vwe," \u2014 "),ej=s(Vwe,"A",{href:!0});var bJr=n(ej);ABo=r(bJr,"ElectraForQuestionAnswering"),bJr.forEach(t),LBo=r(Vwe," (ELECTRA model)"),Vwe.forEach(t),BBo=i(P),rT=s(P,"LI",{});var zwe=n(rT);Use=s(zwe,"STRONG",{});var vJr=n(Use);xBo=r(vJr,"flaubert"),vJr.forEach(t),kBo=r(zwe," \u2014 "),oj=s(zwe,"A",{href:!0});var TJr=n(oj);RBo=r(TJr,"FlaubertForQuestionAnsweringSimple"),TJr.forEach(t),SBo=r(zwe," (FlauBERT model)"),zwe.forEach(t),PBo=i(P),tT=s(P,"LI",{});var Wwe=n(tT);Jse=s(Wwe,"STRONG",{});var FJr=n(Jse);$Bo=r(FJr,"fnet"),FJr.forEach(t),IBo=r(Wwe," \u2014 "),rj=s(Wwe,"A",{href:!0});var CJr=n(rj);DBo=r(CJr,"FNetForQuestionAnswering"),CJr.forEach(t),jBo=r(Wwe," (FNet model)"),Wwe.forEach(t),NBo=i(P),aT=s(P,"LI",{});var Qwe=n(aT);Yse=s(Qwe,"STRONG",{});var MJr=n(Yse);qBo=r(MJr,"funnel"),MJr.forEach(t),GBo=r(Qwe," \u2014 "),tj=s(Qwe,"A",{href:!0});var EJr=n(tj);OBo=r(EJr,"FunnelForQuestionAnswering"),EJr.forEach(t),XBo=r(Qwe," (Funnel Transformer model)"),Qwe.forEach(t),VBo=i(P),sT=s(P,"LI",{});var Hwe=n(sT);Kse=s(Hwe,"STRONG",{});var yJr=n(Kse);zBo=r(yJr,"gptj"),yJr.forEach(t),WBo=r(Hwe," \u2014 "),aj=s(Hwe,"A",{href:!0});var wJr=n(aj);QBo=r(wJr,"GPTJForQuestionAnswering"),wJr.forEach(t),HBo=r(Hwe," (GPT-J model)"),Hwe.forEach(t),UBo=i(P),nT=s(P,"LI",{});var Uwe=n(nT);Zse=s(Uwe,"STRONG",{});var AJr=n(Zse);JBo=r(AJr,"ibert"),AJr.forEach(t),YBo=r(Uwe," \u2014 "),sj=s(Uwe,"A",{href:!0});var LJr=n(sj);KBo=r(LJr,"IBertForQuestionAnswering"),LJr.forEach(t),ZBo=r(Uwe," (I-BERT model)"),Uwe.forEach(t),exo=i(P),lT=s(P,"LI",{});var Jwe=n(lT);ene=s(Jwe,"STRONG",{});var BJr=n(ene);oxo=r(BJr,"layoutlmv2"),BJr.forEach(t),rxo=r(Jwe," \u2014 "),nj=s(Jwe,"A",{href:!0});var xJr=n(nj);txo=r(xJr,"LayoutLMv2ForQuestionAnswering"),xJr.forEach(t),axo=r(Jwe," (LayoutLMv2 model)"),Jwe.forEach(t),sxo=i(P),iT=s(P,"LI",{});var Ywe=n(iT);one=s(Ywe,"STRONG",{});var kJr=n(one);nxo=r(kJr,"led"),kJr.forEach(t),lxo=r(Ywe," \u2014 "),lj=s(Ywe,"A",{href:!0});var RJr=n(lj);ixo=r(RJr,"LEDForQuestionAnswering"),RJr.forEach(t),dxo=r(Ywe," (LED model)"),Ywe.forEach(t),cxo=i(P),dT=s(P,"LI",{});var Kwe=n(dT);rne=s(Kwe,"STRONG",{});var SJr=n(rne);mxo=r(SJr,"longformer"),SJr.forEach(t),fxo=r(Kwe," \u2014 "),ij=s(Kwe,"A",{href:!0});var PJr=n(ij);gxo=r(PJr,"LongformerForQuestionAnswering"),PJr.forEach(t),hxo=r(Kwe," (Longformer model)"),Kwe.forEach(t),uxo=i(P),cT=s(P,"LI",{});var Zwe=n(cT);tne=s(Zwe,"STRONG",{});var $Jr=n(tne);pxo=r($Jr,"lxmert"),$Jr.forEach(t),_xo=r(Zwe," \u2014 "),dj=s(Zwe,"A",{href:!0});var IJr=n(dj);bxo=r(IJr,"LxmertForQuestionAnswering"),IJr.forEach(t),vxo=r(Zwe," (LXMERT model)"),Zwe.forEach(t),Txo=i(P),mT=s(P,"LI",{});var e6e=n(mT);ane=s(e6e,"STRONG",{});var DJr=n(ane);Fxo=r(DJr,"mbart"),DJr.forEach(t),Cxo=r(e6e," \u2014 "),cj=s(e6e,"A",{href:!0});var jJr=n(cj);Mxo=r(jJr,"MBartForQuestionAnswering"),jJr.forEach(t),Exo=r(e6e," (mBART model)"),e6e.forEach(t),yxo=i(P),fT=s(P,"LI",{});var o6e=n(fT);sne=s(o6e,"STRONG",{});var NJr=n(sne);wxo=r(NJr,"megatron-bert"),NJr.forEach(t),Axo=r(o6e," \u2014 "),mj=s(o6e,"A",{href:!0});var qJr=n(mj);Lxo=r(qJr,"MegatronBertForQuestionAnswering"),qJr.forEach(t),Bxo=r(o6e," (MegatronBert model)"),o6e.forEach(t),xxo=i(P),gT=s(P,"LI",{});var r6e=n(gT);nne=s(r6e,"STRONG",{});var GJr=n(nne);kxo=r(GJr,"mobilebert"),GJr.forEach(t),Rxo=r(r6e," \u2014 "),fj=s(r6e,"A",{href:!0});var OJr=n(fj);Sxo=r(OJr,"MobileBertForQuestionAnswering"),OJr.forEach(t),Pxo=r(r6e," (MobileBERT model)"),r6e.forEach(t),$xo=i(P),hT=s(P,"LI",{});var t6e=n(hT);lne=s(t6e,"STRONG",{});var XJr=n(lne);Ixo=r(XJr,"mpnet"),XJr.forEach(t),Dxo=r(t6e," \u2014 "),gj=s(t6e,"A",{href:!0});var VJr=n(gj);jxo=r(VJr,"MPNetForQuestionAnswering"),VJr.forEach(t),Nxo=r(t6e," (MPNet model)"),t6e.forEach(t),qxo=i(P),uT=s(P,"LI",{});var a6e=n(uT);ine=s(a6e,"STRONG",{});var zJr=n(ine);Gxo=r(zJr,"nystromformer"),zJr.forEach(t),Oxo=r(a6e," \u2014 "),hj=s(a6e,"A",{href:!0});var WJr=n(hj);Xxo=r(WJr,"NystromformerForQuestionAnswering"),WJr.forEach(t),Vxo=r(a6e," (Nystromformer model)"),a6e.forEach(t),zxo=i(P),pT=s(P,"LI",{});var s6e=n(pT);dne=s(s6e,"STRONG",{});var QJr=n(dne);Wxo=r(QJr,"qdqbert"),QJr.forEach(t),Qxo=r(s6e," \u2014 "),uj=s(s6e,"A",{href:!0});var HJr=n(uj);Hxo=r(HJr,"QDQBertForQuestionAnswering"),HJr.forEach(t),Uxo=r(s6e," (QDQBert model)"),s6e.forEach(t),Jxo=i(P),_T=s(P,"LI",{});var n6e=n(_T);cne=s(n6e,"STRONG",{});var UJr=n(cne);Yxo=r(UJr,"reformer"),UJr.forEach(t),Kxo=r(n6e," \u2014 "),pj=s(n6e,"A",{href:!0});var JJr=n(pj);Zxo=r(JJr,"ReformerForQuestionAnswering"),JJr.forEach(t),eko=r(n6e," (Reformer model)"),n6e.forEach(t),oko=i(P),bT=s(P,"LI",{});var l6e=n(bT);mne=s(l6e,"STRONG",{});var YJr=n(mne);rko=r(YJr,"rembert"),YJr.forEach(t),tko=r(l6e," \u2014 "),_j=s(l6e,"A",{href:!0});var KJr=n(_j);ako=r(KJr,"RemBertForQuestionAnswering"),KJr.forEach(t),sko=r(l6e," (RemBERT model)"),l6e.forEach(t),nko=i(P),vT=s(P,"LI",{});var i6e=n(vT);fne=s(i6e,"STRONG",{});var ZJr=n(fne);lko=r(ZJr,"roberta"),ZJr.forEach(t),iko=r(i6e," \u2014 "),bj=s(i6e,"A",{href:!0});var eYr=n(bj);dko=r(eYr,"RobertaForQuestionAnswering"),eYr.forEach(t),cko=r(i6e," (RoBERTa model)"),i6e.forEach(t),mko=i(P),TT=s(P,"LI",{});var d6e=n(TT);gne=s(d6e,"STRONG",{});var oYr=n(gne);fko=r(oYr,"roformer"),oYr.forEach(t),gko=r(d6e," \u2014 "),vj=s(d6e,"A",{href:!0});var rYr=n(vj);hko=r(rYr,"RoFormerForQuestionAnswering"),rYr.forEach(t),uko=r(d6e," (RoFormer model)"),d6e.forEach(t),pko=i(P),FT=s(P,"LI",{});var c6e=n(FT);hne=s(c6e,"STRONG",{});var tYr=n(hne);_ko=r(tYr,"splinter"),tYr.forEach(t),bko=r(c6e," \u2014 "),Tj=s(c6e,"A",{href:!0});var aYr=n(Tj);vko=r(aYr,"SplinterForQuestionAnswering"),aYr.forEach(t),Tko=r(c6e," (Splinter model)"),c6e.forEach(t),Fko=i(P),CT=s(P,"LI",{});var m6e=n(CT);une=s(m6e,"STRONG",{});var sYr=n(une);Cko=r(sYr,"squeezebert"),sYr.forEach(t),Mko=r(m6e," \u2014 "),Fj=s(m6e,"A",{href:!0});var nYr=n(Fj);Eko=r(nYr,"SqueezeBertForQuestionAnswering"),nYr.forEach(t),yko=r(m6e," (SqueezeBERT model)"),m6e.forEach(t),wko=i(P),MT=s(P,"LI",{});var f6e=n(MT);pne=s(f6e,"STRONG",{});var lYr=n(pne);Ako=r(lYr,"xlm"),lYr.forEach(t),Lko=r(f6e," \u2014 "),Cj=s(f6e,"A",{href:!0});var iYr=n(Cj);Bko=r(iYr,"XLMForQuestionAnsweringSimple"),iYr.forEach(t),xko=r(f6e," (XLM model)"),f6e.forEach(t),kko=i(P),ET=s(P,"LI",{});var g6e=n(ET);_ne=s(g6e,"STRONG",{});var dYr=n(_ne);Rko=r(dYr,"xlm-roberta"),dYr.forEach(t),Sko=r(g6e," \u2014 "),Mj=s(g6e,"A",{href:!0});var cYr=n(Mj);Pko=r(cYr,"XLMRobertaForQuestionAnswering"),cYr.forEach(t),$ko=r(g6e," (XLM-RoBERTa model)"),g6e.forEach(t),Iko=i(P),yT=s(P,"LI",{});var h6e=n(yT);bne=s(h6e,"STRONG",{});var mYr=n(bne);Dko=r(mYr,"xlm-roberta-xl"),mYr.forEach(t),jko=r(h6e," \u2014 "),Ej=s(h6e,"A",{href:!0});var fYr=n(Ej);Nko=r(fYr,"XLMRobertaXLForQuestionAnswering"),fYr.forEach(t),qko=r(h6e," (XLM-RoBERTa-XL model)"),h6e.forEach(t),Gko=i(P),wT=s(P,"LI",{});var u6e=n(wT);vne=s(u6e,"STRONG",{});var gYr=n(vne);Oko=r(gYr,"xlnet"),gYr.forEach(t),Xko=r(u6e," \u2014 "),yj=s(u6e,"A",{href:!0});var hYr=n(yj);Vko=r(hYr,"XLNetForQuestionAnsweringSimple"),hYr.forEach(t),zko=r(u6e," (XLNet model)"),u6e.forEach(t),Wko=i(P),AT=s(P,"LI",{});var p6e=n(AT);Tne=s(p6e,"STRONG",{});var uYr=n(Tne);Qko=r(uYr,"yoso"),uYr.forEach(t),Hko=r(p6e," \u2014 "),wj=s(p6e,"A",{href:!0});var pYr=n(wj);Uko=r(pYr,"YosoForQuestionAnswering"),pYr.forEach(t),Jko=r(p6e," (YOSO model)"),p6e.forEach(t),P.forEach(t),Yko=i(Vt),LT=s(Vt,"P",{});var _6e=n(LT);Kko=r(_6e,"The model is set in evaluation mode by default using "),Fne=s(_6e,"CODE",{});var _Yr=n(Fne);Zko=r(_Yr,"model.eval()"),_Yr.forEach(t),eRo=r(_6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Cne=s(_6e,"CODE",{});var bYr=n(Cne);oRo=r(bYr,"model.train()"),bYr.forEach(t),_6e.forEach(t),rRo=i(Vt),Mne=s(Vt,"P",{});var vYr=n(Mne);tRo=r(vYr,"Examples:"),vYr.forEach(t),aRo=i(Vt),f(bw.$$.fragment,Vt),Vt.forEach(t),nl.forEach(t),dBe=i(c),vd=s(c,"H2",{class:!0});var pke=n(vd);BT=s(pke,"A",{id:!0,class:!0,href:!0});var TYr=n(BT);Ene=s(TYr,"SPAN",{});var FYr=n(Ene);f(vw.$$.fragment,FYr),FYr.forEach(t),TYr.forEach(t),sRo=i(pke),yne=s(pke,"SPAN",{});var CYr=n(yne);nRo=r(CYr,"AutoModelForTableQuestionAnswering"),CYr.forEach(t),pke.forEach(t),cBe=i(c),tr=s(c,"DIV",{class:!0});var il=n(tr);f(Tw.$$.fragment,il),lRo=i(il),Td=s(il,"P",{});var tz=n(Td);iRo=r(tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),wne=s(tz,"CODE",{});var MYr=n(wne);dRo=r(MYr,"from_pretrained()"),MYr.forEach(t),cRo=r(tz,"class method or the "),Ane=s(tz,"CODE",{});var EYr=n(Ane);mRo=r(EYr,"from_config()"),EYr.forEach(t),fRo=r(tz,`class
method.`),tz.forEach(t),gRo=i(il),Fw=s(il,"P",{});var _ke=n(Fw);hRo=r(_ke,"This class cannot be instantiated directly using "),Lne=s(_ke,"CODE",{});var yYr=n(Lne);uRo=r(yYr,"__init__()"),yYr.forEach(t),pRo=r(_ke," (throws an error)."),_ke.forEach(t),_Ro=i(il),Jr=s(il,"DIV",{class:!0});var dl=n(Jr);f(Cw.$$.fragment,dl),bRo=i(dl),Bne=s(dl,"P",{});var wYr=n(Bne);vRo=r(wYr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),wYr.forEach(t),TRo=i(dl),Fd=s(dl,"P",{});var az=n(Fd);FRo=r(az,`Note:
Loading a model from its configuration file does `),xne=s(az,"STRONG",{});var AYr=n(xne);CRo=r(AYr,"not"),AYr.forEach(t),MRo=r(az,` load the model weights. It only affects the
model\u2019s configuration. Use `),kne=s(az,"CODE",{});var LYr=n(kne);ERo=r(LYr,"from_pretrained()"),LYr.forEach(t),yRo=r(az,"to load the model weights."),az.forEach(t),wRo=i(dl),Rne=s(dl,"P",{});var BYr=n(Rne);ARo=r(BYr,"Examples:"),BYr.forEach(t),LRo=i(dl),f(Mw.$$.fragment,dl),dl.forEach(t),BRo=i(il),Oe=s(il,"DIV",{class:!0});var zt=n(Oe);f(Ew.$$.fragment,zt),xRo=i(zt),Sne=s(zt,"P",{});var xYr=n(Sne);kRo=r(xYr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),xYr.forEach(t),RRo=i(zt),Ua=s(zt,"P",{});var D3=n(Ua);SRo=r(D3,"The model class to instantiate is selected based on the "),Pne=s(D3,"CODE",{});var kYr=n(Pne);PRo=r(kYr,"model_type"),kYr.forEach(t),$Ro=r(D3,` property of the config object (either
passed as an argument or loaded from `),$ne=s(D3,"CODE",{});var RYr=n($ne);IRo=r(RYr,"pretrained_model_name_or_path"),RYr.forEach(t),DRo=r(D3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ine=s(D3,"CODE",{});var SYr=n(Ine);jRo=r(SYr,"pretrained_model_name_or_path"),SYr.forEach(t),NRo=r(D3,":"),D3.forEach(t),qRo=i(zt),Dne=s(zt,"UL",{});var PYr=n(Dne);xT=s(PYr,"LI",{});var b6e=n(xT);jne=s(b6e,"STRONG",{});var $Yr=n(jne);GRo=r($Yr,"tapas"),$Yr.forEach(t),ORo=r(b6e," \u2014 "),Aj=s(b6e,"A",{href:!0});var IYr=n(Aj);XRo=r(IYr,"TapasForQuestionAnswering"),IYr.forEach(t),VRo=r(b6e," (TAPAS model)"),b6e.forEach(t),PYr.forEach(t),zRo=i(zt),kT=s(zt,"P",{});var v6e=n(kT);WRo=r(v6e,"The model is set in evaluation mode by default using "),Nne=s(v6e,"CODE",{});var DYr=n(Nne);QRo=r(DYr,"model.eval()"),DYr.forEach(t),HRo=r(v6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),qne=s(v6e,"CODE",{});var jYr=n(qne);URo=r(jYr,"model.train()"),jYr.forEach(t),v6e.forEach(t),JRo=i(zt),Gne=s(zt,"P",{});var NYr=n(Gne);YRo=r(NYr,"Examples:"),NYr.forEach(t),KRo=i(zt),f(yw.$$.fragment,zt),zt.forEach(t),il.forEach(t),mBe=i(c),Cd=s(c,"H2",{class:!0});var bke=n(Cd);RT=s(bke,"A",{id:!0,class:!0,href:!0});var qYr=n(RT);One=s(qYr,"SPAN",{});var GYr=n(One);f(ww.$$.fragment,GYr),GYr.forEach(t),qYr.forEach(t),ZRo=i(bke),Xne=s(bke,"SPAN",{});var OYr=n(Xne);eSo=r(OYr,"AutoModelForImageClassification"),OYr.forEach(t),bke.forEach(t),fBe=i(c),ar=s(c,"DIV",{class:!0});var cl=n(ar);f(Aw.$$.fragment,cl),oSo=i(cl),Md=s(cl,"P",{});var sz=n(Md);rSo=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Vne=s(sz,"CODE",{});var XYr=n(Vne);tSo=r(XYr,"from_pretrained()"),XYr.forEach(t),aSo=r(sz,"class method or the "),zne=s(sz,"CODE",{});var VYr=n(zne);sSo=r(VYr,"from_config()"),VYr.forEach(t),nSo=r(sz,`class
method.`),sz.forEach(t),lSo=i(cl),Lw=s(cl,"P",{});var vke=n(Lw);iSo=r(vke,"This class cannot be instantiated directly using "),Wne=s(vke,"CODE",{});var zYr=n(Wne);dSo=r(zYr,"__init__()"),zYr.forEach(t),cSo=r(vke," (throws an error)."),vke.forEach(t),mSo=i(cl),Yr=s(cl,"DIV",{class:!0});var ml=n(Yr);f(Bw.$$.fragment,ml),fSo=i(ml),Qne=s(ml,"P",{});var WYr=n(Qne);gSo=r(WYr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),WYr.forEach(t),hSo=i(ml),Ed=s(ml,"P",{});var nz=n(Ed);uSo=r(nz,`Note:
Loading a model from its configuration file does `),Hne=s(nz,"STRONG",{});var QYr=n(Hne);pSo=r(QYr,"not"),QYr.forEach(t),_So=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Une=s(nz,"CODE",{});var HYr=n(Une);bSo=r(HYr,"from_pretrained()"),HYr.forEach(t),vSo=r(nz,"to load the model weights."),nz.forEach(t),TSo=i(ml),Jne=s(ml,"P",{});var UYr=n(Jne);FSo=r(UYr,"Examples:"),UYr.forEach(t),CSo=i(ml),f(xw.$$.fragment,ml),ml.forEach(t),MSo=i(cl),Xe=s(cl,"DIV",{class:!0});var Wt=n(Xe);f(kw.$$.fragment,Wt),ESo=i(Wt),Yne=s(Wt,"P",{});var JYr=n(Yne);ySo=r(JYr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),JYr.forEach(t),wSo=i(Wt),Ja=s(Wt,"P",{});var j3=n(Ja);ASo=r(j3,"The model class to instantiate is selected based on the "),Kne=s(j3,"CODE",{});var YYr=n(Kne);LSo=r(YYr,"model_type"),YYr.forEach(t),BSo=r(j3,` property of the config object (either
passed as an argument or loaded from `),Zne=s(j3,"CODE",{});var KYr=n(Zne);xSo=r(KYr,"pretrained_model_name_or_path"),KYr.forEach(t),kSo=r(j3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ele=s(j3,"CODE",{});var ZYr=n(ele);RSo=r(ZYr,"pretrained_model_name_or_path"),ZYr.forEach(t),SSo=r(j3,":"),j3.forEach(t),PSo=i(Wt),be=s(Wt,"UL",{});var eo=n(be);ST=s(eo,"LI",{});var T6e=n(ST);ole=s(T6e,"STRONG",{});var eKr=n(ole);$So=r(eKr,"beit"),eKr.forEach(t),ISo=r(T6e," \u2014 "),Lj=s(T6e,"A",{href:!0});var oKr=n(Lj);DSo=r(oKr,"BeitForImageClassification"),oKr.forEach(t),jSo=r(T6e," (BEiT model)"),T6e.forEach(t),NSo=i(eo),PT=s(eo,"LI",{});var F6e=n(PT);rle=s(F6e,"STRONG",{});var rKr=n(rle);qSo=r(rKr,"convnext"),rKr.forEach(t),GSo=r(F6e," \u2014 "),Bj=s(F6e,"A",{href:!0});var tKr=n(Bj);OSo=r(tKr,"ConvNextForImageClassification"),tKr.forEach(t),XSo=r(F6e," (ConvNext model)"),F6e.forEach(t),VSo=i(eo),$n=s(eo,"LI",{});var l7=n($n);tle=s(l7,"STRONG",{});var aKr=n(tle);zSo=r(aKr,"deit"),aKr.forEach(t),WSo=r(l7," \u2014 "),xj=s(l7,"A",{href:!0});var sKr=n(xj);QSo=r(sKr,"DeiTForImageClassification"),sKr.forEach(t),HSo=r(l7," or "),kj=s(l7,"A",{href:!0});var nKr=n(kj);USo=r(nKr,"DeiTForImageClassificationWithTeacher"),nKr.forEach(t),JSo=r(l7," (DeiT model)"),l7.forEach(t),YSo=i(eo),$T=s(eo,"LI",{});var C6e=n($T);ale=s(C6e,"STRONG",{});var lKr=n(ale);KSo=r(lKr,"imagegpt"),lKr.forEach(t),ZSo=r(C6e," \u2014 "),Rj=s(C6e,"A",{href:!0});var iKr=n(Rj);ePo=r(iKr,"ImageGPTForImageClassification"),iKr.forEach(t),oPo=r(C6e," (ImageGPT model)"),C6e.forEach(t),rPo=i(eo),la=s(eo,"LI",{});var Mm=n(la);sle=s(Mm,"STRONG",{});var dKr=n(sle);tPo=r(dKr,"perceiver"),dKr.forEach(t),aPo=r(Mm," \u2014 "),Sj=s(Mm,"A",{href:!0});var cKr=n(Sj);sPo=r(cKr,"PerceiverForImageClassificationLearned"),cKr.forEach(t),nPo=r(Mm," or "),Pj=s(Mm,"A",{href:!0});var mKr=n(Pj);lPo=r(mKr,"PerceiverForImageClassificationFourier"),mKr.forEach(t),iPo=r(Mm," or "),$j=s(Mm,"A",{href:!0});var fKr=n($j);dPo=r(fKr,"PerceiverForImageClassificationConvProcessing"),fKr.forEach(t),cPo=r(Mm," (Perceiver model)"),Mm.forEach(t),mPo=i(eo),IT=s(eo,"LI",{});var M6e=n(IT);nle=s(M6e,"STRONG",{});var gKr=n(nle);fPo=r(gKr,"poolformer"),gKr.forEach(t),gPo=r(M6e," \u2014 "),Ij=s(M6e,"A",{href:!0});var hKr=n(Ij);hPo=r(hKr,"PoolFormerForImageClassification"),hKr.forEach(t),uPo=r(M6e," (PoolFormer model)"),M6e.forEach(t),pPo=i(eo),DT=s(eo,"LI",{});var E6e=n(DT);lle=s(E6e,"STRONG",{});var uKr=n(lle);_Po=r(uKr,"segformer"),uKr.forEach(t),bPo=r(E6e," \u2014 "),Dj=s(E6e,"A",{href:!0});var pKr=n(Dj);vPo=r(pKr,"SegformerForImageClassification"),pKr.forEach(t),TPo=r(E6e," (SegFormer model)"),E6e.forEach(t),FPo=i(eo),jT=s(eo,"LI",{});var y6e=n(jT);ile=s(y6e,"STRONG",{});var _Kr=n(ile);CPo=r(_Kr,"swin"),_Kr.forEach(t),MPo=r(y6e," \u2014 "),jj=s(y6e,"A",{href:!0});var bKr=n(jj);EPo=r(bKr,"SwinForImageClassification"),bKr.forEach(t),yPo=r(y6e," (Swin model)"),y6e.forEach(t),wPo=i(eo),NT=s(eo,"LI",{});var w6e=n(NT);dle=s(w6e,"STRONG",{});var vKr=n(dle);APo=r(vKr,"vit"),vKr.forEach(t),LPo=r(w6e," \u2014 "),Nj=s(w6e,"A",{href:!0});var TKr=n(Nj);BPo=r(TKr,"ViTForImageClassification"),TKr.forEach(t),xPo=r(w6e," (ViT model)"),w6e.forEach(t),eo.forEach(t),kPo=i(Wt),qT=s(Wt,"P",{});var A6e=n(qT);RPo=r(A6e,"The model is set in evaluation mode by default using "),cle=s(A6e,"CODE",{});var FKr=n(cle);SPo=r(FKr,"model.eval()"),FKr.forEach(t),PPo=r(A6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mle=s(A6e,"CODE",{});var CKr=n(mle);$Po=r(CKr,"model.train()"),CKr.forEach(t),A6e.forEach(t),IPo=i(Wt),fle=s(Wt,"P",{});var MKr=n(fle);DPo=r(MKr,"Examples:"),MKr.forEach(t),jPo=i(Wt),f(Rw.$$.fragment,Wt),Wt.forEach(t),cl.forEach(t),gBe=i(c),yd=s(c,"H2",{class:!0});var Tke=n(yd);GT=s(Tke,"A",{id:!0,class:!0,href:!0});var EKr=n(GT);gle=s(EKr,"SPAN",{});var yKr=n(gle);f(Sw.$$.fragment,yKr),yKr.forEach(t),EKr.forEach(t),NPo=i(Tke),hle=s(Tke,"SPAN",{});var wKr=n(hle);qPo=r(wKr,"AutoModelForVision2Seq"),wKr.forEach(t),Tke.forEach(t),hBe=i(c),sr=s(c,"DIV",{class:!0});var fl=n(sr);f(Pw.$$.fragment,fl),GPo=i(fl),wd=s(fl,"P",{});var lz=n(wd);OPo=r(lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ule=s(lz,"CODE",{});var AKr=n(ule);XPo=r(AKr,"from_pretrained()"),AKr.forEach(t),VPo=r(lz,"class method or the "),ple=s(lz,"CODE",{});var LKr=n(ple);zPo=r(LKr,"from_config()"),LKr.forEach(t),WPo=r(lz,`class
method.`),lz.forEach(t),QPo=i(fl),$w=s(fl,"P",{});var Fke=n($w);HPo=r(Fke,"This class cannot be instantiated directly using "),_le=s(Fke,"CODE",{});var BKr=n(_le);UPo=r(BKr,"__init__()"),BKr.forEach(t),JPo=r(Fke," (throws an error)."),Fke.forEach(t),YPo=i(fl),Kr=s(fl,"DIV",{class:!0});var gl=n(Kr);f(Iw.$$.fragment,gl),KPo=i(gl),ble=s(gl,"P",{});var xKr=n(ble);ZPo=r(xKr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),xKr.forEach(t),e$o=i(gl),Ad=s(gl,"P",{});var iz=n(Ad);o$o=r(iz,`Note:
Loading a model from its configuration file does `),vle=s(iz,"STRONG",{});var kKr=n(vle);r$o=r(kKr,"not"),kKr.forEach(t),t$o=r(iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Tle=s(iz,"CODE",{});var RKr=n(Tle);a$o=r(RKr,"from_pretrained()"),RKr.forEach(t),s$o=r(iz,"to load the model weights."),iz.forEach(t),n$o=i(gl),Fle=s(gl,"P",{});var SKr=n(Fle);l$o=r(SKr,"Examples:"),SKr.forEach(t),i$o=i(gl),f(Dw.$$.fragment,gl),gl.forEach(t),d$o=i(fl),Ve=s(fl,"DIV",{class:!0});var Qt=n(Ve);f(jw.$$.fragment,Qt),c$o=i(Qt),Cle=s(Qt,"P",{});var PKr=n(Cle);m$o=r(PKr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),PKr.forEach(t),f$o=i(Qt),Ya=s(Qt,"P",{});var N3=n(Ya);g$o=r(N3,"The model class to instantiate is selected based on the "),Mle=s(N3,"CODE",{});var $Kr=n(Mle);h$o=r($Kr,"model_type"),$Kr.forEach(t),u$o=r(N3,` property of the config object (either
passed as an argument or loaded from `),Ele=s(N3,"CODE",{});var IKr=n(Ele);p$o=r(IKr,"pretrained_model_name_or_path"),IKr.forEach(t),_$o=r(N3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yle=s(N3,"CODE",{});var DKr=n(yle);b$o=r(DKr,"pretrained_model_name_or_path"),DKr.forEach(t),v$o=r(N3,":"),N3.forEach(t),T$o=i(Qt),wle=s(Qt,"UL",{});var jKr=n(wle);OT=s(jKr,"LI",{});var L6e=n(OT);Ale=s(L6e,"STRONG",{});var NKr=n(Ale);F$o=r(NKr,"vision-encoder-decoder"),NKr.forEach(t),C$o=r(L6e," \u2014 "),qj=s(L6e,"A",{href:!0});var qKr=n(qj);M$o=r(qKr,"VisionEncoderDecoderModel"),qKr.forEach(t),E$o=r(L6e," (Vision Encoder decoder model)"),L6e.forEach(t),jKr.forEach(t),y$o=i(Qt),XT=s(Qt,"P",{});var B6e=n(XT);w$o=r(B6e,"The model is set in evaluation mode by default using "),Lle=s(B6e,"CODE",{});var GKr=n(Lle);A$o=r(GKr,"model.eval()"),GKr.forEach(t),L$o=r(B6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ble=s(B6e,"CODE",{});var OKr=n(Ble);B$o=r(OKr,"model.train()"),OKr.forEach(t),B6e.forEach(t),x$o=i(Qt),xle=s(Qt,"P",{});var XKr=n(xle);k$o=r(XKr,"Examples:"),XKr.forEach(t),R$o=i(Qt),f(Nw.$$.fragment,Qt),Qt.forEach(t),fl.forEach(t),uBe=i(c),Ld=s(c,"H2",{class:!0});var Cke=n(Ld);VT=s(Cke,"A",{id:!0,class:!0,href:!0});var VKr=n(VT);kle=s(VKr,"SPAN",{});var zKr=n(kle);f(qw.$$.fragment,zKr),zKr.forEach(t),VKr.forEach(t),S$o=i(Cke),Rle=s(Cke,"SPAN",{});var WKr=n(Rle);P$o=r(WKr,"AutoModelForAudioClassification"),WKr.forEach(t),Cke.forEach(t),pBe=i(c),nr=s(c,"DIV",{class:!0});var hl=n(nr);f(Gw.$$.fragment,hl),$$o=i(hl),Bd=s(hl,"P",{});var dz=n(Bd);I$o=r(dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Sle=s(dz,"CODE",{});var QKr=n(Sle);D$o=r(QKr,"from_pretrained()"),QKr.forEach(t),j$o=r(dz,"class method or the "),Ple=s(dz,"CODE",{});var HKr=n(Ple);N$o=r(HKr,"from_config()"),HKr.forEach(t),q$o=r(dz,`class
method.`),dz.forEach(t),G$o=i(hl),Ow=s(hl,"P",{});var Mke=n(Ow);O$o=r(Mke,"This class cannot be instantiated directly using "),$le=s(Mke,"CODE",{});var UKr=n($le);X$o=r(UKr,"__init__()"),UKr.forEach(t),V$o=r(Mke," (throws an error)."),Mke.forEach(t),z$o=i(hl),Zr=s(hl,"DIV",{class:!0});var ul=n(Zr);f(Xw.$$.fragment,ul),W$o=i(ul),Ile=s(ul,"P",{});var JKr=n(Ile);Q$o=r(JKr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),JKr.forEach(t),H$o=i(ul),xd=s(ul,"P",{});var cz=n(xd);U$o=r(cz,`Note:
Loading a model from its configuration file does `),Dle=s(cz,"STRONG",{});var YKr=n(Dle);J$o=r(YKr,"not"),YKr.forEach(t),Y$o=r(cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),jle=s(cz,"CODE",{});var KKr=n(jle);K$o=r(KKr,"from_pretrained()"),KKr.forEach(t),Z$o=r(cz,"to load the model weights."),cz.forEach(t),eIo=i(ul),Nle=s(ul,"P",{});var ZKr=n(Nle);oIo=r(ZKr,"Examples:"),ZKr.forEach(t),rIo=i(ul),f(Vw.$$.fragment,ul),ul.forEach(t),tIo=i(hl),ze=s(hl,"DIV",{class:!0});var Ht=n(ze);f(zw.$$.fragment,Ht),aIo=i(Ht),qle=s(Ht,"P",{});var eZr=n(qle);sIo=r(eZr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),eZr.forEach(t),nIo=i(Ht),Ka=s(Ht,"P",{});var q3=n(Ka);lIo=r(q3,"The model class to instantiate is selected based on the "),Gle=s(q3,"CODE",{});var oZr=n(Gle);iIo=r(oZr,"model_type"),oZr.forEach(t),dIo=r(q3,` property of the config object (either
passed as an argument or loaded from `),Ole=s(q3,"CODE",{});var rZr=n(Ole);cIo=r(rZr,"pretrained_model_name_or_path"),rZr.forEach(t),mIo=r(q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xle=s(q3,"CODE",{});var tZr=n(Xle);fIo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),gIo=r(q3,":"),q3.forEach(t),hIo=i(Ht),Ae=s(Ht,"UL",{});var No=n(Ae);zT=s(No,"LI",{});var x6e=n(zT);Vle=s(x6e,"STRONG",{});var aZr=n(Vle);uIo=r(aZr,"data2vec-audio"),aZr.forEach(t),pIo=r(x6e," \u2014 "),Gj=s(x6e,"A",{href:!0});var sZr=n(Gj);_Io=r(sZr,"Data2VecAudioForSequenceClassification"),sZr.forEach(t),bIo=r(x6e," (Data2VecAudio model)"),x6e.forEach(t),vIo=i(No),WT=s(No,"LI",{});var k6e=n(WT);zle=s(k6e,"STRONG",{});var nZr=n(zle);TIo=r(nZr,"hubert"),nZr.forEach(t),FIo=r(k6e," \u2014 "),Oj=s(k6e,"A",{href:!0});var lZr=n(Oj);CIo=r(lZr,"HubertForSequenceClassification"),lZr.forEach(t),MIo=r(k6e," (Hubert model)"),k6e.forEach(t),EIo=i(No),QT=s(No,"LI",{});var R6e=n(QT);Wle=s(R6e,"STRONG",{});var iZr=n(Wle);yIo=r(iZr,"sew"),iZr.forEach(t),wIo=r(R6e," \u2014 "),Xj=s(R6e,"A",{href:!0});var dZr=n(Xj);AIo=r(dZr,"SEWForSequenceClassification"),dZr.forEach(t),LIo=r(R6e," (SEW model)"),R6e.forEach(t),BIo=i(No),HT=s(No,"LI",{});var S6e=n(HT);Qle=s(S6e,"STRONG",{});var cZr=n(Qle);xIo=r(cZr,"sew-d"),cZr.forEach(t),kIo=r(S6e," \u2014 "),Vj=s(S6e,"A",{href:!0});var mZr=n(Vj);RIo=r(mZr,"SEWDForSequenceClassification"),mZr.forEach(t),SIo=r(S6e," (SEW-D model)"),S6e.forEach(t),PIo=i(No),UT=s(No,"LI",{});var P6e=n(UT);Hle=s(P6e,"STRONG",{});var fZr=n(Hle);$Io=r(fZr,"unispeech"),fZr.forEach(t),IIo=r(P6e," \u2014 "),zj=s(P6e,"A",{href:!0});var gZr=n(zj);DIo=r(gZr,"UniSpeechForSequenceClassification"),gZr.forEach(t),jIo=r(P6e," (UniSpeech model)"),P6e.forEach(t),NIo=i(No),JT=s(No,"LI",{});var $6e=n(JT);Ule=s($6e,"STRONG",{});var hZr=n(Ule);qIo=r(hZr,"unispeech-sat"),hZr.forEach(t),GIo=r($6e," \u2014 "),Wj=s($6e,"A",{href:!0});var uZr=n(Wj);OIo=r(uZr,"UniSpeechSatForSequenceClassification"),uZr.forEach(t),XIo=r($6e," (UniSpeechSat model)"),$6e.forEach(t),VIo=i(No),YT=s(No,"LI",{});var I6e=n(YT);Jle=s(I6e,"STRONG",{});var pZr=n(Jle);zIo=r(pZr,"wav2vec2"),pZr.forEach(t),WIo=r(I6e," \u2014 "),Qj=s(I6e,"A",{href:!0});var _Zr=n(Qj);QIo=r(_Zr,"Wav2Vec2ForSequenceClassification"),_Zr.forEach(t),HIo=r(I6e," (Wav2Vec2 model)"),I6e.forEach(t),UIo=i(No),KT=s(No,"LI",{});var D6e=n(KT);Yle=s(D6e,"STRONG",{});var bZr=n(Yle);JIo=r(bZr,"wavlm"),bZr.forEach(t),YIo=r(D6e," \u2014 "),Hj=s(D6e,"A",{href:!0});var vZr=n(Hj);KIo=r(vZr,"WavLMForSequenceClassification"),vZr.forEach(t),ZIo=r(D6e," (WavLM model)"),D6e.forEach(t),No.forEach(t),eDo=i(Ht),ZT=s(Ht,"P",{});var j6e=n(ZT);oDo=r(j6e,"The model is set in evaluation mode by default using "),Kle=s(j6e,"CODE",{});var TZr=n(Kle);rDo=r(TZr,"model.eval()"),TZr.forEach(t),tDo=r(j6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zle=s(j6e,"CODE",{});var FZr=n(Zle);aDo=r(FZr,"model.train()"),FZr.forEach(t),j6e.forEach(t),sDo=i(Ht),eie=s(Ht,"P",{});var CZr=n(eie);nDo=r(CZr,"Examples:"),CZr.forEach(t),lDo=i(Ht),f(Ww.$$.fragment,Ht),Ht.forEach(t),hl.forEach(t),_Be=i(c),kd=s(c,"H2",{class:!0});var Eke=n(kd);e1=s(Eke,"A",{id:!0,class:!0,href:!0});var MZr=n(e1);oie=s(MZr,"SPAN",{});var EZr=n(oie);f(Qw.$$.fragment,EZr),EZr.forEach(t),MZr.forEach(t),iDo=i(Eke),rie=s(Eke,"SPAN",{});var yZr=n(rie);dDo=r(yZr,"AutoModelForAudioFrameClassification"),yZr.forEach(t),Eke.forEach(t),bBe=i(c),lr=s(c,"DIV",{class:!0});var pl=n(lr);f(Hw.$$.fragment,pl),cDo=i(pl),Rd=s(pl,"P",{});var mz=n(Rd);mDo=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),tie=s(mz,"CODE",{});var wZr=n(tie);fDo=r(wZr,"from_pretrained()"),wZr.forEach(t),gDo=r(mz,"class method or the "),aie=s(mz,"CODE",{});var AZr=n(aie);hDo=r(AZr,"from_config()"),AZr.forEach(t),uDo=r(mz,`class
method.`),mz.forEach(t),pDo=i(pl),Uw=s(pl,"P",{});var yke=n(Uw);_Do=r(yke,"This class cannot be instantiated directly using "),sie=s(yke,"CODE",{});var LZr=n(sie);bDo=r(LZr,"__init__()"),LZr.forEach(t),vDo=r(yke," (throws an error)."),yke.forEach(t),TDo=i(pl),et=s(pl,"DIV",{class:!0});var _l=n(et);f(Jw.$$.fragment,_l),FDo=i(_l),nie=s(_l,"P",{});var BZr=n(nie);CDo=r(BZr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),BZr.forEach(t),MDo=i(_l),Sd=s(_l,"P",{});var fz=n(Sd);EDo=r(fz,`Note:
Loading a model from its configuration file does `),lie=s(fz,"STRONG",{});var xZr=n(lie);yDo=r(xZr,"not"),xZr.forEach(t),wDo=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),iie=s(fz,"CODE",{});var kZr=n(iie);ADo=r(kZr,"from_pretrained()"),kZr.forEach(t),LDo=r(fz,"to load the model weights."),fz.forEach(t),BDo=i(_l),die=s(_l,"P",{});var RZr=n(die);xDo=r(RZr,"Examples:"),RZr.forEach(t),kDo=i(_l),f(Yw.$$.fragment,_l),_l.forEach(t),RDo=i(pl),We=s(pl,"DIV",{class:!0});var Ut=n(We);f(Kw.$$.fragment,Ut),SDo=i(Ut),cie=s(Ut,"P",{});var SZr=n(cie);PDo=r(SZr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),SZr.forEach(t),$Do=i(Ut),Za=s(Ut,"P",{});var G3=n(Za);IDo=r(G3,"The model class to instantiate is selected based on the "),mie=s(G3,"CODE",{});var PZr=n(mie);DDo=r(PZr,"model_type"),PZr.forEach(t),jDo=r(G3,` property of the config object (either
passed as an argument or loaded from `),fie=s(G3,"CODE",{});var $Zr=n(fie);NDo=r($Zr,"pretrained_model_name_or_path"),$Zr.forEach(t),qDo=r(G3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gie=s(G3,"CODE",{});var IZr=n(gie);GDo=r(IZr,"pretrained_model_name_or_path"),IZr.forEach(t),ODo=r(G3,":"),G3.forEach(t),XDo=i(Ut),es=s(Ut,"UL",{});var O3=n(es);o1=s(O3,"LI",{});var N6e=n(o1);hie=s(N6e,"STRONG",{});var DZr=n(hie);VDo=r(DZr,"data2vec-audio"),DZr.forEach(t),zDo=r(N6e," \u2014 "),Uj=s(N6e,"A",{href:!0});var jZr=n(Uj);WDo=r(jZr,"Data2VecAudioForAudioFrameClassification"),jZr.forEach(t),QDo=r(N6e," (Data2VecAudio model)"),N6e.forEach(t),HDo=i(O3),r1=s(O3,"LI",{});var q6e=n(r1);uie=s(q6e,"STRONG",{});var NZr=n(uie);UDo=r(NZr,"unispeech-sat"),NZr.forEach(t),JDo=r(q6e," \u2014 "),Jj=s(q6e,"A",{href:!0});var qZr=n(Jj);YDo=r(qZr,"UniSpeechSatForAudioFrameClassification"),qZr.forEach(t),KDo=r(q6e," (UniSpeechSat model)"),q6e.forEach(t),ZDo=i(O3),t1=s(O3,"LI",{});var G6e=n(t1);pie=s(G6e,"STRONG",{});var GZr=n(pie);ejo=r(GZr,"wav2vec2"),GZr.forEach(t),ojo=r(G6e," \u2014 "),Yj=s(G6e,"A",{href:!0});var OZr=n(Yj);rjo=r(OZr,"Wav2Vec2ForAudioFrameClassification"),OZr.forEach(t),tjo=r(G6e," (Wav2Vec2 model)"),G6e.forEach(t),ajo=i(O3),a1=s(O3,"LI",{});var O6e=n(a1);_ie=s(O6e,"STRONG",{});var XZr=n(_ie);sjo=r(XZr,"wavlm"),XZr.forEach(t),njo=r(O6e," \u2014 "),Kj=s(O6e,"A",{href:!0});var VZr=n(Kj);ljo=r(VZr,"WavLMForAudioFrameClassification"),VZr.forEach(t),ijo=r(O6e," (WavLM model)"),O6e.forEach(t),O3.forEach(t),djo=i(Ut),s1=s(Ut,"P",{});var X6e=n(s1);cjo=r(X6e,"The model is set in evaluation mode by default using "),bie=s(X6e,"CODE",{});var zZr=n(bie);mjo=r(zZr,"model.eval()"),zZr.forEach(t),fjo=r(X6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vie=s(X6e,"CODE",{});var WZr=n(vie);gjo=r(WZr,"model.train()"),WZr.forEach(t),X6e.forEach(t),hjo=i(Ut),Tie=s(Ut,"P",{});var QZr=n(Tie);ujo=r(QZr,"Examples:"),QZr.forEach(t),pjo=i(Ut),f(Zw.$$.fragment,Ut),Ut.forEach(t),pl.forEach(t),vBe=i(c),Pd=s(c,"H2",{class:!0});var wke=n(Pd);n1=s(wke,"A",{id:!0,class:!0,href:!0});var HZr=n(n1);Fie=s(HZr,"SPAN",{});var UZr=n(Fie);f(e6.$$.fragment,UZr),UZr.forEach(t),HZr.forEach(t),_jo=i(wke),Cie=s(wke,"SPAN",{});var JZr=n(Cie);bjo=r(JZr,"AutoModelForCTC"),JZr.forEach(t),wke.forEach(t),TBe=i(c),ir=s(c,"DIV",{class:!0});var bl=n(ir);f(o6.$$.fragment,bl),vjo=i(bl),$d=s(bl,"P",{});var gz=n($d);Tjo=r(gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Mie=s(gz,"CODE",{});var YZr=n(Mie);Fjo=r(YZr,"from_pretrained()"),YZr.forEach(t),Cjo=r(gz,"class method or the "),Eie=s(gz,"CODE",{});var KZr=n(Eie);Mjo=r(KZr,"from_config()"),KZr.forEach(t),Ejo=r(gz,`class
method.`),gz.forEach(t),yjo=i(bl),r6=s(bl,"P",{});var Ake=n(r6);wjo=r(Ake,"This class cannot be instantiated directly using "),yie=s(Ake,"CODE",{});var ZZr=n(yie);Ajo=r(ZZr,"__init__()"),ZZr.forEach(t),Ljo=r(Ake," (throws an error)."),Ake.forEach(t),Bjo=i(bl),ot=s(bl,"DIV",{class:!0});var vl=n(ot);f(t6.$$.fragment,vl),xjo=i(vl),wie=s(vl,"P",{});var eet=n(wie);kjo=r(eet,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),eet.forEach(t),Rjo=i(vl),Id=s(vl,"P",{});var hz=n(Id);Sjo=r(hz,`Note:
Loading a model from its configuration file does `),Aie=s(hz,"STRONG",{});var oet=n(Aie);Pjo=r(oet,"not"),oet.forEach(t),$jo=r(hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lie=s(hz,"CODE",{});var ret=n(Lie);Ijo=r(ret,"from_pretrained()"),ret.forEach(t),Djo=r(hz,"to load the model weights."),hz.forEach(t),jjo=i(vl),Bie=s(vl,"P",{});var tet=n(Bie);Njo=r(tet,"Examples:"),tet.forEach(t),qjo=i(vl),f(a6.$$.fragment,vl),vl.forEach(t),Gjo=i(bl),Qe=s(bl,"DIV",{class:!0});var Jt=n(Qe);f(s6.$$.fragment,Jt),Ojo=i(Jt),xie=s(Jt,"P",{});var aet=n(xie);Xjo=r(aet,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),aet.forEach(t),Vjo=i(Jt),os=s(Jt,"P",{});var X3=n(os);zjo=r(X3,"The model class to instantiate is selected based on the "),kie=s(X3,"CODE",{});var set=n(kie);Wjo=r(set,"model_type"),set.forEach(t),Qjo=r(X3,` property of the config object (either
passed as an argument or loaded from `),Rie=s(X3,"CODE",{});var net=n(Rie);Hjo=r(net,"pretrained_model_name_or_path"),net.forEach(t),Ujo=r(X3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sie=s(X3,"CODE",{});var iet=n(Sie);Jjo=r(iet,"pretrained_model_name_or_path"),iet.forEach(t),Yjo=r(X3,":"),X3.forEach(t),Kjo=i(Jt),Le=s(Jt,"UL",{});var qo=n(Le);l1=s(qo,"LI",{});var V6e=n(l1);Pie=s(V6e,"STRONG",{});var det=n(Pie);Zjo=r(det,"data2vec-audio"),det.forEach(t),eNo=r(V6e," \u2014 "),Zj=s(V6e,"A",{href:!0});var cet=n(Zj);oNo=r(cet,"Data2VecAudioForCTC"),cet.forEach(t),rNo=r(V6e," (Data2VecAudio model)"),V6e.forEach(t),tNo=i(qo),i1=s(qo,"LI",{});var z6e=n(i1);$ie=s(z6e,"STRONG",{});var met=n($ie);aNo=r(met,"hubert"),met.forEach(t),sNo=r(z6e," \u2014 "),eN=s(z6e,"A",{href:!0});var fet=n(eN);nNo=r(fet,"HubertForCTC"),fet.forEach(t),lNo=r(z6e," (Hubert model)"),z6e.forEach(t),iNo=i(qo),d1=s(qo,"LI",{});var W6e=n(d1);Iie=s(W6e,"STRONG",{});var get=n(Iie);dNo=r(get,"sew"),get.forEach(t),cNo=r(W6e," \u2014 "),oN=s(W6e,"A",{href:!0});var het=n(oN);mNo=r(het,"SEWForCTC"),het.forEach(t),fNo=r(W6e," (SEW model)"),W6e.forEach(t),gNo=i(qo),c1=s(qo,"LI",{});var Q6e=n(c1);Die=s(Q6e,"STRONG",{});var uet=n(Die);hNo=r(uet,"sew-d"),uet.forEach(t),uNo=r(Q6e," \u2014 "),rN=s(Q6e,"A",{href:!0});var pet=n(rN);pNo=r(pet,"SEWDForCTC"),pet.forEach(t),_No=r(Q6e," (SEW-D model)"),Q6e.forEach(t),bNo=i(qo),m1=s(qo,"LI",{});var H6e=n(m1);jie=s(H6e,"STRONG",{});var _et=n(jie);vNo=r(_et,"unispeech"),_et.forEach(t),TNo=r(H6e," \u2014 "),tN=s(H6e,"A",{href:!0});var bet=n(tN);FNo=r(bet,"UniSpeechForCTC"),bet.forEach(t),CNo=r(H6e," (UniSpeech model)"),H6e.forEach(t),MNo=i(qo),f1=s(qo,"LI",{});var U6e=n(f1);Nie=s(U6e,"STRONG",{});var vet=n(Nie);ENo=r(vet,"unispeech-sat"),vet.forEach(t),yNo=r(U6e," \u2014 "),aN=s(U6e,"A",{href:!0});var Tet=n(aN);wNo=r(Tet,"UniSpeechSatForCTC"),Tet.forEach(t),ANo=r(U6e," (UniSpeechSat model)"),U6e.forEach(t),LNo=i(qo),g1=s(qo,"LI",{});var J6e=n(g1);qie=s(J6e,"STRONG",{});var Fet=n(qie);BNo=r(Fet,"wav2vec2"),Fet.forEach(t),xNo=r(J6e," \u2014 "),sN=s(J6e,"A",{href:!0});var Cet=n(sN);kNo=r(Cet,"Wav2Vec2ForCTC"),Cet.forEach(t),RNo=r(J6e," (Wav2Vec2 model)"),J6e.forEach(t),SNo=i(qo),h1=s(qo,"LI",{});var Y6e=n(h1);Gie=s(Y6e,"STRONG",{});var Met=n(Gie);PNo=r(Met,"wavlm"),Met.forEach(t),$No=r(Y6e," \u2014 "),nN=s(Y6e,"A",{href:!0});var Eet=n(nN);INo=r(Eet,"WavLMForCTC"),Eet.forEach(t),DNo=r(Y6e," (WavLM model)"),Y6e.forEach(t),qo.forEach(t),jNo=i(Jt),u1=s(Jt,"P",{});var K6e=n(u1);NNo=r(K6e,"The model is set in evaluation mode by default using "),Oie=s(K6e,"CODE",{});var yet=n(Oie);qNo=r(yet,"model.eval()"),yet.forEach(t),GNo=r(K6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Xie=s(K6e,"CODE",{});var wet=n(Xie);ONo=r(wet,"model.train()"),wet.forEach(t),K6e.forEach(t),XNo=i(Jt),Vie=s(Jt,"P",{});var Aet=n(Vie);VNo=r(Aet,"Examples:"),Aet.forEach(t),zNo=i(Jt),f(n6.$$.fragment,Jt),Jt.forEach(t),bl.forEach(t),FBe=i(c),Dd=s(c,"H2",{class:!0});var Lke=n(Dd);p1=s(Lke,"A",{id:!0,class:!0,href:!0});var Let=n(p1);zie=s(Let,"SPAN",{});var Bet=n(zie);f(l6.$$.fragment,Bet),Bet.forEach(t),Let.forEach(t),WNo=i(Lke),Wie=s(Lke,"SPAN",{});var xet=n(Wie);QNo=r(xet,"AutoModelForSpeechSeq2Seq"),xet.forEach(t),Lke.forEach(t),CBe=i(c),dr=s(c,"DIV",{class:!0});var Tl=n(dr);f(i6.$$.fragment,Tl),HNo=i(Tl),jd=s(Tl,"P",{});var uz=n(jd);UNo=r(uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Qie=s(uz,"CODE",{});var ket=n(Qie);JNo=r(ket,"from_pretrained()"),ket.forEach(t),YNo=r(uz,"class method or the "),Hie=s(uz,"CODE",{});var Ret=n(Hie);KNo=r(Ret,"from_config()"),Ret.forEach(t),ZNo=r(uz,`class
method.`),uz.forEach(t),eqo=i(Tl),d6=s(Tl,"P",{});var Bke=n(d6);oqo=r(Bke,"This class cannot be instantiated directly using "),Uie=s(Bke,"CODE",{});var Set=n(Uie);rqo=r(Set,"__init__()"),Set.forEach(t),tqo=r(Bke," (throws an error)."),Bke.forEach(t),aqo=i(Tl),rt=s(Tl,"DIV",{class:!0});var Fl=n(rt);f(c6.$$.fragment,Fl),sqo=i(Fl),Jie=s(Fl,"P",{});var Pet=n(Jie);nqo=r(Pet,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Pet.forEach(t),lqo=i(Fl),Nd=s(Fl,"P",{});var pz=n(Nd);iqo=r(pz,`Note:
Loading a model from its configuration file does `),Yie=s(pz,"STRONG",{});var $et=n(Yie);dqo=r($et,"not"),$et.forEach(t),cqo=r(pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kie=s(pz,"CODE",{});var Iet=n(Kie);mqo=r(Iet,"from_pretrained()"),Iet.forEach(t),fqo=r(pz,"to load the model weights."),pz.forEach(t),gqo=i(Fl),Zie=s(Fl,"P",{});var Det=n(Zie);hqo=r(Det,"Examples:"),Det.forEach(t),uqo=i(Fl),f(m6.$$.fragment,Fl),Fl.forEach(t),pqo=i(Tl),He=s(Tl,"DIV",{class:!0});var Yt=n(He);f(f6.$$.fragment,Yt),_qo=i(Yt),ede=s(Yt,"P",{});var jet=n(ede);bqo=r(jet,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),jet.forEach(t),vqo=i(Yt),rs=s(Yt,"P",{});var V3=n(rs);Tqo=r(V3,"The model class to instantiate is selected based on the "),ode=s(V3,"CODE",{});var Net=n(ode);Fqo=r(Net,"model_type"),Net.forEach(t),Cqo=r(V3,` property of the config object (either
passed as an argument or loaded from `),rde=s(V3,"CODE",{});var qet=n(rde);Mqo=r(qet,"pretrained_model_name_or_path"),qet.forEach(t),Eqo=r(V3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tde=s(V3,"CODE",{});var Get=n(tde);yqo=r(Get,"pretrained_model_name_or_path"),Get.forEach(t),wqo=r(V3,":"),V3.forEach(t),Aqo=i(Yt),g6=s(Yt,"UL",{});var xke=n(g6);_1=s(xke,"LI",{});var Z6e=n(_1);ade=s(Z6e,"STRONG",{});var Oet=n(ade);Lqo=r(Oet,"speech-encoder-decoder"),Oet.forEach(t),Bqo=r(Z6e," \u2014 "),lN=s(Z6e,"A",{href:!0});var Xet=n(lN);xqo=r(Xet,"SpeechEncoderDecoderModel"),Xet.forEach(t),kqo=r(Z6e," (Speech Encoder decoder model)"),Z6e.forEach(t),Rqo=i(xke),b1=s(xke,"LI",{});var eAe=n(b1);sde=s(eAe,"STRONG",{});var Vet=n(sde);Sqo=r(Vet,"speech_to_text"),Vet.forEach(t),Pqo=r(eAe," \u2014 "),iN=s(eAe,"A",{href:!0});var zet=n(iN);$qo=r(zet,"Speech2TextForConditionalGeneration"),zet.forEach(t),Iqo=r(eAe," (Speech2Text model)"),eAe.forEach(t),xke.forEach(t),Dqo=i(Yt),v1=s(Yt,"P",{});var oAe=n(v1);jqo=r(oAe,"The model is set in evaluation mode by default using "),nde=s(oAe,"CODE",{});var Wet=n(nde);Nqo=r(Wet,"model.eval()"),Wet.forEach(t),qqo=r(oAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),lde=s(oAe,"CODE",{});var Qet=n(lde);Gqo=r(Qet,"model.train()"),Qet.forEach(t),oAe.forEach(t),Oqo=i(Yt),ide=s(Yt,"P",{});var Het=n(ide);Xqo=r(Het,"Examples:"),Het.forEach(t),Vqo=i(Yt),f(h6.$$.fragment,Yt),Yt.forEach(t),Tl.forEach(t),MBe=i(c),qd=s(c,"H2",{class:!0});var kke=n(qd);T1=s(kke,"A",{id:!0,class:!0,href:!0});var Uet=n(T1);dde=s(Uet,"SPAN",{});var Jet=n(dde);f(u6.$$.fragment,Jet),Jet.forEach(t),Uet.forEach(t),zqo=i(kke),cde=s(kke,"SPAN",{});var Yet=n(cde);Wqo=r(Yet,"AutoModelForAudioXVector"),Yet.forEach(t),kke.forEach(t),EBe=i(c),cr=s(c,"DIV",{class:!0});var Cl=n(cr);f(p6.$$.fragment,Cl),Qqo=i(Cl),Gd=s(Cl,"P",{});var _z=n(Gd);Hqo=r(_z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),mde=s(_z,"CODE",{});var Ket=n(mde);Uqo=r(Ket,"from_pretrained()"),Ket.forEach(t),Jqo=r(_z,"class method or the "),fde=s(_z,"CODE",{});var Zet=n(fde);Yqo=r(Zet,"from_config()"),Zet.forEach(t),Kqo=r(_z,`class
method.`),_z.forEach(t),Zqo=i(Cl),_6=s(Cl,"P",{});var Rke=n(_6);eGo=r(Rke,"This class cannot be instantiated directly using "),gde=s(Rke,"CODE",{});var eot=n(gde);oGo=r(eot,"__init__()"),eot.forEach(t),rGo=r(Rke," (throws an error)."),Rke.forEach(t),tGo=i(Cl),tt=s(Cl,"DIV",{class:!0});var Ml=n(tt);f(b6.$$.fragment,Ml),aGo=i(Ml),hde=s(Ml,"P",{});var oot=n(hde);sGo=r(oot,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),oot.forEach(t),nGo=i(Ml),Od=s(Ml,"P",{});var bz=n(Od);lGo=r(bz,`Note:
Loading a model from its configuration file does `),ude=s(bz,"STRONG",{});var rot=n(ude);iGo=r(rot,"not"),rot.forEach(t),dGo=r(bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),pde=s(bz,"CODE",{});var tot=n(pde);cGo=r(tot,"from_pretrained()"),tot.forEach(t),mGo=r(bz,"to load the model weights."),bz.forEach(t),fGo=i(Ml),_de=s(Ml,"P",{});var aot=n(_de);gGo=r(aot,"Examples:"),aot.forEach(t),hGo=i(Ml),f(v6.$$.fragment,Ml),Ml.forEach(t),uGo=i(Cl),Ue=s(Cl,"DIV",{class:!0});var Kt=n(Ue);f(T6.$$.fragment,Kt),pGo=i(Kt),bde=s(Kt,"P",{});var sot=n(bde);_Go=r(sot,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),sot.forEach(t),bGo=i(Kt),ts=s(Kt,"P",{});var z3=n(ts);vGo=r(z3,"The model class to instantiate is selected based on the "),vde=s(z3,"CODE",{});var not=n(vde);TGo=r(not,"model_type"),not.forEach(t),FGo=r(z3,` property of the config object (either
passed as an argument or loaded from `),Tde=s(z3,"CODE",{});var lot=n(Tde);CGo=r(lot,"pretrained_model_name_or_path"),lot.forEach(t),MGo=r(z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Fde=s(z3,"CODE",{});var iot=n(Fde);EGo=r(iot,"pretrained_model_name_or_path"),iot.forEach(t),yGo=r(z3,":"),z3.forEach(t),wGo=i(Kt),as=s(Kt,"UL",{});var W3=n(as);F1=s(W3,"LI",{});var rAe=n(F1);Cde=s(rAe,"STRONG",{});var dot=n(Cde);AGo=r(dot,"data2vec-audio"),dot.forEach(t),LGo=r(rAe," \u2014 "),dN=s(rAe,"A",{href:!0});var cot=n(dN);BGo=r(cot,"Data2VecAudioForXVector"),cot.forEach(t),xGo=r(rAe," (Data2VecAudio model)"),rAe.forEach(t),kGo=i(W3),C1=s(W3,"LI",{});var tAe=n(C1);Mde=s(tAe,"STRONG",{});var mot=n(Mde);RGo=r(mot,"unispeech-sat"),mot.forEach(t),SGo=r(tAe," \u2014 "),cN=s(tAe,"A",{href:!0});var fot=n(cN);PGo=r(fot,"UniSpeechSatForXVector"),fot.forEach(t),$Go=r(tAe," (UniSpeechSat model)"),tAe.forEach(t),IGo=i(W3),M1=s(W3,"LI",{});var aAe=n(M1);Ede=s(aAe,"STRONG",{});var got=n(Ede);DGo=r(got,"wav2vec2"),got.forEach(t),jGo=r(aAe," \u2014 "),mN=s(aAe,"A",{href:!0});var hot=n(mN);NGo=r(hot,"Wav2Vec2ForXVector"),hot.forEach(t),qGo=r(aAe," (Wav2Vec2 model)"),aAe.forEach(t),GGo=i(W3),E1=s(W3,"LI",{});var sAe=n(E1);yde=s(sAe,"STRONG",{});var uot=n(yde);OGo=r(uot,"wavlm"),uot.forEach(t),XGo=r(sAe," \u2014 "),fN=s(sAe,"A",{href:!0});var pot=n(fN);VGo=r(pot,"WavLMForXVector"),pot.forEach(t),zGo=r(sAe," (WavLM model)"),sAe.forEach(t),W3.forEach(t),WGo=i(Kt),y1=s(Kt,"P",{});var nAe=n(y1);QGo=r(nAe,"The model is set in evaluation mode by default using "),wde=s(nAe,"CODE",{});var _ot=n(wde);HGo=r(_ot,"model.eval()"),_ot.forEach(t),UGo=r(nAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ade=s(nAe,"CODE",{});var bot=n(Ade);JGo=r(bot,"model.train()"),bot.forEach(t),nAe.forEach(t),YGo=i(Kt),Lde=s(Kt,"P",{});var vot=n(Lde);KGo=r(vot,"Examples:"),vot.forEach(t),ZGo=i(Kt),f(F6.$$.fragment,Kt),Kt.forEach(t),Cl.forEach(t),yBe=i(c),Xd=s(c,"H2",{class:!0});var Ske=n(Xd);w1=s(Ske,"A",{id:!0,class:!0,href:!0});var Tot=n(w1);Bde=s(Tot,"SPAN",{});var Fot=n(Bde);f(C6.$$.fragment,Fot),Fot.forEach(t),Tot.forEach(t),eOo=i(Ske),xde=s(Ske,"SPAN",{});var Cot=n(xde);oOo=r(Cot,"AutoModelForMaskedImageModeling"),Cot.forEach(t),Ske.forEach(t),wBe=i(c),mr=s(c,"DIV",{class:!0});var El=n(mr);f(M6.$$.fragment,El),rOo=i(El),Vd=s(El,"P",{});var vz=n(Vd);tOo=r(vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),kde=s(vz,"CODE",{});var Mot=n(kde);aOo=r(Mot,"from_pretrained()"),Mot.forEach(t),sOo=r(vz,"class method or the "),Rde=s(vz,"CODE",{});var Eot=n(Rde);nOo=r(Eot,"from_config()"),Eot.forEach(t),lOo=r(vz,`class
method.`),vz.forEach(t),iOo=i(El),E6=s(El,"P",{});var Pke=n(E6);dOo=r(Pke,"This class cannot be instantiated directly using "),Sde=s(Pke,"CODE",{});var yot=n(Sde);cOo=r(yot,"__init__()"),yot.forEach(t),mOo=r(Pke," (throws an error)."),Pke.forEach(t),fOo=i(El),at=s(El,"DIV",{class:!0});var yl=n(at);f(y6.$$.fragment,yl),gOo=i(yl),Pde=s(yl,"P",{});var wot=n(Pde);hOo=r(wot,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),wot.forEach(t),uOo=i(yl),zd=s(yl,"P",{});var Tz=n(zd);pOo=r(Tz,`Note:
Loading a model from its configuration file does `),$de=s(Tz,"STRONG",{});var Aot=n($de);_Oo=r(Aot,"not"),Aot.forEach(t),bOo=r(Tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ide=s(Tz,"CODE",{});var Lot=n(Ide);vOo=r(Lot,"from_pretrained()"),Lot.forEach(t),TOo=r(Tz,"to load the model weights."),Tz.forEach(t),FOo=i(yl),Dde=s(yl,"P",{});var Bot=n(Dde);COo=r(Bot,"Examples:"),Bot.forEach(t),MOo=i(yl),f(w6.$$.fragment,yl),yl.forEach(t),EOo=i(El),Je=s(El,"DIV",{class:!0});var Zt=n(Je);f(A6.$$.fragment,Zt),yOo=i(Zt),jde=s(Zt,"P",{});var xot=n(jde);wOo=r(xot,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),xot.forEach(t),AOo=i(Zt),ss=s(Zt,"P",{});var Q3=n(ss);LOo=r(Q3,"The model class to instantiate is selected based on the "),Nde=s(Q3,"CODE",{});var kot=n(Nde);BOo=r(kot,"model_type"),kot.forEach(t),xOo=r(Q3,` property of the config object (either
passed as an argument or loaded from `),qde=s(Q3,"CODE",{});var Rot=n(qde);kOo=r(Rot,"pretrained_model_name_or_path"),Rot.forEach(t),ROo=r(Q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gde=s(Q3,"CODE",{});var Sot=n(Gde);SOo=r(Sot,"pretrained_model_name_or_path"),Sot.forEach(t),POo=r(Q3,":"),Q3.forEach(t),$Oo=i(Zt),Wd=s(Zt,"UL",{});var Fz=n(Wd);A1=s(Fz,"LI",{});var lAe=n(A1);Ode=s(lAe,"STRONG",{});var Pot=n(Ode);IOo=r(Pot,"deit"),Pot.forEach(t),DOo=r(lAe," \u2014 "),gN=s(lAe,"A",{href:!0});var $ot=n(gN);jOo=r($ot,"DeiTForMaskedImageModeling"),$ot.forEach(t),NOo=r(lAe," (DeiT model)"),lAe.forEach(t),qOo=i(Fz),L1=s(Fz,"LI",{});var iAe=n(L1);Xde=s(iAe,"STRONG",{});var Iot=n(Xde);GOo=r(Iot,"swin"),Iot.forEach(t),OOo=r(iAe," \u2014 "),hN=s(iAe,"A",{href:!0});var Dot=n(hN);XOo=r(Dot,"SwinForMaskedImageModeling"),Dot.forEach(t),VOo=r(iAe," (Swin model)"),iAe.forEach(t),zOo=i(Fz),B1=s(Fz,"LI",{});var dAe=n(B1);Vde=s(dAe,"STRONG",{});var jot=n(Vde);WOo=r(jot,"vit"),jot.forEach(t),QOo=r(dAe," \u2014 "),uN=s(dAe,"A",{href:!0});var Not=n(uN);HOo=r(Not,"ViTForMaskedImageModeling"),Not.forEach(t),UOo=r(dAe," (ViT model)"),dAe.forEach(t),Fz.forEach(t),JOo=i(Zt),x1=s(Zt,"P",{});var cAe=n(x1);YOo=r(cAe,"The model is set in evaluation mode by default using "),zde=s(cAe,"CODE",{});var qot=n(zde);KOo=r(qot,"model.eval()"),qot.forEach(t),ZOo=r(cAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wde=s(cAe,"CODE",{});var Got=n(Wde);eXo=r(Got,"model.train()"),Got.forEach(t),cAe.forEach(t),oXo=i(Zt),Qde=s(Zt,"P",{});var Oot=n(Qde);rXo=r(Oot,"Examples:"),Oot.forEach(t),tXo=i(Zt),f(L6.$$.fragment,Zt),Zt.forEach(t),El.forEach(t),ABe=i(c),Qd=s(c,"H2",{class:!0});var $ke=n(Qd);k1=s($ke,"A",{id:!0,class:!0,href:!0});var Xot=n(k1);Hde=s(Xot,"SPAN",{});var Vot=n(Hde);f(B6.$$.fragment,Vot),Vot.forEach(t),Xot.forEach(t),aXo=i($ke),Ude=s($ke,"SPAN",{});var zot=n(Ude);sXo=r(zot,"AutoModelForObjectDetection"),zot.forEach(t),$ke.forEach(t),LBe=i(c),fr=s(c,"DIV",{class:!0});var wl=n(fr);f(x6.$$.fragment,wl),nXo=i(wl),Hd=s(wl,"P",{});var Cz=n(Hd);lXo=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Jde=s(Cz,"CODE",{});var Wot=n(Jde);iXo=r(Wot,"from_pretrained()"),Wot.forEach(t),dXo=r(Cz,"class method or the "),Yde=s(Cz,"CODE",{});var Qot=n(Yde);cXo=r(Qot,"from_config()"),Qot.forEach(t),mXo=r(Cz,`class
method.`),Cz.forEach(t),fXo=i(wl),k6=s(wl,"P",{});var Ike=n(k6);gXo=r(Ike,"This class cannot be instantiated directly using "),Kde=s(Ike,"CODE",{});var Hot=n(Kde);hXo=r(Hot,"__init__()"),Hot.forEach(t),uXo=r(Ike," (throws an error)."),Ike.forEach(t),pXo=i(wl),st=s(wl,"DIV",{class:!0});var Al=n(st);f(R6.$$.fragment,Al),_Xo=i(Al),Zde=s(Al,"P",{});var Uot=n(Zde);bXo=r(Uot,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),Uot.forEach(t),vXo=i(Al),Ud=s(Al,"P",{});var Mz=n(Ud);TXo=r(Mz,`Note:
Loading a model from its configuration file does `),ece=s(Mz,"STRONG",{});var Jot=n(ece);FXo=r(Jot,"not"),Jot.forEach(t),CXo=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),oce=s(Mz,"CODE",{});var Yot=n(oce);MXo=r(Yot,"from_pretrained()"),Yot.forEach(t),EXo=r(Mz,"to load the model weights."),Mz.forEach(t),yXo=i(Al),rce=s(Al,"P",{});var Kot=n(rce);wXo=r(Kot,"Examples:"),Kot.forEach(t),AXo=i(Al),f(S6.$$.fragment,Al),Al.forEach(t),LXo=i(wl),Ye=s(wl,"DIV",{class:!0});var ea=n(Ye);f(P6.$$.fragment,ea),BXo=i(ea),tce=s(ea,"P",{});var Zot=n(tce);xXo=r(Zot,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),Zot.forEach(t),kXo=i(ea),ns=s(ea,"P",{});var H3=n(ns);RXo=r(H3,"The model class to instantiate is selected based on the "),ace=s(H3,"CODE",{});var ert=n(ace);SXo=r(ert,"model_type"),ert.forEach(t),PXo=r(H3,` property of the config object (either
passed as an argument or loaded from `),sce=s(H3,"CODE",{});var ort=n(sce);$Xo=r(ort,"pretrained_model_name_or_path"),ort.forEach(t),IXo=r(H3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nce=s(H3,"CODE",{});var rrt=n(nce);DXo=r(rrt,"pretrained_model_name_or_path"),rrt.forEach(t),jXo=r(H3,":"),H3.forEach(t),NXo=i(ea),lce=s(ea,"UL",{});var trt=n(lce);R1=s(trt,"LI",{});var mAe=n(R1);ice=s(mAe,"STRONG",{});var art=n(ice);qXo=r(art,"detr"),art.forEach(t),GXo=r(mAe," \u2014 "),pN=s(mAe,"A",{href:!0});var srt=n(pN);OXo=r(srt,"DetrForObjectDetection"),srt.forEach(t),XXo=r(mAe," (DETR model)"),mAe.forEach(t),trt.forEach(t),VXo=i(ea),S1=s(ea,"P",{});var fAe=n(S1);zXo=r(fAe,"The model is set in evaluation mode by default using "),dce=s(fAe,"CODE",{});var nrt=n(dce);WXo=r(nrt,"model.eval()"),nrt.forEach(t),QXo=r(fAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cce=s(fAe,"CODE",{});var lrt=n(cce);HXo=r(lrt,"model.train()"),lrt.forEach(t),fAe.forEach(t),UXo=i(ea),mce=s(ea,"P",{});var irt=n(mce);JXo=r(irt,"Examples:"),irt.forEach(t),YXo=i(ea),f($6.$$.fragment,ea),ea.forEach(t),wl.forEach(t),BBe=i(c),Jd=s(c,"H2",{class:!0});var Dke=n(Jd);P1=s(Dke,"A",{id:!0,class:!0,href:!0});var drt=n(P1);fce=s(drt,"SPAN",{});var crt=n(fce);f(I6.$$.fragment,crt),crt.forEach(t),drt.forEach(t),KXo=i(Dke),gce=s(Dke,"SPAN",{});var mrt=n(gce);ZXo=r(mrt,"AutoModelForImageSegmentation"),mrt.forEach(t),Dke.forEach(t),xBe=i(c),gr=s(c,"DIV",{class:!0});var Ll=n(gr);f(D6.$$.fragment,Ll),eVo=i(Ll),Yd=s(Ll,"P",{});var Ez=n(Yd);oVo=r(Ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),hce=s(Ez,"CODE",{});var frt=n(hce);rVo=r(frt,"from_pretrained()"),frt.forEach(t),tVo=r(Ez,"class method or the "),uce=s(Ez,"CODE",{});var grt=n(uce);aVo=r(grt,"from_config()"),grt.forEach(t),sVo=r(Ez,`class
method.`),Ez.forEach(t),nVo=i(Ll),j6=s(Ll,"P",{});var jke=n(j6);lVo=r(jke,"This class cannot be instantiated directly using "),pce=s(jke,"CODE",{});var hrt=n(pce);iVo=r(hrt,"__init__()"),hrt.forEach(t),dVo=r(jke," (throws an error)."),jke.forEach(t),cVo=i(Ll),nt=s(Ll,"DIV",{class:!0});var Bl=n(nt);f(N6.$$.fragment,Bl),mVo=i(Bl),_ce=s(Bl,"P",{});var urt=n(_ce);fVo=r(urt,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),urt.forEach(t),gVo=i(Bl),Kd=s(Bl,"P",{});var yz=n(Kd);hVo=r(yz,`Note:
Loading a model from its configuration file does `),bce=s(yz,"STRONG",{});var prt=n(bce);uVo=r(prt,"not"),prt.forEach(t),pVo=r(yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),vce=s(yz,"CODE",{});var _rt=n(vce);_Vo=r(_rt,"from_pretrained()"),_rt.forEach(t),bVo=r(yz,"to load the model weights."),yz.forEach(t),vVo=i(Bl),Tce=s(Bl,"P",{});var brt=n(Tce);TVo=r(brt,"Examples:"),brt.forEach(t),FVo=i(Bl),f(q6.$$.fragment,Bl),Bl.forEach(t),CVo=i(Ll),Ke=s(Ll,"DIV",{class:!0});var oa=n(Ke);f(G6.$$.fragment,oa),MVo=i(oa),Fce=s(oa,"P",{});var vrt=n(Fce);EVo=r(vrt,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),vrt.forEach(t),yVo=i(oa),ls=s(oa,"P",{});var U3=n(ls);wVo=r(U3,"The model class to instantiate is selected based on the "),Cce=s(U3,"CODE",{});var Trt=n(Cce);AVo=r(Trt,"model_type"),Trt.forEach(t),LVo=r(U3,` property of the config object (either
passed as an argument or loaded from `),Mce=s(U3,"CODE",{});var Frt=n(Mce);BVo=r(Frt,"pretrained_model_name_or_path"),Frt.forEach(t),xVo=r(U3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ece=s(U3,"CODE",{});var Crt=n(Ece);kVo=r(Crt,"pretrained_model_name_or_path"),Crt.forEach(t),RVo=r(U3,":"),U3.forEach(t),SVo=i(oa),yce=s(oa,"UL",{});var Mrt=n(yce);$1=s(Mrt,"LI",{});var gAe=n($1);wce=s(gAe,"STRONG",{});var Ert=n(wce);PVo=r(Ert,"detr"),Ert.forEach(t),$Vo=r(gAe," \u2014 "),_N=s(gAe,"A",{href:!0});var yrt=n(_N);IVo=r(yrt,"DetrForSegmentation"),yrt.forEach(t),DVo=r(gAe," (DETR model)"),gAe.forEach(t),Mrt.forEach(t),jVo=i(oa),I1=s(oa,"P",{});var hAe=n(I1);NVo=r(hAe,"The model is set in evaluation mode by default using "),Ace=s(hAe,"CODE",{});var wrt=n(Ace);qVo=r(wrt,"model.eval()"),wrt.forEach(t),GVo=r(hAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Lce=s(hAe,"CODE",{});var Art=n(Lce);OVo=r(Art,"model.train()"),Art.forEach(t),hAe.forEach(t),XVo=i(oa),Bce=s(oa,"P",{});var Lrt=n(Bce);VVo=r(Lrt,"Examples:"),Lrt.forEach(t),zVo=i(oa),f(O6.$$.fragment,oa),oa.forEach(t),Ll.forEach(t),kBe=i(c),Zd=s(c,"H2",{class:!0});var Nke=n(Zd);D1=s(Nke,"A",{id:!0,class:!0,href:!0});var Brt=n(D1);xce=s(Brt,"SPAN",{});var xrt=n(xce);f(X6.$$.fragment,xrt),xrt.forEach(t),Brt.forEach(t),WVo=i(Nke),kce=s(Nke,"SPAN",{});var krt=n(kce);QVo=r(krt,"AutoModelForSemanticSegmentation"),krt.forEach(t),Nke.forEach(t),RBe=i(c),hr=s(c,"DIV",{class:!0});var xl=n(hr);f(V6.$$.fragment,xl),HVo=i(xl),ec=s(xl,"P",{});var wz=n(ec);UVo=r(wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Rce=s(wz,"CODE",{});var Rrt=n(Rce);JVo=r(Rrt,"from_pretrained()"),Rrt.forEach(t),YVo=r(wz,"class method or the "),Sce=s(wz,"CODE",{});var Srt=n(Sce);KVo=r(Srt,"from_config()"),Srt.forEach(t),ZVo=r(wz,`class
method.`),wz.forEach(t),ezo=i(xl),z6=s(xl,"P",{});var qke=n(z6);ozo=r(qke,"This class cannot be instantiated directly using "),Pce=s(qke,"CODE",{});var Prt=n(Pce);rzo=r(Prt,"__init__()"),Prt.forEach(t),tzo=r(qke," (throws an error)."),qke.forEach(t),azo=i(xl),lt=s(xl,"DIV",{class:!0});var kl=n(lt);f(W6.$$.fragment,kl),szo=i(kl),$ce=s(kl,"P",{});var $rt=n($ce);nzo=r($rt,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),$rt.forEach(t),lzo=i(kl),oc=s(kl,"P",{});var Az=n(oc);izo=r(Az,`Note:
Loading a model from its configuration file does `),Ice=s(Az,"STRONG",{});var Irt=n(Ice);dzo=r(Irt,"not"),Irt.forEach(t),czo=r(Az,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dce=s(Az,"CODE",{});var Drt=n(Dce);mzo=r(Drt,"from_pretrained()"),Drt.forEach(t),fzo=r(Az,"to load the model weights."),Az.forEach(t),gzo=i(kl),jce=s(kl,"P",{});var jrt=n(jce);hzo=r(jrt,"Examples:"),jrt.forEach(t),uzo=i(kl),f(Q6.$$.fragment,kl),kl.forEach(t),pzo=i(xl),Ze=s(xl,"DIV",{class:!0});var ra=n(Ze);f(H6.$$.fragment,ra),_zo=i(ra),Nce=s(ra,"P",{});var Nrt=n(Nce);bzo=r(Nrt,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),Nrt.forEach(t),vzo=i(ra),is=s(ra,"P",{});var J3=n(is);Tzo=r(J3,"The model class to instantiate is selected based on the "),qce=s(J3,"CODE",{});var qrt=n(qce);Fzo=r(qrt,"model_type"),qrt.forEach(t),Czo=r(J3,` property of the config object (either
passed as an argument or loaded from `),Gce=s(J3,"CODE",{});var Grt=n(Gce);Mzo=r(Grt,"pretrained_model_name_or_path"),Grt.forEach(t),Ezo=r(J3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Oce=s(J3,"CODE",{});var Ort=n(Oce);yzo=r(Ort,"pretrained_model_name_or_path"),Ort.forEach(t),wzo=r(J3,":"),J3.forEach(t),Azo=i(ra),U6=s(ra,"UL",{});var Gke=n(U6);j1=s(Gke,"LI",{});var uAe=n(j1);Xce=s(uAe,"STRONG",{});var Xrt=n(Xce);Lzo=r(Xrt,"beit"),Xrt.forEach(t),Bzo=r(uAe," \u2014 "),bN=s(uAe,"A",{href:!0});var Vrt=n(bN);xzo=r(Vrt,"BeitForSemanticSegmentation"),Vrt.forEach(t),kzo=r(uAe," (BEiT model)"),uAe.forEach(t),Rzo=i(Gke),N1=s(Gke,"LI",{});var pAe=n(N1);Vce=s(pAe,"STRONG",{});var zrt=n(Vce);Szo=r(zrt,"segformer"),zrt.forEach(t),Pzo=r(pAe," \u2014 "),vN=s(pAe,"A",{href:!0});var Wrt=n(vN);$zo=r(Wrt,"SegformerForSemanticSegmentation"),Wrt.forEach(t),Izo=r(pAe," (SegFormer model)"),pAe.forEach(t),Gke.forEach(t),Dzo=i(ra),q1=s(ra,"P",{});var _Ae=n(q1);jzo=r(_Ae,"The model is set in evaluation mode by default using "),zce=s(_Ae,"CODE",{});var Qrt=n(zce);Nzo=r(Qrt,"model.eval()"),Qrt.forEach(t),qzo=r(_Ae,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wce=s(_Ae,"CODE",{});var Hrt=n(Wce);Gzo=r(Hrt,"model.train()"),Hrt.forEach(t),_Ae.forEach(t),Ozo=i(ra),Qce=s(ra,"P",{});var Urt=n(Qce);Xzo=r(Urt,"Examples:"),Urt.forEach(t),Vzo=i(ra),f(J6.$$.fragment,ra),ra.forEach(t),xl.forEach(t),SBe=i(c),rc=s(c,"H2",{class:!0});var Oke=n(rc);G1=s(Oke,"A",{id:!0,class:!0,href:!0});var Jrt=n(G1);Hce=s(Jrt,"SPAN",{});var Yrt=n(Hce);f(Y6.$$.fragment,Yrt),Yrt.forEach(t),Jrt.forEach(t),zzo=i(Oke),Uce=s(Oke,"SPAN",{});var Krt=n(Uce);Wzo=r(Krt,"TFAutoModel"),Krt.forEach(t),Oke.forEach(t),PBe=i(c),ur=s(c,"DIV",{class:!0});var Rl=n(ur);f(K6.$$.fragment,Rl),Qzo=i(Rl),tc=s(Rl,"P",{});var Lz=n(tc);Hzo=r(Lz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Jce=s(Lz,"CODE",{});var Zrt=n(Jce);Uzo=r(Zrt,"from_pretrained()"),Zrt.forEach(t),Jzo=r(Lz,"class method or the "),Yce=s(Lz,"CODE",{});var ett=n(Yce);Yzo=r(ett,"from_config()"),ett.forEach(t),Kzo=r(Lz,`class
method.`),Lz.forEach(t),Zzo=i(Rl),Z6=s(Rl,"P",{});var Xke=n(Z6);eWo=r(Xke,"This class cannot be instantiated directly using "),Kce=s(Xke,"CODE",{});var ott=n(Kce);oWo=r(ott,"__init__()"),ott.forEach(t),rWo=r(Xke," (throws an error)."),Xke.forEach(t),tWo=i(Rl),it=s(Rl,"DIV",{class:!0});var Sl=n(it);f(eA.$$.fragment,Sl),aWo=i(Sl),Zce=s(Sl,"P",{});var rtt=n(Zce);sWo=r(rtt,"Instantiates one of the base model classes of the library from a configuration."),rtt.forEach(t),nWo=i(Sl),ac=s(Sl,"P",{});var Bz=n(ac);lWo=r(Bz,`Note:
Loading a model from its configuration file does `),eme=s(Bz,"STRONG",{});var ttt=n(eme);iWo=r(ttt,"not"),ttt.forEach(t),dWo=r(Bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ome=s(Bz,"CODE",{});var att=n(ome);cWo=r(att,"from_pretrained()"),att.forEach(t),mWo=r(Bz,"to load the model weights."),Bz.forEach(t),fWo=i(Sl),rme=s(Sl,"P",{});var stt=n(rme);gWo=r(stt,"Examples:"),stt.forEach(t),hWo=i(Sl),f(oA.$$.fragment,Sl),Sl.forEach(t),uWo=i(Rl),go=s(Rl,"DIV",{class:!0});var ca=n(go);f(rA.$$.fragment,ca),pWo=i(ca),tme=s(ca,"P",{});var ntt=n(tme);_Wo=r(ntt,"Instantiate one of the base model classes of the library from a pretrained model."),ntt.forEach(t),bWo=i(ca),ds=s(ca,"P",{});var Y3=n(ds);vWo=r(Y3,"The model class to instantiate is selected based on the "),ame=s(Y3,"CODE",{});var ltt=n(ame);TWo=r(ltt,"model_type"),ltt.forEach(t),FWo=r(Y3,` property of the config object (either
passed as an argument or loaded from `),sme=s(Y3,"CODE",{});var itt=n(sme);CWo=r(itt,"pretrained_model_name_or_path"),itt.forEach(t),MWo=r(Y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nme=s(Y3,"CODE",{});var dtt=n(nme);EWo=r(dtt,"pretrained_model_name_or_path"),dtt.forEach(t),yWo=r(Y3,":"),Y3.forEach(t),wWo=i(ca),B=s(ca,"UL",{});var x=n(B);O1=s(x,"LI",{});var bAe=n(O1);lme=s(bAe,"STRONG",{});var ctt=n(lme);AWo=r(ctt,"albert"),ctt.forEach(t),LWo=r(bAe," \u2014 "),TN=s(bAe,"A",{href:!0});var mtt=n(TN);BWo=r(mtt,"TFAlbertModel"),mtt.forEach(t),xWo=r(bAe," (ALBERT model)"),bAe.forEach(t),kWo=i(x),X1=s(x,"LI",{});var vAe=n(X1);ime=s(vAe,"STRONG",{});var ftt=n(ime);RWo=r(ftt,"bart"),ftt.forEach(t),SWo=r(vAe," \u2014 "),FN=s(vAe,"A",{href:!0});var gtt=n(FN);PWo=r(gtt,"TFBartModel"),gtt.forEach(t),$Wo=r(vAe," (BART model)"),vAe.forEach(t),IWo=i(x),V1=s(x,"LI",{});var TAe=n(V1);dme=s(TAe,"STRONG",{});var htt=n(dme);DWo=r(htt,"bert"),htt.forEach(t),jWo=r(TAe," \u2014 "),CN=s(TAe,"A",{href:!0});var utt=n(CN);NWo=r(utt,"TFBertModel"),utt.forEach(t),qWo=r(TAe," (BERT model)"),TAe.forEach(t),GWo=i(x),z1=s(x,"LI",{});var FAe=n(z1);cme=s(FAe,"STRONG",{});var ptt=n(cme);OWo=r(ptt,"blenderbot"),ptt.forEach(t),XWo=r(FAe," \u2014 "),MN=s(FAe,"A",{href:!0});var _tt=n(MN);VWo=r(_tt,"TFBlenderbotModel"),_tt.forEach(t),zWo=r(FAe," (Blenderbot model)"),FAe.forEach(t),WWo=i(x),W1=s(x,"LI",{});var CAe=n(W1);mme=s(CAe,"STRONG",{});var btt=n(mme);QWo=r(btt,"blenderbot-small"),btt.forEach(t),HWo=r(CAe," \u2014 "),EN=s(CAe,"A",{href:!0});var vtt=n(EN);UWo=r(vtt,"TFBlenderbotSmallModel"),vtt.forEach(t),JWo=r(CAe," (BlenderbotSmall model)"),CAe.forEach(t),YWo=i(x),Q1=s(x,"LI",{});var MAe=n(Q1);fme=s(MAe,"STRONG",{});var Ttt=n(fme);KWo=r(Ttt,"camembert"),Ttt.forEach(t),ZWo=r(MAe," \u2014 "),yN=s(MAe,"A",{href:!0});var Ftt=n(yN);eQo=r(Ftt,"TFCamembertModel"),Ftt.forEach(t),oQo=r(MAe," (CamemBERT model)"),MAe.forEach(t),rQo=i(x),H1=s(x,"LI",{});var EAe=n(H1);gme=s(EAe,"STRONG",{});var Ctt=n(gme);tQo=r(Ctt,"clip"),Ctt.forEach(t),aQo=r(EAe," \u2014 "),wN=s(EAe,"A",{href:!0});var Mtt=n(wN);sQo=r(Mtt,"TFCLIPModel"),Mtt.forEach(t),nQo=r(EAe," (CLIP model)"),EAe.forEach(t),lQo=i(x),U1=s(x,"LI",{});var yAe=n(U1);hme=s(yAe,"STRONG",{});var Ett=n(hme);iQo=r(Ett,"convbert"),Ett.forEach(t),dQo=r(yAe," \u2014 "),AN=s(yAe,"A",{href:!0});var ytt=n(AN);cQo=r(ytt,"TFConvBertModel"),ytt.forEach(t),mQo=r(yAe," (ConvBERT model)"),yAe.forEach(t),fQo=i(x),J1=s(x,"LI",{});var wAe=n(J1);ume=s(wAe,"STRONG",{});var wtt=n(ume);gQo=r(wtt,"convnext"),wtt.forEach(t),hQo=r(wAe," \u2014 "),LN=s(wAe,"A",{href:!0});var Att=n(LN);uQo=r(Att,"TFConvNextModel"),Att.forEach(t),pQo=r(wAe," (ConvNext model)"),wAe.forEach(t),_Qo=i(x),Y1=s(x,"LI",{});var AAe=n(Y1);pme=s(AAe,"STRONG",{});var Ltt=n(pme);bQo=r(Ltt,"ctrl"),Ltt.forEach(t),vQo=r(AAe," \u2014 "),BN=s(AAe,"A",{href:!0});var Btt=n(BN);TQo=r(Btt,"TFCTRLModel"),Btt.forEach(t),FQo=r(AAe," (CTRL model)"),AAe.forEach(t),CQo=i(x),K1=s(x,"LI",{});var LAe=n(K1);_me=s(LAe,"STRONG",{});var xtt=n(_me);MQo=r(xtt,"deberta"),xtt.forEach(t),EQo=r(LAe," \u2014 "),xN=s(LAe,"A",{href:!0});var ktt=n(xN);yQo=r(ktt,"TFDebertaModel"),ktt.forEach(t),wQo=r(LAe," (DeBERTa model)"),LAe.forEach(t),AQo=i(x),Z1=s(x,"LI",{});var BAe=n(Z1);bme=s(BAe,"STRONG",{});var Rtt=n(bme);LQo=r(Rtt,"deberta-v2"),Rtt.forEach(t),BQo=r(BAe," \u2014 "),kN=s(BAe,"A",{href:!0});var Stt=n(kN);xQo=r(Stt,"TFDebertaV2Model"),Stt.forEach(t),kQo=r(BAe," (DeBERTa-v2 model)"),BAe.forEach(t),RQo=i(x),eF=s(x,"LI",{});var xAe=n(eF);vme=s(xAe,"STRONG",{});var Ptt=n(vme);SQo=r(Ptt,"distilbert"),Ptt.forEach(t),PQo=r(xAe," \u2014 "),RN=s(xAe,"A",{href:!0});var $tt=n(RN);$Qo=r($tt,"TFDistilBertModel"),$tt.forEach(t),IQo=r(xAe," (DistilBERT model)"),xAe.forEach(t),DQo=i(x),oF=s(x,"LI",{});var kAe=n(oF);Tme=s(kAe,"STRONG",{});var Itt=n(Tme);jQo=r(Itt,"dpr"),Itt.forEach(t),NQo=r(kAe," \u2014 "),SN=s(kAe,"A",{href:!0});var Dtt=n(SN);qQo=r(Dtt,"TFDPRQuestionEncoder"),Dtt.forEach(t),GQo=r(kAe," (DPR model)"),kAe.forEach(t),OQo=i(x),rF=s(x,"LI",{});var RAe=n(rF);Fme=s(RAe,"STRONG",{});var jtt=n(Fme);XQo=r(jtt,"electra"),jtt.forEach(t),VQo=r(RAe," \u2014 "),PN=s(RAe,"A",{href:!0});var Ntt=n(PN);zQo=r(Ntt,"TFElectraModel"),Ntt.forEach(t),WQo=r(RAe," (ELECTRA model)"),RAe.forEach(t),QQo=i(x),tF=s(x,"LI",{});var SAe=n(tF);Cme=s(SAe,"STRONG",{});var qtt=n(Cme);HQo=r(qtt,"flaubert"),qtt.forEach(t),UQo=r(SAe," \u2014 "),$N=s(SAe,"A",{href:!0});var Gtt=n($N);JQo=r(Gtt,"TFFlaubertModel"),Gtt.forEach(t),YQo=r(SAe," (FlauBERT model)"),SAe.forEach(t),KQo=i(x),In=s(x,"LI",{});var i7=n(In);Mme=s(i7,"STRONG",{});var Ott=n(Mme);ZQo=r(Ott,"funnel"),Ott.forEach(t),eHo=r(i7," \u2014 "),IN=s(i7,"A",{href:!0});var Xtt=n(IN);oHo=r(Xtt,"TFFunnelModel"),Xtt.forEach(t),rHo=r(i7," or "),DN=s(i7,"A",{href:!0});var Vtt=n(DN);tHo=r(Vtt,"TFFunnelBaseModel"),Vtt.forEach(t),aHo=r(i7," (Funnel Transformer model)"),i7.forEach(t),sHo=i(x),aF=s(x,"LI",{});var PAe=n(aF);Eme=s(PAe,"STRONG",{});var ztt=n(Eme);nHo=r(ztt,"gpt2"),ztt.forEach(t),lHo=r(PAe," \u2014 "),jN=s(PAe,"A",{href:!0});var Wtt=n(jN);iHo=r(Wtt,"TFGPT2Model"),Wtt.forEach(t),dHo=r(PAe," (OpenAI GPT-2 model)"),PAe.forEach(t),cHo=i(x),sF=s(x,"LI",{});var $Ae=n(sF);yme=s($Ae,"STRONG",{});var Qtt=n(yme);mHo=r(Qtt,"hubert"),Qtt.forEach(t),fHo=r($Ae," \u2014 "),NN=s($Ae,"A",{href:!0});var Htt=n(NN);gHo=r(Htt,"TFHubertModel"),Htt.forEach(t),hHo=r($Ae," (Hubert model)"),$Ae.forEach(t),uHo=i(x),nF=s(x,"LI",{});var IAe=n(nF);wme=s(IAe,"STRONG",{});var Utt=n(wme);pHo=r(Utt,"layoutlm"),Utt.forEach(t),_Ho=r(IAe," \u2014 "),qN=s(IAe,"A",{href:!0});var Jtt=n(qN);bHo=r(Jtt,"TFLayoutLMModel"),Jtt.forEach(t),vHo=r(IAe," (LayoutLM model)"),IAe.forEach(t),THo=i(x),lF=s(x,"LI",{});var DAe=n(lF);Ame=s(DAe,"STRONG",{});var Ytt=n(Ame);FHo=r(Ytt,"led"),Ytt.forEach(t),CHo=r(DAe," \u2014 "),GN=s(DAe,"A",{href:!0});var Ktt=n(GN);MHo=r(Ktt,"TFLEDModel"),Ktt.forEach(t),EHo=r(DAe," (LED model)"),DAe.forEach(t),yHo=i(x),iF=s(x,"LI",{});var jAe=n(iF);Lme=s(jAe,"STRONG",{});var Ztt=n(Lme);wHo=r(Ztt,"longformer"),Ztt.forEach(t),AHo=r(jAe," \u2014 "),ON=s(jAe,"A",{href:!0});var eat=n(ON);LHo=r(eat,"TFLongformerModel"),eat.forEach(t),BHo=r(jAe," (Longformer model)"),jAe.forEach(t),xHo=i(x),dF=s(x,"LI",{});var NAe=n(dF);Bme=s(NAe,"STRONG",{});var oat=n(Bme);kHo=r(oat,"lxmert"),oat.forEach(t),RHo=r(NAe," \u2014 "),XN=s(NAe,"A",{href:!0});var rat=n(XN);SHo=r(rat,"TFLxmertModel"),rat.forEach(t),PHo=r(NAe," (LXMERT model)"),NAe.forEach(t),$Ho=i(x),cF=s(x,"LI",{});var qAe=n(cF);xme=s(qAe,"STRONG",{});var tat=n(xme);IHo=r(tat,"marian"),tat.forEach(t),DHo=r(qAe," \u2014 "),VN=s(qAe,"A",{href:!0});var aat=n(VN);jHo=r(aat,"TFMarianModel"),aat.forEach(t),NHo=r(qAe," (Marian model)"),qAe.forEach(t),qHo=i(x),mF=s(x,"LI",{});var GAe=n(mF);kme=s(GAe,"STRONG",{});var sat=n(kme);GHo=r(sat,"mbart"),sat.forEach(t),OHo=r(GAe," \u2014 "),zN=s(GAe,"A",{href:!0});var nat=n(zN);XHo=r(nat,"TFMBartModel"),nat.forEach(t),VHo=r(GAe," (mBART model)"),GAe.forEach(t),zHo=i(x),fF=s(x,"LI",{});var OAe=n(fF);Rme=s(OAe,"STRONG",{});var lat=n(Rme);WHo=r(lat,"mobilebert"),lat.forEach(t),QHo=r(OAe," \u2014 "),WN=s(OAe,"A",{href:!0});var iat=n(WN);HHo=r(iat,"TFMobileBertModel"),iat.forEach(t),UHo=r(OAe," (MobileBERT model)"),OAe.forEach(t),JHo=i(x),gF=s(x,"LI",{});var XAe=n(gF);Sme=s(XAe,"STRONG",{});var dat=n(Sme);YHo=r(dat,"mpnet"),dat.forEach(t),KHo=r(XAe," \u2014 "),QN=s(XAe,"A",{href:!0});var cat=n(QN);ZHo=r(cat,"TFMPNetModel"),cat.forEach(t),eUo=r(XAe," (MPNet model)"),XAe.forEach(t),oUo=i(x),hF=s(x,"LI",{});var VAe=n(hF);Pme=s(VAe,"STRONG",{});var mat=n(Pme);rUo=r(mat,"mt5"),mat.forEach(t),tUo=r(VAe," \u2014 "),HN=s(VAe,"A",{href:!0});var fat=n(HN);aUo=r(fat,"TFMT5Model"),fat.forEach(t),sUo=r(VAe," (mT5 model)"),VAe.forEach(t),nUo=i(x),uF=s(x,"LI",{});var zAe=n(uF);$me=s(zAe,"STRONG",{});var gat=n($me);lUo=r(gat,"openai-gpt"),gat.forEach(t),iUo=r(zAe," \u2014 "),UN=s(zAe,"A",{href:!0});var hat=n(UN);dUo=r(hat,"TFOpenAIGPTModel"),hat.forEach(t),cUo=r(zAe," (OpenAI GPT model)"),zAe.forEach(t),mUo=i(x),pF=s(x,"LI",{});var WAe=n(pF);Ime=s(WAe,"STRONG",{});var uat=n(Ime);fUo=r(uat,"pegasus"),uat.forEach(t),gUo=r(WAe," \u2014 "),JN=s(WAe,"A",{href:!0});var pat=n(JN);hUo=r(pat,"TFPegasusModel"),pat.forEach(t),uUo=r(WAe," (Pegasus model)"),WAe.forEach(t),pUo=i(x),_F=s(x,"LI",{});var QAe=n(_F);Dme=s(QAe,"STRONG",{});var _at=n(Dme);_Uo=r(_at,"rembert"),_at.forEach(t),bUo=r(QAe," \u2014 "),YN=s(QAe,"A",{href:!0});var bat=n(YN);vUo=r(bat,"TFRemBertModel"),bat.forEach(t),TUo=r(QAe," (RemBERT model)"),QAe.forEach(t),FUo=i(x),bF=s(x,"LI",{});var HAe=n(bF);jme=s(HAe,"STRONG",{});var vat=n(jme);CUo=r(vat,"roberta"),vat.forEach(t),MUo=r(HAe," \u2014 "),KN=s(HAe,"A",{href:!0});var Tat=n(KN);EUo=r(Tat,"TFRobertaModel"),Tat.forEach(t),yUo=r(HAe," (RoBERTa model)"),HAe.forEach(t),wUo=i(x),vF=s(x,"LI",{});var UAe=n(vF);Nme=s(UAe,"STRONG",{});var Fat=n(Nme);AUo=r(Fat,"roformer"),Fat.forEach(t),LUo=r(UAe," \u2014 "),ZN=s(UAe,"A",{href:!0});var Cat=n(ZN);BUo=r(Cat,"TFRoFormerModel"),Cat.forEach(t),xUo=r(UAe," (RoFormer model)"),UAe.forEach(t),kUo=i(x),TF=s(x,"LI",{});var JAe=n(TF);qme=s(JAe,"STRONG",{});var Mat=n(qme);RUo=r(Mat,"speech_to_text"),Mat.forEach(t),SUo=r(JAe," \u2014 "),eq=s(JAe,"A",{href:!0});var Eat=n(eq);PUo=r(Eat,"TFSpeech2TextModel"),Eat.forEach(t),$Uo=r(JAe," (Speech2Text model)"),JAe.forEach(t),IUo=i(x),FF=s(x,"LI",{});var YAe=n(FF);Gme=s(YAe,"STRONG",{});var yat=n(Gme);DUo=r(yat,"t5"),yat.forEach(t),jUo=r(YAe," \u2014 "),oq=s(YAe,"A",{href:!0});var wat=n(oq);NUo=r(wat,"TFT5Model"),wat.forEach(t),qUo=r(YAe," (T5 model)"),YAe.forEach(t),GUo=i(x),CF=s(x,"LI",{});var KAe=n(CF);Ome=s(KAe,"STRONG",{});var Aat=n(Ome);OUo=r(Aat,"tapas"),Aat.forEach(t),XUo=r(KAe," \u2014 "),rq=s(KAe,"A",{href:!0});var Lat=n(rq);VUo=r(Lat,"TFTapasModel"),Lat.forEach(t),zUo=r(KAe," (TAPAS model)"),KAe.forEach(t),WUo=i(x),MF=s(x,"LI",{});var ZAe=n(MF);Xme=s(ZAe,"STRONG",{});var Bat=n(Xme);QUo=r(Bat,"transfo-xl"),Bat.forEach(t),HUo=r(ZAe," \u2014 "),tq=s(ZAe,"A",{href:!0});var xat=n(tq);UUo=r(xat,"TFTransfoXLModel"),xat.forEach(t),JUo=r(ZAe," (Transformer-XL model)"),ZAe.forEach(t),YUo=i(x),EF=s(x,"LI",{});var e0e=n(EF);Vme=s(e0e,"STRONG",{});var kat=n(Vme);KUo=r(kat,"vit"),kat.forEach(t),ZUo=r(e0e," \u2014 "),aq=s(e0e,"A",{href:!0});var Rat=n(aq);eJo=r(Rat,"TFViTModel"),Rat.forEach(t),oJo=r(e0e," (ViT model)"),e0e.forEach(t),rJo=i(x),yF=s(x,"LI",{});var o0e=n(yF);zme=s(o0e,"STRONG",{});var Sat=n(zme);tJo=r(Sat,"wav2vec2"),Sat.forEach(t),aJo=r(o0e," \u2014 "),sq=s(o0e,"A",{href:!0});var Pat=n(sq);sJo=r(Pat,"TFWav2Vec2Model"),Pat.forEach(t),nJo=r(o0e," (Wav2Vec2 model)"),o0e.forEach(t),lJo=i(x),wF=s(x,"LI",{});var r0e=n(wF);Wme=s(r0e,"STRONG",{});var $at=n(Wme);iJo=r($at,"xlm"),$at.forEach(t),dJo=r(r0e," \u2014 "),nq=s(r0e,"A",{href:!0});var Iat=n(nq);cJo=r(Iat,"TFXLMModel"),Iat.forEach(t),mJo=r(r0e," (XLM model)"),r0e.forEach(t),fJo=i(x),AF=s(x,"LI",{});var t0e=n(AF);Qme=s(t0e,"STRONG",{});var Dat=n(Qme);gJo=r(Dat,"xlm-roberta"),Dat.forEach(t),hJo=r(t0e," \u2014 "),lq=s(t0e,"A",{href:!0});var jat=n(lq);uJo=r(jat,"TFXLMRobertaModel"),jat.forEach(t),pJo=r(t0e," (XLM-RoBERTa model)"),t0e.forEach(t),_Jo=i(x),LF=s(x,"LI",{});var a0e=n(LF);Hme=s(a0e,"STRONG",{});var Nat=n(Hme);bJo=r(Nat,"xlnet"),Nat.forEach(t),vJo=r(a0e," \u2014 "),iq=s(a0e,"A",{href:!0});var qat=n(iq);TJo=r(qat,"TFXLNetModel"),qat.forEach(t),FJo=r(a0e," (XLNet model)"),a0e.forEach(t),x.forEach(t),CJo=i(ca),Ume=s(ca,"P",{});var Gat=n(Ume);MJo=r(Gat,"Examples:"),Gat.forEach(t),EJo=i(ca),f(tA.$$.fragment,ca),ca.forEach(t),Rl.forEach(t),$Be=i(c),sc=s(c,"H2",{class:!0});var Vke=n(sc);BF=s(Vke,"A",{id:!0,class:!0,href:!0});var Oat=n(BF);Jme=s(Oat,"SPAN",{});var Xat=n(Jme);f(aA.$$.fragment,Xat),Xat.forEach(t),Oat.forEach(t),yJo=i(Vke),Yme=s(Vke,"SPAN",{});var Vat=n(Yme);wJo=r(Vat,"TFAutoModelForPreTraining"),Vat.forEach(t),Vke.forEach(t),IBe=i(c),pr=s(c,"DIV",{class:!0});var Pl=n(pr);f(sA.$$.fragment,Pl),AJo=i(Pl),nc=s(Pl,"P",{});var xz=n(nc);LJo=r(xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Kme=s(xz,"CODE",{});var zat=n(Kme);BJo=r(zat,"from_pretrained()"),zat.forEach(t),xJo=r(xz,"class method or the "),Zme=s(xz,"CODE",{});var Wat=n(Zme);kJo=r(Wat,"from_config()"),Wat.forEach(t),RJo=r(xz,`class
method.`),xz.forEach(t),SJo=i(Pl),nA=s(Pl,"P",{});var zke=n(nA);PJo=r(zke,"This class cannot be instantiated directly using "),efe=s(zke,"CODE",{});var Qat=n(efe);$Jo=r(Qat,"__init__()"),Qat.forEach(t),IJo=r(zke," (throws an error)."),zke.forEach(t),DJo=i(Pl),dt=s(Pl,"DIV",{class:!0});var $l=n(dt);f(lA.$$.fragment,$l),jJo=i($l),ofe=s($l,"P",{});var Hat=n(ofe);NJo=r(Hat,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Hat.forEach(t),qJo=i($l),lc=s($l,"P",{});var kz=n(lc);GJo=r(kz,`Note:
Loading a model from its configuration file does `),rfe=s(kz,"STRONG",{});var Uat=n(rfe);OJo=r(Uat,"not"),Uat.forEach(t),XJo=r(kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),tfe=s(kz,"CODE",{});var Jat=n(tfe);VJo=r(Jat,"from_pretrained()"),Jat.forEach(t),zJo=r(kz,"to load the model weights."),kz.forEach(t),WJo=i($l),afe=s($l,"P",{});var Yat=n(afe);QJo=r(Yat,"Examples:"),Yat.forEach(t),HJo=i($l),f(iA.$$.fragment,$l),$l.forEach(t),UJo=i(Pl),ho=s(Pl,"DIV",{class:!0});var ma=n(ho);f(dA.$$.fragment,ma),JJo=i(ma),sfe=s(ma,"P",{});var Kat=n(sfe);YJo=r(Kat,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Kat.forEach(t),KJo=i(ma),cs=s(ma,"P",{});var K3=n(cs);ZJo=r(K3,"The model class to instantiate is selected based on the "),nfe=s(K3,"CODE",{});var Zat=n(nfe);eYo=r(Zat,"model_type"),Zat.forEach(t),oYo=r(K3,` property of the config object (either
passed as an argument or loaded from `),lfe=s(K3,"CODE",{});var est=n(lfe);rYo=r(est,"pretrained_model_name_or_path"),est.forEach(t),tYo=r(K3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ife=s(K3,"CODE",{});var ost=n(ife);aYo=r(ost,"pretrained_model_name_or_path"),ost.forEach(t),sYo=r(K3,":"),K3.forEach(t),nYo=i(ma),H=s(ma,"UL",{});var U=n(H);xF=s(U,"LI",{});var s0e=n(xF);dfe=s(s0e,"STRONG",{});var rst=n(dfe);lYo=r(rst,"albert"),rst.forEach(t),iYo=r(s0e," \u2014 "),dq=s(s0e,"A",{href:!0});var tst=n(dq);dYo=r(tst,"TFAlbertForPreTraining"),tst.forEach(t),cYo=r(s0e," (ALBERT model)"),s0e.forEach(t),mYo=i(U),kF=s(U,"LI",{});var n0e=n(kF);cfe=s(n0e,"STRONG",{});var ast=n(cfe);fYo=r(ast,"bart"),ast.forEach(t),gYo=r(n0e," \u2014 "),cq=s(n0e,"A",{href:!0});var sst=n(cq);hYo=r(sst,"TFBartForConditionalGeneration"),sst.forEach(t),uYo=r(n0e," (BART model)"),n0e.forEach(t),pYo=i(U),RF=s(U,"LI",{});var l0e=n(RF);mfe=s(l0e,"STRONG",{});var nst=n(mfe);_Yo=r(nst,"bert"),nst.forEach(t),bYo=r(l0e," \u2014 "),mq=s(l0e,"A",{href:!0});var lst=n(mq);vYo=r(lst,"TFBertForPreTraining"),lst.forEach(t),TYo=r(l0e," (BERT model)"),l0e.forEach(t),FYo=i(U),SF=s(U,"LI",{});var i0e=n(SF);ffe=s(i0e,"STRONG",{});var ist=n(ffe);CYo=r(ist,"camembert"),ist.forEach(t),MYo=r(i0e," \u2014 "),fq=s(i0e,"A",{href:!0});var dst=n(fq);EYo=r(dst,"TFCamembertForMaskedLM"),dst.forEach(t),yYo=r(i0e," (CamemBERT model)"),i0e.forEach(t),wYo=i(U),PF=s(U,"LI",{});var d0e=n(PF);gfe=s(d0e,"STRONG",{});var cst=n(gfe);AYo=r(cst,"ctrl"),cst.forEach(t),LYo=r(d0e," \u2014 "),gq=s(d0e,"A",{href:!0});var mst=n(gq);BYo=r(mst,"TFCTRLLMHeadModel"),mst.forEach(t),xYo=r(d0e," (CTRL model)"),d0e.forEach(t),kYo=i(U),$F=s(U,"LI",{});var c0e=n($F);hfe=s(c0e,"STRONG",{});var fst=n(hfe);RYo=r(fst,"distilbert"),fst.forEach(t),SYo=r(c0e," \u2014 "),hq=s(c0e,"A",{href:!0});var gst=n(hq);PYo=r(gst,"TFDistilBertForMaskedLM"),gst.forEach(t),$Yo=r(c0e," (DistilBERT model)"),c0e.forEach(t),IYo=i(U),IF=s(U,"LI",{});var m0e=n(IF);ufe=s(m0e,"STRONG",{});var hst=n(ufe);DYo=r(hst,"electra"),hst.forEach(t),jYo=r(m0e," \u2014 "),uq=s(m0e,"A",{href:!0});var ust=n(uq);NYo=r(ust,"TFElectraForPreTraining"),ust.forEach(t),qYo=r(m0e," (ELECTRA model)"),m0e.forEach(t),GYo=i(U),DF=s(U,"LI",{});var f0e=n(DF);pfe=s(f0e,"STRONG",{});var pst=n(pfe);OYo=r(pst,"flaubert"),pst.forEach(t),XYo=r(f0e," \u2014 "),pq=s(f0e,"A",{href:!0});var _st=n(pq);VYo=r(_st,"TFFlaubertWithLMHeadModel"),_st.forEach(t),zYo=r(f0e," (FlauBERT model)"),f0e.forEach(t),WYo=i(U),jF=s(U,"LI",{});var g0e=n(jF);_fe=s(g0e,"STRONG",{});var bst=n(_fe);QYo=r(bst,"funnel"),bst.forEach(t),HYo=r(g0e," \u2014 "),_q=s(g0e,"A",{href:!0});var vst=n(_q);UYo=r(vst,"TFFunnelForPreTraining"),vst.forEach(t),JYo=r(g0e," (Funnel Transformer model)"),g0e.forEach(t),YYo=i(U),NF=s(U,"LI",{});var h0e=n(NF);bfe=s(h0e,"STRONG",{});var Tst=n(bfe);KYo=r(Tst,"gpt2"),Tst.forEach(t),ZYo=r(h0e," \u2014 "),bq=s(h0e,"A",{href:!0});var Fst=n(bq);eKo=r(Fst,"TFGPT2LMHeadModel"),Fst.forEach(t),oKo=r(h0e," (OpenAI GPT-2 model)"),h0e.forEach(t),rKo=i(U),qF=s(U,"LI",{});var u0e=n(qF);vfe=s(u0e,"STRONG",{});var Cst=n(vfe);tKo=r(Cst,"layoutlm"),Cst.forEach(t),aKo=r(u0e," \u2014 "),vq=s(u0e,"A",{href:!0});var Mst=n(vq);sKo=r(Mst,"TFLayoutLMForMaskedLM"),Mst.forEach(t),nKo=r(u0e," (LayoutLM model)"),u0e.forEach(t),lKo=i(U),GF=s(U,"LI",{});var p0e=n(GF);Tfe=s(p0e,"STRONG",{});var Est=n(Tfe);iKo=r(Est,"lxmert"),Est.forEach(t),dKo=r(p0e," \u2014 "),Tq=s(p0e,"A",{href:!0});var yst=n(Tq);cKo=r(yst,"TFLxmertForPreTraining"),yst.forEach(t),mKo=r(p0e," (LXMERT model)"),p0e.forEach(t),fKo=i(U),OF=s(U,"LI",{});var _0e=n(OF);Ffe=s(_0e,"STRONG",{});var wst=n(Ffe);gKo=r(wst,"mobilebert"),wst.forEach(t),hKo=r(_0e," \u2014 "),Fq=s(_0e,"A",{href:!0});var Ast=n(Fq);uKo=r(Ast,"TFMobileBertForPreTraining"),Ast.forEach(t),pKo=r(_0e," (MobileBERT model)"),_0e.forEach(t),_Ko=i(U),XF=s(U,"LI",{});var b0e=n(XF);Cfe=s(b0e,"STRONG",{});var Lst=n(Cfe);bKo=r(Lst,"mpnet"),Lst.forEach(t),vKo=r(b0e," \u2014 "),Cq=s(b0e,"A",{href:!0});var Bst=n(Cq);TKo=r(Bst,"TFMPNetForMaskedLM"),Bst.forEach(t),FKo=r(b0e," (MPNet model)"),b0e.forEach(t),CKo=i(U),VF=s(U,"LI",{});var v0e=n(VF);Mfe=s(v0e,"STRONG",{});var xst=n(Mfe);MKo=r(xst,"openai-gpt"),xst.forEach(t),EKo=r(v0e," \u2014 "),Mq=s(v0e,"A",{href:!0});var kst=n(Mq);yKo=r(kst,"TFOpenAIGPTLMHeadModel"),kst.forEach(t),wKo=r(v0e," (OpenAI GPT model)"),v0e.forEach(t),AKo=i(U),zF=s(U,"LI",{});var T0e=n(zF);Efe=s(T0e,"STRONG",{});var Rst=n(Efe);LKo=r(Rst,"roberta"),Rst.forEach(t),BKo=r(T0e," \u2014 "),Eq=s(T0e,"A",{href:!0});var Sst=n(Eq);xKo=r(Sst,"TFRobertaForMaskedLM"),Sst.forEach(t),kKo=r(T0e," (RoBERTa model)"),T0e.forEach(t),RKo=i(U),WF=s(U,"LI",{});var F0e=n(WF);yfe=s(F0e,"STRONG",{});var Pst=n(yfe);SKo=r(Pst,"t5"),Pst.forEach(t),PKo=r(F0e," \u2014 "),yq=s(F0e,"A",{href:!0});var $st=n(yq);$Ko=r($st,"TFT5ForConditionalGeneration"),$st.forEach(t),IKo=r(F0e," (T5 model)"),F0e.forEach(t),DKo=i(U),QF=s(U,"LI",{});var C0e=n(QF);wfe=s(C0e,"STRONG",{});var Ist=n(wfe);jKo=r(Ist,"tapas"),Ist.forEach(t),NKo=r(C0e," \u2014 "),wq=s(C0e,"A",{href:!0});var Dst=n(wq);qKo=r(Dst,"TFTapasForMaskedLM"),Dst.forEach(t),GKo=r(C0e," (TAPAS model)"),C0e.forEach(t),OKo=i(U),HF=s(U,"LI",{});var M0e=n(HF);Afe=s(M0e,"STRONG",{});var jst=n(Afe);XKo=r(jst,"transfo-xl"),jst.forEach(t),VKo=r(M0e," \u2014 "),Aq=s(M0e,"A",{href:!0});var Nst=n(Aq);zKo=r(Nst,"TFTransfoXLLMHeadModel"),Nst.forEach(t),WKo=r(M0e," (Transformer-XL model)"),M0e.forEach(t),QKo=i(U),UF=s(U,"LI",{});var E0e=n(UF);Lfe=s(E0e,"STRONG",{});var qst=n(Lfe);HKo=r(qst,"xlm"),qst.forEach(t),UKo=r(E0e," \u2014 "),Lq=s(E0e,"A",{href:!0});var Gst=n(Lq);JKo=r(Gst,"TFXLMWithLMHeadModel"),Gst.forEach(t),YKo=r(E0e," (XLM model)"),E0e.forEach(t),KKo=i(U),JF=s(U,"LI",{});var y0e=n(JF);Bfe=s(y0e,"STRONG",{});var Ost=n(Bfe);ZKo=r(Ost,"xlm-roberta"),Ost.forEach(t),eZo=r(y0e," \u2014 "),Bq=s(y0e,"A",{href:!0});var Xst=n(Bq);oZo=r(Xst,"TFXLMRobertaForMaskedLM"),Xst.forEach(t),rZo=r(y0e," (XLM-RoBERTa model)"),y0e.forEach(t),tZo=i(U),YF=s(U,"LI",{});var w0e=n(YF);xfe=s(w0e,"STRONG",{});var Vst=n(xfe);aZo=r(Vst,"xlnet"),Vst.forEach(t),sZo=r(w0e," \u2014 "),xq=s(w0e,"A",{href:!0});var zst=n(xq);nZo=r(zst,"TFXLNetLMHeadModel"),zst.forEach(t),lZo=r(w0e," (XLNet model)"),w0e.forEach(t),U.forEach(t),iZo=i(ma),kfe=s(ma,"P",{});var Wst=n(kfe);dZo=r(Wst,"Examples:"),Wst.forEach(t),cZo=i(ma),f(cA.$$.fragment,ma),ma.forEach(t),Pl.forEach(t),DBe=i(c),ic=s(c,"H2",{class:!0});var Wke=n(ic);KF=s(Wke,"A",{id:!0,class:!0,href:!0});var Qst=n(KF);Rfe=s(Qst,"SPAN",{});var Hst=n(Rfe);f(mA.$$.fragment,Hst),Hst.forEach(t),Qst.forEach(t),mZo=i(Wke),Sfe=s(Wke,"SPAN",{});var Ust=n(Sfe);fZo=r(Ust,"TFAutoModelForCausalLM"),Ust.forEach(t),Wke.forEach(t),jBe=i(c),_r=s(c,"DIV",{class:!0});var Il=n(_r);f(fA.$$.fragment,Il),gZo=i(Il),dc=s(Il,"P",{});var Rz=n(dc);hZo=r(Rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Pfe=s(Rz,"CODE",{});var Jst=n(Pfe);uZo=r(Jst,"from_pretrained()"),Jst.forEach(t),pZo=r(Rz,"class method or the "),$fe=s(Rz,"CODE",{});var Yst=n($fe);_Zo=r(Yst,"from_config()"),Yst.forEach(t),bZo=r(Rz,`class
method.`),Rz.forEach(t),vZo=i(Il),gA=s(Il,"P",{});var Qke=n(gA);TZo=r(Qke,"This class cannot be instantiated directly using "),Ife=s(Qke,"CODE",{});var Kst=n(Ife);FZo=r(Kst,"__init__()"),Kst.forEach(t),CZo=r(Qke," (throws an error)."),Qke.forEach(t),MZo=i(Il),ct=s(Il,"DIV",{class:!0});var Dl=n(ct);f(hA.$$.fragment,Dl),EZo=i(Dl),Dfe=s(Dl,"P",{});var Zst=n(Dfe);yZo=r(Zst,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Zst.forEach(t),wZo=i(Dl),cc=s(Dl,"P",{});var Sz=n(cc);AZo=r(Sz,`Note:
Loading a model from its configuration file does `),jfe=s(Sz,"STRONG",{});var ent=n(jfe);LZo=r(ent,"not"),ent.forEach(t),BZo=r(Sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Nfe=s(Sz,"CODE",{});var ont=n(Nfe);xZo=r(ont,"from_pretrained()"),ont.forEach(t),kZo=r(Sz,"to load the model weights."),Sz.forEach(t),RZo=i(Dl),qfe=s(Dl,"P",{});var rnt=n(qfe);SZo=r(rnt,"Examples:"),rnt.forEach(t),PZo=i(Dl),f(uA.$$.fragment,Dl),Dl.forEach(t),$Zo=i(Il),uo=s(Il,"DIV",{class:!0});var fa=n(uo);f(pA.$$.fragment,fa),IZo=i(fa),Gfe=s(fa,"P",{});var tnt=n(Gfe);DZo=r(tnt,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),tnt.forEach(t),jZo=i(fa),ms=s(fa,"P",{});var Z3=n(ms);NZo=r(Z3,"The model class to instantiate is selected based on the "),Ofe=s(Z3,"CODE",{});var ant=n(Ofe);qZo=r(ant,"model_type"),ant.forEach(t),GZo=r(Z3,` property of the config object (either
passed as an argument or loaded from `),Xfe=s(Z3,"CODE",{});var snt=n(Xfe);OZo=r(snt,"pretrained_model_name_or_path"),snt.forEach(t),XZo=r(Z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vfe=s(Z3,"CODE",{});var nnt=n(Vfe);VZo=r(nnt,"pretrained_model_name_or_path"),nnt.forEach(t),zZo=r(Z3,":"),Z3.forEach(t),WZo=i(fa),he=s(fa,"UL",{});var Me=n(he);ZF=s(Me,"LI",{});var A0e=n(ZF);zfe=s(A0e,"STRONG",{});var lnt=n(zfe);QZo=r(lnt,"bert"),lnt.forEach(t),HZo=r(A0e," \u2014 "),kq=s(A0e,"A",{href:!0});var int=n(kq);UZo=r(int,"TFBertLMHeadModel"),int.forEach(t),JZo=r(A0e," (BERT model)"),A0e.forEach(t),YZo=i(Me),eC=s(Me,"LI",{});var L0e=n(eC);Wfe=s(L0e,"STRONG",{});var dnt=n(Wfe);KZo=r(dnt,"ctrl"),dnt.forEach(t),ZZo=r(L0e," \u2014 "),Rq=s(L0e,"A",{href:!0});var cnt=n(Rq);eer=r(cnt,"TFCTRLLMHeadModel"),cnt.forEach(t),oer=r(L0e," (CTRL model)"),L0e.forEach(t),rer=i(Me),oC=s(Me,"LI",{});var B0e=n(oC);Qfe=s(B0e,"STRONG",{});var mnt=n(Qfe);ter=r(mnt,"gpt2"),mnt.forEach(t),aer=r(B0e," \u2014 "),Sq=s(B0e,"A",{href:!0});var fnt=n(Sq);ser=r(fnt,"TFGPT2LMHeadModel"),fnt.forEach(t),ner=r(B0e," (OpenAI GPT-2 model)"),B0e.forEach(t),ler=i(Me),rC=s(Me,"LI",{});var x0e=n(rC);Hfe=s(x0e,"STRONG",{});var gnt=n(Hfe);ier=r(gnt,"openai-gpt"),gnt.forEach(t),der=r(x0e," \u2014 "),Pq=s(x0e,"A",{href:!0});var hnt=n(Pq);cer=r(hnt,"TFOpenAIGPTLMHeadModel"),hnt.forEach(t),mer=r(x0e," (OpenAI GPT model)"),x0e.forEach(t),fer=i(Me),tC=s(Me,"LI",{});var k0e=n(tC);Ufe=s(k0e,"STRONG",{});var unt=n(Ufe);ger=r(unt,"rembert"),unt.forEach(t),her=r(k0e," \u2014 "),$q=s(k0e,"A",{href:!0});var pnt=n($q);uer=r(pnt,"TFRemBertForCausalLM"),pnt.forEach(t),per=r(k0e," (RemBERT model)"),k0e.forEach(t),_er=i(Me),aC=s(Me,"LI",{});var R0e=n(aC);Jfe=s(R0e,"STRONG",{});var _nt=n(Jfe);ber=r(_nt,"roberta"),_nt.forEach(t),ver=r(R0e," \u2014 "),Iq=s(R0e,"A",{href:!0});var bnt=n(Iq);Ter=r(bnt,"TFRobertaForCausalLM"),bnt.forEach(t),Fer=r(R0e," (RoBERTa model)"),R0e.forEach(t),Cer=i(Me),sC=s(Me,"LI",{});var S0e=n(sC);Yfe=s(S0e,"STRONG",{});var vnt=n(Yfe);Mer=r(vnt,"roformer"),vnt.forEach(t),Eer=r(S0e," \u2014 "),Dq=s(S0e,"A",{href:!0});var Tnt=n(Dq);yer=r(Tnt,"TFRoFormerForCausalLM"),Tnt.forEach(t),wer=r(S0e," (RoFormer model)"),S0e.forEach(t),Aer=i(Me),nC=s(Me,"LI",{});var P0e=n(nC);Kfe=s(P0e,"STRONG",{});var Fnt=n(Kfe);Ler=r(Fnt,"transfo-xl"),Fnt.forEach(t),Ber=r(P0e," \u2014 "),jq=s(P0e,"A",{href:!0});var Cnt=n(jq);xer=r(Cnt,"TFTransfoXLLMHeadModel"),Cnt.forEach(t),ker=r(P0e," (Transformer-XL model)"),P0e.forEach(t),Rer=i(Me),lC=s(Me,"LI",{});var $0e=n(lC);Zfe=s($0e,"STRONG",{});var Mnt=n(Zfe);Ser=r(Mnt,"xlm"),Mnt.forEach(t),Per=r($0e," \u2014 "),Nq=s($0e,"A",{href:!0});var Ent=n(Nq);$er=r(Ent,"TFXLMWithLMHeadModel"),Ent.forEach(t),Ier=r($0e," (XLM model)"),$0e.forEach(t),Der=i(Me),iC=s(Me,"LI",{});var I0e=n(iC);ege=s(I0e,"STRONG",{});var ynt=n(ege);jer=r(ynt,"xlnet"),ynt.forEach(t),Ner=r(I0e," \u2014 "),qq=s(I0e,"A",{href:!0});var wnt=n(qq);qer=r(wnt,"TFXLNetLMHeadModel"),wnt.forEach(t),Ger=r(I0e," (XLNet model)"),I0e.forEach(t),Me.forEach(t),Oer=i(fa),oge=s(fa,"P",{});var Ant=n(oge);Xer=r(Ant,"Examples:"),Ant.forEach(t),Ver=i(fa),f(_A.$$.fragment,fa),fa.forEach(t),Il.forEach(t),NBe=i(c),mc=s(c,"H2",{class:!0});var Hke=n(mc);dC=s(Hke,"A",{id:!0,class:!0,href:!0});var Lnt=n(dC);rge=s(Lnt,"SPAN",{});var Bnt=n(rge);f(bA.$$.fragment,Bnt),Bnt.forEach(t),Lnt.forEach(t),zer=i(Hke),tge=s(Hke,"SPAN",{});var xnt=n(tge);Wer=r(xnt,"TFAutoModelForImageClassification"),xnt.forEach(t),Hke.forEach(t),qBe=i(c),br=s(c,"DIV",{class:!0});var jl=n(br);f(vA.$$.fragment,jl),Qer=i(jl),fc=s(jl,"P",{});var Pz=n(fc);Her=r(Pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),age=s(Pz,"CODE",{});var knt=n(age);Uer=r(knt,"from_pretrained()"),knt.forEach(t),Jer=r(Pz,"class method or the "),sge=s(Pz,"CODE",{});var Rnt=n(sge);Yer=r(Rnt,"from_config()"),Rnt.forEach(t),Ker=r(Pz,`class
method.`),Pz.forEach(t),Zer=i(jl),TA=s(jl,"P",{});var Uke=n(TA);eor=r(Uke,"This class cannot be instantiated directly using "),nge=s(Uke,"CODE",{});var Snt=n(nge);oor=r(Snt,"__init__()"),Snt.forEach(t),ror=r(Uke," (throws an error)."),Uke.forEach(t),tor=i(jl),mt=s(jl,"DIV",{class:!0});var Nl=n(mt);f(FA.$$.fragment,Nl),aor=i(Nl),lge=s(Nl,"P",{});var Pnt=n(lge);sor=r(Pnt,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Pnt.forEach(t),nor=i(Nl),gc=s(Nl,"P",{});var $z=n(gc);lor=r($z,`Note:
Loading a model from its configuration file does `),ige=s($z,"STRONG",{});var $nt=n(ige);ior=r($nt,"not"),$nt.forEach(t),dor=r($z,` load the model weights. It only affects the
model\u2019s configuration. Use `),dge=s($z,"CODE",{});var Int=n(dge);cor=r(Int,"from_pretrained()"),Int.forEach(t),mor=r($z,"to load the model weights."),$z.forEach(t),gor=i(Nl),cge=s(Nl,"P",{});var Dnt=n(cge);hor=r(Dnt,"Examples:"),Dnt.forEach(t),uor=i(Nl),f(CA.$$.fragment,Nl),Nl.forEach(t),por=i(jl),po=s(jl,"DIV",{class:!0});var ga=n(po);f(MA.$$.fragment,ga),_or=i(ga),mge=s(ga,"P",{});var jnt=n(mge);bor=r(jnt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),jnt.forEach(t),vor=i(ga),fs=s(ga,"P",{});var e5=n(fs);Tor=r(e5,"The model class to instantiate is selected based on the "),fge=s(e5,"CODE",{});var Nnt=n(fge);For=r(Nnt,"model_type"),Nnt.forEach(t),Cor=r(e5,` property of the config object (either
passed as an argument or loaded from `),gge=s(e5,"CODE",{});var qnt=n(gge);Mor=r(qnt,"pretrained_model_name_or_path"),qnt.forEach(t),Eor=r(e5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hge=s(e5,"CODE",{});var Gnt=n(hge);yor=r(Gnt,"pretrained_model_name_or_path"),Gnt.forEach(t),wor=r(e5,":"),e5.forEach(t),Aor=i(ga),EA=s(ga,"UL",{});var Jke=n(EA);cC=s(Jke,"LI",{});var D0e=n(cC);uge=s(D0e,"STRONG",{});var Ont=n(uge);Lor=r(Ont,"convnext"),Ont.forEach(t),Bor=r(D0e," \u2014 "),Gq=s(D0e,"A",{href:!0});var Xnt=n(Gq);xor=r(Xnt,"TFConvNextForImageClassification"),Xnt.forEach(t),kor=r(D0e," (ConvNext model)"),D0e.forEach(t),Ror=i(Jke),mC=s(Jke,"LI",{});var j0e=n(mC);pge=s(j0e,"STRONG",{});var Vnt=n(pge);Sor=r(Vnt,"vit"),Vnt.forEach(t),Por=r(j0e," \u2014 "),Oq=s(j0e,"A",{href:!0});var znt=n(Oq);$or=r(znt,"TFViTForImageClassification"),znt.forEach(t),Ior=r(j0e," (ViT model)"),j0e.forEach(t),Jke.forEach(t),Dor=i(ga),_ge=s(ga,"P",{});var Wnt=n(_ge);jor=r(Wnt,"Examples:"),Wnt.forEach(t),Nor=i(ga),f(yA.$$.fragment,ga),ga.forEach(t),jl.forEach(t),GBe=i(c),hc=s(c,"H2",{class:!0});var Yke=n(hc);fC=s(Yke,"A",{id:!0,class:!0,href:!0});var Qnt=n(fC);bge=s(Qnt,"SPAN",{});var Hnt=n(bge);f(wA.$$.fragment,Hnt),Hnt.forEach(t),Qnt.forEach(t),qor=i(Yke),vge=s(Yke,"SPAN",{});var Unt=n(vge);Gor=r(Unt,"TFAutoModelForMaskedLM"),Unt.forEach(t),Yke.forEach(t),OBe=i(c),vr=s(c,"DIV",{class:!0});var ql=n(vr);f(AA.$$.fragment,ql),Oor=i(ql),uc=s(ql,"P",{});var Iz=n(uc);Xor=r(Iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Tge=s(Iz,"CODE",{});var Jnt=n(Tge);Vor=r(Jnt,"from_pretrained()"),Jnt.forEach(t),zor=r(Iz,"class method or the "),Fge=s(Iz,"CODE",{});var Ynt=n(Fge);Wor=r(Ynt,"from_config()"),Ynt.forEach(t),Qor=r(Iz,`class
method.`),Iz.forEach(t),Hor=i(ql),LA=s(ql,"P",{});var Kke=n(LA);Uor=r(Kke,"This class cannot be instantiated directly using "),Cge=s(Kke,"CODE",{});var Knt=n(Cge);Jor=r(Knt,"__init__()"),Knt.forEach(t),Yor=r(Kke," (throws an error)."),Kke.forEach(t),Kor=i(ql),ft=s(ql,"DIV",{class:!0});var Gl=n(ft);f(BA.$$.fragment,Gl),Zor=i(Gl),Mge=s(Gl,"P",{});var Znt=n(Mge);err=r(Znt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Znt.forEach(t),orr=i(Gl),pc=s(Gl,"P",{});var Dz=n(pc);rrr=r(Dz,`Note:
Loading a model from its configuration file does `),Ege=s(Dz,"STRONG",{});var elt=n(Ege);trr=r(elt,"not"),elt.forEach(t),arr=r(Dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),yge=s(Dz,"CODE",{});var olt=n(yge);srr=r(olt,"from_pretrained()"),olt.forEach(t),nrr=r(Dz,"to load the model weights."),Dz.forEach(t),lrr=i(Gl),wge=s(Gl,"P",{});var rlt=n(wge);irr=r(rlt,"Examples:"),rlt.forEach(t),drr=i(Gl),f(xA.$$.fragment,Gl),Gl.forEach(t),crr=i(ql),_o=s(ql,"DIV",{class:!0});var ha=n(_o);f(kA.$$.fragment,ha),mrr=i(ha),Age=s(ha,"P",{});var tlt=n(Age);frr=r(tlt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),tlt.forEach(t),grr=i(ha),gs=s(ha,"P",{});var o5=n(gs);hrr=r(o5,"The model class to instantiate is selected based on the "),Lge=s(o5,"CODE",{});var alt=n(Lge);urr=r(alt,"model_type"),alt.forEach(t),prr=r(o5,` property of the config object (either
passed as an argument or loaded from `),Bge=s(o5,"CODE",{});var slt=n(Bge);_rr=r(slt,"pretrained_model_name_or_path"),slt.forEach(t),brr=r(o5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xge=s(o5,"CODE",{});var nlt=n(xge);vrr=r(nlt,"pretrained_model_name_or_path"),nlt.forEach(t),Trr=r(o5,":"),o5.forEach(t),Frr=i(ha),Y=s(ha,"UL",{});var ee=n(Y);gC=s(ee,"LI",{});var N0e=n(gC);kge=s(N0e,"STRONG",{});var llt=n(kge);Crr=r(llt,"albert"),llt.forEach(t),Mrr=r(N0e," \u2014 "),Xq=s(N0e,"A",{href:!0});var ilt=n(Xq);Err=r(ilt,"TFAlbertForMaskedLM"),ilt.forEach(t),yrr=r(N0e," (ALBERT model)"),N0e.forEach(t),wrr=i(ee),hC=s(ee,"LI",{});var q0e=n(hC);Rge=s(q0e,"STRONG",{});var dlt=n(Rge);Arr=r(dlt,"bert"),dlt.forEach(t),Lrr=r(q0e," \u2014 "),Vq=s(q0e,"A",{href:!0});var clt=n(Vq);Brr=r(clt,"TFBertForMaskedLM"),clt.forEach(t),xrr=r(q0e," (BERT model)"),q0e.forEach(t),krr=i(ee),uC=s(ee,"LI",{});var G0e=n(uC);Sge=s(G0e,"STRONG",{});var mlt=n(Sge);Rrr=r(mlt,"camembert"),mlt.forEach(t),Srr=r(G0e," \u2014 "),zq=s(G0e,"A",{href:!0});var flt=n(zq);Prr=r(flt,"TFCamembertForMaskedLM"),flt.forEach(t),$rr=r(G0e," (CamemBERT model)"),G0e.forEach(t),Irr=i(ee),pC=s(ee,"LI",{});var O0e=n(pC);Pge=s(O0e,"STRONG",{});var glt=n(Pge);Drr=r(glt,"convbert"),glt.forEach(t),jrr=r(O0e," \u2014 "),Wq=s(O0e,"A",{href:!0});var hlt=n(Wq);Nrr=r(hlt,"TFConvBertForMaskedLM"),hlt.forEach(t),qrr=r(O0e," (ConvBERT model)"),O0e.forEach(t),Grr=i(ee),_C=s(ee,"LI",{});var X0e=n(_C);$ge=s(X0e,"STRONG",{});var ult=n($ge);Orr=r(ult,"deberta"),ult.forEach(t),Xrr=r(X0e," \u2014 "),Qq=s(X0e,"A",{href:!0});var plt=n(Qq);Vrr=r(plt,"TFDebertaForMaskedLM"),plt.forEach(t),zrr=r(X0e," (DeBERTa model)"),X0e.forEach(t),Wrr=i(ee),bC=s(ee,"LI",{});var V0e=n(bC);Ige=s(V0e,"STRONG",{});var _lt=n(Ige);Qrr=r(_lt,"deberta-v2"),_lt.forEach(t),Hrr=r(V0e," \u2014 "),Hq=s(V0e,"A",{href:!0});var blt=n(Hq);Urr=r(blt,"TFDebertaV2ForMaskedLM"),blt.forEach(t),Jrr=r(V0e," (DeBERTa-v2 model)"),V0e.forEach(t),Yrr=i(ee),vC=s(ee,"LI",{});var z0e=n(vC);Dge=s(z0e,"STRONG",{});var vlt=n(Dge);Krr=r(vlt,"distilbert"),vlt.forEach(t),Zrr=r(z0e," \u2014 "),Uq=s(z0e,"A",{href:!0});var Tlt=n(Uq);etr=r(Tlt,"TFDistilBertForMaskedLM"),Tlt.forEach(t),otr=r(z0e," (DistilBERT model)"),z0e.forEach(t),rtr=i(ee),TC=s(ee,"LI",{});var W0e=n(TC);jge=s(W0e,"STRONG",{});var Flt=n(jge);ttr=r(Flt,"electra"),Flt.forEach(t),atr=r(W0e," \u2014 "),Jq=s(W0e,"A",{href:!0});var Clt=n(Jq);str=r(Clt,"TFElectraForMaskedLM"),Clt.forEach(t),ntr=r(W0e," (ELECTRA model)"),W0e.forEach(t),ltr=i(ee),FC=s(ee,"LI",{});var Q0e=n(FC);Nge=s(Q0e,"STRONG",{});var Mlt=n(Nge);itr=r(Mlt,"flaubert"),Mlt.forEach(t),dtr=r(Q0e," \u2014 "),Yq=s(Q0e,"A",{href:!0});var Elt=n(Yq);ctr=r(Elt,"TFFlaubertWithLMHeadModel"),Elt.forEach(t),mtr=r(Q0e," (FlauBERT model)"),Q0e.forEach(t),ftr=i(ee),CC=s(ee,"LI",{});var H0e=n(CC);qge=s(H0e,"STRONG",{});var ylt=n(qge);gtr=r(ylt,"funnel"),ylt.forEach(t),htr=r(H0e," \u2014 "),Kq=s(H0e,"A",{href:!0});var wlt=n(Kq);utr=r(wlt,"TFFunnelForMaskedLM"),wlt.forEach(t),ptr=r(H0e," (Funnel Transformer model)"),H0e.forEach(t),_tr=i(ee),MC=s(ee,"LI",{});var U0e=n(MC);Gge=s(U0e,"STRONG",{});var Alt=n(Gge);btr=r(Alt,"layoutlm"),Alt.forEach(t),vtr=r(U0e," \u2014 "),Zq=s(U0e,"A",{href:!0});var Llt=n(Zq);Ttr=r(Llt,"TFLayoutLMForMaskedLM"),Llt.forEach(t),Ftr=r(U0e," (LayoutLM model)"),U0e.forEach(t),Ctr=i(ee),EC=s(ee,"LI",{});var J0e=n(EC);Oge=s(J0e,"STRONG",{});var Blt=n(Oge);Mtr=r(Blt,"longformer"),Blt.forEach(t),Etr=r(J0e," \u2014 "),eG=s(J0e,"A",{href:!0});var xlt=n(eG);ytr=r(xlt,"TFLongformerForMaskedLM"),xlt.forEach(t),wtr=r(J0e," (Longformer model)"),J0e.forEach(t),Atr=i(ee),yC=s(ee,"LI",{});var Y0e=n(yC);Xge=s(Y0e,"STRONG",{});var klt=n(Xge);Ltr=r(klt,"mobilebert"),klt.forEach(t),Btr=r(Y0e," \u2014 "),oG=s(Y0e,"A",{href:!0});var Rlt=n(oG);xtr=r(Rlt,"TFMobileBertForMaskedLM"),Rlt.forEach(t),ktr=r(Y0e," (MobileBERT model)"),Y0e.forEach(t),Rtr=i(ee),wC=s(ee,"LI",{});var K0e=n(wC);Vge=s(K0e,"STRONG",{});var Slt=n(Vge);Str=r(Slt,"mpnet"),Slt.forEach(t),Ptr=r(K0e," \u2014 "),rG=s(K0e,"A",{href:!0});var Plt=n(rG);$tr=r(Plt,"TFMPNetForMaskedLM"),Plt.forEach(t),Itr=r(K0e," (MPNet model)"),K0e.forEach(t),Dtr=i(ee),AC=s(ee,"LI",{});var Z0e=n(AC);zge=s(Z0e,"STRONG",{});var $lt=n(zge);jtr=r($lt,"rembert"),$lt.forEach(t),Ntr=r(Z0e," \u2014 "),tG=s(Z0e,"A",{href:!0});var Ilt=n(tG);qtr=r(Ilt,"TFRemBertForMaskedLM"),Ilt.forEach(t),Gtr=r(Z0e," (RemBERT model)"),Z0e.forEach(t),Otr=i(ee),LC=s(ee,"LI",{});var eLe=n(LC);Wge=s(eLe,"STRONG",{});var Dlt=n(Wge);Xtr=r(Dlt,"roberta"),Dlt.forEach(t),Vtr=r(eLe," \u2014 "),aG=s(eLe,"A",{href:!0});var jlt=n(aG);ztr=r(jlt,"TFRobertaForMaskedLM"),jlt.forEach(t),Wtr=r(eLe," (RoBERTa model)"),eLe.forEach(t),Qtr=i(ee),BC=s(ee,"LI",{});var oLe=n(BC);Qge=s(oLe,"STRONG",{});var Nlt=n(Qge);Htr=r(Nlt,"roformer"),Nlt.forEach(t),Utr=r(oLe," \u2014 "),sG=s(oLe,"A",{href:!0});var qlt=n(sG);Jtr=r(qlt,"TFRoFormerForMaskedLM"),qlt.forEach(t),Ytr=r(oLe," (RoFormer model)"),oLe.forEach(t),Ktr=i(ee),xC=s(ee,"LI",{});var rLe=n(xC);Hge=s(rLe,"STRONG",{});var Glt=n(Hge);Ztr=r(Glt,"tapas"),Glt.forEach(t),ear=r(rLe," \u2014 "),nG=s(rLe,"A",{href:!0});var Olt=n(nG);oar=r(Olt,"TFTapasForMaskedLM"),Olt.forEach(t),rar=r(rLe," (TAPAS model)"),rLe.forEach(t),tar=i(ee),kC=s(ee,"LI",{});var tLe=n(kC);Uge=s(tLe,"STRONG",{});var Xlt=n(Uge);aar=r(Xlt,"xlm"),Xlt.forEach(t),sar=r(tLe," \u2014 "),lG=s(tLe,"A",{href:!0});var Vlt=n(lG);nar=r(Vlt,"TFXLMWithLMHeadModel"),Vlt.forEach(t),lar=r(tLe," (XLM model)"),tLe.forEach(t),iar=i(ee),RC=s(ee,"LI",{});var aLe=n(RC);Jge=s(aLe,"STRONG",{});var zlt=n(Jge);dar=r(zlt,"xlm-roberta"),zlt.forEach(t),car=r(aLe," \u2014 "),iG=s(aLe,"A",{href:!0});var Wlt=n(iG);mar=r(Wlt,"TFXLMRobertaForMaskedLM"),Wlt.forEach(t),far=r(aLe," (XLM-RoBERTa model)"),aLe.forEach(t),ee.forEach(t),gar=i(ha),Yge=s(ha,"P",{});var Qlt=n(Yge);har=r(Qlt,"Examples:"),Qlt.forEach(t),uar=i(ha),f(RA.$$.fragment,ha),ha.forEach(t),ql.forEach(t),XBe=i(c),_c=s(c,"H2",{class:!0});var Zke=n(_c);SC=s(Zke,"A",{id:!0,class:!0,href:!0});var Hlt=n(SC);Kge=s(Hlt,"SPAN",{});var Ult=n(Kge);f(SA.$$.fragment,Ult),Ult.forEach(t),Hlt.forEach(t),par=i(Zke),Zge=s(Zke,"SPAN",{});var Jlt=n(Zge);_ar=r(Jlt,"TFAutoModelForSeq2SeqLM"),Jlt.forEach(t),Zke.forEach(t),VBe=i(c),Tr=s(c,"DIV",{class:!0});var Ol=n(Tr);f(PA.$$.fragment,Ol),bar=i(Ol),bc=s(Ol,"P",{});var jz=n(bc);Tar=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ehe=s(jz,"CODE",{});var Ylt=n(ehe);Far=r(Ylt,"from_pretrained()"),Ylt.forEach(t),Car=r(jz,"class method or the "),ohe=s(jz,"CODE",{});var Klt=n(ohe);Mar=r(Klt,"from_config()"),Klt.forEach(t),Ear=r(jz,`class
method.`),jz.forEach(t),yar=i(Ol),$A=s(Ol,"P",{});var eRe=n($A);war=r(eRe,"This class cannot be instantiated directly using "),rhe=s(eRe,"CODE",{});var Zlt=n(rhe);Aar=r(Zlt,"__init__()"),Zlt.forEach(t),Lar=r(eRe," (throws an error)."),eRe.forEach(t),Bar=i(Ol),gt=s(Ol,"DIV",{class:!0});var Xl=n(gt);f(IA.$$.fragment,Xl),xar=i(Xl),the=s(Xl,"P",{});var eit=n(the);kar=r(eit,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),eit.forEach(t),Rar=i(Xl),vc=s(Xl,"P",{});var Nz=n(vc);Sar=r(Nz,`Note:
Loading a model from its configuration file does `),ahe=s(Nz,"STRONG",{});var oit=n(ahe);Par=r(oit,"not"),oit.forEach(t),$ar=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),she=s(Nz,"CODE",{});var rit=n(she);Iar=r(rit,"from_pretrained()"),rit.forEach(t),Dar=r(Nz,"to load the model weights."),Nz.forEach(t),jar=i(Xl),nhe=s(Xl,"P",{});var tit=n(nhe);Nar=r(tit,"Examples:"),tit.forEach(t),qar=i(Xl),f(DA.$$.fragment,Xl),Xl.forEach(t),Gar=i(Ol),bo=s(Ol,"DIV",{class:!0});var ua=n(bo);f(jA.$$.fragment,ua),Oar=i(ua),lhe=s(ua,"P",{});var ait=n(lhe);Xar=r(ait,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),ait.forEach(t),Var=i(ua),hs=s(ua,"P",{});var r5=n(hs);zar=r(r5,"The model class to instantiate is selected based on the "),ihe=s(r5,"CODE",{});var sit=n(ihe);War=r(sit,"model_type"),sit.forEach(t),Qar=r(r5,` property of the config object (either
passed as an argument or loaded from `),dhe=s(r5,"CODE",{});var nit=n(dhe);Har=r(nit,"pretrained_model_name_or_path"),nit.forEach(t),Uar=r(r5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),che=s(r5,"CODE",{});var lit=n(che);Jar=r(lit,"pretrained_model_name_or_path"),lit.forEach(t),Yar=r(r5,":"),r5.forEach(t),Kar=i(ua),ue=s(ua,"UL",{});var Ee=n(ue);PC=s(Ee,"LI",{});var sLe=n(PC);mhe=s(sLe,"STRONG",{});var iit=n(mhe);Zar=r(iit,"bart"),iit.forEach(t),esr=r(sLe," \u2014 "),dG=s(sLe,"A",{href:!0});var dit=n(dG);osr=r(dit,"TFBartForConditionalGeneration"),dit.forEach(t),rsr=r(sLe," (BART model)"),sLe.forEach(t),tsr=i(Ee),$C=s(Ee,"LI",{});var nLe=n($C);fhe=s(nLe,"STRONG",{});var cit=n(fhe);asr=r(cit,"blenderbot"),cit.forEach(t),ssr=r(nLe," \u2014 "),cG=s(nLe,"A",{href:!0});var mit=n(cG);nsr=r(mit,"TFBlenderbotForConditionalGeneration"),mit.forEach(t),lsr=r(nLe," (Blenderbot model)"),nLe.forEach(t),isr=i(Ee),IC=s(Ee,"LI",{});var lLe=n(IC);ghe=s(lLe,"STRONG",{});var fit=n(ghe);dsr=r(fit,"blenderbot-small"),fit.forEach(t),csr=r(lLe," \u2014 "),mG=s(lLe,"A",{href:!0});var git=n(mG);msr=r(git,"TFBlenderbotSmallForConditionalGeneration"),git.forEach(t),fsr=r(lLe," (BlenderbotSmall model)"),lLe.forEach(t),gsr=i(Ee),DC=s(Ee,"LI",{});var iLe=n(DC);hhe=s(iLe,"STRONG",{});var hit=n(hhe);hsr=r(hit,"encoder-decoder"),hit.forEach(t),usr=r(iLe," \u2014 "),fG=s(iLe,"A",{href:!0});var uit=n(fG);psr=r(uit,"TFEncoderDecoderModel"),uit.forEach(t),_sr=r(iLe," (Encoder decoder model)"),iLe.forEach(t),bsr=i(Ee),jC=s(Ee,"LI",{});var dLe=n(jC);uhe=s(dLe,"STRONG",{});var pit=n(uhe);vsr=r(pit,"led"),pit.forEach(t),Tsr=r(dLe," \u2014 "),gG=s(dLe,"A",{href:!0});var _it=n(gG);Fsr=r(_it,"TFLEDForConditionalGeneration"),_it.forEach(t),Csr=r(dLe," (LED model)"),dLe.forEach(t),Msr=i(Ee),NC=s(Ee,"LI",{});var cLe=n(NC);phe=s(cLe,"STRONG",{});var bit=n(phe);Esr=r(bit,"marian"),bit.forEach(t),ysr=r(cLe," \u2014 "),hG=s(cLe,"A",{href:!0});var vit=n(hG);wsr=r(vit,"TFMarianMTModel"),vit.forEach(t),Asr=r(cLe," (Marian model)"),cLe.forEach(t),Lsr=i(Ee),qC=s(Ee,"LI",{});var mLe=n(qC);_he=s(mLe,"STRONG",{});var Tit=n(_he);Bsr=r(Tit,"mbart"),Tit.forEach(t),xsr=r(mLe," \u2014 "),uG=s(mLe,"A",{href:!0});var Fit=n(uG);ksr=r(Fit,"TFMBartForConditionalGeneration"),Fit.forEach(t),Rsr=r(mLe," (mBART model)"),mLe.forEach(t),Ssr=i(Ee),GC=s(Ee,"LI",{});var fLe=n(GC);bhe=s(fLe,"STRONG",{});var Cit=n(bhe);Psr=r(Cit,"mt5"),Cit.forEach(t),$sr=r(fLe," \u2014 "),pG=s(fLe,"A",{href:!0});var Mit=n(pG);Isr=r(Mit,"TFMT5ForConditionalGeneration"),Mit.forEach(t),Dsr=r(fLe," (mT5 model)"),fLe.forEach(t),jsr=i(Ee),OC=s(Ee,"LI",{});var gLe=n(OC);vhe=s(gLe,"STRONG",{});var Eit=n(vhe);Nsr=r(Eit,"pegasus"),Eit.forEach(t),qsr=r(gLe," \u2014 "),_G=s(gLe,"A",{href:!0});var yit=n(_G);Gsr=r(yit,"TFPegasusForConditionalGeneration"),yit.forEach(t),Osr=r(gLe," (Pegasus model)"),gLe.forEach(t),Xsr=i(Ee),XC=s(Ee,"LI",{});var hLe=n(XC);The=s(hLe,"STRONG",{});var wit=n(The);Vsr=r(wit,"t5"),wit.forEach(t),zsr=r(hLe," \u2014 "),bG=s(hLe,"A",{href:!0});var Ait=n(bG);Wsr=r(Ait,"TFT5ForConditionalGeneration"),Ait.forEach(t),Qsr=r(hLe," (T5 model)"),hLe.forEach(t),Ee.forEach(t),Hsr=i(ua),Fhe=s(ua,"P",{});var Lit=n(Fhe);Usr=r(Lit,"Examples:"),Lit.forEach(t),Jsr=i(ua),f(NA.$$.fragment,ua),ua.forEach(t),Ol.forEach(t),zBe=i(c),Tc=s(c,"H2",{class:!0});var oRe=n(Tc);VC=s(oRe,"A",{id:!0,class:!0,href:!0});var Bit=n(VC);Che=s(Bit,"SPAN",{});var xit=n(Che);f(qA.$$.fragment,xit),xit.forEach(t),Bit.forEach(t),Ysr=i(oRe),Mhe=s(oRe,"SPAN",{});var kit=n(Mhe);Ksr=r(kit,"TFAutoModelForSequenceClassification"),kit.forEach(t),oRe.forEach(t),WBe=i(c),Fr=s(c,"DIV",{class:!0});var Vl=n(Fr);f(GA.$$.fragment,Vl),Zsr=i(Vl),Fc=s(Vl,"P",{});var qz=n(Fc);enr=r(qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Ehe=s(qz,"CODE",{});var Rit=n(Ehe);onr=r(Rit,"from_pretrained()"),Rit.forEach(t),rnr=r(qz,"class method or the "),yhe=s(qz,"CODE",{});var Sit=n(yhe);tnr=r(Sit,"from_config()"),Sit.forEach(t),anr=r(qz,`class
method.`),qz.forEach(t),snr=i(Vl),OA=s(Vl,"P",{});var rRe=n(OA);nnr=r(rRe,"This class cannot be instantiated directly using "),whe=s(rRe,"CODE",{});var Pit=n(whe);lnr=r(Pit,"__init__()"),Pit.forEach(t),inr=r(rRe," (throws an error)."),rRe.forEach(t),dnr=i(Vl),ht=s(Vl,"DIV",{class:!0});var zl=n(ht);f(XA.$$.fragment,zl),cnr=i(zl),Ahe=s(zl,"P",{});var $it=n(Ahe);mnr=r($it,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),$it.forEach(t),fnr=i(zl),Cc=s(zl,"P",{});var Gz=n(Cc);gnr=r(Gz,`Note:
Loading a model from its configuration file does `),Lhe=s(Gz,"STRONG",{});var Iit=n(Lhe);hnr=r(Iit,"not"),Iit.forEach(t),unr=r(Gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bhe=s(Gz,"CODE",{});var Dit=n(Bhe);pnr=r(Dit,"from_pretrained()"),Dit.forEach(t),_nr=r(Gz,"to load the model weights."),Gz.forEach(t),bnr=i(zl),xhe=s(zl,"P",{});var jit=n(xhe);vnr=r(jit,"Examples:"),jit.forEach(t),Tnr=i(zl),f(VA.$$.fragment,zl),zl.forEach(t),Fnr=i(Vl),vo=s(Vl,"DIV",{class:!0});var pa=n(vo);f(zA.$$.fragment,pa),Cnr=i(pa),khe=s(pa,"P",{});var Nit=n(khe);Mnr=r(Nit,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Nit.forEach(t),Enr=i(pa),us=s(pa,"P",{});var t5=n(us);ynr=r(t5,"The model class to instantiate is selected based on the "),Rhe=s(t5,"CODE",{});var qit=n(Rhe);wnr=r(qit,"model_type"),qit.forEach(t),Anr=r(t5,` property of the config object (either
passed as an argument or loaded from `),She=s(t5,"CODE",{});var Git=n(She);Lnr=r(Git,"pretrained_model_name_or_path"),Git.forEach(t),Bnr=r(t5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Phe=s(t5,"CODE",{});var Oit=n(Phe);xnr=r(Oit,"pretrained_model_name_or_path"),Oit.forEach(t),knr=r(t5,":"),t5.forEach(t),Rnr=i(pa),X=s(pa,"UL",{});var W=n(X);zC=s(W,"LI",{});var uLe=n(zC);$he=s(uLe,"STRONG",{});var Xit=n($he);Snr=r(Xit,"albert"),Xit.forEach(t),Pnr=r(uLe," \u2014 "),vG=s(uLe,"A",{href:!0});var Vit=n(vG);$nr=r(Vit,"TFAlbertForSequenceClassification"),Vit.forEach(t),Inr=r(uLe," (ALBERT model)"),uLe.forEach(t),Dnr=i(W),WC=s(W,"LI",{});var pLe=n(WC);Ihe=s(pLe,"STRONG",{});var zit=n(Ihe);jnr=r(zit,"bert"),zit.forEach(t),Nnr=r(pLe," \u2014 "),TG=s(pLe,"A",{href:!0});var Wit=n(TG);qnr=r(Wit,"TFBertForSequenceClassification"),Wit.forEach(t),Gnr=r(pLe," (BERT model)"),pLe.forEach(t),Onr=i(W),QC=s(W,"LI",{});var _Le=n(QC);Dhe=s(_Le,"STRONG",{});var Qit=n(Dhe);Xnr=r(Qit,"camembert"),Qit.forEach(t),Vnr=r(_Le," \u2014 "),FG=s(_Le,"A",{href:!0});var Hit=n(FG);znr=r(Hit,"TFCamembertForSequenceClassification"),Hit.forEach(t),Wnr=r(_Le," (CamemBERT model)"),_Le.forEach(t),Qnr=i(W),HC=s(W,"LI",{});var bLe=n(HC);jhe=s(bLe,"STRONG",{});var Uit=n(jhe);Hnr=r(Uit,"convbert"),Uit.forEach(t),Unr=r(bLe," \u2014 "),CG=s(bLe,"A",{href:!0});var Jit=n(CG);Jnr=r(Jit,"TFConvBertForSequenceClassification"),Jit.forEach(t),Ynr=r(bLe," (ConvBERT model)"),bLe.forEach(t),Knr=i(W),UC=s(W,"LI",{});var vLe=n(UC);Nhe=s(vLe,"STRONG",{});var Yit=n(Nhe);Znr=r(Yit,"ctrl"),Yit.forEach(t),elr=r(vLe," \u2014 "),MG=s(vLe,"A",{href:!0});var Kit=n(MG);olr=r(Kit,"TFCTRLForSequenceClassification"),Kit.forEach(t),rlr=r(vLe," (CTRL model)"),vLe.forEach(t),tlr=i(W),JC=s(W,"LI",{});var TLe=n(JC);qhe=s(TLe,"STRONG",{});var Zit=n(qhe);alr=r(Zit,"deberta"),Zit.forEach(t),slr=r(TLe," \u2014 "),EG=s(TLe,"A",{href:!0});var edt=n(EG);nlr=r(edt,"TFDebertaForSequenceClassification"),edt.forEach(t),llr=r(TLe," (DeBERTa model)"),TLe.forEach(t),ilr=i(W),YC=s(W,"LI",{});var FLe=n(YC);Ghe=s(FLe,"STRONG",{});var odt=n(Ghe);dlr=r(odt,"deberta-v2"),odt.forEach(t),clr=r(FLe," \u2014 "),yG=s(FLe,"A",{href:!0});var rdt=n(yG);mlr=r(rdt,"TFDebertaV2ForSequenceClassification"),rdt.forEach(t),flr=r(FLe," (DeBERTa-v2 model)"),FLe.forEach(t),glr=i(W),KC=s(W,"LI",{});var CLe=n(KC);Ohe=s(CLe,"STRONG",{});var tdt=n(Ohe);hlr=r(tdt,"distilbert"),tdt.forEach(t),ulr=r(CLe," \u2014 "),wG=s(CLe,"A",{href:!0});var adt=n(wG);plr=r(adt,"TFDistilBertForSequenceClassification"),adt.forEach(t),_lr=r(CLe," (DistilBERT model)"),CLe.forEach(t),blr=i(W),ZC=s(W,"LI",{});var MLe=n(ZC);Xhe=s(MLe,"STRONG",{});var sdt=n(Xhe);vlr=r(sdt,"electra"),sdt.forEach(t),Tlr=r(MLe," \u2014 "),AG=s(MLe,"A",{href:!0});var ndt=n(AG);Flr=r(ndt,"TFElectraForSequenceClassification"),ndt.forEach(t),Clr=r(MLe," (ELECTRA model)"),MLe.forEach(t),Mlr=i(W),e4=s(W,"LI",{});var ELe=n(e4);Vhe=s(ELe,"STRONG",{});var ldt=n(Vhe);Elr=r(ldt,"flaubert"),ldt.forEach(t),ylr=r(ELe," \u2014 "),LG=s(ELe,"A",{href:!0});var idt=n(LG);wlr=r(idt,"TFFlaubertForSequenceClassification"),idt.forEach(t),Alr=r(ELe," (FlauBERT model)"),ELe.forEach(t),Llr=i(W),o4=s(W,"LI",{});var yLe=n(o4);zhe=s(yLe,"STRONG",{});var ddt=n(zhe);Blr=r(ddt,"funnel"),ddt.forEach(t),xlr=r(yLe," \u2014 "),BG=s(yLe,"A",{href:!0});var cdt=n(BG);klr=r(cdt,"TFFunnelForSequenceClassification"),cdt.forEach(t),Rlr=r(yLe," (Funnel Transformer model)"),yLe.forEach(t),Slr=i(W),r4=s(W,"LI",{});var wLe=n(r4);Whe=s(wLe,"STRONG",{});var mdt=n(Whe);Plr=r(mdt,"gpt2"),mdt.forEach(t),$lr=r(wLe," \u2014 "),xG=s(wLe,"A",{href:!0});var fdt=n(xG);Ilr=r(fdt,"TFGPT2ForSequenceClassification"),fdt.forEach(t),Dlr=r(wLe," (OpenAI GPT-2 model)"),wLe.forEach(t),jlr=i(W),t4=s(W,"LI",{});var ALe=n(t4);Qhe=s(ALe,"STRONG",{});var gdt=n(Qhe);Nlr=r(gdt,"layoutlm"),gdt.forEach(t),qlr=r(ALe," \u2014 "),kG=s(ALe,"A",{href:!0});var hdt=n(kG);Glr=r(hdt,"TFLayoutLMForSequenceClassification"),hdt.forEach(t),Olr=r(ALe," (LayoutLM model)"),ALe.forEach(t),Xlr=i(W),a4=s(W,"LI",{});var LLe=n(a4);Hhe=s(LLe,"STRONG",{});var udt=n(Hhe);Vlr=r(udt,"longformer"),udt.forEach(t),zlr=r(LLe," \u2014 "),RG=s(LLe,"A",{href:!0});var pdt=n(RG);Wlr=r(pdt,"TFLongformerForSequenceClassification"),pdt.forEach(t),Qlr=r(LLe," (Longformer model)"),LLe.forEach(t),Hlr=i(W),s4=s(W,"LI",{});var BLe=n(s4);Uhe=s(BLe,"STRONG",{});var _dt=n(Uhe);Ulr=r(_dt,"mobilebert"),_dt.forEach(t),Jlr=r(BLe," \u2014 "),SG=s(BLe,"A",{href:!0});var bdt=n(SG);Ylr=r(bdt,"TFMobileBertForSequenceClassification"),bdt.forEach(t),Klr=r(BLe," (MobileBERT model)"),BLe.forEach(t),Zlr=i(W),n4=s(W,"LI",{});var xLe=n(n4);Jhe=s(xLe,"STRONG",{});var vdt=n(Jhe);eir=r(vdt,"mpnet"),vdt.forEach(t),oir=r(xLe," \u2014 "),PG=s(xLe,"A",{href:!0});var Tdt=n(PG);rir=r(Tdt,"TFMPNetForSequenceClassification"),Tdt.forEach(t),tir=r(xLe," (MPNet model)"),xLe.forEach(t),air=i(W),l4=s(W,"LI",{});var kLe=n(l4);Yhe=s(kLe,"STRONG",{});var Fdt=n(Yhe);sir=r(Fdt,"openai-gpt"),Fdt.forEach(t),nir=r(kLe," \u2014 "),$G=s(kLe,"A",{href:!0});var Cdt=n($G);lir=r(Cdt,"TFOpenAIGPTForSequenceClassification"),Cdt.forEach(t),iir=r(kLe," (OpenAI GPT model)"),kLe.forEach(t),dir=i(W),i4=s(W,"LI",{});var RLe=n(i4);Khe=s(RLe,"STRONG",{});var Mdt=n(Khe);cir=r(Mdt,"rembert"),Mdt.forEach(t),mir=r(RLe," \u2014 "),IG=s(RLe,"A",{href:!0});var Edt=n(IG);fir=r(Edt,"TFRemBertForSequenceClassification"),Edt.forEach(t),gir=r(RLe," (RemBERT model)"),RLe.forEach(t),hir=i(W),d4=s(W,"LI",{});var SLe=n(d4);Zhe=s(SLe,"STRONG",{});var ydt=n(Zhe);uir=r(ydt,"roberta"),ydt.forEach(t),pir=r(SLe," \u2014 "),DG=s(SLe,"A",{href:!0});var wdt=n(DG);_ir=r(wdt,"TFRobertaForSequenceClassification"),wdt.forEach(t),bir=r(SLe," (RoBERTa model)"),SLe.forEach(t),vir=i(W),c4=s(W,"LI",{});var PLe=n(c4);eue=s(PLe,"STRONG",{});var Adt=n(eue);Tir=r(Adt,"roformer"),Adt.forEach(t),Fir=r(PLe," \u2014 "),jG=s(PLe,"A",{href:!0});var Ldt=n(jG);Cir=r(Ldt,"TFRoFormerForSequenceClassification"),Ldt.forEach(t),Mir=r(PLe," (RoFormer model)"),PLe.forEach(t),Eir=i(W),m4=s(W,"LI",{});var $Le=n(m4);oue=s($Le,"STRONG",{});var Bdt=n(oue);yir=r(Bdt,"tapas"),Bdt.forEach(t),wir=r($Le," \u2014 "),NG=s($Le,"A",{href:!0});var xdt=n(NG);Air=r(xdt,"TFTapasForSequenceClassification"),xdt.forEach(t),Lir=r($Le," (TAPAS model)"),$Le.forEach(t),Bir=i(W),f4=s(W,"LI",{});var ILe=n(f4);rue=s(ILe,"STRONG",{});var kdt=n(rue);xir=r(kdt,"transfo-xl"),kdt.forEach(t),kir=r(ILe," \u2014 "),qG=s(ILe,"A",{href:!0});var Rdt=n(qG);Rir=r(Rdt,"TFTransfoXLForSequenceClassification"),Rdt.forEach(t),Sir=r(ILe," (Transformer-XL model)"),ILe.forEach(t),Pir=i(W),g4=s(W,"LI",{});var DLe=n(g4);tue=s(DLe,"STRONG",{});var Sdt=n(tue);$ir=r(Sdt,"xlm"),Sdt.forEach(t),Iir=r(DLe," \u2014 "),GG=s(DLe,"A",{href:!0});var Pdt=n(GG);Dir=r(Pdt,"TFXLMForSequenceClassification"),Pdt.forEach(t),jir=r(DLe," (XLM model)"),DLe.forEach(t),Nir=i(W),h4=s(W,"LI",{});var jLe=n(h4);aue=s(jLe,"STRONG",{});var $dt=n(aue);qir=r($dt,"xlm-roberta"),$dt.forEach(t),Gir=r(jLe," \u2014 "),OG=s(jLe,"A",{href:!0});var Idt=n(OG);Oir=r(Idt,"TFXLMRobertaForSequenceClassification"),Idt.forEach(t),Xir=r(jLe," (XLM-RoBERTa model)"),jLe.forEach(t),Vir=i(W),u4=s(W,"LI",{});var NLe=n(u4);sue=s(NLe,"STRONG",{});var Ddt=n(sue);zir=r(Ddt,"xlnet"),Ddt.forEach(t),Wir=r(NLe," \u2014 "),XG=s(NLe,"A",{href:!0});var jdt=n(XG);Qir=r(jdt,"TFXLNetForSequenceClassification"),jdt.forEach(t),Hir=r(NLe," (XLNet model)"),NLe.forEach(t),W.forEach(t),Uir=i(pa),nue=s(pa,"P",{});var Ndt=n(nue);Jir=r(Ndt,"Examples:"),Ndt.forEach(t),Yir=i(pa),f(WA.$$.fragment,pa),pa.forEach(t),Vl.forEach(t),QBe=i(c),Mc=s(c,"H2",{class:!0});var tRe=n(Mc);p4=s(tRe,"A",{id:!0,class:!0,href:!0});var qdt=n(p4);lue=s(qdt,"SPAN",{});var Gdt=n(lue);f(QA.$$.fragment,Gdt),Gdt.forEach(t),qdt.forEach(t),Kir=i(tRe),iue=s(tRe,"SPAN",{});var Odt=n(iue);Zir=r(Odt,"TFAutoModelForMultipleChoice"),Odt.forEach(t),tRe.forEach(t),HBe=i(c),Cr=s(c,"DIV",{class:!0});var Wl=n(Cr);f(HA.$$.fragment,Wl),edr=i(Wl),Ec=s(Wl,"P",{});var Oz=n(Ec);odr=r(Oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),due=s(Oz,"CODE",{});var Xdt=n(due);rdr=r(Xdt,"from_pretrained()"),Xdt.forEach(t),tdr=r(Oz,"class method or the "),cue=s(Oz,"CODE",{});var Vdt=n(cue);adr=r(Vdt,"from_config()"),Vdt.forEach(t),sdr=r(Oz,`class
method.`),Oz.forEach(t),ndr=i(Wl),UA=s(Wl,"P",{});var aRe=n(UA);ldr=r(aRe,"This class cannot be instantiated directly using "),mue=s(aRe,"CODE",{});var zdt=n(mue);idr=r(zdt,"__init__()"),zdt.forEach(t),ddr=r(aRe," (throws an error)."),aRe.forEach(t),cdr=i(Wl),ut=s(Wl,"DIV",{class:!0});var Ql=n(ut);f(JA.$$.fragment,Ql),mdr=i(Ql),fue=s(Ql,"P",{});var Wdt=n(fue);fdr=r(Wdt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Wdt.forEach(t),gdr=i(Ql),yc=s(Ql,"P",{});var Xz=n(yc);hdr=r(Xz,`Note:
Loading a model from its configuration file does `),gue=s(Xz,"STRONG",{});var Qdt=n(gue);udr=r(Qdt,"not"),Qdt.forEach(t),pdr=r(Xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),hue=s(Xz,"CODE",{});var Hdt=n(hue);_dr=r(Hdt,"from_pretrained()"),Hdt.forEach(t),bdr=r(Xz,"to load the model weights."),Xz.forEach(t),vdr=i(Ql),uue=s(Ql,"P",{});var Udt=n(uue);Tdr=r(Udt,"Examples:"),Udt.forEach(t),Fdr=i(Ql),f(YA.$$.fragment,Ql),Ql.forEach(t),Cdr=i(Wl),To=s(Wl,"DIV",{class:!0});var _a=n(To);f(KA.$$.fragment,_a),Mdr=i(_a),pue=s(_a,"P",{});var Jdt=n(pue);Edr=r(Jdt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Jdt.forEach(t),ydr=i(_a),ps=s(_a,"P",{});var a5=n(ps);wdr=r(a5,"The model class to instantiate is selected based on the "),_ue=s(a5,"CODE",{});var Ydt=n(_ue);Adr=r(Ydt,"model_type"),Ydt.forEach(t),Ldr=r(a5,` property of the config object (either
passed as an argument or loaded from `),bue=s(a5,"CODE",{});var Kdt=n(bue);Bdr=r(Kdt,"pretrained_model_name_or_path"),Kdt.forEach(t),xdr=r(a5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vue=s(a5,"CODE",{});var Zdt=n(vue);kdr=r(Zdt,"pretrained_model_name_or_path"),Zdt.forEach(t),Rdr=r(a5,":"),a5.forEach(t),Sdr=i(_a),te=s(_a,"UL",{});var se=n(te);_4=s(se,"LI",{});var qLe=n(_4);Tue=s(qLe,"STRONG",{});var ect=n(Tue);Pdr=r(ect,"albert"),ect.forEach(t),$dr=r(qLe," \u2014 "),VG=s(qLe,"A",{href:!0});var oct=n(VG);Idr=r(oct,"TFAlbertForMultipleChoice"),oct.forEach(t),Ddr=r(qLe," (ALBERT model)"),qLe.forEach(t),jdr=i(se),b4=s(se,"LI",{});var GLe=n(b4);Fue=s(GLe,"STRONG",{});var rct=n(Fue);Ndr=r(rct,"bert"),rct.forEach(t),qdr=r(GLe," \u2014 "),zG=s(GLe,"A",{href:!0});var tct=n(zG);Gdr=r(tct,"TFBertForMultipleChoice"),tct.forEach(t),Odr=r(GLe," (BERT model)"),GLe.forEach(t),Xdr=i(se),v4=s(se,"LI",{});var OLe=n(v4);Cue=s(OLe,"STRONG",{});var act=n(Cue);Vdr=r(act,"camembert"),act.forEach(t),zdr=r(OLe," \u2014 "),WG=s(OLe,"A",{href:!0});var sct=n(WG);Wdr=r(sct,"TFCamembertForMultipleChoice"),sct.forEach(t),Qdr=r(OLe," (CamemBERT model)"),OLe.forEach(t),Hdr=i(se),T4=s(se,"LI",{});var XLe=n(T4);Mue=s(XLe,"STRONG",{});var nct=n(Mue);Udr=r(nct,"convbert"),nct.forEach(t),Jdr=r(XLe," \u2014 "),QG=s(XLe,"A",{href:!0});var lct=n(QG);Ydr=r(lct,"TFConvBertForMultipleChoice"),lct.forEach(t),Kdr=r(XLe," (ConvBERT model)"),XLe.forEach(t),Zdr=i(se),F4=s(se,"LI",{});var VLe=n(F4);Eue=s(VLe,"STRONG",{});var ict=n(Eue);ecr=r(ict,"distilbert"),ict.forEach(t),ocr=r(VLe," \u2014 "),HG=s(VLe,"A",{href:!0});var dct=n(HG);rcr=r(dct,"TFDistilBertForMultipleChoice"),dct.forEach(t),tcr=r(VLe," (DistilBERT model)"),VLe.forEach(t),acr=i(se),C4=s(se,"LI",{});var zLe=n(C4);yue=s(zLe,"STRONG",{});var cct=n(yue);scr=r(cct,"electra"),cct.forEach(t),ncr=r(zLe," \u2014 "),UG=s(zLe,"A",{href:!0});var mct=n(UG);lcr=r(mct,"TFElectraForMultipleChoice"),mct.forEach(t),icr=r(zLe," (ELECTRA model)"),zLe.forEach(t),dcr=i(se),M4=s(se,"LI",{});var WLe=n(M4);wue=s(WLe,"STRONG",{});var fct=n(wue);ccr=r(fct,"flaubert"),fct.forEach(t),mcr=r(WLe," \u2014 "),JG=s(WLe,"A",{href:!0});var gct=n(JG);fcr=r(gct,"TFFlaubertForMultipleChoice"),gct.forEach(t),gcr=r(WLe," (FlauBERT model)"),WLe.forEach(t),hcr=i(se),E4=s(se,"LI",{});var QLe=n(E4);Aue=s(QLe,"STRONG",{});var hct=n(Aue);ucr=r(hct,"funnel"),hct.forEach(t),pcr=r(QLe," \u2014 "),YG=s(QLe,"A",{href:!0});var uct=n(YG);_cr=r(uct,"TFFunnelForMultipleChoice"),uct.forEach(t),bcr=r(QLe," (Funnel Transformer model)"),QLe.forEach(t),vcr=i(se),y4=s(se,"LI",{});var HLe=n(y4);Lue=s(HLe,"STRONG",{});var pct=n(Lue);Tcr=r(pct,"longformer"),pct.forEach(t),Fcr=r(HLe," \u2014 "),KG=s(HLe,"A",{href:!0});var _ct=n(KG);Ccr=r(_ct,"TFLongformerForMultipleChoice"),_ct.forEach(t),Mcr=r(HLe," (Longformer model)"),HLe.forEach(t),Ecr=i(se),w4=s(se,"LI",{});var ULe=n(w4);Bue=s(ULe,"STRONG",{});var bct=n(Bue);ycr=r(bct,"mobilebert"),bct.forEach(t),wcr=r(ULe," \u2014 "),ZG=s(ULe,"A",{href:!0});var vct=n(ZG);Acr=r(vct,"TFMobileBertForMultipleChoice"),vct.forEach(t),Lcr=r(ULe," (MobileBERT model)"),ULe.forEach(t),Bcr=i(se),A4=s(se,"LI",{});var JLe=n(A4);xue=s(JLe,"STRONG",{});var Tct=n(xue);xcr=r(Tct,"mpnet"),Tct.forEach(t),kcr=r(JLe," \u2014 "),eO=s(JLe,"A",{href:!0});var Fct=n(eO);Rcr=r(Fct,"TFMPNetForMultipleChoice"),Fct.forEach(t),Scr=r(JLe," (MPNet model)"),JLe.forEach(t),Pcr=i(se),L4=s(se,"LI",{});var YLe=n(L4);kue=s(YLe,"STRONG",{});var Cct=n(kue);$cr=r(Cct,"rembert"),Cct.forEach(t),Icr=r(YLe," \u2014 "),oO=s(YLe,"A",{href:!0});var Mct=n(oO);Dcr=r(Mct,"TFRemBertForMultipleChoice"),Mct.forEach(t),jcr=r(YLe," (RemBERT model)"),YLe.forEach(t),Ncr=i(se),B4=s(se,"LI",{});var KLe=n(B4);Rue=s(KLe,"STRONG",{});var Ect=n(Rue);qcr=r(Ect,"roberta"),Ect.forEach(t),Gcr=r(KLe," \u2014 "),rO=s(KLe,"A",{href:!0});var yct=n(rO);Ocr=r(yct,"TFRobertaForMultipleChoice"),yct.forEach(t),Xcr=r(KLe," (RoBERTa model)"),KLe.forEach(t),Vcr=i(se),x4=s(se,"LI",{});var ZLe=n(x4);Sue=s(ZLe,"STRONG",{});var wct=n(Sue);zcr=r(wct,"roformer"),wct.forEach(t),Wcr=r(ZLe," \u2014 "),tO=s(ZLe,"A",{href:!0});var Act=n(tO);Qcr=r(Act,"TFRoFormerForMultipleChoice"),Act.forEach(t),Hcr=r(ZLe," (RoFormer model)"),ZLe.forEach(t),Ucr=i(se),k4=s(se,"LI",{});var e8e=n(k4);Pue=s(e8e,"STRONG",{});var Lct=n(Pue);Jcr=r(Lct,"xlm"),Lct.forEach(t),Ycr=r(e8e," \u2014 "),aO=s(e8e,"A",{href:!0});var Bct=n(aO);Kcr=r(Bct,"TFXLMForMultipleChoice"),Bct.forEach(t),Zcr=r(e8e," (XLM model)"),e8e.forEach(t),emr=i(se),R4=s(se,"LI",{});var o8e=n(R4);$ue=s(o8e,"STRONG",{});var xct=n($ue);omr=r(xct,"xlm-roberta"),xct.forEach(t),rmr=r(o8e," \u2014 "),sO=s(o8e,"A",{href:!0});var kct=n(sO);tmr=r(kct,"TFXLMRobertaForMultipleChoice"),kct.forEach(t),amr=r(o8e," (XLM-RoBERTa model)"),o8e.forEach(t),smr=i(se),S4=s(se,"LI",{});var r8e=n(S4);Iue=s(r8e,"STRONG",{});var Rct=n(Iue);nmr=r(Rct,"xlnet"),Rct.forEach(t),lmr=r(r8e," \u2014 "),nO=s(r8e,"A",{href:!0});var Sct=n(nO);imr=r(Sct,"TFXLNetForMultipleChoice"),Sct.forEach(t),dmr=r(r8e," (XLNet model)"),r8e.forEach(t),se.forEach(t),cmr=i(_a),Due=s(_a,"P",{});var Pct=n(Due);mmr=r(Pct,"Examples:"),Pct.forEach(t),fmr=i(_a),f(ZA.$$.fragment,_a),_a.forEach(t),Wl.forEach(t),UBe=i(c),wc=s(c,"H2",{class:!0});var sRe=n(wc);P4=s(sRe,"A",{id:!0,class:!0,href:!0});var $ct=n(P4);jue=s($ct,"SPAN",{});var Ict=n(jue);f(e0.$$.fragment,Ict),Ict.forEach(t),$ct.forEach(t),gmr=i(sRe),Nue=s(sRe,"SPAN",{});var Dct=n(Nue);hmr=r(Dct,"TFAutoModelForTableQuestionAnswering"),Dct.forEach(t),sRe.forEach(t),JBe=i(c),Mr=s(c,"DIV",{class:!0});var Hl=n(Mr);f(o0.$$.fragment,Hl),umr=i(Hl),Ac=s(Hl,"P",{});var Vz=n(Ac);pmr=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),que=s(Vz,"CODE",{});var jct=n(que);_mr=r(jct,"from_pretrained()"),jct.forEach(t),bmr=r(Vz,"class method or the "),Gue=s(Vz,"CODE",{});var Nct=n(Gue);vmr=r(Nct,"from_config()"),Nct.forEach(t),Tmr=r(Vz,`class
method.`),Vz.forEach(t),Fmr=i(Hl),r0=s(Hl,"P",{});var nRe=n(r0);Cmr=r(nRe,"This class cannot be instantiated directly using "),Oue=s(nRe,"CODE",{});var qct=n(Oue);Mmr=r(qct,"__init__()"),qct.forEach(t),Emr=r(nRe," (throws an error)."),nRe.forEach(t),ymr=i(Hl),pt=s(Hl,"DIV",{class:!0});var Ul=n(pt);f(t0.$$.fragment,Ul),wmr=i(Ul),Xue=s(Ul,"P",{});var Gct=n(Xue);Amr=r(Gct,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Gct.forEach(t),Lmr=i(Ul),Lc=s(Ul,"P",{});var zz=n(Lc);Bmr=r(zz,`Note:
Loading a model from its configuration file does `),Vue=s(zz,"STRONG",{});var Oct=n(Vue);xmr=r(Oct,"not"),Oct.forEach(t),kmr=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),zue=s(zz,"CODE",{});var Xct=n(zue);Rmr=r(Xct,"from_pretrained()"),Xct.forEach(t),Smr=r(zz,"to load the model weights."),zz.forEach(t),Pmr=i(Ul),Wue=s(Ul,"P",{});var Vct=n(Wue);$mr=r(Vct,"Examples:"),Vct.forEach(t),Imr=i(Ul),f(a0.$$.fragment,Ul),Ul.forEach(t),Dmr=i(Hl),Fo=s(Hl,"DIV",{class:!0});var ba=n(Fo);f(s0.$$.fragment,ba),jmr=i(ba),Que=s(ba,"P",{});var zct=n(Que);Nmr=r(zct,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),zct.forEach(t),qmr=i(ba),_s=s(ba,"P",{});var s5=n(_s);Gmr=r(s5,"The model class to instantiate is selected based on the "),Hue=s(s5,"CODE",{});var Wct=n(Hue);Omr=r(Wct,"model_type"),Wct.forEach(t),Xmr=r(s5,` property of the config object (either
passed as an argument or loaded from `),Uue=s(s5,"CODE",{});var Qct=n(Uue);Vmr=r(Qct,"pretrained_model_name_or_path"),Qct.forEach(t),zmr=r(s5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Jue=s(s5,"CODE",{});var Hct=n(Jue);Wmr=r(Hct,"pretrained_model_name_or_path"),Hct.forEach(t),Qmr=r(s5,":"),s5.forEach(t),Hmr=i(ba),Yue=s(ba,"UL",{});var Uct=n(Yue);$4=s(Uct,"LI",{});var t8e=n($4);Kue=s(t8e,"STRONG",{});var Jct=n(Kue);Umr=r(Jct,"tapas"),Jct.forEach(t),Jmr=r(t8e," \u2014 "),lO=s(t8e,"A",{href:!0});var Yct=n(lO);Ymr=r(Yct,"TFTapasForQuestionAnswering"),Yct.forEach(t),Kmr=r(t8e," (TAPAS model)"),t8e.forEach(t),Uct.forEach(t),Zmr=i(ba),Zue=s(ba,"P",{});var Kct=n(Zue);efr=r(Kct,"Examples:"),Kct.forEach(t),ofr=i(ba),f(n0.$$.fragment,ba),ba.forEach(t),Hl.forEach(t),YBe=i(c),Bc=s(c,"H2",{class:!0});var lRe=n(Bc);I4=s(lRe,"A",{id:!0,class:!0,href:!0});var Zct=n(I4);epe=s(Zct,"SPAN",{});var emt=n(epe);f(l0.$$.fragment,emt),emt.forEach(t),Zct.forEach(t),rfr=i(lRe),ope=s(lRe,"SPAN",{});var omt=n(ope);tfr=r(omt,"TFAutoModelForTokenClassification"),omt.forEach(t),lRe.forEach(t),KBe=i(c),Er=s(c,"DIV",{class:!0});var Jl=n(Er);f(i0.$$.fragment,Jl),afr=i(Jl),xc=s(Jl,"P",{});var Wz=n(xc);sfr=r(Wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),rpe=s(Wz,"CODE",{});var rmt=n(rpe);nfr=r(rmt,"from_pretrained()"),rmt.forEach(t),lfr=r(Wz,"class method or the "),tpe=s(Wz,"CODE",{});var tmt=n(tpe);ifr=r(tmt,"from_config()"),tmt.forEach(t),dfr=r(Wz,`class
method.`),Wz.forEach(t),cfr=i(Jl),d0=s(Jl,"P",{});var iRe=n(d0);mfr=r(iRe,"This class cannot be instantiated directly using "),ape=s(iRe,"CODE",{});var amt=n(ape);ffr=r(amt,"__init__()"),amt.forEach(t),gfr=r(iRe," (throws an error)."),iRe.forEach(t),hfr=i(Jl),_t=s(Jl,"DIV",{class:!0});var Yl=n(_t);f(c0.$$.fragment,Yl),ufr=i(Yl),spe=s(Yl,"P",{});var smt=n(spe);pfr=r(smt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),smt.forEach(t),_fr=i(Yl),kc=s(Yl,"P",{});var Qz=n(kc);bfr=r(Qz,`Note:
Loading a model from its configuration file does `),npe=s(Qz,"STRONG",{});var nmt=n(npe);vfr=r(nmt,"not"),nmt.forEach(t),Tfr=r(Qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),lpe=s(Qz,"CODE",{});var lmt=n(lpe);Ffr=r(lmt,"from_pretrained()"),lmt.forEach(t),Cfr=r(Qz,"to load the model weights."),Qz.forEach(t),Mfr=i(Yl),ipe=s(Yl,"P",{});var imt=n(ipe);Efr=r(imt,"Examples:"),imt.forEach(t),yfr=i(Yl),f(m0.$$.fragment,Yl),Yl.forEach(t),wfr=i(Jl),Co=s(Jl,"DIV",{class:!0});var va=n(Co);f(f0.$$.fragment,va),Afr=i(va),dpe=s(va,"P",{});var dmt=n(dpe);Lfr=r(dmt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),dmt.forEach(t),Bfr=i(va),bs=s(va,"P",{});var n5=n(bs);xfr=r(n5,"The model class to instantiate is selected based on the "),cpe=s(n5,"CODE",{});var cmt=n(cpe);kfr=r(cmt,"model_type"),cmt.forEach(t),Rfr=r(n5,` property of the config object (either
passed as an argument or loaded from `),mpe=s(n5,"CODE",{});var mmt=n(mpe);Sfr=r(mmt,"pretrained_model_name_or_path"),mmt.forEach(t),Pfr=r(n5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),fpe=s(n5,"CODE",{});var fmt=n(fpe);$fr=r(fmt,"pretrained_model_name_or_path"),fmt.forEach(t),Ifr=r(n5,":"),n5.forEach(t),Dfr=i(va),K=s(va,"UL",{});var oe=n(K);D4=s(oe,"LI",{});var a8e=n(D4);gpe=s(a8e,"STRONG",{});var gmt=n(gpe);jfr=r(gmt,"albert"),gmt.forEach(t),Nfr=r(a8e," \u2014 "),iO=s(a8e,"A",{href:!0});var hmt=n(iO);qfr=r(hmt,"TFAlbertForTokenClassification"),hmt.forEach(t),Gfr=r(a8e," (ALBERT model)"),a8e.forEach(t),Ofr=i(oe),j4=s(oe,"LI",{});var s8e=n(j4);hpe=s(s8e,"STRONG",{});var umt=n(hpe);Xfr=r(umt,"bert"),umt.forEach(t),Vfr=r(s8e," \u2014 "),dO=s(s8e,"A",{href:!0});var pmt=n(dO);zfr=r(pmt,"TFBertForTokenClassification"),pmt.forEach(t),Wfr=r(s8e," (BERT model)"),s8e.forEach(t),Qfr=i(oe),N4=s(oe,"LI",{});var n8e=n(N4);upe=s(n8e,"STRONG",{});var _mt=n(upe);Hfr=r(_mt,"camembert"),_mt.forEach(t),Ufr=r(n8e," \u2014 "),cO=s(n8e,"A",{href:!0});var bmt=n(cO);Jfr=r(bmt,"TFCamembertForTokenClassification"),bmt.forEach(t),Yfr=r(n8e," (CamemBERT model)"),n8e.forEach(t),Kfr=i(oe),q4=s(oe,"LI",{});var l8e=n(q4);ppe=s(l8e,"STRONG",{});var vmt=n(ppe);Zfr=r(vmt,"convbert"),vmt.forEach(t),egr=r(l8e," \u2014 "),mO=s(l8e,"A",{href:!0});var Tmt=n(mO);ogr=r(Tmt,"TFConvBertForTokenClassification"),Tmt.forEach(t),rgr=r(l8e," (ConvBERT model)"),l8e.forEach(t),tgr=i(oe),G4=s(oe,"LI",{});var i8e=n(G4);_pe=s(i8e,"STRONG",{});var Fmt=n(_pe);agr=r(Fmt,"deberta"),Fmt.forEach(t),sgr=r(i8e," \u2014 "),fO=s(i8e,"A",{href:!0});var Cmt=n(fO);ngr=r(Cmt,"TFDebertaForTokenClassification"),Cmt.forEach(t),lgr=r(i8e," (DeBERTa model)"),i8e.forEach(t),igr=i(oe),O4=s(oe,"LI",{});var d8e=n(O4);bpe=s(d8e,"STRONG",{});var Mmt=n(bpe);dgr=r(Mmt,"deberta-v2"),Mmt.forEach(t),cgr=r(d8e," \u2014 "),gO=s(d8e,"A",{href:!0});var Emt=n(gO);mgr=r(Emt,"TFDebertaV2ForTokenClassification"),Emt.forEach(t),fgr=r(d8e," (DeBERTa-v2 model)"),d8e.forEach(t),ggr=i(oe),X4=s(oe,"LI",{});var c8e=n(X4);vpe=s(c8e,"STRONG",{});var ymt=n(vpe);hgr=r(ymt,"distilbert"),ymt.forEach(t),ugr=r(c8e," \u2014 "),hO=s(c8e,"A",{href:!0});var wmt=n(hO);pgr=r(wmt,"TFDistilBertForTokenClassification"),wmt.forEach(t),_gr=r(c8e," (DistilBERT model)"),c8e.forEach(t),bgr=i(oe),V4=s(oe,"LI",{});var m8e=n(V4);Tpe=s(m8e,"STRONG",{});var Amt=n(Tpe);vgr=r(Amt,"electra"),Amt.forEach(t),Tgr=r(m8e," \u2014 "),uO=s(m8e,"A",{href:!0});var Lmt=n(uO);Fgr=r(Lmt,"TFElectraForTokenClassification"),Lmt.forEach(t),Cgr=r(m8e," (ELECTRA model)"),m8e.forEach(t),Mgr=i(oe),z4=s(oe,"LI",{});var f8e=n(z4);Fpe=s(f8e,"STRONG",{});var Bmt=n(Fpe);Egr=r(Bmt,"flaubert"),Bmt.forEach(t),ygr=r(f8e," \u2014 "),pO=s(f8e,"A",{href:!0});var xmt=n(pO);wgr=r(xmt,"TFFlaubertForTokenClassification"),xmt.forEach(t),Agr=r(f8e," (FlauBERT model)"),f8e.forEach(t),Lgr=i(oe),W4=s(oe,"LI",{});var g8e=n(W4);Cpe=s(g8e,"STRONG",{});var kmt=n(Cpe);Bgr=r(kmt,"funnel"),kmt.forEach(t),xgr=r(g8e," \u2014 "),_O=s(g8e,"A",{href:!0});var Rmt=n(_O);kgr=r(Rmt,"TFFunnelForTokenClassification"),Rmt.forEach(t),Rgr=r(g8e," (Funnel Transformer model)"),g8e.forEach(t),Sgr=i(oe),Q4=s(oe,"LI",{});var h8e=n(Q4);Mpe=s(h8e,"STRONG",{});var Smt=n(Mpe);Pgr=r(Smt,"layoutlm"),Smt.forEach(t),$gr=r(h8e," \u2014 "),bO=s(h8e,"A",{href:!0});var Pmt=n(bO);Igr=r(Pmt,"TFLayoutLMForTokenClassification"),Pmt.forEach(t),Dgr=r(h8e," (LayoutLM model)"),h8e.forEach(t),jgr=i(oe),H4=s(oe,"LI",{});var u8e=n(H4);Epe=s(u8e,"STRONG",{});var $mt=n(Epe);Ngr=r($mt,"longformer"),$mt.forEach(t),qgr=r(u8e," \u2014 "),vO=s(u8e,"A",{href:!0});var Imt=n(vO);Ggr=r(Imt,"TFLongformerForTokenClassification"),Imt.forEach(t),Ogr=r(u8e," (Longformer model)"),u8e.forEach(t),Xgr=i(oe),U4=s(oe,"LI",{});var p8e=n(U4);ype=s(p8e,"STRONG",{});var Dmt=n(ype);Vgr=r(Dmt,"mobilebert"),Dmt.forEach(t),zgr=r(p8e," \u2014 "),TO=s(p8e,"A",{href:!0});var jmt=n(TO);Wgr=r(jmt,"TFMobileBertForTokenClassification"),jmt.forEach(t),Qgr=r(p8e," (MobileBERT model)"),p8e.forEach(t),Hgr=i(oe),J4=s(oe,"LI",{});var _8e=n(J4);wpe=s(_8e,"STRONG",{});var Nmt=n(wpe);Ugr=r(Nmt,"mpnet"),Nmt.forEach(t),Jgr=r(_8e," \u2014 "),FO=s(_8e,"A",{href:!0});var qmt=n(FO);Ygr=r(qmt,"TFMPNetForTokenClassification"),qmt.forEach(t),Kgr=r(_8e," (MPNet model)"),_8e.forEach(t),Zgr=i(oe),Y4=s(oe,"LI",{});var b8e=n(Y4);Ape=s(b8e,"STRONG",{});var Gmt=n(Ape);ehr=r(Gmt,"rembert"),Gmt.forEach(t),ohr=r(b8e," \u2014 "),CO=s(b8e,"A",{href:!0});var Omt=n(CO);rhr=r(Omt,"TFRemBertForTokenClassification"),Omt.forEach(t),thr=r(b8e," (RemBERT model)"),b8e.forEach(t),ahr=i(oe),K4=s(oe,"LI",{});var v8e=n(K4);Lpe=s(v8e,"STRONG",{});var Xmt=n(Lpe);shr=r(Xmt,"roberta"),Xmt.forEach(t),nhr=r(v8e," \u2014 "),MO=s(v8e,"A",{href:!0});var Vmt=n(MO);lhr=r(Vmt,"TFRobertaForTokenClassification"),Vmt.forEach(t),ihr=r(v8e," (RoBERTa model)"),v8e.forEach(t),dhr=i(oe),Z4=s(oe,"LI",{});var T8e=n(Z4);Bpe=s(T8e,"STRONG",{});var zmt=n(Bpe);chr=r(zmt,"roformer"),zmt.forEach(t),mhr=r(T8e," \u2014 "),EO=s(T8e,"A",{href:!0});var Wmt=n(EO);fhr=r(Wmt,"TFRoFormerForTokenClassification"),Wmt.forEach(t),ghr=r(T8e," (RoFormer model)"),T8e.forEach(t),hhr=i(oe),eM=s(oe,"LI",{});var F8e=n(eM);xpe=s(F8e,"STRONG",{});var Qmt=n(xpe);uhr=r(Qmt,"xlm"),Qmt.forEach(t),phr=r(F8e," \u2014 "),yO=s(F8e,"A",{href:!0});var Hmt=n(yO);_hr=r(Hmt,"TFXLMForTokenClassification"),Hmt.forEach(t),bhr=r(F8e," (XLM model)"),F8e.forEach(t),vhr=i(oe),oM=s(oe,"LI",{});var C8e=n(oM);kpe=s(C8e,"STRONG",{});var Umt=n(kpe);Thr=r(Umt,"xlm-roberta"),Umt.forEach(t),Fhr=r(C8e," \u2014 "),wO=s(C8e,"A",{href:!0});var Jmt=n(wO);Chr=r(Jmt,"TFXLMRobertaForTokenClassification"),Jmt.forEach(t),Mhr=r(C8e," (XLM-RoBERTa model)"),C8e.forEach(t),Ehr=i(oe),rM=s(oe,"LI",{});var M8e=n(rM);Rpe=s(M8e,"STRONG",{});var Ymt=n(Rpe);yhr=r(Ymt,"xlnet"),Ymt.forEach(t),whr=r(M8e," \u2014 "),AO=s(M8e,"A",{href:!0});var Kmt=n(AO);Ahr=r(Kmt,"TFXLNetForTokenClassification"),Kmt.forEach(t),Lhr=r(M8e," (XLNet model)"),M8e.forEach(t),oe.forEach(t),Bhr=i(va),Spe=s(va,"P",{});var Zmt=n(Spe);xhr=r(Zmt,"Examples:"),Zmt.forEach(t),khr=i(va),f(g0.$$.fragment,va),va.forEach(t),Jl.forEach(t),ZBe=i(c),Rc=s(c,"H2",{class:!0});var dRe=n(Rc);tM=s(dRe,"A",{id:!0,class:!0,href:!0});var eft=n(tM);Ppe=s(eft,"SPAN",{});var oft=n(Ppe);f(h0.$$.fragment,oft),oft.forEach(t),eft.forEach(t),Rhr=i(dRe),$pe=s(dRe,"SPAN",{});var rft=n($pe);Shr=r(rft,"TFAutoModelForQuestionAnswering"),rft.forEach(t),dRe.forEach(t),exe=i(c),yr=s(c,"DIV",{class:!0});var Kl=n(yr);f(u0.$$.fragment,Kl),Phr=i(Kl),Sc=s(Kl,"P",{});var Hz=n(Sc);$hr=r(Hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Ipe=s(Hz,"CODE",{});var tft=n(Ipe);Ihr=r(tft,"from_pretrained()"),tft.forEach(t),Dhr=r(Hz,"class method or the "),Dpe=s(Hz,"CODE",{});var aft=n(Dpe);jhr=r(aft,"from_config()"),aft.forEach(t),Nhr=r(Hz,`class
method.`),Hz.forEach(t),qhr=i(Kl),p0=s(Kl,"P",{});var cRe=n(p0);Ghr=r(cRe,"This class cannot be instantiated directly using "),jpe=s(cRe,"CODE",{});var sft=n(jpe);Ohr=r(sft,"__init__()"),sft.forEach(t),Xhr=r(cRe," (throws an error)."),cRe.forEach(t),Vhr=i(Kl),bt=s(Kl,"DIV",{class:!0});var Zl=n(bt);f(_0.$$.fragment,Zl),zhr=i(Zl),Npe=s(Zl,"P",{});var nft=n(Npe);Whr=r(nft,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),nft.forEach(t),Qhr=i(Zl),Pc=s(Zl,"P",{});var Uz=n(Pc);Hhr=r(Uz,`Note:
Loading a model from its configuration file does `),qpe=s(Uz,"STRONG",{});var lft=n(qpe);Uhr=r(lft,"not"),lft.forEach(t),Jhr=r(Uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gpe=s(Uz,"CODE",{});var ift=n(Gpe);Yhr=r(ift,"from_pretrained()"),ift.forEach(t),Khr=r(Uz,"to load the model weights."),Uz.forEach(t),Zhr=i(Zl),Ope=s(Zl,"P",{});var dft=n(Ope);eur=r(dft,"Examples:"),dft.forEach(t),our=i(Zl),f(b0.$$.fragment,Zl),Zl.forEach(t),rur=i(Kl),Mo=s(Kl,"DIV",{class:!0});var Ta=n(Mo);f(v0.$$.fragment,Ta),tur=i(Ta),Xpe=s(Ta,"P",{});var cft=n(Xpe);aur=r(cft,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),cft.forEach(t),sur=i(Ta),vs=s(Ta,"P",{});var l5=n(vs);nur=r(l5,"The model class to instantiate is selected based on the "),Vpe=s(l5,"CODE",{});var mft=n(Vpe);lur=r(mft,"model_type"),mft.forEach(t),iur=r(l5,` property of the config object (either
passed as an argument or loaded from `),zpe=s(l5,"CODE",{});var fft=n(zpe);dur=r(fft,"pretrained_model_name_or_path"),fft.forEach(t),cur=r(l5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wpe=s(l5,"CODE",{});var gft=n(Wpe);mur=r(gft,"pretrained_model_name_or_path"),gft.forEach(t),fur=r(l5,":"),l5.forEach(t),gur=i(Ta),Z=s(Ta,"UL",{});var re=n(Z);aM=s(re,"LI",{});var E8e=n(aM);Qpe=s(E8e,"STRONG",{});var hft=n(Qpe);hur=r(hft,"albert"),hft.forEach(t),uur=r(E8e," \u2014 "),LO=s(E8e,"A",{href:!0});var uft=n(LO);pur=r(uft,"TFAlbertForQuestionAnswering"),uft.forEach(t),_ur=r(E8e," (ALBERT model)"),E8e.forEach(t),bur=i(re),sM=s(re,"LI",{});var y8e=n(sM);Hpe=s(y8e,"STRONG",{});var pft=n(Hpe);vur=r(pft,"bert"),pft.forEach(t),Tur=r(y8e," \u2014 "),BO=s(y8e,"A",{href:!0});var _ft=n(BO);Fur=r(_ft,"TFBertForQuestionAnswering"),_ft.forEach(t),Cur=r(y8e," (BERT model)"),y8e.forEach(t),Mur=i(re),nM=s(re,"LI",{});var w8e=n(nM);Upe=s(w8e,"STRONG",{});var bft=n(Upe);Eur=r(bft,"camembert"),bft.forEach(t),yur=r(w8e," \u2014 "),xO=s(w8e,"A",{href:!0});var vft=n(xO);wur=r(vft,"TFCamembertForQuestionAnswering"),vft.forEach(t),Aur=r(w8e," (CamemBERT model)"),w8e.forEach(t),Lur=i(re),lM=s(re,"LI",{});var A8e=n(lM);Jpe=s(A8e,"STRONG",{});var Tft=n(Jpe);Bur=r(Tft,"convbert"),Tft.forEach(t),xur=r(A8e," \u2014 "),kO=s(A8e,"A",{href:!0});var Fft=n(kO);kur=r(Fft,"TFConvBertForQuestionAnswering"),Fft.forEach(t),Rur=r(A8e," (ConvBERT model)"),A8e.forEach(t),Sur=i(re),iM=s(re,"LI",{});var L8e=n(iM);Ype=s(L8e,"STRONG",{});var Cft=n(Ype);Pur=r(Cft,"deberta"),Cft.forEach(t),$ur=r(L8e," \u2014 "),RO=s(L8e,"A",{href:!0});var Mft=n(RO);Iur=r(Mft,"TFDebertaForQuestionAnswering"),Mft.forEach(t),Dur=r(L8e," (DeBERTa model)"),L8e.forEach(t),jur=i(re),dM=s(re,"LI",{});var B8e=n(dM);Kpe=s(B8e,"STRONG",{});var Eft=n(Kpe);Nur=r(Eft,"deberta-v2"),Eft.forEach(t),qur=r(B8e," \u2014 "),SO=s(B8e,"A",{href:!0});var yft=n(SO);Gur=r(yft,"TFDebertaV2ForQuestionAnswering"),yft.forEach(t),Our=r(B8e," (DeBERTa-v2 model)"),B8e.forEach(t),Xur=i(re),cM=s(re,"LI",{});var x8e=n(cM);Zpe=s(x8e,"STRONG",{});var wft=n(Zpe);Vur=r(wft,"distilbert"),wft.forEach(t),zur=r(x8e," \u2014 "),PO=s(x8e,"A",{href:!0});var Aft=n(PO);Wur=r(Aft,"TFDistilBertForQuestionAnswering"),Aft.forEach(t),Qur=r(x8e," (DistilBERT model)"),x8e.forEach(t),Hur=i(re),mM=s(re,"LI",{});var k8e=n(mM);e_e=s(k8e,"STRONG",{});var Lft=n(e_e);Uur=r(Lft,"electra"),Lft.forEach(t),Jur=r(k8e," \u2014 "),$O=s(k8e,"A",{href:!0});var Bft=n($O);Yur=r(Bft,"TFElectraForQuestionAnswering"),Bft.forEach(t),Kur=r(k8e," (ELECTRA model)"),k8e.forEach(t),Zur=i(re),fM=s(re,"LI",{});var R8e=n(fM);o_e=s(R8e,"STRONG",{});var xft=n(o_e);epr=r(xft,"flaubert"),xft.forEach(t),opr=r(R8e," \u2014 "),IO=s(R8e,"A",{href:!0});var kft=n(IO);rpr=r(kft,"TFFlaubertForQuestionAnsweringSimple"),kft.forEach(t),tpr=r(R8e," (FlauBERT model)"),R8e.forEach(t),apr=i(re),gM=s(re,"LI",{});var S8e=n(gM);r_e=s(S8e,"STRONG",{});var Rft=n(r_e);spr=r(Rft,"funnel"),Rft.forEach(t),npr=r(S8e," \u2014 "),DO=s(S8e,"A",{href:!0});var Sft=n(DO);lpr=r(Sft,"TFFunnelForQuestionAnswering"),Sft.forEach(t),ipr=r(S8e," (Funnel Transformer model)"),S8e.forEach(t),dpr=i(re),hM=s(re,"LI",{});var P8e=n(hM);t_e=s(P8e,"STRONG",{});var Pft=n(t_e);cpr=r(Pft,"longformer"),Pft.forEach(t),mpr=r(P8e," \u2014 "),jO=s(P8e,"A",{href:!0});var $ft=n(jO);fpr=r($ft,"TFLongformerForQuestionAnswering"),$ft.forEach(t),gpr=r(P8e," (Longformer model)"),P8e.forEach(t),hpr=i(re),uM=s(re,"LI",{});var $8e=n(uM);a_e=s($8e,"STRONG",{});var Ift=n(a_e);upr=r(Ift,"mobilebert"),Ift.forEach(t),ppr=r($8e," \u2014 "),NO=s($8e,"A",{href:!0});var Dft=n(NO);_pr=r(Dft,"TFMobileBertForQuestionAnswering"),Dft.forEach(t),bpr=r($8e," (MobileBERT model)"),$8e.forEach(t),vpr=i(re),pM=s(re,"LI",{});var I8e=n(pM);s_e=s(I8e,"STRONG",{});var jft=n(s_e);Tpr=r(jft,"mpnet"),jft.forEach(t),Fpr=r(I8e," \u2014 "),qO=s(I8e,"A",{href:!0});var Nft=n(qO);Cpr=r(Nft,"TFMPNetForQuestionAnswering"),Nft.forEach(t),Mpr=r(I8e," (MPNet model)"),I8e.forEach(t),Epr=i(re),_M=s(re,"LI",{});var D8e=n(_M);n_e=s(D8e,"STRONG",{});var qft=n(n_e);ypr=r(qft,"rembert"),qft.forEach(t),wpr=r(D8e," \u2014 "),GO=s(D8e,"A",{href:!0});var Gft=n(GO);Apr=r(Gft,"TFRemBertForQuestionAnswering"),Gft.forEach(t),Lpr=r(D8e," (RemBERT model)"),D8e.forEach(t),Bpr=i(re),bM=s(re,"LI",{});var j8e=n(bM);l_e=s(j8e,"STRONG",{});var Oft=n(l_e);xpr=r(Oft,"roberta"),Oft.forEach(t),kpr=r(j8e," \u2014 "),OO=s(j8e,"A",{href:!0});var Xft=n(OO);Rpr=r(Xft,"TFRobertaForQuestionAnswering"),Xft.forEach(t),Spr=r(j8e," (RoBERTa model)"),j8e.forEach(t),Ppr=i(re),vM=s(re,"LI",{});var N8e=n(vM);i_e=s(N8e,"STRONG",{});var Vft=n(i_e);$pr=r(Vft,"roformer"),Vft.forEach(t),Ipr=r(N8e," \u2014 "),XO=s(N8e,"A",{href:!0});var zft=n(XO);Dpr=r(zft,"TFRoFormerForQuestionAnswering"),zft.forEach(t),jpr=r(N8e," (RoFormer model)"),N8e.forEach(t),Npr=i(re),TM=s(re,"LI",{});var q8e=n(TM);d_e=s(q8e,"STRONG",{});var Wft=n(d_e);qpr=r(Wft,"xlm"),Wft.forEach(t),Gpr=r(q8e," \u2014 "),VO=s(q8e,"A",{href:!0});var Qft=n(VO);Opr=r(Qft,"TFXLMForQuestionAnsweringSimple"),Qft.forEach(t),Xpr=r(q8e," (XLM model)"),q8e.forEach(t),Vpr=i(re),FM=s(re,"LI",{});var G8e=n(FM);c_e=s(G8e,"STRONG",{});var Hft=n(c_e);zpr=r(Hft,"xlm-roberta"),Hft.forEach(t),Wpr=r(G8e," \u2014 "),zO=s(G8e,"A",{href:!0});var Uft=n(zO);Qpr=r(Uft,"TFXLMRobertaForQuestionAnswering"),Uft.forEach(t),Hpr=r(G8e," (XLM-RoBERTa model)"),G8e.forEach(t),Upr=i(re),CM=s(re,"LI",{});var O8e=n(CM);m_e=s(O8e,"STRONG",{});var Jft=n(m_e);Jpr=r(Jft,"xlnet"),Jft.forEach(t),Ypr=r(O8e," \u2014 "),WO=s(O8e,"A",{href:!0});var Yft=n(WO);Kpr=r(Yft,"TFXLNetForQuestionAnsweringSimple"),Yft.forEach(t),Zpr=r(O8e," (XLNet model)"),O8e.forEach(t),re.forEach(t),e_r=i(Ta),f_e=s(Ta,"P",{});var Kft=n(f_e);o_r=r(Kft,"Examples:"),Kft.forEach(t),r_r=i(Ta),f(T0.$$.fragment,Ta),Ta.forEach(t),Kl.forEach(t),oxe=i(c),$c=s(c,"H2",{class:!0});var mRe=n($c);MM=s(mRe,"A",{id:!0,class:!0,href:!0});var Zft=n(MM);g_e=s(Zft,"SPAN",{});var egt=n(g_e);f(F0.$$.fragment,egt),egt.forEach(t),Zft.forEach(t),t_r=i(mRe),h_e=s(mRe,"SPAN",{});var ogt=n(h_e);a_r=r(ogt,"TFAutoModelForVision2Seq"),ogt.forEach(t),mRe.forEach(t),rxe=i(c),wr=s(c,"DIV",{class:!0});var ei=n(wr);f(C0.$$.fragment,ei),s_r=i(ei),Ic=s(ei,"P",{});var Jz=n(Ic);n_r=r(Jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),u_e=s(Jz,"CODE",{});var rgt=n(u_e);l_r=r(rgt,"from_pretrained()"),rgt.forEach(t),i_r=r(Jz,"class method or the "),p_e=s(Jz,"CODE",{});var tgt=n(p_e);d_r=r(tgt,"from_config()"),tgt.forEach(t),c_r=r(Jz,`class
method.`),Jz.forEach(t),m_r=i(ei),M0=s(ei,"P",{});var fRe=n(M0);f_r=r(fRe,"This class cannot be instantiated directly using "),__e=s(fRe,"CODE",{});var agt=n(__e);g_r=r(agt,"__init__()"),agt.forEach(t),h_r=r(fRe," (throws an error)."),fRe.forEach(t),u_r=i(ei),vt=s(ei,"DIV",{class:!0});var oi=n(vt);f(E0.$$.fragment,oi),p_r=i(oi),b_e=s(oi,"P",{});var sgt=n(b_e);__r=r(sgt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),sgt.forEach(t),b_r=i(oi),Dc=s(oi,"P",{});var Yz=n(Dc);v_r=r(Yz,`Note:
Loading a model from its configuration file does `),v_e=s(Yz,"STRONG",{});var ngt=n(v_e);T_r=r(ngt,"not"),ngt.forEach(t),F_r=r(Yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),T_e=s(Yz,"CODE",{});var lgt=n(T_e);C_r=r(lgt,"from_pretrained()"),lgt.forEach(t),M_r=r(Yz,"to load the model weights."),Yz.forEach(t),E_r=i(oi),F_e=s(oi,"P",{});var igt=n(F_e);y_r=r(igt,"Examples:"),igt.forEach(t),w_r=i(oi),f(y0.$$.fragment,oi),oi.forEach(t),A_r=i(ei),Eo=s(ei,"DIV",{class:!0});var Fa=n(Eo);f(w0.$$.fragment,Fa),L_r=i(Fa),C_e=s(Fa,"P",{});var dgt=n(C_e);B_r=r(dgt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),dgt.forEach(t),x_r=i(Fa),Ts=s(Fa,"P",{});var i5=n(Ts);k_r=r(i5,"The model class to instantiate is selected based on the "),M_e=s(i5,"CODE",{});var cgt=n(M_e);R_r=r(cgt,"model_type"),cgt.forEach(t),S_r=r(i5,` property of the config object (either
passed as an argument or loaded from `),E_e=s(i5,"CODE",{});var mgt=n(E_e);P_r=r(mgt,"pretrained_model_name_or_path"),mgt.forEach(t),$_r=r(i5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),y_e=s(i5,"CODE",{});var fgt=n(y_e);I_r=r(fgt,"pretrained_model_name_or_path"),fgt.forEach(t),D_r=r(i5,":"),i5.forEach(t),j_r=i(Fa),w_e=s(Fa,"UL",{});var ggt=n(w_e);EM=s(ggt,"LI",{});var X8e=n(EM);A_e=s(X8e,"STRONG",{});var hgt=n(A_e);N_r=r(hgt,"vision-encoder-decoder"),hgt.forEach(t),q_r=r(X8e," \u2014 "),QO=s(X8e,"A",{href:!0});var ugt=n(QO);G_r=r(ugt,"TFVisionEncoderDecoderModel"),ugt.forEach(t),O_r=r(X8e," (Vision Encoder decoder model)"),X8e.forEach(t),ggt.forEach(t),X_r=i(Fa),L_e=s(Fa,"P",{});var pgt=n(L_e);V_r=r(pgt,"Examples:"),pgt.forEach(t),z_r=i(Fa),f(A0.$$.fragment,Fa),Fa.forEach(t),ei.forEach(t),txe=i(c),jc=s(c,"H2",{class:!0});var gRe=n(jc);yM=s(gRe,"A",{id:!0,class:!0,href:!0});var _gt=n(yM);B_e=s(_gt,"SPAN",{});var bgt=n(B_e);f(L0.$$.fragment,bgt),bgt.forEach(t),_gt.forEach(t),W_r=i(gRe),x_e=s(gRe,"SPAN",{});var vgt=n(x_e);Q_r=r(vgt,"TFAutoModelForSpeechSeq2Seq"),vgt.forEach(t),gRe.forEach(t),axe=i(c),Ar=s(c,"DIV",{class:!0});var ri=n(Ar);f(B0.$$.fragment,ri),H_r=i(ri),Nc=s(ri,"P",{});var Kz=n(Nc);U_r=r(Kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),k_e=s(Kz,"CODE",{});var Tgt=n(k_e);J_r=r(Tgt,"from_pretrained()"),Tgt.forEach(t),Y_r=r(Kz,"class method or the "),R_e=s(Kz,"CODE",{});var Fgt=n(R_e);K_r=r(Fgt,"from_config()"),Fgt.forEach(t),Z_r=r(Kz,`class
method.`),Kz.forEach(t),ebr=i(ri),x0=s(ri,"P",{});var hRe=n(x0);obr=r(hRe,"This class cannot be instantiated directly using "),S_e=s(hRe,"CODE",{});var Cgt=n(S_e);rbr=r(Cgt,"__init__()"),Cgt.forEach(t),tbr=r(hRe," (throws an error)."),hRe.forEach(t),abr=i(ri),Tt=s(ri,"DIV",{class:!0});var ti=n(Tt);f(k0.$$.fragment,ti),sbr=i(ti),P_e=s(ti,"P",{});var Mgt=n(P_e);nbr=r(Mgt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Mgt.forEach(t),lbr=i(ti),qc=s(ti,"P",{});var Zz=n(qc);ibr=r(Zz,`Note:
Loading a model from its configuration file does `),$_e=s(Zz,"STRONG",{});var Egt=n($_e);dbr=r(Egt,"not"),Egt.forEach(t),cbr=r(Zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),I_e=s(Zz,"CODE",{});var ygt=n(I_e);mbr=r(ygt,"from_pretrained()"),ygt.forEach(t),fbr=r(Zz,"to load the model weights."),Zz.forEach(t),gbr=i(ti),D_e=s(ti,"P",{});var wgt=n(D_e);hbr=r(wgt,"Examples:"),wgt.forEach(t),ubr=i(ti),f(R0.$$.fragment,ti),ti.forEach(t),pbr=i(ri),yo=s(ri,"DIV",{class:!0});var Ca=n(yo);f(S0.$$.fragment,Ca),_br=i(Ca),j_e=s(Ca,"P",{});var Agt=n(j_e);bbr=r(Agt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Agt.forEach(t),vbr=i(Ca),Fs=s(Ca,"P",{});var d5=n(Fs);Tbr=r(d5,"The model class to instantiate is selected based on the "),N_e=s(d5,"CODE",{});var Lgt=n(N_e);Fbr=r(Lgt,"model_type"),Lgt.forEach(t),Cbr=r(d5,` property of the config object (either
passed as an argument or loaded from `),q_e=s(d5,"CODE",{});var Bgt=n(q_e);Mbr=r(Bgt,"pretrained_model_name_or_path"),Bgt.forEach(t),Ebr=r(d5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),G_e=s(d5,"CODE",{});var xgt=n(G_e);ybr=r(xgt,"pretrained_model_name_or_path"),xgt.forEach(t),wbr=r(d5,":"),d5.forEach(t),Abr=i(Ca),O_e=s(Ca,"UL",{});var kgt=n(O_e);wM=s(kgt,"LI",{});var V8e=n(wM);X_e=s(V8e,"STRONG",{});var Rgt=n(X_e);Lbr=r(Rgt,"speech_to_text"),Rgt.forEach(t),Bbr=r(V8e," \u2014 "),HO=s(V8e,"A",{href:!0});var Sgt=n(HO);xbr=r(Sgt,"TFSpeech2TextForConditionalGeneration"),Sgt.forEach(t),kbr=r(V8e," (Speech2Text model)"),V8e.forEach(t),kgt.forEach(t),Rbr=i(Ca),V_e=s(Ca,"P",{});var Pgt=n(V_e);Sbr=r(Pgt,"Examples:"),Pgt.forEach(t),Pbr=i(Ca),f(P0.$$.fragment,Ca),Ca.forEach(t),ri.forEach(t),sxe=i(c),Gc=s(c,"H2",{class:!0});var uRe=n(Gc);AM=s(uRe,"A",{id:!0,class:!0,href:!0});var $gt=n(AM);z_e=s($gt,"SPAN",{});var Igt=n(z_e);f($0.$$.fragment,Igt),Igt.forEach(t),$gt.forEach(t),$br=i(uRe),W_e=s(uRe,"SPAN",{});var Dgt=n(W_e);Ibr=r(Dgt,"FlaxAutoModel"),Dgt.forEach(t),uRe.forEach(t),nxe=i(c),Lr=s(c,"DIV",{class:!0});var ai=n(Lr);f(I0.$$.fragment,ai),Dbr=i(ai),Oc=s(ai,"P",{});var eW=n(Oc);jbr=r(eW,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Q_e=s(eW,"CODE",{});var jgt=n(Q_e);Nbr=r(jgt,"from_pretrained()"),jgt.forEach(t),qbr=r(eW,"class method or the "),H_e=s(eW,"CODE",{});var Ngt=n(H_e);Gbr=r(Ngt,"from_config()"),Ngt.forEach(t),Obr=r(eW,`class
method.`),eW.forEach(t),Xbr=i(ai),D0=s(ai,"P",{});var pRe=n(D0);Vbr=r(pRe,"This class cannot be instantiated directly using "),U_e=s(pRe,"CODE",{});var qgt=n(U_e);zbr=r(qgt,"__init__()"),qgt.forEach(t),Wbr=r(pRe," (throws an error)."),pRe.forEach(t),Qbr=i(ai),Ft=s(ai,"DIV",{class:!0});var si=n(Ft);f(j0.$$.fragment,si),Hbr=i(si),J_e=s(si,"P",{});var Ggt=n(J_e);Ubr=r(Ggt,"Instantiates one of the base model classes of the library from a configuration."),Ggt.forEach(t),Jbr=i(si),Xc=s(si,"P",{});var oW=n(Xc);Ybr=r(oW,`Note:
Loading a model from its configuration file does `),Y_e=s(oW,"STRONG",{});var Ogt=n(Y_e);Kbr=r(Ogt,"not"),Ogt.forEach(t),Zbr=r(oW,` load the model weights. It only affects the
model\u2019s configuration. Use `),K_e=s(oW,"CODE",{});var Xgt=n(K_e);e2r=r(Xgt,"from_pretrained()"),Xgt.forEach(t),o2r=r(oW,"to load the model weights."),oW.forEach(t),r2r=i(si),Z_e=s(si,"P",{});var Vgt=n(Z_e);t2r=r(Vgt,"Examples:"),Vgt.forEach(t),a2r=i(si),f(N0.$$.fragment,si),si.forEach(t),s2r=i(ai),wo=s(ai,"DIV",{class:!0});var Ma=n(wo);f(q0.$$.fragment,Ma),n2r=i(Ma),ebe=s(Ma,"P",{});var zgt=n(ebe);l2r=r(zgt,"Instantiate one of the base model classes of the library from a pretrained model."),zgt.forEach(t),i2r=i(Ma),Cs=s(Ma,"P",{});var c5=n(Cs);d2r=r(c5,"The model class to instantiate is selected based on the "),obe=s(c5,"CODE",{});var Wgt=n(obe);c2r=r(Wgt,"model_type"),Wgt.forEach(t),m2r=r(c5,` property of the config object (either
passed as an argument or loaded from `),rbe=s(c5,"CODE",{});var Qgt=n(rbe);f2r=r(Qgt,"pretrained_model_name_or_path"),Qgt.forEach(t),g2r=r(c5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tbe=s(c5,"CODE",{});var Hgt=n(tbe);h2r=r(Hgt,"pretrained_model_name_or_path"),Hgt.forEach(t),u2r=r(c5,":"),c5.forEach(t),p2r=i(Ma),z=s(Ma,"UL",{});var Q=n(z);LM=s(Q,"LI",{});var z8e=n(LM);abe=s(z8e,"STRONG",{});var Ugt=n(abe);_2r=r(Ugt,"albert"),Ugt.forEach(t),b2r=r(z8e," \u2014 "),UO=s(z8e,"A",{href:!0});var Jgt=n(UO);v2r=r(Jgt,"FlaxAlbertModel"),Jgt.forEach(t),T2r=r(z8e," (ALBERT model)"),z8e.forEach(t),F2r=i(Q),BM=s(Q,"LI",{});var W8e=n(BM);sbe=s(W8e,"STRONG",{});var Ygt=n(sbe);C2r=r(Ygt,"bart"),Ygt.forEach(t),M2r=r(W8e," \u2014 "),JO=s(W8e,"A",{href:!0});var Kgt=n(JO);E2r=r(Kgt,"FlaxBartModel"),Kgt.forEach(t),y2r=r(W8e," (BART model)"),W8e.forEach(t),w2r=i(Q),xM=s(Q,"LI",{});var Q8e=n(xM);nbe=s(Q8e,"STRONG",{});var Zgt=n(nbe);A2r=r(Zgt,"beit"),Zgt.forEach(t),L2r=r(Q8e," \u2014 "),YO=s(Q8e,"A",{href:!0});var eht=n(YO);B2r=r(eht,"FlaxBeitModel"),eht.forEach(t),x2r=r(Q8e," (BEiT model)"),Q8e.forEach(t),k2r=i(Q),kM=s(Q,"LI",{});var H8e=n(kM);lbe=s(H8e,"STRONG",{});var oht=n(lbe);R2r=r(oht,"bert"),oht.forEach(t),S2r=r(H8e," \u2014 "),KO=s(H8e,"A",{href:!0});var rht=n(KO);P2r=r(rht,"FlaxBertModel"),rht.forEach(t),$2r=r(H8e," (BERT model)"),H8e.forEach(t),I2r=i(Q),RM=s(Q,"LI",{});var U8e=n(RM);ibe=s(U8e,"STRONG",{});var tht=n(ibe);D2r=r(tht,"big_bird"),tht.forEach(t),j2r=r(U8e," \u2014 "),ZO=s(U8e,"A",{href:!0});var aht=n(ZO);N2r=r(aht,"FlaxBigBirdModel"),aht.forEach(t),q2r=r(U8e," (BigBird model)"),U8e.forEach(t),G2r=i(Q),SM=s(Q,"LI",{});var J8e=n(SM);dbe=s(J8e,"STRONG",{});var sht=n(dbe);O2r=r(sht,"blenderbot"),sht.forEach(t),X2r=r(J8e," \u2014 "),eX=s(J8e,"A",{href:!0});var nht=n(eX);V2r=r(nht,"FlaxBlenderbotModel"),nht.forEach(t),z2r=r(J8e," (Blenderbot model)"),J8e.forEach(t),W2r=i(Q),PM=s(Q,"LI",{});var Y8e=n(PM);cbe=s(Y8e,"STRONG",{});var lht=n(cbe);Q2r=r(lht,"blenderbot-small"),lht.forEach(t),H2r=r(Y8e," \u2014 "),oX=s(Y8e,"A",{href:!0});var iht=n(oX);U2r=r(iht,"FlaxBlenderbotSmallModel"),iht.forEach(t),J2r=r(Y8e," (BlenderbotSmall model)"),Y8e.forEach(t),Y2r=i(Q),$M=s(Q,"LI",{});var K8e=n($M);mbe=s(K8e,"STRONG",{});var dht=n(mbe);K2r=r(dht,"clip"),dht.forEach(t),Z2r=r(K8e," \u2014 "),rX=s(K8e,"A",{href:!0});var cht=n(rX);evr=r(cht,"FlaxCLIPModel"),cht.forEach(t),ovr=r(K8e," (CLIP model)"),K8e.forEach(t),rvr=i(Q),IM=s(Q,"LI",{});var Z8e=n(IM);fbe=s(Z8e,"STRONG",{});var mht=n(fbe);tvr=r(mht,"distilbert"),mht.forEach(t),avr=r(Z8e," \u2014 "),tX=s(Z8e,"A",{href:!0});var fht=n(tX);svr=r(fht,"FlaxDistilBertModel"),fht.forEach(t),nvr=r(Z8e," (DistilBERT model)"),Z8e.forEach(t),lvr=i(Q),DM=s(Q,"LI",{});var e7e=n(DM);gbe=s(e7e,"STRONG",{});var ght=n(gbe);ivr=r(ght,"electra"),ght.forEach(t),dvr=r(e7e," \u2014 "),aX=s(e7e,"A",{href:!0});var hht=n(aX);cvr=r(hht,"FlaxElectraModel"),hht.forEach(t),mvr=r(e7e," (ELECTRA model)"),e7e.forEach(t),fvr=i(Q),jM=s(Q,"LI",{});var o7e=n(jM);hbe=s(o7e,"STRONG",{});var uht=n(hbe);gvr=r(uht,"gpt2"),uht.forEach(t),hvr=r(o7e," \u2014 "),sX=s(o7e,"A",{href:!0});var pht=n(sX);uvr=r(pht,"FlaxGPT2Model"),pht.forEach(t),pvr=r(o7e," (OpenAI GPT-2 model)"),o7e.forEach(t),_vr=i(Q),NM=s(Q,"LI",{});var r7e=n(NM);ube=s(r7e,"STRONG",{});var _ht=n(ube);bvr=r(_ht,"gpt_neo"),_ht.forEach(t),vvr=r(r7e," \u2014 "),nX=s(r7e,"A",{href:!0});var bht=n(nX);Tvr=r(bht,"FlaxGPTNeoModel"),bht.forEach(t),Fvr=r(r7e," (GPT Neo model)"),r7e.forEach(t),Cvr=i(Q),qM=s(Q,"LI",{});var t7e=n(qM);pbe=s(t7e,"STRONG",{});var vht=n(pbe);Mvr=r(vht,"gptj"),vht.forEach(t),Evr=r(t7e," \u2014 "),lX=s(t7e,"A",{href:!0});var Tht=n(lX);yvr=r(Tht,"FlaxGPTJModel"),Tht.forEach(t),wvr=r(t7e," (GPT-J model)"),t7e.forEach(t),Avr=i(Q),GM=s(Q,"LI",{});var a7e=n(GM);_be=s(a7e,"STRONG",{});var Fht=n(_be);Lvr=r(Fht,"marian"),Fht.forEach(t),Bvr=r(a7e," \u2014 "),iX=s(a7e,"A",{href:!0});var Cht=n(iX);xvr=r(Cht,"FlaxMarianModel"),Cht.forEach(t),kvr=r(a7e," (Marian model)"),a7e.forEach(t),Rvr=i(Q),OM=s(Q,"LI",{});var s7e=n(OM);bbe=s(s7e,"STRONG",{});var Mht=n(bbe);Svr=r(Mht,"mbart"),Mht.forEach(t),Pvr=r(s7e," \u2014 "),dX=s(s7e,"A",{href:!0});var Eht=n(dX);$vr=r(Eht,"FlaxMBartModel"),Eht.forEach(t),Ivr=r(s7e," (mBART model)"),s7e.forEach(t),Dvr=i(Q),XM=s(Q,"LI",{});var n7e=n(XM);vbe=s(n7e,"STRONG",{});var yht=n(vbe);jvr=r(yht,"mt5"),yht.forEach(t),Nvr=r(n7e," \u2014 "),cX=s(n7e,"A",{href:!0});var wht=n(cX);qvr=r(wht,"FlaxMT5Model"),wht.forEach(t),Gvr=r(n7e," (mT5 model)"),n7e.forEach(t),Ovr=i(Q),VM=s(Q,"LI",{});var l7e=n(VM);Tbe=s(l7e,"STRONG",{});var Aht=n(Tbe);Xvr=r(Aht,"pegasus"),Aht.forEach(t),Vvr=r(l7e," \u2014 "),mX=s(l7e,"A",{href:!0});var Lht=n(mX);zvr=r(Lht,"FlaxPegasusModel"),Lht.forEach(t),Wvr=r(l7e," (Pegasus model)"),l7e.forEach(t),Qvr=i(Q),zM=s(Q,"LI",{});var i7e=n(zM);Fbe=s(i7e,"STRONG",{});var Bht=n(Fbe);Hvr=r(Bht,"roberta"),Bht.forEach(t),Uvr=r(i7e," \u2014 "),fX=s(i7e,"A",{href:!0});var xht=n(fX);Jvr=r(xht,"FlaxRobertaModel"),xht.forEach(t),Yvr=r(i7e," (RoBERTa model)"),i7e.forEach(t),Kvr=i(Q),WM=s(Q,"LI",{});var d7e=n(WM);Cbe=s(d7e,"STRONG",{});var kht=n(Cbe);Zvr=r(kht,"roformer"),kht.forEach(t),eTr=r(d7e," \u2014 "),gX=s(d7e,"A",{href:!0});var Rht=n(gX);oTr=r(Rht,"FlaxRoFormerModel"),Rht.forEach(t),rTr=r(d7e," (RoFormer model)"),d7e.forEach(t),tTr=i(Q),QM=s(Q,"LI",{});var c7e=n(QM);Mbe=s(c7e,"STRONG",{});var Sht=n(Mbe);aTr=r(Sht,"t5"),Sht.forEach(t),sTr=r(c7e," \u2014 "),hX=s(c7e,"A",{href:!0});var Pht=n(hX);nTr=r(Pht,"FlaxT5Model"),Pht.forEach(t),lTr=r(c7e," (T5 model)"),c7e.forEach(t),iTr=i(Q),HM=s(Q,"LI",{});var m7e=n(HM);Ebe=s(m7e,"STRONG",{});var $ht=n(Ebe);dTr=r($ht,"vision-text-dual-encoder"),$ht.forEach(t),cTr=r(m7e," \u2014 "),uX=s(m7e,"A",{href:!0});var Iht=n(uX);mTr=r(Iht,"FlaxVisionTextDualEncoderModel"),Iht.forEach(t),fTr=r(m7e," (VisionTextDualEncoder model)"),m7e.forEach(t),gTr=i(Q),UM=s(Q,"LI",{});var f7e=n(UM);ybe=s(f7e,"STRONG",{});var Dht=n(ybe);hTr=r(Dht,"vit"),Dht.forEach(t),uTr=r(f7e," \u2014 "),pX=s(f7e,"A",{href:!0});var jht=n(pX);pTr=r(jht,"FlaxViTModel"),jht.forEach(t),_Tr=r(f7e," (ViT model)"),f7e.forEach(t),bTr=i(Q),JM=s(Q,"LI",{});var g7e=n(JM);wbe=s(g7e,"STRONG",{});var Nht=n(wbe);vTr=r(Nht,"wav2vec2"),Nht.forEach(t),TTr=r(g7e," \u2014 "),_X=s(g7e,"A",{href:!0});var qht=n(_X);FTr=r(qht,"FlaxWav2Vec2Model"),qht.forEach(t),CTr=r(g7e," (Wav2Vec2 model)"),g7e.forEach(t),MTr=i(Q),YM=s(Q,"LI",{});var h7e=n(YM);Abe=s(h7e,"STRONG",{});var Ght=n(Abe);ETr=r(Ght,"xglm"),Ght.forEach(t),yTr=r(h7e," \u2014 "),bX=s(h7e,"A",{href:!0});var Oht=n(bX);wTr=r(Oht,"FlaxXGLMModel"),Oht.forEach(t),ATr=r(h7e," (XGLM model)"),h7e.forEach(t),Q.forEach(t),LTr=i(Ma),Lbe=s(Ma,"P",{});var Xht=n(Lbe);BTr=r(Xht,"Examples:"),Xht.forEach(t),xTr=i(Ma),f(G0.$$.fragment,Ma),Ma.forEach(t),ai.forEach(t),lxe=i(c),Vc=s(c,"H2",{class:!0});var _Re=n(Vc);KM=s(_Re,"A",{id:!0,class:!0,href:!0});var Vht=n(KM);Bbe=s(Vht,"SPAN",{});var zht=n(Bbe);f(O0.$$.fragment,zht),zht.forEach(t),Vht.forEach(t),kTr=i(_Re),xbe=s(_Re,"SPAN",{});var Wht=n(xbe);RTr=r(Wht,"FlaxAutoModelForCausalLM"),Wht.forEach(t),_Re.forEach(t),ixe=i(c),Br=s(c,"DIV",{class:!0});var ni=n(Br);f(X0.$$.fragment,ni),STr=i(ni),zc=s(ni,"P",{});var rW=n(zc);PTr=r(rW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),kbe=s(rW,"CODE",{});var Qht=n(kbe);$Tr=r(Qht,"from_pretrained()"),Qht.forEach(t),ITr=r(rW,"class method or the "),Rbe=s(rW,"CODE",{});var Hht=n(Rbe);DTr=r(Hht,"from_config()"),Hht.forEach(t),jTr=r(rW,`class
method.`),rW.forEach(t),NTr=i(ni),V0=s(ni,"P",{});var bRe=n(V0);qTr=r(bRe,"This class cannot be instantiated directly using "),Sbe=s(bRe,"CODE",{});var Uht=n(Sbe);GTr=r(Uht,"__init__()"),Uht.forEach(t),OTr=r(bRe," (throws an error)."),bRe.forEach(t),XTr=i(ni),Ct=s(ni,"DIV",{class:!0});var li=n(Ct);f(z0.$$.fragment,li),VTr=i(li),Pbe=s(li,"P",{});var Jht=n(Pbe);zTr=r(Jht,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Jht.forEach(t),WTr=i(li),Wc=s(li,"P",{});var tW=n(Wc);QTr=r(tW,`Note:
Loading a model from its configuration file does `),$be=s(tW,"STRONG",{});var Yht=n($be);HTr=r(Yht,"not"),Yht.forEach(t),UTr=r(tW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ibe=s(tW,"CODE",{});var Kht=n(Ibe);JTr=r(Kht,"from_pretrained()"),Kht.forEach(t),YTr=r(tW,"to load the model weights."),tW.forEach(t),KTr=i(li),Dbe=s(li,"P",{});var Zht=n(Dbe);ZTr=r(Zht,"Examples:"),Zht.forEach(t),e1r=i(li),f(W0.$$.fragment,li),li.forEach(t),o1r=i(ni),Ao=s(ni,"DIV",{class:!0});var Ea=n(Ao);f(Q0.$$.fragment,Ea),r1r=i(Ea),jbe=s(Ea,"P",{});var eut=n(jbe);t1r=r(eut,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),eut.forEach(t),a1r=i(Ea),Ms=s(Ea,"P",{});var m5=n(Ms);s1r=r(m5,"The model class to instantiate is selected based on the "),Nbe=s(m5,"CODE",{});var out=n(Nbe);n1r=r(out,"model_type"),out.forEach(t),l1r=r(m5,` property of the config object (either
passed as an argument or loaded from `),qbe=s(m5,"CODE",{});var rut=n(qbe);i1r=r(rut,"pretrained_model_name_or_path"),rut.forEach(t),d1r=r(m5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Gbe=s(m5,"CODE",{});var tut=n(Gbe);c1r=r(tut,"pretrained_model_name_or_path"),tut.forEach(t),m1r=r(m5,":"),m5.forEach(t),f1r=i(Ea),Es=s(Ea,"UL",{});var f5=n(Es);ZM=s(f5,"LI",{});var u7e=n(ZM);Obe=s(u7e,"STRONG",{});var aut=n(Obe);g1r=r(aut,"gpt2"),aut.forEach(t),h1r=r(u7e," \u2014 "),vX=s(u7e,"A",{href:!0});var sut=n(vX);u1r=r(sut,"FlaxGPT2LMHeadModel"),sut.forEach(t),p1r=r(u7e," (OpenAI GPT-2 model)"),u7e.forEach(t),_1r=i(f5),eE=s(f5,"LI",{});var p7e=n(eE);Xbe=s(p7e,"STRONG",{});var nut=n(Xbe);b1r=r(nut,"gpt_neo"),nut.forEach(t),v1r=r(p7e," \u2014 "),TX=s(p7e,"A",{href:!0});var lut=n(TX);T1r=r(lut,"FlaxGPTNeoForCausalLM"),lut.forEach(t),F1r=r(p7e," (GPT Neo model)"),p7e.forEach(t),C1r=i(f5),oE=s(f5,"LI",{});var _7e=n(oE);Vbe=s(_7e,"STRONG",{});var iut=n(Vbe);M1r=r(iut,"gptj"),iut.forEach(t),E1r=r(_7e," \u2014 "),FX=s(_7e,"A",{href:!0});var dut=n(FX);y1r=r(dut,"FlaxGPTJForCausalLM"),dut.forEach(t),w1r=r(_7e," (GPT-J model)"),_7e.forEach(t),A1r=i(f5),rE=s(f5,"LI",{});var b7e=n(rE);zbe=s(b7e,"STRONG",{});var cut=n(zbe);L1r=r(cut,"xglm"),cut.forEach(t),B1r=r(b7e," \u2014 "),CX=s(b7e,"A",{href:!0});var mut=n(CX);x1r=r(mut,"FlaxXGLMForCausalLM"),mut.forEach(t),k1r=r(b7e," (XGLM model)"),b7e.forEach(t),f5.forEach(t),R1r=i(Ea),Wbe=s(Ea,"P",{});var fut=n(Wbe);S1r=r(fut,"Examples:"),fut.forEach(t),P1r=i(Ea),f(H0.$$.fragment,Ea),Ea.forEach(t),ni.forEach(t),dxe=i(c),Qc=s(c,"H2",{class:!0});var vRe=n(Qc);tE=s(vRe,"A",{id:!0,class:!0,href:!0});var gut=n(tE);Qbe=s(gut,"SPAN",{});var hut=n(Qbe);f(U0.$$.fragment,hut),hut.forEach(t),gut.forEach(t),$1r=i(vRe),Hbe=s(vRe,"SPAN",{});var uut=n(Hbe);I1r=r(uut,"FlaxAutoModelForPreTraining"),uut.forEach(t),vRe.forEach(t),cxe=i(c),xr=s(c,"DIV",{class:!0});var ii=n(xr);f(J0.$$.fragment,ii),D1r=i(ii),Hc=s(ii,"P",{});var aW=n(Hc);j1r=r(aW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Ube=s(aW,"CODE",{});var put=n(Ube);N1r=r(put,"from_pretrained()"),put.forEach(t),q1r=r(aW,"class method or the "),Jbe=s(aW,"CODE",{});var _ut=n(Jbe);G1r=r(_ut,"from_config()"),_ut.forEach(t),O1r=r(aW,`class
method.`),aW.forEach(t),X1r=i(ii),Y0=s(ii,"P",{});var TRe=n(Y0);V1r=r(TRe,"This class cannot be instantiated directly using "),Ybe=s(TRe,"CODE",{});var but=n(Ybe);z1r=r(but,"__init__()"),but.forEach(t),W1r=r(TRe," (throws an error)."),TRe.forEach(t),Q1r=i(ii),Mt=s(ii,"DIV",{class:!0});var di=n(Mt);f(K0.$$.fragment,di),H1r=i(di),Kbe=s(di,"P",{});var vut=n(Kbe);U1r=r(vut,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),vut.forEach(t),J1r=i(di),Uc=s(di,"P",{});var sW=n(Uc);Y1r=r(sW,`Note:
Loading a model from its configuration file does `),Zbe=s(sW,"STRONG",{});var Tut=n(Zbe);K1r=r(Tut,"not"),Tut.forEach(t),Z1r=r(sW,` load the model weights. It only affects the
model\u2019s configuration. Use `),e2e=s(sW,"CODE",{});var Fut=n(e2e);eFr=r(Fut,"from_pretrained()"),Fut.forEach(t),oFr=r(sW,"to load the model weights."),sW.forEach(t),rFr=i(di),o2e=s(di,"P",{});var Cut=n(o2e);tFr=r(Cut,"Examples:"),Cut.forEach(t),aFr=i(di),f(Z0.$$.fragment,di),di.forEach(t),sFr=i(ii),Lo=s(ii,"DIV",{class:!0});var ya=n(Lo);f(eL.$$.fragment,ya),nFr=i(ya),r2e=s(ya,"P",{});var Mut=n(r2e);lFr=r(Mut,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Mut.forEach(t),iFr=i(ya),ys=s(ya,"P",{});var g5=n(ys);dFr=r(g5,"The model class to instantiate is selected based on the "),t2e=s(g5,"CODE",{});var Eut=n(t2e);cFr=r(Eut,"model_type"),Eut.forEach(t),mFr=r(g5,` property of the config object (either
passed as an argument or loaded from `),a2e=s(g5,"CODE",{});var yut=n(a2e);fFr=r(yut,"pretrained_model_name_or_path"),yut.forEach(t),gFr=r(g5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),s2e=s(g5,"CODE",{});var wut=n(s2e);hFr=r(wut,"pretrained_model_name_or_path"),wut.forEach(t),uFr=r(g5,":"),g5.forEach(t),pFr=i(ya),me=s(ya,"UL",{});var pe=n(me);aE=s(pe,"LI",{});var v7e=n(aE);n2e=s(v7e,"STRONG",{});var Aut=n(n2e);_Fr=r(Aut,"albert"),Aut.forEach(t),bFr=r(v7e," \u2014 "),MX=s(v7e,"A",{href:!0});var Lut=n(MX);vFr=r(Lut,"FlaxAlbertForPreTraining"),Lut.forEach(t),TFr=r(v7e," (ALBERT model)"),v7e.forEach(t),FFr=i(pe),sE=s(pe,"LI",{});var T7e=n(sE);l2e=s(T7e,"STRONG",{});var But=n(l2e);CFr=r(But,"bart"),But.forEach(t),MFr=r(T7e," \u2014 "),EX=s(T7e,"A",{href:!0});var xut=n(EX);EFr=r(xut,"FlaxBartForConditionalGeneration"),xut.forEach(t),yFr=r(T7e," (BART model)"),T7e.forEach(t),wFr=i(pe),nE=s(pe,"LI",{});var F7e=n(nE);i2e=s(F7e,"STRONG",{});var kut=n(i2e);AFr=r(kut,"bert"),kut.forEach(t),LFr=r(F7e," \u2014 "),yX=s(F7e,"A",{href:!0});var Rut=n(yX);BFr=r(Rut,"FlaxBertForPreTraining"),Rut.forEach(t),xFr=r(F7e," (BERT model)"),F7e.forEach(t),kFr=i(pe),lE=s(pe,"LI",{});var C7e=n(lE);d2e=s(C7e,"STRONG",{});var Sut=n(d2e);RFr=r(Sut,"big_bird"),Sut.forEach(t),SFr=r(C7e," \u2014 "),wX=s(C7e,"A",{href:!0});var Put=n(wX);PFr=r(Put,"FlaxBigBirdForPreTraining"),Put.forEach(t),$Fr=r(C7e," (BigBird model)"),C7e.forEach(t),IFr=i(pe),iE=s(pe,"LI",{});var M7e=n(iE);c2e=s(M7e,"STRONG",{});var $ut=n(c2e);DFr=r($ut,"electra"),$ut.forEach(t),jFr=r(M7e," \u2014 "),AX=s(M7e,"A",{href:!0});var Iut=n(AX);NFr=r(Iut,"FlaxElectraForPreTraining"),Iut.forEach(t),qFr=r(M7e," (ELECTRA model)"),M7e.forEach(t),GFr=i(pe),dE=s(pe,"LI",{});var E7e=n(dE);m2e=s(E7e,"STRONG",{});var Dut=n(m2e);OFr=r(Dut,"mbart"),Dut.forEach(t),XFr=r(E7e," \u2014 "),LX=s(E7e,"A",{href:!0});var jut=n(LX);VFr=r(jut,"FlaxMBartForConditionalGeneration"),jut.forEach(t),zFr=r(E7e," (mBART model)"),E7e.forEach(t),WFr=i(pe),cE=s(pe,"LI",{});var y7e=n(cE);f2e=s(y7e,"STRONG",{});var Nut=n(f2e);QFr=r(Nut,"mt5"),Nut.forEach(t),HFr=r(y7e," \u2014 "),BX=s(y7e,"A",{href:!0});var qut=n(BX);UFr=r(qut,"FlaxMT5ForConditionalGeneration"),qut.forEach(t),JFr=r(y7e," (mT5 model)"),y7e.forEach(t),YFr=i(pe),mE=s(pe,"LI",{});var w7e=n(mE);g2e=s(w7e,"STRONG",{});var Gut=n(g2e);KFr=r(Gut,"roberta"),Gut.forEach(t),ZFr=r(w7e," \u2014 "),xX=s(w7e,"A",{href:!0});var Out=n(xX);eCr=r(Out,"FlaxRobertaForMaskedLM"),Out.forEach(t),oCr=r(w7e," (RoBERTa model)"),w7e.forEach(t),rCr=i(pe),fE=s(pe,"LI",{});var A7e=n(fE);h2e=s(A7e,"STRONG",{});var Xut=n(h2e);tCr=r(Xut,"roformer"),Xut.forEach(t),aCr=r(A7e," \u2014 "),kX=s(A7e,"A",{href:!0});var Vut=n(kX);sCr=r(Vut,"FlaxRoFormerForMaskedLM"),Vut.forEach(t),nCr=r(A7e," (RoFormer model)"),A7e.forEach(t),lCr=i(pe),gE=s(pe,"LI",{});var L7e=n(gE);u2e=s(L7e,"STRONG",{});var zut=n(u2e);iCr=r(zut,"t5"),zut.forEach(t),dCr=r(L7e," \u2014 "),RX=s(L7e,"A",{href:!0});var Wut=n(RX);cCr=r(Wut,"FlaxT5ForConditionalGeneration"),Wut.forEach(t),mCr=r(L7e," (T5 model)"),L7e.forEach(t),fCr=i(pe),hE=s(pe,"LI",{});var B7e=n(hE);p2e=s(B7e,"STRONG",{});var Qut=n(p2e);gCr=r(Qut,"wav2vec2"),Qut.forEach(t),hCr=r(B7e," \u2014 "),SX=s(B7e,"A",{href:!0});var Hut=n(SX);uCr=r(Hut,"FlaxWav2Vec2ForPreTraining"),Hut.forEach(t),pCr=r(B7e," (Wav2Vec2 model)"),B7e.forEach(t),pe.forEach(t),_Cr=i(ya),_2e=s(ya,"P",{});var Uut=n(_2e);bCr=r(Uut,"Examples:"),Uut.forEach(t),vCr=i(ya),f(oL.$$.fragment,ya),ya.forEach(t),ii.forEach(t),mxe=i(c),Jc=s(c,"H2",{class:!0});var FRe=n(Jc);uE=s(FRe,"A",{id:!0,class:!0,href:!0});var Jut=n(uE);b2e=s(Jut,"SPAN",{});var Yut=n(b2e);f(rL.$$.fragment,Yut),Yut.forEach(t),Jut.forEach(t),TCr=i(FRe),v2e=s(FRe,"SPAN",{});var Kut=n(v2e);FCr=r(Kut,"FlaxAutoModelForMaskedLM"),Kut.forEach(t),FRe.forEach(t),fxe=i(c),kr=s(c,"DIV",{class:!0});var ci=n(kr);f(tL.$$.fragment,ci),CCr=i(ci),Yc=s(ci,"P",{});var nW=n(Yc);MCr=r(nW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),T2e=s(nW,"CODE",{});var Zut=n(T2e);ECr=r(Zut,"from_pretrained()"),Zut.forEach(t),yCr=r(nW,"class method or the "),F2e=s(nW,"CODE",{});var ept=n(F2e);wCr=r(ept,"from_config()"),ept.forEach(t),ACr=r(nW,`class
method.`),nW.forEach(t),LCr=i(ci),aL=s(ci,"P",{});var CRe=n(aL);BCr=r(CRe,"This class cannot be instantiated directly using "),C2e=s(CRe,"CODE",{});var opt=n(C2e);xCr=r(opt,"__init__()"),opt.forEach(t),kCr=r(CRe," (throws an error)."),CRe.forEach(t),RCr=i(ci),Et=s(ci,"DIV",{class:!0});var mi=n(Et);f(sL.$$.fragment,mi),SCr=i(mi),M2e=s(mi,"P",{});var rpt=n(M2e);PCr=r(rpt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),rpt.forEach(t),$Cr=i(mi),Kc=s(mi,"P",{});var lW=n(Kc);ICr=r(lW,`Note:
Loading a model from its configuration file does `),E2e=s(lW,"STRONG",{});var tpt=n(E2e);DCr=r(tpt,"not"),tpt.forEach(t),jCr=r(lW,` load the model weights. It only affects the
model\u2019s configuration. Use `),y2e=s(lW,"CODE",{});var apt=n(y2e);NCr=r(apt,"from_pretrained()"),apt.forEach(t),qCr=r(lW,"to load the model weights."),lW.forEach(t),GCr=i(mi),w2e=s(mi,"P",{});var spt=n(w2e);OCr=r(spt,"Examples:"),spt.forEach(t),XCr=i(mi),f(nL.$$.fragment,mi),mi.forEach(t),VCr=i(ci),Bo=s(ci,"DIV",{class:!0});var wa=n(Bo);f(lL.$$.fragment,wa),zCr=i(wa),A2e=s(wa,"P",{});var npt=n(A2e);WCr=r(npt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),npt.forEach(t),QCr=i(wa),ws=s(wa,"P",{});var h5=n(ws);HCr=r(h5,"The model class to instantiate is selected based on the "),L2e=s(h5,"CODE",{});var lpt=n(L2e);UCr=r(lpt,"model_type"),lpt.forEach(t),JCr=r(h5,` property of the config object (either
passed as an argument or loaded from `),B2e=s(h5,"CODE",{});var ipt=n(B2e);YCr=r(ipt,"pretrained_model_name_or_path"),ipt.forEach(t),KCr=r(h5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x2e=s(h5,"CODE",{});var dpt=n(x2e);ZCr=r(dpt,"pretrained_model_name_or_path"),dpt.forEach(t),e4r=r(h5,":"),h5.forEach(t),o4r=i(wa),ve=s(wa,"UL",{});var oo=n(ve);pE=s(oo,"LI",{});var x7e=n(pE);k2e=s(x7e,"STRONG",{});var cpt=n(k2e);r4r=r(cpt,"albert"),cpt.forEach(t),t4r=r(x7e," \u2014 "),PX=s(x7e,"A",{href:!0});var mpt=n(PX);a4r=r(mpt,"FlaxAlbertForMaskedLM"),mpt.forEach(t),s4r=r(x7e," (ALBERT model)"),x7e.forEach(t),n4r=i(oo),_E=s(oo,"LI",{});var k7e=n(_E);R2e=s(k7e,"STRONG",{});var fpt=n(R2e);l4r=r(fpt,"bart"),fpt.forEach(t),i4r=r(k7e," \u2014 "),$X=s(k7e,"A",{href:!0});var gpt=n($X);d4r=r(gpt,"FlaxBartForConditionalGeneration"),gpt.forEach(t),c4r=r(k7e," (BART model)"),k7e.forEach(t),m4r=i(oo),bE=s(oo,"LI",{});var R7e=n(bE);S2e=s(R7e,"STRONG",{});var hpt=n(S2e);f4r=r(hpt,"bert"),hpt.forEach(t),g4r=r(R7e," \u2014 "),IX=s(R7e,"A",{href:!0});var upt=n(IX);h4r=r(upt,"FlaxBertForMaskedLM"),upt.forEach(t),u4r=r(R7e," (BERT model)"),R7e.forEach(t),p4r=i(oo),vE=s(oo,"LI",{});var S7e=n(vE);P2e=s(S7e,"STRONG",{});var ppt=n(P2e);_4r=r(ppt,"big_bird"),ppt.forEach(t),b4r=r(S7e," \u2014 "),DX=s(S7e,"A",{href:!0});var _pt=n(DX);v4r=r(_pt,"FlaxBigBirdForMaskedLM"),_pt.forEach(t),T4r=r(S7e," (BigBird model)"),S7e.forEach(t),F4r=i(oo),TE=s(oo,"LI",{});var P7e=n(TE);$2e=s(P7e,"STRONG",{});var bpt=n($2e);C4r=r(bpt,"distilbert"),bpt.forEach(t),M4r=r(P7e," \u2014 "),jX=s(P7e,"A",{href:!0});var vpt=n(jX);E4r=r(vpt,"FlaxDistilBertForMaskedLM"),vpt.forEach(t),y4r=r(P7e," (DistilBERT model)"),P7e.forEach(t),w4r=i(oo),FE=s(oo,"LI",{});var $7e=n(FE);I2e=s($7e,"STRONG",{});var Tpt=n(I2e);A4r=r(Tpt,"electra"),Tpt.forEach(t),L4r=r($7e," \u2014 "),NX=s($7e,"A",{href:!0});var Fpt=n(NX);B4r=r(Fpt,"FlaxElectraForMaskedLM"),Fpt.forEach(t),x4r=r($7e," (ELECTRA model)"),$7e.forEach(t),k4r=i(oo),CE=s(oo,"LI",{});var I7e=n(CE);D2e=s(I7e,"STRONG",{});var Cpt=n(D2e);R4r=r(Cpt,"mbart"),Cpt.forEach(t),S4r=r(I7e," \u2014 "),qX=s(I7e,"A",{href:!0});var Mpt=n(qX);P4r=r(Mpt,"FlaxMBartForConditionalGeneration"),Mpt.forEach(t),$4r=r(I7e," (mBART model)"),I7e.forEach(t),I4r=i(oo),ME=s(oo,"LI",{});var D7e=n(ME);j2e=s(D7e,"STRONG",{});var Ept=n(j2e);D4r=r(Ept,"roberta"),Ept.forEach(t),j4r=r(D7e," \u2014 "),GX=s(D7e,"A",{href:!0});var ypt=n(GX);N4r=r(ypt,"FlaxRobertaForMaskedLM"),ypt.forEach(t),q4r=r(D7e," (RoBERTa model)"),D7e.forEach(t),G4r=i(oo),EE=s(oo,"LI",{});var j7e=n(EE);N2e=s(j7e,"STRONG",{});var wpt=n(N2e);O4r=r(wpt,"roformer"),wpt.forEach(t),X4r=r(j7e," \u2014 "),OX=s(j7e,"A",{href:!0});var Apt=n(OX);V4r=r(Apt,"FlaxRoFormerForMaskedLM"),Apt.forEach(t),z4r=r(j7e," (RoFormer model)"),j7e.forEach(t),oo.forEach(t),W4r=i(wa),q2e=s(wa,"P",{});var Lpt=n(q2e);Q4r=r(Lpt,"Examples:"),Lpt.forEach(t),H4r=i(wa),f(iL.$$.fragment,wa),wa.forEach(t),ci.forEach(t),gxe=i(c),Zc=s(c,"H2",{class:!0});var MRe=n(Zc);yE=s(MRe,"A",{id:!0,class:!0,href:!0});var Bpt=n(yE);G2e=s(Bpt,"SPAN",{});var xpt=n(G2e);f(dL.$$.fragment,xpt),xpt.forEach(t),Bpt.forEach(t),U4r=i(MRe),O2e=s(MRe,"SPAN",{});var kpt=n(O2e);J4r=r(kpt,"FlaxAutoModelForSeq2SeqLM"),kpt.forEach(t),MRe.forEach(t),hxe=i(c),Rr=s(c,"DIV",{class:!0});var fi=n(Rr);f(cL.$$.fragment,fi),Y4r=i(fi),em=s(fi,"P",{});var iW=n(em);K4r=r(iW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),X2e=s(iW,"CODE",{});var Rpt=n(X2e);Z4r=r(Rpt,"from_pretrained()"),Rpt.forEach(t),eMr=r(iW,"class method or the "),V2e=s(iW,"CODE",{});var Spt=n(V2e);oMr=r(Spt,"from_config()"),Spt.forEach(t),rMr=r(iW,`class
method.`),iW.forEach(t),tMr=i(fi),mL=s(fi,"P",{});var ERe=n(mL);aMr=r(ERe,"This class cannot be instantiated directly using "),z2e=s(ERe,"CODE",{});var Ppt=n(z2e);sMr=r(Ppt,"__init__()"),Ppt.forEach(t),nMr=r(ERe," (throws an error)."),ERe.forEach(t),lMr=i(fi),yt=s(fi,"DIV",{class:!0});var gi=n(yt);f(fL.$$.fragment,gi),iMr=i(gi),W2e=s(gi,"P",{});var $pt=n(W2e);dMr=r($pt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$pt.forEach(t),cMr=i(gi),om=s(gi,"P",{});var dW=n(om);mMr=r(dW,`Note:
Loading a model from its configuration file does `),Q2e=s(dW,"STRONG",{});var Ipt=n(Q2e);fMr=r(Ipt,"not"),Ipt.forEach(t),gMr=r(dW,` load the model weights. It only affects the
model\u2019s configuration. Use `),H2e=s(dW,"CODE",{});var Dpt=n(H2e);hMr=r(Dpt,"from_pretrained()"),Dpt.forEach(t),uMr=r(dW,"to load the model weights."),dW.forEach(t),pMr=i(gi),U2e=s(gi,"P",{});var jpt=n(U2e);_Mr=r(jpt,"Examples:"),jpt.forEach(t),bMr=i(gi),f(gL.$$.fragment,gi),gi.forEach(t),vMr=i(fi),xo=s(fi,"DIV",{class:!0});var Aa=n(xo);f(hL.$$.fragment,Aa),TMr=i(Aa),J2e=s(Aa,"P",{});var Npt=n(J2e);FMr=r(Npt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Npt.forEach(t),CMr=i(Aa),As=s(Aa,"P",{});var u5=n(As);MMr=r(u5,"The model class to instantiate is selected based on the "),Y2e=s(u5,"CODE",{});var qpt=n(Y2e);EMr=r(qpt,"model_type"),qpt.forEach(t),yMr=r(u5,` property of the config object (either
passed as an argument or loaded from `),K2e=s(u5,"CODE",{});var Gpt=n(K2e);wMr=r(Gpt,"pretrained_model_name_or_path"),Gpt.forEach(t),AMr=r(u5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Z2e=s(u5,"CODE",{});var Opt=n(Z2e);LMr=r(Opt,"pretrained_model_name_or_path"),Opt.forEach(t),BMr=r(u5,":"),u5.forEach(t),xMr=i(Aa),Te=s(Aa,"UL",{});var ro=n(Te);wE=s(ro,"LI",{});var N7e=n(wE);eve=s(N7e,"STRONG",{});var Xpt=n(eve);kMr=r(Xpt,"bart"),Xpt.forEach(t),RMr=r(N7e," \u2014 "),XX=s(N7e,"A",{href:!0});var Vpt=n(XX);SMr=r(Vpt,"FlaxBartForConditionalGeneration"),Vpt.forEach(t),PMr=r(N7e," (BART model)"),N7e.forEach(t),$Mr=i(ro),AE=s(ro,"LI",{});var q7e=n(AE);ove=s(q7e,"STRONG",{});var zpt=n(ove);IMr=r(zpt,"blenderbot"),zpt.forEach(t),DMr=r(q7e," \u2014 "),VX=s(q7e,"A",{href:!0});var Wpt=n(VX);jMr=r(Wpt,"FlaxBlenderbotForConditionalGeneration"),Wpt.forEach(t),NMr=r(q7e," (Blenderbot model)"),q7e.forEach(t),qMr=i(ro),LE=s(ro,"LI",{});var G7e=n(LE);rve=s(G7e,"STRONG",{});var Qpt=n(rve);GMr=r(Qpt,"blenderbot-small"),Qpt.forEach(t),OMr=r(G7e," \u2014 "),zX=s(G7e,"A",{href:!0});var Hpt=n(zX);XMr=r(Hpt,"FlaxBlenderbotSmallForConditionalGeneration"),Hpt.forEach(t),VMr=r(G7e," (BlenderbotSmall model)"),G7e.forEach(t),zMr=i(ro),BE=s(ro,"LI",{});var O7e=n(BE);tve=s(O7e,"STRONG",{});var Upt=n(tve);WMr=r(Upt,"encoder-decoder"),Upt.forEach(t),QMr=r(O7e," \u2014 "),WX=s(O7e,"A",{href:!0});var Jpt=n(WX);HMr=r(Jpt,"FlaxEncoderDecoderModel"),Jpt.forEach(t),UMr=r(O7e," (Encoder decoder model)"),O7e.forEach(t),JMr=i(ro),xE=s(ro,"LI",{});var X7e=n(xE);ave=s(X7e,"STRONG",{});var Ypt=n(ave);YMr=r(Ypt,"marian"),Ypt.forEach(t),KMr=r(X7e," \u2014 "),QX=s(X7e,"A",{href:!0});var Kpt=n(QX);ZMr=r(Kpt,"FlaxMarianMTModel"),Kpt.forEach(t),eEr=r(X7e," (Marian model)"),X7e.forEach(t),oEr=i(ro),kE=s(ro,"LI",{});var V7e=n(kE);sve=s(V7e,"STRONG",{});var Zpt=n(sve);rEr=r(Zpt,"mbart"),Zpt.forEach(t),tEr=r(V7e," \u2014 "),HX=s(V7e,"A",{href:!0});var e_t=n(HX);aEr=r(e_t,"FlaxMBartForConditionalGeneration"),e_t.forEach(t),sEr=r(V7e," (mBART model)"),V7e.forEach(t),nEr=i(ro),RE=s(ro,"LI",{});var z7e=n(RE);nve=s(z7e,"STRONG",{});var o_t=n(nve);lEr=r(o_t,"mt5"),o_t.forEach(t),iEr=r(z7e," \u2014 "),UX=s(z7e,"A",{href:!0});var r_t=n(UX);dEr=r(r_t,"FlaxMT5ForConditionalGeneration"),r_t.forEach(t),cEr=r(z7e," (mT5 model)"),z7e.forEach(t),mEr=i(ro),SE=s(ro,"LI",{});var W7e=n(SE);lve=s(W7e,"STRONG",{});var t_t=n(lve);fEr=r(t_t,"pegasus"),t_t.forEach(t),gEr=r(W7e," \u2014 "),JX=s(W7e,"A",{href:!0});var a_t=n(JX);hEr=r(a_t,"FlaxPegasusForConditionalGeneration"),a_t.forEach(t),uEr=r(W7e," (Pegasus model)"),W7e.forEach(t),pEr=i(ro),PE=s(ro,"LI",{});var Q7e=n(PE);ive=s(Q7e,"STRONG",{});var s_t=n(ive);_Er=r(s_t,"t5"),s_t.forEach(t),bEr=r(Q7e," \u2014 "),YX=s(Q7e,"A",{href:!0});var n_t=n(YX);vEr=r(n_t,"FlaxT5ForConditionalGeneration"),n_t.forEach(t),TEr=r(Q7e," (T5 model)"),Q7e.forEach(t),ro.forEach(t),FEr=i(Aa),dve=s(Aa,"P",{});var l_t=n(dve);CEr=r(l_t,"Examples:"),l_t.forEach(t),MEr=i(Aa),f(uL.$$.fragment,Aa),Aa.forEach(t),fi.forEach(t),uxe=i(c),rm=s(c,"H2",{class:!0});var yRe=n(rm);$E=s(yRe,"A",{id:!0,class:!0,href:!0});var i_t=n($E);cve=s(i_t,"SPAN",{});var d_t=n(cve);f(pL.$$.fragment,d_t),d_t.forEach(t),i_t.forEach(t),EEr=i(yRe),mve=s(yRe,"SPAN",{});var c_t=n(mve);yEr=r(c_t,"FlaxAutoModelForSequenceClassification"),c_t.forEach(t),yRe.forEach(t),pxe=i(c),Sr=s(c,"DIV",{class:!0});var hi=n(Sr);f(_L.$$.fragment,hi),wEr=i(hi),tm=s(hi,"P",{});var cW=n(tm);AEr=r(cW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),fve=s(cW,"CODE",{});var m_t=n(fve);LEr=r(m_t,"from_pretrained()"),m_t.forEach(t),BEr=r(cW,"class method or the "),gve=s(cW,"CODE",{});var f_t=n(gve);xEr=r(f_t,"from_config()"),f_t.forEach(t),kEr=r(cW,`class
method.`),cW.forEach(t),REr=i(hi),bL=s(hi,"P",{});var wRe=n(bL);SEr=r(wRe,"This class cannot be instantiated directly using "),hve=s(wRe,"CODE",{});var g_t=n(hve);PEr=r(g_t,"__init__()"),g_t.forEach(t),$Er=r(wRe," (throws an error)."),wRe.forEach(t),IEr=i(hi),wt=s(hi,"DIV",{class:!0});var ui=n(wt);f(vL.$$.fragment,ui),DEr=i(ui),uve=s(ui,"P",{});var h_t=n(uve);jEr=r(h_t,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),h_t.forEach(t),NEr=i(ui),am=s(ui,"P",{});var mW=n(am);qEr=r(mW,`Note:
Loading a model from its configuration file does `),pve=s(mW,"STRONG",{});var u_t=n(pve);GEr=r(u_t,"not"),u_t.forEach(t),OEr=r(mW,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ve=s(mW,"CODE",{});var p_t=n(_ve);XEr=r(p_t,"from_pretrained()"),p_t.forEach(t),VEr=r(mW,"to load the model weights."),mW.forEach(t),zEr=i(ui),bve=s(ui,"P",{});var __t=n(bve);WEr=r(__t,"Examples:"),__t.forEach(t),QEr=i(ui),f(TL.$$.fragment,ui),ui.forEach(t),HEr=i(hi),ko=s(hi,"DIV",{class:!0});var La=n(ko);f(FL.$$.fragment,La),UEr=i(La),vve=s(La,"P",{});var b_t=n(vve);JEr=r(b_t,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),b_t.forEach(t),YEr=i(La),Ls=s(La,"P",{});var p5=n(Ls);KEr=r(p5,"The model class to instantiate is selected based on the "),Tve=s(p5,"CODE",{});var v_t=n(Tve);ZEr=r(v_t,"model_type"),v_t.forEach(t),e3r=r(p5,` property of the config object (either
passed as an argument or loaded from `),Fve=s(p5,"CODE",{});var T_t=n(Fve);o3r=r(T_t,"pretrained_model_name_or_path"),T_t.forEach(t),r3r=r(p5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cve=s(p5,"CODE",{});var F_t=n(Cve);t3r=r(F_t,"pretrained_model_name_or_path"),F_t.forEach(t),a3r=r(p5,":"),p5.forEach(t),s3r=i(La),Fe=s(La,"UL",{});var to=n(Fe);IE=s(to,"LI",{});var H7e=n(IE);Mve=s(H7e,"STRONG",{});var C_t=n(Mve);n3r=r(C_t,"albert"),C_t.forEach(t),l3r=r(H7e," \u2014 "),KX=s(H7e,"A",{href:!0});var M_t=n(KX);i3r=r(M_t,"FlaxAlbertForSequenceClassification"),M_t.forEach(t),d3r=r(H7e," (ALBERT model)"),H7e.forEach(t),c3r=i(to),DE=s(to,"LI",{});var U7e=n(DE);Eve=s(U7e,"STRONG",{});var E_t=n(Eve);m3r=r(E_t,"bart"),E_t.forEach(t),f3r=r(U7e," \u2014 "),ZX=s(U7e,"A",{href:!0});var y_t=n(ZX);g3r=r(y_t,"FlaxBartForSequenceClassification"),y_t.forEach(t),h3r=r(U7e," (BART model)"),U7e.forEach(t),u3r=i(to),jE=s(to,"LI",{});var J7e=n(jE);yve=s(J7e,"STRONG",{});var w_t=n(yve);p3r=r(w_t,"bert"),w_t.forEach(t),_3r=r(J7e," \u2014 "),eV=s(J7e,"A",{href:!0});var A_t=n(eV);b3r=r(A_t,"FlaxBertForSequenceClassification"),A_t.forEach(t),v3r=r(J7e," (BERT model)"),J7e.forEach(t),T3r=i(to),NE=s(to,"LI",{});var Y7e=n(NE);wve=s(Y7e,"STRONG",{});var L_t=n(wve);F3r=r(L_t,"big_bird"),L_t.forEach(t),C3r=r(Y7e," \u2014 "),oV=s(Y7e,"A",{href:!0});var B_t=n(oV);M3r=r(B_t,"FlaxBigBirdForSequenceClassification"),B_t.forEach(t),E3r=r(Y7e," (BigBird model)"),Y7e.forEach(t),y3r=i(to),qE=s(to,"LI",{});var K7e=n(qE);Ave=s(K7e,"STRONG",{});var x_t=n(Ave);w3r=r(x_t,"distilbert"),x_t.forEach(t),A3r=r(K7e," \u2014 "),rV=s(K7e,"A",{href:!0});var k_t=n(rV);L3r=r(k_t,"FlaxDistilBertForSequenceClassification"),k_t.forEach(t),B3r=r(K7e," (DistilBERT model)"),K7e.forEach(t),x3r=i(to),GE=s(to,"LI",{});var Z7e=n(GE);Lve=s(Z7e,"STRONG",{});var R_t=n(Lve);k3r=r(R_t,"electra"),R_t.forEach(t),R3r=r(Z7e," \u2014 "),tV=s(Z7e,"A",{href:!0});var S_t=n(tV);S3r=r(S_t,"FlaxElectraForSequenceClassification"),S_t.forEach(t),P3r=r(Z7e," (ELECTRA model)"),Z7e.forEach(t),$3r=i(to),OE=s(to,"LI",{});var e9e=n(OE);Bve=s(e9e,"STRONG",{});var P_t=n(Bve);I3r=r(P_t,"mbart"),P_t.forEach(t),D3r=r(e9e," \u2014 "),aV=s(e9e,"A",{href:!0});var $_t=n(aV);j3r=r($_t,"FlaxMBartForSequenceClassification"),$_t.forEach(t),N3r=r(e9e," (mBART model)"),e9e.forEach(t),q3r=i(to),XE=s(to,"LI",{});var o9e=n(XE);xve=s(o9e,"STRONG",{});var I_t=n(xve);G3r=r(I_t,"roberta"),I_t.forEach(t),O3r=r(o9e," \u2014 "),sV=s(o9e,"A",{href:!0});var D_t=n(sV);X3r=r(D_t,"FlaxRobertaForSequenceClassification"),D_t.forEach(t),V3r=r(o9e," (RoBERTa model)"),o9e.forEach(t),z3r=i(to),VE=s(to,"LI",{});var r9e=n(VE);kve=s(r9e,"STRONG",{});var j_t=n(kve);W3r=r(j_t,"roformer"),j_t.forEach(t),Q3r=r(r9e," \u2014 "),nV=s(r9e,"A",{href:!0});var N_t=n(nV);H3r=r(N_t,"FlaxRoFormerForSequenceClassification"),N_t.forEach(t),U3r=r(r9e," (RoFormer model)"),r9e.forEach(t),to.forEach(t),J3r=i(La),Rve=s(La,"P",{});var q_t=n(Rve);Y3r=r(q_t,"Examples:"),q_t.forEach(t),K3r=i(La),f(CL.$$.fragment,La),La.forEach(t),hi.forEach(t),_xe=i(c),sm=s(c,"H2",{class:!0});var ARe=n(sm);zE=s(ARe,"A",{id:!0,class:!0,href:!0});var G_t=n(zE);Sve=s(G_t,"SPAN",{});var O_t=n(Sve);f(ML.$$.fragment,O_t),O_t.forEach(t),G_t.forEach(t),Z3r=i(ARe),Pve=s(ARe,"SPAN",{});var X_t=n(Pve);e5r=r(X_t,"FlaxAutoModelForQuestionAnswering"),X_t.forEach(t),ARe.forEach(t),bxe=i(c),Pr=s(c,"DIV",{class:!0});var pi=n(Pr);f(EL.$$.fragment,pi),o5r=i(pi),nm=s(pi,"P",{});var fW=n(nm);r5r=r(fW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),$ve=s(fW,"CODE",{});var V_t=n($ve);t5r=r(V_t,"from_pretrained()"),V_t.forEach(t),a5r=r(fW,"class method or the "),Ive=s(fW,"CODE",{});var z_t=n(Ive);s5r=r(z_t,"from_config()"),z_t.forEach(t),n5r=r(fW,`class
method.`),fW.forEach(t),l5r=i(pi),yL=s(pi,"P",{});var LRe=n(yL);i5r=r(LRe,"This class cannot be instantiated directly using "),Dve=s(LRe,"CODE",{});var W_t=n(Dve);d5r=r(W_t,"__init__()"),W_t.forEach(t),c5r=r(LRe," (throws an error)."),LRe.forEach(t),m5r=i(pi),At=s(pi,"DIV",{class:!0});var _i=n(At);f(wL.$$.fragment,_i),f5r=i(_i),jve=s(_i,"P",{});var Q_t=n(jve);g5r=r(Q_t,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Q_t.forEach(t),h5r=i(_i),lm=s(_i,"P",{});var gW=n(lm);u5r=r(gW,`Note:
Loading a model from its configuration file does `),Nve=s(gW,"STRONG",{});var H_t=n(Nve);p5r=r(H_t,"not"),H_t.forEach(t),_5r=r(gW,` load the model weights. It only affects the
model\u2019s configuration. Use `),qve=s(gW,"CODE",{});var U_t=n(qve);b5r=r(U_t,"from_pretrained()"),U_t.forEach(t),v5r=r(gW,"to load the model weights."),gW.forEach(t),T5r=i(_i),Gve=s(_i,"P",{});var J_t=n(Gve);F5r=r(J_t,"Examples:"),J_t.forEach(t),C5r=i(_i),f(AL.$$.fragment,_i),_i.forEach(t),M5r=i(pi),Ro=s(pi,"DIV",{class:!0});var Ba=n(Ro);f(LL.$$.fragment,Ba),E5r=i(Ba),Ove=s(Ba,"P",{});var Y_t=n(Ove);y5r=r(Y_t,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Y_t.forEach(t),w5r=i(Ba),Bs=s(Ba,"P",{});var _5=n(Bs);A5r=r(_5,"The model class to instantiate is selected based on the "),Xve=s(_5,"CODE",{});var K_t=n(Xve);L5r=r(K_t,"model_type"),K_t.forEach(t),B5r=r(_5,` property of the config object (either
passed as an argument or loaded from `),Vve=s(_5,"CODE",{});var Z_t=n(Vve);x5r=r(Z_t,"pretrained_model_name_or_path"),Z_t.forEach(t),k5r=r(_5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zve=s(_5,"CODE",{});var ebt=n(zve);R5r=r(ebt,"pretrained_model_name_or_path"),ebt.forEach(t),S5r=r(_5,":"),_5.forEach(t),P5r=i(Ba),Ce=s(Ba,"UL",{});var ao=n(Ce);WE=s(ao,"LI",{});var t9e=n(WE);Wve=s(t9e,"STRONG",{});var obt=n(Wve);$5r=r(obt,"albert"),obt.forEach(t),I5r=r(t9e," \u2014 "),lV=s(t9e,"A",{href:!0});var rbt=n(lV);D5r=r(rbt,"FlaxAlbertForQuestionAnswering"),rbt.forEach(t),j5r=r(t9e," (ALBERT model)"),t9e.forEach(t),N5r=i(ao),QE=s(ao,"LI",{});var a9e=n(QE);Qve=s(a9e,"STRONG",{});var tbt=n(Qve);q5r=r(tbt,"bart"),tbt.forEach(t),G5r=r(a9e," \u2014 "),iV=s(a9e,"A",{href:!0});var abt=n(iV);O5r=r(abt,"FlaxBartForQuestionAnswering"),abt.forEach(t),X5r=r(a9e," (BART model)"),a9e.forEach(t),V5r=i(ao),HE=s(ao,"LI",{});var s9e=n(HE);Hve=s(s9e,"STRONG",{});var sbt=n(Hve);z5r=r(sbt,"bert"),sbt.forEach(t),W5r=r(s9e," \u2014 "),dV=s(s9e,"A",{href:!0});var nbt=n(dV);Q5r=r(nbt,"FlaxBertForQuestionAnswering"),nbt.forEach(t),H5r=r(s9e," (BERT model)"),s9e.forEach(t),U5r=i(ao),UE=s(ao,"LI",{});var n9e=n(UE);Uve=s(n9e,"STRONG",{});var lbt=n(Uve);J5r=r(lbt,"big_bird"),lbt.forEach(t),Y5r=r(n9e," \u2014 "),cV=s(n9e,"A",{href:!0});var ibt=n(cV);K5r=r(ibt,"FlaxBigBirdForQuestionAnswering"),ibt.forEach(t),Z5r=r(n9e," (BigBird model)"),n9e.forEach(t),eyr=i(ao),JE=s(ao,"LI",{});var l9e=n(JE);Jve=s(l9e,"STRONG",{});var dbt=n(Jve);oyr=r(dbt,"distilbert"),dbt.forEach(t),ryr=r(l9e," \u2014 "),mV=s(l9e,"A",{href:!0});var cbt=n(mV);tyr=r(cbt,"FlaxDistilBertForQuestionAnswering"),cbt.forEach(t),ayr=r(l9e," (DistilBERT model)"),l9e.forEach(t),syr=i(ao),YE=s(ao,"LI",{});var i9e=n(YE);Yve=s(i9e,"STRONG",{});var mbt=n(Yve);nyr=r(mbt,"electra"),mbt.forEach(t),lyr=r(i9e," \u2014 "),fV=s(i9e,"A",{href:!0});var fbt=n(fV);iyr=r(fbt,"FlaxElectraForQuestionAnswering"),fbt.forEach(t),dyr=r(i9e," (ELECTRA model)"),i9e.forEach(t),cyr=i(ao),KE=s(ao,"LI",{});var d9e=n(KE);Kve=s(d9e,"STRONG",{});var gbt=n(Kve);myr=r(gbt,"mbart"),gbt.forEach(t),fyr=r(d9e," \u2014 "),gV=s(d9e,"A",{href:!0});var hbt=n(gV);gyr=r(hbt,"FlaxMBartForQuestionAnswering"),hbt.forEach(t),hyr=r(d9e," (mBART model)"),d9e.forEach(t),uyr=i(ao),ZE=s(ao,"LI",{});var c9e=n(ZE);Zve=s(c9e,"STRONG",{});var ubt=n(Zve);pyr=r(ubt,"roberta"),ubt.forEach(t),_yr=r(c9e," \u2014 "),hV=s(c9e,"A",{href:!0});var pbt=n(hV);byr=r(pbt,"FlaxRobertaForQuestionAnswering"),pbt.forEach(t),vyr=r(c9e," (RoBERTa model)"),c9e.forEach(t),Tyr=i(ao),e3=s(ao,"LI",{});var m9e=n(e3);eTe=s(m9e,"STRONG",{});var _bt=n(eTe);Fyr=r(_bt,"roformer"),_bt.forEach(t),Cyr=r(m9e," \u2014 "),uV=s(m9e,"A",{href:!0});var bbt=n(uV);Myr=r(bbt,"FlaxRoFormerForQuestionAnswering"),bbt.forEach(t),Eyr=r(m9e," (RoFormer model)"),m9e.forEach(t),ao.forEach(t),yyr=i(Ba),oTe=s(Ba,"P",{});var vbt=n(oTe);wyr=r(vbt,"Examples:"),vbt.forEach(t),Ayr=i(Ba),f(BL.$$.fragment,Ba),Ba.forEach(t),pi.forEach(t),vxe=i(c),im=s(c,"H2",{class:!0});var BRe=n(im);o3=s(BRe,"A",{id:!0,class:!0,href:!0});var Tbt=n(o3);rTe=s(Tbt,"SPAN",{});var Fbt=n(rTe);f(xL.$$.fragment,Fbt),Fbt.forEach(t),Tbt.forEach(t),Lyr=i(BRe),tTe=s(BRe,"SPAN",{});var Cbt=n(tTe);Byr=r(Cbt,"FlaxAutoModelForTokenClassification"),Cbt.forEach(t),BRe.forEach(t),Txe=i(c),$r=s(c,"DIV",{class:!0});var bi=n($r);f(kL.$$.fragment,bi),xyr=i(bi),dm=s(bi,"P",{});var hW=n(dm);kyr=r(hW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),aTe=s(hW,"CODE",{});var Mbt=n(aTe);Ryr=r(Mbt,"from_pretrained()"),Mbt.forEach(t),Syr=r(hW,"class method or the "),sTe=s(hW,"CODE",{});var Ebt=n(sTe);Pyr=r(Ebt,"from_config()"),Ebt.forEach(t),$yr=r(hW,`class
method.`),hW.forEach(t),Iyr=i(bi),RL=s(bi,"P",{});var xRe=n(RL);Dyr=r(xRe,"This class cannot be instantiated directly using "),nTe=s(xRe,"CODE",{});var ybt=n(nTe);jyr=r(ybt,"__init__()"),ybt.forEach(t),Nyr=r(xRe," (throws an error)."),xRe.forEach(t),qyr=i(bi),Lt=s(bi,"DIV",{class:!0});var vi=n(Lt);f(SL.$$.fragment,vi),Gyr=i(vi),lTe=s(vi,"P",{});var wbt=n(lTe);Oyr=r(wbt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),wbt.forEach(t),Xyr=i(vi),cm=s(vi,"P",{});var uW=n(cm);Vyr=r(uW,`Note:
Loading a model from its configuration file does `),iTe=s(uW,"STRONG",{});var Abt=n(iTe);zyr=r(Abt,"not"),Abt.forEach(t),Wyr=r(uW,` load the model weights. It only affects the
model\u2019s configuration. Use `),dTe=s(uW,"CODE",{});var Lbt=n(dTe);Qyr=r(Lbt,"from_pretrained()"),Lbt.forEach(t),Hyr=r(uW,"to load the model weights."),uW.forEach(t),Uyr=i(vi),cTe=s(vi,"P",{});var Bbt=n(cTe);Jyr=r(Bbt,"Examples:"),Bbt.forEach(t),Yyr=i(vi),f(PL.$$.fragment,vi),vi.forEach(t),Kyr=i(bi),So=s(bi,"DIV",{class:!0});var xa=n(So);f($L.$$.fragment,xa),Zyr=i(xa),mTe=s(xa,"P",{});var xbt=n(mTe);ewr=r(xbt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),xbt.forEach(t),owr=i(xa),xs=s(xa,"P",{});var b5=n(xs);rwr=r(b5,"The model class to instantiate is selected based on the "),fTe=s(b5,"CODE",{});var kbt=n(fTe);twr=r(kbt,"model_type"),kbt.forEach(t),awr=r(b5,` property of the config object (either
passed as an argument or loaded from `),gTe=s(b5,"CODE",{});var Rbt=n(gTe);swr=r(Rbt,"pretrained_model_name_or_path"),Rbt.forEach(t),nwr=r(b5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hTe=s(b5,"CODE",{});var Sbt=n(hTe);lwr=r(Sbt,"pretrained_model_name_or_path"),Sbt.forEach(t),iwr=r(b5,":"),b5.forEach(t),dwr=i(xa),no=s(xa,"UL",{});var ta=n(no);r3=s(ta,"LI",{});var f9e=n(r3);uTe=s(f9e,"STRONG",{});var Pbt=n(uTe);cwr=r(Pbt,"albert"),Pbt.forEach(t),mwr=r(f9e," \u2014 "),pV=s(f9e,"A",{href:!0});var $bt=n(pV);fwr=r($bt,"FlaxAlbertForTokenClassification"),$bt.forEach(t),gwr=r(f9e," (ALBERT model)"),f9e.forEach(t),hwr=i(ta),t3=s(ta,"LI",{});var g9e=n(t3);pTe=s(g9e,"STRONG",{});var Ibt=n(pTe);uwr=r(Ibt,"bert"),Ibt.forEach(t),pwr=r(g9e," \u2014 "),_V=s(g9e,"A",{href:!0});var Dbt=n(_V);_wr=r(Dbt,"FlaxBertForTokenClassification"),Dbt.forEach(t),bwr=r(g9e," (BERT model)"),g9e.forEach(t),vwr=i(ta),a3=s(ta,"LI",{});var h9e=n(a3);_Te=s(h9e,"STRONG",{});var jbt=n(_Te);Twr=r(jbt,"big_bird"),jbt.forEach(t),Fwr=r(h9e," \u2014 "),bV=s(h9e,"A",{href:!0});var Nbt=n(bV);Cwr=r(Nbt,"FlaxBigBirdForTokenClassification"),Nbt.forEach(t),Mwr=r(h9e," (BigBird model)"),h9e.forEach(t),Ewr=i(ta),s3=s(ta,"LI",{});var u9e=n(s3);bTe=s(u9e,"STRONG",{});var qbt=n(bTe);ywr=r(qbt,"distilbert"),qbt.forEach(t),wwr=r(u9e," \u2014 "),vV=s(u9e,"A",{href:!0});var Gbt=n(vV);Awr=r(Gbt,"FlaxDistilBertForTokenClassification"),Gbt.forEach(t),Lwr=r(u9e," (DistilBERT model)"),u9e.forEach(t),Bwr=i(ta),n3=s(ta,"LI",{});var p9e=n(n3);vTe=s(p9e,"STRONG",{});var Obt=n(vTe);xwr=r(Obt,"electra"),Obt.forEach(t),kwr=r(p9e," \u2014 "),TV=s(p9e,"A",{href:!0});var Xbt=n(TV);Rwr=r(Xbt,"FlaxElectraForTokenClassification"),Xbt.forEach(t),Swr=r(p9e," (ELECTRA model)"),p9e.forEach(t),Pwr=i(ta),l3=s(ta,"LI",{});var _9e=n(l3);TTe=s(_9e,"STRONG",{});var Vbt=n(TTe);$wr=r(Vbt,"roberta"),Vbt.forEach(t),Iwr=r(_9e," \u2014 "),FV=s(_9e,"A",{href:!0});var zbt=n(FV);Dwr=r(zbt,"FlaxRobertaForTokenClassification"),zbt.forEach(t),jwr=r(_9e," (RoBERTa model)"),_9e.forEach(t),Nwr=i(ta),i3=s(ta,"LI",{});var b9e=n(i3);FTe=s(b9e,"STRONG",{});var Wbt=n(FTe);qwr=r(Wbt,"roformer"),Wbt.forEach(t),Gwr=r(b9e," \u2014 "),CV=s(b9e,"A",{href:!0});var Qbt=n(CV);Owr=r(Qbt,"FlaxRoFormerForTokenClassification"),Qbt.forEach(t),Xwr=r(b9e," (RoFormer model)"),b9e.forEach(t),ta.forEach(t),Vwr=i(xa),CTe=s(xa,"P",{});var Hbt=n(CTe);zwr=r(Hbt,"Examples:"),Hbt.forEach(t),Wwr=i(xa),f(IL.$$.fragment,xa),xa.forEach(t),bi.forEach(t),Fxe=i(c),mm=s(c,"H2",{class:!0});var kRe=n(mm);d3=s(kRe,"A",{id:!0,class:!0,href:!0});var Ubt=n(d3);MTe=s(Ubt,"SPAN",{});var Jbt=n(MTe);f(DL.$$.fragment,Jbt),Jbt.forEach(t),Ubt.forEach(t),Qwr=i(kRe),ETe=s(kRe,"SPAN",{});var Ybt=n(ETe);Hwr=r(Ybt,"FlaxAutoModelForMultipleChoice"),Ybt.forEach(t),kRe.forEach(t),Cxe=i(c),Ir=s(c,"DIV",{class:!0});var Ti=n(Ir);f(jL.$$.fragment,Ti),Uwr=i(Ti),fm=s(Ti,"P",{});var pW=n(fm);Jwr=r(pW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),yTe=s(pW,"CODE",{});var Kbt=n(yTe);Ywr=r(Kbt,"from_pretrained()"),Kbt.forEach(t),Kwr=r(pW,"class method or the "),wTe=s(pW,"CODE",{});var Zbt=n(wTe);Zwr=r(Zbt,"from_config()"),Zbt.forEach(t),e6r=r(pW,`class
method.`),pW.forEach(t),o6r=i(Ti),NL=s(Ti,"P",{});var RRe=n(NL);r6r=r(RRe,"This class cannot be instantiated directly using "),ATe=s(RRe,"CODE",{});var e2t=n(ATe);t6r=r(e2t,"__init__()"),e2t.forEach(t),a6r=r(RRe," (throws an error)."),RRe.forEach(t),s6r=i(Ti),Bt=s(Ti,"DIV",{class:!0});var Fi=n(Bt);f(qL.$$.fragment,Fi),n6r=i(Fi),LTe=s(Fi,"P",{});var o2t=n(LTe);l6r=r(o2t,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),o2t.forEach(t),i6r=i(Fi),gm=s(Fi,"P",{});var _W=n(gm);d6r=r(_W,`Note:
Loading a model from its configuration file does `),BTe=s(_W,"STRONG",{});var r2t=n(BTe);c6r=r(r2t,"not"),r2t.forEach(t),m6r=r(_W,` load the model weights. It only affects the
model\u2019s configuration. Use `),xTe=s(_W,"CODE",{});var t2t=n(xTe);f6r=r(t2t,"from_pretrained()"),t2t.forEach(t),g6r=r(_W,"to load the model weights."),_W.forEach(t),h6r=i(Fi),kTe=s(Fi,"P",{});var a2t=n(kTe);u6r=r(a2t,"Examples:"),a2t.forEach(t),p6r=i(Fi),f(GL.$$.fragment,Fi),Fi.forEach(t),_6r=i(Ti),Po=s(Ti,"DIV",{class:!0});var ka=n(Po);f(OL.$$.fragment,ka),b6r=i(ka),RTe=s(ka,"P",{});var s2t=n(RTe);v6r=r(s2t,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),s2t.forEach(t),T6r=i(ka),ks=s(ka,"P",{});var v5=n(ks);F6r=r(v5,"The model class to instantiate is selected based on the "),STe=s(v5,"CODE",{});var n2t=n(STe);C6r=r(n2t,"model_type"),n2t.forEach(t),M6r=r(v5,` property of the config object (either
passed as an argument or loaded from `),PTe=s(v5,"CODE",{});var l2t=n(PTe);E6r=r(l2t,"pretrained_model_name_or_path"),l2t.forEach(t),y6r=r(v5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$Te=s(v5,"CODE",{});var i2t=n($Te);w6r=r(i2t,"pretrained_model_name_or_path"),i2t.forEach(t),A6r=r(v5,":"),v5.forEach(t),L6r=i(ka),lo=s(ka,"UL",{});var aa=n(lo);c3=s(aa,"LI",{});var v9e=n(c3);ITe=s(v9e,"STRONG",{});var d2t=n(ITe);B6r=r(d2t,"albert"),d2t.forEach(t),x6r=r(v9e," \u2014 "),MV=s(v9e,"A",{href:!0});var c2t=n(MV);k6r=r(c2t,"FlaxAlbertForMultipleChoice"),c2t.forEach(t),R6r=r(v9e," (ALBERT model)"),v9e.forEach(t),S6r=i(aa),m3=s(aa,"LI",{});var T9e=n(m3);DTe=s(T9e,"STRONG",{});var m2t=n(DTe);P6r=r(m2t,"bert"),m2t.forEach(t),$6r=r(T9e," \u2014 "),EV=s(T9e,"A",{href:!0});var f2t=n(EV);I6r=r(f2t,"FlaxBertForMultipleChoice"),f2t.forEach(t),D6r=r(T9e," (BERT model)"),T9e.forEach(t),j6r=i(aa),f3=s(aa,"LI",{});var F9e=n(f3);jTe=s(F9e,"STRONG",{});var g2t=n(jTe);N6r=r(g2t,"big_bird"),g2t.forEach(t),q6r=r(F9e," \u2014 "),yV=s(F9e,"A",{href:!0});var h2t=n(yV);G6r=r(h2t,"FlaxBigBirdForMultipleChoice"),h2t.forEach(t),O6r=r(F9e," (BigBird model)"),F9e.forEach(t),X6r=i(aa),g3=s(aa,"LI",{});var C9e=n(g3);NTe=s(C9e,"STRONG",{});var u2t=n(NTe);V6r=r(u2t,"distilbert"),u2t.forEach(t),z6r=r(C9e," \u2014 "),wV=s(C9e,"A",{href:!0});var p2t=n(wV);W6r=r(p2t,"FlaxDistilBertForMultipleChoice"),p2t.forEach(t),Q6r=r(C9e," (DistilBERT model)"),C9e.forEach(t),H6r=i(aa),h3=s(aa,"LI",{});var M9e=n(h3);qTe=s(M9e,"STRONG",{});var _2t=n(qTe);U6r=r(_2t,"electra"),_2t.forEach(t),J6r=r(M9e," \u2014 "),AV=s(M9e,"A",{href:!0});var b2t=n(AV);Y6r=r(b2t,"FlaxElectraForMultipleChoice"),b2t.forEach(t),K6r=r(M9e," (ELECTRA model)"),M9e.forEach(t),Z6r=i(aa),u3=s(aa,"LI",{});var E9e=n(u3);GTe=s(E9e,"STRONG",{});var v2t=n(GTe);eAr=r(v2t,"roberta"),v2t.forEach(t),oAr=r(E9e," \u2014 "),LV=s(E9e,"A",{href:!0});var T2t=n(LV);rAr=r(T2t,"FlaxRobertaForMultipleChoice"),T2t.forEach(t),tAr=r(E9e," (RoBERTa model)"),E9e.forEach(t),aAr=i(aa),p3=s(aa,"LI",{});var y9e=n(p3);OTe=s(y9e,"STRONG",{});var F2t=n(OTe);sAr=r(F2t,"roformer"),F2t.forEach(t),nAr=r(y9e," \u2014 "),BV=s(y9e,"A",{href:!0});var C2t=n(BV);lAr=r(C2t,"FlaxRoFormerForMultipleChoice"),C2t.forEach(t),iAr=r(y9e," (RoFormer model)"),y9e.forEach(t),aa.forEach(t),dAr=i(ka),XTe=s(ka,"P",{});var M2t=n(XTe);cAr=r(M2t,"Examples:"),M2t.forEach(t),mAr=i(ka),f(XL.$$.fragment,ka),ka.forEach(t),Ti.forEach(t),Mxe=i(c),hm=s(c,"H2",{class:!0});var SRe=n(hm);_3=s(SRe,"A",{id:!0,class:!0,href:!0});var E2t=n(_3);VTe=s(E2t,"SPAN",{});var y2t=n(VTe);f(VL.$$.fragment,y2t),y2t.forEach(t),E2t.forEach(t),fAr=i(SRe),zTe=s(SRe,"SPAN",{});var w2t=n(zTe);gAr=r(w2t,"FlaxAutoModelForNextSentencePrediction"),w2t.forEach(t),SRe.forEach(t),Exe=i(c),Dr=s(c,"DIV",{class:!0});var Ci=n(Dr);f(zL.$$.fragment,Ci),hAr=i(Ci),um=s(Ci,"P",{});var bW=n(um);uAr=r(bW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),WTe=s(bW,"CODE",{});var A2t=n(WTe);pAr=r(A2t,"from_pretrained()"),A2t.forEach(t),_Ar=r(bW,"class method or the "),QTe=s(bW,"CODE",{});var L2t=n(QTe);bAr=r(L2t,"from_config()"),L2t.forEach(t),vAr=r(bW,`class
method.`),bW.forEach(t),TAr=i(Ci),WL=s(Ci,"P",{});var PRe=n(WL);FAr=r(PRe,"This class cannot be instantiated directly using "),HTe=s(PRe,"CODE",{});var B2t=n(HTe);CAr=r(B2t,"__init__()"),B2t.forEach(t),MAr=r(PRe," (throws an error)."),PRe.forEach(t),EAr=i(Ci),xt=s(Ci,"DIV",{class:!0});var Mi=n(xt);f(QL.$$.fragment,Mi),yAr=i(Mi),UTe=s(Mi,"P",{});var x2t=n(UTe);wAr=r(x2t,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),x2t.forEach(t),AAr=i(Mi),pm=s(Mi,"P",{});var vW=n(pm);LAr=r(vW,`Note:
Loading a model from its configuration file does `),JTe=s(vW,"STRONG",{});var k2t=n(JTe);BAr=r(k2t,"not"),k2t.forEach(t),xAr=r(vW,` load the model weights. It only affects the
model\u2019s configuration. Use `),YTe=s(vW,"CODE",{});var R2t=n(YTe);kAr=r(R2t,"from_pretrained()"),R2t.forEach(t),RAr=r(vW,"to load the model weights."),vW.forEach(t),SAr=i(Mi),KTe=s(Mi,"P",{});var S2t=n(KTe);PAr=r(S2t,"Examples:"),S2t.forEach(t),$Ar=i(Mi),f(HL.$$.fragment,Mi),Mi.forEach(t),IAr=i(Ci),$o=s(Ci,"DIV",{class:!0});var Ra=n($o);f(UL.$$.fragment,Ra),DAr=i(Ra),ZTe=s(Ra,"P",{});var P2t=n(ZTe);jAr=r(P2t,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),P2t.forEach(t),NAr=i(Ra),Rs=s(Ra,"P",{});var T5=n(Rs);qAr=r(T5,"The model class to instantiate is selected based on the "),e1e=s(T5,"CODE",{});var $2t=n(e1e);GAr=r($2t,"model_type"),$2t.forEach(t),OAr=r(T5,` property of the config object (either
passed as an argument or loaded from `),o1e=s(T5,"CODE",{});var I2t=n(o1e);XAr=r(I2t,"pretrained_model_name_or_path"),I2t.forEach(t),VAr=r(T5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),r1e=s(T5,"CODE",{});var D2t=n(r1e);zAr=r(D2t,"pretrained_model_name_or_path"),D2t.forEach(t),WAr=r(T5,":"),T5.forEach(t),QAr=i(Ra),t1e=s(Ra,"UL",{});var j2t=n(t1e);b3=s(j2t,"LI",{});var w9e=n(b3);a1e=s(w9e,"STRONG",{});var N2t=n(a1e);HAr=r(N2t,"bert"),N2t.forEach(t),UAr=r(w9e," \u2014 "),xV=s(w9e,"A",{href:!0});var q2t=n(xV);JAr=r(q2t,"FlaxBertForNextSentencePrediction"),q2t.forEach(t),YAr=r(w9e," (BERT model)"),w9e.forEach(t),j2t.forEach(t),KAr=i(Ra),s1e=s(Ra,"P",{});var G2t=n(s1e);ZAr=r(G2t,"Examples:"),G2t.forEach(t),e0r=i(Ra),f(JL.$$.fragment,Ra),Ra.forEach(t),Ci.forEach(t),yxe=i(c),_m=s(c,"H2",{class:!0});var $Re=n(_m);v3=s($Re,"A",{id:!0,class:!0,href:!0});var O2t=n(v3);n1e=s(O2t,"SPAN",{});var X2t=n(n1e);f(YL.$$.fragment,X2t),X2t.forEach(t),O2t.forEach(t),o0r=i($Re),l1e=s($Re,"SPAN",{});var V2t=n(l1e);r0r=r(V2t,"FlaxAutoModelForImageClassification"),V2t.forEach(t),$Re.forEach(t),wxe=i(c),jr=s(c,"DIV",{class:!0});var Ei=n(jr);f(KL.$$.fragment,Ei),t0r=i(Ei),bm=s(Ei,"P",{});var TW=n(bm);a0r=r(TW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),i1e=s(TW,"CODE",{});var z2t=n(i1e);s0r=r(z2t,"from_pretrained()"),z2t.forEach(t),n0r=r(TW,"class method or the "),d1e=s(TW,"CODE",{});var W2t=n(d1e);l0r=r(W2t,"from_config()"),W2t.forEach(t),i0r=r(TW,`class
method.`),TW.forEach(t),d0r=i(Ei),ZL=s(Ei,"P",{});var IRe=n(ZL);c0r=r(IRe,"This class cannot be instantiated directly using "),c1e=s(IRe,"CODE",{});var Q2t=n(c1e);m0r=r(Q2t,"__init__()"),Q2t.forEach(t),f0r=r(IRe," (throws an error)."),IRe.forEach(t),g0r=i(Ei),kt=s(Ei,"DIV",{class:!0});var yi=n(kt);f(e8.$$.fragment,yi),h0r=i(yi),m1e=s(yi,"P",{});var H2t=n(m1e);u0r=r(H2t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),H2t.forEach(t),p0r=i(yi),vm=s(yi,"P",{});var FW=n(vm);_0r=r(FW,`Note:
Loading a model from its configuration file does `),f1e=s(FW,"STRONG",{});var U2t=n(f1e);b0r=r(U2t,"not"),U2t.forEach(t),v0r=r(FW,` load the model weights. It only affects the
model\u2019s configuration. Use `),g1e=s(FW,"CODE",{});var J2t=n(g1e);T0r=r(J2t,"from_pretrained()"),J2t.forEach(t),F0r=r(FW,"to load the model weights."),FW.forEach(t),C0r=i(yi),h1e=s(yi,"P",{});var Y2t=n(h1e);M0r=r(Y2t,"Examples:"),Y2t.forEach(t),E0r=i(yi),f(o8.$$.fragment,yi),yi.forEach(t),y0r=i(Ei),Io=s(Ei,"DIV",{class:!0});var Sa=n(Io);f(r8.$$.fragment,Sa),w0r=i(Sa),u1e=s(Sa,"P",{});var K2t=n(u1e);A0r=r(K2t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),K2t.forEach(t),L0r=i(Sa),Ss=s(Sa,"P",{});var F5=n(Ss);B0r=r(F5,"The model class to instantiate is selected based on the "),p1e=s(F5,"CODE",{});var Z2t=n(p1e);x0r=r(Z2t,"model_type"),Z2t.forEach(t),k0r=r(F5,` property of the config object (either
passed as an argument or loaded from `),_1e=s(F5,"CODE",{});var evt=n(_1e);R0r=r(evt,"pretrained_model_name_or_path"),evt.forEach(t),S0r=r(F5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),b1e=s(F5,"CODE",{});var ovt=n(b1e);P0r=r(ovt,"pretrained_model_name_or_path"),ovt.forEach(t),$0r=r(F5,":"),F5.forEach(t),I0r=i(Sa),t8=s(Sa,"UL",{});var DRe=n(t8);T3=s(DRe,"LI",{});var A9e=n(T3);v1e=s(A9e,"STRONG",{});var rvt=n(v1e);D0r=r(rvt,"beit"),rvt.forEach(t),j0r=r(A9e," \u2014 "),kV=s(A9e,"A",{href:!0});var tvt=n(kV);N0r=r(tvt,"FlaxBeitForImageClassification"),tvt.forEach(t),q0r=r(A9e," (BEiT model)"),A9e.forEach(t),G0r=i(DRe),F3=s(DRe,"LI",{});var L9e=n(F3);T1e=s(L9e,"STRONG",{});var avt=n(T1e);O0r=r(avt,"vit"),avt.forEach(t),X0r=r(L9e," \u2014 "),RV=s(L9e,"A",{href:!0});var svt=n(RV);V0r=r(svt,"FlaxViTForImageClassification"),svt.forEach(t),z0r=r(L9e," (ViT model)"),L9e.forEach(t),DRe.forEach(t),W0r=i(Sa),F1e=s(Sa,"P",{});var nvt=n(F1e);Q0r=r(nvt,"Examples:"),nvt.forEach(t),H0r=i(Sa),f(a8.$$.fragment,Sa),Sa.forEach(t),Ei.forEach(t),Axe=i(c),Tm=s(c,"H2",{class:!0});var jRe=n(Tm);C3=s(jRe,"A",{id:!0,class:!0,href:!0});var lvt=n(C3);C1e=s(lvt,"SPAN",{});var ivt=n(C1e);f(s8.$$.fragment,ivt),ivt.forEach(t),lvt.forEach(t),U0r=i(jRe),M1e=s(jRe,"SPAN",{});var dvt=n(M1e);J0r=r(dvt,"FlaxAutoModelForVision2Seq"),dvt.forEach(t),jRe.forEach(t),Lxe=i(c),Nr=s(c,"DIV",{class:!0});var wi=n(Nr);f(n8.$$.fragment,wi),Y0r=i(wi),Fm=s(wi,"P",{});var CW=n(Fm);K0r=r(CW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),E1e=s(CW,"CODE",{});var cvt=n(E1e);Z0r=r(cvt,"from_pretrained()"),cvt.forEach(t),eLr=r(CW,"class method or the "),y1e=s(CW,"CODE",{});var mvt=n(y1e);oLr=r(mvt,"from_config()"),mvt.forEach(t),rLr=r(CW,`class
method.`),CW.forEach(t),tLr=i(wi),l8=s(wi,"P",{});var NRe=n(l8);aLr=r(NRe,"This class cannot be instantiated directly using "),w1e=s(NRe,"CODE",{});var fvt=n(w1e);sLr=r(fvt,"__init__()"),fvt.forEach(t),nLr=r(NRe," (throws an error)."),NRe.forEach(t),lLr=i(wi),Rt=s(wi,"DIV",{class:!0});var Ai=n(Rt);f(i8.$$.fragment,Ai),iLr=i(Ai),A1e=s(Ai,"P",{});var gvt=n(A1e);dLr=r(gvt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),gvt.forEach(t),cLr=i(Ai),Cm=s(Ai,"P",{});var MW=n(Cm);mLr=r(MW,`Note:
Loading a model from its configuration file does `),L1e=s(MW,"STRONG",{});var hvt=n(L1e);fLr=r(hvt,"not"),hvt.forEach(t),gLr=r(MW,` load the model weights. It only affects the
model\u2019s configuration. Use `),B1e=s(MW,"CODE",{});var uvt=n(B1e);hLr=r(uvt,"from_pretrained()"),uvt.forEach(t),uLr=r(MW,"to load the model weights."),MW.forEach(t),pLr=i(Ai),x1e=s(Ai,"P",{});var pvt=n(x1e);_Lr=r(pvt,"Examples:"),pvt.forEach(t),bLr=i(Ai),f(d8.$$.fragment,Ai),Ai.forEach(t),vLr=i(wi),Do=s(wi,"DIV",{class:!0});var Pa=n(Do);f(c8.$$.fragment,Pa),TLr=i(Pa),k1e=s(Pa,"P",{});var _vt=n(k1e);FLr=r(_vt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),_vt.forEach(t),CLr=i(Pa),Ps=s(Pa,"P",{});var C5=n(Ps);MLr=r(C5,"The model class to instantiate is selected based on the "),R1e=s(C5,"CODE",{});var bvt=n(R1e);ELr=r(bvt,"model_type"),bvt.forEach(t),yLr=r(C5,` property of the config object (either
passed as an argument or loaded from `),S1e=s(C5,"CODE",{});var vvt=n(S1e);wLr=r(vvt,"pretrained_model_name_or_path"),vvt.forEach(t),ALr=r(C5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),P1e=s(C5,"CODE",{});var Tvt=n(P1e);LLr=r(Tvt,"pretrained_model_name_or_path"),Tvt.forEach(t),BLr=r(C5,":"),C5.forEach(t),xLr=i(Pa),$1e=s(Pa,"UL",{});var Fvt=n($1e);M3=s(Fvt,"LI",{});var B9e=n(M3);I1e=s(B9e,"STRONG",{});var Cvt=n(I1e);kLr=r(Cvt,"vision-encoder-decoder"),Cvt.forEach(t),RLr=r(B9e," \u2014 "),SV=s(B9e,"A",{href:!0});var Mvt=n(SV);SLr=r(Mvt,"FlaxVisionEncoderDecoderModel"),Mvt.forEach(t),PLr=r(B9e," (Vision Encoder decoder model)"),B9e.forEach(t),Fvt.forEach(t),$Lr=i(Pa),D1e=s(Pa,"P",{});var Evt=n(D1e);ILr=r(Evt,"Examples:"),Evt.forEach(t),DLr=i(Pa),f(m8.$$.fragment,Pa),Pa.forEach(t),wi.forEach(t),this.h()},h(){d(J,"name","hf:doc:metadata"),d(J,"content",JSON.stringify(Svt)),d(fe,"id","auto-classes"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#auto-classes"),d(ie,"class","relative group"),d($s,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),d(Ds,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),d(js,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),d($i,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),d(Lm,"id","extending-the-auto-classes"),d(Lm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lm,"href","#extending-the-auto-classes"),d(Ii,"class","relative group"),d(xm,"id","transformers.AutoConfig"),d(xm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xm,"href","#transformers.AutoConfig"),d(Di,"class","relative group"),d(u7,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),d(p7,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),d(_7,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),d(b7,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),d(v7,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),d(T7,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig"),d(F7,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig"),d(C7,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),d(M7,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),d(E7,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),d(y7,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),d(w7,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),d(A7,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),d(L7,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),d(B7,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig"),d(x7,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),d(k7,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig"),d(R7,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig"),d(S7,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),d(P7,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config"),d($7,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),d(I7,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),d(D7,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),d(j7,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),d(N7,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),d(q7,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),d(G7,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),d(O7,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),d(X7,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),d(V7,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),d(z7,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),d(W7,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),d(Q7,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),d(H7,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),d(U7,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),d(J7,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),d(Y7,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),d(K7,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),d(Z7,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),d(e9,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),d(o9,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),d(r9,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),d(t9,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),d(a9,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),d(s9,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerConfig"),d(n9,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),d(l9,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),d(i9,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),d(d9,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),d(c9,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),d(m9,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig"),d(f9,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),d(g9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),d(h9,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),d(u9,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig"),d(p9,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig"),d(_9,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),d(b9,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig"),d(v9,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),d(T9,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmConfig"),d(F9,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),d(C9,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),d(M9,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),d(E9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),d(y9,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),d(w9,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),d(A9,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),d(L9,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig"),d(B9,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),d(x9,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),d(k9,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),d(R9,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),d(S9,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),d(P9,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig"),d($9,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),d(I9,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),d(D9,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),d(j9,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),d(N9,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),d(q9,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),d(G9,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig"),d(O9,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),d(X9,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),d(V9,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),d(z9,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),d(W9,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig"),d(Q9,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),d(H9,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig"),d(U9,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig"),d(J9,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),d(Y9,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),d(K9,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),d(Z9,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),d(eB,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),d(oB,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig"),d(mo,"class","docstring"),d(bg,"class","docstring"),d(Xo,"class","docstring"),d(vg,"id","transformers.AutoTokenizer"),d(vg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(vg,"href","#transformers.AutoTokenizer"),d(Ni,"class","relative group"),d(rB,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(tB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),d(aB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),d(sB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),d(nB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),d(lB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),d(iB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),d(dB,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),d(cB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),d(mB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),d(fB,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),d(gB,"href","/docs/transformers/master/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),d(hB,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),d(uB,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizer"),d(pB,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),d(_B,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(bB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(vB,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),d(TB,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),d(FB,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),d(CB,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),d(MB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),d(EB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),d(yB,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),d(wB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),d(AB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(LB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),d(BB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),d(xB,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),d(kB,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),d(RB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),d(SB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),d(PB,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),d($B,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),d(IB,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),d(DB,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(jB,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(NB,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),d(qB,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),d(GB,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),d(OB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),d(XB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),d(VB,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),d(zB,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),d(WB,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),d(QB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(HB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(UB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(JB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(YB,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizer"),d(KB,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizerFast"),d(ZB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(ex,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),d(ox,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(rx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),d(tx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),d(ax,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),d(sx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),d(nx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),d(lx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),d(ix,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),d(dx,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),d(cx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),d(mx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),d(fx,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),d(gx,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),d(hx,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),d(ux,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),d(px,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),d(_x,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),d(bx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),d(vx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),d(Tx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),d(Fx,"href","/docs/transformers/master/en/model_doc/mluke#transformers.MLukeTokenizer"),d(Cx,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),d(Mx,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),d(Ex,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),d(yx,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),d(wx,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),d(Ax,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),d(Lx,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),d(Bx,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),d(xx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(kx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(Rx,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverTokenizer"),d(Sx,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),d(Px,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartTokenizer"),d($x,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),d(Ix,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),d(Dx,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),d(jx,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),d(Nx,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmTokenizer"),d(qx,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmTokenizerFast"),d(Gx,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),d(Ox,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),d(Xx,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),d(Vx,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),d(zx,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),d(Wx,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),d(Qx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),d(Hx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(Ux,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),d(Jx,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),d(Yx,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),d(Kx,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),d(Zx,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),d(ek,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),d(ok,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),d(rk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),d(tk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),d(ak,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),d(sk,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),d(nk,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),d(lk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(ik,"href","/docs/transformers/master/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),d(dk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizer"),d(ck,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizerFast"),d(mk,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),d(fk,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),d(gk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),d(hk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),d(uk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),d(pk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),d(fo,"class","docstring"),d(Ug,"class","docstring"),d(Vo,"class","docstring"),d(Jg,"id","transformers.AutoFeatureExtractor"),d(Jg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Jg,"href","#transformers.AutoFeatureExtractor"),d(qi,"class","relative group"),d(_k,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(bk,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),d(vk,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),d(Tk,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(Fk,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),d(Ck,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),d(Mk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(Ek,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),d(yk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),d(wk,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),d(Ak,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),d(Lk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),d(Bk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(xk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(kk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Rk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(xe,"class","docstring"),d(gh,"class","docstring"),d(zo,"class","docstring"),d(hh,"id","transformers.AutoProcessor"),d(hh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(hh,"href","#transformers.AutoProcessor"),d(Gi,"class","relative group"),d(Sk,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(Pk,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),d($k,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),d(Ik,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),d(Dk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),d(jk,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),d(Nk,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),d(qk,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),d(Gk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),d(ke,"class","docstring"),d(Eh,"class","docstring"),d(Wo,"class","docstring"),d(yh,"id","transformers.AutoModel"),d(yh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yh,"href","#transformers.AutoModel"),d(Xi,"class","relative group"),d(qr,"class","docstring"),d(Ok,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),d(Xk,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),d(Vk,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),d(zk,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),d(Wk,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),d(Qk,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel"),d(Hk,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),d(Uk,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),d(Jk,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),d(Yk,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),d(Kk,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),d(Zk,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),d(eR,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),d(oR,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel"),d(rR,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),d(tR,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel"),d(aR,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel"),d(sR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),d(nR,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model"),d(lR,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),d(iR,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),d(dR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),d(cR,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(mR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),d(fR,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),d(gR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),d(hR,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),d(uR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),d(pR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),d(_R,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),d(bR,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),d(vR,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),d(TR,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),d(FR,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),d(CR,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),d(MR,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),d(ER,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),d(yR,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),d(wR,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),d(AR,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),d(LR,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),d(BR,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),d(xR,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),d(kR,"href","/docs/transformers/master/en/model_doc/maskformer#transformers.MaskFormerModel"),d(RR,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),d(SR,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel"),d(PR,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),d($R,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),d(IR,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),d(DR,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel"),d(jR,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),d(NR,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),d(qR,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),d(GR,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel"),d(OR,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel"),d(XR,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),d(VR,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel"),d(zR,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),d(WR,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),d(QR,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),d(HR,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),d(UR,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),d(JR,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),d(YR,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),d(KR,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel"),d(ZR,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),d(eS,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),d(oS,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),d(rS,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinModel"),d(tS,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),d(aS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),d(sS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel"),d(nS,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),d(lS,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),d(iS,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel"),d(dS,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),d(cS,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),d(mS,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),d(fS,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel"),d(gS,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),d(hS,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel"),d(uS,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel"),d(pS,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),d(_S,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),d(bS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),d(vS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),d(TS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),d(FS,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel"),d(Re,"class","docstring"),d(Qo,"class","docstring"),d(sp,"id","transformers.AutoModelForPreTraining"),d(sp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(sp,"href","#transformers.AutoModelForPreTraining"),d(Wi,"class","relative group"),d(Gr,"class","docstring"),d(CS,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),d(MS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(ES,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),d(yS,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),d(wS,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(AS,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(LS,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(BS,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(xS,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(kS,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(RS,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),d(SS,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(PS,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),d($S,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(IS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),d(DS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(jS,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(NS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(qS,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(GS,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),d(OS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),d(XS,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),d(VS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(zS,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(WS,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),d(QS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(HS,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(US,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(JS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(YS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(KS,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),d(ZS,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),d(eP,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),d(oP,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),d(rP,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),d(tP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(aP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(sP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(nP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Se,"class","docstring"),d(Ho,"class","docstring"),d(Wp,"id","transformers.AutoModelForCausalLM"),d(Wp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Wp,"href","#transformers.AutoModelForCausalLM"),d(Ui,"class","relative group"),d(Or,"class","docstring"),d(lP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),d(iP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),d(dP,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),d(cP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),d(mP,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),d(fP,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),d(gP,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),d(hP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),d(uP,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(pP,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM"),d(_P,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM"),d(bP,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(vP,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),d(TP,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),d(FP,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),d(CP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),d(MP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),d(EP,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(yP,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),d(wP,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM"),d(AP,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),d(LP,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),d(BP,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),d(xP,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),d(kP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),d(RP,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),d(SP,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),d(PP,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d($P,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),d(IP,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM"),d(DP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(jP,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),d(NP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),d(qP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),d(GP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Pe,"class","docstring"),d(Uo,"class","docstring"),d(B_,"id","transformers.AutoModelForMaskedLM"),d(B_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(B_,"href","#transformers.AutoModelForMaskedLM"),d(Ki,"class","relative group"),d(Xr,"class","docstring"),d(OP,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),d(XP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(VP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),d(zP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),d(WP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(QP,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),d(HP,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(UP,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(JP,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(YP,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(KP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),d(ZP,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(e$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),d(o$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),d(r$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(t$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(a$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(s$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(n$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),d(l$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),d(i$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(d$,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),d(c$,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(m$,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),d(f$,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),d(g$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),d(h$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(u$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),d(p$,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(_$,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(b$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(v$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(T$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(F$,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM"),d($e,"class","docstring"),d(Jo,"class","docstring"),d(fb,"id","transformers.AutoModelForSeq2SeqLM"),d(fb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fb,"href","#transformers.AutoModelForSeq2SeqLM"),d(od,"class","relative group"),d(Vr,"class","docstring"),d(C$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(M$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),d(E$,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),d(y$,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),d(w$,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),d(A$,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(L$,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),d(B$,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),d(x$,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),d(k$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(R$,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),d(S$,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),d(P$,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),d($$,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),d(I$,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(D$,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),d(Ie,"class","docstring"),d(Yo,"class","docstring"),d(xb,"id","transformers.AutoModelForSequenceClassification"),d(xb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xb,"href","#transformers.AutoModelForSequenceClassification"),d(ad,"class","relative group"),d(zr,"class","docstring"),d(j$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),d(N$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),d(q$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),d(G$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),d(O$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),d(X$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),d(V$,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),d(z$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),d(W$,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),d(Q$,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification"),d(H$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),d(U$,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),d(J$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),d(Y$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),d(K$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),d(Z$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),d(eI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),d(oI,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),d(rI,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),d(tI,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),d(aI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),d(sI,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),d(nI,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),d(lI,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),d(iI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),d(dI,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),d(cI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),d(mI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),d(fI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),d(gI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),d(hI,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),d(uI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(pI,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),d(_I,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),d(bI,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),d(vI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),d(TI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),d(FI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),d(CI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),d(MI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),d(EI,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),d(yI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),d(wI,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),d(AI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),d(LI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),d(BI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification"),d(De,"class","docstring"),d(Ko,"class","docstring"),d(E2,"id","transformers.AutoModelForMultipleChoice"),d(E2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(E2,"href","#transformers.AutoModelForMultipleChoice"),d(ld,"class","relative group"),d(Wr,"class","docstring"),d(xI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),d(kI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),d(RI,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),d(SI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),d(PI,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),d($I,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),d(II,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice"),d(DI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),d(jI,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),d(NI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),d(qI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),d(GI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),d(OI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),d(XI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),d(VI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),d(zI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),d(WI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),d(QI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),d(HI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),d(UI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),d(JI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),d(YI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),d(KI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),d(ZI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),d(eD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),d(oD,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),d(rD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),d(tD,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice"),d(je,"class","docstring"),d(Zo,"class","docstring"),d(ev,"id","transformers.AutoModelForNextSentencePrediction"),d(ev,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ev,"href","#transformers.AutoModelForNextSentencePrediction"),d(cd,"class","relative group"),d(Qr,"class","docstring"),d(aD,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),d(sD,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),d(nD,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),d(lD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),d(iD,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),d(Ne,"class","docstring"),d(er,"class","docstring"),d(lv,"id","transformers.AutoModelForTokenClassification"),d(lv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(lv,"href","#transformers.AutoModelForTokenClassification"),d(gd,"class","relative group"),d(Hr,"class","docstring"),d(dD,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),d(cD,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),d(mD,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),d(fD,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),d(gD,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),d(hD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),d(uD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification"),d(pD,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),d(_D,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),d(bD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),d(vD,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),d(TD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),d(FD,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),d(CD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),d(MD,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),d(ED,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),d(yD,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),d(wD,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),d(AD,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),d(LD,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),d(BD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),d(xD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),d(kD,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),d(RD,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),d(SD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),d(PD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),d($D,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),d(ID,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),d(DD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),d(jD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),d(ND,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),d(qD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),d(GD,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification"),d(qe,"class","docstring"),d(or,"class","docstring"),d(Ov,"id","transformers.AutoModelForQuestionAnswering"),d(Ov,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ov,"href","#transformers.AutoModelForQuestionAnswering"),d(pd,"class","relative group"),d(Ur,"class","docstring"),d(OD,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),d(XD,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),d(VD,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),d(zD,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),d(WD,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),d(QD,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),d(HD,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),d(UD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),d(JD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering"),d(YD,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),d(KD,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),d(ZD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),d(ej,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),d(oj,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),d(rj,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),d(tj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),d(aj,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),d(sj,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),d(nj,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),d(lj,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),d(ij,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),d(dj,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),d(cj,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),d(mj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),d(fj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),d(gj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),d(hj,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),d(uj,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),d(pj,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),d(_j,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),d(bj,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),d(vj,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),d(Tj,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),d(Fj,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),d(Cj,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),d(Mj,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),d(Ej,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),d(yj,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),d(wj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),d(Ge,"class","docstring"),d(rr,"class","docstring"),d(BT,"id","transformers.AutoModelForTableQuestionAnswering"),d(BT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(BT,"href","#transformers.AutoModelForTableQuestionAnswering"),d(vd,"class","relative group"),d(Jr,"class","docstring"),d(Aj,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),d(Oe,"class","docstring"),d(tr,"class","docstring"),d(RT,"id","transformers.AutoModelForImageClassification"),d(RT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(RT,"href","#transformers.AutoModelForImageClassification"),d(Cd,"class","relative group"),d(Yr,"class","docstring"),d(Lj,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),d(Bj,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification"),d(xj,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),d(kj,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),d(Rj,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),d(Sj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(Pj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d($j,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(Ij,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),d(Dj,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),d(jj,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification"),d(Nj,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),d(Xe,"class","docstring"),d(ar,"class","docstring"),d(GT,"id","transformers.AutoModelForVision2Seq"),d(GT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(GT,"href","#transformers.AutoModelForVision2Seq"),d(yd,"class","relative group"),d(Kr,"class","docstring"),d(qj,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),d(Ve,"class","docstring"),d(sr,"class","docstring"),d(VT,"id","transformers.AutoModelForAudioClassification"),d(VT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(VT,"href","#transformers.AutoModelForAudioClassification"),d(Ld,"class","relative group"),d(Zr,"class","docstring"),d(Gj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification"),d(Oj,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),d(Xj,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),d(Vj,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),d(zj,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),d(Wj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),d(Qj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),d(Hj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),d(ze,"class","docstring"),d(nr,"class","docstring"),d(e1,"id","transformers.AutoModelForAudioFrameClassification"),d(e1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(e1,"href","#transformers.AutoModelForAudioFrameClassification"),d(kd,"class","relative group"),d(et,"class","docstring"),d(Uj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification"),d(Jj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),d(Yj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),d(Kj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),d(We,"class","docstring"),d(lr,"class","docstring"),d(n1,"id","transformers.AutoModelForCTC"),d(n1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(n1,"href","#transformers.AutoModelForCTC"),d(Pd,"class","relative group"),d(ot,"class","docstring"),d(Zj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC"),d(eN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),d(oN,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),d(rN,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC"),d(tN,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),d(aN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),d(sN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),d(nN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC"),d(Qe,"class","docstring"),d(ir,"class","docstring"),d(p1,"id","transformers.AutoModelForSpeechSeq2Seq"),d(p1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p1,"href","#transformers.AutoModelForSpeechSeq2Seq"),d(Dd,"class","relative group"),d(rt,"class","docstring"),d(lN,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),d(iN,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),d(He,"class","docstring"),d(dr,"class","docstring"),d(T1,"id","transformers.AutoModelForAudioXVector"),d(T1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(T1,"href","#transformers.AutoModelForAudioXVector"),d(qd,"class","relative group"),d(tt,"class","docstring"),d(dN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector"),d(cN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),d(mN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),d(fN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector"),d(Ue,"class","docstring"),d(cr,"class","docstring"),d(w1,"id","transformers.AutoModelForMaskedImageModeling"),d(w1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w1,"href","#transformers.AutoModelForMaskedImageModeling"),d(Xd,"class","relative group"),d(at,"class","docstring"),d(gN,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),d(hN,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),d(uN,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),d(Je,"class","docstring"),d(mr,"class","docstring"),d(k1,"id","transformers.AutoModelForObjectDetection"),d(k1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(k1,"href","#transformers.AutoModelForObjectDetection"),d(Qd,"class","relative group"),d(st,"class","docstring"),d(pN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),d(Ye,"class","docstring"),d(fr,"class","docstring"),d(P1,"id","transformers.AutoModelForImageSegmentation"),d(P1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P1,"href","#transformers.AutoModelForImageSegmentation"),d(Jd,"class","relative group"),d(nt,"class","docstring"),d(_N,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),d(Ke,"class","docstring"),d(gr,"class","docstring"),d(D1,"id","transformers.AutoModelForSemanticSegmentation"),d(D1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(D1,"href","#transformers.AutoModelForSemanticSegmentation"),d(Zd,"class","relative group"),d(lt,"class","docstring"),d(bN,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),d(vN,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),d(Ze,"class","docstring"),d(hr,"class","docstring"),d(G1,"id","transformers.TFAutoModel"),d(G1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(G1,"href","#transformers.TFAutoModel"),d(rc,"class","relative group"),d(it,"class","docstring"),d(TN,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),d(FN,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),d(CN,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),d(MN,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),d(EN,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),d(yN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),d(wN,"href","/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel"),d(AN,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),d(LN,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel"),d(BN,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),d(xN,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),d(kN,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),d(RN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),d(SN,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(PN,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),d($N,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),d(IN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),d(DN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),d(jN,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),d(NN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),d(qN,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),d(GN,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),d(ON,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),d(XN,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),d(VN,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),d(zN,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),d(WN,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),d(QN,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),d(HN,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),d(UN,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),d(JN,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),d(YN,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),d(KN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),d(ZN,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),d(eq,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),d(oq,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),d(rq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),d(tq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),d(aq,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),d(sq,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),d(nq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),d(lq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),d(iq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),d(go,"class","docstring"),d(ur,"class","docstring"),d(BF,"id","transformers.TFAutoModelForPreTraining"),d(BF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(BF,"href","#transformers.TFAutoModelForPreTraining"),d(sc,"class","relative group"),d(dt,"class","docstring"),d(dq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),d(cq,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(mq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),d(fq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(gq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(hq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(uq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),d(pq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(_q,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),d(bq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(vq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(Tq,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),d(Fq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),d(Cq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(Mq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(Eq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(yq,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(wq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(Aq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(Lq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Bq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(xq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(ho,"class","docstring"),d(pr,"class","docstring"),d(KF,"id","transformers.TFAutoModelForCausalLM"),d(KF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(KF,"href","#transformers.TFAutoModelForCausalLM"),d(ic,"class","relative group"),d(ct,"class","docstring"),d(kq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),d(Rq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(Sq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(Pq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d($q,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),d(Iq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),d(Dq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),d(jq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(Nq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(qq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(uo,"class","docstring"),d(_r,"class","docstring"),d(dC,"id","transformers.TFAutoModelForImageClassification"),d(dC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(dC,"href","#transformers.TFAutoModelForImageClassification"),d(mc,"class","relative group"),d(mt,"class","docstring"),d(Gq,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),d(Oq,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),d(po,"class","docstring"),d(br,"class","docstring"),d(fC,"id","transformers.TFAutoModelForMaskedLM"),d(fC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fC,"href","#transformers.TFAutoModelForMaskedLM"),d(hc,"class","relative group"),d(ft,"class","docstring"),d(Xq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),d(Vq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),d(zq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(Wq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),d(Qq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),d(Hq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),d(Uq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(Jq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),d(Yq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(Kq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),d(Zq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(eG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),d(oG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),d(rG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(tG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),d(aG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(sG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),d(nG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(lG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(iG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(_o,"class","docstring"),d(vr,"class","docstring"),d(SC,"id","transformers.TFAutoModelForSeq2SeqLM"),d(SC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(SC,"href","#transformers.TFAutoModelForSeq2SeqLM"),d(_c,"class","relative group"),d(gt,"class","docstring"),d(dG,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(cG,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),d(mG,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),d(fG,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),d(gG,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),d(hG,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),d(uG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),d(pG,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),d(_G,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),d(bG,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(bo,"class","docstring"),d(Tr,"class","docstring"),d(VC,"id","transformers.TFAutoModelForSequenceClassification"),d(VC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(VC,"href","#transformers.TFAutoModelForSequenceClassification"),d(Tc,"class","relative group"),d(ht,"class","docstring"),d(vG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),d(TG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),d(FG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),d(CG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),d(MG,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),d(EG,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),d(yG,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),d(wG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),d(AG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),d(LG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),d(BG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),d(xG,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),d(kG,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),d(RG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),d(SG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),d(PG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),d($G,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),d(IG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),d(DG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),d(jG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),d(NG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),d(qG,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),d(GG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),d(OG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),d(XG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),d(vo,"class","docstring"),d(Fr,"class","docstring"),d(p4,"id","transformers.TFAutoModelForMultipleChoice"),d(p4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p4,"href","#transformers.TFAutoModelForMultipleChoice"),d(Mc,"class","relative group"),d(ut,"class","docstring"),d(VG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),d(zG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),d(WG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),d(QG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),d(HG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),d(UG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),d(JG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),d(YG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),d(KG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),d(ZG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),d(eO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),d(oO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),d(rO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),d(tO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),d(aO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),d(sO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),d(nO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),d(To,"class","docstring"),d(Cr,"class","docstring"),d(P4,"id","transformers.TFAutoModelForTableQuestionAnswering"),d(P4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P4,"href","#transformers.TFAutoModelForTableQuestionAnswering"),d(wc,"class","relative group"),d(pt,"class","docstring"),d(lO,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),d(Fo,"class","docstring"),d(Mr,"class","docstring"),d(I4,"id","transformers.TFAutoModelForTokenClassification"),d(I4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(I4,"href","#transformers.TFAutoModelForTokenClassification"),d(Bc,"class","relative group"),d(_t,"class","docstring"),d(iO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),d(dO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),d(cO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),d(mO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),d(fO,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),d(gO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),d(hO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),d(uO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),d(pO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),d(_O,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),d(bO,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),d(vO,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),d(TO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),d(FO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),d(CO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),d(MO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),d(EO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),d(yO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),d(wO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),d(AO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),d(Co,"class","docstring"),d(Er,"class","docstring"),d(tM,"id","transformers.TFAutoModelForQuestionAnswering"),d(tM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tM,"href","#transformers.TFAutoModelForQuestionAnswering"),d(Rc,"class","relative group"),d(bt,"class","docstring"),d(LO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),d(BO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),d(xO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),d(kO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),d(RO,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),d(SO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),d(PO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),d($O,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),d(IO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),d(DO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),d(jO,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),d(NO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),d(qO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),d(GO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),d(OO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),d(XO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),d(VO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),d(zO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),d(WO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),d(Mo,"class","docstring"),d(yr,"class","docstring"),d(MM,"id","transformers.TFAutoModelForVision2Seq"),d(MM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(MM,"href","#transformers.TFAutoModelForVision2Seq"),d($c,"class","relative group"),d(vt,"class","docstring"),d(QO,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),d(Eo,"class","docstring"),d(wr,"class","docstring"),d(yM,"id","transformers.TFAutoModelForSpeechSeq2Seq"),d(yM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yM,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),d(jc,"class","relative group"),d(Tt,"class","docstring"),d(HO,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),d(yo,"class","docstring"),d(Ar,"class","docstring"),d(AM,"id","transformers.FlaxAutoModel"),d(AM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(AM,"href","#transformers.FlaxAutoModel"),d(Gc,"class","relative group"),d(Ft,"class","docstring"),d(UO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),d(JO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),d(YO,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),d(KO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),d(ZO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),d(eX,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),d(oX,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),d(rX,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),d(tX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),d(aX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),d(sX,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),d(nX,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),d(lX,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),d(iX,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),d(dX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),d(cX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),d(mX,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),d(fX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),d(gX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel"),d(hX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),d(uX,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),d(pX,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),d(_X,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),d(bX,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel"),d(wo,"class","docstring"),d(Lr,"class","docstring"),d(KM,"id","transformers.FlaxAutoModelForCausalLM"),d(KM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(KM,"href","#transformers.FlaxAutoModelForCausalLM"),d(Vc,"class","relative group"),d(Ct,"class","docstring"),d(vX,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),d(TX,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),d(FX,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),d(CX,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),d(Ao,"class","docstring"),d(Br,"class","docstring"),d(tE,"id","transformers.FlaxAutoModelForPreTraining"),d(tE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(tE,"href","#transformers.FlaxAutoModelForPreTraining"),d(Qc,"class","relative group"),d(Mt,"class","docstring"),d(MX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),d(EX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(yX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),d(wX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),d(AX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),d(LX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(BX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(xX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(kX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(RX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(SX,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),d(Lo,"class","docstring"),d(xr,"class","docstring"),d(uE,"id","transformers.FlaxAutoModelForMaskedLM"),d(uE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(uE,"href","#transformers.FlaxAutoModelForMaskedLM"),d(Jc,"class","relative group"),d(Et,"class","docstring"),d(PX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),d($X,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(IX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),d(DX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),d(jX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),d(NX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),d(qX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(GX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(OX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(Bo,"class","docstring"),d(kr,"class","docstring"),d(yE,"id","transformers.FlaxAutoModelForSeq2SeqLM"),d(yE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(yE,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),d(Zc,"class","relative group"),d(yt,"class","docstring"),d(XX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(VX,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),d(zX,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),d(WX,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),d(QX,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),d(HX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(UX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(JX,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),d(YX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(xo,"class","docstring"),d(Rr,"class","docstring"),d($E,"id","transformers.FlaxAutoModelForSequenceClassification"),d($E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($E,"href","#transformers.FlaxAutoModelForSequenceClassification"),d(rm,"class","relative group"),d(wt,"class","docstring"),d(KX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),d(ZX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),d(eV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),d(oV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),d(rV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),d(tV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),d(aV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),d(sV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),d(nV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),d(ko,"class","docstring"),d(Sr,"class","docstring"),d(zE,"id","transformers.FlaxAutoModelForQuestionAnswering"),d(zE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(zE,"href","#transformers.FlaxAutoModelForQuestionAnswering"),d(sm,"class","relative group"),d(At,"class","docstring"),d(lV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),d(iV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),d(dV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),d(cV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),d(mV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),d(fV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),d(gV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),d(hV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),d(uV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),d(Ro,"class","docstring"),d(Pr,"class","docstring"),d(o3,"id","transformers.FlaxAutoModelForTokenClassification"),d(o3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(o3,"href","#transformers.FlaxAutoModelForTokenClassification"),d(im,"class","relative group"),d(Lt,"class","docstring"),d(pV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),d(_V,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),d(bV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),d(vV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),d(TV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),d(FV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),d(CV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),d(So,"class","docstring"),d($r,"class","docstring"),d(d3,"id","transformers.FlaxAutoModelForMultipleChoice"),d(d3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(d3,"href","#transformers.FlaxAutoModelForMultipleChoice"),d(mm,"class","relative group"),d(Bt,"class","docstring"),d(MV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),d(EV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),d(yV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),d(wV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),d(AV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),d(LV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),d(BV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),d(Po,"class","docstring"),d(Ir,"class","docstring"),d(_3,"id","transformers.FlaxAutoModelForNextSentencePrediction"),d(_3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_3,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),d(hm,"class","relative group"),d(xt,"class","docstring"),d(xV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),d($o,"class","docstring"),d(Dr,"class","docstring"),d(v3,"id","transformers.FlaxAutoModelForImageClassification"),d(v3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(v3,"href","#transformers.FlaxAutoModelForImageClassification"),d(_m,"class","relative group"),d(kt,"class","docstring"),d(kV,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),d(RV,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),d(Io,"class","docstring"),d(jr,"class","docstring"),d(C3,"id","transformers.FlaxAutoModelForVision2Seq"),d(C3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(C3,"href","#transformers.FlaxAutoModelForVision2Seq"),d(Tm,"class","relative group"),d(Rt,"class","docstring"),d(SV,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),d(Do,"class","docstring"),d(Nr,"class","docstring")},m(c,_){e(document.head,J),b(c,Be,_),b(c,ie,_),e(ie,fe),e(fe,so),g(ce,so,null),e(ie,_e),e(ie,Go),e(Go,Bi),b(c,Em,_),b(c,na,_),e(na,xi),e(na,ki),e(ki,M5),e(na,ym),b(c,ye,_),b(c,io,_),e(io,Ri),e(io,$s),e($s,E5),e(io,Is),e(io,Ds),e(Ds,y5),e(io,Si),e(io,js),e(js,w5),e(io,Pi),b(c,wm,_),g($a,c,_),b(c,co,_),b(c,ge,_),e(ge,d7),e(ge,$i),e($i,c7),e(ge,m7),b(c,Oo,_),b(c,Ia,_),e(Ia,f7),e(Ia,Am),e(Am,g7),e(Ia,qRe),b(c,x9e,_),b(c,Ii,_),e(Ii,Lm),e(Lm,EW),g(A5,EW,null),e(Ii,GRe),e(Ii,yW),e(yW,ORe),b(c,k9e,_),b(c,Ns,_),e(Ns,XRe),e(Ns,wW),e(wW,VRe),e(Ns,zRe),e(Ns,AW),e(AW,WRe),e(Ns,QRe),b(c,R9e,_),g(L5,c,_),b(c,S9e,_),b(c,h7,_),e(h7,HRe),b(c,P9e,_),g(Bm,c,_),b(c,$9e,_),b(c,Di,_),e(Di,xm),e(xm,LW),g(B5,LW,null),e(Di,URe),e(Di,BW),e(BW,JRe),b(c,I9e,_),b(c,Xo,_),g(x5,Xo,null),e(Xo,YRe),e(Xo,k5),e(k5,KRe),e(k5,u7),e(u7,ZRe),e(k5,eSe),e(Xo,oSe),e(Xo,R5),e(R5,rSe),e(R5,xW),e(xW,tSe),e(R5,aSe),e(Xo,sSe),e(Xo,mo),g(S5,mo,null),e(mo,nSe),e(mo,kW),e(kW,lSe),e(mo,iSe),e(mo,ji),e(ji,dSe),e(ji,RW),e(RW,cSe),e(ji,mSe),e(ji,SW),e(SW,fSe),e(ji,gSe),e(mo,hSe),e(mo,v),e(v,km),e(km,PW),e(PW,uSe),e(km,pSe),e(km,p7),e(p7,_Se),e(km,bSe),e(v,vSe),e(v,Rm),e(Rm,$W),e($W,TSe),e(Rm,FSe),e(Rm,_7),e(_7,CSe),e(Rm,MSe),e(v,ESe),e(v,Sm),e(Sm,IW),e(IW,ySe),e(Sm,wSe),e(Sm,b7),e(b7,ASe),e(Sm,LSe),e(v,BSe),e(v,Pm),e(Pm,DW),e(DW,xSe),e(Pm,kSe),e(Pm,v7),e(v7,RSe),e(Pm,SSe),e(v,PSe),e(v,$m),e($m,jW),e(jW,$Se),e($m,ISe),e($m,T7),e(T7,DSe),e($m,jSe),e(v,NSe),e(v,Im),e(Im,NW),e(NW,qSe),e(Im,GSe),e(Im,F7),e(F7,OSe),e(Im,XSe),e(v,VSe),e(v,Dm),e(Dm,qW),e(qW,zSe),e(Dm,WSe),e(Dm,C7),e(C7,QSe),e(Dm,HSe),e(v,USe),e(v,jm),e(jm,GW),e(GW,JSe),e(jm,YSe),e(jm,M7),e(M7,KSe),e(jm,ZSe),e(v,ePe),e(v,Nm),e(Nm,OW),e(OW,oPe),e(Nm,rPe),e(Nm,E7),e(E7,tPe),e(Nm,aPe),e(v,sPe),e(v,qm),e(qm,XW),e(XW,nPe),e(qm,lPe),e(qm,y7),e(y7,iPe),e(qm,dPe),e(v,cPe),e(v,Gm),e(Gm,VW),e(VW,mPe),e(Gm,fPe),e(Gm,w7),e(w7,gPe),e(Gm,hPe),e(v,uPe),e(v,Om),e(Om,zW),e(zW,pPe),e(Om,_Pe),e(Om,A7),e(A7,bPe),e(Om,vPe),e(v,TPe),e(v,Xm),e(Xm,WW),e(WW,FPe),e(Xm,CPe),e(Xm,L7),e(L7,MPe),e(Xm,EPe),e(v,yPe),e(v,Vm),e(Vm,QW),e(QW,wPe),e(Vm,APe),e(Vm,B7),e(B7,LPe),e(Vm,BPe),e(v,xPe),e(v,zm),e(zm,HW),e(HW,kPe),e(zm,RPe),e(zm,x7),e(x7,SPe),e(zm,PPe),e(v,$Pe),e(v,Wm),e(Wm,UW),e(UW,IPe),e(Wm,DPe),e(Wm,k7),e(k7,jPe),e(Wm,NPe),e(v,qPe),e(v,Qm),e(Qm,JW),e(JW,GPe),e(Qm,OPe),e(Qm,R7),e(R7,XPe),e(Qm,VPe),e(v,zPe),e(v,Hm),e(Hm,YW),e(YW,WPe),e(Hm,QPe),e(Hm,S7),e(S7,HPe),e(Hm,UPe),e(v,JPe),e(v,Um),e(Um,KW),e(KW,YPe),e(Um,KPe),e(Um,P7),e(P7,ZPe),e(Um,e$e),e(v,o$e),e(v,Jm),e(Jm,ZW),e(ZW,r$e),e(Jm,t$e),e(Jm,$7),e($7,a$e),e(Jm,s$e),e(v,n$e),e(v,Ym),e(Ym,eQ),e(eQ,l$e),e(Ym,i$e),e(Ym,I7),e(I7,d$e),e(Ym,c$e),e(v,m$e),e(v,Km),e(Km,oQ),e(oQ,f$e),e(Km,g$e),e(Km,D7),e(D7,h$e),e(Km,u$e),e(v,p$e),e(v,Zm),e(Zm,rQ),e(rQ,_$e),e(Zm,b$e),e(Zm,j7),e(j7,v$e),e(Zm,T$e),e(v,F$e),e(v,ef),e(ef,tQ),e(tQ,C$e),e(ef,M$e),e(ef,N7),e(N7,E$e),e(ef,y$e),e(v,w$e),e(v,of),e(of,aQ),e(aQ,A$e),e(of,L$e),e(of,q7),e(q7,B$e),e(of,x$e),e(v,k$e),e(v,rf),e(rf,sQ),e(sQ,R$e),e(rf,S$e),e(rf,G7),e(G7,P$e),e(rf,$$e),e(v,I$e),e(v,tf),e(tf,nQ),e(nQ,D$e),e(tf,j$e),e(tf,O7),e(O7,N$e),e(tf,q$e),e(v,G$e),e(v,af),e(af,lQ),e(lQ,O$e),e(af,X$e),e(af,X7),e(X7,V$e),e(af,z$e),e(v,W$e),e(v,sf),e(sf,iQ),e(iQ,Q$e),e(sf,H$e),e(sf,V7),e(V7,U$e),e(sf,J$e),e(v,Y$e),e(v,nf),e(nf,dQ),e(dQ,K$e),e(nf,Z$e),e(nf,z7),e(z7,eIe),e(nf,oIe),e(v,rIe),e(v,lf),e(lf,cQ),e(cQ,tIe),e(lf,aIe),e(lf,W7),e(W7,sIe),e(lf,nIe),e(v,lIe),e(v,df),e(df,mQ),e(mQ,iIe),e(df,dIe),e(df,Q7),e(Q7,cIe),e(df,mIe),e(v,fIe),e(v,cf),e(cf,fQ),e(fQ,gIe),e(cf,hIe),e(cf,H7),e(H7,uIe),e(cf,pIe),e(v,_Ie),e(v,mf),e(mf,gQ),e(gQ,bIe),e(mf,vIe),e(mf,U7),e(U7,TIe),e(mf,FIe),e(v,CIe),e(v,ff),e(ff,hQ),e(hQ,MIe),e(ff,EIe),e(ff,J7),e(J7,yIe),e(ff,wIe),e(v,AIe),e(v,gf),e(gf,uQ),e(uQ,LIe),e(gf,BIe),e(gf,Y7),e(Y7,xIe),e(gf,kIe),e(v,RIe),e(v,hf),e(hf,pQ),e(pQ,SIe),e(hf,PIe),e(hf,K7),e(K7,$Ie),e(hf,IIe),e(v,DIe),e(v,uf),e(uf,_Q),e(_Q,jIe),e(uf,NIe),e(uf,Z7),e(Z7,qIe),e(uf,GIe),e(v,OIe),e(v,pf),e(pf,bQ),e(bQ,XIe),e(pf,VIe),e(pf,e9),e(e9,zIe),e(pf,WIe),e(v,QIe),e(v,_f),e(_f,vQ),e(vQ,HIe),e(_f,UIe),e(_f,o9),e(o9,JIe),e(_f,YIe),e(v,KIe),e(v,bf),e(bf,TQ),e(TQ,ZIe),e(bf,eDe),e(bf,r9),e(r9,oDe),e(bf,rDe),e(v,tDe),e(v,vf),e(vf,FQ),e(FQ,aDe),e(vf,sDe),e(vf,t9),e(t9,nDe),e(vf,lDe),e(v,iDe),e(v,Tf),e(Tf,CQ),e(CQ,dDe),e(Tf,cDe),e(Tf,a9),e(a9,mDe),e(Tf,fDe),e(v,gDe),e(v,Ff),e(Ff,MQ),e(MQ,hDe),e(Ff,uDe),e(Ff,s9),e(s9,pDe),e(Ff,_De),e(v,bDe),e(v,Cf),e(Cf,EQ),e(EQ,vDe),e(Cf,TDe),e(Cf,n9),e(n9,FDe),e(Cf,CDe),e(v,MDe),e(v,Mf),e(Mf,yQ),e(yQ,EDe),e(Mf,yDe),e(Mf,l9),e(l9,wDe),e(Mf,ADe),e(v,LDe),e(v,Ef),e(Ef,wQ),e(wQ,BDe),e(Ef,xDe),e(Ef,i9),e(i9,kDe),e(Ef,RDe),e(v,SDe),e(v,yf),e(yf,AQ),e(AQ,PDe),e(yf,$De),e(yf,d9),e(d9,IDe),e(yf,DDe),e(v,jDe),e(v,wf),e(wf,LQ),e(LQ,NDe),e(wf,qDe),e(wf,c9),e(c9,GDe),e(wf,ODe),e(v,XDe),e(v,Af),e(Af,BQ),e(BQ,VDe),e(Af,zDe),e(Af,m9),e(m9,WDe),e(Af,QDe),e(v,HDe),e(v,Lf),e(Lf,xQ),e(xQ,UDe),e(Lf,JDe),e(Lf,f9),e(f9,YDe),e(Lf,KDe),e(v,ZDe),e(v,Bf),e(Bf,kQ),e(kQ,eje),e(Bf,oje),e(Bf,g9),e(g9,rje),e(Bf,tje),e(v,aje),e(v,xf),e(xf,RQ),e(RQ,sje),e(xf,nje),e(xf,h9),e(h9,lje),e(xf,ije),e(v,dje),e(v,kf),e(kf,SQ),e(SQ,cje),e(kf,mje),e(kf,u9),e(u9,fje),e(kf,gje),e(v,hje),e(v,Rf),e(Rf,PQ),e(PQ,uje),e(Rf,pje),e(Rf,p9),e(p9,_je),e(Rf,bje),e(v,vje),e(v,Sf),e(Sf,$Q),e($Q,Tje),e(Sf,Fje),e(Sf,_9),e(_9,Cje),e(Sf,Mje),e(v,Eje),e(v,Pf),e(Pf,IQ),e(IQ,yje),e(Pf,wje),e(Pf,b9),e(b9,Aje),e(Pf,Lje),e(v,Bje),e(v,$f),e($f,DQ),e(DQ,xje),e($f,kje),e($f,v9),e(v9,Rje),e($f,Sje),e(v,Pje),e(v,If),e(If,jQ),e(jQ,$je),e(If,Ije),e(If,T9),e(T9,Dje),e(If,jje),e(v,Nje),e(v,Df),e(Df,NQ),e(NQ,qje),e(Df,Gje),e(Df,F9),e(F9,Oje),e(Df,Xje),e(v,Vje),e(v,jf),e(jf,qQ),e(qQ,zje),e(jf,Wje),e(jf,C9),e(C9,Qje),e(jf,Hje),e(v,Uje),e(v,Nf),e(Nf,GQ),e(GQ,Jje),e(Nf,Yje),e(Nf,M9),e(M9,Kje),e(Nf,Zje),e(v,eNe),e(v,qf),e(qf,OQ),e(OQ,oNe),e(qf,rNe),e(qf,E9),e(E9,tNe),e(qf,aNe),e(v,sNe),e(v,Gf),e(Gf,XQ),e(XQ,nNe),e(Gf,lNe),e(Gf,y9),e(y9,iNe),e(Gf,dNe),e(v,cNe),e(v,Of),e(Of,VQ),e(VQ,mNe),e(Of,fNe),e(Of,w9),e(w9,gNe),e(Of,hNe),e(v,uNe),e(v,Xf),e(Xf,zQ),e(zQ,pNe),e(Xf,_Ne),e(Xf,A9),e(A9,bNe),e(Xf,vNe),e(v,TNe),e(v,Vf),e(Vf,WQ),e(WQ,FNe),e(Vf,CNe),e(Vf,L9),e(L9,MNe),e(Vf,ENe),e(v,yNe),e(v,zf),e(zf,QQ),e(QQ,wNe),e(zf,ANe),e(zf,B9),e(B9,LNe),e(zf,BNe),e(v,xNe),e(v,Wf),e(Wf,HQ),e(HQ,kNe),e(Wf,RNe),e(Wf,x9),e(x9,SNe),e(Wf,PNe),e(v,$Ne),e(v,Qf),e(Qf,UQ),e(UQ,INe),e(Qf,DNe),e(Qf,k9),e(k9,jNe),e(Qf,NNe),e(v,qNe),e(v,Hf),e(Hf,JQ),e(JQ,GNe),e(Hf,ONe),e(Hf,R9),e(R9,XNe),e(Hf,VNe),e(v,zNe),e(v,Uf),e(Uf,YQ),e(YQ,WNe),e(Uf,QNe),e(Uf,S9),e(S9,HNe),e(Uf,UNe),e(v,JNe),e(v,Jf),e(Jf,KQ),e(KQ,YNe),e(Jf,KNe),e(Jf,P9),e(P9,ZNe),e(Jf,eqe),e(v,oqe),e(v,Yf),e(Yf,ZQ),e(ZQ,rqe),e(Yf,tqe),e(Yf,$9),e($9,aqe),e(Yf,sqe),e(v,nqe),e(v,Kf),e(Kf,eH),e(eH,lqe),e(Kf,iqe),e(Kf,I9),e(I9,dqe),e(Kf,cqe),e(v,mqe),e(v,Zf),e(Zf,oH),e(oH,fqe),e(Zf,gqe),e(Zf,D9),e(D9,hqe),e(Zf,uqe),e(v,pqe),e(v,eg),e(eg,rH),e(rH,_qe),e(eg,bqe),e(eg,j9),e(j9,vqe),e(eg,Tqe),e(v,Fqe),e(v,og),e(og,tH),e(tH,Cqe),e(og,Mqe),e(og,N9),e(N9,Eqe),e(og,yqe),e(v,wqe),e(v,rg),e(rg,aH),e(aH,Aqe),e(rg,Lqe),e(rg,q9),e(q9,Bqe),e(rg,xqe),e(v,kqe),e(v,tg),e(tg,sH),e(sH,Rqe),e(tg,Sqe),e(tg,G9),e(G9,Pqe),e(tg,$qe),e(v,Iqe),e(v,ag),e(ag,nH),e(nH,Dqe),e(ag,jqe),e(ag,O9),e(O9,Nqe),e(ag,qqe),e(v,Gqe),e(v,sg),e(sg,lH),e(lH,Oqe),e(sg,Xqe),e(sg,X9),e(X9,Vqe),e(sg,zqe),e(v,Wqe),e(v,ng),e(ng,iH),e(iH,Qqe),e(ng,Hqe),e(ng,V9),e(V9,Uqe),e(ng,Jqe),e(v,Yqe),e(v,lg),e(lg,dH),e(dH,Kqe),e(lg,Zqe),e(lg,z9),e(z9,eGe),e(lg,oGe),e(v,rGe),e(v,ig),e(ig,cH),e(cH,tGe),e(ig,aGe),e(ig,W9),e(W9,sGe),e(ig,nGe),e(v,lGe),e(v,dg),e(dg,mH),e(mH,iGe),e(dg,dGe),e(dg,Q9),e(Q9,cGe),e(dg,mGe),e(v,fGe),e(v,cg),e(cg,fH),e(fH,gGe),e(cg,hGe),e(cg,H9),e(H9,uGe),e(cg,pGe),e(v,_Ge),e(v,mg),e(mg,gH),e(gH,bGe),e(mg,vGe),e(mg,U9),e(U9,TGe),e(mg,FGe),e(v,CGe),e(v,fg),e(fg,hH),e(hH,MGe),e(fg,EGe),e(fg,J9),e(J9,yGe),e(fg,wGe),e(v,AGe),e(v,gg),e(gg,uH),e(uH,LGe),e(gg,BGe),e(gg,Y9),e(Y9,xGe),e(gg,kGe),e(v,RGe),e(v,hg),e(hg,pH),e(pH,SGe),e(hg,PGe),e(hg,K9),e(K9,$Ge),e(hg,IGe),e(v,DGe),e(v,ug),e(ug,_H),e(_H,jGe),e(ug,NGe),e(ug,Z9),e(Z9,qGe),e(ug,GGe),e(v,OGe),e(v,pg),e(pg,bH),e(bH,XGe),e(pg,VGe),e(pg,eB),e(eB,zGe),e(pg,WGe),e(v,QGe),e(v,_g),e(_g,vH),e(vH,HGe),e(_g,UGe),e(_g,oB),e(oB,JGe),e(_g,YGe),e(mo,KGe),e(mo,TH),e(TH,ZGe),e(mo,eOe),g(P5,mo,null),e(Xo,oOe),e(Xo,bg),g($5,bg,null),e(bg,rOe),e(bg,FH),e(FH,tOe),b(c,D9e,_),b(c,Ni,_),e(Ni,vg),e(vg,CH),g(I5,CH,null),e(Ni,aOe),e(Ni,MH),e(MH,sOe),b(c,j9e,_),b(c,Vo,_),g(D5,Vo,null),e(Vo,nOe),e(Vo,j5),e(j5,lOe),e(j5,rB),e(rB,iOe),e(j5,dOe),e(Vo,cOe),e(Vo,N5),e(N5,mOe),e(N5,EH),e(EH,fOe),e(N5,gOe),e(Vo,hOe),e(Vo,fo),g(q5,fo,null),e(fo,uOe),e(fo,yH),e(yH,pOe),e(fo,_Oe),e(fo,Da),e(Da,bOe),e(Da,wH),e(wH,vOe),e(Da,TOe),e(Da,AH),e(AH,FOe),e(Da,COe),e(Da,LH),e(LH,MOe),e(Da,EOe),e(fo,yOe),e(fo,M),e(M,qs),e(qs,BH),e(BH,wOe),e(qs,AOe),e(qs,tB),e(tB,LOe),e(qs,BOe),e(qs,aB),e(aB,xOe),e(qs,kOe),e(M,ROe),e(M,Gs),e(Gs,xH),e(xH,SOe),e(Gs,POe),e(Gs,sB),e(sB,$Oe),e(Gs,IOe),e(Gs,nB),e(nB,DOe),e(Gs,jOe),e(M,NOe),e(M,Os),e(Os,kH),e(kH,qOe),e(Os,GOe),e(Os,lB),e(lB,OOe),e(Os,XOe),e(Os,iB),e(iB,VOe),e(Os,zOe),e(M,WOe),e(M,Tg),e(Tg,RH),e(RH,QOe),e(Tg,HOe),e(Tg,dB),e(dB,UOe),e(Tg,JOe),e(M,YOe),e(M,Xs),e(Xs,SH),e(SH,KOe),e(Xs,ZOe),e(Xs,cB),e(cB,eXe),e(Xs,oXe),e(Xs,mB),e(mB,rXe),e(Xs,tXe),e(M,aXe),e(M,Fg),e(Fg,PH),e(PH,sXe),e(Fg,nXe),e(Fg,fB),e(fB,lXe),e(Fg,iXe),e(M,dXe),e(M,Cg),e(Cg,$H),e($H,cXe),e(Cg,mXe),e(Cg,gB),e(gB,fXe),e(Cg,gXe),e(M,hXe),e(M,Mg),e(Mg,IH),e(IH,uXe),e(Mg,pXe),e(Mg,hB),e(hB,_Xe),e(Mg,bXe),e(M,vXe),e(M,Vs),e(Vs,DH),e(DH,TXe),e(Vs,FXe),e(Vs,uB),e(uB,CXe),e(Vs,MXe),e(Vs,pB),e(pB,EXe),e(Vs,yXe),e(M,wXe),e(M,zs),e(zs,jH),e(jH,AXe),e(zs,LXe),e(zs,_B),e(_B,BXe),e(zs,xXe),e(zs,bB),e(bB,kXe),e(zs,RXe),e(M,SXe),e(M,Ws),e(Ws,NH),e(NH,PXe),e(Ws,$Xe),e(Ws,vB),e(vB,IXe),e(Ws,DXe),e(Ws,TB),e(TB,jXe),e(Ws,NXe),e(M,qXe),e(M,Eg),e(Eg,qH),e(qH,GXe),e(Eg,OXe),e(Eg,FB),e(FB,XXe),e(Eg,VXe),e(M,zXe),e(M,yg),e(yg,GH),e(GH,WXe),e(yg,QXe),e(yg,CB),e(CB,HXe),e(yg,UXe),e(M,JXe),e(M,Qs),e(Qs,OH),e(OH,YXe),e(Qs,KXe),e(Qs,MB),e(MB,ZXe),e(Qs,eVe),e(Qs,EB),e(EB,oVe),e(Qs,rVe),e(M,tVe),e(M,wg),e(wg,XH),e(XH,aVe),e(wg,sVe),e(wg,yB),e(yB,nVe),e(wg,lVe),e(M,iVe),e(M,Hs),e(Hs,VH),e(VH,dVe),e(Hs,cVe),e(Hs,wB),e(wB,mVe),e(Hs,fVe),e(Hs,AB),e(AB,gVe),e(Hs,hVe),e(M,uVe),e(M,Us),e(Us,zH),e(zH,pVe),e(Us,_Ve),e(Us,LB),e(LB,bVe),e(Us,vVe),e(Us,BB),e(BB,TVe),e(Us,FVe),e(M,CVe),e(M,Js),e(Js,WH),e(WH,MVe),e(Js,EVe),e(Js,xB),e(xB,yVe),e(Js,wVe),e(Js,QH),e(QH,AVe),e(Js,LVe),e(M,BVe),e(M,Ag),e(Ag,HH),e(HH,xVe),e(Ag,kVe),e(Ag,kB),e(kB,RVe),e(Ag,SVe),e(M,PVe),e(M,Ys),e(Ys,UH),e(UH,$Ve),e(Ys,IVe),e(Ys,RB),e(RB,DVe),e(Ys,jVe),e(Ys,SB),e(SB,NVe),e(Ys,qVe),e(M,GVe),e(M,Lg),e(Lg,JH),e(JH,OVe),e(Lg,XVe),e(Lg,PB),e(PB,VVe),e(Lg,zVe),e(M,WVe),e(M,Ks),e(Ks,YH),e(YH,QVe),e(Ks,HVe),e(Ks,$B),e($B,UVe),e(Ks,JVe),e(Ks,IB),e(IB,YVe),e(Ks,KVe),e(M,ZVe),e(M,Zs),e(Zs,KH),e(KH,eze),e(Zs,oze),e(Zs,DB),e(DB,rze),e(Zs,tze),e(Zs,jB),e(jB,aze),e(Zs,sze),e(M,nze),e(M,en),e(en,ZH),e(ZH,lze),e(en,ize),e(en,NB),e(NB,dze),e(en,cze),e(en,qB),e(qB,mze),e(en,fze),e(M,gze),e(M,Bg),e(Bg,eU),e(eU,hze),e(Bg,uze),e(Bg,GB),e(GB,pze),e(Bg,_ze),e(M,bze),e(M,on),e(on,oU),e(oU,vze),e(on,Tze),e(on,OB),e(OB,Fze),e(on,Cze),e(on,XB),e(XB,Mze),e(on,Eze),e(M,yze),e(M,xg),e(xg,rU),e(rU,wze),e(xg,Aze),e(xg,VB),e(VB,Lze),e(xg,Bze),e(M,xze),e(M,rn),e(rn,tU),e(tU,kze),e(rn,Rze),e(rn,zB),e(zB,Sze),e(rn,Pze),e(rn,WB),e(WB,$ze),e(rn,Ize),e(M,Dze),e(M,tn),e(tn,aU),e(aU,jze),e(tn,Nze),e(tn,QB),e(QB,qze),e(tn,Gze),e(tn,HB),e(HB,Oze),e(tn,Xze),e(M,Vze),e(M,an),e(an,sU),e(sU,zze),e(an,Wze),e(an,UB),e(UB,Qze),e(an,Hze),e(an,JB),e(JB,Uze),e(an,Jze),e(M,Yze),e(M,sn),e(sn,nU),e(nU,Kze),e(sn,Zze),e(sn,YB),e(YB,eWe),e(sn,oWe),e(sn,KB),e(KB,rWe),e(sn,tWe),e(M,aWe),e(M,kg),e(kg,lU),e(lU,sWe),e(kg,nWe),e(kg,ZB),e(ZB,lWe),e(kg,iWe),e(M,dWe),e(M,nn),e(nn,iU),e(iU,cWe),e(nn,mWe),e(nn,ex),e(ex,fWe),e(nn,gWe),e(nn,ox),e(ox,hWe),e(nn,uWe),e(M,pWe),e(M,ln),e(ln,dU),e(dU,_We),e(ln,bWe),e(ln,rx),e(rx,vWe),e(ln,TWe),e(ln,tx),e(tx,FWe),e(ln,CWe),e(M,MWe),e(M,dn),e(dn,cU),e(cU,EWe),e(dn,yWe),e(dn,ax),e(ax,wWe),e(dn,AWe),e(dn,sx),e(sx,LWe),e(dn,BWe),e(M,xWe),e(M,cn),e(cn,mU),e(mU,kWe),e(cn,RWe),e(cn,nx),e(nx,SWe),e(cn,PWe),e(cn,lx),e(lx,$We),e(cn,IWe),e(M,DWe),e(M,mn),e(mn,fU),e(fU,jWe),e(mn,NWe),e(mn,ix),e(ix,qWe),e(mn,GWe),e(mn,dx),e(dx,OWe),e(mn,XWe),e(M,VWe),e(M,fn),e(fn,gU),e(gU,zWe),e(fn,WWe),e(fn,cx),e(cx,QWe),e(fn,HWe),e(fn,mx),e(mx,UWe),e(fn,JWe),e(M,YWe),e(M,Rg),e(Rg,hU),e(hU,KWe),e(Rg,ZWe),e(Rg,fx),e(fx,eQe),e(Rg,oQe),e(M,rQe),e(M,gn),e(gn,uU),e(uU,tQe),e(gn,aQe),e(gn,gx),e(gx,sQe),e(gn,nQe),e(gn,hx),e(hx,lQe),e(gn,iQe),e(M,dQe),e(M,Sg),e(Sg,pU),e(pU,cQe),e(Sg,mQe),e(Sg,ux),e(ux,fQe),e(Sg,gQe),e(M,hQe),e(M,Pg),e(Pg,_U),e(_U,uQe),e(Pg,pQe),e(Pg,px),e(px,_Qe),e(Pg,bQe),e(M,vQe),e(M,hn),e(hn,bU),e(bU,TQe),e(hn,FQe),e(hn,_x),e(_x,CQe),e(hn,MQe),e(hn,bx),e(bx,EQe),e(hn,yQe),e(M,wQe),e(M,un),e(un,vU),e(vU,AQe),e(un,LQe),e(un,vx),e(vx,BQe),e(un,xQe),e(un,Tx),e(Tx,kQe),e(un,RQe),e(M,SQe),e(M,$g),e($g,TU),e(TU,PQe),e($g,$Qe),e($g,Fx),e(Fx,IQe),e($g,DQe),e(M,jQe),e(M,pn),e(pn,FU),e(FU,NQe),e(pn,qQe),e(pn,Cx),e(Cx,GQe),e(pn,OQe),e(pn,Mx),e(Mx,XQe),e(pn,VQe),e(M,zQe),e(M,_n),e(_n,CU),e(CU,WQe),e(_n,QQe),e(_n,Ex),e(Ex,HQe),e(_n,UQe),e(_n,yx),e(yx,JQe),e(_n,YQe),e(M,KQe),e(M,bn),e(bn,MU),e(MU,ZQe),e(bn,eHe),e(bn,wx),e(wx,oHe),e(bn,rHe),e(bn,Ax),e(Ax,tHe),e(bn,aHe),e(M,sHe),e(M,vn),e(vn,EU),e(EU,nHe),e(vn,lHe),e(vn,Lx),e(Lx,iHe),e(vn,dHe),e(vn,Bx),e(Bx,cHe),e(vn,mHe),e(M,fHe),e(M,Tn),e(Tn,yU),e(yU,gHe),e(Tn,hHe),e(Tn,xx),e(xx,uHe),e(Tn,pHe),e(Tn,kx),e(kx,_He),e(Tn,bHe),e(M,vHe),e(M,Ig),e(Ig,wU),e(wU,THe),e(Ig,FHe),e(Ig,Rx),e(Rx,CHe),e(Ig,MHe),e(M,EHe),e(M,Dg),e(Dg,AU),e(AU,yHe),e(Dg,wHe),e(Dg,Sx),e(Sx,AHe),e(Dg,LHe),e(M,BHe),e(M,jg),e(jg,LU),e(LU,xHe),e(jg,kHe),e(jg,Px),e(Px,RHe),e(jg,SHe),e(M,PHe),e(M,Ng),e(Ng,BU),e(BU,$He),e(Ng,IHe),e(Ng,$x),e($x,DHe),e(Ng,jHe),e(M,NHe),e(M,Fn),e(Fn,xU),e(xU,qHe),e(Fn,GHe),e(Fn,Ix),e(Ix,OHe),e(Fn,XHe),e(Fn,Dx),e(Dx,VHe),e(Fn,zHe),e(M,WHe),e(M,qg),e(qg,kU),e(kU,QHe),e(qg,HHe),e(qg,jx),e(jx,UHe),e(qg,JHe),e(M,YHe),e(M,Cn),e(Cn,RU),e(RU,KHe),e(Cn,ZHe),e(Cn,Nx),e(Nx,eUe),e(Cn,oUe),e(Cn,qx),e(qx,rUe),e(Cn,tUe),e(M,aUe),e(M,Mn),e(Mn,SU),e(SU,sUe),e(Mn,nUe),e(Mn,Gx),e(Gx,lUe),e(Mn,iUe),e(Mn,Ox),e(Ox,dUe),e(Mn,cUe),e(M,mUe),e(M,En),e(En,PU),e(PU,fUe),e(En,gUe),e(En,Xx),e(Xx,hUe),e(En,uUe),e(En,Vx),e(Vx,pUe),e(En,_Ue),e(M,bUe),e(M,yn),e(yn,$U),e($U,vUe),e(yn,TUe),e(yn,zx),e(zx,FUe),e(yn,CUe),e(yn,Wx),e(Wx,MUe),e(yn,EUe),e(M,yUe),e(M,wn),e(wn,IU),e(IU,wUe),e(wn,AUe),e(wn,Qx),e(Qx,LUe),e(wn,BUe),e(wn,Hx),e(Hx,xUe),e(wn,kUe),e(M,RUe),e(M,An),e(An,DU),e(DU,SUe),e(An,PUe),e(An,Ux),e(Ux,$Ue),e(An,IUe),e(An,Jx),e(Jx,DUe),e(An,jUe),e(M,NUe),e(M,Gg),e(Gg,jU),e(jU,qUe),e(Gg,GUe),e(Gg,Yx),e(Yx,OUe),e(Gg,XUe),e(M,VUe),e(M,Og),e(Og,NU),e(NU,zUe),e(Og,WUe),e(Og,Kx),e(Kx,QUe),e(Og,HUe),e(M,UUe),e(M,Ln),e(Ln,qU),e(qU,JUe),e(Ln,YUe),e(Ln,Zx),e(Zx,KUe),e(Ln,ZUe),e(Ln,ek),e(ek,eJe),e(Ln,oJe),e(M,rJe),e(M,Bn),e(Bn,GU),e(GU,tJe),e(Bn,aJe),e(Bn,ok),e(ok,sJe),e(Bn,nJe),e(Bn,rk),e(rk,lJe),e(Bn,iJe),e(M,dJe),e(M,xn),e(xn,OU),e(OU,cJe),e(xn,mJe),e(xn,tk),e(tk,fJe),e(xn,gJe),e(xn,ak),e(ak,hJe),e(xn,uJe),e(M,pJe),e(M,Xg),e(Xg,XU),e(XU,_Je),e(Xg,bJe),e(Xg,sk),e(sk,vJe),e(Xg,TJe),e(M,FJe),e(M,Vg),e(Vg,VU),e(VU,CJe),e(Vg,MJe),e(Vg,nk),e(nk,EJe),e(Vg,yJe),e(M,wJe),e(M,zg),e(zg,zU),e(zU,AJe),e(zg,LJe),e(zg,lk),e(lk,BJe),e(zg,xJe),e(M,kJe),e(M,Wg),e(Wg,WU),e(WU,RJe),e(Wg,SJe),e(Wg,ik),e(ik,PJe),e(Wg,$Je),e(M,IJe),e(M,kn),e(kn,QU),e(QU,DJe),e(kn,jJe),e(kn,dk),e(dk,NJe),e(kn,qJe),e(kn,ck),e(ck,GJe),e(kn,OJe),e(M,XJe),e(M,Qg),e(Qg,HU),e(HU,VJe),e(Qg,zJe),e(Qg,mk),e(mk,WJe),e(Qg,QJe),e(M,HJe),e(M,Hg),e(Hg,UU),e(UU,UJe),e(Hg,JJe),e(Hg,fk),e(fk,YJe),e(Hg,KJe),e(M,ZJe),e(M,Rn),e(Rn,JU),e(JU,eYe),e(Rn,oYe),e(Rn,gk),e(gk,rYe),e(Rn,tYe),e(Rn,hk),e(hk,aYe),e(Rn,sYe),e(M,nYe),e(M,Sn),e(Sn,YU),e(YU,lYe),e(Sn,iYe),e(Sn,uk),e(uk,dYe),e(Sn,cYe),e(Sn,pk),e(pk,mYe),e(Sn,fYe),e(fo,gYe),e(fo,KU),e(KU,hYe),e(fo,uYe),g(G5,fo,null),e(Vo,pYe),e(Vo,Ug),g(O5,Ug,null),e(Ug,_Ye),e(Ug,ZU),e(ZU,bYe),b(c,N9e,_),b(c,qi,_),e(qi,Jg),e(Jg,eJ),g(X5,eJ,null),e(qi,vYe),e(qi,oJ),e(oJ,TYe),b(c,q9e,_),b(c,zo,_),g(V5,zo,null),e(zo,FYe),e(zo,z5),e(z5,CYe),e(z5,_k),e(_k,MYe),e(z5,EYe),e(zo,yYe),e(zo,W5),e(W5,wYe),e(W5,rJ),e(rJ,AYe),e(W5,LYe),e(zo,BYe),e(zo,xe),g(Q5,xe,null),e(xe,xYe),e(xe,tJ),e(tJ,kYe),e(xe,RYe),e(xe,ja),e(ja,SYe),e(ja,aJ),e(aJ,PYe),e(ja,$Ye),e(ja,sJ),e(sJ,IYe),e(ja,DYe),e(ja,nJ),e(nJ,jYe),e(ja,NYe),e(xe,qYe),e(xe,ne),e(ne,Yg),e(Yg,lJ),e(lJ,GYe),e(Yg,OYe),e(Yg,bk),e(bk,XYe),e(Yg,VYe),e(ne,zYe),e(ne,Kg),e(Kg,iJ),e(iJ,WYe),e(Kg,QYe),e(Kg,vk),e(vk,HYe),e(Kg,UYe),e(ne,JYe),e(ne,Zg),e(Zg,dJ),e(dJ,YYe),e(Zg,KYe),e(Zg,Tk),e(Tk,ZYe),e(Zg,eKe),e(ne,oKe),e(ne,eh),e(eh,cJ),e(cJ,rKe),e(eh,tKe),e(eh,Fk),e(Fk,aKe),e(eh,sKe),e(ne,nKe),e(ne,oh),e(oh,mJ),e(mJ,lKe),e(oh,iKe),e(oh,Ck),e(Ck,dKe),e(oh,cKe),e(ne,mKe),e(ne,rh),e(rh,fJ),e(fJ,fKe),e(rh,gKe),e(rh,Mk),e(Mk,hKe),e(rh,uKe),e(ne,pKe),e(ne,th),e(th,gJ),e(gJ,_Ke),e(th,bKe),e(th,Ek),e(Ek,vKe),e(th,TKe),e(ne,FKe),e(ne,ah),e(ah,hJ),e(hJ,CKe),e(ah,MKe),e(ah,yk),e(yk,EKe),e(ah,yKe),e(ne,wKe),e(ne,sh),e(sh,uJ),e(uJ,AKe),e(sh,LKe),e(sh,wk),e(wk,BKe),e(sh,xKe),e(ne,kKe),e(ne,nh),e(nh,pJ),e(pJ,RKe),e(nh,SKe),e(nh,Ak),e(Ak,PKe),e(nh,$Ke),e(ne,IKe),e(ne,lh),e(lh,_J),e(_J,DKe),e(lh,jKe),e(lh,Lk),e(Lk,NKe),e(lh,qKe),e(ne,GKe),e(ne,ih),e(ih,bJ),e(bJ,OKe),e(ih,XKe),e(ih,Bk),e(Bk,VKe),e(ih,zKe),e(ne,WKe),e(ne,dh),e(dh,vJ),e(vJ,QKe),e(dh,HKe),e(dh,xk),e(xk,UKe),e(dh,JKe),e(ne,YKe),e(ne,ch),e(ch,TJ),e(TJ,KKe),e(ch,ZKe),e(ch,kk),e(kk,eZe),e(ch,oZe),e(ne,rZe),e(ne,mh),e(mh,FJ),e(FJ,tZe),e(mh,aZe),e(mh,Rk),e(Rk,sZe),e(mh,nZe),e(xe,lZe),g(fh,xe,null),e(xe,iZe),e(xe,CJ),e(CJ,dZe),e(xe,cZe),g(H5,xe,null),e(zo,mZe),e(zo,gh),g(U5,gh,null),e(gh,fZe),e(gh,MJ),e(MJ,gZe),b(c,G9e,_),b(c,Gi,_),e(Gi,hh),e(hh,EJ),g(J5,EJ,null),e(Gi,hZe),e(Gi,yJ),e(yJ,uZe),b(c,O9e,_),b(c,Wo,_),g(Y5,Wo,null),e(Wo,pZe),e(Wo,K5),e(K5,_Ze),e(K5,Sk),e(Sk,bZe),e(K5,vZe),e(Wo,TZe),e(Wo,Z5),e(Z5,FZe),e(Z5,wJ),e(wJ,CZe),e(Z5,MZe),e(Wo,EZe),e(Wo,ke),g(ey,ke,null),e(ke,yZe),e(ke,AJ),e(AJ,wZe),e(ke,AZe),e(ke,Oi),e(Oi,LZe),e(Oi,LJ),e(LJ,BZe),e(Oi,xZe),e(Oi,BJ),e(BJ,kZe),e(Oi,RZe),e(ke,SZe),e(ke,we),e(we,uh),e(uh,xJ),e(xJ,PZe),e(uh,$Ze),e(uh,Pk),e(Pk,IZe),e(uh,DZe),e(we,jZe),e(we,ph),e(ph,kJ),e(kJ,NZe),e(ph,qZe),e(ph,$k),e($k,GZe),e(ph,OZe),e(we,XZe),e(we,_h),e(_h,RJ),e(RJ,VZe),e(_h,zZe),e(_h,Ik),e(Ik,WZe),e(_h,QZe),e(we,HZe),e(we,bh),e(bh,SJ),e(SJ,UZe),e(bh,JZe),e(bh,Dk),e(Dk,YZe),e(bh,KZe),e(we,ZZe),e(we,vh),e(vh,PJ),e(PJ,eeo),e(vh,oeo),e(vh,jk),e(jk,reo),e(vh,teo),e(we,aeo),e(we,Th),e(Th,$J),e($J,seo),e(Th,neo),e(Th,Nk),e(Nk,leo),e(Th,ieo),e(we,deo),e(we,Fh),e(Fh,IJ),e(IJ,ceo),e(Fh,meo),e(Fh,qk),e(qk,feo),e(Fh,geo),e(we,heo),e(we,Ch),e(Ch,DJ),e(DJ,ueo),e(Ch,peo),e(Ch,Gk),e(Gk,_eo),e(Ch,beo),e(ke,veo),g(Mh,ke,null),e(ke,Teo),e(ke,jJ),e(jJ,Feo),e(ke,Ceo),g(oy,ke,null),e(Wo,Meo),e(Wo,Eh),g(ry,Eh,null),e(Eh,Eeo),e(Eh,NJ),e(NJ,yeo),b(c,X9e,_),b(c,Xi,_),e(Xi,yh),e(yh,qJ),g(ty,qJ,null),e(Xi,weo),e(Xi,GJ),e(GJ,Aeo),b(c,V9e,_),b(c,Qo,_),g(ay,Qo,null),e(Qo,Leo),e(Qo,Vi),e(Vi,Beo),e(Vi,OJ),e(OJ,xeo),e(Vi,keo),e(Vi,XJ),e(XJ,Reo),e(Vi,Seo),e(Qo,Peo),e(Qo,sy),e(sy,$eo),e(sy,VJ),e(VJ,Ieo),e(sy,Deo),e(Qo,jeo),e(Qo,qr),g(ny,qr,null),e(qr,Neo),e(qr,zJ),e(zJ,qeo),e(qr,Geo),e(qr,zi),e(zi,Oeo),e(zi,WJ),e(WJ,Xeo),e(zi,Veo),e(zi,QJ),e(QJ,zeo),e(zi,Weo),e(qr,Qeo),e(qr,HJ),e(HJ,Heo),e(qr,Ueo),g(ly,qr,null),e(Qo,Jeo),e(Qo,Re),g(iy,Re,null),e(Re,Yeo),e(Re,UJ),e(UJ,Keo),e(Re,Zeo),e(Re,Na),e(Na,eoo),e(Na,JJ),e(JJ,ooo),e(Na,roo),e(Na,YJ),e(YJ,too),e(Na,aoo),e(Na,KJ),e(KJ,soo),e(Na,noo),e(Re,loo),e(Re,F),e(F,wh),e(wh,ZJ),e(ZJ,ioo),e(wh,doo),e(wh,Ok),e(Ok,coo),e(wh,moo),e(F,foo),e(F,Ah),e(Ah,eY),e(eY,goo),e(Ah,hoo),e(Ah,Xk),e(Xk,uoo),e(Ah,poo),e(F,_oo),e(F,Lh),e(Lh,oY),e(oY,boo),e(Lh,voo),e(Lh,Vk),e(Vk,Too),e(Lh,Foo),e(F,Coo),e(F,Bh),e(Bh,rY),e(rY,Moo),e(Bh,Eoo),e(Bh,zk),e(zk,yoo),e(Bh,woo),e(F,Aoo),e(F,xh),e(xh,tY),e(tY,Loo),e(xh,Boo),e(xh,Wk),e(Wk,xoo),e(xh,koo),e(F,Roo),e(F,kh),e(kh,aY),e(aY,Soo),e(kh,Poo),e(kh,Qk),e(Qk,$oo),e(kh,Ioo),e(F,Doo),e(F,Rh),e(Rh,sY),e(sY,joo),e(Rh,Noo),e(Rh,Hk),e(Hk,qoo),e(Rh,Goo),e(F,Ooo),e(F,Sh),e(Sh,nY),e(nY,Xoo),e(Sh,Voo),e(Sh,Uk),e(Uk,zoo),e(Sh,Woo),e(F,Qoo),e(F,Ph),e(Ph,lY),e(lY,Hoo),e(Ph,Uoo),e(Ph,Jk),e(Jk,Joo),e(Ph,Yoo),e(F,Koo),e(F,$h),e($h,iY),e(iY,Zoo),e($h,ero),e($h,Yk),e(Yk,oro),e($h,rro),e(F,tro),e(F,Ih),e(Ih,dY),e(dY,aro),e(Ih,sro),e(Ih,Kk),e(Kk,nro),e(Ih,lro),e(F,iro),e(F,Dh),e(Dh,cY),e(cY,dro),e(Dh,cro),e(Dh,Zk),e(Zk,mro),e(Dh,fro),e(F,gro),e(F,jh),e(jh,mY),e(mY,hro),e(jh,uro),e(jh,eR),e(eR,pro),e(jh,_ro),e(F,bro),e(F,Nh),e(Nh,fY),e(fY,vro),e(Nh,Tro),e(Nh,oR),e(oR,Fro),e(Nh,Cro),e(F,Mro),e(F,qh),e(qh,gY),e(gY,Ero),e(qh,yro),e(qh,rR),e(rR,wro),e(qh,Aro),e(F,Lro),e(F,Gh),e(Gh,hY),e(hY,Bro),e(Gh,xro),e(Gh,tR),e(tR,kro),e(Gh,Rro),e(F,Sro),e(F,Oh),e(Oh,uY),e(uY,Pro),e(Oh,$ro),e(Oh,aR),e(aR,Iro),e(Oh,Dro),e(F,jro),e(F,Xh),e(Xh,pY),e(pY,Nro),e(Xh,qro),e(Xh,sR),e(sR,Gro),e(Xh,Oro),e(F,Xro),e(F,Vh),e(Vh,_Y),e(_Y,Vro),e(Vh,zro),e(Vh,nR),e(nR,Wro),e(Vh,Qro),e(F,Hro),e(F,zh),e(zh,bY),e(bY,Uro),e(zh,Jro),e(zh,lR),e(lR,Yro),e(zh,Kro),e(F,Zro),e(F,Wh),e(Wh,vY),e(vY,eto),e(Wh,oto),e(Wh,iR),e(iR,rto),e(Wh,tto),e(F,ato),e(F,Qh),e(Qh,TY),e(TY,sto),e(Qh,nto),e(Qh,dR),e(dR,lto),e(Qh,ito),e(F,dto),e(F,Hh),e(Hh,FY),e(FY,cto),e(Hh,mto),e(Hh,cR),e(cR,fto),e(Hh,gto),e(F,hto),e(F,Uh),e(Uh,CY),e(CY,uto),e(Uh,pto),e(Uh,mR),e(mR,_to),e(Uh,bto),e(F,vto),e(F,Jh),e(Jh,MY),e(MY,Tto),e(Jh,Fto),e(Jh,fR),e(fR,Cto),e(Jh,Mto),e(F,Eto),e(F,Yh),e(Yh,EY),e(EY,yto),e(Yh,wto),e(Yh,gR),e(gR,Ato),e(Yh,Lto),e(F,Bto),e(F,Kh),e(Kh,yY),e(yY,xto),e(Kh,kto),e(Kh,hR),e(hR,Rto),e(Kh,Sto),e(F,Pto),e(F,Pn),e(Pn,wY),e(wY,$to),e(Pn,Ito),e(Pn,uR),e(uR,Dto),e(Pn,jto),e(Pn,pR),e(pR,Nto),e(Pn,qto),e(F,Gto),e(F,Zh),e(Zh,AY),e(AY,Oto),e(Zh,Xto),e(Zh,_R),e(_R,Vto),e(Zh,zto),e(F,Wto),e(F,eu),e(eu,LY),e(LY,Qto),e(eu,Hto),e(eu,bR),e(bR,Uto),e(eu,Jto),e(F,Yto),e(F,ou),e(ou,BY),e(BY,Kto),e(ou,Zto),e(ou,vR),e(vR,eao),e(ou,oao),e(F,rao),e(F,ru),e(ru,xY),e(xY,tao),e(ru,aao),e(ru,TR),e(TR,sao),e(ru,nao),e(F,lao),e(F,tu),e(tu,kY),e(kY,iao),e(tu,dao),e(tu,FR),e(FR,cao),e(tu,mao),e(F,fao),e(F,au),e(au,RY),e(RY,gao),e(au,hao),e(au,CR),e(CR,uao),e(au,pao),e(F,_ao),e(F,su),e(su,SY),e(SY,bao),e(su,vao),e(su,MR),e(MR,Tao),e(su,Fao),e(F,Cao),e(F,nu),e(nu,PY),e(PY,Mao),e(nu,Eao),e(nu,ER),e(ER,yao),e(nu,wao),e(F,Aao),e(F,lu),e(lu,$Y),e($Y,Lao),e(lu,Bao),e(lu,yR),e(yR,xao),e(lu,kao),e(F,Rao),e(F,iu),e(iu,IY),e(IY,Sao),e(iu,Pao),e(iu,wR),e(wR,$ao),e(iu,Iao),e(F,Dao),e(F,du),e(du,DY),e(DY,jao),e(du,Nao),e(du,AR),e(AR,qao),e(du,Gao),e(F,Oao),e(F,cu),e(cu,jY),e(jY,Xao),e(cu,Vao),e(cu,LR),e(LR,zao),e(cu,Wao),e(F,Qao),e(F,mu),e(mu,NY),e(NY,Hao),e(mu,Uao),e(mu,BR),e(BR,Jao),e(mu,Yao),e(F,Kao),e(F,fu),e(fu,qY),e(qY,Zao),e(fu,eso),e(fu,xR),e(xR,oso),e(fu,rso),e(F,tso),e(F,gu),e(gu,GY),e(GY,aso),e(gu,sso),e(gu,kR),e(kR,nso),e(gu,lso),e(F,iso),e(F,hu),e(hu,OY),e(OY,dso),e(hu,cso),e(hu,RR),e(RR,mso),e(hu,fso),e(F,gso),e(F,uu),e(uu,XY),e(XY,hso),e(uu,uso),e(uu,SR),e(SR,pso),e(uu,_so),e(F,bso),e(F,pu),e(pu,VY),e(VY,vso),e(pu,Tso),e(pu,PR),e(PR,Fso),e(pu,Cso),e(F,Mso),e(F,_u),e(_u,zY),e(zY,Eso),e(_u,yso),e(_u,$R),e($R,wso),e(_u,Aso),e(F,Lso),e(F,bu),e(bu,WY),e(WY,Bso),e(bu,xso),e(bu,IR),e(IR,kso),e(bu,Rso),e(F,Sso),e(F,vu),e(vu,QY),e(QY,Pso),e(vu,$so),e(vu,DR),e(DR,Iso),e(vu,Dso),e(F,jso),e(F,Tu),e(Tu,HY),e(HY,Nso),e(Tu,qso),e(Tu,jR),e(jR,Gso),e(Tu,Oso),e(F,Xso),e(F,Fu),e(Fu,UY),e(UY,Vso),e(Fu,zso),e(Fu,NR),e(NR,Wso),e(Fu,Qso),e(F,Hso),e(F,Cu),e(Cu,JY),e(JY,Uso),e(Cu,Jso),e(Cu,qR),e(qR,Yso),e(Cu,Kso),e(F,Zso),e(F,Mu),e(Mu,YY),e(YY,eno),e(Mu,ono),e(Mu,GR),e(GR,rno),e(Mu,tno),e(F,ano),e(F,Eu),e(Eu,KY),e(KY,sno),e(Eu,nno),e(Eu,OR),e(OR,lno),e(Eu,ino),e(F,dno),e(F,yu),e(yu,ZY),e(ZY,cno),e(yu,mno),e(yu,XR),e(XR,fno),e(yu,gno),e(F,hno),e(F,wu),e(wu,eK),e(eK,uno),e(wu,pno),e(wu,VR),e(VR,_no),e(wu,bno),e(F,vno),e(F,Au),e(Au,oK),e(oK,Tno),e(Au,Fno),e(Au,zR),e(zR,Cno),e(Au,Mno),e(F,Eno),e(F,Lu),e(Lu,rK),e(rK,yno),e(Lu,wno),e(Lu,WR),e(WR,Ano),e(Lu,Lno),e(F,Bno),e(F,Bu),e(Bu,tK),e(tK,xno),e(Bu,kno),e(Bu,QR),e(QR,Rno),e(Bu,Sno),e(F,Pno),e(F,xu),e(xu,aK),e(aK,$no),e(xu,Ino),e(xu,HR),e(HR,Dno),e(xu,jno),e(F,Nno),e(F,ku),e(ku,sK),e(sK,qno),e(ku,Gno),e(ku,UR),e(UR,Ono),e(ku,Xno),e(F,Vno),e(F,Ru),e(Ru,nK),e(nK,zno),e(Ru,Wno),e(Ru,JR),e(JR,Qno),e(Ru,Hno),e(F,Uno),e(F,Su),e(Su,lK),e(lK,Jno),e(Su,Yno),e(Su,YR),e(YR,Kno),e(Su,Zno),e(F,elo),e(F,Pu),e(Pu,iK),e(iK,olo),e(Pu,rlo),e(Pu,KR),e(KR,tlo),e(Pu,alo),e(F,slo),e(F,$u),e($u,dK),e(dK,nlo),e($u,llo),e($u,ZR),e(ZR,ilo),e($u,dlo),e(F,clo),e(F,Iu),e(Iu,cK),e(cK,mlo),e(Iu,flo),e(Iu,eS),e(eS,glo),e(Iu,hlo),e(F,ulo),e(F,Du),e(Du,mK),e(mK,plo),e(Du,_lo),e(Du,oS),e(oS,blo),e(Du,vlo),e(F,Tlo),e(F,ju),e(ju,fK),e(fK,Flo),e(ju,Clo),e(ju,rS),e(rS,Mlo),e(ju,Elo),e(F,ylo),e(F,Nu),e(Nu,gK),e(gK,wlo),e(Nu,Alo),e(Nu,tS),e(tS,Llo),e(Nu,Blo),e(F,xlo),e(F,qu),e(qu,hK),e(hK,klo),e(qu,Rlo),e(qu,aS),e(aS,Slo),e(qu,Plo),e(F,$lo),e(F,Gu),e(Gu,uK),e(uK,Ilo),e(Gu,Dlo),e(Gu,sS),e(sS,jlo),e(Gu,Nlo),e(F,qlo),e(F,Ou),e(Ou,pK),e(pK,Glo),e(Ou,Olo),e(Ou,nS),e(nS,Xlo),e(Ou,Vlo),e(F,zlo),e(F,Xu),e(Xu,_K),e(_K,Wlo),e(Xu,Qlo),e(Xu,lS),e(lS,Hlo),e(Xu,Ulo),e(F,Jlo),e(F,Vu),e(Vu,bK),e(bK,Ylo),e(Vu,Klo),e(Vu,iS),e(iS,Zlo),e(Vu,eio),e(F,oio),e(F,zu),e(zu,vK),e(vK,rio),e(zu,tio),e(zu,dS),e(dS,aio),e(zu,sio),e(F,nio),e(F,Wu),e(Wu,TK),e(TK,lio),e(Wu,iio),e(Wu,cS),e(cS,dio),e(Wu,cio),e(F,mio),e(F,Qu),e(Qu,FK),e(FK,fio),e(Qu,gio),e(Qu,mS),e(mS,hio),e(Qu,uio),e(F,pio),e(F,Hu),e(Hu,CK),e(CK,_io),e(Hu,bio),e(Hu,fS),e(fS,vio),e(Hu,Tio),e(F,Fio),e(F,Uu),e(Uu,MK),e(MK,Cio),e(Uu,Mio),e(Uu,gS),e(gS,Eio),e(Uu,yio),e(F,wio),e(F,Ju),e(Ju,EK),e(EK,Aio),e(Ju,Lio),e(Ju,hS),e(hS,Bio),e(Ju,xio),e(F,kio),e(F,Yu),e(Yu,yK),e(yK,Rio),e(Yu,Sio),e(Yu,uS),e(uS,Pio),e(Yu,$io),e(F,Iio),e(F,Ku),e(Ku,wK),e(wK,Dio),e(Ku,jio),e(Ku,pS),e(pS,Nio),e(Ku,qio),e(F,Gio),e(F,Zu),e(Zu,AK),e(AK,Oio),e(Zu,Xio),e(Zu,_S),e(_S,Vio),e(Zu,zio),e(F,Wio),e(F,ep),e(ep,LK),e(LK,Qio),e(ep,Hio),e(ep,bS),e(bS,Uio),e(ep,Jio),e(F,Yio),e(F,op),e(op,BK),e(BK,Kio),e(op,Zio),e(op,vS),e(vS,edo),e(op,odo),e(F,rdo),e(F,rp),e(rp,xK),e(xK,tdo),e(rp,ado),e(rp,TS),e(TS,sdo),e(rp,ndo),e(F,ldo),e(F,tp),e(tp,kK),e(kK,ido),e(tp,ddo),e(tp,FS),e(FS,cdo),e(tp,mdo),e(Re,fdo),e(Re,ap),e(ap,gdo),e(ap,RK),e(RK,hdo),e(ap,udo),e(ap,SK),e(SK,pdo),e(Re,_do),e(Re,PK),e(PK,bdo),e(Re,vdo),g(dy,Re,null),b(c,z9e,_),b(c,Wi,_),e(Wi,sp),e(sp,$K),g(cy,$K,null),e(Wi,Tdo),e(Wi,IK),e(IK,Fdo),b(c,W9e,_),b(c,Ho,_),g(my,Ho,null),e(Ho,Cdo),e(Ho,Qi),e(Qi,Mdo),e(Qi,DK),e(DK,Edo),e(Qi,ydo),e(Qi,jK),e(jK,wdo),e(Qi,Ado),e(Ho,Ldo),e(Ho,fy),e(fy,Bdo),e(fy,NK),e(NK,xdo),e(fy,kdo),e(Ho,Rdo),e(Ho,Gr),g(gy,Gr,null),e(Gr,Sdo),e(Gr,qK),e(qK,Pdo),e(Gr,$do),e(Gr,Hi),e(Hi,Ido),e(Hi,GK),e(GK,Ddo),e(Hi,jdo),e(Hi,OK),e(OK,Ndo),e(Hi,qdo),e(Gr,Gdo),e(Gr,XK),e(XK,Odo),e(Gr,Xdo),g(hy,Gr,null),e(Ho,Vdo),e(Ho,Se),g(uy,Se,null),e(Se,zdo),e(Se,VK),e(VK,Wdo),e(Se,Qdo),e(Se,qa),e(qa,Hdo),e(qa,zK),e(zK,Udo),e(qa,Jdo),e(qa,WK),e(WK,Ydo),e(qa,Kdo),e(qa,QK),e(QK,Zdo),e(qa,eco),e(Se,oco),e(Se,k),e(k,np),e(np,HK),e(HK,rco),e(np,tco),e(np,CS),e(CS,aco),e(np,sco),e(k,nco),e(k,lp),e(lp,UK),e(UK,lco),e(lp,ico),e(lp,MS),e(MS,dco),e(lp,cco),e(k,mco),e(k,ip),e(ip,JK),e(JK,fco),e(ip,gco),e(ip,ES),e(ES,hco),e(ip,uco),e(k,pco),e(k,dp),e(dp,YK),e(YK,_co),e(dp,bco),e(dp,yS),e(yS,vco),e(dp,Tco),e(k,Fco),e(k,cp),e(cp,KK),e(KK,Cco),e(cp,Mco),e(cp,wS),e(wS,Eco),e(cp,yco),e(k,wco),e(k,mp),e(mp,ZK),e(ZK,Aco),e(mp,Lco),e(mp,AS),e(AS,Bco),e(mp,xco),e(k,kco),e(k,fp),e(fp,eZ),e(eZ,Rco),e(fp,Sco),e(fp,LS),e(LS,Pco),e(fp,$co),e(k,Ico),e(k,gp),e(gp,oZ),e(oZ,Dco),e(gp,jco),e(gp,BS),e(BS,Nco),e(gp,qco),e(k,Gco),e(k,hp),e(hp,rZ),e(rZ,Oco),e(hp,Xco),e(hp,xS),e(xS,Vco),e(hp,zco),e(k,Wco),e(k,up),e(up,tZ),e(tZ,Qco),e(up,Hco),e(up,kS),e(kS,Uco),e(up,Jco),e(k,Yco),e(k,pp),e(pp,aZ),e(aZ,Kco),e(pp,Zco),e(pp,RS),e(RS,emo),e(pp,omo),e(k,rmo),e(k,_p),e(_p,sZ),e(sZ,tmo),e(_p,amo),e(_p,SS),e(SS,smo),e(_p,nmo),e(k,lmo),e(k,bp),e(bp,nZ),e(nZ,imo),e(bp,dmo),e(bp,PS),e(PS,cmo),e(bp,mmo),e(k,fmo),e(k,vp),e(vp,lZ),e(lZ,gmo),e(vp,hmo),e(vp,$S),e($S,umo),e(vp,pmo),e(k,_mo),e(k,Tp),e(Tp,iZ),e(iZ,bmo),e(Tp,vmo),e(Tp,IS),e(IS,Tmo),e(Tp,Fmo),e(k,Cmo),e(k,Fp),e(Fp,dZ),e(dZ,Mmo),e(Fp,Emo),e(Fp,DS),e(DS,ymo),e(Fp,wmo),e(k,Amo),e(k,Cp),e(Cp,cZ),e(cZ,Lmo),e(Cp,Bmo),e(Cp,jS),e(jS,xmo),e(Cp,kmo),e(k,Rmo),e(k,Mp),e(Mp,mZ),e(mZ,Smo),e(Mp,Pmo),e(Mp,NS),e(NS,$mo),e(Mp,Imo),e(k,Dmo),e(k,Ep),e(Ep,fZ),e(fZ,jmo),e(Ep,Nmo),e(Ep,qS),e(qS,qmo),e(Ep,Gmo),e(k,Omo),e(k,yp),e(yp,gZ),e(gZ,Xmo),e(yp,Vmo),e(yp,GS),e(GS,zmo),e(yp,Wmo),e(k,Qmo),e(k,wp),e(wp,hZ),e(hZ,Hmo),e(wp,Umo),e(wp,OS),e(OS,Jmo),e(wp,Ymo),e(k,Kmo),e(k,Ap),e(Ap,uZ),e(uZ,Zmo),e(Ap,efo),e(Ap,XS),e(XS,ofo),e(Ap,rfo),e(k,tfo),e(k,Lp),e(Lp,pZ),e(pZ,afo),e(Lp,sfo),e(Lp,VS),e(VS,nfo),e(Lp,lfo),e(k,ifo),e(k,Bp),e(Bp,_Z),e(_Z,dfo),e(Bp,cfo),e(Bp,zS),e(zS,mfo),e(Bp,ffo),e(k,gfo),e(k,xp),e(xp,bZ),e(bZ,hfo),e(xp,ufo),e(xp,WS),e(WS,pfo),e(xp,_fo),e(k,bfo),e(k,kp),e(kp,vZ),e(vZ,vfo),e(kp,Tfo),e(kp,QS),e(QS,Ffo),e(kp,Cfo),e(k,Mfo),e(k,Rp),e(Rp,TZ),e(TZ,Efo),e(Rp,yfo),e(Rp,HS),e(HS,wfo),e(Rp,Afo),e(k,Lfo),e(k,Sp),e(Sp,FZ),e(FZ,Bfo),e(Sp,xfo),e(Sp,US),e(US,kfo),e(Sp,Rfo),e(k,Sfo),e(k,Pp),e(Pp,CZ),e(CZ,Pfo),e(Pp,$fo),e(Pp,JS),e(JS,Ifo),e(Pp,Dfo),e(k,jfo),e(k,$p),e($p,MZ),e(MZ,Nfo),e($p,qfo),e($p,YS),e(YS,Gfo),e($p,Ofo),e(k,Xfo),e(k,Ip),e(Ip,EZ),e(EZ,Vfo),e(Ip,zfo),e(Ip,KS),e(KS,Wfo),e(Ip,Qfo),e(k,Hfo),e(k,Dp),e(Dp,yZ),e(yZ,Ufo),e(Dp,Jfo),e(Dp,ZS),e(ZS,Yfo),e(Dp,Kfo),e(k,Zfo),e(k,jp),e(jp,wZ),e(wZ,ego),e(jp,ogo),e(jp,eP),e(eP,rgo),e(jp,tgo),e(k,ago),e(k,Np),e(Np,AZ),e(AZ,sgo),e(Np,ngo),e(Np,oP),e(oP,lgo),e(Np,igo),e(k,dgo),e(k,qp),e(qp,LZ),e(LZ,cgo),e(qp,mgo),e(qp,rP),e(rP,fgo),e(qp,ggo),e(k,hgo),e(k,Gp),e(Gp,BZ),e(BZ,ugo),e(Gp,pgo),e(Gp,tP),e(tP,_go),e(Gp,bgo),e(k,vgo),e(k,Op),e(Op,xZ),e(xZ,Tgo),e(Op,Fgo),e(Op,aP),e(aP,Cgo),e(Op,Mgo),e(k,Ego),e(k,Xp),e(Xp,kZ),e(kZ,ygo),e(Xp,wgo),e(Xp,sP),e(sP,Ago),e(Xp,Lgo),e(k,Bgo),e(k,Vp),e(Vp,RZ),e(RZ,xgo),e(Vp,kgo),e(Vp,nP),e(nP,Rgo),e(Vp,Sgo),e(Se,Pgo),e(Se,zp),e(zp,$go),e(zp,SZ),e(SZ,Igo),e(zp,Dgo),e(zp,PZ),e(PZ,jgo),e(Se,Ngo),e(Se,$Z),e($Z,qgo),e(Se,Ggo),g(py,Se,null),b(c,Q9e,_),b(c,Ui,_),e(Ui,Wp),e(Wp,IZ),g(_y,IZ,null),e(Ui,Ogo),e(Ui,DZ),e(DZ,Xgo),b(c,H9e,_),b(c,Uo,_),g(by,Uo,null),e(Uo,Vgo),e(Uo,Ji),e(Ji,zgo),e(Ji,jZ),e(jZ,Wgo),e(Ji,Qgo),e(Ji,NZ),e(NZ,Hgo),e(Ji,Ugo),e(Uo,Jgo),e(Uo,vy),e(vy,Ygo),e(vy,qZ),e(qZ,Kgo),e(vy,Zgo),e(Uo,eho),e(Uo,Or),g(Ty,Or,null),e(Or,oho),e(Or,GZ),e(GZ,rho),e(Or,tho),e(Or,Yi),e(Yi,aho),e(Yi,OZ),e(OZ,sho),e(Yi,nho),e(Yi,XZ),e(XZ,lho),e(Yi,iho),e(Or,dho),e(Or,VZ),e(VZ,cho),e(Or,mho),g(Fy,Or,null),e(Uo,fho),e(Uo,Pe),g(Cy,Pe,null),e(Pe,gho),e(Pe,zZ),e(zZ,hho),e(Pe,uho),e(Pe,Ga),e(Ga,pho),e(Ga,WZ),e(WZ,_ho),e(Ga,bho),e(Ga,QZ),e(QZ,vho),e(Ga,Tho),e(Ga,HZ),e(HZ,Fho),e(Ga,Cho),e(Pe,Mho),e(Pe,$),e($,Qp),e(Qp,UZ),e(UZ,Eho),e(Qp,yho),e(Qp,lP),e(lP,who),e(Qp,Aho),e($,Lho),e($,Hp),e(Hp,JZ),e(JZ,Bho),e(Hp,xho),e(Hp,iP),e(iP,kho),e(Hp,Rho),e($,Sho),e($,Up),e(Up,YZ),e(YZ,Pho),e(Up,$ho),e(Up,dP),e(dP,Iho),e(Up,Dho),e($,jho),e($,Jp),e(Jp,KZ),e(KZ,Nho),e(Jp,qho),e(Jp,cP),e(cP,Gho),e(Jp,Oho),e($,Xho),e($,Yp),e(Yp,ZZ),e(ZZ,Vho),e(Yp,zho),e(Yp,mP),e(mP,Who),e(Yp,Qho),e($,Hho),e($,Kp),e(Kp,eee),e(eee,Uho),e(Kp,Jho),e(Kp,fP),e(fP,Yho),e(Kp,Kho),e($,Zho),e($,Zp),e(Zp,oee),e(oee,euo),e(Zp,ouo),e(Zp,gP),e(gP,ruo),e(Zp,tuo),e($,auo),e($,e_),e(e_,ree),e(ree,suo),e(e_,nuo),e(e_,hP),e(hP,luo),e(e_,iuo),e($,duo),e($,o_),e(o_,tee),e(tee,cuo),e(o_,muo),e(o_,uP),e(uP,fuo),e(o_,guo),e($,huo),e($,r_),e(r_,aee),e(aee,uuo),e(r_,puo),e(r_,pP),e(pP,_uo),e(r_,buo),e($,vuo),e($,t_),e(t_,see),e(see,Tuo),e(t_,Fuo),e(t_,_P),e(_P,Cuo),e(t_,Muo),e($,Euo),e($,a_),e(a_,nee),e(nee,yuo),e(a_,wuo),e(a_,bP),e(bP,Auo),e(a_,Luo),e($,Buo),e($,s_),e(s_,lee),e(lee,xuo),e(s_,kuo),e(s_,vP),e(vP,Ruo),e(s_,Suo),e($,Puo),e($,n_),e(n_,iee),e(iee,$uo),e(n_,Iuo),e(n_,TP),e(TP,Duo),e(n_,juo),e($,Nuo),e($,l_),e(l_,dee),e(dee,quo),e(l_,Guo),e(l_,FP),e(FP,Ouo),e(l_,Xuo),e($,Vuo),e($,i_),e(i_,cee),e(cee,zuo),e(i_,Wuo),e(i_,CP),e(CP,Quo),e(i_,Huo),e($,Uuo),e($,d_),e(d_,mee),e(mee,Juo),e(d_,Yuo),e(d_,MP),e(MP,Kuo),e(d_,Zuo),e($,epo),e($,c_),e(c_,fee),e(fee,opo),e(c_,rpo),e(c_,EP),e(EP,tpo),e(c_,apo),e($,spo),e($,m_),e(m_,gee),e(gee,npo),e(m_,lpo),e(m_,yP),e(yP,ipo),e(m_,dpo),e($,cpo),e($,f_),e(f_,hee),e(hee,mpo),e(f_,fpo),e(f_,wP),e(wP,gpo),e(f_,hpo),e($,upo),e($,g_),e(g_,uee),e(uee,ppo),e(g_,_po),e(g_,AP),e(AP,bpo),e(g_,vpo),e($,Tpo),e($,h_),e(h_,pee),e(pee,Fpo),e(h_,Cpo),e(h_,LP),e(LP,Mpo),e(h_,Epo),e($,ypo),e($,u_),e(u_,_ee),e(_ee,wpo),e(u_,Apo),e(u_,BP),e(BP,Lpo),e(u_,Bpo),e($,xpo),e($,p_),e(p_,bee),e(bee,kpo),e(p_,Rpo),e(p_,xP),e(xP,Spo),e(p_,Ppo),e($,$po),e($,__),e(__,vee),e(vee,Ipo),e(__,Dpo),e(__,kP),e(kP,jpo),e(__,Npo),e($,qpo),e($,b_),e(b_,Tee),e(Tee,Gpo),e(b_,Opo),e(b_,RP),e(RP,Xpo),e(b_,Vpo),e($,zpo),e($,v_),e(v_,Fee),e(Fee,Wpo),e(v_,Qpo),e(v_,SP),e(SP,Hpo),e(v_,Upo),e($,Jpo),e($,T_),e(T_,Cee),e(Cee,Ypo),e(T_,Kpo),e(T_,PP),e(PP,Zpo),e(T_,e_o),e($,o_o),e($,F_),e(F_,Mee),e(Mee,r_o),e(F_,t_o),e(F_,$P),e($P,a_o),e(F_,s_o),e($,n_o),e($,C_),e(C_,Eee),e(Eee,l_o),e(C_,i_o),e(C_,IP),e(IP,d_o),e(C_,c_o),e($,m_o),e($,M_),e(M_,yee),e(yee,f_o),e(M_,g_o),e(M_,DP),e(DP,h_o),e(M_,u_o),e($,p_o),e($,E_),e(E_,wee),e(wee,__o),e(E_,b_o),e(E_,jP),e(jP,v_o),e(E_,T_o),e($,F_o),e($,y_),e(y_,Aee),e(Aee,C_o),e(y_,M_o),e(y_,NP),e(NP,E_o),e(y_,y_o),e($,w_o),e($,w_),e(w_,Lee),e(Lee,A_o),e(w_,L_o),e(w_,qP),e(qP,B_o),e(w_,x_o),e($,k_o),e($,A_),e(A_,Bee),e(Bee,R_o),e(A_,S_o),e(A_,GP),e(GP,P_o),e(A_,$_o),e(Pe,I_o),e(Pe,L_),e(L_,D_o),e(L_,xee),e(xee,j_o),e(L_,N_o),e(L_,kee),e(kee,q_o),e(Pe,G_o),e(Pe,Ree),e(Ree,O_o),e(Pe,X_o),g(My,Pe,null),b(c,U9e,_),b(c,Ki,_),e(Ki,B_),e(B_,See),g(Ey,See,null),e(Ki,V_o),e(Ki,Pee),e(Pee,z_o),b(c,J9e,_),b(c,Jo,_),g(yy,Jo,null),e(Jo,W_o),e(Jo,Zi),e(Zi,Q_o),e(Zi,$ee),e($ee,H_o),e(Zi,U_o),e(Zi,Iee),e(Iee,J_o),e(Zi,Y_o),e(Jo,K_o),e(Jo,wy),e(wy,Z_o),e(wy,Dee),e(Dee,ebo),e(wy,obo),e(Jo,rbo),e(Jo,Xr),g(Ay,Xr,null),e(Xr,tbo),e(Xr,jee),e(jee,abo),e(Xr,sbo),e(Xr,ed),e(ed,nbo),e(ed,Nee),e(Nee,lbo),e(ed,ibo),e(ed,qee),e(qee,dbo),e(ed,cbo),e(Xr,mbo),e(Xr,Gee),e(Gee,fbo),e(Xr,gbo),g(Ly,Xr,null),e(Jo,hbo),e(Jo,$e),g(By,$e,null),e($e,ubo),e($e,Oee),e(Oee,pbo),e($e,_bo),e($e,Oa),e(Oa,bbo),e(Oa,Xee),e(Xee,vbo),e(Oa,Tbo),e(Oa,Vee),e(Vee,Fbo),e(Oa,Cbo),e(Oa,zee),e(zee,Mbo),e(Oa,Ebo),e($e,ybo),e($e,I),e(I,x_),e(x_,Wee),e(Wee,wbo),e(x_,Abo),e(x_,OP),e(OP,Lbo),e(x_,Bbo),e(I,xbo),e(I,k_),e(k_,Qee),e(Qee,kbo),e(k_,Rbo),e(k_,XP),e(XP,Sbo),e(k_,Pbo),e(I,$bo),e(I,R_),e(R_,Hee),e(Hee,Ibo),e(R_,Dbo),e(R_,VP),e(VP,jbo),e(R_,Nbo),e(I,qbo),e(I,S_),e(S_,Uee),e(Uee,Gbo),e(S_,Obo),e(S_,zP),e(zP,Xbo),e(S_,Vbo),e(I,zbo),e(I,P_),e(P_,Jee),e(Jee,Wbo),e(P_,Qbo),e(P_,WP),e(WP,Hbo),e(P_,Ubo),e(I,Jbo),e(I,$_),e($_,Yee),e(Yee,Ybo),e($_,Kbo),e($_,QP),e(QP,Zbo),e($_,e2o),e(I,o2o),e(I,I_),e(I_,Kee),e(Kee,r2o),e(I_,t2o),e(I_,HP),e(HP,a2o),e(I_,s2o),e(I,n2o),e(I,D_),e(D_,Zee),e(Zee,l2o),e(D_,i2o),e(D_,UP),e(UP,d2o),e(D_,c2o),e(I,m2o),e(I,j_),e(j_,eoe),e(eoe,f2o),e(j_,g2o),e(j_,JP),e(JP,h2o),e(j_,u2o),e(I,p2o),e(I,N_),e(N_,ooe),e(ooe,_2o),e(N_,b2o),e(N_,YP),e(YP,v2o),e(N_,T2o),e(I,F2o),e(I,q_),e(q_,roe),e(roe,C2o),e(q_,M2o),e(q_,KP),e(KP,E2o),e(q_,y2o),e(I,w2o),e(I,G_),e(G_,toe),e(toe,A2o),e(G_,L2o),e(G_,ZP),e(ZP,B2o),e(G_,x2o),e(I,k2o),e(I,O_),e(O_,aoe),e(aoe,R2o),e(O_,S2o),e(O_,e$),e(e$,P2o),e(O_,$2o),e(I,I2o),e(I,X_),e(X_,soe),e(soe,D2o),e(X_,j2o),e(X_,o$),e(o$,N2o),e(X_,q2o),e(I,G2o),e(I,V_),e(V_,noe),e(noe,O2o),e(V_,X2o),e(V_,r$),e(r$,V2o),e(V_,z2o),e(I,W2o),e(I,z_),e(z_,loe),e(loe,Q2o),e(z_,H2o),e(z_,t$),e(t$,U2o),e(z_,J2o),e(I,Y2o),e(I,W_),e(W_,ioe),e(ioe,K2o),e(W_,Z2o),e(W_,a$),e(a$,evo),e(W_,ovo),e(I,rvo),e(I,Q_),e(Q_,doe),e(doe,tvo),e(Q_,avo),e(Q_,s$),e(s$,svo),e(Q_,nvo),e(I,lvo),e(I,H_),e(H_,coe),e(coe,ivo),e(H_,dvo),e(H_,n$),e(n$,cvo),e(H_,mvo),e(I,fvo),e(I,U_),e(U_,moe),e(moe,gvo),e(U_,hvo),e(U_,l$),e(l$,uvo),e(U_,pvo),e(I,_vo),e(I,J_),e(J_,foe),e(foe,bvo),e(J_,vvo),e(J_,i$),e(i$,Tvo),e(J_,Fvo),e(I,Cvo),e(I,Y_),e(Y_,goe),e(goe,Mvo),e(Y_,Evo),e(Y_,d$),e(d$,yvo),e(Y_,wvo),e(I,Avo),e(I,K_),e(K_,hoe),e(hoe,Lvo),e(K_,Bvo),e(K_,c$),e(c$,xvo),e(K_,kvo),e(I,Rvo),e(I,Z_),e(Z_,uoe),e(uoe,Svo),e(Z_,Pvo),e(Z_,m$),e(m$,$vo),e(Z_,Ivo),e(I,Dvo),e(I,eb),e(eb,poe),e(poe,jvo),e(eb,Nvo),e(eb,f$),e(f$,qvo),e(eb,Gvo),e(I,Ovo),e(I,ob),e(ob,_oe),e(_oe,Xvo),e(ob,Vvo),e(ob,g$),e(g$,zvo),e(ob,Wvo),e(I,Qvo),e(I,rb),e(rb,boe),e(boe,Hvo),e(rb,Uvo),e(rb,h$),e(h$,Jvo),e(rb,Yvo),e(I,Kvo),e(I,tb),e(tb,voe),e(voe,Zvo),e(tb,eTo),e(tb,u$),e(u$,oTo),e(tb,rTo),e(I,tTo),e(I,ab),e(ab,Toe),e(Toe,aTo),e(ab,sTo),e(ab,p$),e(p$,nTo),e(ab,lTo),e(I,iTo),e(I,sb),e(sb,Foe),e(Foe,dTo),e(sb,cTo),e(sb,_$),e(_$,mTo),e(sb,fTo),e(I,gTo),e(I,nb),e(nb,Coe),e(Coe,hTo),e(nb,uTo),e(nb,Moe),e(Moe,pTo),e(nb,_To),e(I,bTo),e(I,lb),e(lb,Eoe),e(Eoe,vTo),e(lb,TTo),e(lb,b$),e(b$,FTo),e(lb,CTo),e(I,MTo),e(I,ib),e(ib,yoe),e(yoe,ETo),e(ib,yTo),e(ib,v$),e(v$,wTo),e(ib,ATo),e(I,LTo),e(I,db),e(db,woe),e(woe,BTo),e(db,xTo),e(db,T$),e(T$,kTo),e(db,RTo),e(I,STo),e(I,cb),e(cb,Aoe),e(Aoe,PTo),e(cb,$To),e(cb,F$),e(F$,ITo),e(cb,DTo),e($e,jTo),e($e,mb),e(mb,NTo),e(mb,Loe),e(Loe,qTo),e(mb,GTo),e(mb,Boe),e(Boe,OTo),e($e,XTo),e($e,xoe),e(xoe,VTo),e($e,zTo),g(xy,$e,null),b(c,Y9e,_),b(c,od,_),e(od,fb),e(fb,koe),g(ky,koe,null),e(od,WTo),e(od,Roe),e(Roe,QTo),b(c,K9e,_),b(c,Yo,_),g(Ry,Yo,null),e(Yo,HTo),e(Yo,rd),e(rd,UTo),e(rd,Soe),e(Soe,JTo),e(rd,YTo),e(rd,Poe),e(Poe,KTo),e(rd,ZTo),e(Yo,e1o),e(Yo,Sy),e(Sy,o1o),e(Sy,$oe),e($oe,r1o),e(Sy,t1o),e(Yo,a1o),e(Yo,Vr),g(Py,Vr,null),e(Vr,s1o),e(Vr,Ioe),e(Ioe,n1o),e(Vr,l1o),e(Vr,td),e(td,i1o),e(td,Doe),e(Doe,d1o),e(td,c1o),e(td,joe),e(joe,m1o),e(td,f1o),e(Vr,g1o),e(Vr,Noe),e(Noe,h1o),e(Vr,u1o),g($y,Vr,null),e(Yo,p1o),e(Yo,Ie),g(Iy,Ie,null),e(Ie,_1o),e(Ie,qoe),e(qoe,b1o),e(Ie,v1o),e(Ie,Xa),e(Xa,T1o),e(Xa,Goe),e(Goe,F1o),e(Xa,C1o),e(Xa,Ooe),e(Ooe,M1o),e(Xa,E1o),e(Xa,Xoe),e(Xoe,y1o),e(Xa,w1o),e(Ie,A1o),e(Ie,ae),e(ae,gb),e(gb,Voe),e(Voe,L1o),e(gb,B1o),e(gb,C$),e(C$,x1o),e(gb,k1o),e(ae,R1o),e(ae,hb),e(hb,zoe),e(zoe,S1o),e(hb,P1o),e(hb,M$),e(M$,$1o),e(hb,I1o),e(ae,D1o),e(ae,ub),e(ub,Woe),e(Woe,j1o),e(ub,N1o),e(ub,E$),e(E$,q1o),e(ub,G1o),e(ae,O1o),e(ae,pb),e(pb,Qoe),e(Qoe,X1o),e(pb,V1o),e(pb,y$),e(y$,z1o),e(pb,W1o),e(ae,Q1o),e(ae,_b),e(_b,Hoe),e(Hoe,H1o),e(_b,U1o),e(_b,w$),e(w$,J1o),e(_b,Y1o),e(ae,K1o),e(ae,bb),e(bb,Uoe),e(Uoe,Z1o),e(bb,eFo),e(bb,A$),e(A$,oFo),e(bb,rFo),e(ae,tFo),e(ae,vb),e(vb,Joe),e(Joe,aFo),e(vb,sFo),e(vb,L$),e(L$,nFo),e(vb,lFo),e(ae,iFo),e(ae,Tb),e(Tb,Yoe),e(Yoe,dFo),e(Tb,cFo),e(Tb,B$),e(B$,mFo),e(Tb,fFo),e(ae,gFo),e(ae,Fb),e(Fb,Koe),e(Koe,hFo),e(Fb,uFo),e(Fb,x$),e(x$,pFo),e(Fb,_Fo),e(ae,bFo),e(ae,Cb),e(Cb,Zoe),e(Zoe,vFo),e(Cb,TFo),e(Cb,k$),e(k$,FFo),e(Cb,CFo),e(ae,MFo),e(ae,Mb),e(Mb,ere),e(ere,EFo),e(Mb,yFo),e(Mb,R$),e(R$,wFo),e(Mb,AFo),e(ae,LFo),e(ae,Eb),e(Eb,ore),e(ore,BFo),e(Eb,xFo),e(Eb,S$),e(S$,kFo),e(Eb,RFo),e(ae,SFo),e(ae,yb),e(yb,rre),e(rre,PFo),e(yb,$Fo),e(yb,P$),e(P$,IFo),e(yb,DFo),e(ae,jFo),e(ae,wb),e(wb,tre),e(tre,NFo),e(wb,qFo),e(wb,$$),e($$,GFo),e(wb,OFo),e(ae,XFo),e(ae,Ab),e(Ab,are),e(are,VFo),e(Ab,zFo),e(Ab,I$),e(I$,WFo),e(Ab,QFo),e(ae,HFo),e(ae,Lb),e(Lb,sre),e(sre,UFo),e(Lb,JFo),e(Lb,D$),e(D$,YFo),e(Lb,KFo),e(Ie,ZFo),e(Ie,Bb),e(Bb,eCo),e(Bb,nre),e(nre,oCo),e(Bb,rCo),e(Bb,lre),e(lre,tCo),e(Ie,aCo),e(Ie,ire),e(ire,sCo),e(Ie,nCo),g(Dy,Ie,null),b(c,Z9e,_),b(c,ad,_),e(ad,xb),e(xb,dre),g(jy,dre,null),e(ad,lCo),e(ad,cre),e(cre,iCo),b(c,eBe,_),b(c,Ko,_),g(Ny,Ko,null),e(Ko,dCo),e(Ko,sd),e(sd,cCo),e(sd,mre),e(mre,mCo),e(sd,fCo),e(sd,fre),e(fre,gCo),e(sd,hCo),e(Ko,uCo),e(Ko,qy),e(qy,pCo),e(qy,gre),e(gre,_Co),e(qy,bCo),e(Ko,vCo),e(Ko,zr),g(Gy,zr,null),e(zr,TCo),e(zr,hre),e(hre,FCo),e(zr,CCo),e(zr,nd),e(nd,MCo),e(nd,ure),e(ure,ECo),e(nd,yCo),e(nd,pre),e(pre,wCo),e(nd,ACo),e(zr,LCo),e(zr,_re),e(_re,BCo),e(zr,xCo),g(Oy,zr,null),e(Ko,kCo),e(Ko,De),g(Xy,De,null),e(De,RCo),e(De,bre),e(bre,SCo),e(De,PCo),e(De,Va),e(Va,$Co),e(Va,vre),e(vre,ICo),e(Va,DCo),e(Va,Tre),e(Tre,jCo),e(Va,NCo),e(Va,Fre),e(Fre,qCo),e(Va,GCo),e(De,OCo),e(De,A),e(A,kb),e(kb,Cre),e(Cre,XCo),e(kb,VCo),e(kb,j$),e(j$,zCo),e(kb,WCo),e(A,QCo),e(A,Rb),e(Rb,Mre),e(Mre,HCo),e(Rb,UCo),e(Rb,N$),e(N$,JCo),e(Rb,YCo),e(A,KCo),e(A,Sb),e(Sb,Ere),e(Ere,ZCo),e(Sb,e4o),e(Sb,q$),e(q$,o4o),e(Sb,r4o),e(A,t4o),e(A,Pb),e(Pb,yre),e(yre,a4o),e(Pb,s4o),e(Pb,G$),e(G$,n4o),e(Pb,l4o),e(A,i4o),e(A,$b),e($b,wre),e(wre,d4o),e($b,c4o),e($b,O$),e(O$,m4o),e($b,f4o),e(A,g4o),e(A,Ib),e(Ib,Are),e(Are,h4o),e(Ib,u4o),e(Ib,X$),e(X$,p4o),e(Ib,_4o),e(A,b4o),e(A,Db),e(Db,Lre),e(Lre,v4o),e(Db,T4o),e(Db,V$),e(V$,F4o),e(Db,C4o),e(A,M4o),e(A,jb),e(jb,Bre),e(Bre,E4o),e(jb,y4o),e(jb,z$),e(z$,w4o),e(jb,A4o),e(A,L4o),e(A,Nb),e(Nb,xre),e(xre,B4o),e(Nb,x4o),e(Nb,W$),e(W$,k4o),e(Nb,R4o),e(A,S4o),e(A,qb),e(qb,kre),e(kre,P4o),e(qb,$4o),e(qb,Q$),e(Q$,I4o),e(qb,D4o),e(A,j4o),e(A,Gb),e(Gb,Rre),e(Rre,N4o),e(Gb,q4o),e(Gb,H$),e(H$,G4o),e(Gb,O4o),e(A,X4o),e(A,Ob),e(Ob,Sre),e(Sre,V4o),e(Ob,z4o),e(Ob,U$),e(U$,W4o),e(Ob,Q4o),e(A,H4o),e(A,Xb),e(Xb,Pre),e(Pre,U4o),e(Xb,J4o),e(Xb,J$),e(J$,Y4o),e(Xb,K4o),e(A,Z4o),e(A,Vb),e(Vb,$re),e($re,eMo),e(Vb,oMo),e(Vb,Y$),e(Y$,rMo),e(Vb,tMo),e(A,aMo),e(A,zb),e(zb,Ire),e(Ire,sMo),e(zb,nMo),e(zb,K$),e(K$,lMo),e(zb,iMo),e(A,dMo),e(A,Wb),e(Wb,Dre),e(Dre,cMo),e(Wb,mMo),e(Wb,Z$),e(Z$,fMo),e(Wb,gMo),e(A,hMo),e(A,Qb),e(Qb,jre),e(jre,uMo),e(Qb,pMo),e(Qb,eI),e(eI,_Mo),e(Qb,bMo),e(A,vMo),e(A,Hb),e(Hb,Nre),e(Nre,TMo),e(Hb,FMo),e(Hb,oI),e(oI,CMo),e(Hb,MMo),e(A,EMo),e(A,Ub),e(Ub,qre),e(qre,yMo),e(Ub,wMo),e(Ub,rI),e(rI,AMo),e(Ub,LMo),e(A,BMo),e(A,Jb),e(Jb,Gre),e(Gre,xMo),e(Jb,kMo),e(Jb,tI),e(tI,RMo),e(Jb,SMo),e(A,PMo),e(A,Yb),e(Yb,Ore),e(Ore,$Mo),e(Yb,IMo),e(Yb,aI),e(aI,DMo),e(Yb,jMo),e(A,NMo),e(A,Kb),e(Kb,Xre),e(Xre,qMo),e(Kb,GMo),e(Kb,sI),e(sI,OMo),e(Kb,XMo),e(A,VMo),e(A,Zb),e(Zb,Vre),e(Vre,zMo),e(Zb,WMo),e(Zb,nI),e(nI,QMo),e(Zb,HMo),e(A,UMo),e(A,e2),e(e2,zre),e(zre,JMo),e(e2,YMo),e(e2,lI),e(lI,KMo),e(e2,ZMo),e(A,eEo),e(A,o2),e(o2,Wre),e(Wre,oEo),e(o2,rEo),e(o2,iI),e(iI,tEo),e(o2,aEo),e(A,sEo),e(A,r2),e(r2,Qre),e(Qre,nEo),e(r2,lEo),e(r2,dI),e(dI,iEo),e(r2,dEo),e(A,cEo),e(A,t2),e(t2,Hre),e(Hre,mEo),e(t2,fEo),e(t2,cI),e(cI,gEo),e(t2,hEo),e(A,uEo),e(A,a2),e(a2,Ure),e(Ure,pEo),e(a2,_Eo),e(a2,mI),e(mI,bEo),e(a2,vEo),e(A,TEo),e(A,s2),e(s2,Jre),e(Jre,FEo),e(s2,CEo),e(s2,fI),e(fI,MEo),e(s2,EEo),e(A,yEo),e(A,n2),e(n2,Yre),e(Yre,wEo),e(n2,AEo),e(n2,gI),e(gI,LEo),e(n2,BEo),e(A,xEo),e(A,l2),e(l2,Kre),e(Kre,kEo),e(l2,REo),e(l2,hI),e(hI,SEo),e(l2,PEo),e(A,$Eo),e(A,i2),e(i2,Zre),e(Zre,IEo),e(i2,DEo),e(i2,uI),e(uI,jEo),e(i2,NEo),e(A,qEo),e(A,d2),e(d2,ete),e(ete,GEo),e(d2,OEo),e(d2,pI),e(pI,XEo),e(d2,VEo),e(A,zEo),e(A,c2),e(c2,ote),e(ote,WEo),e(c2,QEo),e(c2,_I),e(_I,HEo),e(c2,UEo),e(A,JEo),e(A,m2),e(m2,rte),e(rte,YEo),e(m2,KEo),e(m2,bI),e(bI,ZEo),e(m2,e3o),e(A,o3o),e(A,f2),e(f2,tte),e(tte,r3o),e(f2,t3o),e(f2,vI),e(vI,a3o),e(f2,s3o),e(A,n3o),e(A,g2),e(g2,ate),e(ate,l3o),e(g2,i3o),e(g2,TI),e(TI,d3o),e(g2,c3o),e(A,m3o),e(A,h2),e(h2,ste),e(ste,f3o),e(h2,g3o),e(h2,FI),e(FI,h3o),e(h2,u3o),e(A,p3o),e(A,u2),e(u2,nte),e(nte,_3o),e(u2,b3o),e(u2,CI),e(CI,v3o),e(u2,T3o),e(A,F3o),e(A,p2),e(p2,lte),e(lte,C3o),e(p2,M3o),e(p2,MI),e(MI,E3o),e(p2,y3o),e(A,w3o),e(A,_2),e(_2,ite),e(ite,A3o),e(_2,L3o),e(_2,EI),e(EI,B3o),e(_2,x3o),e(A,k3o),e(A,b2),e(b2,dte),e(dte,R3o),e(b2,S3o),e(b2,yI),e(yI,P3o),e(b2,$3o),e(A,I3o),e(A,v2),e(v2,cte),e(cte,D3o),e(v2,j3o),e(v2,wI),e(wI,N3o),e(v2,q3o),e(A,G3o),e(A,T2),e(T2,mte),e(mte,O3o),e(T2,X3o),e(T2,AI),e(AI,V3o),e(T2,z3o),e(A,W3o),e(A,F2),e(F2,fte),e(fte,Q3o),e(F2,H3o),e(F2,LI),e(LI,U3o),e(F2,J3o),e(A,Y3o),e(A,C2),e(C2,gte),e(gte,K3o),e(C2,Z3o),e(C2,BI),e(BI,e5o),e(C2,o5o),e(De,r5o),e(De,M2),e(M2,t5o),e(M2,hte),e(hte,a5o),e(M2,s5o),e(M2,ute),e(ute,n5o),e(De,l5o),e(De,pte),e(pte,i5o),e(De,d5o),g(Vy,De,null),b(c,oBe,_),b(c,ld,_),e(ld,E2),e(E2,_te),g(zy,_te,null),e(ld,c5o),e(ld,bte),e(bte,m5o),b(c,rBe,_),b(c,Zo,_),g(Wy,Zo,null),e(Zo,f5o),e(Zo,id),e(id,g5o),e(id,vte),e(vte,h5o),e(id,u5o),e(id,Tte),e(Tte,p5o),e(id,_5o),e(Zo,b5o),e(Zo,Qy),e(Qy,v5o),e(Qy,Fte),e(Fte,T5o),e(Qy,F5o),e(Zo,C5o),e(Zo,Wr),g(Hy,Wr,null),e(Wr,M5o),e(Wr,Cte),e(Cte,E5o),e(Wr,y5o),e(Wr,dd),e(dd,w5o),e(dd,Mte),e(Mte,A5o),e(dd,L5o),e(dd,Ete),e(Ete,B5o),e(dd,x5o),e(Wr,k5o),e(Wr,yte),e(yte,R5o),e(Wr,S5o),g(Uy,Wr,null),e(Zo,P5o),e(Zo,je),g(Jy,je,null),e(je,$5o),e(je,wte),e(wte,I5o),e(je,D5o),e(je,za),e(za,j5o),e(za,Ate),e(Ate,N5o),e(za,q5o),e(za,Lte),e(Lte,G5o),e(za,O5o),e(za,Bte),e(Bte,X5o),e(za,V5o),e(je,z5o),e(je,G),e(G,y2),e(y2,xte),e(xte,W5o),e(y2,Q5o),e(y2,xI),e(xI,H5o),e(y2,U5o),e(G,J5o),e(G,w2),e(w2,kte),e(kte,Y5o),e(w2,K5o),e(w2,kI),e(kI,Z5o),e(w2,eyo),e(G,oyo),e(G,A2),e(A2,Rte),e(Rte,ryo),e(A2,tyo),e(A2,RI),e(RI,ayo),e(A2,syo),e(G,nyo),e(G,L2),e(L2,Ste),e(Ste,lyo),e(L2,iyo),e(L2,SI),e(SI,dyo),e(L2,cyo),e(G,myo),e(G,B2),e(B2,Pte),e(Pte,fyo),e(B2,gyo),e(B2,PI),e(PI,hyo),e(B2,uyo),e(G,pyo),e(G,x2),e(x2,$te),e($te,_yo),e(x2,byo),e(x2,$I),e($I,vyo),e(x2,Tyo),e(G,Fyo),e(G,k2),e(k2,Ite),e(Ite,Cyo),e(k2,Myo),e(k2,II),e(II,Eyo),e(k2,yyo),e(G,wyo),e(G,R2),e(R2,Dte),e(Dte,Ayo),e(R2,Lyo),e(R2,DI),e(DI,Byo),e(R2,xyo),e(G,kyo),e(G,S2),e(S2,jte),e(jte,Ryo),e(S2,Syo),e(S2,jI),e(jI,Pyo),e(S2,$yo),e(G,Iyo),e(G,P2),e(P2,Nte),e(Nte,Dyo),e(P2,jyo),e(P2,NI),e(NI,Nyo),e(P2,qyo),e(G,Gyo),e(G,$2),e($2,qte),e(qte,Oyo),e($2,Xyo),e($2,qI),e(qI,Vyo),e($2,zyo),e(G,Wyo),e(G,I2),e(I2,Gte),e(Gte,Qyo),e(I2,Hyo),e(I2,GI),e(GI,Uyo),e(I2,Jyo),e(G,Yyo),e(G,D2),e(D2,Ote),e(Ote,Kyo),e(D2,Zyo),e(D2,OI),e(OI,ewo),e(D2,owo),e(G,rwo),e(G,j2),e(j2,Xte),e(Xte,two),e(j2,awo),e(j2,XI),e(XI,swo),e(j2,nwo),e(G,lwo),e(G,N2),e(N2,Vte),e(Vte,iwo),e(N2,dwo),e(N2,VI),e(VI,cwo),e(N2,mwo),e(G,fwo),e(G,q2),e(q2,zte),e(zte,gwo),e(q2,hwo),e(q2,zI),e(zI,uwo),e(q2,pwo),e(G,_wo),e(G,G2),e(G2,Wte),e(Wte,bwo),e(G2,vwo),e(G2,WI),e(WI,Two),e(G2,Fwo),e(G,Cwo),e(G,O2),e(O2,Qte),e(Qte,Mwo),e(O2,Ewo),e(O2,QI),e(QI,ywo),e(O2,wwo),e(G,Awo),e(G,X2),e(X2,Hte),e(Hte,Lwo),e(X2,Bwo),e(X2,HI),e(HI,xwo),e(X2,kwo),e(G,Rwo),e(G,V2),e(V2,Ute),e(Ute,Swo),e(V2,Pwo),e(V2,UI),e(UI,$wo),e(V2,Iwo),e(G,Dwo),e(G,z2),e(z2,Jte),e(Jte,jwo),e(z2,Nwo),e(z2,JI),e(JI,qwo),e(z2,Gwo),e(G,Owo),e(G,W2),e(W2,Yte),e(Yte,Xwo),e(W2,Vwo),e(W2,YI),e(YI,zwo),e(W2,Wwo),e(G,Qwo),e(G,Q2),e(Q2,Kte),e(Kte,Hwo),e(Q2,Uwo),e(Q2,KI),e(KI,Jwo),e(Q2,Ywo),e(G,Kwo),e(G,H2),e(H2,Zte),e(Zte,Zwo),e(H2,e6o),e(H2,ZI),e(ZI,o6o),e(H2,r6o),e(G,t6o),e(G,U2),e(U2,eae),e(eae,a6o),e(U2,s6o),e(U2,eD),e(eD,n6o),e(U2,l6o),e(G,i6o),e(G,J2),e(J2,oae),e(oae,d6o),e(J2,c6o),e(J2,oD),e(oD,m6o),e(J2,f6o),e(G,g6o),e(G,Y2),e(Y2,rae),e(rae,h6o),e(Y2,u6o),e(Y2,rD),e(rD,p6o),e(Y2,_6o),e(G,b6o),e(G,K2),e(K2,tae),e(tae,v6o),e(K2,T6o),e(K2,tD),e(tD,F6o),e(K2,C6o),e(je,M6o),e(je,Z2),e(Z2,E6o),e(Z2,aae),e(aae,y6o),e(Z2,w6o),e(Z2,sae),e(sae,A6o),e(je,L6o),e(je,nae),e(nae,B6o),e(je,x6o),g(Yy,je,null),b(c,tBe,_),b(c,cd,_),e(cd,ev),e(ev,lae),g(Ky,lae,null),e(cd,k6o),e(cd,iae),e(iae,R6o),b(c,aBe,_),b(c,er,_),g(Zy,er,null),e(er,S6o),e(er,md),e(md,P6o),e(md,dae),e(dae,$6o),e(md,I6o),e(md,cae),e(cae,D6o),e(md,j6o),e(er,N6o),e(er,ew),e(ew,q6o),e(ew,mae),e(mae,G6o),e(ew,O6o),e(er,X6o),e(er,Qr),g(ow,Qr,null),e(Qr,V6o),e(Qr,fae),e(fae,z6o),e(Qr,W6o),e(Qr,fd),e(fd,Q6o),e(fd,gae),e(gae,H6o),e(fd,U6o),e(fd,hae),e(hae,J6o),e(fd,Y6o),e(Qr,K6o),e(Qr,uae),e(uae,Z6o),e(Qr,eAo),g(rw,Qr,null),e(er,oAo),e(er,Ne),g(tw,Ne,null),e(Ne,rAo),e(Ne,pae),e(pae,tAo),e(Ne,aAo),e(Ne,Wa),e(Wa,sAo),e(Wa,_ae),e(_ae,nAo),e(Wa,lAo),e(Wa,bae),e(bae,iAo),e(Wa,dAo),e(Wa,vae),e(vae,cAo),e(Wa,mAo),e(Ne,fAo),e(Ne,sa),e(sa,ov),e(ov,Tae),e(Tae,gAo),e(ov,hAo),e(ov,aD),e(aD,uAo),e(ov,pAo),e(sa,_Ao),e(sa,rv),e(rv,Fae),e(Fae,bAo),e(rv,vAo),e(rv,sD),e(sD,TAo),e(rv,FAo),e(sa,CAo),e(sa,tv),e(tv,Cae),e(Cae,MAo),e(tv,EAo),e(tv,nD),e(nD,yAo),e(tv,wAo),e(sa,AAo),e(sa,av),e(av,Mae),e(Mae,LAo),e(av,BAo),e(av,lD),e(lD,xAo),e(av,kAo),e(sa,RAo),e(sa,sv),e(sv,Eae),e(Eae,SAo),e(sv,PAo),e(sv,iD),e(iD,$Ao),e(sv,IAo),e(Ne,DAo),e(Ne,nv),e(nv,jAo),e(nv,yae),e(yae,NAo),e(nv,qAo),e(nv,wae),e(wae,GAo),e(Ne,OAo),e(Ne,Aae),e(Aae,XAo),e(Ne,VAo),g(aw,Ne,null),b(c,sBe,_),b(c,gd,_),e(gd,lv),e(lv,Lae),g(sw,Lae,null),e(gd,zAo),e(gd,Bae),e(Bae,WAo),b(c,nBe,_),b(c,or,_),g(nw,or,null),e(or,QAo),e(or,hd),e(hd,HAo),e(hd,xae),e(xae,UAo),e(hd,JAo),e(hd,kae),e(kae,YAo),e(hd,KAo),e(or,ZAo),e(or,lw),e(lw,e0o),e(lw,Rae),e(Rae,o0o),e(lw,r0o),e(or,t0o),e(or,Hr),g(iw,Hr,null),e(Hr,a0o),e(Hr,Sae),e(Sae,s0o),e(Hr,n0o),e(Hr,ud),e(ud,l0o),e(ud,Pae),e(Pae,i0o),e(ud,d0o),e(ud,$ae),e($ae,c0o),e(ud,m0o),e(Hr,f0o),e(Hr,Iae),e(Iae,g0o),e(Hr,h0o),g(dw,Hr,null),e(or,u0o),e(or,qe),g(cw,qe,null),e(qe,p0o),e(qe,Dae),e(Dae,_0o),e(qe,b0o),e(qe,Qa),e(Qa,v0o),e(Qa,jae),e(jae,T0o),e(Qa,F0o),e(Qa,Nae),e(Nae,C0o),e(Qa,M0o),e(Qa,qae),e(qae,E0o),e(Qa,y0o),e(qe,w0o),e(qe,N),e(N,iv),e(iv,Gae),e(Gae,A0o),e(iv,L0o),e(iv,dD),e(dD,B0o),e(iv,x0o),e(N,k0o),e(N,dv),e(dv,Oae),e(Oae,R0o),e(dv,S0o),e(dv,cD),e(cD,P0o),e(dv,$0o),e(N,I0o),e(N,cv),e(cv,Xae),e(Xae,D0o),e(cv,j0o),e(cv,mD),e(mD,N0o),e(cv,q0o),e(N,G0o),e(N,mv),e(mv,Vae),e(Vae,O0o),e(mv,X0o),e(mv,fD),e(fD,V0o),e(mv,z0o),e(N,W0o),e(N,fv),e(fv,zae),e(zae,Q0o),e(fv,H0o),e(fv,gD),e(gD,U0o),e(fv,J0o),e(N,Y0o),e(N,gv),e(gv,Wae),e(Wae,K0o),e(gv,Z0o),e(gv,hD),e(hD,eLo),e(gv,oLo),e(N,rLo),e(N,hv),e(hv,Qae),e(Qae,tLo),e(hv,aLo),e(hv,uD),e(uD,sLo),e(hv,nLo),e(N,lLo),e(N,uv),e(uv,Hae),e(Hae,iLo),e(uv,dLo),e(uv,pD),e(pD,cLo),e(uv,mLo),e(N,fLo),e(N,pv),e(pv,Uae),e(Uae,gLo),e(pv,hLo),e(pv,_D),e(_D,uLo),e(pv,pLo),e(N,_Lo),e(N,_v),e(_v,Jae),e(Jae,bLo),e(_v,vLo),e(_v,bD),e(bD,TLo),e(_v,FLo),e(N,CLo),e(N,bv),e(bv,Yae),e(Yae,MLo),e(bv,ELo),e(bv,vD),e(vD,yLo),e(bv,wLo),e(N,ALo),e(N,vv),e(vv,Kae),e(Kae,LLo),e(vv,BLo),e(vv,TD),e(TD,xLo),e(vv,kLo),e(N,RLo),e(N,Tv),e(Tv,Zae),e(Zae,SLo),e(Tv,PLo),e(Tv,FD),e(FD,$Lo),e(Tv,ILo),e(N,DLo),e(N,Fv),e(Fv,ese),e(ese,jLo),e(Fv,NLo),e(Fv,CD),e(CD,qLo),e(Fv,GLo),e(N,OLo),e(N,Cv),e(Cv,ose),e(ose,XLo),e(Cv,VLo),e(Cv,MD),e(MD,zLo),e(Cv,WLo),e(N,QLo),e(N,Mv),e(Mv,rse),e(rse,HLo),e(Mv,ULo),e(Mv,ED),e(ED,JLo),e(Mv,YLo),e(N,KLo),e(N,Ev),e(Ev,tse),e(tse,ZLo),e(Ev,e8o),e(Ev,yD),e(yD,o8o),e(Ev,r8o),e(N,t8o),e(N,yv),e(yv,ase),e(ase,a8o),e(yv,s8o),e(yv,wD),e(wD,n8o),e(yv,l8o),e(N,i8o),e(N,wv),e(wv,sse),e(sse,d8o),e(wv,c8o),e(wv,AD),e(AD,m8o),e(wv,f8o),e(N,g8o),e(N,Av),e(Av,nse),e(nse,h8o),e(Av,u8o),e(Av,LD),e(LD,p8o),e(Av,_8o),e(N,b8o),e(N,Lv),e(Lv,lse),e(lse,v8o),e(Lv,T8o),e(Lv,BD),e(BD,F8o),e(Lv,C8o),e(N,M8o),e(N,Bv),e(Bv,ise),e(ise,E8o),e(Bv,y8o),e(Bv,xD),e(xD,w8o),e(Bv,A8o),e(N,L8o),e(N,xv),e(xv,dse),e(dse,B8o),e(xv,x8o),e(xv,kD),e(kD,k8o),e(xv,R8o),e(N,S8o),e(N,kv),e(kv,cse),e(cse,P8o),e(kv,$8o),e(kv,RD),e(RD,I8o),e(kv,D8o),e(N,j8o),e(N,Rv),e(Rv,mse),e(mse,N8o),e(Rv,q8o),e(Rv,SD),e(SD,G8o),e(Rv,O8o),e(N,X8o),e(N,Sv),e(Sv,fse),e(fse,V8o),e(Sv,z8o),e(Sv,PD),e(PD,W8o),e(Sv,Q8o),e(N,H8o),e(N,Pv),e(Pv,gse),e(gse,U8o),e(Pv,J8o),e(Pv,$D),e($D,Y8o),e(Pv,K8o),e(N,Z8o),e(N,$v),e($v,hse),e(hse,e7o),e($v,o7o),e($v,ID),e(ID,r7o),e($v,t7o),e(N,a7o),e(N,Iv),e(Iv,use),e(use,s7o),e(Iv,n7o),e(Iv,DD),e(DD,l7o),e(Iv,i7o),e(N,d7o),e(N,Dv),e(Dv,pse),e(pse,c7o),e(Dv,m7o),e(Dv,jD),e(jD,f7o),e(Dv,g7o),e(N,h7o),e(N,jv),e(jv,_se),e(_se,u7o),e(jv,p7o),e(jv,ND),e(ND,_7o),e(jv,b7o),e(N,v7o),e(N,Nv),e(Nv,bse),e(bse,T7o),e(Nv,F7o),e(Nv,qD),e(qD,C7o),e(Nv,M7o),e(N,E7o),e(N,qv),e(qv,vse),e(vse,y7o),e(qv,w7o),e(qv,GD),e(GD,A7o),e(qv,L7o),e(qe,B7o),e(qe,Gv),e(Gv,x7o),e(Gv,Tse),e(Tse,k7o),e(Gv,R7o),e(Gv,Fse),e(Fse,S7o),e(qe,P7o),e(qe,Cse),e(Cse,$7o),e(qe,I7o),g(mw,qe,null),b(c,lBe,_),b(c,pd,_),e(pd,Ov),e(Ov,Mse),g(fw,Mse,null),e(pd,D7o),e(pd,Ese),e(Ese,j7o),b(c,iBe,_),b(c,rr,_),g(gw,rr,null),e(rr,N7o),e(rr,_d),e(_d,q7o),e(_d,yse),e(yse,G7o),e(_d,O7o),e(_d,wse),e(wse,X7o),e(_d,V7o),e(rr,z7o),e(rr,hw),e(hw,W7o),e(hw,Ase),e(Ase,Q7o),e(hw,H7o),e(rr,U7o),e(rr,Ur),g(uw,Ur,null),e(Ur,J7o),e(Ur,Lse),e(Lse,Y7o),e(Ur,K7o),e(Ur,bd),e(bd,Z7o),e(bd,Bse),e(Bse,e9o),e(bd,o9o),e(bd,xse),e(xse,r9o),e(bd,t9o),e(Ur,a9o),e(Ur,kse),e(kse,s9o),e(Ur,n9o),g(pw,Ur,null),e(rr,l9o),e(rr,Ge),g(_w,Ge,null),e(Ge,i9o),e(Ge,Rse),e(Rse,d9o),e(Ge,c9o),e(Ge,Ha),e(Ha,m9o),e(Ha,Sse),e(Sse,f9o),e(Ha,g9o),e(Ha,Pse),e(Pse,h9o),e(Ha,u9o),e(Ha,$se),e($se,p9o),e(Ha,_9o),e(Ge,b9o),e(Ge,R),e(R,Xv),e(Xv,Ise),e(Ise,v9o),e(Xv,T9o),e(Xv,OD),e(OD,F9o),e(Xv,C9o),e(R,M9o),e(R,Vv),e(Vv,Dse),e(Dse,E9o),e(Vv,y9o),e(Vv,XD),e(XD,w9o),e(Vv,A9o),e(R,L9o),e(R,zv),e(zv,jse),e(jse,B9o),e(zv,x9o),e(zv,VD),e(VD,k9o),e(zv,R9o),e(R,S9o),e(R,Wv),e(Wv,Nse),e(Nse,P9o),e(Wv,$9o),e(Wv,zD),e(zD,I9o),e(Wv,D9o),e(R,j9o),e(R,Qv),e(Qv,qse),e(qse,N9o),e(Qv,q9o),e(Qv,WD),e(WD,G9o),e(Qv,O9o),e(R,X9o),e(R,Hv),e(Hv,Gse),e(Gse,V9o),e(Hv,z9o),e(Hv,QD),e(QD,W9o),e(Hv,Q9o),e(R,H9o),e(R,Uv),e(Uv,Ose),e(Ose,U9o),e(Uv,J9o),e(Uv,HD),e(HD,Y9o),e(Uv,K9o),e(R,Z9o),e(R,Jv),e(Jv,Xse),e(Xse,eBo),e(Jv,oBo),e(Jv,UD),e(UD,rBo),e(Jv,tBo),e(R,aBo),e(R,Yv),e(Yv,Vse),e(Vse,sBo),e(Yv,nBo),e(Yv,JD),e(JD,lBo),e(Yv,iBo),e(R,dBo),e(R,Kv),e(Kv,zse),e(zse,cBo),e(Kv,mBo),e(Kv,YD),e(YD,fBo),e(Kv,gBo),e(R,hBo),e(R,Zv),e(Zv,Wse),e(Wse,uBo),e(Zv,pBo),e(Zv,KD),e(KD,_Bo),e(Zv,bBo),e(R,vBo),e(R,eT),e(eT,Qse),e(Qse,TBo),e(eT,FBo),e(eT,ZD),e(ZD,CBo),e(eT,MBo),e(R,EBo),e(R,oT),e(oT,Hse),e(Hse,yBo),e(oT,wBo),e(oT,ej),e(ej,ABo),e(oT,LBo),e(R,BBo),e(R,rT),e(rT,Use),e(Use,xBo),e(rT,kBo),e(rT,oj),e(oj,RBo),e(rT,SBo),e(R,PBo),e(R,tT),e(tT,Jse),e(Jse,$Bo),e(tT,IBo),e(tT,rj),e(rj,DBo),e(tT,jBo),e(R,NBo),e(R,aT),e(aT,Yse),e(Yse,qBo),e(aT,GBo),e(aT,tj),e(tj,OBo),e(aT,XBo),e(R,VBo),e(R,sT),e(sT,Kse),e(Kse,zBo),e(sT,WBo),e(sT,aj),e(aj,QBo),e(sT,HBo),e(R,UBo),e(R,nT),e(nT,Zse),e(Zse,JBo),e(nT,YBo),e(nT,sj),e(sj,KBo),e(nT,ZBo),e(R,exo),e(R,lT),e(lT,ene),e(ene,oxo),e(lT,rxo),e(lT,nj),e(nj,txo),e(lT,axo),e(R,sxo),e(R,iT),e(iT,one),e(one,nxo),e(iT,lxo),e(iT,lj),e(lj,ixo),e(iT,dxo),e(R,cxo),e(R,dT),e(dT,rne),e(rne,mxo),e(dT,fxo),e(dT,ij),e(ij,gxo),e(dT,hxo),e(R,uxo),e(R,cT),e(cT,tne),e(tne,pxo),e(cT,_xo),e(cT,dj),e(dj,bxo),e(cT,vxo),e(R,Txo),e(R,mT),e(mT,ane),e(ane,Fxo),e(mT,Cxo),e(mT,cj),e(cj,Mxo),e(mT,Exo),e(R,yxo),e(R,fT),e(fT,sne),e(sne,wxo),e(fT,Axo),e(fT,mj),e(mj,Lxo),e(fT,Bxo),e(R,xxo),e(R,gT),e(gT,nne),e(nne,kxo),e(gT,Rxo),e(gT,fj),e(fj,Sxo),e(gT,Pxo),e(R,$xo),e(R,hT),e(hT,lne),e(lne,Ixo),e(hT,Dxo),e(hT,gj),e(gj,jxo),e(hT,Nxo),e(R,qxo),e(R,uT),e(uT,ine),e(ine,Gxo),e(uT,Oxo),e(uT,hj),e(hj,Xxo),e(uT,Vxo),e(R,zxo),e(R,pT),e(pT,dne),e(dne,Wxo),e(pT,Qxo),e(pT,uj),e(uj,Hxo),e(pT,Uxo),e(R,Jxo),e(R,_T),e(_T,cne),e(cne,Yxo),e(_T,Kxo),e(_T,pj),e(pj,Zxo),e(_T,eko),e(R,oko),e(R,bT),e(bT,mne),e(mne,rko),e(bT,tko),e(bT,_j),e(_j,ako),e(bT,sko),e(R,nko),e(R,vT),e(vT,fne),e(fne,lko),e(vT,iko),e(vT,bj),e(bj,dko),e(vT,cko),e(R,mko),e(R,TT),e(TT,gne),e(gne,fko),e(TT,gko),e(TT,vj),e(vj,hko),e(TT,uko),e(R,pko),e(R,FT),e(FT,hne),e(hne,_ko),e(FT,bko),e(FT,Tj),e(Tj,vko),e(FT,Tko),e(R,Fko),e(R,CT),e(CT,une),e(une,Cko),e(CT,Mko),e(CT,Fj),e(Fj,Eko),e(CT,yko),e(R,wko),e(R,MT),e(MT,pne),e(pne,Ako),e(MT,Lko),e(MT,Cj),e(Cj,Bko),e(MT,xko),e(R,kko),e(R,ET),e(ET,_ne),e(_ne,Rko),e(ET,Sko),e(ET,Mj),e(Mj,Pko),e(ET,$ko),e(R,Iko),e(R,yT),e(yT,bne),e(bne,Dko),e(yT,jko),e(yT,Ej),e(Ej,Nko),e(yT,qko),e(R,Gko),e(R,wT),e(wT,vne),e(vne,Oko),e(wT,Xko),e(wT,yj),e(yj,Vko),e(wT,zko),e(R,Wko),e(R,AT),e(AT,Tne),e(Tne,Qko),e(AT,Hko),e(AT,wj),e(wj,Uko),e(AT,Jko),e(Ge,Yko),e(Ge,LT),e(LT,Kko),e(LT,Fne),e(Fne,Zko),e(LT,eRo),e(LT,Cne),e(Cne,oRo),e(Ge,rRo),e(Ge,Mne),e(Mne,tRo),e(Ge,aRo),g(bw,Ge,null),b(c,dBe,_),b(c,vd,_),e(vd,BT),e(BT,Ene),g(vw,Ene,null),e(vd,sRo),e(vd,yne),e(yne,nRo),b(c,cBe,_),b(c,tr,_),g(Tw,tr,null),e(tr,lRo),e(tr,Td),e(Td,iRo),e(Td,wne),e(wne,dRo),e(Td,cRo),e(Td,Ane),e(Ane,mRo),e(Td,fRo),e(tr,gRo),e(tr,Fw),e(Fw,hRo),e(Fw,Lne),e(Lne,uRo),e(Fw,pRo),e(tr,_Ro),e(tr,Jr),g(Cw,Jr,null),e(Jr,bRo),e(Jr,Bne),e(Bne,vRo),e(Jr,TRo),e(Jr,Fd),e(Fd,FRo),e(Fd,xne),e(xne,CRo),e(Fd,MRo),e(Fd,kne),e(kne,ERo),e(Fd,yRo),e(Jr,wRo),e(Jr,Rne),e(Rne,ARo),e(Jr,LRo),g(Mw,Jr,null),e(tr,BRo),e(tr,Oe),g(Ew,Oe,null),e(Oe,xRo),e(Oe,Sne),e(Sne,kRo),e(Oe,RRo),e(Oe,Ua),e(Ua,SRo),e(Ua,Pne),e(Pne,PRo),e(Ua,$Ro),e(Ua,$ne),e($ne,IRo),e(Ua,DRo),e(Ua,Ine),e(Ine,jRo),e(Ua,NRo),e(Oe,qRo),e(Oe,Dne),e(Dne,xT),e(xT,jne),e(jne,GRo),e(xT,ORo),e(xT,Aj),e(Aj,XRo),e(xT,VRo),e(Oe,zRo),e(Oe,kT),e(kT,WRo),e(kT,Nne),e(Nne,QRo),e(kT,HRo),e(kT,qne),e(qne,URo),e(Oe,JRo),e(Oe,Gne),e(Gne,YRo),e(Oe,KRo),g(yw,Oe,null),b(c,mBe,_),b(c,Cd,_),e(Cd,RT),e(RT,One),g(ww,One,null),e(Cd,ZRo),e(Cd,Xne),e(Xne,eSo),b(c,fBe,_),b(c,ar,_),g(Aw,ar,null),e(ar,oSo),e(ar,Md),e(Md,rSo),e(Md,Vne),e(Vne,tSo),e(Md,aSo),e(Md,zne),e(zne,sSo),e(Md,nSo),e(ar,lSo),e(ar,Lw),e(Lw,iSo),e(Lw,Wne),e(Wne,dSo),e(Lw,cSo),e(ar,mSo),e(ar,Yr),g(Bw,Yr,null),e(Yr,fSo),e(Yr,Qne),e(Qne,gSo),e(Yr,hSo),e(Yr,Ed),e(Ed,uSo),e(Ed,Hne),e(Hne,pSo),e(Ed,_So),e(Ed,Une),e(Une,bSo),e(Ed,vSo),e(Yr,TSo),e(Yr,Jne),e(Jne,FSo),e(Yr,CSo),g(xw,Yr,null),e(ar,MSo),e(ar,Xe),g(kw,Xe,null),e(Xe,ESo),e(Xe,Yne),e(Yne,ySo),e(Xe,wSo),e(Xe,Ja),e(Ja,ASo),e(Ja,Kne),e(Kne,LSo),e(Ja,BSo),e(Ja,Zne),e(Zne,xSo),e(Ja,kSo),e(Ja,ele),e(ele,RSo),e(Ja,SSo),e(Xe,PSo),e(Xe,be),e(be,ST),e(ST,ole),e(ole,$So),e(ST,ISo),e(ST,Lj),e(Lj,DSo),e(ST,jSo),e(be,NSo),e(be,PT),e(PT,rle),e(rle,qSo),e(PT,GSo),e(PT,Bj),e(Bj,OSo),e(PT,XSo),e(be,VSo),e(be,$n),e($n,tle),e(tle,zSo),e($n,WSo),e($n,xj),e(xj,QSo),e($n,HSo),e($n,kj),e(kj,USo),e($n,JSo),e(be,YSo),e(be,$T),e($T,ale),e(ale,KSo),e($T,ZSo),e($T,Rj),e(Rj,ePo),e($T,oPo),e(be,rPo),e(be,la),e(la,sle),e(sle,tPo),e(la,aPo),e(la,Sj),e(Sj,sPo),e(la,nPo),e(la,Pj),e(Pj,lPo),e(la,iPo),e(la,$j),e($j,dPo),e(la,cPo),e(be,mPo),e(be,IT),e(IT,nle),e(nle,fPo),e(IT,gPo),e(IT,Ij),e(Ij,hPo),e(IT,uPo),e(be,pPo),e(be,DT),e(DT,lle),e(lle,_Po),e(DT,bPo),e(DT,Dj),e(Dj,vPo),e(DT,TPo),e(be,FPo),e(be,jT),e(jT,ile),e(ile,CPo),e(jT,MPo),e(jT,jj),e(jj,EPo),e(jT,yPo),e(be,wPo),e(be,NT),e(NT,dle),e(dle,APo),e(NT,LPo),e(NT,Nj),e(Nj,BPo),e(NT,xPo),e(Xe,kPo),e(Xe,qT),e(qT,RPo),e(qT,cle),e(cle,SPo),e(qT,PPo),e(qT,mle),e(mle,$Po),e(Xe,IPo),e(Xe,fle),e(fle,DPo),e(Xe,jPo),g(Rw,Xe,null),b(c,gBe,_),b(c,yd,_),e(yd,GT),e(GT,gle),g(Sw,gle,null),e(yd,NPo),e(yd,hle),e(hle,qPo),b(c,hBe,_),b(c,sr,_),g(Pw,sr,null),e(sr,GPo),e(sr,wd),e(wd,OPo),e(wd,ule),e(ule,XPo),e(wd,VPo),e(wd,ple),e(ple,zPo),e(wd,WPo),e(sr,QPo),e(sr,$w),e($w,HPo),e($w,_le),e(_le,UPo),e($w,JPo),e(sr,YPo),e(sr,Kr),g(Iw,Kr,null),e(Kr,KPo),e(Kr,ble),e(ble,ZPo),e(Kr,e$o),e(Kr,Ad),e(Ad,o$o),e(Ad,vle),e(vle,r$o),e(Ad,t$o),e(Ad,Tle),e(Tle,a$o),e(Ad,s$o),e(Kr,n$o),e(Kr,Fle),e(Fle,l$o),e(Kr,i$o),g(Dw,Kr,null),e(sr,d$o),e(sr,Ve),g(jw,Ve,null),e(Ve,c$o),e(Ve,Cle),e(Cle,m$o),e(Ve,f$o),e(Ve,Ya),e(Ya,g$o),e(Ya,Mle),e(Mle,h$o),e(Ya,u$o),e(Ya,Ele),e(Ele,p$o),e(Ya,_$o),e(Ya,yle),e(yle,b$o),e(Ya,v$o),e(Ve,T$o),e(Ve,wle),e(wle,OT),e(OT,Ale),e(Ale,F$o),e(OT,C$o),e(OT,qj),e(qj,M$o),e(OT,E$o),e(Ve,y$o),e(Ve,XT),e(XT,w$o),e(XT,Lle),e(Lle,A$o),e(XT,L$o),e(XT,Ble),e(Ble,B$o),e(Ve,x$o),e(Ve,xle),e(xle,k$o),e(Ve,R$o),g(Nw,Ve,null),b(c,uBe,_),b(c,Ld,_),e(Ld,VT),e(VT,kle),g(qw,kle,null),e(Ld,S$o),e(Ld,Rle),e(Rle,P$o),b(c,pBe,_),b(c,nr,_),g(Gw,nr,null),e(nr,$$o),e(nr,Bd),e(Bd,I$o),e(Bd,Sle),e(Sle,D$o),e(Bd,j$o),e(Bd,Ple),e(Ple,N$o),e(Bd,q$o),e(nr,G$o),e(nr,Ow),e(Ow,O$o),e(Ow,$le),e($le,X$o),e(Ow,V$o),e(nr,z$o),e(nr,Zr),g(Xw,Zr,null),e(Zr,W$o),e(Zr,Ile),e(Ile,Q$o),e(Zr,H$o),e(Zr,xd),e(xd,U$o),e(xd,Dle),e(Dle,J$o),e(xd,Y$o),e(xd,jle),e(jle,K$o),e(xd,Z$o),e(Zr,eIo),e(Zr,Nle),e(Nle,oIo),e(Zr,rIo),g(Vw,Zr,null),e(nr,tIo),e(nr,ze),g(zw,ze,null),e(ze,aIo),e(ze,qle),e(qle,sIo),e(ze,nIo),e(ze,Ka),e(Ka,lIo),e(Ka,Gle),e(Gle,iIo),e(Ka,dIo),e(Ka,Ole),e(Ole,cIo),e(Ka,mIo),e(Ka,Xle),e(Xle,fIo),e(Ka,gIo),e(ze,hIo),e(ze,Ae),e(Ae,zT),e(zT,Vle),e(Vle,uIo),e(zT,pIo),e(zT,Gj),e(Gj,_Io),e(zT,bIo),e(Ae,vIo),e(Ae,WT),e(WT,zle),e(zle,TIo),e(WT,FIo),e(WT,Oj),e(Oj,CIo),e(WT,MIo),e(Ae,EIo),e(Ae,QT),e(QT,Wle),e(Wle,yIo),e(QT,wIo),e(QT,Xj),e(Xj,AIo),e(QT,LIo),e(Ae,BIo),e(Ae,HT),e(HT,Qle),e(Qle,xIo),e(HT,kIo),e(HT,Vj),e(Vj,RIo),e(HT,SIo),e(Ae,PIo),e(Ae,UT),e(UT,Hle),e(Hle,$Io),e(UT,IIo),e(UT,zj),e(zj,DIo),e(UT,jIo),e(Ae,NIo),e(Ae,JT),e(JT,Ule),e(Ule,qIo),e(JT,GIo),e(JT,Wj),e(Wj,OIo),e(JT,XIo),e(Ae,VIo),e(Ae,YT),e(YT,Jle),e(Jle,zIo),e(YT,WIo),e(YT,Qj),e(Qj,QIo),e(YT,HIo),e(Ae,UIo),e(Ae,KT),e(KT,Yle),e(Yle,JIo),e(KT,YIo),e(KT,Hj),e(Hj,KIo),e(KT,ZIo),e(ze,eDo),e(ze,ZT),e(ZT,oDo),e(ZT,Kle),e(Kle,rDo),e(ZT,tDo),e(ZT,Zle),e(Zle,aDo),e(ze,sDo),e(ze,eie),e(eie,nDo),e(ze,lDo),g(Ww,ze,null),b(c,_Be,_),b(c,kd,_),e(kd,e1),e(e1,oie),g(Qw,oie,null),e(kd,iDo),e(kd,rie),e(rie,dDo),b(c,bBe,_),b(c,lr,_),g(Hw,lr,null),e(lr,cDo),e(lr,Rd),e(Rd,mDo),e(Rd,tie),e(tie,fDo),e(Rd,gDo),e(Rd,aie),e(aie,hDo),e(Rd,uDo),e(lr,pDo),e(lr,Uw),e(Uw,_Do),e(Uw,sie),e(sie,bDo),e(Uw,vDo),e(lr,TDo),e(lr,et),g(Jw,et,null),e(et,FDo),e(et,nie),e(nie,CDo),e(et,MDo),e(et,Sd),e(Sd,EDo),e(Sd,lie),e(lie,yDo),e(Sd,wDo),e(Sd,iie),e(iie,ADo),e(Sd,LDo),e(et,BDo),e(et,die),e(die,xDo),e(et,kDo),g(Yw,et,null),e(lr,RDo),e(lr,We),g(Kw,We,null),e(We,SDo),e(We,cie),e(cie,PDo),e(We,$Do),e(We,Za),e(Za,IDo),e(Za,mie),e(mie,DDo),e(Za,jDo),e(Za,fie),e(fie,NDo),e(Za,qDo),e(Za,gie),e(gie,GDo),e(Za,ODo),e(We,XDo),e(We,es),e(es,o1),e(o1,hie),e(hie,VDo),e(o1,zDo),e(o1,Uj),e(Uj,WDo),e(o1,QDo),e(es,HDo),e(es,r1),e(r1,uie),e(uie,UDo),e(r1,JDo),e(r1,Jj),e(Jj,YDo),e(r1,KDo),e(es,ZDo),e(es,t1),e(t1,pie),e(pie,ejo),e(t1,ojo),e(t1,Yj),e(Yj,rjo),e(t1,tjo),e(es,ajo),e(es,a1),e(a1,_ie),e(_ie,sjo),e(a1,njo),e(a1,Kj),e(Kj,ljo),e(a1,ijo),e(We,djo),e(We,s1),e(s1,cjo),e(s1,bie),e(bie,mjo),e(s1,fjo),e(s1,vie),e(vie,gjo),e(We,hjo),e(We,Tie),e(Tie,ujo),e(We,pjo),g(Zw,We,null),b(c,vBe,_),b(c,Pd,_),e(Pd,n1),e(n1,Fie),g(e6,Fie,null),e(Pd,_jo),e(Pd,Cie),e(Cie,bjo),b(c,TBe,_),b(c,ir,_),g(o6,ir,null),e(ir,vjo),e(ir,$d),e($d,Tjo),e($d,Mie),e(Mie,Fjo),e($d,Cjo),e($d,Eie),e(Eie,Mjo),e($d,Ejo),e(ir,yjo),e(ir,r6),e(r6,wjo),e(r6,yie),e(yie,Ajo),e(r6,Ljo),e(ir,Bjo),e(ir,ot),g(t6,ot,null),e(ot,xjo),e(ot,wie),e(wie,kjo),e(ot,Rjo),e(ot,Id),e(Id,Sjo),e(Id,Aie),e(Aie,Pjo),e(Id,$jo),e(Id,Lie),e(Lie,Ijo),e(Id,Djo),e(ot,jjo),e(ot,Bie),e(Bie,Njo),e(ot,qjo),g(a6,ot,null),e(ir,Gjo),e(ir,Qe),g(s6,Qe,null),e(Qe,Ojo),e(Qe,xie),e(xie,Xjo),e(Qe,Vjo),e(Qe,os),e(os,zjo),e(os,kie),e(kie,Wjo),e(os,Qjo),e(os,Rie),e(Rie,Hjo),e(os,Ujo),e(os,Sie),e(Sie,Jjo),e(os,Yjo),e(Qe,Kjo),e(Qe,Le),e(Le,l1),e(l1,Pie),e(Pie,Zjo),e(l1,eNo),e(l1,Zj),e(Zj,oNo),e(l1,rNo),e(Le,tNo),e(Le,i1),e(i1,$ie),e($ie,aNo),e(i1,sNo),e(i1,eN),e(eN,nNo),e(i1,lNo),e(Le,iNo),e(Le,d1),e(d1,Iie),e(Iie,dNo),e(d1,cNo),e(d1,oN),e(oN,mNo),e(d1,fNo),e(Le,gNo),e(Le,c1),e(c1,Die),e(Die,hNo),e(c1,uNo),e(c1,rN),e(rN,pNo),e(c1,_No),e(Le,bNo),e(Le,m1),e(m1,jie),e(jie,vNo),e(m1,TNo),e(m1,tN),e(tN,FNo),e(m1,CNo),e(Le,MNo),e(Le,f1),e(f1,Nie),e(Nie,ENo),e(f1,yNo),e(f1,aN),e(aN,wNo),e(f1,ANo),e(Le,LNo),e(Le,g1),e(g1,qie),e(qie,BNo),e(g1,xNo),e(g1,sN),e(sN,kNo),e(g1,RNo),e(Le,SNo),e(Le,h1),e(h1,Gie),e(Gie,PNo),e(h1,$No),e(h1,nN),e(nN,INo),e(h1,DNo),e(Qe,jNo),e(Qe,u1),e(u1,NNo),e(u1,Oie),e(Oie,qNo),e(u1,GNo),e(u1,Xie),e(Xie,ONo),e(Qe,XNo),e(Qe,Vie),e(Vie,VNo),e(Qe,zNo),g(n6,Qe,null),b(c,FBe,_),b(c,Dd,_),e(Dd,p1),e(p1,zie),g(l6,zie,null),e(Dd,WNo),e(Dd,Wie),e(Wie,QNo),b(c,CBe,_),b(c,dr,_),g(i6,dr,null),e(dr,HNo),e(dr,jd),e(jd,UNo),e(jd,Qie),e(Qie,JNo),e(jd,YNo),e(jd,Hie),e(Hie,KNo),e(jd,ZNo),e(dr,eqo),e(dr,d6),e(d6,oqo),e(d6,Uie),e(Uie,rqo),e(d6,tqo),e(dr,aqo),e(dr,rt),g(c6,rt,null),e(rt,sqo),e(rt,Jie),e(Jie,nqo),e(rt,lqo),e(rt,Nd),e(Nd,iqo),e(Nd,Yie),e(Yie,dqo),e(Nd,cqo),e(Nd,Kie),e(Kie,mqo),e(Nd,fqo),e(rt,gqo),e(rt,Zie),e(Zie,hqo),e(rt,uqo),g(m6,rt,null),e(dr,pqo),e(dr,He),g(f6,He,null),e(He,_qo),e(He,ede),e(ede,bqo),e(He,vqo),e(He,rs),e(rs,Tqo),e(rs,ode),e(ode,Fqo),e(rs,Cqo),e(rs,rde),e(rde,Mqo),e(rs,Eqo),e(rs,tde),e(tde,yqo),e(rs,wqo),e(He,Aqo),e(He,g6),e(g6,_1),e(_1,ade),e(ade,Lqo),e(_1,Bqo),e(_1,lN),e(lN,xqo),e(_1,kqo),e(g6,Rqo),e(g6,b1),e(b1,sde),e(sde,Sqo),e(b1,Pqo),e(b1,iN),e(iN,$qo),e(b1,Iqo),e(He,Dqo),e(He,v1),e(v1,jqo),e(v1,nde),e(nde,Nqo),e(v1,qqo),e(v1,lde),e(lde,Gqo),e(He,Oqo),e(He,ide),e(ide,Xqo),e(He,Vqo),g(h6,He,null),b(c,MBe,_),b(c,qd,_),e(qd,T1),e(T1,dde),g(u6,dde,null),e(qd,zqo),e(qd,cde),e(cde,Wqo),b(c,EBe,_),b(c,cr,_),g(p6,cr,null),e(cr,Qqo),e(cr,Gd),e(Gd,Hqo),e(Gd,mde),e(mde,Uqo),e(Gd,Jqo),e(Gd,fde),e(fde,Yqo),e(Gd,Kqo),e(cr,Zqo),e(cr,_6),e(_6,eGo),e(_6,gde),e(gde,oGo),e(_6,rGo),e(cr,tGo),e(cr,tt),g(b6,tt,null),e(tt,aGo),e(tt,hde),e(hde,sGo),e(tt,nGo),e(tt,Od),e(Od,lGo),e(Od,ude),e(ude,iGo),e(Od,dGo),e(Od,pde),e(pde,cGo),e(Od,mGo),e(tt,fGo),e(tt,_de),e(_de,gGo),e(tt,hGo),g(v6,tt,null),e(cr,uGo),e(cr,Ue),g(T6,Ue,null),e(Ue,pGo),e(Ue,bde),e(bde,_Go),e(Ue,bGo),e(Ue,ts),e(ts,vGo),e(ts,vde),e(vde,TGo),e(ts,FGo),e(ts,Tde),e(Tde,CGo),e(ts,MGo),e(ts,Fde),e(Fde,EGo),e(ts,yGo),e(Ue,wGo),e(Ue,as),e(as,F1),e(F1,Cde),e(Cde,AGo),e(F1,LGo),e(F1,dN),e(dN,BGo),e(F1,xGo),e(as,kGo),e(as,C1),e(C1,Mde),e(Mde,RGo),e(C1,SGo),e(C1,cN),e(cN,PGo),e(C1,$Go),e(as,IGo),e(as,M1),e(M1,Ede),e(Ede,DGo),e(M1,jGo),e(M1,mN),e(mN,NGo),e(M1,qGo),e(as,GGo),e(as,E1),e(E1,yde),e(yde,OGo),e(E1,XGo),e(E1,fN),e(fN,VGo),e(E1,zGo),e(Ue,WGo),e(Ue,y1),e(y1,QGo),e(y1,wde),e(wde,HGo),e(y1,UGo),e(y1,Ade),e(Ade,JGo),e(Ue,YGo),e(Ue,Lde),e(Lde,KGo),e(Ue,ZGo),g(F6,Ue,null),b(c,yBe,_),b(c,Xd,_),e(Xd,w1),e(w1,Bde),g(C6,Bde,null),e(Xd,eOo),e(Xd,xde),e(xde,oOo),b(c,wBe,_),b(c,mr,_),g(M6,mr,null),e(mr,rOo),e(mr,Vd),e(Vd,tOo),e(Vd,kde),e(kde,aOo),e(Vd,sOo),e(Vd,Rde),e(Rde,nOo),e(Vd,lOo),e(mr,iOo),e(mr,E6),e(E6,dOo),e(E6,Sde),e(Sde,cOo),e(E6,mOo),e(mr,fOo),e(mr,at),g(y6,at,null),e(at,gOo),e(at,Pde),e(Pde,hOo),e(at,uOo),e(at,zd),e(zd,pOo),e(zd,$de),e($de,_Oo),e(zd,bOo),e(zd,Ide),e(Ide,vOo),e(zd,TOo),e(at,FOo),e(at,Dde),e(Dde,COo),e(at,MOo),g(w6,at,null),e(mr,EOo),e(mr,Je),g(A6,Je,null),e(Je,yOo),e(Je,jde),e(jde,wOo),e(Je,AOo),e(Je,ss),e(ss,LOo),e(ss,Nde),e(Nde,BOo),e(ss,xOo),e(ss,qde),e(qde,kOo),e(ss,ROo),e(ss,Gde),e(Gde,SOo),e(ss,POo),e(Je,$Oo),e(Je,Wd),e(Wd,A1),e(A1,Ode),e(Ode,IOo),e(A1,DOo),e(A1,gN),e(gN,jOo),e(A1,NOo),e(Wd,qOo),e(Wd,L1),e(L1,Xde),e(Xde,GOo),e(L1,OOo),e(L1,hN),e(hN,XOo),e(L1,VOo),e(Wd,zOo),e(Wd,B1),e(B1,Vde),e(Vde,WOo),e(B1,QOo),e(B1,uN),e(uN,HOo),e(B1,UOo),e(Je,JOo),e(Je,x1),e(x1,YOo),e(x1,zde),e(zde,KOo),e(x1,ZOo),e(x1,Wde),e(Wde,eXo),e(Je,oXo),e(Je,Qde),e(Qde,rXo),e(Je,tXo),g(L6,Je,null),b(c,ABe,_),b(c,Qd,_),e(Qd,k1),e(k1,Hde),g(B6,Hde,null),e(Qd,aXo),e(Qd,Ude),e(Ude,sXo),b(c,LBe,_),b(c,fr,_),g(x6,fr,null),e(fr,nXo),e(fr,Hd),e(Hd,lXo),e(Hd,Jde),e(Jde,iXo),e(Hd,dXo),e(Hd,Yde),e(Yde,cXo),e(Hd,mXo),e(fr,fXo),e(fr,k6),e(k6,gXo),e(k6,Kde),e(Kde,hXo),e(k6,uXo),e(fr,pXo),e(fr,st),g(R6,st,null),e(st,_Xo),e(st,Zde),e(Zde,bXo),e(st,vXo),e(st,Ud),e(Ud,TXo),e(Ud,ece),e(ece,FXo),e(Ud,CXo),e(Ud,oce),e(oce,MXo),e(Ud,EXo),e(st,yXo),e(st,rce),e(rce,wXo),e(st,AXo),g(S6,st,null),e(fr,LXo),e(fr,Ye),g(P6,Ye,null),e(Ye,BXo),e(Ye,tce),e(tce,xXo),e(Ye,kXo),e(Ye,ns),e(ns,RXo),e(ns,ace),e(ace,SXo),e(ns,PXo),e(ns,sce),e(sce,$Xo),e(ns,IXo),e(ns,nce),e(nce,DXo),e(ns,jXo),e(Ye,NXo),e(Ye,lce),e(lce,R1),e(R1,ice),e(ice,qXo),e(R1,GXo),e(R1,pN),e(pN,OXo),e(R1,XXo),e(Ye,VXo),e(Ye,S1),e(S1,zXo),e(S1,dce),e(dce,WXo),e(S1,QXo),e(S1,cce),e(cce,HXo),e(Ye,UXo),e(Ye,mce),e(mce,JXo),e(Ye,YXo),g($6,Ye,null),b(c,BBe,_),b(c,Jd,_),e(Jd,P1),e(P1,fce),g(I6,fce,null),e(Jd,KXo),e(Jd,gce),e(gce,ZXo),b(c,xBe,_),b(c,gr,_),g(D6,gr,null),e(gr,eVo),e(gr,Yd),e(Yd,oVo),e(Yd,hce),e(hce,rVo),e(Yd,tVo),e(Yd,uce),e(uce,aVo),e(Yd,sVo),e(gr,nVo),e(gr,j6),e(j6,lVo),e(j6,pce),e(pce,iVo),e(j6,dVo),e(gr,cVo),e(gr,nt),g(N6,nt,null),e(nt,mVo),e(nt,_ce),e(_ce,fVo),e(nt,gVo),e(nt,Kd),e(Kd,hVo),e(Kd,bce),e(bce,uVo),e(Kd,pVo),e(Kd,vce),e(vce,_Vo),e(Kd,bVo),e(nt,vVo),e(nt,Tce),e(Tce,TVo),e(nt,FVo),g(q6,nt,null),e(gr,CVo),e(gr,Ke),g(G6,Ke,null),e(Ke,MVo),e(Ke,Fce),e(Fce,EVo),e(Ke,yVo),e(Ke,ls),e(ls,wVo),e(ls,Cce),e(Cce,AVo),e(ls,LVo),e(ls,Mce),e(Mce,BVo),e(ls,xVo),e(ls,Ece),e(Ece,kVo),e(ls,RVo),e(Ke,SVo),e(Ke,yce),e(yce,$1),e($1,wce),e(wce,PVo),e($1,$Vo),e($1,_N),e(_N,IVo),e($1,DVo),e(Ke,jVo),e(Ke,I1),e(I1,NVo),e(I1,Ace),e(Ace,qVo),e(I1,GVo),e(I1,Lce),e(Lce,OVo),e(Ke,XVo),e(Ke,Bce),e(Bce,VVo),e(Ke,zVo),g(O6,Ke,null),b(c,kBe,_),b(c,Zd,_),e(Zd,D1),e(D1,xce),g(X6,xce,null),e(Zd,WVo),e(Zd,kce),e(kce,QVo),b(c,RBe,_),b(c,hr,_),g(V6,hr,null),e(hr,HVo),e(hr,ec),e(ec,UVo),e(ec,Rce),e(Rce,JVo),e(ec,YVo),e(ec,Sce),e(Sce,KVo),e(ec,ZVo),e(hr,ezo),e(hr,z6),e(z6,ozo),e(z6,Pce),e(Pce,rzo),e(z6,tzo),e(hr,azo),e(hr,lt),g(W6,lt,null),e(lt,szo),e(lt,$ce),e($ce,nzo),e(lt,lzo),e(lt,oc),e(oc,izo),e(oc,Ice),e(Ice,dzo),e(oc,czo),e(oc,Dce),e(Dce,mzo),e(oc,fzo),e(lt,gzo),e(lt,jce),e(jce,hzo),e(lt,uzo),g(Q6,lt,null),e(hr,pzo),e(hr,Ze),g(H6,Ze,null),e(Ze,_zo),e(Ze,Nce),e(Nce,bzo),e(Ze,vzo),e(Ze,is),e(is,Tzo),e(is,qce),e(qce,Fzo),e(is,Czo),e(is,Gce),e(Gce,Mzo),e(is,Ezo),e(is,Oce),e(Oce,yzo),e(is,wzo),e(Ze,Azo),e(Ze,U6),e(U6,j1),e(j1,Xce),e(Xce,Lzo),e(j1,Bzo),e(j1,bN),e(bN,xzo),e(j1,kzo),e(U6,Rzo),e(U6,N1),e(N1,Vce),e(Vce,Szo),e(N1,Pzo),e(N1,vN),e(vN,$zo),e(N1,Izo),e(Ze,Dzo),e(Ze,q1),e(q1,jzo),e(q1,zce),e(zce,Nzo),e(q1,qzo),e(q1,Wce),e(Wce,Gzo),e(Ze,Ozo),e(Ze,Qce),e(Qce,Xzo),e(Ze,Vzo),g(J6,Ze,null),b(c,SBe,_),b(c,rc,_),e(rc,G1),e(G1,Hce),g(Y6,Hce,null),e(rc,zzo),e(rc,Uce),e(Uce,Wzo),b(c,PBe,_),b(c,ur,_),g(K6,ur,null),e(ur,Qzo),e(ur,tc),e(tc,Hzo),e(tc,Jce),e(Jce,Uzo),e(tc,Jzo),e(tc,Yce),e(Yce,Yzo),e(tc,Kzo),e(ur,Zzo),e(ur,Z6),e(Z6,eWo),e(Z6,Kce),e(Kce,oWo),e(Z6,rWo),e(ur,tWo),e(ur,it),g(eA,it,null),e(it,aWo),e(it,Zce),e(Zce,sWo),e(it,nWo),e(it,ac),e(ac,lWo),e(ac,eme),e(eme,iWo),e(ac,dWo),e(ac,ome),e(ome,cWo),e(ac,mWo),e(it,fWo),e(it,rme),e(rme,gWo),e(it,hWo),g(oA,it,null),e(ur,uWo),e(ur,go),g(rA,go,null),e(go,pWo),e(go,tme),e(tme,_Wo),e(go,bWo),e(go,ds),e(ds,vWo),e(ds,ame),e(ame,TWo),e(ds,FWo),e(ds,sme),e(sme,CWo),e(ds,MWo),e(ds,nme),e(nme,EWo),e(ds,yWo),e(go,wWo),e(go,B),e(B,O1),e(O1,lme),e(lme,AWo),e(O1,LWo),e(O1,TN),e(TN,BWo),e(O1,xWo),e(B,kWo),e(B,X1),e(X1,ime),e(ime,RWo),e(X1,SWo),e(X1,FN),e(FN,PWo),e(X1,$Wo),e(B,IWo),e(B,V1),e(V1,dme),e(dme,DWo),e(V1,jWo),e(V1,CN),e(CN,NWo),e(V1,qWo),e(B,GWo),e(B,z1),e(z1,cme),e(cme,OWo),e(z1,XWo),e(z1,MN),e(MN,VWo),e(z1,zWo),e(B,WWo),e(B,W1),e(W1,mme),e(mme,QWo),e(W1,HWo),e(W1,EN),e(EN,UWo),e(W1,JWo),e(B,YWo),e(B,Q1),e(Q1,fme),e(fme,KWo),e(Q1,ZWo),e(Q1,yN),e(yN,eQo),e(Q1,oQo),e(B,rQo),e(B,H1),e(H1,gme),e(gme,tQo),e(H1,aQo),e(H1,wN),e(wN,sQo),e(H1,nQo),e(B,lQo),e(B,U1),e(U1,hme),e(hme,iQo),e(U1,dQo),e(U1,AN),e(AN,cQo),e(U1,mQo),e(B,fQo),e(B,J1),e(J1,ume),e(ume,gQo),e(J1,hQo),e(J1,LN),e(LN,uQo),e(J1,pQo),e(B,_Qo),e(B,Y1),e(Y1,pme),e(pme,bQo),e(Y1,vQo),e(Y1,BN),e(BN,TQo),e(Y1,FQo),e(B,CQo),e(B,K1),e(K1,_me),e(_me,MQo),e(K1,EQo),e(K1,xN),e(xN,yQo),e(K1,wQo),e(B,AQo),e(B,Z1),e(Z1,bme),e(bme,LQo),e(Z1,BQo),e(Z1,kN),e(kN,xQo),e(Z1,kQo),e(B,RQo),e(B,eF),e(eF,vme),e(vme,SQo),e(eF,PQo),e(eF,RN),e(RN,$Qo),e(eF,IQo),e(B,DQo),e(B,oF),e(oF,Tme),e(Tme,jQo),e(oF,NQo),e(oF,SN),e(SN,qQo),e(oF,GQo),e(B,OQo),e(B,rF),e(rF,Fme),e(Fme,XQo),e(rF,VQo),e(rF,PN),e(PN,zQo),e(rF,WQo),e(B,QQo),e(B,tF),e(tF,Cme),e(Cme,HQo),e(tF,UQo),e(tF,$N),e($N,JQo),e(tF,YQo),e(B,KQo),e(B,In),e(In,Mme),e(Mme,ZQo),e(In,eHo),e(In,IN),e(IN,oHo),e(In,rHo),e(In,DN),e(DN,tHo),e(In,aHo),e(B,sHo),e(B,aF),e(aF,Eme),e(Eme,nHo),e(aF,lHo),e(aF,jN),e(jN,iHo),e(aF,dHo),e(B,cHo),e(B,sF),e(sF,yme),e(yme,mHo),e(sF,fHo),e(sF,NN),e(NN,gHo),e(sF,hHo),e(B,uHo),e(B,nF),e(nF,wme),e(wme,pHo),e(nF,_Ho),e(nF,qN),e(qN,bHo),e(nF,vHo),e(B,THo),e(B,lF),e(lF,Ame),e(Ame,FHo),e(lF,CHo),e(lF,GN),e(GN,MHo),e(lF,EHo),e(B,yHo),e(B,iF),e(iF,Lme),e(Lme,wHo),e(iF,AHo),e(iF,ON),e(ON,LHo),e(iF,BHo),e(B,xHo),e(B,dF),e(dF,Bme),e(Bme,kHo),e(dF,RHo),e(dF,XN),e(XN,SHo),e(dF,PHo),e(B,$Ho),e(B,cF),e(cF,xme),e(xme,IHo),e(cF,DHo),e(cF,VN),e(VN,jHo),e(cF,NHo),e(B,qHo),e(B,mF),e(mF,kme),e(kme,GHo),e(mF,OHo),e(mF,zN),e(zN,XHo),e(mF,VHo),e(B,zHo),e(B,fF),e(fF,Rme),e(Rme,WHo),e(fF,QHo),e(fF,WN),e(WN,HHo),e(fF,UHo),e(B,JHo),e(B,gF),e(gF,Sme),e(Sme,YHo),e(gF,KHo),e(gF,QN),e(QN,ZHo),e(gF,eUo),e(B,oUo),e(B,hF),e(hF,Pme),e(Pme,rUo),e(hF,tUo),e(hF,HN),e(HN,aUo),e(hF,sUo),e(B,nUo),e(B,uF),e(uF,$me),e($me,lUo),e(uF,iUo),e(uF,UN),e(UN,dUo),e(uF,cUo),e(B,mUo),e(B,pF),e(pF,Ime),e(Ime,fUo),e(pF,gUo),e(pF,JN),e(JN,hUo),e(pF,uUo),e(B,pUo),e(B,_F),e(_F,Dme),e(Dme,_Uo),e(_F,bUo),e(_F,YN),e(YN,vUo),e(_F,TUo),e(B,FUo),e(B,bF),e(bF,jme),e(jme,CUo),e(bF,MUo),e(bF,KN),e(KN,EUo),e(bF,yUo),e(B,wUo),e(B,vF),e(vF,Nme),e(Nme,AUo),e(vF,LUo),e(vF,ZN),e(ZN,BUo),e(vF,xUo),e(B,kUo),e(B,TF),e(TF,qme),e(qme,RUo),e(TF,SUo),e(TF,eq),e(eq,PUo),e(TF,$Uo),e(B,IUo),e(B,FF),e(FF,Gme),e(Gme,DUo),e(FF,jUo),e(FF,oq),e(oq,NUo),e(FF,qUo),e(B,GUo),e(B,CF),e(CF,Ome),e(Ome,OUo),e(CF,XUo),e(CF,rq),e(rq,VUo),e(CF,zUo),e(B,WUo),e(B,MF),e(MF,Xme),e(Xme,QUo),e(MF,HUo),e(MF,tq),e(tq,UUo),e(MF,JUo),e(B,YUo),e(B,EF),e(EF,Vme),e(Vme,KUo),e(EF,ZUo),e(EF,aq),e(aq,eJo),e(EF,oJo),e(B,rJo),e(B,yF),e(yF,zme),e(zme,tJo),e(yF,aJo),e(yF,sq),e(sq,sJo),e(yF,nJo),e(B,lJo),e(B,wF),e(wF,Wme),e(Wme,iJo),e(wF,dJo),e(wF,nq),e(nq,cJo),e(wF,mJo),e(B,fJo),e(B,AF),e(AF,Qme),e(Qme,gJo),e(AF,hJo),e(AF,lq),e(lq,uJo),e(AF,pJo),e(B,_Jo),e(B,LF),e(LF,Hme),e(Hme,bJo),e(LF,vJo),e(LF,iq),e(iq,TJo),e(LF,FJo),e(go,CJo),e(go,Ume),e(Ume,MJo),e(go,EJo),g(tA,go,null),b(c,$Be,_),b(c,sc,_),e(sc,BF),e(BF,Jme),g(aA,Jme,null),e(sc,yJo),e(sc,Yme),e(Yme,wJo),b(c,IBe,_),b(c,pr,_),g(sA,pr,null),e(pr,AJo),e(pr,nc),e(nc,LJo),e(nc,Kme),e(Kme,BJo),e(nc,xJo),e(nc,Zme),e(Zme,kJo),e(nc,RJo),e(pr,SJo),e(pr,nA),e(nA,PJo),e(nA,efe),e(efe,$Jo),e(nA,IJo),e(pr,DJo),e(pr,dt),g(lA,dt,null),e(dt,jJo),e(dt,ofe),e(ofe,NJo),e(dt,qJo),e(dt,lc),e(lc,GJo),e(lc,rfe),e(rfe,OJo),e(lc,XJo),e(lc,tfe),e(tfe,VJo),e(lc,zJo),e(dt,WJo),e(dt,afe),e(afe,QJo),e(dt,HJo),g(iA,dt,null),e(pr,UJo),e(pr,ho),g(dA,ho,null),e(ho,JJo),e(ho,sfe),e(sfe,YJo),e(ho,KJo),e(ho,cs),e(cs,ZJo),e(cs,nfe),e(nfe,eYo),e(cs,oYo),e(cs,lfe),e(lfe,rYo),e(cs,tYo),e(cs,ife),e(ife,aYo),e(cs,sYo),e(ho,nYo),e(ho,H),e(H,xF),e(xF,dfe),e(dfe,lYo),e(xF,iYo),e(xF,dq),e(dq,dYo),e(xF,cYo),e(H,mYo),e(H,kF),e(kF,cfe),e(cfe,fYo),e(kF,gYo),e(kF,cq),e(cq,hYo),e(kF,uYo),e(H,pYo),e(H,RF),e(RF,mfe),e(mfe,_Yo),e(RF,bYo),e(RF,mq),e(mq,vYo),e(RF,TYo),e(H,FYo),e(H,SF),e(SF,ffe),e(ffe,CYo),e(SF,MYo),e(SF,fq),e(fq,EYo),e(SF,yYo),e(H,wYo),e(H,PF),e(PF,gfe),e(gfe,AYo),e(PF,LYo),e(PF,gq),e(gq,BYo),e(PF,xYo),e(H,kYo),e(H,$F),e($F,hfe),e(hfe,RYo),e($F,SYo),e($F,hq),e(hq,PYo),e($F,$Yo),e(H,IYo),e(H,IF),e(IF,ufe),e(ufe,DYo),e(IF,jYo),e(IF,uq),e(uq,NYo),e(IF,qYo),e(H,GYo),e(H,DF),e(DF,pfe),e(pfe,OYo),e(DF,XYo),e(DF,pq),e(pq,VYo),e(DF,zYo),e(H,WYo),e(H,jF),e(jF,_fe),e(_fe,QYo),e(jF,HYo),e(jF,_q),e(_q,UYo),e(jF,JYo),e(H,YYo),e(H,NF),e(NF,bfe),e(bfe,KYo),e(NF,ZYo),e(NF,bq),e(bq,eKo),e(NF,oKo),e(H,rKo),e(H,qF),e(qF,vfe),e(vfe,tKo),e(qF,aKo),e(qF,vq),e(vq,sKo),e(qF,nKo),e(H,lKo),e(H,GF),e(GF,Tfe),e(Tfe,iKo),e(GF,dKo),e(GF,Tq),e(Tq,cKo),e(GF,mKo),e(H,fKo),e(H,OF),e(OF,Ffe),e(Ffe,gKo),e(OF,hKo),e(OF,Fq),e(Fq,uKo),e(OF,pKo),e(H,_Ko),e(H,XF),e(XF,Cfe),e(Cfe,bKo),e(XF,vKo),e(XF,Cq),e(Cq,TKo),e(XF,FKo),e(H,CKo),e(H,VF),e(VF,Mfe),e(Mfe,MKo),e(VF,EKo),e(VF,Mq),e(Mq,yKo),e(VF,wKo),e(H,AKo),e(H,zF),e(zF,Efe),e(Efe,LKo),e(zF,BKo),e(zF,Eq),e(Eq,xKo),e(zF,kKo),e(H,RKo),e(H,WF),e(WF,yfe),e(yfe,SKo),e(WF,PKo),e(WF,yq),e(yq,$Ko),e(WF,IKo),e(H,DKo),e(H,QF),e(QF,wfe),e(wfe,jKo),e(QF,NKo),e(QF,wq),e(wq,qKo),e(QF,GKo),e(H,OKo),e(H,HF),e(HF,Afe),e(Afe,XKo),e(HF,VKo),e(HF,Aq),e(Aq,zKo),e(HF,WKo),e(H,QKo),e(H,UF),e(UF,Lfe),e(Lfe,HKo),e(UF,UKo),e(UF,Lq),e(Lq,JKo),e(UF,YKo),e(H,KKo),e(H,JF),e(JF,Bfe),e(Bfe,ZKo),e(JF,eZo),e(JF,Bq),e(Bq,oZo),e(JF,rZo),e(H,tZo),e(H,YF),e(YF,xfe),e(xfe,aZo),e(YF,sZo),e(YF,xq),e(xq,nZo),e(YF,lZo),e(ho,iZo),e(ho,kfe),e(kfe,dZo),e(ho,cZo),g(cA,ho,null),b(c,DBe,_),b(c,ic,_),e(ic,KF),e(KF,Rfe),g(mA,Rfe,null),e(ic,mZo),e(ic,Sfe),e(Sfe,fZo),b(c,jBe,_),b(c,_r,_),g(fA,_r,null),e(_r,gZo),e(_r,dc),e(dc,hZo),e(dc,Pfe),e(Pfe,uZo),e(dc,pZo),e(dc,$fe),e($fe,_Zo),e(dc,bZo),e(_r,vZo),e(_r,gA),e(gA,TZo),e(gA,Ife),e(Ife,FZo),e(gA,CZo),e(_r,MZo),e(_r,ct),g(hA,ct,null),e(ct,EZo),e(ct,Dfe),e(Dfe,yZo),e(ct,wZo),e(ct,cc),e(cc,AZo),e(cc,jfe),e(jfe,LZo),e(cc,BZo),e(cc,Nfe),e(Nfe,xZo),e(cc,kZo),e(ct,RZo),e(ct,qfe),e(qfe,SZo),e(ct,PZo),g(uA,ct,null),e(_r,$Zo),e(_r,uo),g(pA,uo,null),e(uo,IZo),e(uo,Gfe),e(Gfe,DZo),e(uo,jZo),e(uo,ms),e(ms,NZo),e(ms,Ofe),e(Ofe,qZo),e(ms,GZo),e(ms,Xfe),e(Xfe,OZo),e(ms,XZo),e(ms,Vfe),e(Vfe,VZo),e(ms,zZo),e(uo,WZo),e(uo,he),e(he,ZF),e(ZF,zfe),e(zfe,QZo),e(ZF,HZo),e(ZF,kq),e(kq,UZo),e(ZF,JZo),e(he,YZo),e(he,eC),e(eC,Wfe),e(Wfe,KZo),e(eC,ZZo),e(eC,Rq),e(Rq,eer),e(eC,oer),e(he,rer),e(he,oC),e(oC,Qfe),e(Qfe,ter),e(oC,aer),e(oC,Sq),e(Sq,ser),e(oC,ner),e(he,ler),e(he,rC),e(rC,Hfe),e(Hfe,ier),e(rC,der),e(rC,Pq),e(Pq,cer),e(rC,mer),e(he,fer),e(he,tC),e(tC,Ufe),e(Ufe,ger),e(tC,her),e(tC,$q),e($q,uer),e(tC,per),e(he,_er),e(he,aC),e(aC,Jfe),e(Jfe,ber),e(aC,ver),e(aC,Iq),e(Iq,Ter),e(aC,Fer),e(he,Cer),e(he,sC),e(sC,Yfe),e(Yfe,Mer),e(sC,Eer),e(sC,Dq),e(Dq,yer),e(sC,wer),e(he,Aer),e(he,nC),e(nC,Kfe),e(Kfe,Ler),e(nC,Ber),e(nC,jq),e(jq,xer),e(nC,ker),e(he,Rer),e(he,lC),e(lC,Zfe),e(Zfe,Ser),e(lC,Per),e(lC,Nq),e(Nq,$er),e(lC,Ier),e(he,Der),e(he,iC),e(iC,ege),e(ege,jer),e(iC,Ner),e(iC,qq),e(qq,qer),e(iC,Ger),e(uo,Oer),e(uo,oge),e(oge,Xer),e(uo,Ver),g(_A,uo,null),b(c,NBe,_),b(c,mc,_),e(mc,dC),e(dC,rge),g(bA,rge,null),e(mc,zer),e(mc,tge),e(tge,Wer),b(c,qBe,_),b(c,br,_),g(vA,br,null),e(br,Qer),e(br,fc),e(fc,Her),e(fc,age),e(age,Uer),e(fc,Jer),e(fc,sge),e(sge,Yer),e(fc,Ker),e(br,Zer),e(br,TA),e(TA,eor),e(TA,nge),e(nge,oor),e(TA,ror),e(br,tor),e(br,mt),g(FA,mt,null),e(mt,aor),e(mt,lge),e(lge,sor),e(mt,nor),e(mt,gc),e(gc,lor),e(gc,ige),e(ige,ior),e(gc,dor),e(gc,dge),e(dge,cor),e(gc,mor),e(mt,gor),e(mt,cge),e(cge,hor),e(mt,uor),g(CA,mt,null),e(br,por),e(br,po),g(MA,po,null),e(po,_or),e(po,mge),e(mge,bor),e(po,vor),e(po,fs),e(fs,Tor),e(fs,fge),e(fge,For),e(fs,Cor),e(fs,gge),e(gge,Mor),e(fs,Eor),e(fs,hge),e(hge,yor),e(fs,wor),e(po,Aor),e(po,EA),e(EA,cC),e(cC,uge),e(uge,Lor),e(cC,Bor),e(cC,Gq),e(Gq,xor),e(cC,kor),e(EA,Ror),e(EA,mC),e(mC,pge),e(pge,Sor),e(mC,Por),e(mC,Oq),e(Oq,$or),e(mC,Ior),e(po,Dor),e(po,_ge),e(_ge,jor),e(po,Nor),g(yA,po,null),b(c,GBe,_),b(c,hc,_),e(hc,fC),e(fC,bge),g(wA,bge,null),e(hc,qor),e(hc,vge),e(vge,Gor),b(c,OBe,_),b(c,vr,_),g(AA,vr,null),e(vr,Oor),e(vr,uc),e(uc,Xor),e(uc,Tge),e(Tge,Vor),e(uc,zor),e(uc,Fge),e(Fge,Wor),e(uc,Qor),e(vr,Hor),e(vr,LA),e(LA,Uor),e(LA,Cge),e(Cge,Jor),e(LA,Yor),e(vr,Kor),e(vr,ft),g(BA,ft,null),e(ft,Zor),e(ft,Mge),e(Mge,err),e(ft,orr),e(ft,pc),e(pc,rrr),e(pc,Ege),e(Ege,trr),e(pc,arr),e(pc,yge),e(yge,srr),e(pc,nrr),e(ft,lrr),e(ft,wge),e(wge,irr),e(ft,drr),g(xA,ft,null),e(vr,crr),e(vr,_o),g(kA,_o,null),e(_o,mrr),e(_o,Age),e(Age,frr),e(_o,grr),e(_o,gs),e(gs,hrr),e(gs,Lge),e(Lge,urr),e(gs,prr),e(gs,Bge),e(Bge,_rr),e(gs,brr),e(gs,xge),e(xge,vrr),e(gs,Trr),e(_o,Frr),e(_o,Y),e(Y,gC),e(gC,kge),e(kge,Crr),e(gC,Mrr),e(gC,Xq),e(Xq,Err),e(gC,yrr),e(Y,wrr),e(Y,hC),e(hC,Rge),e(Rge,Arr),e(hC,Lrr),e(hC,Vq),e(Vq,Brr),e(hC,xrr),e(Y,krr),e(Y,uC),e(uC,Sge),e(Sge,Rrr),e(uC,Srr),e(uC,zq),e(zq,Prr),e(uC,$rr),e(Y,Irr),e(Y,pC),e(pC,Pge),e(Pge,Drr),e(pC,jrr),e(pC,Wq),e(Wq,Nrr),e(pC,qrr),e(Y,Grr),e(Y,_C),e(_C,$ge),e($ge,Orr),e(_C,Xrr),e(_C,Qq),e(Qq,Vrr),e(_C,zrr),e(Y,Wrr),e(Y,bC),e(bC,Ige),e(Ige,Qrr),e(bC,Hrr),e(bC,Hq),e(Hq,Urr),e(bC,Jrr),e(Y,Yrr),e(Y,vC),e(vC,Dge),e(Dge,Krr),e(vC,Zrr),e(vC,Uq),e(Uq,etr),e(vC,otr),e(Y,rtr),e(Y,TC),e(TC,jge),e(jge,ttr),e(TC,atr),e(TC,Jq),e(Jq,str),e(TC,ntr),e(Y,ltr),e(Y,FC),e(FC,Nge),e(Nge,itr),e(FC,dtr),e(FC,Yq),e(Yq,ctr),e(FC,mtr),e(Y,ftr),e(Y,CC),e(CC,qge),e(qge,gtr),e(CC,htr),e(CC,Kq),e(Kq,utr),e(CC,ptr),e(Y,_tr),e(Y,MC),e(MC,Gge),e(Gge,btr),e(MC,vtr),e(MC,Zq),e(Zq,Ttr),e(MC,Ftr),e(Y,Ctr),e(Y,EC),e(EC,Oge),e(Oge,Mtr),e(EC,Etr),e(EC,eG),e(eG,ytr),e(EC,wtr),e(Y,Atr),e(Y,yC),e(yC,Xge),e(Xge,Ltr),e(yC,Btr),e(yC,oG),e(oG,xtr),e(yC,ktr),e(Y,Rtr),e(Y,wC),e(wC,Vge),e(Vge,Str),e(wC,Ptr),e(wC,rG),e(rG,$tr),e(wC,Itr),e(Y,Dtr),e(Y,AC),e(AC,zge),e(zge,jtr),e(AC,Ntr),e(AC,tG),e(tG,qtr),e(AC,Gtr),e(Y,Otr),e(Y,LC),e(LC,Wge),e(Wge,Xtr),e(LC,Vtr),e(LC,aG),e(aG,ztr),e(LC,Wtr),e(Y,Qtr),e(Y,BC),e(BC,Qge),e(Qge,Htr),e(BC,Utr),e(BC,sG),e(sG,Jtr),e(BC,Ytr),e(Y,Ktr),e(Y,xC),e(xC,Hge),e(Hge,Ztr),e(xC,ear),e(xC,nG),e(nG,oar),e(xC,rar),e(Y,tar),e(Y,kC),e(kC,Uge),e(Uge,aar),e(kC,sar),e(kC,lG),e(lG,nar),e(kC,lar),e(Y,iar),e(Y,RC),e(RC,Jge),e(Jge,dar),e(RC,car),e(RC,iG),e(iG,mar),e(RC,far),e(_o,gar),e(_o,Yge),e(Yge,har),e(_o,uar),g(RA,_o,null),b(c,XBe,_),b(c,_c,_),e(_c,SC),e(SC,Kge),g(SA,Kge,null),e(_c,par),e(_c,Zge),e(Zge,_ar),b(c,VBe,_),b(c,Tr,_),g(PA,Tr,null),e(Tr,bar),e(Tr,bc),e(bc,Tar),e(bc,ehe),e(ehe,Far),e(bc,Car),e(bc,ohe),e(ohe,Mar),e(bc,Ear),e(Tr,yar),e(Tr,$A),e($A,war),e($A,rhe),e(rhe,Aar),e($A,Lar),e(Tr,Bar),e(Tr,gt),g(IA,gt,null),e(gt,xar),e(gt,the),e(the,kar),e(gt,Rar),e(gt,vc),e(vc,Sar),e(vc,ahe),e(ahe,Par),e(vc,$ar),e(vc,she),e(she,Iar),e(vc,Dar),e(gt,jar),e(gt,nhe),e(nhe,Nar),e(gt,qar),g(DA,gt,null),e(Tr,Gar),e(Tr,bo),g(jA,bo,null),e(bo,Oar),e(bo,lhe),e(lhe,Xar),e(bo,Var),e(bo,hs),e(hs,zar),e(hs,ihe),e(ihe,War),e(hs,Qar),e(hs,dhe),e(dhe,Har),e(hs,Uar),e(hs,che),e(che,Jar),e(hs,Yar),e(bo,Kar),e(bo,ue),e(ue,PC),e(PC,mhe),e(mhe,Zar),e(PC,esr),e(PC,dG),e(dG,osr),e(PC,rsr),e(ue,tsr),e(ue,$C),e($C,fhe),e(fhe,asr),e($C,ssr),e($C,cG),e(cG,nsr),e($C,lsr),e(ue,isr),e(ue,IC),e(IC,ghe),e(ghe,dsr),e(IC,csr),e(IC,mG),e(mG,msr),e(IC,fsr),e(ue,gsr),e(ue,DC),e(DC,hhe),e(hhe,hsr),e(DC,usr),e(DC,fG),e(fG,psr),e(DC,_sr),e(ue,bsr),e(ue,jC),e(jC,uhe),e(uhe,vsr),e(jC,Tsr),e(jC,gG),e(gG,Fsr),e(jC,Csr),e(ue,Msr),e(ue,NC),e(NC,phe),e(phe,Esr),e(NC,ysr),e(NC,hG),e(hG,wsr),e(NC,Asr),e(ue,Lsr),e(ue,qC),e(qC,_he),e(_he,Bsr),e(qC,xsr),e(qC,uG),e(uG,ksr),e(qC,Rsr),e(ue,Ssr),e(ue,GC),e(GC,bhe),e(bhe,Psr),e(GC,$sr),e(GC,pG),e(pG,Isr),e(GC,Dsr),e(ue,jsr),e(ue,OC),e(OC,vhe),e(vhe,Nsr),e(OC,qsr),e(OC,_G),e(_G,Gsr),e(OC,Osr),e(ue,Xsr),e(ue,XC),e(XC,The),e(The,Vsr),e(XC,zsr),e(XC,bG),e(bG,Wsr),e(XC,Qsr),e(bo,Hsr),e(bo,Fhe),e(Fhe,Usr),e(bo,Jsr),g(NA,bo,null),b(c,zBe,_),b(c,Tc,_),e(Tc,VC),e(VC,Che),g(qA,Che,null),e(Tc,Ysr),e(Tc,Mhe),e(Mhe,Ksr),b(c,WBe,_),b(c,Fr,_),g(GA,Fr,null),e(Fr,Zsr),e(Fr,Fc),e(Fc,enr),e(Fc,Ehe),e(Ehe,onr),e(Fc,rnr),e(Fc,yhe),e(yhe,tnr),e(Fc,anr),e(Fr,snr),e(Fr,OA),e(OA,nnr),e(OA,whe),e(whe,lnr),e(OA,inr),e(Fr,dnr),e(Fr,ht),g(XA,ht,null),e(ht,cnr),e(ht,Ahe),e(Ahe,mnr),e(ht,fnr),e(ht,Cc),e(Cc,gnr),e(Cc,Lhe),e(Lhe,hnr),e(Cc,unr),e(Cc,Bhe),e(Bhe,pnr),e(Cc,_nr),e(ht,bnr),e(ht,xhe),e(xhe,vnr),e(ht,Tnr),g(VA,ht,null),e(Fr,Fnr),e(Fr,vo),g(zA,vo,null),e(vo,Cnr),e(vo,khe),e(khe,Mnr),e(vo,Enr),e(vo,us),e(us,ynr),e(us,Rhe),e(Rhe,wnr),e(us,Anr),e(us,She),e(She,Lnr),e(us,Bnr),e(us,Phe),e(Phe,xnr),e(us,knr),e(vo,Rnr),e(vo,X),e(X,zC),e(zC,$he),e($he,Snr),e(zC,Pnr),e(zC,vG),e(vG,$nr),e(zC,Inr),e(X,Dnr),e(X,WC),e(WC,Ihe),e(Ihe,jnr),e(WC,Nnr),e(WC,TG),e(TG,qnr),e(WC,Gnr),e(X,Onr),e(X,QC),e(QC,Dhe),e(Dhe,Xnr),e(QC,Vnr),e(QC,FG),e(FG,znr),e(QC,Wnr),e(X,Qnr),e(X,HC),e(HC,jhe),e(jhe,Hnr),e(HC,Unr),e(HC,CG),e(CG,Jnr),e(HC,Ynr),e(X,Knr),e(X,UC),e(UC,Nhe),e(Nhe,Znr),e(UC,elr),e(UC,MG),e(MG,olr),e(UC,rlr),e(X,tlr),e(X,JC),e(JC,qhe),e(qhe,alr),e(JC,slr),e(JC,EG),e(EG,nlr),e(JC,llr),e(X,ilr),e(X,YC),e(YC,Ghe),e(Ghe,dlr),e(YC,clr),e(YC,yG),e(yG,mlr),e(YC,flr),e(X,glr),e(X,KC),e(KC,Ohe),e(Ohe,hlr),e(KC,ulr),e(KC,wG),e(wG,plr),e(KC,_lr),e(X,blr),e(X,ZC),e(ZC,Xhe),e(Xhe,vlr),e(ZC,Tlr),e(ZC,AG),e(AG,Flr),e(ZC,Clr),e(X,Mlr),e(X,e4),e(e4,Vhe),e(Vhe,Elr),e(e4,ylr),e(e4,LG),e(LG,wlr),e(e4,Alr),e(X,Llr),e(X,o4),e(o4,zhe),e(zhe,Blr),e(o4,xlr),e(o4,BG),e(BG,klr),e(o4,Rlr),e(X,Slr),e(X,r4),e(r4,Whe),e(Whe,Plr),e(r4,$lr),e(r4,xG),e(xG,Ilr),e(r4,Dlr),e(X,jlr),e(X,t4),e(t4,Qhe),e(Qhe,Nlr),e(t4,qlr),e(t4,kG),e(kG,Glr),e(t4,Olr),e(X,Xlr),e(X,a4),e(a4,Hhe),e(Hhe,Vlr),e(a4,zlr),e(a4,RG),e(RG,Wlr),e(a4,Qlr),e(X,Hlr),e(X,s4),e(s4,Uhe),e(Uhe,Ulr),e(s4,Jlr),e(s4,SG),e(SG,Ylr),e(s4,Klr),e(X,Zlr),e(X,n4),e(n4,Jhe),e(Jhe,eir),e(n4,oir),e(n4,PG),e(PG,rir),e(n4,tir),e(X,air),e(X,l4),e(l4,Yhe),e(Yhe,sir),e(l4,nir),e(l4,$G),e($G,lir),e(l4,iir),e(X,dir),e(X,i4),e(i4,Khe),e(Khe,cir),e(i4,mir),e(i4,IG),e(IG,fir),e(i4,gir),e(X,hir),e(X,d4),e(d4,Zhe),e(Zhe,uir),e(d4,pir),e(d4,DG),e(DG,_ir),e(d4,bir),e(X,vir),e(X,c4),e(c4,eue),e(eue,Tir),e(c4,Fir),e(c4,jG),e(jG,Cir),e(c4,Mir),e(X,Eir),e(X,m4),e(m4,oue),e(oue,yir),e(m4,wir),e(m4,NG),e(NG,Air),e(m4,Lir),e(X,Bir),e(X,f4),e(f4,rue),e(rue,xir),e(f4,kir),e(f4,qG),e(qG,Rir),e(f4,Sir),e(X,Pir),e(X,g4),e(g4,tue),e(tue,$ir),e(g4,Iir),e(g4,GG),e(GG,Dir),e(g4,jir),e(X,Nir),e(X,h4),e(h4,aue),e(aue,qir),e(h4,Gir),e(h4,OG),e(OG,Oir),e(h4,Xir),e(X,Vir),e(X,u4),e(u4,sue),e(sue,zir),e(u4,Wir),e(u4,XG),e(XG,Qir),e(u4,Hir),e(vo,Uir),e(vo,nue),e(nue,Jir),e(vo,Yir),g(WA,vo,null),b(c,QBe,_),b(c,Mc,_),e(Mc,p4),e(p4,lue),g(QA,lue,null),e(Mc,Kir),e(Mc,iue),e(iue,Zir),b(c,HBe,_),b(c,Cr,_),g(HA,Cr,null),e(Cr,edr),e(Cr,Ec),e(Ec,odr),e(Ec,due),e(due,rdr),e(Ec,tdr),e(Ec,cue),e(cue,adr),e(Ec,sdr),e(Cr,ndr),e(Cr,UA),e(UA,ldr),e(UA,mue),e(mue,idr),e(UA,ddr),e(Cr,cdr),e(Cr,ut),g(JA,ut,null),e(ut,mdr),e(ut,fue),e(fue,fdr),e(ut,gdr),e(ut,yc),e(yc,hdr),e(yc,gue),e(gue,udr),e(yc,pdr),e(yc,hue),e(hue,_dr),e(yc,bdr),e(ut,vdr),e(ut,uue),e(uue,Tdr),e(ut,Fdr),g(YA,ut,null),e(Cr,Cdr),e(Cr,To),g(KA,To,null),e(To,Mdr),e(To,pue),e(pue,Edr),e(To,ydr),e(To,ps),e(ps,wdr),e(ps,_ue),e(_ue,Adr),e(ps,Ldr),e(ps,bue),e(bue,Bdr),e(ps,xdr),e(ps,vue),e(vue,kdr),e(ps,Rdr),e(To,Sdr),e(To,te),e(te,_4),e(_4,Tue),e(Tue,Pdr),e(_4,$dr),e(_4,VG),e(VG,Idr),e(_4,Ddr),e(te,jdr),e(te,b4),e(b4,Fue),e(Fue,Ndr),e(b4,qdr),e(b4,zG),e(zG,Gdr),e(b4,Odr),e(te,Xdr),e(te,v4),e(v4,Cue),e(Cue,Vdr),e(v4,zdr),e(v4,WG),e(WG,Wdr),e(v4,Qdr),e(te,Hdr),e(te,T4),e(T4,Mue),e(Mue,Udr),e(T4,Jdr),e(T4,QG),e(QG,Ydr),e(T4,Kdr),e(te,Zdr),e(te,F4),e(F4,Eue),e(Eue,ecr),e(F4,ocr),e(F4,HG),e(HG,rcr),e(F4,tcr),e(te,acr),e(te,C4),e(C4,yue),e(yue,scr),e(C4,ncr),e(C4,UG),e(UG,lcr),e(C4,icr),e(te,dcr),e(te,M4),e(M4,wue),e(wue,ccr),e(M4,mcr),e(M4,JG),e(JG,fcr),e(M4,gcr),e(te,hcr),e(te,E4),e(E4,Aue),e(Aue,ucr),e(E4,pcr),e(E4,YG),e(YG,_cr),e(E4,bcr),e(te,vcr),e(te,y4),e(y4,Lue),e(Lue,Tcr),e(y4,Fcr),e(y4,KG),e(KG,Ccr),e(y4,Mcr),e(te,Ecr),e(te,w4),e(w4,Bue),e(Bue,ycr),e(w4,wcr),e(w4,ZG),e(ZG,Acr),e(w4,Lcr),e(te,Bcr),e(te,A4),e(A4,xue),e(xue,xcr),e(A4,kcr),e(A4,eO),e(eO,Rcr),e(A4,Scr),e(te,Pcr),e(te,L4),e(L4,kue),e(kue,$cr),e(L4,Icr),e(L4,oO),e(oO,Dcr),e(L4,jcr),e(te,Ncr),e(te,B4),e(B4,Rue),e(Rue,qcr),e(B4,Gcr),e(B4,rO),e(rO,Ocr),e(B4,Xcr),e(te,Vcr),e(te,x4),e(x4,Sue),e(Sue,zcr),e(x4,Wcr),e(x4,tO),e(tO,Qcr),e(x4,Hcr),e(te,Ucr),e(te,k4),e(k4,Pue),e(Pue,Jcr),e(k4,Ycr),e(k4,aO),e(aO,Kcr),e(k4,Zcr),e(te,emr),e(te,R4),e(R4,$ue),e($ue,omr),e(R4,rmr),e(R4,sO),e(sO,tmr),e(R4,amr),e(te,smr),e(te,S4),e(S4,Iue),e(Iue,nmr),e(S4,lmr),e(S4,nO),e(nO,imr),e(S4,dmr),e(To,cmr),e(To,Due),e(Due,mmr),e(To,fmr),g(ZA,To,null),b(c,UBe,_),b(c,wc,_),e(wc,P4),e(P4,jue),g(e0,jue,null),e(wc,gmr),e(wc,Nue),e(Nue,hmr),b(c,JBe,_),b(c,Mr,_),g(o0,Mr,null),e(Mr,umr),e(Mr,Ac),e(Ac,pmr),e(Ac,que),e(que,_mr),e(Ac,bmr),e(Ac,Gue),e(Gue,vmr),e(Ac,Tmr),e(Mr,Fmr),e(Mr,r0),e(r0,Cmr),e(r0,Oue),e(Oue,Mmr),e(r0,Emr),e(Mr,ymr),e(Mr,pt),g(t0,pt,null),e(pt,wmr),e(pt,Xue),e(Xue,Amr),e(pt,Lmr),e(pt,Lc),e(Lc,Bmr),e(Lc,Vue),e(Vue,xmr),e(Lc,kmr),e(Lc,zue),e(zue,Rmr),e(Lc,Smr),e(pt,Pmr),e(pt,Wue),e(Wue,$mr),e(pt,Imr),g(a0,pt,null),e(Mr,Dmr),e(Mr,Fo),g(s0,Fo,null),e(Fo,jmr),e(Fo,Que),e(Que,Nmr),e(Fo,qmr),e(Fo,_s),e(_s,Gmr),e(_s,Hue),e(Hue,Omr),e(_s,Xmr),e(_s,Uue),e(Uue,Vmr),e(_s,zmr),e(_s,Jue),e(Jue,Wmr),e(_s,Qmr),e(Fo,Hmr),e(Fo,Yue),e(Yue,$4),e($4,Kue),e(Kue,Umr),e($4,Jmr),e($4,lO),e(lO,Ymr),e($4,Kmr),e(Fo,Zmr),e(Fo,Zue),e(Zue,efr),e(Fo,ofr),g(n0,Fo,null),b(c,YBe,_),b(c,Bc,_),e(Bc,I4),e(I4,epe),g(l0,epe,null),e(Bc,rfr),e(Bc,ope),e(ope,tfr),b(c,KBe,_),b(c,Er,_),g(i0,Er,null),e(Er,afr),e(Er,xc),e(xc,sfr),e(xc,rpe),e(rpe,nfr),e(xc,lfr),e(xc,tpe),e(tpe,ifr),e(xc,dfr),e(Er,cfr),e(Er,d0),e(d0,mfr),e(d0,ape),e(ape,ffr),e(d0,gfr),e(Er,hfr),e(Er,_t),g(c0,_t,null),e(_t,ufr),e(_t,spe),e(spe,pfr),e(_t,_fr),e(_t,kc),e(kc,bfr),e(kc,npe),e(npe,vfr),e(kc,Tfr),e(kc,lpe),e(lpe,Ffr),e(kc,Cfr),e(_t,Mfr),e(_t,ipe),e(ipe,Efr),e(_t,yfr),g(m0,_t,null),e(Er,wfr),e(Er,Co),g(f0,Co,null),e(Co,Afr),e(Co,dpe),e(dpe,Lfr),e(Co,Bfr),e(Co,bs),e(bs,xfr),e(bs,cpe),e(cpe,kfr),e(bs,Rfr),e(bs,mpe),e(mpe,Sfr),e(bs,Pfr),e(bs,fpe),e(fpe,$fr),e(bs,Ifr),e(Co,Dfr),e(Co,K),e(K,D4),e(D4,gpe),e(gpe,jfr),e(D4,Nfr),e(D4,iO),e(iO,qfr),e(D4,Gfr),e(K,Ofr),e(K,j4),e(j4,hpe),e(hpe,Xfr),e(j4,Vfr),e(j4,dO),e(dO,zfr),e(j4,Wfr),e(K,Qfr),e(K,N4),e(N4,upe),e(upe,Hfr),e(N4,Ufr),e(N4,cO),e(cO,Jfr),e(N4,Yfr),e(K,Kfr),e(K,q4),e(q4,ppe),e(ppe,Zfr),e(q4,egr),e(q4,mO),e(mO,ogr),e(q4,rgr),e(K,tgr),e(K,G4),e(G4,_pe),e(_pe,agr),e(G4,sgr),e(G4,fO),e(fO,ngr),e(G4,lgr),e(K,igr),e(K,O4),e(O4,bpe),e(bpe,dgr),e(O4,cgr),e(O4,gO),e(gO,mgr),e(O4,fgr),e(K,ggr),e(K,X4),e(X4,vpe),e(vpe,hgr),e(X4,ugr),e(X4,hO),e(hO,pgr),e(X4,_gr),e(K,bgr),e(K,V4),e(V4,Tpe),e(Tpe,vgr),e(V4,Tgr),e(V4,uO),e(uO,Fgr),e(V4,Cgr),e(K,Mgr),e(K,z4),e(z4,Fpe),e(Fpe,Egr),e(z4,ygr),e(z4,pO),e(pO,wgr),e(z4,Agr),e(K,Lgr),e(K,W4),e(W4,Cpe),e(Cpe,Bgr),e(W4,xgr),e(W4,_O),e(_O,kgr),e(W4,Rgr),e(K,Sgr),e(K,Q4),e(Q4,Mpe),e(Mpe,Pgr),e(Q4,$gr),e(Q4,bO),e(bO,Igr),e(Q4,Dgr),e(K,jgr),e(K,H4),e(H4,Epe),e(Epe,Ngr),e(H4,qgr),e(H4,vO),e(vO,Ggr),e(H4,Ogr),e(K,Xgr),e(K,U4),e(U4,ype),e(ype,Vgr),e(U4,zgr),e(U4,TO),e(TO,Wgr),e(U4,Qgr),e(K,Hgr),e(K,J4),e(J4,wpe),e(wpe,Ugr),e(J4,Jgr),e(J4,FO),e(FO,Ygr),e(J4,Kgr),e(K,Zgr),e(K,Y4),e(Y4,Ape),e(Ape,ehr),e(Y4,ohr),e(Y4,CO),e(CO,rhr),e(Y4,thr),e(K,ahr),e(K,K4),e(K4,Lpe),e(Lpe,shr),e(K4,nhr),e(K4,MO),e(MO,lhr),e(K4,ihr),e(K,dhr),e(K,Z4),e(Z4,Bpe),e(Bpe,chr),e(Z4,mhr),e(Z4,EO),e(EO,fhr),e(Z4,ghr),e(K,hhr),e(K,eM),e(eM,xpe),e(xpe,uhr),e(eM,phr),e(eM,yO),e(yO,_hr),e(eM,bhr),e(K,vhr),e(K,oM),e(oM,kpe),e(kpe,Thr),e(oM,Fhr),e(oM,wO),e(wO,Chr),e(oM,Mhr),e(K,Ehr),e(K,rM),e(rM,Rpe),e(Rpe,yhr),e(rM,whr),e(rM,AO),e(AO,Ahr),e(rM,Lhr),e(Co,Bhr),e(Co,Spe),e(Spe,xhr),e(Co,khr),g(g0,Co,null),b(c,ZBe,_),b(c,Rc,_),e(Rc,tM),e(tM,Ppe),g(h0,Ppe,null),e(Rc,Rhr),e(Rc,$pe),e($pe,Shr),b(c,exe,_),b(c,yr,_),g(u0,yr,null),e(yr,Phr),e(yr,Sc),e(Sc,$hr),e(Sc,Ipe),e(Ipe,Ihr),e(Sc,Dhr),e(Sc,Dpe),e(Dpe,jhr),e(Sc,Nhr),e(yr,qhr),e(yr,p0),e(p0,Ghr),e(p0,jpe),e(jpe,Ohr),e(p0,Xhr),e(yr,Vhr),e(yr,bt),g(_0,bt,null),e(bt,zhr),e(bt,Npe),e(Npe,Whr),e(bt,Qhr),e(bt,Pc),e(Pc,Hhr),e(Pc,qpe),e(qpe,Uhr),e(Pc,Jhr),e(Pc,Gpe),e(Gpe,Yhr),e(Pc,Khr),e(bt,Zhr),e(bt,Ope),e(Ope,eur),e(bt,our),g(b0,bt,null),e(yr,rur),e(yr,Mo),g(v0,Mo,null),e(Mo,tur),e(Mo,Xpe),e(Xpe,aur),e(Mo,sur),e(Mo,vs),e(vs,nur),e(vs,Vpe),e(Vpe,lur),e(vs,iur),e(vs,zpe),e(zpe,dur),e(vs,cur),e(vs,Wpe),e(Wpe,mur),e(vs,fur),e(Mo,gur),e(Mo,Z),e(Z,aM),e(aM,Qpe),e(Qpe,hur),e(aM,uur),e(aM,LO),e(LO,pur),e(aM,_ur),e(Z,bur),e(Z,sM),e(sM,Hpe),e(Hpe,vur),e(sM,Tur),e(sM,BO),e(BO,Fur),e(sM,Cur),e(Z,Mur),e(Z,nM),e(nM,Upe),e(Upe,Eur),e(nM,yur),e(nM,xO),e(xO,wur),e(nM,Aur),e(Z,Lur),e(Z,lM),e(lM,Jpe),e(Jpe,Bur),e(lM,xur),e(lM,kO),e(kO,kur),e(lM,Rur),e(Z,Sur),e(Z,iM),e(iM,Ype),e(Ype,Pur),e(iM,$ur),e(iM,RO),e(RO,Iur),e(iM,Dur),e(Z,jur),e(Z,dM),e(dM,Kpe),e(Kpe,Nur),e(dM,qur),e(dM,SO),e(SO,Gur),e(dM,Our),e(Z,Xur),e(Z,cM),e(cM,Zpe),e(Zpe,Vur),e(cM,zur),e(cM,PO),e(PO,Wur),e(cM,Qur),e(Z,Hur),e(Z,mM),e(mM,e_e),e(e_e,Uur),e(mM,Jur),e(mM,$O),e($O,Yur),e(mM,Kur),e(Z,Zur),e(Z,fM),e(fM,o_e),e(o_e,epr),e(fM,opr),e(fM,IO),e(IO,rpr),e(fM,tpr),e(Z,apr),e(Z,gM),e(gM,r_e),e(r_e,spr),e(gM,npr),e(gM,DO),e(DO,lpr),e(gM,ipr),e(Z,dpr),e(Z,hM),e(hM,t_e),e(t_e,cpr),e(hM,mpr),e(hM,jO),e(jO,fpr),e(hM,gpr),e(Z,hpr),e(Z,uM),e(uM,a_e),e(a_e,upr),e(uM,ppr),e(uM,NO),e(NO,_pr),e(uM,bpr),e(Z,vpr),e(Z,pM),e(pM,s_e),e(s_e,Tpr),e(pM,Fpr),e(pM,qO),e(qO,Cpr),e(pM,Mpr),e(Z,Epr),e(Z,_M),e(_M,n_e),e(n_e,ypr),e(_M,wpr),e(_M,GO),e(GO,Apr),e(_M,Lpr),e(Z,Bpr),e(Z,bM),e(bM,l_e),e(l_e,xpr),e(bM,kpr),e(bM,OO),e(OO,Rpr),e(bM,Spr),e(Z,Ppr),e(Z,vM),e(vM,i_e),e(i_e,$pr),e(vM,Ipr),e(vM,XO),e(XO,Dpr),e(vM,jpr),e(Z,Npr),e(Z,TM),e(TM,d_e),e(d_e,qpr),e(TM,Gpr),e(TM,VO),e(VO,Opr),e(TM,Xpr),e(Z,Vpr),e(Z,FM),e(FM,c_e),e(c_e,zpr),e(FM,Wpr),e(FM,zO),e(zO,Qpr),e(FM,Hpr),e(Z,Upr),e(Z,CM),e(CM,m_e),e(m_e,Jpr),e(CM,Ypr),e(CM,WO),e(WO,Kpr),e(CM,Zpr),e(Mo,e_r),e(Mo,f_e),e(f_e,o_r),e(Mo,r_r),g(T0,Mo,null),b(c,oxe,_),b(c,$c,_),e($c,MM),e(MM,g_e),g(F0,g_e,null),e($c,t_r),e($c,h_e),e(h_e,a_r),b(c,rxe,_),b(c,wr,_),g(C0,wr,null),e(wr,s_r),e(wr,Ic),e(Ic,n_r),e(Ic,u_e),e(u_e,l_r),e(Ic,i_r),e(Ic,p_e),e(p_e,d_r),e(Ic,c_r),e(wr,m_r),e(wr,M0),e(M0,f_r),e(M0,__e),e(__e,g_r),e(M0,h_r),e(wr,u_r),e(wr,vt),g(E0,vt,null),e(vt,p_r),e(vt,b_e),e(b_e,__r),e(vt,b_r),e(vt,Dc),e(Dc,v_r),e(Dc,v_e),e(v_e,T_r),e(Dc,F_r),e(Dc,T_e),e(T_e,C_r),e(Dc,M_r),e(vt,E_r),e(vt,F_e),e(F_e,y_r),e(vt,w_r),g(y0,vt,null),e(wr,A_r),e(wr,Eo),g(w0,Eo,null),e(Eo,L_r),e(Eo,C_e),e(C_e,B_r),e(Eo,x_r),e(Eo,Ts),e(Ts,k_r),e(Ts,M_e),e(M_e,R_r),e(Ts,S_r),e(Ts,E_e),e(E_e,P_r),e(Ts,$_r),e(Ts,y_e),e(y_e,I_r),e(Ts,D_r),e(Eo,j_r),e(Eo,w_e),e(w_e,EM),e(EM,A_e),e(A_e,N_r),e(EM,q_r),e(EM,QO),e(QO,G_r),e(EM,O_r),e(Eo,X_r),e(Eo,L_e),e(L_e,V_r),e(Eo,z_r),g(A0,Eo,null),b(c,txe,_),b(c,jc,_),e(jc,yM),e(yM,B_e),g(L0,B_e,null),e(jc,W_r),e(jc,x_e),e(x_e,Q_r),b(c,axe,_),b(c,Ar,_),g(B0,Ar,null),e(Ar,H_r),e(Ar,Nc),e(Nc,U_r),e(Nc,k_e),e(k_e,J_r),e(Nc,Y_r),e(Nc,R_e),e(R_e,K_r),e(Nc,Z_r),e(Ar,ebr),e(Ar,x0),e(x0,obr),e(x0,S_e),e(S_e,rbr),e(x0,tbr),e(Ar,abr),e(Ar,Tt),g(k0,Tt,null),e(Tt,sbr),e(Tt,P_e),e(P_e,nbr),e(Tt,lbr),e(Tt,qc),e(qc,ibr),e(qc,$_e),e($_e,dbr),e(qc,cbr),e(qc,I_e),e(I_e,mbr),e(qc,fbr),e(Tt,gbr),e(Tt,D_e),e(D_e,hbr),e(Tt,ubr),g(R0,Tt,null),e(Ar,pbr),e(Ar,yo),g(S0,yo,null),e(yo,_br),e(yo,j_e),e(j_e,bbr),e(yo,vbr),e(yo,Fs),e(Fs,Tbr),e(Fs,N_e),e(N_e,Fbr),e(Fs,Cbr),e(Fs,q_e),e(q_e,Mbr),e(Fs,Ebr),e(Fs,G_e),e(G_e,ybr),e(Fs,wbr),e(yo,Abr),e(yo,O_e),e(O_e,wM),e(wM,X_e),e(X_e,Lbr),e(wM,Bbr),e(wM,HO),e(HO,xbr),e(wM,kbr),e(yo,Rbr),e(yo,V_e),e(V_e,Sbr),e(yo,Pbr),g(P0,yo,null),b(c,sxe,_),b(c,Gc,_),e(Gc,AM),e(AM,z_e),g($0,z_e,null),e(Gc,$br),e(Gc,W_e),e(W_e,Ibr),b(c,nxe,_),b(c,Lr,_),g(I0,Lr,null),e(Lr,Dbr),e(Lr,Oc),e(Oc,jbr),e(Oc,Q_e),e(Q_e,Nbr),e(Oc,qbr),e(Oc,H_e),e(H_e,Gbr),e(Oc,Obr),e(Lr,Xbr),e(Lr,D0),e(D0,Vbr),e(D0,U_e),e(U_e,zbr),e(D0,Wbr),e(Lr,Qbr),e(Lr,Ft),g(j0,Ft,null),e(Ft,Hbr),e(Ft,J_e),e(J_e,Ubr),e(Ft,Jbr),e(Ft,Xc),e(Xc,Ybr),e(Xc,Y_e),e(Y_e,Kbr),e(Xc,Zbr),e(Xc,K_e),e(K_e,e2r),e(Xc,o2r),e(Ft,r2r),e(Ft,Z_e),e(Z_e,t2r),e(Ft,a2r),g(N0,Ft,null),e(Lr,s2r),e(Lr,wo),g(q0,wo,null),e(wo,n2r),e(wo,ebe),e(ebe,l2r),e(wo,i2r),e(wo,Cs),e(Cs,d2r),e(Cs,obe),e(obe,c2r),e(Cs,m2r),e(Cs,rbe),e(rbe,f2r),e(Cs,g2r),e(Cs,tbe),e(tbe,h2r),e(Cs,u2r),e(wo,p2r),e(wo,z),e(z,LM),e(LM,abe),e(abe,_2r),e(LM,b2r),e(LM,UO),e(UO,v2r),e(LM,T2r),e(z,F2r),e(z,BM),e(BM,sbe),e(sbe,C2r),e(BM,M2r),e(BM,JO),e(JO,E2r),e(BM,y2r),e(z,w2r),e(z,xM),e(xM,nbe),e(nbe,A2r),e(xM,L2r),e(xM,YO),e(YO,B2r),e(xM,x2r),e(z,k2r),e(z,kM),e(kM,lbe),e(lbe,R2r),e(kM,S2r),e(kM,KO),e(KO,P2r),e(kM,$2r),e(z,I2r),e(z,RM),e(RM,ibe),e(ibe,D2r),e(RM,j2r),e(RM,ZO),e(ZO,N2r),e(RM,q2r),e(z,G2r),e(z,SM),e(SM,dbe),e(dbe,O2r),e(SM,X2r),e(SM,eX),e(eX,V2r),e(SM,z2r),e(z,W2r),e(z,PM),e(PM,cbe),e(cbe,Q2r),e(PM,H2r),e(PM,oX),e(oX,U2r),e(PM,J2r),e(z,Y2r),e(z,$M),e($M,mbe),e(mbe,K2r),e($M,Z2r),e($M,rX),e(rX,evr),e($M,ovr),e(z,rvr),e(z,IM),e(IM,fbe),e(fbe,tvr),e(IM,avr),e(IM,tX),e(tX,svr),e(IM,nvr),e(z,lvr),e(z,DM),e(DM,gbe),e(gbe,ivr),e(DM,dvr),e(DM,aX),e(aX,cvr),e(DM,mvr),e(z,fvr),e(z,jM),e(jM,hbe),e(hbe,gvr),e(jM,hvr),e(jM,sX),e(sX,uvr),e(jM,pvr),e(z,_vr),e(z,NM),e(NM,ube),e(ube,bvr),e(NM,vvr),e(NM,nX),e(nX,Tvr),e(NM,Fvr),e(z,Cvr),e(z,qM),e(qM,pbe),e(pbe,Mvr),e(qM,Evr),e(qM,lX),e(lX,yvr),e(qM,wvr),e(z,Avr),e(z,GM),e(GM,_be),e(_be,Lvr),e(GM,Bvr),e(GM,iX),e(iX,xvr),e(GM,kvr),e(z,Rvr),e(z,OM),e(OM,bbe),e(bbe,Svr),e(OM,Pvr),e(OM,dX),e(dX,$vr),e(OM,Ivr),e(z,Dvr),e(z,XM),e(XM,vbe),e(vbe,jvr),e(XM,Nvr),e(XM,cX),e(cX,qvr),e(XM,Gvr),e(z,Ovr),e(z,VM),e(VM,Tbe),e(Tbe,Xvr),e(VM,Vvr),e(VM,mX),e(mX,zvr),e(VM,Wvr),e(z,Qvr),e(z,zM),e(zM,Fbe),e(Fbe,Hvr),e(zM,Uvr),e(zM,fX),e(fX,Jvr),e(zM,Yvr),e(z,Kvr),e(z,WM),e(WM,Cbe),e(Cbe,Zvr),e(WM,eTr),e(WM,gX),e(gX,oTr),e(WM,rTr),e(z,tTr),e(z,QM),e(QM,Mbe),e(Mbe,aTr),e(QM,sTr),e(QM,hX),e(hX,nTr),e(QM,lTr),e(z,iTr),e(z,HM),e(HM,Ebe),e(Ebe,dTr),e(HM,cTr),e(HM,uX),e(uX,mTr),e(HM,fTr),e(z,gTr),e(z,UM),e(UM,ybe),e(ybe,hTr),e(UM,uTr),e(UM,pX),e(pX,pTr),e(UM,_Tr),e(z,bTr),e(z,JM),e(JM,wbe),e(wbe,vTr),e(JM,TTr),e(JM,_X),e(_X,FTr),e(JM,CTr),e(z,MTr),e(z,YM),e(YM,Abe),e(Abe,ETr),e(YM,yTr),e(YM,bX),e(bX,wTr),e(YM,ATr),e(wo,LTr),e(wo,Lbe),e(Lbe,BTr),e(wo,xTr),g(G0,wo,null),b(c,lxe,_),b(c,Vc,_),e(Vc,KM),e(KM,Bbe),g(O0,Bbe,null),e(Vc,kTr),e(Vc,xbe),e(xbe,RTr),b(c,ixe,_),b(c,Br,_),g(X0,Br,null),e(Br,STr),e(Br,zc),e(zc,PTr),e(zc,kbe),e(kbe,$Tr),e(zc,ITr),e(zc,Rbe),e(Rbe,DTr),e(zc,jTr),e(Br,NTr),e(Br,V0),e(V0,qTr),e(V0,Sbe),e(Sbe,GTr),e(V0,OTr),e(Br,XTr),e(Br,Ct),g(z0,Ct,null),e(Ct,VTr),e(Ct,Pbe),e(Pbe,zTr),e(Ct,WTr),e(Ct,Wc),e(Wc,QTr),e(Wc,$be),e($be,HTr),e(Wc,UTr),e(Wc,Ibe),e(Ibe,JTr),e(Wc,YTr),e(Ct,KTr),e(Ct,Dbe),e(Dbe,ZTr),e(Ct,e1r),g(W0,Ct,null),e(Br,o1r),e(Br,Ao),g(Q0,Ao,null),e(Ao,r1r),e(Ao,jbe),e(jbe,t1r),e(Ao,a1r),e(Ao,Ms),e(Ms,s1r),e(Ms,Nbe),e(Nbe,n1r),e(Ms,l1r),e(Ms,qbe),e(qbe,i1r),e(Ms,d1r),e(Ms,Gbe),e(Gbe,c1r),e(Ms,m1r),e(Ao,f1r),e(Ao,Es),e(Es,ZM),e(ZM,Obe),e(Obe,g1r),e(ZM,h1r),e(ZM,vX),e(vX,u1r),e(ZM,p1r),e(Es,_1r),e(Es,eE),e(eE,Xbe),e(Xbe,b1r),e(eE,v1r),e(eE,TX),e(TX,T1r),e(eE,F1r),e(Es,C1r),e(Es,oE),e(oE,Vbe),e(Vbe,M1r),e(oE,E1r),e(oE,FX),e(FX,y1r),e(oE,w1r),e(Es,A1r),e(Es,rE),e(rE,zbe),e(zbe,L1r),e(rE,B1r),e(rE,CX),e(CX,x1r),e(rE,k1r),e(Ao,R1r),e(Ao,Wbe),e(Wbe,S1r),e(Ao,P1r),g(H0,Ao,null),b(c,dxe,_),b(c,Qc,_),e(Qc,tE),e(tE,Qbe),g(U0,Qbe,null),e(Qc,$1r),e(Qc,Hbe),e(Hbe,I1r),b(c,cxe,_),b(c,xr,_),g(J0,xr,null),e(xr,D1r),e(xr,Hc),e(Hc,j1r),e(Hc,Ube),e(Ube,N1r),e(Hc,q1r),e(Hc,Jbe),e(Jbe,G1r),e(Hc,O1r),e(xr,X1r),e(xr,Y0),e(Y0,V1r),e(Y0,Ybe),e(Ybe,z1r),e(Y0,W1r),e(xr,Q1r),e(xr,Mt),g(K0,Mt,null),e(Mt,H1r),e(Mt,Kbe),e(Kbe,U1r),e(Mt,J1r),e(Mt,Uc),e(Uc,Y1r),e(Uc,Zbe),e(Zbe,K1r),e(Uc,Z1r),e(Uc,e2e),e(e2e,eFr),e(Uc,oFr),e(Mt,rFr),e(Mt,o2e),e(o2e,tFr),e(Mt,aFr),g(Z0,Mt,null),e(xr,sFr),e(xr,Lo),g(eL,Lo,null),e(Lo,nFr),e(Lo,r2e),e(r2e,lFr),e(Lo,iFr),e(Lo,ys),e(ys,dFr),e(ys,t2e),e(t2e,cFr),e(ys,mFr),e(ys,a2e),e(a2e,fFr),e(ys,gFr),e(ys,s2e),e(s2e,hFr),e(ys,uFr),e(Lo,pFr),e(Lo,me),e(me,aE),e(aE,n2e),e(n2e,_Fr),e(aE,bFr),e(aE,MX),e(MX,vFr),e(aE,TFr),e(me,FFr),e(me,sE),e(sE,l2e),e(l2e,CFr),e(sE,MFr),e(sE,EX),e(EX,EFr),e(sE,yFr),e(me,wFr),e(me,nE),e(nE,i2e),e(i2e,AFr),e(nE,LFr),e(nE,yX),e(yX,BFr),e(nE,xFr),e(me,kFr),e(me,lE),e(lE,d2e),e(d2e,RFr),e(lE,SFr),e(lE,wX),e(wX,PFr),e(lE,$Fr),e(me,IFr),e(me,iE),e(iE,c2e),e(c2e,DFr),e(iE,jFr),e(iE,AX),e(AX,NFr),e(iE,qFr),e(me,GFr),e(me,dE),e(dE,m2e),e(m2e,OFr),e(dE,XFr),e(dE,LX),e(LX,VFr),e(dE,zFr),e(me,WFr),e(me,cE),e(cE,f2e),e(f2e,QFr),e(cE,HFr),e(cE,BX),e(BX,UFr),e(cE,JFr),e(me,YFr),e(me,mE),e(mE,g2e),e(g2e,KFr),e(mE,ZFr),e(mE,xX),e(xX,eCr),e(mE,oCr),e(me,rCr),e(me,fE),e(fE,h2e),e(h2e,tCr),e(fE,aCr),e(fE,kX),e(kX,sCr),e(fE,nCr),e(me,lCr),e(me,gE),e(gE,u2e),e(u2e,iCr),e(gE,dCr),e(gE,RX),e(RX,cCr),e(gE,mCr),e(me,fCr),e(me,hE),e(hE,p2e),e(p2e,gCr),e(hE,hCr),e(hE,SX),e(SX,uCr),e(hE,pCr),e(Lo,_Cr),e(Lo,_2e),e(_2e,bCr),e(Lo,vCr),g(oL,Lo,null),b(c,mxe,_),b(c,Jc,_),e(Jc,uE),e(uE,b2e),g(rL,b2e,null),e(Jc,TCr),e(Jc,v2e),e(v2e,FCr),b(c,fxe,_),b(c,kr,_),g(tL,kr,null),e(kr,CCr),e(kr,Yc),e(Yc,MCr),e(Yc,T2e),e(T2e,ECr),e(Yc,yCr),e(Yc,F2e),e(F2e,wCr),e(Yc,ACr),e(kr,LCr),e(kr,aL),e(aL,BCr),e(aL,C2e),e(C2e,xCr),e(aL,kCr),e(kr,RCr),e(kr,Et),g(sL,Et,null),e(Et,SCr),e(Et,M2e),e(M2e,PCr),e(Et,$Cr),e(Et,Kc),e(Kc,ICr),e(Kc,E2e),e(E2e,DCr),e(Kc,jCr),e(Kc,y2e),e(y2e,NCr),e(Kc,qCr),e(Et,GCr),e(Et,w2e),e(w2e,OCr),e(Et,XCr),g(nL,Et,null),e(kr,VCr),e(kr,Bo),g(lL,Bo,null),e(Bo,zCr),e(Bo,A2e),e(A2e,WCr),e(Bo,QCr),e(Bo,ws),e(ws,HCr),e(ws,L2e),e(L2e,UCr),e(ws,JCr),e(ws,B2e),e(B2e,YCr),e(ws,KCr),e(ws,x2e),e(x2e,ZCr),e(ws,e4r),e(Bo,o4r),e(Bo,ve),e(ve,pE),e(pE,k2e),e(k2e,r4r),e(pE,t4r),e(pE,PX),e(PX,a4r),e(pE,s4r),e(ve,n4r),e(ve,_E),e(_E,R2e),e(R2e,l4r),e(_E,i4r),e(_E,$X),e($X,d4r),e(_E,c4r),e(ve,m4r),e(ve,bE),e(bE,S2e),e(S2e,f4r),e(bE,g4r),e(bE,IX),e(IX,h4r),e(bE,u4r),e(ve,p4r),e(ve,vE),e(vE,P2e),e(P2e,_4r),e(vE,b4r),e(vE,DX),e(DX,v4r),e(vE,T4r),e(ve,F4r),e(ve,TE),e(TE,$2e),e($2e,C4r),e(TE,M4r),e(TE,jX),e(jX,E4r),e(TE,y4r),e(ve,w4r),e(ve,FE),e(FE,I2e),e(I2e,A4r),e(FE,L4r),e(FE,NX),e(NX,B4r),e(FE,x4r),e(ve,k4r),e(ve,CE),e(CE,D2e),e(D2e,R4r),e(CE,S4r),e(CE,qX),e(qX,P4r),e(CE,$4r),e(ve,I4r),e(ve,ME),e(ME,j2e),e(j2e,D4r),e(ME,j4r),e(ME,GX),e(GX,N4r),e(ME,q4r),e(ve,G4r),e(ve,EE),e(EE,N2e),e(N2e,O4r),e(EE,X4r),e(EE,OX),e(OX,V4r),e(EE,z4r),e(Bo,W4r),e(Bo,q2e),e(q2e,Q4r),e(Bo,H4r),g(iL,Bo,null),b(c,gxe,_),b(c,Zc,_),e(Zc,yE),e(yE,G2e),g(dL,G2e,null),e(Zc,U4r),e(Zc,O2e),e(O2e,J4r),b(c,hxe,_),b(c,Rr,_),g(cL,Rr,null),e(Rr,Y4r),e(Rr,em),e(em,K4r),e(em,X2e),e(X2e,Z4r),e(em,eMr),e(em,V2e),e(V2e,oMr),e(em,rMr),e(Rr,tMr),e(Rr,mL),e(mL,aMr),e(mL,z2e),e(z2e,sMr),e(mL,nMr),e(Rr,lMr),e(Rr,yt),g(fL,yt,null),e(yt,iMr),e(yt,W2e),e(W2e,dMr),e(yt,cMr),e(yt,om),e(om,mMr),e(om,Q2e),e(Q2e,fMr),e(om,gMr),e(om,H2e),e(H2e,hMr),e(om,uMr),e(yt,pMr),e(yt,U2e),e(U2e,_Mr),e(yt,bMr),g(gL,yt,null),e(Rr,vMr),e(Rr,xo),g(hL,xo,null),e(xo,TMr),e(xo,J2e),e(J2e,FMr),e(xo,CMr),e(xo,As),e(As,MMr),e(As,Y2e),e(Y2e,EMr),e(As,yMr),e(As,K2e),e(K2e,wMr),e(As,AMr),e(As,Z2e),e(Z2e,LMr),e(As,BMr),e(xo,xMr),e(xo,Te),e(Te,wE),e(wE,eve),e(eve,kMr),e(wE,RMr),e(wE,XX),e(XX,SMr),e(wE,PMr),e(Te,$Mr),e(Te,AE),e(AE,ove),e(ove,IMr),e(AE,DMr),e(AE,VX),e(VX,jMr),e(AE,NMr),e(Te,qMr),e(Te,LE),e(LE,rve),e(rve,GMr),e(LE,OMr),e(LE,zX),e(zX,XMr),e(LE,VMr),e(Te,zMr),e(Te,BE),e(BE,tve),e(tve,WMr),e(BE,QMr),e(BE,WX),e(WX,HMr),e(BE,UMr),e(Te,JMr),e(Te,xE),e(xE,ave),e(ave,YMr),e(xE,KMr),e(xE,QX),e(QX,ZMr),e(xE,eEr),e(Te,oEr),e(Te,kE),e(kE,sve),e(sve,rEr),e(kE,tEr),e(kE,HX),e(HX,aEr),e(kE,sEr),e(Te,nEr),e(Te,RE),e(RE,nve),e(nve,lEr),e(RE,iEr),e(RE,UX),e(UX,dEr),e(RE,cEr),e(Te,mEr),e(Te,SE),e(SE,lve),e(lve,fEr),e(SE,gEr),e(SE,JX),e(JX,hEr),e(SE,uEr),e(Te,pEr),e(Te,PE),e(PE,ive),e(ive,_Er),e(PE,bEr),e(PE,YX),e(YX,vEr),e(PE,TEr),e(xo,FEr),e(xo,dve),e(dve,CEr),e(xo,MEr),g(uL,xo,null),b(c,uxe,_),b(c,rm,_),e(rm,$E),e($E,cve),g(pL,cve,null),e(rm,EEr),e(rm,mve),e(mve,yEr),b(c,pxe,_),b(c,Sr,_),g(_L,Sr,null),e(Sr,wEr),e(Sr,tm),e(tm,AEr),e(tm,fve),e(fve,LEr),e(tm,BEr),e(tm,gve),e(gve,xEr),e(tm,kEr),e(Sr,REr),e(Sr,bL),e(bL,SEr),e(bL,hve),e(hve,PEr),e(bL,$Er),e(Sr,IEr),e(Sr,wt),g(vL,wt,null),e(wt,DEr),e(wt,uve),e(uve,jEr),e(wt,NEr),e(wt,am),e(am,qEr),e(am,pve),e(pve,GEr),e(am,OEr),e(am,_ve),e(_ve,XEr),e(am,VEr),e(wt,zEr),e(wt,bve),e(bve,WEr),e(wt,QEr),g(TL,wt,null),e(Sr,HEr),e(Sr,ko),g(FL,ko,null),e(ko,UEr),e(ko,vve),e(vve,JEr),e(ko,YEr),e(ko,Ls),e(Ls,KEr),e(Ls,Tve),e(Tve,ZEr),e(Ls,e3r),e(Ls,Fve),e(Fve,o3r),e(Ls,r3r),e(Ls,Cve),e(Cve,t3r),e(Ls,a3r),e(ko,s3r),e(ko,Fe),e(Fe,IE),e(IE,Mve),e(Mve,n3r),e(IE,l3r),e(IE,KX),e(KX,i3r),e(IE,d3r),e(Fe,c3r),e(Fe,DE),e(DE,Eve),e(Eve,m3r),e(DE,f3r),e(DE,ZX),e(ZX,g3r),e(DE,h3r),e(Fe,u3r),e(Fe,jE),e(jE,yve),e(yve,p3r),e(jE,_3r),e(jE,eV),e(eV,b3r),e(jE,v3r),e(Fe,T3r),e(Fe,NE),e(NE,wve),e(wve,F3r),e(NE,C3r),e(NE,oV),e(oV,M3r),e(NE,E3r),e(Fe,y3r),e(Fe,qE),e(qE,Ave),e(Ave,w3r),e(qE,A3r),e(qE,rV),e(rV,L3r),e(qE,B3r),e(Fe,x3r),e(Fe,GE),e(GE,Lve),e(Lve,k3r),e(GE,R3r),e(GE,tV),e(tV,S3r),e(GE,P3r),e(Fe,$3r),e(Fe,OE),e(OE,Bve),e(Bve,I3r),e(OE,D3r),e(OE,aV),e(aV,j3r),e(OE,N3r),e(Fe,q3r),e(Fe,XE),e(XE,xve),e(xve,G3r),e(XE,O3r),e(XE,sV),e(sV,X3r),e(XE,V3r),e(Fe,z3r),e(Fe,VE),e(VE,kve),e(kve,W3r),e(VE,Q3r),e(VE,nV),e(nV,H3r),e(VE,U3r),e(ko,J3r),e(ko,Rve),e(Rve,Y3r),e(ko,K3r),g(CL,ko,null),b(c,_xe,_),b(c,sm,_),e(sm,zE),e(zE,Sve),g(ML,Sve,null),e(sm,Z3r),e(sm,Pve),e(Pve,e5r),b(c,bxe,_),b(c,Pr,_),g(EL,Pr,null),e(Pr,o5r),e(Pr,nm),e(nm,r5r),e(nm,$ve),e($ve,t5r),e(nm,a5r),e(nm,Ive),e(Ive,s5r),e(nm,n5r),e(Pr,l5r),e(Pr,yL),e(yL,i5r),e(yL,Dve),e(Dve,d5r),e(yL,c5r),e(Pr,m5r),e(Pr,At),g(wL,At,null),e(At,f5r),e(At,jve),e(jve,g5r),e(At,h5r),e(At,lm),e(lm,u5r),e(lm,Nve),e(Nve,p5r),e(lm,_5r),e(lm,qve),e(qve,b5r),e(lm,v5r),e(At,T5r),e(At,Gve),e(Gve,F5r),e(At,C5r),g(AL,At,null),e(Pr,M5r),e(Pr,Ro),g(LL,Ro,null),e(Ro,E5r),e(Ro,Ove),e(Ove,y5r),e(Ro,w5r),e(Ro,Bs),e(Bs,A5r),e(Bs,Xve),e(Xve,L5r),e(Bs,B5r),e(Bs,Vve),e(Vve,x5r),e(Bs,k5r),e(Bs,zve),e(zve,R5r),e(Bs,S5r),e(Ro,P5r),e(Ro,Ce),e(Ce,WE),e(WE,Wve),e(Wve,$5r),e(WE,I5r),e(WE,lV),e(lV,D5r),e(WE,j5r),e(Ce,N5r),e(Ce,QE),e(QE,Qve),e(Qve,q5r),e(QE,G5r),e(QE,iV),e(iV,O5r),e(QE,X5r),e(Ce,V5r),e(Ce,HE),e(HE,Hve),e(Hve,z5r),e(HE,W5r),e(HE,dV),e(dV,Q5r),e(HE,H5r),e(Ce,U5r),e(Ce,UE),e(UE,Uve),e(Uve,J5r),e(UE,Y5r),e(UE,cV),e(cV,K5r),e(UE,Z5r),e(Ce,eyr),e(Ce,JE),e(JE,Jve),e(Jve,oyr),e(JE,ryr),e(JE,mV),e(mV,tyr),e(JE,ayr),e(Ce,syr),e(Ce,YE),e(YE,Yve),e(Yve,nyr),e(YE,lyr),e(YE,fV),e(fV,iyr),e(YE,dyr),e(Ce,cyr),e(Ce,KE),e(KE,Kve),e(Kve,myr),e(KE,fyr),e(KE,gV),e(gV,gyr),e(KE,hyr),e(Ce,uyr),e(Ce,ZE),e(ZE,Zve),e(Zve,pyr),e(ZE,_yr),e(ZE,hV),e(hV,byr),e(ZE,vyr),e(Ce,Tyr),e(Ce,e3),e(e3,eTe),e(eTe,Fyr),e(e3,Cyr),e(e3,uV),e(uV,Myr),e(e3,Eyr),e(Ro,yyr),e(Ro,oTe),e(oTe,wyr),e(Ro,Ayr),g(BL,Ro,null),b(c,vxe,_),b(c,im,_),e(im,o3),e(o3,rTe),g(xL,rTe,null),e(im,Lyr),e(im,tTe),e(tTe,Byr),b(c,Txe,_),b(c,$r,_),g(kL,$r,null),e($r,xyr),e($r,dm),e(dm,kyr),e(dm,aTe),e(aTe,Ryr),e(dm,Syr),e(dm,sTe),e(sTe,Pyr),e(dm,$yr),e($r,Iyr),e($r,RL),e(RL,Dyr),e(RL,nTe),e(nTe,jyr),e(RL,Nyr),e($r,qyr),e($r,Lt),g(SL,Lt,null),e(Lt,Gyr),e(Lt,lTe),e(lTe,Oyr),e(Lt,Xyr),e(Lt,cm),e(cm,Vyr),e(cm,iTe),e(iTe,zyr),e(cm,Wyr),e(cm,dTe),e(dTe,Qyr),e(cm,Hyr),e(Lt,Uyr),e(Lt,cTe),e(cTe,Jyr),e(Lt,Yyr),g(PL,Lt,null),e($r,Kyr),e($r,So),g($L,So,null),e(So,Zyr),e(So,mTe),e(mTe,ewr),e(So,owr),e(So,xs),e(xs,rwr),e(xs,fTe),e(fTe,twr),e(xs,awr),e(xs,gTe),e(gTe,swr),e(xs,nwr),e(xs,hTe),e(hTe,lwr),e(xs,iwr),e(So,dwr),e(So,no),e(no,r3),e(r3,uTe),e(uTe,cwr),e(r3,mwr),e(r3,pV),e(pV,fwr),e(r3,gwr),e(no,hwr),e(no,t3),e(t3,pTe),e(pTe,uwr),e(t3,pwr),e(t3,_V),e(_V,_wr),e(t3,bwr),e(no,vwr),e(no,a3),e(a3,_Te),e(_Te,Twr),e(a3,Fwr),e(a3,bV),e(bV,Cwr),e(a3,Mwr),e(no,Ewr),e(no,s3),e(s3,bTe),e(bTe,ywr),e(s3,wwr),e(s3,vV),e(vV,Awr),e(s3,Lwr),e(no,Bwr),e(no,n3),e(n3,vTe),e(vTe,xwr),e(n3,kwr),e(n3,TV),e(TV,Rwr),e(n3,Swr),e(no,Pwr),e(no,l3),e(l3,TTe),e(TTe,$wr),e(l3,Iwr),e(l3,FV),e(FV,Dwr),e(l3,jwr),e(no,Nwr),e(no,i3),e(i3,FTe),e(FTe,qwr),e(i3,Gwr),e(i3,CV),e(CV,Owr),e(i3,Xwr),e(So,Vwr),e(So,CTe),e(CTe,zwr),e(So,Wwr),g(IL,So,null),b(c,Fxe,_),b(c,mm,_),e(mm,d3),e(d3,MTe),g(DL,MTe,null),e(mm,Qwr),e(mm,ETe),e(ETe,Hwr),b(c,Cxe,_),b(c,Ir,_),g(jL,Ir,null),e(Ir,Uwr),e(Ir,fm),e(fm,Jwr),e(fm,yTe),e(yTe,Ywr),e(fm,Kwr),e(fm,wTe),e(wTe,Zwr),e(fm,e6r),e(Ir,o6r),e(Ir,NL),e(NL,r6r),e(NL,ATe),e(ATe,t6r),e(NL,a6r),e(Ir,s6r),e(Ir,Bt),g(qL,Bt,null),e(Bt,n6r),e(Bt,LTe),e(LTe,l6r),e(Bt,i6r),e(Bt,gm),e(gm,d6r),e(gm,BTe),e(BTe,c6r),e(gm,m6r),e(gm,xTe),e(xTe,f6r),e(gm,g6r),e(Bt,h6r),e(Bt,kTe),e(kTe,u6r),e(Bt,p6r),g(GL,Bt,null),e(Ir,_6r),e(Ir,Po),g(OL,Po,null),e(Po,b6r),e(Po,RTe),e(RTe,v6r),e(Po,T6r),e(Po,ks),e(ks,F6r),e(ks,STe),e(STe,C6r),e(ks,M6r),e(ks,PTe),e(PTe,E6r),e(ks,y6r),e(ks,$Te),e($Te,w6r),e(ks,A6r),e(Po,L6r),e(Po,lo),e(lo,c3),e(c3,ITe),e(ITe,B6r),e(c3,x6r),e(c3,MV),e(MV,k6r),e(c3,R6r),e(lo,S6r),e(lo,m3),e(m3,DTe),e(DTe,P6r),e(m3,$6r),e(m3,EV),e(EV,I6r),e(m3,D6r),e(lo,j6r),e(lo,f3),e(f3,jTe),e(jTe,N6r),e(f3,q6r),e(f3,yV),e(yV,G6r),e(f3,O6r),e(lo,X6r),e(lo,g3),e(g3,NTe),e(NTe,V6r),e(g3,z6r),e(g3,wV),e(wV,W6r),e(g3,Q6r),e(lo,H6r),e(lo,h3),e(h3,qTe),e(qTe,U6r),e(h3,J6r),e(h3,AV),e(AV,Y6r),e(h3,K6r),e(lo,Z6r),e(lo,u3),e(u3,GTe),e(GTe,eAr),e(u3,oAr),e(u3,LV),e(LV,rAr),e(u3,tAr),e(lo,aAr),e(lo,p3),e(p3,OTe),e(OTe,sAr),e(p3,nAr),e(p3,BV),e(BV,lAr),e(p3,iAr),e(Po,dAr),e(Po,XTe),e(XTe,cAr),e(Po,mAr),g(XL,Po,null),b(c,Mxe,_),b(c,hm,_),e(hm,_3),e(_3,VTe),g(VL,VTe,null),e(hm,fAr),e(hm,zTe),e(zTe,gAr),b(c,Exe,_),b(c,Dr,_),g(zL,Dr,null),e(Dr,hAr),e(Dr,um),e(um,uAr),e(um,WTe),e(WTe,pAr),e(um,_Ar),e(um,QTe),e(QTe,bAr),e(um,vAr),e(Dr,TAr),e(Dr,WL),e(WL,FAr),e(WL,HTe),e(HTe,CAr),e(WL,MAr),e(Dr,EAr),e(Dr,xt),g(QL,xt,null),e(xt,yAr),e(xt,UTe),e(UTe,wAr),e(xt,AAr),e(xt,pm),e(pm,LAr),e(pm,JTe),e(JTe,BAr),e(pm,xAr),e(pm,YTe),e(YTe,kAr),e(pm,RAr),e(xt,SAr),e(xt,KTe),e(KTe,PAr),e(xt,$Ar),g(HL,xt,null),e(Dr,IAr),e(Dr,$o),g(UL,$o,null),e($o,DAr),e($o,ZTe),e(ZTe,jAr),e($o,NAr),e($o,Rs),e(Rs,qAr),e(Rs,e1e),e(e1e,GAr),e(Rs,OAr),e(Rs,o1e),e(o1e,XAr),e(Rs,VAr),e(Rs,r1e),e(r1e,zAr),e(Rs,WAr),e($o,QAr),e($o,t1e),e(t1e,b3),e(b3,a1e),e(a1e,HAr),e(b3,UAr),e(b3,xV),e(xV,JAr),e(b3,YAr),e($o,KAr),e($o,s1e),e(s1e,ZAr),e($o,e0r),g(JL,$o,null),b(c,yxe,_),b(c,_m,_),e(_m,v3),e(v3,n1e),g(YL,n1e,null),e(_m,o0r),e(_m,l1e),e(l1e,r0r),b(c,wxe,_),b(c,jr,_),g(KL,jr,null),e(jr,t0r),e(jr,bm),e(bm,a0r),e(bm,i1e),e(i1e,s0r),e(bm,n0r),e(bm,d1e),e(d1e,l0r),e(bm,i0r),e(jr,d0r),e(jr,ZL),e(ZL,c0r),e(ZL,c1e),e(c1e,m0r),e(ZL,f0r),e(jr,g0r),e(jr,kt),g(e8,kt,null),e(kt,h0r),e(kt,m1e),e(m1e,u0r),e(kt,p0r),e(kt,vm),e(vm,_0r),e(vm,f1e),e(f1e,b0r),e(vm,v0r),e(vm,g1e),e(g1e,T0r),e(vm,F0r),e(kt,C0r),e(kt,h1e),e(h1e,M0r),e(kt,E0r),g(o8,kt,null),e(jr,y0r),e(jr,Io),g(r8,Io,null),e(Io,w0r),e(Io,u1e),e(u1e,A0r),e(Io,L0r),e(Io,Ss),e(Ss,B0r),e(Ss,p1e),e(p1e,x0r),e(Ss,k0r),e(Ss,_1e),e(_1e,R0r),e(Ss,S0r),e(Ss,b1e),e(b1e,P0r),e(Ss,$0r),e(Io,I0r),e(Io,t8),e(t8,T3),e(T3,v1e),e(v1e,D0r),e(T3,j0r),e(T3,kV),e(kV,N0r),e(T3,q0r),e(t8,G0r),e(t8,F3),e(F3,T1e),e(T1e,O0r),e(F3,X0r),e(F3,RV),e(RV,V0r),e(F3,z0r),e(Io,W0r),e(Io,F1e),e(F1e,Q0r),e(Io,H0r),g(a8,Io,null),b(c,Axe,_),b(c,Tm,_),e(Tm,C3),e(C3,C1e),g(s8,C1e,null),e(Tm,U0r),e(Tm,M1e),e(M1e,J0r),b(c,Lxe,_),b(c,Nr,_),g(n8,Nr,null),e(Nr,Y0r),e(Nr,Fm),e(Fm,K0r),e(Fm,E1e),e(E1e,Z0r),e(Fm,eLr),e(Fm,y1e),e(y1e,oLr),e(Fm,rLr),e(Nr,tLr),e(Nr,l8),e(l8,aLr),e(l8,w1e),e(w1e,sLr),e(l8,nLr),e(Nr,lLr),e(Nr,Rt),g(i8,Rt,null),e(Rt,iLr),e(Rt,A1e),e(A1e,dLr),e(Rt,cLr),e(Rt,Cm),e(Cm,mLr),e(Cm,L1e),e(L1e,fLr),e(Cm,gLr),e(Cm,B1e),e(B1e,hLr),e(Cm,uLr),e(Rt,pLr),e(Rt,x1e),e(x1e,_Lr),e(Rt,bLr),g(d8,Rt,null),e(Nr,vLr),e(Nr,Do),g(c8,Do,null),e(Do,TLr),e(Do,k1e),e(k1e,FLr),e(Do,CLr),e(Do,Ps),e(Ps,MLr),e(Ps,R1e),e(R1e,ELr),e(Ps,yLr),e(Ps,S1e),e(S1e,wLr),e(Ps,ALr),e(Ps,P1e),e(P1e,LLr),e(Ps,BLr),e(Do,xLr),e(Do,$1e),e($1e,M3),e(M3,I1e),e(I1e,kLr),e(M3,RLr),e(M3,SV),e(SV,SLr),e(M3,PLr),e(Do,$Lr),e(Do,D1e),e(D1e,ILr),e(Do,DLr),g(m8,Do,null),Bxe=!0},p(c,[_]){const f8={};_&2&&(f8.$$scope={dirty:_,ctx:c}),Bm.$set(f8);const j1e={};_&2&&(j1e.$$scope={dirty:_,ctx:c}),fh.$set(j1e);const N1e={};_&2&&(N1e.$$scope={dirty:_,ctx:c}),Mh.$set(N1e)},i(c){Bxe||(h(ce.$$.fragment,c),h($a.$$.fragment,c),h(A5.$$.fragment,c),h(L5.$$.fragment,c),h(Bm.$$.fragment,c),h(B5.$$.fragment,c),h(x5.$$.fragment,c),h(S5.$$.fragment,c),h(P5.$$.fragment,c),h($5.$$.fragment,c),h(I5.$$.fragment,c),h(D5.$$.fragment,c),h(q5.$$.fragment,c),h(G5.$$.fragment,c),h(O5.$$.fragment,c),h(X5.$$.fragment,c),h(V5.$$.fragment,c),h(Q5.$$.fragment,c),h(fh.$$.fragment,c),h(H5.$$.fragment,c),h(U5.$$.fragment,c),h(J5.$$.fragment,c),h(Y5.$$.fragment,c),h(ey.$$.fragment,c),h(Mh.$$.fragment,c),h(oy.$$.fragment,c),h(ry.$$.fragment,c),h(ty.$$.fragment,c),h(ay.$$.fragment,c),h(ny.$$.fragment,c),h(ly.$$.fragment,c),h(iy.$$.fragment,c),h(dy.$$.fragment,c),h(cy.$$.fragment,c),h(my.$$.fragment,c),h(gy.$$.fragment,c),h(hy.$$.fragment,c),h(uy.$$.fragment,c),h(py.$$.fragment,c),h(_y.$$.fragment,c),h(by.$$.fragment,c),h(Ty.$$.fragment,c),h(Fy.$$.fragment,c),h(Cy.$$.fragment,c),h(My.$$.fragment,c),h(Ey.$$.fragment,c),h(yy.$$.fragment,c),h(Ay.$$.fragment,c),h(Ly.$$.fragment,c),h(By.$$.fragment,c),h(xy.$$.fragment,c),h(ky.$$.fragment,c),h(Ry.$$.fragment,c),h(Py.$$.fragment,c),h($y.$$.fragment,c),h(Iy.$$.fragment,c),h(Dy.$$.fragment,c),h(jy.$$.fragment,c),h(Ny.$$.fragment,c),h(Gy.$$.fragment,c),h(Oy.$$.fragment,c),h(Xy.$$.fragment,c),h(Vy.$$.fragment,c),h(zy.$$.fragment,c),h(Wy.$$.fragment,c),h(Hy.$$.fragment,c),h(Uy.$$.fragment,c),h(Jy.$$.fragment,c),h(Yy.$$.fragment,c),h(Ky.$$.fragment,c),h(Zy.$$.fragment,c),h(ow.$$.fragment,c),h(rw.$$.fragment,c),h(tw.$$.fragment,c),h(aw.$$.fragment,c),h(sw.$$.fragment,c),h(nw.$$.fragment,c),h(iw.$$.fragment,c),h(dw.$$.fragment,c),h(cw.$$.fragment,c),h(mw.$$.fragment,c),h(fw.$$.fragment,c),h(gw.$$.fragment,c),h(uw.$$.fragment,c),h(pw.$$.fragment,c),h(_w.$$.fragment,c),h(bw.$$.fragment,c),h(vw.$$.fragment,c),h(Tw.$$.fragment,c),h(Cw.$$.fragment,c),h(Mw.$$.fragment,c),h(Ew.$$.fragment,c),h(yw.$$.fragment,c),h(ww.$$.fragment,c),h(Aw.$$.fragment,c),h(Bw.$$.fragment,c),h(xw.$$.fragment,c),h(kw.$$.fragment,c),h(Rw.$$.fragment,c),h(Sw.$$.fragment,c),h(Pw.$$.fragment,c),h(Iw.$$.fragment,c),h(Dw.$$.fragment,c),h(jw.$$.fragment,c),h(Nw.$$.fragment,c),h(qw.$$.fragment,c),h(Gw.$$.fragment,c),h(Xw.$$.fragment,c),h(Vw.$$.fragment,c),h(zw.$$.fragment,c),h(Ww.$$.fragment,c),h(Qw.$$.fragment,c),h(Hw.$$.fragment,c),h(Jw.$$.fragment,c),h(Yw.$$.fragment,c),h(Kw.$$.fragment,c),h(Zw.$$.fragment,c),h(e6.$$.fragment,c),h(o6.$$.fragment,c),h(t6.$$.fragment,c),h(a6.$$.fragment,c),h(s6.$$.fragment,c),h(n6.$$.fragment,c),h(l6.$$.fragment,c),h(i6.$$.fragment,c),h(c6.$$.fragment,c),h(m6.$$.fragment,c),h(f6.$$.fragment,c),h(h6.$$.fragment,c),h(u6.$$.fragment,c),h(p6.$$.fragment,c),h(b6.$$.fragment,c),h(v6.$$.fragment,c),h(T6.$$.fragment,c),h(F6.$$.fragment,c),h(C6.$$.fragment,c),h(M6.$$.fragment,c),h(y6.$$.fragment,c),h(w6.$$.fragment,c),h(A6.$$.fragment,c),h(L6.$$.fragment,c),h(B6.$$.fragment,c),h(x6.$$.fragment,c),h(R6.$$.fragment,c),h(S6.$$.fragment,c),h(P6.$$.fragment,c),h($6.$$.fragment,c),h(I6.$$.fragment,c),h(D6.$$.fragment,c),h(N6.$$.fragment,c),h(q6.$$.fragment,c),h(G6.$$.fragment,c),h(O6.$$.fragment,c),h(X6.$$.fragment,c),h(V6.$$.fragment,c),h(W6.$$.fragment,c),h(Q6.$$.fragment,c),h(H6.$$.fragment,c),h(J6.$$.fragment,c),h(Y6.$$.fragment,c),h(K6.$$.fragment,c),h(eA.$$.fragment,c),h(oA.$$.fragment,c),h(rA.$$.fragment,c),h(tA.$$.fragment,c),h(aA.$$.fragment,c),h(sA.$$.fragment,c),h(lA.$$.fragment,c),h(iA.$$.fragment,c),h(dA.$$.fragment,c),h(cA.$$.fragment,c),h(mA.$$.fragment,c),h(fA.$$.fragment,c),h(hA.$$.fragment,c),h(uA.$$.fragment,c),h(pA.$$.fragment,c),h(_A.$$.fragment,c),h(bA.$$.fragment,c),h(vA.$$.fragment,c),h(FA.$$.fragment,c),h(CA.$$.fragment,c),h(MA.$$.fragment,c),h(yA.$$.fragment,c),h(wA.$$.fragment,c),h(AA.$$.fragment,c),h(BA.$$.fragment,c),h(xA.$$.fragment,c),h(kA.$$.fragment,c),h(RA.$$.fragment,c),h(SA.$$.fragment,c),h(PA.$$.fragment,c),h(IA.$$.fragment,c),h(DA.$$.fragment,c),h(jA.$$.fragment,c),h(NA.$$.fragment,c),h(qA.$$.fragment,c),h(GA.$$.fragment,c),h(XA.$$.fragment,c),h(VA.$$.fragment,c),h(zA.$$.fragment,c),h(WA.$$.fragment,c),h(QA.$$.fragment,c),h(HA.$$.fragment,c),h(JA.$$.fragment,c),h(YA.$$.fragment,c),h(KA.$$.fragment,c),h(ZA.$$.fragment,c),h(e0.$$.fragment,c),h(o0.$$.fragment,c),h(t0.$$.fragment,c),h(a0.$$.fragment,c),h(s0.$$.fragment,c),h(n0.$$.fragment,c),h(l0.$$.fragment,c),h(i0.$$.fragment,c),h(c0.$$.fragment,c),h(m0.$$.fragment,c),h(f0.$$.fragment,c),h(g0.$$.fragment,c),h(h0.$$.fragment,c),h(u0.$$.fragment,c),h(_0.$$.fragment,c),h(b0.$$.fragment,c),h(v0.$$.fragment,c),h(T0.$$.fragment,c),h(F0.$$.fragment,c),h(C0.$$.fragment,c),h(E0.$$.fragment,c),h(y0.$$.fragment,c),h(w0.$$.fragment,c),h(A0.$$.fragment,c),h(L0.$$.fragment,c),h(B0.$$.fragment,c),h(k0.$$.fragment,c),h(R0.$$.fragment,c),h(S0.$$.fragment,c),h(P0.$$.fragment,c),h($0.$$.fragment,c),h(I0.$$.fragment,c),h(j0.$$.fragment,c),h(N0.$$.fragment,c),h(q0.$$.fragment,c),h(G0.$$.fragment,c),h(O0.$$.fragment,c),h(X0.$$.fragment,c),h(z0.$$.fragment,c),h(W0.$$.fragment,c),h(Q0.$$.fragment,c),h(H0.$$.fragment,c),h(U0.$$.fragment,c),h(J0.$$.fragment,c),h(K0.$$.fragment,c),h(Z0.$$.fragment,c),h(eL.$$.fragment,c),h(oL.$$.fragment,c),h(rL.$$.fragment,c),h(tL.$$.fragment,c),h(sL.$$.fragment,c),h(nL.$$.fragment,c),h(lL.$$.fragment,c),h(iL.$$.fragment,c),h(dL.$$.fragment,c),h(cL.$$.fragment,c),h(fL.$$.fragment,c),h(gL.$$.fragment,c),h(hL.$$.fragment,c),h(uL.$$.fragment,c),h(pL.$$.fragment,c),h(_L.$$.fragment,c),h(vL.$$.fragment,c),h(TL.$$.fragment,c),h(FL.$$.fragment,c),h(CL.$$.fragment,c),h(ML.$$.fragment,c),h(EL.$$.fragment,c),h(wL.$$.fragment,c),h(AL.$$.fragment,c),h(LL.$$.fragment,c),h(BL.$$.fragment,c),h(xL.$$.fragment,c),h(kL.$$.fragment,c),h(SL.$$.fragment,c),h(PL.$$.fragment,c),h($L.$$.fragment,c),h(IL.$$.fragment,c),h(DL.$$.fragment,c),h(jL.$$.fragment,c),h(qL.$$.fragment,c),h(GL.$$.fragment,c),h(OL.$$.fragment,c),h(XL.$$.fragment,c),h(VL.$$.fragment,c),h(zL.$$.fragment,c),h(QL.$$.fragment,c),h(HL.$$.fragment,c),h(UL.$$.fragment,c),h(JL.$$.fragment,c),h(YL.$$.fragment,c),h(KL.$$.fragment,c),h(e8.$$.fragment,c),h(o8.$$.fragment,c),h(r8.$$.fragment,c),h(a8.$$.fragment,c),h(s8.$$.fragment,c),h(n8.$$.fragment,c),h(i8.$$.fragment,c),h(d8.$$.fragment,c),h(c8.$$.fragment,c),h(m8.$$.fragment,c),Bxe=!0)},o(c){u(ce.$$.fragment,c),u($a.$$.fragment,c),u(A5.$$.fragment,c),u(L5.$$.fragment,c),u(Bm.$$.fragment,c),u(B5.$$.fragment,c),u(x5.$$.fragment,c),u(S5.$$.fragment,c),u(P5.$$.fragment,c),u($5.$$.fragment,c),u(I5.$$.fragment,c),u(D5.$$.fragment,c),u(q5.$$.fragment,c),u(G5.$$.fragment,c),u(O5.$$.fragment,c),u(X5.$$.fragment,c),u(V5.$$.fragment,c),u(Q5.$$.fragment,c),u(fh.$$.fragment,c),u(H5.$$.fragment,c),u(U5.$$.fragment,c),u(J5.$$.fragment,c),u(Y5.$$.fragment,c),u(ey.$$.fragment,c),u(Mh.$$.fragment,c),u(oy.$$.fragment,c),u(ry.$$.fragment,c),u(ty.$$.fragment,c),u(ay.$$.fragment,c),u(ny.$$.fragment,c),u(ly.$$.fragment,c),u(iy.$$.fragment,c),u(dy.$$.fragment,c),u(cy.$$.fragment,c),u(my.$$.fragment,c),u(gy.$$.fragment,c),u(hy.$$.fragment,c),u(uy.$$.fragment,c),u(py.$$.fragment,c),u(_y.$$.fragment,c),u(by.$$.fragment,c),u(Ty.$$.fragment,c),u(Fy.$$.fragment,c),u(Cy.$$.fragment,c),u(My.$$.fragment,c),u(Ey.$$.fragment,c),u(yy.$$.fragment,c),u(Ay.$$.fragment,c),u(Ly.$$.fragment,c),u(By.$$.fragment,c),u(xy.$$.fragment,c),u(ky.$$.fragment,c),u(Ry.$$.fragment,c),u(Py.$$.fragment,c),u($y.$$.fragment,c),u(Iy.$$.fragment,c),u(Dy.$$.fragment,c),u(jy.$$.fragment,c),u(Ny.$$.fragment,c),u(Gy.$$.fragment,c),u(Oy.$$.fragment,c),u(Xy.$$.fragment,c),u(Vy.$$.fragment,c),u(zy.$$.fragment,c),u(Wy.$$.fragment,c),u(Hy.$$.fragment,c),u(Uy.$$.fragment,c),u(Jy.$$.fragment,c),u(Yy.$$.fragment,c),u(Ky.$$.fragment,c),u(Zy.$$.fragment,c),u(ow.$$.fragment,c),u(rw.$$.fragment,c),u(tw.$$.fragment,c),u(aw.$$.fragment,c),u(sw.$$.fragment,c),u(nw.$$.fragment,c),u(iw.$$.fragment,c),u(dw.$$.fragment,c),u(cw.$$.fragment,c),u(mw.$$.fragment,c),u(fw.$$.fragment,c),u(gw.$$.fragment,c),u(uw.$$.fragment,c),u(pw.$$.fragment,c),u(_w.$$.fragment,c),u(bw.$$.fragment,c),u(vw.$$.fragment,c),u(Tw.$$.fragment,c),u(Cw.$$.fragment,c),u(Mw.$$.fragment,c),u(Ew.$$.fragment,c),u(yw.$$.fragment,c),u(ww.$$.fragment,c),u(Aw.$$.fragment,c),u(Bw.$$.fragment,c),u(xw.$$.fragment,c),u(kw.$$.fragment,c),u(Rw.$$.fragment,c),u(Sw.$$.fragment,c),u(Pw.$$.fragment,c),u(Iw.$$.fragment,c),u(Dw.$$.fragment,c),u(jw.$$.fragment,c),u(Nw.$$.fragment,c),u(qw.$$.fragment,c),u(Gw.$$.fragment,c),u(Xw.$$.fragment,c),u(Vw.$$.fragment,c),u(zw.$$.fragment,c),u(Ww.$$.fragment,c),u(Qw.$$.fragment,c),u(Hw.$$.fragment,c),u(Jw.$$.fragment,c),u(Yw.$$.fragment,c),u(Kw.$$.fragment,c),u(Zw.$$.fragment,c),u(e6.$$.fragment,c),u(o6.$$.fragment,c),u(t6.$$.fragment,c),u(a6.$$.fragment,c),u(s6.$$.fragment,c),u(n6.$$.fragment,c),u(l6.$$.fragment,c),u(i6.$$.fragment,c),u(c6.$$.fragment,c),u(m6.$$.fragment,c),u(f6.$$.fragment,c),u(h6.$$.fragment,c),u(u6.$$.fragment,c),u(p6.$$.fragment,c),u(b6.$$.fragment,c),u(v6.$$.fragment,c),u(T6.$$.fragment,c),u(F6.$$.fragment,c),u(C6.$$.fragment,c),u(M6.$$.fragment,c),u(y6.$$.fragment,c),u(w6.$$.fragment,c),u(A6.$$.fragment,c),u(L6.$$.fragment,c),u(B6.$$.fragment,c),u(x6.$$.fragment,c),u(R6.$$.fragment,c),u(S6.$$.fragment,c),u(P6.$$.fragment,c),u($6.$$.fragment,c),u(I6.$$.fragment,c),u(D6.$$.fragment,c),u(N6.$$.fragment,c),u(q6.$$.fragment,c),u(G6.$$.fragment,c),u(O6.$$.fragment,c),u(X6.$$.fragment,c),u(V6.$$.fragment,c),u(W6.$$.fragment,c),u(Q6.$$.fragment,c),u(H6.$$.fragment,c),u(J6.$$.fragment,c),u(Y6.$$.fragment,c),u(K6.$$.fragment,c),u(eA.$$.fragment,c),u(oA.$$.fragment,c),u(rA.$$.fragment,c),u(tA.$$.fragment,c),u(aA.$$.fragment,c),u(sA.$$.fragment,c),u(lA.$$.fragment,c),u(iA.$$.fragment,c),u(dA.$$.fragment,c),u(cA.$$.fragment,c),u(mA.$$.fragment,c),u(fA.$$.fragment,c),u(hA.$$.fragment,c),u(uA.$$.fragment,c),u(pA.$$.fragment,c),u(_A.$$.fragment,c),u(bA.$$.fragment,c),u(vA.$$.fragment,c),u(FA.$$.fragment,c),u(CA.$$.fragment,c),u(MA.$$.fragment,c),u(yA.$$.fragment,c),u(wA.$$.fragment,c),u(AA.$$.fragment,c),u(BA.$$.fragment,c),u(xA.$$.fragment,c),u(kA.$$.fragment,c),u(RA.$$.fragment,c),u(SA.$$.fragment,c),u(PA.$$.fragment,c),u(IA.$$.fragment,c),u(DA.$$.fragment,c),u(jA.$$.fragment,c),u(NA.$$.fragment,c),u(qA.$$.fragment,c),u(GA.$$.fragment,c),u(XA.$$.fragment,c),u(VA.$$.fragment,c),u(zA.$$.fragment,c),u(WA.$$.fragment,c),u(QA.$$.fragment,c),u(HA.$$.fragment,c),u(JA.$$.fragment,c),u(YA.$$.fragment,c),u(KA.$$.fragment,c),u(ZA.$$.fragment,c),u(e0.$$.fragment,c),u(o0.$$.fragment,c),u(t0.$$.fragment,c),u(a0.$$.fragment,c),u(s0.$$.fragment,c),u(n0.$$.fragment,c),u(l0.$$.fragment,c),u(i0.$$.fragment,c),u(c0.$$.fragment,c),u(m0.$$.fragment,c),u(f0.$$.fragment,c),u(g0.$$.fragment,c),u(h0.$$.fragment,c),u(u0.$$.fragment,c),u(_0.$$.fragment,c),u(b0.$$.fragment,c),u(v0.$$.fragment,c),u(T0.$$.fragment,c),u(F0.$$.fragment,c),u(C0.$$.fragment,c),u(E0.$$.fragment,c),u(y0.$$.fragment,c),u(w0.$$.fragment,c),u(A0.$$.fragment,c),u(L0.$$.fragment,c),u(B0.$$.fragment,c),u(k0.$$.fragment,c),u(R0.$$.fragment,c),u(S0.$$.fragment,c),u(P0.$$.fragment,c),u($0.$$.fragment,c),u(I0.$$.fragment,c),u(j0.$$.fragment,c),u(N0.$$.fragment,c),u(q0.$$.fragment,c),u(G0.$$.fragment,c),u(O0.$$.fragment,c),u(X0.$$.fragment,c),u(z0.$$.fragment,c),u(W0.$$.fragment,c),u(Q0.$$.fragment,c),u(H0.$$.fragment,c),u(U0.$$.fragment,c),u(J0.$$.fragment,c),u(K0.$$.fragment,c),u(Z0.$$.fragment,c),u(eL.$$.fragment,c),u(oL.$$.fragment,c),u(rL.$$.fragment,c),u(tL.$$.fragment,c),u(sL.$$.fragment,c),u(nL.$$.fragment,c),u(lL.$$.fragment,c),u(iL.$$.fragment,c),u(dL.$$.fragment,c),u(cL.$$.fragment,c),u(fL.$$.fragment,c),u(gL.$$.fragment,c),u(hL.$$.fragment,c),u(uL.$$.fragment,c),u(pL.$$.fragment,c),u(_L.$$.fragment,c),u(vL.$$.fragment,c),u(TL.$$.fragment,c),u(FL.$$.fragment,c),u(CL.$$.fragment,c),u(ML.$$.fragment,c),u(EL.$$.fragment,c),u(wL.$$.fragment,c),u(AL.$$.fragment,c),u(LL.$$.fragment,c),u(BL.$$.fragment,c),u(xL.$$.fragment,c),u(kL.$$.fragment,c),u(SL.$$.fragment,c),u(PL.$$.fragment,c),u($L.$$.fragment,c),u(IL.$$.fragment,c),u(DL.$$.fragment,c),u(jL.$$.fragment,c),u(qL.$$.fragment,c),u(GL.$$.fragment,c),u(OL.$$.fragment,c),u(XL.$$.fragment,c),u(VL.$$.fragment,c),u(zL.$$.fragment,c),u(QL.$$.fragment,c),u(HL.$$.fragment,c),u(UL.$$.fragment,c),u(JL.$$.fragment,c),u(YL.$$.fragment,c),u(KL.$$.fragment,c),u(e8.$$.fragment,c),u(o8.$$.fragment,c),u(r8.$$.fragment,c),u(a8.$$.fragment,c),u(s8.$$.fragment,c),u(n8.$$.fragment,c),u(i8.$$.fragment,c),u(d8.$$.fragment,c),u(c8.$$.fragment,c),u(m8.$$.fragment,c),Bxe=!1},d(c){t(J),c&&t(Be),c&&t(ie),p(ce),c&&t(Em),c&&t(na),c&&t(ye),c&&t(io),c&&t(wm),p($a,c),c&&t(co),c&&t(ge),c&&t(Oo),c&&t(Ia),c&&t(x9e),c&&t(Ii),p(A5),c&&t(k9e),c&&t(Ns),c&&t(R9e),p(L5,c),c&&t(S9e),c&&t(h7),c&&t(P9e),p(Bm,c),c&&t($9e),c&&t(Di),p(B5),c&&t(I9e),c&&t(Xo),p(x5),p(S5),p(P5),p($5),c&&t(D9e),c&&t(Ni),p(I5),c&&t(j9e),c&&t(Vo),p(D5),p(q5),p(G5),p(O5),c&&t(N9e),c&&t(qi),p(X5),c&&t(q9e),c&&t(zo),p(V5),p(Q5),p(fh),p(H5),p(U5),c&&t(G9e),c&&t(Gi),p(J5),c&&t(O9e),c&&t(Wo),p(Y5),p(ey),p(Mh),p(oy),p(ry),c&&t(X9e),c&&t(Xi),p(ty),c&&t(V9e),c&&t(Qo),p(ay),p(ny),p(ly),p(iy),p(dy),c&&t(z9e),c&&t(Wi),p(cy),c&&t(W9e),c&&t(Ho),p(my),p(gy),p(hy),p(uy),p(py),c&&t(Q9e),c&&t(Ui),p(_y),c&&t(H9e),c&&t(Uo),p(by),p(Ty),p(Fy),p(Cy),p(My),c&&t(U9e),c&&t(Ki),p(Ey),c&&t(J9e),c&&t(Jo),p(yy),p(Ay),p(Ly),p(By),p(xy),c&&t(Y9e),c&&t(od),p(ky),c&&t(K9e),c&&t(Yo),p(Ry),p(Py),p($y),p(Iy),p(Dy),c&&t(Z9e),c&&t(ad),p(jy),c&&t(eBe),c&&t(Ko),p(Ny),p(Gy),p(Oy),p(Xy),p(Vy),c&&t(oBe),c&&t(ld),p(zy),c&&t(rBe),c&&t(Zo),p(Wy),p(Hy),p(Uy),p(Jy),p(Yy),c&&t(tBe),c&&t(cd),p(Ky),c&&t(aBe),c&&t(er),p(Zy),p(ow),p(rw),p(tw),p(aw),c&&t(sBe),c&&t(gd),p(sw),c&&t(nBe),c&&t(or),p(nw),p(iw),p(dw),p(cw),p(mw),c&&t(lBe),c&&t(pd),p(fw),c&&t(iBe),c&&t(rr),p(gw),p(uw),p(pw),p(_w),p(bw),c&&t(dBe),c&&t(vd),p(vw),c&&t(cBe),c&&t(tr),p(Tw),p(Cw),p(Mw),p(Ew),p(yw),c&&t(mBe),c&&t(Cd),p(ww),c&&t(fBe),c&&t(ar),p(Aw),p(Bw),p(xw),p(kw),p(Rw),c&&t(gBe),c&&t(yd),p(Sw),c&&t(hBe),c&&t(sr),p(Pw),p(Iw),p(Dw),p(jw),p(Nw),c&&t(uBe),c&&t(Ld),p(qw),c&&t(pBe),c&&t(nr),p(Gw),p(Xw),p(Vw),p(zw),p(Ww),c&&t(_Be),c&&t(kd),p(Qw),c&&t(bBe),c&&t(lr),p(Hw),p(Jw),p(Yw),p(Kw),p(Zw),c&&t(vBe),c&&t(Pd),p(e6),c&&t(TBe),c&&t(ir),p(o6),p(t6),p(a6),p(s6),p(n6),c&&t(FBe),c&&t(Dd),p(l6),c&&t(CBe),c&&t(dr),p(i6),p(c6),p(m6),p(f6),p(h6),c&&t(MBe),c&&t(qd),p(u6),c&&t(EBe),c&&t(cr),p(p6),p(b6),p(v6),p(T6),p(F6),c&&t(yBe),c&&t(Xd),p(C6),c&&t(wBe),c&&t(mr),p(M6),p(y6),p(w6),p(A6),p(L6),c&&t(ABe),c&&t(Qd),p(B6),c&&t(LBe),c&&t(fr),p(x6),p(R6),p(S6),p(P6),p($6),c&&t(BBe),c&&t(Jd),p(I6),c&&t(xBe),c&&t(gr),p(D6),p(N6),p(q6),p(G6),p(O6),c&&t(kBe),c&&t(Zd),p(X6),c&&t(RBe),c&&t(hr),p(V6),p(W6),p(Q6),p(H6),p(J6),c&&t(SBe),c&&t(rc),p(Y6),c&&t(PBe),c&&t(ur),p(K6),p(eA),p(oA),p(rA),p(tA),c&&t($Be),c&&t(sc),p(aA),c&&t(IBe),c&&t(pr),p(sA),p(lA),p(iA),p(dA),p(cA),c&&t(DBe),c&&t(ic),p(mA),c&&t(jBe),c&&t(_r),p(fA),p(hA),p(uA),p(pA),p(_A),c&&t(NBe),c&&t(mc),p(bA),c&&t(qBe),c&&t(br),p(vA),p(FA),p(CA),p(MA),p(yA),c&&t(GBe),c&&t(hc),p(wA),c&&t(OBe),c&&t(vr),p(AA),p(BA),p(xA),p(kA),p(RA),c&&t(XBe),c&&t(_c),p(SA),c&&t(VBe),c&&t(Tr),p(PA),p(IA),p(DA),p(jA),p(NA),c&&t(zBe),c&&t(Tc),p(qA),c&&t(WBe),c&&t(Fr),p(GA),p(XA),p(VA),p(zA),p(WA),c&&t(QBe),c&&t(Mc),p(QA),c&&t(HBe),c&&t(Cr),p(HA),p(JA),p(YA),p(KA),p(ZA),c&&t(UBe),c&&t(wc),p(e0),c&&t(JBe),c&&t(Mr),p(o0),p(t0),p(a0),p(s0),p(n0),c&&t(YBe),c&&t(Bc),p(l0),c&&t(KBe),c&&t(Er),p(i0),p(c0),p(m0),p(f0),p(g0),c&&t(ZBe),c&&t(Rc),p(h0),c&&t(exe),c&&t(yr),p(u0),p(_0),p(b0),p(v0),p(T0),c&&t(oxe),c&&t($c),p(F0),c&&t(rxe),c&&t(wr),p(C0),p(E0),p(y0),p(w0),p(A0),c&&t(txe),c&&t(jc),p(L0),c&&t(axe),c&&t(Ar),p(B0),p(k0),p(R0),p(S0),p(P0),c&&t(sxe),c&&t(Gc),p($0),c&&t(nxe),c&&t(Lr),p(I0),p(j0),p(N0),p(q0),p(G0),c&&t(lxe),c&&t(Vc),p(O0),c&&t(ixe),c&&t(Br),p(X0),p(z0),p(W0),p(Q0),p(H0),c&&t(dxe),c&&t(Qc),p(U0),c&&t(cxe),c&&t(xr),p(J0),p(K0),p(Z0),p(eL),p(oL),c&&t(mxe),c&&t(Jc),p(rL),c&&t(fxe),c&&t(kr),p(tL),p(sL),p(nL),p(lL),p(iL),c&&t(gxe),c&&t(Zc),p(dL),c&&t(hxe),c&&t(Rr),p(cL),p(fL),p(gL),p(hL),p(uL),c&&t(uxe),c&&t(rm),p(pL),c&&t(pxe),c&&t(Sr),p(_L),p(vL),p(TL),p(FL),p(CL),c&&t(_xe),c&&t(sm),p(ML),c&&t(bxe),c&&t(Pr),p(EL),p(wL),p(AL),p(LL),p(BL),c&&t(vxe),c&&t(im),p(xL),c&&t(Txe),c&&t($r),p(kL),p(SL),p(PL),p($L),p(IL),c&&t(Fxe),c&&t(mm),p(DL),c&&t(Cxe),c&&t(Ir),p(jL),p(qL),p(GL),p(OL),p(XL),c&&t(Mxe),c&&t(hm),p(VL),c&&t(Exe),c&&t(Dr),p(zL),p(QL),p(HL),p(UL),p(JL),c&&t(yxe),c&&t(_m),p(YL),c&&t(wxe),c&&t(jr),p(KL),p(e8),p(o8),p(r8),p(a8),c&&t(Axe),c&&t(Tm),p(s8),c&&t(Lxe),c&&t(Nr),p(n8),p(i8),p(d8),p(c8),p(m8)}}}const Svt={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function Pvt(Li,J,Be){let{fw:ie}=J;return Li.$$set=fe=>{"fw"in fe&&Be(0,ie=fe.fw)},[ie]}class Gvt extends yvt{constructor(J){super();wvt(this,J,Pvt,Rvt,Avt,{fw:0})}}export{Gvt as default,Svt as metadata};
