import{S as _pt,i as bpt,s as vpt,e as a,k as l,w as m,t as o,M as Tpt,c as s,d as t,m as i,a as n,x as f,h as r,b as c,F as e,g as b,y as g,q as h,o as u,B as p}from"../../chunks/vendor-4833417e.js";import{T as pAr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-44c5af16.js";import{C as w}from"../../chunks/CodeBlock-90ffda97.js";import{I as z}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-04a16537.js";function Fpt(yi){let J,Ae,ie,fe,to,ce,_e,Do,wi,Mm,na,Ai,Li,o5,Em,ye,io,Bi,Ss,r5,Ps,$s,t5,ki,Is,a5,xi,ym,$a;return{c(){J=a("p"),Ae=o("If your "),ie=a("code"),fe=o("NewModelConfig"),to=o(" is a subclass of "),ce=a("code"),_e=o("PretrainedConfig"),Do=o(`, make sure its
`),wi=a("code"),Mm=o("model_type"),na=o(" attribute is set to the same key you use when registering the config (here "),Ai=a("code"),Li=o('"new-model"'),o5=o(")."),Em=l(),ye=a("p"),io=o("Likewise, if your "),Bi=a("code"),Ss=o("NewModel"),r5=o(" is a subclass of "),Ps=a("a"),$s=o("PreTrainedModel"),t5=o(`, make sure its
`),ki=a("code"),Is=o("config_class"),a5=o(` attribute is set to the same class you use when registering the model (here
`),xi=a("code"),ym=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=s(co,"P",{});var ge=n(J);Ae=r(ge,"If your "),ie=s(ge,"CODE",{});var D7=n(ie);fe=r(D7,"NewModelConfig"),D7.forEach(t),to=r(ge," is a subclass of "),ce=s(ge,"CODE",{});var Ri=n(ce);_e=r(Ri,"PretrainedConfig"),Ri.forEach(t),Do=r(ge,`, make sure its
`),wi=s(ge,"CODE",{});var q7=n(wi);Mm=r(q7,"model_type"),q7.forEach(t),na=r(ge," attribute is set to the same key you use when registering the config (here "),Ai=s(ge,"CODE",{});var G7=n(Ai);Li=r(G7,'"new-model"'),G7.forEach(t),o5=r(ge,")."),ge.forEach(t),Em=i(co),ye=s(co,"P",{});var qo=n(ye);io=r(qo,"Likewise, if your "),Bi=s(qo,"CODE",{});var Ia=n(Bi);Ss=r(Ia,"NewModel"),Ia.forEach(t),r5=r(qo," is a subclass of "),Ps=s(qo,"A",{href:!0});var O7=n(Ps);$s=r(O7,"PreTrainedModel"),O7.forEach(t),t5=r(qo,`, make sure its
`),ki=s(qo,"CODE",{});var wm=n(ki);Is=r(wm,"config_class"),wm.forEach(t),a5=r(qo,` attribute is set to the same class you use when registering the model (here
`),xi=s(qo,"CODE",{});var X7=n(xi);ym=r(X7,"NewModelConfig"),X7.forEach(t),$a=r(qo,")."),qo.forEach(t),this.h()},h(){c(Ps,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Ae),e(J,ie),e(ie,fe),e(J,to),e(J,ce),e(ce,_e),e(J,Do),e(J,wi),e(wi,Mm),e(J,na),e(J,Ai),e(Ai,Li),e(J,o5),b(co,Em,ge),b(co,ye,ge),e(ye,io),e(ye,Bi),e(Bi,Ss),e(ye,r5),e(ye,Ps),e(Ps,$s),e(ye,t5),e(ye,ki),e(ki,Is),e(ye,a5),e(ye,xi),e(xi,ym),e(ye,$a)},d(co){co&&t(J),co&&t(Em),co&&t(ye)}}}function Cpt(yi){let J,Ae,ie,fe,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),fe=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Ae=r(_e,"Passing "),ie=s(_e,"CODE",{});var Do=n(ie);fe=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Ae),e(J,ie),e(ie,fe),e(J,to)},d(ce){ce&&t(J)}}}function Mpt(yi){let J,Ae,ie,fe,to;return{c(){J=a("p"),Ae=o("Passing "),ie=a("code"),fe=o("use_auth_token=True"),to=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Ae=r(_e,"Passing "),ie=s(_e,"CODE",{});var Do=n(ie);fe=r(Do,"use_auth_token=True"),Do.forEach(t),to=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Ae),e(J,ie),e(ie,fe),e(J,to)},d(ce){ce&&t(J)}}}function Ept(yi){let J,Ae,ie,fe,to,ce,_e,Do,wi,Mm,na,Ai,Li,o5,Em,ye,io,Bi,Ss,r5,Ps,$s,t5,ki,Is,a5,xi,ym,$a,co,ge,D7,Ri,q7,G7,qo,Ia,O7,wm,X7,fxe,t8e,Si,Am,$V,s5,gxe,IV,hxe,a8e,js,uxe,jV,pxe,_xe,NV,bxe,vxe,s8e,n5,n8e,z7,Txe,l8e,Lm,i8e,Pi,Bm,DV,l5,Fxe,qV,Cxe,d8e,Go,i5,Mxe,d5,Exe,V7,yxe,wxe,Axe,c5,Lxe,GV,Bxe,kxe,xxe,mo,m5,Rxe,OV,Sxe,Pxe,$i,$xe,XV,Ixe,jxe,zV,Nxe,Dxe,qxe,v,km,VV,Gxe,Oxe,W7,Xxe,zxe,Vxe,xm,WV,Wxe,Qxe,Q7,Hxe,Uxe,Jxe,Rm,QV,Yxe,Kxe,H7,Zxe,eRe,oRe,Sm,HV,rRe,tRe,U7,aRe,sRe,nRe,Pm,UV,lRe,iRe,J7,dRe,cRe,mRe,$m,JV,fRe,gRe,Y7,hRe,uRe,pRe,Im,YV,_Re,bRe,K7,vRe,TRe,FRe,jm,KV,CRe,MRe,Z7,ERe,yRe,wRe,Nm,ZV,ARe,LRe,e8,BRe,kRe,xRe,Dm,eW,RRe,SRe,o8,PRe,$Re,IRe,qm,oW,jRe,NRe,r8,DRe,qRe,GRe,Gm,rW,ORe,XRe,t8,zRe,VRe,WRe,Om,tW,QRe,HRe,a8,URe,JRe,YRe,Xm,aW,KRe,ZRe,s8,eSe,oSe,rSe,zm,sW,tSe,aSe,n8,sSe,nSe,lSe,Vm,nW,iSe,dSe,l8,cSe,mSe,fSe,Wm,lW,gSe,hSe,i8,uSe,pSe,_Se,Qm,iW,bSe,vSe,d8,TSe,FSe,CSe,Hm,dW,MSe,ESe,c8,ySe,wSe,ASe,Um,cW,LSe,BSe,m8,kSe,xSe,RSe,Jm,mW,SSe,PSe,f8,$Se,ISe,jSe,Ym,fW,NSe,DSe,g8,qSe,GSe,OSe,Km,gW,XSe,zSe,h8,VSe,WSe,QSe,Zm,hW,HSe,USe,u8,JSe,YSe,KSe,ef,uW,ZSe,ePe,p8,oPe,rPe,tPe,of,pW,aPe,sPe,_8,nPe,lPe,iPe,rf,_W,dPe,cPe,b8,mPe,fPe,gPe,tf,bW,hPe,uPe,v8,pPe,_Pe,bPe,af,vW,vPe,TPe,T8,FPe,CPe,MPe,sf,TW,EPe,yPe,F8,wPe,APe,LPe,nf,FW,BPe,kPe,C8,xPe,RPe,SPe,lf,CW,PPe,$Pe,M8,IPe,jPe,NPe,df,MW,DPe,qPe,E8,GPe,OPe,XPe,cf,EW,zPe,VPe,y8,WPe,QPe,HPe,mf,yW,UPe,JPe,w8,YPe,KPe,ZPe,ff,wW,e$e,o$e,A8,r$e,t$e,a$e,gf,AW,s$e,n$e,L8,l$e,i$e,d$e,hf,LW,c$e,m$e,B8,f$e,g$e,h$e,uf,BW,u$e,p$e,k8,_$e,b$e,v$e,pf,kW,T$e,F$e,x8,C$e,M$e,E$e,_f,xW,y$e,w$e,R8,A$e,L$e,B$e,bf,RW,k$e,x$e,S8,R$e,S$e,P$e,vf,SW,$$e,I$e,P8,j$e,N$e,D$e,Tf,PW,q$e,G$e,$8,O$e,X$e,z$e,Ff,$W,V$e,W$e,I8,Q$e,H$e,U$e,Cf,IW,J$e,Y$e,j8,K$e,Z$e,eIe,Mf,jW,oIe,rIe,N8,tIe,aIe,sIe,Ef,NW,nIe,lIe,D8,iIe,dIe,cIe,yf,DW,mIe,fIe,q8,gIe,hIe,uIe,wf,qW,pIe,_Ie,G8,bIe,vIe,TIe,Af,GW,FIe,CIe,O8,MIe,EIe,yIe,Lf,OW,wIe,AIe,X8,LIe,BIe,kIe,Bf,XW,xIe,RIe,z8,SIe,PIe,$Ie,kf,zW,IIe,jIe,V8,NIe,DIe,qIe,xf,VW,GIe,OIe,W8,XIe,zIe,VIe,Rf,WW,WIe,QIe,Q8,HIe,UIe,JIe,Sf,QW,YIe,KIe,H8,ZIe,eje,oje,Pf,HW,rje,tje,U8,aje,sje,nje,$f,UW,lje,ije,J8,dje,cje,mje,If,JW,fje,gje,Y8,hje,uje,pje,jf,YW,_je,bje,K8,vje,Tje,Fje,Nf,KW,Cje,Mje,Z8,Eje,yje,wje,Df,ZW,Aje,Lje,e9,Bje,kje,xje,qf,eQ,Rje,Sje,o9,Pje,$je,Ije,Gf,oQ,jje,Nje,r9,Dje,qje,Gje,Of,rQ,Oje,Xje,t9,zje,Vje,Wje,Xf,tQ,Qje,Hje,a9,Uje,Jje,Yje,zf,aQ,Kje,Zje,s9,eNe,oNe,rNe,Vf,sQ,tNe,aNe,n9,sNe,nNe,lNe,Wf,nQ,iNe,dNe,l9,cNe,mNe,fNe,Qf,lQ,gNe,hNe,i9,uNe,pNe,_Ne,Hf,iQ,bNe,vNe,d9,TNe,FNe,CNe,Uf,dQ,MNe,ENe,c9,yNe,wNe,ANe,Jf,cQ,LNe,BNe,m9,kNe,xNe,RNe,Yf,mQ,SNe,PNe,f9,$Ne,INe,jNe,Kf,fQ,NNe,DNe,g9,qNe,GNe,ONe,Zf,gQ,XNe,zNe,h9,VNe,WNe,QNe,eg,hQ,HNe,UNe,u9,JNe,YNe,KNe,og,uQ,ZNe,eDe,p9,oDe,rDe,tDe,rg,pQ,aDe,sDe,_9,nDe,lDe,iDe,tg,_Q,dDe,cDe,b9,mDe,fDe,gDe,ag,bQ,hDe,uDe,v9,pDe,_De,bDe,sg,vQ,vDe,TDe,T9,FDe,CDe,MDe,ng,TQ,EDe,yDe,F9,wDe,ADe,LDe,lg,FQ,BDe,kDe,C9,xDe,RDe,SDe,ig,CQ,PDe,$De,M9,IDe,jDe,NDe,dg,MQ,DDe,qDe,E9,GDe,ODe,XDe,cg,EQ,zDe,VDe,y9,WDe,QDe,HDe,mg,yQ,UDe,JDe,w9,YDe,KDe,ZDe,fg,wQ,eqe,oqe,A9,rqe,tqe,aqe,gg,AQ,sqe,nqe,L9,lqe,iqe,dqe,LQ,cqe,mqe,f5,fqe,hg,g5,gqe,BQ,hqe,c8e,Ii,ug,kQ,h5,uqe,xQ,pqe,m8e,Oo,u5,_qe,p5,bqe,B9,vqe,Tqe,Fqe,_5,Cqe,RQ,Mqe,Eqe,yqe,fo,b5,wqe,SQ,Aqe,Lqe,ja,Bqe,PQ,kqe,xqe,$Q,Rqe,Sqe,IQ,Pqe,$qe,Iqe,M,Ns,jQ,jqe,Nqe,k9,Dqe,qqe,x9,Gqe,Oqe,Xqe,Ds,NQ,zqe,Vqe,R9,Wqe,Qqe,S9,Hqe,Uqe,Jqe,qs,DQ,Yqe,Kqe,P9,Zqe,eGe,$9,oGe,rGe,tGe,pg,qQ,aGe,sGe,I9,nGe,lGe,iGe,Gs,GQ,dGe,cGe,j9,mGe,fGe,N9,gGe,hGe,uGe,_g,OQ,pGe,_Ge,D9,bGe,vGe,TGe,bg,XQ,FGe,CGe,q9,MGe,EGe,yGe,vg,zQ,wGe,AGe,G9,LGe,BGe,kGe,Os,VQ,xGe,RGe,O9,SGe,PGe,X9,$Ge,IGe,jGe,Xs,WQ,NGe,DGe,z9,qGe,GGe,V9,OGe,XGe,zGe,zs,QQ,VGe,WGe,W9,QGe,HGe,Q9,UGe,JGe,YGe,Tg,HQ,KGe,ZGe,H9,eOe,oOe,rOe,Fg,UQ,tOe,aOe,U9,sOe,nOe,lOe,Vs,JQ,iOe,dOe,J9,cOe,mOe,Y9,fOe,gOe,hOe,Cg,YQ,uOe,pOe,K9,_Oe,bOe,vOe,Ws,KQ,TOe,FOe,Z9,COe,MOe,eB,EOe,yOe,wOe,Qs,ZQ,AOe,LOe,oB,BOe,kOe,rB,xOe,ROe,SOe,Hs,eH,POe,$Oe,tB,IOe,jOe,oH,NOe,DOe,qOe,Mg,rH,GOe,OOe,aB,XOe,zOe,VOe,Us,tH,WOe,QOe,sB,HOe,UOe,nB,JOe,YOe,KOe,Eg,aH,ZOe,eXe,lB,oXe,rXe,tXe,Js,sH,aXe,sXe,iB,nXe,lXe,dB,iXe,dXe,cXe,Ys,nH,mXe,fXe,cB,gXe,hXe,mB,uXe,pXe,_Xe,Ks,lH,bXe,vXe,fB,TXe,FXe,gB,CXe,MXe,EXe,yg,iH,yXe,wXe,hB,AXe,LXe,BXe,Zs,dH,kXe,xXe,uB,RXe,SXe,pB,PXe,$Xe,IXe,wg,cH,jXe,NXe,_B,DXe,qXe,GXe,en,mH,OXe,XXe,bB,zXe,VXe,vB,WXe,QXe,HXe,on,fH,UXe,JXe,TB,YXe,KXe,FB,ZXe,eze,oze,rn,gH,rze,tze,CB,aze,sze,MB,nze,lze,ize,tn,hH,dze,cze,EB,mze,fze,yB,gze,hze,uze,Ag,uH,pze,_ze,wB,bze,vze,Tze,an,pH,Fze,Cze,AB,Mze,Eze,LB,yze,wze,Aze,sn,_H,Lze,Bze,BB,kze,xze,kB,Rze,Sze,Pze,nn,bH,$ze,Ize,xB,jze,Nze,RB,Dze,qze,Gze,ln,vH,Oze,Xze,SB,zze,Vze,PB,Wze,Qze,Hze,dn,TH,Uze,Jze,$B,Yze,Kze,IB,Zze,eVe,oVe,cn,FH,rVe,tVe,jB,aVe,sVe,NB,nVe,lVe,iVe,Lg,CH,dVe,cVe,DB,mVe,fVe,gVe,mn,MH,hVe,uVe,qB,pVe,_Ve,GB,bVe,vVe,TVe,Bg,EH,FVe,CVe,OB,MVe,EVe,yVe,kg,yH,wVe,AVe,XB,LVe,BVe,kVe,fn,wH,xVe,RVe,zB,SVe,PVe,VB,$Ve,IVe,jVe,gn,AH,NVe,DVe,WB,qVe,GVe,QB,OVe,XVe,zVe,xg,LH,VVe,WVe,HB,QVe,HVe,UVe,hn,BH,JVe,YVe,UB,KVe,ZVe,JB,eWe,oWe,rWe,un,kH,tWe,aWe,YB,sWe,nWe,KB,lWe,iWe,dWe,pn,xH,cWe,mWe,ZB,fWe,gWe,ek,hWe,uWe,pWe,_n,RH,_We,bWe,ok,vWe,TWe,rk,FWe,CWe,MWe,bn,SH,EWe,yWe,tk,wWe,AWe,ak,LWe,BWe,kWe,Rg,PH,xWe,RWe,sk,SWe,PWe,$We,Sg,$H,IWe,jWe,nk,NWe,DWe,qWe,Pg,IH,GWe,OWe,lk,XWe,zWe,VWe,$g,jH,WWe,QWe,ik,HWe,UWe,JWe,vn,NH,YWe,KWe,dk,ZWe,eQe,ck,oQe,rQe,tQe,Ig,DH,aQe,sQe,mk,nQe,lQe,iQe,Tn,qH,dQe,cQe,fk,mQe,fQe,gk,gQe,hQe,uQe,Fn,GH,pQe,_Qe,hk,bQe,vQe,uk,TQe,FQe,CQe,Cn,OH,MQe,EQe,pk,yQe,wQe,_k,AQe,LQe,BQe,Mn,XH,kQe,xQe,bk,RQe,SQe,vk,PQe,$Qe,IQe,En,zH,jQe,NQe,Tk,DQe,qQe,Fk,GQe,OQe,XQe,jg,VH,zQe,VQe,Ck,WQe,QQe,HQe,Ng,WH,UQe,JQe,Mk,YQe,KQe,ZQe,yn,QH,eHe,oHe,Ek,rHe,tHe,yk,aHe,sHe,nHe,wn,HH,lHe,iHe,wk,dHe,cHe,Ak,mHe,fHe,gHe,An,UH,hHe,uHe,Lk,pHe,_He,Bk,bHe,vHe,THe,Dg,JH,FHe,CHe,kk,MHe,EHe,yHe,qg,YH,wHe,AHe,xk,LHe,BHe,kHe,Gg,KH,xHe,RHe,Rk,SHe,PHe,$He,Og,ZH,IHe,jHe,Sk,NHe,DHe,qHe,Ln,eU,GHe,OHe,Pk,XHe,zHe,$k,VHe,WHe,QHe,Xg,oU,HHe,UHe,Ik,JHe,YHe,KHe,zg,rU,ZHe,eUe,jk,oUe,rUe,tUe,Bn,tU,aUe,sUe,Nk,nUe,lUe,Dk,iUe,dUe,cUe,kn,aU,mUe,fUe,qk,gUe,hUe,Gk,uUe,pUe,_Ue,sU,bUe,vUe,v5,TUe,Vg,T5,FUe,nU,CUe,f8e,ji,Wg,lU,F5,MUe,iU,EUe,g8e,Xo,C5,yUe,M5,wUe,Ok,AUe,LUe,BUe,E5,kUe,dU,xUe,RUe,SUe,Le,y5,PUe,cU,$Ue,IUe,Na,jUe,mU,NUe,DUe,fU,qUe,GUe,gU,OUe,XUe,zUe,ne,Qg,hU,VUe,WUe,Xk,QUe,HUe,UUe,Hg,uU,JUe,YUe,zk,KUe,ZUe,eJe,Ug,pU,oJe,rJe,Vk,tJe,aJe,sJe,Jg,_U,nJe,lJe,Wk,iJe,dJe,cJe,Yg,bU,mJe,fJe,Qk,gJe,hJe,uJe,Kg,vU,pJe,_Je,Hk,bJe,vJe,TJe,Zg,TU,FJe,CJe,Uk,MJe,EJe,yJe,eh,FU,wJe,AJe,Jk,LJe,BJe,kJe,oh,CU,xJe,RJe,Yk,SJe,PJe,$Je,rh,MU,IJe,jJe,Kk,NJe,DJe,qJe,th,EU,GJe,OJe,Zk,XJe,zJe,VJe,ah,yU,WJe,QJe,ex,HJe,UJe,JJe,sh,wU,YJe,KJe,ox,ZJe,eYe,oYe,nh,AU,rYe,tYe,rx,aYe,sYe,nYe,lh,LU,lYe,iYe,tx,dYe,cYe,mYe,ih,fYe,BU,gYe,hYe,w5,uYe,dh,A5,pYe,kU,_Ye,h8e,Ni,ch,xU,L5,bYe,RU,vYe,u8e,zo,B5,TYe,k5,FYe,ax,CYe,MYe,EYe,x5,yYe,SU,wYe,AYe,LYe,Be,R5,BYe,PU,kYe,xYe,Di,RYe,$U,SYe,PYe,IU,$Ye,IYe,jYe,we,mh,jU,NYe,DYe,sx,qYe,GYe,OYe,fh,NU,XYe,zYe,nx,VYe,WYe,QYe,gh,DU,HYe,UYe,lx,JYe,YYe,KYe,hh,qU,ZYe,eKe,ix,oKe,rKe,tKe,uh,GU,aKe,sKe,dx,nKe,lKe,iKe,ph,OU,dKe,cKe,cx,mKe,fKe,gKe,_h,XU,hKe,uKe,mx,pKe,_Ke,bKe,bh,zU,vKe,TKe,fx,FKe,CKe,MKe,vh,EKe,VU,yKe,wKe,S5,AKe,Th,P5,LKe,WU,BKe,p8e,qi,Fh,QU,$5,kKe,HU,xKe,_8e,Vo,I5,RKe,Gi,SKe,UU,PKe,$Ke,JU,IKe,jKe,NKe,j5,DKe,YU,qKe,GKe,OKe,Nr,N5,XKe,KU,zKe,VKe,Oi,WKe,ZU,QKe,HKe,eJ,UKe,JKe,YKe,oJ,KKe,ZKe,D5,eZe,ke,q5,oZe,rJ,rZe,tZe,Da,aZe,tJ,sZe,nZe,aJ,lZe,iZe,sJ,dZe,cZe,mZe,F,Ch,nJ,fZe,gZe,gx,hZe,uZe,pZe,Mh,lJ,_Ze,bZe,hx,vZe,TZe,FZe,Eh,iJ,CZe,MZe,ux,EZe,yZe,wZe,yh,dJ,AZe,LZe,px,BZe,kZe,xZe,wh,cJ,RZe,SZe,_x,PZe,$Ze,IZe,Ah,mJ,jZe,NZe,bx,DZe,qZe,GZe,Lh,fJ,OZe,XZe,vx,zZe,VZe,WZe,Bh,gJ,QZe,HZe,Tx,UZe,JZe,YZe,kh,hJ,KZe,ZZe,Fx,eeo,oeo,reo,xh,uJ,teo,aeo,Cx,seo,neo,leo,Rh,pJ,ieo,deo,Mx,ceo,meo,feo,Sh,_J,geo,heo,Ex,ueo,peo,_eo,Ph,bJ,beo,veo,yx,Teo,Feo,Ceo,$h,vJ,Meo,Eeo,wx,yeo,weo,Aeo,Ih,TJ,Leo,Beo,Ax,keo,xeo,Reo,jh,FJ,Seo,Peo,Lx,$eo,Ieo,jeo,Nh,CJ,Neo,Deo,Bx,qeo,Geo,Oeo,Dh,MJ,Xeo,zeo,kx,Veo,Weo,Qeo,qh,EJ,Heo,Ueo,xx,Jeo,Yeo,Keo,Gh,yJ,Zeo,eoo,Rx,ooo,roo,too,Oh,wJ,aoo,soo,Sx,noo,loo,ioo,Xh,AJ,doo,coo,Px,moo,foo,goo,zh,LJ,hoo,uoo,$x,poo,_oo,boo,Vh,BJ,voo,Too,Ix,Foo,Coo,Moo,Wh,kJ,Eoo,yoo,jx,woo,Aoo,Loo,xn,xJ,Boo,koo,Nx,xoo,Roo,Dx,Soo,Poo,$oo,Qh,RJ,Ioo,joo,qx,Noo,Doo,qoo,Hh,SJ,Goo,Ooo,Gx,Xoo,zoo,Voo,Uh,PJ,Woo,Qoo,Ox,Hoo,Uoo,Joo,Jh,$J,Yoo,Koo,Xx,Zoo,ero,oro,Yh,IJ,rro,tro,zx,aro,sro,nro,Kh,jJ,lro,iro,Vx,dro,cro,mro,Zh,NJ,fro,gro,Wx,hro,uro,pro,eu,DJ,_ro,bro,Qx,vro,Tro,Fro,ou,qJ,Cro,Mro,Hx,Ero,yro,wro,ru,GJ,Aro,Lro,Ux,Bro,kro,xro,tu,OJ,Rro,Sro,Jx,Pro,$ro,Iro,au,XJ,jro,Nro,Yx,Dro,qro,Gro,su,zJ,Oro,Xro,Kx,zro,Vro,Wro,nu,VJ,Qro,Hro,Zx,Uro,Jro,Yro,lu,WJ,Kro,Zro,eR,eto,oto,rto,iu,QJ,tto,ato,oR,sto,nto,lto,du,HJ,ito,dto,rR,cto,mto,fto,cu,UJ,gto,hto,tR,uto,pto,_to,mu,JJ,bto,vto,aR,Tto,Fto,Cto,fu,YJ,Mto,Eto,sR,yto,wto,Ato,gu,KJ,Lto,Bto,nR,kto,xto,Rto,hu,ZJ,Sto,Pto,lR,$to,Ito,jto,uu,eY,Nto,Dto,iR,qto,Gto,Oto,pu,oY,Xto,zto,dR,Vto,Wto,Qto,_u,rY,Hto,Uto,cR,Jto,Yto,Kto,bu,tY,Zto,eao,mR,oao,rao,tao,vu,aY,aao,sao,fR,nao,lao,iao,Tu,sY,dao,cao,gR,mao,fao,gao,Fu,nY,hao,uao,hR,pao,_ao,bao,Cu,lY,vao,Tao,uR,Fao,Cao,Mao,Mu,iY,Eao,yao,pR,wao,Aao,Lao,Eu,dY,Bao,kao,_R,xao,Rao,Sao,yu,cY,Pao,$ao,bR,Iao,jao,Nao,wu,mY,Dao,qao,vR,Gao,Oao,Xao,Au,fY,zao,Vao,TR,Wao,Qao,Hao,Lu,gY,Uao,Jao,FR,Yao,Kao,Zao,Bu,hY,eso,oso,CR,rso,tso,aso,ku,uY,sso,nso,MR,lso,iso,dso,xu,pY,cso,mso,ER,fso,gso,hso,Ru,_Y,uso,pso,yR,_so,bso,vso,Su,bY,Tso,Fso,wR,Cso,Mso,Eso,Pu,vY,yso,wso,AR,Aso,Lso,Bso,$u,TY,kso,xso,LR,Rso,Sso,Pso,Iu,FY,$so,Iso,BR,jso,Nso,Dso,ju,CY,qso,Gso,kR,Oso,Xso,zso,Nu,MY,Vso,Wso,xR,Qso,Hso,Uso,Du,EY,Jso,Yso,RR,Kso,Zso,eno,qu,yY,ono,rno,SR,tno,ano,sno,Gu,wY,nno,lno,PR,ino,dno,cno,Ou,AY,mno,fno,$R,gno,hno,uno,Xu,LY,pno,_no,IR,bno,vno,Tno,zu,BY,Fno,Cno,jR,Mno,Eno,yno,Vu,kY,wno,Ano,NR,Lno,Bno,kno,Wu,xY,xno,Rno,DR,Sno,Pno,$no,Qu,RY,Ino,jno,qR,Nno,Dno,qno,Hu,SY,Gno,Ono,GR,Xno,zno,Vno,Uu,PY,Wno,Qno,OR,Hno,Uno,Jno,Ju,$Y,Yno,Kno,XR,Zno,elo,olo,Yu,rlo,IY,tlo,alo,jY,slo,nlo,NY,llo,ilo,G5,b8e,Xi,Ku,DY,O5,dlo,qY,clo,v8e,Wo,X5,mlo,zi,flo,GY,glo,hlo,OY,ulo,plo,_lo,z5,blo,XY,vlo,Tlo,Flo,Dr,V5,Clo,zY,Mlo,Elo,Vi,ylo,VY,wlo,Alo,WY,Llo,Blo,klo,QY,xlo,Rlo,W5,Slo,xe,Q5,Plo,HY,$lo,Ilo,qa,jlo,UY,Nlo,Dlo,JY,qlo,Glo,YY,Olo,Xlo,zlo,x,Zu,KY,Vlo,Wlo,zR,Qlo,Hlo,Ulo,ep,ZY,Jlo,Ylo,VR,Klo,Zlo,eio,op,eK,oio,rio,WR,tio,aio,sio,rp,oK,nio,lio,QR,iio,dio,cio,tp,rK,mio,fio,HR,gio,hio,uio,ap,tK,pio,_io,UR,bio,vio,Tio,sp,aK,Fio,Cio,JR,Mio,Eio,yio,np,sK,wio,Aio,YR,Lio,Bio,kio,lp,nK,xio,Rio,KR,Sio,Pio,$io,ip,lK,Iio,jio,ZR,Nio,Dio,qio,dp,iK,Gio,Oio,eS,Xio,zio,Vio,cp,dK,Wio,Qio,oS,Hio,Uio,Jio,mp,cK,Yio,Kio,rS,Zio,edo,odo,fp,mK,rdo,tdo,tS,ado,sdo,ndo,gp,fK,ldo,ido,aS,ddo,cdo,mdo,hp,gK,fdo,gdo,sS,hdo,udo,pdo,up,hK,_do,bdo,nS,vdo,Tdo,Fdo,pp,uK,Cdo,Mdo,lS,Edo,ydo,wdo,_p,pK,Ado,Ldo,iS,Bdo,kdo,xdo,bp,_K,Rdo,Sdo,dS,Pdo,$do,Ido,vp,bK,jdo,Ndo,cS,Ddo,qdo,Gdo,Tp,vK,Odo,Xdo,mS,zdo,Vdo,Wdo,Fp,TK,Qdo,Hdo,fS,Udo,Jdo,Ydo,Cp,FK,Kdo,Zdo,gS,eco,oco,rco,Mp,CK,tco,aco,hS,sco,nco,lco,Ep,MK,ico,dco,uS,cco,mco,fco,yp,EK,gco,hco,pS,uco,pco,_co,wp,yK,bco,vco,_S,Tco,Fco,Cco,Ap,wK,Mco,Eco,bS,yco,wco,Aco,Lp,AK,Lco,Bco,vS,kco,xco,Rco,Bp,LK,Sco,Pco,TS,$co,Ico,jco,kp,BK,Nco,Dco,FS,qco,Gco,Oco,xp,kK,Xco,zco,CS,Vco,Wco,Qco,Rp,xK,Hco,Uco,MS,Jco,Yco,Kco,Sp,RK,Zco,emo,ES,omo,rmo,tmo,Pp,SK,amo,smo,yS,nmo,lmo,imo,$p,PK,dmo,cmo,wS,mmo,fmo,gmo,Ip,$K,hmo,umo,AS,pmo,_mo,bmo,jp,vmo,IK,Tmo,Fmo,jK,Cmo,Mmo,NK,Emo,ymo,H5,T8e,Wi,Np,DK,U5,wmo,qK,Amo,F8e,Qo,J5,Lmo,Qi,Bmo,GK,kmo,xmo,OK,Rmo,Smo,Pmo,Y5,$mo,XK,Imo,jmo,Nmo,qr,K5,Dmo,zK,qmo,Gmo,Hi,Omo,VK,Xmo,zmo,WK,Vmo,Wmo,Qmo,QK,Hmo,Umo,Z5,Jmo,Re,ey,Ymo,HK,Kmo,Zmo,Ga,efo,UK,ofo,rfo,JK,tfo,afo,YK,sfo,nfo,lfo,$,Dp,KK,ifo,dfo,LS,cfo,mfo,ffo,qp,ZK,gfo,hfo,BS,ufo,pfo,_fo,Gp,eZ,bfo,vfo,kS,Tfo,Ffo,Cfo,Op,oZ,Mfo,Efo,xS,yfo,wfo,Afo,Xp,rZ,Lfo,Bfo,RS,kfo,xfo,Rfo,zp,tZ,Sfo,Pfo,SS,$fo,Ifo,jfo,Vp,aZ,Nfo,Dfo,PS,qfo,Gfo,Ofo,Wp,sZ,Xfo,zfo,$S,Vfo,Wfo,Qfo,Qp,nZ,Hfo,Ufo,IS,Jfo,Yfo,Kfo,Hp,lZ,Zfo,ego,jS,ogo,rgo,tgo,Up,iZ,ago,sgo,NS,ngo,lgo,igo,Jp,dZ,dgo,cgo,DS,mgo,fgo,ggo,Yp,cZ,hgo,ugo,qS,pgo,_go,bgo,Kp,mZ,vgo,Tgo,GS,Fgo,Cgo,Mgo,Zp,fZ,Ego,ygo,OS,wgo,Ago,Lgo,e_,gZ,Bgo,kgo,XS,xgo,Rgo,Sgo,o_,hZ,Pgo,$go,zS,Igo,jgo,Ngo,r_,uZ,Dgo,qgo,VS,Ggo,Ogo,Xgo,t_,pZ,zgo,Vgo,WS,Wgo,Qgo,Hgo,a_,_Z,Ugo,Jgo,QS,Ygo,Kgo,Zgo,s_,bZ,eho,oho,HS,rho,tho,aho,n_,vZ,sho,nho,US,lho,iho,dho,l_,TZ,cho,mho,JS,fho,gho,hho,i_,FZ,uho,pho,YS,_ho,bho,vho,d_,CZ,Tho,Fho,KS,Cho,Mho,Eho,c_,MZ,yho,who,ZS,Aho,Lho,Bho,m_,EZ,kho,xho,eP,Rho,Sho,Pho,f_,yZ,$ho,Iho,oP,jho,Nho,Dho,g_,wZ,qho,Gho,rP,Oho,Xho,zho,h_,AZ,Vho,Who,tP,Qho,Hho,Uho,u_,LZ,Jho,Yho,aP,Kho,Zho,euo,p_,BZ,ouo,ruo,sP,tuo,auo,suo,__,kZ,nuo,luo,nP,iuo,duo,cuo,b_,xZ,muo,fuo,lP,guo,huo,uuo,v_,puo,RZ,_uo,buo,SZ,vuo,Tuo,PZ,Fuo,Cuo,oy,C8e,Ui,T_,$Z,ry,Muo,IZ,Euo,M8e,Ho,ty,yuo,Ji,wuo,jZ,Auo,Luo,NZ,Buo,kuo,xuo,ay,Ruo,DZ,Suo,Puo,$uo,Gr,sy,Iuo,qZ,juo,Nuo,Yi,Duo,GZ,quo,Guo,OZ,Ouo,Xuo,zuo,XZ,Vuo,Wuo,ny,Quo,Se,ly,Huo,zZ,Uuo,Juo,Oa,Yuo,VZ,Kuo,Zuo,WZ,epo,opo,QZ,rpo,tpo,apo,I,F_,HZ,spo,npo,iP,lpo,ipo,dpo,C_,UZ,cpo,mpo,dP,fpo,gpo,hpo,M_,JZ,upo,ppo,cP,_po,bpo,vpo,E_,YZ,Tpo,Fpo,mP,Cpo,Mpo,Epo,y_,KZ,ypo,wpo,fP,Apo,Lpo,Bpo,w_,ZZ,kpo,xpo,gP,Rpo,Spo,Ppo,A_,eee,$po,Ipo,hP,jpo,Npo,Dpo,L_,oee,qpo,Gpo,uP,Opo,Xpo,zpo,B_,ree,Vpo,Wpo,pP,Qpo,Hpo,Upo,k_,tee,Jpo,Ypo,_P,Kpo,Zpo,e_o,x_,aee,o_o,r_o,bP,t_o,a_o,s_o,R_,see,n_o,l_o,vP,i_o,d_o,c_o,S_,nee,m_o,f_o,TP,g_o,h_o,u_o,P_,lee,p_o,__o,FP,b_o,v_o,T_o,$_,iee,F_o,C_o,CP,M_o,E_o,y_o,I_,dee,w_o,A_o,MP,L_o,B_o,k_o,j_,cee,x_o,R_o,EP,S_o,P_o,$_o,N_,mee,I_o,j_o,yP,N_o,D_o,q_o,D_,fee,G_o,O_o,wP,X_o,z_o,V_o,q_,gee,W_o,Q_o,AP,H_o,U_o,J_o,G_,hee,Y_o,K_o,LP,Z_o,ebo,obo,O_,uee,rbo,tbo,BP,abo,sbo,nbo,X_,pee,lbo,ibo,kP,dbo,cbo,mbo,z_,_ee,fbo,gbo,xP,hbo,ubo,pbo,V_,bee,_bo,bbo,RP,vbo,Tbo,Fbo,W_,vee,Cbo,Mbo,SP,Ebo,ybo,wbo,Q_,Tee,Abo,Lbo,PP,Bbo,kbo,xbo,H_,Fee,Rbo,Sbo,$P,Pbo,$bo,Ibo,U_,Cee,jbo,Nbo,IP,Dbo,qbo,Gbo,J_,Mee,Obo,Xbo,Eee,zbo,Vbo,Wbo,Y_,yee,Qbo,Hbo,jP,Ubo,Jbo,Ybo,K_,wee,Kbo,Zbo,NP,e2o,o2o,r2o,Z_,Aee,t2o,a2o,DP,s2o,n2o,l2o,eb,Lee,i2o,d2o,qP,c2o,m2o,f2o,ob,g2o,Bee,h2o,u2o,kee,p2o,_2o,xee,b2o,v2o,iy,E8e,Ki,rb,Ree,dy,T2o,See,F2o,y8e,Uo,cy,C2o,Zi,M2o,Pee,E2o,y2o,$ee,w2o,A2o,L2o,my,B2o,Iee,k2o,x2o,R2o,Or,fy,S2o,jee,P2o,$2o,ed,I2o,Nee,j2o,N2o,Dee,D2o,q2o,G2o,qee,O2o,X2o,gy,z2o,Pe,hy,V2o,Gee,W2o,Q2o,Xa,H2o,Oee,U2o,J2o,Xee,Y2o,K2o,zee,Z2o,evo,ovo,ae,tb,Vee,rvo,tvo,GP,avo,svo,nvo,ab,Wee,lvo,ivo,OP,dvo,cvo,mvo,sb,Qee,fvo,gvo,XP,hvo,uvo,pvo,nb,Hee,_vo,bvo,zP,vvo,Tvo,Fvo,lb,Uee,Cvo,Mvo,VP,Evo,yvo,wvo,ib,Jee,Avo,Lvo,WP,Bvo,kvo,xvo,db,Yee,Rvo,Svo,QP,Pvo,$vo,Ivo,cb,Kee,jvo,Nvo,HP,Dvo,qvo,Gvo,mb,Zee,Ovo,Xvo,UP,zvo,Vvo,Wvo,fb,eoe,Qvo,Hvo,JP,Uvo,Jvo,Yvo,gb,ooe,Kvo,Zvo,YP,eTo,oTo,rTo,hb,roe,tTo,aTo,KP,sTo,nTo,lTo,ub,toe,iTo,dTo,ZP,cTo,mTo,fTo,pb,aoe,gTo,hTo,e$,uTo,pTo,_To,_b,soe,bTo,vTo,o$,TTo,FTo,CTo,bb,noe,MTo,ETo,r$,yTo,wTo,ATo,vb,LTo,loe,BTo,kTo,ioe,xTo,RTo,doe,STo,PTo,uy,w8e,od,Tb,coe,py,$To,moe,ITo,A8e,Jo,_y,jTo,rd,NTo,foe,DTo,qTo,goe,GTo,OTo,XTo,by,zTo,hoe,VTo,WTo,QTo,Xr,vy,HTo,uoe,UTo,JTo,td,YTo,poe,KTo,ZTo,_oe,e1o,o1o,r1o,boe,t1o,a1o,Ty,s1o,$e,Fy,n1o,voe,l1o,i1o,za,d1o,Toe,c1o,m1o,Foe,f1o,g1o,Coe,h1o,u1o,p1o,A,Fb,Moe,_1o,b1o,t$,v1o,T1o,F1o,Cb,Eoe,C1o,M1o,a$,E1o,y1o,w1o,Mb,yoe,A1o,L1o,s$,B1o,k1o,x1o,Eb,woe,R1o,S1o,n$,P1o,$1o,I1o,yb,Aoe,j1o,N1o,l$,D1o,q1o,G1o,wb,Loe,O1o,X1o,i$,z1o,V1o,W1o,Ab,Boe,Q1o,H1o,d$,U1o,J1o,Y1o,Lb,koe,K1o,Z1o,c$,eFo,oFo,rFo,Bb,xoe,tFo,aFo,m$,sFo,nFo,lFo,kb,Roe,iFo,dFo,f$,cFo,mFo,fFo,xb,Soe,gFo,hFo,g$,uFo,pFo,_Fo,Rb,Poe,bFo,vFo,h$,TFo,FFo,CFo,Sb,$oe,MFo,EFo,u$,yFo,wFo,AFo,Pb,Ioe,LFo,BFo,p$,kFo,xFo,RFo,$b,joe,SFo,PFo,_$,$Fo,IFo,jFo,Ib,Noe,NFo,DFo,b$,qFo,GFo,OFo,jb,Doe,XFo,zFo,v$,VFo,WFo,QFo,Nb,qoe,HFo,UFo,T$,JFo,YFo,KFo,Db,Goe,ZFo,eCo,F$,oCo,rCo,tCo,qb,Ooe,aCo,sCo,C$,nCo,lCo,iCo,Gb,Xoe,dCo,cCo,M$,mCo,fCo,gCo,Ob,zoe,hCo,uCo,E$,pCo,_Co,bCo,Xb,Voe,vCo,TCo,y$,FCo,CCo,MCo,zb,Woe,ECo,yCo,w$,wCo,ACo,LCo,Vb,Qoe,BCo,kCo,A$,xCo,RCo,SCo,Wb,Hoe,PCo,$Co,L$,ICo,jCo,NCo,Qb,Uoe,DCo,qCo,B$,GCo,OCo,XCo,Hb,Joe,zCo,VCo,k$,WCo,QCo,HCo,Ub,Yoe,UCo,JCo,x$,YCo,KCo,ZCo,Jb,Koe,e4o,o4o,R$,r4o,t4o,a4o,Yb,Zoe,s4o,n4o,S$,l4o,i4o,d4o,Kb,ere,c4o,m4o,P$,f4o,g4o,h4o,Zb,ore,u4o,p4o,$$,_4o,b4o,v4o,e2,rre,T4o,F4o,I$,C4o,M4o,E4o,o2,tre,y4o,w4o,j$,A4o,L4o,B4o,r2,are,k4o,x4o,N$,R4o,S4o,P4o,t2,sre,$4o,I4o,D$,j4o,N4o,D4o,a2,nre,q4o,G4o,q$,O4o,X4o,z4o,s2,lre,V4o,W4o,G$,Q4o,H4o,U4o,n2,ire,J4o,Y4o,O$,K4o,Z4o,eMo,l2,dre,oMo,rMo,X$,tMo,aMo,sMo,i2,cre,nMo,lMo,z$,iMo,dMo,cMo,d2,mre,mMo,fMo,V$,gMo,hMo,uMo,c2,fre,pMo,_Mo,W$,bMo,vMo,TMo,m2,gre,FMo,CMo,Q$,MMo,EMo,yMo,f2,wMo,hre,AMo,LMo,ure,BMo,kMo,pre,xMo,RMo,Cy,L8e,ad,g2,_re,My,SMo,bre,PMo,B8e,Yo,Ey,$Mo,sd,IMo,vre,jMo,NMo,Tre,DMo,qMo,GMo,yy,OMo,Fre,XMo,zMo,VMo,zr,wy,WMo,Cre,QMo,HMo,nd,UMo,Mre,JMo,YMo,Ere,KMo,ZMo,eEo,yre,oEo,rEo,Ay,tEo,Ie,Ly,aEo,wre,sEo,nEo,Va,lEo,Are,iEo,dEo,Lre,cEo,mEo,Bre,fEo,gEo,hEo,G,h2,kre,uEo,pEo,H$,_Eo,bEo,vEo,u2,xre,TEo,FEo,U$,CEo,MEo,EEo,p2,Rre,yEo,wEo,J$,AEo,LEo,BEo,_2,Sre,kEo,xEo,Y$,REo,SEo,PEo,b2,Pre,$Eo,IEo,K$,jEo,NEo,DEo,v2,$re,qEo,GEo,Z$,OEo,XEo,zEo,T2,Ire,VEo,WEo,eI,QEo,HEo,UEo,F2,jre,JEo,YEo,oI,KEo,ZEo,e3o,C2,Nre,o3o,r3o,rI,t3o,a3o,s3o,M2,Dre,n3o,l3o,tI,i3o,d3o,c3o,E2,qre,m3o,f3o,aI,g3o,h3o,u3o,y2,Gre,p3o,_3o,sI,b3o,v3o,T3o,w2,Ore,F3o,C3o,nI,M3o,E3o,y3o,A2,Xre,w3o,A3o,lI,L3o,B3o,k3o,L2,zre,x3o,R3o,iI,S3o,P3o,$3o,B2,Vre,I3o,j3o,dI,N3o,D3o,q3o,k2,Wre,G3o,O3o,cI,X3o,z3o,V3o,x2,Qre,W3o,Q3o,mI,H3o,U3o,J3o,R2,Hre,Y3o,K3o,fI,Z3o,e5o,o5o,S2,Ure,r5o,t5o,gI,a5o,s5o,n5o,P2,Jre,l5o,i5o,hI,d5o,c5o,m5o,$2,Yre,f5o,g5o,uI,h5o,u5o,p5o,I2,Kre,_5o,b5o,pI,v5o,T5o,F5o,j2,Zre,C5o,M5o,_I,E5o,y5o,w5o,N2,ete,A5o,L5o,bI,B5o,k5o,x5o,D2,ote,R5o,S5o,vI,P5o,$5o,I5o,q2,rte,j5o,N5o,TI,D5o,q5o,G5o,G2,O5o,tte,X5o,z5o,ate,V5o,W5o,ste,Q5o,H5o,By,k8e,ld,O2,nte,ky,U5o,lte,J5o,x8e,Ko,xy,Y5o,id,K5o,ite,Z5o,eyo,dte,oyo,ryo,tyo,Ry,ayo,cte,syo,nyo,lyo,Vr,Sy,iyo,mte,dyo,cyo,dd,myo,fte,fyo,gyo,gte,hyo,uyo,pyo,hte,_yo,byo,Py,vyo,je,$y,Tyo,ute,Fyo,Cyo,Wa,Myo,pte,Eyo,yyo,_te,wyo,Ayo,bte,Lyo,Byo,kyo,sa,X2,vte,xyo,Ryo,FI,Syo,Pyo,$yo,z2,Tte,Iyo,jyo,CI,Nyo,Dyo,qyo,V2,Fte,Gyo,Oyo,MI,Xyo,zyo,Vyo,W2,Cte,Wyo,Qyo,EI,Hyo,Uyo,Jyo,Q2,Mte,Yyo,Kyo,yI,Zyo,ewo,owo,H2,rwo,Ete,two,awo,yte,swo,nwo,wte,lwo,iwo,Iy,R8e,cd,U2,Ate,jy,dwo,Lte,cwo,S8e,Zo,Ny,mwo,md,fwo,Bte,gwo,hwo,kte,uwo,pwo,_wo,Dy,bwo,xte,vwo,Two,Fwo,Wr,qy,Cwo,Rte,Mwo,Ewo,fd,ywo,Ste,wwo,Awo,Pte,Lwo,Bwo,kwo,$te,xwo,Rwo,Gy,Swo,Ne,Oy,Pwo,Ite,$wo,Iwo,Qa,jwo,jte,Nwo,Dwo,Nte,qwo,Gwo,Dte,Owo,Xwo,zwo,D,J2,qte,Vwo,Wwo,wI,Qwo,Hwo,Uwo,Y2,Gte,Jwo,Ywo,AI,Kwo,Zwo,eAo,K2,Ote,oAo,rAo,LI,tAo,aAo,sAo,Z2,Xte,nAo,lAo,BI,iAo,dAo,cAo,ev,zte,mAo,fAo,kI,gAo,hAo,uAo,ov,Vte,pAo,_Ao,xI,bAo,vAo,TAo,rv,Wte,FAo,CAo,RI,MAo,EAo,yAo,tv,Qte,wAo,AAo,SI,LAo,BAo,kAo,av,Hte,xAo,RAo,PI,SAo,PAo,$Ao,sv,Ute,IAo,jAo,$I,NAo,DAo,qAo,nv,Jte,GAo,OAo,II,XAo,zAo,VAo,lv,Yte,WAo,QAo,jI,HAo,UAo,JAo,iv,Kte,YAo,KAo,NI,ZAo,e6o,o6o,dv,Zte,r6o,t6o,DI,a6o,s6o,n6o,cv,eae,l6o,i6o,qI,d6o,c6o,m6o,mv,oae,f6o,g6o,GI,h6o,u6o,p6o,fv,rae,_6o,b6o,OI,v6o,T6o,F6o,gv,tae,C6o,M6o,XI,E6o,y6o,w6o,hv,aae,A6o,L6o,zI,B6o,k6o,x6o,uv,sae,R6o,S6o,VI,P6o,$6o,I6o,pv,nae,j6o,N6o,WI,D6o,q6o,G6o,_v,lae,O6o,X6o,QI,z6o,V6o,W6o,bv,iae,Q6o,H6o,HI,U6o,J6o,Y6o,vv,dae,K6o,Z6o,UI,e0o,o0o,r0o,Tv,cae,t0o,a0o,JI,s0o,n0o,l0o,Fv,mae,i0o,d0o,YI,c0o,m0o,f0o,Cv,fae,g0o,h0o,KI,u0o,p0o,_0o,Mv,gae,b0o,v0o,ZI,T0o,F0o,C0o,Ev,hae,M0o,E0o,ej,y0o,w0o,A0o,yv,uae,L0o,B0o,oj,k0o,x0o,R0o,wv,pae,S0o,P0o,rj,$0o,I0o,j0o,Av,_ae,N0o,D0o,tj,q0o,G0o,O0o,Lv,X0o,bae,z0o,V0o,vae,W0o,Q0o,Tae,H0o,U0o,Xy,P8e,gd,Bv,Fae,zy,J0o,Cae,Y0o,$8e,er,Vy,K0o,hd,Z0o,Mae,eLo,oLo,Eae,rLo,tLo,aLo,Wy,sLo,yae,nLo,lLo,iLo,Qr,Qy,dLo,wae,cLo,mLo,ud,fLo,Aae,gLo,hLo,Lae,uLo,pLo,_Lo,Bae,bLo,vLo,Hy,TLo,De,Uy,FLo,kae,CLo,MLo,Ha,ELo,xae,yLo,wLo,Rae,ALo,LLo,Sae,BLo,kLo,xLo,R,kv,Pae,RLo,SLo,aj,PLo,$Lo,ILo,xv,$ae,jLo,NLo,sj,DLo,qLo,GLo,Rv,Iae,OLo,XLo,nj,zLo,VLo,WLo,Sv,jae,QLo,HLo,lj,ULo,JLo,YLo,Pv,Nae,KLo,ZLo,ij,e7o,o7o,r7o,$v,Dae,t7o,a7o,dj,s7o,n7o,l7o,Iv,qae,i7o,d7o,cj,c7o,m7o,f7o,jv,Gae,g7o,h7o,mj,u7o,p7o,_7o,Nv,Oae,b7o,v7o,fj,T7o,F7o,C7o,Dv,Xae,M7o,E7o,gj,y7o,w7o,A7o,qv,zae,L7o,B7o,hj,k7o,x7o,R7o,Gv,Vae,S7o,P7o,uj,$7o,I7o,j7o,Ov,Wae,N7o,D7o,pj,q7o,G7o,O7o,Xv,Qae,X7o,z7o,_j,V7o,W7o,Q7o,zv,Hae,H7o,U7o,bj,J7o,Y7o,K7o,Vv,Uae,Z7o,e8o,vj,o8o,r8o,t8o,Wv,Jae,a8o,s8o,Tj,n8o,l8o,i8o,Qv,Yae,d8o,c8o,Fj,m8o,f8o,g8o,Hv,Kae,h8o,u8o,Cj,p8o,_8o,b8o,Uv,Zae,v8o,T8o,Mj,F8o,C8o,M8o,Jv,ese,E8o,y8o,Ej,w8o,A8o,L8o,Yv,ose,B8o,k8o,yj,x8o,R8o,S8o,Kv,rse,P8o,$8o,wj,I8o,j8o,N8o,Zv,tse,D8o,q8o,Aj,G8o,O8o,X8o,eT,ase,z8o,V8o,Lj,W8o,Q8o,H8o,oT,sse,U8o,J8o,Bj,Y8o,K8o,Z8o,rT,nse,e9o,o9o,kj,r9o,t9o,a9o,tT,lse,s9o,n9o,xj,l9o,i9o,d9o,aT,ise,c9o,m9o,Rj,f9o,g9o,h9o,sT,dse,u9o,p9o,Sj,_9o,b9o,v9o,nT,cse,T9o,F9o,Pj,C9o,M9o,E9o,lT,mse,y9o,w9o,$j,A9o,L9o,B9o,iT,fse,k9o,x9o,Ij,R9o,S9o,P9o,dT,gse,$9o,I9o,jj,j9o,N9o,D9o,cT,hse,q9o,G9o,Nj,O9o,X9o,z9o,mT,use,V9o,W9o,Dj,Q9o,H9o,U9o,fT,pse,J9o,Y9o,qj,K9o,Z9o,eBo,gT,_se,oBo,rBo,Gj,tBo,aBo,sBo,hT,nBo,bse,lBo,iBo,vse,dBo,cBo,Tse,mBo,fBo,Jy,I8e,pd,uT,Fse,Yy,gBo,Cse,hBo,j8e,or,Ky,uBo,_d,pBo,Mse,_Bo,bBo,Ese,vBo,TBo,FBo,Zy,CBo,yse,MBo,EBo,yBo,Hr,ew,wBo,wse,ABo,LBo,bd,BBo,Ase,kBo,xBo,Lse,RBo,SBo,PBo,Bse,$Bo,IBo,ow,jBo,qe,rw,NBo,kse,DBo,qBo,Ua,GBo,xse,OBo,XBo,Rse,zBo,VBo,Sse,WBo,QBo,HBo,Pse,pT,$se,UBo,JBo,Oj,YBo,KBo,ZBo,_T,eko,Ise,oko,rko,jse,tko,ako,Nse,sko,nko,tw,N8e,vd,bT,Dse,aw,lko,qse,iko,D8e,rr,sw,dko,Td,cko,Gse,mko,fko,Ose,gko,hko,uko,nw,pko,Xse,_ko,bko,vko,Ur,lw,Tko,zse,Fko,Cko,Fd,Mko,Vse,Eko,yko,Wse,wko,Ako,Lko,Qse,Bko,kko,iw,xko,Ge,dw,Rko,Hse,Sko,Pko,Ja,$ko,Use,Iko,jko,Jse,Nko,Dko,Yse,qko,Gko,Oko,be,vT,Kse,Xko,zko,Xj,Vko,Wko,Qko,TT,Zse,Hko,Uko,zj,Jko,Yko,Kko,Rn,ene,Zko,exo,Vj,oxo,rxo,Wj,txo,axo,sxo,FT,one,nxo,lxo,Qj,ixo,dxo,cxo,la,rne,mxo,fxo,Hj,gxo,hxo,Uj,uxo,pxo,Jj,_xo,bxo,vxo,CT,tne,Txo,Fxo,Yj,Cxo,Mxo,Exo,MT,ane,yxo,wxo,Kj,Axo,Lxo,Bxo,ET,sne,kxo,xxo,Zj,Rxo,Sxo,Pxo,yT,nne,$xo,Ixo,eN,jxo,Nxo,Dxo,wT,qxo,lne,Gxo,Oxo,ine,Xxo,zxo,dne,Vxo,Wxo,cw,q8e,Cd,AT,cne,mw,Qxo,mne,Hxo,G8e,tr,fw,Uxo,Md,Jxo,fne,Yxo,Kxo,gne,Zxo,eRo,oRo,gw,rRo,hne,tRo,aRo,sRo,Jr,hw,nRo,une,lRo,iRo,Ed,dRo,pne,cRo,mRo,_ne,fRo,gRo,hRo,bne,uRo,pRo,uw,_Ro,Oe,pw,bRo,vne,vRo,TRo,Ya,FRo,Tne,CRo,MRo,Fne,ERo,yRo,Cne,wRo,ARo,LRo,Mne,LT,Ene,BRo,kRo,oN,xRo,RRo,SRo,BT,PRo,yne,$Ro,IRo,wne,jRo,NRo,Ane,DRo,qRo,_w,O8e,yd,kT,Lne,bw,GRo,Bne,ORo,X8e,ar,vw,XRo,wd,zRo,kne,VRo,WRo,xne,QRo,HRo,URo,Tw,JRo,Rne,YRo,KRo,ZRo,Yr,Fw,eSo,Sne,oSo,rSo,Ad,tSo,Pne,aSo,sSo,$ne,nSo,lSo,iSo,Ine,dSo,cSo,Cw,mSo,Xe,Mw,fSo,jne,gSo,hSo,Ka,uSo,Nne,pSo,_So,Dne,bSo,vSo,qne,TSo,FSo,CSo,ao,xT,Gne,MSo,ESo,rN,ySo,wSo,ASo,RT,One,LSo,BSo,tN,kSo,xSo,RSo,ST,Xne,SSo,PSo,aN,$So,ISo,jSo,PT,zne,NSo,DSo,sN,qSo,GSo,OSo,$T,Vne,XSo,zSo,nN,VSo,WSo,QSo,IT,Wne,HSo,USo,lN,JSo,YSo,KSo,jT,Qne,ZSo,ePo,iN,oPo,rPo,tPo,NT,aPo,Hne,sPo,nPo,Une,lPo,iPo,Jne,dPo,cPo,Ew,z8e,Ld,DT,Yne,yw,mPo,Kne,fPo,V8e,sr,ww,gPo,Bd,hPo,Zne,uPo,pPo,ele,_Po,bPo,vPo,Aw,TPo,ole,FPo,CPo,MPo,Kr,Lw,EPo,rle,yPo,wPo,kd,APo,tle,LPo,BPo,ale,kPo,xPo,RPo,sle,SPo,PPo,Bw,$Po,ze,kw,IPo,nle,jPo,NPo,Za,DPo,lle,qPo,GPo,ile,OPo,XPo,dle,zPo,VPo,WPo,xd,qT,cle,QPo,HPo,dN,UPo,JPo,YPo,GT,mle,KPo,ZPo,cN,e$o,o$o,r$o,OT,fle,t$o,a$o,mN,s$o,n$o,l$o,XT,i$o,gle,d$o,c$o,hle,m$o,f$o,ule,g$o,h$o,xw,W8e,Rd,zT,ple,Rw,u$o,_le,p$o,Q8e,nr,Sw,_$o,Sd,b$o,ble,v$o,T$o,vle,F$o,C$o,M$o,Pw,E$o,Tle,y$o,w$o,A$o,Zr,$w,L$o,Fle,B$o,k$o,Pd,x$o,Cle,R$o,S$o,Mle,P$o,$$o,I$o,Ele,j$o,N$o,Iw,D$o,Ve,jw,q$o,yle,G$o,O$o,es,X$o,wle,z$o,V$o,Ale,W$o,Q$o,Lle,H$o,U$o,J$o,so,VT,Ble,Y$o,K$o,fN,Z$o,eIo,oIo,WT,kle,rIo,tIo,gN,aIo,sIo,nIo,QT,xle,lIo,iIo,hN,dIo,cIo,mIo,HT,Rle,fIo,gIo,uN,hIo,uIo,pIo,UT,Sle,_Io,bIo,pN,vIo,TIo,FIo,JT,Ple,CIo,MIo,_N,EIo,yIo,wIo,YT,$le,AIo,LIo,bN,BIo,kIo,xIo,KT,RIo,Ile,SIo,PIo,jle,$Io,IIo,Nle,jIo,NIo,Nw,H8e,$d,ZT,Dle,Dw,DIo,qle,qIo,U8e,lr,qw,GIo,Id,OIo,Gle,XIo,zIo,Ole,VIo,WIo,QIo,Gw,HIo,Xle,UIo,JIo,YIo,et,Ow,KIo,zle,ZIo,ejo,jd,ojo,Vle,rjo,tjo,Wle,ajo,sjo,njo,Qle,ljo,ijo,Xw,djo,We,zw,cjo,Hle,mjo,fjo,os,gjo,Ule,hjo,ujo,Jle,pjo,_jo,Yle,bjo,vjo,Tjo,Vw,e1,Kle,Fjo,Cjo,vN,Mjo,Ejo,yjo,o1,Zle,wjo,Ajo,TN,Ljo,Bjo,kjo,r1,xjo,eie,Rjo,Sjo,oie,Pjo,$jo,rie,Ijo,jjo,Ww,J8e,Nd,t1,tie,Qw,Njo,aie,Djo,Y8e,ir,Hw,qjo,Dd,Gjo,sie,Ojo,Xjo,nie,zjo,Vjo,Wjo,Uw,Qjo,lie,Hjo,Ujo,Jjo,ot,Jw,Yjo,iie,Kjo,Zjo,qd,eNo,die,oNo,rNo,cie,tNo,aNo,sNo,mie,nNo,lNo,Yw,iNo,Qe,Kw,dNo,fie,cNo,mNo,rs,fNo,gie,gNo,hNo,hie,uNo,pNo,uie,_No,bNo,vNo,Gd,a1,pie,TNo,FNo,FN,CNo,MNo,ENo,s1,_ie,yNo,wNo,CN,ANo,LNo,BNo,n1,bie,kNo,xNo,MN,RNo,SNo,PNo,l1,$No,vie,INo,jNo,Tie,NNo,DNo,Fie,qNo,GNo,Zw,K8e,Od,i1,Cie,eA,ONo,Mie,XNo,Z8e,dr,oA,zNo,Xd,VNo,Eie,WNo,QNo,yie,HNo,UNo,JNo,rA,YNo,wie,KNo,ZNo,eDo,rt,tA,oDo,Aie,rDo,tDo,zd,aDo,Lie,sDo,nDo,Bie,lDo,iDo,dDo,kie,cDo,mDo,aA,fDo,He,sA,gDo,xie,hDo,uDo,ts,pDo,Rie,_Do,bDo,Sie,vDo,TDo,Pie,FDo,CDo,MDo,Vd,d1,$ie,EDo,yDo,EN,wDo,ADo,LDo,c1,Iie,BDo,kDo,yN,xDo,RDo,SDo,m1,jie,PDo,$Do,wN,IDo,jDo,NDo,f1,DDo,Nie,qDo,GDo,Die,ODo,XDo,qie,zDo,VDo,nA,e9e,Wd,g1,Gie,lA,WDo,Oie,QDo,o9e,cr,iA,HDo,Qd,UDo,Xie,JDo,YDo,zie,KDo,ZDo,eqo,dA,oqo,Vie,rqo,tqo,aqo,tt,cA,sqo,Wie,nqo,lqo,Hd,iqo,Qie,dqo,cqo,Hie,mqo,fqo,gqo,Uie,hqo,uqo,mA,pqo,Ue,fA,_qo,Jie,bqo,vqo,as,Tqo,Yie,Fqo,Cqo,Kie,Mqo,Eqo,Zie,yqo,wqo,Aqo,ede,h1,ode,Lqo,Bqo,AN,kqo,xqo,Rqo,u1,Sqo,rde,Pqo,$qo,tde,Iqo,jqo,ade,Nqo,Dqo,gA,r9e,Ud,p1,sde,hA,qqo,nde,Gqo,t9e,mr,uA,Oqo,Jd,Xqo,lde,zqo,Vqo,ide,Wqo,Qqo,Hqo,pA,Uqo,dde,Jqo,Yqo,Kqo,at,_A,Zqo,cde,eGo,oGo,Yd,rGo,mde,tGo,aGo,fde,sGo,nGo,lGo,gde,iGo,dGo,bA,cGo,Je,vA,mGo,hde,fGo,gGo,ss,hGo,ude,uGo,pGo,pde,_Go,bGo,_de,vGo,TGo,FGo,bde,_1,vde,CGo,MGo,LN,EGo,yGo,wGo,b1,AGo,Tde,LGo,BGo,Fde,kGo,xGo,Cde,RGo,SGo,TA,a9e,Kd,v1,Mde,FA,PGo,Ede,$Go,s9e,fr,CA,IGo,Zd,jGo,yde,NGo,DGo,wde,qGo,GGo,OGo,MA,XGo,Ade,zGo,VGo,WGo,st,EA,QGo,Lde,HGo,UGo,ec,JGo,Bde,YGo,KGo,kde,ZGo,eOo,oOo,xde,rOo,tOo,yA,aOo,Ye,wA,sOo,Rde,nOo,lOo,ns,iOo,Sde,dOo,cOo,Pde,mOo,fOo,$de,gOo,hOo,uOo,AA,T1,Ide,pOo,_Oo,BN,bOo,vOo,TOo,F1,jde,FOo,COo,kN,MOo,EOo,yOo,C1,wOo,Nde,AOo,LOo,Dde,BOo,kOo,qde,xOo,ROo,LA,n9e,oc,M1,Gde,BA,SOo,Ode,POo,l9e,gr,kA,$Oo,rc,IOo,Xde,jOo,NOo,zde,DOo,qOo,GOo,xA,OOo,Vde,XOo,zOo,VOo,nt,RA,WOo,Wde,QOo,HOo,tc,UOo,Qde,JOo,YOo,Hde,KOo,ZOo,eXo,Ude,oXo,rXo,SA,tXo,go,PA,aXo,Jde,sXo,nXo,ls,lXo,Yde,iXo,dXo,Kde,cXo,mXo,Zde,fXo,gXo,hXo,B,E1,ece,uXo,pXo,xN,_Xo,bXo,vXo,y1,oce,TXo,FXo,RN,CXo,MXo,EXo,w1,rce,yXo,wXo,SN,AXo,LXo,BXo,A1,tce,kXo,xXo,PN,RXo,SXo,PXo,L1,ace,$Xo,IXo,$N,jXo,NXo,DXo,B1,sce,qXo,GXo,IN,OXo,XXo,zXo,k1,nce,VXo,WXo,jN,QXo,HXo,UXo,x1,lce,JXo,YXo,NN,KXo,ZXo,ezo,R1,ice,ozo,rzo,DN,tzo,azo,szo,S1,dce,nzo,lzo,qN,izo,dzo,czo,P1,cce,mzo,fzo,GN,gzo,hzo,uzo,$1,mce,pzo,_zo,ON,bzo,vzo,Tzo,I1,fce,Fzo,Czo,XN,Mzo,Ezo,yzo,j1,gce,wzo,Azo,zN,Lzo,Bzo,kzo,N1,hce,xzo,Rzo,VN,Szo,Pzo,$zo,Sn,uce,Izo,jzo,WN,Nzo,Dzo,QN,qzo,Gzo,Ozo,D1,pce,Xzo,zzo,HN,Vzo,Wzo,Qzo,q1,_ce,Hzo,Uzo,UN,Jzo,Yzo,Kzo,G1,bce,Zzo,eVo,JN,oVo,rVo,tVo,O1,vce,aVo,sVo,YN,nVo,lVo,iVo,X1,Tce,dVo,cVo,KN,mVo,fVo,gVo,z1,Fce,hVo,uVo,ZN,pVo,_Vo,bVo,V1,Cce,vVo,TVo,eD,FVo,CVo,MVo,W1,Mce,EVo,yVo,oD,wVo,AVo,LVo,Q1,Ece,BVo,kVo,rD,xVo,RVo,SVo,H1,yce,PVo,$Vo,tD,IVo,jVo,NVo,U1,wce,DVo,qVo,aD,GVo,OVo,XVo,J1,Ace,zVo,VVo,sD,WVo,QVo,HVo,Y1,Lce,UVo,JVo,nD,YVo,KVo,ZVo,K1,Bce,eWo,oWo,lD,rWo,tWo,aWo,Z1,kce,sWo,nWo,iD,lWo,iWo,dWo,eF,xce,cWo,mWo,dD,fWo,gWo,hWo,oF,Rce,uWo,pWo,cD,_Wo,bWo,vWo,rF,Sce,TWo,FWo,mD,CWo,MWo,EWo,tF,Pce,yWo,wWo,fD,AWo,LWo,BWo,aF,$ce,kWo,xWo,gD,RWo,SWo,PWo,sF,Ice,$Wo,IWo,hD,jWo,NWo,DWo,nF,jce,qWo,GWo,uD,OWo,XWo,zWo,lF,Nce,VWo,WWo,pD,QWo,HWo,UWo,iF,Dce,JWo,YWo,_D,KWo,ZWo,eQo,dF,qce,oQo,rQo,bD,tQo,aQo,sQo,Gce,nQo,lQo,$A,i9e,ac,cF,Oce,IA,iQo,Xce,dQo,d9e,hr,jA,cQo,sc,mQo,zce,fQo,gQo,Vce,hQo,uQo,pQo,NA,_Qo,Wce,bQo,vQo,TQo,lt,DA,FQo,Qce,CQo,MQo,nc,EQo,Hce,yQo,wQo,Uce,AQo,LQo,BQo,Jce,kQo,xQo,qA,RQo,ho,GA,SQo,Yce,PQo,$Qo,is,IQo,Kce,jQo,NQo,Zce,DQo,qQo,eme,GQo,OQo,XQo,H,mF,ome,zQo,VQo,vD,WQo,QQo,HQo,fF,rme,UQo,JQo,TD,YQo,KQo,ZQo,gF,tme,eHo,oHo,FD,rHo,tHo,aHo,hF,ame,sHo,nHo,CD,lHo,iHo,dHo,uF,sme,cHo,mHo,MD,fHo,gHo,hHo,pF,nme,uHo,pHo,ED,_Ho,bHo,vHo,_F,lme,THo,FHo,yD,CHo,MHo,EHo,bF,ime,yHo,wHo,wD,AHo,LHo,BHo,vF,dme,kHo,xHo,AD,RHo,SHo,PHo,TF,cme,$Ho,IHo,LD,jHo,NHo,DHo,FF,mme,qHo,GHo,BD,OHo,XHo,zHo,CF,fme,VHo,WHo,kD,QHo,HHo,UHo,MF,gme,JHo,YHo,xD,KHo,ZHo,eUo,EF,hme,oUo,rUo,RD,tUo,aUo,sUo,yF,ume,nUo,lUo,SD,iUo,dUo,cUo,wF,pme,mUo,fUo,PD,gUo,hUo,uUo,AF,_me,pUo,_Uo,$D,bUo,vUo,TUo,LF,bme,FUo,CUo,ID,MUo,EUo,yUo,BF,vme,wUo,AUo,jD,LUo,BUo,kUo,kF,Tme,xUo,RUo,ND,SUo,PUo,$Uo,xF,Fme,IUo,jUo,DD,NUo,DUo,qUo,RF,Cme,GUo,OUo,qD,XUo,zUo,VUo,Mme,WUo,QUo,OA,c9e,lc,SF,Eme,XA,HUo,yme,UUo,m9e,ur,zA,JUo,ic,YUo,wme,KUo,ZUo,Ame,eJo,oJo,rJo,VA,tJo,Lme,aJo,sJo,nJo,it,WA,lJo,Bme,iJo,dJo,dc,cJo,kme,mJo,fJo,xme,gJo,hJo,uJo,Rme,pJo,_Jo,QA,bJo,uo,HA,vJo,Sme,TJo,FJo,ds,CJo,Pme,MJo,EJo,$me,yJo,wJo,Ime,AJo,LJo,BJo,he,PF,jme,kJo,xJo,GD,RJo,SJo,PJo,$F,Nme,$Jo,IJo,OD,jJo,NJo,DJo,IF,Dme,qJo,GJo,XD,OJo,XJo,zJo,jF,qme,VJo,WJo,zD,QJo,HJo,UJo,NF,Gme,JJo,YJo,VD,KJo,ZJo,eYo,DF,Ome,oYo,rYo,WD,tYo,aYo,sYo,qF,Xme,nYo,lYo,QD,iYo,dYo,cYo,GF,zme,mYo,fYo,HD,gYo,hYo,uYo,OF,Vme,pYo,_Yo,UD,bYo,vYo,TYo,XF,Wme,FYo,CYo,JD,MYo,EYo,yYo,Qme,wYo,AYo,UA,f9e,cc,zF,Hme,JA,LYo,Ume,BYo,g9e,pr,YA,kYo,mc,xYo,Jme,RYo,SYo,Yme,PYo,$Yo,IYo,KA,jYo,Kme,NYo,DYo,qYo,dt,ZA,GYo,Zme,OYo,XYo,fc,zYo,efe,VYo,WYo,ofe,QYo,HYo,UYo,rfe,JYo,YYo,e6,KYo,po,o6,ZYo,tfe,eKo,oKo,cs,rKo,afe,tKo,aKo,sfe,sKo,nKo,nfe,lKo,iKo,dKo,lfe,VF,ife,cKo,mKo,YD,fKo,gKo,hKo,dfe,uKo,pKo,r6,h9e,gc,WF,cfe,t6,_Ko,mfe,bKo,u9e,_r,a6,vKo,hc,TKo,ffe,FKo,CKo,gfe,MKo,EKo,yKo,s6,wKo,hfe,AKo,LKo,BKo,ct,n6,kKo,ufe,xKo,RKo,uc,SKo,pfe,PKo,$Ko,_fe,IKo,jKo,NKo,bfe,DKo,qKo,l6,GKo,_o,i6,OKo,vfe,XKo,zKo,ms,VKo,Tfe,WKo,QKo,Ffe,HKo,UKo,Cfe,JKo,YKo,KKo,Y,QF,Mfe,ZKo,eZo,KD,oZo,rZo,tZo,HF,Efe,aZo,sZo,ZD,nZo,lZo,iZo,UF,yfe,dZo,cZo,eq,mZo,fZo,gZo,JF,wfe,hZo,uZo,oq,pZo,_Zo,bZo,YF,Afe,vZo,TZo,rq,FZo,CZo,MZo,KF,Lfe,EZo,yZo,tq,wZo,AZo,LZo,ZF,Bfe,BZo,kZo,aq,xZo,RZo,SZo,eC,kfe,PZo,$Zo,sq,IZo,jZo,NZo,oC,xfe,DZo,qZo,nq,GZo,OZo,XZo,rC,Rfe,zZo,VZo,lq,WZo,QZo,HZo,tC,Sfe,UZo,JZo,iq,YZo,KZo,ZZo,aC,Pfe,eer,oer,dq,rer,ter,aer,sC,$fe,ser,ner,cq,ler,ier,der,nC,Ife,cer,mer,mq,fer,ger,her,lC,jfe,uer,per,fq,_er,ber,ver,iC,Nfe,Ter,Fer,gq,Cer,Mer,Eer,dC,Dfe,yer,wer,hq,Aer,Ler,Ber,cC,qfe,ker,xer,uq,Rer,Ser,Per,mC,Gfe,$er,Ier,pq,jer,Ner,Der,fC,Ofe,qer,Ger,_q,Oer,Xer,zer,Xfe,Ver,Wer,d6,p9e,pc,gC,zfe,c6,Qer,Vfe,Her,_9e,br,m6,Uer,_c,Jer,Wfe,Yer,Ker,Qfe,Zer,eor,oor,f6,ror,Hfe,tor,aor,sor,mt,g6,nor,Ufe,lor,ior,bc,dor,Jfe,cor,mor,Yfe,gor,hor,uor,Kfe,por,_or,h6,bor,bo,u6,vor,Zfe,Tor,For,fs,Cor,ege,Mor,Eor,oge,yor,wor,rge,Aor,Lor,Bor,ue,hC,tge,kor,xor,bq,Ror,Sor,Por,uC,age,$or,Ior,vq,jor,Nor,Dor,pC,sge,qor,Gor,Tq,Oor,Xor,zor,_C,nge,Vor,Wor,Fq,Qor,Hor,Uor,bC,lge,Jor,Yor,Cq,Kor,Zor,err,vC,ige,orr,rrr,Mq,trr,arr,srr,TC,dge,nrr,lrr,Eq,irr,drr,crr,FC,cge,mrr,frr,yq,grr,hrr,urr,CC,mge,prr,_rr,wq,brr,vrr,Trr,MC,fge,Frr,Crr,Aq,Mrr,Err,yrr,gge,wrr,Arr,p6,b9e,vc,EC,hge,_6,Lrr,uge,Brr,v9e,vr,b6,krr,Tc,xrr,pge,Rrr,Srr,_ge,Prr,$rr,Irr,v6,jrr,bge,Nrr,Drr,qrr,ft,T6,Grr,vge,Orr,Xrr,Fc,zrr,Tge,Vrr,Wrr,Fge,Qrr,Hrr,Urr,Cge,Jrr,Yrr,F6,Krr,vo,C6,Zrr,Mge,etr,otr,gs,rtr,Ege,ttr,atr,yge,str,ntr,wge,ltr,itr,dtr,X,yC,Age,ctr,mtr,Lq,ftr,gtr,htr,wC,Lge,utr,ptr,Bq,_tr,btr,vtr,AC,Bge,Ttr,Ftr,kq,Ctr,Mtr,Etr,LC,kge,ytr,wtr,xq,Atr,Ltr,Btr,BC,xge,ktr,xtr,Rq,Rtr,Str,Ptr,kC,Rge,$tr,Itr,Sq,jtr,Ntr,Dtr,xC,Sge,qtr,Gtr,Pq,Otr,Xtr,ztr,RC,Pge,Vtr,Wtr,$q,Qtr,Htr,Utr,SC,$ge,Jtr,Ytr,Iq,Ktr,Ztr,ear,PC,Ige,oar,rar,jq,tar,aar,sar,$C,jge,nar,lar,Nq,iar,dar,car,IC,Nge,mar,far,Dq,gar,har,uar,jC,Dge,par,_ar,qq,bar,Tar,Far,NC,qge,Car,Mar,Gq,Ear,yar,war,DC,Gge,Aar,Lar,Oq,Bar,kar,xar,qC,Oge,Rar,Sar,Xq,Par,$ar,Iar,GC,Xge,jar,Nar,zq,Dar,qar,Gar,OC,zge,Oar,Xar,Vq,zar,Var,War,XC,Vge,Qar,Har,Wq,Uar,Jar,Yar,zC,Wge,Kar,Zar,Qq,esr,osr,rsr,VC,Qge,tsr,asr,Hq,ssr,nsr,lsr,WC,Hge,isr,dsr,Uq,csr,msr,fsr,QC,Uge,gsr,hsr,Jq,usr,psr,_sr,HC,Jge,bsr,vsr,Yq,Tsr,Fsr,Csr,UC,Yge,Msr,Esr,Kq,ysr,wsr,Asr,Kge,Lsr,Bsr,M6,T9e,Cc,JC,Zge,E6,ksr,ehe,xsr,F9e,Tr,y6,Rsr,Mc,Ssr,ohe,Psr,$sr,rhe,Isr,jsr,Nsr,w6,Dsr,the,qsr,Gsr,Osr,gt,A6,Xsr,ahe,zsr,Vsr,Ec,Wsr,she,Qsr,Hsr,nhe,Usr,Jsr,Ysr,lhe,Ksr,Zsr,L6,enr,To,B6,onr,ihe,rnr,tnr,hs,anr,dhe,snr,nnr,che,lnr,inr,mhe,dnr,cnr,mnr,te,YC,fhe,fnr,gnr,Zq,hnr,unr,pnr,KC,ghe,_nr,bnr,eG,vnr,Tnr,Fnr,ZC,hhe,Cnr,Mnr,oG,Enr,ynr,wnr,e4,uhe,Anr,Lnr,rG,Bnr,knr,xnr,o4,phe,Rnr,Snr,tG,Pnr,$nr,Inr,r4,_he,jnr,Nnr,aG,Dnr,qnr,Gnr,t4,bhe,Onr,Xnr,sG,znr,Vnr,Wnr,a4,vhe,Qnr,Hnr,nG,Unr,Jnr,Ynr,s4,The,Knr,Znr,lG,elr,olr,rlr,n4,Fhe,tlr,alr,iG,slr,nlr,llr,l4,Che,ilr,dlr,dG,clr,mlr,flr,i4,Mhe,glr,hlr,cG,ulr,plr,_lr,d4,Ehe,blr,vlr,mG,Tlr,Flr,Clr,c4,yhe,Mlr,Elr,fG,ylr,wlr,Alr,m4,whe,Llr,Blr,gG,klr,xlr,Rlr,f4,Ahe,Slr,Plr,hG,$lr,Ilr,jlr,g4,Lhe,Nlr,Dlr,uG,qlr,Glr,Olr,Bhe,Xlr,zlr,k6,C9e,yc,h4,khe,x6,Vlr,xhe,Wlr,M9e,Fr,R6,Qlr,wc,Hlr,Rhe,Ulr,Jlr,She,Ylr,Klr,Zlr,S6,eir,Phe,oir,rir,tir,ht,P6,air,$he,sir,nir,Ac,lir,Ihe,iir,dir,jhe,cir,mir,fir,Nhe,gir,hir,$6,uir,Fo,I6,pir,Dhe,_ir,bir,us,vir,qhe,Tir,Fir,Ghe,Cir,Mir,Ohe,Eir,yir,wir,Xhe,u4,zhe,Air,Lir,pG,Bir,kir,xir,Vhe,Rir,Sir,j6,E9e,Lc,p4,Whe,N6,Pir,Qhe,$ir,y9e,Cr,D6,Iir,Bc,jir,Hhe,Nir,Dir,Uhe,qir,Gir,Oir,q6,Xir,Jhe,zir,Vir,Wir,ut,G6,Qir,Yhe,Hir,Uir,kc,Jir,Khe,Yir,Kir,Zhe,Zir,edr,odr,eue,rdr,tdr,O6,adr,Co,X6,sdr,oue,ndr,ldr,ps,idr,rue,ddr,cdr,tue,mdr,fdr,aue,gdr,hdr,udr,K,_4,sue,pdr,_dr,_G,bdr,vdr,Tdr,b4,nue,Fdr,Cdr,bG,Mdr,Edr,ydr,v4,lue,wdr,Adr,vG,Ldr,Bdr,kdr,T4,iue,xdr,Rdr,TG,Sdr,Pdr,$dr,F4,due,Idr,jdr,FG,Ndr,Ddr,qdr,C4,cue,Gdr,Odr,CG,Xdr,zdr,Vdr,M4,mue,Wdr,Qdr,MG,Hdr,Udr,Jdr,E4,fue,Ydr,Kdr,EG,Zdr,ecr,ocr,y4,gue,rcr,tcr,yG,acr,scr,ncr,w4,hue,lcr,icr,wG,dcr,ccr,mcr,A4,uue,fcr,gcr,AG,hcr,ucr,pcr,L4,pue,_cr,bcr,LG,vcr,Tcr,Fcr,B4,_ue,Ccr,Mcr,BG,Ecr,ycr,wcr,k4,bue,Acr,Lcr,kG,Bcr,kcr,xcr,x4,vue,Rcr,Scr,xG,Pcr,$cr,Icr,R4,Tue,jcr,Ncr,RG,Dcr,qcr,Gcr,S4,Fue,Ocr,Xcr,SG,zcr,Vcr,Wcr,P4,Cue,Qcr,Hcr,PG,Ucr,Jcr,Ycr,$4,Mue,Kcr,Zcr,$G,emr,omr,rmr,I4,Eue,tmr,amr,IG,smr,nmr,lmr,yue,imr,dmr,z6,w9e,xc,j4,wue,V6,cmr,Aue,mmr,A9e,Mr,W6,fmr,Rc,gmr,Lue,hmr,umr,Bue,pmr,_mr,bmr,Q6,vmr,kue,Tmr,Fmr,Cmr,pt,H6,Mmr,xue,Emr,ymr,Sc,wmr,Rue,Amr,Lmr,Sue,Bmr,kmr,xmr,Pue,Rmr,Smr,U6,Pmr,Mo,J6,$mr,$ue,Imr,jmr,_s,Nmr,Iue,Dmr,qmr,jue,Gmr,Omr,Nue,Xmr,zmr,Vmr,Z,N4,Due,Wmr,Qmr,jG,Hmr,Umr,Jmr,D4,que,Ymr,Kmr,NG,Zmr,efr,ofr,q4,Gue,rfr,tfr,DG,afr,sfr,nfr,G4,Oue,lfr,ifr,qG,dfr,cfr,mfr,O4,Xue,ffr,gfr,GG,hfr,ufr,pfr,X4,zue,_fr,bfr,OG,vfr,Tfr,Ffr,z4,Vue,Cfr,Mfr,XG,Efr,yfr,wfr,V4,Wue,Afr,Lfr,zG,Bfr,kfr,xfr,W4,Que,Rfr,Sfr,VG,Pfr,$fr,Ifr,Q4,Hue,jfr,Nfr,WG,Dfr,qfr,Gfr,H4,Uue,Ofr,Xfr,QG,zfr,Vfr,Wfr,U4,Jue,Qfr,Hfr,HG,Ufr,Jfr,Yfr,J4,Yue,Kfr,Zfr,UG,egr,ogr,rgr,Y4,Kue,tgr,agr,JG,sgr,ngr,lgr,K4,Zue,igr,dgr,YG,cgr,mgr,fgr,Z4,epe,ggr,hgr,KG,ugr,pgr,_gr,eM,ope,bgr,vgr,ZG,Tgr,Fgr,Cgr,oM,rpe,Mgr,Egr,eO,ygr,wgr,Agr,rM,tpe,Lgr,Bgr,oO,kgr,xgr,Rgr,ape,Sgr,Pgr,Y6,L9e,Pc,tM,spe,K6,$gr,npe,Igr,B9e,Er,Z6,jgr,$c,Ngr,lpe,Dgr,qgr,ipe,Ggr,Ogr,Xgr,e0,zgr,dpe,Vgr,Wgr,Qgr,_t,o0,Hgr,cpe,Ugr,Jgr,Ic,Ygr,mpe,Kgr,Zgr,fpe,ehr,ohr,rhr,gpe,thr,ahr,r0,shr,Eo,t0,nhr,hpe,lhr,ihr,bs,dhr,upe,chr,mhr,ppe,fhr,ghr,_pe,hhr,uhr,phr,bpe,aM,vpe,_hr,bhr,rO,vhr,Thr,Fhr,Tpe,Chr,Mhr,a0,k9e,jc,sM,Fpe,s0,Ehr,Cpe,yhr,x9e,yr,n0,whr,Nc,Ahr,Mpe,Lhr,Bhr,Epe,khr,xhr,Rhr,l0,Shr,ype,Phr,$hr,Ihr,bt,i0,jhr,wpe,Nhr,Dhr,Dc,qhr,Ape,Ghr,Ohr,Lpe,Xhr,zhr,Vhr,Bpe,Whr,Qhr,d0,Hhr,yo,c0,Uhr,kpe,Jhr,Yhr,vs,Khr,xpe,Zhr,eur,Rpe,our,rur,Spe,tur,aur,sur,Ppe,nM,$pe,nur,lur,tO,iur,dur,cur,Ipe,mur,fur,m0,R9e,qc,lM,jpe,f0,gur,Npe,hur,S9e,wr,g0,uur,Gc,pur,Dpe,_ur,bur,qpe,vur,Tur,Fur,h0,Cur,Gpe,Mur,Eur,yur,vt,u0,wur,Ope,Aur,Lur,Oc,Bur,Xpe,kur,xur,zpe,Rur,Sur,Pur,Vpe,$ur,Iur,p0,jur,wo,_0,Nur,Wpe,Dur,qur,Ts,Gur,Qpe,Our,Xur,Hpe,zur,Vur,Upe,Wur,Qur,Hur,V,iM,Jpe,Uur,Jur,aO,Yur,Kur,Zur,dM,Ype,epr,opr,sO,rpr,tpr,apr,cM,Kpe,spr,npr,nO,lpr,ipr,dpr,mM,Zpe,cpr,mpr,lO,fpr,gpr,hpr,fM,e_e,upr,ppr,iO,_pr,bpr,vpr,gM,o_e,Tpr,Fpr,dO,Cpr,Mpr,Epr,hM,r_e,ypr,wpr,cO,Apr,Lpr,Bpr,uM,t_e,kpr,xpr,mO,Rpr,Spr,Ppr,pM,a_e,$pr,Ipr,fO,jpr,Npr,Dpr,_M,s_e,qpr,Gpr,gO,Opr,Xpr,zpr,bM,n_e,Vpr,Wpr,hO,Qpr,Hpr,Upr,vM,l_e,Jpr,Ypr,uO,Kpr,Zpr,e_r,TM,i_e,o_r,r_r,pO,t_r,a_r,s_r,FM,d_e,n_r,l_r,_O,i_r,d_r,c_r,CM,c_e,m_r,f_r,bO,g_r,h_r,u_r,MM,m_e,p_r,__r,vO,b_r,v_r,T_r,EM,f_e,F_r,C_r,TO,M_r,E_r,y_r,yM,g_e,w_r,A_r,FO,L_r,B_r,k_r,wM,h_e,x_r,R_r,CO,S_r,P_r,$_r,AM,u_e,I_r,j_r,MO,N_r,D_r,q_r,LM,p_e,G_r,O_r,EO,X_r,z_r,V_r,BM,__e,W_r,Q_r,yO,H_r,U_r,J_r,kM,b_e,Y_r,K_r,wO,Z_r,ebr,obr,xM,v_e,rbr,tbr,AO,abr,sbr,nbr,T_e,lbr,ibr,b0,P9e,Xc,RM,F_e,v0,dbr,C_e,cbr,$9e,Ar,T0,mbr,zc,fbr,M_e,gbr,hbr,E_e,ubr,pbr,_br,F0,bbr,y_e,vbr,Tbr,Fbr,Tt,C0,Cbr,w_e,Mbr,Ebr,Vc,ybr,A_e,wbr,Abr,L_e,Lbr,Bbr,kbr,B_e,xbr,Rbr,M0,Sbr,Ao,E0,Pbr,k_e,$br,Ibr,Fs,jbr,x_e,Nbr,Dbr,R_e,qbr,Gbr,S_e,Obr,Xbr,zbr,Cs,SM,P_e,Vbr,Wbr,LO,Qbr,Hbr,Ubr,PM,$_e,Jbr,Ybr,BO,Kbr,Zbr,e2r,$M,I_e,o2r,r2r,kO,t2r,a2r,s2r,IM,j_e,n2r,l2r,xO,i2r,d2r,c2r,N_e,m2r,f2r,y0,I9e,Wc,jM,D_e,w0,g2r,q_e,h2r,j9e,Lr,A0,u2r,Qc,p2r,G_e,_2r,b2r,O_e,v2r,T2r,F2r,L0,C2r,X_e,M2r,E2r,y2r,Ft,B0,w2r,z_e,A2r,L2r,Hc,B2r,V_e,k2r,x2r,W_e,R2r,S2r,P2r,Q_e,$2r,I2r,k0,j2r,Lo,x0,N2r,H_e,D2r,q2r,Ms,G2r,U_e,O2r,X2r,J_e,z2r,V2r,Y_e,W2r,Q2r,H2r,me,NM,K_e,U2r,J2r,RO,Y2r,K2r,Z2r,DM,Z_e,evr,ovr,SO,rvr,tvr,avr,qM,ebe,svr,nvr,PO,lvr,ivr,dvr,GM,obe,cvr,mvr,$O,fvr,gvr,hvr,OM,rbe,uvr,pvr,IO,_vr,bvr,vvr,XM,tbe,Tvr,Fvr,jO,Cvr,Mvr,Evr,zM,abe,yvr,wvr,NO,Avr,Lvr,Bvr,VM,sbe,kvr,xvr,DO,Rvr,Svr,Pvr,WM,nbe,$vr,Ivr,qO,jvr,Nvr,Dvr,QM,lbe,qvr,Gvr,GO,Ovr,Xvr,zvr,HM,ibe,Vvr,Wvr,OO,Qvr,Hvr,Uvr,dbe,Jvr,Yvr,R0,N9e,Uc,UM,cbe,S0,Kvr,mbe,Zvr,D9e,Br,P0,eTr,Jc,oTr,fbe,rTr,tTr,gbe,aTr,sTr,nTr,$0,lTr,hbe,iTr,dTr,cTr,Ct,I0,mTr,ube,fTr,gTr,Yc,hTr,pbe,uTr,pTr,_be,_Tr,bTr,vTr,bbe,TTr,FTr,j0,CTr,Bo,N0,MTr,vbe,ETr,yTr,Es,wTr,Tbe,ATr,LTr,Fbe,BTr,kTr,Cbe,xTr,RTr,STr,ve,JM,Mbe,PTr,$Tr,XO,ITr,jTr,NTr,YM,Ebe,DTr,qTr,zO,GTr,OTr,XTr,KM,ybe,zTr,VTr,VO,WTr,QTr,HTr,ZM,wbe,UTr,JTr,WO,YTr,KTr,ZTr,eE,Abe,e1r,o1r,QO,r1r,t1r,a1r,oE,Lbe,s1r,n1r,HO,l1r,i1r,d1r,rE,Bbe,c1r,m1r,UO,f1r,g1r,h1r,tE,kbe,u1r,p1r,JO,_1r,b1r,v1r,aE,xbe,T1r,F1r,YO,C1r,M1r,E1r,Rbe,y1r,w1r,D0,q9e,Kc,sE,Sbe,q0,A1r,Pbe,L1r,G9e,kr,G0,B1r,Zc,k1r,$be,x1r,R1r,Ibe,S1r,P1r,$1r,O0,I1r,jbe,j1r,N1r,D1r,Mt,X0,q1r,Nbe,G1r,O1r,em,X1r,Dbe,z1r,V1r,qbe,W1r,Q1r,H1r,Gbe,U1r,J1r,z0,Y1r,ko,V0,K1r,Obe,Z1r,eFr,ys,oFr,Xbe,rFr,tFr,zbe,aFr,sFr,Vbe,nFr,lFr,iFr,Te,nE,Wbe,dFr,cFr,KO,mFr,fFr,gFr,lE,Qbe,hFr,uFr,ZO,pFr,_Fr,bFr,iE,Hbe,vFr,TFr,eX,FFr,CFr,MFr,dE,Ube,EFr,yFr,oX,wFr,AFr,LFr,cE,Jbe,BFr,kFr,rX,xFr,RFr,SFr,mE,Ybe,PFr,$Fr,tX,IFr,jFr,NFr,fE,Kbe,DFr,qFr,aX,GFr,OFr,XFr,gE,Zbe,zFr,VFr,sX,WFr,QFr,HFr,hE,e2e,UFr,JFr,nX,YFr,KFr,ZFr,o2e,eCr,oCr,W0,O9e,om,uE,r2e,Q0,rCr,t2e,tCr,X9e,xr,H0,aCr,rm,sCr,a2e,nCr,lCr,s2e,iCr,dCr,cCr,U0,mCr,n2e,fCr,gCr,hCr,Et,J0,uCr,l2e,pCr,_Cr,tm,bCr,i2e,vCr,TCr,d2e,FCr,CCr,MCr,c2e,ECr,yCr,Y0,wCr,xo,K0,ACr,m2e,LCr,BCr,ws,kCr,f2e,xCr,RCr,g2e,SCr,PCr,h2e,$Cr,ICr,jCr,Fe,pE,u2e,NCr,DCr,lX,qCr,GCr,OCr,_E,p2e,XCr,zCr,iX,VCr,WCr,QCr,bE,_2e,HCr,UCr,dX,JCr,YCr,KCr,vE,b2e,ZCr,e4r,cX,o4r,r4r,t4r,TE,v2e,a4r,s4r,mX,n4r,l4r,i4r,FE,T2e,d4r,c4r,fX,m4r,f4r,g4r,CE,F2e,h4r,u4r,gX,p4r,_4r,b4r,ME,C2e,v4r,T4r,hX,F4r,C4r,M4r,EE,M2e,E4r,y4r,uX,w4r,A4r,L4r,E2e,B4r,k4r,Z0,z9e,am,yE,y2e,eL,x4r,w2e,R4r,V9e,Rr,oL,S4r,sm,P4r,A2e,$4r,I4r,L2e,j4r,N4r,D4r,rL,q4r,B2e,G4r,O4r,X4r,yt,tL,z4r,k2e,V4r,W4r,nm,Q4r,x2e,H4r,U4r,R2e,J4r,Y4r,K4r,S2e,Z4r,eMr,aL,oMr,Ro,sL,rMr,P2e,tMr,aMr,As,sMr,$2e,nMr,lMr,I2e,iMr,dMr,j2e,cMr,mMr,fMr,Ce,wE,N2e,gMr,hMr,pX,uMr,pMr,_Mr,AE,D2e,bMr,vMr,_X,TMr,FMr,CMr,LE,q2e,MMr,EMr,bX,yMr,wMr,AMr,BE,G2e,LMr,BMr,vX,kMr,xMr,RMr,kE,O2e,SMr,PMr,TX,$Mr,IMr,jMr,xE,X2e,NMr,DMr,FX,qMr,GMr,OMr,RE,z2e,XMr,zMr,CX,VMr,WMr,QMr,SE,V2e,HMr,UMr,MX,JMr,YMr,KMr,PE,W2e,ZMr,eEr,EX,oEr,rEr,tEr,Q2e,aEr,sEr,nL,W9e,lm,$E,H2e,lL,nEr,U2e,lEr,Q9e,Sr,iL,iEr,im,dEr,J2e,cEr,mEr,Y2e,fEr,gEr,hEr,dL,uEr,K2e,pEr,_Er,bEr,wt,cL,vEr,Z2e,TEr,FEr,dm,CEr,eve,MEr,EEr,ove,yEr,wEr,AEr,rve,LEr,BEr,mL,kEr,So,fL,xEr,tve,REr,SEr,Ls,PEr,ave,$Er,IEr,sve,jEr,NEr,nve,DEr,qEr,GEr,no,IE,lve,OEr,XEr,yX,zEr,VEr,WEr,jE,ive,QEr,HEr,wX,UEr,JEr,YEr,NE,dve,KEr,ZEr,AX,e3r,o3r,r3r,DE,cve,t3r,a3r,LX,s3r,n3r,l3r,qE,mve,i3r,d3r,BX,c3r,m3r,f3r,GE,fve,g3r,h3r,kX,u3r,p3r,_3r,OE,gve,b3r,v3r,xX,T3r,F3r,C3r,hve,M3r,E3r,gL,H9e,cm,XE,uve,hL,y3r,pve,w3r,U9e,Pr,uL,A3r,mm,L3r,_ve,B3r,k3r,bve,x3r,R3r,S3r,pL,P3r,vve,$3r,I3r,j3r,At,_L,N3r,Tve,D3r,q3r,fm,G3r,Fve,O3r,X3r,Cve,z3r,V3r,W3r,Mve,Q3r,H3r,bL,U3r,Po,vL,J3r,Eve,Y3r,K3r,Bs,Z3r,yve,e5r,o5r,wve,r5r,t5r,Ave,a5r,s5r,n5r,lo,zE,Lve,l5r,i5r,RX,d5r,c5r,m5r,VE,Bve,f5r,g5r,SX,h5r,u5r,p5r,WE,kve,_5r,b5r,PX,v5r,T5r,F5r,QE,xve,C5r,M5r,$X,E5r,y5r,w5r,HE,Rve,A5r,L5r,IX,B5r,k5r,x5r,UE,Sve,R5r,S5r,jX,P5r,$5r,I5r,JE,Pve,j5r,N5r,NX,D5r,q5r,G5r,$ve,O5r,X5r,TL,J9e,gm,YE,Ive,FL,z5r,jve,V5r,Y9e,$r,CL,W5r,hm,Q5r,Nve,H5r,U5r,Dve,J5r,Y5r,K5r,ML,Z5r,qve,eyr,oyr,ryr,Lt,EL,tyr,Gve,ayr,syr,um,nyr,Ove,lyr,iyr,Xve,dyr,cyr,myr,zve,fyr,gyr,yL,hyr,$o,wL,uyr,Vve,pyr,_yr,ks,byr,Wve,vyr,Tyr,Qve,Fyr,Cyr,Hve,Myr,Eyr,yyr,Uve,KE,Jve,wyr,Ayr,DX,Lyr,Byr,kyr,Yve,xyr,Ryr,AL,K9e,pm,ZE,Kve,LL,Syr,Zve,Pyr,Z9e,Ir,BL,$yr,_m,Iyr,eTe,jyr,Nyr,oTe,Dyr,qyr,Gyr,kL,Oyr,rTe,Xyr,zyr,Vyr,Bt,xL,Wyr,tTe,Qyr,Hyr,bm,Uyr,aTe,Jyr,Yyr,sTe,Kyr,Zyr,ewr,nTe,owr,rwr,RL,twr,Io,SL,awr,lTe,swr,nwr,xs,lwr,iTe,iwr,dwr,dTe,cwr,mwr,cTe,fwr,gwr,hwr,PL,e3,mTe,uwr,pwr,qX,_wr,bwr,vwr,o3,fTe,Twr,Fwr,GX,Cwr,Mwr,Ewr,gTe,ywr,wwr,$L,eBe,vm,r3,hTe,IL,Awr,uTe,Lwr,oBe,jr,jL,Bwr,Tm,kwr,pTe,xwr,Rwr,_Te,Swr,Pwr,$wr,NL,Iwr,bTe,jwr,Nwr,Dwr,kt,DL,qwr,vTe,Gwr,Owr,Fm,Xwr,TTe,zwr,Vwr,FTe,Wwr,Qwr,Hwr,CTe,Uwr,Jwr,qL,Ywr,jo,GL,Kwr,MTe,Zwr,eAr,Rs,oAr,ETe,rAr,tAr,yTe,aAr,sAr,wTe,nAr,lAr,iAr,ATe,t3,LTe,dAr,cAr,OX,mAr,fAr,gAr,BTe,hAr,uAr,OL,rBe;return ce=new z({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),s5=new z({}),n5=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Lm=new pAr({props:{warning:"&lcub;true}",$$slots:{default:[Fpt]},$$scope:{ctx:yi}}}),l5=new z({}),i5=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L515"}}),m5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L538",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),f5=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),g5=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L660",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),h5=new z({}),u5=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L351"}}),b5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),v5=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),T5=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),F5=new z({}),C5=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),y5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ih=new pAr({props:{$$slots:{default:[Cpt]},$$scope:{ctx:yi}}}),w5=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),A5=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),L5=new z({}),B5=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L71"}}),R5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),vh=new pAr({props:{$$slots:{default:[Mpt]},$$scope:{ctx:yi}}}),S5=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),P5=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),$5=new z({}),I5=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L672"}}),N5=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),D5=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),q5=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),G5=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),O5=new z({}),X5=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L679"}}),V5=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),W5=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),Q5=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),H5=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),U5=new z({}),J5=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L694"}}),K5=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Z5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),ey=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),oy=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ry=new z({}),ty=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L701"}}),sy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),ny=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),ly=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dy=new z({}),cy=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L708"}}),fy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),hy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),py=new z({}),_y=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L717"}}),vy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),Fy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),My=new z({}),Ey=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L751"}}),wy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Ay=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),Ly=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),By=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ky=new z({}),xy=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L758"}}),Sy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),Py=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),$y=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Iy=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),jy=new z({}),Ny=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L744"}}),qy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),Oy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Xy=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),zy=new z({}),Vy=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L726"}}),Qy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Hy=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),Uy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Jy=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Yy=new z({}),Ky=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L733"}}),ew=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),ow=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),rw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),tw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),aw=new z({}),sw=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L767"}}),lw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),dw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),cw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),mw=new z({}),fw=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L797"}}),hw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),pw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_w=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),bw=new z({}),vw=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L804"}}),Fw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),Mw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ew=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),yw=new z({}),ww=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L827"}}),Lw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Bw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Rw=new z({}),Sw=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L811"}}),$w=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),jw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Nw=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Dw=new z({}),qw=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L818"}}),Ow=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Xw=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),zw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ww=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Qw=new z({}),Hw=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L836"}}),Jw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Yw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),Kw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Zw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),eA=new z({}),oA=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L843"}}),tA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),aA=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),sA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nA=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lA=new z({}),iA=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L790"}}),cA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),mA=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),fA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gA=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hA=new z({}),uA=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L774"}}),_A=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),bA=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),vA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),TA=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),FA=new z({}),CA=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L781"}}),EA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),yA=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),wA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),LA=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),BA=new z({}),kA=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L371"}}),RA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),SA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),PA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$A=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),IA=new z({}),jA=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L378"}}),DA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),qA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),GA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),OA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),XA=new z({}),zA=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L393"}}),WA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),QA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),HA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),UA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),JA=new z({}),YA=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L400"}}),ZA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),e6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),o6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),r6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),t6=new z({}),a6=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),n6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),l6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),i6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),d6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),c6=new z({}),m6=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),g6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),h6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),u6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),p6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),_6=new z({}),b6=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L430"}}),T6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),F6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),C6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),M6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),E6=new z({}),y6=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L466"}}),A6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),L6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),B6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),k6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),x6=new z({}),R6=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L446"}}),P6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),$6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),I6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),j6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),N6=new z({}),D6=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L457"}}),G6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),O6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),X6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),z6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),V6=new z({}),W6=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L439"}}),H6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),U6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),J6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Y6=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),K6=new z({}),Z6=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L407"}}),o0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),r0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),t0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),s0=new z({}),n0=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L482"}}),i0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),d0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),c0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),f0=new z({}),g0=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L220"}}),u0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),p0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),_0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v0=new z({}),T0=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L234"}}),C0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),M0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),E0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),y0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),w0=new z({}),A0=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L227"}}),B0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),k0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),x0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),R0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),S0=new z({}),P0=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L241"}}),I0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),j0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),N0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),D0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),q0=new z({}),G0=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),X0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),z0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),V0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),W0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Q0=new z({}),H0=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),J0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Y0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),K0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Z0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),eL=new z({}),oL=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),tL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),aL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),sL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),nL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),lL=new z({}),iL=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),cL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),mL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),fL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hL=new z({}),uL=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),_L=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),bL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),vL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),TL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),FL=new z({}),CL=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),EL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),yL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),wL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),AL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),LL=new z({}),BL=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),xL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),RL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),SL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$L=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),IL=new z({}),jL=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),DL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),qL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),GL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),OL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Ae=l(),ie=a("h1"),fe=a("a"),to=a("span"),m(ce.$$.fragment),_e=l(),Do=a("span"),wi=o("Auto Classes"),Mm=l(),na=a("p"),Ai=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=a("code"),o5=o("from_pretrained()"),Em=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),Bi=o("Instantiating one of "),Ss=a("a"),r5=o("AutoConfig"),Ps=o(", "),$s=a("a"),t5=o("AutoModel"),ki=o(`, and
`),Is=a("a"),a5=o("AutoTokenizer"),xi=o(" will directly create a class of the relevant architecture. For instance"),ym=l(),m($a.$$.fragment),co=l(),ge=a("p"),D7=o("will create a model that is an instance of "),Ri=a("a"),q7=o("BertModel"),G7=o("."),qo=l(),Ia=a("p"),O7=o("There is one class of "),wm=a("code"),X7=o("AutoModel"),fxe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),t8e=l(),Si=a("h2"),Am=a("a"),$V=a("span"),m(s5.$$.fragment),gxe=l(),IV=a("span"),hxe=o("Extending the Auto Classes"),a8e=l(),js=a("p"),uxe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=a("code"),pxe=o("NewModel"),_xe=o(", make sure you have a "),NV=a("code"),bxe=o("NewModelConfig"),vxe=o(` then you can add those to the auto
classes like this:`),s8e=l(),m(n5.$$.fragment),n8e=l(),z7=a("p"),Txe=o("You will then be able to use the auto classes like you would usually do!"),l8e=l(),m(Lm.$$.fragment),i8e=l(),Pi=a("h2"),Bm=a("a"),DV=a("span"),m(l5.$$.fragment),Fxe=l(),qV=a("span"),Cxe=o("AutoConfig"),d8e=l(),Go=a("div"),m(i5.$$.fragment),Mxe=l(),d5=a("p"),Exe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),V7=a("a"),yxe=o("from_pretrained()"),wxe=o(" class method."),Axe=l(),c5=a("p"),Lxe=o("This class cannot be instantiated directly using "),GV=a("code"),Bxe=o("__init__()"),kxe=o(" (throws an error)."),xxe=l(),mo=a("div"),m(m5.$$.fragment),Rxe=l(),OV=a("p"),Sxe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),Pxe=l(),$i=a("p"),$xe=o("The configuration class to instantiate is selected based on the "),XV=a("code"),Ixe=o("model_type"),jxe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=a("code"),Nxe=o("pretrained_model_name_or_path"),Dxe=o(":"),qxe=l(),v=a("ul"),km=a("li"),VV=a("strong"),Gxe=o("albert"),Oxe=o(" \u2014 "),W7=a("a"),Xxe=o("AlbertConfig"),zxe=o(" (ALBERT model)"),Vxe=l(),xm=a("li"),WV=a("strong"),Wxe=o("bart"),Qxe=o(" \u2014 "),Q7=a("a"),Hxe=o("BartConfig"),Uxe=o(" (BART model)"),Jxe=l(),Rm=a("li"),QV=a("strong"),Yxe=o("beit"),Kxe=o(" \u2014 "),H7=a("a"),Zxe=o("BeitConfig"),eRe=o(" (BEiT model)"),oRe=l(),Sm=a("li"),HV=a("strong"),rRe=o("bert"),tRe=o(" \u2014 "),U7=a("a"),aRe=o("BertConfig"),sRe=o(" (BERT model)"),nRe=l(),Pm=a("li"),UV=a("strong"),lRe=o("bert-generation"),iRe=o(" \u2014 "),J7=a("a"),dRe=o("BertGenerationConfig"),cRe=o(" (Bert Generation model)"),mRe=l(),$m=a("li"),JV=a("strong"),fRe=o("big_bird"),gRe=o(" \u2014 "),Y7=a("a"),hRe=o("BigBirdConfig"),uRe=o(" (BigBird model)"),pRe=l(),Im=a("li"),YV=a("strong"),_Re=o("bigbird_pegasus"),bRe=o(" \u2014 "),K7=a("a"),vRe=o("BigBirdPegasusConfig"),TRe=o(" (BigBirdPegasus model)"),FRe=l(),jm=a("li"),KV=a("strong"),CRe=o("blenderbot"),MRe=o(" \u2014 "),Z7=a("a"),ERe=o("BlenderbotConfig"),yRe=o(" (Blenderbot model)"),wRe=l(),Nm=a("li"),ZV=a("strong"),ARe=o("blenderbot-small"),LRe=o(" \u2014 "),e8=a("a"),BRe=o("BlenderbotSmallConfig"),kRe=o(" (BlenderbotSmall model)"),xRe=l(),Dm=a("li"),eW=a("strong"),RRe=o("camembert"),SRe=o(" \u2014 "),o8=a("a"),PRe=o("CamembertConfig"),$Re=o(" (CamemBERT model)"),IRe=l(),qm=a("li"),oW=a("strong"),jRe=o("canine"),NRe=o(" \u2014 "),r8=a("a"),DRe=o("CanineConfig"),qRe=o(" (Canine model)"),GRe=l(),Gm=a("li"),rW=a("strong"),ORe=o("clip"),XRe=o(" \u2014 "),t8=a("a"),zRe=o("CLIPConfig"),VRe=o(" (CLIP model)"),WRe=l(),Om=a("li"),tW=a("strong"),QRe=o("convbert"),HRe=o(" \u2014 "),a8=a("a"),URe=o("ConvBertConfig"),JRe=o(" (ConvBERT model)"),YRe=l(),Xm=a("li"),aW=a("strong"),KRe=o("convnext"),ZRe=o(" \u2014 "),s8=a("a"),eSe=o("ConvNextConfig"),oSe=o(" (ConvNext model)"),rSe=l(),zm=a("li"),sW=a("strong"),tSe=o("ctrl"),aSe=o(" \u2014 "),n8=a("a"),sSe=o("CTRLConfig"),nSe=o(" (CTRL model)"),lSe=l(),Vm=a("li"),nW=a("strong"),iSe=o("deberta"),dSe=o(" \u2014 "),l8=a("a"),cSe=o("DebertaConfig"),mSe=o(" (DeBERTa model)"),fSe=l(),Wm=a("li"),lW=a("strong"),gSe=o("deberta-v2"),hSe=o(" \u2014 "),i8=a("a"),uSe=o("DebertaV2Config"),pSe=o(" (DeBERTa-v2 model)"),_Se=l(),Qm=a("li"),iW=a("strong"),bSe=o("deit"),vSe=o(" \u2014 "),d8=a("a"),TSe=o("DeiTConfig"),FSe=o(" (DeiT model)"),CSe=l(),Hm=a("li"),dW=a("strong"),MSe=o("detr"),ESe=o(" \u2014 "),c8=a("a"),ySe=o("DetrConfig"),wSe=o(" (DETR model)"),ASe=l(),Um=a("li"),cW=a("strong"),LSe=o("distilbert"),BSe=o(" \u2014 "),m8=a("a"),kSe=o("DistilBertConfig"),xSe=o(" (DistilBERT model)"),RSe=l(),Jm=a("li"),mW=a("strong"),SSe=o("dpr"),PSe=o(" \u2014 "),f8=a("a"),$Se=o("DPRConfig"),ISe=o(" (DPR model)"),jSe=l(),Ym=a("li"),fW=a("strong"),NSe=o("electra"),DSe=o(" \u2014 "),g8=a("a"),qSe=o("ElectraConfig"),GSe=o(" (ELECTRA model)"),OSe=l(),Km=a("li"),gW=a("strong"),XSe=o("encoder-decoder"),zSe=o(" \u2014 "),h8=a("a"),VSe=o("EncoderDecoderConfig"),WSe=o(" (Encoder decoder model)"),QSe=l(),Zm=a("li"),hW=a("strong"),HSe=o("flaubert"),USe=o(" \u2014 "),u8=a("a"),JSe=o("FlaubertConfig"),YSe=o(" (FlauBERT model)"),KSe=l(),ef=a("li"),uW=a("strong"),ZSe=o("fnet"),ePe=o(" \u2014 "),p8=a("a"),oPe=o("FNetConfig"),rPe=o(" (FNet model)"),tPe=l(),of=a("li"),pW=a("strong"),aPe=o("fsmt"),sPe=o(" \u2014 "),_8=a("a"),nPe=o("FSMTConfig"),lPe=o(" (FairSeq Machine-Translation model)"),iPe=l(),rf=a("li"),_W=a("strong"),dPe=o("funnel"),cPe=o(" \u2014 "),b8=a("a"),mPe=o("FunnelConfig"),fPe=o(" (Funnel Transformer model)"),gPe=l(),tf=a("li"),bW=a("strong"),hPe=o("gpt2"),uPe=o(" \u2014 "),v8=a("a"),pPe=o("GPT2Config"),_Pe=o(" (OpenAI GPT-2 model)"),bPe=l(),af=a("li"),vW=a("strong"),vPe=o("gpt_neo"),TPe=o(" \u2014 "),T8=a("a"),FPe=o("GPTNeoConfig"),CPe=o(" (GPT Neo model)"),MPe=l(),sf=a("li"),TW=a("strong"),EPe=o("gptj"),yPe=o(" \u2014 "),F8=a("a"),wPe=o("GPTJConfig"),APe=o(" (GPT-J model)"),LPe=l(),nf=a("li"),FW=a("strong"),BPe=o("hubert"),kPe=o(" \u2014 "),C8=a("a"),xPe=o("HubertConfig"),RPe=o(" (Hubert model)"),SPe=l(),lf=a("li"),CW=a("strong"),PPe=o("ibert"),$Pe=o(" \u2014 "),M8=a("a"),IPe=o("IBertConfig"),jPe=o(" (I-BERT model)"),NPe=l(),df=a("li"),MW=a("strong"),DPe=o("imagegpt"),qPe=o(" \u2014 "),E8=a("a"),GPe=o("ImageGPTConfig"),OPe=o(" (ImageGPT model)"),XPe=l(),cf=a("li"),EW=a("strong"),zPe=o("layoutlm"),VPe=o(" \u2014 "),y8=a("a"),WPe=o("LayoutLMConfig"),QPe=o(" (LayoutLM model)"),HPe=l(),mf=a("li"),yW=a("strong"),UPe=o("layoutlmv2"),JPe=o(" \u2014 "),w8=a("a"),YPe=o("LayoutLMv2Config"),KPe=o(" (LayoutLMv2 model)"),ZPe=l(),ff=a("li"),wW=a("strong"),e$e=o("led"),o$e=o(" \u2014 "),A8=a("a"),r$e=o("LEDConfig"),t$e=o(" (LED model)"),a$e=l(),gf=a("li"),AW=a("strong"),s$e=o("longformer"),n$e=o(" \u2014 "),L8=a("a"),l$e=o("LongformerConfig"),i$e=o(" (Longformer model)"),d$e=l(),hf=a("li"),LW=a("strong"),c$e=o("luke"),m$e=o(" \u2014 "),B8=a("a"),f$e=o("LukeConfig"),g$e=o(" (LUKE model)"),h$e=l(),uf=a("li"),BW=a("strong"),u$e=o("lxmert"),p$e=o(" \u2014 "),k8=a("a"),_$e=o("LxmertConfig"),b$e=o(" (LXMERT model)"),v$e=l(),pf=a("li"),kW=a("strong"),T$e=o("m2m_100"),F$e=o(" \u2014 "),x8=a("a"),C$e=o("M2M100Config"),M$e=o(" (M2M100 model)"),E$e=l(),_f=a("li"),xW=a("strong"),y$e=o("marian"),w$e=o(" \u2014 "),R8=a("a"),A$e=o("MarianConfig"),L$e=o(" (Marian model)"),B$e=l(),bf=a("li"),RW=a("strong"),k$e=o("mbart"),x$e=o(" \u2014 "),S8=a("a"),R$e=o("MBartConfig"),S$e=o(" (mBART model)"),P$e=l(),vf=a("li"),SW=a("strong"),$$e=o("megatron-bert"),I$e=o(" \u2014 "),P8=a("a"),j$e=o("MegatronBertConfig"),N$e=o(" (MegatronBert model)"),D$e=l(),Tf=a("li"),PW=a("strong"),q$e=o("mobilebert"),G$e=o(" \u2014 "),$8=a("a"),O$e=o("MobileBertConfig"),X$e=o(" (MobileBERT model)"),z$e=l(),Ff=a("li"),$W=a("strong"),V$e=o("mpnet"),W$e=o(" \u2014 "),I8=a("a"),Q$e=o("MPNetConfig"),H$e=o(" (MPNet model)"),U$e=l(),Cf=a("li"),IW=a("strong"),J$e=o("mt5"),Y$e=o(" \u2014 "),j8=a("a"),K$e=o("MT5Config"),Z$e=o(" (mT5 model)"),eIe=l(),Mf=a("li"),jW=a("strong"),oIe=o("nystromformer"),rIe=o(" \u2014 "),N8=a("a"),tIe=o("NystromformerConfig"),aIe=o(" (Nystromformer model)"),sIe=l(),Ef=a("li"),NW=a("strong"),nIe=o("openai-gpt"),lIe=o(" \u2014 "),D8=a("a"),iIe=o("OpenAIGPTConfig"),dIe=o(" (OpenAI GPT model)"),cIe=l(),yf=a("li"),DW=a("strong"),mIe=o("pegasus"),fIe=o(" \u2014 "),q8=a("a"),gIe=o("PegasusConfig"),hIe=o(" (Pegasus model)"),uIe=l(),wf=a("li"),qW=a("strong"),pIe=o("perceiver"),_Ie=o(" \u2014 "),G8=a("a"),bIe=o("PerceiverConfig"),vIe=o(" (Perceiver model)"),TIe=l(),Af=a("li"),GW=a("strong"),FIe=o("plbart"),CIe=o(" \u2014 "),O8=a("a"),MIe=o("PLBartConfig"),EIe=o(" (PLBart model)"),yIe=l(),Lf=a("li"),OW=a("strong"),wIe=o("poolformer"),AIe=o(" \u2014 "),X8=a("a"),LIe=o("PoolFormerConfig"),BIe=o(" (PoolFormer model)"),kIe=l(),Bf=a("li"),XW=a("strong"),xIe=o("prophetnet"),RIe=o(" \u2014 "),z8=a("a"),SIe=o("ProphetNetConfig"),PIe=o(" (ProphetNet model)"),$Ie=l(),kf=a("li"),zW=a("strong"),IIe=o("qdqbert"),jIe=o(" \u2014 "),V8=a("a"),NIe=o("QDQBertConfig"),DIe=o(" (QDQBert model)"),qIe=l(),xf=a("li"),VW=a("strong"),GIe=o("rag"),OIe=o(" \u2014 "),W8=a("a"),XIe=o("RagConfig"),zIe=o(" (RAG model)"),VIe=l(),Rf=a("li"),WW=a("strong"),WIe=o("realm"),QIe=o(" \u2014 "),Q8=a("a"),HIe=o("RealmConfig"),UIe=o(" (Realm model)"),JIe=l(),Sf=a("li"),QW=a("strong"),YIe=o("reformer"),KIe=o(" \u2014 "),H8=a("a"),ZIe=o("ReformerConfig"),eje=o(" (Reformer model)"),oje=l(),Pf=a("li"),HW=a("strong"),rje=o("rembert"),tje=o(" \u2014 "),U8=a("a"),aje=o("RemBertConfig"),sje=o(" (RemBERT model)"),nje=l(),$f=a("li"),UW=a("strong"),lje=o("retribert"),ije=o(" \u2014 "),J8=a("a"),dje=o("RetriBertConfig"),cje=o(" (RetriBERT model)"),mje=l(),If=a("li"),JW=a("strong"),fje=o("roberta"),gje=o(" \u2014 "),Y8=a("a"),hje=o("RobertaConfig"),uje=o(" (RoBERTa model)"),pje=l(),jf=a("li"),YW=a("strong"),_je=o("roformer"),bje=o(" \u2014 "),K8=a("a"),vje=o("RoFormerConfig"),Tje=o(" (RoFormer model)"),Fje=l(),Nf=a("li"),KW=a("strong"),Cje=o("segformer"),Mje=o(" \u2014 "),Z8=a("a"),Eje=o("SegformerConfig"),yje=o(" (SegFormer model)"),wje=l(),Df=a("li"),ZW=a("strong"),Aje=o("sew"),Lje=o(" \u2014 "),e9=a("a"),Bje=o("SEWConfig"),kje=o(" (SEW model)"),xje=l(),qf=a("li"),eQ=a("strong"),Rje=o("sew-d"),Sje=o(" \u2014 "),o9=a("a"),Pje=o("SEWDConfig"),$je=o(" (SEW-D model)"),Ije=l(),Gf=a("li"),oQ=a("strong"),jje=o("speech-encoder-decoder"),Nje=o(" \u2014 "),r9=a("a"),Dje=o("SpeechEncoderDecoderConfig"),qje=o(" (Speech Encoder decoder model)"),Gje=l(),Of=a("li"),rQ=a("strong"),Oje=o("speech_to_text"),Xje=o(" \u2014 "),t9=a("a"),zje=o("Speech2TextConfig"),Vje=o(" (Speech2Text model)"),Wje=l(),Xf=a("li"),tQ=a("strong"),Qje=o("speech_to_text_2"),Hje=o(" \u2014 "),a9=a("a"),Uje=o("Speech2Text2Config"),Jje=o(" (Speech2Text2 model)"),Yje=l(),zf=a("li"),aQ=a("strong"),Kje=o("splinter"),Zje=o(" \u2014 "),s9=a("a"),eNe=o("SplinterConfig"),oNe=o(" (Splinter model)"),rNe=l(),Vf=a("li"),sQ=a("strong"),tNe=o("squeezebert"),aNe=o(" \u2014 "),n9=a("a"),sNe=o("SqueezeBertConfig"),nNe=o(" (SqueezeBERT model)"),lNe=l(),Wf=a("li"),nQ=a("strong"),iNe=o("swin"),dNe=o(" \u2014 "),l9=a("a"),cNe=o("SwinConfig"),mNe=o(" (Swin model)"),fNe=l(),Qf=a("li"),lQ=a("strong"),gNe=o("t5"),hNe=o(" \u2014 "),i9=a("a"),uNe=o("T5Config"),pNe=o(" (T5 model)"),_Ne=l(),Hf=a("li"),iQ=a("strong"),bNe=o("tapas"),vNe=o(" \u2014 "),d9=a("a"),TNe=o("TapasConfig"),FNe=o(" (TAPAS model)"),CNe=l(),Uf=a("li"),dQ=a("strong"),MNe=o("transfo-xl"),ENe=o(" \u2014 "),c9=a("a"),yNe=o("TransfoXLConfig"),wNe=o(" (Transformer-XL model)"),ANe=l(),Jf=a("li"),cQ=a("strong"),LNe=o("trocr"),BNe=o(" \u2014 "),m9=a("a"),kNe=o("TrOCRConfig"),xNe=o(" (TrOCR model)"),RNe=l(),Yf=a("li"),mQ=a("strong"),SNe=o("unispeech"),PNe=o(" \u2014 "),f9=a("a"),$Ne=o("UniSpeechConfig"),INe=o(" (UniSpeech model)"),jNe=l(),Kf=a("li"),fQ=a("strong"),NNe=o("unispeech-sat"),DNe=o(" \u2014 "),g9=a("a"),qNe=o("UniSpeechSatConfig"),GNe=o(" (UniSpeechSat model)"),ONe=l(),Zf=a("li"),gQ=a("strong"),XNe=o("vilt"),zNe=o(" \u2014 "),h9=a("a"),VNe=o("ViltConfig"),WNe=o(" (ViLT model)"),QNe=l(),eg=a("li"),hQ=a("strong"),HNe=o("vision-encoder-decoder"),UNe=o(" \u2014 "),u9=a("a"),JNe=o("VisionEncoderDecoderConfig"),YNe=o(" (Vision Encoder decoder model)"),KNe=l(),og=a("li"),uQ=a("strong"),ZNe=o("vision-text-dual-encoder"),eDe=o(" \u2014 "),p9=a("a"),oDe=o("VisionTextDualEncoderConfig"),rDe=o(" (VisionTextDualEncoder model)"),tDe=l(),rg=a("li"),pQ=a("strong"),aDe=o("visual_bert"),sDe=o(" \u2014 "),_9=a("a"),nDe=o("VisualBertConfig"),lDe=o(" (VisualBert model)"),iDe=l(),tg=a("li"),_Q=a("strong"),dDe=o("vit"),cDe=o(" \u2014 "),b9=a("a"),mDe=o("ViTConfig"),fDe=o(" (ViT model)"),gDe=l(),ag=a("li"),bQ=a("strong"),hDe=o("vit_mae"),uDe=o(" \u2014 "),v9=a("a"),pDe=o("ViTMAEConfig"),_De=o(" (ViTMAE model)"),bDe=l(),sg=a("li"),vQ=a("strong"),vDe=o("wav2vec2"),TDe=o(" \u2014 "),T9=a("a"),FDe=o("Wav2Vec2Config"),CDe=o(" (Wav2Vec2 model)"),MDe=l(),ng=a("li"),TQ=a("strong"),EDe=o("wavlm"),yDe=o(" \u2014 "),F9=a("a"),wDe=o("WavLMConfig"),ADe=o(" (WavLM model)"),LDe=l(),lg=a("li"),FQ=a("strong"),BDe=o("xglm"),kDe=o(" \u2014 "),C9=a("a"),xDe=o("XGLMConfig"),RDe=o(" (XGLM model)"),SDe=l(),ig=a("li"),CQ=a("strong"),PDe=o("xlm"),$De=o(" \u2014 "),M9=a("a"),IDe=o("XLMConfig"),jDe=o(" (XLM model)"),NDe=l(),dg=a("li"),MQ=a("strong"),DDe=o("xlm-prophetnet"),qDe=o(" \u2014 "),E9=a("a"),GDe=o("XLMProphetNetConfig"),ODe=o(" (XLMProphetNet model)"),XDe=l(),cg=a("li"),EQ=a("strong"),zDe=o("xlm-roberta"),VDe=o(" \u2014 "),y9=a("a"),WDe=o("XLMRobertaConfig"),QDe=o(" (XLM-RoBERTa model)"),HDe=l(),mg=a("li"),yQ=a("strong"),UDe=o("xlm-roberta-xl"),JDe=o(" \u2014 "),w9=a("a"),YDe=o("XLMRobertaXLConfig"),KDe=o(" (XLM-RoBERTa-XL model)"),ZDe=l(),fg=a("li"),wQ=a("strong"),eqe=o("xlnet"),oqe=o(" \u2014 "),A9=a("a"),rqe=o("XLNetConfig"),tqe=o(" (XLNet model)"),aqe=l(),gg=a("li"),AQ=a("strong"),sqe=o("yoso"),nqe=o(" \u2014 "),L9=a("a"),lqe=o("YosoConfig"),iqe=o(" (YOSO model)"),dqe=l(),LQ=a("p"),cqe=o("Examples:"),mqe=l(),m(f5.$$.fragment),fqe=l(),hg=a("div"),m(g5.$$.fragment),gqe=l(),BQ=a("p"),hqe=o("Register a new configuration for this class."),c8e=l(),Ii=a("h2"),ug=a("a"),kQ=a("span"),m(h5.$$.fragment),uqe=l(),xQ=a("span"),pqe=o("AutoTokenizer"),m8e=l(),Oo=a("div"),m(u5.$$.fragment),_qe=l(),p5=a("p"),bqe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B9=a("a"),vqe=o("AutoTokenizer.from_pretrained()"),Tqe=o(" class method."),Fqe=l(),_5=a("p"),Cqe=o("This class cannot be instantiated directly using "),RQ=a("code"),Mqe=o("__init__()"),Eqe=o(" (throws an error)."),yqe=l(),fo=a("div"),m(b5.$$.fragment),wqe=l(),SQ=a("p"),Aqe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),Lqe=l(),ja=a("p"),Bqe=o("The tokenizer class to instantiate is selected based on the "),PQ=a("code"),kqe=o("model_type"),xqe=o(` property of the config object (either
passed as an argument or loaded from `),$Q=a("code"),Rqe=o("pretrained_model_name_or_path"),Sqe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=a("code"),Pqe=o("pretrained_model_name_or_path"),$qe=o(":"),Iqe=l(),M=a("ul"),Ns=a("li"),jQ=a("strong"),jqe=o("albert"),Nqe=o(" \u2014 "),k9=a("a"),Dqe=o("AlbertTokenizer"),qqe=o(" or "),x9=a("a"),Gqe=o("AlbertTokenizerFast"),Oqe=o(" (ALBERT model)"),Xqe=l(),Ds=a("li"),NQ=a("strong"),zqe=o("bart"),Vqe=o(" \u2014 "),R9=a("a"),Wqe=o("BartTokenizer"),Qqe=o(" or "),S9=a("a"),Hqe=o("BartTokenizerFast"),Uqe=o(" (BART model)"),Jqe=l(),qs=a("li"),DQ=a("strong"),Yqe=o("barthez"),Kqe=o(" \u2014 "),P9=a("a"),Zqe=o("BarthezTokenizer"),eGe=o(" or "),$9=a("a"),oGe=o("BarthezTokenizerFast"),rGe=o(" (BARThez model)"),tGe=l(),pg=a("li"),qQ=a("strong"),aGe=o("bartpho"),sGe=o(" \u2014 "),I9=a("a"),nGe=o("BartphoTokenizer"),lGe=o(" (BARTpho model)"),iGe=l(),Gs=a("li"),GQ=a("strong"),dGe=o("bert"),cGe=o(" \u2014 "),j9=a("a"),mGe=o("BertTokenizer"),fGe=o(" or "),N9=a("a"),gGe=o("BertTokenizerFast"),hGe=o(" (BERT model)"),uGe=l(),_g=a("li"),OQ=a("strong"),pGe=o("bert-generation"),_Ge=o(" \u2014 "),D9=a("a"),bGe=o("BertGenerationTokenizer"),vGe=o(" (Bert Generation model)"),TGe=l(),bg=a("li"),XQ=a("strong"),FGe=o("bert-japanese"),CGe=o(" \u2014 "),q9=a("a"),MGe=o("BertJapaneseTokenizer"),EGe=o(" (BertJapanese model)"),yGe=l(),vg=a("li"),zQ=a("strong"),wGe=o("bertweet"),AGe=o(" \u2014 "),G9=a("a"),LGe=o("BertweetTokenizer"),BGe=o(" (Bertweet model)"),kGe=l(),Os=a("li"),VQ=a("strong"),xGe=o("big_bird"),RGe=o(" \u2014 "),O9=a("a"),SGe=o("BigBirdTokenizer"),PGe=o(" or "),X9=a("a"),$Ge=o("BigBirdTokenizerFast"),IGe=o(" (BigBird model)"),jGe=l(),Xs=a("li"),WQ=a("strong"),NGe=o("bigbird_pegasus"),DGe=o(" \u2014 "),z9=a("a"),qGe=o("PegasusTokenizer"),GGe=o(" or "),V9=a("a"),OGe=o("PegasusTokenizerFast"),XGe=o(" (BigBirdPegasus model)"),zGe=l(),zs=a("li"),QQ=a("strong"),VGe=o("blenderbot"),WGe=o(" \u2014 "),W9=a("a"),QGe=o("BlenderbotTokenizer"),HGe=o(" or "),Q9=a("a"),UGe=o("BlenderbotTokenizerFast"),JGe=o(" (Blenderbot model)"),YGe=l(),Tg=a("li"),HQ=a("strong"),KGe=o("blenderbot-small"),ZGe=o(" \u2014 "),H9=a("a"),eOe=o("BlenderbotSmallTokenizer"),oOe=o(" (BlenderbotSmall model)"),rOe=l(),Fg=a("li"),UQ=a("strong"),tOe=o("byt5"),aOe=o(" \u2014 "),U9=a("a"),sOe=o("ByT5Tokenizer"),nOe=o(" (ByT5 model)"),lOe=l(),Vs=a("li"),JQ=a("strong"),iOe=o("camembert"),dOe=o(" \u2014 "),J9=a("a"),cOe=o("CamembertTokenizer"),mOe=o(" or "),Y9=a("a"),fOe=o("CamembertTokenizerFast"),gOe=o(" (CamemBERT model)"),hOe=l(),Cg=a("li"),YQ=a("strong"),uOe=o("canine"),pOe=o(" \u2014 "),K9=a("a"),_Oe=o("CanineTokenizer"),bOe=o(" (Canine model)"),vOe=l(),Ws=a("li"),KQ=a("strong"),TOe=o("clip"),FOe=o(" \u2014 "),Z9=a("a"),COe=o("CLIPTokenizer"),MOe=o(" or "),eB=a("a"),EOe=o("CLIPTokenizerFast"),yOe=o(" (CLIP model)"),wOe=l(),Qs=a("li"),ZQ=a("strong"),AOe=o("convbert"),LOe=o(" \u2014 "),oB=a("a"),BOe=o("ConvBertTokenizer"),kOe=o(" or "),rB=a("a"),xOe=o("ConvBertTokenizerFast"),ROe=o(" (ConvBERT model)"),SOe=l(),Hs=a("li"),eH=a("strong"),POe=o("cpm"),$Oe=o(" \u2014 "),tB=a("a"),IOe=o("CpmTokenizer"),jOe=o(" or "),oH=a("code"),NOe=o("CpmTokenizerFast"),DOe=o(" (CPM model)"),qOe=l(),Mg=a("li"),rH=a("strong"),GOe=o("ctrl"),OOe=o(" \u2014 "),aB=a("a"),XOe=o("CTRLTokenizer"),zOe=o(" (CTRL model)"),VOe=l(),Us=a("li"),tH=a("strong"),WOe=o("deberta"),QOe=o(" \u2014 "),sB=a("a"),HOe=o("DebertaTokenizer"),UOe=o(" or "),nB=a("a"),JOe=o("DebertaTokenizerFast"),YOe=o(" (DeBERTa model)"),KOe=l(),Eg=a("li"),aH=a("strong"),ZOe=o("deberta-v2"),eXe=o(" \u2014 "),lB=a("a"),oXe=o("DebertaV2Tokenizer"),rXe=o(" (DeBERTa-v2 model)"),tXe=l(),Js=a("li"),sH=a("strong"),aXe=o("distilbert"),sXe=o(" \u2014 "),iB=a("a"),nXe=o("DistilBertTokenizer"),lXe=o(" or "),dB=a("a"),iXe=o("DistilBertTokenizerFast"),dXe=o(" (DistilBERT model)"),cXe=l(),Ys=a("li"),nH=a("strong"),mXe=o("dpr"),fXe=o(" \u2014 "),cB=a("a"),gXe=o("DPRQuestionEncoderTokenizer"),hXe=o(" or "),mB=a("a"),uXe=o("DPRQuestionEncoderTokenizerFast"),pXe=o(" (DPR model)"),_Xe=l(),Ks=a("li"),lH=a("strong"),bXe=o("electra"),vXe=o(" \u2014 "),fB=a("a"),TXe=o("ElectraTokenizer"),FXe=o(" or "),gB=a("a"),CXe=o("ElectraTokenizerFast"),MXe=o(" (ELECTRA model)"),EXe=l(),yg=a("li"),iH=a("strong"),yXe=o("flaubert"),wXe=o(" \u2014 "),hB=a("a"),AXe=o("FlaubertTokenizer"),LXe=o(" (FlauBERT model)"),BXe=l(),Zs=a("li"),dH=a("strong"),kXe=o("fnet"),xXe=o(" \u2014 "),uB=a("a"),RXe=o("FNetTokenizer"),SXe=o(" or "),pB=a("a"),PXe=o("FNetTokenizerFast"),$Xe=o(" (FNet model)"),IXe=l(),wg=a("li"),cH=a("strong"),jXe=o("fsmt"),NXe=o(" \u2014 "),_B=a("a"),DXe=o("FSMTTokenizer"),qXe=o(" (FairSeq Machine-Translation model)"),GXe=l(),en=a("li"),mH=a("strong"),OXe=o("funnel"),XXe=o(" \u2014 "),bB=a("a"),zXe=o("FunnelTokenizer"),VXe=o(" or "),vB=a("a"),WXe=o("FunnelTokenizerFast"),QXe=o(" (Funnel Transformer model)"),HXe=l(),on=a("li"),fH=a("strong"),UXe=o("gpt2"),JXe=o(" \u2014 "),TB=a("a"),YXe=o("GPT2Tokenizer"),KXe=o(" or "),FB=a("a"),ZXe=o("GPT2TokenizerFast"),eze=o(" (OpenAI GPT-2 model)"),oze=l(),rn=a("li"),gH=a("strong"),rze=o("gpt_neo"),tze=o(" \u2014 "),CB=a("a"),aze=o("GPT2Tokenizer"),sze=o(" or "),MB=a("a"),nze=o("GPT2TokenizerFast"),lze=o(" (GPT Neo model)"),ize=l(),tn=a("li"),hH=a("strong"),dze=o("herbert"),cze=o(" \u2014 "),EB=a("a"),mze=o("HerbertTokenizer"),fze=o(" or "),yB=a("a"),gze=o("HerbertTokenizerFast"),hze=o(" (HerBERT model)"),uze=l(),Ag=a("li"),uH=a("strong"),pze=o("hubert"),_ze=o(" \u2014 "),wB=a("a"),bze=o("Wav2Vec2CTCTokenizer"),vze=o(" (Hubert model)"),Tze=l(),an=a("li"),pH=a("strong"),Fze=o("ibert"),Cze=o(" \u2014 "),AB=a("a"),Mze=o("RobertaTokenizer"),Eze=o(" or "),LB=a("a"),yze=o("RobertaTokenizerFast"),wze=o(" (I-BERT model)"),Aze=l(),sn=a("li"),_H=a("strong"),Lze=o("layoutlm"),Bze=o(" \u2014 "),BB=a("a"),kze=o("LayoutLMTokenizer"),xze=o(" or "),kB=a("a"),Rze=o("LayoutLMTokenizerFast"),Sze=o(" (LayoutLM model)"),Pze=l(),nn=a("li"),bH=a("strong"),$ze=o("layoutlmv2"),Ize=o(" \u2014 "),xB=a("a"),jze=o("LayoutLMv2Tokenizer"),Nze=o(" or "),RB=a("a"),Dze=o("LayoutLMv2TokenizerFast"),qze=o(" (LayoutLMv2 model)"),Gze=l(),ln=a("li"),vH=a("strong"),Oze=o("layoutxlm"),Xze=o(" \u2014 "),SB=a("a"),zze=o("LayoutXLMTokenizer"),Vze=o(" or "),PB=a("a"),Wze=o("LayoutXLMTokenizerFast"),Qze=o(" (LayoutXLM model)"),Hze=l(),dn=a("li"),TH=a("strong"),Uze=o("led"),Jze=o(" \u2014 "),$B=a("a"),Yze=o("LEDTokenizer"),Kze=o(" or "),IB=a("a"),Zze=o("LEDTokenizerFast"),eVe=o(" (LED model)"),oVe=l(),cn=a("li"),FH=a("strong"),rVe=o("longformer"),tVe=o(" \u2014 "),jB=a("a"),aVe=o("LongformerTokenizer"),sVe=o(" or "),NB=a("a"),nVe=o("LongformerTokenizerFast"),lVe=o(" (Longformer model)"),iVe=l(),Lg=a("li"),CH=a("strong"),dVe=o("luke"),cVe=o(" \u2014 "),DB=a("a"),mVe=o("LukeTokenizer"),fVe=o(" (LUKE model)"),gVe=l(),mn=a("li"),MH=a("strong"),hVe=o("lxmert"),uVe=o(" \u2014 "),qB=a("a"),pVe=o("LxmertTokenizer"),_Ve=o(" or "),GB=a("a"),bVe=o("LxmertTokenizerFast"),vVe=o(" (LXMERT model)"),TVe=l(),Bg=a("li"),EH=a("strong"),FVe=o("m2m_100"),CVe=o(" \u2014 "),OB=a("a"),MVe=o("M2M100Tokenizer"),EVe=o(" (M2M100 model)"),yVe=l(),kg=a("li"),yH=a("strong"),wVe=o("marian"),AVe=o(" \u2014 "),XB=a("a"),LVe=o("MarianTokenizer"),BVe=o(" (Marian model)"),kVe=l(),fn=a("li"),wH=a("strong"),xVe=o("mbart"),RVe=o(" \u2014 "),zB=a("a"),SVe=o("MBartTokenizer"),PVe=o(" or "),VB=a("a"),$Ve=o("MBartTokenizerFast"),IVe=o(" (mBART model)"),jVe=l(),gn=a("li"),AH=a("strong"),NVe=o("mbart50"),DVe=o(" \u2014 "),WB=a("a"),qVe=o("MBart50Tokenizer"),GVe=o(" or "),QB=a("a"),OVe=o("MBart50TokenizerFast"),XVe=o(" (mBART-50 model)"),zVe=l(),xg=a("li"),LH=a("strong"),VVe=o("mluke"),WVe=o(" \u2014 "),HB=a("a"),QVe=o("MLukeTokenizer"),HVe=o(" (mLUKE model)"),UVe=l(),hn=a("li"),BH=a("strong"),JVe=o("mobilebert"),YVe=o(" \u2014 "),UB=a("a"),KVe=o("MobileBertTokenizer"),ZVe=o(" or "),JB=a("a"),eWe=o("MobileBertTokenizerFast"),oWe=o(" (MobileBERT model)"),rWe=l(),un=a("li"),kH=a("strong"),tWe=o("mpnet"),aWe=o(" \u2014 "),YB=a("a"),sWe=o("MPNetTokenizer"),nWe=o(" or "),KB=a("a"),lWe=o("MPNetTokenizerFast"),iWe=o(" (MPNet model)"),dWe=l(),pn=a("li"),xH=a("strong"),cWe=o("mt5"),mWe=o(" \u2014 "),ZB=a("a"),fWe=o("MT5Tokenizer"),gWe=o(" or "),ek=a("a"),hWe=o("MT5TokenizerFast"),uWe=o(" (mT5 model)"),pWe=l(),_n=a("li"),RH=a("strong"),_We=o("openai-gpt"),bWe=o(" \u2014 "),ok=a("a"),vWe=o("OpenAIGPTTokenizer"),TWe=o(" or "),rk=a("a"),FWe=o("OpenAIGPTTokenizerFast"),CWe=o(" (OpenAI GPT model)"),MWe=l(),bn=a("li"),SH=a("strong"),EWe=o("pegasus"),yWe=o(" \u2014 "),tk=a("a"),wWe=o("PegasusTokenizer"),AWe=o(" or "),ak=a("a"),LWe=o("PegasusTokenizerFast"),BWe=o(" (Pegasus model)"),kWe=l(),Rg=a("li"),PH=a("strong"),xWe=o("perceiver"),RWe=o(" \u2014 "),sk=a("a"),SWe=o("PerceiverTokenizer"),PWe=o(" (Perceiver model)"),$We=l(),Sg=a("li"),$H=a("strong"),IWe=o("phobert"),jWe=o(" \u2014 "),nk=a("a"),NWe=o("PhobertTokenizer"),DWe=o(" (PhoBERT model)"),qWe=l(),Pg=a("li"),IH=a("strong"),GWe=o("plbart"),OWe=o(" \u2014 "),lk=a("a"),XWe=o("PLBartTokenizer"),zWe=o(" (PLBart model)"),VWe=l(),$g=a("li"),jH=a("strong"),WWe=o("prophetnet"),QWe=o(" \u2014 "),ik=a("a"),HWe=o("ProphetNetTokenizer"),UWe=o(" (ProphetNet model)"),JWe=l(),vn=a("li"),NH=a("strong"),YWe=o("qdqbert"),KWe=o(" \u2014 "),dk=a("a"),ZWe=o("BertTokenizer"),eQe=o(" or "),ck=a("a"),oQe=o("BertTokenizerFast"),rQe=o(" (QDQBert model)"),tQe=l(),Ig=a("li"),DH=a("strong"),aQe=o("rag"),sQe=o(" \u2014 "),mk=a("a"),nQe=o("RagTokenizer"),lQe=o(" (RAG model)"),iQe=l(),Tn=a("li"),qH=a("strong"),dQe=o("reformer"),cQe=o(" \u2014 "),fk=a("a"),mQe=o("ReformerTokenizer"),fQe=o(" or "),gk=a("a"),gQe=o("ReformerTokenizerFast"),hQe=o(" (Reformer model)"),uQe=l(),Fn=a("li"),GH=a("strong"),pQe=o("rembert"),_Qe=o(" \u2014 "),hk=a("a"),bQe=o("RemBertTokenizer"),vQe=o(" or "),uk=a("a"),TQe=o("RemBertTokenizerFast"),FQe=o(" (RemBERT model)"),CQe=l(),Cn=a("li"),OH=a("strong"),MQe=o("retribert"),EQe=o(" \u2014 "),pk=a("a"),yQe=o("RetriBertTokenizer"),wQe=o(" or "),_k=a("a"),AQe=o("RetriBertTokenizerFast"),LQe=o(" (RetriBERT model)"),BQe=l(),Mn=a("li"),XH=a("strong"),kQe=o("roberta"),xQe=o(" \u2014 "),bk=a("a"),RQe=o("RobertaTokenizer"),SQe=o(" or "),vk=a("a"),PQe=o("RobertaTokenizerFast"),$Qe=o(" (RoBERTa model)"),IQe=l(),En=a("li"),zH=a("strong"),jQe=o("roformer"),NQe=o(" \u2014 "),Tk=a("a"),DQe=o("RoFormerTokenizer"),qQe=o(" or "),Fk=a("a"),GQe=o("RoFormerTokenizerFast"),OQe=o(" (RoFormer model)"),XQe=l(),jg=a("li"),VH=a("strong"),zQe=o("speech_to_text"),VQe=o(" \u2014 "),Ck=a("a"),WQe=o("Speech2TextTokenizer"),QQe=o(" (Speech2Text model)"),HQe=l(),Ng=a("li"),WH=a("strong"),UQe=o("speech_to_text_2"),JQe=o(" \u2014 "),Mk=a("a"),YQe=o("Speech2Text2Tokenizer"),KQe=o(" (Speech2Text2 model)"),ZQe=l(),yn=a("li"),QH=a("strong"),eHe=o("splinter"),oHe=o(" \u2014 "),Ek=a("a"),rHe=o("SplinterTokenizer"),tHe=o(" or "),yk=a("a"),aHe=o("SplinterTokenizerFast"),sHe=o(" (Splinter model)"),nHe=l(),wn=a("li"),HH=a("strong"),lHe=o("squeezebert"),iHe=o(" \u2014 "),wk=a("a"),dHe=o("SqueezeBertTokenizer"),cHe=o(" or "),Ak=a("a"),mHe=o("SqueezeBertTokenizerFast"),fHe=o(" (SqueezeBERT model)"),gHe=l(),An=a("li"),UH=a("strong"),hHe=o("t5"),uHe=o(" \u2014 "),Lk=a("a"),pHe=o("T5Tokenizer"),_He=o(" or "),Bk=a("a"),bHe=o("T5TokenizerFast"),vHe=o(" (T5 model)"),THe=l(),Dg=a("li"),JH=a("strong"),FHe=o("tapas"),CHe=o(" \u2014 "),kk=a("a"),MHe=o("TapasTokenizer"),EHe=o(" (TAPAS model)"),yHe=l(),qg=a("li"),YH=a("strong"),wHe=o("transfo-xl"),AHe=o(" \u2014 "),xk=a("a"),LHe=o("TransfoXLTokenizer"),BHe=o(" (Transformer-XL model)"),kHe=l(),Gg=a("li"),KH=a("strong"),xHe=o("wav2vec2"),RHe=o(" \u2014 "),Rk=a("a"),SHe=o("Wav2Vec2CTCTokenizer"),PHe=o(" (Wav2Vec2 model)"),$He=l(),Og=a("li"),ZH=a("strong"),IHe=o("wav2vec2_phoneme"),jHe=o(" \u2014 "),Sk=a("a"),NHe=o("Wav2Vec2PhonemeCTCTokenizer"),DHe=o(" (Wav2Vec2Phoneme model)"),qHe=l(),Ln=a("li"),eU=a("strong"),GHe=o("xglm"),OHe=o(" \u2014 "),Pk=a("a"),XHe=o("XGLMTokenizer"),zHe=o(" or "),$k=a("a"),VHe=o("XGLMTokenizerFast"),WHe=o(" (XGLM model)"),QHe=l(),Xg=a("li"),oU=a("strong"),HHe=o("xlm"),UHe=o(" \u2014 "),Ik=a("a"),JHe=o("XLMTokenizer"),YHe=o(" (XLM model)"),KHe=l(),zg=a("li"),rU=a("strong"),ZHe=o("xlm-prophetnet"),eUe=o(" \u2014 "),jk=a("a"),oUe=o("XLMProphetNetTokenizer"),rUe=o(" (XLMProphetNet model)"),tUe=l(),Bn=a("li"),tU=a("strong"),aUe=o("xlm-roberta"),sUe=o(" \u2014 "),Nk=a("a"),nUe=o("XLMRobertaTokenizer"),lUe=o(" or "),Dk=a("a"),iUe=o("XLMRobertaTokenizerFast"),dUe=o(" (XLM-RoBERTa model)"),cUe=l(),kn=a("li"),aU=a("strong"),mUe=o("xlnet"),fUe=o(" \u2014 "),qk=a("a"),gUe=o("XLNetTokenizer"),hUe=o(" or "),Gk=a("a"),uUe=o("XLNetTokenizerFast"),pUe=o(" (XLNet model)"),_Ue=l(),sU=a("p"),bUe=o("Examples:"),vUe=l(),m(v5.$$.fragment),TUe=l(),Vg=a("div"),m(T5.$$.fragment),FUe=l(),nU=a("p"),CUe=o("Register a new tokenizer in this mapping."),f8e=l(),ji=a("h2"),Wg=a("a"),lU=a("span"),m(F5.$$.fragment),MUe=l(),iU=a("span"),EUe=o("AutoFeatureExtractor"),g8e=l(),Xo=a("div"),m(C5.$$.fragment),yUe=l(),M5=a("p"),wUe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=a("a"),AUe=o("AutoFeatureExtractor.from_pretrained()"),LUe=o(" class method."),BUe=l(),E5=a("p"),kUe=o("This class cannot be instantiated directly using "),dU=a("code"),xUe=o("__init__()"),RUe=o(" (throws an error)."),SUe=l(),Le=a("div"),m(y5.$$.fragment),PUe=l(),cU=a("p"),$Ue=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),IUe=l(),Na=a("p"),jUe=o("The feature extractor class to instantiate is selected based on the "),mU=a("code"),NUe=o("model_type"),DUe=o(` property of the config object
(either passed as an argument or loaded from `),fU=a("code"),qUe=o("pretrained_model_name_or_path"),GUe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=a("code"),OUe=o("pretrained_model_name_or_path"),XUe=o(":"),zUe=l(),ne=a("ul"),Qg=a("li"),hU=a("strong"),VUe=o("beit"),WUe=o(" \u2014 "),Xk=a("a"),QUe=o("BeitFeatureExtractor"),HUe=o(" (BEiT model)"),UUe=l(),Hg=a("li"),uU=a("strong"),JUe=o("clip"),YUe=o(" \u2014 "),zk=a("a"),KUe=o("CLIPFeatureExtractor"),ZUe=o(" (CLIP model)"),eJe=l(),Ug=a("li"),pU=a("strong"),oJe=o("convnext"),rJe=o(" \u2014 "),Vk=a("a"),tJe=o("ConvNextFeatureExtractor"),aJe=o(" (ConvNext model)"),sJe=l(),Jg=a("li"),_U=a("strong"),nJe=o("deit"),lJe=o(" \u2014 "),Wk=a("a"),iJe=o("DeiTFeatureExtractor"),dJe=o(" (DeiT model)"),cJe=l(),Yg=a("li"),bU=a("strong"),mJe=o("detr"),fJe=o(" \u2014 "),Qk=a("a"),gJe=o("DetrFeatureExtractor"),hJe=o(" (DETR model)"),uJe=l(),Kg=a("li"),vU=a("strong"),pJe=o("hubert"),_Je=o(" \u2014 "),Hk=a("a"),bJe=o("Wav2Vec2FeatureExtractor"),vJe=o(" (Hubert model)"),TJe=l(),Zg=a("li"),TU=a("strong"),FJe=o("layoutlmv2"),CJe=o(" \u2014 "),Uk=a("a"),MJe=o("LayoutLMv2FeatureExtractor"),EJe=o(" (LayoutLMv2 model)"),yJe=l(),eh=a("li"),FU=a("strong"),wJe=o("perceiver"),AJe=o(" \u2014 "),Jk=a("a"),LJe=o("PerceiverFeatureExtractor"),BJe=o(" (Perceiver model)"),kJe=l(),oh=a("li"),CU=a("strong"),xJe=o("poolformer"),RJe=o(" \u2014 "),Yk=a("a"),SJe=o("PoolFormerFeatureExtractor"),PJe=o(" (PoolFormer model)"),$Je=l(),rh=a("li"),MU=a("strong"),IJe=o("segformer"),jJe=o(" \u2014 "),Kk=a("a"),NJe=o("SegformerFeatureExtractor"),DJe=o(" (SegFormer model)"),qJe=l(),th=a("li"),EU=a("strong"),GJe=o("speech_to_text"),OJe=o(" \u2014 "),Zk=a("a"),XJe=o("Speech2TextFeatureExtractor"),zJe=o(" (Speech2Text model)"),VJe=l(),ah=a("li"),yU=a("strong"),WJe=o("swin"),QJe=o(" \u2014 "),ex=a("a"),HJe=o("ViTFeatureExtractor"),UJe=o(" (Swin model)"),JJe=l(),sh=a("li"),wU=a("strong"),YJe=o("vit"),KJe=o(" \u2014 "),ox=a("a"),ZJe=o("ViTFeatureExtractor"),eYe=o(" (ViT model)"),oYe=l(),nh=a("li"),AU=a("strong"),rYe=o("vit_mae"),tYe=o(" \u2014 "),rx=a("a"),aYe=o("ViTFeatureExtractor"),sYe=o(" (ViTMAE model)"),nYe=l(),lh=a("li"),LU=a("strong"),lYe=o("wav2vec2"),iYe=o(" \u2014 "),tx=a("a"),dYe=o("Wav2Vec2FeatureExtractor"),cYe=o(" (Wav2Vec2 model)"),mYe=l(),m(ih.$$.fragment),fYe=l(),BU=a("p"),gYe=o("Examples:"),hYe=l(),m(w5.$$.fragment),uYe=l(),dh=a("div"),m(A5.$$.fragment),pYe=l(),kU=a("p"),_Ye=o("Register a new feature extractor for this class."),h8e=l(),Ni=a("h2"),ch=a("a"),xU=a("span"),m(L5.$$.fragment),bYe=l(),RU=a("span"),vYe=o("AutoProcessor"),u8e=l(),zo=a("div"),m(B5.$$.fragment),TYe=l(),k5=a("p"),FYe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=a("a"),CYe=o("AutoProcessor.from_pretrained()"),MYe=o(" class method."),EYe=l(),x5=a("p"),yYe=o("This class cannot be instantiated directly using "),SU=a("code"),wYe=o("__init__()"),AYe=o(" (throws an error)."),LYe=l(),Be=a("div"),m(R5.$$.fragment),BYe=l(),PU=a("p"),kYe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),xYe=l(),Di=a("p"),RYe=o("The processor class to instantiate is selected based on the "),$U=a("code"),SYe=o("model_type"),PYe=o(` property of the config object (either
passed as an argument or loaded from `),IU=a("code"),$Ye=o("pretrained_model_name_or_path"),IYe=o(" if possible):"),jYe=l(),we=a("ul"),mh=a("li"),jU=a("strong"),NYe=o("clip"),DYe=o(" \u2014 "),sx=a("a"),qYe=o("CLIPProcessor"),GYe=o(" (CLIP model)"),OYe=l(),fh=a("li"),NU=a("strong"),XYe=o("layoutlmv2"),zYe=o(" \u2014 "),nx=a("a"),VYe=o("LayoutLMv2Processor"),WYe=o(" (LayoutLMv2 model)"),QYe=l(),gh=a("li"),DU=a("strong"),HYe=o("layoutxlm"),UYe=o(" \u2014 "),lx=a("a"),JYe=o("LayoutXLMProcessor"),YYe=o(" (LayoutXLM model)"),KYe=l(),hh=a("li"),qU=a("strong"),ZYe=o("speech_to_text"),eKe=o(" \u2014 "),ix=a("a"),oKe=o("Speech2TextProcessor"),rKe=o(" (Speech2Text model)"),tKe=l(),uh=a("li"),GU=a("strong"),aKe=o("speech_to_text_2"),sKe=o(" \u2014 "),dx=a("a"),nKe=o("Speech2Text2Processor"),lKe=o(" (Speech2Text2 model)"),iKe=l(),ph=a("li"),OU=a("strong"),dKe=o("trocr"),cKe=o(" \u2014 "),cx=a("a"),mKe=o("TrOCRProcessor"),fKe=o(" (TrOCR model)"),gKe=l(),_h=a("li"),XU=a("strong"),hKe=o("vision-text-dual-encoder"),uKe=o(" \u2014 "),mx=a("a"),pKe=o("VisionTextDualEncoderProcessor"),_Ke=o(" (VisionTextDualEncoder model)"),bKe=l(),bh=a("li"),zU=a("strong"),vKe=o("wav2vec2"),TKe=o(" \u2014 "),fx=a("a"),FKe=o("Wav2Vec2Processor"),CKe=o(" (Wav2Vec2 model)"),MKe=l(),m(vh.$$.fragment),EKe=l(),VU=a("p"),yKe=o("Examples:"),wKe=l(),m(S5.$$.fragment),AKe=l(),Th=a("div"),m(P5.$$.fragment),LKe=l(),WU=a("p"),BKe=o("Register a new processor for this class."),p8e=l(),qi=a("h2"),Fh=a("a"),QU=a("span"),m($5.$$.fragment),kKe=l(),HU=a("span"),xKe=o("AutoModel"),_8e=l(),Vo=a("div"),m(I5.$$.fragment),RKe=l(),Gi=a("p"),SKe=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=a("code"),PKe=o("from_pretrained()"),$Ke=o("class method or the "),JU=a("code"),IKe=o("from_config()"),jKe=o(`class
method.`),NKe=l(),j5=a("p"),DKe=o("This class cannot be instantiated directly using "),YU=a("code"),qKe=o("__init__()"),GKe=o(" (throws an error)."),OKe=l(),Nr=a("div"),m(N5.$$.fragment),XKe=l(),KU=a("p"),zKe=o("Instantiates one of the base model classes of the library from a configuration."),VKe=l(),Oi=a("p"),WKe=o(`Note:
Loading a model from its configuration file does `),ZU=a("strong"),QKe=o("not"),HKe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=a("code"),UKe=o("from_pretrained()"),JKe=o("to load the model weights."),YKe=l(),oJ=a("p"),KKe=o("Examples:"),ZKe=l(),m(D5.$$.fragment),eZe=l(),ke=a("div"),m(q5.$$.fragment),oZe=l(),rJ=a("p"),rZe=o("Instantiate one of the base model classes of the library from a pretrained model."),tZe=l(),Da=a("p"),aZe=o("The model class to instantiate is selected based on the "),tJ=a("code"),sZe=o("model_type"),nZe=o(` property of the config object (either
passed as an argument or loaded from `),aJ=a("code"),lZe=o("pretrained_model_name_or_path"),iZe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sJ=a("code"),dZe=o("pretrained_model_name_or_path"),cZe=o(":"),mZe=l(),F=a("ul"),Ch=a("li"),nJ=a("strong"),fZe=o("albert"),gZe=o(" \u2014 "),gx=a("a"),hZe=o("AlbertModel"),uZe=o(" (ALBERT model)"),pZe=l(),Mh=a("li"),lJ=a("strong"),_Ze=o("bart"),bZe=o(" \u2014 "),hx=a("a"),vZe=o("BartModel"),TZe=o(" (BART model)"),FZe=l(),Eh=a("li"),iJ=a("strong"),CZe=o("beit"),MZe=o(" \u2014 "),ux=a("a"),EZe=o("BeitModel"),yZe=o(" (BEiT model)"),wZe=l(),yh=a("li"),dJ=a("strong"),AZe=o("bert"),LZe=o(" \u2014 "),px=a("a"),BZe=o("BertModel"),kZe=o(" (BERT model)"),xZe=l(),wh=a("li"),cJ=a("strong"),RZe=o("bert-generation"),SZe=o(" \u2014 "),_x=a("a"),PZe=o("BertGenerationEncoder"),$Ze=o(" (Bert Generation model)"),IZe=l(),Ah=a("li"),mJ=a("strong"),jZe=o("big_bird"),NZe=o(" \u2014 "),bx=a("a"),DZe=o("BigBirdModel"),qZe=o(" (BigBird model)"),GZe=l(),Lh=a("li"),fJ=a("strong"),OZe=o("bigbird_pegasus"),XZe=o(" \u2014 "),vx=a("a"),zZe=o("BigBirdPegasusModel"),VZe=o(" (BigBirdPegasus model)"),WZe=l(),Bh=a("li"),gJ=a("strong"),QZe=o("blenderbot"),HZe=o(" \u2014 "),Tx=a("a"),UZe=o("BlenderbotModel"),JZe=o(" (Blenderbot model)"),YZe=l(),kh=a("li"),hJ=a("strong"),KZe=o("blenderbot-small"),ZZe=o(" \u2014 "),Fx=a("a"),eeo=o("BlenderbotSmallModel"),oeo=o(" (BlenderbotSmall model)"),reo=l(),xh=a("li"),uJ=a("strong"),teo=o("camembert"),aeo=o(" \u2014 "),Cx=a("a"),seo=o("CamembertModel"),neo=o(" (CamemBERT model)"),leo=l(),Rh=a("li"),pJ=a("strong"),ieo=o("canine"),deo=o(" \u2014 "),Mx=a("a"),ceo=o("CanineModel"),meo=o(" (Canine model)"),feo=l(),Sh=a("li"),_J=a("strong"),geo=o("clip"),heo=o(" \u2014 "),Ex=a("a"),ueo=o("CLIPModel"),peo=o(" (CLIP model)"),_eo=l(),Ph=a("li"),bJ=a("strong"),beo=o("convbert"),veo=o(" \u2014 "),yx=a("a"),Teo=o("ConvBertModel"),Feo=o(" (ConvBERT model)"),Ceo=l(),$h=a("li"),vJ=a("strong"),Meo=o("convnext"),Eeo=o(" \u2014 "),wx=a("a"),yeo=o("ConvNextModel"),weo=o(" (ConvNext model)"),Aeo=l(),Ih=a("li"),TJ=a("strong"),Leo=o("ctrl"),Beo=o(" \u2014 "),Ax=a("a"),keo=o("CTRLModel"),xeo=o(" (CTRL model)"),Reo=l(),jh=a("li"),FJ=a("strong"),Seo=o("deberta"),Peo=o(" \u2014 "),Lx=a("a"),$eo=o("DebertaModel"),Ieo=o(" (DeBERTa model)"),jeo=l(),Nh=a("li"),CJ=a("strong"),Neo=o("deberta-v2"),Deo=o(" \u2014 "),Bx=a("a"),qeo=o("DebertaV2Model"),Geo=o(" (DeBERTa-v2 model)"),Oeo=l(),Dh=a("li"),MJ=a("strong"),Xeo=o("deit"),zeo=o(" \u2014 "),kx=a("a"),Veo=o("DeiTModel"),Weo=o(" (DeiT model)"),Qeo=l(),qh=a("li"),EJ=a("strong"),Heo=o("detr"),Ueo=o(" \u2014 "),xx=a("a"),Jeo=o("DetrModel"),Yeo=o(" (DETR model)"),Keo=l(),Gh=a("li"),yJ=a("strong"),Zeo=o("distilbert"),eoo=o(" \u2014 "),Rx=a("a"),ooo=o("DistilBertModel"),roo=o(" (DistilBERT model)"),too=l(),Oh=a("li"),wJ=a("strong"),aoo=o("dpr"),soo=o(" \u2014 "),Sx=a("a"),noo=o("DPRQuestionEncoder"),loo=o(" (DPR model)"),ioo=l(),Xh=a("li"),AJ=a("strong"),doo=o("electra"),coo=o(" \u2014 "),Px=a("a"),moo=o("ElectraModel"),foo=o(" (ELECTRA model)"),goo=l(),zh=a("li"),LJ=a("strong"),hoo=o("flaubert"),uoo=o(" \u2014 "),$x=a("a"),poo=o("FlaubertModel"),_oo=o(" (FlauBERT model)"),boo=l(),Vh=a("li"),BJ=a("strong"),voo=o("fnet"),Too=o(" \u2014 "),Ix=a("a"),Foo=o("FNetModel"),Coo=o(" (FNet model)"),Moo=l(),Wh=a("li"),kJ=a("strong"),Eoo=o("fsmt"),yoo=o(" \u2014 "),jx=a("a"),woo=o("FSMTModel"),Aoo=o(" (FairSeq Machine-Translation model)"),Loo=l(),xn=a("li"),xJ=a("strong"),Boo=o("funnel"),koo=o(" \u2014 "),Nx=a("a"),xoo=o("FunnelModel"),Roo=o(" or "),Dx=a("a"),Soo=o("FunnelBaseModel"),Poo=o(" (Funnel Transformer model)"),$oo=l(),Qh=a("li"),RJ=a("strong"),Ioo=o("gpt2"),joo=o(" \u2014 "),qx=a("a"),Noo=o("GPT2Model"),Doo=o(" (OpenAI GPT-2 model)"),qoo=l(),Hh=a("li"),SJ=a("strong"),Goo=o("gpt_neo"),Ooo=o(" \u2014 "),Gx=a("a"),Xoo=o("GPTNeoModel"),zoo=o(" (GPT Neo model)"),Voo=l(),Uh=a("li"),PJ=a("strong"),Woo=o("gptj"),Qoo=o(" \u2014 "),Ox=a("a"),Hoo=o("GPTJModel"),Uoo=o(" (GPT-J model)"),Joo=l(),Jh=a("li"),$J=a("strong"),Yoo=o("hubert"),Koo=o(" \u2014 "),Xx=a("a"),Zoo=o("HubertModel"),ero=o(" (Hubert model)"),oro=l(),Yh=a("li"),IJ=a("strong"),rro=o("ibert"),tro=o(" \u2014 "),zx=a("a"),aro=o("IBertModel"),sro=o(" (I-BERT model)"),nro=l(),Kh=a("li"),jJ=a("strong"),lro=o("imagegpt"),iro=o(" \u2014 "),Vx=a("a"),dro=o("ImageGPTModel"),cro=o(" (ImageGPT model)"),mro=l(),Zh=a("li"),NJ=a("strong"),fro=o("layoutlm"),gro=o(" \u2014 "),Wx=a("a"),hro=o("LayoutLMModel"),uro=o(" (LayoutLM model)"),pro=l(),eu=a("li"),DJ=a("strong"),_ro=o("layoutlmv2"),bro=o(" \u2014 "),Qx=a("a"),vro=o("LayoutLMv2Model"),Tro=o(" (LayoutLMv2 model)"),Fro=l(),ou=a("li"),qJ=a("strong"),Cro=o("led"),Mro=o(" \u2014 "),Hx=a("a"),Ero=o("LEDModel"),yro=o(" (LED model)"),wro=l(),ru=a("li"),GJ=a("strong"),Aro=o("longformer"),Lro=o(" \u2014 "),Ux=a("a"),Bro=o("LongformerModel"),kro=o(" (Longformer model)"),xro=l(),tu=a("li"),OJ=a("strong"),Rro=o("luke"),Sro=o(" \u2014 "),Jx=a("a"),Pro=o("LukeModel"),$ro=o(" (LUKE model)"),Iro=l(),au=a("li"),XJ=a("strong"),jro=o("lxmert"),Nro=o(" \u2014 "),Yx=a("a"),Dro=o("LxmertModel"),qro=o(" (LXMERT model)"),Gro=l(),su=a("li"),zJ=a("strong"),Oro=o("m2m_100"),Xro=o(" \u2014 "),Kx=a("a"),zro=o("M2M100Model"),Vro=o(" (M2M100 model)"),Wro=l(),nu=a("li"),VJ=a("strong"),Qro=o("marian"),Hro=o(" \u2014 "),Zx=a("a"),Uro=o("MarianModel"),Jro=o(" (Marian model)"),Yro=l(),lu=a("li"),WJ=a("strong"),Kro=o("mbart"),Zro=o(" \u2014 "),eR=a("a"),eto=o("MBartModel"),oto=o(" (mBART model)"),rto=l(),iu=a("li"),QJ=a("strong"),tto=o("megatron-bert"),ato=o(" \u2014 "),oR=a("a"),sto=o("MegatronBertModel"),nto=o(" (MegatronBert model)"),lto=l(),du=a("li"),HJ=a("strong"),ito=o("mobilebert"),dto=o(" \u2014 "),rR=a("a"),cto=o("MobileBertModel"),mto=o(" (MobileBERT model)"),fto=l(),cu=a("li"),UJ=a("strong"),gto=o("mpnet"),hto=o(" \u2014 "),tR=a("a"),uto=o("MPNetModel"),pto=o(" (MPNet model)"),_to=l(),mu=a("li"),JJ=a("strong"),bto=o("mt5"),vto=o(" \u2014 "),aR=a("a"),Tto=o("MT5Model"),Fto=o(" (mT5 model)"),Cto=l(),fu=a("li"),YJ=a("strong"),Mto=o("nystromformer"),Eto=o(" \u2014 "),sR=a("a"),yto=o("NystromformerModel"),wto=o(" (Nystromformer model)"),Ato=l(),gu=a("li"),KJ=a("strong"),Lto=o("openai-gpt"),Bto=o(" \u2014 "),nR=a("a"),kto=o("OpenAIGPTModel"),xto=o(" (OpenAI GPT model)"),Rto=l(),hu=a("li"),ZJ=a("strong"),Sto=o("pegasus"),Pto=o(" \u2014 "),lR=a("a"),$to=o("PegasusModel"),Ito=o(" (Pegasus model)"),jto=l(),uu=a("li"),eY=a("strong"),Nto=o("perceiver"),Dto=o(" \u2014 "),iR=a("a"),qto=o("PerceiverModel"),Gto=o(" (Perceiver model)"),Oto=l(),pu=a("li"),oY=a("strong"),Xto=o("plbart"),zto=o(" \u2014 "),dR=a("a"),Vto=o("PLBartModel"),Wto=o(" (PLBart model)"),Qto=l(),_u=a("li"),rY=a("strong"),Hto=o("poolformer"),Uto=o(" \u2014 "),cR=a("a"),Jto=o("PoolFormerModel"),Yto=o(" (PoolFormer model)"),Kto=l(),bu=a("li"),tY=a("strong"),Zto=o("prophetnet"),eao=o(" \u2014 "),mR=a("a"),oao=o("ProphetNetModel"),rao=o(" (ProphetNet model)"),tao=l(),vu=a("li"),aY=a("strong"),aao=o("qdqbert"),sao=o(" \u2014 "),fR=a("a"),nao=o("QDQBertModel"),lao=o(" (QDQBert model)"),iao=l(),Tu=a("li"),sY=a("strong"),dao=o("reformer"),cao=o(" \u2014 "),gR=a("a"),mao=o("ReformerModel"),fao=o(" (Reformer model)"),gao=l(),Fu=a("li"),nY=a("strong"),hao=o("rembert"),uao=o(" \u2014 "),hR=a("a"),pao=o("RemBertModel"),_ao=o(" (RemBERT model)"),bao=l(),Cu=a("li"),lY=a("strong"),vao=o("retribert"),Tao=o(" \u2014 "),uR=a("a"),Fao=o("RetriBertModel"),Cao=o(" (RetriBERT model)"),Mao=l(),Mu=a("li"),iY=a("strong"),Eao=o("roberta"),yao=o(" \u2014 "),pR=a("a"),wao=o("RobertaModel"),Aao=o(" (RoBERTa model)"),Lao=l(),Eu=a("li"),dY=a("strong"),Bao=o("roformer"),kao=o(" \u2014 "),_R=a("a"),xao=o("RoFormerModel"),Rao=o(" (RoFormer model)"),Sao=l(),yu=a("li"),cY=a("strong"),Pao=o("segformer"),$ao=o(" \u2014 "),bR=a("a"),Iao=o("SegformerModel"),jao=o(" (SegFormer model)"),Nao=l(),wu=a("li"),mY=a("strong"),Dao=o("sew"),qao=o(" \u2014 "),vR=a("a"),Gao=o("SEWModel"),Oao=o(" (SEW model)"),Xao=l(),Au=a("li"),fY=a("strong"),zao=o("sew-d"),Vao=o(" \u2014 "),TR=a("a"),Wao=o("SEWDModel"),Qao=o(" (SEW-D model)"),Hao=l(),Lu=a("li"),gY=a("strong"),Uao=o("speech_to_text"),Jao=o(" \u2014 "),FR=a("a"),Yao=o("Speech2TextModel"),Kao=o(" (Speech2Text model)"),Zao=l(),Bu=a("li"),hY=a("strong"),eso=o("splinter"),oso=o(" \u2014 "),CR=a("a"),rso=o("SplinterModel"),tso=o(" (Splinter model)"),aso=l(),ku=a("li"),uY=a("strong"),sso=o("squeezebert"),nso=o(" \u2014 "),MR=a("a"),lso=o("SqueezeBertModel"),iso=o(" (SqueezeBERT model)"),dso=l(),xu=a("li"),pY=a("strong"),cso=o("swin"),mso=o(" \u2014 "),ER=a("a"),fso=o("SwinModel"),gso=o(" (Swin model)"),hso=l(),Ru=a("li"),_Y=a("strong"),uso=o("t5"),pso=o(" \u2014 "),yR=a("a"),_so=o("T5Model"),bso=o(" (T5 model)"),vso=l(),Su=a("li"),bY=a("strong"),Tso=o("tapas"),Fso=o(" \u2014 "),wR=a("a"),Cso=o("TapasModel"),Mso=o(" (TAPAS model)"),Eso=l(),Pu=a("li"),vY=a("strong"),yso=o("transfo-xl"),wso=o(" \u2014 "),AR=a("a"),Aso=o("TransfoXLModel"),Lso=o(" (Transformer-XL model)"),Bso=l(),$u=a("li"),TY=a("strong"),kso=o("unispeech"),xso=o(" \u2014 "),LR=a("a"),Rso=o("UniSpeechModel"),Sso=o(" (UniSpeech model)"),Pso=l(),Iu=a("li"),FY=a("strong"),$so=o("unispeech-sat"),Iso=o(" \u2014 "),BR=a("a"),jso=o("UniSpeechSatModel"),Nso=o(" (UniSpeechSat model)"),Dso=l(),ju=a("li"),CY=a("strong"),qso=o("vilt"),Gso=o(" \u2014 "),kR=a("a"),Oso=o("ViltModel"),Xso=o(" (ViLT model)"),zso=l(),Nu=a("li"),MY=a("strong"),Vso=o("vision-text-dual-encoder"),Wso=o(" \u2014 "),xR=a("a"),Qso=o("VisionTextDualEncoderModel"),Hso=o(" (VisionTextDualEncoder model)"),Uso=l(),Du=a("li"),EY=a("strong"),Jso=o("visual_bert"),Yso=o(" \u2014 "),RR=a("a"),Kso=o("VisualBertModel"),Zso=o(" (VisualBert model)"),eno=l(),qu=a("li"),yY=a("strong"),ono=o("vit"),rno=o(" \u2014 "),SR=a("a"),tno=o("ViTModel"),ano=o(" (ViT model)"),sno=l(),Gu=a("li"),wY=a("strong"),nno=o("vit_mae"),lno=o(" \u2014 "),PR=a("a"),ino=o("ViTMAEModel"),dno=o(" (ViTMAE model)"),cno=l(),Ou=a("li"),AY=a("strong"),mno=o("wav2vec2"),fno=o(" \u2014 "),$R=a("a"),gno=o("Wav2Vec2Model"),hno=o(" (Wav2Vec2 model)"),uno=l(),Xu=a("li"),LY=a("strong"),pno=o("wavlm"),_no=o(" \u2014 "),IR=a("a"),bno=o("WavLMModel"),vno=o(" (WavLM model)"),Tno=l(),zu=a("li"),BY=a("strong"),Fno=o("xglm"),Cno=o(" \u2014 "),jR=a("a"),Mno=o("XGLMModel"),Eno=o(" (XGLM model)"),yno=l(),Vu=a("li"),kY=a("strong"),wno=o("xlm"),Ano=o(" \u2014 "),NR=a("a"),Lno=o("XLMModel"),Bno=o(" (XLM model)"),kno=l(),Wu=a("li"),xY=a("strong"),xno=o("xlm-prophetnet"),Rno=o(" \u2014 "),DR=a("a"),Sno=o("XLMProphetNetModel"),Pno=o(" (XLMProphetNet model)"),$no=l(),Qu=a("li"),RY=a("strong"),Ino=o("xlm-roberta"),jno=o(" \u2014 "),qR=a("a"),Nno=o("XLMRobertaModel"),Dno=o(" (XLM-RoBERTa model)"),qno=l(),Hu=a("li"),SY=a("strong"),Gno=o("xlm-roberta-xl"),Ono=o(" \u2014 "),GR=a("a"),Xno=o("XLMRobertaXLModel"),zno=o(" (XLM-RoBERTa-XL model)"),Vno=l(),Uu=a("li"),PY=a("strong"),Wno=o("xlnet"),Qno=o(" \u2014 "),OR=a("a"),Hno=o("XLNetModel"),Uno=o(" (XLNet model)"),Jno=l(),Ju=a("li"),$Y=a("strong"),Yno=o("yoso"),Kno=o(" \u2014 "),XR=a("a"),Zno=o("YosoModel"),elo=o(" (YOSO model)"),olo=l(),Yu=a("p"),rlo=o("The model is set in evaluation mode by default using "),IY=a("code"),tlo=o("model.eval()"),alo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=a("code"),slo=o("model.train()"),nlo=l(),NY=a("p"),llo=o("Examples:"),ilo=l(),m(G5.$$.fragment),b8e=l(),Xi=a("h2"),Ku=a("a"),DY=a("span"),m(O5.$$.fragment),dlo=l(),qY=a("span"),clo=o("AutoModelForPreTraining"),v8e=l(),Wo=a("div"),m(X5.$$.fragment),mlo=l(),zi=a("p"),flo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=a("code"),glo=o("from_pretrained()"),hlo=o("class method or the "),OY=a("code"),ulo=o("from_config()"),plo=o(`class
method.`),_lo=l(),z5=a("p"),blo=o("This class cannot be instantiated directly using "),XY=a("code"),vlo=o("__init__()"),Tlo=o(" (throws an error)."),Flo=l(),Dr=a("div"),m(V5.$$.fragment),Clo=l(),zY=a("p"),Mlo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Elo=l(),Vi=a("p"),ylo=o(`Note:
Loading a model from its configuration file does `),VY=a("strong"),wlo=o("not"),Alo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=a("code"),Llo=o("from_pretrained()"),Blo=o("to load the model weights."),klo=l(),QY=a("p"),xlo=o("Examples:"),Rlo=l(),m(W5.$$.fragment),Slo=l(),xe=a("div"),m(Q5.$$.fragment),Plo=l(),HY=a("p"),$lo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Ilo=l(),qa=a("p"),jlo=o("The model class to instantiate is selected based on the "),UY=a("code"),Nlo=o("model_type"),Dlo=o(` property of the config object (either
passed as an argument or loaded from `),JY=a("code"),qlo=o("pretrained_model_name_or_path"),Glo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=a("code"),Olo=o("pretrained_model_name_or_path"),Xlo=o(":"),zlo=l(),x=a("ul"),Zu=a("li"),KY=a("strong"),Vlo=o("albert"),Wlo=o(" \u2014 "),zR=a("a"),Qlo=o("AlbertForPreTraining"),Hlo=o(" (ALBERT model)"),Ulo=l(),ep=a("li"),ZY=a("strong"),Jlo=o("bart"),Ylo=o(" \u2014 "),VR=a("a"),Klo=o("BartForConditionalGeneration"),Zlo=o(" (BART model)"),eio=l(),op=a("li"),eK=a("strong"),oio=o("bert"),rio=o(" \u2014 "),WR=a("a"),tio=o("BertForPreTraining"),aio=o(" (BERT model)"),sio=l(),rp=a("li"),oK=a("strong"),nio=o("big_bird"),lio=o(" \u2014 "),QR=a("a"),iio=o("BigBirdForPreTraining"),dio=o(" (BigBird model)"),cio=l(),tp=a("li"),rK=a("strong"),mio=o("camembert"),fio=o(" \u2014 "),HR=a("a"),gio=o("CamembertForMaskedLM"),hio=o(" (CamemBERT model)"),uio=l(),ap=a("li"),tK=a("strong"),pio=o("ctrl"),_io=o(" \u2014 "),UR=a("a"),bio=o("CTRLLMHeadModel"),vio=o(" (CTRL model)"),Tio=l(),sp=a("li"),aK=a("strong"),Fio=o("deberta"),Cio=o(" \u2014 "),JR=a("a"),Mio=o("DebertaForMaskedLM"),Eio=o(" (DeBERTa model)"),yio=l(),np=a("li"),sK=a("strong"),wio=o("deberta-v2"),Aio=o(" \u2014 "),YR=a("a"),Lio=o("DebertaV2ForMaskedLM"),Bio=o(" (DeBERTa-v2 model)"),kio=l(),lp=a("li"),nK=a("strong"),xio=o("distilbert"),Rio=o(" \u2014 "),KR=a("a"),Sio=o("DistilBertForMaskedLM"),Pio=o(" (DistilBERT model)"),$io=l(),ip=a("li"),lK=a("strong"),Iio=o("electra"),jio=o(" \u2014 "),ZR=a("a"),Nio=o("ElectraForPreTraining"),Dio=o(" (ELECTRA model)"),qio=l(),dp=a("li"),iK=a("strong"),Gio=o("flaubert"),Oio=o(" \u2014 "),eS=a("a"),Xio=o("FlaubertWithLMHeadModel"),zio=o(" (FlauBERT model)"),Vio=l(),cp=a("li"),dK=a("strong"),Wio=o("fnet"),Qio=o(" \u2014 "),oS=a("a"),Hio=o("FNetForPreTraining"),Uio=o(" (FNet model)"),Jio=l(),mp=a("li"),cK=a("strong"),Yio=o("fsmt"),Kio=o(" \u2014 "),rS=a("a"),Zio=o("FSMTForConditionalGeneration"),edo=o(" (FairSeq Machine-Translation model)"),odo=l(),fp=a("li"),mK=a("strong"),rdo=o("funnel"),tdo=o(" \u2014 "),tS=a("a"),ado=o("FunnelForPreTraining"),sdo=o(" (Funnel Transformer model)"),ndo=l(),gp=a("li"),fK=a("strong"),ldo=o("gpt2"),ido=o(" \u2014 "),aS=a("a"),ddo=o("GPT2LMHeadModel"),cdo=o(" (OpenAI GPT-2 model)"),mdo=l(),hp=a("li"),gK=a("strong"),fdo=o("ibert"),gdo=o(" \u2014 "),sS=a("a"),hdo=o("IBertForMaskedLM"),udo=o(" (I-BERT model)"),pdo=l(),up=a("li"),hK=a("strong"),_do=o("layoutlm"),bdo=o(" \u2014 "),nS=a("a"),vdo=o("LayoutLMForMaskedLM"),Tdo=o(" (LayoutLM model)"),Fdo=l(),pp=a("li"),uK=a("strong"),Cdo=o("longformer"),Mdo=o(" \u2014 "),lS=a("a"),Edo=o("LongformerForMaskedLM"),ydo=o(" (Longformer model)"),wdo=l(),_p=a("li"),pK=a("strong"),Ado=o("lxmert"),Ldo=o(" \u2014 "),iS=a("a"),Bdo=o("LxmertForPreTraining"),kdo=o(" (LXMERT model)"),xdo=l(),bp=a("li"),_K=a("strong"),Rdo=o("megatron-bert"),Sdo=o(" \u2014 "),dS=a("a"),Pdo=o("MegatronBertForPreTraining"),$do=o(" (MegatronBert model)"),Ido=l(),vp=a("li"),bK=a("strong"),jdo=o("mobilebert"),Ndo=o(" \u2014 "),cS=a("a"),Ddo=o("MobileBertForPreTraining"),qdo=o(" (MobileBERT model)"),Gdo=l(),Tp=a("li"),vK=a("strong"),Odo=o("mpnet"),Xdo=o(" \u2014 "),mS=a("a"),zdo=o("MPNetForMaskedLM"),Vdo=o(" (MPNet model)"),Wdo=l(),Fp=a("li"),TK=a("strong"),Qdo=o("openai-gpt"),Hdo=o(" \u2014 "),fS=a("a"),Udo=o("OpenAIGPTLMHeadModel"),Jdo=o(" (OpenAI GPT model)"),Ydo=l(),Cp=a("li"),FK=a("strong"),Kdo=o("retribert"),Zdo=o(" \u2014 "),gS=a("a"),eco=o("RetriBertModel"),oco=o(" (RetriBERT model)"),rco=l(),Mp=a("li"),CK=a("strong"),tco=o("roberta"),aco=o(" \u2014 "),hS=a("a"),sco=o("RobertaForMaskedLM"),nco=o(" (RoBERTa model)"),lco=l(),Ep=a("li"),MK=a("strong"),ico=o("squeezebert"),dco=o(" \u2014 "),uS=a("a"),cco=o("SqueezeBertForMaskedLM"),mco=o(" (SqueezeBERT model)"),fco=l(),yp=a("li"),EK=a("strong"),gco=o("t5"),hco=o(" \u2014 "),pS=a("a"),uco=o("T5ForConditionalGeneration"),pco=o(" (T5 model)"),_co=l(),wp=a("li"),yK=a("strong"),bco=o("tapas"),vco=o(" \u2014 "),_S=a("a"),Tco=o("TapasForMaskedLM"),Fco=o(" (TAPAS model)"),Cco=l(),Ap=a("li"),wK=a("strong"),Mco=o("transfo-xl"),Eco=o(" \u2014 "),bS=a("a"),yco=o("TransfoXLLMHeadModel"),wco=o(" (Transformer-XL model)"),Aco=l(),Lp=a("li"),AK=a("strong"),Lco=o("unispeech"),Bco=o(" \u2014 "),vS=a("a"),kco=o("UniSpeechForPreTraining"),xco=o(" (UniSpeech model)"),Rco=l(),Bp=a("li"),LK=a("strong"),Sco=o("unispeech-sat"),Pco=o(" \u2014 "),TS=a("a"),$co=o("UniSpeechSatForPreTraining"),Ico=o(" (UniSpeechSat model)"),jco=l(),kp=a("li"),BK=a("strong"),Nco=o("visual_bert"),Dco=o(" \u2014 "),FS=a("a"),qco=o("VisualBertForPreTraining"),Gco=o(" (VisualBert model)"),Oco=l(),xp=a("li"),kK=a("strong"),Xco=o("vit_mae"),zco=o(" \u2014 "),CS=a("a"),Vco=o("ViTMAEForPreTraining"),Wco=o(" (ViTMAE model)"),Qco=l(),Rp=a("li"),xK=a("strong"),Hco=o("wav2vec2"),Uco=o(" \u2014 "),MS=a("a"),Jco=o("Wav2Vec2ForPreTraining"),Yco=o(" (Wav2Vec2 model)"),Kco=l(),Sp=a("li"),RK=a("strong"),Zco=o("xlm"),emo=o(" \u2014 "),ES=a("a"),omo=o("XLMWithLMHeadModel"),rmo=o(" (XLM model)"),tmo=l(),Pp=a("li"),SK=a("strong"),amo=o("xlm-roberta"),smo=o(" \u2014 "),yS=a("a"),nmo=o("XLMRobertaForMaskedLM"),lmo=o(" (XLM-RoBERTa model)"),imo=l(),$p=a("li"),PK=a("strong"),dmo=o("xlm-roberta-xl"),cmo=o(" \u2014 "),wS=a("a"),mmo=o("XLMRobertaXLForMaskedLM"),fmo=o(" (XLM-RoBERTa-XL model)"),gmo=l(),Ip=a("li"),$K=a("strong"),hmo=o("xlnet"),umo=o(" \u2014 "),AS=a("a"),pmo=o("XLNetLMHeadModel"),_mo=o(" (XLNet model)"),bmo=l(),jp=a("p"),vmo=o("The model is set in evaluation mode by default using "),IK=a("code"),Tmo=o("model.eval()"),Fmo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=a("code"),Cmo=o("model.train()"),Mmo=l(),NK=a("p"),Emo=o("Examples:"),ymo=l(),m(H5.$$.fragment),T8e=l(),Wi=a("h2"),Np=a("a"),DK=a("span"),m(U5.$$.fragment),wmo=l(),qK=a("span"),Amo=o("AutoModelForCausalLM"),F8e=l(),Qo=a("div"),m(J5.$$.fragment),Lmo=l(),Qi=a("p"),Bmo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=a("code"),kmo=o("from_pretrained()"),xmo=o("class method or the "),OK=a("code"),Rmo=o("from_config()"),Smo=o(`class
method.`),Pmo=l(),Y5=a("p"),$mo=o("This class cannot be instantiated directly using "),XK=a("code"),Imo=o("__init__()"),jmo=o(" (throws an error)."),Nmo=l(),qr=a("div"),m(K5.$$.fragment),Dmo=l(),zK=a("p"),qmo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Gmo=l(),Hi=a("p"),Omo=o(`Note:
Loading a model from its configuration file does `),VK=a("strong"),Xmo=o("not"),zmo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=a("code"),Vmo=o("from_pretrained()"),Wmo=o("to load the model weights."),Qmo=l(),QK=a("p"),Hmo=o("Examples:"),Umo=l(),m(Z5.$$.fragment),Jmo=l(),Re=a("div"),m(ey.$$.fragment),Ymo=l(),HK=a("p"),Kmo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Zmo=l(),Ga=a("p"),efo=o("The model class to instantiate is selected based on the "),UK=a("code"),ofo=o("model_type"),rfo=o(` property of the config object (either
passed as an argument or loaded from `),JK=a("code"),tfo=o("pretrained_model_name_or_path"),afo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=a("code"),sfo=o("pretrained_model_name_or_path"),nfo=o(":"),lfo=l(),$=a("ul"),Dp=a("li"),KK=a("strong"),ifo=o("bart"),dfo=o(" \u2014 "),LS=a("a"),cfo=o("BartForCausalLM"),mfo=o(" (BART model)"),ffo=l(),qp=a("li"),ZK=a("strong"),gfo=o("bert"),hfo=o(" \u2014 "),BS=a("a"),ufo=o("BertLMHeadModel"),pfo=o(" (BERT model)"),_fo=l(),Gp=a("li"),eZ=a("strong"),bfo=o("bert-generation"),vfo=o(" \u2014 "),kS=a("a"),Tfo=o("BertGenerationDecoder"),Ffo=o(" (Bert Generation model)"),Cfo=l(),Op=a("li"),oZ=a("strong"),Mfo=o("big_bird"),Efo=o(" \u2014 "),xS=a("a"),yfo=o("BigBirdForCausalLM"),wfo=o(" (BigBird model)"),Afo=l(),Xp=a("li"),rZ=a("strong"),Lfo=o("bigbird_pegasus"),Bfo=o(" \u2014 "),RS=a("a"),kfo=o("BigBirdPegasusForCausalLM"),xfo=o(" (BigBirdPegasus model)"),Rfo=l(),zp=a("li"),tZ=a("strong"),Sfo=o("blenderbot"),Pfo=o(" \u2014 "),SS=a("a"),$fo=o("BlenderbotForCausalLM"),Ifo=o(" (Blenderbot model)"),jfo=l(),Vp=a("li"),aZ=a("strong"),Nfo=o("blenderbot-small"),Dfo=o(" \u2014 "),PS=a("a"),qfo=o("BlenderbotSmallForCausalLM"),Gfo=o(" (BlenderbotSmall model)"),Ofo=l(),Wp=a("li"),sZ=a("strong"),Xfo=o("camembert"),zfo=o(" \u2014 "),$S=a("a"),Vfo=o("CamembertForCausalLM"),Wfo=o(" (CamemBERT model)"),Qfo=l(),Qp=a("li"),nZ=a("strong"),Hfo=o("ctrl"),Ufo=o(" \u2014 "),IS=a("a"),Jfo=o("CTRLLMHeadModel"),Yfo=o(" (CTRL model)"),Kfo=l(),Hp=a("li"),lZ=a("strong"),Zfo=o("electra"),ego=o(" \u2014 "),jS=a("a"),ogo=o("ElectraForCausalLM"),rgo=o(" (ELECTRA model)"),tgo=l(),Up=a("li"),iZ=a("strong"),ago=o("gpt2"),sgo=o(" \u2014 "),NS=a("a"),ngo=o("GPT2LMHeadModel"),lgo=o(" (OpenAI GPT-2 model)"),igo=l(),Jp=a("li"),dZ=a("strong"),dgo=o("gpt_neo"),cgo=o(" \u2014 "),DS=a("a"),mgo=o("GPTNeoForCausalLM"),fgo=o(" (GPT Neo model)"),ggo=l(),Yp=a("li"),cZ=a("strong"),hgo=o("gptj"),ugo=o(" \u2014 "),qS=a("a"),pgo=o("GPTJForCausalLM"),_go=o(" (GPT-J model)"),bgo=l(),Kp=a("li"),mZ=a("strong"),vgo=o("marian"),Tgo=o(" \u2014 "),GS=a("a"),Fgo=o("MarianForCausalLM"),Cgo=o(" (Marian model)"),Mgo=l(),Zp=a("li"),fZ=a("strong"),Ego=o("mbart"),ygo=o(" \u2014 "),OS=a("a"),wgo=o("MBartForCausalLM"),Ago=o(" (mBART model)"),Lgo=l(),e_=a("li"),gZ=a("strong"),Bgo=o("megatron-bert"),kgo=o(" \u2014 "),XS=a("a"),xgo=o("MegatronBertForCausalLM"),Rgo=o(" (MegatronBert model)"),Sgo=l(),o_=a("li"),hZ=a("strong"),Pgo=o("openai-gpt"),$go=o(" \u2014 "),zS=a("a"),Igo=o("OpenAIGPTLMHeadModel"),jgo=o(" (OpenAI GPT model)"),Ngo=l(),r_=a("li"),uZ=a("strong"),Dgo=o("pegasus"),qgo=o(" \u2014 "),VS=a("a"),Ggo=o("PegasusForCausalLM"),Ogo=o(" (Pegasus model)"),Xgo=l(),t_=a("li"),pZ=a("strong"),zgo=o("plbart"),Vgo=o(" \u2014 "),WS=a("a"),Wgo=o("PLBartForCausalLM"),Qgo=o(" (PLBart model)"),Hgo=l(),a_=a("li"),_Z=a("strong"),Ugo=o("prophetnet"),Jgo=o(" \u2014 "),QS=a("a"),Ygo=o("ProphetNetForCausalLM"),Kgo=o(" (ProphetNet model)"),Zgo=l(),s_=a("li"),bZ=a("strong"),eho=o("qdqbert"),oho=o(" \u2014 "),HS=a("a"),rho=o("QDQBertLMHeadModel"),tho=o(" (QDQBert model)"),aho=l(),n_=a("li"),vZ=a("strong"),sho=o("reformer"),nho=o(" \u2014 "),US=a("a"),lho=o("ReformerModelWithLMHead"),iho=o(" (Reformer model)"),dho=l(),l_=a("li"),TZ=a("strong"),cho=o("rembert"),mho=o(" \u2014 "),JS=a("a"),fho=o("RemBertForCausalLM"),gho=o(" (RemBERT model)"),hho=l(),i_=a("li"),FZ=a("strong"),uho=o("roberta"),pho=o(" \u2014 "),YS=a("a"),_ho=o("RobertaForCausalLM"),bho=o(" (RoBERTa model)"),vho=l(),d_=a("li"),CZ=a("strong"),Tho=o("roformer"),Fho=o(" \u2014 "),KS=a("a"),Cho=o("RoFormerForCausalLM"),Mho=o(" (RoFormer model)"),Eho=l(),c_=a("li"),MZ=a("strong"),yho=o("speech_to_text_2"),who=o(" \u2014 "),ZS=a("a"),Aho=o("Speech2Text2ForCausalLM"),Lho=o(" (Speech2Text2 model)"),Bho=l(),m_=a("li"),EZ=a("strong"),kho=o("transfo-xl"),xho=o(" \u2014 "),eP=a("a"),Rho=o("TransfoXLLMHeadModel"),Sho=o(" (Transformer-XL model)"),Pho=l(),f_=a("li"),yZ=a("strong"),$ho=o("trocr"),Iho=o(" \u2014 "),oP=a("a"),jho=o("TrOCRForCausalLM"),Nho=o(" (TrOCR model)"),Dho=l(),g_=a("li"),wZ=a("strong"),qho=o("xglm"),Gho=o(" \u2014 "),rP=a("a"),Oho=o("XGLMForCausalLM"),Xho=o(" (XGLM model)"),zho=l(),h_=a("li"),AZ=a("strong"),Vho=o("xlm"),Who=o(" \u2014 "),tP=a("a"),Qho=o("XLMWithLMHeadModel"),Hho=o(" (XLM model)"),Uho=l(),u_=a("li"),LZ=a("strong"),Jho=o("xlm-prophetnet"),Yho=o(" \u2014 "),aP=a("a"),Kho=o("XLMProphetNetForCausalLM"),Zho=o(" (XLMProphetNet model)"),euo=l(),p_=a("li"),BZ=a("strong"),ouo=o("xlm-roberta"),ruo=o(" \u2014 "),sP=a("a"),tuo=o("XLMRobertaForCausalLM"),auo=o(" (XLM-RoBERTa model)"),suo=l(),__=a("li"),kZ=a("strong"),nuo=o("xlm-roberta-xl"),luo=o(" \u2014 "),nP=a("a"),iuo=o("XLMRobertaXLForCausalLM"),duo=o(" (XLM-RoBERTa-XL model)"),cuo=l(),b_=a("li"),xZ=a("strong"),muo=o("xlnet"),fuo=o(" \u2014 "),lP=a("a"),guo=o("XLNetLMHeadModel"),huo=o(" (XLNet model)"),uuo=l(),v_=a("p"),puo=o("The model is set in evaluation mode by default using "),RZ=a("code"),_uo=o("model.eval()"),buo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=a("code"),vuo=o("model.train()"),Tuo=l(),PZ=a("p"),Fuo=o("Examples:"),Cuo=l(),m(oy.$$.fragment),C8e=l(),Ui=a("h2"),T_=a("a"),$Z=a("span"),m(ry.$$.fragment),Muo=l(),IZ=a("span"),Euo=o("AutoModelForMaskedLM"),M8e=l(),Ho=a("div"),m(ty.$$.fragment),yuo=l(),Ji=a("p"),wuo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=a("code"),Auo=o("from_pretrained()"),Luo=o("class method or the "),NZ=a("code"),Buo=o("from_config()"),kuo=o(`class
method.`),xuo=l(),ay=a("p"),Ruo=o("This class cannot be instantiated directly using "),DZ=a("code"),Suo=o("__init__()"),Puo=o(" (throws an error)."),$uo=l(),Gr=a("div"),m(sy.$$.fragment),Iuo=l(),qZ=a("p"),juo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Nuo=l(),Yi=a("p"),Duo=o(`Note:
Loading a model from its configuration file does `),GZ=a("strong"),quo=o("not"),Guo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=a("code"),Ouo=o("from_pretrained()"),Xuo=o("to load the model weights."),zuo=l(),XZ=a("p"),Vuo=o("Examples:"),Wuo=l(),m(ny.$$.fragment),Quo=l(),Se=a("div"),m(ly.$$.fragment),Huo=l(),zZ=a("p"),Uuo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Juo=l(),Oa=a("p"),Yuo=o("The model class to instantiate is selected based on the "),VZ=a("code"),Kuo=o("model_type"),Zuo=o(` property of the config object (either
passed as an argument or loaded from `),WZ=a("code"),epo=o("pretrained_model_name_or_path"),opo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=a("code"),rpo=o("pretrained_model_name_or_path"),tpo=o(":"),apo=l(),I=a("ul"),F_=a("li"),HZ=a("strong"),spo=o("albert"),npo=o(" \u2014 "),iP=a("a"),lpo=o("AlbertForMaskedLM"),ipo=o(" (ALBERT model)"),dpo=l(),C_=a("li"),UZ=a("strong"),cpo=o("bart"),mpo=o(" \u2014 "),dP=a("a"),fpo=o("BartForConditionalGeneration"),gpo=o(" (BART model)"),hpo=l(),M_=a("li"),JZ=a("strong"),upo=o("bert"),ppo=o(" \u2014 "),cP=a("a"),_po=o("BertForMaskedLM"),bpo=o(" (BERT model)"),vpo=l(),E_=a("li"),YZ=a("strong"),Tpo=o("big_bird"),Fpo=o(" \u2014 "),mP=a("a"),Cpo=o("BigBirdForMaskedLM"),Mpo=o(" (BigBird model)"),Epo=l(),y_=a("li"),KZ=a("strong"),ypo=o("camembert"),wpo=o(" \u2014 "),fP=a("a"),Apo=o("CamembertForMaskedLM"),Lpo=o(" (CamemBERT model)"),Bpo=l(),w_=a("li"),ZZ=a("strong"),kpo=o("convbert"),xpo=o(" \u2014 "),gP=a("a"),Rpo=o("ConvBertForMaskedLM"),Spo=o(" (ConvBERT model)"),Ppo=l(),A_=a("li"),eee=a("strong"),$po=o("deberta"),Ipo=o(" \u2014 "),hP=a("a"),jpo=o("DebertaForMaskedLM"),Npo=o(" (DeBERTa model)"),Dpo=l(),L_=a("li"),oee=a("strong"),qpo=o("deberta-v2"),Gpo=o(" \u2014 "),uP=a("a"),Opo=o("DebertaV2ForMaskedLM"),Xpo=o(" (DeBERTa-v2 model)"),zpo=l(),B_=a("li"),ree=a("strong"),Vpo=o("distilbert"),Wpo=o(" \u2014 "),pP=a("a"),Qpo=o("DistilBertForMaskedLM"),Hpo=o(" (DistilBERT model)"),Upo=l(),k_=a("li"),tee=a("strong"),Jpo=o("electra"),Ypo=o(" \u2014 "),_P=a("a"),Kpo=o("ElectraForMaskedLM"),Zpo=o(" (ELECTRA model)"),e_o=l(),x_=a("li"),aee=a("strong"),o_o=o("flaubert"),r_o=o(" \u2014 "),bP=a("a"),t_o=o("FlaubertWithLMHeadModel"),a_o=o(" (FlauBERT model)"),s_o=l(),R_=a("li"),see=a("strong"),n_o=o("fnet"),l_o=o(" \u2014 "),vP=a("a"),i_o=o("FNetForMaskedLM"),d_o=o(" (FNet model)"),c_o=l(),S_=a("li"),nee=a("strong"),m_o=o("funnel"),f_o=o(" \u2014 "),TP=a("a"),g_o=o("FunnelForMaskedLM"),h_o=o(" (Funnel Transformer model)"),u_o=l(),P_=a("li"),lee=a("strong"),p_o=o("ibert"),__o=o(" \u2014 "),FP=a("a"),b_o=o("IBertForMaskedLM"),v_o=o(" (I-BERT model)"),T_o=l(),$_=a("li"),iee=a("strong"),F_o=o("layoutlm"),C_o=o(" \u2014 "),CP=a("a"),M_o=o("LayoutLMForMaskedLM"),E_o=o(" (LayoutLM model)"),y_o=l(),I_=a("li"),dee=a("strong"),w_o=o("longformer"),A_o=o(" \u2014 "),MP=a("a"),L_o=o("LongformerForMaskedLM"),B_o=o(" (Longformer model)"),k_o=l(),j_=a("li"),cee=a("strong"),x_o=o("mbart"),R_o=o(" \u2014 "),EP=a("a"),S_o=o("MBartForConditionalGeneration"),P_o=o(" (mBART model)"),$_o=l(),N_=a("li"),mee=a("strong"),I_o=o("megatron-bert"),j_o=o(" \u2014 "),yP=a("a"),N_o=o("MegatronBertForMaskedLM"),D_o=o(" (MegatronBert model)"),q_o=l(),D_=a("li"),fee=a("strong"),G_o=o("mobilebert"),O_o=o(" \u2014 "),wP=a("a"),X_o=o("MobileBertForMaskedLM"),z_o=o(" (MobileBERT model)"),V_o=l(),q_=a("li"),gee=a("strong"),W_o=o("mpnet"),Q_o=o(" \u2014 "),AP=a("a"),H_o=o("MPNetForMaskedLM"),U_o=o(" (MPNet model)"),J_o=l(),G_=a("li"),hee=a("strong"),Y_o=o("nystromformer"),K_o=o(" \u2014 "),LP=a("a"),Z_o=o("NystromformerForMaskedLM"),ebo=o(" (Nystromformer model)"),obo=l(),O_=a("li"),uee=a("strong"),rbo=o("perceiver"),tbo=o(" \u2014 "),BP=a("a"),abo=o("PerceiverForMaskedLM"),sbo=o(" (Perceiver model)"),nbo=l(),X_=a("li"),pee=a("strong"),lbo=o("qdqbert"),ibo=o(" \u2014 "),kP=a("a"),dbo=o("QDQBertForMaskedLM"),cbo=o(" (QDQBert model)"),mbo=l(),z_=a("li"),_ee=a("strong"),fbo=o("reformer"),gbo=o(" \u2014 "),xP=a("a"),hbo=o("ReformerForMaskedLM"),ubo=o(" (Reformer model)"),pbo=l(),V_=a("li"),bee=a("strong"),_bo=o("rembert"),bbo=o(" \u2014 "),RP=a("a"),vbo=o("RemBertForMaskedLM"),Tbo=o(" (RemBERT model)"),Fbo=l(),W_=a("li"),vee=a("strong"),Cbo=o("roberta"),Mbo=o(" \u2014 "),SP=a("a"),Ebo=o("RobertaForMaskedLM"),ybo=o(" (RoBERTa model)"),wbo=l(),Q_=a("li"),Tee=a("strong"),Abo=o("roformer"),Lbo=o(" \u2014 "),PP=a("a"),Bbo=o("RoFormerForMaskedLM"),kbo=o(" (RoFormer model)"),xbo=l(),H_=a("li"),Fee=a("strong"),Rbo=o("squeezebert"),Sbo=o(" \u2014 "),$P=a("a"),Pbo=o("SqueezeBertForMaskedLM"),$bo=o(" (SqueezeBERT model)"),Ibo=l(),U_=a("li"),Cee=a("strong"),jbo=o("tapas"),Nbo=o(" \u2014 "),IP=a("a"),Dbo=o("TapasForMaskedLM"),qbo=o(" (TAPAS model)"),Gbo=l(),J_=a("li"),Mee=a("strong"),Obo=o("wav2vec2"),Xbo=o(" \u2014 "),Eee=a("code"),zbo=o("Wav2Vec2ForMaskedLM"),Vbo=o("(Wav2Vec2 model)"),Wbo=l(),Y_=a("li"),yee=a("strong"),Qbo=o("xlm"),Hbo=o(" \u2014 "),jP=a("a"),Ubo=o("XLMWithLMHeadModel"),Jbo=o(" (XLM model)"),Ybo=l(),K_=a("li"),wee=a("strong"),Kbo=o("xlm-roberta"),Zbo=o(" \u2014 "),NP=a("a"),e2o=o("XLMRobertaForMaskedLM"),o2o=o(" (XLM-RoBERTa model)"),r2o=l(),Z_=a("li"),Aee=a("strong"),t2o=o("xlm-roberta-xl"),a2o=o(" \u2014 "),DP=a("a"),s2o=o("XLMRobertaXLForMaskedLM"),n2o=o(" (XLM-RoBERTa-XL model)"),l2o=l(),eb=a("li"),Lee=a("strong"),i2o=o("yoso"),d2o=o(" \u2014 "),qP=a("a"),c2o=o("YosoForMaskedLM"),m2o=o(" (YOSO model)"),f2o=l(),ob=a("p"),g2o=o("The model is set in evaluation mode by default using "),Bee=a("code"),h2o=o("model.eval()"),u2o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=a("code"),p2o=o("model.train()"),_2o=l(),xee=a("p"),b2o=o("Examples:"),v2o=l(),m(iy.$$.fragment),E8e=l(),Ki=a("h2"),rb=a("a"),Ree=a("span"),m(dy.$$.fragment),T2o=l(),See=a("span"),F2o=o("AutoModelForSeq2SeqLM"),y8e=l(),Uo=a("div"),m(cy.$$.fragment),C2o=l(),Zi=a("p"),M2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=a("code"),E2o=o("from_pretrained()"),y2o=o("class method or the "),$ee=a("code"),w2o=o("from_config()"),A2o=o(`class
method.`),L2o=l(),my=a("p"),B2o=o("This class cannot be instantiated directly using "),Iee=a("code"),k2o=o("__init__()"),x2o=o(" (throws an error)."),R2o=l(),Or=a("div"),m(fy.$$.fragment),S2o=l(),jee=a("p"),P2o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),$2o=l(),ed=a("p"),I2o=o(`Note:
Loading a model from its configuration file does `),Nee=a("strong"),j2o=o("not"),N2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=a("code"),D2o=o("from_pretrained()"),q2o=o("to load the model weights."),G2o=l(),qee=a("p"),O2o=o("Examples:"),X2o=l(),m(gy.$$.fragment),z2o=l(),Pe=a("div"),m(hy.$$.fragment),V2o=l(),Gee=a("p"),W2o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Q2o=l(),Xa=a("p"),H2o=o("The model class to instantiate is selected based on the "),Oee=a("code"),U2o=o("model_type"),J2o=o(` property of the config object (either
passed as an argument or loaded from `),Xee=a("code"),Y2o=o("pretrained_model_name_or_path"),K2o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=a("code"),Z2o=o("pretrained_model_name_or_path"),evo=o(":"),ovo=l(),ae=a("ul"),tb=a("li"),Vee=a("strong"),rvo=o("bart"),tvo=o(" \u2014 "),GP=a("a"),avo=o("BartForConditionalGeneration"),svo=o(" (BART model)"),nvo=l(),ab=a("li"),Wee=a("strong"),lvo=o("bigbird_pegasus"),ivo=o(" \u2014 "),OP=a("a"),dvo=o("BigBirdPegasusForConditionalGeneration"),cvo=o(" (BigBirdPegasus model)"),mvo=l(),sb=a("li"),Qee=a("strong"),fvo=o("blenderbot"),gvo=o(" \u2014 "),XP=a("a"),hvo=o("BlenderbotForConditionalGeneration"),uvo=o(" (Blenderbot model)"),pvo=l(),nb=a("li"),Hee=a("strong"),_vo=o("blenderbot-small"),bvo=o(" \u2014 "),zP=a("a"),vvo=o("BlenderbotSmallForConditionalGeneration"),Tvo=o(" (BlenderbotSmall model)"),Fvo=l(),lb=a("li"),Uee=a("strong"),Cvo=o("encoder-decoder"),Mvo=o(" \u2014 "),VP=a("a"),Evo=o("EncoderDecoderModel"),yvo=o(" (Encoder decoder model)"),wvo=l(),ib=a("li"),Jee=a("strong"),Avo=o("fsmt"),Lvo=o(" \u2014 "),WP=a("a"),Bvo=o("FSMTForConditionalGeneration"),kvo=o(" (FairSeq Machine-Translation model)"),xvo=l(),db=a("li"),Yee=a("strong"),Rvo=o("led"),Svo=o(" \u2014 "),QP=a("a"),Pvo=o("LEDForConditionalGeneration"),$vo=o(" (LED model)"),Ivo=l(),cb=a("li"),Kee=a("strong"),jvo=o("m2m_100"),Nvo=o(" \u2014 "),HP=a("a"),Dvo=o("M2M100ForConditionalGeneration"),qvo=o(" (M2M100 model)"),Gvo=l(),mb=a("li"),Zee=a("strong"),Ovo=o("marian"),Xvo=o(" \u2014 "),UP=a("a"),zvo=o("MarianMTModel"),Vvo=o(" (Marian model)"),Wvo=l(),fb=a("li"),eoe=a("strong"),Qvo=o("mbart"),Hvo=o(" \u2014 "),JP=a("a"),Uvo=o("MBartForConditionalGeneration"),Jvo=o(" (mBART model)"),Yvo=l(),gb=a("li"),ooe=a("strong"),Kvo=o("mt5"),Zvo=o(" \u2014 "),YP=a("a"),eTo=o("MT5ForConditionalGeneration"),oTo=o(" (mT5 model)"),rTo=l(),hb=a("li"),roe=a("strong"),tTo=o("pegasus"),aTo=o(" \u2014 "),KP=a("a"),sTo=o("PegasusForConditionalGeneration"),nTo=o(" (Pegasus model)"),lTo=l(),ub=a("li"),toe=a("strong"),iTo=o("plbart"),dTo=o(" \u2014 "),ZP=a("a"),cTo=o("PLBartForConditionalGeneration"),mTo=o(" (PLBart model)"),fTo=l(),pb=a("li"),aoe=a("strong"),gTo=o("prophetnet"),hTo=o(" \u2014 "),e$=a("a"),uTo=o("ProphetNetForConditionalGeneration"),pTo=o(" (ProphetNet model)"),_To=l(),_b=a("li"),soe=a("strong"),bTo=o("t5"),vTo=o(" \u2014 "),o$=a("a"),TTo=o("T5ForConditionalGeneration"),FTo=o(" (T5 model)"),CTo=l(),bb=a("li"),noe=a("strong"),MTo=o("xlm-prophetnet"),ETo=o(" \u2014 "),r$=a("a"),yTo=o("XLMProphetNetForConditionalGeneration"),wTo=o(" (XLMProphetNet model)"),ATo=l(),vb=a("p"),LTo=o("The model is set in evaluation mode by default using "),loe=a("code"),BTo=o("model.eval()"),kTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=a("code"),xTo=o("model.train()"),RTo=l(),doe=a("p"),STo=o("Examples:"),PTo=l(),m(uy.$$.fragment),w8e=l(),od=a("h2"),Tb=a("a"),coe=a("span"),m(py.$$.fragment),$To=l(),moe=a("span"),ITo=o("AutoModelForSequenceClassification"),A8e=l(),Jo=a("div"),m(_y.$$.fragment),jTo=l(),rd=a("p"),NTo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),foe=a("code"),DTo=o("from_pretrained()"),qTo=o("class method or the "),goe=a("code"),GTo=o("from_config()"),OTo=o(`class
method.`),XTo=l(),by=a("p"),zTo=o("This class cannot be instantiated directly using "),hoe=a("code"),VTo=o("__init__()"),WTo=o(" (throws an error)."),QTo=l(),Xr=a("div"),m(vy.$$.fragment),HTo=l(),uoe=a("p"),UTo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),JTo=l(),td=a("p"),YTo=o(`Note:
Loading a model from its configuration file does `),poe=a("strong"),KTo=o("not"),ZTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_oe=a("code"),e1o=o("from_pretrained()"),o1o=o("to load the model weights."),r1o=l(),boe=a("p"),t1o=o("Examples:"),a1o=l(),m(Ty.$$.fragment),s1o=l(),$e=a("div"),m(Fy.$$.fragment),n1o=l(),voe=a("p"),l1o=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),i1o=l(),za=a("p"),d1o=o("The model class to instantiate is selected based on the "),Toe=a("code"),c1o=o("model_type"),m1o=o(` property of the config object (either
passed as an argument or loaded from `),Foe=a("code"),f1o=o("pretrained_model_name_or_path"),g1o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=a("code"),h1o=o("pretrained_model_name_or_path"),u1o=o(":"),p1o=l(),A=a("ul"),Fb=a("li"),Moe=a("strong"),_1o=o("albert"),b1o=o(" \u2014 "),t$=a("a"),v1o=o("AlbertForSequenceClassification"),T1o=o(" (ALBERT model)"),F1o=l(),Cb=a("li"),Eoe=a("strong"),C1o=o("bart"),M1o=o(" \u2014 "),a$=a("a"),E1o=o("BartForSequenceClassification"),y1o=o(" (BART model)"),w1o=l(),Mb=a("li"),yoe=a("strong"),A1o=o("bert"),L1o=o(" \u2014 "),s$=a("a"),B1o=o("BertForSequenceClassification"),k1o=o(" (BERT model)"),x1o=l(),Eb=a("li"),woe=a("strong"),R1o=o("big_bird"),S1o=o(" \u2014 "),n$=a("a"),P1o=o("BigBirdForSequenceClassification"),$1o=o(" (BigBird model)"),I1o=l(),yb=a("li"),Aoe=a("strong"),j1o=o("bigbird_pegasus"),N1o=o(" \u2014 "),l$=a("a"),D1o=o("BigBirdPegasusForSequenceClassification"),q1o=o(" (BigBirdPegasus model)"),G1o=l(),wb=a("li"),Loe=a("strong"),O1o=o("camembert"),X1o=o(" \u2014 "),i$=a("a"),z1o=o("CamembertForSequenceClassification"),V1o=o(" (CamemBERT model)"),W1o=l(),Ab=a("li"),Boe=a("strong"),Q1o=o("canine"),H1o=o(" \u2014 "),d$=a("a"),U1o=o("CanineForSequenceClassification"),J1o=o(" (Canine model)"),Y1o=l(),Lb=a("li"),koe=a("strong"),K1o=o("convbert"),Z1o=o(" \u2014 "),c$=a("a"),eFo=o("ConvBertForSequenceClassification"),oFo=o(" (ConvBERT model)"),rFo=l(),Bb=a("li"),xoe=a("strong"),tFo=o("ctrl"),aFo=o(" \u2014 "),m$=a("a"),sFo=o("CTRLForSequenceClassification"),nFo=o(" (CTRL model)"),lFo=l(),kb=a("li"),Roe=a("strong"),iFo=o("deberta"),dFo=o(" \u2014 "),f$=a("a"),cFo=o("DebertaForSequenceClassification"),mFo=o(" (DeBERTa model)"),fFo=l(),xb=a("li"),Soe=a("strong"),gFo=o("deberta-v2"),hFo=o(" \u2014 "),g$=a("a"),uFo=o("DebertaV2ForSequenceClassification"),pFo=o(" (DeBERTa-v2 model)"),_Fo=l(),Rb=a("li"),Poe=a("strong"),bFo=o("distilbert"),vFo=o(" \u2014 "),h$=a("a"),TFo=o("DistilBertForSequenceClassification"),FFo=o(" (DistilBERT model)"),CFo=l(),Sb=a("li"),$oe=a("strong"),MFo=o("electra"),EFo=o(" \u2014 "),u$=a("a"),yFo=o("ElectraForSequenceClassification"),wFo=o(" (ELECTRA model)"),AFo=l(),Pb=a("li"),Ioe=a("strong"),LFo=o("flaubert"),BFo=o(" \u2014 "),p$=a("a"),kFo=o("FlaubertForSequenceClassification"),xFo=o(" (FlauBERT model)"),RFo=l(),$b=a("li"),joe=a("strong"),SFo=o("fnet"),PFo=o(" \u2014 "),_$=a("a"),$Fo=o("FNetForSequenceClassification"),IFo=o(" (FNet model)"),jFo=l(),Ib=a("li"),Noe=a("strong"),NFo=o("funnel"),DFo=o(" \u2014 "),b$=a("a"),qFo=o("FunnelForSequenceClassification"),GFo=o(" (Funnel Transformer model)"),OFo=l(),jb=a("li"),Doe=a("strong"),XFo=o("gpt2"),zFo=o(" \u2014 "),v$=a("a"),VFo=o("GPT2ForSequenceClassification"),WFo=o(" (OpenAI GPT-2 model)"),QFo=l(),Nb=a("li"),qoe=a("strong"),HFo=o("gpt_neo"),UFo=o(" \u2014 "),T$=a("a"),JFo=o("GPTNeoForSequenceClassification"),YFo=o(" (GPT Neo model)"),KFo=l(),Db=a("li"),Goe=a("strong"),ZFo=o("gptj"),eCo=o(" \u2014 "),F$=a("a"),oCo=o("GPTJForSequenceClassification"),rCo=o(" (GPT-J model)"),tCo=l(),qb=a("li"),Ooe=a("strong"),aCo=o("ibert"),sCo=o(" \u2014 "),C$=a("a"),nCo=o("IBertForSequenceClassification"),lCo=o(" (I-BERT model)"),iCo=l(),Gb=a("li"),Xoe=a("strong"),dCo=o("layoutlm"),cCo=o(" \u2014 "),M$=a("a"),mCo=o("LayoutLMForSequenceClassification"),fCo=o(" (LayoutLM model)"),gCo=l(),Ob=a("li"),zoe=a("strong"),hCo=o("layoutlmv2"),uCo=o(" \u2014 "),E$=a("a"),pCo=o("LayoutLMv2ForSequenceClassification"),_Co=o(" (LayoutLMv2 model)"),bCo=l(),Xb=a("li"),Voe=a("strong"),vCo=o("led"),TCo=o(" \u2014 "),y$=a("a"),FCo=o("LEDForSequenceClassification"),CCo=o(" (LED model)"),MCo=l(),zb=a("li"),Woe=a("strong"),ECo=o("longformer"),yCo=o(" \u2014 "),w$=a("a"),wCo=o("LongformerForSequenceClassification"),ACo=o(" (Longformer model)"),LCo=l(),Vb=a("li"),Qoe=a("strong"),BCo=o("mbart"),kCo=o(" \u2014 "),A$=a("a"),xCo=o("MBartForSequenceClassification"),RCo=o(" (mBART model)"),SCo=l(),Wb=a("li"),Hoe=a("strong"),PCo=o("megatron-bert"),$Co=o(" \u2014 "),L$=a("a"),ICo=o("MegatronBertForSequenceClassification"),jCo=o(" (MegatronBert model)"),NCo=l(),Qb=a("li"),Uoe=a("strong"),DCo=o("mobilebert"),qCo=o(" \u2014 "),B$=a("a"),GCo=o("MobileBertForSequenceClassification"),OCo=o(" (MobileBERT model)"),XCo=l(),Hb=a("li"),Joe=a("strong"),zCo=o("mpnet"),VCo=o(" \u2014 "),k$=a("a"),WCo=o("MPNetForSequenceClassification"),QCo=o(" (MPNet model)"),HCo=l(),Ub=a("li"),Yoe=a("strong"),UCo=o("nystromformer"),JCo=o(" \u2014 "),x$=a("a"),YCo=o("NystromformerForSequenceClassification"),KCo=o(" (Nystromformer model)"),ZCo=l(),Jb=a("li"),Koe=a("strong"),e4o=o("openai-gpt"),o4o=o(" \u2014 "),R$=a("a"),r4o=o("OpenAIGPTForSequenceClassification"),t4o=o(" (OpenAI GPT model)"),a4o=l(),Yb=a("li"),Zoe=a("strong"),s4o=o("perceiver"),n4o=o(" \u2014 "),S$=a("a"),l4o=o("PerceiverForSequenceClassification"),i4o=o(" (Perceiver model)"),d4o=l(),Kb=a("li"),ere=a("strong"),c4o=o("plbart"),m4o=o(" \u2014 "),P$=a("a"),f4o=o("PLBartForSequenceClassification"),g4o=o(" (PLBart model)"),h4o=l(),Zb=a("li"),ore=a("strong"),u4o=o("qdqbert"),p4o=o(" \u2014 "),$$=a("a"),_4o=o("QDQBertForSequenceClassification"),b4o=o(" (QDQBert model)"),v4o=l(),e2=a("li"),rre=a("strong"),T4o=o("reformer"),F4o=o(" \u2014 "),I$=a("a"),C4o=o("ReformerForSequenceClassification"),M4o=o(" (Reformer model)"),E4o=l(),o2=a("li"),tre=a("strong"),y4o=o("rembert"),w4o=o(" \u2014 "),j$=a("a"),A4o=o("RemBertForSequenceClassification"),L4o=o(" (RemBERT model)"),B4o=l(),r2=a("li"),are=a("strong"),k4o=o("roberta"),x4o=o(" \u2014 "),N$=a("a"),R4o=o("RobertaForSequenceClassification"),S4o=o(" (RoBERTa model)"),P4o=l(),t2=a("li"),sre=a("strong"),$4o=o("roformer"),I4o=o(" \u2014 "),D$=a("a"),j4o=o("RoFormerForSequenceClassification"),N4o=o(" (RoFormer model)"),D4o=l(),a2=a("li"),nre=a("strong"),q4o=o("squeezebert"),G4o=o(" \u2014 "),q$=a("a"),O4o=o("SqueezeBertForSequenceClassification"),X4o=o(" (SqueezeBERT model)"),z4o=l(),s2=a("li"),lre=a("strong"),V4o=o("tapas"),W4o=o(" \u2014 "),G$=a("a"),Q4o=o("TapasForSequenceClassification"),H4o=o(" (TAPAS model)"),U4o=l(),n2=a("li"),ire=a("strong"),J4o=o("transfo-xl"),Y4o=o(" \u2014 "),O$=a("a"),K4o=o("TransfoXLForSequenceClassification"),Z4o=o(" (Transformer-XL model)"),eMo=l(),l2=a("li"),dre=a("strong"),oMo=o("xlm"),rMo=o(" \u2014 "),X$=a("a"),tMo=o("XLMForSequenceClassification"),aMo=o(" (XLM model)"),sMo=l(),i2=a("li"),cre=a("strong"),nMo=o("xlm-roberta"),lMo=o(" \u2014 "),z$=a("a"),iMo=o("XLMRobertaForSequenceClassification"),dMo=o(" (XLM-RoBERTa model)"),cMo=l(),d2=a("li"),mre=a("strong"),mMo=o("xlm-roberta-xl"),fMo=o(" \u2014 "),V$=a("a"),gMo=o("XLMRobertaXLForSequenceClassification"),hMo=o(" (XLM-RoBERTa-XL model)"),uMo=l(),c2=a("li"),fre=a("strong"),pMo=o("xlnet"),_Mo=o(" \u2014 "),W$=a("a"),bMo=o("XLNetForSequenceClassification"),vMo=o(" (XLNet model)"),TMo=l(),m2=a("li"),gre=a("strong"),FMo=o("yoso"),CMo=o(" \u2014 "),Q$=a("a"),MMo=o("YosoForSequenceClassification"),EMo=o(" (YOSO model)"),yMo=l(),f2=a("p"),wMo=o("The model is set in evaluation mode by default using "),hre=a("code"),AMo=o("model.eval()"),LMo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ure=a("code"),BMo=o("model.train()"),kMo=l(),pre=a("p"),xMo=o("Examples:"),RMo=l(),m(Cy.$$.fragment),L8e=l(),ad=a("h2"),g2=a("a"),_re=a("span"),m(My.$$.fragment),SMo=l(),bre=a("span"),PMo=o("AutoModelForMultipleChoice"),B8e=l(),Yo=a("div"),m(Ey.$$.fragment),$Mo=l(),sd=a("p"),IMo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=a("code"),jMo=o("from_pretrained()"),NMo=o("class method or the "),Tre=a("code"),DMo=o("from_config()"),qMo=o(`class
method.`),GMo=l(),yy=a("p"),OMo=o("This class cannot be instantiated directly using "),Fre=a("code"),XMo=o("__init__()"),zMo=o(" (throws an error)."),VMo=l(),zr=a("div"),m(wy.$$.fragment),WMo=l(),Cre=a("p"),QMo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),HMo=l(),nd=a("p"),UMo=o(`Note:
Loading a model from its configuration file does `),Mre=a("strong"),JMo=o("not"),YMo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=a("code"),KMo=o("from_pretrained()"),ZMo=o("to load the model weights."),eEo=l(),yre=a("p"),oEo=o("Examples:"),rEo=l(),m(Ay.$$.fragment),tEo=l(),Ie=a("div"),m(Ly.$$.fragment),aEo=l(),wre=a("p"),sEo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),nEo=l(),Va=a("p"),lEo=o("The model class to instantiate is selected based on the "),Are=a("code"),iEo=o("model_type"),dEo=o(` property of the config object (either
passed as an argument or loaded from `),Lre=a("code"),cEo=o("pretrained_model_name_or_path"),mEo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=a("code"),fEo=o("pretrained_model_name_or_path"),gEo=o(":"),hEo=l(),G=a("ul"),h2=a("li"),kre=a("strong"),uEo=o("albert"),pEo=o(" \u2014 "),H$=a("a"),_Eo=o("AlbertForMultipleChoice"),bEo=o(" (ALBERT model)"),vEo=l(),u2=a("li"),xre=a("strong"),TEo=o("bert"),FEo=o(" \u2014 "),U$=a("a"),CEo=o("BertForMultipleChoice"),MEo=o(" (BERT model)"),EEo=l(),p2=a("li"),Rre=a("strong"),yEo=o("big_bird"),wEo=o(" \u2014 "),J$=a("a"),AEo=o("BigBirdForMultipleChoice"),LEo=o(" (BigBird model)"),BEo=l(),_2=a("li"),Sre=a("strong"),kEo=o("camembert"),xEo=o(" \u2014 "),Y$=a("a"),REo=o("CamembertForMultipleChoice"),SEo=o(" (CamemBERT model)"),PEo=l(),b2=a("li"),Pre=a("strong"),$Eo=o("canine"),IEo=o(" \u2014 "),K$=a("a"),jEo=o("CanineForMultipleChoice"),NEo=o(" (Canine model)"),DEo=l(),v2=a("li"),$re=a("strong"),qEo=o("convbert"),GEo=o(" \u2014 "),Z$=a("a"),OEo=o("ConvBertForMultipleChoice"),XEo=o(" (ConvBERT model)"),zEo=l(),T2=a("li"),Ire=a("strong"),VEo=o("distilbert"),WEo=o(" \u2014 "),eI=a("a"),QEo=o("DistilBertForMultipleChoice"),HEo=o(" (DistilBERT model)"),UEo=l(),F2=a("li"),jre=a("strong"),JEo=o("electra"),YEo=o(" \u2014 "),oI=a("a"),KEo=o("ElectraForMultipleChoice"),ZEo=o(" (ELECTRA model)"),e3o=l(),C2=a("li"),Nre=a("strong"),o3o=o("flaubert"),r3o=o(" \u2014 "),rI=a("a"),t3o=o("FlaubertForMultipleChoice"),a3o=o(" (FlauBERT model)"),s3o=l(),M2=a("li"),Dre=a("strong"),n3o=o("fnet"),l3o=o(" \u2014 "),tI=a("a"),i3o=o("FNetForMultipleChoice"),d3o=o(" (FNet model)"),c3o=l(),E2=a("li"),qre=a("strong"),m3o=o("funnel"),f3o=o(" \u2014 "),aI=a("a"),g3o=o("FunnelForMultipleChoice"),h3o=o(" (Funnel Transformer model)"),u3o=l(),y2=a("li"),Gre=a("strong"),p3o=o("ibert"),_3o=o(" \u2014 "),sI=a("a"),b3o=o("IBertForMultipleChoice"),v3o=o(" (I-BERT model)"),T3o=l(),w2=a("li"),Ore=a("strong"),F3o=o("longformer"),C3o=o(" \u2014 "),nI=a("a"),M3o=o("LongformerForMultipleChoice"),E3o=o(" (Longformer model)"),y3o=l(),A2=a("li"),Xre=a("strong"),w3o=o("megatron-bert"),A3o=o(" \u2014 "),lI=a("a"),L3o=o("MegatronBertForMultipleChoice"),B3o=o(" (MegatronBert model)"),k3o=l(),L2=a("li"),zre=a("strong"),x3o=o("mobilebert"),R3o=o(" \u2014 "),iI=a("a"),S3o=o("MobileBertForMultipleChoice"),P3o=o(" (MobileBERT model)"),$3o=l(),B2=a("li"),Vre=a("strong"),I3o=o("mpnet"),j3o=o(" \u2014 "),dI=a("a"),N3o=o("MPNetForMultipleChoice"),D3o=o(" (MPNet model)"),q3o=l(),k2=a("li"),Wre=a("strong"),G3o=o("nystromformer"),O3o=o(" \u2014 "),cI=a("a"),X3o=o("NystromformerForMultipleChoice"),z3o=o(" (Nystromformer model)"),V3o=l(),x2=a("li"),Qre=a("strong"),W3o=o("qdqbert"),Q3o=o(" \u2014 "),mI=a("a"),H3o=o("QDQBertForMultipleChoice"),U3o=o(" (QDQBert model)"),J3o=l(),R2=a("li"),Hre=a("strong"),Y3o=o("rembert"),K3o=o(" \u2014 "),fI=a("a"),Z3o=o("RemBertForMultipleChoice"),e5o=o(" (RemBERT model)"),o5o=l(),S2=a("li"),Ure=a("strong"),r5o=o("roberta"),t5o=o(" \u2014 "),gI=a("a"),a5o=o("RobertaForMultipleChoice"),s5o=o(" (RoBERTa model)"),n5o=l(),P2=a("li"),Jre=a("strong"),l5o=o("roformer"),i5o=o(" \u2014 "),hI=a("a"),d5o=o("RoFormerForMultipleChoice"),c5o=o(" (RoFormer model)"),m5o=l(),$2=a("li"),Yre=a("strong"),f5o=o("squeezebert"),g5o=o(" \u2014 "),uI=a("a"),h5o=o("SqueezeBertForMultipleChoice"),u5o=o(" (SqueezeBERT model)"),p5o=l(),I2=a("li"),Kre=a("strong"),_5o=o("xlm"),b5o=o(" \u2014 "),pI=a("a"),v5o=o("XLMForMultipleChoice"),T5o=o(" (XLM model)"),F5o=l(),j2=a("li"),Zre=a("strong"),C5o=o("xlm-roberta"),M5o=o(" \u2014 "),_I=a("a"),E5o=o("XLMRobertaForMultipleChoice"),y5o=o(" (XLM-RoBERTa model)"),w5o=l(),N2=a("li"),ete=a("strong"),A5o=o("xlm-roberta-xl"),L5o=o(" \u2014 "),bI=a("a"),B5o=o("XLMRobertaXLForMultipleChoice"),k5o=o(" (XLM-RoBERTa-XL model)"),x5o=l(),D2=a("li"),ote=a("strong"),R5o=o("xlnet"),S5o=o(" \u2014 "),vI=a("a"),P5o=o("XLNetForMultipleChoice"),$5o=o(" (XLNet model)"),I5o=l(),q2=a("li"),rte=a("strong"),j5o=o("yoso"),N5o=o(" \u2014 "),TI=a("a"),D5o=o("YosoForMultipleChoice"),q5o=o(" (YOSO model)"),G5o=l(),G2=a("p"),O5o=o("The model is set in evaluation mode by default using "),tte=a("code"),X5o=o("model.eval()"),z5o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=a("code"),V5o=o("model.train()"),W5o=l(),ste=a("p"),Q5o=o("Examples:"),H5o=l(),m(By.$$.fragment),k8e=l(),ld=a("h2"),O2=a("a"),nte=a("span"),m(ky.$$.fragment),U5o=l(),lte=a("span"),J5o=o("AutoModelForNextSentencePrediction"),x8e=l(),Ko=a("div"),m(xy.$$.fragment),Y5o=l(),id=a("p"),K5o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=a("code"),Z5o=o("from_pretrained()"),eyo=o("class method or the "),dte=a("code"),oyo=o("from_config()"),ryo=o(`class
method.`),tyo=l(),Ry=a("p"),ayo=o("This class cannot be instantiated directly using "),cte=a("code"),syo=o("__init__()"),nyo=o(" (throws an error)."),lyo=l(),Vr=a("div"),m(Sy.$$.fragment),iyo=l(),mte=a("p"),dyo=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),cyo=l(),dd=a("p"),myo=o(`Note:
Loading a model from its configuration file does `),fte=a("strong"),fyo=o("not"),gyo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=a("code"),hyo=o("from_pretrained()"),uyo=o("to load the model weights."),pyo=l(),hte=a("p"),_yo=o("Examples:"),byo=l(),m(Py.$$.fragment),vyo=l(),je=a("div"),m($y.$$.fragment),Tyo=l(),ute=a("p"),Fyo=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Cyo=l(),Wa=a("p"),Myo=o("The model class to instantiate is selected based on the "),pte=a("code"),Eyo=o("model_type"),yyo=o(` property of the config object (either
passed as an argument or loaded from `),_te=a("code"),wyo=o("pretrained_model_name_or_path"),Ayo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=a("code"),Lyo=o("pretrained_model_name_or_path"),Byo=o(":"),kyo=l(),sa=a("ul"),X2=a("li"),vte=a("strong"),xyo=o("bert"),Ryo=o(" \u2014 "),FI=a("a"),Syo=o("BertForNextSentencePrediction"),Pyo=o(" (BERT model)"),$yo=l(),z2=a("li"),Tte=a("strong"),Iyo=o("fnet"),jyo=o(" \u2014 "),CI=a("a"),Nyo=o("FNetForNextSentencePrediction"),Dyo=o(" (FNet model)"),qyo=l(),V2=a("li"),Fte=a("strong"),Gyo=o("megatron-bert"),Oyo=o(" \u2014 "),MI=a("a"),Xyo=o("MegatronBertForNextSentencePrediction"),zyo=o(" (MegatronBert model)"),Vyo=l(),W2=a("li"),Cte=a("strong"),Wyo=o("mobilebert"),Qyo=o(" \u2014 "),EI=a("a"),Hyo=o("MobileBertForNextSentencePrediction"),Uyo=o(" (MobileBERT model)"),Jyo=l(),Q2=a("li"),Mte=a("strong"),Yyo=o("qdqbert"),Kyo=o(" \u2014 "),yI=a("a"),Zyo=o("QDQBertForNextSentencePrediction"),ewo=o(" (QDQBert model)"),owo=l(),H2=a("p"),rwo=o("The model is set in evaluation mode by default using "),Ete=a("code"),two=o("model.eval()"),awo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=a("code"),swo=o("model.train()"),nwo=l(),wte=a("p"),lwo=o("Examples:"),iwo=l(),m(Iy.$$.fragment),R8e=l(),cd=a("h2"),U2=a("a"),Ate=a("span"),m(jy.$$.fragment),dwo=l(),Lte=a("span"),cwo=o("AutoModelForTokenClassification"),S8e=l(),Zo=a("div"),m(Ny.$$.fragment),mwo=l(),md=a("p"),fwo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=a("code"),gwo=o("from_pretrained()"),hwo=o("class method or the "),kte=a("code"),uwo=o("from_config()"),pwo=o(`class
method.`),_wo=l(),Dy=a("p"),bwo=o("This class cannot be instantiated directly using "),xte=a("code"),vwo=o("__init__()"),Two=o(" (throws an error)."),Fwo=l(),Wr=a("div"),m(qy.$$.fragment),Cwo=l(),Rte=a("p"),Mwo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Ewo=l(),fd=a("p"),ywo=o(`Note:
Loading a model from its configuration file does `),Ste=a("strong"),wwo=o("not"),Awo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=a("code"),Lwo=o("from_pretrained()"),Bwo=o("to load the model weights."),kwo=l(),$te=a("p"),xwo=o("Examples:"),Rwo=l(),m(Gy.$$.fragment),Swo=l(),Ne=a("div"),m(Oy.$$.fragment),Pwo=l(),Ite=a("p"),$wo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Iwo=l(),Qa=a("p"),jwo=o("The model class to instantiate is selected based on the "),jte=a("code"),Nwo=o("model_type"),Dwo=o(` property of the config object (either
passed as an argument or loaded from `),Nte=a("code"),qwo=o("pretrained_model_name_or_path"),Gwo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=a("code"),Owo=o("pretrained_model_name_or_path"),Xwo=o(":"),zwo=l(),D=a("ul"),J2=a("li"),qte=a("strong"),Vwo=o("albert"),Wwo=o(" \u2014 "),wI=a("a"),Qwo=o("AlbertForTokenClassification"),Hwo=o(" (ALBERT model)"),Uwo=l(),Y2=a("li"),Gte=a("strong"),Jwo=o("bert"),Ywo=o(" \u2014 "),AI=a("a"),Kwo=o("BertForTokenClassification"),Zwo=o(" (BERT model)"),eAo=l(),K2=a("li"),Ote=a("strong"),oAo=o("big_bird"),rAo=o(" \u2014 "),LI=a("a"),tAo=o("BigBirdForTokenClassification"),aAo=o(" (BigBird model)"),sAo=l(),Z2=a("li"),Xte=a("strong"),nAo=o("camembert"),lAo=o(" \u2014 "),BI=a("a"),iAo=o("CamembertForTokenClassification"),dAo=o(" (CamemBERT model)"),cAo=l(),ev=a("li"),zte=a("strong"),mAo=o("canine"),fAo=o(" \u2014 "),kI=a("a"),gAo=o("CanineForTokenClassification"),hAo=o(" (Canine model)"),uAo=l(),ov=a("li"),Vte=a("strong"),pAo=o("convbert"),_Ao=o(" \u2014 "),xI=a("a"),bAo=o("ConvBertForTokenClassification"),vAo=o(" (ConvBERT model)"),TAo=l(),rv=a("li"),Wte=a("strong"),FAo=o("deberta"),CAo=o(" \u2014 "),RI=a("a"),MAo=o("DebertaForTokenClassification"),EAo=o(" (DeBERTa model)"),yAo=l(),tv=a("li"),Qte=a("strong"),wAo=o("deberta-v2"),AAo=o(" \u2014 "),SI=a("a"),LAo=o("DebertaV2ForTokenClassification"),BAo=o(" (DeBERTa-v2 model)"),kAo=l(),av=a("li"),Hte=a("strong"),xAo=o("distilbert"),RAo=o(" \u2014 "),PI=a("a"),SAo=o("DistilBertForTokenClassification"),PAo=o(" (DistilBERT model)"),$Ao=l(),sv=a("li"),Ute=a("strong"),IAo=o("electra"),jAo=o(" \u2014 "),$I=a("a"),NAo=o("ElectraForTokenClassification"),DAo=o(" (ELECTRA model)"),qAo=l(),nv=a("li"),Jte=a("strong"),GAo=o("flaubert"),OAo=o(" \u2014 "),II=a("a"),XAo=o("FlaubertForTokenClassification"),zAo=o(" (FlauBERT model)"),VAo=l(),lv=a("li"),Yte=a("strong"),WAo=o("fnet"),QAo=o(" \u2014 "),jI=a("a"),HAo=o("FNetForTokenClassification"),UAo=o(" (FNet model)"),JAo=l(),iv=a("li"),Kte=a("strong"),YAo=o("funnel"),KAo=o(" \u2014 "),NI=a("a"),ZAo=o("FunnelForTokenClassification"),e6o=o(" (Funnel Transformer model)"),o6o=l(),dv=a("li"),Zte=a("strong"),r6o=o("gpt2"),t6o=o(" \u2014 "),DI=a("a"),a6o=o("GPT2ForTokenClassification"),s6o=o(" (OpenAI GPT-2 model)"),n6o=l(),cv=a("li"),eae=a("strong"),l6o=o("ibert"),i6o=o(" \u2014 "),qI=a("a"),d6o=o("IBertForTokenClassification"),c6o=o(" (I-BERT model)"),m6o=l(),mv=a("li"),oae=a("strong"),f6o=o("layoutlm"),g6o=o(" \u2014 "),GI=a("a"),h6o=o("LayoutLMForTokenClassification"),u6o=o(" (LayoutLM model)"),p6o=l(),fv=a("li"),rae=a("strong"),_6o=o("layoutlmv2"),b6o=o(" \u2014 "),OI=a("a"),v6o=o("LayoutLMv2ForTokenClassification"),T6o=o(" (LayoutLMv2 model)"),F6o=l(),gv=a("li"),tae=a("strong"),C6o=o("longformer"),M6o=o(" \u2014 "),XI=a("a"),E6o=o("LongformerForTokenClassification"),y6o=o(" (Longformer model)"),w6o=l(),hv=a("li"),aae=a("strong"),A6o=o("megatron-bert"),L6o=o(" \u2014 "),zI=a("a"),B6o=o("MegatronBertForTokenClassification"),k6o=o(" (MegatronBert model)"),x6o=l(),uv=a("li"),sae=a("strong"),R6o=o("mobilebert"),S6o=o(" \u2014 "),VI=a("a"),P6o=o("MobileBertForTokenClassification"),$6o=o(" (MobileBERT model)"),I6o=l(),pv=a("li"),nae=a("strong"),j6o=o("mpnet"),N6o=o(" \u2014 "),WI=a("a"),D6o=o("MPNetForTokenClassification"),q6o=o(" (MPNet model)"),G6o=l(),_v=a("li"),lae=a("strong"),O6o=o("nystromformer"),X6o=o(" \u2014 "),QI=a("a"),z6o=o("NystromformerForTokenClassification"),V6o=o(" (Nystromformer model)"),W6o=l(),bv=a("li"),iae=a("strong"),Q6o=o("qdqbert"),H6o=o(" \u2014 "),HI=a("a"),U6o=o("QDQBertForTokenClassification"),J6o=o(" (QDQBert model)"),Y6o=l(),vv=a("li"),dae=a("strong"),K6o=o("rembert"),Z6o=o(" \u2014 "),UI=a("a"),e0o=o("RemBertForTokenClassification"),o0o=o(" (RemBERT model)"),r0o=l(),Tv=a("li"),cae=a("strong"),t0o=o("roberta"),a0o=o(" \u2014 "),JI=a("a"),s0o=o("RobertaForTokenClassification"),n0o=o(" (RoBERTa model)"),l0o=l(),Fv=a("li"),mae=a("strong"),i0o=o("roformer"),d0o=o(" \u2014 "),YI=a("a"),c0o=o("RoFormerForTokenClassification"),m0o=o(" (RoFormer model)"),f0o=l(),Cv=a("li"),fae=a("strong"),g0o=o("squeezebert"),h0o=o(" \u2014 "),KI=a("a"),u0o=o("SqueezeBertForTokenClassification"),p0o=o(" (SqueezeBERT model)"),_0o=l(),Mv=a("li"),gae=a("strong"),b0o=o("xlm"),v0o=o(" \u2014 "),ZI=a("a"),T0o=o("XLMForTokenClassification"),F0o=o(" (XLM model)"),C0o=l(),Ev=a("li"),hae=a("strong"),M0o=o("xlm-roberta"),E0o=o(" \u2014 "),ej=a("a"),y0o=o("XLMRobertaForTokenClassification"),w0o=o(" (XLM-RoBERTa model)"),A0o=l(),yv=a("li"),uae=a("strong"),L0o=o("xlm-roberta-xl"),B0o=o(" \u2014 "),oj=a("a"),k0o=o("XLMRobertaXLForTokenClassification"),x0o=o(" (XLM-RoBERTa-XL model)"),R0o=l(),wv=a("li"),pae=a("strong"),S0o=o("xlnet"),P0o=o(" \u2014 "),rj=a("a"),$0o=o("XLNetForTokenClassification"),I0o=o(" (XLNet model)"),j0o=l(),Av=a("li"),_ae=a("strong"),N0o=o("yoso"),D0o=o(" \u2014 "),tj=a("a"),q0o=o("YosoForTokenClassification"),G0o=o(" (YOSO model)"),O0o=l(),Lv=a("p"),X0o=o("The model is set in evaluation mode by default using "),bae=a("code"),z0o=o("model.eval()"),V0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=a("code"),W0o=o("model.train()"),Q0o=l(),Tae=a("p"),H0o=o("Examples:"),U0o=l(),m(Xy.$$.fragment),P8e=l(),gd=a("h2"),Bv=a("a"),Fae=a("span"),m(zy.$$.fragment),J0o=l(),Cae=a("span"),Y0o=o("AutoModelForQuestionAnswering"),$8e=l(),er=a("div"),m(Vy.$$.fragment),K0o=l(),hd=a("p"),Z0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=a("code"),eLo=o("from_pretrained()"),oLo=o("class method or the "),Eae=a("code"),rLo=o("from_config()"),tLo=o(`class
method.`),aLo=l(),Wy=a("p"),sLo=o("This class cannot be instantiated directly using "),yae=a("code"),nLo=o("__init__()"),lLo=o(" (throws an error)."),iLo=l(),Qr=a("div"),m(Qy.$$.fragment),dLo=l(),wae=a("p"),cLo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),mLo=l(),ud=a("p"),fLo=o(`Note:
Loading a model from its configuration file does `),Aae=a("strong"),gLo=o("not"),hLo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=a("code"),uLo=o("from_pretrained()"),pLo=o("to load the model weights."),_Lo=l(),Bae=a("p"),bLo=o("Examples:"),vLo=l(),m(Hy.$$.fragment),TLo=l(),De=a("div"),m(Uy.$$.fragment),FLo=l(),kae=a("p"),CLo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),MLo=l(),Ha=a("p"),ELo=o("The model class to instantiate is selected based on the "),xae=a("code"),yLo=o("model_type"),wLo=o(` property of the config object (either
passed as an argument or loaded from `),Rae=a("code"),ALo=o("pretrained_model_name_or_path"),LLo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=a("code"),BLo=o("pretrained_model_name_or_path"),kLo=o(":"),xLo=l(),R=a("ul"),kv=a("li"),Pae=a("strong"),RLo=o("albert"),SLo=o(" \u2014 "),aj=a("a"),PLo=o("AlbertForQuestionAnswering"),$Lo=o(" (ALBERT model)"),ILo=l(),xv=a("li"),$ae=a("strong"),jLo=o("bart"),NLo=o(" \u2014 "),sj=a("a"),DLo=o("BartForQuestionAnswering"),qLo=o(" (BART model)"),GLo=l(),Rv=a("li"),Iae=a("strong"),OLo=o("bert"),XLo=o(" \u2014 "),nj=a("a"),zLo=o("BertForQuestionAnswering"),VLo=o(" (BERT model)"),WLo=l(),Sv=a("li"),jae=a("strong"),QLo=o("big_bird"),HLo=o(" \u2014 "),lj=a("a"),ULo=o("BigBirdForQuestionAnswering"),JLo=o(" (BigBird model)"),YLo=l(),Pv=a("li"),Nae=a("strong"),KLo=o("bigbird_pegasus"),ZLo=o(" \u2014 "),ij=a("a"),e7o=o("BigBirdPegasusForQuestionAnswering"),o7o=o(" (BigBirdPegasus model)"),r7o=l(),$v=a("li"),Dae=a("strong"),t7o=o("camembert"),a7o=o(" \u2014 "),dj=a("a"),s7o=o("CamembertForQuestionAnswering"),n7o=o(" (CamemBERT model)"),l7o=l(),Iv=a("li"),qae=a("strong"),i7o=o("canine"),d7o=o(" \u2014 "),cj=a("a"),c7o=o("CanineForQuestionAnswering"),m7o=o(" (Canine model)"),f7o=l(),jv=a("li"),Gae=a("strong"),g7o=o("convbert"),h7o=o(" \u2014 "),mj=a("a"),u7o=o("ConvBertForQuestionAnswering"),p7o=o(" (ConvBERT model)"),_7o=l(),Nv=a("li"),Oae=a("strong"),b7o=o("deberta"),v7o=o(" \u2014 "),fj=a("a"),T7o=o("DebertaForQuestionAnswering"),F7o=o(" (DeBERTa model)"),C7o=l(),Dv=a("li"),Xae=a("strong"),M7o=o("deberta-v2"),E7o=o(" \u2014 "),gj=a("a"),y7o=o("DebertaV2ForQuestionAnswering"),w7o=o(" (DeBERTa-v2 model)"),A7o=l(),qv=a("li"),zae=a("strong"),L7o=o("distilbert"),B7o=o(" \u2014 "),hj=a("a"),k7o=o("DistilBertForQuestionAnswering"),x7o=o(" (DistilBERT model)"),R7o=l(),Gv=a("li"),Vae=a("strong"),S7o=o("electra"),P7o=o(" \u2014 "),uj=a("a"),$7o=o("ElectraForQuestionAnswering"),I7o=o(" (ELECTRA model)"),j7o=l(),Ov=a("li"),Wae=a("strong"),N7o=o("flaubert"),D7o=o(" \u2014 "),pj=a("a"),q7o=o("FlaubertForQuestionAnsweringSimple"),G7o=o(" (FlauBERT model)"),O7o=l(),Xv=a("li"),Qae=a("strong"),X7o=o("fnet"),z7o=o(" \u2014 "),_j=a("a"),V7o=o("FNetForQuestionAnswering"),W7o=o(" (FNet model)"),Q7o=l(),zv=a("li"),Hae=a("strong"),H7o=o("funnel"),U7o=o(" \u2014 "),bj=a("a"),J7o=o("FunnelForQuestionAnswering"),Y7o=o(" (Funnel Transformer model)"),K7o=l(),Vv=a("li"),Uae=a("strong"),Z7o=o("gptj"),e8o=o(" \u2014 "),vj=a("a"),o8o=o("GPTJForQuestionAnswering"),r8o=o(" (GPT-J model)"),t8o=l(),Wv=a("li"),Jae=a("strong"),a8o=o("ibert"),s8o=o(" \u2014 "),Tj=a("a"),n8o=o("IBertForQuestionAnswering"),l8o=o(" (I-BERT model)"),i8o=l(),Qv=a("li"),Yae=a("strong"),d8o=o("layoutlmv2"),c8o=o(" \u2014 "),Fj=a("a"),m8o=o("LayoutLMv2ForQuestionAnswering"),f8o=o(" (LayoutLMv2 model)"),g8o=l(),Hv=a("li"),Kae=a("strong"),h8o=o("led"),u8o=o(" \u2014 "),Cj=a("a"),p8o=o("LEDForQuestionAnswering"),_8o=o(" (LED model)"),b8o=l(),Uv=a("li"),Zae=a("strong"),v8o=o("longformer"),T8o=o(" \u2014 "),Mj=a("a"),F8o=o("LongformerForQuestionAnswering"),C8o=o(" (Longformer model)"),M8o=l(),Jv=a("li"),ese=a("strong"),E8o=o("lxmert"),y8o=o(" \u2014 "),Ej=a("a"),w8o=o("LxmertForQuestionAnswering"),A8o=o(" (LXMERT model)"),L8o=l(),Yv=a("li"),ose=a("strong"),B8o=o("mbart"),k8o=o(" \u2014 "),yj=a("a"),x8o=o("MBartForQuestionAnswering"),R8o=o(" (mBART model)"),S8o=l(),Kv=a("li"),rse=a("strong"),P8o=o("megatron-bert"),$8o=o(" \u2014 "),wj=a("a"),I8o=o("MegatronBertForQuestionAnswering"),j8o=o(" (MegatronBert model)"),N8o=l(),Zv=a("li"),tse=a("strong"),D8o=o("mobilebert"),q8o=o(" \u2014 "),Aj=a("a"),G8o=o("MobileBertForQuestionAnswering"),O8o=o(" (MobileBERT model)"),X8o=l(),eT=a("li"),ase=a("strong"),z8o=o("mpnet"),V8o=o(" \u2014 "),Lj=a("a"),W8o=o("MPNetForQuestionAnswering"),Q8o=o(" (MPNet model)"),H8o=l(),oT=a("li"),sse=a("strong"),U8o=o("nystromformer"),J8o=o(" \u2014 "),Bj=a("a"),Y8o=o("NystromformerForQuestionAnswering"),K8o=o(" (Nystromformer model)"),Z8o=l(),rT=a("li"),nse=a("strong"),e9o=o("qdqbert"),o9o=o(" \u2014 "),kj=a("a"),r9o=o("QDQBertForQuestionAnswering"),t9o=o(" (QDQBert model)"),a9o=l(),tT=a("li"),lse=a("strong"),s9o=o("reformer"),n9o=o(" \u2014 "),xj=a("a"),l9o=o("ReformerForQuestionAnswering"),i9o=o(" (Reformer model)"),d9o=l(),aT=a("li"),ise=a("strong"),c9o=o("rembert"),m9o=o(" \u2014 "),Rj=a("a"),f9o=o("RemBertForQuestionAnswering"),g9o=o(" (RemBERT model)"),h9o=l(),sT=a("li"),dse=a("strong"),u9o=o("roberta"),p9o=o(" \u2014 "),Sj=a("a"),_9o=o("RobertaForQuestionAnswering"),b9o=o(" (RoBERTa model)"),v9o=l(),nT=a("li"),cse=a("strong"),T9o=o("roformer"),F9o=o(" \u2014 "),Pj=a("a"),C9o=o("RoFormerForQuestionAnswering"),M9o=o(" (RoFormer model)"),E9o=l(),lT=a("li"),mse=a("strong"),y9o=o("splinter"),w9o=o(" \u2014 "),$j=a("a"),A9o=o("SplinterForQuestionAnswering"),L9o=o(" (Splinter model)"),B9o=l(),iT=a("li"),fse=a("strong"),k9o=o("squeezebert"),x9o=o(" \u2014 "),Ij=a("a"),R9o=o("SqueezeBertForQuestionAnswering"),S9o=o(" (SqueezeBERT model)"),P9o=l(),dT=a("li"),gse=a("strong"),$9o=o("xlm"),I9o=o(" \u2014 "),jj=a("a"),j9o=o("XLMForQuestionAnsweringSimple"),N9o=o(" (XLM model)"),D9o=l(),cT=a("li"),hse=a("strong"),q9o=o("xlm-roberta"),G9o=o(" \u2014 "),Nj=a("a"),O9o=o("XLMRobertaForQuestionAnswering"),X9o=o(" (XLM-RoBERTa model)"),z9o=l(),mT=a("li"),use=a("strong"),V9o=o("xlm-roberta-xl"),W9o=o(" \u2014 "),Dj=a("a"),Q9o=o("XLMRobertaXLForQuestionAnswering"),H9o=o(" (XLM-RoBERTa-XL model)"),U9o=l(),fT=a("li"),pse=a("strong"),J9o=o("xlnet"),Y9o=o(" \u2014 "),qj=a("a"),K9o=o("XLNetForQuestionAnsweringSimple"),Z9o=o(" (XLNet model)"),eBo=l(),gT=a("li"),_se=a("strong"),oBo=o("yoso"),rBo=o(" \u2014 "),Gj=a("a"),tBo=o("YosoForQuestionAnswering"),aBo=o(" (YOSO model)"),sBo=l(),hT=a("p"),nBo=o("The model is set in evaluation mode by default using "),bse=a("code"),lBo=o("model.eval()"),iBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vse=a("code"),dBo=o("model.train()"),cBo=l(),Tse=a("p"),mBo=o("Examples:"),fBo=l(),m(Jy.$$.fragment),I8e=l(),pd=a("h2"),uT=a("a"),Fse=a("span"),m(Yy.$$.fragment),gBo=l(),Cse=a("span"),hBo=o("AutoModelForTableQuestionAnswering"),j8e=l(),or=a("div"),m(Ky.$$.fragment),uBo=l(),_d=a("p"),pBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mse=a("code"),_Bo=o("from_pretrained()"),bBo=o("class method or the "),Ese=a("code"),vBo=o("from_config()"),TBo=o(`class
method.`),FBo=l(),Zy=a("p"),CBo=o("This class cannot be instantiated directly using "),yse=a("code"),MBo=o("__init__()"),EBo=o(" (throws an error)."),yBo=l(),Hr=a("div"),m(ew.$$.fragment),wBo=l(),wse=a("p"),ABo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),LBo=l(),bd=a("p"),BBo=o(`Note:
Loading a model from its configuration file does `),Ase=a("strong"),kBo=o("not"),xBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lse=a("code"),RBo=o("from_pretrained()"),SBo=o("to load the model weights."),PBo=l(),Bse=a("p"),$Bo=o("Examples:"),IBo=l(),m(ow.$$.fragment),jBo=l(),qe=a("div"),m(rw.$$.fragment),NBo=l(),kse=a("p"),DBo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),qBo=l(),Ua=a("p"),GBo=o("The model class to instantiate is selected based on the "),xse=a("code"),OBo=o("model_type"),XBo=o(` property of the config object (either
passed as an argument or loaded from `),Rse=a("code"),zBo=o("pretrained_model_name_or_path"),VBo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sse=a("code"),WBo=o("pretrained_model_name_or_path"),QBo=o(":"),HBo=l(),Pse=a("ul"),pT=a("li"),$se=a("strong"),UBo=o("tapas"),JBo=o(" \u2014 "),Oj=a("a"),YBo=o("TapasForQuestionAnswering"),KBo=o(" (TAPAS model)"),ZBo=l(),_T=a("p"),eko=o("The model is set in evaluation mode by default using "),Ise=a("code"),oko=o("model.eval()"),rko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jse=a("code"),tko=o("model.train()"),ako=l(),Nse=a("p"),sko=o("Examples:"),nko=l(),m(tw.$$.fragment),N8e=l(),vd=a("h2"),bT=a("a"),Dse=a("span"),m(aw.$$.fragment),lko=l(),qse=a("span"),iko=o("AutoModelForImageClassification"),D8e=l(),rr=a("div"),m(sw.$$.fragment),dko=l(),Td=a("p"),cko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gse=a("code"),mko=o("from_pretrained()"),fko=o("class method or the "),Ose=a("code"),gko=o("from_config()"),hko=o(`class
method.`),uko=l(),nw=a("p"),pko=o("This class cannot be instantiated directly using "),Xse=a("code"),_ko=o("__init__()"),bko=o(" (throws an error)."),vko=l(),Ur=a("div"),m(lw.$$.fragment),Tko=l(),zse=a("p"),Fko=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Cko=l(),Fd=a("p"),Mko=o(`Note:
Loading a model from its configuration file does `),Vse=a("strong"),Eko=o("not"),yko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=a("code"),wko=o("from_pretrained()"),Ako=o("to load the model weights."),Lko=l(),Qse=a("p"),Bko=o("Examples:"),kko=l(),m(iw.$$.fragment),xko=l(),Ge=a("div"),m(dw.$$.fragment),Rko=l(),Hse=a("p"),Sko=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Pko=l(),Ja=a("p"),$ko=o("The model class to instantiate is selected based on the "),Use=a("code"),Iko=o("model_type"),jko=o(` property of the config object (either
passed as an argument or loaded from `),Jse=a("code"),Nko=o("pretrained_model_name_or_path"),Dko=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yse=a("code"),qko=o("pretrained_model_name_or_path"),Gko=o(":"),Oko=l(),be=a("ul"),vT=a("li"),Kse=a("strong"),Xko=o("beit"),zko=o(" \u2014 "),Xj=a("a"),Vko=o("BeitForImageClassification"),Wko=o(" (BEiT model)"),Qko=l(),TT=a("li"),Zse=a("strong"),Hko=o("convnext"),Uko=o(" \u2014 "),zj=a("a"),Jko=o("ConvNextForImageClassification"),Yko=o(" (ConvNext model)"),Kko=l(),Rn=a("li"),ene=a("strong"),Zko=o("deit"),exo=o(" \u2014 "),Vj=a("a"),oxo=o("DeiTForImageClassification"),rxo=o(" or "),Wj=a("a"),txo=o("DeiTForImageClassificationWithTeacher"),axo=o(" (DeiT model)"),sxo=l(),FT=a("li"),one=a("strong"),nxo=o("imagegpt"),lxo=o(" \u2014 "),Qj=a("a"),ixo=o("ImageGPTForImageClassification"),dxo=o(" (ImageGPT model)"),cxo=l(),la=a("li"),rne=a("strong"),mxo=o("perceiver"),fxo=o(" \u2014 "),Hj=a("a"),gxo=o("PerceiverForImageClassificationLearned"),hxo=o(" or "),Uj=a("a"),uxo=o("PerceiverForImageClassificationFourier"),pxo=o(" or "),Jj=a("a"),_xo=o("PerceiverForImageClassificationConvProcessing"),bxo=o(" (Perceiver model)"),vxo=l(),CT=a("li"),tne=a("strong"),Txo=o("poolformer"),Fxo=o(" \u2014 "),Yj=a("a"),Cxo=o("PoolFormerForImageClassification"),Mxo=o(" (PoolFormer model)"),Exo=l(),MT=a("li"),ane=a("strong"),yxo=o("segformer"),wxo=o(" \u2014 "),Kj=a("a"),Axo=o("SegformerForImageClassification"),Lxo=o(" (SegFormer model)"),Bxo=l(),ET=a("li"),sne=a("strong"),kxo=o("swin"),xxo=o(" \u2014 "),Zj=a("a"),Rxo=o("SwinForImageClassification"),Sxo=o(" (Swin model)"),Pxo=l(),yT=a("li"),nne=a("strong"),$xo=o("vit"),Ixo=o(" \u2014 "),eN=a("a"),jxo=o("ViTForImageClassification"),Nxo=o(" (ViT model)"),Dxo=l(),wT=a("p"),qxo=o("The model is set in evaluation mode by default using "),lne=a("code"),Gxo=o("model.eval()"),Oxo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ine=a("code"),Xxo=o("model.train()"),zxo=l(),dne=a("p"),Vxo=o("Examples:"),Wxo=l(),m(cw.$$.fragment),q8e=l(),Cd=a("h2"),AT=a("a"),cne=a("span"),m(mw.$$.fragment),Qxo=l(),mne=a("span"),Hxo=o("AutoModelForVision2Seq"),G8e=l(),tr=a("div"),m(fw.$$.fragment),Uxo=l(),Md=a("p"),Jxo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),fne=a("code"),Yxo=o("from_pretrained()"),Kxo=o("class method or the "),gne=a("code"),Zxo=o("from_config()"),eRo=o(`class
method.`),oRo=l(),gw=a("p"),rRo=o("This class cannot be instantiated directly using "),hne=a("code"),tRo=o("__init__()"),aRo=o(" (throws an error)."),sRo=l(),Jr=a("div"),m(hw.$$.fragment),nRo=l(),une=a("p"),lRo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),iRo=l(),Ed=a("p"),dRo=o(`Note:
Loading a model from its configuration file does `),pne=a("strong"),cRo=o("not"),mRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ne=a("code"),fRo=o("from_pretrained()"),gRo=o("to load the model weights."),hRo=l(),bne=a("p"),uRo=o("Examples:"),pRo=l(),m(uw.$$.fragment),_Ro=l(),Oe=a("div"),m(pw.$$.fragment),bRo=l(),vne=a("p"),vRo=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),TRo=l(),Ya=a("p"),FRo=o("The model class to instantiate is selected based on the "),Tne=a("code"),CRo=o("model_type"),MRo=o(` property of the config object (either
passed as an argument or loaded from `),Fne=a("code"),ERo=o("pretrained_model_name_or_path"),yRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cne=a("code"),wRo=o("pretrained_model_name_or_path"),ARo=o(":"),LRo=l(),Mne=a("ul"),LT=a("li"),Ene=a("strong"),BRo=o("vision-encoder-decoder"),kRo=o(" \u2014 "),oN=a("a"),xRo=o("VisionEncoderDecoderModel"),RRo=o(" (Vision Encoder decoder model)"),SRo=l(),BT=a("p"),PRo=o("The model is set in evaluation mode by default using "),yne=a("code"),$Ro=o("model.eval()"),IRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wne=a("code"),jRo=o("model.train()"),NRo=l(),Ane=a("p"),DRo=o("Examples:"),qRo=l(),m(_w.$$.fragment),O8e=l(),yd=a("h2"),kT=a("a"),Lne=a("span"),m(bw.$$.fragment),GRo=l(),Bne=a("span"),ORo=o("AutoModelForAudioClassification"),X8e=l(),ar=a("div"),m(vw.$$.fragment),XRo=l(),wd=a("p"),zRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kne=a("code"),VRo=o("from_pretrained()"),WRo=o("class method or the "),xne=a("code"),QRo=o("from_config()"),HRo=o(`class
method.`),URo=l(),Tw=a("p"),JRo=o("This class cannot be instantiated directly using "),Rne=a("code"),YRo=o("__init__()"),KRo=o(" (throws an error)."),ZRo=l(),Yr=a("div"),m(Fw.$$.fragment),eSo=l(),Sne=a("p"),oSo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),rSo=l(),Ad=a("p"),tSo=o(`Note:
Loading a model from its configuration file does `),Pne=a("strong"),aSo=o("not"),sSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$ne=a("code"),nSo=o("from_pretrained()"),lSo=o("to load the model weights."),iSo=l(),Ine=a("p"),dSo=o("Examples:"),cSo=l(),m(Cw.$$.fragment),mSo=l(),Xe=a("div"),m(Mw.$$.fragment),fSo=l(),jne=a("p"),gSo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),hSo=l(),Ka=a("p"),uSo=o("The model class to instantiate is selected based on the "),Nne=a("code"),pSo=o("model_type"),_So=o(` property of the config object (either
passed as an argument or loaded from `),Dne=a("code"),bSo=o("pretrained_model_name_or_path"),vSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qne=a("code"),TSo=o("pretrained_model_name_or_path"),FSo=o(":"),CSo=l(),ao=a("ul"),xT=a("li"),Gne=a("strong"),MSo=o("hubert"),ESo=o(" \u2014 "),rN=a("a"),ySo=o("HubertForSequenceClassification"),wSo=o(" (Hubert model)"),ASo=l(),RT=a("li"),One=a("strong"),LSo=o("sew"),BSo=o(" \u2014 "),tN=a("a"),kSo=o("SEWForSequenceClassification"),xSo=o(" (SEW model)"),RSo=l(),ST=a("li"),Xne=a("strong"),SSo=o("sew-d"),PSo=o(" \u2014 "),aN=a("a"),$So=o("SEWDForSequenceClassification"),ISo=o(" (SEW-D model)"),jSo=l(),PT=a("li"),zne=a("strong"),NSo=o("unispeech"),DSo=o(" \u2014 "),sN=a("a"),qSo=o("UniSpeechForSequenceClassification"),GSo=o(" (UniSpeech model)"),OSo=l(),$T=a("li"),Vne=a("strong"),XSo=o("unispeech-sat"),zSo=o(" \u2014 "),nN=a("a"),VSo=o("UniSpeechSatForSequenceClassification"),WSo=o(" (UniSpeechSat model)"),QSo=l(),IT=a("li"),Wne=a("strong"),HSo=o("wav2vec2"),USo=o(" \u2014 "),lN=a("a"),JSo=o("Wav2Vec2ForSequenceClassification"),YSo=o(" (Wav2Vec2 model)"),KSo=l(),jT=a("li"),Qne=a("strong"),ZSo=o("wavlm"),ePo=o(" \u2014 "),iN=a("a"),oPo=o("WavLMForSequenceClassification"),rPo=o(" (WavLM model)"),tPo=l(),NT=a("p"),aPo=o("The model is set in evaluation mode by default using "),Hne=a("code"),sPo=o("model.eval()"),nPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Une=a("code"),lPo=o("model.train()"),iPo=l(),Jne=a("p"),dPo=o("Examples:"),cPo=l(),m(Ew.$$.fragment),z8e=l(),Ld=a("h2"),DT=a("a"),Yne=a("span"),m(yw.$$.fragment),mPo=l(),Kne=a("span"),fPo=o("AutoModelForAudioFrameClassification"),V8e=l(),sr=a("div"),m(ww.$$.fragment),gPo=l(),Bd=a("p"),hPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zne=a("code"),uPo=o("from_pretrained()"),pPo=o("class method or the "),ele=a("code"),_Po=o("from_config()"),bPo=o(`class
method.`),vPo=l(),Aw=a("p"),TPo=o("This class cannot be instantiated directly using "),ole=a("code"),FPo=o("__init__()"),CPo=o(" (throws an error)."),MPo=l(),Kr=a("div"),m(Lw.$$.fragment),EPo=l(),rle=a("p"),yPo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),wPo=l(),kd=a("p"),APo=o(`Note:
Loading a model from its configuration file does `),tle=a("strong"),LPo=o("not"),BPo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=a("code"),kPo=o("from_pretrained()"),xPo=o("to load the model weights."),RPo=l(),sle=a("p"),SPo=o("Examples:"),PPo=l(),m(Bw.$$.fragment),$Po=l(),ze=a("div"),m(kw.$$.fragment),IPo=l(),nle=a("p"),jPo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),NPo=l(),Za=a("p"),DPo=o("The model class to instantiate is selected based on the "),lle=a("code"),qPo=o("model_type"),GPo=o(` property of the config object (either
passed as an argument or loaded from `),ile=a("code"),OPo=o("pretrained_model_name_or_path"),XPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=a("code"),zPo=o("pretrained_model_name_or_path"),VPo=o(":"),WPo=l(),xd=a("ul"),qT=a("li"),cle=a("strong"),QPo=o("unispeech-sat"),HPo=o(" \u2014 "),dN=a("a"),UPo=o("UniSpeechSatForAudioFrameClassification"),JPo=o(" (UniSpeechSat model)"),YPo=l(),GT=a("li"),mle=a("strong"),KPo=o("wav2vec2"),ZPo=o(" \u2014 "),cN=a("a"),e$o=o("Wav2Vec2ForAudioFrameClassification"),o$o=o(" (Wav2Vec2 model)"),r$o=l(),OT=a("li"),fle=a("strong"),t$o=o("wavlm"),a$o=o(" \u2014 "),mN=a("a"),s$o=o("WavLMForAudioFrameClassification"),n$o=o(" (WavLM model)"),l$o=l(),XT=a("p"),i$o=o("The model is set in evaluation mode by default using "),gle=a("code"),d$o=o("model.eval()"),c$o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=a("code"),m$o=o("model.train()"),f$o=l(),ule=a("p"),g$o=o("Examples:"),h$o=l(),m(xw.$$.fragment),W8e=l(),Rd=a("h2"),zT=a("a"),ple=a("span"),m(Rw.$$.fragment),u$o=l(),_le=a("span"),p$o=o("AutoModelForCTC"),Q8e=l(),nr=a("div"),m(Sw.$$.fragment),_$o=l(),Sd=a("p"),b$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=a("code"),v$o=o("from_pretrained()"),T$o=o("class method or the "),vle=a("code"),F$o=o("from_config()"),C$o=o(`class
method.`),M$o=l(),Pw=a("p"),E$o=o("This class cannot be instantiated directly using "),Tle=a("code"),y$o=o("__init__()"),w$o=o(" (throws an error)."),A$o=l(),Zr=a("div"),m($w.$$.fragment),L$o=l(),Fle=a("p"),B$o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),k$o=l(),Pd=a("p"),x$o=o(`Note:
Loading a model from its configuration file does `),Cle=a("strong"),R$o=o("not"),S$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=a("code"),P$o=o("from_pretrained()"),$$o=o("to load the model weights."),I$o=l(),Ele=a("p"),j$o=o("Examples:"),N$o=l(),m(Iw.$$.fragment),D$o=l(),Ve=a("div"),m(jw.$$.fragment),q$o=l(),yle=a("p"),G$o=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),O$o=l(),es=a("p"),X$o=o("The model class to instantiate is selected based on the "),wle=a("code"),z$o=o("model_type"),V$o=o(` property of the config object (either
passed as an argument or loaded from `),Ale=a("code"),W$o=o("pretrained_model_name_or_path"),Q$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=a("code"),H$o=o("pretrained_model_name_or_path"),U$o=o(":"),J$o=l(),so=a("ul"),VT=a("li"),Ble=a("strong"),Y$o=o("hubert"),K$o=o(" \u2014 "),fN=a("a"),Z$o=o("HubertForCTC"),eIo=o(" (Hubert model)"),oIo=l(),WT=a("li"),kle=a("strong"),rIo=o("sew"),tIo=o(" \u2014 "),gN=a("a"),aIo=o("SEWForCTC"),sIo=o(" (SEW model)"),nIo=l(),QT=a("li"),xle=a("strong"),lIo=o("sew-d"),iIo=o(" \u2014 "),hN=a("a"),dIo=o("SEWDForCTC"),cIo=o(" (SEW-D model)"),mIo=l(),HT=a("li"),Rle=a("strong"),fIo=o("unispeech"),gIo=o(" \u2014 "),uN=a("a"),hIo=o("UniSpeechForCTC"),uIo=o(" (UniSpeech model)"),pIo=l(),UT=a("li"),Sle=a("strong"),_Io=o("unispeech-sat"),bIo=o(" \u2014 "),pN=a("a"),vIo=o("UniSpeechSatForCTC"),TIo=o(" (UniSpeechSat model)"),FIo=l(),JT=a("li"),Ple=a("strong"),CIo=o("wav2vec2"),MIo=o(" \u2014 "),_N=a("a"),EIo=o("Wav2Vec2ForCTC"),yIo=o(" (Wav2Vec2 model)"),wIo=l(),YT=a("li"),$le=a("strong"),AIo=o("wavlm"),LIo=o(" \u2014 "),bN=a("a"),BIo=o("WavLMForCTC"),kIo=o(" (WavLM model)"),xIo=l(),KT=a("p"),RIo=o("The model is set in evaluation mode by default using "),Ile=a("code"),SIo=o("model.eval()"),PIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=a("code"),$Io=o("model.train()"),IIo=l(),Nle=a("p"),jIo=o("Examples:"),NIo=l(),m(Nw.$$.fragment),H8e=l(),$d=a("h2"),ZT=a("a"),Dle=a("span"),m(Dw.$$.fragment),DIo=l(),qle=a("span"),qIo=o("AutoModelForSpeechSeq2Seq"),U8e=l(),lr=a("div"),m(qw.$$.fragment),GIo=l(),Id=a("p"),OIo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=a("code"),XIo=o("from_pretrained()"),zIo=o("class method or the "),Ole=a("code"),VIo=o("from_config()"),WIo=o(`class
method.`),QIo=l(),Gw=a("p"),HIo=o("This class cannot be instantiated directly using "),Xle=a("code"),UIo=o("__init__()"),JIo=o(" (throws an error)."),YIo=l(),et=a("div"),m(Ow.$$.fragment),KIo=l(),zle=a("p"),ZIo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),ejo=l(),jd=a("p"),ojo=o(`Note:
Loading a model from its configuration file does `),Vle=a("strong"),rjo=o("not"),tjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=a("code"),ajo=o("from_pretrained()"),sjo=o("to load the model weights."),njo=l(),Qle=a("p"),ljo=o("Examples:"),ijo=l(),m(Xw.$$.fragment),djo=l(),We=a("div"),m(zw.$$.fragment),cjo=l(),Hle=a("p"),mjo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),fjo=l(),os=a("p"),gjo=o("The model class to instantiate is selected based on the "),Ule=a("code"),hjo=o("model_type"),ujo=o(` property of the config object (either
passed as an argument or loaded from `),Jle=a("code"),pjo=o("pretrained_model_name_or_path"),_jo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=a("code"),bjo=o("pretrained_model_name_or_path"),vjo=o(":"),Tjo=l(),Vw=a("ul"),e1=a("li"),Kle=a("strong"),Fjo=o("speech-encoder-decoder"),Cjo=o(" \u2014 "),vN=a("a"),Mjo=o("SpeechEncoderDecoderModel"),Ejo=o(" (Speech Encoder decoder model)"),yjo=l(),o1=a("li"),Zle=a("strong"),wjo=o("speech_to_text"),Ajo=o(" \u2014 "),TN=a("a"),Ljo=o("Speech2TextForConditionalGeneration"),Bjo=o(" (Speech2Text model)"),kjo=l(),r1=a("p"),xjo=o("The model is set in evaluation mode by default using "),eie=a("code"),Rjo=o("model.eval()"),Sjo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=a("code"),Pjo=o("model.train()"),$jo=l(),rie=a("p"),Ijo=o("Examples:"),jjo=l(),m(Ww.$$.fragment),J8e=l(),Nd=a("h2"),t1=a("a"),tie=a("span"),m(Qw.$$.fragment),Njo=l(),aie=a("span"),Djo=o("AutoModelForAudioXVector"),Y8e=l(),ir=a("div"),m(Hw.$$.fragment),qjo=l(),Dd=a("p"),Gjo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),sie=a("code"),Ojo=o("from_pretrained()"),Xjo=o("class method or the "),nie=a("code"),zjo=o("from_config()"),Vjo=o(`class
method.`),Wjo=l(),Uw=a("p"),Qjo=o("This class cannot be instantiated directly using "),lie=a("code"),Hjo=o("__init__()"),Ujo=o(" (throws an error)."),Jjo=l(),ot=a("div"),m(Jw.$$.fragment),Yjo=l(),iie=a("p"),Kjo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),Zjo=l(),qd=a("p"),eNo=o(`Note:
Loading a model from its configuration file does `),die=a("strong"),oNo=o("not"),rNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=a("code"),tNo=o("from_pretrained()"),aNo=o("to load the model weights."),sNo=l(),mie=a("p"),nNo=o("Examples:"),lNo=l(),m(Yw.$$.fragment),iNo=l(),Qe=a("div"),m(Kw.$$.fragment),dNo=l(),fie=a("p"),cNo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),mNo=l(),rs=a("p"),fNo=o("The model class to instantiate is selected based on the "),gie=a("code"),gNo=o("model_type"),hNo=o(` property of the config object (either
passed as an argument or loaded from `),hie=a("code"),uNo=o("pretrained_model_name_or_path"),pNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),uie=a("code"),_No=o("pretrained_model_name_or_path"),bNo=o(":"),vNo=l(),Gd=a("ul"),a1=a("li"),pie=a("strong"),TNo=o("unispeech-sat"),FNo=o(" \u2014 "),FN=a("a"),CNo=o("UniSpeechSatForXVector"),MNo=o(" (UniSpeechSat model)"),ENo=l(),s1=a("li"),_ie=a("strong"),yNo=o("wav2vec2"),wNo=o(" \u2014 "),CN=a("a"),ANo=o("Wav2Vec2ForXVector"),LNo=o(" (Wav2Vec2 model)"),BNo=l(),n1=a("li"),bie=a("strong"),kNo=o("wavlm"),xNo=o(" \u2014 "),MN=a("a"),RNo=o("WavLMForXVector"),SNo=o(" (WavLM model)"),PNo=l(),l1=a("p"),$No=o("The model is set in evaluation mode by default using "),vie=a("code"),INo=o("model.eval()"),jNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=a("code"),NNo=o("model.train()"),DNo=l(),Fie=a("p"),qNo=o("Examples:"),GNo=l(),m(Zw.$$.fragment),K8e=l(),Od=a("h2"),i1=a("a"),Cie=a("span"),m(eA.$$.fragment),ONo=l(),Mie=a("span"),XNo=o("AutoModelForMaskedImageModeling"),Z8e=l(),dr=a("div"),m(oA.$$.fragment),zNo=l(),Xd=a("p"),VNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=a("code"),WNo=o("from_pretrained()"),QNo=o("class method or the "),yie=a("code"),HNo=o("from_config()"),UNo=o(`class
method.`),JNo=l(),rA=a("p"),YNo=o("This class cannot be instantiated directly using "),wie=a("code"),KNo=o("__init__()"),ZNo=o(" (throws an error)."),eDo=l(),rt=a("div"),m(tA.$$.fragment),oDo=l(),Aie=a("p"),rDo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),tDo=l(),zd=a("p"),aDo=o(`Note:
Loading a model from its configuration file does `),Lie=a("strong"),sDo=o("not"),nDo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=a("code"),lDo=o("from_pretrained()"),iDo=o("to load the model weights."),dDo=l(),kie=a("p"),cDo=o("Examples:"),mDo=l(),m(aA.$$.fragment),fDo=l(),He=a("div"),m(sA.$$.fragment),gDo=l(),xie=a("p"),hDo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),uDo=l(),ts=a("p"),pDo=o("The model class to instantiate is selected based on the "),Rie=a("code"),_Do=o("model_type"),bDo=o(` property of the config object (either
passed as an argument or loaded from `),Sie=a("code"),vDo=o("pretrained_model_name_or_path"),TDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=a("code"),FDo=o("pretrained_model_name_or_path"),CDo=o(":"),MDo=l(),Vd=a("ul"),d1=a("li"),$ie=a("strong"),EDo=o("deit"),yDo=o(" \u2014 "),EN=a("a"),wDo=o("DeiTForMaskedImageModeling"),ADo=o(" (DeiT model)"),LDo=l(),c1=a("li"),Iie=a("strong"),BDo=o("swin"),kDo=o(" \u2014 "),yN=a("a"),xDo=o("SwinForMaskedImageModeling"),RDo=o(" (Swin model)"),SDo=l(),m1=a("li"),jie=a("strong"),PDo=o("vit"),$Do=o(" \u2014 "),wN=a("a"),IDo=o("ViTForMaskedImageModeling"),jDo=o(" (ViT model)"),NDo=l(),f1=a("p"),DDo=o("The model is set in evaluation mode by default using "),Nie=a("code"),qDo=o("model.eval()"),GDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=a("code"),ODo=o("model.train()"),XDo=l(),qie=a("p"),zDo=o("Examples:"),VDo=l(),m(nA.$$.fragment),e9e=l(),Wd=a("h2"),g1=a("a"),Gie=a("span"),m(lA.$$.fragment),WDo=l(),Oie=a("span"),QDo=o("AutoModelForObjectDetection"),o9e=l(),cr=a("div"),m(iA.$$.fragment),HDo=l(),Qd=a("p"),UDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=a("code"),JDo=o("from_pretrained()"),YDo=o("class method or the "),zie=a("code"),KDo=o("from_config()"),ZDo=o(`class
method.`),eqo=l(),dA=a("p"),oqo=o("This class cannot be instantiated directly using "),Vie=a("code"),rqo=o("__init__()"),tqo=o(" (throws an error)."),aqo=l(),tt=a("div"),m(cA.$$.fragment),sqo=l(),Wie=a("p"),nqo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),lqo=l(),Hd=a("p"),iqo=o(`Note:
Loading a model from its configuration file does `),Qie=a("strong"),dqo=o("not"),cqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=a("code"),mqo=o("from_pretrained()"),fqo=o("to load the model weights."),gqo=l(),Uie=a("p"),hqo=o("Examples:"),uqo=l(),m(mA.$$.fragment),pqo=l(),Ue=a("div"),m(fA.$$.fragment),_qo=l(),Jie=a("p"),bqo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),vqo=l(),as=a("p"),Tqo=o("The model class to instantiate is selected based on the "),Yie=a("code"),Fqo=o("model_type"),Cqo=o(` property of the config object (either
passed as an argument or loaded from `),Kie=a("code"),Mqo=o("pretrained_model_name_or_path"),Eqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=a("code"),yqo=o("pretrained_model_name_or_path"),wqo=o(":"),Aqo=l(),ede=a("ul"),h1=a("li"),ode=a("strong"),Lqo=o("detr"),Bqo=o(" \u2014 "),AN=a("a"),kqo=o("DetrForObjectDetection"),xqo=o(" (DETR model)"),Rqo=l(),u1=a("p"),Sqo=o("The model is set in evaluation mode by default using "),rde=a("code"),Pqo=o("model.eval()"),$qo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=a("code"),Iqo=o("model.train()"),jqo=l(),ade=a("p"),Nqo=o("Examples:"),Dqo=l(),m(gA.$$.fragment),r9e=l(),Ud=a("h2"),p1=a("a"),sde=a("span"),m(hA.$$.fragment),qqo=l(),nde=a("span"),Gqo=o("AutoModelForImageSegmentation"),t9e=l(),mr=a("div"),m(uA.$$.fragment),Oqo=l(),Jd=a("p"),Xqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=a("code"),zqo=o("from_pretrained()"),Vqo=o("class method or the "),ide=a("code"),Wqo=o("from_config()"),Qqo=o(`class
method.`),Hqo=l(),pA=a("p"),Uqo=o("This class cannot be instantiated directly using "),dde=a("code"),Jqo=o("__init__()"),Yqo=o(" (throws an error)."),Kqo=l(),at=a("div"),m(_A.$$.fragment),Zqo=l(),cde=a("p"),eGo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),oGo=l(),Yd=a("p"),rGo=o(`Note:
Loading a model from its configuration file does `),mde=a("strong"),tGo=o("not"),aGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fde=a("code"),sGo=o("from_pretrained()"),nGo=o("to load the model weights."),lGo=l(),gde=a("p"),iGo=o("Examples:"),dGo=l(),m(bA.$$.fragment),cGo=l(),Je=a("div"),m(vA.$$.fragment),mGo=l(),hde=a("p"),fGo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gGo=l(),ss=a("p"),hGo=o("The model class to instantiate is selected based on the "),ude=a("code"),uGo=o("model_type"),pGo=o(` property of the config object (either
passed as an argument or loaded from `),pde=a("code"),_Go=o("pretrained_model_name_or_path"),bGo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_de=a("code"),vGo=o("pretrained_model_name_or_path"),TGo=o(":"),FGo=l(),bde=a("ul"),_1=a("li"),vde=a("strong"),CGo=o("detr"),MGo=o(" \u2014 "),LN=a("a"),EGo=o("DetrForSegmentation"),yGo=o(" (DETR model)"),wGo=l(),b1=a("p"),AGo=o("The model is set in evaluation mode by default using "),Tde=a("code"),LGo=o("model.eval()"),BGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=a("code"),kGo=o("model.train()"),xGo=l(),Cde=a("p"),RGo=o("Examples:"),SGo=l(),m(TA.$$.fragment),a9e=l(),Kd=a("h2"),v1=a("a"),Mde=a("span"),m(FA.$$.fragment),PGo=l(),Ede=a("span"),$Go=o("AutoModelForSemanticSegmentation"),s9e=l(),fr=a("div"),m(CA.$$.fragment),IGo=l(),Zd=a("p"),jGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=a("code"),NGo=o("from_pretrained()"),DGo=o("class method or the "),wde=a("code"),qGo=o("from_config()"),GGo=o(`class
method.`),OGo=l(),MA=a("p"),XGo=o("This class cannot be instantiated directly using "),Ade=a("code"),zGo=o("__init__()"),VGo=o(" (throws an error)."),WGo=l(),st=a("div"),m(EA.$$.fragment),QGo=l(),Lde=a("p"),HGo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),UGo=l(),ec=a("p"),JGo=o(`Note:
Loading a model from its configuration file does `),Bde=a("strong"),YGo=o("not"),KGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=a("code"),ZGo=o("from_pretrained()"),eOo=o("to load the model weights."),oOo=l(),xde=a("p"),rOo=o("Examples:"),tOo=l(),m(yA.$$.fragment),aOo=l(),Ye=a("div"),m(wA.$$.fragment),sOo=l(),Rde=a("p"),nOo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),lOo=l(),ns=a("p"),iOo=o("The model class to instantiate is selected based on the "),Sde=a("code"),dOo=o("model_type"),cOo=o(` property of the config object (either
passed as an argument or loaded from `),Pde=a("code"),mOo=o("pretrained_model_name_or_path"),fOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=a("code"),gOo=o("pretrained_model_name_or_path"),hOo=o(":"),uOo=l(),AA=a("ul"),T1=a("li"),Ide=a("strong"),pOo=o("beit"),_Oo=o(" \u2014 "),BN=a("a"),bOo=o("BeitForSemanticSegmentation"),vOo=o(" (BEiT model)"),TOo=l(),F1=a("li"),jde=a("strong"),FOo=o("segformer"),COo=o(" \u2014 "),kN=a("a"),MOo=o("SegformerForSemanticSegmentation"),EOo=o(" (SegFormer model)"),yOo=l(),C1=a("p"),wOo=o("The model is set in evaluation mode by default using "),Nde=a("code"),AOo=o("model.eval()"),LOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=a("code"),BOo=o("model.train()"),kOo=l(),qde=a("p"),xOo=o("Examples:"),ROo=l(),m(LA.$$.fragment),n9e=l(),oc=a("h2"),M1=a("a"),Gde=a("span"),m(BA.$$.fragment),SOo=l(),Ode=a("span"),POo=o("TFAutoModel"),l9e=l(),gr=a("div"),m(kA.$$.fragment),$Oo=l(),rc=a("p"),IOo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=a("code"),jOo=o("from_pretrained()"),NOo=o("class method or the "),zde=a("code"),DOo=o("from_config()"),qOo=o(`class
method.`),GOo=l(),xA=a("p"),OOo=o("This class cannot be instantiated directly using "),Vde=a("code"),XOo=o("__init__()"),zOo=o(" (throws an error)."),VOo=l(),nt=a("div"),m(RA.$$.fragment),WOo=l(),Wde=a("p"),QOo=o("Instantiates one of the base model classes of the library from a configuration."),HOo=l(),tc=a("p"),UOo=o(`Note:
Loading a model from its configuration file does `),Qde=a("strong"),JOo=o("not"),YOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=a("code"),KOo=o("from_pretrained()"),ZOo=o("to load the model weights."),eXo=l(),Ude=a("p"),oXo=o("Examples:"),rXo=l(),m(SA.$$.fragment),tXo=l(),go=a("div"),m(PA.$$.fragment),aXo=l(),Jde=a("p"),sXo=o("Instantiate one of the base model classes of the library from a pretrained model."),nXo=l(),ls=a("p"),lXo=o("The model class to instantiate is selected based on the "),Yde=a("code"),iXo=o("model_type"),dXo=o(` property of the config object (either
passed as an argument or loaded from `),Kde=a("code"),cXo=o("pretrained_model_name_or_path"),mXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=a("code"),fXo=o("pretrained_model_name_or_path"),gXo=o(":"),hXo=l(),B=a("ul"),E1=a("li"),ece=a("strong"),uXo=o("albert"),pXo=o(" \u2014 "),xN=a("a"),_Xo=o("TFAlbertModel"),bXo=o(" (ALBERT model)"),vXo=l(),y1=a("li"),oce=a("strong"),TXo=o("bart"),FXo=o(" \u2014 "),RN=a("a"),CXo=o("TFBartModel"),MXo=o(" (BART model)"),EXo=l(),w1=a("li"),rce=a("strong"),yXo=o("bert"),wXo=o(" \u2014 "),SN=a("a"),AXo=o("TFBertModel"),LXo=o(" (BERT model)"),BXo=l(),A1=a("li"),tce=a("strong"),kXo=o("blenderbot"),xXo=o(" \u2014 "),PN=a("a"),RXo=o("TFBlenderbotModel"),SXo=o(" (Blenderbot model)"),PXo=l(),L1=a("li"),ace=a("strong"),$Xo=o("blenderbot-small"),IXo=o(" \u2014 "),$N=a("a"),jXo=o("TFBlenderbotSmallModel"),NXo=o(" (BlenderbotSmall model)"),DXo=l(),B1=a("li"),sce=a("strong"),qXo=o("camembert"),GXo=o(" \u2014 "),IN=a("a"),OXo=o("TFCamembertModel"),XXo=o(" (CamemBERT model)"),zXo=l(),k1=a("li"),nce=a("strong"),VXo=o("clip"),WXo=o(" \u2014 "),jN=a("a"),QXo=o("TFCLIPModel"),HXo=o(" (CLIP model)"),UXo=l(),x1=a("li"),lce=a("strong"),JXo=o("convbert"),YXo=o(" \u2014 "),NN=a("a"),KXo=o("TFConvBertModel"),ZXo=o(" (ConvBERT model)"),ezo=l(),R1=a("li"),ice=a("strong"),ozo=o("ctrl"),rzo=o(" \u2014 "),DN=a("a"),tzo=o("TFCTRLModel"),azo=o(" (CTRL model)"),szo=l(),S1=a("li"),dce=a("strong"),nzo=o("deberta"),lzo=o(" \u2014 "),qN=a("a"),izo=o("TFDebertaModel"),dzo=o(" (DeBERTa model)"),czo=l(),P1=a("li"),cce=a("strong"),mzo=o("deberta-v2"),fzo=o(" \u2014 "),GN=a("a"),gzo=o("TFDebertaV2Model"),hzo=o(" (DeBERTa-v2 model)"),uzo=l(),$1=a("li"),mce=a("strong"),pzo=o("distilbert"),_zo=o(" \u2014 "),ON=a("a"),bzo=o("TFDistilBertModel"),vzo=o(" (DistilBERT model)"),Tzo=l(),I1=a("li"),fce=a("strong"),Fzo=o("dpr"),Czo=o(" \u2014 "),XN=a("a"),Mzo=o("TFDPRQuestionEncoder"),Ezo=o(" (DPR model)"),yzo=l(),j1=a("li"),gce=a("strong"),wzo=o("electra"),Azo=o(" \u2014 "),zN=a("a"),Lzo=o("TFElectraModel"),Bzo=o(" (ELECTRA model)"),kzo=l(),N1=a("li"),hce=a("strong"),xzo=o("flaubert"),Rzo=o(" \u2014 "),VN=a("a"),Szo=o("TFFlaubertModel"),Pzo=o(" (FlauBERT model)"),$zo=l(),Sn=a("li"),uce=a("strong"),Izo=o("funnel"),jzo=o(" \u2014 "),WN=a("a"),Nzo=o("TFFunnelModel"),Dzo=o(" or "),QN=a("a"),qzo=o("TFFunnelBaseModel"),Gzo=o(" (Funnel Transformer model)"),Ozo=l(),D1=a("li"),pce=a("strong"),Xzo=o("gpt2"),zzo=o(" \u2014 "),HN=a("a"),Vzo=o("TFGPT2Model"),Wzo=o(" (OpenAI GPT-2 model)"),Qzo=l(),q1=a("li"),_ce=a("strong"),Hzo=o("hubert"),Uzo=o(" \u2014 "),UN=a("a"),Jzo=o("TFHubertModel"),Yzo=o(" (Hubert model)"),Kzo=l(),G1=a("li"),bce=a("strong"),Zzo=o("layoutlm"),eVo=o(" \u2014 "),JN=a("a"),oVo=o("TFLayoutLMModel"),rVo=o(" (LayoutLM model)"),tVo=l(),O1=a("li"),vce=a("strong"),aVo=o("led"),sVo=o(" \u2014 "),YN=a("a"),nVo=o("TFLEDModel"),lVo=o(" (LED model)"),iVo=l(),X1=a("li"),Tce=a("strong"),dVo=o("longformer"),cVo=o(" \u2014 "),KN=a("a"),mVo=o("TFLongformerModel"),fVo=o(" (Longformer model)"),gVo=l(),z1=a("li"),Fce=a("strong"),hVo=o("lxmert"),uVo=o(" \u2014 "),ZN=a("a"),pVo=o("TFLxmertModel"),_Vo=o(" (LXMERT model)"),bVo=l(),V1=a("li"),Cce=a("strong"),vVo=o("marian"),TVo=o(" \u2014 "),eD=a("a"),FVo=o("TFMarianModel"),CVo=o(" (Marian model)"),MVo=l(),W1=a("li"),Mce=a("strong"),EVo=o("mbart"),yVo=o(" \u2014 "),oD=a("a"),wVo=o("TFMBartModel"),AVo=o(" (mBART model)"),LVo=l(),Q1=a("li"),Ece=a("strong"),BVo=o("mobilebert"),kVo=o(" \u2014 "),rD=a("a"),xVo=o("TFMobileBertModel"),RVo=o(" (MobileBERT model)"),SVo=l(),H1=a("li"),yce=a("strong"),PVo=o("mpnet"),$Vo=o(" \u2014 "),tD=a("a"),IVo=o("TFMPNetModel"),jVo=o(" (MPNet model)"),NVo=l(),U1=a("li"),wce=a("strong"),DVo=o("mt5"),qVo=o(" \u2014 "),aD=a("a"),GVo=o("TFMT5Model"),OVo=o(" (mT5 model)"),XVo=l(),J1=a("li"),Ace=a("strong"),zVo=o("openai-gpt"),VVo=o(" \u2014 "),sD=a("a"),WVo=o("TFOpenAIGPTModel"),QVo=o(" (OpenAI GPT model)"),HVo=l(),Y1=a("li"),Lce=a("strong"),UVo=o("pegasus"),JVo=o(" \u2014 "),nD=a("a"),YVo=o("TFPegasusModel"),KVo=o(" (Pegasus model)"),ZVo=l(),K1=a("li"),Bce=a("strong"),eWo=o("rembert"),oWo=o(" \u2014 "),lD=a("a"),rWo=o("TFRemBertModel"),tWo=o(" (RemBERT model)"),aWo=l(),Z1=a("li"),kce=a("strong"),sWo=o("roberta"),nWo=o(" \u2014 "),iD=a("a"),lWo=o("TFRobertaModel"),iWo=o(" (RoBERTa model)"),dWo=l(),eF=a("li"),xce=a("strong"),cWo=o("roformer"),mWo=o(" \u2014 "),dD=a("a"),fWo=o("TFRoFormerModel"),gWo=o(" (RoFormer model)"),hWo=l(),oF=a("li"),Rce=a("strong"),uWo=o("speech_to_text"),pWo=o(" \u2014 "),cD=a("a"),_Wo=o("TFSpeech2TextModel"),bWo=o(" (Speech2Text model)"),vWo=l(),rF=a("li"),Sce=a("strong"),TWo=o("t5"),FWo=o(" \u2014 "),mD=a("a"),CWo=o("TFT5Model"),MWo=o(" (T5 model)"),EWo=l(),tF=a("li"),Pce=a("strong"),yWo=o("tapas"),wWo=o(" \u2014 "),fD=a("a"),AWo=o("TFTapasModel"),LWo=o(" (TAPAS model)"),BWo=l(),aF=a("li"),$ce=a("strong"),kWo=o("transfo-xl"),xWo=o(" \u2014 "),gD=a("a"),RWo=o("TFTransfoXLModel"),SWo=o(" (Transformer-XL model)"),PWo=l(),sF=a("li"),Ice=a("strong"),$Wo=o("vit"),IWo=o(" \u2014 "),hD=a("a"),jWo=o("TFViTModel"),NWo=o(" (ViT model)"),DWo=l(),nF=a("li"),jce=a("strong"),qWo=o("wav2vec2"),GWo=o(" \u2014 "),uD=a("a"),OWo=o("TFWav2Vec2Model"),XWo=o(" (Wav2Vec2 model)"),zWo=l(),lF=a("li"),Nce=a("strong"),VWo=o("xlm"),WWo=o(" \u2014 "),pD=a("a"),QWo=o("TFXLMModel"),HWo=o(" (XLM model)"),UWo=l(),iF=a("li"),Dce=a("strong"),JWo=o("xlm-roberta"),YWo=o(" \u2014 "),_D=a("a"),KWo=o("TFXLMRobertaModel"),ZWo=o(" (XLM-RoBERTa model)"),eQo=l(),dF=a("li"),qce=a("strong"),oQo=o("xlnet"),rQo=o(" \u2014 "),bD=a("a"),tQo=o("TFXLNetModel"),aQo=o(" (XLNet model)"),sQo=l(),Gce=a("p"),nQo=o("Examples:"),lQo=l(),m($A.$$.fragment),i9e=l(),ac=a("h2"),cF=a("a"),Oce=a("span"),m(IA.$$.fragment),iQo=l(),Xce=a("span"),dQo=o("TFAutoModelForPreTraining"),d9e=l(),hr=a("div"),m(jA.$$.fragment),cQo=l(),sc=a("p"),mQo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=a("code"),fQo=o("from_pretrained()"),gQo=o("class method or the "),Vce=a("code"),hQo=o("from_config()"),uQo=o(`class
method.`),pQo=l(),NA=a("p"),_Qo=o("This class cannot be instantiated directly using "),Wce=a("code"),bQo=o("__init__()"),vQo=o(" (throws an error)."),TQo=l(),lt=a("div"),m(DA.$$.fragment),FQo=l(),Qce=a("p"),CQo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),MQo=l(),nc=a("p"),EQo=o(`Note:
Loading a model from its configuration file does `),Hce=a("strong"),yQo=o("not"),wQo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=a("code"),AQo=o("from_pretrained()"),LQo=o("to load the model weights."),BQo=l(),Jce=a("p"),kQo=o("Examples:"),xQo=l(),m(qA.$$.fragment),RQo=l(),ho=a("div"),m(GA.$$.fragment),SQo=l(),Yce=a("p"),PQo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),$Qo=l(),is=a("p"),IQo=o("The model class to instantiate is selected based on the "),Kce=a("code"),jQo=o("model_type"),NQo=o(` property of the config object (either
passed as an argument or loaded from `),Zce=a("code"),DQo=o("pretrained_model_name_or_path"),qQo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eme=a("code"),GQo=o("pretrained_model_name_or_path"),OQo=o(":"),XQo=l(),H=a("ul"),mF=a("li"),ome=a("strong"),zQo=o("albert"),VQo=o(" \u2014 "),vD=a("a"),WQo=o("TFAlbertForPreTraining"),QQo=o(" (ALBERT model)"),HQo=l(),fF=a("li"),rme=a("strong"),UQo=o("bart"),JQo=o(" \u2014 "),TD=a("a"),YQo=o("TFBartForConditionalGeneration"),KQo=o(" (BART model)"),ZQo=l(),gF=a("li"),tme=a("strong"),eHo=o("bert"),oHo=o(" \u2014 "),FD=a("a"),rHo=o("TFBertForPreTraining"),tHo=o(" (BERT model)"),aHo=l(),hF=a("li"),ame=a("strong"),sHo=o("camembert"),nHo=o(" \u2014 "),CD=a("a"),lHo=o("TFCamembertForMaskedLM"),iHo=o(" (CamemBERT model)"),dHo=l(),uF=a("li"),sme=a("strong"),cHo=o("ctrl"),mHo=o(" \u2014 "),MD=a("a"),fHo=o("TFCTRLLMHeadModel"),gHo=o(" (CTRL model)"),hHo=l(),pF=a("li"),nme=a("strong"),uHo=o("distilbert"),pHo=o(" \u2014 "),ED=a("a"),_Ho=o("TFDistilBertForMaskedLM"),bHo=o(" (DistilBERT model)"),vHo=l(),_F=a("li"),lme=a("strong"),THo=o("electra"),FHo=o(" \u2014 "),yD=a("a"),CHo=o("TFElectraForPreTraining"),MHo=o(" (ELECTRA model)"),EHo=l(),bF=a("li"),ime=a("strong"),yHo=o("flaubert"),wHo=o(" \u2014 "),wD=a("a"),AHo=o("TFFlaubertWithLMHeadModel"),LHo=o(" (FlauBERT model)"),BHo=l(),vF=a("li"),dme=a("strong"),kHo=o("funnel"),xHo=o(" \u2014 "),AD=a("a"),RHo=o("TFFunnelForPreTraining"),SHo=o(" (Funnel Transformer model)"),PHo=l(),TF=a("li"),cme=a("strong"),$Ho=o("gpt2"),IHo=o(" \u2014 "),LD=a("a"),jHo=o("TFGPT2LMHeadModel"),NHo=o(" (OpenAI GPT-2 model)"),DHo=l(),FF=a("li"),mme=a("strong"),qHo=o("layoutlm"),GHo=o(" \u2014 "),BD=a("a"),OHo=o("TFLayoutLMForMaskedLM"),XHo=o(" (LayoutLM model)"),zHo=l(),CF=a("li"),fme=a("strong"),VHo=o("lxmert"),WHo=o(" \u2014 "),kD=a("a"),QHo=o("TFLxmertForPreTraining"),HHo=o(" (LXMERT model)"),UHo=l(),MF=a("li"),gme=a("strong"),JHo=o("mobilebert"),YHo=o(" \u2014 "),xD=a("a"),KHo=o("TFMobileBertForPreTraining"),ZHo=o(" (MobileBERT model)"),eUo=l(),EF=a("li"),hme=a("strong"),oUo=o("mpnet"),rUo=o(" \u2014 "),RD=a("a"),tUo=o("TFMPNetForMaskedLM"),aUo=o(" (MPNet model)"),sUo=l(),yF=a("li"),ume=a("strong"),nUo=o("openai-gpt"),lUo=o(" \u2014 "),SD=a("a"),iUo=o("TFOpenAIGPTLMHeadModel"),dUo=o(" (OpenAI GPT model)"),cUo=l(),wF=a("li"),pme=a("strong"),mUo=o("roberta"),fUo=o(" \u2014 "),PD=a("a"),gUo=o("TFRobertaForMaskedLM"),hUo=o(" (RoBERTa model)"),uUo=l(),AF=a("li"),_me=a("strong"),pUo=o("t5"),_Uo=o(" \u2014 "),$D=a("a"),bUo=o("TFT5ForConditionalGeneration"),vUo=o(" (T5 model)"),TUo=l(),LF=a("li"),bme=a("strong"),FUo=o("tapas"),CUo=o(" \u2014 "),ID=a("a"),MUo=o("TFTapasForMaskedLM"),EUo=o(" (TAPAS model)"),yUo=l(),BF=a("li"),vme=a("strong"),wUo=o("transfo-xl"),AUo=o(" \u2014 "),jD=a("a"),LUo=o("TFTransfoXLLMHeadModel"),BUo=o(" (Transformer-XL model)"),kUo=l(),kF=a("li"),Tme=a("strong"),xUo=o("xlm"),RUo=o(" \u2014 "),ND=a("a"),SUo=o("TFXLMWithLMHeadModel"),PUo=o(" (XLM model)"),$Uo=l(),xF=a("li"),Fme=a("strong"),IUo=o("xlm-roberta"),jUo=o(" \u2014 "),DD=a("a"),NUo=o("TFXLMRobertaForMaskedLM"),DUo=o(" (XLM-RoBERTa model)"),qUo=l(),RF=a("li"),Cme=a("strong"),GUo=o("xlnet"),OUo=o(" \u2014 "),qD=a("a"),XUo=o("TFXLNetLMHeadModel"),zUo=o(" (XLNet model)"),VUo=l(),Mme=a("p"),WUo=o("Examples:"),QUo=l(),m(OA.$$.fragment),c9e=l(),lc=a("h2"),SF=a("a"),Eme=a("span"),m(XA.$$.fragment),HUo=l(),yme=a("span"),UUo=o("TFAutoModelForCausalLM"),m9e=l(),ur=a("div"),m(zA.$$.fragment),JUo=l(),ic=a("p"),YUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wme=a("code"),KUo=o("from_pretrained()"),ZUo=o("class method or the "),Ame=a("code"),eJo=o("from_config()"),oJo=o(`class
method.`),rJo=l(),VA=a("p"),tJo=o("This class cannot be instantiated directly using "),Lme=a("code"),aJo=o("__init__()"),sJo=o(" (throws an error)."),nJo=l(),it=a("div"),m(WA.$$.fragment),lJo=l(),Bme=a("p"),iJo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),dJo=l(),dc=a("p"),cJo=o(`Note:
Loading a model from its configuration file does `),kme=a("strong"),mJo=o("not"),fJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xme=a("code"),gJo=o("from_pretrained()"),hJo=o("to load the model weights."),uJo=l(),Rme=a("p"),pJo=o("Examples:"),_Jo=l(),m(QA.$$.fragment),bJo=l(),uo=a("div"),m(HA.$$.fragment),vJo=l(),Sme=a("p"),TJo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),FJo=l(),ds=a("p"),CJo=o("The model class to instantiate is selected based on the "),Pme=a("code"),MJo=o("model_type"),EJo=o(` property of the config object (either
passed as an argument or loaded from `),$me=a("code"),yJo=o("pretrained_model_name_or_path"),wJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ime=a("code"),AJo=o("pretrained_model_name_or_path"),LJo=o(":"),BJo=l(),he=a("ul"),PF=a("li"),jme=a("strong"),kJo=o("bert"),xJo=o(" \u2014 "),GD=a("a"),RJo=o("TFBertLMHeadModel"),SJo=o(" (BERT model)"),PJo=l(),$F=a("li"),Nme=a("strong"),$Jo=o("ctrl"),IJo=o(" \u2014 "),OD=a("a"),jJo=o("TFCTRLLMHeadModel"),NJo=o(" (CTRL model)"),DJo=l(),IF=a("li"),Dme=a("strong"),qJo=o("gpt2"),GJo=o(" \u2014 "),XD=a("a"),OJo=o("TFGPT2LMHeadModel"),XJo=o(" (OpenAI GPT-2 model)"),zJo=l(),jF=a("li"),qme=a("strong"),VJo=o("openai-gpt"),WJo=o(" \u2014 "),zD=a("a"),QJo=o("TFOpenAIGPTLMHeadModel"),HJo=o(" (OpenAI GPT model)"),UJo=l(),NF=a("li"),Gme=a("strong"),JJo=o("rembert"),YJo=o(" \u2014 "),VD=a("a"),KJo=o("TFRemBertForCausalLM"),ZJo=o(" (RemBERT model)"),eYo=l(),DF=a("li"),Ome=a("strong"),oYo=o("roberta"),rYo=o(" \u2014 "),WD=a("a"),tYo=o("TFRobertaForCausalLM"),aYo=o(" (RoBERTa model)"),sYo=l(),qF=a("li"),Xme=a("strong"),nYo=o("roformer"),lYo=o(" \u2014 "),QD=a("a"),iYo=o("TFRoFormerForCausalLM"),dYo=o(" (RoFormer model)"),cYo=l(),GF=a("li"),zme=a("strong"),mYo=o("transfo-xl"),fYo=o(" \u2014 "),HD=a("a"),gYo=o("TFTransfoXLLMHeadModel"),hYo=o(" (Transformer-XL model)"),uYo=l(),OF=a("li"),Vme=a("strong"),pYo=o("xlm"),_Yo=o(" \u2014 "),UD=a("a"),bYo=o("TFXLMWithLMHeadModel"),vYo=o(" (XLM model)"),TYo=l(),XF=a("li"),Wme=a("strong"),FYo=o("xlnet"),CYo=o(" \u2014 "),JD=a("a"),MYo=o("TFXLNetLMHeadModel"),EYo=o(" (XLNet model)"),yYo=l(),Qme=a("p"),wYo=o("Examples:"),AYo=l(),m(UA.$$.fragment),f9e=l(),cc=a("h2"),zF=a("a"),Hme=a("span"),m(JA.$$.fragment),LYo=l(),Ume=a("span"),BYo=o("TFAutoModelForImageClassification"),g9e=l(),pr=a("div"),m(YA.$$.fragment),kYo=l(),mc=a("p"),xYo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jme=a("code"),RYo=o("from_pretrained()"),SYo=o("class method or the "),Yme=a("code"),PYo=o("from_config()"),$Yo=o(`class
method.`),IYo=l(),KA=a("p"),jYo=o("This class cannot be instantiated directly using "),Kme=a("code"),NYo=o("__init__()"),DYo=o(" (throws an error)."),qYo=l(),dt=a("div"),m(ZA.$$.fragment),GYo=l(),Zme=a("p"),OYo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),XYo=l(),fc=a("p"),zYo=o(`Note:
Loading a model from its configuration file does `),efe=a("strong"),VYo=o("not"),WYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ofe=a("code"),QYo=o("from_pretrained()"),HYo=o("to load the model weights."),UYo=l(),rfe=a("p"),JYo=o("Examples:"),YYo=l(),m(e6.$$.fragment),KYo=l(),po=a("div"),m(o6.$$.fragment),ZYo=l(),tfe=a("p"),eKo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),oKo=l(),cs=a("p"),rKo=o("The model class to instantiate is selected based on the "),afe=a("code"),tKo=o("model_type"),aKo=o(` property of the config object (either
passed as an argument or loaded from `),sfe=a("code"),sKo=o("pretrained_model_name_or_path"),nKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nfe=a("code"),lKo=o("pretrained_model_name_or_path"),iKo=o(":"),dKo=l(),lfe=a("ul"),VF=a("li"),ife=a("strong"),cKo=o("vit"),mKo=o(" \u2014 "),YD=a("a"),fKo=o("TFViTForImageClassification"),gKo=o(" (ViT model)"),hKo=l(),dfe=a("p"),uKo=o("Examples:"),pKo=l(),m(r6.$$.fragment),h9e=l(),gc=a("h2"),WF=a("a"),cfe=a("span"),m(t6.$$.fragment),_Ko=l(),mfe=a("span"),bKo=o("TFAutoModelForMaskedLM"),u9e=l(),_r=a("div"),m(a6.$$.fragment),vKo=l(),hc=a("p"),TKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),ffe=a("code"),FKo=o("from_pretrained()"),CKo=o("class method or the "),gfe=a("code"),MKo=o("from_config()"),EKo=o(`class
method.`),yKo=l(),s6=a("p"),wKo=o("This class cannot be instantiated directly using "),hfe=a("code"),AKo=o("__init__()"),LKo=o(" (throws an error)."),BKo=l(),ct=a("div"),m(n6.$$.fragment),kKo=l(),ufe=a("p"),xKo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),RKo=l(),uc=a("p"),SKo=o(`Note:
Loading a model from its configuration file does `),pfe=a("strong"),PKo=o("not"),$Ko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_fe=a("code"),IKo=o("from_pretrained()"),jKo=o("to load the model weights."),NKo=l(),bfe=a("p"),DKo=o("Examples:"),qKo=l(),m(l6.$$.fragment),GKo=l(),_o=a("div"),m(i6.$$.fragment),OKo=l(),vfe=a("p"),XKo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),zKo=l(),ms=a("p"),VKo=o("The model class to instantiate is selected based on the "),Tfe=a("code"),WKo=o("model_type"),QKo=o(` property of the config object (either
passed as an argument or loaded from `),Ffe=a("code"),HKo=o("pretrained_model_name_or_path"),UKo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cfe=a("code"),JKo=o("pretrained_model_name_or_path"),YKo=o(":"),KKo=l(),Y=a("ul"),QF=a("li"),Mfe=a("strong"),ZKo=o("albert"),eZo=o(" \u2014 "),KD=a("a"),oZo=o("TFAlbertForMaskedLM"),rZo=o(" (ALBERT model)"),tZo=l(),HF=a("li"),Efe=a("strong"),aZo=o("bert"),sZo=o(" \u2014 "),ZD=a("a"),nZo=o("TFBertForMaskedLM"),lZo=o(" (BERT model)"),iZo=l(),UF=a("li"),yfe=a("strong"),dZo=o("camembert"),cZo=o(" \u2014 "),eq=a("a"),mZo=o("TFCamembertForMaskedLM"),fZo=o(" (CamemBERT model)"),gZo=l(),JF=a("li"),wfe=a("strong"),hZo=o("convbert"),uZo=o(" \u2014 "),oq=a("a"),pZo=o("TFConvBertForMaskedLM"),_Zo=o(" (ConvBERT model)"),bZo=l(),YF=a("li"),Afe=a("strong"),vZo=o("deberta"),TZo=o(" \u2014 "),rq=a("a"),FZo=o("TFDebertaForMaskedLM"),CZo=o(" (DeBERTa model)"),MZo=l(),KF=a("li"),Lfe=a("strong"),EZo=o("deberta-v2"),yZo=o(" \u2014 "),tq=a("a"),wZo=o("TFDebertaV2ForMaskedLM"),AZo=o(" (DeBERTa-v2 model)"),LZo=l(),ZF=a("li"),Bfe=a("strong"),BZo=o("distilbert"),kZo=o(" \u2014 "),aq=a("a"),xZo=o("TFDistilBertForMaskedLM"),RZo=o(" (DistilBERT model)"),SZo=l(),eC=a("li"),kfe=a("strong"),PZo=o("electra"),$Zo=o(" \u2014 "),sq=a("a"),IZo=o("TFElectraForMaskedLM"),jZo=o(" (ELECTRA model)"),NZo=l(),oC=a("li"),xfe=a("strong"),DZo=o("flaubert"),qZo=o(" \u2014 "),nq=a("a"),GZo=o("TFFlaubertWithLMHeadModel"),OZo=o(" (FlauBERT model)"),XZo=l(),rC=a("li"),Rfe=a("strong"),zZo=o("funnel"),VZo=o(" \u2014 "),lq=a("a"),WZo=o("TFFunnelForMaskedLM"),QZo=o(" (Funnel Transformer model)"),HZo=l(),tC=a("li"),Sfe=a("strong"),UZo=o("layoutlm"),JZo=o(" \u2014 "),iq=a("a"),YZo=o("TFLayoutLMForMaskedLM"),KZo=o(" (LayoutLM model)"),ZZo=l(),aC=a("li"),Pfe=a("strong"),eer=o("longformer"),oer=o(" \u2014 "),dq=a("a"),rer=o("TFLongformerForMaskedLM"),ter=o(" (Longformer model)"),aer=l(),sC=a("li"),$fe=a("strong"),ser=o("mobilebert"),ner=o(" \u2014 "),cq=a("a"),ler=o("TFMobileBertForMaskedLM"),ier=o(" (MobileBERT model)"),der=l(),nC=a("li"),Ife=a("strong"),cer=o("mpnet"),mer=o(" \u2014 "),mq=a("a"),fer=o("TFMPNetForMaskedLM"),ger=o(" (MPNet model)"),her=l(),lC=a("li"),jfe=a("strong"),uer=o("rembert"),per=o(" \u2014 "),fq=a("a"),_er=o("TFRemBertForMaskedLM"),ber=o(" (RemBERT model)"),ver=l(),iC=a("li"),Nfe=a("strong"),Ter=o("roberta"),Fer=o(" \u2014 "),gq=a("a"),Cer=o("TFRobertaForMaskedLM"),Mer=o(" (RoBERTa model)"),Eer=l(),dC=a("li"),Dfe=a("strong"),yer=o("roformer"),wer=o(" \u2014 "),hq=a("a"),Aer=o("TFRoFormerForMaskedLM"),Ler=o(" (RoFormer model)"),Ber=l(),cC=a("li"),qfe=a("strong"),ker=o("tapas"),xer=o(" \u2014 "),uq=a("a"),Rer=o("TFTapasForMaskedLM"),Ser=o(" (TAPAS model)"),Per=l(),mC=a("li"),Gfe=a("strong"),$er=o("xlm"),Ier=o(" \u2014 "),pq=a("a"),jer=o("TFXLMWithLMHeadModel"),Ner=o(" (XLM model)"),Der=l(),fC=a("li"),Ofe=a("strong"),qer=o("xlm-roberta"),Ger=o(" \u2014 "),_q=a("a"),Oer=o("TFXLMRobertaForMaskedLM"),Xer=o(" (XLM-RoBERTa model)"),zer=l(),Xfe=a("p"),Ver=o("Examples:"),Wer=l(),m(d6.$$.fragment),p9e=l(),pc=a("h2"),gC=a("a"),zfe=a("span"),m(c6.$$.fragment),Qer=l(),Vfe=a("span"),Her=o("TFAutoModelForSeq2SeqLM"),_9e=l(),br=a("div"),m(m6.$$.fragment),Uer=l(),_c=a("p"),Jer=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wfe=a("code"),Yer=o("from_pretrained()"),Ker=o("class method or the "),Qfe=a("code"),Zer=o("from_config()"),eor=o(`class
method.`),oor=l(),f6=a("p"),ror=o("This class cannot be instantiated directly using "),Hfe=a("code"),tor=o("__init__()"),aor=o(" (throws an error)."),sor=l(),mt=a("div"),m(g6.$$.fragment),nor=l(),Ufe=a("p"),lor=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),ior=l(),bc=a("p"),dor=o(`Note:
Loading a model from its configuration file does `),Jfe=a("strong"),cor=o("not"),mor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yfe=a("code"),gor=o("from_pretrained()"),hor=o("to load the model weights."),uor=l(),Kfe=a("p"),por=o("Examples:"),_or=l(),m(h6.$$.fragment),bor=l(),bo=a("div"),m(u6.$$.fragment),vor=l(),Zfe=a("p"),Tor=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),For=l(),fs=a("p"),Cor=o("The model class to instantiate is selected based on the "),ege=a("code"),Mor=o("model_type"),Eor=o(` property of the config object (either
passed as an argument or loaded from `),oge=a("code"),yor=o("pretrained_model_name_or_path"),wor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=a("code"),Aor=o("pretrained_model_name_or_path"),Lor=o(":"),Bor=l(),ue=a("ul"),hC=a("li"),tge=a("strong"),kor=o("bart"),xor=o(" \u2014 "),bq=a("a"),Ror=o("TFBartForConditionalGeneration"),Sor=o(" (BART model)"),Por=l(),uC=a("li"),age=a("strong"),$or=o("blenderbot"),Ior=o(" \u2014 "),vq=a("a"),jor=o("TFBlenderbotForConditionalGeneration"),Nor=o(" (Blenderbot model)"),Dor=l(),pC=a("li"),sge=a("strong"),qor=o("blenderbot-small"),Gor=o(" \u2014 "),Tq=a("a"),Oor=o("TFBlenderbotSmallForConditionalGeneration"),Xor=o(" (BlenderbotSmall model)"),zor=l(),_C=a("li"),nge=a("strong"),Vor=o("encoder-decoder"),Wor=o(" \u2014 "),Fq=a("a"),Qor=o("TFEncoderDecoderModel"),Hor=o(" (Encoder decoder model)"),Uor=l(),bC=a("li"),lge=a("strong"),Jor=o("led"),Yor=o(" \u2014 "),Cq=a("a"),Kor=o("TFLEDForConditionalGeneration"),Zor=o(" (LED model)"),err=l(),vC=a("li"),ige=a("strong"),orr=o("marian"),rrr=o(" \u2014 "),Mq=a("a"),trr=o("TFMarianMTModel"),arr=o(" (Marian model)"),srr=l(),TC=a("li"),dge=a("strong"),nrr=o("mbart"),lrr=o(" \u2014 "),Eq=a("a"),irr=o("TFMBartForConditionalGeneration"),drr=o(" (mBART model)"),crr=l(),FC=a("li"),cge=a("strong"),mrr=o("mt5"),frr=o(" \u2014 "),yq=a("a"),grr=o("TFMT5ForConditionalGeneration"),hrr=o(" (mT5 model)"),urr=l(),CC=a("li"),mge=a("strong"),prr=o("pegasus"),_rr=o(" \u2014 "),wq=a("a"),brr=o("TFPegasusForConditionalGeneration"),vrr=o(" (Pegasus model)"),Trr=l(),MC=a("li"),fge=a("strong"),Frr=o("t5"),Crr=o(" \u2014 "),Aq=a("a"),Mrr=o("TFT5ForConditionalGeneration"),Err=o(" (T5 model)"),yrr=l(),gge=a("p"),wrr=o("Examples:"),Arr=l(),m(p6.$$.fragment),b9e=l(),vc=a("h2"),EC=a("a"),hge=a("span"),m(_6.$$.fragment),Lrr=l(),uge=a("span"),Brr=o("TFAutoModelForSequenceClassification"),v9e=l(),vr=a("div"),m(b6.$$.fragment),krr=l(),Tc=a("p"),xrr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),pge=a("code"),Rrr=o("from_pretrained()"),Srr=o("class method or the "),_ge=a("code"),Prr=o("from_config()"),$rr=o(`class
method.`),Irr=l(),v6=a("p"),jrr=o("This class cannot be instantiated directly using "),bge=a("code"),Nrr=o("__init__()"),Drr=o(" (throws an error)."),qrr=l(),ft=a("div"),m(T6.$$.fragment),Grr=l(),vge=a("p"),Orr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Xrr=l(),Fc=a("p"),zrr=o(`Note:
Loading a model from its configuration file does `),Tge=a("strong"),Vrr=o("not"),Wrr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=a("code"),Qrr=o("from_pretrained()"),Hrr=o("to load the model weights."),Urr=l(),Cge=a("p"),Jrr=o("Examples:"),Yrr=l(),m(F6.$$.fragment),Krr=l(),vo=a("div"),m(C6.$$.fragment),Zrr=l(),Mge=a("p"),etr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),otr=l(),gs=a("p"),rtr=o("The model class to instantiate is selected based on the "),Ege=a("code"),ttr=o("model_type"),atr=o(` property of the config object (either
passed as an argument or loaded from `),yge=a("code"),str=o("pretrained_model_name_or_path"),ntr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=a("code"),ltr=o("pretrained_model_name_or_path"),itr=o(":"),dtr=l(),X=a("ul"),yC=a("li"),Age=a("strong"),ctr=o("albert"),mtr=o(" \u2014 "),Lq=a("a"),ftr=o("TFAlbertForSequenceClassification"),gtr=o(" (ALBERT model)"),htr=l(),wC=a("li"),Lge=a("strong"),utr=o("bert"),ptr=o(" \u2014 "),Bq=a("a"),_tr=o("TFBertForSequenceClassification"),btr=o(" (BERT model)"),vtr=l(),AC=a("li"),Bge=a("strong"),Ttr=o("camembert"),Ftr=o(" \u2014 "),kq=a("a"),Ctr=o("TFCamembertForSequenceClassification"),Mtr=o(" (CamemBERT model)"),Etr=l(),LC=a("li"),kge=a("strong"),ytr=o("convbert"),wtr=o(" \u2014 "),xq=a("a"),Atr=o("TFConvBertForSequenceClassification"),Ltr=o(" (ConvBERT model)"),Btr=l(),BC=a("li"),xge=a("strong"),ktr=o("ctrl"),xtr=o(" \u2014 "),Rq=a("a"),Rtr=o("TFCTRLForSequenceClassification"),Str=o(" (CTRL model)"),Ptr=l(),kC=a("li"),Rge=a("strong"),$tr=o("deberta"),Itr=o(" \u2014 "),Sq=a("a"),jtr=o("TFDebertaForSequenceClassification"),Ntr=o(" (DeBERTa model)"),Dtr=l(),xC=a("li"),Sge=a("strong"),qtr=o("deberta-v2"),Gtr=o(" \u2014 "),Pq=a("a"),Otr=o("TFDebertaV2ForSequenceClassification"),Xtr=o(" (DeBERTa-v2 model)"),ztr=l(),RC=a("li"),Pge=a("strong"),Vtr=o("distilbert"),Wtr=o(" \u2014 "),$q=a("a"),Qtr=o("TFDistilBertForSequenceClassification"),Htr=o(" (DistilBERT model)"),Utr=l(),SC=a("li"),$ge=a("strong"),Jtr=o("electra"),Ytr=o(" \u2014 "),Iq=a("a"),Ktr=o("TFElectraForSequenceClassification"),Ztr=o(" (ELECTRA model)"),ear=l(),PC=a("li"),Ige=a("strong"),oar=o("flaubert"),rar=o(" \u2014 "),jq=a("a"),tar=o("TFFlaubertForSequenceClassification"),aar=o(" (FlauBERT model)"),sar=l(),$C=a("li"),jge=a("strong"),nar=o("funnel"),lar=o(" \u2014 "),Nq=a("a"),iar=o("TFFunnelForSequenceClassification"),dar=o(" (Funnel Transformer model)"),car=l(),IC=a("li"),Nge=a("strong"),mar=o("gpt2"),far=o(" \u2014 "),Dq=a("a"),gar=o("TFGPT2ForSequenceClassification"),har=o(" (OpenAI GPT-2 model)"),uar=l(),jC=a("li"),Dge=a("strong"),par=o("layoutlm"),_ar=o(" \u2014 "),qq=a("a"),bar=o("TFLayoutLMForSequenceClassification"),Tar=o(" (LayoutLM model)"),Far=l(),NC=a("li"),qge=a("strong"),Car=o("longformer"),Mar=o(" \u2014 "),Gq=a("a"),Ear=o("TFLongformerForSequenceClassification"),yar=o(" (Longformer model)"),war=l(),DC=a("li"),Gge=a("strong"),Aar=o("mobilebert"),Lar=o(" \u2014 "),Oq=a("a"),Bar=o("TFMobileBertForSequenceClassification"),kar=o(" (MobileBERT model)"),xar=l(),qC=a("li"),Oge=a("strong"),Rar=o("mpnet"),Sar=o(" \u2014 "),Xq=a("a"),Par=o("TFMPNetForSequenceClassification"),$ar=o(" (MPNet model)"),Iar=l(),GC=a("li"),Xge=a("strong"),jar=o("openai-gpt"),Nar=o(" \u2014 "),zq=a("a"),Dar=o("TFOpenAIGPTForSequenceClassification"),qar=o(" (OpenAI GPT model)"),Gar=l(),OC=a("li"),zge=a("strong"),Oar=o("rembert"),Xar=o(" \u2014 "),Vq=a("a"),zar=o("TFRemBertForSequenceClassification"),Var=o(" (RemBERT model)"),War=l(),XC=a("li"),Vge=a("strong"),Qar=o("roberta"),Har=o(" \u2014 "),Wq=a("a"),Uar=o("TFRobertaForSequenceClassification"),Jar=o(" (RoBERTa model)"),Yar=l(),zC=a("li"),Wge=a("strong"),Kar=o("roformer"),Zar=o(" \u2014 "),Qq=a("a"),esr=o("TFRoFormerForSequenceClassification"),osr=o(" (RoFormer model)"),rsr=l(),VC=a("li"),Qge=a("strong"),tsr=o("tapas"),asr=o(" \u2014 "),Hq=a("a"),ssr=o("TFTapasForSequenceClassification"),nsr=o(" (TAPAS model)"),lsr=l(),WC=a("li"),Hge=a("strong"),isr=o("transfo-xl"),dsr=o(" \u2014 "),Uq=a("a"),csr=o("TFTransfoXLForSequenceClassification"),msr=o(" (Transformer-XL model)"),fsr=l(),QC=a("li"),Uge=a("strong"),gsr=o("xlm"),hsr=o(" \u2014 "),Jq=a("a"),usr=o("TFXLMForSequenceClassification"),psr=o(" (XLM model)"),_sr=l(),HC=a("li"),Jge=a("strong"),bsr=o("xlm-roberta"),vsr=o(" \u2014 "),Yq=a("a"),Tsr=o("TFXLMRobertaForSequenceClassification"),Fsr=o(" (XLM-RoBERTa model)"),Csr=l(),UC=a("li"),Yge=a("strong"),Msr=o("xlnet"),Esr=o(" \u2014 "),Kq=a("a"),ysr=o("TFXLNetForSequenceClassification"),wsr=o(" (XLNet model)"),Asr=l(),Kge=a("p"),Lsr=o("Examples:"),Bsr=l(),m(M6.$$.fragment),T9e=l(),Cc=a("h2"),JC=a("a"),Zge=a("span"),m(E6.$$.fragment),ksr=l(),ehe=a("span"),xsr=o("TFAutoModelForMultipleChoice"),F9e=l(),Tr=a("div"),m(y6.$$.fragment),Rsr=l(),Mc=a("p"),Ssr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=a("code"),Psr=o("from_pretrained()"),$sr=o("class method or the "),rhe=a("code"),Isr=o("from_config()"),jsr=o(`class
method.`),Nsr=l(),w6=a("p"),Dsr=o("This class cannot be instantiated directly using "),the=a("code"),qsr=o("__init__()"),Gsr=o(" (throws an error)."),Osr=l(),gt=a("div"),m(A6.$$.fragment),Xsr=l(),ahe=a("p"),zsr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Vsr=l(),Ec=a("p"),Wsr=o(`Note:
Loading a model from its configuration file does `),she=a("strong"),Qsr=o("not"),Hsr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nhe=a("code"),Usr=o("from_pretrained()"),Jsr=o("to load the model weights."),Ysr=l(),lhe=a("p"),Ksr=o("Examples:"),Zsr=l(),m(L6.$$.fragment),enr=l(),To=a("div"),m(B6.$$.fragment),onr=l(),ihe=a("p"),rnr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),tnr=l(),hs=a("p"),anr=o("The model class to instantiate is selected based on the "),dhe=a("code"),snr=o("model_type"),nnr=o(` property of the config object (either
passed as an argument or loaded from `),che=a("code"),lnr=o("pretrained_model_name_or_path"),inr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mhe=a("code"),dnr=o("pretrained_model_name_or_path"),cnr=o(":"),mnr=l(),te=a("ul"),YC=a("li"),fhe=a("strong"),fnr=o("albert"),gnr=o(" \u2014 "),Zq=a("a"),hnr=o("TFAlbertForMultipleChoice"),unr=o(" (ALBERT model)"),pnr=l(),KC=a("li"),ghe=a("strong"),_nr=o("bert"),bnr=o(" \u2014 "),eG=a("a"),vnr=o("TFBertForMultipleChoice"),Tnr=o(" (BERT model)"),Fnr=l(),ZC=a("li"),hhe=a("strong"),Cnr=o("camembert"),Mnr=o(" \u2014 "),oG=a("a"),Enr=o("TFCamembertForMultipleChoice"),ynr=o(" (CamemBERT model)"),wnr=l(),e4=a("li"),uhe=a("strong"),Anr=o("convbert"),Lnr=o(" \u2014 "),rG=a("a"),Bnr=o("TFConvBertForMultipleChoice"),knr=o(" (ConvBERT model)"),xnr=l(),o4=a("li"),phe=a("strong"),Rnr=o("distilbert"),Snr=o(" \u2014 "),tG=a("a"),Pnr=o("TFDistilBertForMultipleChoice"),$nr=o(" (DistilBERT model)"),Inr=l(),r4=a("li"),_he=a("strong"),jnr=o("electra"),Nnr=o(" \u2014 "),aG=a("a"),Dnr=o("TFElectraForMultipleChoice"),qnr=o(" (ELECTRA model)"),Gnr=l(),t4=a("li"),bhe=a("strong"),Onr=o("flaubert"),Xnr=o(" \u2014 "),sG=a("a"),znr=o("TFFlaubertForMultipleChoice"),Vnr=o(" (FlauBERT model)"),Wnr=l(),a4=a("li"),vhe=a("strong"),Qnr=o("funnel"),Hnr=o(" \u2014 "),nG=a("a"),Unr=o("TFFunnelForMultipleChoice"),Jnr=o(" (Funnel Transformer model)"),Ynr=l(),s4=a("li"),The=a("strong"),Knr=o("longformer"),Znr=o(" \u2014 "),lG=a("a"),elr=o("TFLongformerForMultipleChoice"),olr=o(" (Longformer model)"),rlr=l(),n4=a("li"),Fhe=a("strong"),tlr=o("mobilebert"),alr=o(" \u2014 "),iG=a("a"),slr=o("TFMobileBertForMultipleChoice"),nlr=o(" (MobileBERT model)"),llr=l(),l4=a("li"),Che=a("strong"),ilr=o("mpnet"),dlr=o(" \u2014 "),dG=a("a"),clr=o("TFMPNetForMultipleChoice"),mlr=o(" (MPNet model)"),flr=l(),i4=a("li"),Mhe=a("strong"),glr=o("rembert"),hlr=o(" \u2014 "),cG=a("a"),ulr=o("TFRemBertForMultipleChoice"),plr=o(" (RemBERT model)"),_lr=l(),d4=a("li"),Ehe=a("strong"),blr=o("roberta"),vlr=o(" \u2014 "),mG=a("a"),Tlr=o("TFRobertaForMultipleChoice"),Flr=o(" (RoBERTa model)"),Clr=l(),c4=a("li"),yhe=a("strong"),Mlr=o("roformer"),Elr=o(" \u2014 "),fG=a("a"),ylr=o("TFRoFormerForMultipleChoice"),wlr=o(" (RoFormer model)"),Alr=l(),m4=a("li"),whe=a("strong"),Llr=o("xlm"),Blr=o(" \u2014 "),gG=a("a"),klr=o("TFXLMForMultipleChoice"),xlr=o(" (XLM model)"),Rlr=l(),f4=a("li"),Ahe=a("strong"),Slr=o("xlm-roberta"),Plr=o(" \u2014 "),hG=a("a"),$lr=o("TFXLMRobertaForMultipleChoice"),Ilr=o(" (XLM-RoBERTa model)"),jlr=l(),g4=a("li"),Lhe=a("strong"),Nlr=o("xlnet"),Dlr=o(" \u2014 "),uG=a("a"),qlr=o("TFXLNetForMultipleChoice"),Glr=o(" (XLNet model)"),Olr=l(),Bhe=a("p"),Xlr=o("Examples:"),zlr=l(),m(k6.$$.fragment),C9e=l(),yc=a("h2"),h4=a("a"),khe=a("span"),m(x6.$$.fragment),Vlr=l(),xhe=a("span"),Wlr=o("TFAutoModelForTableQuestionAnswering"),M9e=l(),Fr=a("div"),m(R6.$$.fragment),Qlr=l(),wc=a("p"),Hlr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=a("code"),Ulr=o("from_pretrained()"),Jlr=o("class method or the "),She=a("code"),Ylr=o("from_config()"),Klr=o(`class
method.`),Zlr=l(),S6=a("p"),eir=o("This class cannot be instantiated directly using "),Phe=a("code"),oir=o("__init__()"),rir=o(" (throws an error)."),tir=l(),ht=a("div"),m(P6.$$.fragment),air=l(),$he=a("p"),sir=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),nir=l(),Ac=a("p"),lir=o(`Note:
Loading a model from its configuration file does `),Ihe=a("strong"),iir=o("not"),dir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=a("code"),cir=o("from_pretrained()"),mir=o("to load the model weights."),fir=l(),Nhe=a("p"),gir=o("Examples:"),hir=l(),m($6.$$.fragment),uir=l(),Fo=a("div"),m(I6.$$.fragment),pir=l(),Dhe=a("p"),_ir=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),bir=l(),us=a("p"),vir=o("The model class to instantiate is selected based on the "),qhe=a("code"),Tir=o("model_type"),Fir=o(` property of the config object (either
passed as an argument or loaded from `),Ghe=a("code"),Cir=o("pretrained_model_name_or_path"),Mir=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=a("code"),Eir=o("pretrained_model_name_or_path"),yir=o(":"),wir=l(),Xhe=a("ul"),u4=a("li"),zhe=a("strong"),Air=o("tapas"),Lir=o(" \u2014 "),pG=a("a"),Bir=o("TFTapasForQuestionAnswering"),kir=o(" (TAPAS model)"),xir=l(),Vhe=a("p"),Rir=o("Examples:"),Sir=l(),m(j6.$$.fragment),E9e=l(),Lc=a("h2"),p4=a("a"),Whe=a("span"),m(N6.$$.fragment),Pir=l(),Qhe=a("span"),$ir=o("TFAutoModelForTokenClassification"),y9e=l(),Cr=a("div"),m(D6.$$.fragment),Iir=l(),Bc=a("p"),jir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=a("code"),Nir=o("from_pretrained()"),Dir=o("class method or the "),Uhe=a("code"),qir=o("from_config()"),Gir=o(`class
method.`),Oir=l(),q6=a("p"),Xir=o("This class cannot be instantiated directly using "),Jhe=a("code"),zir=o("__init__()"),Vir=o(" (throws an error)."),Wir=l(),ut=a("div"),m(G6.$$.fragment),Qir=l(),Yhe=a("p"),Hir=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Uir=l(),kc=a("p"),Jir=o(`Note:
Loading a model from its configuration file does `),Khe=a("strong"),Yir=o("not"),Kir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=a("code"),Zir=o("from_pretrained()"),edr=o("to load the model weights."),odr=l(),eue=a("p"),rdr=o("Examples:"),tdr=l(),m(O6.$$.fragment),adr=l(),Co=a("div"),m(X6.$$.fragment),sdr=l(),oue=a("p"),ndr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),ldr=l(),ps=a("p"),idr=o("The model class to instantiate is selected based on the "),rue=a("code"),ddr=o("model_type"),cdr=o(` property of the config object (either
passed as an argument or loaded from `),tue=a("code"),mdr=o("pretrained_model_name_or_path"),fdr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aue=a("code"),gdr=o("pretrained_model_name_or_path"),hdr=o(":"),udr=l(),K=a("ul"),_4=a("li"),sue=a("strong"),pdr=o("albert"),_dr=o(" \u2014 "),_G=a("a"),bdr=o("TFAlbertForTokenClassification"),vdr=o(" (ALBERT model)"),Tdr=l(),b4=a("li"),nue=a("strong"),Fdr=o("bert"),Cdr=o(" \u2014 "),bG=a("a"),Mdr=o("TFBertForTokenClassification"),Edr=o(" (BERT model)"),ydr=l(),v4=a("li"),lue=a("strong"),wdr=o("camembert"),Adr=o(" \u2014 "),vG=a("a"),Ldr=o("TFCamembertForTokenClassification"),Bdr=o(" (CamemBERT model)"),kdr=l(),T4=a("li"),iue=a("strong"),xdr=o("convbert"),Rdr=o(" \u2014 "),TG=a("a"),Sdr=o("TFConvBertForTokenClassification"),Pdr=o(" (ConvBERT model)"),$dr=l(),F4=a("li"),due=a("strong"),Idr=o("deberta"),jdr=o(" \u2014 "),FG=a("a"),Ndr=o("TFDebertaForTokenClassification"),Ddr=o(" (DeBERTa model)"),qdr=l(),C4=a("li"),cue=a("strong"),Gdr=o("deberta-v2"),Odr=o(" \u2014 "),CG=a("a"),Xdr=o("TFDebertaV2ForTokenClassification"),zdr=o(" (DeBERTa-v2 model)"),Vdr=l(),M4=a("li"),mue=a("strong"),Wdr=o("distilbert"),Qdr=o(" \u2014 "),MG=a("a"),Hdr=o("TFDistilBertForTokenClassification"),Udr=o(" (DistilBERT model)"),Jdr=l(),E4=a("li"),fue=a("strong"),Ydr=o("electra"),Kdr=o(" \u2014 "),EG=a("a"),Zdr=o("TFElectraForTokenClassification"),ecr=o(" (ELECTRA model)"),ocr=l(),y4=a("li"),gue=a("strong"),rcr=o("flaubert"),tcr=o(" \u2014 "),yG=a("a"),acr=o("TFFlaubertForTokenClassification"),scr=o(" (FlauBERT model)"),ncr=l(),w4=a("li"),hue=a("strong"),lcr=o("funnel"),icr=o(" \u2014 "),wG=a("a"),dcr=o("TFFunnelForTokenClassification"),ccr=o(" (Funnel Transformer model)"),mcr=l(),A4=a("li"),uue=a("strong"),fcr=o("layoutlm"),gcr=o(" \u2014 "),AG=a("a"),hcr=o("TFLayoutLMForTokenClassification"),ucr=o(" (LayoutLM model)"),pcr=l(),L4=a("li"),pue=a("strong"),_cr=o("longformer"),bcr=o(" \u2014 "),LG=a("a"),vcr=o("TFLongformerForTokenClassification"),Tcr=o(" (Longformer model)"),Fcr=l(),B4=a("li"),_ue=a("strong"),Ccr=o("mobilebert"),Mcr=o(" \u2014 "),BG=a("a"),Ecr=o("TFMobileBertForTokenClassification"),ycr=o(" (MobileBERT model)"),wcr=l(),k4=a("li"),bue=a("strong"),Acr=o("mpnet"),Lcr=o(" \u2014 "),kG=a("a"),Bcr=o("TFMPNetForTokenClassification"),kcr=o(" (MPNet model)"),xcr=l(),x4=a("li"),vue=a("strong"),Rcr=o("rembert"),Scr=o(" \u2014 "),xG=a("a"),Pcr=o("TFRemBertForTokenClassification"),$cr=o(" (RemBERT model)"),Icr=l(),R4=a("li"),Tue=a("strong"),jcr=o("roberta"),Ncr=o(" \u2014 "),RG=a("a"),Dcr=o("TFRobertaForTokenClassification"),qcr=o(" (RoBERTa model)"),Gcr=l(),S4=a("li"),Fue=a("strong"),Ocr=o("roformer"),Xcr=o(" \u2014 "),SG=a("a"),zcr=o("TFRoFormerForTokenClassification"),Vcr=o(" (RoFormer model)"),Wcr=l(),P4=a("li"),Cue=a("strong"),Qcr=o("xlm"),Hcr=o(" \u2014 "),PG=a("a"),Ucr=o("TFXLMForTokenClassification"),Jcr=o(" (XLM model)"),Ycr=l(),$4=a("li"),Mue=a("strong"),Kcr=o("xlm-roberta"),Zcr=o(" \u2014 "),$G=a("a"),emr=o("TFXLMRobertaForTokenClassification"),omr=o(" (XLM-RoBERTa model)"),rmr=l(),I4=a("li"),Eue=a("strong"),tmr=o("xlnet"),amr=o(" \u2014 "),IG=a("a"),smr=o("TFXLNetForTokenClassification"),nmr=o(" (XLNet model)"),lmr=l(),yue=a("p"),imr=o("Examples:"),dmr=l(),m(z6.$$.fragment),w9e=l(),xc=a("h2"),j4=a("a"),wue=a("span"),m(V6.$$.fragment),cmr=l(),Aue=a("span"),mmr=o("TFAutoModelForQuestionAnswering"),A9e=l(),Mr=a("div"),m(W6.$$.fragment),fmr=l(),Rc=a("p"),gmr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lue=a("code"),hmr=o("from_pretrained()"),umr=o("class method or the "),Bue=a("code"),pmr=o("from_config()"),_mr=o(`class
method.`),bmr=l(),Q6=a("p"),vmr=o("This class cannot be instantiated directly using "),kue=a("code"),Tmr=o("__init__()"),Fmr=o(" (throws an error)."),Cmr=l(),pt=a("div"),m(H6.$$.fragment),Mmr=l(),xue=a("p"),Emr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),ymr=l(),Sc=a("p"),wmr=o(`Note:
Loading a model from its configuration file does `),Rue=a("strong"),Amr=o("not"),Lmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Sue=a("code"),Bmr=o("from_pretrained()"),kmr=o("to load the model weights."),xmr=l(),Pue=a("p"),Rmr=o("Examples:"),Smr=l(),m(U6.$$.fragment),Pmr=l(),Mo=a("div"),m(J6.$$.fragment),$mr=l(),$ue=a("p"),Imr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),jmr=l(),_s=a("p"),Nmr=o("The model class to instantiate is selected based on the "),Iue=a("code"),Dmr=o("model_type"),qmr=o(` property of the config object (either
passed as an argument or loaded from `),jue=a("code"),Gmr=o("pretrained_model_name_or_path"),Omr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=a("code"),Xmr=o("pretrained_model_name_or_path"),zmr=o(":"),Vmr=l(),Z=a("ul"),N4=a("li"),Due=a("strong"),Wmr=o("albert"),Qmr=o(" \u2014 "),jG=a("a"),Hmr=o("TFAlbertForQuestionAnswering"),Umr=o(" (ALBERT model)"),Jmr=l(),D4=a("li"),que=a("strong"),Ymr=o("bert"),Kmr=o(" \u2014 "),NG=a("a"),Zmr=o("TFBertForQuestionAnswering"),efr=o(" (BERT model)"),ofr=l(),q4=a("li"),Gue=a("strong"),rfr=o("camembert"),tfr=o(" \u2014 "),DG=a("a"),afr=o("TFCamembertForQuestionAnswering"),sfr=o(" (CamemBERT model)"),nfr=l(),G4=a("li"),Oue=a("strong"),lfr=o("convbert"),ifr=o(" \u2014 "),qG=a("a"),dfr=o("TFConvBertForQuestionAnswering"),cfr=o(" (ConvBERT model)"),mfr=l(),O4=a("li"),Xue=a("strong"),ffr=o("deberta"),gfr=o(" \u2014 "),GG=a("a"),hfr=o("TFDebertaForQuestionAnswering"),ufr=o(" (DeBERTa model)"),pfr=l(),X4=a("li"),zue=a("strong"),_fr=o("deberta-v2"),bfr=o(" \u2014 "),OG=a("a"),vfr=o("TFDebertaV2ForQuestionAnswering"),Tfr=o(" (DeBERTa-v2 model)"),Ffr=l(),z4=a("li"),Vue=a("strong"),Cfr=o("distilbert"),Mfr=o(" \u2014 "),XG=a("a"),Efr=o("TFDistilBertForQuestionAnswering"),yfr=o(" (DistilBERT model)"),wfr=l(),V4=a("li"),Wue=a("strong"),Afr=o("electra"),Lfr=o(" \u2014 "),zG=a("a"),Bfr=o("TFElectraForQuestionAnswering"),kfr=o(" (ELECTRA model)"),xfr=l(),W4=a("li"),Que=a("strong"),Rfr=o("flaubert"),Sfr=o(" \u2014 "),VG=a("a"),Pfr=o("TFFlaubertForQuestionAnsweringSimple"),$fr=o(" (FlauBERT model)"),Ifr=l(),Q4=a("li"),Hue=a("strong"),jfr=o("funnel"),Nfr=o(" \u2014 "),WG=a("a"),Dfr=o("TFFunnelForQuestionAnswering"),qfr=o(" (Funnel Transformer model)"),Gfr=l(),H4=a("li"),Uue=a("strong"),Ofr=o("longformer"),Xfr=o(" \u2014 "),QG=a("a"),zfr=o("TFLongformerForQuestionAnswering"),Vfr=o(" (Longformer model)"),Wfr=l(),U4=a("li"),Jue=a("strong"),Qfr=o("mobilebert"),Hfr=o(" \u2014 "),HG=a("a"),Ufr=o("TFMobileBertForQuestionAnswering"),Jfr=o(" (MobileBERT model)"),Yfr=l(),J4=a("li"),Yue=a("strong"),Kfr=o("mpnet"),Zfr=o(" \u2014 "),UG=a("a"),egr=o("TFMPNetForQuestionAnswering"),ogr=o(" (MPNet model)"),rgr=l(),Y4=a("li"),Kue=a("strong"),tgr=o("rembert"),agr=o(" \u2014 "),JG=a("a"),sgr=o("TFRemBertForQuestionAnswering"),ngr=o(" (RemBERT model)"),lgr=l(),K4=a("li"),Zue=a("strong"),igr=o("roberta"),dgr=o(" \u2014 "),YG=a("a"),cgr=o("TFRobertaForQuestionAnswering"),mgr=o(" (RoBERTa model)"),fgr=l(),Z4=a("li"),epe=a("strong"),ggr=o("roformer"),hgr=o(" \u2014 "),KG=a("a"),ugr=o("TFRoFormerForQuestionAnswering"),pgr=o(" (RoFormer model)"),_gr=l(),eM=a("li"),ope=a("strong"),bgr=o("xlm"),vgr=o(" \u2014 "),ZG=a("a"),Tgr=o("TFXLMForQuestionAnsweringSimple"),Fgr=o(" (XLM model)"),Cgr=l(),oM=a("li"),rpe=a("strong"),Mgr=o("xlm-roberta"),Egr=o(" \u2014 "),eO=a("a"),ygr=o("TFXLMRobertaForQuestionAnswering"),wgr=o(" (XLM-RoBERTa model)"),Agr=l(),rM=a("li"),tpe=a("strong"),Lgr=o("xlnet"),Bgr=o(" \u2014 "),oO=a("a"),kgr=o("TFXLNetForQuestionAnsweringSimple"),xgr=o(" (XLNet model)"),Rgr=l(),ape=a("p"),Sgr=o("Examples:"),Pgr=l(),m(Y6.$$.fragment),L9e=l(),Pc=a("h2"),tM=a("a"),spe=a("span"),m(K6.$$.fragment),$gr=l(),npe=a("span"),Igr=o("TFAutoModelForVision2Seq"),B9e=l(),Er=a("div"),m(Z6.$$.fragment),jgr=l(),$c=a("p"),Ngr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),lpe=a("code"),Dgr=o("from_pretrained()"),qgr=o("class method or the "),ipe=a("code"),Ggr=o("from_config()"),Ogr=o(`class
method.`),Xgr=l(),e0=a("p"),zgr=o("This class cannot be instantiated directly using "),dpe=a("code"),Vgr=o("__init__()"),Wgr=o(" (throws an error)."),Qgr=l(),_t=a("div"),m(o0.$$.fragment),Hgr=l(),cpe=a("p"),Ugr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Jgr=l(),Ic=a("p"),Ygr=o(`Note:
Loading a model from its configuration file does `),mpe=a("strong"),Kgr=o("not"),Zgr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fpe=a("code"),ehr=o("from_pretrained()"),ohr=o("to load the model weights."),rhr=l(),gpe=a("p"),thr=o("Examples:"),ahr=l(),m(r0.$$.fragment),shr=l(),Eo=a("div"),m(t0.$$.fragment),nhr=l(),hpe=a("p"),lhr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),ihr=l(),bs=a("p"),dhr=o("The model class to instantiate is selected based on the "),upe=a("code"),chr=o("model_type"),mhr=o(` property of the config object (either
passed as an argument or loaded from `),ppe=a("code"),fhr=o("pretrained_model_name_or_path"),ghr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_pe=a("code"),hhr=o("pretrained_model_name_or_path"),uhr=o(":"),phr=l(),bpe=a("ul"),aM=a("li"),vpe=a("strong"),_hr=o("vision-encoder-decoder"),bhr=o(" \u2014 "),rO=a("a"),vhr=o("TFVisionEncoderDecoderModel"),Thr=o(" (Vision Encoder decoder model)"),Fhr=l(),Tpe=a("p"),Chr=o("Examples:"),Mhr=l(),m(a0.$$.fragment),k9e=l(),jc=a("h2"),sM=a("a"),Fpe=a("span"),m(s0.$$.fragment),Ehr=l(),Cpe=a("span"),yhr=o("TFAutoModelForSpeechSeq2Seq"),x9e=l(),yr=a("div"),m(n0.$$.fragment),whr=l(),Nc=a("p"),Ahr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Mpe=a("code"),Lhr=o("from_pretrained()"),Bhr=o("class method or the "),Epe=a("code"),khr=o("from_config()"),xhr=o(`class
method.`),Rhr=l(),l0=a("p"),Shr=o("This class cannot be instantiated directly using "),ype=a("code"),Phr=o("__init__()"),$hr=o(" (throws an error)."),Ihr=l(),bt=a("div"),m(i0.$$.fragment),jhr=l(),wpe=a("p"),Nhr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),Dhr=l(),Dc=a("p"),qhr=o(`Note:
Loading a model from its configuration file does `),Ape=a("strong"),Ghr=o("not"),Ohr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lpe=a("code"),Xhr=o("from_pretrained()"),zhr=o("to load the model weights."),Vhr=l(),Bpe=a("p"),Whr=o("Examples:"),Qhr=l(),m(d0.$$.fragment),Hhr=l(),yo=a("div"),m(c0.$$.fragment),Uhr=l(),kpe=a("p"),Jhr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Yhr=l(),vs=a("p"),Khr=o("The model class to instantiate is selected based on the "),xpe=a("code"),Zhr=o("model_type"),eur=o(` property of the config object (either
passed as an argument or loaded from `),Rpe=a("code"),our=o("pretrained_model_name_or_path"),rur=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Spe=a("code"),tur=o("pretrained_model_name_or_path"),aur=o(":"),sur=l(),Ppe=a("ul"),nM=a("li"),$pe=a("strong"),nur=o("speech_to_text"),lur=o(" \u2014 "),tO=a("a"),iur=o("TFSpeech2TextForConditionalGeneration"),dur=o(" (Speech2Text model)"),cur=l(),Ipe=a("p"),mur=o("Examples:"),fur=l(),m(m0.$$.fragment),R9e=l(),qc=a("h2"),lM=a("a"),jpe=a("span"),m(f0.$$.fragment),gur=l(),Npe=a("span"),hur=o("FlaxAutoModel"),S9e=l(),wr=a("div"),m(g0.$$.fragment),uur=l(),Gc=a("p"),pur=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Dpe=a("code"),_ur=o("from_pretrained()"),bur=o("class method or the "),qpe=a("code"),vur=o("from_config()"),Tur=o(`class
method.`),Fur=l(),h0=a("p"),Cur=o("This class cannot be instantiated directly using "),Gpe=a("code"),Mur=o("__init__()"),Eur=o(" (throws an error)."),yur=l(),vt=a("div"),m(u0.$$.fragment),wur=l(),Ope=a("p"),Aur=o("Instantiates one of the base model classes of the library from a configuration."),Lur=l(),Oc=a("p"),Bur=o(`Note:
Loading a model from its configuration file does `),Xpe=a("strong"),kur=o("not"),xur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),zpe=a("code"),Rur=o("from_pretrained()"),Sur=o("to load the model weights."),Pur=l(),Vpe=a("p"),$ur=o("Examples:"),Iur=l(),m(p0.$$.fragment),jur=l(),wo=a("div"),m(_0.$$.fragment),Nur=l(),Wpe=a("p"),Dur=o("Instantiate one of the base model classes of the library from a pretrained model."),qur=l(),Ts=a("p"),Gur=o("The model class to instantiate is selected based on the "),Qpe=a("code"),Our=o("model_type"),Xur=o(` property of the config object (either
passed as an argument or loaded from `),Hpe=a("code"),zur=o("pretrained_model_name_or_path"),Vur=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Upe=a("code"),Wur=o("pretrained_model_name_or_path"),Qur=o(":"),Hur=l(),V=a("ul"),iM=a("li"),Jpe=a("strong"),Uur=o("albert"),Jur=o(" \u2014 "),aO=a("a"),Yur=o("FlaxAlbertModel"),Kur=o(" (ALBERT model)"),Zur=l(),dM=a("li"),Ype=a("strong"),epr=o("bart"),opr=o(" \u2014 "),sO=a("a"),rpr=o("FlaxBartModel"),tpr=o(" (BART model)"),apr=l(),cM=a("li"),Kpe=a("strong"),spr=o("beit"),npr=o(" \u2014 "),nO=a("a"),lpr=o("FlaxBeitModel"),ipr=o(" (BEiT model)"),dpr=l(),mM=a("li"),Zpe=a("strong"),cpr=o("bert"),mpr=o(" \u2014 "),lO=a("a"),fpr=o("FlaxBertModel"),gpr=o(" (BERT model)"),hpr=l(),fM=a("li"),e_e=a("strong"),upr=o("big_bird"),ppr=o(" \u2014 "),iO=a("a"),_pr=o("FlaxBigBirdModel"),bpr=o(" (BigBird model)"),vpr=l(),gM=a("li"),o_e=a("strong"),Tpr=o("blenderbot"),Fpr=o(" \u2014 "),dO=a("a"),Cpr=o("FlaxBlenderbotModel"),Mpr=o(" (Blenderbot model)"),Epr=l(),hM=a("li"),r_e=a("strong"),ypr=o("blenderbot-small"),wpr=o(" \u2014 "),cO=a("a"),Apr=o("FlaxBlenderbotSmallModel"),Lpr=o(" (BlenderbotSmall model)"),Bpr=l(),uM=a("li"),t_e=a("strong"),kpr=o("clip"),xpr=o(" \u2014 "),mO=a("a"),Rpr=o("FlaxCLIPModel"),Spr=o(" (CLIP model)"),Ppr=l(),pM=a("li"),a_e=a("strong"),$pr=o("distilbert"),Ipr=o(" \u2014 "),fO=a("a"),jpr=o("FlaxDistilBertModel"),Npr=o(" (DistilBERT model)"),Dpr=l(),_M=a("li"),s_e=a("strong"),qpr=o("electra"),Gpr=o(" \u2014 "),gO=a("a"),Opr=o("FlaxElectraModel"),Xpr=o(" (ELECTRA model)"),zpr=l(),bM=a("li"),n_e=a("strong"),Vpr=o("gpt2"),Wpr=o(" \u2014 "),hO=a("a"),Qpr=o("FlaxGPT2Model"),Hpr=o(" (OpenAI GPT-2 model)"),Upr=l(),vM=a("li"),l_e=a("strong"),Jpr=o("gpt_neo"),Ypr=o(" \u2014 "),uO=a("a"),Kpr=o("FlaxGPTNeoModel"),Zpr=o(" (GPT Neo model)"),e_r=l(),TM=a("li"),i_e=a("strong"),o_r=o("gptj"),r_r=o(" \u2014 "),pO=a("a"),t_r=o("FlaxGPTJModel"),a_r=o(" (GPT-J model)"),s_r=l(),FM=a("li"),d_e=a("strong"),n_r=o("marian"),l_r=o(" \u2014 "),_O=a("a"),i_r=o("FlaxMarianModel"),d_r=o(" (Marian model)"),c_r=l(),CM=a("li"),c_e=a("strong"),m_r=o("mbart"),f_r=o(" \u2014 "),bO=a("a"),g_r=o("FlaxMBartModel"),h_r=o(" (mBART model)"),u_r=l(),MM=a("li"),m_e=a("strong"),p_r=o("mt5"),__r=o(" \u2014 "),vO=a("a"),b_r=o("FlaxMT5Model"),v_r=o(" (mT5 model)"),T_r=l(),EM=a("li"),f_e=a("strong"),F_r=o("pegasus"),C_r=o(" \u2014 "),TO=a("a"),M_r=o("FlaxPegasusModel"),E_r=o(" (Pegasus model)"),y_r=l(),yM=a("li"),g_e=a("strong"),w_r=o("roberta"),A_r=o(" \u2014 "),FO=a("a"),L_r=o("FlaxRobertaModel"),B_r=o(" (RoBERTa model)"),k_r=l(),wM=a("li"),h_e=a("strong"),x_r=o("roformer"),R_r=o(" \u2014 "),CO=a("a"),S_r=o("FlaxRoFormerModel"),P_r=o(" (RoFormer model)"),$_r=l(),AM=a("li"),u_e=a("strong"),I_r=o("t5"),j_r=o(" \u2014 "),MO=a("a"),N_r=o("FlaxT5Model"),D_r=o(" (T5 model)"),q_r=l(),LM=a("li"),p_e=a("strong"),G_r=o("vision-text-dual-encoder"),O_r=o(" \u2014 "),EO=a("a"),X_r=o("FlaxVisionTextDualEncoderModel"),z_r=o(" (VisionTextDualEncoder model)"),V_r=l(),BM=a("li"),__e=a("strong"),W_r=o("vit"),Q_r=o(" \u2014 "),yO=a("a"),H_r=o("FlaxViTModel"),U_r=o(" (ViT model)"),J_r=l(),kM=a("li"),b_e=a("strong"),Y_r=o("wav2vec2"),K_r=o(" \u2014 "),wO=a("a"),Z_r=o("FlaxWav2Vec2Model"),ebr=o(" (Wav2Vec2 model)"),obr=l(),xM=a("li"),v_e=a("strong"),rbr=o("xglm"),tbr=o(" \u2014 "),AO=a("a"),abr=o("FlaxXGLMModel"),sbr=o(" (XGLM model)"),nbr=l(),T_e=a("p"),lbr=o("Examples:"),ibr=l(),m(b0.$$.fragment),P9e=l(),Xc=a("h2"),RM=a("a"),F_e=a("span"),m(v0.$$.fragment),dbr=l(),C_e=a("span"),cbr=o("FlaxAutoModelForCausalLM"),$9e=l(),Ar=a("div"),m(T0.$$.fragment),mbr=l(),zc=a("p"),fbr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),M_e=a("code"),gbr=o("from_pretrained()"),hbr=o("class method or the "),E_e=a("code"),ubr=o("from_config()"),pbr=o(`class
method.`),_br=l(),F0=a("p"),bbr=o("This class cannot be instantiated directly using "),y_e=a("code"),vbr=o("__init__()"),Tbr=o(" (throws an error)."),Fbr=l(),Tt=a("div"),m(C0.$$.fragment),Cbr=l(),w_e=a("p"),Mbr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Ebr=l(),Vc=a("p"),ybr=o(`Note:
Loading a model from its configuration file does `),A_e=a("strong"),wbr=o("not"),Abr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=a("code"),Lbr=o("from_pretrained()"),Bbr=o("to load the model weights."),kbr=l(),B_e=a("p"),xbr=o("Examples:"),Rbr=l(),m(M0.$$.fragment),Sbr=l(),Ao=a("div"),m(E0.$$.fragment),Pbr=l(),k_e=a("p"),$br=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Ibr=l(),Fs=a("p"),jbr=o("The model class to instantiate is selected based on the "),x_e=a("code"),Nbr=o("model_type"),Dbr=o(` property of the config object (either
passed as an argument or loaded from `),R_e=a("code"),qbr=o("pretrained_model_name_or_path"),Gbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=a("code"),Obr=o("pretrained_model_name_or_path"),Xbr=o(":"),zbr=l(),Cs=a("ul"),SM=a("li"),P_e=a("strong"),Vbr=o("gpt2"),Wbr=o(" \u2014 "),LO=a("a"),Qbr=o("FlaxGPT2LMHeadModel"),Hbr=o(" (OpenAI GPT-2 model)"),Ubr=l(),PM=a("li"),$_e=a("strong"),Jbr=o("gpt_neo"),Ybr=o(" \u2014 "),BO=a("a"),Kbr=o("FlaxGPTNeoForCausalLM"),Zbr=o(" (GPT Neo model)"),e2r=l(),$M=a("li"),I_e=a("strong"),o2r=o("gptj"),r2r=o(" \u2014 "),kO=a("a"),t2r=o("FlaxGPTJForCausalLM"),a2r=o(" (GPT-J model)"),s2r=l(),IM=a("li"),j_e=a("strong"),n2r=o("xglm"),l2r=o(" \u2014 "),xO=a("a"),i2r=o("FlaxXGLMForCausalLM"),d2r=o(" (XGLM model)"),c2r=l(),N_e=a("p"),m2r=o("Examples:"),f2r=l(),m(y0.$$.fragment),I9e=l(),Wc=a("h2"),jM=a("a"),D_e=a("span"),m(w0.$$.fragment),g2r=l(),q_e=a("span"),h2r=o("FlaxAutoModelForPreTraining"),j9e=l(),Lr=a("div"),m(A0.$$.fragment),u2r=l(),Qc=a("p"),p2r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),G_e=a("code"),_2r=o("from_pretrained()"),b2r=o("class method or the "),O_e=a("code"),v2r=o("from_config()"),T2r=o(`class
method.`),F2r=l(),L0=a("p"),C2r=o("This class cannot be instantiated directly using "),X_e=a("code"),M2r=o("__init__()"),E2r=o(" (throws an error)."),y2r=l(),Ft=a("div"),m(B0.$$.fragment),w2r=l(),z_e=a("p"),A2r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),L2r=l(),Hc=a("p"),B2r=o(`Note:
Loading a model from its configuration file does `),V_e=a("strong"),k2r=o("not"),x2r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),W_e=a("code"),R2r=o("from_pretrained()"),S2r=o("to load the model weights."),P2r=l(),Q_e=a("p"),$2r=o("Examples:"),I2r=l(),m(k0.$$.fragment),j2r=l(),Lo=a("div"),m(x0.$$.fragment),N2r=l(),H_e=a("p"),D2r=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),q2r=l(),Ms=a("p"),G2r=o("The model class to instantiate is selected based on the "),U_e=a("code"),O2r=o("model_type"),X2r=o(` property of the config object (either
passed as an argument or loaded from `),J_e=a("code"),z2r=o("pretrained_model_name_or_path"),V2r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Y_e=a("code"),W2r=o("pretrained_model_name_or_path"),Q2r=o(":"),H2r=l(),me=a("ul"),NM=a("li"),K_e=a("strong"),U2r=o("albert"),J2r=o(" \u2014 "),RO=a("a"),Y2r=o("FlaxAlbertForPreTraining"),K2r=o(" (ALBERT model)"),Z2r=l(),DM=a("li"),Z_e=a("strong"),evr=o("bart"),ovr=o(" \u2014 "),SO=a("a"),rvr=o("FlaxBartForConditionalGeneration"),tvr=o(" (BART model)"),avr=l(),qM=a("li"),ebe=a("strong"),svr=o("bert"),nvr=o(" \u2014 "),PO=a("a"),lvr=o("FlaxBertForPreTraining"),ivr=o(" (BERT model)"),dvr=l(),GM=a("li"),obe=a("strong"),cvr=o("big_bird"),mvr=o(" \u2014 "),$O=a("a"),fvr=o("FlaxBigBirdForPreTraining"),gvr=o(" (BigBird model)"),hvr=l(),OM=a("li"),rbe=a("strong"),uvr=o("electra"),pvr=o(" \u2014 "),IO=a("a"),_vr=o("FlaxElectraForPreTraining"),bvr=o(" (ELECTRA model)"),vvr=l(),XM=a("li"),tbe=a("strong"),Tvr=o("mbart"),Fvr=o(" \u2014 "),jO=a("a"),Cvr=o("FlaxMBartForConditionalGeneration"),Mvr=o(" (mBART model)"),Evr=l(),zM=a("li"),abe=a("strong"),yvr=o("mt5"),wvr=o(" \u2014 "),NO=a("a"),Avr=o("FlaxMT5ForConditionalGeneration"),Lvr=o(" (mT5 model)"),Bvr=l(),VM=a("li"),sbe=a("strong"),kvr=o("roberta"),xvr=o(" \u2014 "),DO=a("a"),Rvr=o("FlaxRobertaForMaskedLM"),Svr=o(" (RoBERTa model)"),Pvr=l(),WM=a("li"),nbe=a("strong"),$vr=o("roformer"),Ivr=o(" \u2014 "),qO=a("a"),jvr=o("FlaxRoFormerForMaskedLM"),Nvr=o(" (RoFormer model)"),Dvr=l(),QM=a("li"),lbe=a("strong"),qvr=o("t5"),Gvr=o(" \u2014 "),GO=a("a"),Ovr=o("FlaxT5ForConditionalGeneration"),Xvr=o(" (T5 model)"),zvr=l(),HM=a("li"),ibe=a("strong"),Vvr=o("wav2vec2"),Wvr=o(" \u2014 "),OO=a("a"),Qvr=o("FlaxWav2Vec2ForPreTraining"),Hvr=o(" (Wav2Vec2 model)"),Uvr=l(),dbe=a("p"),Jvr=o("Examples:"),Yvr=l(),m(R0.$$.fragment),N9e=l(),Uc=a("h2"),UM=a("a"),cbe=a("span"),m(S0.$$.fragment),Kvr=l(),mbe=a("span"),Zvr=o("FlaxAutoModelForMaskedLM"),D9e=l(),Br=a("div"),m(P0.$$.fragment),eTr=l(),Jc=a("p"),oTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),fbe=a("code"),rTr=o("from_pretrained()"),tTr=o("class method or the "),gbe=a("code"),aTr=o("from_config()"),sTr=o(`class
method.`),nTr=l(),$0=a("p"),lTr=o("This class cannot be instantiated directly using "),hbe=a("code"),iTr=o("__init__()"),dTr=o(" (throws an error)."),cTr=l(),Ct=a("div"),m(I0.$$.fragment),mTr=l(),ube=a("p"),fTr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),gTr=l(),Yc=a("p"),hTr=o(`Note:
Loading a model from its configuration file does `),pbe=a("strong"),uTr=o("not"),pTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_be=a("code"),_Tr=o("from_pretrained()"),bTr=o("to load the model weights."),vTr=l(),bbe=a("p"),TTr=o("Examples:"),FTr=l(),m(j0.$$.fragment),CTr=l(),Bo=a("div"),m(N0.$$.fragment),MTr=l(),vbe=a("p"),ETr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),yTr=l(),Es=a("p"),wTr=o("The model class to instantiate is selected based on the "),Tbe=a("code"),ATr=o("model_type"),LTr=o(` property of the config object (either
passed as an argument or loaded from `),Fbe=a("code"),BTr=o("pretrained_model_name_or_path"),kTr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cbe=a("code"),xTr=o("pretrained_model_name_or_path"),RTr=o(":"),STr=l(),ve=a("ul"),JM=a("li"),Mbe=a("strong"),PTr=o("albert"),$Tr=o(" \u2014 "),XO=a("a"),ITr=o("FlaxAlbertForMaskedLM"),jTr=o(" (ALBERT model)"),NTr=l(),YM=a("li"),Ebe=a("strong"),DTr=o("bart"),qTr=o(" \u2014 "),zO=a("a"),GTr=o("FlaxBartForConditionalGeneration"),OTr=o(" (BART model)"),XTr=l(),KM=a("li"),ybe=a("strong"),zTr=o("bert"),VTr=o(" \u2014 "),VO=a("a"),WTr=o("FlaxBertForMaskedLM"),QTr=o(" (BERT model)"),HTr=l(),ZM=a("li"),wbe=a("strong"),UTr=o("big_bird"),JTr=o(" \u2014 "),WO=a("a"),YTr=o("FlaxBigBirdForMaskedLM"),KTr=o(" (BigBird model)"),ZTr=l(),eE=a("li"),Abe=a("strong"),e1r=o("distilbert"),o1r=o(" \u2014 "),QO=a("a"),r1r=o("FlaxDistilBertForMaskedLM"),t1r=o(" (DistilBERT model)"),a1r=l(),oE=a("li"),Lbe=a("strong"),s1r=o("electra"),n1r=o(" \u2014 "),HO=a("a"),l1r=o("FlaxElectraForMaskedLM"),i1r=o(" (ELECTRA model)"),d1r=l(),rE=a("li"),Bbe=a("strong"),c1r=o("mbart"),m1r=o(" \u2014 "),UO=a("a"),f1r=o("FlaxMBartForConditionalGeneration"),g1r=o(" (mBART model)"),h1r=l(),tE=a("li"),kbe=a("strong"),u1r=o("roberta"),p1r=o(" \u2014 "),JO=a("a"),_1r=o("FlaxRobertaForMaskedLM"),b1r=o(" (RoBERTa model)"),v1r=l(),aE=a("li"),xbe=a("strong"),T1r=o("roformer"),F1r=o(" \u2014 "),YO=a("a"),C1r=o("FlaxRoFormerForMaskedLM"),M1r=o(" (RoFormer model)"),E1r=l(),Rbe=a("p"),y1r=o("Examples:"),w1r=l(),m(D0.$$.fragment),q9e=l(),Kc=a("h2"),sE=a("a"),Sbe=a("span"),m(q0.$$.fragment),A1r=l(),Pbe=a("span"),L1r=o("FlaxAutoModelForSeq2SeqLM"),G9e=l(),kr=a("div"),m(G0.$$.fragment),B1r=l(),Zc=a("p"),k1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$be=a("code"),x1r=o("from_pretrained()"),R1r=o("class method or the "),Ibe=a("code"),S1r=o("from_config()"),P1r=o(`class
method.`),$1r=l(),O0=a("p"),I1r=o("This class cannot be instantiated directly using "),jbe=a("code"),j1r=o("__init__()"),N1r=o(" (throws an error)."),D1r=l(),Mt=a("div"),m(X0.$$.fragment),q1r=l(),Nbe=a("p"),G1r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),O1r=l(),em=a("p"),X1r=o(`Note:
Loading a model from its configuration file does `),Dbe=a("strong"),z1r=o("not"),V1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qbe=a("code"),W1r=o("from_pretrained()"),Q1r=o("to load the model weights."),H1r=l(),Gbe=a("p"),U1r=o("Examples:"),J1r=l(),m(z0.$$.fragment),Y1r=l(),ko=a("div"),m(V0.$$.fragment),K1r=l(),Obe=a("p"),Z1r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),eFr=l(),ys=a("p"),oFr=o("The model class to instantiate is selected based on the "),Xbe=a("code"),rFr=o("model_type"),tFr=o(` property of the config object (either
passed as an argument or loaded from `),zbe=a("code"),aFr=o("pretrained_model_name_or_path"),sFr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vbe=a("code"),nFr=o("pretrained_model_name_or_path"),lFr=o(":"),iFr=l(),Te=a("ul"),nE=a("li"),Wbe=a("strong"),dFr=o("bart"),cFr=o(" \u2014 "),KO=a("a"),mFr=o("FlaxBartForConditionalGeneration"),fFr=o(" (BART model)"),gFr=l(),lE=a("li"),Qbe=a("strong"),hFr=o("blenderbot"),uFr=o(" \u2014 "),ZO=a("a"),pFr=o("FlaxBlenderbotForConditionalGeneration"),_Fr=o(" (Blenderbot model)"),bFr=l(),iE=a("li"),Hbe=a("strong"),vFr=o("blenderbot-small"),TFr=o(" \u2014 "),eX=a("a"),FFr=o("FlaxBlenderbotSmallForConditionalGeneration"),CFr=o(" (BlenderbotSmall model)"),MFr=l(),dE=a("li"),Ube=a("strong"),EFr=o("encoder-decoder"),yFr=o(" \u2014 "),oX=a("a"),wFr=o("FlaxEncoderDecoderModel"),AFr=o(" (Encoder decoder model)"),LFr=l(),cE=a("li"),Jbe=a("strong"),BFr=o("marian"),kFr=o(" \u2014 "),rX=a("a"),xFr=o("FlaxMarianMTModel"),RFr=o(" (Marian model)"),SFr=l(),mE=a("li"),Ybe=a("strong"),PFr=o("mbart"),$Fr=o(" \u2014 "),tX=a("a"),IFr=o("FlaxMBartForConditionalGeneration"),jFr=o(" (mBART model)"),NFr=l(),fE=a("li"),Kbe=a("strong"),DFr=o("mt5"),qFr=o(" \u2014 "),aX=a("a"),GFr=o("FlaxMT5ForConditionalGeneration"),OFr=o(" (mT5 model)"),XFr=l(),gE=a("li"),Zbe=a("strong"),zFr=o("pegasus"),VFr=o(" \u2014 "),sX=a("a"),WFr=o("FlaxPegasusForConditionalGeneration"),QFr=o(" (Pegasus model)"),HFr=l(),hE=a("li"),e2e=a("strong"),UFr=o("t5"),JFr=o(" \u2014 "),nX=a("a"),YFr=o("FlaxT5ForConditionalGeneration"),KFr=o(" (T5 model)"),ZFr=l(),o2e=a("p"),eCr=o("Examples:"),oCr=l(),m(W0.$$.fragment),O9e=l(),om=a("h2"),uE=a("a"),r2e=a("span"),m(Q0.$$.fragment),rCr=l(),t2e=a("span"),tCr=o("FlaxAutoModelForSequenceClassification"),X9e=l(),xr=a("div"),m(H0.$$.fragment),aCr=l(),rm=a("p"),sCr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),a2e=a("code"),nCr=o("from_pretrained()"),lCr=o("class method or the "),s2e=a("code"),iCr=o("from_config()"),dCr=o(`class
method.`),cCr=l(),U0=a("p"),mCr=o("This class cannot be instantiated directly using "),n2e=a("code"),fCr=o("__init__()"),gCr=o(" (throws an error)."),hCr=l(),Et=a("div"),m(J0.$$.fragment),uCr=l(),l2e=a("p"),pCr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),_Cr=l(),tm=a("p"),bCr=o(`Note:
Loading a model from its configuration file does `),i2e=a("strong"),vCr=o("not"),TCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),d2e=a("code"),FCr=o("from_pretrained()"),CCr=o("to load the model weights."),MCr=l(),c2e=a("p"),ECr=o("Examples:"),yCr=l(),m(Y0.$$.fragment),wCr=l(),xo=a("div"),m(K0.$$.fragment),ACr=l(),m2e=a("p"),LCr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),BCr=l(),ws=a("p"),kCr=o("The model class to instantiate is selected based on the "),f2e=a("code"),xCr=o("model_type"),RCr=o(` property of the config object (either
passed as an argument or loaded from `),g2e=a("code"),SCr=o("pretrained_model_name_or_path"),PCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h2e=a("code"),$Cr=o("pretrained_model_name_or_path"),ICr=o(":"),jCr=l(),Fe=a("ul"),pE=a("li"),u2e=a("strong"),NCr=o("albert"),DCr=o(" \u2014 "),lX=a("a"),qCr=o("FlaxAlbertForSequenceClassification"),GCr=o(" (ALBERT model)"),OCr=l(),_E=a("li"),p2e=a("strong"),XCr=o("bart"),zCr=o(" \u2014 "),iX=a("a"),VCr=o("FlaxBartForSequenceClassification"),WCr=o(" (BART model)"),QCr=l(),bE=a("li"),_2e=a("strong"),HCr=o("bert"),UCr=o(" \u2014 "),dX=a("a"),JCr=o("FlaxBertForSequenceClassification"),YCr=o(" (BERT model)"),KCr=l(),vE=a("li"),b2e=a("strong"),ZCr=o("big_bird"),e4r=o(" \u2014 "),cX=a("a"),o4r=o("FlaxBigBirdForSequenceClassification"),r4r=o(" (BigBird model)"),t4r=l(),TE=a("li"),v2e=a("strong"),a4r=o("distilbert"),s4r=o(" \u2014 "),mX=a("a"),n4r=o("FlaxDistilBertForSequenceClassification"),l4r=o(" (DistilBERT model)"),i4r=l(),FE=a("li"),T2e=a("strong"),d4r=o("electra"),c4r=o(" \u2014 "),fX=a("a"),m4r=o("FlaxElectraForSequenceClassification"),f4r=o(" (ELECTRA model)"),g4r=l(),CE=a("li"),F2e=a("strong"),h4r=o("mbart"),u4r=o(" \u2014 "),gX=a("a"),p4r=o("FlaxMBartForSequenceClassification"),_4r=o(" (mBART model)"),b4r=l(),ME=a("li"),C2e=a("strong"),v4r=o("roberta"),T4r=o(" \u2014 "),hX=a("a"),F4r=o("FlaxRobertaForSequenceClassification"),C4r=o(" (RoBERTa model)"),M4r=l(),EE=a("li"),M2e=a("strong"),E4r=o("roformer"),y4r=o(" \u2014 "),uX=a("a"),w4r=o("FlaxRoFormerForSequenceClassification"),A4r=o(" (RoFormer model)"),L4r=l(),E2e=a("p"),B4r=o("Examples:"),k4r=l(),m(Z0.$$.fragment),z9e=l(),am=a("h2"),yE=a("a"),y2e=a("span"),m(eL.$$.fragment),x4r=l(),w2e=a("span"),R4r=o("FlaxAutoModelForQuestionAnswering"),V9e=l(),Rr=a("div"),m(oL.$$.fragment),S4r=l(),sm=a("p"),P4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A2e=a("code"),$4r=o("from_pretrained()"),I4r=o("class method or the "),L2e=a("code"),j4r=o("from_config()"),N4r=o(`class
method.`),D4r=l(),rL=a("p"),q4r=o("This class cannot be instantiated directly using "),B2e=a("code"),G4r=o("__init__()"),O4r=o(" (throws an error)."),X4r=l(),yt=a("div"),m(tL.$$.fragment),z4r=l(),k2e=a("p"),V4r=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),W4r=l(),nm=a("p"),Q4r=o(`Note:
Loading a model from its configuration file does `),x2e=a("strong"),H4r=o("not"),U4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),R2e=a("code"),J4r=o("from_pretrained()"),Y4r=o("to load the model weights."),K4r=l(),S2e=a("p"),Z4r=o("Examples:"),eMr=l(),m(aL.$$.fragment),oMr=l(),Ro=a("div"),m(sL.$$.fragment),rMr=l(),P2e=a("p"),tMr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),aMr=l(),As=a("p"),sMr=o("The model class to instantiate is selected based on the "),$2e=a("code"),nMr=o("model_type"),lMr=o(` property of the config object (either
passed as an argument or loaded from `),I2e=a("code"),iMr=o("pretrained_model_name_or_path"),dMr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j2e=a("code"),cMr=o("pretrained_model_name_or_path"),mMr=o(":"),fMr=l(),Ce=a("ul"),wE=a("li"),N2e=a("strong"),gMr=o("albert"),hMr=o(" \u2014 "),pX=a("a"),uMr=o("FlaxAlbertForQuestionAnswering"),pMr=o(" (ALBERT model)"),_Mr=l(),AE=a("li"),D2e=a("strong"),bMr=o("bart"),vMr=o(" \u2014 "),_X=a("a"),TMr=o("FlaxBartForQuestionAnswering"),FMr=o(" (BART model)"),CMr=l(),LE=a("li"),q2e=a("strong"),MMr=o("bert"),EMr=o(" \u2014 "),bX=a("a"),yMr=o("FlaxBertForQuestionAnswering"),wMr=o(" (BERT model)"),AMr=l(),BE=a("li"),G2e=a("strong"),LMr=o("big_bird"),BMr=o(" \u2014 "),vX=a("a"),kMr=o("FlaxBigBirdForQuestionAnswering"),xMr=o(" (BigBird model)"),RMr=l(),kE=a("li"),O2e=a("strong"),SMr=o("distilbert"),PMr=o(" \u2014 "),TX=a("a"),$Mr=o("FlaxDistilBertForQuestionAnswering"),IMr=o(" (DistilBERT model)"),jMr=l(),xE=a("li"),X2e=a("strong"),NMr=o("electra"),DMr=o(" \u2014 "),FX=a("a"),qMr=o("FlaxElectraForQuestionAnswering"),GMr=o(" (ELECTRA model)"),OMr=l(),RE=a("li"),z2e=a("strong"),XMr=o("mbart"),zMr=o(" \u2014 "),CX=a("a"),VMr=o("FlaxMBartForQuestionAnswering"),WMr=o(" (mBART model)"),QMr=l(),SE=a("li"),V2e=a("strong"),HMr=o("roberta"),UMr=o(" \u2014 "),MX=a("a"),JMr=o("FlaxRobertaForQuestionAnswering"),YMr=o(" (RoBERTa model)"),KMr=l(),PE=a("li"),W2e=a("strong"),ZMr=o("roformer"),eEr=o(" \u2014 "),EX=a("a"),oEr=o("FlaxRoFormerForQuestionAnswering"),rEr=o(" (RoFormer model)"),tEr=l(),Q2e=a("p"),aEr=o("Examples:"),sEr=l(),m(nL.$$.fragment),W9e=l(),lm=a("h2"),$E=a("a"),H2e=a("span"),m(lL.$$.fragment),nEr=l(),U2e=a("span"),lEr=o("FlaxAutoModelForTokenClassification"),Q9e=l(),Sr=a("div"),m(iL.$$.fragment),iEr=l(),im=a("p"),dEr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),J2e=a("code"),cEr=o("from_pretrained()"),mEr=o("class method or the "),Y2e=a("code"),fEr=o("from_config()"),gEr=o(`class
method.`),hEr=l(),dL=a("p"),uEr=o("This class cannot be instantiated directly using "),K2e=a("code"),pEr=o("__init__()"),_Er=o(" (throws an error)."),bEr=l(),wt=a("div"),m(cL.$$.fragment),vEr=l(),Z2e=a("p"),TEr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),FEr=l(),dm=a("p"),CEr=o(`Note:
Loading a model from its configuration file does `),eve=a("strong"),MEr=o("not"),EEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ove=a("code"),yEr=o("from_pretrained()"),wEr=o("to load the model weights."),AEr=l(),rve=a("p"),LEr=o("Examples:"),BEr=l(),m(mL.$$.fragment),kEr=l(),So=a("div"),m(fL.$$.fragment),xEr=l(),tve=a("p"),REr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),SEr=l(),Ls=a("p"),PEr=o("The model class to instantiate is selected based on the "),ave=a("code"),$Er=o("model_type"),IEr=o(` property of the config object (either
passed as an argument or loaded from `),sve=a("code"),jEr=o("pretrained_model_name_or_path"),NEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nve=a("code"),DEr=o("pretrained_model_name_or_path"),qEr=o(":"),GEr=l(),no=a("ul"),IE=a("li"),lve=a("strong"),OEr=o("albert"),XEr=o(" \u2014 "),yX=a("a"),zEr=o("FlaxAlbertForTokenClassification"),VEr=o(" (ALBERT model)"),WEr=l(),jE=a("li"),ive=a("strong"),QEr=o("bert"),HEr=o(" \u2014 "),wX=a("a"),UEr=o("FlaxBertForTokenClassification"),JEr=o(" (BERT model)"),YEr=l(),NE=a("li"),dve=a("strong"),KEr=o("big_bird"),ZEr=o(" \u2014 "),AX=a("a"),e3r=o("FlaxBigBirdForTokenClassification"),o3r=o(" (BigBird model)"),r3r=l(),DE=a("li"),cve=a("strong"),t3r=o("distilbert"),a3r=o(" \u2014 "),LX=a("a"),s3r=o("FlaxDistilBertForTokenClassification"),n3r=o(" (DistilBERT model)"),l3r=l(),qE=a("li"),mve=a("strong"),i3r=o("electra"),d3r=o(" \u2014 "),BX=a("a"),c3r=o("FlaxElectraForTokenClassification"),m3r=o(" (ELECTRA model)"),f3r=l(),GE=a("li"),fve=a("strong"),g3r=o("roberta"),h3r=o(" \u2014 "),kX=a("a"),u3r=o("FlaxRobertaForTokenClassification"),p3r=o(" (RoBERTa model)"),_3r=l(),OE=a("li"),gve=a("strong"),b3r=o("roformer"),v3r=o(" \u2014 "),xX=a("a"),T3r=o("FlaxRoFormerForTokenClassification"),F3r=o(" (RoFormer model)"),C3r=l(),hve=a("p"),M3r=o("Examples:"),E3r=l(),m(gL.$$.fragment),H9e=l(),cm=a("h2"),XE=a("a"),uve=a("span"),m(hL.$$.fragment),y3r=l(),pve=a("span"),w3r=o("FlaxAutoModelForMultipleChoice"),U9e=l(),Pr=a("div"),m(uL.$$.fragment),A3r=l(),mm=a("p"),L3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),_ve=a("code"),B3r=o("from_pretrained()"),k3r=o("class method or the "),bve=a("code"),x3r=o("from_config()"),R3r=o(`class
method.`),S3r=l(),pL=a("p"),P3r=o("This class cannot be instantiated directly using "),vve=a("code"),$3r=o("__init__()"),I3r=o(" (throws an error)."),j3r=l(),At=a("div"),m(_L.$$.fragment),N3r=l(),Tve=a("p"),D3r=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),q3r=l(),fm=a("p"),G3r=o(`Note:
Loading a model from its configuration file does `),Fve=a("strong"),O3r=o("not"),X3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cve=a("code"),z3r=o("from_pretrained()"),V3r=o("to load the model weights."),W3r=l(),Mve=a("p"),Q3r=o("Examples:"),H3r=l(),m(bL.$$.fragment),U3r=l(),Po=a("div"),m(vL.$$.fragment),J3r=l(),Eve=a("p"),Y3r=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),K3r=l(),Bs=a("p"),Z3r=o("The model class to instantiate is selected based on the "),yve=a("code"),e5r=o("model_type"),o5r=o(` property of the config object (either
passed as an argument or loaded from `),wve=a("code"),r5r=o("pretrained_model_name_or_path"),t5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ave=a("code"),a5r=o("pretrained_model_name_or_path"),s5r=o(":"),n5r=l(),lo=a("ul"),zE=a("li"),Lve=a("strong"),l5r=o("albert"),i5r=o(" \u2014 "),RX=a("a"),d5r=o("FlaxAlbertForMultipleChoice"),c5r=o(" (ALBERT model)"),m5r=l(),VE=a("li"),Bve=a("strong"),f5r=o("bert"),g5r=o(" \u2014 "),SX=a("a"),h5r=o("FlaxBertForMultipleChoice"),u5r=o(" (BERT model)"),p5r=l(),WE=a("li"),kve=a("strong"),_5r=o("big_bird"),b5r=o(" \u2014 "),PX=a("a"),v5r=o("FlaxBigBirdForMultipleChoice"),T5r=o(" (BigBird model)"),F5r=l(),QE=a("li"),xve=a("strong"),C5r=o("distilbert"),M5r=o(" \u2014 "),$X=a("a"),E5r=o("FlaxDistilBertForMultipleChoice"),y5r=o(" (DistilBERT model)"),w5r=l(),HE=a("li"),Rve=a("strong"),A5r=o("electra"),L5r=o(" \u2014 "),IX=a("a"),B5r=o("FlaxElectraForMultipleChoice"),k5r=o(" (ELECTRA model)"),x5r=l(),UE=a("li"),Sve=a("strong"),R5r=o("roberta"),S5r=o(" \u2014 "),jX=a("a"),P5r=o("FlaxRobertaForMultipleChoice"),$5r=o(" (RoBERTa model)"),I5r=l(),JE=a("li"),Pve=a("strong"),j5r=o("roformer"),N5r=o(" \u2014 "),NX=a("a"),D5r=o("FlaxRoFormerForMultipleChoice"),q5r=o(" (RoFormer model)"),G5r=l(),$ve=a("p"),O5r=o("Examples:"),X5r=l(),m(TL.$$.fragment),J9e=l(),gm=a("h2"),YE=a("a"),Ive=a("span"),m(FL.$$.fragment),z5r=l(),jve=a("span"),V5r=o("FlaxAutoModelForNextSentencePrediction"),Y9e=l(),$r=a("div"),m(CL.$$.fragment),W5r=l(),hm=a("p"),Q5r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Nve=a("code"),H5r=o("from_pretrained()"),U5r=o("class method or the "),Dve=a("code"),J5r=o("from_config()"),Y5r=o(`class
method.`),K5r=l(),ML=a("p"),Z5r=o("This class cannot be instantiated directly using "),qve=a("code"),eyr=o("__init__()"),oyr=o(" (throws an error)."),ryr=l(),Lt=a("div"),m(EL.$$.fragment),tyr=l(),Gve=a("p"),ayr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),syr=l(),um=a("p"),nyr=o(`Note:
Loading a model from its configuration file does `),Ove=a("strong"),lyr=o("not"),iyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xve=a("code"),dyr=o("from_pretrained()"),cyr=o("to load the model weights."),myr=l(),zve=a("p"),fyr=o("Examples:"),gyr=l(),m(yL.$$.fragment),hyr=l(),$o=a("div"),m(wL.$$.fragment),uyr=l(),Vve=a("p"),pyr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),_yr=l(),ks=a("p"),byr=o("The model class to instantiate is selected based on the "),Wve=a("code"),vyr=o("model_type"),Tyr=o(` property of the config object (either
passed as an argument or loaded from `),Qve=a("code"),Fyr=o("pretrained_model_name_or_path"),Cyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hve=a("code"),Myr=o("pretrained_model_name_or_path"),Eyr=o(":"),yyr=l(),Uve=a("ul"),KE=a("li"),Jve=a("strong"),wyr=o("bert"),Ayr=o(" \u2014 "),DX=a("a"),Lyr=o("FlaxBertForNextSentencePrediction"),Byr=o(" (BERT model)"),kyr=l(),Yve=a("p"),xyr=o("Examples:"),Ryr=l(),m(AL.$$.fragment),K9e=l(),pm=a("h2"),ZE=a("a"),Kve=a("span"),m(LL.$$.fragment),Syr=l(),Zve=a("span"),Pyr=o("FlaxAutoModelForImageClassification"),Z9e=l(),Ir=a("div"),m(BL.$$.fragment),$yr=l(),_m=a("p"),Iyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),eTe=a("code"),jyr=o("from_pretrained()"),Nyr=o("class method or the "),oTe=a("code"),Dyr=o("from_config()"),qyr=o(`class
method.`),Gyr=l(),kL=a("p"),Oyr=o("This class cannot be instantiated directly using "),rTe=a("code"),Xyr=o("__init__()"),zyr=o(" (throws an error)."),Vyr=l(),Bt=a("div"),m(xL.$$.fragment),Wyr=l(),tTe=a("p"),Qyr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Hyr=l(),bm=a("p"),Uyr=o(`Note:
Loading a model from its configuration file does `),aTe=a("strong"),Jyr=o("not"),Yyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sTe=a("code"),Kyr=o("from_pretrained()"),Zyr=o("to load the model weights."),ewr=l(),nTe=a("p"),owr=o("Examples:"),rwr=l(),m(RL.$$.fragment),twr=l(),Io=a("div"),m(SL.$$.fragment),awr=l(),lTe=a("p"),swr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),nwr=l(),xs=a("p"),lwr=o("The model class to instantiate is selected based on the "),iTe=a("code"),iwr=o("model_type"),dwr=o(` property of the config object (either
passed as an argument or loaded from `),dTe=a("code"),cwr=o("pretrained_model_name_or_path"),mwr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cTe=a("code"),fwr=o("pretrained_model_name_or_path"),gwr=o(":"),hwr=l(),PL=a("ul"),e3=a("li"),mTe=a("strong"),uwr=o("beit"),pwr=o(" \u2014 "),qX=a("a"),_wr=o("FlaxBeitForImageClassification"),bwr=o(" (BEiT model)"),vwr=l(),o3=a("li"),fTe=a("strong"),Twr=o("vit"),Fwr=o(" \u2014 "),GX=a("a"),Cwr=o("FlaxViTForImageClassification"),Mwr=o(" (ViT model)"),Ewr=l(),gTe=a("p"),ywr=o("Examples:"),wwr=l(),m($L.$$.fragment),eBe=l(),vm=a("h2"),r3=a("a"),hTe=a("span"),m(IL.$$.fragment),Awr=l(),uTe=a("span"),Lwr=o("FlaxAutoModelForVision2Seq"),oBe=l(),jr=a("div"),m(jL.$$.fragment),Bwr=l(),Tm=a("p"),kwr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),pTe=a("code"),xwr=o("from_pretrained()"),Rwr=o("class method or the "),_Te=a("code"),Swr=o("from_config()"),Pwr=o(`class
method.`),$wr=l(),NL=a("p"),Iwr=o("This class cannot be instantiated directly using "),bTe=a("code"),jwr=o("__init__()"),Nwr=o(" (throws an error)."),Dwr=l(),kt=a("div"),m(DL.$$.fragment),qwr=l(),vTe=a("p"),Gwr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Owr=l(),Fm=a("p"),Xwr=o(`Note:
Loading a model from its configuration file does `),TTe=a("strong"),zwr=o("not"),Vwr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),FTe=a("code"),Wwr=o("from_pretrained()"),Qwr=o("to load the model weights."),Hwr=l(),CTe=a("p"),Uwr=o("Examples:"),Jwr=l(),m(qL.$$.fragment),Ywr=l(),jo=a("div"),m(GL.$$.fragment),Kwr=l(),MTe=a("p"),Zwr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),eAr=l(),Rs=a("p"),oAr=o("The model class to instantiate is selected based on the "),ETe=a("code"),rAr=o("model_type"),tAr=o(` property of the config object (either
passed as an argument or loaded from `),yTe=a("code"),aAr=o("pretrained_model_name_or_path"),sAr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wTe=a("code"),nAr=o("pretrained_model_name_or_path"),lAr=o(":"),iAr=l(),ATe=a("ul"),t3=a("li"),LTe=a("strong"),dAr=o("vision-encoder-decoder"),cAr=o(" \u2014 "),OX=a("a"),mAr=o("FlaxVisionEncoderDecoderModel"),fAr=o(" (Vision Encoder decoder model)"),gAr=l(),BTe=a("p"),hAr=o("Examples:"),uAr=l(),m(OL.$$.fragment),this.h()},l(d){const _=Tpt('[data-svelte="svelte-1phssyn"]',document.head);J=s(_,"META",{name:!0,content:!0}),_.forEach(t),Ae=i(d),ie=s(d,"H1",{class:!0});var XL=n(ie);fe=s(XL,"A",{id:!0,class:!0,href:!0});var kTe=n(fe);to=s(kTe,"SPAN",{});var xTe=n(to);f(ce.$$.fragment,xTe),xTe.forEach(t),kTe.forEach(t),_e=i(XL),Do=s(XL,"SPAN",{});var _Ar=n(Do);wi=r(_Ar,"Auto Classes"),_Ar.forEach(t),XL.forEach(t),Mm=i(d),na=s(d,"P",{});var tBe=n(na);Ai=r(tBe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Li=s(tBe,"CODE",{});var bAr=n(Li);o5=r(bAr,"from_pretrained()"),bAr.forEach(t),Em=r(tBe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),tBe.forEach(t),ye=i(d),io=s(d,"P",{});var a3=n(io);Bi=r(a3,"Instantiating one of "),Ss=s(a3,"A",{href:!0});var vAr=n(Ss);r5=r(vAr,"AutoConfig"),vAr.forEach(t),Ps=r(a3,", "),$s=s(a3,"A",{href:!0});var TAr=n($s);t5=r(TAr,"AutoModel"),TAr.forEach(t),ki=r(a3,`, and
`),Is=s(a3,"A",{href:!0});var FAr=n(Is);a5=r(FAr,"AutoTokenizer"),FAr.forEach(t),xi=r(a3," will directly create a class of the relevant architecture. For instance"),a3.forEach(t),ym=i(d),f($a.$$.fragment,d),co=i(d),ge=s(d,"P",{});var aBe=n(ge);D7=r(aBe,"will create a model that is an instance of "),Ri=s(aBe,"A",{href:!0});var CAr=n(Ri);q7=r(CAr,"BertModel"),CAr.forEach(t),G7=r(aBe,"."),aBe.forEach(t),qo=i(d),Ia=s(d,"P",{});var sBe=n(Ia);O7=r(sBe,"There is one class of "),wm=s(sBe,"CODE",{});var MAr=n(wm);X7=r(MAr,"AutoModel"),MAr.forEach(t),fxe=r(sBe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),sBe.forEach(t),t8e=i(d),Si=s(d,"H2",{class:!0});var nBe=n(Si);Am=s(nBe,"A",{id:!0,class:!0,href:!0});var EAr=n(Am);$V=s(EAr,"SPAN",{});var yAr=n($V);f(s5.$$.fragment,yAr),yAr.forEach(t),EAr.forEach(t),gxe=i(nBe),IV=s(nBe,"SPAN",{});var wAr=n(IV);hxe=r(wAr,"Extending the Auto Classes"),wAr.forEach(t),nBe.forEach(t),a8e=i(d),js=s(d,"P",{});var XX=n(js);uxe=r(XX,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),jV=s(XX,"CODE",{});var AAr=n(jV);pxe=r(AAr,"NewModel"),AAr.forEach(t),_xe=r(XX,", make sure you have a "),NV=s(XX,"CODE",{});var LAr=n(NV);bxe=r(LAr,"NewModelConfig"),LAr.forEach(t),vxe=r(XX,` then you can add those to the auto
classes like this:`),XX.forEach(t),s8e=i(d),f(n5.$$.fragment,d),n8e=i(d),z7=s(d,"P",{});var BAr=n(z7);Txe=r(BAr,"You will then be able to use the auto classes like you would usually do!"),BAr.forEach(t),l8e=i(d),f(Lm.$$.fragment,d),i8e=i(d),Pi=s(d,"H2",{class:!0});var lBe=n(Pi);Bm=s(lBe,"A",{id:!0,class:!0,href:!0});var kAr=n(Bm);DV=s(kAr,"SPAN",{});var xAr=n(DV);f(l5.$$.fragment,xAr),xAr.forEach(t),kAr.forEach(t),Fxe=i(lBe),qV=s(lBe,"SPAN",{});var RAr=n(qV);Cxe=r(RAr,"AutoConfig"),RAr.forEach(t),lBe.forEach(t),d8e=i(d),Go=s(d,"DIV",{class:!0});var Pn=n(Go);f(i5.$$.fragment,Pn),Mxe=i(Pn),d5=s(Pn,"P",{});var iBe=n(d5);Exe=r(iBe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),V7=s(iBe,"A",{href:!0});var SAr=n(V7);yxe=r(SAr,"from_pretrained()"),SAr.forEach(t),wxe=r(iBe," class method."),iBe.forEach(t),Axe=i(Pn),c5=s(Pn,"P",{});var dBe=n(c5);Lxe=r(dBe,"This class cannot be instantiated directly using "),GV=s(dBe,"CODE",{});var PAr=n(GV);Bxe=r(PAr,"__init__()"),PAr.forEach(t),kxe=r(dBe," (throws an error)."),dBe.forEach(t),xxe=i(Pn),mo=s(Pn,"DIV",{class:!0});var ia=n(mo);f(m5.$$.fragment,ia),Rxe=i(ia),OV=s(ia,"P",{});var $Ar=n(OV);Sxe=r($Ar,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),$Ar.forEach(t),Pxe=i(ia),$i=s(ia,"P",{});var zX=n($i);$xe=r(zX,"The configuration class to instantiate is selected based on the "),XV=s(zX,"CODE",{});var IAr=n(XV);Ixe=r(IAr,"model_type"),IAr.forEach(t),jxe=r(zX,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),zV=s(zX,"CODE",{});var jAr=n(zV);Nxe=r(jAr,"pretrained_model_name_or_path"),jAr.forEach(t),Dxe=r(zX,":"),zX.forEach(t),qxe=i(ia),v=s(ia,"UL",{});var T=n(v);km=s(T,"LI",{});var RTe=n(km);VV=s(RTe,"STRONG",{});var NAr=n(VV);Gxe=r(NAr,"albert"),NAr.forEach(t),Oxe=r(RTe," \u2014 "),W7=s(RTe,"A",{href:!0});var DAr=n(W7);Xxe=r(DAr,"AlbertConfig"),DAr.forEach(t),zxe=r(RTe," (ALBERT model)"),RTe.forEach(t),Vxe=i(T),xm=s(T,"LI",{});var STe=n(xm);WV=s(STe,"STRONG",{});var qAr=n(WV);Wxe=r(qAr,"bart"),qAr.forEach(t),Qxe=r(STe," \u2014 "),Q7=s(STe,"A",{href:!0});var GAr=n(Q7);Hxe=r(GAr,"BartConfig"),GAr.forEach(t),Uxe=r(STe," (BART model)"),STe.forEach(t),Jxe=i(T),Rm=s(T,"LI",{});var PTe=n(Rm);QV=s(PTe,"STRONG",{});var OAr=n(QV);Yxe=r(OAr,"beit"),OAr.forEach(t),Kxe=r(PTe," \u2014 "),H7=s(PTe,"A",{href:!0});var XAr=n(H7);Zxe=r(XAr,"BeitConfig"),XAr.forEach(t),eRe=r(PTe," (BEiT model)"),PTe.forEach(t),oRe=i(T),Sm=s(T,"LI",{});var $Te=n(Sm);HV=s($Te,"STRONG",{});var zAr=n(HV);rRe=r(zAr,"bert"),zAr.forEach(t),tRe=r($Te," \u2014 "),U7=s($Te,"A",{href:!0});var VAr=n(U7);aRe=r(VAr,"BertConfig"),VAr.forEach(t),sRe=r($Te," (BERT model)"),$Te.forEach(t),nRe=i(T),Pm=s(T,"LI",{});var ITe=n(Pm);UV=s(ITe,"STRONG",{});var WAr=n(UV);lRe=r(WAr,"bert-generation"),WAr.forEach(t),iRe=r(ITe," \u2014 "),J7=s(ITe,"A",{href:!0});var QAr=n(J7);dRe=r(QAr,"BertGenerationConfig"),QAr.forEach(t),cRe=r(ITe," (Bert Generation model)"),ITe.forEach(t),mRe=i(T),$m=s(T,"LI",{});var jTe=n($m);JV=s(jTe,"STRONG",{});var HAr=n(JV);fRe=r(HAr,"big_bird"),HAr.forEach(t),gRe=r(jTe," \u2014 "),Y7=s(jTe,"A",{href:!0});var UAr=n(Y7);hRe=r(UAr,"BigBirdConfig"),UAr.forEach(t),uRe=r(jTe," (BigBird model)"),jTe.forEach(t),pRe=i(T),Im=s(T,"LI",{});var NTe=n(Im);YV=s(NTe,"STRONG",{});var JAr=n(YV);_Re=r(JAr,"bigbird_pegasus"),JAr.forEach(t),bRe=r(NTe," \u2014 "),K7=s(NTe,"A",{href:!0});var YAr=n(K7);vRe=r(YAr,"BigBirdPegasusConfig"),YAr.forEach(t),TRe=r(NTe," (BigBirdPegasus model)"),NTe.forEach(t),FRe=i(T),jm=s(T,"LI",{});var DTe=n(jm);KV=s(DTe,"STRONG",{});var KAr=n(KV);CRe=r(KAr,"blenderbot"),KAr.forEach(t),MRe=r(DTe," \u2014 "),Z7=s(DTe,"A",{href:!0});var ZAr=n(Z7);ERe=r(ZAr,"BlenderbotConfig"),ZAr.forEach(t),yRe=r(DTe," (Blenderbot model)"),DTe.forEach(t),wRe=i(T),Nm=s(T,"LI",{});var qTe=n(Nm);ZV=s(qTe,"STRONG",{});var e6r=n(ZV);ARe=r(e6r,"blenderbot-small"),e6r.forEach(t),LRe=r(qTe," \u2014 "),e8=s(qTe,"A",{href:!0});var o6r=n(e8);BRe=r(o6r,"BlenderbotSmallConfig"),o6r.forEach(t),kRe=r(qTe," (BlenderbotSmall model)"),qTe.forEach(t),xRe=i(T),Dm=s(T,"LI",{});var GTe=n(Dm);eW=s(GTe,"STRONG",{});var r6r=n(eW);RRe=r(r6r,"camembert"),r6r.forEach(t),SRe=r(GTe," \u2014 "),o8=s(GTe,"A",{href:!0});var t6r=n(o8);PRe=r(t6r,"CamembertConfig"),t6r.forEach(t),$Re=r(GTe," (CamemBERT model)"),GTe.forEach(t),IRe=i(T),qm=s(T,"LI",{});var OTe=n(qm);oW=s(OTe,"STRONG",{});var a6r=n(oW);jRe=r(a6r,"canine"),a6r.forEach(t),NRe=r(OTe," \u2014 "),r8=s(OTe,"A",{href:!0});var s6r=n(r8);DRe=r(s6r,"CanineConfig"),s6r.forEach(t),qRe=r(OTe," (Canine model)"),OTe.forEach(t),GRe=i(T),Gm=s(T,"LI",{});var XTe=n(Gm);rW=s(XTe,"STRONG",{});var n6r=n(rW);ORe=r(n6r,"clip"),n6r.forEach(t),XRe=r(XTe," \u2014 "),t8=s(XTe,"A",{href:!0});var l6r=n(t8);zRe=r(l6r,"CLIPConfig"),l6r.forEach(t),VRe=r(XTe," (CLIP model)"),XTe.forEach(t),WRe=i(T),Om=s(T,"LI",{});var zTe=n(Om);tW=s(zTe,"STRONG",{});var i6r=n(tW);QRe=r(i6r,"convbert"),i6r.forEach(t),HRe=r(zTe," \u2014 "),a8=s(zTe,"A",{href:!0});var d6r=n(a8);URe=r(d6r,"ConvBertConfig"),d6r.forEach(t),JRe=r(zTe," (ConvBERT model)"),zTe.forEach(t),YRe=i(T),Xm=s(T,"LI",{});var VTe=n(Xm);aW=s(VTe,"STRONG",{});var c6r=n(aW);KRe=r(c6r,"convnext"),c6r.forEach(t),ZRe=r(VTe," \u2014 "),s8=s(VTe,"A",{href:!0});var m6r=n(s8);eSe=r(m6r,"ConvNextConfig"),m6r.forEach(t),oSe=r(VTe," (ConvNext model)"),VTe.forEach(t),rSe=i(T),zm=s(T,"LI",{});var WTe=n(zm);sW=s(WTe,"STRONG",{});var f6r=n(sW);tSe=r(f6r,"ctrl"),f6r.forEach(t),aSe=r(WTe," \u2014 "),n8=s(WTe,"A",{href:!0});var g6r=n(n8);sSe=r(g6r,"CTRLConfig"),g6r.forEach(t),nSe=r(WTe," (CTRL model)"),WTe.forEach(t),lSe=i(T),Vm=s(T,"LI",{});var QTe=n(Vm);nW=s(QTe,"STRONG",{});var h6r=n(nW);iSe=r(h6r,"deberta"),h6r.forEach(t),dSe=r(QTe," \u2014 "),l8=s(QTe,"A",{href:!0});var u6r=n(l8);cSe=r(u6r,"DebertaConfig"),u6r.forEach(t),mSe=r(QTe," (DeBERTa model)"),QTe.forEach(t),fSe=i(T),Wm=s(T,"LI",{});var HTe=n(Wm);lW=s(HTe,"STRONG",{});var p6r=n(lW);gSe=r(p6r,"deberta-v2"),p6r.forEach(t),hSe=r(HTe," \u2014 "),i8=s(HTe,"A",{href:!0});var _6r=n(i8);uSe=r(_6r,"DebertaV2Config"),_6r.forEach(t),pSe=r(HTe," (DeBERTa-v2 model)"),HTe.forEach(t),_Se=i(T),Qm=s(T,"LI",{});var UTe=n(Qm);iW=s(UTe,"STRONG",{});var b6r=n(iW);bSe=r(b6r,"deit"),b6r.forEach(t),vSe=r(UTe," \u2014 "),d8=s(UTe,"A",{href:!0});var v6r=n(d8);TSe=r(v6r,"DeiTConfig"),v6r.forEach(t),FSe=r(UTe," (DeiT model)"),UTe.forEach(t),CSe=i(T),Hm=s(T,"LI",{});var JTe=n(Hm);dW=s(JTe,"STRONG",{});var T6r=n(dW);MSe=r(T6r,"detr"),T6r.forEach(t),ESe=r(JTe," \u2014 "),c8=s(JTe,"A",{href:!0});var F6r=n(c8);ySe=r(F6r,"DetrConfig"),F6r.forEach(t),wSe=r(JTe," (DETR model)"),JTe.forEach(t),ASe=i(T),Um=s(T,"LI",{});var YTe=n(Um);cW=s(YTe,"STRONG",{});var C6r=n(cW);LSe=r(C6r,"distilbert"),C6r.forEach(t),BSe=r(YTe," \u2014 "),m8=s(YTe,"A",{href:!0});var M6r=n(m8);kSe=r(M6r,"DistilBertConfig"),M6r.forEach(t),xSe=r(YTe," (DistilBERT model)"),YTe.forEach(t),RSe=i(T),Jm=s(T,"LI",{});var KTe=n(Jm);mW=s(KTe,"STRONG",{});var E6r=n(mW);SSe=r(E6r,"dpr"),E6r.forEach(t),PSe=r(KTe," \u2014 "),f8=s(KTe,"A",{href:!0});var y6r=n(f8);$Se=r(y6r,"DPRConfig"),y6r.forEach(t),ISe=r(KTe," (DPR model)"),KTe.forEach(t),jSe=i(T),Ym=s(T,"LI",{});var ZTe=n(Ym);fW=s(ZTe,"STRONG",{});var w6r=n(fW);NSe=r(w6r,"electra"),w6r.forEach(t),DSe=r(ZTe," \u2014 "),g8=s(ZTe,"A",{href:!0});var A6r=n(g8);qSe=r(A6r,"ElectraConfig"),A6r.forEach(t),GSe=r(ZTe," (ELECTRA model)"),ZTe.forEach(t),OSe=i(T),Km=s(T,"LI",{});var e1e=n(Km);gW=s(e1e,"STRONG",{});var L6r=n(gW);XSe=r(L6r,"encoder-decoder"),L6r.forEach(t),zSe=r(e1e," \u2014 "),h8=s(e1e,"A",{href:!0});var B6r=n(h8);VSe=r(B6r,"EncoderDecoderConfig"),B6r.forEach(t),WSe=r(e1e," (Encoder decoder model)"),e1e.forEach(t),QSe=i(T),Zm=s(T,"LI",{});var o1e=n(Zm);hW=s(o1e,"STRONG",{});var k6r=n(hW);HSe=r(k6r,"flaubert"),k6r.forEach(t),USe=r(o1e," \u2014 "),u8=s(o1e,"A",{href:!0});var x6r=n(u8);JSe=r(x6r,"FlaubertConfig"),x6r.forEach(t),YSe=r(o1e," (FlauBERT model)"),o1e.forEach(t),KSe=i(T),ef=s(T,"LI",{});var r1e=n(ef);uW=s(r1e,"STRONG",{});var R6r=n(uW);ZSe=r(R6r,"fnet"),R6r.forEach(t),ePe=r(r1e," \u2014 "),p8=s(r1e,"A",{href:!0});var S6r=n(p8);oPe=r(S6r,"FNetConfig"),S6r.forEach(t),rPe=r(r1e," (FNet model)"),r1e.forEach(t),tPe=i(T),of=s(T,"LI",{});var t1e=n(of);pW=s(t1e,"STRONG",{});var P6r=n(pW);aPe=r(P6r,"fsmt"),P6r.forEach(t),sPe=r(t1e," \u2014 "),_8=s(t1e,"A",{href:!0});var $6r=n(_8);nPe=r($6r,"FSMTConfig"),$6r.forEach(t),lPe=r(t1e," (FairSeq Machine-Translation model)"),t1e.forEach(t),iPe=i(T),rf=s(T,"LI",{});var a1e=n(rf);_W=s(a1e,"STRONG",{});var I6r=n(_W);dPe=r(I6r,"funnel"),I6r.forEach(t),cPe=r(a1e," \u2014 "),b8=s(a1e,"A",{href:!0});var j6r=n(b8);mPe=r(j6r,"FunnelConfig"),j6r.forEach(t),fPe=r(a1e," (Funnel Transformer model)"),a1e.forEach(t),gPe=i(T),tf=s(T,"LI",{});var s1e=n(tf);bW=s(s1e,"STRONG",{});var N6r=n(bW);hPe=r(N6r,"gpt2"),N6r.forEach(t),uPe=r(s1e," \u2014 "),v8=s(s1e,"A",{href:!0});var D6r=n(v8);pPe=r(D6r,"GPT2Config"),D6r.forEach(t),_Pe=r(s1e," (OpenAI GPT-2 model)"),s1e.forEach(t),bPe=i(T),af=s(T,"LI",{});var n1e=n(af);vW=s(n1e,"STRONG",{});var q6r=n(vW);vPe=r(q6r,"gpt_neo"),q6r.forEach(t),TPe=r(n1e," \u2014 "),T8=s(n1e,"A",{href:!0});var G6r=n(T8);FPe=r(G6r,"GPTNeoConfig"),G6r.forEach(t),CPe=r(n1e," (GPT Neo model)"),n1e.forEach(t),MPe=i(T),sf=s(T,"LI",{});var l1e=n(sf);TW=s(l1e,"STRONG",{});var O6r=n(TW);EPe=r(O6r,"gptj"),O6r.forEach(t),yPe=r(l1e," \u2014 "),F8=s(l1e,"A",{href:!0});var X6r=n(F8);wPe=r(X6r,"GPTJConfig"),X6r.forEach(t),APe=r(l1e," (GPT-J model)"),l1e.forEach(t),LPe=i(T),nf=s(T,"LI",{});var i1e=n(nf);FW=s(i1e,"STRONG",{});var z6r=n(FW);BPe=r(z6r,"hubert"),z6r.forEach(t),kPe=r(i1e," \u2014 "),C8=s(i1e,"A",{href:!0});var V6r=n(C8);xPe=r(V6r,"HubertConfig"),V6r.forEach(t),RPe=r(i1e," (Hubert model)"),i1e.forEach(t),SPe=i(T),lf=s(T,"LI",{});var d1e=n(lf);CW=s(d1e,"STRONG",{});var W6r=n(CW);PPe=r(W6r,"ibert"),W6r.forEach(t),$Pe=r(d1e," \u2014 "),M8=s(d1e,"A",{href:!0});var Q6r=n(M8);IPe=r(Q6r,"IBertConfig"),Q6r.forEach(t),jPe=r(d1e," (I-BERT model)"),d1e.forEach(t),NPe=i(T),df=s(T,"LI",{});var c1e=n(df);MW=s(c1e,"STRONG",{});var H6r=n(MW);DPe=r(H6r,"imagegpt"),H6r.forEach(t),qPe=r(c1e," \u2014 "),E8=s(c1e,"A",{href:!0});var U6r=n(E8);GPe=r(U6r,"ImageGPTConfig"),U6r.forEach(t),OPe=r(c1e," (ImageGPT model)"),c1e.forEach(t),XPe=i(T),cf=s(T,"LI",{});var m1e=n(cf);EW=s(m1e,"STRONG",{});var J6r=n(EW);zPe=r(J6r,"layoutlm"),J6r.forEach(t),VPe=r(m1e," \u2014 "),y8=s(m1e,"A",{href:!0});var Y6r=n(y8);WPe=r(Y6r,"LayoutLMConfig"),Y6r.forEach(t),QPe=r(m1e," (LayoutLM model)"),m1e.forEach(t),HPe=i(T),mf=s(T,"LI",{});var f1e=n(mf);yW=s(f1e,"STRONG",{});var K6r=n(yW);UPe=r(K6r,"layoutlmv2"),K6r.forEach(t),JPe=r(f1e," \u2014 "),w8=s(f1e,"A",{href:!0});var Z6r=n(w8);YPe=r(Z6r,"LayoutLMv2Config"),Z6r.forEach(t),KPe=r(f1e," (LayoutLMv2 model)"),f1e.forEach(t),ZPe=i(T),ff=s(T,"LI",{});var g1e=n(ff);wW=s(g1e,"STRONG",{});var e0r=n(wW);e$e=r(e0r,"led"),e0r.forEach(t),o$e=r(g1e," \u2014 "),A8=s(g1e,"A",{href:!0});var o0r=n(A8);r$e=r(o0r,"LEDConfig"),o0r.forEach(t),t$e=r(g1e," (LED model)"),g1e.forEach(t),a$e=i(T),gf=s(T,"LI",{});var h1e=n(gf);AW=s(h1e,"STRONG",{});var r0r=n(AW);s$e=r(r0r,"longformer"),r0r.forEach(t),n$e=r(h1e," \u2014 "),L8=s(h1e,"A",{href:!0});var t0r=n(L8);l$e=r(t0r,"LongformerConfig"),t0r.forEach(t),i$e=r(h1e," (Longformer model)"),h1e.forEach(t),d$e=i(T),hf=s(T,"LI",{});var u1e=n(hf);LW=s(u1e,"STRONG",{});var a0r=n(LW);c$e=r(a0r,"luke"),a0r.forEach(t),m$e=r(u1e," \u2014 "),B8=s(u1e,"A",{href:!0});var s0r=n(B8);f$e=r(s0r,"LukeConfig"),s0r.forEach(t),g$e=r(u1e," (LUKE model)"),u1e.forEach(t),h$e=i(T),uf=s(T,"LI",{});var p1e=n(uf);BW=s(p1e,"STRONG",{});var n0r=n(BW);u$e=r(n0r,"lxmert"),n0r.forEach(t),p$e=r(p1e," \u2014 "),k8=s(p1e,"A",{href:!0});var l0r=n(k8);_$e=r(l0r,"LxmertConfig"),l0r.forEach(t),b$e=r(p1e," (LXMERT model)"),p1e.forEach(t),v$e=i(T),pf=s(T,"LI",{});var _1e=n(pf);kW=s(_1e,"STRONG",{});var i0r=n(kW);T$e=r(i0r,"m2m_100"),i0r.forEach(t),F$e=r(_1e," \u2014 "),x8=s(_1e,"A",{href:!0});var d0r=n(x8);C$e=r(d0r,"M2M100Config"),d0r.forEach(t),M$e=r(_1e," (M2M100 model)"),_1e.forEach(t),E$e=i(T),_f=s(T,"LI",{});var b1e=n(_f);xW=s(b1e,"STRONG",{});var c0r=n(xW);y$e=r(c0r,"marian"),c0r.forEach(t),w$e=r(b1e," \u2014 "),R8=s(b1e,"A",{href:!0});var m0r=n(R8);A$e=r(m0r,"MarianConfig"),m0r.forEach(t),L$e=r(b1e," (Marian model)"),b1e.forEach(t),B$e=i(T),bf=s(T,"LI",{});var v1e=n(bf);RW=s(v1e,"STRONG",{});var f0r=n(RW);k$e=r(f0r,"mbart"),f0r.forEach(t),x$e=r(v1e," \u2014 "),S8=s(v1e,"A",{href:!0});var g0r=n(S8);R$e=r(g0r,"MBartConfig"),g0r.forEach(t),S$e=r(v1e," (mBART model)"),v1e.forEach(t),P$e=i(T),vf=s(T,"LI",{});var T1e=n(vf);SW=s(T1e,"STRONG",{});var h0r=n(SW);$$e=r(h0r,"megatron-bert"),h0r.forEach(t),I$e=r(T1e," \u2014 "),P8=s(T1e,"A",{href:!0});var u0r=n(P8);j$e=r(u0r,"MegatronBertConfig"),u0r.forEach(t),N$e=r(T1e," (MegatronBert model)"),T1e.forEach(t),D$e=i(T),Tf=s(T,"LI",{});var F1e=n(Tf);PW=s(F1e,"STRONG",{});var p0r=n(PW);q$e=r(p0r,"mobilebert"),p0r.forEach(t),G$e=r(F1e," \u2014 "),$8=s(F1e,"A",{href:!0});var _0r=n($8);O$e=r(_0r,"MobileBertConfig"),_0r.forEach(t),X$e=r(F1e," (MobileBERT model)"),F1e.forEach(t),z$e=i(T),Ff=s(T,"LI",{});var C1e=n(Ff);$W=s(C1e,"STRONG",{});var b0r=n($W);V$e=r(b0r,"mpnet"),b0r.forEach(t),W$e=r(C1e," \u2014 "),I8=s(C1e,"A",{href:!0});var v0r=n(I8);Q$e=r(v0r,"MPNetConfig"),v0r.forEach(t),H$e=r(C1e," (MPNet model)"),C1e.forEach(t),U$e=i(T),Cf=s(T,"LI",{});var M1e=n(Cf);IW=s(M1e,"STRONG",{});var T0r=n(IW);J$e=r(T0r,"mt5"),T0r.forEach(t),Y$e=r(M1e," \u2014 "),j8=s(M1e,"A",{href:!0});var F0r=n(j8);K$e=r(F0r,"MT5Config"),F0r.forEach(t),Z$e=r(M1e," (mT5 model)"),M1e.forEach(t),eIe=i(T),Mf=s(T,"LI",{});var E1e=n(Mf);jW=s(E1e,"STRONG",{});var C0r=n(jW);oIe=r(C0r,"nystromformer"),C0r.forEach(t),rIe=r(E1e," \u2014 "),N8=s(E1e,"A",{href:!0});var M0r=n(N8);tIe=r(M0r,"NystromformerConfig"),M0r.forEach(t),aIe=r(E1e," (Nystromformer model)"),E1e.forEach(t),sIe=i(T),Ef=s(T,"LI",{});var y1e=n(Ef);NW=s(y1e,"STRONG",{});var E0r=n(NW);nIe=r(E0r,"openai-gpt"),E0r.forEach(t),lIe=r(y1e," \u2014 "),D8=s(y1e,"A",{href:!0});var y0r=n(D8);iIe=r(y0r,"OpenAIGPTConfig"),y0r.forEach(t),dIe=r(y1e," (OpenAI GPT model)"),y1e.forEach(t),cIe=i(T),yf=s(T,"LI",{});var w1e=n(yf);DW=s(w1e,"STRONG",{});var w0r=n(DW);mIe=r(w0r,"pegasus"),w0r.forEach(t),fIe=r(w1e," \u2014 "),q8=s(w1e,"A",{href:!0});var A0r=n(q8);gIe=r(A0r,"PegasusConfig"),A0r.forEach(t),hIe=r(w1e," (Pegasus model)"),w1e.forEach(t),uIe=i(T),wf=s(T,"LI",{});var A1e=n(wf);qW=s(A1e,"STRONG",{});var L0r=n(qW);pIe=r(L0r,"perceiver"),L0r.forEach(t),_Ie=r(A1e," \u2014 "),G8=s(A1e,"A",{href:!0});var B0r=n(G8);bIe=r(B0r,"PerceiverConfig"),B0r.forEach(t),vIe=r(A1e," (Perceiver model)"),A1e.forEach(t),TIe=i(T),Af=s(T,"LI",{});var L1e=n(Af);GW=s(L1e,"STRONG",{});var k0r=n(GW);FIe=r(k0r,"plbart"),k0r.forEach(t),CIe=r(L1e," \u2014 "),O8=s(L1e,"A",{href:!0});var x0r=n(O8);MIe=r(x0r,"PLBartConfig"),x0r.forEach(t),EIe=r(L1e," (PLBart model)"),L1e.forEach(t),yIe=i(T),Lf=s(T,"LI",{});var B1e=n(Lf);OW=s(B1e,"STRONG",{});var R0r=n(OW);wIe=r(R0r,"poolformer"),R0r.forEach(t),AIe=r(B1e," \u2014 "),X8=s(B1e,"A",{href:!0});var S0r=n(X8);LIe=r(S0r,"PoolFormerConfig"),S0r.forEach(t),BIe=r(B1e," (PoolFormer model)"),B1e.forEach(t),kIe=i(T),Bf=s(T,"LI",{});var k1e=n(Bf);XW=s(k1e,"STRONG",{});var P0r=n(XW);xIe=r(P0r,"prophetnet"),P0r.forEach(t),RIe=r(k1e," \u2014 "),z8=s(k1e,"A",{href:!0});var $0r=n(z8);SIe=r($0r,"ProphetNetConfig"),$0r.forEach(t),PIe=r(k1e," (ProphetNet model)"),k1e.forEach(t),$Ie=i(T),kf=s(T,"LI",{});var x1e=n(kf);zW=s(x1e,"STRONG",{});var I0r=n(zW);IIe=r(I0r,"qdqbert"),I0r.forEach(t),jIe=r(x1e," \u2014 "),V8=s(x1e,"A",{href:!0});var j0r=n(V8);NIe=r(j0r,"QDQBertConfig"),j0r.forEach(t),DIe=r(x1e," (QDQBert model)"),x1e.forEach(t),qIe=i(T),xf=s(T,"LI",{});var R1e=n(xf);VW=s(R1e,"STRONG",{});var N0r=n(VW);GIe=r(N0r,"rag"),N0r.forEach(t),OIe=r(R1e," \u2014 "),W8=s(R1e,"A",{href:!0});var D0r=n(W8);XIe=r(D0r,"RagConfig"),D0r.forEach(t),zIe=r(R1e," (RAG model)"),R1e.forEach(t),VIe=i(T),Rf=s(T,"LI",{});var S1e=n(Rf);WW=s(S1e,"STRONG",{});var q0r=n(WW);WIe=r(q0r,"realm"),q0r.forEach(t),QIe=r(S1e," \u2014 "),Q8=s(S1e,"A",{href:!0});var G0r=n(Q8);HIe=r(G0r,"RealmConfig"),G0r.forEach(t),UIe=r(S1e," (Realm model)"),S1e.forEach(t),JIe=i(T),Sf=s(T,"LI",{});var P1e=n(Sf);QW=s(P1e,"STRONG",{});var O0r=n(QW);YIe=r(O0r,"reformer"),O0r.forEach(t),KIe=r(P1e," \u2014 "),H8=s(P1e,"A",{href:!0});var X0r=n(H8);ZIe=r(X0r,"ReformerConfig"),X0r.forEach(t),eje=r(P1e," (Reformer model)"),P1e.forEach(t),oje=i(T),Pf=s(T,"LI",{});var $1e=n(Pf);HW=s($1e,"STRONG",{});var z0r=n(HW);rje=r(z0r,"rembert"),z0r.forEach(t),tje=r($1e," \u2014 "),U8=s($1e,"A",{href:!0});var V0r=n(U8);aje=r(V0r,"RemBertConfig"),V0r.forEach(t),sje=r($1e," (RemBERT model)"),$1e.forEach(t),nje=i(T),$f=s(T,"LI",{});var I1e=n($f);UW=s(I1e,"STRONG",{});var W0r=n(UW);lje=r(W0r,"retribert"),W0r.forEach(t),ije=r(I1e," \u2014 "),J8=s(I1e,"A",{href:!0});var Q0r=n(J8);dje=r(Q0r,"RetriBertConfig"),Q0r.forEach(t),cje=r(I1e," (RetriBERT model)"),I1e.forEach(t),mje=i(T),If=s(T,"LI",{});var j1e=n(If);JW=s(j1e,"STRONG",{});var H0r=n(JW);fje=r(H0r,"roberta"),H0r.forEach(t),gje=r(j1e," \u2014 "),Y8=s(j1e,"A",{href:!0});var U0r=n(Y8);hje=r(U0r,"RobertaConfig"),U0r.forEach(t),uje=r(j1e," (RoBERTa model)"),j1e.forEach(t),pje=i(T),jf=s(T,"LI",{});var N1e=n(jf);YW=s(N1e,"STRONG",{});var J0r=n(YW);_je=r(J0r,"roformer"),J0r.forEach(t),bje=r(N1e," \u2014 "),K8=s(N1e,"A",{href:!0});var Y0r=n(K8);vje=r(Y0r,"RoFormerConfig"),Y0r.forEach(t),Tje=r(N1e," (RoFormer model)"),N1e.forEach(t),Fje=i(T),Nf=s(T,"LI",{});var D1e=n(Nf);KW=s(D1e,"STRONG",{});var K0r=n(KW);Cje=r(K0r,"segformer"),K0r.forEach(t),Mje=r(D1e," \u2014 "),Z8=s(D1e,"A",{href:!0});var Z0r=n(Z8);Eje=r(Z0r,"SegformerConfig"),Z0r.forEach(t),yje=r(D1e," (SegFormer model)"),D1e.forEach(t),wje=i(T),Df=s(T,"LI",{});var q1e=n(Df);ZW=s(q1e,"STRONG",{});var eLr=n(ZW);Aje=r(eLr,"sew"),eLr.forEach(t),Lje=r(q1e," \u2014 "),e9=s(q1e,"A",{href:!0});var oLr=n(e9);Bje=r(oLr,"SEWConfig"),oLr.forEach(t),kje=r(q1e," (SEW model)"),q1e.forEach(t),xje=i(T),qf=s(T,"LI",{});var G1e=n(qf);eQ=s(G1e,"STRONG",{});var rLr=n(eQ);Rje=r(rLr,"sew-d"),rLr.forEach(t),Sje=r(G1e," \u2014 "),o9=s(G1e,"A",{href:!0});var tLr=n(o9);Pje=r(tLr,"SEWDConfig"),tLr.forEach(t),$je=r(G1e," (SEW-D model)"),G1e.forEach(t),Ije=i(T),Gf=s(T,"LI",{});var O1e=n(Gf);oQ=s(O1e,"STRONG",{});var aLr=n(oQ);jje=r(aLr,"speech-encoder-decoder"),aLr.forEach(t),Nje=r(O1e," \u2014 "),r9=s(O1e,"A",{href:!0});var sLr=n(r9);Dje=r(sLr,"SpeechEncoderDecoderConfig"),sLr.forEach(t),qje=r(O1e," (Speech Encoder decoder model)"),O1e.forEach(t),Gje=i(T),Of=s(T,"LI",{});var X1e=n(Of);rQ=s(X1e,"STRONG",{});var nLr=n(rQ);Oje=r(nLr,"speech_to_text"),nLr.forEach(t),Xje=r(X1e," \u2014 "),t9=s(X1e,"A",{href:!0});var lLr=n(t9);zje=r(lLr,"Speech2TextConfig"),lLr.forEach(t),Vje=r(X1e," (Speech2Text model)"),X1e.forEach(t),Wje=i(T),Xf=s(T,"LI",{});var z1e=n(Xf);tQ=s(z1e,"STRONG",{});var iLr=n(tQ);Qje=r(iLr,"speech_to_text_2"),iLr.forEach(t),Hje=r(z1e," \u2014 "),a9=s(z1e,"A",{href:!0});var dLr=n(a9);Uje=r(dLr,"Speech2Text2Config"),dLr.forEach(t),Jje=r(z1e," (Speech2Text2 model)"),z1e.forEach(t),Yje=i(T),zf=s(T,"LI",{});var V1e=n(zf);aQ=s(V1e,"STRONG",{});var cLr=n(aQ);Kje=r(cLr,"splinter"),cLr.forEach(t),Zje=r(V1e," \u2014 "),s9=s(V1e,"A",{href:!0});var mLr=n(s9);eNe=r(mLr,"SplinterConfig"),mLr.forEach(t),oNe=r(V1e," (Splinter model)"),V1e.forEach(t),rNe=i(T),Vf=s(T,"LI",{});var W1e=n(Vf);sQ=s(W1e,"STRONG",{});var fLr=n(sQ);tNe=r(fLr,"squeezebert"),fLr.forEach(t),aNe=r(W1e," \u2014 "),n9=s(W1e,"A",{href:!0});var gLr=n(n9);sNe=r(gLr,"SqueezeBertConfig"),gLr.forEach(t),nNe=r(W1e," (SqueezeBERT model)"),W1e.forEach(t),lNe=i(T),Wf=s(T,"LI",{});var Q1e=n(Wf);nQ=s(Q1e,"STRONG",{});var hLr=n(nQ);iNe=r(hLr,"swin"),hLr.forEach(t),dNe=r(Q1e," \u2014 "),l9=s(Q1e,"A",{href:!0});var uLr=n(l9);cNe=r(uLr,"SwinConfig"),uLr.forEach(t),mNe=r(Q1e," (Swin model)"),Q1e.forEach(t),fNe=i(T),Qf=s(T,"LI",{});var H1e=n(Qf);lQ=s(H1e,"STRONG",{});var pLr=n(lQ);gNe=r(pLr,"t5"),pLr.forEach(t),hNe=r(H1e," \u2014 "),i9=s(H1e,"A",{href:!0});var _Lr=n(i9);uNe=r(_Lr,"T5Config"),_Lr.forEach(t),pNe=r(H1e," (T5 model)"),H1e.forEach(t),_Ne=i(T),Hf=s(T,"LI",{});var U1e=n(Hf);iQ=s(U1e,"STRONG",{});var bLr=n(iQ);bNe=r(bLr,"tapas"),bLr.forEach(t),vNe=r(U1e," \u2014 "),d9=s(U1e,"A",{href:!0});var vLr=n(d9);TNe=r(vLr,"TapasConfig"),vLr.forEach(t),FNe=r(U1e," (TAPAS model)"),U1e.forEach(t),CNe=i(T),Uf=s(T,"LI",{});var J1e=n(Uf);dQ=s(J1e,"STRONG",{});var TLr=n(dQ);MNe=r(TLr,"transfo-xl"),TLr.forEach(t),ENe=r(J1e," \u2014 "),c9=s(J1e,"A",{href:!0});var FLr=n(c9);yNe=r(FLr,"TransfoXLConfig"),FLr.forEach(t),wNe=r(J1e," (Transformer-XL model)"),J1e.forEach(t),ANe=i(T),Jf=s(T,"LI",{});var Y1e=n(Jf);cQ=s(Y1e,"STRONG",{});var CLr=n(cQ);LNe=r(CLr,"trocr"),CLr.forEach(t),BNe=r(Y1e," \u2014 "),m9=s(Y1e,"A",{href:!0});var MLr=n(m9);kNe=r(MLr,"TrOCRConfig"),MLr.forEach(t),xNe=r(Y1e," (TrOCR model)"),Y1e.forEach(t),RNe=i(T),Yf=s(T,"LI",{});var K1e=n(Yf);mQ=s(K1e,"STRONG",{});var ELr=n(mQ);SNe=r(ELr,"unispeech"),ELr.forEach(t),PNe=r(K1e," \u2014 "),f9=s(K1e,"A",{href:!0});var yLr=n(f9);$Ne=r(yLr,"UniSpeechConfig"),yLr.forEach(t),INe=r(K1e," (UniSpeech model)"),K1e.forEach(t),jNe=i(T),Kf=s(T,"LI",{});var Z1e=n(Kf);fQ=s(Z1e,"STRONG",{});var wLr=n(fQ);NNe=r(wLr,"unispeech-sat"),wLr.forEach(t),DNe=r(Z1e," \u2014 "),g9=s(Z1e,"A",{href:!0});var ALr=n(g9);qNe=r(ALr,"UniSpeechSatConfig"),ALr.forEach(t),GNe=r(Z1e," (UniSpeechSat model)"),Z1e.forEach(t),ONe=i(T),Zf=s(T,"LI",{});var eFe=n(Zf);gQ=s(eFe,"STRONG",{});var LLr=n(gQ);XNe=r(LLr,"vilt"),LLr.forEach(t),zNe=r(eFe," \u2014 "),h9=s(eFe,"A",{href:!0});var BLr=n(h9);VNe=r(BLr,"ViltConfig"),BLr.forEach(t),WNe=r(eFe," (ViLT model)"),eFe.forEach(t),QNe=i(T),eg=s(T,"LI",{});var oFe=n(eg);hQ=s(oFe,"STRONG",{});var kLr=n(hQ);HNe=r(kLr,"vision-encoder-decoder"),kLr.forEach(t),UNe=r(oFe," \u2014 "),u9=s(oFe,"A",{href:!0});var xLr=n(u9);JNe=r(xLr,"VisionEncoderDecoderConfig"),xLr.forEach(t),YNe=r(oFe," (Vision Encoder decoder model)"),oFe.forEach(t),KNe=i(T),og=s(T,"LI",{});var rFe=n(og);uQ=s(rFe,"STRONG",{});var RLr=n(uQ);ZNe=r(RLr,"vision-text-dual-encoder"),RLr.forEach(t),eDe=r(rFe," \u2014 "),p9=s(rFe,"A",{href:!0});var SLr=n(p9);oDe=r(SLr,"VisionTextDualEncoderConfig"),SLr.forEach(t),rDe=r(rFe," (VisionTextDualEncoder model)"),rFe.forEach(t),tDe=i(T),rg=s(T,"LI",{});var tFe=n(rg);pQ=s(tFe,"STRONG",{});var PLr=n(pQ);aDe=r(PLr,"visual_bert"),PLr.forEach(t),sDe=r(tFe," \u2014 "),_9=s(tFe,"A",{href:!0});var $Lr=n(_9);nDe=r($Lr,"VisualBertConfig"),$Lr.forEach(t),lDe=r(tFe," (VisualBert model)"),tFe.forEach(t),iDe=i(T),tg=s(T,"LI",{});var aFe=n(tg);_Q=s(aFe,"STRONG",{});var ILr=n(_Q);dDe=r(ILr,"vit"),ILr.forEach(t),cDe=r(aFe," \u2014 "),b9=s(aFe,"A",{href:!0});var jLr=n(b9);mDe=r(jLr,"ViTConfig"),jLr.forEach(t),fDe=r(aFe," (ViT model)"),aFe.forEach(t),gDe=i(T),ag=s(T,"LI",{});var sFe=n(ag);bQ=s(sFe,"STRONG",{});var NLr=n(bQ);hDe=r(NLr,"vit_mae"),NLr.forEach(t),uDe=r(sFe," \u2014 "),v9=s(sFe,"A",{href:!0});var DLr=n(v9);pDe=r(DLr,"ViTMAEConfig"),DLr.forEach(t),_De=r(sFe," (ViTMAE model)"),sFe.forEach(t),bDe=i(T),sg=s(T,"LI",{});var nFe=n(sg);vQ=s(nFe,"STRONG",{});var qLr=n(vQ);vDe=r(qLr,"wav2vec2"),qLr.forEach(t),TDe=r(nFe," \u2014 "),T9=s(nFe,"A",{href:!0});var GLr=n(T9);FDe=r(GLr,"Wav2Vec2Config"),GLr.forEach(t),CDe=r(nFe," (Wav2Vec2 model)"),nFe.forEach(t),MDe=i(T),ng=s(T,"LI",{});var lFe=n(ng);TQ=s(lFe,"STRONG",{});var OLr=n(TQ);EDe=r(OLr,"wavlm"),OLr.forEach(t),yDe=r(lFe," \u2014 "),F9=s(lFe,"A",{href:!0});var XLr=n(F9);wDe=r(XLr,"WavLMConfig"),XLr.forEach(t),ADe=r(lFe," (WavLM model)"),lFe.forEach(t),LDe=i(T),lg=s(T,"LI",{});var iFe=n(lg);FQ=s(iFe,"STRONG",{});var zLr=n(FQ);BDe=r(zLr,"xglm"),zLr.forEach(t),kDe=r(iFe," \u2014 "),C9=s(iFe,"A",{href:!0});var VLr=n(C9);xDe=r(VLr,"XGLMConfig"),VLr.forEach(t),RDe=r(iFe," (XGLM model)"),iFe.forEach(t),SDe=i(T),ig=s(T,"LI",{});var dFe=n(ig);CQ=s(dFe,"STRONG",{});var WLr=n(CQ);PDe=r(WLr,"xlm"),WLr.forEach(t),$De=r(dFe," \u2014 "),M9=s(dFe,"A",{href:!0});var QLr=n(M9);IDe=r(QLr,"XLMConfig"),QLr.forEach(t),jDe=r(dFe," (XLM model)"),dFe.forEach(t),NDe=i(T),dg=s(T,"LI",{});var cFe=n(dg);MQ=s(cFe,"STRONG",{});var HLr=n(MQ);DDe=r(HLr,"xlm-prophetnet"),HLr.forEach(t),qDe=r(cFe," \u2014 "),E9=s(cFe,"A",{href:!0});var ULr=n(E9);GDe=r(ULr,"XLMProphetNetConfig"),ULr.forEach(t),ODe=r(cFe," (XLMProphetNet model)"),cFe.forEach(t),XDe=i(T),cg=s(T,"LI",{});var mFe=n(cg);EQ=s(mFe,"STRONG",{});var JLr=n(EQ);zDe=r(JLr,"xlm-roberta"),JLr.forEach(t),VDe=r(mFe," \u2014 "),y9=s(mFe,"A",{href:!0});var YLr=n(y9);WDe=r(YLr,"XLMRobertaConfig"),YLr.forEach(t),QDe=r(mFe," (XLM-RoBERTa model)"),mFe.forEach(t),HDe=i(T),mg=s(T,"LI",{});var fFe=n(mg);yQ=s(fFe,"STRONG",{});var KLr=n(yQ);UDe=r(KLr,"xlm-roberta-xl"),KLr.forEach(t),JDe=r(fFe," \u2014 "),w9=s(fFe,"A",{href:!0});var ZLr=n(w9);YDe=r(ZLr,"XLMRobertaXLConfig"),ZLr.forEach(t),KDe=r(fFe," (XLM-RoBERTa-XL model)"),fFe.forEach(t),ZDe=i(T),fg=s(T,"LI",{});var gFe=n(fg);wQ=s(gFe,"STRONG",{});var e7r=n(wQ);eqe=r(e7r,"xlnet"),e7r.forEach(t),oqe=r(gFe," \u2014 "),A9=s(gFe,"A",{href:!0});var o7r=n(A9);rqe=r(o7r,"XLNetConfig"),o7r.forEach(t),tqe=r(gFe," (XLNet model)"),gFe.forEach(t),aqe=i(T),gg=s(T,"LI",{});var hFe=n(gg);AQ=s(hFe,"STRONG",{});var r7r=n(AQ);sqe=r(r7r,"yoso"),r7r.forEach(t),nqe=r(hFe," \u2014 "),L9=s(hFe,"A",{href:!0});var t7r=n(L9);lqe=r(t7r,"YosoConfig"),t7r.forEach(t),iqe=r(hFe," (YOSO model)"),hFe.forEach(t),T.forEach(t),dqe=i(ia),LQ=s(ia,"P",{});var a7r=n(LQ);cqe=r(a7r,"Examples:"),a7r.forEach(t),mqe=i(ia),f(f5.$$.fragment,ia),ia.forEach(t),fqe=i(Pn),hg=s(Pn,"DIV",{class:!0});var cBe=n(hg);f(g5.$$.fragment,cBe),gqe=i(cBe),BQ=s(cBe,"P",{});var s7r=n(BQ);hqe=r(s7r,"Register a new configuration for this class."),s7r.forEach(t),cBe.forEach(t),Pn.forEach(t),c8e=i(d),Ii=s(d,"H2",{class:!0});var mBe=n(Ii);ug=s(mBe,"A",{id:!0,class:!0,href:!0});var n7r=n(ug);kQ=s(n7r,"SPAN",{});var l7r=n(kQ);f(h5.$$.fragment,l7r),l7r.forEach(t),n7r.forEach(t),uqe=i(mBe),xQ=s(mBe,"SPAN",{});var i7r=n(xQ);pqe=r(i7r,"AutoTokenizer"),i7r.forEach(t),mBe.forEach(t),m8e=i(d),Oo=s(d,"DIV",{class:!0});var $n=n(Oo);f(u5.$$.fragment,$n),_qe=i($n),p5=s($n,"P",{});var fBe=n(p5);bqe=r(fBe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),B9=s(fBe,"A",{href:!0});var d7r=n(B9);vqe=r(d7r,"AutoTokenizer.from_pretrained()"),d7r.forEach(t),Tqe=r(fBe," class method."),fBe.forEach(t),Fqe=i($n),_5=s($n,"P",{});var gBe=n(_5);Cqe=r(gBe,"This class cannot be instantiated directly using "),RQ=s(gBe,"CODE",{});var c7r=n(RQ);Mqe=r(c7r,"__init__()"),c7r.forEach(t),Eqe=r(gBe," (throws an error)."),gBe.forEach(t),yqe=i($n),fo=s($n,"DIV",{class:!0});var da=n(fo);f(b5.$$.fragment,da),wqe=i(da),SQ=s(da,"P",{});var m7r=n(SQ);Aqe=r(m7r,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),m7r.forEach(t),Lqe=i(da),ja=s(da,"P",{});var s3=n(ja);Bqe=r(s3,"The tokenizer class to instantiate is selected based on the "),PQ=s(s3,"CODE",{});var f7r=n(PQ);kqe=r(f7r,"model_type"),f7r.forEach(t),xqe=r(s3,` property of the config object (either
passed as an argument or loaded from `),$Q=s(s3,"CODE",{});var g7r=n($Q);Rqe=r(g7r,"pretrained_model_name_or_path"),g7r.forEach(t),Sqe=r(s3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IQ=s(s3,"CODE",{});var h7r=n(IQ);Pqe=r(h7r,"pretrained_model_name_or_path"),h7r.forEach(t),$qe=r(s3,":"),s3.forEach(t),Iqe=i(da),M=s(da,"UL",{});var y=n(M);Ns=s(y,"LI",{});var zL=n(Ns);jQ=s(zL,"STRONG",{});var u7r=n(jQ);jqe=r(u7r,"albert"),u7r.forEach(t),Nqe=r(zL," \u2014 "),k9=s(zL,"A",{href:!0});var p7r=n(k9);Dqe=r(p7r,"AlbertTokenizer"),p7r.forEach(t),qqe=r(zL," or "),x9=s(zL,"A",{href:!0});var _7r=n(x9);Gqe=r(_7r,"AlbertTokenizerFast"),_7r.forEach(t),Oqe=r(zL," (ALBERT model)"),zL.forEach(t),Xqe=i(y),Ds=s(y,"LI",{});var VL=n(Ds);NQ=s(VL,"STRONG",{});var b7r=n(NQ);zqe=r(b7r,"bart"),b7r.forEach(t),Vqe=r(VL," \u2014 "),R9=s(VL,"A",{href:!0});var v7r=n(R9);Wqe=r(v7r,"BartTokenizer"),v7r.forEach(t),Qqe=r(VL," or "),S9=s(VL,"A",{href:!0});var T7r=n(S9);Hqe=r(T7r,"BartTokenizerFast"),T7r.forEach(t),Uqe=r(VL," (BART model)"),VL.forEach(t),Jqe=i(y),qs=s(y,"LI",{});var WL=n(qs);DQ=s(WL,"STRONG",{});var F7r=n(DQ);Yqe=r(F7r,"barthez"),F7r.forEach(t),Kqe=r(WL," \u2014 "),P9=s(WL,"A",{href:!0});var C7r=n(P9);Zqe=r(C7r,"BarthezTokenizer"),C7r.forEach(t),eGe=r(WL," or "),$9=s(WL,"A",{href:!0});var M7r=n($9);oGe=r(M7r,"BarthezTokenizerFast"),M7r.forEach(t),rGe=r(WL," (BARThez model)"),WL.forEach(t),tGe=i(y),pg=s(y,"LI",{});var uFe=n(pg);qQ=s(uFe,"STRONG",{});var E7r=n(qQ);aGe=r(E7r,"bartpho"),E7r.forEach(t),sGe=r(uFe," \u2014 "),I9=s(uFe,"A",{href:!0});var y7r=n(I9);nGe=r(y7r,"BartphoTokenizer"),y7r.forEach(t),lGe=r(uFe," (BARTpho model)"),uFe.forEach(t),iGe=i(y),Gs=s(y,"LI",{});var QL=n(Gs);GQ=s(QL,"STRONG",{});var w7r=n(GQ);dGe=r(w7r,"bert"),w7r.forEach(t),cGe=r(QL," \u2014 "),j9=s(QL,"A",{href:!0});var A7r=n(j9);mGe=r(A7r,"BertTokenizer"),A7r.forEach(t),fGe=r(QL," or "),N9=s(QL,"A",{href:!0});var L7r=n(N9);gGe=r(L7r,"BertTokenizerFast"),L7r.forEach(t),hGe=r(QL," (BERT model)"),QL.forEach(t),uGe=i(y),_g=s(y,"LI",{});var pFe=n(_g);OQ=s(pFe,"STRONG",{});var B7r=n(OQ);pGe=r(B7r,"bert-generation"),B7r.forEach(t),_Ge=r(pFe," \u2014 "),D9=s(pFe,"A",{href:!0});var k7r=n(D9);bGe=r(k7r,"BertGenerationTokenizer"),k7r.forEach(t),vGe=r(pFe," (Bert Generation model)"),pFe.forEach(t),TGe=i(y),bg=s(y,"LI",{});var _Fe=n(bg);XQ=s(_Fe,"STRONG",{});var x7r=n(XQ);FGe=r(x7r,"bert-japanese"),x7r.forEach(t),CGe=r(_Fe," \u2014 "),q9=s(_Fe,"A",{href:!0});var R7r=n(q9);MGe=r(R7r,"BertJapaneseTokenizer"),R7r.forEach(t),EGe=r(_Fe," (BertJapanese model)"),_Fe.forEach(t),yGe=i(y),vg=s(y,"LI",{});var bFe=n(vg);zQ=s(bFe,"STRONG",{});var S7r=n(zQ);wGe=r(S7r,"bertweet"),S7r.forEach(t),AGe=r(bFe," \u2014 "),G9=s(bFe,"A",{href:!0});var P7r=n(G9);LGe=r(P7r,"BertweetTokenizer"),P7r.forEach(t),BGe=r(bFe," (Bertweet model)"),bFe.forEach(t),kGe=i(y),Os=s(y,"LI",{});var HL=n(Os);VQ=s(HL,"STRONG",{});var $7r=n(VQ);xGe=r($7r,"big_bird"),$7r.forEach(t),RGe=r(HL," \u2014 "),O9=s(HL,"A",{href:!0});var I7r=n(O9);SGe=r(I7r,"BigBirdTokenizer"),I7r.forEach(t),PGe=r(HL," or "),X9=s(HL,"A",{href:!0});var j7r=n(X9);$Ge=r(j7r,"BigBirdTokenizerFast"),j7r.forEach(t),IGe=r(HL," (BigBird model)"),HL.forEach(t),jGe=i(y),Xs=s(y,"LI",{});var UL=n(Xs);WQ=s(UL,"STRONG",{});var N7r=n(WQ);NGe=r(N7r,"bigbird_pegasus"),N7r.forEach(t),DGe=r(UL," \u2014 "),z9=s(UL,"A",{href:!0});var D7r=n(z9);qGe=r(D7r,"PegasusTokenizer"),D7r.forEach(t),GGe=r(UL," or "),V9=s(UL,"A",{href:!0});var q7r=n(V9);OGe=r(q7r,"PegasusTokenizerFast"),q7r.forEach(t),XGe=r(UL," (BigBirdPegasus model)"),UL.forEach(t),zGe=i(y),zs=s(y,"LI",{});var JL=n(zs);QQ=s(JL,"STRONG",{});var G7r=n(QQ);VGe=r(G7r,"blenderbot"),G7r.forEach(t),WGe=r(JL," \u2014 "),W9=s(JL,"A",{href:!0});var O7r=n(W9);QGe=r(O7r,"BlenderbotTokenizer"),O7r.forEach(t),HGe=r(JL," or "),Q9=s(JL,"A",{href:!0});var X7r=n(Q9);UGe=r(X7r,"BlenderbotTokenizerFast"),X7r.forEach(t),JGe=r(JL," (Blenderbot model)"),JL.forEach(t),YGe=i(y),Tg=s(y,"LI",{});var vFe=n(Tg);HQ=s(vFe,"STRONG",{});var z7r=n(HQ);KGe=r(z7r,"blenderbot-small"),z7r.forEach(t),ZGe=r(vFe," \u2014 "),H9=s(vFe,"A",{href:!0});var V7r=n(H9);eOe=r(V7r,"BlenderbotSmallTokenizer"),V7r.forEach(t),oOe=r(vFe," (BlenderbotSmall model)"),vFe.forEach(t),rOe=i(y),Fg=s(y,"LI",{});var TFe=n(Fg);UQ=s(TFe,"STRONG",{});var W7r=n(UQ);tOe=r(W7r,"byt5"),W7r.forEach(t),aOe=r(TFe," \u2014 "),U9=s(TFe,"A",{href:!0});var Q7r=n(U9);sOe=r(Q7r,"ByT5Tokenizer"),Q7r.forEach(t),nOe=r(TFe," (ByT5 model)"),TFe.forEach(t),lOe=i(y),Vs=s(y,"LI",{});var YL=n(Vs);JQ=s(YL,"STRONG",{});var H7r=n(JQ);iOe=r(H7r,"camembert"),H7r.forEach(t),dOe=r(YL," \u2014 "),J9=s(YL,"A",{href:!0});var U7r=n(J9);cOe=r(U7r,"CamembertTokenizer"),U7r.forEach(t),mOe=r(YL," or "),Y9=s(YL,"A",{href:!0});var J7r=n(Y9);fOe=r(J7r,"CamembertTokenizerFast"),J7r.forEach(t),gOe=r(YL," (CamemBERT model)"),YL.forEach(t),hOe=i(y),Cg=s(y,"LI",{});var FFe=n(Cg);YQ=s(FFe,"STRONG",{});var Y7r=n(YQ);uOe=r(Y7r,"canine"),Y7r.forEach(t),pOe=r(FFe," \u2014 "),K9=s(FFe,"A",{href:!0});var K7r=n(K9);_Oe=r(K7r,"CanineTokenizer"),K7r.forEach(t),bOe=r(FFe," (Canine model)"),FFe.forEach(t),vOe=i(y),Ws=s(y,"LI",{});var KL=n(Ws);KQ=s(KL,"STRONG",{});var Z7r=n(KQ);TOe=r(Z7r,"clip"),Z7r.forEach(t),FOe=r(KL," \u2014 "),Z9=s(KL,"A",{href:!0});var e8r=n(Z9);COe=r(e8r,"CLIPTokenizer"),e8r.forEach(t),MOe=r(KL," or "),eB=s(KL,"A",{href:!0});var o8r=n(eB);EOe=r(o8r,"CLIPTokenizerFast"),o8r.forEach(t),yOe=r(KL," (CLIP model)"),KL.forEach(t),wOe=i(y),Qs=s(y,"LI",{});var ZL=n(Qs);ZQ=s(ZL,"STRONG",{});var r8r=n(ZQ);AOe=r(r8r,"convbert"),r8r.forEach(t),LOe=r(ZL," \u2014 "),oB=s(ZL,"A",{href:!0});var t8r=n(oB);BOe=r(t8r,"ConvBertTokenizer"),t8r.forEach(t),kOe=r(ZL," or "),rB=s(ZL,"A",{href:!0});var a8r=n(rB);xOe=r(a8r,"ConvBertTokenizerFast"),a8r.forEach(t),ROe=r(ZL," (ConvBERT model)"),ZL.forEach(t),SOe=i(y),Hs=s(y,"LI",{});var e7=n(Hs);eH=s(e7,"STRONG",{});var s8r=n(eH);POe=r(s8r,"cpm"),s8r.forEach(t),$Oe=r(e7," \u2014 "),tB=s(e7,"A",{href:!0});var n8r=n(tB);IOe=r(n8r,"CpmTokenizer"),n8r.forEach(t),jOe=r(e7," or "),oH=s(e7,"CODE",{});var l8r=n(oH);NOe=r(l8r,"CpmTokenizerFast"),l8r.forEach(t),DOe=r(e7," (CPM model)"),e7.forEach(t),qOe=i(y),Mg=s(y,"LI",{});var CFe=n(Mg);rH=s(CFe,"STRONG",{});var i8r=n(rH);GOe=r(i8r,"ctrl"),i8r.forEach(t),OOe=r(CFe," \u2014 "),aB=s(CFe,"A",{href:!0});var d8r=n(aB);XOe=r(d8r,"CTRLTokenizer"),d8r.forEach(t),zOe=r(CFe," (CTRL model)"),CFe.forEach(t),VOe=i(y),Us=s(y,"LI",{});var o7=n(Us);tH=s(o7,"STRONG",{});var c8r=n(tH);WOe=r(c8r,"deberta"),c8r.forEach(t),QOe=r(o7," \u2014 "),sB=s(o7,"A",{href:!0});var m8r=n(sB);HOe=r(m8r,"DebertaTokenizer"),m8r.forEach(t),UOe=r(o7," or "),nB=s(o7,"A",{href:!0});var f8r=n(nB);JOe=r(f8r,"DebertaTokenizerFast"),f8r.forEach(t),YOe=r(o7," (DeBERTa model)"),o7.forEach(t),KOe=i(y),Eg=s(y,"LI",{});var MFe=n(Eg);aH=s(MFe,"STRONG",{});var g8r=n(aH);ZOe=r(g8r,"deberta-v2"),g8r.forEach(t),eXe=r(MFe," \u2014 "),lB=s(MFe,"A",{href:!0});var h8r=n(lB);oXe=r(h8r,"DebertaV2Tokenizer"),h8r.forEach(t),rXe=r(MFe," (DeBERTa-v2 model)"),MFe.forEach(t),tXe=i(y),Js=s(y,"LI",{});var r7=n(Js);sH=s(r7,"STRONG",{});var u8r=n(sH);aXe=r(u8r,"distilbert"),u8r.forEach(t),sXe=r(r7," \u2014 "),iB=s(r7,"A",{href:!0});var p8r=n(iB);nXe=r(p8r,"DistilBertTokenizer"),p8r.forEach(t),lXe=r(r7," or "),dB=s(r7,"A",{href:!0});var _8r=n(dB);iXe=r(_8r,"DistilBertTokenizerFast"),_8r.forEach(t),dXe=r(r7," (DistilBERT model)"),r7.forEach(t),cXe=i(y),Ys=s(y,"LI",{});var t7=n(Ys);nH=s(t7,"STRONG",{});var b8r=n(nH);mXe=r(b8r,"dpr"),b8r.forEach(t),fXe=r(t7," \u2014 "),cB=s(t7,"A",{href:!0});var v8r=n(cB);gXe=r(v8r,"DPRQuestionEncoderTokenizer"),v8r.forEach(t),hXe=r(t7," or "),mB=s(t7,"A",{href:!0});var T8r=n(mB);uXe=r(T8r,"DPRQuestionEncoderTokenizerFast"),T8r.forEach(t),pXe=r(t7," (DPR model)"),t7.forEach(t),_Xe=i(y),Ks=s(y,"LI",{});var a7=n(Ks);lH=s(a7,"STRONG",{});var F8r=n(lH);bXe=r(F8r,"electra"),F8r.forEach(t),vXe=r(a7," \u2014 "),fB=s(a7,"A",{href:!0});var C8r=n(fB);TXe=r(C8r,"ElectraTokenizer"),C8r.forEach(t),FXe=r(a7," or "),gB=s(a7,"A",{href:!0});var M8r=n(gB);CXe=r(M8r,"ElectraTokenizerFast"),M8r.forEach(t),MXe=r(a7," (ELECTRA model)"),a7.forEach(t),EXe=i(y),yg=s(y,"LI",{});var EFe=n(yg);iH=s(EFe,"STRONG",{});var E8r=n(iH);yXe=r(E8r,"flaubert"),E8r.forEach(t),wXe=r(EFe," \u2014 "),hB=s(EFe,"A",{href:!0});var y8r=n(hB);AXe=r(y8r,"FlaubertTokenizer"),y8r.forEach(t),LXe=r(EFe," (FlauBERT model)"),EFe.forEach(t),BXe=i(y),Zs=s(y,"LI",{});var s7=n(Zs);dH=s(s7,"STRONG",{});var w8r=n(dH);kXe=r(w8r,"fnet"),w8r.forEach(t),xXe=r(s7," \u2014 "),uB=s(s7,"A",{href:!0});var A8r=n(uB);RXe=r(A8r,"FNetTokenizer"),A8r.forEach(t),SXe=r(s7," or "),pB=s(s7,"A",{href:!0});var L8r=n(pB);PXe=r(L8r,"FNetTokenizerFast"),L8r.forEach(t),$Xe=r(s7," (FNet model)"),s7.forEach(t),IXe=i(y),wg=s(y,"LI",{});var yFe=n(wg);cH=s(yFe,"STRONG",{});var B8r=n(cH);jXe=r(B8r,"fsmt"),B8r.forEach(t),NXe=r(yFe," \u2014 "),_B=s(yFe,"A",{href:!0});var k8r=n(_B);DXe=r(k8r,"FSMTTokenizer"),k8r.forEach(t),qXe=r(yFe," (FairSeq Machine-Translation model)"),yFe.forEach(t),GXe=i(y),en=s(y,"LI",{});var n7=n(en);mH=s(n7,"STRONG",{});var x8r=n(mH);OXe=r(x8r,"funnel"),x8r.forEach(t),XXe=r(n7," \u2014 "),bB=s(n7,"A",{href:!0});var R8r=n(bB);zXe=r(R8r,"FunnelTokenizer"),R8r.forEach(t),VXe=r(n7," or "),vB=s(n7,"A",{href:!0});var S8r=n(vB);WXe=r(S8r,"FunnelTokenizerFast"),S8r.forEach(t),QXe=r(n7," (Funnel Transformer model)"),n7.forEach(t),HXe=i(y),on=s(y,"LI",{});var l7=n(on);fH=s(l7,"STRONG",{});var P8r=n(fH);UXe=r(P8r,"gpt2"),P8r.forEach(t),JXe=r(l7," \u2014 "),TB=s(l7,"A",{href:!0});var $8r=n(TB);YXe=r($8r,"GPT2Tokenizer"),$8r.forEach(t),KXe=r(l7," or "),FB=s(l7,"A",{href:!0});var I8r=n(FB);ZXe=r(I8r,"GPT2TokenizerFast"),I8r.forEach(t),eze=r(l7," (OpenAI GPT-2 model)"),l7.forEach(t),oze=i(y),rn=s(y,"LI",{});var i7=n(rn);gH=s(i7,"STRONG",{});var j8r=n(gH);rze=r(j8r,"gpt_neo"),j8r.forEach(t),tze=r(i7," \u2014 "),CB=s(i7,"A",{href:!0});var N8r=n(CB);aze=r(N8r,"GPT2Tokenizer"),N8r.forEach(t),sze=r(i7," or "),MB=s(i7,"A",{href:!0});var D8r=n(MB);nze=r(D8r,"GPT2TokenizerFast"),D8r.forEach(t),lze=r(i7," (GPT Neo model)"),i7.forEach(t),ize=i(y),tn=s(y,"LI",{});var d7=n(tn);hH=s(d7,"STRONG",{});var q8r=n(hH);dze=r(q8r,"herbert"),q8r.forEach(t),cze=r(d7," \u2014 "),EB=s(d7,"A",{href:!0});var G8r=n(EB);mze=r(G8r,"HerbertTokenizer"),G8r.forEach(t),fze=r(d7," or "),yB=s(d7,"A",{href:!0});var O8r=n(yB);gze=r(O8r,"HerbertTokenizerFast"),O8r.forEach(t),hze=r(d7," (HerBERT model)"),d7.forEach(t),uze=i(y),Ag=s(y,"LI",{});var wFe=n(Ag);uH=s(wFe,"STRONG",{});var X8r=n(uH);pze=r(X8r,"hubert"),X8r.forEach(t),_ze=r(wFe," \u2014 "),wB=s(wFe,"A",{href:!0});var z8r=n(wB);bze=r(z8r,"Wav2Vec2CTCTokenizer"),z8r.forEach(t),vze=r(wFe," (Hubert model)"),wFe.forEach(t),Tze=i(y),an=s(y,"LI",{});var c7=n(an);pH=s(c7,"STRONG",{});var V8r=n(pH);Fze=r(V8r,"ibert"),V8r.forEach(t),Cze=r(c7," \u2014 "),AB=s(c7,"A",{href:!0});var W8r=n(AB);Mze=r(W8r,"RobertaTokenizer"),W8r.forEach(t),Eze=r(c7," or "),LB=s(c7,"A",{href:!0});var Q8r=n(LB);yze=r(Q8r,"RobertaTokenizerFast"),Q8r.forEach(t),wze=r(c7," (I-BERT model)"),c7.forEach(t),Aze=i(y),sn=s(y,"LI",{});var m7=n(sn);_H=s(m7,"STRONG",{});var H8r=n(_H);Lze=r(H8r,"layoutlm"),H8r.forEach(t),Bze=r(m7," \u2014 "),BB=s(m7,"A",{href:!0});var U8r=n(BB);kze=r(U8r,"LayoutLMTokenizer"),U8r.forEach(t),xze=r(m7," or "),kB=s(m7,"A",{href:!0});var J8r=n(kB);Rze=r(J8r,"LayoutLMTokenizerFast"),J8r.forEach(t),Sze=r(m7," (LayoutLM model)"),m7.forEach(t),Pze=i(y),nn=s(y,"LI",{});var f7=n(nn);bH=s(f7,"STRONG",{});var Y8r=n(bH);$ze=r(Y8r,"layoutlmv2"),Y8r.forEach(t),Ize=r(f7," \u2014 "),xB=s(f7,"A",{href:!0});var K8r=n(xB);jze=r(K8r,"LayoutLMv2Tokenizer"),K8r.forEach(t),Nze=r(f7," or "),RB=s(f7,"A",{href:!0});var Z8r=n(RB);Dze=r(Z8r,"LayoutLMv2TokenizerFast"),Z8r.forEach(t),qze=r(f7," (LayoutLMv2 model)"),f7.forEach(t),Gze=i(y),ln=s(y,"LI",{});var g7=n(ln);vH=s(g7,"STRONG",{});var e9r=n(vH);Oze=r(e9r,"layoutxlm"),e9r.forEach(t),Xze=r(g7," \u2014 "),SB=s(g7,"A",{href:!0});var o9r=n(SB);zze=r(o9r,"LayoutXLMTokenizer"),o9r.forEach(t),Vze=r(g7," or "),PB=s(g7,"A",{href:!0});var r9r=n(PB);Wze=r(r9r,"LayoutXLMTokenizerFast"),r9r.forEach(t),Qze=r(g7," (LayoutXLM model)"),g7.forEach(t),Hze=i(y),dn=s(y,"LI",{});var h7=n(dn);TH=s(h7,"STRONG",{});var t9r=n(TH);Uze=r(t9r,"led"),t9r.forEach(t),Jze=r(h7," \u2014 "),$B=s(h7,"A",{href:!0});var a9r=n($B);Yze=r(a9r,"LEDTokenizer"),a9r.forEach(t),Kze=r(h7," or "),IB=s(h7,"A",{href:!0});var s9r=n(IB);Zze=r(s9r,"LEDTokenizerFast"),s9r.forEach(t),eVe=r(h7," (LED model)"),h7.forEach(t),oVe=i(y),cn=s(y,"LI",{});var u7=n(cn);FH=s(u7,"STRONG",{});var n9r=n(FH);rVe=r(n9r,"longformer"),n9r.forEach(t),tVe=r(u7," \u2014 "),jB=s(u7,"A",{href:!0});var l9r=n(jB);aVe=r(l9r,"LongformerTokenizer"),l9r.forEach(t),sVe=r(u7," or "),NB=s(u7,"A",{href:!0});var i9r=n(NB);nVe=r(i9r,"LongformerTokenizerFast"),i9r.forEach(t),lVe=r(u7," (Longformer model)"),u7.forEach(t),iVe=i(y),Lg=s(y,"LI",{});var AFe=n(Lg);CH=s(AFe,"STRONG",{});var d9r=n(CH);dVe=r(d9r,"luke"),d9r.forEach(t),cVe=r(AFe," \u2014 "),DB=s(AFe,"A",{href:!0});var c9r=n(DB);mVe=r(c9r,"LukeTokenizer"),c9r.forEach(t),fVe=r(AFe," (LUKE model)"),AFe.forEach(t),gVe=i(y),mn=s(y,"LI",{});var p7=n(mn);MH=s(p7,"STRONG",{});var m9r=n(MH);hVe=r(m9r,"lxmert"),m9r.forEach(t),uVe=r(p7," \u2014 "),qB=s(p7,"A",{href:!0});var f9r=n(qB);pVe=r(f9r,"LxmertTokenizer"),f9r.forEach(t),_Ve=r(p7," or "),GB=s(p7,"A",{href:!0});var g9r=n(GB);bVe=r(g9r,"LxmertTokenizerFast"),g9r.forEach(t),vVe=r(p7," (LXMERT model)"),p7.forEach(t),TVe=i(y),Bg=s(y,"LI",{});var LFe=n(Bg);EH=s(LFe,"STRONG",{});var h9r=n(EH);FVe=r(h9r,"m2m_100"),h9r.forEach(t),CVe=r(LFe," \u2014 "),OB=s(LFe,"A",{href:!0});var u9r=n(OB);MVe=r(u9r,"M2M100Tokenizer"),u9r.forEach(t),EVe=r(LFe," (M2M100 model)"),LFe.forEach(t),yVe=i(y),kg=s(y,"LI",{});var BFe=n(kg);yH=s(BFe,"STRONG",{});var p9r=n(yH);wVe=r(p9r,"marian"),p9r.forEach(t),AVe=r(BFe," \u2014 "),XB=s(BFe,"A",{href:!0});var _9r=n(XB);LVe=r(_9r,"MarianTokenizer"),_9r.forEach(t),BVe=r(BFe," (Marian model)"),BFe.forEach(t),kVe=i(y),fn=s(y,"LI",{});var _7=n(fn);wH=s(_7,"STRONG",{});var b9r=n(wH);xVe=r(b9r,"mbart"),b9r.forEach(t),RVe=r(_7," \u2014 "),zB=s(_7,"A",{href:!0});var v9r=n(zB);SVe=r(v9r,"MBartTokenizer"),v9r.forEach(t),PVe=r(_7," or "),VB=s(_7,"A",{href:!0});var T9r=n(VB);$Ve=r(T9r,"MBartTokenizerFast"),T9r.forEach(t),IVe=r(_7," (mBART model)"),_7.forEach(t),jVe=i(y),gn=s(y,"LI",{});var b7=n(gn);AH=s(b7,"STRONG",{});var F9r=n(AH);NVe=r(F9r,"mbart50"),F9r.forEach(t),DVe=r(b7," \u2014 "),WB=s(b7,"A",{href:!0});var C9r=n(WB);qVe=r(C9r,"MBart50Tokenizer"),C9r.forEach(t),GVe=r(b7," or "),QB=s(b7,"A",{href:!0});var M9r=n(QB);OVe=r(M9r,"MBart50TokenizerFast"),M9r.forEach(t),XVe=r(b7," (mBART-50 model)"),b7.forEach(t),zVe=i(y),xg=s(y,"LI",{});var kFe=n(xg);LH=s(kFe,"STRONG",{});var E9r=n(LH);VVe=r(E9r,"mluke"),E9r.forEach(t),WVe=r(kFe," \u2014 "),HB=s(kFe,"A",{href:!0});var y9r=n(HB);QVe=r(y9r,"MLukeTokenizer"),y9r.forEach(t),HVe=r(kFe," (mLUKE model)"),kFe.forEach(t),UVe=i(y),hn=s(y,"LI",{});var v7=n(hn);BH=s(v7,"STRONG",{});var w9r=n(BH);JVe=r(w9r,"mobilebert"),w9r.forEach(t),YVe=r(v7," \u2014 "),UB=s(v7,"A",{href:!0});var A9r=n(UB);KVe=r(A9r,"MobileBertTokenizer"),A9r.forEach(t),ZVe=r(v7," or "),JB=s(v7,"A",{href:!0});var L9r=n(JB);eWe=r(L9r,"MobileBertTokenizerFast"),L9r.forEach(t),oWe=r(v7," (MobileBERT model)"),v7.forEach(t),rWe=i(y),un=s(y,"LI",{});var T7=n(un);kH=s(T7,"STRONG",{});var B9r=n(kH);tWe=r(B9r,"mpnet"),B9r.forEach(t),aWe=r(T7," \u2014 "),YB=s(T7,"A",{href:!0});var k9r=n(YB);sWe=r(k9r,"MPNetTokenizer"),k9r.forEach(t),nWe=r(T7," or "),KB=s(T7,"A",{href:!0});var x9r=n(KB);lWe=r(x9r,"MPNetTokenizerFast"),x9r.forEach(t),iWe=r(T7," (MPNet model)"),T7.forEach(t),dWe=i(y),pn=s(y,"LI",{});var F7=n(pn);xH=s(F7,"STRONG",{});var R9r=n(xH);cWe=r(R9r,"mt5"),R9r.forEach(t),mWe=r(F7," \u2014 "),ZB=s(F7,"A",{href:!0});var S9r=n(ZB);fWe=r(S9r,"MT5Tokenizer"),S9r.forEach(t),gWe=r(F7," or "),ek=s(F7,"A",{href:!0});var P9r=n(ek);hWe=r(P9r,"MT5TokenizerFast"),P9r.forEach(t),uWe=r(F7," (mT5 model)"),F7.forEach(t),pWe=i(y),_n=s(y,"LI",{});var C7=n(_n);RH=s(C7,"STRONG",{});var $9r=n(RH);_We=r($9r,"openai-gpt"),$9r.forEach(t),bWe=r(C7," \u2014 "),ok=s(C7,"A",{href:!0});var I9r=n(ok);vWe=r(I9r,"OpenAIGPTTokenizer"),I9r.forEach(t),TWe=r(C7," or "),rk=s(C7,"A",{href:!0});var j9r=n(rk);FWe=r(j9r,"OpenAIGPTTokenizerFast"),j9r.forEach(t),CWe=r(C7," (OpenAI GPT model)"),C7.forEach(t),MWe=i(y),bn=s(y,"LI",{});var M7=n(bn);SH=s(M7,"STRONG",{});var N9r=n(SH);EWe=r(N9r,"pegasus"),N9r.forEach(t),yWe=r(M7," \u2014 "),tk=s(M7,"A",{href:!0});var D9r=n(tk);wWe=r(D9r,"PegasusTokenizer"),D9r.forEach(t),AWe=r(M7," or "),ak=s(M7,"A",{href:!0});var q9r=n(ak);LWe=r(q9r,"PegasusTokenizerFast"),q9r.forEach(t),BWe=r(M7," (Pegasus model)"),M7.forEach(t),kWe=i(y),Rg=s(y,"LI",{});var xFe=n(Rg);PH=s(xFe,"STRONG",{});var G9r=n(PH);xWe=r(G9r,"perceiver"),G9r.forEach(t),RWe=r(xFe," \u2014 "),sk=s(xFe,"A",{href:!0});var O9r=n(sk);SWe=r(O9r,"PerceiverTokenizer"),O9r.forEach(t),PWe=r(xFe," (Perceiver model)"),xFe.forEach(t),$We=i(y),Sg=s(y,"LI",{});var RFe=n(Sg);$H=s(RFe,"STRONG",{});var X9r=n($H);IWe=r(X9r,"phobert"),X9r.forEach(t),jWe=r(RFe," \u2014 "),nk=s(RFe,"A",{href:!0});var z9r=n(nk);NWe=r(z9r,"PhobertTokenizer"),z9r.forEach(t),DWe=r(RFe," (PhoBERT model)"),RFe.forEach(t),qWe=i(y),Pg=s(y,"LI",{});var SFe=n(Pg);IH=s(SFe,"STRONG",{});var V9r=n(IH);GWe=r(V9r,"plbart"),V9r.forEach(t),OWe=r(SFe," \u2014 "),lk=s(SFe,"A",{href:!0});var W9r=n(lk);XWe=r(W9r,"PLBartTokenizer"),W9r.forEach(t),zWe=r(SFe," (PLBart model)"),SFe.forEach(t),VWe=i(y),$g=s(y,"LI",{});var PFe=n($g);jH=s(PFe,"STRONG",{});var Q9r=n(jH);WWe=r(Q9r,"prophetnet"),Q9r.forEach(t),QWe=r(PFe," \u2014 "),ik=s(PFe,"A",{href:!0});var H9r=n(ik);HWe=r(H9r,"ProphetNetTokenizer"),H9r.forEach(t),UWe=r(PFe," (ProphetNet model)"),PFe.forEach(t),JWe=i(y),vn=s(y,"LI",{});var E7=n(vn);NH=s(E7,"STRONG",{});var U9r=n(NH);YWe=r(U9r,"qdqbert"),U9r.forEach(t),KWe=r(E7," \u2014 "),dk=s(E7,"A",{href:!0});var J9r=n(dk);ZWe=r(J9r,"BertTokenizer"),J9r.forEach(t),eQe=r(E7," or "),ck=s(E7,"A",{href:!0});var Y9r=n(ck);oQe=r(Y9r,"BertTokenizerFast"),Y9r.forEach(t),rQe=r(E7," (QDQBert model)"),E7.forEach(t),tQe=i(y),Ig=s(y,"LI",{});var $Fe=n(Ig);DH=s($Fe,"STRONG",{});var K9r=n(DH);aQe=r(K9r,"rag"),K9r.forEach(t),sQe=r($Fe," \u2014 "),mk=s($Fe,"A",{href:!0});var Z9r=n(mk);nQe=r(Z9r,"RagTokenizer"),Z9r.forEach(t),lQe=r($Fe," (RAG model)"),$Fe.forEach(t),iQe=i(y),Tn=s(y,"LI",{});var y7=n(Tn);qH=s(y7,"STRONG",{});var eBr=n(qH);dQe=r(eBr,"reformer"),eBr.forEach(t),cQe=r(y7," \u2014 "),fk=s(y7,"A",{href:!0});var oBr=n(fk);mQe=r(oBr,"ReformerTokenizer"),oBr.forEach(t),fQe=r(y7," or "),gk=s(y7,"A",{href:!0});var rBr=n(gk);gQe=r(rBr,"ReformerTokenizerFast"),rBr.forEach(t),hQe=r(y7," (Reformer model)"),y7.forEach(t),uQe=i(y),Fn=s(y,"LI",{});var w7=n(Fn);GH=s(w7,"STRONG",{});var tBr=n(GH);pQe=r(tBr,"rembert"),tBr.forEach(t),_Qe=r(w7," \u2014 "),hk=s(w7,"A",{href:!0});var aBr=n(hk);bQe=r(aBr,"RemBertTokenizer"),aBr.forEach(t),vQe=r(w7," or "),uk=s(w7,"A",{href:!0});var sBr=n(uk);TQe=r(sBr,"RemBertTokenizerFast"),sBr.forEach(t),FQe=r(w7," (RemBERT model)"),w7.forEach(t),CQe=i(y),Cn=s(y,"LI",{});var A7=n(Cn);OH=s(A7,"STRONG",{});var nBr=n(OH);MQe=r(nBr,"retribert"),nBr.forEach(t),EQe=r(A7," \u2014 "),pk=s(A7,"A",{href:!0});var lBr=n(pk);yQe=r(lBr,"RetriBertTokenizer"),lBr.forEach(t),wQe=r(A7," or "),_k=s(A7,"A",{href:!0});var iBr=n(_k);AQe=r(iBr,"RetriBertTokenizerFast"),iBr.forEach(t),LQe=r(A7," (RetriBERT model)"),A7.forEach(t),BQe=i(y),Mn=s(y,"LI",{});var L7=n(Mn);XH=s(L7,"STRONG",{});var dBr=n(XH);kQe=r(dBr,"roberta"),dBr.forEach(t),xQe=r(L7," \u2014 "),bk=s(L7,"A",{href:!0});var cBr=n(bk);RQe=r(cBr,"RobertaTokenizer"),cBr.forEach(t),SQe=r(L7," or "),vk=s(L7,"A",{href:!0});var mBr=n(vk);PQe=r(mBr,"RobertaTokenizerFast"),mBr.forEach(t),$Qe=r(L7," (RoBERTa model)"),L7.forEach(t),IQe=i(y),En=s(y,"LI",{});var B7=n(En);zH=s(B7,"STRONG",{});var fBr=n(zH);jQe=r(fBr,"roformer"),fBr.forEach(t),NQe=r(B7," \u2014 "),Tk=s(B7,"A",{href:!0});var gBr=n(Tk);DQe=r(gBr,"RoFormerTokenizer"),gBr.forEach(t),qQe=r(B7," or "),Fk=s(B7,"A",{href:!0});var hBr=n(Fk);GQe=r(hBr,"RoFormerTokenizerFast"),hBr.forEach(t),OQe=r(B7," (RoFormer model)"),B7.forEach(t),XQe=i(y),jg=s(y,"LI",{});var IFe=n(jg);VH=s(IFe,"STRONG",{});var uBr=n(VH);zQe=r(uBr,"speech_to_text"),uBr.forEach(t),VQe=r(IFe," \u2014 "),Ck=s(IFe,"A",{href:!0});var pBr=n(Ck);WQe=r(pBr,"Speech2TextTokenizer"),pBr.forEach(t),QQe=r(IFe," (Speech2Text model)"),IFe.forEach(t),HQe=i(y),Ng=s(y,"LI",{});var jFe=n(Ng);WH=s(jFe,"STRONG",{});var _Br=n(WH);UQe=r(_Br,"speech_to_text_2"),_Br.forEach(t),JQe=r(jFe," \u2014 "),Mk=s(jFe,"A",{href:!0});var bBr=n(Mk);YQe=r(bBr,"Speech2Text2Tokenizer"),bBr.forEach(t),KQe=r(jFe," (Speech2Text2 model)"),jFe.forEach(t),ZQe=i(y),yn=s(y,"LI",{});var k7=n(yn);QH=s(k7,"STRONG",{});var vBr=n(QH);eHe=r(vBr,"splinter"),vBr.forEach(t),oHe=r(k7," \u2014 "),Ek=s(k7,"A",{href:!0});var TBr=n(Ek);rHe=r(TBr,"SplinterTokenizer"),TBr.forEach(t),tHe=r(k7," or "),yk=s(k7,"A",{href:!0});var FBr=n(yk);aHe=r(FBr,"SplinterTokenizerFast"),FBr.forEach(t),sHe=r(k7," (Splinter model)"),k7.forEach(t),nHe=i(y),wn=s(y,"LI",{});var x7=n(wn);HH=s(x7,"STRONG",{});var CBr=n(HH);lHe=r(CBr,"squeezebert"),CBr.forEach(t),iHe=r(x7," \u2014 "),wk=s(x7,"A",{href:!0});var MBr=n(wk);dHe=r(MBr,"SqueezeBertTokenizer"),MBr.forEach(t),cHe=r(x7," or "),Ak=s(x7,"A",{href:!0});var EBr=n(Ak);mHe=r(EBr,"SqueezeBertTokenizerFast"),EBr.forEach(t),fHe=r(x7," (SqueezeBERT model)"),x7.forEach(t),gHe=i(y),An=s(y,"LI",{});var R7=n(An);UH=s(R7,"STRONG",{});var yBr=n(UH);hHe=r(yBr,"t5"),yBr.forEach(t),uHe=r(R7," \u2014 "),Lk=s(R7,"A",{href:!0});var wBr=n(Lk);pHe=r(wBr,"T5Tokenizer"),wBr.forEach(t),_He=r(R7," or "),Bk=s(R7,"A",{href:!0});var ABr=n(Bk);bHe=r(ABr,"T5TokenizerFast"),ABr.forEach(t),vHe=r(R7," (T5 model)"),R7.forEach(t),THe=i(y),Dg=s(y,"LI",{});var NFe=n(Dg);JH=s(NFe,"STRONG",{});var LBr=n(JH);FHe=r(LBr,"tapas"),LBr.forEach(t),CHe=r(NFe," \u2014 "),kk=s(NFe,"A",{href:!0});var BBr=n(kk);MHe=r(BBr,"TapasTokenizer"),BBr.forEach(t),EHe=r(NFe," (TAPAS model)"),NFe.forEach(t),yHe=i(y),qg=s(y,"LI",{});var DFe=n(qg);YH=s(DFe,"STRONG",{});var kBr=n(YH);wHe=r(kBr,"transfo-xl"),kBr.forEach(t),AHe=r(DFe," \u2014 "),xk=s(DFe,"A",{href:!0});var xBr=n(xk);LHe=r(xBr,"TransfoXLTokenizer"),xBr.forEach(t),BHe=r(DFe," (Transformer-XL model)"),DFe.forEach(t),kHe=i(y),Gg=s(y,"LI",{});var qFe=n(Gg);KH=s(qFe,"STRONG",{});var RBr=n(KH);xHe=r(RBr,"wav2vec2"),RBr.forEach(t),RHe=r(qFe," \u2014 "),Rk=s(qFe,"A",{href:!0});var SBr=n(Rk);SHe=r(SBr,"Wav2Vec2CTCTokenizer"),SBr.forEach(t),PHe=r(qFe," (Wav2Vec2 model)"),qFe.forEach(t),$He=i(y),Og=s(y,"LI",{});var GFe=n(Og);ZH=s(GFe,"STRONG",{});var PBr=n(ZH);IHe=r(PBr,"wav2vec2_phoneme"),PBr.forEach(t),jHe=r(GFe," \u2014 "),Sk=s(GFe,"A",{href:!0});var $Br=n(Sk);NHe=r($Br,"Wav2Vec2PhonemeCTCTokenizer"),$Br.forEach(t),DHe=r(GFe," (Wav2Vec2Phoneme model)"),GFe.forEach(t),qHe=i(y),Ln=s(y,"LI",{});var S7=n(Ln);eU=s(S7,"STRONG",{});var IBr=n(eU);GHe=r(IBr,"xglm"),IBr.forEach(t),OHe=r(S7," \u2014 "),Pk=s(S7,"A",{href:!0});var jBr=n(Pk);XHe=r(jBr,"XGLMTokenizer"),jBr.forEach(t),zHe=r(S7," or "),$k=s(S7,"A",{href:!0});var NBr=n($k);VHe=r(NBr,"XGLMTokenizerFast"),NBr.forEach(t),WHe=r(S7," (XGLM model)"),S7.forEach(t),QHe=i(y),Xg=s(y,"LI",{});var OFe=n(Xg);oU=s(OFe,"STRONG",{});var DBr=n(oU);HHe=r(DBr,"xlm"),DBr.forEach(t),UHe=r(OFe," \u2014 "),Ik=s(OFe,"A",{href:!0});var qBr=n(Ik);JHe=r(qBr,"XLMTokenizer"),qBr.forEach(t),YHe=r(OFe," (XLM model)"),OFe.forEach(t),KHe=i(y),zg=s(y,"LI",{});var XFe=n(zg);rU=s(XFe,"STRONG",{});var GBr=n(rU);ZHe=r(GBr,"xlm-prophetnet"),GBr.forEach(t),eUe=r(XFe," \u2014 "),jk=s(XFe,"A",{href:!0});var OBr=n(jk);oUe=r(OBr,"XLMProphetNetTokenizer"),OBr.forEach(t),rUe=r(XFe," (XLMProphetNet model)"),XFe.forEach(t),tUe=i(y),Bn=s(y,"LI",{});var P7=n(Bn);tU=s(P7,"STRONG",{});var XBr=n(tU);aUe=r(XBr,"xlm-roberta"),XBr.forEach(t),sUe=r(P7," \u2014 "),Nk=s(P7,"A",{href:!0});var zBr=n(Nk);nUe=r(zBr,"XLMRobertaTokenizer"),zBr.forEach(t),lUe=r(P7," or "),Dk=s(P7,"A",{href:!0});var VBr=n(Dk);iUe=r(VBr,"XLMRobertaTokenizerFast"),VBr.forEach(t),dUe=r(P7," (XLM-RoBERTa model)"),P7.forEach(t),cUe=i(y),kn=s(y,"LI",{});var $7=n(kn);aU=s($7,"STRONG",{});var WBr=n(aU);mUe=r(WBr,"xlnet"),WBr.forEach(t),fUe=r($7," \u2014 "),qk=s($7,"A",{href:!0});var QBr=n(qk);gUe=r(QBr,"XLNetTokenizer"),QBr.forEach(t),hUe=r($7," or "),Gk=s($7,"A",{href:!0});var HBr=n(Gk);uUe=r(HBr,"XLNetTokenizerFast"),HBr.forEach(t),pUe=r($7," (XLNet model)"),$7.forEach(t),y.forEach(t),_Ue=i(da),sU=s(da,"P",{});var UBr=n(sU);bUe=r(UBr,"Examples:"),UBr.forEach(t),vUe=i(da),f(v5.$$.fragment,da),da.forEach(t),TUe=i($n),Vg=s($n,"DIV",{class:!0});var hBe=n(Vg);f(T5.$$.fragment,hBe),FUe=i(hBe),nU=s(hBe,"P",{});var JBr=n(nU);CUe=r(JBr,"Register a new tokenizer in this mapping."),JBr.forEach(t),hBe.forEach(t),$n.forEach(t),f8e=i(d),ji=s(d,"H2",{class:!0});var uBe=n(ji);Wg=s(uBe,"A",{id:!0,class:!0,href:!0});var YBr=n(Wg);lU=s(YBr,"SPAN",{});var KBr=n(lU);f(F5.$$.fragment,KBr),KBr.forEach(t),YBr.forEach(t),MUe=i(uBe),iU=s(uBe,"SPAN",{});var ZBr=n(iU);EUe=r(ZBr,"AutoFeatureExtractor"),ZBr.forEach(t),uBe.forEach(t),g8e=i(d),Xo=s(d,"DIV",{class:!0});var In=n(Xo);f(C5.$$.fragment,In),yUe=i(In),M5=s(In,"P",{});var pBe=n(M5);wUe=r(pBe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),Ok=s(pBe,"A",{href:!0});var ekr=n(Ok);AUe=r(ekr,"AutoFeatureExtractor.from_pretrained()"),ekr.forEach(t),LUe=r(pBe," class method."),pBe.forEach(t),BUe=i(In),E5=s(In,"P",{});var _Be=n(E5);kUe=r(_Be,"This class cannot be instantiated directly using "),dU=s(_Be,"CODE",{});var okr=n(dU);xUe=r(okr,"__init__()"),okr.forEach(t),RUe=r(_Be," (throws an error)."),_Be.forEach(t),SUe=i(In),Le=s(In,"DIV",{class:!0});var xt=n(Le);f(y5.$$.fragment,xt),PUe=i(xt),cU=s(xt,"P",{});var rkr=n(cU);$Ue=r(rkr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),rkr.forEach(t),IUe=i(xt),Na=s(xt,"P",{});var n3=n(Na);jUe=r(n3,"The feature extractor class to instantiate is selected based on the "),mU=s(n3,"CODE",{});var tkr=n(mU);NUe=r(tkr,"model_type"),tkr.forEach(t),DUe=r(n3,` property of the config object
(either passed as an argument or loaded from `),fU=s(n3,"CODE",{});var akr=n(fU);qUe=r(akr,"pretrained_model_name_or_path"),akr.forEach(t),GUe=r(n3,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),gU=s(n3,"CODE",{});var skr=n(gU);OUe=r(skr,"pretrained_model_name_or_path"),skr.forEach(t),XUe=r(n3,":"),n3.forEach(t),zUe=i(xt),ne=s(xt,"UL",{});var de=n(ne);Qg=s(de,"LI",{});var zFe=n(Qg);hU=s(zFe,"STRONG",{});var nkr=n(hU);VUe=r(nkr,"beit"),nkr.forEach(t),WUe=r(zFe," \u2014 "),Xk=s(zFe,"A",{href:!0});var lkr=n(Xk);QUe=r(lkr,"BeitFeatureExtractor"),lkr.forEach(t),HUe=r(zFe," (BEiT model)"),zFe.forEach(t),UUe=i(de),Hg=s(de,"LI",{});var VFe=n(Hg);uU=s(VFe,"STRONG",{});var ikr=n(uU);JUe=r(ikr,"clip"),ikr.forEach(t),YUe=r(VFe," \u2014 "),zk=s(VFe,"A",{href:!0});var dkr=n(zk);KUe=r(dkr,"CLIPFeatureExtractor"),dkr.forEach(t),ZUe=r(VFe," (CLIP model)"),VFe.forEach(t),eJe=i(de),Ug=s(de,"LI",{});var WFe=n(Ug);pU=s(WFe,"STRONG",{});var ckr=n(pU);oJe=r(ckr,"convnext"),ckr.forEach(t),rJe=r(WFe," \u2014 "),Vk=s(WFe,"A",{href:!0});var mkr=n(Vk);tJe=r(mkr,"ConvNextFeatureExtractor"),mkr.forEach(t),aJe=r(WFe," (ConvNext model)"),WFe.forEach(t),sJe=i(de),Jg=s(de,"LI",{});var QFe=n(Jg);_U=s(QFe,"STRONG",{});var fkr=n(_U);nJe=r(fkr,"deit"),fkr.forEach(t),lJe=r(QFe," \u2014 "),Wk=s(QFe,"A",{href:!0});var gkr=n(Wk);iJe=r(gkr,"DeiTFeatureExtractor"),gkr.forEach(t),dJe=r(QFe," (DeiT model)"),QFe.forEach(t),cJe=i(de),Yg=s(de,"LI",{});var HFe=n(Yg);bU=s(HFe,"STRONG",{});var hkr=n(bU);mJe=r(hkr,"detr"),hkr.forEach(t),fJe=r(HFe," \u2014 "),Qk=s(HFe,"A",{href:!0});var ukr=n(Qk);gJe=r(ukr,"DetrFeatureExtractor"),ukr.forEach(t),hJe=r(HFe," (DETR model)"),HFe.forEach(t),uJe=i(de),Kg=s(de,"LI",{});var UFe=n(Kg);vU=s(UFe,"STRONG",{});var pkr=n(vU);pJe=r(pkr,"hubert"),pkr.forEach(t),_Je=r(UFe," \u2014 "),Hk=s(UFe,"A",{href:!0});var _kr=n(Hk);bJe=r(_kr,"Wav2Vec2FeatureExtractor"),_kr.forEach(t),vJe=r(UFe," (Hubert model)"),UFe.forEach(t),TJe=i(de),Zg=s(de,"LI",{});var JFe=n(Zg);TU=s(JFe,"STRONG",{});var bkr=n(TU);FJe=r(bkr,"layoutlmv2"),bkr.forEach(t),CJe=r(JFe," \u2014 "),Uk=s(JFe,"A",{href:!0});var vkr=n(Uk);MJe=r(vkr,"LayoutLMv2FeatureExtractor"),vkr.forEach(t),EJe=r(JFe," (LayoutLMv2 model)"),JFe.forEach(t),yJe=i(de),eh=s(de,"LI",{});var YFe=n(eh);FU=s(YFe,"STRONG",{});var Tkr=n(FU);wJe=r(Tkr,"perceiver"),Tkr.forEach(t),AJe=r(YFe," \u2014 "),Jk=s(YFe,"A",{href:!0});var Fkr=n(Jk);LJe=r(Fkr,"PerceiverFeatureExtractor"),Fkr.forEach(t),BJe=r(YFe," (Perceiver model)"),YFe.forEach(t),kJe=i(de),oh=s(de,"LI",{});var KFe=n(oh);CU=s(KFe,"STRONG",{});var Ckr=n(CU);xJe=r(Ckr,"poolformer"),Ckr.forEach(t),RJe=r(KFe," \u2014 "),Yk=s(KFe,"A",{href:!0});var Mkr=n(Yk);SJe=r(Mkr,"PoolFormerFeatureExtractor"),Mkr.forEach(t),PJe=r(KFe," (PoolFormer model)"),KFe.forEach(t),$Je=i(de),rh=s(de,"LI",{});var ZFe=n(rh);MU=s(ZFe,"STRONG",{});var Ekr=n(MU);IJe=r(Ekr,"segformer"),Ekr.forEach(t),jJe=r(ZFe," \u2014 "),Kk=s(ZFe,"A",{href:!0});var ykr=n(Kk);NJe=r(ykr,"SegformerFeatureExtractor"),ykr.forEach(t),DJe=r(ZFe," (SegFormer model)"),ZFe.forEach(t),qJe=i(de),th=s(de,"LI",{});var eCe=n(th);EU=s(eCe,"STRONG",{});var wkr=n(EU);GJe=r(wkr,"speech_to_text"),wkr.forEach(t),OJe=r(eCe," \u2014 "),Zk=s(eCe,"A",{href:!0});var Akr=n(Zk);XJe=r(Akr,"Speech2TextFeatureExtractor"),Akr.forEach(t),zJe=r(eCe," (Speech2Text model)"),eCe.forEach(t),VJe=i(de),ah=s(de,"LI",{});var oCe=n(ah);yU=s(oCe,"STRONG",{});var Lkr=n(yU);WJe=r(Lkr,"swin"),Lkr.forEach(t),QJe=r(oCe," \u2014 "),ex=s(oCe,"A",{href:!0});var Bkr=n(ex);HJe=r(Bkr,"ViTFeatureExtractor"),Bkr.forEach(t),UJe=r(oCe," (Swin model)"),oCe.forEach(t),JJe=i(de),sh=s(de,"LI",{});var rCe=n(sh);wU=s(rCe,"STRONG",{});var kkr=n(wU);YJe=r(kkr,"vit"),kkr.forEach(t),KJe=r(rCe," \u2014 "),ox=s(rCe,"A",{href:!0});var xkr=n(ox);ZJe=r(xkr,"ViTFeatureExtractor"),xkr.forEach(t),eYe=r(rCe," (ViT model)"),rCe.forEach(t),oYe=i(de),nh=s(de,"LI",{});var tCe=n(nh);AU=s(tCe,"STRONG",{});var Rkr=n(AU);rYe=r(Rkr,"vit_mae"),Rkr.forEach(t),tYe=r(tCe," \u2014 "),rx=s(tCe,"A",{href:!0});var Skr=n(rx);aYe=r(Skr,"ViTFeatureExtractor"),Skr.forEach(t),sYe=r(tCe," (ViTMAE model)"),tCe.forEach(t),nYe=i(de),lh=s(de,"LI",{});var aCe=n(lh);LU=s(aCe,"STRONG",{});var Pkr=n(LU);lYe=r(Pkr,"wav2vec2"),Pkr.forEach(t),iYe=r(aCe," \u2014 "),tx=s(aCe,"A",{href:!0});var $kr=n(tx);dYe=r($kr,"Wav2Vec2FeatureExtractor"),$kr.forEach(t),cYe=r(aCe," (Wav2Vec2 model)"),aCe.forEach(t),de.forEach(t),mYe=i(xt),f(ih.$$.fragment,xt),fYe=i(xt),BU=s(xt,"P",{});var Ikr=n(BU);gYe=r(Ikr,"Examples:"),Ikr.forEach(t),hYe=i(xt),f(w5.$$.fragment,xt),xt.forEach(t),uYe=i(In),dh=s(In,"DIV",{class:!0});var bBe=n(dh);f(A5.$$.fragment,bBe),pYe=i(bBe),kU=s(bBe,"P",{});var jkr=n(kU);_Ye=r(jkr,"Register a new feature extractor for this class."),jkr.forEach(t),bBe.forEach(t),In.forEach(t),h8e=i(d),Ni=s(d,"H2",{class:!0});var vBe=n(Ni);ch=s(vBe,"A",{id:!0,class:!0,href:!0});var Nkr=n(ch);xU=s(Nkr,"SPAN",{});var Dkr=n(xU);f(L5.$$.fragment,Dkr),Dkr.forEach(t),Nkr.forEach(t),bYe=i(vBe),RU=s(vBe,"SPAN",{});var qkr=n(RU);vYe=r(qkr,"AutoProcessor"),qkr.forEach(t),vBe.forEach(t),u8e=i(d),zo=s(d,"DIV",{class:!0});var jn=n(zo);f(B5.$$.fragment,jn),TYe=i(jn),k5=s(jn,"P",{});var TBe=n(k5);FYe=r(TBe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),ax=s(TBe,"A",{href:!0});var Gkr=n(ax);CYe=r(Gkr,"AutoProcessor.from_pretrained()"),Gkr.forEach(t),MYe=r(TBe," class method."),TBe.forEach(t),EYe=i(jn),x5=s(jn,"P",{});var FBe=n(x5);yYe=r(FBe,"This class cannot be instantiated directly using "),SU=s(FBe,"CODE",{});var Okr=n(SU);wYe=r(Okr,"__init__()"),Okr.forEach(t),AYe=r(FBe," (throws an error)."),FBe.forEach(t),LYe=i(jn),Be=s(jn,"DIV",{class:!0});var Rt=n(Be);f(R5.$$.fragment,Rt),BYe=i(Rt),PU=s(Rt,"P",{});var Xkr=n(PU);kYe=r(Xkr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),Xkr.forEach(t),xYe=i(Rt),Di=s(Rt,"P",{});var VX=n(Di);RYe=r(VX,"The processor class to instantiate is selected based on the "),$U=s(VX,"CODE",{});var zkr=n($U);SYe=r(zkr,"model_type"),zkr.forEach(t),PYe=r(VX,` property of the config object (either
passed as an argument or loaded from `),IU=s(VX,"CODE",{});var Vkr=n(IU);$Ye=r(Vkr,"pretrained_model_name_or_path"),Vkr.forEach(t),IYe=r(VX," if possible):"),VX.forEach(t),jYe=i(Rt),we=s(Rt,"UL",{});var No=n(we);mh=s(No,"LI",{});var sCe=n(mh);jU=s(sCe,"STRONG",{});var Wkr=n(jU);NYe=r(Wkr,"clip"),Wkr.forEach(t),DYe=r(sCe," \u2014 "),sx=s(sCe,"A",{href:!0});var Qkr=n(sx);qYe=r(Qkr,"CLIPProcessor"),Qkr.forEach(t),GYe=r(sCe," (CLIP model)"),sCe.forEach(t),OYe=i(No),fh=s(No,"LI",{});var nCe=n(fh);NU=s(nCe,"STRONG",{});var Hkr=n(NU);XYe=r(Hkr,"layoutlmv2"),Hkr.forEach(t),zYe=r(nCe," \u2014 "),nx=s(nCe,"A",{href:!0});var Ukr=n(nx);VYe=r(Ukr,"LayoutLMv2Processor"),Ukr.forEach(t),WYe=r(nCe," (LayoutLMv2 model)"),nCe.forEach(t),QYe=i(No),gh=s(No,"LI",{});var lCe=n(gh);DU=s(lCe,"STRONG",{});var Jkr=n(DU);HYe=r(Jkr,"layoutxlm"),Jkr.forEach(t),UYe=r(lCe," \u2014 "),lx=s(lCe,"A",{href:!0});var Ykr=n(lx);JYe=r(Ykr,"LayoutXLMProcessor"),Ykr.forEach(t),YYe=r(lCe," (LayoutXLM model)"),lCe.forEach(t),KYe=i(No),hh=s(No,"LI",{});var iCe=n(hh);qU=s(iCe,"STRONG",{});var Kkr=n(qU);ZYe=r(Kkr,"speech_to_text"),Kkr.forEach(t),eKe=r(iCe," \u2014 "),ix=s(iCe,"A",{href:!0});var Zkr=n(ix);oKe=r(Zkr,"Speech2TextProcessor"),Zkr.forEach(t),rKe=r(iCe," (Speech2Text model)"),iCe.forEach(t),tKe=i(No),uh=s(No,"LI",{});var dCe=n(uh);GU=s(dCe,"STRONG",{});var exr=n(GU);aKe=r(exr,"speech_to_text_2"),exr.forEach(t),sKe=r(dCe," \u2014 "),dx=s(dCe,"A",{href:!0});var oxr=n(dx);nKe=r(oxr,"Speech2Text2Processor"),oxr.forEach(t),lKe=r(dCe," (Speech2Text2 model)"),dCe.forEach(t),iKe=i(No),ph=s(No,"LI",{});var cCe=n(ph);OU=s(cCe,"STRONG",{});var rxr=n(OU);dKe=r(rxr,"trocr"),rxr.forEach(t),cKe=r(cCe," \u2014 "),cx=s(cCe,"A",{href:!0});var txr=n(cx);mKe=r(txr,"TrOCRProcessor"),txr.forEach(t),fKe=r(cCe," (TrOCR model)"),cCe.forEach(t),gKe=i(No),_h=s(No,"LI",{});var mCe=n(_h);XU=s(mCe,"STRONG",{});var axr=n(XU);hKe=r(axr,"vision-text-dual-encoder"),axr.forEach(t),uKe=r(mCe," \u2014 "),mx=s(mCe,"A",{href:!0});var sxr=n(mx);pKe=r(sxr,"VisionTextDualEncoderProcessor"),sxr.forEach(t),_Ke=r(mCe," (VisionTextDualEncoder model)"),mCe.forEach(t),bKe=i(No),bh=s(No,"LI",{});var fCe=n(bh);zU=s(fCe,"STRONG",{});var nxr=n(zU);vKe=r(nxr,"wav2vec2"),nxr.forEach(t),TKe=r(fCe," \u2014 "),fx=s(fCe,"A",{href:!0});var lxr=n(fx);FKe=r(lxr,"Wav2Vec2Processor"),lxr.forEach(t),CKe=r(fCe," (Wav2Vec2 model)"),fCe.forEach(t),No.forEach(t),MKe=i(Rt),f(vh.$$.fragment,Rt),EKe=i(Rt),VU=s(Rt,"P",{});var ixr=n(VU);yKe=r(ixr,"Examples:"),ixr.forEach(t),wKe=i(Rt),f(S5.$$.fragment,Rt),Rt.forEach(t),AKe=i(jn),Th=s(jn,"DIV",{class:!0});var CBe=n(Th);f(P5.$$.fragment,CBe),LKe=i(CBe),WU=s(CBe,"P",{});var dxr=n(WU);BKe=r(dxr,"Register a new processor for this class."),dxr.forEach(t),CBe.forEach(t),jn.forEach(t),p8e=i(d),qi=s(d,"H2",{class:!0});var MBe=n(qi);Fh=s(MBe,"A",{id:!0,class:!0,href:!0});var cxr=n(Fh);QU=s(cxr,"SPAN",{});var mxr=n(QU);f($5.$$.fragment,mxr),mxr.forEach(t),cxr.forEach(t),kKe=i(MBe),HU=s(MBe,"SPAN",{});var fxr=n(HU);xKe=r(fxr,"AutoModel"),fxr.forEach(t),MBe.forEach(t),_8e=i(d),Vo=s(d,"DIV",{class:!0});var Nn=n(Vo);f(I5.$$.fragment,Nn),RKe=i(Nn),Gi=s(Nn,"P",{});var WX=n(Gi);SKe=r(WX,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),UU=s(WX,"CODE",{});var gxr=n(UU);PKe=r(gxr,"from_pretrained()"),gxr.forEach(t),$Ke=r(WX,"class method or the "),JU=s(WX,"CODE",{});var hxr=n(JU);IKe=r(hxr,"from_config()"),hxr.forEach(t),jKe=r(WX,`class
method.`),WX.forEach(t),NKe=i(Nn),j5=s(Nn,"P",{});var EBe=n(j5);DKe=r(EBe,"This class cannot be instantiated directly using "),YU=s(EBe,"CODE",{});var uxr=n(YU);qKe=r(uxr,"__init__()"),uxr.forEach(t),GKe=r(EBe," (throws an error)."),EBe.forEach(t),OKe=i(Nn),Nr=s(Nn,"DIV",{class:!0});var Dn=n(Nr);f(N5.$$.fragment,Dn),XKe=i(Dn),KU=s(Dn,"P",{});var pxr=n(KU);zKe=r(pxr,"Instantiates one of the base model classes of the library from a configuration."),pxr.forEach(t),VKe=i(Dn),Oi=s(Dn,"P",{});var QX=n(Oi);WKe=r(QX,`Note:
Loading a model from its configuration file does `),ZU=s(QX,"STRONG",{});var _xr=n(ZU);QKe=r(_xr,"not"),_xr.forEach(t),HKe=r(QX,` load the model weights. It only affects the
model\u2019s configuration. Use `),eJ=s(QX,"CODE",{});var bxr=n(eJ);UKe=r(bxr,"from_pretrained()"),bxr.forEach(t),JKe=r(QX,"to load the model weights."),QX.forEach(t),YKe=i(Dn),oJ=s(Dn,"P",{});var vxr=n(oJ);KKe=r(vxr,"Examples:"),vxr.forEach(t),ZKe=i(Dn),f(D5.$$.fragment,Dn),Dn.forEach(t),eZe=i(Nn),ke=s(Nn,"DIV",{class:!0});var St=n(ke);f(q5.$$.fragment,St),oZe=i(St),rJ=s(St,"P",{});var Txr=n(rJ);rZe=r(Txr,"Instantiate one of the base model classes of the library from a pretrained model."),Txr.forEach(t),tZe=i(St),Da=s(St,"P",{});var l3=n(Da);aZe=r(l3,"The model class to instantiate is selected based on the "),tJ=s(l3,"CODE",{});var Fxr=n(tJ);sZe=r(Fxr,"model_type"),Fxr.forEach(t),nZe=r(l3,` property of the config object (either
passed as an argument or loaded from `),aJ=s(l3,"CODE",{});var Cxr=n(aJ);lZe=r(Cxr,"pretrained_model_name_or_path"),Cxr.forEach(t),iZe=r(l3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),sJ=s(l3,"CODE",{});var Mxr=n(sJ);dZe=r(Mxr,"pretrained_model_name_or_path"),Mxr.forEach(t),cZe=r(l3,":"),l3.forEach(t),mZe=i(St),F=s(St,"UL",{});var C=n(F);Ch=s(C,"LI",{});var gCe=n(Ch);nJ=s(gCe,"STRONG",{});var Exr=n(nJ);fZe=r(Exr,"albert"),Exr.forEach(t),gZe=r(gCe," \u2014 "),gx=s(gCe,"A",{href:!0});var yxr=n(gx);hZe=r(yxr,"AlbertModel"),yxr.forEach(t),uZe=r(gCe," (ALBERT model)"),gCe.forEach(t),pZe=i(C),Mh=s(C,"LI",{});var hCe=n(Mh);lJ=s(hCe,"STRONG",{});var wxr=n(lJ);_Ze=r(wxr,"bart"),wxr.forEach(t),bZe=r(hCe," \u2014 "),hx=s(hCe,"A",{href:!0});var Axr=n(hx);vZe=r(Axr,"BartModel"),Axr.forEach(t),TZe=r(hCe," (BART model)"),hCe.forEach(t),FZe=i(C),Eh=s(C,"LI",{});var uCe=n(Eh);iJ=s(uCe,"STRONG",{});var Lxr=n(iJ);CZe=r(Lxr,"beit"),Lxr.forEach(t),MZe=r(uCe," \u2014 "),ux=s(uCe,"A",{href:!0});var Bxr=n(ux);EZe=r(Bxr,"BeitModel"),Bxr.forEach(t),yZe=r(uCe," (BEiT model)"),uCe.forEach(t),wZe=i(C),yh=s(C,"LI",{});var pCe=n(yh);dJ=s(pCe,"STRONG",{});var kxr=n(dJ);AZe=r(kxr,"bert"),kxr.forEach(t),LZe=r(pCe," \u2014 "),px=s(pCe,"A",{href:!0});var xxr=n(px);BZe=r(xxr,"BertModel"),xxr.forEach(t),kZe=r(pCe," (BERT model)"),pCe.forEach(t),xZe=i(C),wh=s(C,"LI",{});var _Ce=n(wh);cJ=s(_Ce,"STRONG",{});var Rxr=n(cJ);RZe=r(Rxr,"bert-generation"),Rxr.forEach(t),SZe=r(_Ce," \u2014 "),_x=s(_Ce,"A",{href:!0});var Sxr=n(_x);PZe=r(Sxr,"BertGenerationEncoder"),Sxr.forEach(t),$Ze=r(_Ce," (Bert Generation model)"),_Ce.forEach(t),IZe=i(C),Ah=s(C,"LI",{});var bCe=n(Ah);mJ=s(bCe,"STRONG",{});var Pxr=n(mJ);jZe=r(Pxr,"big_bird"),Pxr.forEach(t),NZe=r(bCe," \u2014 "),bx=s(bCe,"A",{href:!0});var $xr=n(bx);DZe=r($xr,"BigBirdModel"),$xr.forEach(t),qZe=r(bCe," (BigBird model)"),bCe.forEach(t),GZe=i(C),Lh=s(C,"LI",{});var vCe=n(Lh);fJ=s(vCe,"STRONG",{});var Ixr=n(fJ);OZe=r(Ixr,"bigbird_pegasus"),Ixr.forEach(t),XZe=r(vCe," \u2014 "),vx=s(vCe,"A",{href:!0});var jxr=n(vx);zZe=r(jxr,"BigBirdPegasusModel"),jxr.forEach(t),VZe=r(vCe," (BigBirdPegasus model)"),vCe.forEach(t),WZe=i(C),Bh=s(C,"LI",{});var TCe=n(Bh);gJ=s(TCe,"STRONG",{});var Nxr=n(gJ);QZe=r(Nxr,"blenderbot"),Nxr.forEach(t),HZe=r(TCe," \u2014 "),Tx=s(TCe,"A",{href:!0});var Dxr=n(Tx);UZe=r(Dxr,"BlenderbotModel"),Dxr.forEach(t),JZe=r(TCe," (Blenderbot model)"),TCe.forEach(t),YZe=i(C),kh=s(C,"LI",{});var FCe=n(kh);hJ=s(FCe,"STRONG",{});var qxr=n(hJ);KZe=r(qxr,"blenderbot-small"),qxr.forEach(t),ZZe=r(FCe," \u2014 "),Fx=s(FCe,"A",{href:!0});var Gxr=n(Fx);eeo=r(Gxr,"BlenderbotSmallModel"),Gxr.forEach(t),oeo=r(FCe," (BlenderbotSmall model)"),FCe.forEach(t),reo=i(C),xh=s(C,"LI",{});var CCe=n(xh);uJ=s(CCe,"STRONG",{});var Oxr=n(uJ);teo=r(Oxr,"camembert"),Oxr.forEach(t),aeo=r(CCe," \u2014 "),Cx=s(CCe,"A",{href:!0});var Xxr=n(Cx);seo=r(Xxr,"CamembertModel"),Xxr.forEach(t),neo=r(CCe," (CamemBERT model)"),CCe.forEach(t),leo=i(C),Rh=s(C,"LI",{});var MCe=n(Rh);pJ=s(MCe,"STRONG",{});var zxr=n(pJ);ieo=r(zxr,"canine"),zxr.forEach(t),deo=r(MCe," \u2014 "),Mx=s(MCe,"A",{href:!0});var Vxr=n(Mx);ceo=r(Vxr,"CanineModel"),Vxr.forEach(t),meo=r(MCe," (Canine model)"),MCe.forEach(t),feo=i(C),Sh=s(C,"LI",{});var ECe=n(Sh);_J=s(ECe,"STRONG",{});var Wxr=n(_J);geo=r(Wxr,"clip"),Wxr.forEach(t),heo=r(ECe," \u2014 "),Ex=s(ECe,"A",{href:!0});var Qxr=n(Ex);ueo=r(Qxr,"CLIPModel"),Qxr.forEach(t),peo=r(ECe," (CLIP model)"),ECe.forEach(t),_eo=i(C),Ph=s(C,"LI",{});var yCe=n(Ph);bJ=s(yCe,"STRONG",{});var Hxr=n(bJ);beo=r(Hxr,"convbert"),Hxr.forEach(t),veo=r(yCe," \u2014 "),yx=s(yCe,"A",{href:!0});var Uxr=n(yx);Teo=r(Uxr,"ConvBertModel"),Uxr.forEach(t),Feo=r(yCe," (ConvBERT model)"),yCe.forEach(t),Ceo=i(C),$h=s(C,"LI",{});var wCe=n($h);vJ=s(wCe,"STRONG",{});var Jxr=n(vJ);Meo=r(Jxr,"convnext"),Jxr.forEach(t),Eeo=r(wCe," \u2014 "),wx=s(wCe,"A",{href:!0});var Yxr=n(wx);yeo=r(Yxr,"ConvNextModel"),Yxr.forEach(t),weo=r(wCe," (ConvNext model)"),wCe.forEach(t),Aeo=i(C),Ih=s(C,"LI",{});var ACe=n(Ih);TJ=s(ACe,"STRONG",{});var Kxr=n(TJ);Leo=r(Kxr,"ctrl"),Kxr.forEach(t),Beo=r(ACe," \u2014 "),Ax=s(ACe,"A",{href:!0});var Zxr=n(Ax);keo=r(Zxr,"CTRLModel"),Zxr.forEach(t),xeo=r(ACe," (CTRL model)"),ACe.forEach(t),Reo=i(C),jh=s(C,"LI",{});var LCe=n(jh);FJ=s(LCe,"STRONG",{});var eRr=n(FJ);Seo=r(eRr,"deberta"),eRr.forEach(t),Peo=r(LCe," \u2014 "),Lx=s(LCe,"A",{href:!0});var oRr=n(Lx);$eo=r(oRr,"DebertaModel"),oRr.forEach(t),Ieo=r(LCe," (DeBERTa model)"),LCe.forEach(t),jeo=i(C),Nh=s(C,"LI",{});var BCe=n(Nh);CJ=s(BCe,"STRONG",{});var rRr=n(CJ);Neo=r(rRr,"deberta-v2"),rRr.forEach(t),Deo=r(BCe," \u2014 "),Bx=s(BCe,"A",{href:!0});var tRr=n(Bx);qeo=r(tRr,"DebertaV2Model"),tRr.forEach(t),Geo=r(BCe," (DeBERTa-v2 model)"),BCe.forEach(t),Oeo=i(C),Dh=s(C,"LI",{});var kCe=n(Dh);MJ=s(kCe,"STRONG",{});var aRr=n(MJ);Xeo=r(aRr,"deit"),aRr.forEach(t),zeo=r(kCe," \u2014 "),kx=s(kCe,"A",{href:!0});var sRr=n(kx);Veo=r(sRr,"DeiTModel"),sRr.forEach(t),Weo=r(kCe," (DeiT model)"),kCe.forEach(t),Qeo=i(C),qh=s(C,"LI",{});var xCe=n(qh);EJ=s(xCe,"STRONG",{});var nRr=n(EJ);Heo=r(nRr,"detr"),nRr.forEach(t),Ueo=r(xCe," \u2014 "),xx=s(xCe,"A",{href:!0});var lRr=n(xx);Jeo=r(lRr,"DetrModel"),lRr.forEach(t),Yeo=r(xCe," (DETR model)"),xCe.forEach(t),Keo=i(C),Gh=s(C,"LI",{});var RCe=n(Gh);yJ=s(RCe,"STRONG",{});var iRr=n(yJ);Zeo=r(iRr,"distilbert"),iRr.forEach(t),eoo=r(RCe," \u2014 "),Rx=s(RCe,"A",{href:!0});var dRr=n(Rx);ooo=r(dRr,"DistilBertModel"),dRr.forEach(t),roo=r(RCe," (DistilBERT model)"),RCe.forEach(t),too=i(C),Oh=s(C,"LI",{});var SCe=n(Oh);wJ=s(SCe,"STRONG",{});var cRr=n(wJ);aoo=r(cRr,"dpr"),cRr.forEach(t),soo=r(SCe," \u2014 "),Sx=s(SCe,"A",{href:!0});var mRr=n(Sx);noo=r(mRr,"DPRQuestionEncoder"),mRr.forEach(t),loo=r(SCe," (DPR model)"),SCe.forEach(t),ioo=i(C),Xh=s(C,"LI",{});var PCe=n(Xh);AJ=s(PCe,"STRONG",{});var fRr=n(AJ);doo=r(fRr,"electra"),fRr.forEach(t),coo=r(PCe," \u2014 "),Px=s(PCe,"A",{href:!0});var gRr=n(Px);moo=r(gRr,"ElectraModel"),gRr.forEach(t),foo=r(PCe," (ELECTRA model)"),PCe.forEach(t),goo=i(C),zh=s(C,"LI",{});var $Ce=n(zh);LJ=s($Ce,"STRONG",{});var hRr=n(LJ);hoo=r(hRr,"flaubert"),hRr.forEach(t),uoo=r($Ce," \u2014 "),$x=s($Ce,"A",{href:!0});var uRr=n($x);poo=r(uRr,"FlaubertModel"),uRr.forEach(t),_oo=r($Ce," (FlauBERT model)"),$Ce.forEach(t),boo=i(C),Vh=s(C,"LI",{});var ICe=n(Vh);BJ=s(ICe,"STRONG",{});var pRr=n(BJ);voo=r(pRr,"fnet"),pRr.forEach(t),Too=r(ICe," \u2014 "),Ix=s(ICe,"A",{href:!0});var _Rr=n(Ix);Foo=r(_Rr,"FNetModel"),_Rr.forEach(t),Coo=r(ICe," (FNet model)"),ICe.forEach(t),Moo=i(C),Wh=s(C,"LI",{});var jCe=n(Wh);kJ=s(jCe,"STRONG",{});var bRr=n(kJ);Eoo=r(bRr,"fsmt"),bRr.forEach(t),yoo=r(jCe," \u2014 "),jx=s(jCe,"A",{href:!0});var vRr=n(jx);woo=r(vRr,"FSMTModel"),vRr.forEach(t),Aoo=r(jCe," (FairSeq Machine-Translation model)"),jCe.forEach(t),Loo=i(C),xn=s(C,"LI",{});var I7=n(xn);xJ=s(I7,"STRONG",{});var TRr=n(xJ);Boo=r(TRr,"funnel"),TRr.forEach(t),koo=r(I7," \u2014 "),Nx=s(I7,"A",{href:!0});var FRr=n(Nx);xoo=r(FRr,"FunnelModel"),FRr.forEach(t),Roo=r(I7," or "),Dx=s(I7,"A",{href:!0});var CRr=n(Dx);Soo=r(CRr,"FunnelBaseModel"),CRr.forEach(t),Poo=r(I7," (Funnel Transformer model)"),I7.forEach(t),$oo=i(C),Qh=s(C,"LI",{});var NCe=n(Qh);RJ=s(NCe,"STRONG",{});var MRr=n(RJ);Ioo=r(MRr,"gpt2"),MRr.forEach(t),joo=r(NCe," \u2014 "),qx=s(NCe,"A",{href:!0});var ERr=n(qx);Noo=r(ERr,"GPT2Model"),ERr.forEach(t),Doo=r(NCe," (OpenAI GPT-2 model)"),NCe.forEach(t),qoo=i(C),Hh=s(C,"LI",{});var DCe=n(Hh);SJ=s(DCe,"STRONG",{});var yRr=n(SJ);Goo=r(yRr,"gpt_neo"),yRr.forEach(t),Ooo=r(DCe," \u2014 "),Gx=s(DCe,"A",{href:!0});var wRr=n(Gx);Xoo=r(wRr,"GPTNeoModel"),wRr.forEach(t),zoo=r(DCe," (GPT Neo model)"),DCe.forEach(t),Voo=i(C),Uh=s(C,"LI",{});var qCe=n(Uh);PJ=s(qCe,"STRONG",{});var ARr=n(PJ);Woo=r(ARr,"gptj"),ARr.forEach(t),Qoo=r(qCe," \u2014 "),Ox=s(qCe,"A",{href:!0});var LRr=n(Ox);Hoo=r(LRr,"GPTJModel"),LRr.forEach(t),Uoo=r(qCe," (GPT-J model)"),qCe.forEach(t),Joo=i(C),Jh=s(C,"LI",{});var GCe=n(Jh);$J=s(GCe,"STRONG",{});var BRr=n($J);Yoo=r(BRr,"hubert"),BRr.forEach(t),Koo=r(GCe," \u2014 "),Xx=s(GCe,"A",{href:!0});var kRr=n(Xx);Zoo=r(kRr,"HubertModel"),kRr.forEach(t),ero=r(GCe," (Hubert model)"),GCe.forEach(t),oro=i(C),Yh=s(C,"LI",{});var OCe=n(Yh);IJ=s(OCe,"STRONG",{});var xRr=n(IJ);rro=r(xRr,"ibert"),xRr.forEach(t),tro=r(OCe," \u2014 "),zx=s(OCe,"A",{href:!0});var RRr=n(zx);aro=r(RRr,"IBertModel"),RRr.forEach(t),sro=r(OCe," (I-BERT model)"),OCe.forEach(t),nro=i(C),Kh=s(C,"LI",{});var XCe=n(Kh);jJ=s(XCe,"STRONG",{});var SRr=n(jJ);lro=r(SRr,"imagegpt"),SRr.forEach(t),iro=r(XCe," \u2014 "),Vx=s(XCe,"A",{href:!0});var PRr=n(Vx);dro=r(PRr,"ImageGPTModel"),PRr.forEach(t),cro=r(XCe," (ImageGPT model)"),XCe.forEach(t),mro=i(C),Zh=s(C,"LI",{});var zCe=n(Zh);NJ=s(zCe,"STRONG",{});var $Rr=n(NJ);fro=r($Rr,"layoutlm"),$Rr.forEach(t),gro=r(zCe," \u2014 "),Wx=s(zCe,"A",{href:!0});var IRr=n(Wx);hro=r(IRr,"LayoutLMModel"),IRr.forEach(t),uro=r(zCe," (LayoutLM model)"),zCe.forEach(t),pro=i(C),eu=s(C,"LI",{});var VCe=n(eu);DJ=s(VCe,"STRONG",{});var jRr=n(DJ);_ro=r(jRr,"layoutlmv2"),jRr.forEach(t),bro=r(VCe," \u2014 "),Qx=s(VCe,"A",{href:!0});var NRr=n(Qx);vro=r(NRr,"LayoutLMv2Model"),NRr.forEach(t),Tro=r(VCe," (LayoutLMv2 model)"),VCe.forEach(t),Fro=i(C),ou=s(C,"LI",{});var WCe=n(ou);qJ=s(WCe,"STRONG",{});var DRr=n(qJ);Cro=r(DRr,"led"),DRr.forEach(t),Mro=r(WCe," \u2014 "),Hx=s(WCe,"A",{href:!0});var qRr=n(Hx);Ero=r(qRr,"LEDModel"),qRr.forEach(t),yro=r(WCe," (LED model)"),WCe.forEach(t),wro=i(C),ru=s(C,"LI",{});var QCe=n(ru);GJ=s(QCe,"STRONG",{});var GRr=n(GJ);Aro=r(GRr,"longformer"),GRr.forEach(t),Lro=r(QCe," \u2014 "),Ux=s(QCe,"A",{href:!0});var ORr=n(Ux);Bro=r(ORr,"LongformerModel"),ORr.forEach(t),kro=r(QCe," (Longformer model)"),QCe.forEach(t),xro=i(C),tu=s(C,"LI",{});var HCe=n(tu);OJ=s(HCe,"STRONG",{});var XRr=n(OJ);Rro=r(XRr,"luke"),XRr.forEach(t),Sro=r(HCe," \u2014 "),Jx=s(HCe,"A",{href:!0});var zRr=n(Jx);Pro=r(zRr,"LukeModel"),zRr.forEach(t),$ro=r(HCe," (LUKE model)"),HCe.forEach(t),Iro=i(C),au=s(C,"LI",{});var UCe=n(au);XJ=s(UCe,"STRONG",{});var VRr=n(XJ);jro=r(VRr,"lxmert"),VRr.forEach(t),Nro=r(UCe," \u2014 "),Yx=s(UCe,"A",{href:!0});var WRr=n(Yx);Dro=r(WRr,"LxmertModel"),WRr.forEach(t),qro=r(UCe," (LXMERT model)"),UCe.forEach(t),Gro=i(C),su=s(C,"LI",{});var JCe=n(su);zJ=s(JCe,"STRONG",{});var QRr=n(zJ);Oro=r(QRr,"m2m_100"),QRr.forEach(t),Xro=r(JCe," \u2014 "),Kx=s(JCe,"A",{href:!0});var HRr=n(Kx);zro=r(HRr,"M2M100Model"),HRr.forEach(t),Vro=r(JCe," (M2M100 model)"),JCe.forEach(t),Wro=i(C),nu=s(C,"LI",{});var YCe=n(nu);VJ=s(YCe,"STRONG",{});var URr=n(VJ);Qro=r(URr,"marian"),URr.forEach(t),Hro=r(YCe," \u2014 "),Zx=s(YCe,"A",{href:!0});var JRr=n(Zx);Uro=r(JRr,"MarianModel"),JRr.forEach(t),Jro=r(YCe," (Marian model)"),YCe.forEach(t),Yro=i(C),lu=s(C,"LI",{});var KCe=n(lu);WJ=s(KCe,"STRONG",{});var YRr=n(WJ);Kro=r(YRr,"mbart"),YRr.forEach(t),Zro=r(KCe," \u2014 "),eR=s(KCe,"A",{href:!0});var KRr=n(eR);eto=r(KRr,"MBartModel"),KRr.forEach(t),oto=r(KCe," (mBART model)"),KCe.forEach(t),rto=i(C),iu=s(C,"LI",{});var ZCe=n(iu);QJ=s(ZCe,"STRONG",{});var ZRr=n(QJ);tto=r(ZRr,"megatron-bert"),ZRr.forEach(t),ato=r(ZCe," \u2014 "),oR=s(ZCe,"A",{href:!0});var eSr=n(oR);sto=r(eSr,"MegatronBertModel"),eSr.forEach(t),nto=r(ZCe," (MegatronBert model)"),ZCe.forEach(t),lto=i(C),du=s(C,"LI",{});var e4e=n(du);HJ=s(e4e,"STRONG",{});var oSr=n(HJ);ito=r(oSr,"mobilebert"),oSr.forEach(t),dto=r(e4e," \u2014 "),rR=s(e4e,"A",{href:!0});var rSr=n(rR);cto=r(rSr,"MobileBertModel"),rSr.forEach(t),mto=r(e4e," (MobileBERT model)"),e4e.forEach(t),fto=i(C),cu=s(C,"LI",{});var o4e=n(cu);UJ=s(o4e,"STRONG",{});var tSr=n(UJ);gto=r(tSr,"mpnet"),tSr.forEach(t),hto=r(o4e," \u2014 "),tR=s(o4e,"A",{href:!0});var aSr=n(tR);uto=r(aSr,"MPNetModel"),aSr.forEach(t),pto=r(o4e," (MPNet model)"),o4e.forEach(t),_to=i(C),mu=s(C,"LI",{});var r4e=n(mu);JJ=s(r4e,"STRONG",{});var sSr=n(JJ);bto=r(sSr,"mt5"),sSr.forEach(t),vto=r(r4e," \u2014 "),aR=s(r4e,"A",{href:!0});var nSr=n(aR);Tto=r(nSr,"MT5Model"),nSr.forEach(t),Fto=r(r4e," (mT5 model)"),r4e.forEach(t),Cto=i(C),fu=s(C,"LI",{});var t4e=n(fu);YJ=s(t4e,"STRONG",{});var lSr=n(YJ);Mto=r(lSr,"nystromformer"),lSr.forEach(t),Eto=r(t4e," \u2014 "),sR=s(t4e,"A",{href:!0});var iSr=n(sR);yto=r(iSr,"NystromformerModel"),iSr.forEach(t),wto=r(t4e," (Nystromformer model)"),t4e.forEach(t),Ato=i(C),gu=s(C,"LI",{});var a4e=n(gu);KJ=s(a4e,"STRONG",{});var dSr=n(KJ);Lto=r(dSr,"openai-gpt"),dSr.forEach(t),Bto=r(a4e," \u2014 "),nR=s(a4e,"A",{href:!0});var cSr=n(nR);kto=r(cSr,"OpenAIGPTModel"),cSr.forEach(t),xto=r(a4e," (OpenAI GPT model)"),a4e.forEach(t),Rto=i(C),hu=s(C,"LI",{});var s4e=n(hu);ZJ=s(s4e,"STRONG",{});var mSr=n(ZJ);Sto=r(mSr,"pegasus"),mSr.forEach(t),Pto=r(s4e," \u2014 "),lR=s(s4e,"A",{href:!0});var fSr=n(lR);$to=r(fSr,"PegasusModel"),fSr.forEach(t),Ito=r(s4e," (Pegasus model)"),s4e.forEach(t),jto=i(C),uu=s(C,"LI",{});var n4e=n(uu);eY=s(n4e,"STRONG",{});var gSr=n(eY);Nto=r(gSr,"perceiver"),gSr.forEach(t),Dto=r(n4e," \u2014 "),iR=s(n4e,"A",{href:!0});var hSr=n(iR);qto=r(hSr,"PerceiverModel"),hSr.forEach(t),Gto=r(n4e," (Perceiver model)"),n4e.forEach(t),Oto=i(C),pu=s(C,"LI",{});var l4e=n(pu);oY=s(l4e,"STRONG",{});var uSr=n(oY);Xto=r(uSr,"plbart"),uSr.forEach(t),zto=r(l4e," \u2014 "),dR=s(l4e,"A",{href:!0});var pSr=n(dR);Vto=r(pSr,"PLBartModel"),pSr.forEach(t),Wto=r(l4e," (PLBart model)"),l4e.forEach(t),Qto=i(C),_u=s(C,"LI",{});var i4e=n(_u);rY=s(i4e,"STRONG",{});var _Sr=n(rY);Hto=r(_Sr,"poolformer"),_Sr.forEach(t),Uto=r(i4e," \u2014 "),cR=s(i4e,"A",{href:!0});var bSr=n(cR);Jto=r(bSr,"PoolFormerModel"),bSr.forEach(t),Yto=r(i4e," (PoolFormer model)"),i4e.forEach(t),Kto=i(C),bu=s(C,"LI",{});var d4e=n(bu);tY=s(d4e,"STRONG",{});var vSr=n(tY);Zto=r(vSr,"prophetnet"),vSr.forEach(t),eao=r(d4e," \u2014 "),mR=s(d4e,"A",{href:!0});var TSr=n(mR);oao=r(TSr,"ProphetNetModel"),TSr.forEach(t),rao=r(d4e," (ProphetNet model)"),d4e.forEach(t),tao=i(C),vu=s(C,"LI",{});var c4e=n(vu);aY=s(c4e,"STRONG",{});var FSr=n(aY);aao=r(FSr,"qdqbert"),FSr.forEach(t),sao=r(c4e," \u2014 "),fR=s(c4e,"A",{href:!0});var CSr=n(fR);nao=r(CSr,"QDQBertModel"),CSr.forEach(t),lao=r(c4e," (QDQBert model)"),c4e.forEach(t),iao=i(C),Tu=s(C,"LI",{});var m4e=n(Tu);sY=s(m4e,"STRONG",{});var MSr=n(sY);dao=r(MSr,"reformer"),MSr.forEach(t),cao=r(m4e," \u2014 "),gR=s(m4e,"A",{href:!0});var ESr=n(gR);mao=r(ESr,"ReformerModel"),ESr.forEach(t),fao=r(m4e," (Reformer model)"),m4e.forEach(t),gao=i(C),Fu=s(C,"LI",{});var f4e=n(Fu);nY=s(f4e,"STRONG",{});var ySr=n(nY);hao=r(ySr,"rembert"),ySr.forEach(t),uao=r(f4e," \u2014 "),hR=s(f4e,"A",{href:!0});var wSr=n(hR);pao=r(wSr,"RemBertModel"),wSr.forEach(t),_ao=r(f4e," (RemBERT model)"),f4e.forEach(t),bao=i(C),Cu=s(C,"LI",{});var g4e=n(Cu);lY=s(g4e,"STRONG",{});var ASr=n(lY);vao=r(ASr,"retribert"),ASr.forEach(t),Tao=r(g4e," \u2014 "),uR=s(g4e,"A",{href:!0});var LSr=n(uR);Fao=r(LSr,"RetriBertModel"),LSr.forEach(t),Cao=r(g4e," (RetriBERT model)"),g4e.forEach(t),Mao=i(C),Mu=s(C,"LI",{});var h4e=n(Mu);iY=s(h4e,"STRONG",{});var BSr=n(iY);Eao=r(BSr,"roberta"),BSr.forEach(t),yao=r(h4e," \u2014 "),pR=s(h4e,"A",{href:!0});var kSr=n(pR);wao=r(kSr,"RobertaModel"),kSr.forEach(t),Aao=r(h4e," (RoBERTa model)"),h4e.forEach(t),Lao=i(C),Eu=s(C,"LI",{});var u4e=n(Eu);dY=s(u4e,"STRONG",{});var xSr=n(dY);Bao=r(xSr,"roformer"),xSr.forEach(t),kao=r(u4e," \u2014 "),_R=s(u4e,"A",{href:!0});var RSr=n(_R);xao=r(RSr,"RoFormerModel"),RSr.forEach(t),Rao=r(u4e," (RoFormer model)"),u4e.forEach(t),Sao=i(C),yu=s(C,"LI",{});var p4e=n(yu);cY=s(p4e,"STRONG",{});var SSr=n(cY);Pao=r(SSr,"segformer"),SSr.forEach(t),$ao=r(p4e," \u2014 "),bR=s(p4e,"A",{href:!0});var PSr=n(bR);Iao=r(PSr,"SegformerModel"),PSr.forEach(t),jao=r(p4e," (SegFormer model)"),p4e.forEach(t),Nao=i(C),wu=s(C,"LI",{});var _4e=n(wu);mY=s(_4e,"STRONG",{});var $Sr=n(mY);Dao=r($Sr,"sew"),$Sr.forEach(t),qao=r(_4e," \u2014 "),vR=s(_4e,"A",{href:!0});var ISr=n(vR);Gao=r(ISr,"SEWModel"),ISr.forEach(t),Oao=r(_4e," (SEW model)"),_4e.forEach(t),Xao=i(C),Au=s(C,"LI",{});var b4e=n(Au);fY=s(b4e,"STRONG",{});var jSr=n(fY);zao=r(jSr,"sew-d"),jSr.forEach(t),Vao=r(b4e," \u2014 "),TR=s(b4e,"A",{href:!0});var NSr=n(TR);Wao=r(NSr,"SEWDModel"),NSr.forEach(t),Qao=r(b4e," (SEW-D model)"),b4e.forEach(t),Hao=i(C),Lu=s(C,"LI",{});var v4e=n(Lu);gY=s(v4e,"STRONG",{});var DSr=n(gY);Uao=r(DSr,"speech_to_text"),DSr.forEach(t),Jao=r(v4e," \u2014 "),FR=s(v4e,"A",{href:!0});var qSr=n(FR);Yao=r(qSr,"Speech2TextModel"),qSr.forEach(t),Kao=r(v4e," (Speech2Text model)"),v4e.forEach(t),Zao=i(C),Bu=s(C,"LI",{});var T4e=n(Bu);hY=s(T4e,"STRONG",{});var GSr=n(hY);eso=r(GSr,"splinter"),GSr.forEach(t),oso=r(T4e," \u2014 "),CR=s(T4e,"A",{href:!0});var OSr=n(CR);rso=r(OSr,"SplinterModel"),OSr.forEach(t),tso=r(T4e," (Splinter model)"),T4e.forEach(t),aso=i(C),ku=s(C,"LI",{});var F4e=n(ku);uY=s(F4e,"STRONG",{});var XSr=n(uY);sso=r(XSr,"squeezebert"),XSr.forEach(t),nso=r(F4e," \u2014 "),MR=s(F4e,"A",{href:!0});var zSr=n(MR);lso=r(zSr,"SqueezeBertModel"),zSr.forEach(t),iso=r(F4e," (SqueezeBERT model)"),F4e.forEach(t),dso=i(C),xu=s(C,"LI",{});var C4e=n(xu);pY=s(C4e,"STRONG",{});var VSr=n(pY);cso=r(VSr,"swin"),VSr.forEach(t),mso=r(C4e," \u2014 "),ER=s(C4e,"A",{href:!0});var WSr=n(ER);fso=r(WSr,"SwinModel"),WSr.forEach(t),gso=r(C4e," (Swin model)"),C4e.forEach(t),hso=i(C),Ru=s(C,"LI",{});var M4e=n(Ru);_Y=s(M4e,"STRONG",{});var QSr=n(_Y);uso=r(QSr,"t5"),QSr.forEach(t),pso=r(M4e," \u2014 "),yR=s(M4e,"A",{href:!0});var HSr=n(yR);_so=r(HSr,"T5Model"),HSr.forEach(t),bso=r(M4e," (T5 model)"),M4e.forEach(t),vso=i(C),Su=s(C,"LI",{});var E4e=n(Su);bY=s(E4e,"STRONG",{});var USr=n(bY);Tso=r(USr,"tapas"),USr.forEach(t),Fso=r(E4e," \u2014 "),wR=s(E4e,"A",{href:!0});var JSr=n(wR);Cso=r(JSr,"TapasModel"),JSr.forEach(t),Mso=r(E4e," (TAPAS model)"),E4e.forEach(t),Eso=i(C),Pu=s(C,"LI",{});var y4e=n(Pu);vY=s(y4e,"STRONG",{});var YSr=n(vY);yso=r(YSr,"transfo-xl"),YSr.forEach(t),wso=r(y4e," \u2014 "),AR=s(y4e,"A",{href:!0});var KSr=n(AR);Aso=r(KSr,"TransfoXLModel"),KSr.forEach(t),Lso=r(y4e," (Transformer-XL model)"),y4e.forEach(t),Bso=i(C),$u=s(C,"LI",{});var w4e=n($u);TY=s(w4e,"STRONG",{});var ZSr=n(TY);kso=r(ZSr,"unispeech"),ZSr.forEach(t),xso=r(w4e," \u2014 "),LR=s(w4e,"A",{href:!0});var ePr=n(LR);Rso=r(ePr,"UniSpeechModel"),ePr.forEach(t),Sso=r(w4e," (UniSpeech model)"),w4e.forEach(t),Pso=i(C),Iu=s(C,"LI",{});var A4e=n(Iu);FY=s(A4e,"STRONG",{});var oPr=n(FY);$so=r(oPr,"unispeech-sat"),oPr.forEach(t),Iso=r(A4e," \u2014 "),BR=s(A4e,"A",{href:!0});var rPr=n(BR);jso=r(rPr,"UniSpeechSatModel"),rPr.forEach(t),Nso=r(A4e," (UniSpeechSat model)"),A4e.forEach(t),Dso=i(C),ju=s(C,"LI",{});var L4e=n(ju);CY=s(L4e,"STRONG",{});var tPr=n(CY);qso=r(tPr,"vilt"),tPr.forEach(t),Gso=r(L4e," \u2014 "),kR=s(L4e,"A",{href:!0});var aPr=n(kR);Oso=r(aPr,"ViltModel"),aPr.forEach(t),Xso=r(L4e," (ViLT model)"),L4e.forEach(t),zso=i(C),Nu=s(C,"LI",{});var B4e=n(Nu);MY=s(B4e,"STRONG",{});var sPr=n(MY);Vso=r(sPr,"vision-text-dual-encoder"),sPr.forEach(t),Wso=r(B4e," \u2014 "),xR=s(B4e,"A",{href:!0});var nPr=n(xR);Qso=r(nPr,"VisionTextDualEncoderModel"),nPr.forEach(t),Hso=r(B4e," (VisionTextDualEncoder model)"),B4e.forEach(t),Uso=i(C),Du=s(C,"LI",{});var k4e=n(Du);EY=s(k4e,"STRONG",{});var lPr=n(EY);Jso=r(lPr,"visual_bert"),lPr.forEach(t),Yso=r(k4e," \u2014 "),RR=s(k4e,"A",{href:!0});var iPr=n(RR);Kso=r(iPr,"VisualBertModel"),iPr.forEach(t),Zso=r(k4e," (VisualBert model)"),k4e.forEach(t),eno=i(C),qu=s(C,"LI",{});var x4e=n(qu);yY=s(x4e,"STRONG",{});var dPr=n(yY);ono=r(dPr,"vit"),dPr.forEach(t),rno=r(x4e," \u2014 "),SR=s(x4e,"A",{href:!0});var cPr=n(SR);tno=r(cPr,"ViTModel"),cPr.forEach(t),ano=r(x4e," (ViT model)"),x4e.forEach(t),sno=i(C),Gu=s(C,"LI",{});var R4e=n(Gu);wY=s(R4e,"STRONG",{});var mPr=n(wY);nno=r(mPr,"vit_mae"),mPr.forEach(t),lno=r(R4e," \u2014 "),PR=s(R4e,"A",{href:!0});var fPr=n(PR);ino=r(fPr,"ViTMAEModel"),fPr.forEach(t),dno=r(R4e," (ViTMAE model)"),R4e.forEach(t),cno=i(C),Ou=s(C,"LI",{});var S4e=n(Ou);AY=s(S4e,"STRONG",{});var gPr=n(AY);mno=r(gPr,"wav2vec2"),gPr.forEach(t),fno=r(S4e," \u2014 "),$R=s(S4e,"A",{href:!0});var hPr=n($R);gno=r(hPr,"Wav2Vec2Model"),hPr.forEach(t),hno=r(S4e," (Wav2Vec2 model)"),S4e.forEach(t),uno=i(C),Xu=s(C,"LI",{});var P4e=n(Xu);LY=s(P4e,"STRONG",{});var uPr=n(LY);pno=r(uPr,"wavlm"),uPr.forEach(t),_no=r(P4e," \u2014 "),IR=s(P4e,"A",{href:!0});var pPr=n(IR);bno=r(pPr,"WavLMModel"),pPr.forEach(t),vno=r(P4e," (WavLM model)"),P4e.forEach(t),Tno=i(C),zu=s(C,"LI",{});var $4e=n(zu);BY=s($4e,"STRONG",{});var _Pr=n(BY);Fno=r(_Pr,"xglm"),_Pr.forEach(t),Cno=r($4e," \u2014 "),jR=s($4e,"A",{href:!0});var bPr=n(jR);Mno=r(bPr,"XGLMModel"),bPr.forEach(t),Eno=r($4e," (XGLM model)"),$4e.forEach(t),yno=i(C),Vu=s(C,"LI",{});var I4e=n(Vu);kY=s(I4e,"STRONG",{});var vPr=n(kY);wno=r(vPr,"xlm"),vPr.forEach(t),Ano=r(I4e," \u2014 "),NR=s(I4e,"A",{href:!0});var TPr=n(NR);Lno=r(TPr,"XLMModel"),TPr.forEach(t),Bno=r(I4e," (XLM model)"),I4e.forEach(t),kno=i(C),Wu=s(C,"LI",{});var j4e=n(Wu);xY=s(j4e,"STRONG",{});var FPr=n(xY);xno=r(FPr,"xlm-prophetnet"),FPr.forEach(t),Rno=r(j4e," \u2014 "),DR=s(j4e,"A",{href:!0});var CPr=n(DR);Sno=r(CPr,"XLMProphetNetModel"),CPr.forEach(t),Pno=r(j4e," (XLMProphetNet model)"),j4e.forEach(t),$no=i(C),Qu=s(C,"LI",{});var N4e=n(Qu);RY=s(N4e,"STRONG",{});var MPr=n(RY);Ino=r(MPr,"xlm-roberta"),MPr.forEach(t),jno=r(N4e," \u2014 "),qR=s(N4e,"A",{href:!0});var EPr=n(qR);Nno=r(EPr,"XLMRobertaModel"),EPr.forEach(t),Dno=r(N4e," (XLM-RoBERTa model)"),N4e.forEach(t),qno=i(C),Hu=s(C,"LI",{});var D4e=n(Hu);SY=s(D4e,"STRONG",{});var yPr=n(SY);Gno=r(yPr,"xlm-roberta-xl"),yPr.forEach(t),Ono=r(D4e," \u2014 "),GR=s(D4e,"A",{href:!0});var wPr=n(GR);Xno=r(wPr,"XLMRobertaXLModel"),wPr.forEach(t),zno=r(D4e," (XLM-RoBERTa-XL model)"),D4e.forEach(t),Vno=i(C),Uu=s(C,"LI",{});var q4e=n(Uu);PY=s(q4e,"STRONG",{});var APr=n(PY);Wno=r(APr,"xlnet"),APr.forEach(t),Qno=r(q4e," \u2014 "),OR=s(q4e,"A",{href:!0});var LPr=n(OR);Hno=r(LPr,"XLNetModel"),LPr.forEach(t),Uno=r(q4e," (XLNet model)"),q4e.forEach(t),Jno=i(C),Ju=s(C,"LI",{});var G4e=n(Ju);$Y=s(G4e,"STRONG",{});var BPr=n($Y);Yno=r(BPr,"yoso"),BPr.forEach(t),Kno=r(G4e," \u2014 "),XR=s(G4e,"A",{href:!0});var kPr=n(XR);Zno=r(kPr,"YosoModel"),kPr.forEach(t),elo=r(G4e," (YOSO model)"),G4e.forEach(t),C.forEach(t),olo=i(St),Yu=s(St,"P",{});var O4e=n(Yu);rlo=r(O4e,"The model is set in evaluation mode by default using "),IY=s(O4e,"CODE",{});var xPr=n(IY);tlo=r(xPr,"model.eval()"),xPr.forEach(t),alo=r(O4e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jY=s(O4e,"CODE",{});var RPr=n(jY);slo=r(RPr,"model.train()"),RPr.forEach(t),O4e.forEach(t),nlo=i(St),NY=s(St,"P",{});var SPr=n(NY);llo=r(SPr,"Examples:"),SPr.forEach(t),ilo=i(St),f(G5.$$.fragment,St),St.forEach(t),Nn.forEach(t),b8e=i(d),Xi=s(d,"H2",{class:!0});var yBe=n(Xi);Ku=s(yBe,"A",{id:!0,class:!0,href:!0});var PPr=n(Ku);DY=s(PPr,"SPAN",{});var $Pr=n(DY);f(O5.$$.fragment,$Pr),$Pr.forEach(t),PPr.forEach(t),dlo=i(yBe),qY=s(yBe,"SPAN",{});var IPr=n(qY);clo=r(IPr,"AutoModelForPreTraining"),IPr.forEach(t),yBe.forEach(t),v8e=i(d),Wo=s(d,"DIV",{class:!0});var qn=n(Wo);f(X5.$$.fragment,qn),mlo=i(qn),zi=s(qn,"P",{});var HX=n(zi);flo=r(HX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),GY=s(HX,"CODE",{});var jPr=n(GY);glo=r(jPr,"from_pretrained()"),jPr.forEach(t),hlo=r(HX,"class method or the "),OY=s(HX,"CODE",{});var NPr=n(OY);ulo=r(NPr,"from_config()"),NPr.forEach(t),plo=r(HX,`class
method.`),HX.forEach(t),_lo=i(qn),z5=s(qn,"P",{});var wBe=n(z5);blo=r(wBe,"This class cannot be instantiated directly using "),XY=s(wBe,"CODE",{});var DPr=n(XY);vlo=r(DPr,"__init__()"),DPr.forEach(t),Tlo=r(wBe," (throws an error)."),wBe.forEach(t),Flo=i(qn),Dr=s(qn,"DIV",{class:!0});var Gn=n(Dr);f(V5.$$.fragment,Gn),Clo=i(Gn),zY=s(Gn,"P",{});var qPr=n(zY);Mlo=r(qPr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),qPr.forEach(t),Elo=i(Gn),Vi=s(Gn,"P",{});var UX=n(Vi);ylo=r(UX,`Note:
Loading a model from its configuration file does `),VY=s(UX,"STRONG",{});var GPr=n(VY);wlo=r(GPr,"not"),GPr.forEach(t),Alo=r(UX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WY=s(UX,"CODE",{});var OPr=n(WY);Llo=r(OPr,"from_pretrained()"),OPr.forEach(t),Blo=r(UX,"to load the model weights."),UX.forEach(t),klo=i(Gn),QY=s(Gn,"P",{});var XPr=n(QY);xlo=r(XPr,"Examples:"),XPr.forEach(t),Rlo=i(Gn),f(W5.$$.fragment,Gn),Gn.forEach(t),Slo=i(qn),xe=s(qn,"DIV",{class:!0});var Pt=n(xe);f(Q5.$$.fragment,Pt),Plo=i(Pt),HY=s(Pt,"P",{});var zPr=n(HY);$lo=r(zPr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),zPr.forEach(t),Ilo=i(Pt),qa=s(Pt,"P",{});var i3=n(qa);jlo=r(i3,"The model class to instantiate is selected based on the "),UY=s(i3,"CODE",{});var VPr=n(UY);Nlo=r(VPr,"model_type"),VPr.forEach(t),Dlo=r(i3,` property of the config object (either
passed as an argument or loaded from `),JY=s(i3,"CODE",{});var WPr=n(JY);qlo=r(WPr,"pretrained_model_name_or_path"),WPr.forEach(t),Glo=r(i3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YY=s(i3,"CODE",{});var QPr=n(YY);Olo=r(QPr,"pretrained_model_name_or_path"),QPr.forEach(t),Xlo=r(i3,":"),i3.forEach(t),zlo=i(Pt),x=s(Pt,"UL",{});var S=n(x);Zu=s(S,"LI",{});var X4e=n(Zu);KY=s(X4e,"STRONG",{});var HPr=n(KY);Vlo=r(HPr,"albert"),HPr.forEach(t),Wlo=r(X4e," \u2014 "),zR=s(X4e,"A",{href:!0});var UPr=n(zR);Qlo=r(UPr,"AlbertForPreTraining"),UPr.forEach(t),Hlo=r(X4e," (ALBERT model)"),X4e.forEach(t),Ulo=i(S),ep=s(S,"LI",{});var z4e=n(ep);ZY=s(z4e,"STRONG",{});var JPr=n(ZY);Jlo=r(JPr,"bart"),JPr.forEach(t),Ylo=r(z4e," \u2014 "),VR=s(z4e,"A",{href:!0});var YPr=n(VR);Klo=r(YPr,"BartForConditionalGeneration"),YPr.forEach(t),Zlo=r(z4e," (BART model)"),z4e.forEach(t),eio=i(S),op=s(S,"LI",{});var V4e=n(op);eK=s(V4e,"STRONG",{});var KPr=n(eK);oio=r(KPr,"bert"),KPr.forEach(t),rio=r(V4e," \u2014 "),WR=s(V4e,"A",{href:!0});var ZPr=n(WR);tio=r(ZPr,"BertForPreTraining"),ZPr.forEach(t),aio=r(V4e," (BERT model)"),V4e.forEach(t),sio=i(S),rp=s(S,"LI",{});var W4e=n(rp);oK=s(W4e,"STRONG",{});var e$r=n(oK);nio=r(e$r,"big_bird"),e$r.forEach(t),lio=r(W4e," \u2014 "),QR=s(W4e,"A",{href:!0});var o$r=n(QR);iio=r(o$r,"BigBirdForPreTraining"),o$r.forEach(t),dio=r(W4e," (BigBird model)"),W4e.forEach(t),cio=i(S),tp=s(S,"LI",{});var Q4e=n(tp);rK=s(Q4e,"STRONG",{});var r$r=n(rK);mio=r(r$r,"camembert"),r$r.forEach(t),fio=r(Q4e," \u2014 "),HR=s(Q4e,"A",{href:!0});var t$r=n(HR);gio=r(t$r,"CamembertForMaskedLM"),t$r.forEach(t),hio=r(Q4e," (CamemBERT model)"),Q4e.forEach(t),uio=i(S),ap=s(S,"LI",{});var H4e=n(ap);tK=s(H4e,"STRONG",{});var a$r=n(tK);pio=r(a$r,"ctrl"),a$r.forEach(t),_io=r(H4e," \u2014 "),UR=s(H4e,"A",{href:!0});var s$r=n(UR);bio=r(s$r,"CTRLLMHeadModel"),s$r.forEach(t),vio=r(H4e," (CTRL model)"),H4e.forEach(t),Tio=i(S),sp=s(S,"LI",{});var U4e=n(sp);aK=s(U4e,"STRONG",{});var n$r=n(aK);Fio=r(n$r,"deberta"),n$r.forEach(t),Cio=r(U4e," \u2014 "),JR=s(U4e,"A",{href:!0});var l$r=n(JR);Mio=r(l$r,"DebertaForMaskedLM"),l$r.forEach(t),Eio=r(U4e," (DeBERTa model)"),U4e.forEach(t),yio=i(S),np=s(S,"LI",{});var J4e=n(np);sK=s(J4e,"STRONG",{});var i$r=n(sK);wio=r(i$r,"deberta-v2"),i$r.forEach(t),Aio=r(J4e," \u2014 "),YR=s(J4e,"A",{href:!0});var d$r=n(YR);Lio=r(d$r,"DebertaV2ForMaskedLM"),d$r.forEach(t),Bio=r(J4e," (DeBERTa-v2 model)"),J4e.forEach(t),kio=i(S),lp=s(S,"LI",{});var Y4e=n(lp);nK=s(Y4e,"STRONG",{});var c$r=n(nK);xio=r(c$r,"distilbert"),c$r.forEach(t),Rio=r(Y4e," \u2014 "),KR=s(Y4e,"A",{href:!0});var m$r=n(KR);Sio=r(m$r,"DistilBertForMaskedLM"),m$r.forEach(t),Pio=r(Y4e," (DistilBERT model)"),Y4e.forEach(t),$io=i(S),ip=s(S,"LI",{});var K4e=n(ip);lK=s(K4e,"STRONG",{});var f$r=n(lK);Iio=r(f$r,"electra"),f$r.forEach(t),jio=r(K4e," \u2014 "),ZR=s(K4e,"A",{href:!0});var g$r=n(ZR);Nio=r(g$r,"ElectraForPreTraining"),g$r.forEach(t),Dio=r(K4e," (ELECTRA model)"),K4e.forEach(t),qio=i(S),dp=s(S,"LI",{});var Z4e=n(dp);iK=s(Z4e,"STRONG",{});var h$r=n(iK);Gio=r(h$r,"flaubert"),h$r.forEach(t),Oio=r(Z4e," \u2014 "),eS=s(Z4e,"A",{href:!0});var u$r=n(eS);Xio=r(u$r,"FlaubertWithLMHeadModel"),u$r.forEach(t),zio=r(Z4e," (FlauBERT model)"),Z4e.forEach(t),Vio=i(S),cp=s(S,"LI",{});var eMe=n(cp);dK=s(eMe,"STRONG",{});var p$r=n(dK);Wio=r(p$r,"fnet"),p$r.forEach(t),Qio=r(eMe," \u2014 "),oS=s(eMe,"A",{href:!0});var _$r=n(oS);Hio=r(_$r,"FNetForPreTraining"),_$r.forEach(t),Uio=r(eMe," (FNet model)"),eMe.forEach(t),Jio=i(S),mp=s(S,"LI",{});var oMe=n(mp);cK=s(oMe,"STRONG",{});var b$r=n(cK);Yio=r(b$r,"fsmt"),b$r.forEach(t),Kio=r(oMe," \u2014 "),rS=s(oMe,"A",{href:!0});var v$r=n(rS);Zio=r(v$r,"FSMTForConditionalGeneration"),v$r.forEach(t),edo=r(oMe," (FairSeq Machine-Translation model)"),oMe.forEach(t),odo=i(S),fp=s(S,"LI",{});var rMe=n(fp);mK=s(rMe,"STRONG",{});var T$r=n(mK);rdo=r(T$r,"funnel"),T$r.forEach(t),tdo=r(rMe," \u2014 "),tS=s(rMe,"A",{href:!0});var F$r=n(tS);ado=r(F$r,"FunnelForPreTraining"),F$r.forEach(t),sdo=r(rMe," (Funnel Transformer model)"),rMe.forEach(t),ndo=i(S),gp=s(S,"LI",{});var tMe=n(gp);fK=s(tMe,"STRONG",{});var C$r=n(fK);ldo=r(C$r,"gpt2"),C$r.forEach(t),ido=r(tMe," \u2014 "),aS=s(tMe,"A",{href:!0});var M$r=n(aS);ddo=r(M$r,"GPT2LMHeadModel"),M$r.forEach(t),cdo=r(tMe," (OpenAI GPT-2 model)"),tMe.forEach(t),mdo=i(S),hp=s(S,"LI",{});var aMe=n(hp);gK=s(aMe,"STRONG",{});var E$r=n(gK);fdo=r(E$r,"ibert"),E$r.forEach(t),gdo=r(aMe," \u2014 "),sS=s(aMe,"A",{href:!0});var y$r=n(sS);hdo=r(y$r,"IBertForMaskedLM"),y$r.forEach(t),udo=r(aMe," (I-BERT model)"),aMe.forEach(t),pdo=i(S),up=s(S,"LI",{});var sMe=n(up);hK=s(sMe,"STRONG",{});var w$r=n(hK);_do=r(w$r,"layoutlm"),w$r.forEach(t),bdo=r(sMe," \u2014 "),nS=s(sMe,"A",{href:!0});var A$r=n(nS);vdo=r(A$r,"LayoutLMForMaskedLM"),A$r.forEach(t),Tdo=r(sMe," (LayoutLM model)"),sMe.forEach(t),Fdo=i(S),pp=s(S,"LI",{});var nMe=n(pp);uK=s(nMe,"STRONG",{});var L$r=n(uK);Cdo=r(L$r,"longformer"),L$r.forEach(t),Mdo=r(nMe," \u2014 "),lS=s(nMe,"A",{href:!0});var B$r=n(lS);Edo=r(B$r,"LongformerForMaskedLM"),B$r.forEach(t),ydo=r(nMe," (Longformer model)"),nMe.forEach(t),wdo=i(S),_p=s(S,"LI",{});var lMe=n(_p);pK=s(lMe,"STRONG",{});var k$r=n(pK);Ado=r(k$r,"lxmert"),k$r.forEach(t),Ldo=r(lMe," \u2014 "),iS=s(lMe,"A",{href:!0});var x$r=n(iS);Bdo=r(x$r,"LxmertForPreTraining"),x$r.forEach(t),kdo=r(lMe," (LXMERT model)"),lMe.forEach(t),xdo=i(S),bp=s(S,"LI",{});var iMe=n(bp);_K=s(iMe,"STRONG",{});var R$r=n(_K);Rdo=r(R$r,"megatron-bert"),R$r.forEach(t),Sdo=r(iMe," \u2014 "),dS=s(iMe,"A",{href:!0});var S$r=n(dS);Pdo=r(S$r,"MegatronBertForPreTraining"),S$r.forEach(t),$do=r(iMe," (MegatronBert model)"),iMe.forEach(t),Ido=i(S),vp=s(S,"LI",{});var dMe=n(vp);bK=s(dMe,"STRONG",{});var P$r=n(bK);jdo=r(P$r,"mobilebert"),P$r.forEach(t),Ndo=r(dMe," \u2014 "),cS=s(dMe,"A",{href:!0});var $$r=n(cS);Ddo=r($$r,"MobileBertForPreTraining"),$$r.forEach(t),qdo=r(dMe," (MobileBERT model)"),dMe.forEach(t),Gdo=i(S),Tp=s(S,"LI",{});var cMe=n(Tp);vK=s(cMe,"STRONG",{});var I$r=n(vK);Odo=r(I$r,"mpnet"),I$r.forEach(t),Xdo=r(cMe," \u2014 "),mS=s(cMe,"A",{href:!0});var j$r=n(mS);zdo=r(j$r,"MPNetForMaskedLM"),j$r.forEach(t),Vdo=r(cMe," (MPNet model)"),cMe.forEach(t),Wdo=i(S),Fp=s(S,"LI",{});var mMe=n(Fp);TK=s(mMe,"STRONG",{});var N$r=n(TK);Qdo=r(N$r,"openai-gpt"),N$r.forEach(t),Hdo=r(mMe," \u2014 "),fS=s(mMe,"A",{href:!0});var D$r=n(fS);Udo=r(D$r,"OpenAIGPTLMHeadModel"),D$r.forEach(t),Jdo=r(mMe," (OpenAI GPT model)"),mMe.forEach(t),Ydo=i(S),Cp=s(S,"LI",{});var fMe=n(Cp);FK=s(fMe,"STRONG",{});var q$r=n(FK);Kdo=r(q$r,"retribert"),q$r.forEach(t),Zdo=r(fMe," \u2014 "),gS=s(fMe,"A",{href:!0});var G$r=n(gS);eco=r(G$r,"RetriBertModel"),G$r.forEach(t),oco=r(fMe," (RetriBERT model)"),fMe.forEach(t),rco=i(S),Mp=s(S,"LI",{});var gMe=n(Mp);CK=s(gMe,"STRONG",{});var O$r=n(CK);tco=r(O$r,"roberta"),O$r.forEach(t),aco=r(gMe," \u2014 "),hS=s(gMe,"A",{href:!0});var X$r=n(hS);sco=r(X$r,"RobertaForMaskedLM"),X$r.forEach(t),nco=r(gMe," (RoBERTa model)"),gMe.forEach(t),lco=i(S),Ep=s(S,"LI",{});var hMe=n(Ep);MK=s(hMe,"STRONG",{});var z$r=n(MK);ico=r(z$r,"squeezebert"),z$r.forEach(t),dco=r(hMe," \u2014 "),uS=s(hMe,"A",{href:!0});var V$r=n(uS);cco=r(V$r,"SqueezeBertForMaskedLM"),V$r.forEach(t),mco=r(hMe," (SqueezeBERT model)"),hMe.forEach(t),fco=i(S),yp=s(S,"LI",{});var uMe=n(yp);EK=s(uMe,"STRONG",{});var W$r=n(EK);gco=r(W$r,"t5"),W$r.forEach(t),hco=r(uMe," \u2014 "),pS=s(uMe,"A",{href:!0});var Q$r=n(pS);uco=r(Q$r,"T5ForConditionalGeneration"),Q$r.forEach(t),pco=r(uMe," (T5 model)"),uMe.forEach(t),_co=i(S),wp=s(S,"LI",{});var pMe=n(wp);yK=s(pMe,"STRONG",{});var H$r=n(yK);bco=r(H$r,"tapas"),H$r.forEach(t),vco=r(pMe," \u2014 "),_S=s(pMe,"A",{href:!0});var U$r=n(_S);Tco=r(U$r,"TapasForMaskedLM"),U$r.forEach(t),Fco=r(pMe," (TAPAS model)"),pMe.forEach(t),Cco=i(S),Ap=s(S,"LI",{});var _Me=n(Ap);wK=s(_Me,"STRONG",{});var J$r=n(wK);Mco=r(J$r,"transfo-xl"),J$r.forEach(t),Eco=r(_Me," \u2014 "),bS=s(_Me,"A",{href:!0});var Y$r=n(bS);yco=r(Y$r,"TransfoXLLMHeadModel"),Y$r.forEach(t),wco=r(_Me," (Transformer-XL model)"),_Me.forEach(t),Aco=i(S),Lp=s(S,"LI",{});var bMe=n(Lp);AK=s(bMe,"STRONG",{});var K$r=n(AK);Lco=r(K$r,"unispeech"),K$r.forEach(t),Bco=r(bMe," \u2014 "),vS=s(bMe,"A",{href:!0});var Z$r=n(vS);kco=r(Z$r,"UniSpeechForPreTraining"),Z$r.forEach(t),xco=r(bMe," (UniSpeech model)"),bMe.forEach(t),Rco=i(S),Bp=s(S,"LI",{});var vMe=n(Bp);LK=s(vMe,"STRONG",{});var eIr=n(LK);Sco=r(eIr,"unispeech-sat"),eIr.forEach(t),Pco=r(vMe," \u2014 "),TS=s(vMe,"A",{href:!0});var oIr=n(TS);$co=r(oIr,"UniSpeechSatForPreTraining"),oIr.forEach(t),Ico=r(vMe," (UniSpeechSat model)"),vMe.forEach(t),jco=i(S),kp=s(S,"LI",{});var TMe=n(kp);BK=s(TMe,"STRONG",{});var rIr=n(BK);Nco=r(rIr,"visual_bert"),rIr.forEach(t),Dco=r(TMe," \u2014 "),FS=s(TMe,"A",{href:!0});var tIr=n(FS);qco=r(tIr,"VisualBertForPreTraining"),tIr.forEach(t),Gco=r(TMe," (VisualBert model)"),TMe.forEach(t),Oco=i(S),xp=s(S,"LI",{});var FMe=n(xp);kK=s(FMe,"STRONG",{});var aIr=n(kK);Xco=r(aIr,"vit_mae"),aIr.forEach(t),zco=r(FMe," \u2014 "),CS=s(FMe,"A",{href:!0});var sIr=n(CS);Vco=r(sIr,"ViTMAEForPreTraining"),sIr.forEach(t),Wco=r(FMe," (ViTMAE model)"),FMe.forEach(t),Qco=i(S),Rp=s(S,"LI",{});var CMe=n(Rp);xK=s(CMe,"STRONG",{});var nIr=n(xK);Hco=r(nIr,"wav2vec2"),nIr.forEach(t),Uco=r(CMe," \u2014 "),MS=s(CMe,"A",{href:!0});var lIr=n(MS);Jco=r(lIr,"Wav2Vec2ForPreTraining"),lIr.forEach(t),Yco=r(CMe," (Wav2Vec2 model)"),CMe.forEach(t),Kco=i(S),Sp=s(S,"LI",{});var MMe=n(Sp);RK=s(MMe,"STRONG",{});var iIr=n(RK);Zco=r(iIr,"xlm"),iIr.forEach(t),emo=r(MMe," \u2014 "),ES=s(MMe,"A",{href:!0});var dIr=n(ES);omo=r(dIr,"XLMWithLMHeadModel"),dIr.forEach(t),rmo=r(MMe," (XLM model)"),MMe.forEach(t),tmo=i(S),Pp=s(S,"LI",{});var EMe=n(Pp);SK=s(EMe,"STRONG",{});var cIr=n(SK);amo=r(cIr,"xlm-roberta"),cIr.forEach(t),smo=r(EMe," \u2014 "),yS=s(EMe,"A",{href:!0});var mIr=n(yS);nmo=r(mIr,"XLMRobertaForMaskedLM"),mIr.forEach(t),lmo=r(EMe," (XLM-RoBERTa model)"),EMe.forEach(t),imo=i(S),$p=s(S,"LI",{});var yMe=n($p);PK=s(yMe,"STRONG",{});var fIr=n(PK);dmo=r(fIr,"xlm-roberta-xl"),fIr.forEach(t),cmo=r(yMe," \u2014 "),wS=s(yMe,"A",{href:!0});var gIr=n(wS);mmo=r(gIr,"XLMRobertaXLForMaskedLM"),gIr.forEach(t),fmo=r(yMe," (XLM-RoBERTa-XL model)"),yMe.forEach(t),gmo=i(S),Ip=s(S,"LI",{});var wMe=n(Ip);$K=s(wMe,"STRONG",{});var hIr=n($K);hmo=r(hIr,"xlnet"),hIr.forEach(t),umo=r(wMe," \u2014 "),AS=s(wMe,"A",{href:!0});var uIr=n(AS);pmo=r(uIr,"XLNetLMHeadModel"),uIr.forEach(t),_mo=r(wMe," (XLNet model)"),wMe.forEach(t),S.forEach(t),bmo=i(Pt),jp=s(Pt,"P",{});var AMe=n(jp);vmo=r(AMe,"The model is set in evaluation mode by default using "),IK=s(AMe,"CODE",{});var pIr=n(IK);Tmo=r(pIr,"model.eval()"),pIr.forEach(t),Fmo=r(AMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jK=s(AMe,"CODE",{});var _Ir=n(jK);Cmo=r(_Ir,"model.train()"),_Ir.forEach(t),AMe.forEach(t),Mmo=i(Pt),NK=s(Pt,"P",{});var bIr=n(NK);Emo=r(bIr,"Examples:"),bIr.forEach(t),ymo=i(Pt),f(H5.$$.fragment,Pt),Pt.forEach(t),qn.forEach(t),T8e=i(d),Wi=s(d,"H2",{class:!0});var ABe=n(Wi);Np=s(ABe,"A",{id:!0,class:!0,href:!0});var vIr=n(Np);DK=s(vIr,"SPAN",{});var TIr=n(DK);f(U5.$$.fragment,TIr),TIr.forEach(t),vIr.forEach(t),wmo=i(ABe),qK=s(ABe,"SPAN",{});var FIr=n(qK);Amo=r(FIr,"AutoModelForCausalLM"),FIr.forEach(t),ABe.forEach(t),F8e=i(d),Qo=s(d,"DIV",{class:!0});var On=n(Qo);f(J5.$$.fragment,On),Lmo=i(On),Qi=s(On,"P",{});var JX=n(Qi);Bmo=r(JX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),GK=s(JX,"CODE",{});var CIr=n(GK);kmo=r(CIr,"from_pretrained()"),CIr.forEach(t),xmo=r(JX,"class method or the "),OK=s(JX,"CODE",{});var MIr=n(OK);Rmo=r(MIr,"from_config()"),MIr.forEach(t),Smo=r(JX,`class
method.`),JX.forEach(t),Pmo=i(On),Y5=s(On,"P",{});var LBe=n(Y5);$mo=r(LBe,"This class cannot be instantiated directly using "),XK=s(LBe,"CODE",{});var EIr=n(XK);Imo=r(EIr,"__init__()"),EIr.forEach(t),jmo=r(LBe," (throws an error)."),LBe.forEach(t),Nmo=i(On),qr=s(On,"DIV",{class:!0});var Xn=n(qr);f(K5.$$.fragment,Xn),Dmo=i(Xn),zK=s(Xn,"P",{});var yIr=n(zK);qmo=r(yIr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),yIr.forEach(t),Gmo=i(Xn),Hi=s(Xn,"P",{});var YX=n(Hi);Omo=r(YX,`Note:
Loading a model from its configuration file does `),VK=s(YX,"STRONG",{});var wIr=n(VK);Xmo=r(wIr,"not"),wIr.forEach(t),zmo=r(YX,` load the model weights. It only affects the
model\u2019s configuration. Use `),WK=s(YX,"CODE",{});var AIr=n(WK);Vmo=r(AIr,"from_pretrained()"),AIr.forEach(t),Wmo=r(YX,"to load the model weights."),YX.forEach(t),Qmo=i(Xn),QK=s(Xn,"P",{});var LIr=n(QK);Hmo=r(LIr,"Examples:"),LIr.forEach(t),Umo=i(Xn),f(Z5.$$.fragment,Xn),Xn.forEach(t),Jmo=i(On),Re=s(On,"DIV",{class:!0});var $t=n(Re);f(ey.$$.fragment,$t),Ymo=i($t),HK=s($t,"P",{});var BIr=n(HK);Kmo=r(BIr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),BIr.forEach(t),Zmo=i($t),Ga=s($t,"P",{});var d3=n(Ga);efo=r(d3,"The model class to instantiate is selected based on the "),UK=s(d3,"CODE",{});var kIr=n(UK);ofo=r(kIr,"model_type"),kIr.forEach(t),rfo=r(d3,` property of the config object (either
passed as an argument or loaded from `),JK=s(d3,"CODE",{});var xIr=n(JK);tfo=r(xIr,"pretrained_model_name_or_path"),xIr.forEach(t),afo=r(d3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),YK=s(d3,"CODE",{});var RIr=n(YK);sfo=r(RIr,"pretrained_model_name_or_path"),RIr.forEach(t),nfo=r(d3,":"),d3.forEach(t),lfo=i($t),$=s($t,"UL",{});var j=n($);Dp=s(j,"LI",{});var LMe=n(Dp);KK=s(LMe,"STRONG",{});var SIr=n(KK);ifo=r(SIr,"bart"),SIr.forEach(t),dfo=r(LMe," \u2014 "),LS=s(LMe,"A",{href:!0});var PIr=n(LS);cfo=r(PIr,"BartForCausalLM"),PIr.forEach(t),mfo=r(LMe," (BART model)"),LMe.forEach(t),ffo=i(j),qp=s(j,"LI",{});var BMe=n(qp);ZK=s(BMe,"STRONG",{});var $Ir=n(ZK);gfo=r($Ir,"bert"),$Ir.forEach(t),hfo=r(BMe," \u2014 "),BS=s(BMe,"A",{href:!0});var IIr=n(BS);ufo=r(IIr,"BertLMHeadModel"),IIr.forEach(t),pfo=r(BMe," (BERT model)"),BMe.forEach(t),_fo=i(j),Gp=s(j,"LI",{});var kMe=n(Gp);eZ=s(kMe,"STRONG",{});var jIr=n(eZ);bfo=r(jIr,"bert-generation"),jIr.forEach(t),vfo=r(kMe," \u2014 "),kS=s(kMe,"A",{href:!0});var NIr=n(kS);Tfo=r(NIr,"BertGenerationDecoder"),NIr.forEach(t),Ffo=r(kMe," (Bert Generation model)"),kMe.forEach(t),Cfo=i(j),Op=s(j,"LI",{});var xMe=n(Op);oZ=s(xMe,"STRONG",{});var DIr=n(oZ);Mfo=r(DIr,"big_bird"),DIr.forEach(t),Efo=r(xMe," \u2014 "),xS=s(xMe,"A",{href:!0});var qIr=n(xS);yfo=r(qIr,"BigBirdForCausalLM"),qIr.forEach(t),wfo=r(xMe," (BigBird model)"),xMe.forEach(t),Afo=i(j),Xp=s(j,"LI",{});var RMe=n(Xp);rZ=s(RMe,"STRONG",{});var GIr=n(rZ);Lfo=r(GIr,"bigbird_pegasus"),GIr.forEach(t),Bfo=r(RMe," \u2014 "),RS=s(RMe,"A",{href:!0});var OIr=n(RS);kfo=r(OIr,"BigBirdPegasusForCausalLM"),OIr.forEach(t),xfo=r(RMe," (BigBirdPegasus model)"),RMe.forEach(t),Rfo=i(j),zp=s(j,"LI",{});var SMe=n(zp);tZ=s(SMe,"STRONG",{});var XIr=n(tZ);Sfo=r(XIr,"blenderbot"),XIr.forEach(t),Pfo=r(SMe," \u2014 "),SS=s(SMe,"A",{href:!0});var zIr=n(SS);$fo=r(zIr,"BlenderbotForCausalLM"),zIr.forEach(t),Ifo=r(SMe," (Blenderbot model)"),SMe.forEach(t),jfo=i(j),Vp=s(j,"LI",{});var PMe=n(Vp);aZ=s(PMe,"STRONG",{});var VIr=n(aZ);Nfo=r(VIr,"blenderbot-small"),VIr.forEach(t),Dfo=r(PMe," \u2014 "),PS=s(PMe,"A",{href:!0});var WIr=n(PS);qfo=r(WIr,"BlenderbotSmallForCausalLM"),WIr.forEach(t),Gfo=r(PMe," (BlenderbotSmall model)"),PMe.forEach(t),Ofo=i(j),Wp=s(j,"LI",{});var $Me=n(Wp);sZ=s($Me,"STRONG",{});var QIr=n(sZ);Xfo=r(QIr,"camembert"),QIr.forEach(t),zfo=r($Me," \u2014 "),$S=s($Me,"A",{href:!0});var HIr=n($S);Vfo=r(HIr,"CamembertForCausalLM"),HIr.forEach(t),Wfo=r($Me," (CamemBERT model)"),$Me.forEach(t),Qfo=i(j),Qp=s(j,"LI",{});var IMe=n(Qp);nZ=s(IMe,"STRONG",{});var UIr=n(nZ);Hfo=r(UIr,"ctrl"),UIr.forEach(t),Ufo=r(IMe," \u2014 "),IS=s(IMe,"A",{href:!0});var JIr=n(IS);Jfo=r(JIr,"CTRLLMHeadModel"),JIr.forEach(t),Yfo=r(IMe," (CTRL model)"),IMe.forEach(t),Kfo=i(j),Hp=s(j,"LI",{});var jMe=n(Hp);lZ=s(jMe,"STRONG",{});var YIr=n(lZ);Zfo=r(YIr,"electra"),YIr.forEach(t),ego=r(jMe," \u2014 "),jS=s(jMe,"A",{href:!0});var KIr=n(jS);ogo=r(KIr,"ElectraForCausalLM"),KIr.forEach(t),rgo=r(jMe," (ELECTRA model)"),jMe.forEach(t),tgo=i(j),Up=s(j,"LI",{});var NMe=n(Up);iZ=s(NMe,"STRONG",{});var ZIr=n(iZ);ago=r(ZIr,"gpt2"),ZIr.forEach(t),sgo=r(NMe," \u2014 "),NS=s(NMe,"A",{href:!0});var ejr=n(NS);ngo=r(ejr,"GPT2LMHeadModel"),ejr.forEach(t),lgo=r(NMe," (OpenAI GPT-2 model)"),NMe.forEach(t),igo=i(j),Jp=s(j,"LI",{});var DMe=n(Jp);dZ=s(DMe,"STRONG",{});var ojr=n(dZ);dgo=r(ojr,"gpt_neo"),ojr.forEach(t),cgo=r(DMe," \u2014 "),DS=s(DMe,"A",{href:!0});var rjr=n(DS);mgo=r(rjr,"GPTNeoForCausalLM"),rjr.forEach(t),fgo=r(DMe," (GPT Neo model)"),DMe.forEach(t),ggo=i(j),Yp=s(j,"LI",{});var qMe=n(Yp);cZ=s(qMe,"STRONG",{});var tjr=n(cZ);hgo=r(tjr,"gptj"),tjr.forEach(t),ugo=r(qMe," \u2014 "),qS=s(qMe,"A",{href:!0});var ajr=n(qS);pgo=r(ajr,"GPTJForCausalLM"),ajr.forEach(t),_go=r(qMe," (GPT-J model)"),qMe.forEach(t),bgo=i(j),Kp=s(j,"LI",{});var GMe=n(Kp);mZ=s(GMe,"STRONG",{});var sjr=n(mZ);vgo=r(sjr,"marian"),sjr.forEach(t),Tgo=r(GMe," \u2014 "),GS=s(GMe,"A",{href:!0});var njr=n(GS);Fgo=r(njr,"MarianForCausalLM"),njr.forEach(t),Cgo=r(GMe," (Marian model)"),GMe.forEach(t),Mgo=i(j),Zp=s(j,"LI",{});var OMe=n(Zp);fZ=s(OMe,"STRONG",{});var ljr=n(fZ);Ego=r(ljr,"mbart"),ljr.forEach(t),ygo=r(OMe," \u2014 "),OS=s(OMe,"A",{href:!0});var ijr=n(OS);wgo=r(ijr,"MBartForCausalLM"),ijr.forEach(t),Ago=r(OMe," (mBART model)"),OMe.forEach(t),Lgo=i(j),e_=s(j,"LI",{});var XMe=n(e_);gZ=s(XMe,"STRONG",{});var djr=n(gZ);Bgo=r(djr,"megatron-bert"),djr.forEach(t),kgo=r(XMe," \u2014 "),XS=s(XMe,"A",{href:!0});var cjr=n(XS);xgo=r(cjr,"MegatronBertForCausalLM"),cjr.forEach(t),Rgo=r(XMe," (MegatronBert model)"),XMe.forEach(t),Sgo=i(j),o_=s(j,"LI",{});var zMe=n(o_);hZ=s(zMe,"STRONG",{});var mjr=n(hZ);Pgo=r(mjr,"openai-gpt"),mjr.forEach(t),$go=r(zMe," \u2014 "),zS=s(zMe,"A",{href:!0});var fjr=n(zS);Igo=r(fjr,"OpenAIGPTLMHeadModel"),fjr.forEach(t),jgo=r(zMe," (OpenAI GPT model)"),zMe.forEach(t),Ngo=i(j),r_=s(j,"LI",{});var VMe=n(r_);uZ=s(VMe,"STRONG",{});var gjr=n(uZ);Dgo=r(gjr,"pegasus"),gjr.forEach(t),qgo=r(VMe," \u2014 "),VS=s(VMe,"A",{href:!0});var hjr=n(VS);Ggo=r(hjr,"PegasusForCausalLM"),hjr.forEach(t),Ogo=r(VMe," (Pegasus model)"),VMe.forEach(t),Xgo=i(j),t_=s(j,"LI",{});var WMe=n(t_);pZ=s(WMe,"STRONG",{});var ujr=n(pZ);zgo=r(ujr,"plbart"),ujr.forEach(t),Vgo=r(WMe," \u2014 "),WS=s(WMe,"A",{href:!0});var pjr=n(WS);Wgo=r(pjr,"PLBartForCausalLM"),pjr.forEach(t),Qgo=r(WMe," (PLBart model)"),WMe.forEach(t),Hgo=i(j),a_=s(j,"LI",{});var QMe=n(a_);_Z=s(QMe,"STRONG",{});var _jr=n(_Z);Ugo=r(_jr,"prophetnet"),_jr.forEach(t),Jgo=r(QMe," \u2014 "),QS=s(QMe,"A",{href:!0});var bjr=n(QS);Ygo=r(bjr,"ProphetNetForCausalLM"),bjr.forEach(t),Kgo=r(QMe," (ProphetNet model)"),QMe.forEach(t),Zgo=i(j),s_=s(j,"LI",{});var HMe=n(s_);bZ=s(HMe,"STRONG",{});var vjr=n(bZ);eho=r(vjr,"qdqbert"),vjr.forEach(t),oho=r(HMe," \u2014 "),HS=s(HMe,"A",{href:!0});var Tjr=n(HS);rho=r(Tjr,"QDQBertLMHeadModel"),Tjr.forEach(t),tho=r(HMe," (QDQBert model)"),HMe.forEach(t),aho=i(j),n_=s(j,"LI",{});var UMe=n(n_);vZ=s(UMe,"STRONG",{});var Fjr=n(vZ);sho=r(Fjr,"reformer"),Fjr.forEach(t),nho=r(UMe," \u2014 "),US=s(UMe,"A",{href:!0});var Cjr=n(US);lho=r(Cjr,"ReformerModelWithLMHead"),Cjr.forEach(t),iho=r(UMe," (Reformer model)"),UMe.forEach(t),dho=i(j),l_=s(j,"LI",{});var JMe=n(l_);TZ=s(JMe,"STRONG",{});var Mjr=n(TZ);cho=r(Mjr,"rembert"),Mjr.forEach(t),mho=r(JMe," \u2014 "),JS=s(JMe,"A",{href:!0});var Ejr=n(JS);fho=r(Ejr,"RemBertForCausalLM"),Ejr.forEach(t),gho=r(JMe," (RemBERT model)"),JMe.forEach(t),hho=i(j),i_=s(j,"LI",{});var YMe=n(i_);FZ=s(YMe,"STRONG",{});var yjr=n(FZ);uho=r(yjr,"roberta"),yjr.forEach(t),pho=r(YMe," \u2014 "),YS=s(YMe,"A",{href:!0});var wjr=n(YS);_ho=r(wjr,"RobertaForCausalLM"),wjr.forEach(t),bho=r(YMe," (RoBERTa model)"),YMe.forEach(t),vho=i(j),d_=s(j,"LI",{});var KMe=n(d_);CZ=s(KMe,"STRONG",{});var Ajr=n(CZ);Tho=r(Ajr,"roformer"),Ajr.forEach(t),Fho=r(KMe," \u2014 "),KS=s(KMe,"A",{href:!0});var Ljr=n(KS);Cho=r(Ljr,"RoFormerForCausalLM"),Ljr.forEach(t),Mho=r(KMe," (RoFormer model)"),KMe.forEach(t),Eho=i(j),c_=s(j,"LI",{});var ZMe=n(c_);MZ=s(ZMe,"STRONG",{});var Bjr=n(MZ);yho=r(Bjr,"speech_to_text_2"),Bjr.forEach(t),who=r(ZMe," \u2014 "),ZS=s(ZMe,"A",{href:!0});var kjr=n(ZS);Aho=r(kjr,"Speech2Text2ForCausalLM"),kjr.forEach(t),Lho=r(ZMe," (Speech2Text2 model)"),ZMe.forEach(t),Bho=i(j),m_=s(j,"LI",{});var eEe=n(m_);EZ=s(eEe,"STRONG",{});var xjr=n(EZ);kho=r(xjr,"transfo-xl"),xjr.forEach(t),xho=r(eEe," \u2014 "),eP=s(eEe,"A",{href:!0});var Rjr=n(eP);Rho=r(Rjr,"TransfoXLLMHeadModel"),Rjr.forEach(t),Sho=r(eEe," (Transformer-XL model)"),eEe.forEach(t),Pho=i(j),f_=s(j,"LI",{});var oEe=n(f_);yZ=s(oEe,"STRONG",{});var Sjr=n(yZ);$ho=r(Sjr,"trocr"),Sjr.forEach(t),Iho=r(oEe," \u2014 "),oP=s(oEe,"A",{href:!0});var Pjr=n(oP);jho=r(Pjr,"TrOCRForCausalLM"),Pjr.forEach(t),Nho=r(oEe," (TrOCR model)"),oEe.forEach(t),Dho=i(j),g_=s(j,"LI",{});var rEe=n(g_);wZ=s(rEe,"STRONG",{});var $jr=n(wZ);qho=r($jr,"xglm"),$jr.forEach(t),Gho=r(rEe," \u2014 "),rP=s(rEe,"A",{href:!0});var Ijr=n(rP);Oho=r(Ijr,"XGLMForCausalLM"),Ijr.forEach(t),Xho=r(rEe," (XGLM model)"),rEe.forEach(t),zho=i(j),h_=s(j,"LI",{});var tEe=n(h_);AZ=s(tEe,"STRONG",{});var jjr=n(AZ);Vho=r(jjr,"xlm"),jjr.forEach(t),Who=r(tEe," \u2014 "),tP=s(tEe,"A",{href:!0});var Njr=n(tP);Qho=r(Njr,"XLMWithLMHeadModel"),Njr.forEach(t),Hho=r(tEe," (XLM model)"),tEe.forEach(t),Uho=i(j),u_=s(j,"LI",{});var aEe=n(u_);LZ=s(aEe,"STRONG",{});var Djr=n(LZ);Jho=r(Djr,"xlm-prophetnet"),Djr.forEach(t),Yho=r(aEe," \u2014 "),aP=s(aEe,"A",{href:!0});var qjr=n(aP);Kho=r(qjr,"XLMProphetNetForCausalLM"),qjr.forEach(t),Zho=r(aEe," (XLMProphetNet model)"),aEe.forEach(t),euo=i(j),p_=s(j,"LI",{});var sEe=n(p_);BZ=s(sEe,"STRONG",{});var Gjr=n(BZ);ouo=r(Gjr,"xlm-roberta"),Gjr.forEach(t),ruo=r(sEe," \u2014 "),sP=s(sEe,"A",{href:!0});var Ojr=n(sP);tuo=r(Ojr,"XLMRobertaForCausalLM"),Ojr.forEach(t),auo=r(sEe," (XLM-RoBERTa model)"),sEe.forEach(t),suo=i(j),__=s(j,"LI",{});var nEe=n(__);kZ=s(nEe,"STRONG",{});var Xjr=n(kZ);nuo=r(Xjr,"xlm-roberta-xl"),Xjr.forEach(t),luo=r(nEe," \u2014 "),nP=s(nEe,"A",{href:!0});var zjr=n(nP);iuo=r(zjr,"XLMRobertaXLForCausalLM"),zjr.forEach(t),duo=r(nEe," (XLM-RoBERTa-XL model)"),nEe.forEach(t),cuo=i(j),b_=s(j,"LI",{});var lEe=n(b_);xZ=s(lEe,"STRONG",{});var Vjr=n(xZ);muo=r(Vjr,"xlnet"),Vjr.forEach(t),fuo=r(lEe," \u2014 "),lP=s(lEe,"A",{href:!0});var Wjr=n(lP);guo=r(Wjr,"XLNetLMHeadModel"),Wjr.forEach(t),huo=r(lEe," (XLNet model)"),lEe.forEach(t),j.forEach(t),uuo=i($t),v_=s($t,"P",{});var iEe=n(v_);puo=r(iEe,"The model is set in evaluation mode by default using "),RZ=s(iEe,"CODE",{});var Qjr=n(RZ);_uo=r(Qjr,"model.eval()"),Qjr.forEach(t),buo=r(iEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),SZ=s(iEe,"CODE",{});var Hjr=n(SZ);vuo=r(Hjr,"model.train()"),Hjr.forEach(t),iEe.forEach(t),Tuo=i($t),PZ=s($t,"P",{});var Ujr=n(PZ);Fuo=r(Ujr,"Examples:"),Ujr.forEach(t),Cuo=i($t),f(oy.$$.fragment,$t),$t.forEach(t),On.forEach(t),C8e=i(d),Ui=s(d,"H2",{class:!0});var BBe=n(Ui);T_=s(BBe,"A",{id:!0,class:!0,href:!0});var Jjr=n(T_);$Z=s(Jjr,"SPAN",{});var Yjr=n($Z);f(ry.$$.fragment,Yjr),Yjr.forEach(t),Jjr.forEach(t),Muo=i(BBe),IZ=s(BBe,"SPAN",{});var Kjr=n(IZ);Euo=r(Kjr,"AutoModelForMaskedLM"),Kjr.forEach(t),BBe.forEach(t),M8e=i(d),Ho=s(d,"DIV",{class:!0});var zn=n(Ho);f(ty.$$.fragment,zn),yuo=i(zn),Ji=s(zn,"P",{});var KX=n(Ji);wuo=r(KX,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),jZ=s(KX,"CODE",{});var Zjr=n(jZ);Auo=r(Zjr,"from_pretrained()"),Zjr.forEach(t),Luo=r(KX,"class method or the "),NZ=s(KX,"CODE",{});var eNr=n(NZ);Buo=r(eNr,"from_config()"),eNr.forEach(t),kuo=r(KX,`class
method.`),KX.forEach(t),xuo=i(zn),ay=s(zn,"P",{});var kBe=n(ay);Ruo=r(kBe,"This class cannot be instantiated directly using "),DZ=s(kBe,"CODE",{});var oNr=n(DZ);Suo=r(oNr,"__init__()"),oNr.forEach(t),Puo=r(kBe," (throws an error)."),kBe.forEach(t),$uo=i(zn),Gr=s(zn,"DIV",{class:!0});var Vn=n(Gr);f(sy.$$.fragment,Vn),Iuo=i(Vn),qZ=s(Vn,"P",{});var rNr=n(qZ);juo=r(rNr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),rNr.forEach(t),Nuo=i(Vn),Yi=s(Vn,"P",{});var ZX=n(Yi);Duo=r(ZX,`Note:
Loading a model from its configuration file does `),GZ=s(ZX,"STRONG",{});var tNr=n(GZ);quo=r(tNr,"not"),tNr.forEach(t),Guo=r(ZX,` load the model weights. It only affects the
model\u2019s configuration. Use `),OZ=s(ZX,"CODE",{});var aNr=n(OZ);Ouo=r(aNr,"from_pretrained()"),aNr.forEach(t),Xuo=r(ZX,"to load the model weights."),ZX.forEach(t),zuo=i(Vn),XZ=s(Vn,"P",{});var sNr=n(XZ);Vuo=r(sNr,"Examples:"),sNr.forEach(t),Wuo=i(Vn),f(ny.$$.fragment,Vn),Vn.forEach(t),Quo=i(zn),Se=s(zn,"DIV",{class:!0});var It=n(Se);f(ly.$$.fragment,It),Huo=i(It),zZ=s(It,"P",{});var nNr=n(zZ);Uuo=r(nNr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),nNr.forEach(t),Juo=i(It),Oa=s(It,"P",{});var c3=n(Oa);Yuo=r(c3,"The model class to instantiate is selected based on the "),VZ=s(c3,"CODE",{});var lNr=n(VZ);Kuo=r(lNr,"model_type"),lNr.forEach(t),Zuo=r(c3,` property of the config object (either
passed as an argument or loaded from `),WZ=s(c3,"CODE",{});var iNr=n(WZ);epo=r(iNr,"pretrained_model_name_or_path"),iNr.forEach(t),opo=r(c3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),QZ=s(c3,"CODE",{});var dNr=n(QZ);rpo=r(dNr,"pretrained_model_name_or_path"),dNr.forEach(t),tpo=r(c3,":"),c3.forEach(t),apo=i(It),I=s(It,"UL",{});var N=n(I);F_=s(N,"LI",{});var dEe=n(F_);HZ=s(dEe,"STRONG",{});var cNr=n(HZ);spo=r(cNr,"albert"),cNr.forEach(t),npo=r(dEe," \u2014 "),iP=s(dEe,"A",{href:!0});var mNr=n(iP);lpo=r(mNr,"AlbertForMaskedLM"),mNr.forEach(t),ipo=r(dEe," (ALBERT model)"),dEe.forEach(t),dpo=i(N),C_=s(N,"LI",{});var cEe=n(C_);UZ=s(cEe,"STRONG",{});var fNr=n(UZ);cpo=r(fNr,"bart"),fNr.forEach(t),mpo=r(cEe," \u2014 "),dP=s(cEe,"A",{href:!0});var gNr=n(dP);fpo=r(gNr,"BartForConditionalGeneration"),gNr.forEach(t),gpo=r(cEe," (BART model)"),cEe.forEach(t),hpo=i(N),M_=s(N,"LI",{});var mEe=n(M_);JZ=s(mEe,"STRONG",{});var hNr=n(JZ);upo=r(hNr,"bert"),hNr.forEach(t),ppo=r(mEe," \u2014 "),cP=s(mEe,"A",{href:!0});var uNr=n(cP);_po=r(uNr,"BertForMaskedLM"),uNr.forEach(t),bpo=r(mEe," (BERT model)"),mEe.forEach(t),vpo=i(N),E_=s(N,"LI",{});var fEe=n(E_);YZ=s(fEe,"STRONG",{});var pNr=n(YZ);Tpo=r(pNr,"big_bird"),pNr.forEach(t),Fpo=r(fEe," \u2014 "),mP=s(fEe,"A",{href:!0});var _Nr=n(mP);Cpo=r(_Nr,"BigBirdForMaskedLM"),_Nr.forEach(t),Mpo=r(fEe," (BigBird model)"),fEe.forEach(t),Epo=i(N),y_=s(N,"LI",{});var gEe=n(y_);KZ=s(gEe,"STRONG",{});var bNr=n(KZ);ypo=r(bNr,"camembert"),bNr.forEach(t),wpo=r(gEe," \u2014 "),fP=s(gEe,"A",{href:!0});var vNr=n(fP);Apo=r(vNr,"CamembertForMaskedLM"),vNr.forEach(t),Lpo=r(gEe," (CamemBERT model)"),gEe.forEach(t),Bpo=i(N),w_=s(N,"LI",{});var hEe=n(w_);ZZ=s(hEe,"STRONG",{});var TNr=n(ZZ);kpo=r(TNr,"convbert"),TNr.forEach(t),xpo=r(hEe," \u2014 "),gP=s(hEe,"A",{href:!0});var FNr=n(gP);Rpo=r(FNr,"ConvBertForMaskedLM"),FNr.forEach(t),Spo=r(hEe," (ConvBERT model)"),hEe.forEach(t),Ppo=i(N),A_=s(N,"LI",{});var uEe=n(A_);eee=s(uEe,"STRONG",{});var CNr=n(eee);$po=r(CNr,"deberta"),CNr.forEach(t),Ipo=r(uEe," \u2014 "),hP=s(uEe,"A",{href:!0});var MNr=n(hP);jpo=r(MNr,"DebertaForMaskedLM"),MNr.forEach(t),Npo=r(uEe," (DeBERTa model)"),uEe.forEach(t),Dpo=i(N),L_=s(N,"LI",{});var pEe=n(L_);oee=s(pEe,"STRONG",{});var ENr=n(oee);qpo=r(ENr,"deberta-v2"),ENr.forEach(t),Gpo=r(pEe," \u2014 "),uP=s(pEe,"A",{href:!0});var yNr=n(uP);Opo=r(yNr,"DebertaV2ForMaskedLM"),yNr.forEach(t),Xpo=r(pEe," (DeBERTa-v2 model)"),pEe.forEach(t),zpo=i(N),B_=s(N,"LI",{});var _Ee=n(B_);ree=s(_Ee,"STRONG",{});var wNr=n(ree);Vpo=r(wNr,"distilbert"),wNr.forEach(t),Wpo=r(_Ee," \u2014 "),pP=s(_Ee,"A",{href:!0});var ANr=n(pP);Qpo=r(ANr,"DistilBertForMaskedLM"),ANr.forEach(t),Hpo=r(_Ee," (DistilBERT model)"),_Ee.forEach(t),Upo=i(N),k_=s(N,"LI",{});var bEe=n(k_);tee=s(bEe,"STRONG",{});var LNr=n(tee);Jpo=r(LNr,"electra"),LNr.forEach(t),Ypo=r(bEe," \u2014 "),_P=s(bEe,"A",{href:!0});var BNr=n(_P);Kpo=r(BNr,"ElectraForMaskedLM"),BNr.forEach(t),Zpo=r(bEe," (ELECTRA model)"),bEe.forEach(t),e_o=i(N),x_=s(N,"LI",{});var vEe=n(x_);aee=s(vEe,"STRONG",{});var kNr=n(aee);o_o=r(kNr,"flaubert"),kNr.forEach(t),r_o=r(vEe," \u2014 "),bP=s(vEe,"A",{href:!0});var xNr=n(bP);t_o=r(xNr,"FlaubertWithLMHeadModel"),xNr.forEach(t),a_o=r(vEe," (FlauBERT model)"),vEe.forEach(t),s_o=i(N),R_=s(N,"LI",{});var TEe=n(R_);see=s(TEe,"STRONG",{});var RNr=n(see);n_o=r(RNr,"fnet"),RNr.forEach(t),l_o=r(TEe," \u2014 "),vP=s(TEe,"A",{href:!0});var SNr=n(vP);i_o=r(SNr,"FNetForMaskedLM"),SNr.forEach(t),d_o=r(TEe," (FNet model)"),TEe.forEach(t),c_o=i(N),S_=s(N,"LI",{});var FEe=n(S_);nee=s(FEe,"STRONG",{});var PNr=n(nee);m_o=r(PNr,"funnel"),PNr.forEach(t),f_o=r(FEe," \u2014 "),TP=s(FEe,"A",{href:!0});var $Nr=n(TP);g_o=r($Nr,"FunnelForMaskedLM"),$Nr.forEach(t),h_o=r(FEe," (Funnel Transformer model)"),FEe.forEach(t),u_o=i(N),P_=s(N,"LI",{});var CEe=n(P_);lee=s(CEe,"STRONG",{});var INr=n(lee);p_o=r(INr,"ibert"),INr.forEach(t),__o=r(CEe," \u2014 "),FP=s(CEe,"A",{href:!0});var jNr=n(FP);b_o=r(jNr,"IBertForMaskedLM"),jNr.forEach(t),v_o=r(CEe," (I-BERT model)"),CEe.forEach(t),T_o=i(N),$_=s(N,"LI",{});var MEe=n($_);iee=s(MEe,"STRONG",{});var NNr=n(iee);F_o=r(NNr,"layoutlm"),NNr.forEach(t),C_o=r(MEe," \u2014 "),CP=s(MEe,"A",{href:!0});var DNr=n(CP);M_o=r(DNr,"LayoutLMForMaskedLM"),DNr.forEach(t),E_o=r(MEe," (LayoutLM model)"),MEe.forEach(t),y_o=i(N),I_=s(N,"LI",{});var EEe=n(I_);dee=s(EEe,"STRONG",{});var qNr=n(dee);w_o=r(qNr,"longformer"),qNr.forEach(t),A_o=r(EEe," \u2014 "),MP=s(EEe,"A",{href:!0});var GNr=n(MP);L_o=r(GNr,"LongformerForMaskedLM"),GNr.forEach(t),B_o=r(EEe," (Longformer model)"),EEe.forEach(t),k_o=i(N),j_=s(N,"LI",{});var yEe=n(j_);cee=s(yEe,"STRONG",{});var ONr=n(cee);x_o=r(ONr,"mbart"),ONr.forEach(t),R_o=r(yEe," \u2014 "),EP=s(yEe,"A",{href:!0});var XNr=n(EP);S_o=r(XNr,"MBartForConditionalGeneration"),XNr.forEach(t),P_o=r(yEe," (mBART model)"),yEe.forEach(t),$_o=i(N),N_=s(N,"LI",{});var wEe=n(N_);mee=s(wEe,"STRONG",{});var zNr=n(mee);I_o=r(zNr,"megatron-bert"),zNr.forEach(t),j_o=r(wEe," \u2014 "),yP=s(wEe,"A",{href:!0});var VNr=n(yP);N_o=r(VNr,"MegatronBertForMaskedLM"),VNr.forEach(t),D_o=r(wEe," (MegatronBert model)"),wEe.forEach(t),q_o=i(N),D_=s(N,"LI",{});var AEe=n(D_);fee=s(AEe,"STRONG",{});var WNr=n(fee);G_o=r(WNr,"mobilebert"),WNr.forEach(t),O_o=r(AEe," \u2014 "),wP=s(AEe,"A",{href:!0});var QNr=n(wP);X_o=r(QNr,"MobileBertForMaskedLM"),QNr.forEach(t),z_o=r(AEe," (MobileBERT model)"),AEe.forEach(t),V_o=i(N),q_=s(N,"LI",{});var LEe=n(q_);gee=s(LEe,"STRONG",{});var HNr=n(gee);W_o=r(HNr,"mpnet"),HNr.forEach(t),Q_o=r(LEe," \u2014 "),AP=s(LEe,"A",{href:!0});var UNr=n(AP);H_o=r(UNr,"MPNetForMaskedLM"),UNr.forEach(t),U_o=r(LEe," (MPNet model)"),LEe.forEach(t),J_o=i(N),G_=s(N,"LI",{});var BEe=n(G_);hee=s(BEe,"STRONG",{});var JNr=n(hee);Y_o=r(JNr,"nystromformer"),JNr.forEach(t),K_o=r(BEe," \u2014 "),LP=s(BEe,"A",{href:!0});var YNr=n(LP);Z_o=r(YNr,"NystromformerForMaskedLM"),YNr.forEach(t),ebo=r(BEe," (Nystromformer model)"),BEe.forEach(t),obo=i(N),O_=s(N,"LI",{});var kEe=n(O_);uee=s(kEe,"STRONG",{});var KNr=n(uee);rbo=r(KNr,"perceiver"),KNr.forEach(t),tbo=r(kEe," \u2014 "),BP=s(kEe,"A",{href:!0});var ZNr=n(BP);abo=r(ZNr,"PerceiverForMaskedLM"),ZNr.forEach(t),sbo=r(kEe," (Perceiver model)"),kEe.forEach(t),nbo=i(N),X_=s(N,"LI",{});var xEe=n(X_);pee=s(xEe,"STRONG",{});var eDr=n(pee);lbo=r(eDr,"qdqbert"),eDr.forEach(t),ibo=r(xEe," \u2014 "),kP=s(xEe,"A",{href:!0});var oDr=n(kP);dbo=r(oDr,"QDQBertForMaskedLM"),oDr.forEach(t),cbo=r(xEe," (QDQBert model)"),xEe.forEach(t),mbo=i(N),z_=s(N,"LI",{});var REe=n(z_);_ee=s(REe,"STRONG",{});var rDr=n(_ee);fbo=r(rDr,"reformer"),rDr.forEach(t),gbo=r(REe," \u2014 "),xP=s(REe,"A",{href:!0});var tDr=n(xP);hbo=r(tDr,"ReformerForMaskedLM"),tDr.forEach(t),ubo=r(REe," (Reformer model)"),REe.forEach(t),pbo=i(N),V_=s(N,"LI",{});var SEe=n(V_);bee=s(SEe,"STRONG",{});var aDr=n(bee);_bo=r(aDr,"rembert"),aDr.forEach(t),bbo=r(SEe," \u2014 "),RP=s(SEe,"A",{href:!0});var sDr=n(RP);vbo=r(sDr,"RemBertForMaskedLM"),sDr.forEach(t),Tbo=r(SEe," (RemBERT model)"),SEe.forEach(t),Fbo=i(N),W_=s(N,"LI",{});var PEe=n(W_);vee=s(PEe,"STRONG",{});var nDr=n(vee);Cbo=r(nDr,"roberta"),nDr.forEach(t),Mbo=r(PEe," \u2014 "),SP=s(PEe,"A",{href:!0});var lDr=n(SP);Ebo=r(lDr,"RobertaForMaskedLM"),lDr.forEach(t),ybo=r(PEe," (RoBERTa model)"),PEe.forEach(t),wbo=i(N),Q_=s(N,"LI",{});var $Ee=n(Q_);Tee=s($Ee,"STRONG",{});var iDr=n(Tee);Abo=r(iDr,"roformer"),iDr.forEach(t),Lbo=r($Ee," \u2014 "),PP=s($Ee,"A",{href:!0});var dDr=n(PP);Bbo=r(dDr,"RoFormerForMaskedLM"),dDr.forEach(t),kbo=r($Ee," (RoFormer model)"),$Ee.forEach(t),xbo=i(N),H_=s(N,"LI",{});var IEe=n(H_);Fee=s(IEe,"STRONG",{});var cDr=n(Fee);Rbo=r(cDr,"squeezebert"),cDr.forEach(t),Sbo=r(IEe," \u2014 "),$P=s(IEe,"A",{href:!0});var mDr=n($P);Pbo=r(mDr,"SqueezeBertForMaskedLM"),mDr.forEach(t),$bo=r(IEe," (SqueezeBERT model)"),IEe.forEach(t),Ibo=i(N),U_=s(N,"LI",{});var jEe=n(U_);Cee=s(jEe,"STRONG",{});var fDr=n(Cee);jbo=r(fDr,"tapas"),fDr.forEach(t),Nbo=r(jEe," \u2014 "),IP=s(jEe,"A",{href:!0});var gDr=n(IP);Dbo=r(gDr,"TapasForMaskedLM"),gDr.forEach(t),qbo=r(jEe," (TAPAS model)"),jEe.forEach(t),Gbo=i(N),J_=s(N,"LI",{});var NEe=n(J_);Mee=s(NEe,"STRONG",{});var hDr=n(Mee);Obo=r(hDr,"wav2vec2"),hDr.forEach(t),Xbo=r(NEe," \u2014 "),Eee=s(NEe,"CODE",{});var uDr=n(Eee);zbo=r(uDr,"Wav2Vec2ForMaskedLM"),uDr.forEach(t),Vbo=r(NEe,"(Wav2Vec2 model)"),NEe.forEach(t),Wbo=i(N),Y_=s(N,"LI",{});var DEe=n(Y_);yee=s(DEe,"STRONG",{});var pDr=n(yee);Qbo=r(pDr,"xlm"),pDr.forEach(t),Hbo=r(DEe," \u2014 "),jP=s(DEe,"A",{href:!0});var _Dr=n(jP);Ubo=r(_Dr,"XLMWithLMHeadModel"),_Dr.forEach(t),Jbo=r(DEe," (XLM model)"),DEe.forEach(t),Ybo=i(N),K_=s(N,"LI",{});var qEe=n(K_);wee=s(qEe,"STRONG",{});var bDr=n(wee);Kbo=r(bDr,"xlm-roberta"),bDr.forEach(t),Zbo=r(qEe," \u2014 "),NP=s(qEe,"A",{href:!0});var vDr=n(NP);e2o=r(vDr,"XLMRobertaForMaskedLM"),vDr.forEach(t),o2o=r(qEe," (XLM-RoBERTa model)"),qEe.forEach(t),r2o=i(N),Z_=s(N,"LI",{});var GEe=n(Z_);Aee=s(GEe,"STRONG",{});var TDr=n(Aee);t2o=r(TDr,"xlm-roberta-xl"),TDr.forEach(t),a2o=r(GEe," \u2014 "),DP=s(GEe,"A",{href:!0});var FDr=n(DP);s2o=r(FDr,"XLMRobertaXLForMaskedLM"),FDr.forEach(t),n2o=r(GEe," (XLM-RoBERTa-XL model)"),GEe.forEach(t),l2o=i(N),eb=s(N,"LI",{});var OEe=n(eb);Lee=s(OEe,"STRONG",{});var CDr=n(Lee);i2o=r(CDr,"yoso"),CDr.forEach(t),d2o=r(OEe," \u2014 "),qP=s(OEe,"A",{href:!0});var MDr=n(qP);c2o=r(MDr,"YosoForMaskedLM"),MDr.forEach(t),m2o=r(OEe," (YOSO model)"),OEe.forEach(t),N.forEach(t),f2o=i(It),ob=s(It,"P",{});var XEe=n(ob);g2o=r(XEe,"The model is set in evaluation mode by default using "),Bee=s(XEe,"CODE",{});var EDr=n(Bee);h2o=r(EDr,"model.eval()"),EDr.forEach(t),u2o=r(XEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),kee=s(XEe,"CODE",{});var yDr=n(kee);p2o=r(yDr,"model.train()"),yDr.forEach(t),XEe.forEach(t),_2o=i(It),xee=s(It,"P",{});var wDr=n(xee);b2o=r(wDr,"Examples:"),wDr.forEach(t),v2o=i(It),f(iy.$$.fragment,It),It.forEach(t),zn.forEach(t),E8e=i(d),Ki=s(d,"H2",{class:!0});var xBe=n(Ki);rb=s(xBe,"A",{id:!0,class:!0,href:!0});var ADr=n(rb);Ree=s(ADr,"SPAN",{});var LDr=n(Ree);f(dy.$$.fragment,LDr),LDr.forEach(t),ADr.forEach(t),T2o=i(xBe),See=s(xBe,"SPAN",{});var BDr=n(See);F2o=r(BDr,"AutoModelForSeq2SeqLM"),BDr.forEach(t),xBe.forEach(t),y8e=i(d),Uo=s(d,"DIV",{class:!0});var Wn=n(Uo);f(cy.$$.fragment,Wn),C2o=i(Wn),Zi=s(Wn,"P",{});var ez=n(Zi);M2o=r(ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Pee=s(ez,"CODE",{});var kDr=n(Pee);E2o=r(kDr,"from_pretrained()"),kDr.forEach(t),y2o=r(ez,"class method or the "),$ee=s(ez,"CODE",{});var xDr=n($ee);w2o=r(xDr,"from_config()"),xDr.forEach(t),A2o=r(ez,`class
method.`),ez.forEach(t),L2o=i(Wn),my=s(Wn,"P",{});var RBe=n(my);B2o=r(RBe,"This class cannot be instantiated directly using "),Iee=s(RBe,"CODE",{});var RDr=n(Iee);k2o=r(RDr,"__init__()"),RDr.forEach(t),x2o=r(RBe," (throws an error)."),RBe.forEach(t),R2o=i(Wn),Or=s(Wn,"DIV",{class:!0});var Qn=n(Or);f(fy.$$.fragment,Qn),S2o=i(Qn),jee=s(Qn,"P",{});var SDr=n(jee);P2o=r(SDr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),SDr.forEach(t),$2o=i(Qn),ed=s(Qn,"P",{});var oz=n(ed);I2o=r(oz,`Note:
Loading a model from its configuration file does `),Nee=s(oz,"STRONG",{});var PDr=n(Nee);j2o=r(PDr,"not"),PDr.forEach(t),N2o=r(oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Dee=s(oz,"CODE",{});var $Dr=n(Dee);D2o=r($Dr,"from_pretrained()"),$Dr.forEach(t),q2o=r(oz,"to load the model weights."),oz.forEach(t),G2o=i(Qn),qee=s(Qn,"P",{});var IDr=n(qee);O2o=r(IDr,"Examples:"),IDr.forEach(t),X2o=i(Qn),f(gy.$$.fragment,Qn),Qn.forEach(t),z2o=i(Wn),Pe=s(Wn,"DIV",{class:!0});var jt=n(Pe);f(hy.$$.fragment,jt),V2o=i(jt),Gee=s(jt,"P",{});var jDr=n(Gee);W2o=r(jDr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),jDr.forEach(t),Q2o=i(jt),Xa=s(jt,"P",{});var m3=n(Xa);H2o=r(m3,"The model class to instantiate is selected based on the "),Oee=s(m3,"CODE",{});var NDr=n(Oee);U2o=r(NDr,"model_type"),NDr.forEach(t),J2o=r(m3,` property of the config object (either
passed as an argument or loaded from `),Xee=s(m3,"CODE",{});var DDr=n(Xee);Y2o=r(DDr,"pretrained_model_name_or_path"),DDr.forEach(t),K2o=r(m3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zee=s(m3,"CODE",{});var qDr=n(zee);Z2o=r(qDr,"pretrained_model_name_or_path"),qDr.forEach(t),evo=r(m3,":"),m3.forEach(t),ovo=i(jt),ae=s(jt,"UL",{});var le=n(ae);tb=s(le,"LI",{});var zEe=n(tb);Vee=s(zEe,"STRONG",{});var GDr=n(Vee);rvo=r(GDr,"bart"),GDr.forEach(t),tvo=r(zEe," \u2014 "),GP=s(zEe,"A",{href:!0});var ODr=n(GP);avo=r(ODr,"BartForConditionalGeneration"),ODr.forEach(t),svo=r(zEe," (BART model)"),zEe.forEach(t),nvo=i(le),ab=s(le,"LI",{});var VEe=n(ab);Wee=s(VEe,"STRONG",{});var XDr=n(Wee);lvo=r(XDr,"bigbird_pegasus"),XDr.forEach(t),ivo=r(VEe," \u2014 "),OP=s(VEe,"A",{href:!0});var zDr=n(OP);dvo=r(zDr,"BigBirdPegasusForConditionalGeneration"),zDr.forEach(t),cvo=r(VEe," (BigBirdPegasus model)"),VEe.forEach(t),mvo=i(le),sb=s(le,"LI",{});var WEe=n(sb);Qee=s(WEe,"STRONG",{});var VDr=n(Qee);fvo=r(VDr,"blenderbot"),VDr.forEach(t),gvo=r(WEe," \u2014 "),XP=s(WEe,"A",{href:!0});var WDr=n(XP);hvo=r(WDr,"BlenderbotForConditionalGeneration"),WDr.forEach(t),uvo=r(WEe," (Blenderbot model)"),WEe.forEach(t),pvo=i(le),nb=s(le,"LI",{});var QEe=n(nb);Hee=s(QEe,"STRONG",{});var QDr=n(Hee);_vo=r(QDr,"blenderbot-small"),QDr.forEach(t),bvo=r(QEe," \u2014 "),zP=s(QEe,"A",{href:!0});var HDr=n(zP);vvo=r(HDr,"BlenderbotSmallForConditionalGeneration"),HDr.forEach(t),Tvo=r(QEe," (BlenderbotSmall model)"),QEe.forEach(t),Fvo=i(le),lb=s(le,"LI",{});var HEe=n(lb);Uee=s(HEe,"STRONG",{});var UDr=n(Uee);Cvo=r(UDr,"encoder-decoder"),UDr.forEach(t),Mvo=r(HEe," \u2014 "),VP=s(HEe,"A",{href:!0});var JDr=n(VP);Evo=r(JDr,"EncoderDecoderModel"),JDr.forEach(t),yvo=r(HEe," (Encoder decoder model)"),HEe.forEach(t),wvo=i(le),ib=s(le,"LI",{});var UEe=n(ib);Jee=s(UEe,"STRONG",{});var YDr=n(Jee);Avo=r(YDr,"fsmt"),YDr.forEach(t),Lvo=r(UEe," \u2014 "),WP=s(UEe,"A",{href:!0});var KDr=n(WP);Bvo=r(KDr,"FSMTForConditionalGeneration"),KDr.forEach(t),kvo=r(UEe," (FairSeq Machine-Translation model)"),UEe.forEach(t),xvo=i(le),db=s(le,"LI",{});var JEe=n(db);Yee=s(JEe,"STRONG",{});var ZDr=n(Yee);Rvo=r(ZDr,"led"),ZDr.forEach(t),Svo=r(JEe," \u2014 "),QP=s(JEe,"A",{href:!0});var eqr=n(QP);Pvo=r(eqr,"LEDForConditionalGeneration"),eqr.forEach(t),$vo=r(JEe," (LED model)"),JEe.forEach(t),Ivo=i(le),cb=s(le,"LI",{});var YEe=n(cb);Kee=s(YEe,"STRONG",{});var oqr=n(Kee);jvo=r(oqr,"m2m_100"),oqr.forEach(t),Nvo=r(YEe," \u2014 "),HP=s(YEe,"A",{href:!0});var rqr=n(HP);Dvo=r(rqr,"M2M100ForConditionalGeneration"),rqr.forEach(t),qvo=r(YEe," (M2M100 model)"),YEe.forEach(t),Gvo=i(le),mb=s(le,"LI",{});var KEe=n(mb);Zee=s(KEe,"STRONG",{});var tqr=n(Zee);Ovo=r(tqr,"marian"),tqr.forEach(t),Xvo=r(KEe," \u2014 "),UP=s(KEe,"A",{href:!0});var aqr=n(UP);zvo=r(aqr,"MarianMTModel"),aqr.forEach(t),Vvo=r(KEe," (Marian model)"),KEe.forEach(t),Wvo=i(le),fb=s(le,"LI",{});var ZEe=n(fb);eoe=s(ZEe,"STRONG",{});var sqr=n(eoe);Qvo=r(sqr,"mbart"),sqr.forEach(t),Hvo=r(ZEe," \u2014 "),JP=s(ZEe,"A",{href:!0});var nqr=n(JP);Uvo=r(nqr,"MBartForConditionalGeneration"),nqr.forEach(t),Jvo=r(ZEe," (mBART model)"),ZEe.forEach(t),Yvo=i(le),gb=s(le,"LI",{});var e3e=n(gb);ooe=s(e3e,"STRONG",{});var lqr=n(ooe);Kvo=r(lqr,"mt5"),lqr.forEach(t),Zvo=r(e3e," \u2014 "),YP=s(e3e,"A",{href:!0});var iqr=n(YP);eTo=r(iqr,"MT5ForConditionalGeneration"),iqr.forEach(t),oTo=r(e3e," (mT5 model)"),e3e.forEach(t),rTo=i(le),hb=s(le,"LI",{});var o3e=n(hb);roe=s(o3e,"STRONG",{});var dqr=n(roe);tTo=r(dqr,"pegasus"),dqr.forEach(t),aTo=r(o3e," \u2014 "),KP=s(o3e,"A",{href:!0});var cqr=n(KP);sTo=r(cqr,"PegasusForConditionalGeneration"),cqr.forEach(t),nTo=r(o3e," (Pegasus model)"),o3e.forEach(t),lTo=i(le),ub=s(le,"LI",{});var r3e=n(ub);toe=s(r3e,"STRONG",{});var mqr=n(toe);iTo=r(mqr,"plbart"),mqr.forEach(t),dTo=r(r3e," \u2014 "),ZP=s(r3e,"A",{href:!0});var fqr=n(ZP);cTo=r(fqr,"PLBartForConditionalGeneration"),fqr.forEach(t),mTo=r(r3e," (PLBart model)"),r3e.forEach(t),fTo=i(le),pb=s(le,"LI",{});var t3e=n(pb);aoe=s(t3e,"STRONG",{});var gqr=n(aoe);gTo=r(gqr,"prophetnet"),gqr.forEach(t),hTo=r(t3e," \u2014 "),e$=s(t3e,"A",{href:!0});var hqr=n(e$);uTo=r(hqr,"ProphetNetForConditionalGeneration"),hqr.forEach(t),pTo=r(t3e," (ProphetNet model)"),t3e.forEach(t),_To=i(le),_b=s(le,"LI",{});var a3e=n(_b);soe=s(a3e,"STRONG",{});var uqr=n(soe);bTo=r(uqr,"t5"),uqr.forEach(t),vTo=r(a3e," \u2014 "),o$=s(a3e,"A",{href:!0});var pqr=n(o$);TTo=r(pqr,"T5ForConditionalGeneration"),pqr.forEach(t),FTo=r(a3e," (T5 model)"),a3e.forEach(t),CTo=i(le),bb=s(le,"LI",{});var s3e=n(bb);noe=s(s3e,"STRONG",{});var _qr=n(noe);MTo=r(_qr,"xlm-prophetnet"),_qr.forEach(t),ETo=r(s3e," \u2014 "),r$=s(s3e,"A",{href:!0});var bqr=n(r$);yTo=r(bqr,"XLMProphetNetForConditionalGeneration"),bqr.forEach(t),wTo=r(s3e," (XLMProphetNet model)"),s3e.forEach(t),le.forEach(t),ATo=i(jt),vb=s(jt,"P",{});var n3e=n(vb);LTo=r(n3e,"The model is set in evaluation mode by default using "),loe=s(n3e,"CODE",{});var vqr=n(loe);BTo=r(vqr,"model.eval()"),vqr.forEach(t),kTo=r(n3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ioe=s(n3e,"CODE",{});var Tqr=n(ioe);xTo=r(Tqr,"model.train()"),Tqr.forEach(t),n3e.forEach(t),RTo=i(jt),doe=s(jt,"P",{});var Fqr=n(doe);STo=r(Fqr,"Examples:"),Fqr.forEach(t),PTo=i(jt),f(uy.$$.fragment,jt),jt.forEach(t),Wn.forEach(t),w8e=i(d),od=s(d,"H2",{class:!0});var SBe=n(od);Tb=s(SBe,"A",{id:!0,class:!0,href:!0});var Cqr=n(Tb);coe=s(Cqr,"SPAN",{});var Mqr=n(coe);f(py.$$.fragment,Mqr),Mqr.forEach(t),Cqr.forEach(t),$To=i(SBe),moe=s(SBe,"SPAN",{});var Eqr=n(moe);ITo=r(Eqr,"AutoModelForSequenceClassification"),Eqr.forEach(t),SBe.forEach(t),A8e=i(d),Jo=s(d,"DIV",{class:!0});var Hn=n(Jo);f(_y.$$.fragment,Hn),jTo=i(Hn),rd=s(Hn,"P",{});var rz=n(rd);NTo=r(rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),foe=s(rz,"CODE",{});var yqr=n(foe);DTo=r(yqr,"from_pretrained()"),yqr.forEach(t),qTo=r(rz,"class method or the "),goe=s(rz,"CODE",{});var wqr=n(goe);GTo=r(wqr,"from_config()"),wqr.forEach(t),OTo=r(rz,`class
method.`),rz.forEach(t),XTo=i(Hn),by=s(Hn,"P",{});var PBe=n(by);zTo=r(PBe,"This class cannot be instantiated directly using "),hoe=s(PBe,"CODE",{});var Aqr=n(hoe);VTo=r(Aqr,"__init__()"),Aqr.forEach(t),WTo=r(PBe," (throws an error)."),PBe.forEach(t),QTo=i(Hn),Xr=s(Hn,"DIV",{class:!0});var Un=n(Xr);f(vy.$$.fragment,Un),HTo=i(Un),uoe=s(Un,"P",{});var Lqr=n(uoe);UTo=r(Lqr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Lqr.forEach(t),JTo=i(Un),td=s(Un,"P",{});var tz=n(td);YTo=r(tz,`Note:
Loading a model from its configuration file does `),poe=s(tz,"STRONG",{});var Bqr=n(poe);KTo=r(Bqr,"not"),Bqr.forEach(t),ZTo=r(tz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_oe=s(tz,"CODE",{});var kqr=n(_oe);e1o=r(kqr,"from_pretrained()"),kqr.forEach(t),o1o=r(tz,"to load the model weights."),tz.forEach(t),r1o=i(Un),boe=s(Un,"P",{});var xqr=n(boe);t1o=r(xqr,"Examples:"),xqr.forEach(t),a1o=i(Un),f(Ty.$$.fragment,Un),Un.forEach(t),s1o=i(Hn),$e=s(Hn,"DIV",{class:!0});var Nt=n($e);f(Fy.$$.fragment,Nt),n1o=i(Nt),voe=s(Nt,"P",{});var Rqr=n(voe);l1o=r(Rqr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Rqr.forEach(t),i1o=i(Nt),za=s(Nt,"P",{});var f3=n(za);d1o=r(f3,"The model class to instantiate is selected based on the "),Toe=s(f3,"CODE",{});var Sqr=n(Toe);c1o=r(Sqr,"model_type"),Sqr.forEach(t),m1o=r(f3,` property of the config object (either
passed as an argument or loaded from `),Foe=s(f3,"CODE",{});var Pqr=n(Foe);f1o=r(Pqr,"pretrained_model_name_or_path"),Pqr.forEach(t),g1o=r(f3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Coe=s(f3,"CODE",{});var $qr=n(Coe);h1o=r($qr,"pretrained_model_name_or_path"),$qr.forEach(t),u1o=r(f3,":"),f3.forEach(t),p1o=i(Nt),A=s(Nt,"UL",{});var L=n(A);Fb=s(L,"LI",{});var l3e=n(Fb);Moe=s(l3e,"STRONG",{});var Iqr=n(Moe);_1o=r(Iqr,"albert"),Iqr.forEach(t),b1o=r(l3e," \u2014 "),t$=s(l3e,"A",{href:!0});var jqr=n(t$);v1o=r(jqr,"AlbertForSequenceClassification"),jqr.forEach(t),T1o=r(l3e," (ALBERT model)"),l3e.forEach(t),F1o=i(L),Cb=s(L,"LI",{});var i3e=n(Cb);Eoe=s(i3e,"STRONG",{});var Nqr=n(Eoe);C1o=r(Nqr,"bart"),Nqr.forEach(t),M1o=r(i3e," \u2014 "),a$=s(i3e,"A",{href:!0});var Dqr=n(a$);E1o=r(Dqr,"BartForSequenceClassification"),Dqr.forEach(t),y1o=r(i3e," (BART model)"),i3e.forEach(t),w1o=i(L),Mb=s(L,"LI",{});var d3e=n(Mb);yoe=s(d3e,"STRONG",{});var qqr=n(yoe);A1o=r(qqr,"bert"),qqr.forEach(t),L1o=r(d3e," \u2014 "),s$=s(d3e,"A",{href:!0});var Gqr=n(s$);B1o=r(Gqr,"BertForSequenceClassification"),Gqr.forEach(t),k1o=r(d3e," (BERT model)"),d3e.forEach(t),x1o=i(L),Eb=s(L,"LI",{});var c3e=n(Eb);woe=s(c3e,"STRONG",{});var Oqr=n(woe);R1o=r(Oqr,"big_bird"),Oqr.forEach(t),S1o=r(c3e," \u2014 "),n$=s(c3e,"A",{href:!0});var Xqr=n(n$);P1o=r(Xqr,"BigBirdForSequenceClassification"),Xqr.forEach(t),$1o=r(c3e," (BigBird model)"),c3e.forEach(t),I1o=i(L),yb=s(L,"LI",{});var m3e=n(yb);Aoe=s(m3e,"STRONG",{});var zqr=n(Aoe);j1o=r(zqr,"bigbird_pegasus"),zqr.forEach(t),N1o=r(m3e," \u2014 "),l$=s(m3e,"A",{href:!0});var Vqr=n(l$);D1o=r(Vqr,"BigBirdPegasusForSequenceClassification"),Vqr.forEach(t),q1o=r(m3e," (BigBirdPegasus model)"),m3e.forEach(t),G1o=i(L),wb=s(L,"LI",{});var f3e=n(wb);Loe=s(f3e,"STRONG",{});var Wqr=n(Loe);O1o=r(Wqr,"camembert"),Wqr.forEach(t),X1o=r(f3e," \u2014 "),i$=s(f3e,"A",{href:!0});var Qqr=n(i$);z1o=r(Qqr,"CamembertForSequenceClassification"),Qqr.forEach(t),V1o=r(f3e," (CamemBERT model)"),f3e.forEach(t),W1o=i(L),Ab=s(L,"LI",{});var g3e=n(Ab);Boe=s(g3e,"STRONG",{});var Hqr=n(Boe);Q1o=r(Hqr,"canine"),Hqr.forEach(t),H1o=r(g3e," \u2014 "),d$=s(g3e,"A",{href:!0});var Uqr=n(d$);U1o=r(Uqr,"CanineForSequenceClassification"),Uqr.forEach(t),J1o=r(g3e," (Canine model)"),g3e.forEach(t),Y1o=i(L),Lb=s(L,"LI",{});var h3e=n(Lb);koe=s(h3e,"STRONG",{});var Jqr=n(koe);K1o=r(Jqr,"convbert"),Jqr.forEach(t),Z1o=r(h3e," \u2014 "),c$=s(h3e,"A",{href:!0});var Yqr=n(c$);eFo=r(Yqr,"ConvBertForSequenceClassification"),Yqr.forEach(t),oFo=r(h3e," (ConvBERT model)"),h3e.forEach(t),rFo=i(L),Bb=s(L,"LI",{});var u3e=n(Bb);xoe=s(u3e,"STRONG",{});var Kqr=n(xoe);tFo=r(Kqr,"ctrl"),Kqr.forEach(t),aFo=r(u3e," \u2014 "),m$=s(u3e,"A",{href:!0});var Zqr=n(m$);sFo=r(Zqr,"CTRLForSequenceClassification"),Zqr.forEach(t),nFo=r(u3e," (CTRL model)"),u3e.forEach(t),lFo=i(L),kb=s(L,"LI",{});var p3e=n(kb);Roe=s(p3e,"STRONG",{});var eGr=n(Roe);iFo=r(eGr,"deberta"),eGr.forEach(t),dFo=r(p3e," \u2014 "),f$=s(p3e,"A",{href:!0});var oGr=n(f$);cFo=r(oGr,"DebertaForSequenceClassification"),oGr.forEach(t),mFo=r(p3e," (DeBERTa model)"),p3e.forEach(t),fFo=i(L),xb=s(L,"LI",{});var _3e=n(xb);Soe=s(_3e,"STRONG",{});var rGr=n(Soe);gFo=r(rGr,"deberta-v2"),rGr.forEach(t),hFo=r(_3e," \u2014 "),g$=s(_3e,"A",{href:!0});var tGr=n(g$);uFo=r(tGr,"DebertaV2ForSequenceClassification"),tGr.forEach(t),pFo=r(_3e," (DeBERTa-v2 model)"),_3e.forEach(t),_Fo=i(L),Rb=s(L,"LI",{});var b3e=n(Rb);Poe=s(b3e,"STRONG",{});var aGr=n(Poe);bFo=r(aGr,"distilbert"),aGr.forEach(t),vFo=r(b3e," \u2014 "),h$=s(b3e,"A",{href:!0});var sGr=n(h$);TFo=r(sGr,"DistilBertForSequenceClassification"),sGr.forEach(t),FFo=r(b3e," (DistilBERT model)"),b3e.forEach(t),CFo=i(L),Sb=s(L,"LI",{});var v3e=n(Sb);$oe=s(v3e,"STRONG",{});var nGr=n($oe);MFo=r(nGr,"electra"),nGr.forEach(t),EFo=r(v3e," \u2014 "),u$=s(v3e,"A",{href:!0});var lGr=n(u$);yFo=r(lGr,"ElectraForSequenceClassification"),lGr.forEach(t),wFo=r(v3e," (ELECTRA model)"),v3e.forEach(t),AFo=i(L),Pb=s(L,"LI",{});var T3e=n(Pb);Ioe=s(T3e,"STRONG",{});var iGr=n(Ioe);LFo=r(iGr,"flaubert"),iGr.forEach(t),BFo=r(T3e," \u2014 "),p$=s(T3e,"A",{href:!0});var dGr=n(p$);kFo=r(dGr,"FlaubertForSequenceClassification"),dGr.forEach(t),xFo=r(T3e," (FlauBERT model)"),T3e.forEach(t),RFo=i(L),$b=s(L,"LI",{});var F3e=n($b);joe=s(F3e,"STRONG",{});var cGr=n(joe);SFo=r(cGr,"fnet"),cGr.forEach(t),PFo=r(F3e," \u2014 "),_$=s(F3e,"A",{href:!0});var mGr=n(_$);$Fo=r(mGr,"FNetForSequenceClassification"),mGr.forEach(t),IFo=r(F3e," (FNet model)"),F3e.forEach(t),jFo=i(L),Ib=s(L,"LI",{});var C3e=n(Ib);Noe=s(C3e,"STRONG",{});var fGr=n(Noe);NFo=r(fGr,"funnel"),fGr.forEach(t),DFo=r(C3e," \u2014 "),b$=s(C3e,"A",{href:!0});var gGr=n(b$);qFo=r(gGr,"FunnelForSequenceClassification"),gGr.forEach(t),GFo=r(C3e," (Funnel Transformer model)"),C3e.forEach(t),OFo=i(L),jb=s(L,"LI",{});var M3e=n(jb);Doe=s(M3e,"STRONG",{});var hGr=n(Doe);XFo=r(hGr,"gpt2"),hGr.forEach(t),zFo=r(M3e," \u2014 "),v$=s(M3e,"A",{href:!0});var uGr=n(v$);VFo=r(uGr,"GPT2ForSequenceClassification"),uGr.forEach(t),WFo=r(M3e," (OpenAI GPT-2 model)"),M3e.forEach(t),QFo=i(L),Nb=s(L,"LI",{});var E3e=n(Nb);qoe=s(E3e,"STRONG",{});var pGr=n(qoe);HFo=r(pGr,"gpt_neo"),pGr.forEach(t),UFo=r(E3e," \u2014 "),T$=s(E3e,"A",{href:!0});var _Gr=n(T$);JFo=r(_Gr,"GPTNeoForSequenceClassification"),_Gr.forEach(t),YFo=r(E3e," (GPT Neo model)"),E3e.forEach(t),KFo=i(L),Db=s(L,"LI",{});var y3e=n(Db);Goe=s(y3e,"STRONG",{});var bGr=n(Goe);ZFo=r(bGr,"gptj"),bGr.forEach(t),eCo=r(y3e," \u2014 "),F$=s(y3e,"A",{href:!0});var vGr=n(F$);oCo=r(vGr,"GPTJForSequenceClassification"),vGr.forEach(t),rCo=r(y3e," (GPT-J model)"),y3e.forEach(t),tCo=i(L),qb=s(L,"LI",{});var w3e=n(qb);Ooe=s(w3e,"STRONG",{});var TGr=n(Ooe);aCo=r(TGr,"ibert"),TGr.forEach(t),sCo=r(w3e," \u2014 "),C$=s(w3e,"A",{href:!0});var FGr=n(C$);nCo=r(FGr,"IBertForSequenceClassification"),FGr.forEach(t),lCo=r(w3e," (I-BERT model)"),w3e.forEach(t),iCo=i(L),Gb=s(L,"LI",{});var A3e=n(Gb);Xoe=s(A3e,"STRONG",{});var CGr=n(Xoe);dCo=r(CGr,"layoutlm"),CGr.forEach(t),cCo=r(A3e," \u2014 "),M$=s(A3e,"A",{href:!0});var MGr=n(M$);mCo=r(MGr,"LayoutLMForSequenceClassification"),MGr.forEach(t),fCo=r(A3e," (LayoutLM model)"),A3e.forEach(t),gCo=i(L),Ob=s(L,"LI",{});var L3e=n(Ob);zoe=s(L3e,"STRONG",{});var EGr=n(zoe);hCo=r(EGr,"layoutlmv2"),EGr.forEach(t),uCo=r(L3e," \u2014 "),E$=s(L3e,"A",{href:!0});var yGr=n(E$);pCo=r(yGr,"LayoutLMv2ForSequenceClassification"),yGr.forEach(t),_Co=r(L3e," (LayoutLMv2 model)"),L3e.forEach(t),bCo=i(L),Xb=s(L,"LI",{});var B3e=n(Xb);Voe=s(B3e,"STRONG",{});var wGr=n(Voe);vCo=r(wGr,"led"),wGr.forEach(t),TCo=r(B3e," \u2014 "),y$=s(B3e,"A",{href:!0});var AGr=n(y$);FCo=r(AGr,"LEDForSequenceClassification"),AGr.forEach(t),CCo=r(B3e," (LED model)"),B3e.forEach(t),MCo=i(L),zb=s(L,"LI",{});var k3e=n(zb);Woe=s(k3e,"STRONG",{});var LGr=n(Woe);ECo=r(LGr,"longformer"),LGr.forEach(t),yCo=r(k3e," \u2014 "),w$=s(k3e,"A",{href:!0});var BGr=n(w$);wCo=r(BGr,"LongformerForSequenceClassification"),BGr.forEach(t),ACo=r(k3e," (Longformer model)"),k3e.forEach(t),LCo=i(L),Vb=s(L,"LI",{});var x3e=n(Vb);Qoe=s(x3e,"STRONG",{});var kGr=n(Qoe);BCo=r(kGr,"mbart"),kGr.forEach(t),kCo=r(x3e," \u2014 "),A$=s(x3e,"A",{href:!0});var xGr=n(A$);xCo=r(xGr,"MBartForSequenceClassification"),xGr.forEach(t),RCo=r(x3e," (mBART model)"),x3e.forEach(t),SCo=i(L),Wb=s(L,"LI",{});var R3e=n(Wb);Hoe=s(R3e,"STRONG",{});var RGr=n(Hoe);PCo=r(RGr,"megatron-bert"),RGr.forEach(t),$Co=r(R3e," \u2014 "),L$=s(R3e,"A",{href:!0});var SGr=n(L$);ICo=r(SGr,"MegatronBertForSequenceClassification"),SGr.forEach(t),jCo=r(R3e," (MegatronBert model)"),R3e.forEach(t),NCo=i(L),Qb=s(L,"LI",{});var S3e=n(Qb);Uoe=s(S3e,"STRONG",{});var PGr=n(Uoe);DCo=r(PGr,"mobilebert"),PGr.forEach(t),qCo=r(S3e," \u2014 "),B$=s(S3e,"A",{href:!0});var $Gr=n(B$);GCo=r($Gr,"MobileBertForSequenceClassification"),$Gr.forEach(t),OCo=r(S3e," (MobileBERT model)"),S3e.forEach(t),XCo=i(L),Hb=s(L,"LI",{});var P3e=n(Hb);Joe=s(P3e,"STRONG",{});var IGr=n(Joe);zCo=r(IGr,"mpnet"),IGr.forEach(t),VCo=r(P3e," \u2014 "),k$=s(P3e,"A",{href:!0});var jGr=n(k$);WCo=r(jGr,"MPNetForSequenceClassification"),jGr.forEach(t),QCo=r(P3e," (MPNet model)"),P3e.forEach(t),HCo=i(L),Ub=s(L,"LI",{});var $3e=n(Ub);Yoe=s($3e,"STRONG",{});var NGr=n(Yoe);UCo=r(NGr,"nystromformer"),NGr.forEach(t),JCo=r($3e," \u2014 "),x$=s($3e,"A",{href:!0});var DGr=n(x$);YCo=r(DGr,"NystromformerForSequenceClassification"),DGr.forEach(t),KCo=r($3e," (Nystromformer model)"),$3e.forEach(t),ZCo=i(L),Jb=s(L,"LI",{});var I3e=n(Jb);Koe=s(I3e,"STRONG",{});var qGr=n(Koe);e4o=r(qGr,"openai-gpt"),qGr.forEach(t),o4o=r(I3e," \u2014 "),R$=s(I3e,"A",{href:!0});var GGr=n(R$);r4o=r(GGr,"OpenAIGPTForSequenceClassification"),GGr.forEach(t),t4o=r(I3e," (OpenAI GPT model)"),I3e.forEach(t),a4o=i(L),Yb=s(L,"LI",{});var j3e=n(Yb);Zoe=s(j3e,"STRONG",{});var OGr=n(Zoe);s4o=r(OGr,"perceiver"),OGr.forEach(t),n4o=r(j3e," \u2014 "),S$=s(j3e,"A",{href:!0});var XGr=n(S$);l4o=r(XGr,"PerceiverForSequenceClassification"),XGr.forEach(t),i4o=r(j3e," (Perceiver model)"),j3e.forEach(t),d4o=i(L),Kb=s(L,"LI",{});var N3e=n(Kb);ere=s(N3e,"STRONG",{});var zGr=n(ere);c4o=r(zGr,"plbart"),zGr.forEach(t),m4o=r(N3e," \u2014 "),P$=s(N3e,"A",{href:!0});var VGr=n(P$);f4o=r(VGr,"PLBartForSequenceClassification"),VGr.forEach(t),g4o=r(N3e," (PLBart model)"),N3e.forEach(t),h4o=i(L),Zb=s(L,"LI",{});var D3e=n(Zb);ore=s(D3e,"STRONG",{});var WGr=n(ore);u4o=r(WGr,"qdqbert"),WGr.forEach(t),p4o=r(D3e," \u2014 "),$$=s(D3e,"A",{href:!0});var QGr=n($$);_4o=r(QGr,"QDQBertForSequenceClassification"),QGr.forEach(t),b4o=r(D3e," (QDQBert model)"),D3e.forEach(t),v4o=i(L),e2=s(L,"LI",{});var q3e=n(e2);rre=s(q3e,"STRONG",{});var HGr=n(rre);T4o=r(HGr,"reformer"),HGr.forEach(t),F4o=r(q3e," \u2014 "),I$=s(q3e,"A",{href:!0});var UGr=n(I$);C4o=r(UGr,"ReformerForSequenceClassification"),UGr.forEach(t),M4o=r(q3e," (Reformer model)"),q3e.forEach(t),E4o=i(L),o2=s(L,"LI",{});var G3e=n(o2);tre=s(G3e,"STRONG",{});var JGr=n(tre);y4o=r(JGr,"rembert"),JGr.forEach(t),w4o=r(G3e," \u2014 "),j$=s(G3e,"A",{href:!0});var YGr=n(j$);A4o=r(YGr,"RemBertForSequenceClassification"),YGr.forEach(t),L4o=r(G3e," (RemBERT model)"),G3e.forEach(t),B4o=i(L),r2=s(L,"LI",{});var O3e=n(r2);are=s(O3e,"STRONG",{});var KGr=n(are);k4o=r(KGr,"roberta"),KGr.forEach(t),x4o=r(O3e," \u2014 "),N$=s(O3e,"A",{href:!0});var ZGr=n(N$);R4o=r(ZGr,"RobertaForSequenceClassification"),ZGr.forEach(t),S4o=r(O3e," (RoBERTa model)"),O3e.forEach(t),P4o=i(L),t2=s(L,"LI",{});var X3e=n(t2);sre=s(X3e,"STRONG",{});var eOr=n(sre);$4o=r(eOr,"roformer"),eOr.forEach(t),I4o=r(X3e," \u2014 "),D$=s(X3e,"A",{href:!0});var oOr=n(D$);j4o=r(oOr,"RoFormerForSequenceClassification"),oOr.forEach(t),N4o=r(X3e," (RoFormer model)"),X3e.forEach(t),D4o=i(L),a2=s(L,"LI",{});var z3e=n(a2);nre=s(z3e,"STRONG",{});var rOr=n(nre);q4o=r(rOr,"squeezebert"),rOr.forEach(t),G4o=r(z3e," \u2014 "),q$=s(z3e,"A",{href:!0});var tOr=n(q$);O4o=r(tOr,"SqueezeBertForSequenceClassification"),tOr.forEach(t),X4o=r(z3e," (SqueezeBERT model)"),z3e.forEach(t),z4o=i(L),s2=s(L,"LI",{});var V3e=n(s2);lre=s(V3e,"STRONG",{});var aOr=n(lre);V4o=r(aOr,"tapas"),aOr.forEach(t),W4o=r(V3e," \u2014 "),G$=s(V3e,"A",{href:!0});var sOr=n(G$);Q4o=r(sOr,"TapasForSequenceClassification"),sOr.forEach(t),H4o=r(V3e," (TAPAS model)"),V3e.forEach(t),U4o=i(L),n2=s(L,"LI",{});var W3e=n(n2);ire=s(W3e,"STRONG",{});var nOr=n(ire);J4o=r(nOr,"transfo-xl"),nOr.forEach(t),Y4o=r(W3e," \u2014 "),O$=s(W3e,"A",{href:!0});var lOr=n(O$);K4o=r(lOr,"TransfoXLForSequenceClassification"),lOr.forEach(t),Z4o=r(W3e," (Transformer-XL model)"),W3e.forEach(t),eMo=i(L),l2=s(L,"LI",{});var Q3e=n(l2);dre=s(Q3e,"STRONG",{});var iOr=n(dre);oMo=r(iOr,"xlm"),iOr.forEach(t),rMo=r(Q3e," \u2014 "),X$=s(Q3e,"A",{href:!0});var dOr=n(X$);tMo=r(dOr,"XLMForSequenceClassification"),dOr.forEach(t),aMo=r(Q3e," (XLM model)"),Q3e.forEach(t),sMo=i(L),i2=s(L,"LI",{});var H3e=n(i2);cre=s(H3e,"STRONG",{});var cOr=n(cre);nMo=r(cOr,"xlm-roberta"),cOr.forEach(t),lMo=r(H3e," \u2014 "),z$=s(H3e,"A",{href:!0});var mOr=n(z$);iMo=r(mOr,"XLMRobertaForSequenceClassification"),mOr.forEach(t),dMo=r(H3e," (XLM-RoBERTa model)"),H3e.forEach(t),cMo=i(L),d2=s(L,"LI",{});var U3e=n(d2);mre=s(U3e,"STRONG",{});var fOr=n(mre);mMo=r(fOr,"xlm-roberta-xl"),fOr.forEach(t),fMo=r(U3e," \u2014 "),V$=s(U3e,"A",{href:!0});var gOr=n(V$);gMo=r(gOr,"XLMRobertaXLForSequenceClassification"),gOr.forEach(t),hMo=r(U3e," (XLM-RoBERTa-XL model)"),U3e.forEach(t),uMo=i(L),c2=s(L,"LI",{});var J3e=n(c2);fre=s(J3e,"STRONG",{});var hOr=n(fre);pMo=r(hOr,"xlnet"),hOr.forEach(t),_Mo=r(J3e," \u2014 "),W$=s(J3e,"A",{href:!0});var uOr=n(W$);bMo=r(uOr,"XLNetForSequenceClassification"),uOr.forEach(t),vMo=r(J3e," (XLNet model)"),J3e.forEach(t),TMo=i(L),m2=s(L,"LI",{});var Y3e=n(m2);gre=s(Y3e,"STRONG",{});var pOr=n(gre);FMo=r(pOr,"yoso"),pOr.forEach(t),CMo=r(Y3e," \u2014 "),Q$=s(Y3e,"A",{href:!0});var _Or=n(Q$);MMo=r(_Or,"YosoForSequenceClassification"),_Or.forEach(t),EMo=r(Y3e," (YOSO model)"),Y3e.forEach(t),L.forEach(t),yMo=i(Nt),f2=s(Nt,"P",{});var K3e=n(f2);wMo=r(K3e,"The model is set in evaluation mode by default using "),hre=s(K3e,"CODE",{});var bOr=n(hre);AMo=r(bOr,"model.eval()"),bOr.forEach(t),LMo=r(K3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ure=s(K3e,"CODE",{});var vOr=n(ure);BMo=r(vOr,"model.train()"),vOr.forEach(t),K3e.forEach(t),kMo=i(Nt),pre=s(Nt,"P",{});var TOr=n(pre);xMo=r(TOr,"Examples:"),TOr.forEach(t),RMo=i(Nt),f(Cy.$$.fragment,Nt),Nt.forEach(t),Hn.forEach(t),L8e=i(d),ad=s(d,"H2",{class:!0});var $Be=n(ad);g2=s($Be,"A",{id:!0,class:!0,href:!0});var FOr=n(g2);_re=s(FOr,"SPAN",{});var COr=n(_re);f(My.$$.fragment,COr),COr.forEach(t),FOr.forEach(t),SMo=i($Be),bre=s($Be,"SPAN",{});var MOr=n(bre);PMo=r(MOr,"AutoModelForMultipleChoice"),MOr.forEach(t),$Be.forEach(t),B8e=i(d),Yo=s(d,"DIV",{class:!0});var Jn=n(Yo);f(Ey.$$.fragment,Jn),$Mo=i(Jn),sd=s(Jn,"P",{});var az=n(sd);IMo=r(az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),vre=s(az,"CODE",{});var EOr=n(vre);jMo=r(EOr,"from_pretrained()"),EOr.forEach(t),NMo=r(az,"class method or the "),Tre=s(az,"CODE",{});var yOr=n(Tre);DMo=r(yOr,"from_config()"),yOr.forEach(t),qMo=r(az,`class
method.`),az.forEach(t),GMo=i(Jn),yy=s(Jn,"P",{});var IBe=n(yy);OMo=r(IBe,"This class cannot be instantiated directly using "),Fre=s(IBe,"CODE",{});var wOr=n(Fre);XMo=r(wOr,"__init__()"),wOr.forEach(t),zMo=r(IBe," (throws an error)."),IBe.forEach(t),VMo=i(Jn),zr=s(Jn,"DIV",{class:!0});var Yn=n(zr);f(wy.$$.fragment,Yn),WMo=i(Yn),Cre=s(Yn,"P",{});var AOr=n(Cre);QMo=r(AOr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),AOr.forEach(t),HMo=i(Yn),nd=s(Yn,"P",{});var sz=n(nd);UMo=r(sz,`Note:
Loading a model from its configuration file does `),Mre=s(sz,"STRONG",{});var LOr=n(Mre);JMo=r(LOr,"not"),LOr.forEach(t),YMo=r(sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ere=s(sz,"CODE",{});var BOr=n(Ere);KMo=r(BOr,"from_pretrained()"),BOr.forEach(t),ZMo=r(sz,"to load the model weights."),sz.forEach(t),eEo=i(Yn),yre=s(Yn,"P",{});var kOr=n(yre);oEo=r(kOr,"Examples:"),kOr.forEach(t),rEo=i(Yn),f(Ay.$$.fragment,Yn),Yn.forEach(t),tEo=i(Jn),Ie=s(Jn,"DIV",{class:!0});var Dt=n(Ie);f(Ly.$$.fragment,Dt),aEo=i(Dt),wre=s(Dt,"P",{});var xOr=n(wre);sEo=r(xOr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),xOr.forEach(t),nEo=i(Dt),Va=s(Dt,"P",{});var g3=n(Va);lEo=r(g3,"The model class to instantiate is selected based on the "),Are=s(g3,"CODE",{});var ROr=n(Are);iEo=r(ROr,"model_type"),ROr.forEach(t),dEo=r(g3,` property of the config object (either
passed as an argument or loaded from `),Lre=s(g3,"CODE",{});var SOr=n(Lre);cEo=r(SOr,"pretrained_model_name_or_path"),SOr.forEach(t),mEo=r(g3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bre=s(g3,"CODE",{});var POr=n(Bre);fEo=r(POr,"pretrained_model_name_or_path"),POr.forEach(t),gEo=r(g3,":"),g3.forEach(t),hEo=i(Dt),G=s(Dt,"UL",{});var O=n(G);h2=s(O,"LI",{});var Z3e=n(h2);kre=s(Z3e,"STRONG",{});var $Or=n(kre);uEo=r($Or,"albert"),$Or.forEach(t),pEo=r(Z3e," \u2014 "),H$=s(Z3e,"A",{href:!0});var IOr=n(H$);_Eo=r(IOr,"AlbertForMultipleChoice"),IOr.forEach(t),bEo=r(Z3e," (ALBERT model)"),Z3e.forEach(t),vEo=i(O),u2=s(O,"LI",{});var e5e=n(u2);xre=s(e5e,"STRONG",{});var jOr=n(xre);TEo=r(jOr,"bert"),jOr.forEach(t),FEo=r(e5e," \u2014 "),U$=s(e5e,"A",{href:!0});var NOr=n(U$);CEo=r(NOr,"BertForMultipleChoice"),NOr.forEach(t),MEo=r(e5e," (BERT model)"),e5e.forEach(t),EEo=i(O),p2=s(O,"LI",{});var o5e=n(p2);Rre=s(o5e,"STRONG",{});var DOr=n(Rre);yEo=r(DOr,"big_bird"),DOr.forEach(t),wEo=r(o5e," \u2014 "),J$=s(o5e,"A",{href:!0});var qOr=n(J$);AEo=r(qOr,"BigBirdForMultipleChoice"),qOr.forEach(t),LEo=r(o5e," (BigBird model)"),o5e.forEach(t),BEo=i(O),_2=s(O,"LI",{});var r5e=n(_2);Sre=s(r5e,"STRONG",{});var GOr=n(Sre);kEo=r(GOr,"camembert"),GOr.forEach(t),xEo=r(r5e," \u2014 "),Y$=s(r5e,"A",{href:!0});var OOr=n(Y$);REo=r(OOr,"CamembertForMultipleChoice"),OOr.forEach(t),SEo=r(r5e," (CamemBERT model)"),r5e.forEach(t),PEo=i(O),b2=s(O,"LI",{});var t5e=n(b2);Pre=s(t5e,"STRONG",{});var XOr=n(Pre);$Eo=r(XOr,"canine"),XOr.forEach(t),IEo=r(t5e," \u2014 "),K$=s(t5e,"A",{href:!0});var zOr=n(K$);jEo=r(zOr,"CanineForMultipleChoice"),zOr.forEach(t),NEo=r(t5e," (Canine model)"),t5e.forEach(t),DEo=i(O),v2=s(O,"LI",{});var a5e=n(v2);$re=s(a5e,"STRONG",{});var VOr=n($re);qEo=r(VOr,"convbert"),VOr.forEach(t),GEo=r(a5e," \u2014 "),Z$=s(a5e,"A",{href:!0});var WOr=n(Z$);OEo=r(WOr,"ConvBertForMultipleChoice"),WOr.forEach(t),XEo=r(a5e," (ConvBERT model)"),a5e.forEach(t),zEo=i(O),T2=s(O,"LI",{});var s5e=n(T2);Ire=s(s5e,"STRONG",{});var QOr=n(Ire);VEo=r(QOr,"distilbert"),QOr.forEach(t),WEo=r(s5e," \u2014 "),eI=s(s5e,"A",{href:!0});var HOr=n(eI);QEo=r(HOr,"DistilBertForMultipleChoice"),HOr.forEach(t),HEo=r(s5e," (DistilBERT model)"),s5e.forEach(t),UEo=i(O),F2=s(O,"LI",{});var n5e=n(F2);jre=s(n5e,"STRONG",{});var UOr=n(jre);JEo=r(UOr,"electra"),UOr.forEach(t),YEo=r(n5e," \u2014 "),oI=s(n5e,"A",{href:!0});var JOr=n(oI);KEo=r(JOr,"ElectraForMultipleChoice"),JOr.forEach(t),ZEo=r(n5e," (ELECTRA model)"),n5e.forEach(t),e3o=i(O),C2=s(O,"LI",{});var l5e=n(C2);Nre=s(l5e,"STRONG",{});var YOr=n(Nre);o3o=r(YOr,"flaubert"),YOr.forEach(t),r3o=r(l5e," \u2014 "),rI=s(l5e,"A",{href:!0});var KOr=n(rI);t3o=r(KOr,"FlaubertForMultipleChoice"),KOr.forEach(t),a3o=r(l5e," (FlauBERT model)"),l5e.forEach(t),s3o=i(O),M2=s(O,"LI",{});var i5e=n(M2);Dre=s(i5e,"STRONG",{});var ZOr=n(Dre);n3o=r(ZOr,"fnet"),ZOr.forEach(t),l3o=r(i5e," \u2014 "),tI=s(i5e,"A",{href:!0});var eXr=n(tI);i3o=r(eXr,"FNetForMultipleChoice"),eXr.forEach(t),d3o=r(i5e," (FNet model)"),i5e.forEach(t),c3o=i(O),E2=s(O,"LI",{});var d5e=n(E2);qre=s(d5e,"STRONG",{});var oXr=n(qre);m3o=r(oXr,"funnel"),oXr.forEach(t),f3o=r(d5e," \u2014 "),aI=s(d5e,"A",{href:!0});var rXr=n(aI);g3o=r(rXr,"FunnelForMultipleChoice"),rXr.forEach(t),h3o=r(d5e," (Funnel Transformer model)"),d5e.forEach(t),u3o=i(O),y2=s(O,"LI",{});var c5e=n(y2);Gre=s(c5e,"STRONG",{});var tXr=n(Gre);p3o=r(tXr,"ibert"),tXr.forEach(t),_3o=r(c5e," \u2014 "),sI=s(c5e,"A",{href:!0});var aXr=n(sI);b3o=r(aXr,"IBertForMultipleChoice"),aXr.forEach(t),v3o=r(c5e," (I-BERT model)"),c5e.forEach(t),T3o=i(O),w2=s(O,"LI",{});var m5e=n(w2);Ore=s(m5e,"STRONG",{});var sXr=n(Ore);F3o=r(sXr,"longformer"),sXr.forEach(t),C3o=r(m5e," \u2014 "),nI=s(m5e,"A",{href:!0});var nXr=n(nI);M3o=r(nXr,"LongformerForMultipleChoice"),nXr.forEach(t),E3o=r(m5e," (Longformer model)"),m5e.forEach(t),y3o=i(O),A2=s(O,"LI",{});var f5e=n(A2);Xre=s(f5e,"STRONG",{});var lXr=n(Xre);w3o=r(lXr,"megatron-bert"),lXr.forEach(t),A3o=r(f5e," \u2014 "),lI=s(f5e,"A",{href:!0});var iXr=n(lI);L3o=r(iXr,"MegatronBertForMultipleChoice"),iXr.forEach(t),B3o=r(f5e," (MegatronBert model)"),f5e.forEach(t),k3o=i(O),L2=s(O,"LI",{});var g5e=n(L2);zre=s(g5e,"STRONG",{});var dXr=n(zre);x3o=r(dXr,"mobilebert"),dXr.forEach(t),R3o=r(g5e," \u2014 "),iI=s(g5e,"A",{href:!0});var cXr=n(iI);S3o=r(cXr,"MobileBertForMultipleChoice"),cXr.forEach(t),P3o=r(g5e," (MobileBERT model)"),g5e.forEach(t),$3o=i(O),B2=s(O,"LI",{});var h5e=n(B2);Vre=s(h5e,"STRONG",{});var mXr=n(Vre);I3o=r(mXr,"mpnet"),mXr.forEach(t),j3o=r(h5e," \u2014 "),dI=s(h5e,"A",{href:!0});var fXr=n(dI);N3o=r(fXr,"MPNetForMultipleChoice"),fXr.forEach(t),D3o=r(h5e," (MPNet model)"),h5e.forEach(t),q3o=i(O),k2=s(O,"LI",{});var u5e=n(k2);Wre=s(u5e,"STRONG",{});var gXr=n(Wre);G3o=r(gXr,"nystromformer"),gXr.forEach(t),O3o=r(u5e," \u2014 "),cI=s(u5e,"A",{href:!0});var hXr=n(cI);X3o=r(hXr,"NystromformerForMultipleChoice"),hXr.forEach(t),z3o=r(u5e," (Nystromformer model)"),u5e.forEach(t),V3o=i(O),x2=s(O,"LI",{});var p5e=n(x2);Qre=s(p5e,"STRONG",{});var uXr=n(Qre);W3o=r(uXr,"qdqbert"),uXr.forEach(t),Q3o=r(p5e," \u2014 "),mI=s(p5e,"A",{href:!0});var pXr=n(mI);H3o=r(pXr,"QDQBertForMultipleChoice"),pXr.forEach(t),U3o=r(p5e," (QDQBert model)"),p5e.forEach(t),J3o=i(O),R2=s(O,"LI",{});var _5e=n(R2);Hre=s(_5e,"STRONG",{});var _Xr=n(Hre);Y3o=r(_Xr,"rembert"),_Xr.forEach(t),K3o=r(_5e," \u2014 "),fI=s(_5e,"A",{href:!0});var bXr=n(fI);Z3o=r(bXr,"RemBertForMultipleChoice"),bXr.forEach(t),e5o=r(_5e," (RemBERT model)"),_5e.forEach(t),o5o=i(O),S2=s(O,"LI",{});var b5e=n(S2);Ure=s(b5e,"STRONG",{});var vXr=n(Ure);r5o=r(vXr,"roberta"),vXr.forEach(t),t5o=r(b5e," \u2014 "),gI=s(b5e,"A",{href:!0});var TXr=n(gI);a5o=r(TXr,"RobertaForMultipleChoice"),TXr.forEach(t),s5o=r(b5e," (RoBERTa model)"),b5e.forEach(t),n5o=i(O),P2=s(O,"LI",{});var v5e=n(P2);Jre=s(v5e,"STRONG",{});var FXr=n(Jre);l5o=r(FXr,"roformer"),FXr.forEach(t),i5o=r(v5e," \u2014 "),hI=s(v5e,"A",{href:!0});var CXr=n(hI);d5o=r(CXr,"RoFormerForMultipleChoice"),CXr.forEach(t),c5o=r(v5e," (RoFormer model)"),v5e.forEach(t),m5o=i(O),$2=s(O,"LI",{});var T5e=n($2);Yre=s(T5e,"STRONG",{});var MXr=n(Yre);f5o=r(MXr,"squeezebert"),MXr.forEach(t),g5o=r(T5e," \u2014 "),uI=s(T5e,"A",{href:!0});var EXr=n(uI);h5o=r(EXr,"SqueezeBertForMultipleChoice"),EXr.forEach(t),u5o=r(T5e," (SqueezeBERT model)"),T5e.forEach(t),p5o=i(O),I2=s(O,"LI",{});var F5e=n(I2);Kre=s(F5e,"STRONG",{});var yXr=n(Kre);_5o=r(yXr,"xlm"),yXr.forEach(t),b5o=r(F5e," \u2014 "),pI=s(F5e,"A",{href:!0});var wXr=n(pI);v5o=r(wXr,"XLMForMultipleChoice"),wXr.forEach(t),T5o=r(F5e," (XLM model)"),F5e.forEach(t),F5o=i(O),j2=s(O,"LI",{});var C5e=n(j2);Zre=s(C5e,"STRONG",{});var AXr=n(Zre);C5o=r(AXr,"xlm-roberta"),AXr.forEach(t),M5o=r(C5e," \u2014 "),_I=s(C5e,"A",{href:!0});var LXr=n(_I);E5o=r(LXr,"XLMRobertaForMultipleChoice"),LXr.forEach(t),y5o=r(C5e," (XLM-RoBERTa model)"),C5e.forEach(t),w5o=i(O),N2=s(O,"LI",{});var M5e=n(N2);ete=s(M5e,"STRONG",{});var BXr=n(ete);A5o=r(BXr,"xlm-roberta-xl"),BXr.forEach(t),L5o=r(M5e," \u2014 "),bI=s(M5e,"A",{href:!0});var kXr=n(bI);B5o=r(kXr,"XLMRobertaXLForMultipleChoice"),kXr.forEach(t),k5o=r(M5e," (XLM-RoBERTa-XL model)"),M5e.forEach(t),x5o=i(O),D2=s(O,"LI",{});var E5e=n(D2);ote=s(E5e,"STRONG",{});var xXr=n(ote);R5o=r(xXr,"xlnet"),xXr.forEach(t),S5o=r(E5e," \u2014 "),vI=s(E5e,"A",{href:!0});var RXr=n(vI);P5o=r(RXr,"XLNetForMultipleChoice"),RXr.forEach(t),$5o=r(E5e," (XLNet model)"),E5e.forEach(t),I5o=i(O),q2=s(O,"LI",{});var y5e=n(q2);rte=s(y5e,"STRONG",{});var SXr=n(rte);j5o=r(SXr,"yoso"),SXr.forEach(t),N5o=r(y5e," \u2014 "),TI=s(y5e,"A",{href:!0});var PXr=n(TI);D5o=r(PXr,"YosoForMultipleChoice"),PXr.forEach(t),q5o=r(y5e," (YOSO model)"),y5e.forEach(t),O.forEach(t),G5o=i(Dt),G2=s(Dt,"P",{});var w5e=n(G2);O5o=r(w5e,"The model is set in evaluation mode by default using "),tte=s(w5e,"CODE",{});var $Xr=n(tte);X5o=r($Xr,"model.eval()"),$Xr.forEach(t),z5o=r(w5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=s(w5e,"CODE",{});var IXr=n(ate);V5o=r(IXr,"model.train()"),IXr.forEach(t),w5e.forEach(t),W5o=i(Dt),ste=s(Dt,"P",{});var jXr=n(ste);Q5o=r(jXr,"Examples:"),jXr.forEach(t),H5o=i(Dt),f(By.$$.fragment,Dt),Dt.forEach(t),Jn.forEach(t),k8e=i(d),ld=s(d,"H2",{class:!0});var jBe=n(ld);O2=s(jBe,"A",{id:!0,class:!0,href:!0});var NXr=n(O2);nte=s(NXr,"SPAN",{});var DXr=n(nte);f(ky.$$.fragment,DXr),DXr.forEach(t),NXr.forEach(t),U5o=i(jBe),lte=s(jBe,"SPAN",{});var qXr=n(lte);J5o=r(qXr,"AutoModelForNextSentencePrediction"),qXr.forEach(t),jBe.forEach(t),x8e=i(d),Ko=s(d,"DIV",{class:!0});var Kn=n(Ko);f(xy.$$.fragment,Kn),Y5o=i(Kn),id=s(Kn,"P",{});var nz=n(id);K5o=r(nz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),ite=s(nz,"CODE",{});var GXr=n(ite);Z5o=r(GXr,"from_pretrained()"),GXr.forEach(t),eyo=r(nz,"class method or the "),dte=s(nz,"CODE",{});var OXr=n(dte);oyo=r(OXr,"from_config()"),OXr.forEach(t),ryo=r(nz,`class
method.`),nz.forEach(t),tyo=i(Kn),Ry=s(Kn,"P",{});var NBe=n(Ry);ayo=r(NBe,"This class cannot be instantiated directly using "),cte=s(NBe,"CODE",{});var XXr=n(cte);syo=r(XXr,"__init__()"),XXr.forEach(t),nyo=r(NBe," (throws an error)."),NBe.forEach(t),lyo=i(Kn),Vr=s(Kn,"DIV",{class:!0});var Zn=n(Vr);f(Sy.$$.fragment,Zn),iyo=i(Zn),mte=s(Zn,"P",{});var zXr=n(mte);dyo=r(zXr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),zXr.forEach(t),cyo=i(Zn),dd=s(Zn,"P",{});var lz=n(dd);myo=r(lz,`Note:
Loading a model from its configuration file does `),fte=s(lz,"STRONG",{});var VXr=n(fte);fyo=r(VXr,"not"),VXr.forEach(t),gyo=r(lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=s(lz,"CODE",{});var WXr=n(gte);hyo=r(WXr,"from_pretrained()"),WXr.forEach(t),uyo=r(lz,"to load the model weights."),lz.forEach(t),pyo=i(Zn),hte=s(Zn,"P",{});var QXr=n(hte);_yo=r(QXr,"Examples:"),QXr.forEach(t),byo=i(Zn),f(Py.$$.fragment,Zn),Zn.forEach(t),vyo=i(Kn),je=s(Kn,"DIV",{class:!0});var qt=n(je);f($y.$$.fragment,qt),Tyo=i(qt),ute=s(qt,"P",{});var HXr=n(ute);Fyo=r(HXr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),HXr.forEach(t),Cyo=i(qt),Wa=s(qt,"P",{});var h3=n(Wa);Myo=r(h3,"The model class to instantiate is selected based on the "),pte=s(h3,"CODE",{});var UXr=n(pte);Eyo=r(UXr,"model_type"),UXr.forEach(t),yyo=r(h3,` property of the config object (either
passed as an argument or loaded from `),_te=s(h3,"CODE",{});var JXr=n(_te);wyo=r(JXr,"pretrained_model_name_or_path"),JXr.forEach(t),Ayo=r(h3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=s(h3,"CODE",{});var YXr=n(bte);Lyo=r(YXr,"pretrained_model_name_or_path"),YXr.forEach(t),Byo=r(h3,":"),h3.forEach(t),kyo=i(qt),sa=s(qt,"UL",{});var el=n(sa);X2=s(el,"LI",{});var A5e=n(X2);vte=s(A5e,"STRONG",{});var KXr=n(vte);xyo=r(KXr,"bert"),KXr.forEach(t),Ryo=r(A5e," \u2014 "),FI=s(A5e,"A",{href:!0});var ZXr=n(FI);Syo=r(ZXr,"BertForNextSentencePrediction"),ZXr.forEach(t),Pyo=r(A5e," (BERT model)"),A5e.forEach(t),$yo=i(el),z2=s(el,"LI",{});var L5e=n(z2);Tte=s(L5e,"STRONG",{});var ezr=n(Tte);Iyo=r(ezr,"fnet"),ezr.forEach(t),jyo=r(L5e," \u2014 "),CI=s(L5e,"A",{href:!0});var ozr=n(CI);Nyo=r(ozr,"FNetForNextSentencePrediction"),ozr.forEach(t),Dyo=r(L5e," (FNet model)"),L5e.forEach(t),qyo=i(el),V2=s(el,"LI",{});var B5e=n(V2);Fte=s(B5e,"STRONG",{});var rzr=n(Fte);Gyo=r(rzr,"megatron-bert"),rzr.forEach(t),Oyo=r(B5e," \u2014 "),MI=s(B5e,"A",{href:!0});var tzr=n(MI);Xyo=r(tzr,"MegatronBertForNextSentencePrediction"),tzr.forEach(t),zyo=r(B5e," (MegatronBert model)"),B5e.forEach(t),Vyo=i(el),W2=s(el,"LI",{});var k5e=n(W2);Cte=s(k5e,"STRONG",{});var azr=n(Cte);Wyo=r(azr,"mobilebert"),azr.forEach(t),Qyo=r(k5e," \u2014 "),EI=s(k5e,"A",{href:!0});var szr=n(EI);Hyo=r(szr,"MobileBertForNextSentencePrediction"),szr.forEach(t),Uyo=r(k5e," (MobileBERT model)"),k5e.forEach(t),Jyo=i(el),Q2=s(el,"LI",{});var x5e=n(Q2);Mte=s(x5e,"STRONG",{});var nzr=n(Mte);Yyo=r(nzr,"qdqbert"),nzr.forEach(t),Kyo=r(x5e," \u2014 "),yI=s(x5e,"A",{href:!0});var lzr=n(yI);Zyo=r(lzr,"QDQBertForNextSentencePrediction"),lzr.forEach(t),ewo=r(x5e," (QDQBert model)"),x5e.forEach(t),el.forEach(t),owo=i(qt),H2=s(qt,"P",{});var R5e=n(H2);rwo=r(R5e,"The model is set in evaluation mode by default using "),Ete=s(R5e,"CODE",{});var izr=n(Ete);two=r(izr,"model.eval()"),izr.forEach(t),awo=r(R5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),yte=s(R5e,"CODE",{});var dzr=n(yte);swo=r(dzr,"model.train()"),dzr.forEach(t),R5e.forEach(t),nwo=i(qt),wte=s(qt,"P",{});var czr=n(wte);lwo=r(czr,"Examples:"),czr.forEach(t),iwo=i(qt),f(Iy.$$.fragment,qt),qt.forEach(t),Kn.forEach(t),R8e=i(d),cd=s(d,"H2",{class:!0});var DBe=n(cd);U2=s(DBe,"A",{id:!0,class:!0,href:!0});var mzr=n(U2);Ate=s(mzr,"SPAN",{});var fzr=n(Ate);f(jy.$$.fragment,fzr),fzr.forEach(t),mzr.forEach(t),dwo=i(DBe),Lte=s(DBe,"SPAN",{});var gzr=n(Lte);cwo=r(gzr,"AutoModelForTokenClassification"),gzr.forEach(t),DBe.forEach(t),S8e=i(d),Zo=s(d,"DIV",{class:!0});var ol=n(Zo);f(Ny.$$.fragment,ol),mwo=i(ol),md=s(ol,"P",{});var iz=n(md);fwo=r(iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Bte=s(iz,"CODE",{});var hzr=n(Bte);gwo=r(hzr,"from_pretrained()"),hzr.forEach(t),hwo=r(iz,"class method or the "),kte=s(iz,"CODE",{});var uzr=n(kte);uwo=r(uzr,"from_config()"),uzr.forEach(t),pwo=r(iz,`class
method.`),iz.forEach(t),_wo=i(ol),Dy=s(ol,"P",{});var qBe=n(Dy);bwo=r(qBe,"This class cannot be instantiated directly using "),xte=s(qBe,"CODE",{});var pzr=n(xte);vwo=r(pzr,"__init__()"),pzr.forEach(t),Two=r(qBe," (throws an error)."),qBe.forEach(t),Fwo=i(ol),Wr=s(ol,"DIV",{class:!0});var rl=n(Wr);f(qy.$$.fragment,rl),Cwo=i(rl),Rte=s(rl,"P",{});var _zr=n(Rte);Mwo=r(_zr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),_zr.forEach(t),Ewo=i(rl),fd=s(rl,"P",{});var dz=n(fd);ywo=r(dz,`Note:
Loading a model from its configuration file does `),Ste=s(dz,"STRONG",{});var bzr=n(Ste);wwo=r(bzr,"not"),bzr.forEach(t),Awo=r(dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pte=s(dz,"CODE",{});var vzr=n(Pte);Lwo=r(vzr,"from_pretrained()"),vzr.forEach(t),Bwo=r(dz,"to load the model weights."),dz.forEach(t),kwo=i(rl),$te=s(rl,"P",{});var Tzr=n($te);xwo=r(Tzr,"Examples:"),Tzr.forEach(t),Rwo=i(rl),f(Gy.$$.fragment,rl),rl.forEach(t),Swo=i(ol),Ne=s(ol,"DIV",{class:!0});var Gt=n(Ne);f(Oy.$$.fragment,Gt),Pwo=i(Gt),Ite=s(Gt,"P",{});var Fzr=n(Ite);$wo=r(Fzr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Fzr.forEach(t),Iwo=i(Gt),Qa=s(Gt,"P",{});var u3=n(Qa);jwo=r(u3,"The model class to instantiate is selected based on the "),jte=s(u3,"CODE",{});var Czr=n(jte);Nwo=r(Czr,"model_type"),Czr.forEach(t),Dwo=r(u3,` property of the config object (either
passed as an argument or loaded from `),Nte=s(u3,"CODE",{});var Mzr=n(Nte);qwo=r(Mzr,"pretrained_model_name_or_path"),Mzr.forEach(t),Gwo=r(u3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Dte=s(u3,"CODE",{});var Ezr=n(Dte);Owo=r(Ezr,"pretrained_model_name_or_path"),Ezr.forEach(t),Xwo=r(u3,":"),u3.forEach(t),zwo=i(Gt),D=s(Gt,"UL",{});var q=n(D);J2=s(q,"LI",{});var S5e=n(J2);qte=s(S5e,"STRONG",{});var yzr=n(qte);Vwo=r(yzr,"albert"),yzr.forEach(t),Wwo=r(S5e," \u2014 "),wI=s(S5e,"A",{href:!0});var wzr=n(wI);Qwo=r(wzr,"AlbertForTokenClassification"),wzr.forEach(t),Hwo=r(S5e," (ALBERT model)"),S5e.forEach(t),Uwo=i(q),Y2=s(q,"LI",{});var P5e=n(Y2);Gte=s(P5e,"STRONG",{});var Azr=n(Gte);Jwo=r(Azr,"bert"),Azr.forEach(t),Ywo=r(P5e," \u2014 "),AI=s(P5e,"A",{href:!0});var Lzr=n(AI);Kwo=r(Lzr,"BertForTokenClassification"),Lzr.forEach(t),Zwo=r(P5e," (BERT model)"),P5e.forEach(t),eAo=i(q),K2=s(q,"LI",{});var $5e=n(K2);Ote=s($5e,"STRONG",{});var Bzr=n(Ote);oAo=r(Bzr,"big_bird"),Bzr.forEach(t),rAo=r($5e," \u2014 "),LI=s($5e,"A",{href:!0});var kzr=n(LI);tAo=r(kzr,"BigBirdForTokenClassification"),kzr.forEach(t),aAo=r($5e," (BigBird model)"),$5e.forEach(t),sAo=i(q),Z2=s(q,"LI",{});var I5e=n(Z2);Xte=s(I5e,"STRONG",{});var xzr=n(Xte);nAo=r(xzr,"camembert"),xzr.forEach(t),lAo=r(I5e," \u2014 "),BI=s(I5e,"A",{href:!0});var Rzr=n(BI);iAo=r(Rzr,"CamembertForTokenClassification"),Rzr.forEach(t),dAo=r(I5e," (CamemBERT model)"),I5e.forEach(t),cAo=i(q),ev=s(q,"LI",{});var j5e=n(ev);zte=s(j5e,"STRONG",{});var Szr=n(zte);mAo=r(Szr,"canine"),Szr.forEach(t),fAo=r(j5e," \u2014 "),kI=s(j5e,"A",{href:!0});var Pzr=n(kI);gAo=r(Pzr,"CanineForTokenClassification"),Pzr.forEach(t),hAo=r(j5e," (Canine model)"),j5e.forEach(t),uAo=i(q),ov=s(q,"LI",{});var N5e=n(ov);Vte=s(N5e,"STRONG",{});var $zr=n(Vte);pAo=r($zr,"convbert"),$zr.forEach(t),_Ao=r(N5e," \u2014 "),xI=s(N5e,"A",{href:!0});var Izr=n(xI);bAo=r(Izr,"ConvBertForTokenClassification"),Izr.forEach(t),vAo=r(N5e," (ConvBERT model)"),N5e.forEach(t),TAo=i(q),rv=s(q,"LI",{});var D5e=n(rv);Wte=s(D5e,"STRONG",{});var jzr=n(Wte);FAo=r(jzr,"deberta"),jzr.forEach(t),CAo=r(D5e," \u2014 "),RI=s(D5e,"A",{href:!0});var Nzr=n(RI);MAo=r(Nzr,"DebertaForTokenClassification"),Nzr.forEach(t),EAo=r(D5e," (DeBERTa model)"),D5e.forEach(t),yAo=i(q),tv=s(q,"LI",{});var q5e=n(tv);Qte=s(q5e,"STRONG",{});var Dzr=n(Qte);wAo=r(Dzr,"deberta-v2"),Dzr.forEach(t),AAo=r(q5e," \u2014 "),SI=s(q5e,"A",{href:!0});var qzr=n(SI);LAo=r(qzr,"DebertaV2ForTokenClassification"),qzr.forEach(t),BAo=r(q5e," (DeBERTa-v2 model)"),q5e.forEach(t),kAo=i(q),av=s(q,"LI",{});var G5e=n(av);Hte=s(G5e,"STRONG",{});var Gzr=n(Hte);xAo=r(Gzr,"distilbert"),Gzr.forEach(t),RAo=r(G5e," \u2014 "),PI=s(G5e,"A",{href:!0});var Ozr=n(PI);SAo=r(Ozr,"DistilBertForTokenClassification"),Ozr.forEach(t),PAo=r(G5e," (DistilBERT model)"),G5e.forEach(t),$Ao=i(q),sv=s(q,"LI",{});var O5e=n(sv);Ute=s(O5e,"STRONG",{});var Xzr=n(Ute);IAo=r(Xzr,"electra"),Xzr.forEach(t),jAo=r(O5e," \u2014 "),$I=s(O5e,"A",{href:!0});var zzr=n($I);NAo=r(zzr,"ElectraForTokenClassification"),zzr.forEach(t),DAo=r(O5e," (ELECTRA model)"),O5e.forEach(t),qAo=i(q),nv=s(q,"LI",{});var X5e=n(nv);Jte=s(X5e,"STRONG",{});var Vzr=n(Jte);GAo=r(Vzr,"flaubert"),Vzr.forEach(t),OAo=r(X5e," \u2014 "),II=s(X5e,"A",{href:!0});var Wzr=n(II);XAo=r(Wzr,"FlaubertForTokenClassification"),Wzr.forEach(t),zAo=r(X5e," (FlauBERT model)"),X5e.forEach(t),VAo=i(q),lv=s(q,"LI",{});var z5e=n(lv);Yte=s(z5e,"STRONG",{});var Qzr=n(Yte);WAo=r(Qzr,"fnet"),Qzr.forEach(t),QAo=r(z5e," \u2014 "),jI=s(z5e,"A",{href:!0});var Hzr=n(jI);HAo=r(Hzr,"FNetForTokenClassification"),Hzr.forEach(t),UAo=r(z5e," (FNet model)"),z5e.forEach(t),JAo=i(q),iv=s(q,"LI",{});var V5e=n(iv);Kte=s(V5e,"STRONG",{});var Uzr=n(Kte);YAo=r(Uzr,"funnel"),Uzr.forEach(t),KAo=r(V5e," \u2014 "),NI=s(V5e,"A",{href:!0});var Jzr=n(NI);ZAo=r(Jzr,"FunnelForTokenClassification"),Jzr.forEach(t),e6o=r(V5e," (Funnel Transformer model)"),V5e.forEach(t),o6o=i(q),dv=s(q,"LI",{});var W5e=n(dv);Zte=s(W5e,"STRONG",{});var Yzr=n(Zte);r6o=r(Yzr,"gpt2"),Yzr.forEach(t),t6o=r(W5e," \u2014 "),DI=s(W5e,"A",{href:!0});var Kzr=n(DI);a6o=r(Kzr,"GPT2ForTokenClassification"),Kzr.forEach(t),s6o=r(W5e," (OpenAI GPT-2 model)"),W5e.forEach(t),n6o=i(q),cv=s(q,"LI",{});var Q5e=n(cv);eae=s(Q5e,"STRONG",{});var Zzr=n(eae);l6o=r(Zzr,"ibert"),Zzr.forEach(t),i6o=r(Q5e," \u2014 "),qI=s(Q5e,"A",{href:!0});var eVr=n(qI);d6o=r(eVr,"IBertForTokenClassification"),eVr.forEach(t),c6o=r(Q5e," (I-BERT model)"),Q5e.forEach(t),m6o=i(q),mv=s(q,"LI",{});var H5e=n(mv);oae=s(H5e,"STRONG",{});var oVr=n(oae);f6o=r(oVr,"layoutlm"),oVr.forEach(t),g6o=r(H5e," \u2014 "),GI=s(H5e,"A",{href:!0});var rVr=n(GI);h6o=r(rVr,"LayoutLMForTokenClassification"),rVr.forEach(t),u6o=r(H5e," (LayoutLM model)"),H5e.forEach(t),p6o=i(q),fv=s(q,"LI",{});var U5e=n(fv);rae=s(U5e,"STRONG",{});var tVr=n(rae);_6o=r(tVr,"layoutlmv2"),tVr.forEach(t),b6o=r(U5e," \u2014 "),OI=s(U5e,"A",{href:!0});var aVr=n(OI);v6o=r(aVr,"LayoutLMv2ForTokenClassification"),aVr.forEach(t),T6o=r(U5e," (LayoutLMv2 model)"),U5e.forEach(t),F6o=i(q),gv=s(q,"LI",{});var J5e=n(gv);tae=s(J5e,"STRONG",{});var sVr=n(tae);C6o=r(sVr,"longformer"),sVr.forEach(t),M6o=r(J5e," \u2014 "),XI=s(J5e,"A",{href:!0});var nVr=n(XI);E6o=r(nVr,"LongformerForTokenClassification"),nVr.forEach(t),y6o=r(J5e," (Longformer model)"),J5e.forEach(t),w6o=i(q),hv=s(q,"LI",{});var Y5e=n(hv);aae=s(Y5e,"STRONG",{});var lVr=n(aae);A6o=r(lVr,"megatron-bert"),lVr.forEach(t),L6o=r(Y5e," \u2014 "),zI=s(Y5e,"A",{href:!0});var iVr=n(zI);B6o=r(iVr,"MegatronBertForTokenClassification"),iVr.forEach(t),k6o=r(Y5e," (MegatronBert model)"),Y5e.forEach(t),x6o=i(q),uv=s(q,"LI",{});var K5e=n(uv);sae=s(K5e,"STRONG",{});var dVr=n(sae);R6o=r(dVr,"mobilebert"),dVr.forEach(t),S6o=r(K5e," \u2014 "),VI=s(K5e,"A",{href:!0});var cVr=n(VI);P6o=r(cVr,"MobileBertForTokenClassification"),cVr.forEach(t),$6o=r(K5e," (MobileBERT model)"),K5e.forEach(t),I6o=i(q),pv=s(q,"LI",{});var Z5e=n(pv);nae=s(Z5e,"STRONG",{});var mVr=n(nae);j6o=r(mVr,"mpnet"),mVr.forEach(t),N6o=r(Z5e," \u2014 "),WI=s(Z5e,"A",{href:!0});var fVr=n(WI);D6o=r(fVr,"MPNetForTokenClassification"),fVr.forEach(t),q6o=r(Z5e," (MPNet model)"),Z5e.forEach(t),G6o=i(q),_v=s(q,"LI",{});var eye=n(_v);lae=s(eye,"STRONG",{});var gVr=n(lae);O6o=r(gVr,"nystromformer"),gVr.forEach(t),X6o=r(eye," \u2014 "),QI=s(eye,"A",{href:!0});var hVr=n(QI);z6o=r(hVr,"NystromformerForTokenClassification"),hVr.forEach(t),V6o=r(eye," (Nystromformer model)"),eye.forEach(t),W6o=i(q),bv=s(q,"LI",{});var oye=n(bv);iae=s(oye,"STRONG",{});var uVr=n(iae);Q6o=r(uVr,"qdqbert"),uVr.forEach(t),H6o=r(oye," \u2014 "),HI=s(oye,"A",{href:!0});var pVr=n(HI);U6o=r(pVr,"QDQBertForTokenClassification"),pVr.forEach(t),J6o=r(oye," (QDQBert model)"),oye.forEach(t),Y6o=i(q),vv=s(q,"LI",{});var rye=n(vv);dae=s(rye,"STRONG",{});var _Vr=n(dae);K6o=r(_Vr,"rembert"),_Vr.forEach(t),Z6o=r(rye," \u2014 "),UI=s(rye,"A",{href:!0});var bVr=n(UI);e0o=r(bVr,"RemBertForTokenClassification"),bVr.forEach(t),o0o=r(rye," (RemBERT model)"),rye.forEach(t),r0o=i(q),Tv=s(q,"LI",{});var tye=n(Tv);cae=s(tye,"STRONG",{});var vVr=n(cae);t0o=r(vVr,"roberta"),vVr.forEach(t),a0o=r(tye," \u2014 "),JI=s(tye,"A",{href:!0});var TVr=n(JI);s0o=r(TVr,"RobertaForTokenClassification"),TVr.forEach(t),n0o=r(tye," (RoBERTa model)"),tye.forEach(t),l0o=i(q),Fv=s(q,"LI",{});var aye=n(Fv);mae=s(aye,"STRONG",{});var FVr=n(mae);i0o=r(FVr,"roformer"),FVr.forEach(t),d0o=r(aye," \u2014 "),YI=s(aye,"A",{href:!0});var CVr=n(YI);c0o=r(CVr,"RoFormerForTokenClassification"),CVr.forEach(t),m0o=r(aye," (RoFormer model)"),aye.forEach(t),f0o=i(q),Cv=s(q,"LI",{});var sye=n(Cv);fae=s(sye,"STRONG",{});var MVr=n(fae);g0o=r(MVr,"squeezebert"),MVr.forEach(t),h0o=r(sye," \u2014 "),KI=s(sye,"A",{href:!0});var EVr=n(KI);u0o=r(EVr,"SqueezeBertForTokenClassification"),EVr.forEach(t),p0o=r(sye," (SqueezeBERT model)"),sye.forEach(t),_0o=i(q),Mv=s(q,"LI",{});var nye=n(Mv);gae=s(nye,"STRONG",{});var yVr=n(gae);b0o=r(yVr,"xlm"),yVr.forEach(t),v0o=r(nye," \u2014 "),ZI=s(nye,"A",{href:!0});var wVr=n(ZI);T0o=r(wVr,"XLMForTokenClassification"),wVr.forEach(t),F0o=r(nye," (XLM model)"),nye.forEach(t),C0o=i(q),Ev=s(q,"LI",{});var lye=n(Ev);hae=s(lye,"STRONG",{});var AVr=n(hae);M0o=r(AVr,"xlm-roberta"),AVr.forEach(t),E0o=r(lye," \u2014 "),ej=s(lye,"A",{href:!0});var LVr=n(ej);y0o=r(LVr,"XLMRobertaForTokenClassification"),LVr.forEach(t),w0o=r(lye," (XLM-RoBERTa model)"),lye.forEach(t),A0o=i(q),yv=s(q,"LI",{});var iye=n(yv);uae=s(iye,"STRONG",{});var BVr=n(uae);L0o=r(BVr,"xlm-roberta-xl"),BVr.forEach(t),B0o=r(iye," \u2014 "),oj=s(iye,"A",{href:!0});var kVr=n(oj);k0o=r(kVr,"XLMRobertaXLForTokenClassification"),kVr.forEach(t),x0o=r(iye," (XLM-RoBERTa-XL model)"),iye.forEach(t),R0o=i(q),wv=s(q,"LI",{});var dye=n(wv);pae=s(dye,"STRONG",{});var xVr=n(pae);S0o=r(xVr,"xlnet"),xVr.forEach(t),P0o=r(dye," \u2014 "),rj=s(dye,"A",{href:!0});var RVr=n(rj);$0o=r(RVr,"XLNetForTokenClassification"),RVr.forEach(t),I0o=r(dye," (XLNet model)"),dye.forEach(t),j0o=i(q),Av=s(q,"LI",{});var cye=n(Av);_ae=s(cye,"STRONG",{});var SVr=n(_ae);N0o=r(SVr,"yoso"),SVr.forEach(t),D0o=r(cye," \u2014 "),tj=s(cye,"A",{href:!0});var PVr=n(tj);q0o=r(PVr,"YosoForTokenClassification"),PVr.forEach(t),G0o=r(cye," (YOSO model)"),cye.forEach(t),q.forEach(t),O0o=i(Gt),Lv=s(Gt,"P",{});var mye=n(Lv);X0o=r(mye,"The model is set in evaluation mode by default using "),bae=s(mye,"CODE",{});var $Vr=n(bae);z0o=r($Vr,"model.eval()"),$Vr.forEach(t),V0o=r(mye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vae=s(mye,"CODE",{});var IVr=n(vae);W0o=r(IVr,"model.train()"),IVr.forEach(t),mye.forEach(t),Q0o=i(Gt),Tae=s(Gt,"P",{});var jVr=n(Tae);H0o=r(jVr,"Examples:"),jVr.forEach(t),U0o=i(Gt),f(Xy.$$.fragment,Gt),Gt.forEach(t),ol.forEach(t),P8e=i(d),gd=s(d,"H2",{class:!0});var GBe=n(gd);Bv=s(GBe,"A",{id:!0,class:!0,href:!0});var NVr=n(Bv);Fae=s(NVr,"SPAN",{});var DVr=n(Fae);f(zy.$$.fragment,DVr),DVr.forEach(t),NVr.forEach(t),J0o=i(GBe),Cae=s(GBe,"SPAN",{});var qVr=n(Cae);Y0o=r(qVr,"AutoModelForQuestionAnswering"),qVr.forEach(t),GBe.forEach(t),$8e=i(d),er=s(d,"DIV",{class:!0});var tl=n(er);f(Vy.$$.fragment,tl),K0o=i(tl),hd=s(tl,"P",{});var cz=n(hd);Z0o=r(cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mae=s(cz,"CODE",{});var GVr=n(Mae);eLo=r(GVr,"from_pretrained()"),GVr.forEach(t),oLo=r(cz,"class method or the "),Eae=s(cz,"CODE",{});var OVr=n(Eae);rLo=r(OVr,"from_config()"),OVr.forEach(t),tLo=r(cz,`class
method.`),cz.forEach(t),aLo=i(tl),Wy=s(tl,"P",{});var OBe=n(Wy);sLo=r(OBe,"This class cannot be instantiated directly using "),yae=s(OBe,"CODE",{});var XVr=n(yae);nLo=r(XVr,"__init__()"),XVr.forEach(t),lLo=r(OBe," (throws an error)."),OBe.forEach(t),iLo=i(tl),Qr=s(tl,"DIV",{class:!0});var al=n(Qr);f(Qy.$$.fragment,al),dLo=i(al),wae=s(al,"P",{});var zVr=n(wae);cLo=r(zVr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),zVr.forEach(t),mLo=i(al),ud=s(al,"P",{});var mz=n(ud);fLo=r(mz,`Note:
Loading a model from its configuration file does `),Aae=s(mz,"STRONG",{});var VVr=n(Aae);gLo=r(VVr,"not"),VVr.forEach(t),hLo=r(mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lae=s(mz,"CODE",{});var WVr=n(Lae);uLo=r(WVr,"from_pretrained()"),WVr.forEach(t),pLo=r(mz,"to load the model weights."),mz.forEach(t),_Lo=i(al),Bae=s(al,"P",{});var QVr=n(Bae);bLo=r(QVr,"Examples:"),QVr.forEach(t),vLo=i(al),f(Hy.$$.fragment,al),al.forEach(t),TLo=i(tl),De=s(tl,"DIV",{class:!0});var Ot=n(De);f(Uy.$$.fragment,Ot),FLo=i(Ot),kae=s(Ot,"P",{});var HVr=n(kae);CLo=r(HVr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),HVr.forEach(t),MLo=i(Ot),Ha=s(Ot,"P",{});var p3=n(Ha);ELo=r(p3,"The model class to instantiate is selected based on the "),xae=s(p3,"CODE",{});var UVr=n(xae);yLo=r(UVr,"model_type"),UVr.forEach(t),wLo=r(p3,` property of the config object (either
passed as an argument or loaded from `),Rae=s(p3,"CODE",{});var JVr=n(Rae);ALo=r(JVr,"pretrained_model_name_or_path"),JVr.forEach(t),LLo=r(p3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sae=s(p3,"CODE",{});var YVr=n(Sae);BLo=r(YVr,"pretrained_model_name_or_path"),YVr.forEach(t),kLo=r(p3,":"),p3.forEach(t),xLo=i(Ot),R=s(Ot,"UL",{});var P=n(R);kv=s(P,"LI",{});var fye=n(kv);Pae=s(fye,"STRONG",{});var KVr=n(Pae);RLo=r(KVr,"albert"),KVr.forEach(t),SLo=r(fye," \u2014 "),aj=s(fye,"A",{href:!0});var ZVr=n(aj);PLo=r(ZVr,"AlbertForQuestionAnswering"),ZVr.forEach(t),$Lo=r(fye," (ALBERT model)"),fye.forEach(t),ILo=i(P),xv=s(P,"LI",{});var gye=n(xv);$ae=s(gye,"STRONG",{});var eWr=n($ae);jLo=r(eWr,"bart"),eWr.forEach(t),NLo=r(gye," \u2014 "),sj=s(gye,"A",{href:!0});var oWr=n(sj);DLo=r(oWr,"BartForQuestionAnswering"),oWr.forEach(t),qLo=r(gye," (BART model)"),gye.forEach(t),GLo=i(P),Rv=s(P,"LI",{});var hye=n(Rv);Iae=s(hye,"STRONG",{});var rWr=n(Iae);OLo=r(rWr,"bert"),rWr.forEach(t),XLo=r(hye," \u2014 "),nj=s(hye,"A",{href:!0});var tWr=n(nj);zLo=r(tWr,"BertForQuestionAnswering"),tWr.forEach(t),VLo=r(hye," (BERT model)"),hye.forEach(t),WLo=i(P),Sv=s(P,"LI",{});var uye=n(Sv);jae=s(uye,"STRONG",{});var aWr=n(jae);QLo=r(aWr,"big_bird"),aWr.forEach(t),HLo=r(uye," \u2014 "),lj=s(uye,"A",{href:!0});var sWr=n(lj);ULo=r(sWr,"BigBirdForQuestionAnswering"),sWr.forEach(t),JLo=r(uye," (BigBird model)"),uye.forEach(t),YLo=i(P),Pv=s(P,"LI",{});var pye=n(Pv);Nae=s(pye,"STRONG",{});var nWr=n(Nae);KLo=r(nWr,"bigbird_pegasus"),nWr.forEach(t),ZLo=r(pye," \u2014 "),ij=s(pye,"A",{href:!0});var lWr=n(ij);e7o=r(lWr,"BigBirdPegasusForQuestionAnswering"),lWr.forEach(t),o7o=r(pye," (BigBirdPegasus model)"),pye.forEach(t),r7o=i(P),$v=s(P,"LI",{});var _ye=n($v);Dae=s(_ye,"STRONG",{});var iWr=n(Dae);t7o=r(iWr,"camembert"),iWr.forEach(t),a7o=r(_ye," \u2014 "),dj=s(_ye,"A",{href:!0});var dWr=n(dj);s7o=r(dWr,"CamembertForQuestionAnswering"),dWr.forEach(t),n7o=r(_ye," (CamemBERT model)"),_ye.forEach(t),l7o=i(P),Iv=s(P,"LI",{});var bye=n(Iv);qae=s(bye,"STRONG",{});var cWr=n(qae);i7o=r(cWr,"canine"),cWr.forEach(t),d7o=r(bye," \u2014 "),cj=s(bye,"A",{href:!0});var mWr=n(cj);c7o=r(mWr,"CanineForQuestionAnswering"),mWr.forEach(t),m7o=r(bye," (Canine model)"),bye.forEach(t),f7o=i(P),jv=s(P,"LI",{});var vye=n(jv);Gae=s(vye,"STRONG",{});var fWr=n(Gae);g7o=r(fWr,"convbert"),fWr.forEach(t),h7o=r(vye," \u2014 "),mj=s(vye,"A",{href:!0});var gWr=n(mj);u7o=r(gWr,"ConvBertForQuestionAnswering"),gWr.forEach(t),p7o=r(vye," (ConvBERT model)"),vye.forEach(t),_7o=i(P),Nv=s(P,"LI",{});var Tye=n(Nv);Oae=s(Tye,"STRONG",{});var hWr=n(Oae);b7o=r(hWr,"deberta"),hWr.forEach(t),v7o=r(Tye," \u2014 "),fj=s(Tye,"A",{href:!0});var uWr=n(fj);T7o=r(uWr,"DebertaForQuestionAnswering"),uWr.forEach(t),F7o=r(Tye," (DeBERTa model)"),Tye.forEach(t),C7o=i(P),Dv=s(P,"LI",{});var Fye=n(Dv);Xae=s(Fye,"STRONG",{});var pWr=n(Xae);M7o=r(pWr,"deberta-v2"),pWr.forEach(t),E7o=r(Fye," \u2014 "),gj=s(Fye,"A",{href:!0});var _Wr=n(gj);y7o=r(_Wr,"DebertaV2ForQuestionAnswering"),_Wr.forEach(t),w7o=r(Fye," (DeBERTa-v2 model)"),Fye.forEach(t),A7o=i(P),qv=s(P,"LI",{});var Cye=n(qv);zae=s(Cye,"STRONG",{});var bWr=n(zae);L7o=r(bWr,"distilbert"),bWr.forEach(t),B7o=r(Cye," \u2014 "),hj=s(Cye,"A",{href:!0});var vWr=n(hj);k7o=r(vWr,"DistilBertForQuestionAnswering"),vWr.forEach(t),x7o=r(Cye," (DistilBERT model)"),Cye.forEach(t),R7o=i(P),Gv=s(P,"LI",{});var Mye=n(Gv);Vae=s(Mye,"STRONG",{});var TWr=n(Vae);S7o=r(TWr,"electra"),TWr.forEach(t),P7o=r(Mye," \u2014 "),uj=s(Mye,"A",{href:!0});var FWr=n(uj);$7o=r(FWr,"ElectraForQuestionAnswering"),FWr.forEach(t),I7o=r(Mye," (ELECTRA model)"),Mye.forEach(t),j7o=i(P),Ov=s(P,"LI",{});var Eye=n(Ov);Wae=s(Eye,"STRONG",{});var CWr=n(Wae);N7o=r(CWr,"flaubert"),CWr.forEach(t),D7o=r(Eye," \u2014 "),pj=s(Eye,"A",{href:!0});var MWr=n(pj);q7o=r(MWr,"FlaubertForQuestionAnsweringSimple"),MWr.forEach(t),G7o=r(Eye," (FlauBERT model)"),Eye.forEach(t),O7o=i(P),Xv=s(P,"LI",{});var yye=n(Xv);Qae=s(yye,"STRONG",{});var EWr=n(Qae);X7o=r(EWr,"fnet"),EWr.forEach(t),z7o=r(yye," \u2014 "),_j=s(yye,"A",{href:!0});var yWr=n(_j);V7o=r(yWr,"FNetForQuestionAnswering"),yWr.forEach(t),W7o=r(yye," (FNet model)"),yye.forEach(t),Q7o=i(P),zv=s(P,"LI",{});var wye=n(zv);Hae=s(wye,"STRONG",{});var wWr=n(Hae);H7o=r(wWr,"funnel"),wWr.forEach(t),U7o=r(wye," \u2014 "),bj=s(wye,"A",{href:!0});var AWr=n(bj);J7o=r(AWr,"FunnelForQuestionAnswering"),AWr.forEach(t),Y7o=r(wye," (Funnel Transformer model)"),wye.forEach(t),K7o=i(P),Vv=s(P,"LI",{});var Aye=n(Vv);Uae=s(Aye,"STRONG",{});var LWr=n(Uae);Z7o=r(LWr,"gptj"),LWr.forEach(t),e8o=r(Aye," \u2014 "),vj=s(Aye,"A",{href:!0});var BWr=n(vj);o8o=r(BWr,"GPTJForQuestionAnswering"),BWr.forEach(t),r8o=r(Aye," (GPT-J model)"),Aye.forEach(t),t8o=i(P),Wv=s(P,"LI",{});var Lye=n(Wv);Jae=s(Lye,"STRONG",{});var kWr=n(Jae);a8o=r(kWr,"ibert"),kWr.forEach(t),s8o=r(Lye," \u2014 "),Tj=s(Lye,"A",{href:!0});var xWr=n(Tj);n8o=r(xWr,"IBertForQuestionAnswering"),xWr.forEach(t),l8o=r(Lye," (I-BERT model)"),Lye.forEach(t),i8o=i(P),Qv=s(P,"LI",{});var Bye=n(Qv);Yae=s(Bye,"STRONG",{});var RWr=n(Yae);d8o=r(RWr,"layoutlmv2"),RWr.forEach(t),c8o=r(Bye," \u2014 "),Fj=s(Bye,"A",{href:!0});var SWr=n(Fj);m8o=r(SWr,"LayoutLMv2ForQuestionAnswering"),SWr.forEach(t),f8o=r(Bye," (LayoutLMv2 model)"),Bye.forEach(t),g8o=i(P),Hv=s(P,"LI",{});var kye=n(Hv);Kae=s(kye,"STRONG",{});var PWr=n(Kae);h8o=r(PWr,"led"),PWr.forEach(t),u8o=r(kye," \u2014 "),Cj=s(kye,"A",{href:!0});var $Wr=n(Cj);p8o=r($Wr,"LEDForQuestionAnswering"),$Wr.forEach(t),_8o=r(kye," (LED model)"),kye.forEach(t),b8o=i(P),Uv=s(P,"LI",{});var xye=n(Uv);Zae=s(xye,"STRONG",{});var IWr=n(Zae);v8o=r(IWr,"longformer"),IWr.forEach(t),T8o=r(xye," \u2014 "),Mj=s(xye,"A",{href:!0});var jWr=n(Mj);F8o=r(jWr,"LongformerForQuestionAnswering"),jWr.forEach(t),C8o=r(xye," (Longformer model)"),xye.forEach(t),M8o=i(P),Jv=s(P,"LI",{});var Rye=n(Jv);ese=s(Rye,"STRONG",{});var NWr=n(ese);E8o=r(NWr,"lxmert"),NWr.forEach(t),y8o=r(Rye," \u2014 "),Ej=s(Rye,"A",{href:!0});var DWr=n(Ej);w8o=r(DWr,"LxmertForQuestionAnswering"),DWr.forEach(t),A8o=r(Rye," (LXMERT model)"),Rye.forEach(t),L8o=i(P),Yv=s(P,"LI",{});var Sye=n(Yv);ose=s(Sye,"STRONG",{});var qWr=n(ose);B8o=r(qWr,"mbart"),qWr.forEach(t),k8o=r(Sye," \u2014 "),yj=s(Sye,"A",{href:!0});var GWr=n(yj);x8o=r(GWr,"MBartForQuestionAnswering"),GWr.forEach(t),R8o=r(Sye," (mBART model)"),Sye.forEach(t),S8o=i(P),Kv=s(P,"LI",{});var Pye=n(Kv);rse=s(Pye,"STRONG",{});var OWr=n(rse);P8o=r(OWr,"megatron-bert"),OWr.forEach(t),$8o=r(Pye," \u2014 "),wj=s(Pye,"A",{href:!0});var XWr=n(wj);I8o=r(XWr,"MegatronBertForQuestionAnswering"),XWr.forEach(t),j8o=r(Pye," (MegatronBert model)"),Pye.forEach(t),N8o=i(P),Zv=s(P,"LI",{});var $ye=n(Zv);tse=s($ye,"STRONG",{});var zWr=n(tse);D8o=r(zWr,"mobilebert"),zWr.forEach(t),q8o=r($ye," \u2014 "),Aj=s($ye,"A",{href:!0});var VWr=n(Aj);G8o=r(VWr,"MobileBertForQuestionAnswering"),VWr.forEach(t),O8o=r($ye," (MobileBERT model)"),$ye.forEach(t),X8o=i(P),eT=s(P,"LI",{});var Iye=n(eT);ase=s(Iye,"STRONG",{});var WWr=n(ase);z8o=r(WWr,"mpnet"),WWr.forEach(t),V8o=r(Iye," \u2014 "),Lj=s(Iye,"A",{href:!0});var QWr=n(Lj);W8o=r(QWr,"MPNetForQuestionAnswering"),QWr.forEach(t),Q8o=r(Iye," (MPNet model)"),Iye.forEach(t),H8o=i(P),oT=s(P,"LI",{});var jye=n(oT);sse=s(jye,"STRONG",{});var HWr=n(sse);U8o=r(HWr,"nystromformer"),HWr.forEach(t),J8o=r(jye," \u2014 "),Bj=s(jye,"A",{href:!0});var UWr=n(Bj);Y8o=r(UWr,"NystromformerForQuestionAnswering"),UWr.forEach(t),K8o=r(jye," (Nystromformer model)"),jye.forEach(t),Z8o=i(P),rT=s(P,"LI",{});var Nye=n(rT);nse=s(Nye,"STRONG",{});var JWr=n(nse);e9o=r(JWr,"qdqbert"),JWr.forEach(t),o9o=r(Nye," \u2014 "),kj=s(Nye,"A",{href:!0});var YWr=n(kj);r9o=r(YWr,"QDQBertForQuestionAnswering"),YWr.forEach(t),t9o=r(Nye," (QDQBert model)"),Nye.forEach(t),a9o=i(P),tT=s(P,"LI",{});var Dye=n(tT);lse=s(Dye,"STRONG",{});var KWr=n(lse);s9o=r(KWr,"reformer"),KWr.forEach(t),n9o=r(Dye," \u2014 "),xj=s(Dye,"A",{href:!0});var ZWr=n(xj);l9o=r(ZWr,"ReformerForQuestionAnswering"),ZWr.forEach(t),i9o=r(Dye," (Reformer model)"),Dye.forEach(t),d9o=i(P),aT=s(P,"LI",{});var qye=n(aT);ise=s(qye,"STRONG",{});var eQr=n(ise);c9o=r(eQr,"rembert"),eQr.forEach(t),m9o=r(qye," \u2014 "),Rj=s(qye,"A",{href:!0});var oQr=n(Rj);f9o=r(oQr,"RemBertForQuestionAnswering"),oQr.forEach(t),g9o=r(qye," (RemBERT model)"),qye.forEach(t),h9o=i(P),sT=s(P,"LI",{});var Gye=n(sT);dse=s(Gye,"STRONG",{});var rQr=n(dse);u9o=r(rQr,"roberta"),rQr.forEach(t),p9o=r(Gye," \u2014 "),Sj=s(Gye,"A",{href:!0});var tQr=n(Sj);_9o=r(tQr,"RobertaForQuestionAnswering"),tQr.forEach(t),b9o=r(Gye," (RoBERTa model)"),Gye.forEach(t),v9o=i(P),nT=s(P,"LI",{});var Oye=n(nT);cse=s(Oye,"STRONG",{});var aQr=n(cse);T9o=r(aQr,"roformer"),aQr.forEach(t),F9o=r(Oye," \u2014 "),Pj=s(Oye,"A",{href:!0});var sQr=n(Pj);C9o=r(sQr,"RoFormerForQuestionAnswering"),sQr.forEach(t),M9o=r(Oye," (RoFormer model)"),Oye.forEach(t),E9o=i(P),lT=s(P,"LI",{});var Xye=n(lT);mse=s(Xye,"STRONG",{});var nQr=n(mse);y9o=r(nQr,"splinter"),nQr.forEach(t),w9o=r(Xye," \u2014 "),$j=s(Xye,"A",{href:!0});var lQr=n($j);A9o=r(lQr,"SplinterForQuestionAnswering"),lQr.forEach(t),L9o=r(Xye," (Splinter model)"),Xye.forEach(t),B9o=i(P),iT=s(P,"LI",{});var zye=n(iT);fse=s(zye,"STRONG",{});var iQr=n(fse);k9o=r(iQr,"squeezebert"),iQr.forEach(t),x9o=r(zye," \u2014 "),Ij=s(zye,"A",{href:!0});var dQr=n(Ij);R9o=r(dQr,"SqueezeBertForQuestionAnswering"),dQr.forEach(t),S9o=r(zye," (SqueezeBERT model)"),zye.forEach(t),P9o=i(P),dT=s(P,"LI",{});var Vye=n(dT);gse=s(Vye,"STRONG",{});var cQr=n(gse);$9o=r(cQr,"xlm"),cQr.forEach(t),I9o=r(Vye," \u2014 "),jj=s(Vye,"A",{href:!0});var mQr=n(jj);j9o=r(mQr,"XLMForQuestionAnsweringSimple"),mQr.forEach(t),N9o=r(Vye," (XLM model)"),Vye.forEach(t),D9o=i(P),cT=s(P,"LI",{});var Wye=n(cT);hse=s(Wye,"STRONG",{});var fQr=n(hse);q9o=r(fQr,"xlm-roberta"),fQr.forEach(t),G9o=r(Wye," \u2014 "),Nj=s(Wye,"A",{href:!0});var gQr=n(Nj);O9o=r(gQr,"XLMRobertaForQuestionAnswering"),gQr.forEach(t),X9o=r(Wye," (XLM-RoBERTa model)"),Wye.forEach(t),z9o=i(P),mT=s(P,"LI",{});var Qye=n(mT);use=s(Qye,"STRONG",{});var hQr=n(use);V9o=r(hQr,"xlm-roberta-xl"),hQr.forEach(t),W9o=r(Qye," \u2014 "),Dj=s(Qye,"A",{href:!0});var uQr=n(Dj);Q9o=r(uQr,"XLMRobertaXLForQuestionAnswering"),uQr.forEach(t),H9o=r(Qye," (XLM-RoBERTa-XL model)"),Qye.forEach(t),U9o=i(P),fT=s(P,"LI",{});var Hye=n(fT);pse=s(Hye,"STRONG",{});var pQr=n(pse);J9o=r(pQr,"xlnet"),pQr.forEach(t),Y9o=r(Hye," \u2014 "),qj=s(Hye,"A",{href:!0});var _Qr=n(qj);K9o=r(_Qr,"XLNetForQuestionAnsweringSimple"),_Qr.forEach(t),Z9o=r(Hye," (XLNet model)"),Hye.forEach(t),eBo=i(P),gT=s(P,"LI",{});var Uye=n(gT);_se=s(Uye,"STRONG",{});var bQr=n(_se);oBo=r(bQr,"yoso"),bQr.forEach(t),rBo=r(Uye," \u2014 "),Gj=s(Uye,"A",{href:!0});var vQr=n(Gj);tBo=r(vQr,"YosoForQuestionAnswering"),vQr.forEach(t),aBo=r(Uye," (YOSO model)"),Uye.forEach(t),P.forEach(t),sBo=i(Ot),hT=s(Ot,"P",{});var Jye=n(hT);nBo=r(Jye,"The model is set in evaluation mode by default using "),bse=s(Jye,"CODE",{});var TQr=n(bse);lBo=r(TQr,"model.eval()"),TQr.forEach(t),iBo=r(Jye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vse=s(Jye,"CODE",{});var FQr=n(vse);dBo=r(FQr,"model.train()"),FQr.forEach(t),Jye.forEach(t),cBo=i(Ot),Tse=s(Ot,"P",{});var CQr=n(Tse);mBo=r(CQr,"Examples:"),CQr.forEach(t),fBo=i(Ot),f(Jy.$$.fragment,Ot),Ot.forEach(t),tl.forEach(t),I8e=i(d),pd=s(d,"H2",{class:!0});var XBe=n(pd);uT=s(XBe,"A",{id:!0,class:!0,href:!0});var MQr=n(uT);Fse=s(MQr,"SPAN",{});var EQr=n(Fse);f(Yy.$$.fragment,EQr),EQr.forEach(t),MQr.forEach(t),gBo=i(XBe),Cse=s(XBe,"SPAN",{});var yQr=n(Cse);hBo=r(yQr,"AutoModelForTableQuestionAnswering"),yQr.forEach(t),XBe.forEach(t),j8e=i(d),or=s(d,"DIV",{class:!0});var sl=n(or);f(Ky.$$.fragment,sl),uBo=i(sl),_d=s(sl,"P",{});var fz=n(_d);pBo=r(fz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Mse=s(fz,"CODE",{});var wQr=n(Mse);_Bo=r(wQr,"from_pretrained()"),wQr.forEach(t),bBo=r(fz,"class method or the "),Ese=s(fz,"CODE",{});var AQr=n(Ese);vBo=r(AQr,"from_config()"),AQr.forEach(t),TBo=r(fz,`class
method.`),fz.forEach(t),FBo=i(sl),Zy=s(sl,"P",{});var zBe=n(Zy);CBo=r(zBe,"This class cannot be instantiated directly using "),yse=s(zBe,"CODE",{});var LQr=n(yse);MBo=r(LQr,"__init__()"),LQr.forEach(t),EBo=r(zBe," (throws an error)."),zBe.forEach(t),yBo=i(sl),Hr=s(sl,"DIV",{class:!0});var nl=n(Hr);f(ew.$$.fragment,nl),wBo=i(nl),wse=s(nl,"P",{});var BQr=n(wse);ABo=r(BQr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),BQr.forEach(t),LBo=i(nl),bd=s(nl,"P",{});var gz=n(bd);BBo=r(gz,`Note:
Loading a model from its configuration file does `),Ase=s(gz,"STRONG",{});var kQr=n(Ase);kBo=r(kQr,"not"),kQr.forEach(t),xBo=r(gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lse=s(gz,"CODE",{});var xQr=n(Lse);RBo=r(xQr,"from_pretrained()"),xQr.forEach(t),SBo=r(gz,"to load the model weights."),gz.forEach(t),PBo=i(nl),Bse=s(nl,"P",{});var RQr=n(Bse);$Bo=r(RQr,"Examples:"),RQr.forEach(t),IBo=i(nl),f(ow.$$.fragment,nl),nl.forEach(t),jBo=i(sl),qe=s(sl,"DIV",{class:!0});var Xt=n(qe);f(rw.$$.fragment,Xt),NBo=i(Xt),kse=s(Xt,"P",{});var SQr=n(kse);DBo=r(SQr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),SQr.forEach(t),qBo=i(Xt),Ua=s(Xt,"P",{});var _3=n(Ua);GBo=r(_3,"The model class to instantiate is selected based on the "),xse=s(_3,"CODE",{});var PQr=n(xse);OBo=r(PQr,"model_type"),PQr.forEach(t),XBo=r(_3,` property of the config object (either
passed as an argument or loaded from `),Rse=s(_3,"CODE",{});var $Qr=n(Rse);zBo=r($Qr,"pretrained_model_name_or_path"),$Qr.forEach(t),VBo=r(_3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sse=s(_3,"CODE",{});var IQr=n(Sse);WBo=r(IQr,"pretrained_model_name_or_path"),IQr.forEach(t),QBo=r(_3,":"),_3.forEach(t),HBo=i(Xt),Pse=s(Xt,"UL",{});var jQr=n(Pse);pT=s(jQr,"LI",{});var Yye=n(pT);$se=s(Yye,"STRONG",{});var NQr=n($se);UBo=r(NQr,"tapas"),NQr.forEach(t),JBo=r(Yye," \u2014 "),Oj=s(Yye,"A",{href:!0});var DQr=n(Oj);YBo=r(DQr,"TapasForQuestionAnswering"),DQr.forEach(t),KBo=r(Yye," (TAPAS model)"),Yye.forEach(t),jQr.forEach(t),ZBo=i(Xt),_T=s(Xt,"P",{});var Kye=n(_T);eko=r(Kye,"The model is set in evaluation mode by default using "),Ise=s(Kye,"CODE",{});var qQr=n(Ise);oko=r(qQr,"model.eval()"),qQr.forEach(t),rko=r(Kye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jse=s(Kye,"CODE",{});var GQr=n(jse);tko=r(GQr,"model.train()"),GQr.forEach(t),Kye.forEach(t),ako=i(Xt),Nse=s(Xt,"P",{});var OQr=n(Nse);sko=r(OQr,"Examples:"),OQr.forEach(t),nko=i(Xt),f(tw.$$.fragment,Xt),Xt.forEach(t),sl.forEach(t),N8e=i(d),vd=s(d,"H2",{class:!0});var VBe=n(vd);bT=s(VBe,"A",{id:!0,class:!0,href:!0});var XQr=n(bT);Dse=s(XQr,"SPAN",{});var zQr=n(Dse);f(aw.$$.fragment,zQr),zQr.forEach(t),XQr.forEach(t),lko=i(VBe),qse=s(VBe,"SPAN",{});var VQr=n(qse);iko=r(VQr,"AutoModelForImageClassification"),VQr.forEach(t),VBe.forEach(t),D8e=i(d),rr=s(d,"DIV",{class:!0});var ll=n(rr);f(sw.$$.fragment,ll),dko=i(ll),Td=s(ll,"P",{});var hz=n(Td);cko=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Gse=s(hz,"CODE",{});var WQr=n(Gse);mko=r(WQr,"from_pretrained()"),WQr.forEach(t),fko=r(hz,"class method or the "),Ose=s(hz,"CODE",{});var QQr=n(Ose);gko=r(QQr,"from_config()"),QQr.forEach(t),hko=r(hz,`class
method.`),hz.forEach(t),uko=i(ll),nw=s(ll,"P",{});var WBe=n(nw);pko=r(WBe,"This class cannot be instantiated directly using "),Xse=s(WBe,"CODE",{});var HQr=n(Xse);_ko=r(HQr,"__init__()"),HQr.forEach(t),bko=r(WBe," (throws an error)."),WBe.forEach(t),vko=i(ll),Ur=s(ll,"DIV",{class:!0});var il=n(Ur);f(lw.$$.fragment,il),Tko=i(il),zse=s(il,"P",{});var UQr=n(zse);Fko=r(UQr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),UQr.forEach(t),Cko=i(il),Fd=s(il,"P",{});var uz=n(Fd);Mko=r(uz,`Note:
Loading a model from its configuration file does `),Vse=s(uz,"STRONG",{});var JQr=n(Vse);Eko=r(JQr,"not"),JQr.forEach(t),yko=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wse=s(uz,"CODE",{});var YQr=n(Wse);wko=r(YQr,"from_pretrained()"),YQr.forEach(t),Ako=r(uz,"to load the model weights."),uz.forEach(t),Lko=i(il),Qse=s(il,"P",{});var KQr=n(Qse);Bko=r(KQr,"Examples:"),KQr.forEach(t),kko=i(il),f(iw.$$.fragment,il),il.forEach(t),xko=i(ll),Ge=s(ll,"DIV",{class:!0});var zt=n(Ge);f(dw.$$.fragment,zt),Rko=i(zt),Hse=s(zt,"P",{});var ZQr=n(Hse);Sko=r(ZQr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ZQr.forEach(t),Pko=i(zt),Ja=s(zt,"P",{});var b3=n(Ja);$ko=r(b3,"The model class to instantiate is selected based on the "),Use=s(b3,"CODE",{});var eHr=n(Use);Iko=r(eHr,"model_type"),eHr.forEach(t),jko=r(b3,` property of the config object (either
passed as an argument or loaded from `),Jse=s(b3,"CODE",{});var oHr=n(Jse);Nko=r(oHr,"pretrained_model_name_or_path"),oHr.forEach(t),Dko=r(b3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yse=s(b3,"CODE",{});var rHr=n(Yse);qko=r(rHr,"pretrained_model_name_or_path"),rHr.forEach(t),Gko=r(b3,":"),b3.forEach(t),Oko=i(zt),be=s(zt,"UL",{});var Ke=n(be);vT=s(Ke,"LI",{});var Zye=n(vT);Kse=s(Zye,"STRONG",{});var tHr=n(Kse);Xko=r(tHr,"beit"),tHr.forEach(t),zko=r(Zye," \u2014 "),Xj=s(Zye,"A",{href:!0});var aHr=n(Xj);Vko=r(aHr,"BeitForImageClassification"),aHr.forEach(t),Wko=r(Zye," (BEiT model)"),Zye.forEach(t),Qko=i(Ke),TT=s(Ke,"LI",{});var ewe=n(TT);Zse=s(ewe,"STRONG",{});var sHr=n(Zse);Hko=r(sHr,"convnext"),sHr.forEach(t),Uko=r(ewe," \u2014 "),zj=s(ewe,"A",{href:!0});var nHr=n(zj);Jko=r(nHr,"ConvNextForImageClassification"),nHr.forEach(t),Yko=r(ewe," (ConvNext model)"),ewe.forEach(t),Kko=i(Ke),Rn=s(Ke,"LI",{});var j7=n(Rn);ene=s(j7,"STRONG",{});var lHr=n(ene);Zko=r(lHr,"deit"),lHr.forEach(t),exo=r(j7," \u2014 "),Vj=s(j7,"A",{href:!0});var iHr=n(Vj);oxo=r(iHr,"DeiTForImageClassification"),iHr.forEach(t),rxo=r(j7," or "),Wj=s(j7,"A",{href:!0});var dHr=n(Wj);txo=r(dHr,"DeiTForImageClassificationWithTeacher"),dHr.forEach(t),axo=r(j7," (DeiT model)"),j7.forEach(t),sxo=i(Ke),FT=s(Ke,"LI",{});var owe=n(FT);one=s(owe,"STRONG",{});var cHr=n(one);nxo=r(cHr,"imagegpt"),cHr.forEach(t),lxo=r(owe," \u2014 "),Qj=s(owe,"A",{href:!0});var mHr=n(Qj);ixo=r(mHr,"ImageGPTForImageClassification"),mHr.forEach(t),dxo=r(owe," (ImageGPT model)"),owe.forEach(t),cxo=i(Ke),la=s(Ke,"LI",{});var Cm=n(la);rne=s(Cm,"STRONG",{});var fHr=n(rne);mxo=r(fHr,"perceiver"),fHr.forEach(t),fxo=r(Cm," \u2014 "),Hj=s(Cm,"A",{href:!0});var gHr=n(Hj);gxo=r(gHr,"PerceiverForImageClassificationLearned"),gHr.forEach(t),hxo=r(Cm," or "),Uj=s(Cm,"A",{href:!0});var hHr=n(Uj);uxo=r(hHr,"PerceiverForImageClassificationFourier"),hHr.forEach(t),pxo=r(Cm," or "),Jj=s(Cm,"A",{href:!0});var uHr=n(Jj);_xo=r(uHr,"PerceiverForImageClassificationConvProcessing"),uHr.forEach(t),bxo=r(Cm," (Perceiver model)"),Cm.forEach(t),vxo=i(Ke),CT=s(Ke,"LI",{});var rwe=n(CT);tne=s(rwe,"STRONG",{});var pHr=n(tne);Txo=r(pHr,"poolformer"),pHr.forEach(t),Fxo=r(rwe," \u2014 "),Yj=s(rwe,"A",{href:!0});var _Hr=n(Yj);Cxo=r(_Hr,"PoolFormerForImageClassification"),_Hr.forEach(t),Mxo=r(rwe," (PoolFormer model)"),rwe.forEach(t),Exo=i(Ke),MT=s(Ke,"LI",{});var twe=n(MT);ane=s(twe,"STRONG",{});var bHr=n(ane);yxo=r(bHr,"segformer"),bHr.forEach(t),wxo=r(twe," \u2014 "),Kj=s(twe,"A",{href:!0});var vHr=n(Kj);Axo=r(vHr,"SegformerForImageClassification"),vHr.forEach(t),Lxo=r(twe," (SegFormer model)"),twe.forEach(t),Bxo=i(Ke),ET=s(Ke,"LI",{});var awe=n(ET);sne=s(awe,"STRONG",{});var THr=n(sne);kxo=r(THr,"swin"),THr.forEach(t),xxo=r(awe," \u2014 "),Zj=s(awe,"A",{href:!0});var FHr=n(Zj);Rxo=r(FHr,"SwinForImageClassification"),FHr.forEach(t),Sxo=r(awe," (Swin model)"),awe.forEach(t),Pxo=i(Ke),yT=s(Ke,"LI",{});var swe=n(yT);nne=s(swe,"STRONG",{});var CHr=n(nne);$xo=r(CHr,"vit"),CHr.forEach(t),Ixo=r(swe," \u2014 "),eN=s(swe,"A",{href:!0});var MHr=n(eN);jxo=r(MHr,"ViTForImageClassification"),MHr.forEach(t),Nxo=r(swe," (ViT model)"),swe.forEach(t),Ke.forEach(t),Dxo=i(zt),wT=s(zt,"P",{});var nwe=n(wT);qxo=r(nwe,"The model is set in evaluation mode by default using "),lne=s(nwe,"CODE",{});var EHr=n(lne);Gxo=r(EHr,"model.eval()"),EHr.forEach(t),Oxo=r(nwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ine=s(nwe,"CODE",{});var yHr=n(ine);Xxo=r(yHr,"model.train()"),yHr.forEach(t),nwe.forEach(t),zxo=i(zt),dne=s(zt,"P",{});var wHr=n(dne);Vxo=r(wHr,"Examples:"),wHr.forEach(t),Wxo=i(zt),f(cw.$$.fragment,zt),zt.forEach(t),ll.forEach(t),q8e=i(d),Cd=s(d,"H2",{class:!0});var QBe=n(Cd);AT=s(QBe,"A",{id:!0,class:!0,href:!0});var AHr=n(AT);cne=s(AHr,"SPAN",{});var LHr=n(cne);f(mw.$$.fragment,LHr),LHr.forEach(t),AHr.forEach(t),Qxo=i(QBe),mne=s(QBe,"SPAN",{});var BHr=n(mne);Hxo=r(BHr,"AutoModelForVision2Seq"),BHr.forEach(t),QBe.forEach(t),G8e=i(d),tr=s(d,"DIV",{class:!0});var dl=n(tr);f(fw.$$.fragment,dl),Uxo=i(dl),Md=s(dl,"P",{});var pz=n(Md);Jxo=r(pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),fne=s(pz,"CODE",{});var kHr=n(fne);Yxo=r(kHr,"from_pretrained()"),kHr.forEach(t),Kxo=r(pz,"class method or the "),gne=s(pz,"CODE",{});var xHr=n(gne);Zxo=r(xHr,"from_config()"),xHr.forEach(t),eRo=r(pz,`class
method.`),pz.forEach(t),oRo=i(dl),gw=s(dl,"P",{});var HBe=n(gw);rRo=r(HBe,"This class cannot be instantiated directly using "),hne=s(HBe,"CODE",{});var RHr=n(hne);tRo=r(RHr,"__init__()"),RHr.forEach(t),aRo=r(HBe," (throws an error)."),HBe.forEach(t),sRo=i(dl),Jr=s(dl,"DIV",{class:!0});var cl=n(Jr);f(hw.$$.fragment,cl),nRo=i(cl),une=s(cl,"P",{});var SHr=n(une);lRo=r(SHr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),SHr.forEach(t),iRo=i(cl),Ed=s(cl,"P",{});var _z=n(Ed);dRo=r(_z,`Note:
Loading a model from its configuration file does `),pne=s(_z,"STRONG",{});var PHr=n(pne);cRo=r(PHr,"not"),PHr.forEach(t),mRo=r(_z,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ne=s(_z,"CODE",{});var $Hr=n(_ne);fRo=r($Hr,"from_pretrained()"),$Hr.forEach(t),gRo=r(_z,"to load the model weights."),_z.forEach(t),hRo=i(cl),bne=s(cl,"P",{});var IHr=n(bne);uRo=r(IHr,"Examples:"),IHr.forEach(t),pRo=i(cl),f(uw.$$.fragment,cl),cl.forEach(t),_Ro=i(dl),Oe=s(dl,"DIV",{class:!0});var Vt=n(Oe);f(pw.$$.fragment,Vt),bRo=i(Vt),vne=s(Vt,"P",{});var jHr=n(vne);vRo=r(jHr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),jHr.forEach(t),TRo=i(Vt),Ya=s(Vt,"P",{});var v3=n(Ya);FRo=r(v3,"The model class to instantiate is selected based on the "),Tne=s(v3,"CODE",{});var NHr=n(Tne);CRo=r(NHr,"model_type"),NHr.forEach(t),MRo=r(v3,` property of the config object (either
passed as an argument or loaded from `),Fne=s(v3,"CODE",{});var DHr=n(Fne);ERo=r(DHr,"pretrained_model_name_or_path"),DHr.forEach(t),yRo=r(v3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cne=s(v3,"CODE",{});var qHr=n(Cne);wRo=r(qHr,"pretrained_model_name_or_path"),qHr.forEach(t),ARo=r(v3,":"),v3.forEach(t),LRo=i(Vt),Mne=s(Vt,"UL",{});var GHr=n(Mne);LT=s(GHr,"LI",{});var lwe=n(LT);Ene=s(lwe,"STRONG",{});var OHr=n(Ene);BRo=r(OHr,"vision-encoder-decoder"),OHr.forEach(t),kRo=r(lwe," \u2014 "),oN=s(lwe,"A",{href:!0});var XHr=n(oN);xRo=r(XHr,"VisionEncoderDecoderModel"),XHr.forEach(t),RRo=r(lwe," (Vision Encoder decoder model)"),lwe.forEach(t),GHr.forEach(t),SRo=i(Vt),BT=s(Vt,"P",{});var iwe=n(BT);PRo=r(iwe,"The model is set in evaluation mode by default using "),yne=s(iwe,"CODE",{});var zHr=n(yne);$Ro=r(zHr,"model.eval()"),zHr.forEach(t),IRo=r(iwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),wne=s(iwe,"CODE",{});var VHr=n(wne);jRo=r(VHr,"model.train()"),VHr.forEach(t),iwe.forEach(t),NRo=i(Vt),Ane=s(Vt,"P",{});var WHr=n(Ane);DRo=r(WHr,"Examples:"),WHr.forEach(t),qRo=i(Vt),f(_w.$$.fragment,Vt),Vt.forEach(t),dl.forEach(t),O8e=i(d),yd=s(d,"H2",{class:!0});var UBe=n(yd);kT=s(UBe,"A",{id:!0,class:!0,href:!0});var QHr=n(kT);Lne=s(QHr,"SPAN",{});var HHr=n(Lne);f(bw.$$.fragment,HHr),HHr.forEach(t),QHr.forEach(t),GRo=i(UBe),Bne=s(UBe,"SPAN",{});var UHr=n(Bne);ORo=r(UHr,"AutoModelForAudioClassification"),UHr.forEach(t),UBe.forEach(t),X8e=i(d),ar=s(d,"DIV",{class:!0});var ml=n(ar);f(vw.$$.fragment,ml),XRo=i(ml),wd=s(ml,"P",{});var bz=n(wd);zRo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),kne=s(bz,"CODE",{});var JHr=n(kne);VRo=r(JHr,"from_pretrained()"),JHr.forEach(t),WRo=r(bz,"class method or the "),xne=s(bz,"CODE",{});var YHr=n(xne);QRo=r(YHr,"from_config()"),YHr.forEach(t),HRo=r(bz,`class
method.`),bz.forEach(t),URo=i(ml),Tw=s(ml,"P",{});var JBe=n(Tw);JRo=r(JBe,"This class cannot be instantiated directly using "),Rne=s(JBe,"CODE",{});var KHr=n(Rne);YRo=r(KHr,"__init__()"),KHr.forEach(t),KRo=r(JBe," (throws an error)."),JBe.forEach(t),ZRo=i(ml),Yr=s(ml,"DIV",{class:!0});var fl=n(Yr);f(Fw.$$.fragment,fl),eSo=i(fl),Sne=s(fl,"P",{});var ZHr=n(Sne);oSo=r(ZHr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),ZHr.forEach(t),rSo=i(fl),Ad=s(fl,"P",{});var vz=n(Ad);tSo=r(vz,`Note:
Loading a model from its configuration file does `),Pne=s(vz,"STRONG",{});var eUr=n(Pne);aSo=r(eUr,"not"),eUr.forEach(t),sSo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),$ne=s(vz,"CODE",{});var oUr=n($ne);nSo=r(oUr,"from_pretrained()"),oUr.forEach(t),lSo=r(vz,"to load the model weights."),vz.forEach(t),iSo=i(fl),Ine=s(fl,"P",{});var rUr=n(Ine);dSo=r(rUr,"Examples:"),rUr.forEach(t),cSo=i(fl),f(Cw.$$.fragment,fl),fl.forEach(t),mSo=i(ml),Xe=s(ml,"DIV",{class:!0});var Wt=n(Xe);f(Mw.$$.fragment,Wt),fSo=i(Wt),jne=s(Wt,"P",{});var tUr=n(jne);gSo=r(tUr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),tUr.forEach(t),hSo=i(Wt),Ka=s(Wt,"P",{});var T3=n(Ka);uSo=r(T3,"The model class to instantiate is selected based on the "),Nne=s(T3,"CODE",{});var aUr=n(Nne);pSo=r(aUr,"model_type"),aUr.forEach(t),_So=r(T3,` property of the config object (either
passed as an argument or loaded from `),Dne=s(T3,"CODE",{});var sUr=n(Dne);bSo=r(sUr,"pretrained_model_name_or_path"),sUr.forEach(t),vSo=r(T3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),qne=s(T3,"CODE",{});var nUr=n(qne);TSo=r(nUr,"pretrained_model_name_or_path"),nUr.forEach(t),FSo=r(T3,":"),T3.forEach(t),CSo=i(Wt),ao=s(Wt,"UL",{});var Qt=n(ao);xT=s(Qt,"LI",{});var dwe=n(xT);Gne=s(dwe,"STRONG",{});var lUr=n(Gne);MSo=r(lUr,"hubert"),lUr.forEach(t),ESo=r(dwe," \u2014 "),rN=s(dwe,"A",{href:!0});var iUr=n(rN);ySo=r(iUr,"HubertForSequenceClassification"),iUr.forEach(t),wSo=r(dwe," (Hubert model)"),dwe.forEach(t),ASo=i(Qt),RT=s(Qt,"LI",{});var cwe=n(RT);One=s(cwe,"STRONG",{});var dUr=n(One);LSo=r(dUr,"sew"),dUr.forEach(t),BSo=r(cwe," \u2014 "),tN=s(cwe,"A",{href:!0});var cUr=n(tN);kSo=r(cUr,"SEWForSequenceClassification"),cUr.forEach(t),xSo=r(cwe," (SEW model)"),cwe.forEach(t),RSo=i(Qt),ST=s(Qt,"LI",{});var mwe=n(ST);Xne=s(mwe,"STRONG",{});var mUr=n(Xne);SSo=r(mUr,"sew-d"),mUr.forEach(t),PSo=r(mwe," \u2014 "),aN=s(mwe,"A",{href:!0});var fUr=n(aN);$So=r(fUr,"SEWDForSequenceClassification"),fUr.forEach(t),ISo=r(mwe," (SEW-D model)"),mwe.forEach(t),jSo=i(Qt),PT=s(Qt,"LI",{});var fwe=n(PT);zne=s(fwe,"STRONG",{});var gUr=n(zne);NSo=r(gUr,"unispeech"),gUr.forEach(t),DSo=r(fwe," \u2014 "),sN=s(fwe,"A",{href:!0});var hUr=n(sN);qSo=r(hUr,"UniSpeechForSequenceClassification"),hUr.forEach(t),GSo=r(fwe," (UniSpeech model)"),fwe.forEach(t),OSo=i(Qt),$T=s(Qt,"LI",{});var gwe=n($T);Vne=s(gwe,"STRONG",{});var uUr=n(Vne);XSo=r(uUr,"unispeech-sat"),uUr.forEach(t),zSo=r(gwe," \u2014 "),nN=s(gwe,"A",{href:!0});var pUr=n(nN);VSo=r(pUr,"UniSpeechSatForSequenceClassification"),pUr.forEach(t),WSo=r(gwe," (UniSpeechSat model)"),gwe.forEach(t),QSo=i(Qt),IT=s(Qt,"LI",{});var hwe=n(IT);Wne=s(hwe,"STRONG",{});var _Ur=n(Wne);HSo=r(_Ur,"wav2vec2"),_Ur.forEach(t),USo=r(hwe," \u2014 "),lN=s(hwe,"A",{href:!0});var bUr=n(lN);JSo=r(bUr,"Wav2Vec2ForSequenceClassification"),bUr.forEach(t),YSo=r(hwe," (Wav2Vec2 model)"),hwe.forEach(t),KSo=i(Qt),jT=s(Qt,"LI",{});var uwe=n(jT);Qne=s(uwe,"STRONG",{});var vUr=n(Qne);ZSo=r(vUr,"wavlm"),vUr.forEach(t),ePo=r(uwe," \u2014 "),iN=s(uwe,"A",{href:!0});var TUr=n(iN);oPo=r(TUr,"WavLMForSequenceClassification"),TUr.forEach(t),rPo=r(uwe," (WavLM model)"),uwe.forEach(t),Qt.forEach(t),tPo=i(Wt),NT=s(Wt,"P",{});var pwe=n(NT);aPo=r(pwe,"The model is set in evaluation mode by default using "),Hne=s(pwe,"CODE",{});var FUr=n(Hne);sPo=r(FUr,"model.eval()"),FUr.forEach(t),nPo=r(pwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Une=s(pwe,"CODE",{});var CUr=n(Une);lPo=r(CUr,"model.train()"),CUr.forEach(t),pwe.forEach(t),iPo=i(Wt),Jne=s(Wt,"P",{});var MUr=n(Jne);dPo=r(MUr,"Examples:"),MUr.forEach(t),cPo=i(Wt),f(Ew.$$.fragment,Wt),Wt.forEach(t),ml.forEach(t),z8e=i(d),Ld=s(d,"H2",{class:!0});var YBe=n(Ld);DT=s(YBe,"A",{id:!0,class:!0,href:!0});var EUr=n(DT);Yne=s(EUr,"SPAN",{});var yUr=n(Yne);f(yw.$$.fragment,yUr),yUr.forEach(t),EUr.forEach(t),mPo=i(YBe),Kne=s(YBe,"SPAN",{});var wUr=n(Kne);fPo=r(wUr,"AutoModelForAudioFrameClassification"),wUr.forEach(t),YBe.forEach(t),V8e=i(d),sr=s(d,"DIV",{class:!0});var gl=n(sr);f(ww.$$.fragment,gl),gPo=i(gl),Bd=s(gl,"P",{});var Tz=n(Bd);hPo=r(Tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Zne=s(Tz,"CODE",{});var AUr=n(Zne);uPo=r(AUr,"from_pretrained()"),AUr.forEach(t),pPo=r(Tz,"class method or the "),ele=s(Tz,"CODE",{});var LUr=n(ele);_Po=r(LUr,"from_config()"),LUr.forEach(t),bPo=r(Tz,`class
method.`),Tz.forEach(t),vPo=i(gl),Aw=s(gl,"P",{});var KBe=n(Aw);TPo=r(KBe,"This class cannot be instantiated directly using "),ole=s(KBe,"CODE",{});var BUr=n(ole);FPo=r(BUr,"__init__()"),BUr.forEach(t),CPo=r(KBe," (throws an error)."),KBe.forEach(t),MPo=i(gl),Kr=s(gl,"DIV",{class:!0});var hl=n(Kr);f(Lw.$$.fragment,hl),EPo=i(hl),rle=s(hl,"P",{});var kUr=n(rle);yPo=r(kUr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),kUr.forEach(t),wPo=i(hl),kd=s(hl,"P",{});var Fz=n(kd);APo=r(Fz,`Note:
Loading a model from its configuration file does `),tle=s(Fz,"STRONG",{});var xUr=n(tle);LPo=r(xUr,"not"),xUr.forEach(t),BPo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ale=s(Fz,"CODE",{});var RUr=n(ale);kPo=r(RUr,"from_pretrained()"),RUr.forEach(t),xPo=r(Fz,"to load the model weights."),Fz.forEach(t),RPo=i(hl),sle=s(hl,"P",{});var SUr=n(sle);SPo=r(SUr,"Examples:"),SUr.forEach(t),PPo=i(hl),f(Bw.$$.fragment,hl),hl.forEach(t),$Po=i(gl),ze=s(gl,"DIV",{class:!0});var Ht=n(ze);f(kw.$$.fragment,Ht),IPo=i(Ht),nle=s(Ht,"P",{});var PUr=n(nle);jPo=r(PUr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),PUr.forEach(t),NPo=i(Ht),Za=s(Ht,"P",{});var F3=n(Za);DPo=r(F3,"The model class to instantiate is selected based on the "),lle=s(F3,"CODE",{});var $Ur=n(lle);qPo=r($Ur,"model_type"),$Ur.forEach(t),GPo=r(F3,` property of the config object (either
passed as an argument or loaded from `),ile=s(F3,"CODE",{});var IUr=n(ile);OPo=r(IUr,"pretrained_model_name_or_path"),IUr.forEach(t),XPo=r(F3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),dle=s(F3,"CODE",{});var jUr=n(dle);zPo=r(jUr,"pretrained_model_name_or_path"),jUr.forEach(t),VPo=r(F3,":"),F3.forEach(t),WPo=i(Ht),xd=s(Ht,"UL",{});var Cz=n(xd);qT=s(Cz,"LI",{});var _we=n(qT);cle=s(_we,"STRONG",{});var NUr=n(cle);QPo=r(NUr,"unispeech-sat"),NUr.forEach(t),HPo=r(_we," \u2014 "),dN=s(_we,"A",{href:!0});var DUr=n(dN);UPo=r(DUr,"UniSpeechSatForAudioFrameClassification"),DUr.forEach(t),JPo=r(_we," (UniSpeechSat model)"),_we.forEach(t),YPo=i(Cz),GT=s(Cz,"LI",{});var bwe=n(GT);mle=s(bwe,"STRONG",{});var qUr=n(mle);KPo=r(qUr,"wav2vec2"),qUr.forEach(t),ZPo=r(bwe," \u2014 "),cN=s(bwe,"A",{href:!0});var GUr=n(cN);e$o=r(GUr,"Wav2Vec2ForAudioFrameClassification"),GUr.forEach(t),o$o=r(bwe," (Wav2Vec2 model)"),bwe.forEach(t),r$o=i(Cz),OT=s(Cz,"LI",{});var vwe=n(OT);fle=s(vwe,"STRONG",{});var OUr=n(fle);t$o=r(OUr,"wavlm"),OUr.forEach(t),a$o=r(vwe," \u2014 "),mN=s(vwe,"A",{href:!0});var XUr=n(mN);s$o=r(XUr,"WavLMForAudioFrameClassification"),XUr.forEach(t),n$o=r(vwe," (WavLM model)"),vwe.forEach(t),Cz.forEach(t),l$o=i(Ht),XT=s(Ht,"P",{});var Twe=n(XT);i$o=r(Twe,"The model is set in evaluation mode by default using "),gle=s(Twe,"CODE",{});var zUr=n(gle);d$o=r(zUr,"model.eval()"),zUr.forEach(t),c$o=r(Twe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),hle=s(Twe,"CODE",{});var VUr=n(hle);m$o=r(VUr,"model.train()"),VUr.forEach(t),Twe.forEach(t),f$o=i(Ht),ule=s(Ht,"P",{});var WUr=n(ule);g$o=r(WUr,"Examples:"),WUr.forEach(t),h$o=i(Ht),f(xw.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),W8e=i(d),Rd=s(d,"H2",{class:!0});var ZBe=n(Rd);zT=s(ZBe,"A",{id:!0,class:!0,href:!0});var QUr=n(zT);ple=s(QUr,"SPAN",{});var HUr=n(ple);f(Rw.$$.fragment,HUr),HUr.forEach(t),QUr.forEach(t),u$o=i(ZBe),_le=s(ZBe,"SPAN",{});var UUr=n(_le);p$o=r(UUr,"AutoModelForCTC"),UUr.forEach(t),ZBe.forEach(t),Q8e=i(d),nr=s(d,"DIV",{class:!0});var ul=n(nr);f(Sw.$$.fragment,ul),_$o=i(ul),Sd=s(ul,"P",{});var Mz=n(Sd);b$o=r(Mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),ble=s(Mz,"CODE",{});var JUr=n(ble);v$o=r(JUr,"from_pretrained()"),JUr.forEach(t),T$o=r(Mz,"class method or the "),vle=s(Mz,"CODE",{});var YUr=n(vle);F$o=r(YUr,"from_config()"),YUr.forEach(t),C$o=r(Mz,`class
method.`),Mz.forEach(t),M$o=i(ul),Pw=s(ul,"P",{});var eke=n(Pw);E$o=r(eke,"This class cannot be instantiated directly using "),Tle=s(eke,"CODE",{});var KUr=n(Tle);y$o=r(KUr,"__init__()"),KUr.forEach(t),w$o=r(eke," (throws an error)."),eke.forEach(t),A$o=i(ul),Zr=s(ul,"DIV",{class:!0});var pl=n(Zr);f($w.$$.fragment,pl),L$o=i(pl),Fle=s(pl,"P",{});var ZUr=n(Fle);B$o=r(ZUr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),ZUr.forEach(t),k$o=i(pl),Pd=s(pl,"P",{});var Ez=n(Pd);x$o=r(Ez,`Note:
Loading a model from its configuration file does `),Cle=s(Ez,"STRONG",{});var eJr=n(Cle);R$o=r(eJr,"not"),eJr.forEach(t),S$o=r(Ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),Mle=s(Ez,"CODE",{});var oJr=n(Mle);P$o=r(oJr,"from_pretrained()"),oJr.forEach(t),$$o=r(Ez,"to load the model weights."),Ez.forEach(t),I$o=i(pl),Ele=s(pl,"P",{});var rJr=n(Ele);j$o=r(rJr,"Examples:"),rJr.forEach(t),N$o=i(pl),f(Iw.$$.fragment,pl),pl.forEach(t),D$o=i(ul),Ve=s(ul,"DIV",{class:!0});var Ut=n(Ve);f(jw.$$.fragment,Ut),q$o=i(Ut),yle=s(Ut,"P",{});var tJr=n(yle);G$o=r(tJr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),tJr.forEach(t),O$o=i(Ut),es=s(Ut,"P",{});var C3=n(es);X$o=r(C3,"The model class to instantiate is selected based on the "),wle=s(C3,"CODE",{});var aJr=n(wle);z$o=r(aJr,"model_type"),aJr.forEach(t),V$o=r(C3,` property of the config object (either
passed as an argument or loaded from `),Ale=s(C3,"CODE",{});var sJr=n(Ale);W$o=r(sJr,"pretrained_model_name_or_path"),sJr.forEach(t),Q$o=r(C3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Lle=s(C3,"CODE",{});var nJr=n(Lle);H$o=r(nJr,"pretrained_model_name_or_path"),nJr.forEach(t),U$o=r(C3,":"),C3.forEach(t),J$o=i(Ut),so=s(Ut,"UL",{});var Jt=n(so);VT=s(Jt,"LI",{});var Fwe=n(VT);Ble=s(Fwe,"STRONG",{});var lJr=n(Ble);Y$o=r(lJr,"hubert"),lJr.forEach(t),K$o=r(Fwe," \u2014 "),fN=s(Fwe,"A",{href:!0});var iJr=n(fN);Z$o=r(iJr,"HubertForCTC"),iJr.forEach(t),eIo=r(Fwe," (Hubert model)"),Fwe.forEach(t),oIo=i(Jt),WT=s(Jt,"LI",{});var Cwe=n(WT);kle=s(Cwe,"STRONG",{});var dJr=n(kle);rIo=r(dJr,"sew"),dJr.forEach(t),tIo=r(Cwe," \u2014 "),gN=s(Cwe,"A",{href:!0});var cJr=n(gN);aIo=r(cJr,"SEWForCTC"),cJr.forEach(t),sIo=r(Cwe," (SEW model)"),Cwe.forEach(t),nIo=i(Jt),QT=s(Jt,"LI",{});var Mwe=n(QT);xle=s(Mwe,"STRONG",{});var mJr=n(xle);lIo=r(mJr,"sew-d"),mJr.forEach(t),iIo=r(Mwe," \u2014 "),hN=s(Mwe,"A",{href:!0});var fJr=n(hN);dIo=r(fJr,"SEWDForCTC"),fJr.forEach(t),cIo=r(Mwe," (SEW-D model)"),Mwe.forEach(t),mIo=i(Jt),HT=s(Jt,"LI",{});var Ewe=n(HT);Rle=s(Ewe,"STRONG",{});var gJr=n(Rle);fIo=r(gJr,"unispeech"),gJr.forEach(t),gIo=r(Ewe," \u2014 "),uN=s(Ewe,"A",{href:!0});var hJr=n(uN);hIo=r(hJr,"UniSpeechForCTC"),hJr.forEach(t),uIo=r(Ewe," (UniSpeech model)"),Ewe.forEach(t),pIo=i(Jt),UT=s(Jt,"LI",{});var ywe=n(UT);Sle=s(ywe,"STRONG",{});var uJr=n(Sle);_Io=r(uJr,"unispeech-sat"),uJr.forEach(t),bIo=r(ywe," \u2014 "),pN=s(ywe,"A",{href:!0});var pJr=n(pN);vIo=r(pJr,"UniSpeechSatForCTC"),pJr.forEach(t),TIo=r(ywe," (UniSpeechSat model)"),ywe.forEach(t),FIo=i(Jt),JT=s(Jt,"LI",{});var wwe=n(JT);Ple=s(wwe,"STRONG",{});var _Jr=n(Ple);CIo=r(_Jr,"wav2vec2"),_Jr.forEach(t),MIo=r(wwe," \u2014 "),_N=s(wwe,"A",{href:!0});var bJr=n(_N);EIo=r(bJr,"Wav2Vec2ForCTC"),bJr.forEach(t),yIo=r(wwe," (Wav2Vec2 model)"),wwe.forEach(t),wIo=i(Jt),YT=s(Jt,"LI",{});var Awe=n(YT);$le=s(Awe,"STRONG",{});var vJr=n($le);AIo=r(vJr,"wavlm"),vJr.forEach(t),LIo=r(Awe," \u2014 "),bN=s(Awe,"A",{href:!0});var TJr=n(bN);BIo=r(TJr,"WavLMForCTC"),TJr.forEach(t),kIo=r(Awe," (WavLM model)"),Awe.forEach(t),Jt.forEach(t),xIo=i(Ut),KT=s(Ut,"P",{});var Lwe=n(KT);RIo=r(Lwe,"The model is set in evaluation mode by default using "),Ile=s(Lwe,"CODE",{});var FJr=n(Ile);SIo=r(FJr,"model.eval()"),FJr.forEach(t),PIo=r(Lwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),jle=s(Lwe,"CODE",{});var CJr=n(jle);$Io=r(CJr,"model.train()"),CJr.forEach(t),Lwe.forEach(t),IIo=i(Ut),Nle=s(Ut,"P",{});var MJr=n(Nle);jIo=r(MJr,"Examples:"),MJr.forEach(t),NIo=i(Ut),f(Nw.$$.fragment,Ut),Ut.forEach(t),ul.forEach(t),H8e=i(d),$d=s(d,"H2",{class:!0});var oke=n($d);ZT=s(oke,"A",{id:!0,class:!0,href:!0});var EJr=n(ZT);Dle=s(EJr,"SPAN",{});var yJr=n(Dle);f(Dw.$$.fragment,yJr),yJr.forEach(t),EJr.forEach(t),DIo=i(oke),qle=s(oke,"SPAN",{});var wJr=n(qle);qIo=r(wJr,"AutoModelForSpeechSeq2Seq"),wJr.forEach(t),oke.forEach(t),U8e=i(d),lr=s(d,"DIV",{class:!0});var _l=n(lr);f(qw.$$.fragment,_l),GIo=i(_l),Id=s(_l,"P",{});var yz=n(Id);OIo=r(yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Gle=s(yz,"CODE",{});var AJr=n(Gle);XIo=r(AJr,"from_pretrained()"),AJr.forEach(t),zIo=r(yz,"class method or the "),Ole=s(yz,"CODE",{});var LJr=n(Ole);VIo=r(LJr,"from_config()"),LJr.forEach(t),WIo=r(yz,`class
method.`),yz.forEach(t),QIo=i(_l),Gw=s(_l,"P",{});var rke=n(Gw);HIo=r(rke,"This class cannot be instantiated directly using "),Xle=s(rke,"CODE",{});var BJr=n(Xle);UIo=r(BJr,"__init__()"),BJr.forEach(t),JIo=r(rke," (throws an error)."),rke.forEach(t),YIo=i(_l),et=s(_l,"DIV",{class:!0});var bl=n(et);f(Ow.$$.fragment,bl),KIo=i(bl),zle=s(bl,"P",{});var kJr=n(zle);ZIo=r(kJr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),kJr.forEach(t),ejo=i(bl),jd=s(bl,"P",{});var wz=n(jd);ojo=r(wz,`Note:
Loading a model from its configuration file does `),Vle=s(wz,"STRONG",{});var xJr=n(Vle);rjo=r(xJr,"not"),xJr.forEach(t),tjo=r(wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wle=s(wz,"CODE",{});var RJr=n(Wle);ajo=r(RJr,"from_pretrained()"),RJr.forEach(t),sjo=r(wz,"to load the model weights."),wz.forEach(t),njo=i(bl),Qle=s(bl,"P",{});var SJr=n(Qle);ljo=r(SJr,"Examples:"),SJr.forEach(t),ijo=i(bl),f(Xw.$$.fragment,bl),bl.forEach(t),djo=i(_l),We=s(_l,"DIV",{class:!0});var Yt=n(We);f(zw.$$.fragment,Yt),cjo=i(Yt),Hle=s(Yt,"P",{});var PJr=n(Hle);mjo=r(PJr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),PJr.forEach(t),fjo=i(Yt),os=s(Yt,"P",{});var M3=n(os);gjo=r(M3,"The model class to instantiate is selected based on the "),Ule=s(M3,"CODE",{});var $Jr=n(Ule);hjo=r($Jr,"model_type"),$Jr.forEach(t),ujo=r(M3,` property of the config object (either
passed as an argument or loaded from `),Jle=s(M3,"CODE",{});var IJr=n(Jle);pjo=r(IJr,"pretrained_model_name_or_path"),IJr.forEach(t),_jo=r(M3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yle=s(M3,"CODE",{});var jJr=n(Yle);bjo=r(jJr,"pretrained_model_name_or_path"),jJr.forEach(t),vjo=r(M3,":"),M3.forEach(t),Tjo=i(Yt),Vw=s(Yt,"UL",{});var tke=n(Vw);e1=s(tke,"LI",{});var Bwe=n(e1);Kle=s(Bwe,"STRONG",{});var NJr=n(Kle);Fjo=r(NJr,"speech-encoder-decoder"),NJr.forEach(t),Cjo=r(Bwe," \u2014 "),vN=s(Bwe,"A",{href:!0});var DJr=n(vN);Mjo=r(DJr,"SpeechEncoderDecoderModel"),DJr.forEach(t),Ejo=r(Bwe," (Speech Encoder decoder model)"),Bwe.forEach(t),yjo=i(tke),o1=s(tke,"LI",{});var kwe=n(o1);Zle=s(kwe,"STRONG",{});var qJr=n(Zle);wjo=r(qJr,"speech_to_text"),qJr.forEach(t),Ajo=r(kwe," \u2014 "),TN=s(kwe,"A",{href:!0});var GJr=n(TN);Ljo=r(GJr,"Speech2TextForConditionalGeneration"),GJr.forEach(t),Bjo=r(kwe," (Speech2Text model)"),kwe.forEach(t),tke.forEach(t),kjo=i(Yt),r1=s(Yt,"P",{});var xwe=n(r1);xjo=r(xwe,"The model is set in evaluation mode by default using "),eie=s(xwe,"CODE",{});var OJr=n(eie);Rjo=r(OJr,"model.eval()"),OJr.forEach(t),Sjo=r(xwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),oie=s(xwe,"CODE",{});var XJr=n(oie);Pjo=r(XJr,"model.train()"),XJr.forEach(t),xwe.forEach(t),$jo=i(Yt),rie=s(Yt,"P",{});var zJr=n(rie);Ijo=r(zJr,"Examples:"),zJr.forEach(t),jjo=i(Yt),f(Ww.$$.fragment,Yt),Yt.forEach(t),_l.forEach(t),J8e=i(d),Nd=s(d,"H2",{class:!0});var ake=n(Nd);t1=s(ake,"A",{id:!0,class:!0,href:!0});var VJr=n(t1);tie=s(VJr,"SPAN",{});var WJr=n(tie);f(Qw.$$.fragment,WJr),WJr.forEach(t),VJr.forEach(t),Njo=i(ake),aie=s(ake,"SPAN",{});var QJr=n(aie);Djo=r(QJr,"AutoModelForAudioXVector"),QJr.forEach(t),ake.forEach(t),Y8e=i(d),ir=s(d,"DIV",{class:!0});var vl=n(ir);f(Hw.$$.fragment,vl),qjo=i(vl),Dd=s(vl,"P",{});var Az=n(Dd);Gjo=r(Az,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),sie=s(Az,"CODE",{});var HJr=n(sie);Ojo=r(HJr,"from_pretrained()"),HJr.forEach(t),Xjo=r(Az,"class method or the "),nie=s(Az,"CODE",{});var UJr=n(nie);zjo=r(UJr,"from_config()"),UJr.forEach(t),Vjo=r(Az,`class
method.`),Az.forEach(t),Wjo=i(vl),Uw=s(vl,"P",{});var ske=n(Uw);Qjo=r(ske,"This class cannot be instantiated directly using "),lie=s(ske,"CODE",{});var JJr=n(lie);Hjo=r(JJr,"__init__()"),JJr.forEach(t),Ujo=r(ske," (throws an error)."),ske.forEach(t),Jjo=i(vl),ot=s(vl,"DIV",{class:!0});var Tl=n(ot);f(Jw.$$.fragment,Tl),Yjo=i(Tl),iie=s(Tl,"P",{});var YJr=n(iie);Kjo=r(YJr,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),YJr.forEach(t),Zjo=i(Tl),qd=s(Tl,"P",{});var Lz=n(qd);eNo=r(Lz,`Note:
Loading a model from its configuration file does `),die=s(Lz,"STRONG",{});var KJr=n(die);oNo=r(KJr,"not"),KJr.forEach(t),rNo=r(Lz,` load the model weights. It only affects the
model\u2019s configuration. Use `),cie=s(Lz,"CODE",{});var ZJr=n(cie);tNo=r(ZJr,"from_pretrained()"),ZJr.forEach(t),aNo=r(Lz,"to load the model weights."),Lz.forEach(t),sNo=i(Tl),mie=s(Tl,"P",{});var eYr=n(mie);nNo=r(eYr,"Examples:"),eYr.forEach(t),lNo=i(Tl),f(Yw.$$.fragment,Tl),Tl.forEach(t),iNo=i(vl),Qe=s(vl,"DIV",{class:!0});var Kt=n(Qe);f(Kw.$$.fragment,Kt),dNo=i(Kt),fie=s(Kt,"P",{});var oYr=n(fie);cNo=r(oYr,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),oYr.forEach(t),mNo=i(Kt),rs=s(Kt,"P",{});var E3=n(rs);fNo=r(E3,"The model class to instantiate is selected based on the "),gie=s(E3,"CODE",{});var rYr=n(gie);gNo=r(rYr,"model_type"),rYr.forEach(t),hNo=r(E3,` property of the config object (either
passed as an argument or loaded from `),hie=s(E3,"CODE",{});var tYr=n(hie);uNo=r(tYr,"pretrained_model_name_or_path"),tYr.forEach(t),pNo=r(E3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),uie=s(E3,"CODE",{});var aYr=n(uie);_No=r(aYr,"pretrained_model_name_or_path"),aYr.forEach(t),bNo=r(E3,":"),E3.forEach(t),vNo=i(Kt),Gd=s(Kt,"UL",{});var Bz=n(Gd);a1=s(Bz,"LI",{});var Rwe=n(a1);pie=s(Rwe,"STRONG",{});var sYr=n(pie);TNo=r(sYr,"unispeech-sat"),sYr.forEach(t),FNo=r(Rwe," \u2014 "),FN=s(Rwe,"A",{href:!0});var nYr=n(FN);CNo=r(nYr,"UniSpeechSatForXVector"),nYr.forEach(t),MNo=r(Rwe," (UniSpeechSat model)"),Rwe.forEach(t),ENo=i(Bz),s1=s(Bz,"LI",{});var Swe=n(s1);_ie=s(Swe,"STRONG",{});var lYr=n(_ie);yNo=r(lYr,"wav2vec2"),lYr.forEach(t),wNo=r(Swe," \u2014 "),CN=s(Swe,"A",{href:!0});var iYr=n(CN);ANo=r(iYr,"Wav2Vec2ForXVector"),iYr.forEach(t),LNo=r(Swe," (Wav2Vec2 model)"),Swe.forEach(t),BNo=i(Bz),n1=s(Bz,"LI",{});var Pwe=n(n1);bie=s(Pwe,"STRONG",{});var dYr=n(bie);kNo=r(dYr,"wavlm"),dYr.forEach(t),xNo=r(Pwe," \u2014 "),MN=s(Pwe,"A",{href:!0});var cYr=n(MN);RNo=r(cYr,"WavLMForXVector"),cYr.forEach(t),SNo=r(Pwe," (WavLM model)"),Pwe.forEach(t),Bz.forEach(t),PNo=i(Kt),l1=s(Kt,"P",{});var $we=n(l1);$No=r($we,"The model is set in evaluation mode by default using "),vie=s($we,"CODE",{});var mYr=n(vie);INo=r(mYr,"model.eval()"),mYr.forEach(t),jNo=r($we,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tie=s($we,"CODE",{});var fYr=n(Tie);NNo=r(fYr,"model.train()"),fYr.forEach(t),$we.forEach(t),DNo=i(Kt),Fie=s(Kt,"P",{});var gYr=n(Fie);qNo=r(gYr,"Examples:"),gYr.forEach(t),GNo=i(Kt),f(Zw.$$.fragment,Kt),Kt.forEach(t),vl.forEach(t),K8e=i(d),Od=s(d,"H2",{class:!0});var nke=n(Od);i1=s(nke,"A",{id:!0,class:!0,href:!0});var hYr=n(i1);Cie=s(hYr,"SPAN",{});var uYr=n(Cie);f(eA.$$.fragment,uYr),uYr.forEach(t),hYr.forEach(t),ONo=i(nke),Mie=s(nke,"SPAN",{});var pYr=n(Mie);XNo=r(pYr,"AutoModelForMaskedImageModeling"),pYr.forEach(t),nke.forEach(t),Z8e=i(d),dr=s(d,"DIV",{class:!0});var Fl=n(dr);f(oA.$$.fragment,Fl),zNo=i(Fl),Xd=s(Fl,"P",{});var kz=n(Xd);VNo=r(kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Eie=s(kz,"CODE",{});var _Yr=n(Eie);WNo=r(_Yr,"from_pretrained()"),_Yr.forEach(t),QNo=r(kz,"class method or the "),yie=s(kz,"CODE",{});var bYr=n(yie);HNo=r(bYr,"from_config()"),bYr.forEach(t),UNo=r(kz,`class
method.`),kz.forEach(t),JNo=i(Fl),rA=s(Fl,"P",{});var lke=n(rA);YNo=r(lke,"This class cannot be instantiated directly using "),wie=s(lke,"CODE",{});var vYr=n(wie);KNo=r(vYr,"__init__()"),vYr.forEach(t),ZNo=r(lke," (throws an error)."),lke.forEach(t),eDo=i(Fl),rt=s(Fl,"DIV",{class:!0});var Cl=n(rt);f(tA.$$.fragment,Cl),oDo=i(Cl),Aie=s(Cl,"P",{});var TYr=n(Aie);rDo=r(TYr,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),TYr.forEach(t),tDo=i(Cl),zd=s(Cl,"P",{});var xz=n(zd);aDo=r(xz,`Note:
Loading a model from its configuration file does `),Lie=s(xz,"STRONG",{});var FYr=n(Lie);sDo=r(FYr,"not"),FYr.forEach(t),nDo=r(xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bie=s(xz,"CODE",{});var CYr=n(Bie);lDo=r(CYr,"from_pretrained()"),CYr.forEach(t),iDo=r(xz,"to load the model weights."),xz.forEach(t),dDo=i(Cl),kie=s(Cl,"P",{});var MYr=n(kie);cDo=r(MYr,"Examples:"),MYr.forEach(t),mDo=i(Cl),f(aA.$$.fragment,Cl),Cl.forEach(t),fDo=i(Fl),He=s(Fl,"DIV",{class:!0});var Zt=n(He);f(sA.$$.fragment,Zt),gDo=i(Zt),xie=s(Zt,"P",{});var EYr=n(xie);hDo=r(EYr,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),EYr.forEach(t),uDo=i(Zt),ts=s(Zt,"P",{});var y3=n(ts);pDo=r(y3,"The model class to instantiate is selected based on the "),Rie=s(y3,"CODE",{});var yYr=n(Rie);_Do=r(yYr,"model_type"),yYr.forEach(t),bDo=r(y3,` property of the config object (either
passed as an argument or loaded from `),Sie=s(y3,"CODE",{});var wYr=n(Sie);vDo=r(wYr,"pretrained_model_name_or_path"),wYr.forEach(t),TDo=r(y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pie=s(y3,"CODE",{});var AYr=n(Pie);FDo=r(AYr,"pretrained_model_name_or_path"),AYr.forEach(t),CDo=r(y3,":"),y3.forEach(t),MDo=i(Zt),Vd=s(Zt,"UL",{});var Rz=n(Vd);d1=s(Rz,"LI",{});var Iwe=n(d1);$ie=s(Iwe,"STRONG",{});var LYr=n($ie);EDo=r(LYr,"deit"),LYr.forEach(t),yDo=r(Iwe," \u2014 "),EN=s(Iwe,"A",{href:!0});var BYr=n(EN);wDo=r(BYr,"DeiTForMaskedImageModeling"),BYr.forEach(t),ADo=r(Iwe," (DeiT model)"),Iwe.forEach(t),LDo=i(Rz),c1=s(Rz,"LI",{});var jwe=n(c1);Iie=s(jwe,"STRONG",{});var kYr=n(Iie);BDo=r(kYr,"swin"),kYr.forEach(t),kDo=r(jwe," \u2014 "),yN=s(jwe,"A",{href:!0});var xYr=n(yN);xDo=r(xYr,"SwinForMaskedImageModeling"),xYr.forEach(t),RDo=r(jwe," (Swin model)"),jwe.forEach(t),SDo=i(Rz),m1=s(Rz,"LI",{});var Nwe=n(m1);jie=s(Nwe,"STRONG",{});var RYr=n(jie);PDo=r(RYr,"vit"),RYr.forEach(t),$Do=r(Nwe," \u2014 "),wN=s(Nwe,"A",{href:!0});var SYr=n(wN);IDo=r(SYr,"ViTForMaskedImageModeling"),SYr.forEach(t),jDo=r(Nwe," (ViT model)"),Nwe.forEach(t),Rz.forEach(t),NDo=i(Zt),f1=s(Zt,"P",{});var Dwe=n(f1);DDo=r(Dwe,"The model is set in evaluation mode by default using "),Nie=s(Dwe,"CODE",{});var PYr=n(Nie);qDo=r(PYr,"model.eval()"),PYr.forEach(t),GDo=r(Dwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Die=s(Dwe,"CODE",{});var $Yr=n(Die);ODo=r($Yr,"model.train()"),$Yr.forEach(t),Dwe.forEach(t),XDo=i(Zt),qie=s(Zt,"P",{});var IYr=n(qie);zDo=r(IYr,"Examples:"),IYr.forEach(t),VDo=i(Zt),f(nA.$$.fragment,Zt),Zt.forEach(t),Fl.forEach(t),e9e=i(d),Wd=s(d,"H2",{class:!0});var ike=n(Wd);g1=s(ike,"A",{id:!0,class:!0,href:!0});var jYr=n(g1);Gie=s(jYr,"SPAN",{});var NYr=n(Gie);f(lA.$$.fragment,NYr),NYr.forEach(t),jYr.forEach(t),WDo=i(ike),Oie=s(ike,"SPAN",{});var DYr=n(Oie);QDo=r(DYr,"AutoModelForObjectDetection"),DYr.forEach(t),ike.forEach(t),o9e=i(d),cr=s(d,"DIV",{class:!0});var Ml=n(cr);f(iA.$$.fragment,Ml),HDo=i(Ml),Qd=s(Ml,"P",{});var Sz=n(Qd);UDo=r(Sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Xie=s(Sz,"CODE",{});var qYr=n(Xie);JDo=r(qYr,"from_pretrained()"),qYr.forEach(t),YDo=r(Sz,"class method or the "),zie=s(Sz,"CODE",{});var GYr=n(zie);KDo=r(GYr,"from_config()"),GYr.forEach(t),ZDo=r(Sz,`class
method.`),Sz.forEach(t),eqo=i(Ml),dA=s(Ml,"P",{});var dke=n(dA);oqo=r(dke,"This class cannot be instantiated directly using "),Vie=s(dke,"CODE",{});var OYr=n(Vie);rqo=r(OYr,"__init__()"),OYr.forEach(t),tqo=r(dke," (throws an error)."),dke.forEach(t),aqo=i(Ml),tt=s(Ml,"DIV",{class:!0});var El=n(tt);f(cA.$$.fragment,El),sqo=i(El),Wie=s(El,"P",{});var XYr=n(Wie);nqo=r(XYr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),XYr.forEach(t),lqo=i(El),Hd=s(El,"P",{});var Pz=n(Hd);iqo=r(Pz,`Note:
Loading a model from its configuration file does `),Qie=s(Pz,"STRONG",{});var zYr=n(Qie);dqo=r(zYr,"not"),zYr.forEach(t),cqo=r(Pz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hie=s(Pz,"CODE",{});var VYr=n(Hie);mqo=r(VYr,"from_pretrained()"),VYr.forEach(t),fqo=r(Pz,"to load the model weights."),Pz.forEach(t),gqo=i(El),Uie=s(El,"P",{});var WYr=n(Uie);hqo=r(WYr,"Examples:"),WYr.forEach(t),uqo=i(El),f(mA.$$.fragment,El),El.forEach(t),pqo=i(Ml),Ue=s(Ml,"DIV",{class:!0});var ea=n(Ue);f(fA.$$.fragment,ea),_qo=i(ea),Jie=s(ea,"P",{});var QYr=n(Jie);bqo=r(QYr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),QYr.forEach(t),vqo=i(ea),as=s(ea,"P",{});var w3=n(as);Tqo=r(w3,"The model class to instantiate is selected based on the "),Yie=s(w3,"CODE",{});var HYr=n(Yie);Fqo=r(HYr,"model_type"),HYr.forEach(t),Cqo=r(w3,` property of the config object (either
passed as an argument or loaded from `),Kie=s(w3,"CODE",{});var UYr=n(Kie);Mqo=r(UYr,"pretrained_model_name_or_path"),UYr.forEach(t),Eqo=r(w3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zie=s(w3,"CODE",{});var JYr=n(Zie);yqo=r(JYr,"pretrained_model_name_or_path"),JYr.forEach(t),wqo=r(w3,":"),w3.forEach(t),Aqo=i(ea),ede=s(ea,"UL",{});var YYr=n(ede);h1=s(YYr,"LI",{});var qwe=n(h1);ode=s(qwe,"STRONG",{});var KYr=n(ode);Lqo=r(KYr,"detr"),KYr.forEach(t),Bqo=r(qwe," \u2014 "),AN=s(qwe,"A",{href:!0});var ZYr=n(AN);kqo=r(ZYr,"DetrForObjectDetection"),ZYr.forEach(t),xqo=r(qwe," (DETR model)"),qwe.forEach(t),YYr.forEach(t),Rqo=i(ea),u1=s(ea,"P",{});var Gwe=n(u1);Sqo=r(Gwe,"The model is set in evaluation mode by default using "),rde=s(Gwe,"CODE",{});var eKr=n(rde);Pqo=r(eKr,"model.eval()"),eKr.forEach(t),$qo=r(Gwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tde=s(Gwe,"CODE",{});var oKr=n(tde);Iqo=r(oKr,"model.train()"),oKr.forEach(t),Gwe.forEach(t),jqo=i(ea),ade=s(ea,"P",{});var rKr=n(ade);Nqo=r(rKr,"Examples:"),rKr.forEach(t),Dqo=i(ea),f(gA.$$.fragment,ea),ea.forEach(t),Ml.forEach(t),r9e=i(d),Ud=s(d,"H2",{class:!0});var cke=n(Ud);p1=s(cke,"A",{id:!0,class:!0,href:!0});var tKr=n(p1);sde=s(tKr,"SPAN",{});var aKr=n(sde);f(hA.$$.fragment,aKr),aKr.forEach(t),tKr.forEach(t),qqo=i(cke),nde=s(cke,"SPAN",{});var sKr=n(nde);Gqo=r(sKr,"AutoModelForImageSegmentation"),sKr.forEach(t),cke.forEach(t),t9e=i(d),mr=s(d,"DIV",{class:!0});var yl=n(mr);f(uA.$$.fragment,yl),Oqo=i(yl),Jd=s(yl,"P",{});var $z=n(Jd);Xqo=r($z,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),lde=s($z,"CODE",{});var nKr=n(lde);zqo=r(nKr,"from_pretrained()"),nKr.forEach(t),Vqo=r($z,"class method or the "),ide=s($z,"CODE",{});var lKr=n(ide);Wqo=r(lKr,"from_config()"),lKr.forEach(t),Qqo=r($z,`class
method.`),$z.forEach(t),Hqo=i(yl),pA=s(yl,"P",{});var mke=n(pA);Uqo=r(mke,"This class cannot be instantiated directly using "),dde=s(mke,"CODE",{});var iKr=n(dde);Jqo=r(iKr,"__init__()"),iKr.forEach(t),Yqo=r(mke," (throws an error)."),mke.forEach(t),Kqo=i(yl),at=s(yl,"DIV",{class:!0});var wl=n(at);f(_A.$$.fragment,wl),Zqo=i(wl),cde=s(wl,"P",{});var dKr=n(cde);eGo=r(dKr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),dKr.forEach(t),oGo=i(wl),Yd=s(wl,"P",{});var Iz=n(Yd);rGo=r(Iz,`Note:
Loading a model from its configuration file does `),mde=s(Iz,"STRONG",{});var cKr=n(mde);tGo=r(cKr,"not"),cKr.forEach(t),aGo=r(Iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),fde=s(Iz,"CODE",{});var mKr=n(fde);sGo=r(mKr,"from_pretrained()"),mKr.forEach(t),nGo=r(Iz,"to load the model weights."),Iz.forEach(t),lGo=i(wl),gde=s(wl,"P",{});var fKr=n(gde);iGo=r(fKr,"Examples:"),fKr.forEach(t),dGo=i(wl),f(bA.$$.fragment,wl),wl.forEach(t),cGo=i(yl),Je=s(yl,"DIV",{class:!0});var oa=n(Je);f(vA.$$.fragment,oa),mGo=i(oa),hde=s(oa,"P",{});var gKr=n(hde);fGo=r(gKr,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),gKr.forEach(t),gGo=i(oa),ss=s(oa,"P",{});var A3=n(ss);hGo=r(A3,"The model class to instantiate is selected based on the "),ude=s(A3,"CODE",{});var hKr=n(ude);uGo=r(hKr,"model_type"),hKr.forEach(t),pGo=r(A3,` property of the config object (either
passed as an argument or loaded from `),pde=s(A3,"CODE",{});var uKr=n(pde);_Go=r(uKr,"pretrained_model_name_or_path"),uKr.forEach(t),bGo=r(A3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_de=s(A3,"CODE",{});var pKr=n(_de);vGo=r(pKr,"pretrained_model_name_or_path"),pKr.forEach(t),TGo=r(A3,":"),A3.forEach(t),FGo=i(oa),bde=s(oa,"UL",{});var _Kr=n(bde);_1=s(_Kr,"LI",{});var Owe=n(_1);vde=s(Owe,"STRONG",{});var bKr=n(vde);CGo=r(bKr,"detr"),bKr.forEach(t),MGo=r(Owe," \u2014 "),LN=s(Owe,"A",{href:!0});var vKr=n(LN);EGo=r(vKr,"DetrForSegmentation"),vKr.forEach(t),yGo=r(Owe," (DETR model)"),Owe.forEach(t),_Kr.forEach(t),wGo=i(oa),b1=s(oa,"P",{});var Xwe=n(b1);AGo=r(Xwe,"The model is set in evaluation mode by default using "),Tde=s(Xwe,"CODE",{});var TKr=n(Tde);LGo=r(TKr,"model.eval()"),TKr.forEach(t),BGo=r(Xwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fde=s(Xwe,"CODE",{});var FKr=n(Fde);kGo=r(FKr,"model.train()"),FKr.forEach(t),Xwe.forEach(t),xGo=i(oa),Cde=s(oa,"P",{});var CKr=n(Cde);RGo=r(CKr,"Examples:"),CKr.forEach(t),SGo=i(oa),f(TA.$$.fragment,oa),oa.forEach(t),yl.forEach(t),a9e=i(d),Kd=s(d,"H2",{class:!0});var fke=n(Kd);v1=s(fke,"A",{id:!0,class:!0,href:!0});var MKr=n(v1);Mde=s(MKr,"SPAN",{});var EKr=n(Mde);f(FA.$$.fragment,EKr),EKr.forEach(t),MKr.forEach(t),PGo=i(fke),Ede=s(fke,"SPAN",{});var yKr=n(Ede);$Go=r(yKr,"AutoModelForSemanticSegmentation"),yKr.forEach(t),fke.forEach(t),s9e=i(d),fr=s(d,"DIV",{class:!0});var Al=n(fr);f(CA.$$.fragment,Al),IGo=i(Al),Zd=s(Al,"P",{});var jz=n(Zd);jGo=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),yde=s(jz,"CODE",{});var wKr=n(yde);NGo=r(wKr,"from_pretrained()"),wKr.forEach(t),DGo=r(jz,"class method or the "),wde=s(jz,"CODE",{});var AKr=n(wde);qGo=r(AKr,"from_config()"),AKr.forEach(t),GGo=r(jz,`class
method.`),jz.forEach(t),OGo=i(Al),MA=s(Al,"P",{});var gke=n(MA);XGo=r(gke,"This class cannot be instantiated directly using "),Ade=s(gke,"CODE",{});var LKr=n(Ade);zGo=r(LKr,"__init__()"),LKr.forEach(t),VGo=r(gke," (throws an error)."),gke.forEach(t),WGo=i(Al),st=s(Al,"DIV",{class:!0});var Ll=n(st);f(EA.$$.fragment,Ll),QGo=i(Ll),Lde=s(Ll,"P",{});var BKr=n(Lde);HGo=r(BKr,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),BKr.forEach(t),UGo=i(Ll),ec=s(Ll,"P",{});var Nz=n(ec);JGo=r(Nz,`Note:
Loading a model from its configuration file does `),Bde=s(Nz,"STRONG",{});var kKr=n(Bde);YGo=r(kKr,"not"),kKr.forEach(t),KGo=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),kde=s(Nz,"CODE",{});var xKr=n(kde);ZGo=r(xKr,"from_pretrained()"),xKr.forEach(t),eOo=r(Nz,"to load the model weights."),Nz.forEach(t),oOo=i(Ll),xde=s(Ll,"P",{});var RKr=n(xde);rOo=r(RKr,"Examples:"),RKr.forEach(t),tOo=i(Ll),f(yA.$$.fragment,Ll),Ll.forEach(t),aOo=i(Al),Ye=s(Al,"DIV",{class:!0});var ra=n(Ye);f(wA.$$.fragment,ra),sOo=i(ra),Rde=s(ra,"P",{});var SKr=n(Rde);nOo=r(SKr,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),SKr.forEach(t),lOo=i(ra),ns=s(ra,"P",{});var L3=n(ns);iOo=r(L3,"The model class to instantiate is selected based on the "),Sde=s(L3,"CODE",{});var PKr=n(Sde);dOo=r(PKr,"model_type"),PKr.forEach(t),cOo=r(L3,` property of the config object (either
passed as an argument or loaded from `),Pde=s(L3,"CODE",{});var $Kr=n(Pde);mOo=r($Kr,"pretrained_model_name_or_path"),$Kr.forEach(t),fOo=r(L3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$de=s(L3,"CODE",{});var IKr=n($de);gOo=r(IKr,"pretrained_model_name_or_path"),IKr.forEach(t),hOo=r(L3,":"),L3.forEach(t),uOo=i(ra),AA=s(ra,"UL",{});var hke=n(AA);T1=s(hke,"LI",{});var zwe=n(T1);Ide=s(zwe,"STRONG",{});var jKr=n(Ide);pOo=r(jKr,"beit"),jKr.forEach(t),_Oo=r(zwe," \u2014 "),BN=s(zwe,"A",{href:!0});var NKr=n(BN);bOo=r(NKr,"BeitForSemanticSegmentation"),NKr.forEach(t),vOo=r(zwe," (BEiT model)"),zwe.forEach(t),TOo=i(hke),F1=s(hke,"LI",{});var Vwe=n(F1);jde=s(Vwe,"STRONG",{});var DKr=n(jde);FOo=r(DKr,"segformer"),DKr.forEach(t),COo=r(Vwe," \u2014 "),kN=s(Vwe,"A",{href:!0});var qKr=n(kN);MOo=r(qKr,"SegformerForSemanticSegmentation"),qKr.forEach(t),EOo=r(Vwe," (SegFormer model)"),Vwe.forEach(t),hke.forEach(t),yOo=i(ra),C1=s(ra,"P",{});var Wwe=n(C1);wOo=r(Wwe,"The model is set in evaluation mode by default using "),Nde=s(Wwe,"CODE",{});var GKr=n(Nde);AOo=r(GKr,"model.eval()"),GKr.forEach(t),LOo=r(Wwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Dde=s(Wwe,"CODE",{});var OKr=n(Dde);BOo=r(OKr,"model.train()"),OKr.forEach(t),Wwe.forEach(t),kOo=i(ra),qde=s(ra,"P",{});var XKr=n(qde);xOo=r(XKr,"Examples:"),XKr.forEach(t),ROo=i(ra),f(LA.$$.fragment,ra),ra.forEach(t),Al.forEach(t),n9e=i(d),oc=s(d,"H2",{class:!0});var uke=n(oc);M1=s(uke,"A",{id:!0,class:!0,href:!0});var zKr=n(M1);Gde=s(zKr,"SPAN",{});var VKr=n(Gde);f(BA.$$.fragment,VKr),VKr.forEach(t),zKr.forEach(t),SOo=i(uke),Ode=s(uke,"SPAN",{});var WKr=n(Ode);POo=r(WKr,"TFAutoModel"),WKr.forEach(t),uke.forEach(t),l9e=i(d),gr=s(d,"DIV",{class:!0});var Bl=n(gr);f(kA.$$.fragment,Bl),$Oo=i(Bl),rc=s(Bl,"P",{});var Dz=n(rc);IOo=r(Dz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Xde=s(Dz,"CODE",{});var QKr=n(Xde);jOo=r(QKr,"from_pretrained()"),QKr.forEach(t),NOo=r(Dz,"class method or the "),zde=s(Dz,"CODE",{});var HKr=n(zde);DOo=r(HKr,"from_config()"),HKr.forEach(t),qOo=r(Dz,`class
method.`),Dz.forEach(t),GOo=i(Bl),xA=s(Bl,"P",{});var pke=n(xA);OOo=r(pke,"This class cannot be instantiated directly using "),Vde=s(pke,"CODE",{});var UKr=n(Vde);XOo=r(UKr,"__init__()"),UKr.forEach(t),zOo=r(pke," (throws an error)."),pke.forEach(t),VOo=i(Bl),nt=s(Bl,"DIV",{class:!0});var kl=n(nt);f(RA.$$.fragment,kl),WOo=i(kl),Wde=s(kl,"P",{});var JKr=n(Wde);QOo=r(JKr,"Instantiates one of the base model classes of the library from a configuration."),JKr.forEach(t),HOo=i(kl),tc=s(kl,"P",{});var qz=n(tc);UOo=r(qz,`Note:
Loading a model from its configuration file does `),Qde=s(qz,"STRONG",{});var YKr=n(Qde);JOo=r(YKr,"not"),YKr.forEach(t),YOo=r(qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hde=s(qz,"CODE",{});var KKr=n(Hde);KOo=r(KKr,"from_pretrained()"),KKr.forEach(t),ZOo=r(qz,"to load the model weights."),qz.forEach(t),eXo=i(kl),Ude=s(kl,"P",{});var ZKr=n(Ude);oXo=r(ZKr,"Examples:"),ZKr.forEach(t),rXo=i(kl),f(SA.$$.fragment,kl),kl.forEach(t),tXo=i(Bl),go=s(Bl,"DIV",{class:!0});var ca=n(go);f(PA.$$.fragment,ca),aXo=i(ca),Jde=s(ca,"P",{});var eZr=n(Jde);sXo=r(eZr,"Instantiate one of the base model classes of the library from a pretrained model."),eZr.forEach(t),nXo=i(ca),ls=s(ca,"P",{});var B3=n(ls);lXo=r(B3,"The model class to instantiate is selected based on the "),Yde=s(B3,"CODE",{});var oZr=n(Yde);iXo=r(oZr,"model_type"),oZr.forEach(t),dXo=r(B3,` property of the config object (either
passed as an argument or loaded from `),Kde=s(B3,"CODE",{});var rZr=n(Kde);cXo=r(rZr,"pretrained_model_name_or_path"),rZr.forEach(t),mXo=r(B3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zde=s(B3,"CODE",{});var tZr=n(Zde);fXo=r(tZr,"pretrained_model_name_or_path"),tZr.forEach(t),gXo=r(B3,":"),B3.forEach(t),hXo=i(ca),B=s(ca,"UL",{});var k=n(B);E1=s(k,"LI",{});var Qwe=n(E1);ece=s(Qwe,"STRONG",{});var aZr=n(ece);uXo=r(aZr,"albert"),aZr.forEach(t),pXo=r(Qwe," \u2014 "),xN=s(Qwe,"A",{href:!0});var sZr=n(xN);_Xo=r(sZr,"TFAlbertModel"),sZr.forEach(t),bXo=r(Qwe," (ALBERT model)"),Qwe.forEach(t),vXo=i(k),y1=s(k,"LI",{});var Hwe=n(y1);oce=s(Hwe,"STRONG",{});var nZr=n(oce);TXo=r(nZr,"bart"),nZr.forEach(t),FXo=r(Hwe," \u2014 "),RN=s(Hwe,"A",{href:!0});var lZr=n(RN);CXo=r(lZr,"TFBartModel"),lZr.forEach(t),MXo=r(Hwe," (BART model)"),Hwe.forEach(t),EXo=i(k),w1=s(k,"LI",{});var Uwe=n(w1);rce=s(Uwe,"STRONG",{});var iZr=n(rce);yXo=r(iZr,"bert"),iZr.forEach(t),wXo=r(Uwe," \u2014 "),SN=s(Uwe,"A",{href:!0});var dZr=n(SN);AXo=r(dZr,"TFBertModel"),dZr.forEach(t),LXo=r(Uwe," (BERT model)"),Uwe.forEach(t),BXo=i(k),A1=s(k,"LI",{});var Jwe=n(A1);tce=s(Jwe,"STRONG",{});var cZr=n(tce);kXo=r(cZr,"blenderbot"),cZr.forEach(t),xXo=r(Jwe," \u2014 "),PN=s(Jwe,"A",{href:!0});var mZr=n(PN);RXo=r(mZr,"TFBlenderbotModel"),mZr.forEach(t),SXo=r(Jwe," (Blenderbot model)"),Jwe.forEach(t),PXo=i(k),L1=s(k,"LI",{});var Ywe=n(L1);ace=s(Ywe,"STRONG",{});var fZr=n(ace);$Xo=r(fZr,"blenderbot-small"),fZr.forEach(t),IXo=r(Ywe," \u2014 "),$N=s(Ywe,"A",{href:!0});var gZr=n($N);jXo=r(gZr,"TFBlenderbotSmallModel"),gZr.forEach(t),NXo=r(Ywe," (BlenderbotSmall model)"),Ywe.forEach(t),DXo=i(k),B1=s(k,"LI",{});var Kwe=n(B1);sce=s(Kwe,"STRONG",{});var hZr=n(sce);qXo=r(hZr,"camembert"),hZr.forEach(t),GXo=r(Kwe," \u2014 "),IN=s(Kwe,"A",{href:!0});var uZr=n(IN);OXo=r(uZr,"TFCamembertModel"),uZr.forEach(t),XXo=r(Kwe," (CamemBERT model)"),Kwe.forEach(t),zXo=i(k),k1=s(k,"LI",{});var Zwe=n(k1);nce=s(Zwe,"STRONG",{});var pZr=n(nce);VXo=r(pZr,"clip"),pZr.forEach(t),WXo=r(Zwe," \u2014 "),jN=s(Zwe,"A",{href:!0});var _Zr=n(jN);QXo=r(_Zr,"TFCLIPModel"),_Zr.forEach(t),HXo=r(Zwe," (CLIP model)"),Zwe.forEach(t),UXo=i(k),x1=s(k,"LI",{});var eAe=n(x1);lce=s(eAe,"STRONG",{});var bZr=n(lce);JXo=r(bZr,"convbert"),bZr.forEach(t),YXo=r(eAe," \u2014 "),NN=s(eAe,"A",{href:!0});var vZr=n(NN);KXo=r(vZr,"TFConvBertModel"),vZr.forEach(t),ZXo=r(eAe," (ConvBERT model)"),eAe.forEach(t),ezo=i(k),R1=s(k,"LI",{});var oAe=n(R1);ice=s(oAe,"STRONG",{});var TZr=n(ice);ozo=r(TZr,"ctrl"),TZr.forEach(t),rzo=r(oAe," \u2014 "),DN=s(oAe,"A",{href:!0});var FZr=n(DN);tzo=r(FZr,"TFCTRLModel"),FZr.forEach(t),azo=r(oAe," (CTRL model)"),oAe.forEach(t),szo=i(k),S1=s(k,"LI",{});var rAe=n(S1);dce=s(rAe,"STRONG",{});var CZr=n(dce);nzo=r(CZr,"deberta"),CZr.forEach(t),lzo=r(rAe," \u2014 "),qN=s(rAe,"A",{href:!0});var MZr=n(qN);izo=r(MZr,"TFDebertaModel"),MZr.forEach(t),dzo=r(rAe," (DeBERTa model)"),rAe.forEach(t),czo=i(k),P1=s(k,"LI",{});var tAe=n(P1);cce=s(tAe,"STRONG",{});var EZr=n(cce);mzo=r(EZr,"deberta-v2"),EZr.forEach(t),fzo=r(tAe," \u2014 "),GN=s(tAe,"A",{href:!0});var yZr=n(GN);gzo=r(yZr,"TFDebertaV2Model"),yZr.forEach(t),hzo=r(tAe," (DeBERTa-v2 model)"),tAe.forEach(t),uzo=i(k),$1=s(k,"LI",{});var aAe=n($1);mce=s(aAe,"STRONG",{});var wZr=n(mce);pzo=r(wZr,"distilbert"),wZr.forEach(t),_zo=r(aAe," \u2014 "),ON=s(aAe,"A",{href:!0});var AZr=n(ON);bzo=r(AZr,"TFDistilBertModel"),AZr.forEach(t),vzo=r(aAe," (DistilBERT model)"),aAe.forEach(t),Tzo=i(k),I1=s(k,"LI",{});var sAe=n(I1);fce=s(sAe,"STRONG",{});var LZr=n(fce);Fzo=r(LZr,"dpr"),LZr.forEach(t),Czo=r(sAe," \u2014 "),XN=s(sAe,"A",{href:!0});var BZr=n(XN);Mzo=r(BZr,"TFDPRQuestionEncoder"),BZr.forEach(t),Ezo=r(sAe," (DPR model)"),sAe.forEach(t),yzo=i(k),j1=s(k,"LI",{});var nAe=n(j1);gce=s(nAe,"STRONG",{});var kZr=n(gce);wzo=r(kZr,"electra"),kZr.forEach(t),Azo=r(nAe," \u2014 "),zN=s(nAe,"A",{href:!0});var xZr=n(zN);Lzo=r(xZr,"TFElectraModel"),xZr.forEach(t),Bzo=r(nAe," (ELECTRA model)"),nAe.forEach(t),kzo=i(k),N1=s(k,"LI",{});var lAe=n(N1);hce=s(lAe,"STRONG",{});var RZr=n(hce);xzo=r(RZr,"flaubert"),RZr.forEach(t),Rzo=r(lAe," \u2014 "),VN=s(lAe,"A",{href:!0});var SZr=n(VN);Szo=r(SZr,"TFFlaubertModel"),SZr.forEach(t),Pzo=r(lAe," (FlauBERT model)"),lAe.forEach(t),$zo=i(k),Sn=s(k,"LI",{});var N7=n(Sn);uce=s(N7,"STRONG",{});var PZr=n(uce);Izo=r(PZr,"funnel"),PZr.forEach(t),jzo=r(N7," \u2014 "),WN=s(N7,"A",{href:!0});var $Zr=n(WN);Nzo=r($Zr,"TFFunnelModel"),$Zr.forEach(t),Dzo=r(N7," or "),QN=s(N7,"A",{href:!0});var IZr=n(QN);qzo=r(IZr,"TFFunnelBaseModel"),IZr.forEach(t),Gzo=r(N7," (Funnel Transformer model)"),N7.forEach(t),Ozo=i(k),D1=s(k,"LI",{});var iAe=n(D1);pce=s(iAe,"STRONG",{});var jZr=n(pce);Xzo=r(jZr,"gpt2"),jZr.forEach(t),zzo=r(iAe," \u2014 "),HN=s(iAe,"A",{href:!0});var NZr=n(HN);Vzo=r(NZr,"TFGPT2Model"),NZr.forEach(t),Wzo=r(iAe," (OpenAI GPT-2 model)"),iAe.forEach(t),Qzo=i(k),q1=s(k,"LI",{});var dAe=n(q1);_ce=s(dAe,"STRONG",{});var DZr=n(_ce);Hzo=r(DZr,"hubert"),DZr.forEach(t),Uzo=r(dAe," \u2014 "),UN=s(dAe,"A",{href:!0});var qZr=n(UN);Jzo=r(qZr,"TFHubertModel"),qZr.forEach(t),Yzo=r(dAe," (Hubert model)"),dAe.forEach(t),Kzo=i(k),G1=s(k,"LI",{});var cAe=n(G1);bce=s(cAe,"STRONG",{});var GZr=n(bce);Zzo=r(GZr,"layoutlm"),GZr.forEach(t),eVo=r(cAe," \u2014 "),JN=s(cAe,"A",{href:!0});var OZr=n(JN);oVo=r(OZr,"TFLayoutLMModel"),OZr.forEach(t),rVo=r(cAe," (LayoutLM model)"),cAe.forEach(t),tVo=i(k),O1=s(k,"LI",{});var mAe=n(O1);vce=s(mAe,"STRONG",{});var XZr=n(vce);aVo=r(XZr,"led"),XZr.forEach(t),sVo=r(mAe," \u2014 "),YN=s(mAe,"A",{href:!0});var zZr=n(YN);nVo=r(zZr,"TFLEDModel"),zZr.forEach(t),lVo=r(mAe," (LED model)"),mAe.forEach(t),iVo=i(k),X1=s(k,"LI",{});var fAe=n(X1);Tce=s(fAe,"STRONG",{});var VZr=n(Tce);dVo=r(VZr,"longformer"),VZr.forEach(t),cVo=r(fAe," \u2014 "),KN=s(fAe,"A",{href:!0});var WZr=n(KN);mVo=r(WZr,"TFLongformerModel"),WZr.forEach(t),fVo=r(fAe," (Longformer model)"),fAe.forEach(t),gVo=i(k),z1=s(k,"LI",{});var gAe=n(z1);Fce=s(gAe,"STRONG",{});var QZr=n(Fce);hVo=r(QZr,"lxmert"),QZr.forEach(t),uVo=r(gAe," \u2014 "),ZN=s(gAe,"A",{href:!0});var HZr=n(ZN);pVo=r(HZr,"TFLxmertModel"),HZr.forEach(t),_Vo=r(gAe," (LXMERT model)"),gAe.forEach(t),bVo=i(k),V1=s(k,"LI",{});var hAe=n(V1);Cce=s(hAe,"STRONG",{});var UZr=n(Cce);vVo=r(UZr,"marian"),UZr.forEach(t),TVo=r(hAe," \u2014 "),eD=s(hAe,"A",{href:!0});var JZr=n(eD);FVo=r(JZr,"TFMarianModel"),JZr.forEach(t),CVo=r(hAe," (Marian model)"),hAe.forEach(t),MVo=i(k),W1=s(k,"LI",{});var uAe=n(W1);Mce=s(uAe,"STRONG",{});var YZr=n(Mce);EVo=r(YZr,"mbart"),YZr.forEach(t),yVo=r(uAe," \u2014 "),oD=s(uAe,"A",{href:!0});var KZr=n(oD);wVo=r(KZr,"TFMBartModel"),KZr.forEach(t),AVo=r(uAe," (mBART model)"),uAe.forEach(t),LVo=i(k),Q1=s(k,"LI",{});var pAe=n(Q1);Ece=s(pAe,"STRONG",{});var ZZr=n(Ece);BVo=r(ZZr,"mobilebert"),ZZr.forEach(t),kVo=r(pAe," \u2014 "),rD=s(pAe,"A",{href:!0});var eet=n(rD);xVo=r(eet,"TFMobileBertModel"),eet.forEach(t),RVo=r(pAe," (MobileBERT model)"),pAe.forEach(t),SVo=i(k),H1=s(k,"LI",{});var _Ae=n(H1);yce=s(_Ae,"STRONG",{});var oet=n(yce);PVo=r(oet,"mpnet"),oet.forEach(t),$Vo=r(_Ae," \u2014 "),tD=s(_Ae,"A",{href:!0});var ret=n(tD);IVo=r(ret,"TFMPNetModel"),ret.forEach(t),jVo=r(_Ae," (MPNet model)"),_Ae.forEach(t),NVo=i(k),U1=s(k,"LI",{});var bAe=n(U1);wce=s(bAe,"STRONG",{});var tet=n(wce);DVo=r(tet,"mt5"),tet.forEach(t),qVo=r(bAe," \u2014 "),aD=s(bAe,"A",{href:!0});var aet=n(aD);GVo=r(aet,"TFMT5Model"),aet.forEach(t),OVo=r(bAe," (mT5 model)"),bAe.forEach(t),XVo=i(k),J1=s(k,"LI",{});var vAe=n(J1);Ace=s(vAe,"STRONG",{});var set=n(Ace);zVo=r(set,"openai-gpt"),set.forEach(t),VVo=r(vAe," \u2014 "),sD=s(vAe,"A",{href:!0});var net=n(sD);WVo=r(net,"TFOpenAIGPTModel"),net.forEach(t),QVo=r(vAe," (OpenAI GPT model)"),vAe.forEach(t),HVo=i(k),Y1=s(k,"LI",{});var TAe=n(Y1);Lce=s(TAe,"STRONG",{});var iet=n(Lce);UVo=r(iet,"pegasus"),iet.forEach(t),JVo=r(TAe," \u2014 "),nD=s(TAe,"A",{href:!0});var det=n(nD);YVo=r(det,"TFPegasusModel"),det.forEach(t),KVo=r(TAe," (Pegasus model)"),TAe.forEach(t),ZVo=i(k),K1=s(k,"LI",{});var FAe=n(K1);Bce=s(FAe,"STRONG",{});var cet=n(Bce);eWo=r(cet,"rembert"),cet.forEach(t),oWo=r(FAe," \u2014 "),lD=s(FAe,"A",{href:!0});var met=n(lD);rWo=r(met,"TFRemBertModel"),met.forEach(t),tWo=r(FAe," (RemBERT model)"),FAe.forEach(t),aWo=i(k),Z1=s(k,"LI",{});var CAe=n(Z1);kce=s(CAe,"STRONG",{});var fet=n(kce);sWo=r(fet,"roberta"),fet.forEach(t),nWo=r(CAe," \u2014 "),iD=s(CAe,"A",{href:!0});var get=n(iD);lWo=r(get,"TFRobertaModel"),get.forEach(t),iWo=r(CAe," (RoBERTa model)"),CAe.forEach(t),dWo=i(k),eF=s(k,"LI",{});var MAe=n(eF);xce=s(MAe,"STRONG",{});var het=n(xce);cWo=r(het,"roformer"),het.forEach(t),mWo=r(MAe," \u2014 "),dD=s(MAe,"A",{href:!0});var uet=n(dD);fWo=r(uet,"TFRoFormerModel"),uet.forEach(t),gWo=r(MAe," (RoFormer model)"),MAe.forEach(t),hWo=i(k),oF=s(k,"LI",{});var EAe=n(oF);Rce=s(EAe,"STRONG",{});var pet=n(Rce);uWo=r(pet,"speech_to_text"),pet.forEach(t),pWo=r(EAe," \u2014 "),cD=s(EAe,"A",{href:!0});var _et=n(cD);_Wo=r(_et,"TFSpeech2TextModel"),_et.forEach(t),bWo=r(EAe," (Speech2Text model)"),EAe.forEach(t),vWo=i(k),rF=s(k,"LI",{});var yAe=n(rF);Sce=s(yAe,"STRONG",{});var bet=n(Sce);TWo=r(bet,"t5"),bet.forEach(t),FWo=r(yAe," \u2014 "),mD=s(yAe,"A",{href:!0});var vet=n(mD);CWo=r(vet,"TFT5Model"),vet.forEach(t),MWo=r(yAe," (T5 model)"),yAe.forEach(t),EWo=i(k),tF=s(k,"LI",{});var wAe=n(tF);Pce=s(wAe,"STRONG",{});var Tet=n(Pce);yWo=r(Tet,"tapas"),Tet.forEach(t),wWo=r(wAe," \u2014 "),fD=s(wAe,"A",{href:!0});var Fet=n(fD);AWo=r(Fet,"TFTapasModel"),Fet.forEach(t),LWo=r(wAe," (TAPAS model)"),wAe.forEach(t),BWo=i(k),aF=s(k,"LI",{});var AAe=n(aF);$ce=s(AAe,"STRONG",{});var Cet=n($ce);kWo=r(Cet,"transfo-xl"),Cet.forEach(t),xWo=r(AAe," \u2014 "),gD=s(AAe,"A",{href:!0});var Met=n(gD);RWo=r(Met,"TFTransfoXLModel"),Met.forEach(t),SWo=r(AAe," (Transformer-XL model)"),AAe.forEach(t),PWo=i(k),sF=s(k,"LI",{});var LAe=n(sF);Ice=s(LAe,"STRONG",{});var Eet=n(Ice);$Wo=r(Eet,"vit"),Eet.forEach(t),IWo=r(LAe," \u2014 "),hD=s(LAe,"A",{href:!0});var yet=n(hD);jWo=r(yet,"TFViTModel"),yet.forEach(t),NWo=r(LAe," (ViT model)"),LAe.forEach(t),DWo=i(k),nF=s(k,"LI",{});var BAe=n(nF);jce=s(BAe,"STRONG",{});var wet=n(jce);qWo=r(wet,"wav2vec2"),wet.forEach(t),GWo=r(BAe," \u2014 "),uD=s(BAe,"A",{href:!0});var Aet=n(uD);OWo=r(Aet,"TFWav2Vec2Model"),Aet.forEach(t),XWo=r(BAe," (Wav2Vec2 model)"),BAe.forEach(t),zWo=i(k),lF=s(k,"LI",{});var kAe=n(lF);Nce=s(kAe,"STRONG",{});var Let=n(Nce);VWo=r(Let,"xlm"),Let.forEach(t),WWo=r(kAe," \u2014 "),pD=s(kAe,"A",{href:!0});var Bet=n(pD);QWo=r(Bet,"TFXLMModel"),Bet.forEach(t),HWo=r(kAe," (XLM model)"),kAe.forEach(t),UWo=i(k),iF=s(k,"LI",{});var xAe=n(iF);Dce=s(xAe,"STRONG",{});var ket=n(Dce);JWo=r(ket,"xlm-roberta"),ket.forEach(t),YWo=r(xAe," \u2014 "),_D=s(xAe,"A",{href:!0});var xet=n(_D);KWo=r(xet,"TFXLMRobertaModel"),xet.forEach(t),ZWo=r(xAe," (XLM-RoBERTa model)"),xAe.forEach(t),eQo=i(k),dF=s(k,"LI",{});var RAe=n(dF);qce=s(RAe,"STRONG",{});var Ret=n(qce);oQo=r(Ret,"xlnet"),Ret.forEach(t),rQo=r(RAe," \u2014 "),bD=s(RAe,"A",{href:!0});var Set=n(bD);tQo=r(Set,"TFXLNetModel"),Set.forEach(t),aQo=r(RAe," (XLNet model)"),RAe.forEach(t),k.forEach(t),sQo=i(ca),Gce=s(ca,"P",{});var Pet=n(Gce);nQo=r(Pet,"Examples:"),Pet.forEach(t),lQo=i(ca),f($A.$$.fragment,ca),ca.forEach(t),Bl.forEach(t),i9e=i(d),ac=s(d,"H2",{class:!0});var _ke=n(ac);cF=s(_ke,"A",{id:!0,class:!0,href:!0});var $et=n(cF);Oce=s($et,"SPAN",{});var Iet=n(Oce);f(IA.$$.fragment,Iet),Iet.forEach(t),$et.forEach(t),iQo=i(_ke),Xce=s(_ke,"SPAN",{});var jet=n(Xce);dQo=r(jet,"TFAutoModelForPreTraining"),jet.forEach(t),_ke.forEach(t),d9e=i(d),hr=s(d,"DIV",{class:!0});var xl=n(hr);f(jA.$$.fragment,xl),cQo=i(xl),sc=s(xl,"P",{});var Gz=n(sc);mQo=r(Gz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),zce=s(Gz,"CODE",{});var Net=n(zce);fQo=r(Net,"from_pretrained()"),Net.forEach(t),gQo=r(Gz,"class method or the "),Vce=s(Gz,"CODE",{});var Det=n(Vce);hQo=r(Det,"from_config()"),Det.forEach(t),uQo=r(Gz,`class
method.`),Gz.forEach(t),pQo=i(xl),NA=s(xl,"P",{});var bke=n(NA);_Qo=r(bke,"This class cannot be instantiated directly using "),Wce=s(bke,"CODE",{});var qet=n(Wce);bQo=r(qet,"__init__()"),qet.forEach(t),vQo=r(bke," (throws an error)."),bke.forEach(t),TQo=i(xl),lt=s(xl,"DIV",{class:!0});var Rl=n(lt);f(DA.$$.fragment,Rl),FQo=i(Rl),Qce=s(Rl,"P",{});var Get=n(Qce);CQo=r(Get,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Get.forEach(t),MQo=i(Rl),nc=s(Rl,"P",{});var Oz=n(nc);EQo=r(Oz,`Note:
Loading a model from its configuration file does `),Hce=s(Oz,"STRONG",{});var Oet=n(Hce);yQo=r(Oet,"not"),Oet.forEach(t),wQo=r(Oz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Uce=s(Oz,"CODE",{});var Xet=n(Uce);AQo=r(Xet,"from_pretrained()"),Xet.forEach(t),LQo=r(Oz,"to load the model weights."),Oz.forEach(t),BQo=i(Rl),Jce=s(Rl,"P",{});var zet=n(Jce);kQo=r(zet,"Examples:"),zet.forEach(t),xQo=i(Rl),f(qA.$$.fragment,Rl),Rl.forEach(t),RQo=i(xl),ho=s(xl,"DIV",{class:!0});var ma=n(ho);f(GA.$$.fragment,ma),SQo=i(ma),Yce=s(ma,"P",{});var Vet=n(Yce);PQo=r(Vet,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),Vet.forEach(t),$Qo=i(ma),is=s(ma,"P",{});var k3=n(is);IQo=r(k3,"The model class to instantiate is selected based on the "),Kce=s(k3,"CODE",{});var Wet=n(Kce);jQo=r(Wet,"model_type"),Wet.forEach(t),NQo=r(k3,` property of the config object (either
passed as an argument or loaded from `),Zce=s(k3,"CODE",{});var Qet=n(Zce);DQo=r(Qet,"pretrained_model_name_or_path"),Qet.forEach(t),qQo=r(k3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),eme=s(k3,"CODE",{});var Het=n(eme);GQo=r(Het,"pretrained_model_name_or_path"),Het.forEach(t),OQo=r(k3,":"),k3.forEach(t),XQo=i(ma),H=s(ma,"UL",{});var U=n(H);mF=s(U,"LI",{});var SAe=n(mF);ome=s(SAe,"STRONG",{});var Uet=n(ome);zQo=r(Uet,"albert"),Uet.forEach(t),VQo=r(SAe," \u2014 "),vD=s(SAe,"A",{href:!0});var Jet=n(vD);WQo=r(Jet,"TFAlbertForPreTraining"),Jet.forEach(t),QQo=r(SAe," (ALBERT model)"),SAe.forEach(t),HQo=i(U),fF=s(U,"LI",{});var PAe=n(fF);rme=s(PAe,"STRONG",{});var Yet=n(rme);UQo=r(Yet,"bart"),Yet.forEach(t),JQo=r(PAe," \u2014 "),TD=s(PAe,"A",{href:!0});var Ket=n(TD);YQo=r(Ket,"TFBartForConditionalGeneration"),Ket.forEach(t),KQo=r(PAe," (BART model)"),PAe.forEach(t),ZQo=i(U),gF=s(U,"LI",{});var $Ae=n(gF);tme=s($Ae,"STRONG",{});var Zet=n(tme);eHo=r(Zet,"bert"),Zet.forEach(t),oHo=r($Ae," \u2014 "),FD=s($Ae,"A",{href:!0});var eot=n(FD);rHo=r(eot,"TFBertForPreTraining"),eot.forEach(t),tHo=r($Ae," (BERT model)"),$Ae.forEach(t),aHo=i(U),hF=s(U,"LI",{});var IAe=n(hF);ame=s(IAe,"STRONG",{});var oot=n(ame);sHo=r(oot,"camembert"),oot.forEach(t),nHo=r(IAe," \u2014 "),CD=s(IAe,"A",{href:!0});var rot=n(CD);lHo=r(rot,"TFCamembertForMaskedLM"),rot.forEach(t),iHo=r(IAe," (CamemBERT model)"),IAe.forEach(t),dHo=i(U),uF=s(U,"LI",{});var jAe=n(uF);sme=s(jAe,"STRONG",{});var tot=n(sme);cHo=r(tot,"ctrl"),tot.forEach(t),mHo=r(jAe," \u2014 "),MD=s(jAe,"A",{href:!0});var aot=n(MD);fHo=r(aot,"TFCTRLLMHeadModel"),aot.forEach(t),gHo=r(jAe," (CTRL model)"),jAe.forEach(t),hHo=i(U),pF=s(U,"LI",{});var NAe=n(pF);nme=s(NAe,"STRONG",{});var sot=n(nme);uHo=r(sot,"distilbert"),sot.forEach(t),pHo=r(NAe," \u2014 "),ED=s(NAe,"A",{href:!0});var not=n(ED);_Ho=r(not,"TFDistilBertForMaskedLM"),not.forEach(t),bHo=r(NAe," (DistilBERT model)"),NAe.forEach(t),vHo=i(U),_F=s(U,"LI",{});var DAe=n(_F);lme=s(DAe,"STRONG",{});var lot=n(lme);THo=r(lot,"electra"),lot.forEach(t),FHo=r(DAe," \u2014 "),yD=s(DAe,"A",{href:!0});var iot=n(yD);CHo=r(iot,"TFElectraForPreTraining"),iot.forEach(t),MHo=r(DAe," (ELECTRA model)"),DAe.forEach(t),EHo=i(U),bF=s(U,"LI",{});var qAe=n(bF);ime=s(qAe,"STRONG",{});var dot=n(ime);yHo=r(dot,"flaubert"),dot.forEach(t),wHo=r(qAe," \u2014 "),wD=s(qAe,"A",{href:!0});var cot=n(wD);AHo=r(cot,"TFFlaubertWithLMHeadModel"),cot.forEach(t),LHo=r(qAe," (FlauBERT model)"),qAe.forEach(t),BHo=i(U),vF=s(U,"LI",{});var GAe=n(vF);dme=s(GAe,"STRONG",{});var mot=n(dme);kHo=r(mot,"funnel"),mot.forEach(t),xHo=r(GAe," \u2014 "),AD=s(GAe,"A",{href:!0});var fot=n(AD);RHo=r(fot,"TFFunnelForPreTraining"),fot.forEach(t),SHo=r(GAe," (Funnel Transformer model)"),GAe.forEach(t),PHo=i(U),TF=s(U,"LI",{});var OAe=n(TF);cme=s(OAe,"STRONG",{});var got=n(cme);$Ho=r(got,"gpt2"),got.forEach(t),IHo=r(OAe," \u2014 "),LD=s(OAe,"A",{href:!0});var hot=n(LD);jHo=r(hot,"TFGPT2LMHeadModel"),hot.forEach(t),NHo=r(OAe," (OpenAI GPT-2 model)"),OAe.forEach(t),DHo=i(U),FF=s(U,"LI",{});var XAe=n(FF);mme=s(XAe,"STRONG",{});var uot=n(mme);qHo=r(uot,"layoutlm"),uot.forEach(t),GHo=r(XAe," \u2014 "),BD=s(XAe,"A",{href:!0});var pot=n(BD);OHo=r(pot,"TFLayoutLMForMaskedLM"),pot.forEach(t),XHo=r(XAe," (LayoutLM model)"),XAe.forEach(t),zHo=i(U),CF=s(U,"LI",{});var zAe=n(CF);fme=s(zAe,"STRONG",{});var _ot=n(fme);VHo=r(_ot,"lxmert"),_ot.forEach(t),WHo=r(zAe," \u2014 "),kD=s(zAe,"A",{href:!0});var bot=n(kD);QHo=r(bot,"TFLxmertForPreTraining"),bot.forEach(t),HHo=r(zAe," (LXMERT model)"),zAe.forEach(t),UHo=i(U),MF=s(U,"LI",{});var VAe=n(MF);gme=s(VAe,"STRONG",{});var vot=n(gme);JHo=r(vot,"mobilebert"),vot.forEach(t),YHo=r(VAe," \u2014 "),xD=s(VAe,"A",{href:!0});var Tot=n(xD);KHo=r(Tot,"TFMobileBertForPreTraining"),Tot.forEach(t),ZHo=r(VAe," (MobileBERT model)"),VAe.forEach(t),eUo=i(U),EF=s(U,"LI",{});var WAe=n(EF);hme=s(WAe,"STRONG",{});var Fot=n(hme);oUo=r(Fot,"mpnet"),Fot.forEach(t),rUo=r(WAe," \u2014 "),RD=s(WAe,"A",{href:!0});var Cot=n(RD);tUo=r(Cot,"TFMPNetForMaskedLM"),Cot.forEach(t),aUo=r(WAe," (MPNet model)"),WAe.forEach(t),sUo=i(U),yF=s(U,"LI",{});var QAe=n(yF);ume=s(QAe,"STRONG",{});var Mot=n(ume);nUo=r(Mot,"openai-gpt"),Mot.forEach(t),lUo=r(QAe," \u2014 "),SD=s(QAe,"A",{href:!0});var Eot=n(SD);iUo=r(Eot,"TFOpenAIGPTLMHeadModel"),Eot.forEach(t),dUo=r(QAe," (OpenAI GPT model)"),QAe.forEach(t),cUo=i(U),wF=s(U,"LI",{});var HAe=n(wF);pme=s(HAe,"STRONG",{});var yot=n(pme);mUo=r(yot,"roberta"),yot.forEach(t),fUo=r(HAe," \u2014 "),PD=s(HAe,"A",{href:!0});var wot=n(PD);gUo=r(wot,"TFRobertaForMaskedLM"),wot.forEach(t),hUo=r(HAe," (RoBERTa model)"),HAe.forEach(t),uUo=i(U),AF=s(U,"LI",{});var UAe=n(AF);_me=s(UAe,"STRONG",{});var Aot=n(_me);pUo=r(Aot,"t5"),Aot.forEach(t),_Uo=r(UAe," \u2014 "),$D=s(UAe,"A",{href:!0});var Lot=n($D);bUo=r(Lot,"TFT5ForConditionalGeneration"),Lot.forEach(t),vUo=r(UAe," (T5 model)"),UAe.forEach(t),TUo=i(U),LF=s(U,"LI",{});var JAe=n(LF);bme=s(JAe,"STRONG",{});var Bot=n(bme);FUo=r(Bot,"tapas"),Bot.forEach(t),CUo=r(JAe," \u2014 "),ID=s(JAe,"A",{href:!0});var kot=n(ID);MUo=r(kot,"TFTapasForMaskedLM"),kot.forEach(t),EUo=r(JAe," (TAPAS model)"),JAe.forEach(t),yUo=i(U),BF=s(U,"LI",{});var YAe=n(BF);vme=s(YAe,"STRONG",{});var xot=n(vme);wUo=r(xot,"transfo-xl"),xot.forEach(t),AUo=r(YAe," \u2014 "),jD=s(YAe,"A",{href:!0});var Rot=n(jD);LUo=r(Rot,"TFTransfoXLLMHeadModel"),Rot.forEach(t),BUo=r(YAe," (Transformer-XL model)"),YAe.forEach(t),kUo=i(U),kF=s(U,"LI",{});var KAe=n(kF);Tme=s(KAe,"STRONG",{});var Sot=n(Tme);xUo=r(Sot,"xlm"),Sot.forEach(t),RUo=r(KAe," \u2014 "),ND=s(KAe,"A",{href:!0});var Pot=n(ND);SUo=r(Pot,"TFXLMWithLMHeadModel"),Pot.forEach(t),PUo=r(KAe," (XLM model)"),KAe.forEach(t),$Uo=i(U),xF=s(U,"LI",{});var ZAe=n(xF);Fme=s(ZAe,"STRONG",{});var $ot=n(Fme);IUo=r($ot,"xlm-roberta"),$ot.forEach(t),jUo=r(ZAe," \u2014 "),DD=s(ZAe,"A",{href:!0});var Iot=n(DD);NUo=r(Iot,"TFXLMRobertaForMaskedLM"),Iot.forEach(t),DUo=r(ZAe," (XLM-RoBERTa model)"),ZAe.forEach(t),qUo=i(U),RF=s(U,"LI",{});var e6e=n(RF);Cme=s(e6e,"STRONG",{});var jot=n(Cme);GUo=r(jot,"xlnet"),jot.forEach(t),OUo=r(e6e," \u2014 "),qD=s(e6e,"A",{href:!0});var Not=n(qD);XUo=r(Not,"TFXLNetLMHeadModel"),Not.forEach(t),zUo=r(e6e," (XLNet model)"),e6e.forEach(t),U.forEach(t),VUo=i(ma),Mme=s(ma,"P",{});var Dot=n(Mme);WUo=r(Dot,"Examples:"),Dot.forEach(t),QUo=i(ma),f(OA.$$.fragment,ma),ma.forEach(t),xl.forEach(t),c9e=i(d),lc=s(d,"H2",{class:!0});var vke=n(lc);SF=s(vke,"A",{id:!0,class:!0,href:!0});var qot=n(SF);Eme=s(qot,"SPAN",{});var Got=n(Eme);f(XA.$$.fragment,Got),Got.forEach(t),qot.forEach(t),HUo=i(vke),yme=s(vke,"SPAN",{});var Oot=n(yme);UUo=r(Oot,"TFAutoModelForCausalLM"),Oot.forEach(t),vke.forEach(t),m9e=i(d),ur=s(d,"DIV",{class:!0});var Sl=n(ur);f(zA.$$.fragment,Sl),JUo=i(Sl),ic=s(Sl,"P",{});var Xz=n(ic);YUo=r(Xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),wme=s(Xz,"CODE",{});var Xot=n(wme);KUo=r(Xot,"from_pretrained()"),Xot.forEach(t),ZUo=r(Xz,"class method or the "),Ame=s(Xz,"CODE",{});var zot=n(Ame);eJo=r(zot,"from_config()"),zot.forEach(t),oJo=r(Xz,`class
method.`),Xz.forEach(t),rJo=i(Sl),VA=s(Sl,"P",{});var Tke=n(VA);tJo=r(Tke,"This class cannot be instantiated directly using "),Lme=s(Tke,"CODE",{});var Vot=n(Lme);aJo=r(Vot,"__init__()"),Vot.forEach(t),sJo=r(Tke," (throws an error)."),Tke.forEach(t),nJo=i(Sl),it=s(Sl,"DIV",{class:!0});var Pl=n(it);f(WA.$$.fragment,Pl),lJo=i(Pl),Bme=s(Pl,"P",{});var Wot=n(Bme);iJo=r(Wot,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Wot.forEach(t),dJo=i(Pl),dc=s(Pl,"P",{});var zz=n(dc);cJo=r(zz,`Note:
Loading a model from its configuration file does `),kme=s(zz,"STRONG",{});var Qot=n(kme);mJo=r(Qot,"not"),Qot.forEach(t),fJo=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xme=s(zz,"CODE",{});var Hot=n(xme);gJo=r(Hot,"from_pretrained()"),Hot.forEach(t),hJo=r(zz,"to load the model weights."),zz.forEach(t),uJo=i(Pl),Rme=s(Pl,"P",{});var Uot=n(Rme);pJo=r(Uot,"Examples:"),Uot.forEach(t),_Jo=i(Pl),f(QA.$$.fragment,Pl),Pl.forEach(t),bJo=i(Sl),uo=s(Sl,"DIV",{class:!0});var fa=n(uo);f(HA.$$.fragment,fa),vJo=i(fa),Sme=s(fa,"P",{});var Jot=n(Sme);TJo=r(Jot,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Jot.forEach(t),FJo=i(fa),ds=s(fa,"P",{});var x3=n(ds);CJo=r(x3,"The model class to instantiate is selected based on the "),Pme=s(x3,"CODE",{});var Yot=n(Pme);MJo=r(Yot,"model_type"),Yot.forEach(t),EJo=r(x3,` property of the config object (either
passed as an argument or loaded from `),$me=s(x3,"CODE",{});var Kot=n($me);yJo=r(Kot,"pretrained_model_name_or_path"),Kot.forEach(t),wJo=r(x3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ime=s(x3,"CODE",{});var Zot=n(Ime);AJo=r(Zot,"pretrained_model_name_or_path"),Zot.forEach(t),LJo=r(x3,":"),x3.forEach(t),BJo=i(fa),he=s(fa,"UL",{});var Me=n(he);PF=s(Me,"LI",{});var o6e=n(PF);jme=s(o6e,"STRONG",{});var ert=n(jme);kJo=r(ert,"bert"),ert.forEach(t),xJo=r(o6e," \u2014 "),GD=s(o6e,"A",{href:!0});var ort=n(GD);RJo=r(ort,"TFBertLMHeadModel"),ort.forEach(t),SJo=r(o6e," (BERT model)"),o6e.forEach(t),PJo=i(Me),$F=s(Me,"LI",{});var r6e=n($F);Nme=s(r6e,"STRONG",{});var rrt=n(Nme);$Jo=r(rrt,"ctrl"),rrt.forEach(t),IJo=r(r6e," \u2014 "),OD=s(r6e,"A",{href:!0});var trt=n(OD);jJo=r(trt,"TFCTRLLMHeadModel"),trt.forEach(t),NJo=r(r6e," (CTRL model)"),r6e.forEach(t),DJo=i(Me),IF=s(Me,"LI",{});var t6e=n(IF);Dme=s(t6e,"STRONG",{});var art=n(Dme);qJo=r(art,"gpt2"),art.forEach(t),GJo=r(t6e," \u2014 "),XD=s(t6e,"A",{href:!0});var srt=n(XD);OJo=r(srt,"TFGPT2LMHeadModel"),srt.forEach(t),XJo=r(t6e," (OpenAI GPT-2 model)"),t6e.forEach(t),zJo=i(Me),jF=s(Me,"LI",{});var a6e=n(jF);qme=s(a6e,"STRONG",{});var nrt=n(qme);VJo=r(nrt,"openai-gpt"),nrt.forEach(t),WJo=r(a6e," \u2014 "),zD=s(a6e,"A",{href:!0});var lrt=n(zD);QJo=r(lrt,"TFOpenAIGPTLMHeadModel"),lrt.forEach(t),HJo=r(a6e," (OpenAI GPT model)"),a6e.forEach(t),UJo=i(Me),NF=s(Me,"LI",{});var s6e=n(NF);Gme=s(s6e,"STRONG",{});var irt=n(Gme);JJo=r(irt,"rembert"),irt.forEach(t),YJo=r(s6e," \u2014 "),VD=s(s6e,"A",{href:!0});var drt=n(VD);KJo=r(drt,"TFRemBertForCausalLM"),drt.forEach(t),ZJo=r(s6e," (RemBERT model)"),s6e.forEach(t),eYo=i(Me),DF=s(Me,"LI",{});var n6e=n(DF);Ome=s(n6e,"STRONG",{});var crt=n(Ome);oYo=r(crt,"roberta"),crt.forEach(t),rYo=r(n6e," \u2014 "),WD=s(n6e,"A",{href:!0});var mrt=n(WD);tYo=r(mrt,"TFRobertaForCausalLM"),mrt.forEach(t),aYo=r(n6e," (RoBERTa model)"),n6e.forEach(t),sYo=i(Me),qF=s(Me,"LI",{});var l6e=n(qF);Xme=s(l6e,"STRONG",{});var frt=n(Xme);nYo=r(frt,"roformer"),frt.forEach(t),lYo=r(l6e," \u2014 "),QD=s(l6e,"A",{href:!0});var grt=n(QD);iYo=r(grt,"TFRoFormerForCausalLM"),grt.forEach(t),dYo=r(l6e," (RoFormer model)"),l6e.forEach(t),cYo=i(Me),GF=s(Me,"LI",{});var i6e=n(GF);zme=s(i6e,"STRONG",{});var hrt=n(zme);mYo=r(hrt,"transfo-xl"),hrt.forEach(t),fYo=r(i6e," \u2014 "),HD=s(i6e,"A",{href:!0});var urt=n(HD);gYo=r(urt,"TFTransfoXLLMHeadModel"),urt.forEach(t),hYo=r(i6e," (Transformer-XL model)"),i6e.forEach(t),uYo=i(Me),OF=s(Me,"LI",{});var d6e=n(OF);Vme=s(d6e,"STRONG",{});var prt=n(Vme);pYo=r(prt,"xlm"),prt.forEach(t),_Yo=r(d6e," \u2014 "),UD=s(d6e,"A",{href:!0});var _rt=n(UD);bYo=r(_rt,"TFXLMWithLMHeadModel"),_rt.forEach(t),vYo=r(d6e," (XLM model)"),d6e.forEach(t),TYo=i(Me),XF=s(Me,"LI",{});var c6e=n(XF);Wme=s(c6e,"STRONG",{});var brt=n(Wme);FYo=r(brt,"xlnet"),brt.forEach(t),CYo=r(c6e," \u2014 "),JD=s(c6e,"A",{href:!0});var vrt=n(JD);MYo=r(vrt,"TFXLNetLMHeadModel"),vrt.forEach(t),EYo=r(c6e," (XLNet model)"),c6e.forEach(t),Me.forEach(t),yYo=i(fa),Qme=s(fa,"P",{});var Trt=n(Qme);wYo=r(Trt,"Examples:"),Trt.forEach(t),AYo=i(fa),f(UA.$$.fragment,fa),fa.forEach(t),Sl.forEach(t),f9e=i(d),cc=s(d,"H2",{class:!0});var Fke=n(cc);zF=s(Fke,"A",{id:!0,class:!0,href:!0});var Frt=n(zF);Hme=s(Frt,"SPAN",{});var Crt=n(Hme);f(JA.$$.fragment,Crt),Crt.forEach(t),Frt.forEach(t),LYo=i(Fke),Ume=s(Fke,"SPAN",{});var Mrt=n(Ume);BYo=r(Mrt,"TFAutoModelForImageClassification"),Mrt.forEach(t),Fke.forEach(t),g9e=i(d),pr=s(d,"DIV",{class:!0});var $l=n(pr);f(YA.$$.fragment,$l),kYo=i($l),mc=s($l,"P",{});var Vz=n(mc);xYo=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Jme=s(Vz,"CODE",{});var Ert=n(Jme);RYo=r(Ert,"from_pretrained()"),Ert.forEach(t),SYo=r(Vz,"class method or the "),Yme=s(Vz,"CODE",{});var yrt=n(Yme);PYo=r(yrt,"from_config()"),yrt.forEach(t),$Yo=r(Vz,`class
method.`),Vz.forEach(t),IYo=i($l),KA=s($l,"P",{});var Cke=n(KA);jYo=r(Cke,"This class cannot be instantiated directly using "),Kme=s(Cke,"CODE",{});var wrt=n(Kme);NYo=r(wrt,"__init__()"),wrt.forEach(t),DYo=r(Cke," (throws an error)."),Cke.forEach(t),qYo=i($l),dt=s($l,"DIV",{class:!0});var Il=n(dt);f(ZA.$$.fragment,Il),GYo=i(Il),Zme=s(Il,"P",{});var Art=n(Zme);OYo=r(Art,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Art.forEach(t),XYo=i(Il),fc=s(Il,"P",{});var Wz=n(fc);zYo=r(Wz,`Note:
Loading a model from its configuration file does `),efe=s(Wz,"STRONG",{});var Lrt=n(efe);VYo=r(Lrt,"not"),Lrt.forEach(t),WYo=r(Wz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ofe=s(Wz,"CODE",{});var Brt=n(ofe);QYo=r(Brt,"from_pretrained()"),Brt.forEach(t),HYo=r(Wz,"to load the model weights."),Wz.forEach(t),UYo=i(Il),rfe=s(Il,"P",{});var krt=n(rfe);JYo=r(krt,"Examples:"),krt.forEach(t),YYo=i(Il),f(e6.$$.fragment,Il),Il.forEach(t),KYo=i($l),po=s($l,"DIV",{class:!0});var ga=n(po);f(o6.$$.fragment,ga),ZYo=i(ga),tfe=s(ga,"P",{});var xrt=n(tfe);eKo=r(xrt,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),xrt.forEach(t),oKo=i(ga),cs=s(ga,"P",{});var R3=n(cs);rKo=r(R3,"The model class to instantiate is selected based on the "),afe=s(R3,"CODE",{});var Rrt=n(afe);tKo=r(Rrt,"model_type"),Rrt.forEach(t),aKo=r(R3,` property of the config object (either
passed as an argument or loaded from `),sfe=s(R3,"CODE",{});var Srt=n(sfe);sKo=r(Srt,"pretrained_model_name_or_path"),Srt.forEach(t),nKo=r(R3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nfe=s(R3,"CODE",{});var Prt=n(nfe);lKo=r(Prt,"pretrained_model_name_or_path"),Prt.forEach(t),iKo=r(R3,":"),R3.forEach(t),dKo=i(ga),lfe=s(ga,"UL",{});var $rt=n(lfe);VF=s($rt,"LI",{});var m6e=n(VF);ife=s(m6e,"STRONG",{});var Irt=n(ife);cKo=r(Irt,"vit"),Irt.forEach(t),mKo=r(m6e," \u2014 "),YD=s(m6e,"A",{href:!0});var jrt=n(YD);fKo=r(jrt,"TFViTForImageClassification"),jrt.forEach(t),gKo=r(m6e," (ViT model)"),m6e.forEach(t),$rt.forEach(t),hKo=i(ga),dfe=s(ga,"P",{});var Nrt=n(dfe);uKo=r(Nrt,"Examples:"),Nrt.forEach(t),pKo=i(ga),f(r6.$$.fragment,ga),ga.forEach(t),$l.forEach(t),h9e=i(d),gc=s(d,"H2",{class:!0});var Mke=n(gc);WF=s(Mke,"A",{id:!0,class:!0,href:!0});var Drt=n(WF);cfe=s(Drt,"SPAN",{});var qrt=n(cfe);f(t6.$$.fragment,qrt),qrt.forEach(t),Drt.forEach(t),_Ko=i(Mke),mfe=s(Mke,"SPAN",{});var Grt=n(mfe);bKo=r(Grt,"TFAutoModelForMaskedLM"),Grt.forEach(t),Mke.forEach(t),u9e=i(d),_r=s(d,"DIV",{class:!0});var jl=n(_r);f(a6.$$.fragment,jl),vKo=i(jl),hc=s(jl,"P",{});var Qz=n(hc);TKo=r(Qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),ffe=s(Qz,"CODE",{});var Ort=n(ffe);FKo=r(Ort,"from_pretrained()"),Ort.forEach(t),CKo=r(Qz,"class method or the "),gfe=s(Qz,"CODE",{});var Xrt=n(gfe);MKo=r(Xrt,"from_config()"),Xrt.forEach(t),EKo=r(Qz,`class
method.`),Qz.forEach(t),yKo=i(jl),s6=s(jl,"P",{});var Eke=n(s6);wKo=r(Eke,"This class cannot be instantiated directly using "),hfe=s(Eke,"CODE",{});var zrt=n(hfe);AKo=r(zrt,"__init__()"),zrt.forEach(t),LKo=r(Eke," (throws an error)."),Eke.forEach(t),BKo=i(jl),ct=s(jl,"DIV",{class:!0});var Nl=n(ct);f(n6.$$.fragment,Nl),kKo=i(Nl),ufe=s(Nl,"P",{});var Vrt=n(ufe);xKo=r(Vrt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Vrt.forEach(t),RKo=i(Nl),uc=s(Nl,"P",{});var Hz=n(uc);SKo=r(Hz,`Note:
Loading a model from its configuration file does `),pfe=s(Hz,"STRONG",{});var Wrt=n(pfe);PKo=r(Wrt,"not"),Wrt.forEach(t),$Ko=r(Hz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_fe=s(Hz,"CODE",{});var Qrt=n(_fe);IKo=r(Qrt,"from_pretrained()"),Qrt.forEach(t),jKo=r(Hz,"to load the model weights."),Hz.forEach(t),NKo=i(Nl),bfe=s(Nl,"P",{});var Hrt=n(bfe);DKo=r(Hrt,"Examples:"),Hrt.forEach(t),qKo=i(Nl),f(l6.$$.fragment,Nl),Nl.forEach(t),GKo=i(jl),_o=s(jl,"DIV",{class:!0});var ha=n(_o);f(i6.$$.fragment,ha),OKo=i(ha),vfe=s(ha,"P",{});var Urt=n(vfe);XKo=r(Urt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Urt.forEach(t),zKo=i(ha),ms=s(ha,"P",{});var S3=n(ms);VKo=r(S3,"The model class to instantiate is selected based on the "),Tfe=s(S3,"CODE",{});var Jrt=n(Tfe);WKo=r(Jrt,"model_type"),Jrt.forEach(t),QKo=r(S3,` property of the config object (either
passed as an argument or loaded from `),Ffe=s(S3,"CODE",{});var Yrt=n(Ffe);HKo=r(Yrt,"pretrained_model_name_or_path"),Yrt.forEach(t),UKo=r(S3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cfe=s(S3,"CODE",{});var Krt=n(Cfe);JKo=r(Krt,"pretrained_model_name_or_path"),Krt.forEach(t),YKo=r(S3,":"),S3.forEach(t),KKo=i(ha),Y=s(ha,"UL",{});var ee=n(Y);QF=s(ee,"LI",{});var f6e=n(QF);Mfe=s(f6e,"STRONG",{});var Zrt=n(Mfe);ZKo=r(Zrt,"albert"),Zrt.forEach(t),eZo=r(f6e," \u2014 "),KD=s(f6e,"A",{href:!0});var ett=n(KD);oZo=r(ett,"TFAlbertForMaskedLM"),ett.forEach(t),rZo=r(f6e," (ALBERT model)"),f6e.forEach(t),tZo=i(ee),HF=s(ee,"LI",{});var g6e=n(HF);Efe=s(g6e,"STRONG",{});var ott=n(Efe);aZo=r(ott,"bert"),ott.forEach(t),sZo=r(g6e," \u2014 "),ZD=s(g6e,"A",{href:!0});var rtt=n(ZD);nZo=r(rtt,"TFBertForMaskedLM"),rtt.forEach(t),lZo=r(g6e," (BERT model)"),g6e.forEach(t),iZo=i(ee),UF=s(ee,"LI",{});var h6e=n(UF);yfe=s(h6e,"STRONG",{});var ttt=n(yfe);dZo=r(ttt,"camembert"),ttt.forEach(t),cZo=r(h6e," \u2014 "),eq=s(h6e,"A",{href:!0});var att=n(eq);mZo=r(att,"TFCamembertForMaskedLM"),att.forEach(t),fZo=r(h6e," (CamemBERT model)"),h6e.forEach(t),gZo=i(ee),JF=s(ee,"LI",{});var u6e=n(JF);wfe=s(u6e,"STRONG",{});var stt=n(wfe);hZo=r(stt,"convbert"),stt.forEach(t),uZo=r(u6e," \u2014 "),oq=s(u6e,"A",{href:!0});var ntt=n(oq);pZo=r(ntt,"TFConvBertForMaskedLM"),ntt.forEach(t),_Zo=r(u6e," (ConvBERT model)"),u6e.forEach(t),bZo=i(ee),YF=s(ee,"LI",{});var p6e=n(YF);Afe=s(p6e,"STRONG",{});var ltt=n(Afe);vZo=r(ltt,"deberta"),ltt.forEach(t),TZo=r(p6e," \u2014 "),rq=s(p6e,"A",{href:!0});var itt=n(rq);FZo=r(itt,"TFDebertaForMaskedLM"),itt.forEach(t),CZo=r(p6e," (DeBERTa model)"),p6e.forEach(t),MZo=i(ee),KF=s(ee,"LI",{});var _6e=n(KF);Lfe=s(_6e,"STRONG",{});var dtt=n(Lfe);EZo=r(dtt,"deberta-v2"),dtt.forEach(t),yZo=r(_6e," \u2014 "),tq=s(_6e,"A",{href:!0});var ctt=n(tq);wZo=r(ctt,"TFDebertaV2ForMaskedLM"),ctt.forEach(t),AZo=r(_6e," (DeBERTa-v2 model)"),_6e.forEach(t),LZo=i(ee),ZF=s(ee,"LI",{});var b6e=n(ZF);Bfe=s(b6e,"STRONG",{});var mtt=n(Bfe);BZo=r(mtt,"distilbert"),mtt.forEach(t),kZo=r(b6e," \u2014 "),aq=s(b6e,"A",{href:!0});var ftt=n(aq);xZo=r(ftt,"TFDistilBertForMaskedLM"),ftt.forEach(t),RZo=r(b6e," (DistilBERT model)"),b6e.forEach(t),SZo=i(ee),eC=s(ee,"LI",{});var v6e=n(eC);kfe=s(v6e,"STRONG",{});var gtt=n(kfe);PZo=r(gtt,"electra"),gtt.forEach(t),$Zo=r(v6e," \u2014 "),sq=s(v6e,"A",{href:!0});var htt=n(sq);IZo=r(htt,"TFElectraForMaskedLM"),htt.forEach(t),jZo=r(v6e," (ELECTRA model)"),v6e.forEach(t),NZo=i(ee),oC=s(ee,"LI",{});var T6e=n(oC);xfe=s(T6e,"STRONG",{});var utt=n(xfe);DZo=r(utt,"flaubert"),utt.forEach(t),qZo=r(T6e," \u2014 "),nq=s(T6e,"A",{href:!0});var ptt=n(nq);GZo=r(ptt,"TFFlaubertWithLMHeadModel"),ptt.forEach(t),OZo=r(T6e," (FlauBERT model)"),T6e.forEach(t),XZo=i(ee),rC=s(ee,"LI",{});var F6e=n(rC);Rfe=s(F6e,"STRONG",{});var _tt=n(Rfe);zZo=r(_tt,"funnel"),_tt.forEach(t),VZo=r(F6e," \u2014 "),lq=s(F6e,"A",{href:!0});var btt=n(lq);WZo=r(btt,"TFFunnelForMaskedLM"),btt.forEach(t),QZo=r(F6e," (Funnel Transformer model)"),F6e.forEach(t),HZo=i(ee),tC=s(ee,"LI",{});var C6e=n(tC);Sfe=s(C6e,"STRONG",{});var vtt=n(Sfe);UZo=r(vtt,"layoutlm"),vtt.forEach(t),JZo=r(C6e," \u2014 "),iq=s(C6e,"A",{href:!0});var Ttt=n(iq);YZo=r(Ttt,"TFLayoutLMForMaskedLM"),Ttt.forEach(t),KZo=r(C6e," (LayoutLM model)"),C6e.forEach(t),ZZo=i(ee),aC=s(ee,"LI",{});var M6e=n(aC);Pfe=s(M6e,"STRONG",{});var Ftt=n(Pfe);eer=r(Ftt,"longformer"),Ftt.forEach(t),oer=r(M6e," \u2014 "),dq=s(M6e,"A",{href:!0});var Ctt=n(dq);rer=r(Ctt,"TFLongformerForMaskedLM"),Ctt.forEach(t),ter=r(M6e," (Longformer model)"),M6e.forEach(t),aer=i(ee),sC=s(ee,"LI",{});var E6e=n(sC);$fe=s(E6e,"STRONG",{});var Mtt=n($fe);ser=r(Mtt,"mobilebert"),Mtt.forEach(t),ner=r(E6e," \u2014 "),cq=s(E6e,"A",{href:!0});var Ett=n(cq);ler=r(Ett,"TFMobileBertForMaskedLM"),Ett.forEach(t),ier=r(E6e," (MobileBERT model)"),E6e.forEach(t),der=i(ee),nC=s(ee,"LI",{});var y6e=n(nC);Ife=s(y6e,"STRONG",{});var ytt=n(Ife);cer=r(ytt,"mpnet"),ytt.forEach(t),mer=r(y6e," \u2014 "),mq=s(y6e,"A",{href:!0});var wtt=n(mq);fer=r(wtt,"TFMPNetForMaskedLM"),wtt.forEach(t),ger=r(y6e," (MPNet model)"),y6e.forEach(t),her=i(ee),lC=s(ee,"LI",{});var w6e=n(lC);jfe=s(w6e,"STRONG",{});var Att=n(jfe);uer=r(Att,"rembert"),Att.forEach(t),per=r(w6e," \u2014 "),fq=s(w6e,"A",{href:!0});var Ltt=n(fq);_er=r(Ltt,"TFRemBertForMaskedLM"),Ltt.forEach(t),ber=r(w6e," (RemBERT model)"),w6e.forEach(t),ver=i(ee),iC=s(ee,"LI",{});var A6e=n(iC);Nfe=s(A6e,"STRONG",{});var Btt=n(Nfe);Ter=r(Btt,"roberta"),Btt.forEach(t),Fer=r(A6e," \u2014 "),gq=s(A6e,"A",{href:!0});var ktt=n(gq);Cer=r(ktt,"TFRobertaForMaskedLM"),ktt.forEach(t),Mer=r(A6e," (RoBERTa model)"),A6e.forEach(t),Eer=i(ee),dC=s(ee,"LI",{});var L6e=n(dC);Dfe=s(L6e,"STRONG",{});var xtt=n(Dfe);yer=r(xtt,"roformer"),xtt.forEach(t),wer=r(L6e," \u2014 "),hq=s(L6e,"A",{href:!0});var Rtt=n(hq);Aer=r(Rtt,"TFRoFormerForMaskedLM"),Rtt.forEach(t),Ler=r(L6e," (RoFormer model)"),L6e.forEach(t),Ber=i(ee),cC=s(ee,"LI",{});var B6e=n(cC);qfe=s(B6e,"STRONG",{});var Stt=n(qfe);ker=r(Stt,"tapas"),Stt.forEach(t),xer=r(B6e," \u2014 "),uq=s(B6e,"A",{href:!0});var Ptt=n(uq);Rer=r(Ptt,"TFTapasForMaskedLM"),Ptt.forEach(t),Ser=r(B6e," (TAPAS model)"),B6e.forEach(t),Per=i(ee),mC=s(ee,"LI",{});var k6e=n(mC);Gfe=s(k6e,"STRONG",{});var $tt=n(Gfe);$er=r($tt,"xlm"),$tt.forEach(t),Ier=r(k6e," \u2014 "),pq=s(k6e,"A",{href:!0});var Itt=n(pq);jer=r(Itt,"TFXLMWithLMHeadModel"),Itt.forEach(t),Ner=r(k6e," (XLM model)"),k6e.forEach(t),Der=i(ee),fC=s(ee,"LI",{});var x6e=n(fC);Ofe=s(x6e,"STRONG",{});var jtt=n(Ofe);qer=r(jtt,"xlm-roberta"),jtt.forEach(t),Ger=r(x6e," \u2014 "),_q=s(x6e,"A",{href:!0});var Ntt=n(_q);Oer=r(Ntt,"TFXLMRobertaForMaskedLM"),Ntt.forEach(t),Xer=r(x6e," (XLM-RoBERTa model)"),x6e.forEach(t),ee.forEach(t),zer=i(ha),Xfe=s(ha,"P",{});var Dtt=n(Xfe);Ver=r(Dtt,"Examples:"),Dtt.forEach(t),Wer=i(ha),f(d6.$$.fragment,ha),ha.forEach(t),jl.forEach(t),p9e=i(d),pc=s(d,"H2",{class:!0});var yke=n(pc);gC=s(yke,"A",{id:!0,class:!0,href:!0});var qtt=n(gC);zfe=s(qtt,"SPAN",{});var Gtt=n(zfe);f(c6.$$.fragment,Gtt),Gtt.forEach(t),qtt.forEach(t),Qer=i(yke),Vfe=s(yke,"SPAN",{});var Ott=n(Vfe);Her=r(Ott,"TFAutoModelForSeq2SeqLM"),Ott.forEach(t),yke.forEach(t),_9e=i(d),br=s(d,"DIV",{class:!0});var Dl=n(br);f(m6.$$.fragment,Dl),Uer=i(Dl),_c=s(Dl,"P",{});var Uz=n(_c);Jer=r(Uz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Wfe=s(Uz,"CODE",{});var Xtt=n(Wfe);Yer=r(Xtt,"from_pretrained()"),Xtt.forEach(t),Ker=r(Uz,"class method or the "),Qfe=s(Uz,"CODE",{});var ztt=n(Qfe);Zer=r(ztt,"from_config()"),ztt.forEach(t),eor=r(Uz,`class
method.`),Uz.forEach(t),oor=i(Dl),f6=s(Dl,"P",{});var wke=n(f6);ror=r(wke,"This class cannot be instantiated directly using "),Hfe=s(wke,"CODE",{});var Vtt=n(Hfe);tor=r(Vtt,"__init__()"),Vtt.forEach(t),aor=r(wke," (throws an error)."),wke.forEach(t),sor=i(Dl),mt=s(Dl,"DIV",{class:!0});var ql=n(mt);f(g6.$$.fragment,ql),nor=i(ql),Ufe=s(ql,"P",{});var Wtt=n(Ufe);lor=r(Wtt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Wtt.forEach(t),ior=i(ql),bc=s(ql,"P",{});var Jz=n(bc);dor=r(Jz,`Note:
Loading a model from its configuration file does `),Jfe=s(Jz,"STRONG",{});var Qtt=n(Jfe);cor=r(Qtt,"not"),Qtt.forEach(t),mor=r(Jz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yfe=s(Jz,"CODE",{});var Htt=n(Yfe);gor=r(Htt,"from_pretrained()"),Htt.forEach(t),hor=r(Jz,"to load the model weights."),Jz.forEach(t),uor=i(ql),Kfe=s(ql,"P",{});var Utt=n(Kfe);por=r(Utt,"Examples:"),Utt.forEach(t),_or=i(ql),f(h6.$$.fragment,ql),ql.forEach(t),bor=i(Dl),bo=s(Dl,"DIV",{class:!0});var ua=n(bo);f(u6.$$.fragment,ua),vor=i(ua),Zfe=s(ua,"P",{});var Jtt=n(Zfe);Tor=r(Jtt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Jtt.forEach(t),For=i(ua),fs=s(ua,"P",{});var P3=n(fs);Cor=r(P3,"The model class to instantiate is selected based on the "),ege=s(P3,"CODE",{});var Ytt=n(ege);Mor=r(Ytt,"model_type"),Ytt.forEach(t),Eor=r(P3,` property of the config object (either
passed as an argument or loaded from `),oge=s(P3,"CODE",{});var Ktt=n(oge);yor=r(Ktt,"pretrained_model_name_or_path"),Ktt.forEach(t),wor=r(P3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rge=s(P3,"CODE",{});var Ztt=n(rge);Aor=r(Ztt,"pretrained_model_name_or_path"),Ztt.forEach(t),Lor=r(P3,":"),P3.forEach(t),Bor=i(ua),ue=s(ua,"UL",{});var Ee=n(ue);hC=s(Ee,"LI",{});var R6e=n(hC);tge=s(R6e,"STRONG",{});var eat=n(tge);kor=r(eat,"bart"),eat.forEach(t),xor=r(R6e," \u2014 "),bq=s(R6e,"A",{href:!0});var oat=n(bq);Ror=r(oat,"TFBartForConditionalGeneration"),oat.forEach(t),Sor=r(R6e," (BART model)"),R6e.forEach(t),Por=i(Ee),uC=s(Ee,"LI",{});var S6e=n(uC);age=s(S6e,"STRONG",{});var rat=n(age);$or=r(rat,"blenderbot"),rat.forEach(t),Ior=r(S6e," \u2014 "),vq=s(S6e,"A",{href:!0});var tat=n(vq);jor=r(tat,"TFBlenderbotForConditionalGeneration"),tat.forEach(t),Nor=r(S6e," (Blenderbot model)"),S6e.forEach(t),Dor=i(Ee),pC=s(Ee,"LI",{});var P6e=n(pC);sge=s(P6e,"STRONG",{});var aat=n(sge);qor=r(aat,"blenderbot-small"),aat.forEach(t),Gor=r(P6e," \u2014 "),Tq=s(P6e,"A",{href:!0});var sat=n(Tq);Oor=r(sat,"TFBlenderbotSmallForConditionalGeneration"),sat.forEach(t),Xor=r(P6e," (BlenderbotSmall model)"),P6e.forEach(t),zor=i(Ee),_C=s(Ee,"LI",{});var $6e=n(_C);nge=s($6e,"STRONG",{});var nat=n(nge);Vor=r(nat,"encoder-decoder"),nat.forEach(t),Wor=r($6e," \u2014 "),Fq=s($6e,"A",{href:!0});var lat=n(Fq);Qor=r(lat,"TFEncoderDecoderModel"),lat.forEach(t),Hor=r($6e," (Encoder decoder model)"),$6e.forEach(t),Uor=i(Ee),bC=s(Ee,"LI",{});var I6e=n(bC);lge=s(I6e,"STRONG",{});var iat=n(lge);Jor=r(iat,"led"),iat.forEach(t),Yor=r(I6e," \u2014 "),Cq=s(I6e,"A",{href:!0});var dat=n(Cq);Kor=r(dat,"TFLEDForConditionalGeneration"),dat.forEach(t),Zor=r(I6e," (LED model)"),I6e.forEach(t),err=i(Ee),vC=s(Ee,"LI",{});var j6e=n(vC);ige=s(j6e,"STRONG",{});var cat=n(ige);orr=r(cat,"marian"),cat.forEach(t),rrr=r(j6e," \u2014 "),Mq=s(j6e,"A",{href:!0});var mat=n(Mq);trr=r(mat,"TFMarianMTModel"),mat.forEach(t),arr=r(j6e," (Marian model)"),j6e.forEach(t),srr=i(Ee),TC=s(Ee,"LI",{});var N6e=n(TC);dge=s(N6e,"STRONG",{});var fat=n(dge);nrr=r(fat,"mbart"),fat.forEach(t),lrr=r(N6e," \u2014 "),Eq=s(N6e,"A",{href:!0});var gat=n(Eq);irr=r(gat,"TFMBartForConditionalGeneration"),gat.forEach(t),drr=r(N6e," (mBART model)"),N6e.forEach(t),crr=i(Ee),FC=s(Ee,"LI",{});var D6e=n(FC);cge=s(D6e,"STRONG",{});var hat=n(cge);mrr=r(hat,"mt5"),hat.forEach(t),frr=r(D6e," \u2014 "),yq=s(D6e,"A",{href:!0});var uat=n(yq);grr=r(uat,"TFMT5ForConditionalGeneration"),uat.forEach(t),hrr=r(D6e," (mT5 model)"),D6e.forEach(t),urr=i(Ee),CC=s(Ee,"LI",{});var q6e=n(CC);mge=s(q6e,"STRONG",{});var pat=n(mge);prr=r(pat,"pegasus"),pat.forEach(t),_rr=r(q6e," \u2014 "),wq=s(q6e,"A",{href:!0});var _at=n(wq);brr=r(_at,"TFPegasusForConditionalGeneration"),_at.forEach(t),vrr=r(q6e," (Pegasus model)"),q6e.forEach(t),Trr=i(Ee),MC=s(Ee,"LI",{});var G6e=n(MC);fge=s(G6e,"STRONG",{});var bat=n(fge);Frr=r(bat,"t5"),bat.forEach(t),Crr=r(G6e," \u2014 "),Aq=s(G6e,"A",{href:!0});var vat=n(Aq);Mrr=r(vat,"TFT5ForConditionalGeneration"),vat.forEach(t),Err=r(G6e," (T5 model)"),G6e.forEach(t),Ee.forEach(t),yrr=i(ua),gge=s(ua,"P",{});var Tat=n(gge);wrr=r(Tat,"Examples:"),Tat.forEach(t),Arr=i(ua),f(p6.$$.fragment,ua),ua.forEach(t),Dl.forEach(t),b9e=i(d),vc=s(d,"H2",{class:!0});var Ake=n(vc);EC=s(Ake,"A",{id:!0,class:!0,href:!0});var Fat=n(EC);hge=s(Fat,"SPAN",{});var Cat=n(hge);f(_6.$$.fragment,Cat),Cat.forEach(t),Fat.forEach(t),Lrr=i(Ake),uge=s(Ake,"SPAN",{});var Mat=n(uge);Brr=r(Mat,"TFAutoModelForSequenceClassification"),Mat.forEach(t),Ake.forEach(t),v9e=i(d),vr=s(d,"DIV",{class:!0});var Gl=n(vr);f(b6.$$.fragment,Gl),krr=i(Gl),Tc=s(Gl,"P",{});var Yz=n(Tc);xrr=r(Yz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),pge=s(Yz,"CODE",{});var Eat=n(pge);Rrr=r(Eat,"from_pretrained()"),Eat.forEach(t),Srr=r(Yz,"class method or the "),_ge=s(Yz,"CODE",{});var yat=n(_ge);Prr=r(yat,"from_config()"),yat.forEach(t),$rr=r(Yz,`class
method.`),Yz.forEach(t),Irr=i(Gl),v6=s(Gl,"P",{});var Lke=n(v6);jrr=r(Lke,"This class cannot be instantiated directly using "),bge=s(Lke,"CODE",{});var wat=n(bge);Nrr=r(wat,"__init__()"),wat.forEach(t),Drr=r(Lke," (throws an error)."),Lke.forEach(t),qrr=i(Gl),ft=s(Gl,"DIV",{class:!0});var Ol=n(ft);f(T6.$$.fragment,Ol),Grr=i(Ol),vge=s(Ol,"P",{});var Aat=n(vge);Orr=r(Aat,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Aat.forEach(t),Xrr=i(Ol),Fc=s(Ol,"P",{});var Kz=n(Fc);zrr=r(Kz,`Note:
Loading a model from its configuration file does `),Tge=s(Kz,"STRONG",{});var Lat=n(Tge);Vrr=r(Lat,"not"),Lat.forEach(t),Wrr=r(Kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fge=s(Kz,"CODE",{});var Bat=n(Fge);Qrr=r(Bat,"from_pretrained()"),Bat.forEach(t),Hrr=r(Kz,"to load the model weights."),Kz.forEach(t),Urr=i(Ol),Cge=s(Ol,"P",{});var kat=n(Cge);Jrr=r(kat,"Examples:"),kat.forEach(t),Yrr=i(Ol),f(F6.$$.fragment,Ol),Ol.forEach(t),Krr=i(Gl),vo=s(Gl,"DIV",{class:!0});var pa=n(vo);f(C6.$$.fragment,pa),Zrr=i(pa),Mge=s(pa,"P",{});var xat=n(Mge);etr=r(xat,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),xat.forEach(t),otr=i(pa),gs=s(pa,"P",{});var $3=n(gs);rtr=r($3,"The model class to instantiate is selected based on the "),Ege=s($3,"CODE",{});var Rat=n(Ege);ttr=r(Rat,"model_type"),Rat.forEach(t),atr=r($3,` property of the config object (either
passed as an argument or loaded from `),yge=s($3,"CODE",{});var Sat=n(yge);str=r(Sat,"pretrained_model_name_or_path"),Sat.forEach(t),ntr=r($3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wge=s($3,"CODE",{});var Pat=n(wge);ltr=r(Pat,"pretrained_model_name_or_path"),Pat.forEach(t),itr=r($3,":"),$3.forEach(t),dtr=i(pa),X=s(pa,"UL",{});var W=n(X);yC=s(W,"LI",{});var O6e=n(yC);Age=s(O6e,"STRONG",{});var $at=n(Age);ctr=r($at,"albert"),$at.forEach(t),mtr=r(O6e," \u2014 "),Lq=s(O6e,"A",{href:!0});var Iat=n(Lq);ftr=r(Iat,"TFAlbertForSequenceClassification"),Iat.forEach(t),gtr=r(O6e," (ALBERT model)"),O6e.forEach(t),htr=i(W),wC=s(W,"LI",{});var X6e=n(wC);Lge=s(X6e,"STRONG",{});var jat=n(Lge);utr=r(jat,"bert"),jat.forEach(t),ptr=r(X6e," \u2014 "),Bq=s(X6e,"A",{href:!0});var Nat=n(Bq);_tr=r(Nat,"TFBertForSequenceClassification"),Nat.forEach(t),btr=r(X6e," (BERT model)"),X6e.forEach(t),vtr=i(W),AC=s(W,"LI",{});var z6e=n(AC);Bge=s(z6e,"STRONG",{});var Dat=n(Bge);Ttr=r(Dat,"camembert"),Dat.forEach(t),Ftr=r(z6e," \u2014 "),kq=s(z6e,"A",{href:!0});var qat=n(kq);Ctr=r(qat,"TFCamembertForSequenceClassification"),qat.forEach(t),Mtr=r(z6e," (CamemBERT model)"),z6e.forEach(t),Etr=i(W),LC=s(W,"LI",{});var V6e=n(LC);kge=s(V6e,"STRONG",{});var Gat=n(kge);ytr=r(Gat,"convbert"),Gat.forEach(t),wtr=r(V6e," \u2014 "),xq=s(V6e,"A",{href:!0});var Oat=n(xq);Atr=r(Oat,"TFConvBertForSequenceClassification"),Oat.forEach(t),Ltr=r(V6e," (ConvBERT model)"),V6e.forEach(t),Btr=i(W),BC=s(W,"LI",{});var W6e=n(BC);xge=s(W6e,"STRONG",{});var Xat=n(xge);ktr=r(Xat,"ctrl"),Xat.forEach(t),xtr=r(W6e," \u2014 "),Rq=s(W6e,"A",{href:!0});var zat=n(Rq);Rtr=r(zat,"TFCTRLForSequenceClassification"),zat.forEach(t),Str=r(W6e," (CTRL model)"),W6e.forEach(t),Ptr=i(W),kC=s(W,"LI",{});var Q6e=n(kC);Rge=s(Q6e,"STRONG",{});var Vat=n(Rge);$tr=r(Vat,"deberta"),Vat.forEach(t),Itr=r(Q6e," \u2014 "),Sq=s(Q6e,"A",{href:!0});var Wat=n(Sq);jtr=r(Wat,"TFDebertaForSequenceClassification"),Wat.forEach(t),Ntr=r(Q6e," (DeBERTa model)"),Q6e.forEach(t),Dtr=i(W),xC=s(W,"LI",{});var H6e=n(xC);Sge=s(H6e,"STRONG",{});var Qat=n(Sge);qtr=r(Qat,"deberta-v2"),Qat.forEach(t),Gtr=r(H6e," \u2014 "),Pq=s(H6e,"A",{href:!0});var Hat=n(Pq);Otr=r(Hat,"TFDebertaV2ForSequenceClassification"),Hat.forEach(t),Xtr=r(H6e," (DeBERTa-v2 model)"),H6e.forEach(t),ztr=i(W),RC=s(W,"LI",{});var U6e=n(RC);Pge=s(U6e,"STRONG",{});var Uat=n(Pge);Vtr=r(Uat,"distilbert"),Uat.forEach(t),Wtr=r(U6e," \u2014 "),$q=s(U6e,"A",{href:!0});var Jat=n($q);Qtr=r(Jat,"TFDistilBertForSequenceClassification"),Jat.forEach(t),Htr=r(U6e," (DistilBERT model)"),U6e.forEach(t),Utr=i(W),SC=s(W,"LI",{});var J6e=n(SC);$ge=s(J6e,"STRONG",{});var Yat=n($ge);Jtr=r(Yat,"electra"),Yat.forEach(t),Ytr=r(J6e," \u2014 "),Iq=s(J6e,"A",{href:!0});var Kat=n(Iq);Ktr=r(Kat,"TFElectraForSequenceClassification"),Kat.forEach(t),Ztr=r(J6e," (ELECTRA model)"),J6e.forEach(t),ear=i(W),PC=s(W,"LI",{});var Y6e=n(PC);Ige=s(Y6e,"STRONG",{});var Zat=n(Ige);oar=r(Zat,"flaubert"),Zat.forEach(t),rar=r(Y6e," \u2014 "),jq=s(Y6e,"A",{href:!0});var est=n(jq);tar=r(est,"TFFlaubertForSequenceClassification"),est.forEach(t),aar=r(Y6e," (FlauBERT model)"),Y6e.forEach(t),sar=i(W),$C=s(W,"LI",{});var K6e=n($C);jge=s(K6e,"STRONG",{});var ost=n(jge);nar=r(ost,"funnel"),ost.forEach(t),lar=r(K6e," \u2014 "),Nq=s(K6e,"A",{href:!0});var rst=n(Nq);iar=r(rst,"TFFunnelForSequenceClassification"),rst.forEach(t),dar=r(K6e," (Funnel Transformer model)"),K6e.forEach(t),car=i(W),IC=s(W,"LI",{});var Z6e=n(IC);Nge=s(Z6e,"STRONG",{});var tst=n(Nge);mar=r(tst,"gpt2"),tst.forEach(t),far=r(Z6e," \u2014 "),Dq=s(Z6e,"A",{href:!0});var ast=n(Dq);gar=r(ast,"TFGPT2ForSequenceClassification"),ast.forEach(t),har=r(Z6e," (OpenAI GPT-2 model)"),Z6e.forEach(t),uar=i(W),jC=s(W,"LI",{});var e0e=n(jC);Dge=s(e0e,"STRONG",{});var sst=n(Dge);par=r(sst,"layoutlm"),sst.forEach(t),_ar=r(e0e," \u2014 "),qq=s(e0e,"A",{href:!0});var nst=n(qq);bar=r(nst,"TFLayoutLMForSequenceClassification"),nst.forEach(t),Tar=r(e0e," (LayoutLM model)"),e0e.forEach(t),Far=i(W),NC=s(W,"LI",{});var o0e=n(NC);qge=s(o0e,"STRONG",{});var lst=n(qge);Car=r(lst,"longformer"),lst.forEach(t),Mar=r(o0e," \u2014 "),Gq=s(o0e,"A",{href:!0});var ist=n(Gq);Ear=r(ist,"TFLongformerForSequenceClassification"),ist.forEach(t),yar=r(o0e," (Longformer model)"),o0e.forEach(t),war=i(W),DC=s(W,"LI",{});var r0e=n(DC);Gge=s(r0e,"STRONG",{});var dst=n(Gge);Aar=r(dst,"mobilebert"),dst.forEach(t),Lar=r(r0e," \u2014 "),Oq=s(r0e,"A",{href:!0});var cst=n(Oq);Bar=r(cst,"TFMobileBertForSequenceClassification"),cst.forEach(t),kar=r(r0e," (MobileBERT model)"),r0e.forEach(t),xar=i(W),qC=s(W,"LI",{});var t0e=n(qC);Oge=s(t0e,"STRONG",{});var mst=n(Oge);Rar=r(mst,"mpnet"),mst.forEach(t),Sar=r(t0e," \u2014 "),Xq=s(t0e,"A",{href:!0});var fst=n(Xq);Par=r(fst,"TFMPNetForSequenceClassification"),fst.forEach(t),$ar=r(t0e," (MPNet model)"),t0e.forEach(t),Iar=i(W),GC=s(W,"LI",{});var a0e=n(GC);Xge=s(a0e,"STRONG",{});var gst=n(Xge);jar=r(gst,"openai-gpt"),gst.forEach(t),Nar=r(a0e," \u2014 "),zq=s(a0e,"A",{href:!0});var hst=n(zq);Dar=r(hst,"TFOpenAIGPTForSequenceClassification"),hst.forEach(t),qar=r(a0e," (OpenAI GPT model)"),a0e.forEach(t),Gar=i(W),OC=s(W,"LI",{});var s0e=n(OC);zge=s(s0e,"STRONG",{});var ust=n(zge);Oar=r(ust,"rembert"),ust.forEach(t),Xar=r(s0e," \u2014 "),Vq=s(s0e,"A",{href:!0});var pst=n(Vq);zar=r(pst,"TFRemBertForSequenceClassification"),pst.forEach(t),Var=r(s0e," (RemBERT model)"),s0e.forEach(t),War=i(W),XC=s(W,"LI",{});var n0e=n(XC);Vge=s(n0e,"STRONG",{});var _st=n(Vge);Qar=r(_st,"roberta"),_st.forEach(t),Har=r(n0e," \u2014 "),Wq=s(n0e,"A",{href:!0});var bst=n(Wq);Uar=r(bst,"TFRobertaForSequenceClassification"),bst.forEach(t),Jar=r(n0e," (RoBERTa model)"),n0e.forEach(t),Yar=i(W),zC=s(W,"LI",{});var l0e=n(zC);Wge=s(l0e,"STRONG",{});var vst=n(Wge);Kar=r(vst,"roformer"),vst.forEach(t),Zar=r(l0e," \u2014 "),Qq=s(l0e,"A",{href:!0});var Tst=n(Qq);esr=r(Tst,"TFRoFormerForSequenceClassification"),Tst.forEach(t),osr=r(l0e," (RoFormer model)"),l0e.forEach(t),rsr=i(W),VC=s(W,"LI",{});var i0e=n(VC);Qge=s(i0e,"STRONG",{});var Fst=n(Qge);tsr=r(Fst,"tapas"),Fst.forEach(t),asr=r(i0e," \u2014 "),Hq=s(i0e,"A",{href:!0});var Cst=n(Hq);ssr=r(Cst,"TFTapasForSequenceClassification"),Cst.forEach(t),nsr=r(i0e," (TAPAS model)"),i0e.forEach(t),lsr=i(W),WC=s(W,"LI",{});var d0e=n(WC);Hge=s(d0e,"STRONG",{});var Mst=n(Hge);isr=r(Mst,"transfo-xl"),Mst.forEach(t),dsr=r(d0e," \u2014 "),Uq=s(d0e,"A",{href:!0});var Est=n(Uq);csr=r(Est,"TFTransfoXLForSequenceClassification"),Est.forEach(t),msr=r(d0e," (Transformer-XL model)"),d0e.forEach(t),fsr=i(W),QC=s(W,"LI",{});var c0e=n(QC);Uge=s(c0e,"STRONG",{});var yst=n(Uge);gsr=r(yst,"xlm"),yst.forEach(t),hsr=r(c0e," \u2014 "),Jq=s(c0e,"A",{href:!0});var wst=n(Jq);usr=r(wst,"TFXLMForSequenceClassification"),wst.forEach(t),psr=r(c0e," (XLM model)"),c0e.forEach(t),_sr=i(W),HC=s(W,"LI",{});var m0e=n(HC);Jge=s(m0e,"STRONG",{});var Ast=n(Jge);bsr=r(Ast,"xlm-roberta"),Ast.forEach(t),vsr=r(m0e," \u2014 "),Yq=s(m0e,"A",{href:!0});var Lst=n(Yq);Tsr=r(Lst,"TFXLMRobertaForSequenceClassification"),Lst.forEach(t),Fsr=r(m0e," (XLM-RoBERTa model)"),m0e.forEach(t),Csr=i(W),UC=s(W,"LI",{});var f0e=n(UC);Yge=s(f0e,"STRONG",{});var Bst=n(Yge);Msr=r(Bst,"xlnet"),Bst.forEach(t),Esr=r(f0e," \u2014 "),Kq=s(f0e,"A",{href:!0});var kst=n(Kq);ysr=r(kst,"TFXLNetForSequenceClassification"),kst.forEach(t),wsr=r(f0e," (XLNet model)"),f0e.forEach(t),W.forEach(t),Asr=i(pa),Kge=s(pa,"P",{});var xst=n(Kge);Lsr=r(xst,"Examples:"),xst.forEach(t),Bsr=i(pa),f(M6.$$.fragment,pa),pa.forEach(t),Gl.forEach(t),T9e=i(d),Cc=s(d,"H2",{class:!0});var Bke=n(Cc);JC=s(Bke,"A",{id:!0,class:!0,href:!0});var Rst=n(JC);Zge=s(Rst,"SPAN",{});var Sst=n(Zge);f(E6.$$.fragment,Sst),Sst.forEach(t),Rst.forEach(t),ksr=i(Bke),ehe=s(Bke,"SPAN",{});var Pst=n(ehe);xsr=r(Pst,"TFAutoModelForMultipleChoice"),Pst.forEach(t),Bke.forEach(t),F9e=i(d),Tr=s(d,"DIV",{class:!0});var Xl=n(Tr);f(y6.$$.fragment,Xl),Rsr=i(Xl),Mc=s(Xl,"P",{});var Zz=n(Mc);Ssr=r(Zz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ohe=s(Zz,"CODE",{});var $st=n(ohe);Psr=r($st,"from_pretrained()"),$st.forEach(t),$sr=r(Zz,"class method or the "),rhe=s(Zz,"CODE",{});var Ist=n(rhe);Isr=r(Ist,"from_config()"),Ist.forEach(t),jsr=r(Zz,`class
method.`),Zz.forEach(t),Nsr=i(Xl),w6=s(Xl,"P",{});var kke=n(w6);Dsr=r(kke,"This class cannot be instantiated directly using "),the=s(kke,"CODE",{});var jst=n(the);qsr=r(jst,"__init__()"),jst.forEach(t),Gsr=r(kke," (throws an error)."),kke.forEach(t),Osr=i(Xl),gt=s(Xl,"DIV",{class:!0});var zl=n(gt);f(A6.$$.fragment,zl),Xsr=i(zl),ahe=s(zl,"P",{});var Nst=n(ahe);zsr=r(Nst,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Nst.forEach(t),Vsr=i(zl),Ec=s(zl,"P",{});var eV=n(Ec);Wsr=r(eV,`Note:
Loading a model from its configuration file does `),she=s(eV,"STRONG",{});var Dst=n(she);Qsr=r(Dst,"not"),Dst.forEach(t),Hsr=r(eV,` load the model weights. It only affects the
model\u2019s configuration. Use `),nhe=s(eV,"CODE",{});var qst=n(nhe);Usr=r(qst,"from_pretrained()"),qst.forEach(t),Jsr=r(eV,"to load the model weights."),eV.forEach(t),Ysr=i(zl),lhe=s(zl,"P",{});var Gst=n(lhe);Ksr=r(Gst,"Examples:"),Gst.forEach(t),Zsr=i(zl),f(L6.$$.fragment,zl),zl.forEach(t),enr=i(Xl),To=s(Xl,"DIV",{class:!0});var _a=n(To);f(B6.$$.fragment,_a),onr=i(_a),ihe=s(_a,"P",{});var Ost=n(ihe);rnr=r(Ost,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Ost.forEach(t),tnr=i(_a),hs=s(_a,"P",{});var I3=n(hs);anr=r(I3,"The model class to instantiate is selected based on the "),dhe=s(I3,"CODE",{});var Xst=n(dhe);snr=r(Xst,"model_type"),Xst.forEach(t),nnr=r(I3,` property of the config object (either
passed as an argument or loaded from `),che=s(I3,"CODE",{});var zst=n(che);lnr=r(zst,"pretrained_model_name_or_path"),zst.forEach(t),inr=r(I3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mhe=s(I3,"CODE",{});var Vst=n(mhe);dnr=r(Vst,"pretrained_model_name_or_path"),Vst.forEach(t),cnr=r(I3,":"),I3.forEach(t),mnr=i(_a),te=s(_a,"UL",{});var se=n(te);YC=s(se,"LI",{});var g0e=n(YC);fhe=s(g0e,"STRONG",{});var Wst=n(fhe);fnr=r(Wst,"albert"),Wst.forEach(t),gnr=r(g0e," \u2014 "),Zq=s(g0e,"A",{href:!0});var Qst=n(Zq);hnr=r(Qst,"TFAlbertForMultipleChoice"),Qst.forEach(t),unr=r(g0e," (ALBERT model)"),g0e.forEach(t),pnr=i(se),KC=s(se,"LI",{});var h0e=n(KC);ghe=s(h0e,"STRONG",{});var Hst=n(ghe);_nr=r(Hst,"bert"),Hst.forEach(t),bnr=r(h0e," \u2014 "),eG=s(h0e,"A",{href:!0});var Ust=n(eG);vnr=r(Ust,"TFBertForMultipleChoice"),Ust.forEach(t),Tnr=r(h0e," (BERT model)"),h0e.forEach(t),Fnr=i(se),ZC=s(se,"LI",{});var u0e=n(ZC);hhe=s(u0e,"STRONG",{});var Jst=n(hhe);Cnr=r(Jst,"camembert"),Jst.forEach(t),Mnr=r(u0e," \u2014 "),oG=s(u0e,"A",{href:!0});var Yst=n(oG);Enr=r(Yst,"TFCamembertForMultipleChoice"),Yst.forEach(t),ynr=r(u0e," (CamemBERT model)"),u0e.forEach(t),wnr=i(se),e4=s(se,"LI",{});var p0e=n(e4);uhe=s(p0e,"STRONG",{});var Kst=n(uhe);Anr=r(Kst,"convbert"),Kst.forEach(t),Lnr=r(p0e," \u2014 "),rG=s(p0e,"A",{href:!0});var Zst=n(rG);Bnr=r(Zst,"TFConvBertForMultipleChoice"),Zst.forEach(t),knr=r(p0e," (ConvBERT model)"),p0e.forEach(t),xnr=i(se),o4=s(se,"LI",{});var _0e=n(o4);phe=s(_0e,"STRONG",{});var ent=n(phe);Rnr=r(ent,"distilbert"),ent.forEach(t),Snr=r(_0e," \u2014 "),tG=s(_0e,"A",{href:!0});var ont=n(tG);Pnr=r(ont,"TFDistilBertForMultipleChoice"),ont.forEach(t),$nr=r(_0e," (DistilBERT model)"),_0e.forEach(t),Inr=i(se),r4=s(se,"LI",{});var b0e=n(r4);_he=s(b0e,"STRONG",{});var rnt=n(_he);jnr=r(rnt,"electra"),rnt.forEach(t),Nnr=r(b0e," \u2014 "),aG=s(b0e,"A",{href:!0});var tnt=n(aG);Dnr=r(tnt,"TFElectraForMultipleChoice"),tnt.forEach(t),qnr=r(b0e," (ELECTRA model)"),b0e.forEach(t),Gnr=i(se),t4=s(se,"LI",{});var v0e=n(t4);bhe=s(v0e,"STRONG",{});var ant=n(bhe);Onr=r(ant,"flaubert"),ant.forEach(t),Xnr=r(v0e," \u2014 "),sG=s(v0e,"A",{href:!0});var snt=n(sG);znr=r(snt,"TFFlaubertForMultipleChoice"),snt.forEach(t),Vnr=r(v0e," (FlauBERT model)"),v0e.forEach(t),Wnr=i(se),a4=s(se,"LI",{});var T0e=n(a4);vhe=s(T0e,"STRONG",{});var nnt=n(vhe);Qnr=r(nnt,"funnel"),nnt.forEach(t),Hnr=r(T0e," \u2014 "),nG=s(T0e,"A",{href:!0});var lnt=n(nG);Unr=r(lnt,"TFFunnelForMultipleChoice"),lnt.forEach(t),Jnr=r(T0e," (Funnel Transformer model)"),T0e.forEach(t),Ynr=i(se),s4=s(se,"LI",{});var F0e=n(s4);The=s(F0e,"STRONG",{});var int=n(The);Knr=r(int,"longformer"),int.forEach(t),Znr=r(F0e," \u2014 "),lG=s(F0e,"A",{href:!0});var dnt=n(lG);elr=r(dnt,"TFLongformerForMultipleChoice"),dnt.forEach(t),olr=r(F0e," (Longformer model)"),F0e.forEach(t),rlr=i(se),n4=s(se,"LI",{});var C0e=n(n4);Fhe=s(C0e,"STRONG",{});var cnt=n(Fhe);tlr=r(cnt,"mobilebert"),cnt.forEach(t),alr=r(C0e," \u2014 "),iG=s(C0e,"A",{href:!0});var mnt=n(iG);slr=r(mnt,"TFMobileBertForMultipleChoice"),mnt.forEach(t),nlr=r(C0e," (MobileBERT model)"),C0e.forEach(t),llr=i(se),l4=s(se,"LI",{});var M0e=n(l4);Che=s(M0e,"STRONG",{});var fnt=n(Che);ilr=r(fnt,"mpnet"),fnt.forEach(t),dlr=r(M0e," \u2014 "),dG=s(M0e,"A",{href:!0});var gnt=n(dG);clr=r(gnt,"TFMPNetForMultipleChoice"),gnt.forEach(t),mlr=r(M0e," (MPNet model)"),M0e.forEach(t),flr=i(se),i4=s(se,"LI",{});var E0e=n(i4);Mhe=s(E0e,"STRONG",{});var hnt=n(Mhe);glr=r(hnt,"rembert"),hnt.forEach(t),hlr=r(E0e," \u2014 "),cG=s(E0e,"A",{href:!0});var unt=n(cG);ulr=r(unt,"TFRemBertForMultipleChoice"),unt.forEach(t),plr=r(E0e," (RemBERT model)"),E0e.forEach(t),_lr=i(se),d4=s(se,"LI",{});var y0e=n(d4);Ehe=s(y0e,"STRONG",{});var pnt=n(Ehe);blr=r(pnt,"roberta"),pnt.forEach(t),vlr=r(y0e," \u2014 "),mG=s(y0e,"A",{href:!0});var _nt=n(mG);Tlr=r(_nt,"TFRobertaForMultipleChoice"),_nt.forEach(t),Flr=r(y0e," (RoBERTa model)"),y0e.forEach(t),Clr=i(se),c4=s(se,"LI",{});var w0e=n(c4);yhe=s(w0e,"STRONG",{});var bnt=n(yhe);Mlr=r(bnt,"roformer"),bnt.forEach(t),Elr=r(w0e," \u2014 "),fG=s(w0e,"A",{href:!0});var vnt=n(fG);ylr=r(vnt,"TFRoFormerForMultipleChoice"),vnt.forEach(t),wlr=r(w0e," (RoFormer model)"),w0e.forEach(t),Alr=i(se),m4=s(se,"LI",{});var A0e=n(m4);whe=s(A0e,"STRONG",{});var Tnt=n(whe);Llr=r(Tnt,"xlm"),Tnt.forEach(t),Blr=r(A0e," \u2014 "),gG=s(A0e,"A",{href:!0});var Fnt=n(gG);klr=r(Fnt,"TFXLMForMultipleChoice"),Fnt.forEach(t),xlr=r(A0e," (XLM model)"),A0e.forEach(t),Rlr=i(se),f4=s(se,"LI",{});var L0e=n(f4);Ahe=s(L0e,"STRONG",{});var Cnt=n(Ahe);Slr=r(Cnt,"xlm-roberta"),Cnt.forEach(t),Plr=r(L0e," \u2014 "),hG=s(L0e,"A",{href:!0});var Mnt=n(hG);$lr=r(Mnt,"TFXLMRobertaForMultipleChoice"),Mnt.forEach(t),Ilr=r(L0e," (XLM-RoBERTa model)"),L0e.forEach(t),jlr=i(se),g4=s(se,"LI",{});var B0e=n(g4);Lhe=s(B0e,"STRONG",{});var Ent=n(Lhe);Nlr=r(Ent,"xlnet"),Ent.forEach(t),Dlr=r(B0e," \u2014 "),uG=s(B0e,"A",{href:!0});var ynt=n(uG);qlr=r(ynt,"TFXLNetForMultipleChoice"),ynt.forEach(t),Glr=r(B0e," (XLNet model)"),B0e.forEach(t),se.forEach(t),Olr=i(_a),Bhe=s(_a,"P",{});var wnt=n(Bhe);Xlr=r(wnt,"Examples:"),wnt.forEach(t),zlr=i(_a),f(k6.$$.fragment,_a),_a.forEach(t),Xl.forEach(t),C9e=i(d),yc=s(d,"H2",{class:!0});var xke=n(yc);h4=s(xke,"A",{id:!0,class:!0,href:!0});var Ant=n(h4);khe=s(Ant,"SPAN",{});var Lnt=n(khe);f(x6.$$.fragment,Lnt),Lnt.forEach(t),Ant.forEach(t),Vlr=i(xke),xhe=s(xke,"SPAN",{});var Bnt=n(xhe);Wlr=r(Bnt,"TFAutoModelForTableQuestionAnswering"),Bnt.forEach(t),xke.forEach(t),M9e=i(d),Fr=s(d,"DIV",{class:!0});var Vl=n(Fr);f(R6.$$.fragment,Vl),Qlr=i(Vl),wc=s(Vl,"P",{});var oV=n(wc);Hlr=r(oV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Rhe=s(oV,"CODE",{});var knt=n(Rhe);Ulr=r(knt,"from_pretrained()"),knt.forEach(t),Jlr=r(oV,"class method or the "),She=s(oV,"CODE",{});var xnt=n(She);Ylr=r(xnt,"from_config()"),xnt.forEach(t),Klr=r(oV,`class
method.`),oV.forEach(t),Zlr=i(Vl),S6=s(Vl,"P",{});var Rke=n(S6);eir=r(Rke,"This class cannot be instantiated directly using "),Phe=s(Rke,"CODE",{});var Rnt=n(Phe);oir=r(Rnt,"__init__()"),Rnt.forEach(t),rir=r(Rke," (throws an error)."),Rke.forEach(t),tir=i(Vl),ht=s(Vl,"DIV",{class:!0});var Wl=n(ht);f(P6.$$.fragment,Wl),air=i(Wl),$he=s(Wl,"P",{});var Snt=n($he);sir=r(Snt,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Snt.forEach(t),nir=i(Wl),Ac=s(Wl,"P",{});var rV=n(Ac);lir=r(rV,`Note:
Loading a model from its configuration file does `),Ihe=s(rV,"STRONG",{});var Pnt=n(Ihe);iir=r(Pnt,"not"),Pnt.forEach(t),dir=r(rV,` load the model weights. It only affects the
model\u2019s configuration. Use `),jhe=s(rV,"CODE",{});var $nt=n(jhe);cir=r($nt,"from_pretrained()"),$nt.forEach(t),mir=r(rV,"to load the model weights."),rV.forEach(t),fir=i(Wl),Nhe=s(Wl,"P",{});var Int=n(Nhe);gir=r(Int,"Examples:"),Int.forEach(t),hir=i(Wl),f($6.$$.fragment,Wl),Wl.forEach(t),uir=i(Vl),Fo=s(Vl,"DIV",{class:!0});var ba=n(Fo);f(I6.$$.fragment,ba),pir=i(ba),Dhe=s(ba,"P",{});var jnt=n(Dhe);_ir=r(jnt,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),jnt.forEach(t),bir=i(ba),us=s(ba,"P",{});var j3=n(us);vir=r(j3,"The model class to instantiate is selected based on the "),qhe=s(j3,"CODE",{});var Nnt=n(qhe);Tir=r(Nnt,"model_type"),Nnt.forEach(t),Fir=r(j3,` property of the config object (either
passed as an argument or loaded from `),Ghe=s(j3,"CODE",{});var Dnt=n(Ghe);Cir=r(Dnt,"pretrained_model_name_or_path"),Dnt.forEach(t),Mir=r(j3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ohe=s(j3,"CODE",{});var qnt=n(Ohe);Eir=r(qnt,"pretrained_model_name_or_path"),qnt.forEach(t),yir=r(j3,":"),j3.forEach(t),wir=i(ba),Xhe=s(ba,"UL",{});var Gnt=n(Xhe);u4=s(Gnt,"LI",{});var k0e=n(u4);zhe=s(k0e,"STRONG",{});var Ont=n(zhe);Air=r(Ont,"tapas"),Ont.forEach(t),Lir=r(k0e," \u2014 "),pG=s(k0e,"A",{href:!0});var Xnt=n(pG);Bir=r(Xnt,"TFTapasForQuestionAnswering"),Xnt.forEach(t),kir=r(k0e," (TAPAS model)"),k0e.forEach(t),Gnt.forEach(t),xir=i(ba),Vhe=s(ba,"P",{});var znt=n(Vhe);Rir=r(znt,"Examples:"),znt.forEach(t),Sir=i(ba),f(j6.$$.fragment,ba),ba.forEach(t),Vl.forEach(t),E9e=i(d),Lc=s(d,"H2",{class:!0});var Ske=n(Lc);p4=s(Ske,"A",{id:!0,class:!0,href:!0});var Vnt=n(p4);Whe=s(Vnt,"SPAN",{});var Wnt=n(Whe);f(N6.$$.fragment,Wnt),Wnt.forEach(t),Vnt.forEach(t),Pir=i(Ske),Qhe=s(Ske,"SPAN",{});var Qnt=n(Qhe);$ir=r(Qnt,"TFAutoModelForTokenClassification"),Qnt.forEach(t),Ske.forEach(t),y9e=i(d),Cr=s(d,"DIV",{class:!0});var Ql=n(Cr);f(D6.$$.fragment,Ql),Iir=i(Ql),Bc=s(Ql,"P",{});var tV=n(Bc);jir=r(tV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Hhe=s(tV,"CODE",{});var Hnt=n(Hhe);Nir=r(Hnt,"from_pretrained()"),Hnt.forEach(t),Dir=r(tV,"class method or the "),Uhe=s(tV,"CODE",{});var Unt=n(Uhe);qir=r(Unt,"from_config()"),Unt.forEach(t),Gir=r(tV,`class
method.`),tV.forEach(t),Oir=i(Ql),q6=s(Ql,"P",{});var Pke=n(q6);Xir=r(Pke,"This class cannot be instantiated directly using "),Jhe=s(Pke,"CODE",{});var Jnt=n(Jhe);zir=r(Jnt,"__init__()"),Jnt.forEach(t),Vir=r(Pke," (throws an error)."),Pke.forEach(t),Wir=i(Ql),ut=s(Ql,"DIV",{class:!0});var Hl=n(ut);f(G6.$$.fragment,Hl),Qir=i(Hl),Yhe=s(Hl,"P",{});var Ynt=n(Yhe);Hir=r(Ynt,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Ynt.forEach(t),Uir=i(Hl),kc=s(Hl,"P",{});var aV=n(kc);Jir=r(aV,`Note:
Loading a model from its configuration file does `),Khe=s(aV,"STRONG",{});var Knt=n(Khe);Yir=r(Knt,"not"),Knt.forEach(t),Kir=r(aV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Zhe=s(aV,"CODE",{});var Znt=n(Zhe);Zir=r(Znt,"from_pretrained()"),Znt.forEach(t),edr=r(aV,"to load the model weights."),aV.forEach(t),odr=i(Hl),eue=s(Hl,"P",{});var elt=n(eue);rdr=r(elt,"Examples:"),elt.forEach(t),tdr=i(Hl),f(O6.$$.fragment,Hl),Hl.forEach(t),adr=i(Ql),Co=s(Ql,"DIV",{class:!0});var va=n(Co);f(X6.$$.fragment,va),sdr=i(va),oue=s(va,"P",{});var olt=n(oue);ndr=r(olt,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),olt.forEach(t),ldr=i(va),ps=s(va,"P",{});var N3=n(ps);idr=r(N3,"The model class to instantiate is selected based on the "),rue=s(N3,"CODE",{});var rlt=n(rue);ddr=r(rlt,"model_type"),rlt.forEach(t),cdr=r(N3,` property of the config object (either
passed as an argument or loaded from `),tue=s(N3,"CODE",{});var tlt=n(tue);mdr=r(tlt,"pretrained_model_name_or_path"),tlt.forEach(t),fdr=r(N3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),aue=s(N3,"CODE",{});var alt=n(aue);gdr=r(alt,"pretrained_model_name_or_path"),alt.forEach(t),hdr=r(N3,":"),N3.forEach(t),udr=i(va),K=s(va,"UL",{});var oe=n(K);_4=s(oe,"LI",{});var x0e=n(_4);sue=s(x0e,"STRONG",{});var slt=n(sue);pdr=r(slt,"albert"),slt.forEach(t),_dr=r(x0e," \u2014 "),_G=s(x0e,"A",{href:!0});var nlt=n(_G);bdr=r(nlt,"TFAlbertForTokenClassification"),nlt.forEach(t),vdr=r(x0e," (ALBERT model)"),x0e.forEach(t),Tdr=i(oe),b4=s(oe,"LI",{});var R0e=n(b4);nue=s(R0e,"STRONG",{});var llt=n(nue);Fdr=r(llt,"bert"),llt.forEach(t),Cdr=r(R0e," \u2014 "),bG=s(R0e,"A",{href:!0});var ilt=n(bG);Mdr=r(ilt,"TFBertForTokenClassification"),ilt.forEach(t),Edr=r(R0e," (BERT model)"),R0e.forEach(t),ydr=i(oe),v4=s(oe,"LI",{});var S0e=n(v4);lue=s(S0e,"STRONG",{});var dlt=n(lue);wdr=r(dlt,"camembert"),dlt.forEach(t),Adr=r(S0e," \u2014 "),vG=s(S0e,"A",{href:!0});var clt=n(vG);Ldr=r(clt,"TFCamembertForTokenClassification"),clt.forEach(t),Bdr=r(S0e," (CamemBERT model)"),S0e.forEach(t),kdr=i(oe),T4=s(oe,"LI",{});var P0e=n(T4);iue=s(P0e,"STRONG",{});var mlt=n(iue);xdr=r(mlt,"convbert"),mlt.forEach(t),Rdr=r(P0e," \u2014 "),TG=s(P0e,"A",{href:!0});var flt=n(TG);Sdr=r(flt,"TFConvBertForTokenClassification"),flt.forEach(t),Pdr=r(P0e," (ConvBERT model)"),P0e.forEach(t),$dr=i(oe),F4=s(oe,"LI",{});var $0e=n(F4);due=s($0e,"STRONG",{});var glt=n(due);Idr=r(glt,"deberta"),glt.forEach(t),jdr=r($0e," \u2014 "),FG=s($0e,"A",{href:!0});var hlt=n(FG);Ndr=r(hlt,"TFDebertaForTokenClassification"),hlt.forEach(t),Ddr=r($0e," (DeBERTa model)"),$0e.forEach(t),qdr=i(oe),C4=s(oe,"LI",{});var I0e=n(C4);cue=s(I0e,"STRONG",{});var ult=n(cue);Gdr=r(ult,"deberta-v2"),ult.forEach(t),Odr=r(I0e," \u2014 "),CG=s(I0e,"A",{href:!0});var plt=n(CG);Xdr=r(plt,"TFDebertaV2ForTokenClassification"),plt.forEach(t),zdr=r(I0e," (DeBERTa-v2 model)"),I0e.forEach(t),Vdr=i(oe),M4=s(oe,"LI",{});var j0e=n(M4);mue=s(j0e,"STRONG",{});var _lt=n(mue);Wdr=r(_lt,"distilbert"),_lt.forEach(t),Qdr=r(j0e," \u2014 "),MG=s(j0e,"A",{href:!0});var blt=n(MG);Hdr=r(blt,"TFDistilBertForTokenClassification"),blt.forEach(t),Udr=r(j0e," (DistilBERT model)"),j0e.forEach(t),Jdr=i(oe),E4=s(oe,"LI",{});var N0e=n(E4);fue=s(N0e,"STRONG",{});var vlt=n(fue);Ydr=r(vlt,"electra"),vlt.forEach(t),Kdr=r(N0e," \u2014 "),EG=s(N0e,"A",{href:!0});var Tlt=n(EG);Zdr=r(Tlt,"TFElectraForTokenClassification"),Tlt.forEach(t),ecr=r(N0e," (ELECTRA model)"),N0e.forEach(t),ocr=i(oe),y4=s(oe,"LI",{});var D0e=n(y4);gue=s(D0e,"STRONG",{});var Flt=n(gue);rcr=r(Flt,"flaubert"),Flt.forEach(t),tcr=r(D0e," \u2014 "),yG=s(D0e,"A",{href:!0});var Clt=n(yG);acr=r(Clt,"TFFlaubertForTokenClassification"),Clt.forEach(t),scr=r(D0e," (FlauBERT model)"),D0e.forEach(t),ncr=i(oe),w4=s(oe,"LI",{});var q0e=n(w4);hue=s(q0e,"STRONG",{});var Mlt=n(hue);lcr=r(Mlt,"funnel"),Mlt.forEach(t),icr=r(q0e," \u2014 "),wG=s(q0e,"A",{href:!0});var Elt=n(wG);dcr=r(Elt,"TFFunnelForTokenClassification"),Elt.forEach(t),ccr=r(q0e," (Funnel Transformer model)"),q0e.forEach(t),mcr=i(oe),A4=s(oe,"LI",{});var G0e=n(A4);uue=s(G0e,"STRONG",{});var ylt=n(uue);fcr=r(ylt,"layoutlm"),ylt.forEach(t),gcr=r(G0e," \u2014 "),AG=s(G0e,"A",{href:!0});var wlt=n(AG);hcr=r(wlt,"TFLayoutLMForTokenClassification"),wlt.forEach(t),ucr=r(G0e," (LayoutLM model)"),G0e.forEach(t),pcr=i(oe),L4=s(oe,"LI",{});var O0e=n(L4);pue=s(O0e,"STRONG",{});var Alt=n(pue);_cr=r(Alt,"longformer"),Alt.forEach(t),bcr=r(O0e," \u2014 "),LG=s(O0e,"A",{href:!0});var Llt=n(LG);vcr=r(Llt,"TFLongformerForTokenClassification"),Llt.forEach(t),Tcr=r(O0e," (Longformer model)"),O0e.forEach(t),Fcr=i(oe),B4=s(oe,"LI",{});var X0e=n(B4);_ue=s(X0e,"STRONG",{});var Blt=n(_ue);Ccr=r(Blt,"mobilebert"),Blt.forEach(t),Mcr=r(X0e," \u2014 "),BG=s(X0e,"A",{href:!0});var klt=n(BG);Ecr=r(klt,"TFMobileBertForTokenClassification"),klt.forEach(t),ycr=r(X0e," (MobileBERT model)"),X0e.forEach(t),wcr=i(oe),k4=s(oe,"LI",{});var z0e=n(k4);bue=s(z0e,"STRONG",{});var xlt=n(bue);Acr=r(xlt,"mpnet"),xlt.forEach(t),Lcr=r(z0e," \u2014 "),kG=s(z0e,"A",{href:!0});var Rlt=n(kG);Bcr=r(Rlt,"TFMPNetForTokenClassification"),Rlt.forEach(t),kcr=r(z0e," (MPNet model)"),z0e.forEach(t),xcr=i(oe),x4=s(oe,"LI",{});var V0e=n(x4);vue=s(V0e,"STRONG",{});var Slt=n(vue);Rcr=r(Slt,"rembert"),Slt.forEach(t),Scr=r(V0e," \u2014 "),xG=s(V0e,"A",{href:!0});var Plt=n(xG);Pcr=r(Plt,"TFRemBertForTokenClassification"),Plt.forEach(t),$cr=r(V0e," (RemBERT model)"),V0e.forEach(t),Icr=i(oe),R4=s(oe,"LI",{});var W0e=n(R4);Tue=s(W0e,"STRONG",{});var $lt=n(Tue);jcr=r($lt,"roberta"),$lt.forEach(t),Ncr=r(W0e," \u2014 "),RG=s(W0e,"A",{href:!0});var Ilt=n(RG);Dcr=r(Ilt,"TFRobertaForTokenClassification"),Ilt.forEach(t),qcr=r(W0e," (RoBERTa model)"),W0e.forEach(t),Gcr=i(oe),S4=s(oe,"LI",{});var Q0e=n(S4);Fue=s(Q0e,"STRONG",{});var jlt=n(Fue);Ocr=r(jlt,"roformer"),jlt.forEach(t),Xcr=r(Q0e," \u2014 "),SG=s(Q0e,"A",{href:!0});var Nlt=n(SG);zcr=r(Nlt,"TFRoFormerForTokenClassification"),Nlt.forEach(t),Vcr=r(Q0e," (RoFormer model)"),Q0e.forEach(t),Wcr=i(oe),P4=s(oe,"LI",{});var H0e=n(P4);Cue=s(H0e,"STRONG",{});var Dlt=n(Cue);Qcr=r(Dlt,"xlm"),Dlt.forEach(t),Hcr=r(H0e," \u2014 "),PG=s(H0e,"A",{href:!0});var qlt=n(PG);Ucr=r(qlt,"TFXLMForTokenClassification"),qlt.forEach(t),Jcr=r(H0e," (XLM model)"),H0e.forEach(t),Ycr=i(oe),$4=s(oe,"LI",{});var U0e=n($4);Mue=s(U0e,"STRONG",{});var Glt=n(Mue);Kcr=r(Glt,"xlm-roberta"),Glt.forEach(t),Zcr=r(U0e," \u2014 "),$G=s(U0e,"A",{href:!0});var Olt=n($G);emr=r(Olt,"TFXLMRobertaForTokenClassification"),Olt.forEach(t),omr=r(U0e," (XLM-RoBERTa model)"),U0e.forEach(t),rmr=i(oe),I4=s(oe,"LI",{});var J0e=n(I4);Eue=s(J0e,"STRONG",{});var Xlt=n(Eue);tmr=r(Xlt,"xlnet"),Xlt.forEach(t),amr=r(J0e," \u2014 "),IG=s(J0e,"A",{href:!0});var zlt=n(IG);smr=r(zlt,"TFXLNetForTokenClassification"),zlt.forEach(t),nmr=r(J0e," (XLNet model)"),J0e.forEach(t),oe.forEach(t),lmr=i(va),yue=s(va,"P",{});var Vlt=n(yue);imr=r(Vlt,"Examples:"),Vlt.forEach(t),dmr=i(va),f(z6.$$.fragment,va),va.forEach(t),Ql.forEach(t),w9e=i(d),xc=s(d,"H2",{class:!0});var $ke=n(xc);j4=s($ke,"A",{id:!0,class:!0,href:!0});var Wlt=n(j4);wue=s(Wlt,"SPAN",{});var Qlt=n(wue);f(V6.$$.fragment,Qlt),Qlt.forEach(t),Wlt.forEach(t),cmr=i($ke),Aue=s($ke,"SPAN",{});var Hlt=n(Aue);mmr=r(Hlt,"TFAutoModelForQuestionAnswering"),Hlt.forEach(t),$ke.forEach(t),A9e=i(d),Mr=s(d,"DIV",{class:!0});var Ul=n(Mr);f(W6.$$.fragment,Ul),fmr=i(Ul),Rc=s(Ul,"P",{});var sV=n(Rc);gmr=r(sV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Lue=s(sV,"CODE",{});var Ult=n(Lue);hmr=r(Ult,"from_pretrained()"),Ult.forEach(t),umr=r(sV,"class method or the "),Bue=s(sV,"CODE",{});var Jlt=n(Bue);pmr=r(Jlt,"from_config()"),Jlt.forEach(t),_mr=r(sV,`class
method.`),sV.forEach(t),bmr=i(Ul),Q6=s(Ul,"P",{});var Ike=n(Q6);vmr=r(Ike,"This class cannot be instantiated directly using "),kue=s(Ike,"CODE",{});var Ylt=n(kue);Tmr=r(Ylt,"__init__()"),Ylt.forEach(t),Fmr=r(Ike," (throws an error)."),Ike.forEach(t),Cmr=i(Ul),pt=s(Ul,"DIV",{class:!0});var Jl=n(pt);f(H6.$$.fragment,Jl),Mmr=i(Jl),xue=s(Jl,"P",{});var Klt=n(xue);Emr=r(Klt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Klt.forEach(t),ymr=i(Jl),Sc=s(Jl,"P",{});var nV=n(Sc);wmr=r(nV,`Note:
Loading a model from its configuration file does `),Rue=s(nV,"STRONG",{});var Zlt=n(Rue);Amr=r(Zlt,"not"),Zlt.forEach(t),Lmr=r(nV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Sue=s(nV,"CODE",{});var eit=n(Sue);Bmr=r(eit,"from_pretrained()"),eit.forEach(t),kmr=r(nV,"to load the model weights."),nV.forEach(t),xmr=i(Jl),Pue=s(Jl,"P",{});var oit=n(Pue);Rmr=r(oit,"Examples:"),oit.forEach(t),Smr=i(Jl),f(U6.$$.fragment,Jl),Jl.forEach(t),Pmr=i(Ul),Mo=s(Ul,"DIV",{class:!0});var Ta=n(Mo);f(J6.$$.fragment,Ta),$mr=i(Ta),$ue=s(Ta,"P",{});var rit=n($ue);Imr=r(rit,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),rit.forEach(t),jmr=i(Ta),_s=s(Ta,"P",{});var D3=n(_s);Nmr=r(D3,"The model class to instantiate is selected based on the "),Iue=s(D3,"CODE",{});var tit=n(Iue);Dmr=r(tit,"model_type"),tit.forEach(t),qmr=r(D3,` property of the config object (either
passed as an argument or loaded from `),jue=s(D3,"CODE",{});var ait=n(jue);Gmr=r(ait,"pretrained_model_name_or_path"),ait.forEach(t),Omr=r(D3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=s(D3,"CODE",{});var sit=n(Nue);Xmr=r(sit,"pretrained_model_name_or_path"),sit.forEach(t),zmr=r(D3,":"),D3.forEach(t),Vmr=i(Ta),Z=s(Ta,"UL",{});var re=n(Z);N4=s(re,"LI",{});var Y0e=n(N4);Due=s(Y0e,"STRONG",{});var nit=n(Due);Wmr=r(nit,"albert"),nit.forEach(t),Qmr=r(Y0e," \u2014 "),jG=s(Y0e,"A",{href:!0});var lit=n(jG);Hmr=r(lit,"TFAlbertForQuestionAnswering"),lit.forEach(t),Umr=r(Y0e," (ALBERT model)"),Y0e.forEach(t),Jmr=i(re),D4=s(re,"LI",{});var K0e=n(D4);que=s(K0e,"STRONG",{});var iit=n(que);Ymr=r(iit,"bert"),iit.forEach(t),Kmr=r(K0e," \u2014 "),NG=s(K0e,"A",{href:!0});var dit=n(NG);Zmr=r(dit,"TFBertForQuestionAnswering"),dit.forEach(t),efr=r(K0e," (BERT model)"),K0e.forEach(t),ofr=i(re),q4=s(re,"LI",{});var Z0e=n(q4);Gue=s(Z0e,"STRONG",{});var cit=n(Gue);rfr=r(cit,"camembert"),cit.forEach(t),tfr=r(Z0e," \u2014 "),DG=s(Z0e,"A",{href:!0});var mit=n(DG);afr=r(mit,"TFCamembertForQuestionAnswering"),mit.forEach(t),sfr=r(Z0e," (CamemBERT model)"),Z0e.forEach(t),nfr=i(re),G4=s(re,"LI",{});var eLe=n(G4);Oue=s(eLe,"STRONG",{});var fit=n(Oue);lfr=r(fit,"convbert"),fit.forEach(t),ifr=r(eLe," \u2014 "),qG=s(eLe,"A",{href:!0});var git=n(qG);dfr=r(git,"TFConvBertForQuestionAnswering"),git.forEach(t),cfr=r(eLe," (ConvBERT model)"),eLe.forEach(t),mfr=i(re),O4=s(re,"LI",{});var oLe=n(O4);Xue=s(oLe,"STRONG",{});var hit=n(Xue);ffr=r(hit,"deberta"),hit.forEach(t),gfr=r(oLe," \u2014 "),GG=s(oLe,"A",{href:!0});var uit=n(GG);hfr=r(uit,"TFDebertaForQuestionAnswering"),uit.forEach(t),ufr=r(oLe," (DeBERTa model)"),oLe.forEach(t),pfr=i(re),X4=s(re,"LI",{});var rLe=n(X4);zue=s(rLe,"STRONG",{});var pit=n(zue);_fr=r(pit,"deberta-v2"),pit.forEach(t),bfr=r(rLe," \u2014 "),OG=s(rLe,"A",{href:!0});var _it=n(OG);vfr=r(_it,"TFDebertaV2ForQuestionAnswering"),_it.forEach(t),Tfr=r(rLe," (DeBERTa-v2 model)"),rLe.forEach(t),Ffr=i(re),z4=s(re,"LI",{});var tLe=n(z4);Vue=s(tLe,"STRONG",{});var bit=n(Vue);Cfr=r(bit,"distilbert"),bit.forEach(t),Mfr=r(tLe," \u2014 "),XG=s(tLe,"A",{href:!0});var vit=n(XG);Efr=r(vit,"TFDistilBertForQuestionAnswering"),vit.forEach(t),yfr=r(tLe," (DistilBERT model)"),tLe.forEach(t),wfr=i(re),V4=s(re,"LI",{});var aLe=n(V4);Wue=s(aLe,"STRONG",{});var Tit=n(Wue);Afr=r(Tit,"electra"),Tit.forEach(t),Lfr=r(aLe," \u2014 "),zG=s(aLe,"A",{href:!0});var Fit=n(zG);Bfr=r(Fit,"TFElectraForQuestionAnswering"),Fit.forEach(t),kfr=r(aLe," (ELECTRA model)"),aLe.forEach(t),xfr=i(re),W4=s(re,"LI",{});var sLe=n(W4);Que=s(sLe,"STRONG",{});var Cit=n(Que);Rfr=r(Cit,"flaubert"),Cit.forEach(t),Sfr=r(sLe," \u2014 "),VG=s(sLe,"A",{href:!0});var Mit=n(VG);Pfr=r(Mit,"TFFlaubertForQuestionAnsweringSimple"),Mit.forEach(t),$fr=r(sLe," (FlauBERT model)"),sLe.forEach(t),Ifr=i(re),Q4=s(re,"LI",{});var nLe=n(Q4);Hue=s(nLe,"STRONG",{});var Eit=n(Hue);jfr=r(Eit,"funnel"),Eit.forEach(t),Nfr=r(nLe," \u2014 "),WG=s(nLe,"A",{href:!0});var yit=n(WG);Dfr=r(yit,"TFFunnelForQuestionAnswering"),yit.forEach(t),qfr=r(nLe," (Funnel Transformer model)"),nLe.forEach(t),Gfr=i(re),H4=s(re,"LI",{});var lLe=n(H4);Uue=s(lLe,"STRONG",{});var wit=n(Uue);Ofr=r(wit,"longformer"),wit.forEach(t),Xfr=r(lLe," \u2014 "),QG=s(lLe,"A",{href:!0});var Ait=n(QG);zfr=r(Ait,"TFLongformerForQuestionAnswering"),Ait.forEach(t),Vfr=r(lLe," (Longformer model)"),lLe.forEach(t),Wfr=i(re),U4=s(re,"LI",{});var iLe=n(U4);Jue=s(iLe,"STRONG",{});var Lit=n(Jue);Qfr=r(Lit,"mobilebert"),Lit.forEach(t),Hfr=r(iLe," \u2014 "),HG=s(iLe,"A",{href:!0});var Bit=n(HG);Ufr=r(Bit,"TFMobileBertForQuestionAnswering"),Bit.forEach(t),Jfr=r(iLe," (MobileBERT model)"),iLe.forEach(t),Yfr=i(re),J4=s(re,"LI",{});var dLe=n(J4);Yue=s(dLe,"STRONG",{});var kit=n(Yue);Kfr=r(kit,"mpnet"),kit.forEach(t),Zfr=r(dLe," \u2014 "),UG=s(dLe,"A",{href:!0});var xit=n(UG);egr=r(xit,"TFMPNetForQuestionAnswering"),xit.forEach(t),ogr=r(dLe," (MPNet model)"),dLe.forEach(t),rgr=i(re),Y4=s(re,"LI",{});var cLe=n(Y4);Kue=s(cLe,"STRONG",{});var Rit=n(Kue);tgr=r(Rit,"rembert"),Rit.forEach(t),agr=r(cLe," \u2014 "),JG=s(cLe,"A",{href:!0});var Sit=n(JG);sgr=r(Sit,"TFRemBertForQuestionAnswering"),Sit.forEach(t),ngr=r(cLe," (RemBERT model)"),cLe.forEach(t),lgr=i(re),K4=s(re,"LI",{});var mLe=n(K4);Zue=s(mLe,"STRONG",{});var Pit=n(Zue);igr=r(Pit,"roberta"),Pit.forEach(t),dgr=r(mLe," \u2014 "),YG=s(mLe,"A",{href:!0});var $it=n(YG);cgr=r($it,"TFRobertaForQuestionAnswering"),$it.forEach(t),mgr=r(mLe," (RoBERTa model)"),mLe.forEach(t),fgr=i(re),Z4=s(re,"LI",{});var fLe=n(Z4);epe=s(fLe,"STRONG",{});var Iit=n(epe);ggr=r(Iit,"roformer"),Iit.forEach(t),hgr=r(fLe," \u2014 "),KG=s(fLe,"A",{href:!0});var jit=n(KG);ugr=r(jit,"TFRoFormerForQuestionAnswering"),jit.forEach(t),pgr=r(fLe," (RoFormer model)"),fLe.forEach(t),_gr=i(re),eM=s(re,"LI",{});var gLe=n(eM);ope=s(gLe,"STRONG",{});var Nit=n(ope);bgr=r(Nit,"xlm"),Nit.forEach(t),vgr=r(gLe," \u2014 "),ZG=s(gLe,"A",{href:!0});var Dit=n(ZG);Tgr=r(Dit,"TFXLMForQuestionAnsweringSimple"),Dit.forEach(t),Fgr=r(gLe," (XLM model)"),gLe.forEach(t),Cgr=i(re),oM=s(re,"LI",{});var hLe=n(oM);rpe=s(hLe,"STRONG",{});var qit=n(rpe);Mgr=r(qit,"xlm-roberta"),qit.forEach(t),Egr=r(hLe," \u2014 "),eO=s(hLe,"A",{href:!0});var Git=n(eO);ygr=r(Git,"TFXLMRobertaForQuestionAnswering"),Git.forEach(t),wgr=r(hLe," (XLM-RoBERTa model)"),hLe.forEach(t),Agr=i(re),rM=s(re,"LI",{});var uLe=n(rM);tpe=s(uLe,"STRONG",{});var Oit=n(tpe);Lgr=r(Oit,"xlnet"),Oit.forEach(t),Bgr=r(uLe," \u2014 "),oO=s(uLe,"A",{href:!0});var Xit=n(oO);kgr=r(Xit,"TFXLNetForQuestionAnsweringSimple"),Xit.forEach(t),xgr=r(uLe," (XLNet model)"),uLe.forEach(t),re.forEach(t),Rgr=i(Ta),ape=s(Ta,"P",{});var zit=n(ape);Sgr=r(zit,"Examples:"),zit.forEach(t),Pgr=i(Ta),f(Y6.$$.fragment,Ta),Ta.forEach(t),Ul.forEach(t),L9e=i(d),Pc=s(d,"H2",{class:!0});var jke=n(Pc);tM=s(jke,"A",{id:!0,class:!0,href:!0});var Vit=n(tM);spe=s(Vit,"SPAN",{});var Wit=n(spe);f(K6.$$.fragment,Wit),Wit.forEach(t),Vit.forEach(t),$gr=i(jke),npe=s(jke,"SPAN",{});var Qit=n(npe);Igr=r(Qit,"TFAutoModelForVision2Seq"),Qit.forEach(t),jke.forEach(t),B9e=i(d),Er=s(d,"DIV",{class:!0});var Yl=n(Er);f(Z6.$$.fragment,Yl),jgr=i(Yl),$c=s(Yl,"P",{});var lV=n($c);Ngr=r(lV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),lpe=s(lV,"CODE",{});var Hit=n(lpe);Dgr=r(Hit,"from_pretrained()"),Hit.forEach(t),qgr=r(lV,"class method or the "),ipe=s(lV,"CODE",{});var Uit=n(ipe);Ggr=r(Uit,"from_config()"),Uit.forEach(t),Ogr=r(lV,`class
method.`),lV.forEach(t),Xgr=i(Yl),e0=s(Yl,"P",{});var Nke=n(e0);zgr=r(Nke,"This class cannot be instantiated directly using "),dpe=s(Nke,"CODE",{});var Jit=n(dpe);Vgr=r(Jit,"__init__()"),Jit.forEach(t),Wgr=r(Nke," (throws an error)."),Nke.forEach(t),Qgr=i(Yl),_t=s(Yl,"DIV",{class:!0});var Kl=n(_t);f(o0.$$.fragment,Kl),Hgr=i(Kl),cpe=s(Kl,"P",{});var Yit=n(cpe);Ugr=r(Yit,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Yit.forEach(t),Jgr=i(Kl),Ic=s(Kl,"P",{});var iV=n(Ic);Ygr=r(iV,`Note:
Loading a model from its configuration file does `),mpe=s(iV,"STRONG",{});var Kit=n(mpe);Kgr=r(Kit,"not"),Kit.forEach(t),Zgr=r(iV,` load the model weights. It only affects the
model\u2019s configuration. Use `),fpe=s(iV,"CODE",{});var Zit=n(fpe);ehr=r(Zit,"from_pretrained()"),Zit.forEach(t),ohr=r(iV,"to load the model weights."),iV.forEach(t),rhr=i(Kl),gpe=s(Kl,"P",{});var edt=n(gpe);thr=r(edt,"Examples:"),edt.forEach(t),ahr=i(Kl),f(r0.$$.fragment,Kl),Kl.forEach(t),shr=i(Yl),Eo=s(Yl,"DIV",{class:!0});var Fa=n(Eo);f(t0.$$.fragment,Fa),nhr=i(Fa),hpe=s(Fa,"P",{});var odt=n(hpe);lhr=r(odt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),odt.forEach(t),ihr=i(Fa),bs=s(Fa,"P",{});var q3=n(bs);dhr=r(q3,"The model class to instantiate is selected based on the "),upe=s(q3,"CODE",{});var rdt=n(upe);chr=r(rdt,"model_type"),rdt.forEach(t),mhr=r(q3,` property of the config object (either
passed as an argument or loaded from `),ppe=s(q3,"CODE",{});var tdt=n(ppe);fhr=r(tdt,"pretrained_model_name_or_path"),tdt.forEach(t),ghr=r(q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),_pe=s(q3,"CODE",{});var adt=n(_pe);hhr=r(adt,"pretrained_model_name_or_path"),adt.forEach(t),uhr=r(q3,":"),q3.forEach(t),phr=i(Fa),bpe=s(Fa,"UL",{});var sdt=n(bpe);aM=s(sdt,"LI",{});var pLe=n(aM);vpe=s(pLe,"STRONG",{});var ndt=n(vpe);_hr=r(ndt,"vision-encoder-decoder"),ndt.forEach(t),bhr=r(pLe," \u2014 "),rO=s(pLe,"A",{href:!0});var ldt=n(rO);vhr=r(ldt,"TFVisionEncoderDecoderModel"),ldt.forEach(t),Thr=r(pLe," (Vision Encoder decoder model)"),pLe.forEach(t),sdt.forEach(t),Fhr=i(Fa),Tpe=s(Fa,"P",{});var idt=n(Tpe);Chr=r(idt,"Examples:"),idt.forEach(t),Mhr=i(Fa),f(a0.$$.fragment,Fa),Fa.forEach(t),Yl.forEach(t),k9e=i(d),jc=s(d,"H2",{class:!0});var Dke=n(jc);sM=s(Dke,"A",{id:!0,class:!0,href:!0});var ddt=n(sM);Fpe=s(ddt,"SPAN",{});var cdt=n(Fpe);f(s0.$$.fragment,cdt),cdt.forEach(t),ddt.forEach(t),Ehr=i(Dke),Cpe=s(Dke,"SPAN",{});var mdt=n(Cpe);yhr=r(mdt,"TFAutoModelForSpeechSeq2Seq"),mdt.forEach(t),Dke.forEach(t),x9e=i(d),yr=s(d,"DIV",{class:!0});var Zl=n(yr);f(n0.$$.fragment,Zl),whr=i(Zl),Nc=s(Zl,"P",{});var dV=n(Nc);Ahr=r(dV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Mpe=s(dV,"CODE",{});var fdt=n(Mpe);Lhr=r(fdt,"from_pretrained()"),fdt.forEach(t),Bhr=r(dV,"class method or the "),Epe=s(dV,"CODE",{});var gdt=n(Epe);khr=r(gdt,"from_config()"),gdt.forEach(t),xhr=r(dV,`class
method.`),dV.forEach(t),Rhr=i(Zl),l0=s(Zl,"P",{});var qke=n(l0);Shr=r(qke,"This class cannot be instantiated directly using "),ype=s(qke,"CODE",{});var hdt=n(ype);Phr=r(hdt,"__init__()"),hdt.forEach(t),$hr=r(qke," (throws an error)."),qke.forEach(t),Ihr=i(Zl),bt=s(Zl,"DIV",{class:!0});var ei=n(bt);f(i0.$$.fragment,ei),jhr=i(ei),wpe=s(ei,"P",{});var udt=n(wpe);Nhr=r(udt,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),udt.forEach(t),Dhr=i(ei),Dc=s(ei,"P",{});var cV=n(Dc);qhr=r(cV,`Note:
Loading a model from its configuration file does `),Ape=s(cV,"STRONG",{});var pdt=n(Ape);Ghr=r(pdt,"not"),pdt.forEach(t),Ohr=r(cV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lpe=s(cV,"CODE",{});var _dt=n(Lpe);Xhr=r(_dt,"from_pretrained()"),_dt.forEach(t),zhr=r(cV,"to load the model weights."),cV.forEach(t),Vhr=i(ei),Bpe=s(ei,"P",{});var bdt=n(Bpe);Whr=r(bdt,"Examples:"),bdt.forEach(t),Qhr=i(ei),f(d0.$$.fragment,ei),ei.forEach(t),Hhr=i(Zl),yo=s(Zl,"DIV",{class:!0});var Ca=n(yo);f(c0.$$.fragment,Ca),Uhr=i(Ca),kpe=s(Ca,"P",{});var vdt=n(kpe);Jhr=r(vdt,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),vdt.forEach(t),Yhr=i(Ca),vs=s(Ca,"P",{});var G3=n(vs);Khr=r(G3,"The model class to instantiate is selected based on the "),xpe=s(G3,"CODE",{});var Tdt=n(xpe);Zhr=r(Tdt,"model_type"),Tdt.forEach(t),eur=r(G3,` property of the config object (either
passed as an argument or loaded from `),Rpe=s(G3,"CODE",{});var Fdt=n(Rpe);our=r(Fdt,"pretrained_model_name_or_path"),Fdt.forEach(t),rur=r(G3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Spe=s(G3,"CODE",{});var Cdt=n(Spe);tur=r(Cdt,"pretrained_model_name_or_path"),Cdt.forEach(t),aur=r(G3,":"),G3.forEach(t),sur=i(Ca),Ppe=s(Ca,"UL",{});var Mdt=n(Ppe);nM=s(Mdt,"LI",{});var _Le=n(nM);$pe=s(_Le,"STRONG",{});var Edt=n($pe);nur=r(Edt,"speech_to_text"),Edt.forEach(t),lur=r(_Le," \u2014 "),tO=s(_Le,"A",{href:!0});var ydt=n(tO);iur=r(ydt,"TFSpeech2TextForConditionalGeneration"),ydt.forEach(t),dur=r(_Le," (Speech2Text model)"),_Le.forEach(t),Mdt.forEach(t),cur=i(Ca),Ipe=s(Ca,"P",{});var wdt=n(Ipe);mur=r(wdt,"Examples:"),wdt.forEach(t),fur=i(Ca),f(m0.$$.fragment,Ca),Ca.forEach(t),Zl.forEach(t),R9e=i(d),qc=s(d,"H2",{class:!0});var Gke=n(qc);lM=s(Gke,"A",{id:!0,class:!0,href:!0});var Adt=n(lM);jpe=s(Adt,"SPAN",{});var Ldt=n(jpe);f(f0.$$.fragment,Ldt),Ldt.forEach(t),Adt.forEach(t),gur=i(Gke),Npe=s(Gke,"SPAN",{});var Bdt=n(Npe);hur=r(Bdt,"FlaxAutoModel"),Bdt.forEach(t),Gke.forEach(t),S9e=i(d),wr=s(d,"DIV",{class:!0});var oi=n(wr);f(g0.$$.fragment,oi),uur=i(oi),Gc=s(oi,"P",{});var mV=n(Gc);pur=r(mV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Dpe=s(mV,"CODE",{});var kdt=n(Dpe);_ur=r(kdt,"from_pretrained()"),kdt.forEach(t),bur=r(mV,"class method or the "),qpe=s(mV,"CODE",{});var xdt=n(qpe);vur=r(xdt,"from_config()"),xdt.forEach(t),Tur=r(mV,`class
method.`),mV.forEach(t),Fur=i(oi),h0=s(oi,"P",{});var Oke=n(h0);Cur=r(Oke,"This class cannot be instantiated directly using "),Gpe=s(Oke,"CODE",{});var Rdt=n(Gpe);Mur=r(Rdt,"__init__()"),Rdt.forEach(t),Eur=r(Oke," (throws an error)."),Oke.forEach(t),yur=i(oi),vt=s(oi,"DIV",{class:!0});var ri=n(vt);f(u0.$$.fragment,ri),wur=i(ri),Ope=s(ri,"P",{});var Sdt=n(Ope);Aur=r(Sdt,"Instantiates one of the base model classes of the library from a configuration."),Sdt.forEach(t),Lur=i(ri),Oc=s(ri,"P",{});var fV=n(Oc);Bur=r(fV,`Note:
Loading a model from its configuration file does `),Xpe=s(fV,"STRONG",{});var Pdt=n(Xpe);kur=r(Pdt,"not"),Pdt.forEach(t),xur=r(fV,` load the model weights. It only affects the
model\u2019s configuration. Use `),zpe=s(fV,"CODE",{});var $dt=n(zpe);Rur=r($dt,"from_pretrained()"),$dt.forEach(t),Sur=r(fV,"to load the model weights."),fV.forEach(t),Pur=i(ri),Vpe=s(ri,"P",{});var Idt=n(Vpe);$ur=r(Idt,"Examples:"),Idt.forEach(t),Iur=i(ri),f(p0.$$.fragment,ri),ri.forEach(t),jur=i(oi),wo=s(oi,"DIV",{class:!0});var Ma=n(wo);f(_0.$$.fragment,Ma),Nur=i(Ma),Wpe=s(Ma,"P",{});var jdt=n(Wpe);Dur=r(jdt,"Instantiate one of the base model classes of the library from a pretrained model."),jdt.forEach(t),qur=i(Ma),Ts=s(Ma,"P",{});var O3=n(Ts);Gur=r(O3,"The model class to instantiate is selected based on the "),Qpe=s(O3,"CODE",{});var Ndt=n(Qpe);Our=r(Ndt,"model_type"),Ndt.forEach(t),Xur=r(O3,` property of the config object (either
passed as an argument or loaded from `),Hpe=s(O3,"CODE",{});var Ddt=n(Hpe);zur=r(Ddt,"pretrained_model_name_or_path"),Ddt.forEach(t),Vur=r(O3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Upe=s(O3,"CODE",{});var qdt=n(Upe);Wur=r(qdt,"pretrained_model_name_or_path"),qdt.forEach(t),Qur=r(O3,":"),O3.forEach(t),Hur=i(Ma),V=s(Ma,"UL",{});var Q=n(V);iM=s(Q,"LI",{});var bLe=n(iM);Jpe=s(bLe,"STRONG",{});var Gdt=n(Jpe);Uur=r(Gdt,"albert"),Gdt.forEach(t),Jur=r(bLe," \u2014 "),aO=s(bLe,"A",{href:!0});var Odt=n(aO);Yur=r(Odt,"FlaxAlbertModel"),Odt.forEach(t),Kur=r(bLe," (ALBERT model)"),bLe.forEach(t),Zur=i(Q),dM=s(Q,"LI",{});var vLe=n(dM);Ype=s(vLe,"STRONG",{});var Xdt=n(Ype);epr=r(Xdt,"bart"),Xdt.forEach(t),opr=r(vLe," \u2014 "),sO=s(vLe,"A",{href:!0});var zdt=n(sO);rpr=r(zdt,"FlaxBartModel"),zdt.forEach(t),tpr=r(vLe," (BART model)"),vLe.forEach(t),apr=i(Q),cM=s(Q,"LI",{});var TLe=n(cM);Kpe=s(TLe,"STRONG",{});var Vdt=n(Kpe);spr=r(Vdt,"beit"),Vdt.forEach(t),npr=r(TLe," \u2014 "),nO=s(TLe,"A",{href:!0});var Wdt=n(nO);lpr=r(Wdt,"FlaxBeitModel"),Wdt.forEach(t),ipr=r(TLe," (BEiT model)"),TLe.forEach(t),dpr=i(Q),mM=s(Q,"LI",{});var FLe=n(mM);Zpe=s(FLe,"STRONG",{});var Qdt=n(Zpe);cpr=r(Qdt,"bert"),Qdt.forEach(t),mpr=r(FLe," \u2014 "),lO=s(FLe,"A",{href:!0});var Hdt=n(lO);fpr=r(Hdt,"FlaxBertModel"),Hdt.forEach(t),gpr=r(FLe," (BERT model)"),FLe.forEach(t),hpr=i(Q),fM=s(Q,"LI",{});var CLe=n(fM);e_e=s(CLe,"STRONG",{});var Udt=n(e_e);upr=r(Udt,"big_bird"),Udt.forEach(t),ppr=r(CLe," \u2014 "),iO=s(CLe,"A",{href:!0});var Jdt=n(iO);_pr=r(Jdt,"FlaxBigBirdModel"),Jdt.forEach(t),bpr=r(CLe," (BigBird model)"),CLe.forEach(t),vpr=i(Q),gM=s(Q,"LI",{});var MLe=n(gM);o_e=s(MLe,"STRONG",{});var Ydt=n(o_e);Tpr=r(Ydt,"blenderbot"),Ydt.forEach(t),Fpr=r(MLe," \u2014 "),dO=s(MLe,"A",{href:!0});var Kdt=n(dO);Cpr=r(Kdt,"FlaxBlenderbotModel"),Kdt.forEach(t),Mpr=r(MLe," (Blenderbot model)"),MLe.forEach(t),Epr=i(Q),hM=s(Q,"LI",{});var ELe=n(hM);r_e=s(ELe,"STRONG",{});var Zdt=n(r_e);ypr=r(Zdt,"blenderbot-small"),Zdt.forEach(t),wpr=r(ELe," \u2014 "),cO=s(ELe,"A",{href:!0});var ect=n(cO);Apr=r(ect,"FlaxBlenderbotSmallModel"),ect.forEach(t),Lpr=r(ELe," (BlenderbotSmall model)"),ELe.forEach(t),Bpr=i(Q),uM=s(Q,"LI",{});var yLe=n(uM);t_e=s(yLe,"STRONG",{});var oct=n(t_e);kpr=r(oct,"clip"),oct.forEach(t),xpr=r(yLe," \u2014 "),mO=s(yLe,"A",{href:!0});var rct=n(mO);Rpr=r(rct,"FlaxCLIPModel"),rct.forEach(t),Spr=r(yLe," (CLIP model)"),yLe.forEach(t),Ppr=i(Q),pM=s(Q,"LI",{});var wLe=n(pM);a_e=s(wLe,"STRONG",{});var tct=n(a_e);$pr=r(tct,"distilbert"),tct.forEach(t),Ipr=r(wLe," \u2014 "),fO=s(wLe,"A",{href:!0});var act=n(fO);jpr=r(act,"FlaxDistilBertModel"),act.forEach(t),Npr=r(wLe," (DistilBERT model)"),wLe.forEach(t),Dpr=i(Q),_M=s(Q,"LI",{});var ALe=n(_M);s_e=s(ALe,"STRONG",{});var sct=n(s_e);qpr=r(sct,"electra"),sct.forEach(t),Gpr=r(ALe," \u2014 "),gO=s(ALe,"A",{href:!0});var nct=n(gO);Opr=r(nct,"FlaxElectraModel"),nct.forEach(t),Xpr=r(ALe," (ELECTRA model)"),ALe.forEach(t),zpr=i(Q),bM=s(Q,"LI",{});var LLe=n(bM);n_e=s(LLe,"STRONG",{});var lct=n(n_e);Vpr=r(lct,"gpt2"),lct.forEach(t),Wpr=r(LLe," \u2014 "),hO=s(LLe,"A",{href:!0});var ict=n(hO);Qpr=r(ict,"FlaxGPT2Model"),ict.forEach(t),Hpr=r(LLe," (OpenAI GPT-2 model)"),LLe.forEach(t),Upr=i(Q),vM=s(Q,"LI",{});var BLe=n(vM);l_e=s(BLe,"STRONG",{});var dct=n(l_e);Jpr=r(dct,"gpt_neo"),dct.forEach(t),Ypr=r(BLe," \u2014 "),uO=s(BLe,"A",{href:!0});var cct=n(uO);Kpr=r(cct,"FlaxGPTNeoModel"),cct.forEach(t),Zpr=r(BLe," (GPT Neo model)"),BLe.forEach(t),e_r=i(Q),TM=s(Q,"LI",{});var kLe=n(TM);i_e=s(kLe,"STRONG",{});var mct=n(i_e);o_r=r(mct,"gptj"),mct.forEach(t),r_r=r(kLe," \u2014 "),pO=s(kLe,"A",{href:!0});var fct=n(pO);t_r=r(fct,"FlaxGPTJModel"),fct.forEach(t),a_r=r(kLe," (GPT-J model)"),kLe.forEach(t),s_r=i(Q),FM=s(Q,"LI",{});var xLe=n(FM);d_e=s(xLe,"STRONG",{});var gct=n(d_e);n_r=r(gct,"marian"),gct.forEach(t),l_r=r(xLe," \u2014 "),_O=s(xLe,"A",{href:!0});var hct=n(_O);i_r=r(hct,"FlaxMarianModel"),hct.forEach(t),d_r=r(xLe," (Marian model)"),xLe.forEach(t),c_r=i(Q),CM=s(Q,"LI",{});var RLe=n(CM);c_e=s(RLe,"STRONG",{});var uct=n(c_e);m_r=r(uct,"mbart"),uct.forEach(t),f_r=r(RLe," \u2014 "),bO=s(RLe,"A",{href:!0});var pct=n(bO);g_r=r(pct,"FlaxMBartModel"),pct.forEach(t),h_r=r(RLe," (mBART model)"),RLe.forEach(t),u_r=i(Q),MM=s(Q,"LI",{});var SLe=n(MM);m_e=s(SLe,"STRONG",{});var _ct=n(m_e);p_r=r(_ct,"mt5"),_ct.forEach(t),__r=r(SLe," \u2014 "),vO=s(SLe,"A",{href:!0});var bct=n(vO);b_r=r(bct,"FlaxMT5Model"),bct.forEach(t),v_r=r(SLe," (mT5 model)"),SLe.forEach(t),T_r=i(Q),EM=s(Q,"LI",{});var PLe=n(EM);f_e=s(PLe,"STRONG",{});var vct=n(f_e);F_r=r(vct,"pegasus"),vct.forEach(t),C_r=r(PLe," \u2014 "),TO=s(PLe,"A",{href:!0});var Tct=n(TO);M_r=r(Tct,"FlaxPegasusModel"),Tct.forEach(t),E_r=r(PLe," (Pegasus model)"),PLe.forEach(t),y_r=i(Q),yM=s(Q,"LI",{});var $Le=n(yM);g_e=s($Le,"STRONG",{});var Fct=n(g_e);w_r=r(Fct,"roberta"),Fct.forEach(t),A_r=r($Le," \u2014 "),FO=s($Le,"A",{href:!0});var Cct=n(FO);L_r=r(Cct,"FlaxRobertaModel"),Cct.forEach(t),B_r=r($Le," (RoBERTa model)"),$Le.forEach(t),k_r=i(Q),wM=s(Q,"LI",{});var ILe=n(wM);h_e=s(ILe,"STRONG",{});var Mct=n(h_e);x_r=r(Mct,"roformer"),Mct.forEach(t),R_r=r(ILe," \u2014 "),CO=s(ILe,"A",{href:!0});var Ect=n(CO);S_r=r(Ect,"FlaxRoFormerModel"),Ect.forEach(t),P_r=r(ILe," (RoFormer model)"),ILe.forEach(t),$_r=i(Q),AM=s(Q,"LI",{});var jLe=n(AM);u_e=s(jLe,"STRONG",{});var yct=n(u_e);I_r=r(yct,"t5"),yct.forEach(t),j_r=r(jLe," \u2014 "),MO=s(jLe,"A",{href:!0});var wct=n(MO);N_r=r(wct,"FlaxT5Model"),wct.forEach(t),D_r=r(jLe," (T5 model)"),jLe.forEach(t),q_r=i(Q),LM=s(Q,"LI",{});var NLe=n(LM);p_e=s(NLe,"STRONG",{});var Act=n(p_e);G_r=r(Act,"vision-text-dual-encoder"),Act.forEach(t),O_r=r(NLe," \u2014 "),EO=s(NLe,"A",{href:!0});var Lct=n(EO);X_r=r(Lct,"FlaxVisionTextDualEncoderModel"),Lct.forEach(t),z_r=r(NLe," (VisionTextDualEncoder model)"),NLe.forEach(t),V_r=i(Q),BM=s(Q,"LI",{});var DLe=n(BM);__e=s(DLe,"STRONG",{});var Bct=n(__e);W_r=r(Bct,"vit"),Bct.forEach(t),Q_r=r(DLe," \u2014 "),yO=s(DLe,"A",{href:!0});var kct=n(yO);H_r=r(kct,"FlaxViTModel"),kct.forEach(t),U_r=r(DLe," (ViT model)"),DLe.forEach(t),J_r=i(Q),kM=s(Q,"LI",{});var qLe=n(kM);b_e=s(qLe,"STRONG",{});var xct=n(b_e);Y_r=r(xct,"wav2vec2"),xct.forEach(t),K_r=r(qLe," \u2014 "),wO=s(qLe,"A",{href:!0});var Rct=n(wO);Z_r=r(Rct,"FlaxWav2Vec2Model"),Rct.forEach(t),ebr=r(qLe," (Wav2Vec2 model)"),qLe.forEach(t),obr=i(Q),xM=s(Q,"LI",{});var GLe=n(xM);v_e=s(GLe,"STRONG",{});var Sct=n(v_e);rbr=r(Sct,"xglm"),Sct.forEach(t),tbr=r(GLe," \u2014 "),AO=s(GLe,"A",{href:!0});var Pct=n(AO);abr=r(Pct,"FlaxXGLMModel"),Pct.forEach(t),sbr=r(GLe," (XGLM model)"),GLe.forEach(t),Q.forEach(t),nbr=i(Ma),T_e=s(Ma,"P",{});var $ct=n(T_e);lbr=r($ct,"Examples:"),$ct.forEach(t),ibr=i(Ma),f(b0.$$.fragment,Ma),Ma.forEach(t),oi.forEach(t),P9e=i(d),Xc=s(d,"H2",{class:!0});var Xke=n(Xc);RM=s(Xke,"A",{id:!0,class:!0,href:!0});var Ict=n(RM);F_e=s(Ict,"SPAN",{});var jct=n(F_e);f(v0.$$.fragment,jct),jct.forEach(t),Ict.forEach(t),dbr=i(Xke),C_e=s(Xke,"SPAN",{});var Nct=n(C_e);cbr=r(Nct,"FlaxAutoModelForCausalLM"),Nct.forEach(t),Xke.forEach(t),$9e=i(d),Ar=s(d,"DIV",{class:!0});var ti=n(Ar);f(T0.$$.fragment,ti),mbr=i(ti),zc=s(ti,"P",{});var gV=n(zc);fbr=r(gV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),M_e=s(gV,"CODE",{});var Dct=n(M_e);gbr=r(Dct,"from_pretrained()"),Dct.forEach(t),hbr=r(gV,"class method or the "),E_e=s(gV,"CODE",{});var qct=n(E_e);ubr=r(qct,"from_config()"),qct.forEach(t),pbr=r(gV,`class
method.`),gV.forEach(t),_br=i(ti),F0=s(ti,"P",{});var zke=n(F0);bbr=r(zke,"This class cannot be instantiated directly using "),y_e=s(zke,"CODE",{});var Gct=n(y_e);vbr=r(Gct,"__init__()"),Gct.forEach(t),Tbr=r(zke," (throws an error)."),zke.forEach(t),Fbr=i(ti),Tt=s(ti,"DIV",{class:!0});var ai=n(Tt);f(C0.$$.fragment,ai),Cbr=i(ai),w_e=s(ai,"P",{});var Oct=n(w_e);Mbr=r(Oct,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Oct.forEach(t),Ebr=i(ai),Vc=s(ai,"P",{});var hV=n(Vc);ybr=r(hV,`Note:
Loading a model from its configuration file does `),A_e=s(hV,"STRONG",{});var Xct=n(A_e);wbr=r(Xct,"not"),Xct.forEach(t),Abr=r(hV,` load the model weights. It only affects the
model\u2019s configuration. Use `),L_e=s(hV,"CODE",{});var zct=n(L_e);Lbr=r(zct,"from_pretrained()"),zct.forEach(t),Bbr=r(hV,"to load the model weights."),hV.forEach(t),kbr=i(ai),B_e=s(ai,"P",{});var Vct=n(B_e);xbr=r(Vct,"Examples:"),Vct.forEach(t),Rbr=i(ai),f(M0.$$.fragment,ai),ai.forEach(t),Sbr=i(ti),Ao=s(ti,"DIV",{class:!0});var Ea=n(Ao);f(E0.$$.fragment,Ea),Pbr=i(Ea),k_e=s(Ea,"P",{});var Wct=n(k_e);$br=r(Wct,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Wct.forEach(t),Ibr=i(Ea),Fs=s(Ea,"P",{});var X3=n(Fs);jbr=r(X3,"The model class to instantiate is selected based on the "),x_e=s(X3,"CODE",{});var Qct=n(x_e);Nbr=r(Qct,"model_type"),Qct.forEach(t),Dbr=r(X3,` property of the config object (either
passed as an argument or loaded from `),R_e=s(X3,"CODE",{});var Hct=n(R_e);qbr=r(Hct,"pretrained_model_name_or_path"),Hct.forEach(t),Gbr=r(X3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),S_e=s(X3,"CODE",{});var Uct=n(S_e);Obr=r(Uct,"pretrained_model_name_or_path"),Uct.forEach(t),Xbr=r(X3,":"),X3.forEach(t),zbr=i(Ea),Cs=s(Ea,"UL",{});var z3=n(Cs);SM=s(z3,"LI",{});var OLe=n(SM);P_e=s(OLe,"STRONG",{});var Jct=n(P_e);Vbr=r(Jct,"gpt2"),Jct.forEach(t),Wbr=r(OLe," \u2014 "),LO=s(OLe,"A",{href:!0});var Yct=n(LO);Qbr=r(Yct,"FlaxGPT2LMHeadModel"),Yct.forEach(t),Hbr=r(OLe," (OpenAI GPT-2 model)"),OLe.forEach(t),Ubr=i(z3),PM=s(z3,"LI",{});var XLe=n(PM);$_e=s(XLe,"STRONG",{});var Kct=n($_e);Jbr=r(Kct,"gpt_neo"),Kct.forEach(t),Ybr=r(XLe," \u2014 "),BO=s(XLe,"A",{href:!0});var Zct=n(BO);Kbr=r(Zct,"FlaxGPTNeoForCausalLM"),Zct.forEach(t),Zbr=r(XLe," (GPT Neo model)"),XLe.forEach(t),e2r=i(z3),$M=s(z3,"LI",{});var zLe=n($M);I_e=s(zLe,"STRONG",{});var emt=n(I_e);o2r=r(emt,"gptj"),emt.forEach(t),r2r=r(zLe," \u2014 "),kO=s(zLe,"A",{href:!0});var omt=n(kO);t2r=r(omt,"FlaxGPTJForCausalLM"),omt.forEach(t),a2r=r(zLe," (GPT-J model)"),zLe.forEach(t),s2r=i(z3),IM=s(z3,"LI",{});var VLe=n(IM);j_e=s(VLe,"STRONG",{});var rmt=n(j_e);n2r=r(rmt,"xglm"),rmt.forEach(t),l2r=r(VLe," \u2014 "),xO=s(VLe,"A",{href:!0});var tmt=n(xO);i2r=r(tmt,"FlaxXGLMForCausalLM"),tmt.forEach(t),d2r=r(VLe," (XGLM model)"),VLe.forEach(t),z3.forEach(t),c2r=i(Ea),N_e=s(Ea,"P",{});var amt=n(N_e);m2r=r(amt,"Examples:"),amt.forEach(t),f2r=i(Ea),f(y0.$$.fragment,Ea),Ea.forEach(t),ti.forEach(t),I9e=i(d),Wc=s(d,"H2",{class:!0});var Vke=n(Wc);jM=s(Vke,"A",{id:!0,class:!0,href:!0});var smt=n(jM);D_e=s(smt,"SPAN",{});var nmt=n(D_e);f(w0.$$.fragment,nmt),nmt.forEach(t),smt.forEach(t),g2r=i(Vke),q_e=s(Vke,"SPAN",{});var lmt=n(q_e);h2r=r(lmt,"FlaxAutoModelForPreTraining"),lmt.forEach(t),Vke.forEach(t),j9e=i(d),Lr=s(d,"DIV",{class:!0});var si=n(Lr);f(A0.$$.fragment,si),u2r=i(si),Qc=s(si,"P",{});var uV=n(Qc);p2r=r(uV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),G_e=s(uV,"CODE",{});var imt=n(G_e);_2r=r(imt,"from_pretrained()"),imt.forEach(t),b2r=r(uV,"class method or the "),O_e=s(uV,"CODE",{});var dmt=n(O_e);v2r=r(dmt,"from_config()"),dmt.forEach(t),T2r=r(uV,`class
method.`),uV.forEach(t),F2r=i(si),L0=s(si,"P",{});var Wke=n(L0);C2r=r(Wke,"This class cannot be instantiated directly using "),X_e=s(Wke,"CODE",{});var cmt=n(X_e);M2r=r(cmt,"__init__()"),cmt.forEach(t),E2r=r(Wke," (throws an error)."),Wke.forEach(t),y2r=i(si),Ft=s(si,"DIV",{class:!0});var ni=n(Ft);f(B0.$$.fragment,ni),w2r=i(ni),z_e=s(ni,"P",{});var mmt=n(z_e);A2r=r(mmt,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),mmt.forEach(t),L2r=i(ni),Hc=s(ni,"P",{});var pV=n(Hc);B2r=r(pV,`Note:
Loading a model from its configuration file does `),V_e=s(pV,"STRONG",{});var fmt=n(V_e);k2r=r(fmt,"not"),fmt.forEach(t),x2r=r(pV,` load the model weights. It only affects the
model\u2019s configuration. Use `),W_e=s(pV,"CODE",{});var gmt=n(W_e);R2r=r(gmt,"from_pretrained()"),gmt.forEach(t),S2r=r(pV,"to load the model weights."),pV.forEach(t),P2r=i(ni),Q_e=s(ni,"P",{});var hmt=n(Q_e);$2r=r(hmt,"Examples:"),hmt.forEach(t),I2r=i(ni),f(k0.$$.fragment,ni),ni.forEach(t),j2r=i(si),Lo=s(si,"DIV",{class:!0});var ya=n(Lo);f(x0.$$.fragment,ya),N2r=i(ya),H_e=s(ya,"P",{});var umt=n(H_e);D2r=r(umt,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),umt.forEach(t),q2r=i(ya),Ms=s(ya,"P",{});var V3=n(Ms);G2r=r(V3,"The model class to instantiate is selected based on the "),U_e=s(V3,"CODE",{});var pmt=n(U_e);O2r=r(pmt,"model_type"),pmt.forEach(t),X2r=r(V3,` property of the config object (either
passed as an argument or loaded from `),J_e=s(V3,"CODE",{});var _mt=n(J_e);z2r=r(_mt,"pretrained_model_name_or_path"),_mt.forEach(t),V2r=r(V3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Y_e=s(V3,"CODE",{});var bmt=n(Y_e);W2r=r(bmt,"pretrained_model_name_or_path"),bmt.forEach(t),Q2r=r(V3,":"),V3.forEach(t),H2r=i(ya),me=s(ya,"UL",{});var pe=n(me);NM=s(pe,"LI",{});var WLe=n(NM);K_e=s(WLe,"STRONG",{});var vmt=n(K_e);U2r=r(vmt,"albert"),vmt.forEach(t),J2r=r(WLe," \u2014 "),RO=s(WLe,"A",{href:!0});var Tmt=n(RO);Y2r=r(Tmt,"FlaxAlbertForPreTraining"),Tmt.forEach(t),K2r=r(WLe," (ALBERT model)"),WLe.forEach(t),Z2r=i(pe),DM=s(pe,"LI",{});var QLe=n(DM);Z_e=s(QLe,"STRONG",{});var Fmt=n(Z_e);evr=r(Fmt,"bart"),Fmt.forEach(t),ovr=r(QLe," \u2014 "),SO=s(QLe,"A",{href:!0});var Cmt=n(SO);rvr=r(Cmt,"FlaxBartForConditionalGeneration"),Cmt.forEach(t),tvr=r(QLe," (BART model)"),QLe.forEach(t),avr=i(pe),qM=s(pe,"LI",{});var HLe=n(qM);ebe=s(HLe,"STRONG",{});var Mmt=n(ebe);svr=r(Mmt,"bert"),Mmt.forEach(t),nvr=r(HLe," \u2014 "),PO=s(HLe,"A",{href:!0});var Emt=n(PO);lvr=r(Emt,"FlaxBertForPreTraining"),Emt.forEach(t),ivr=r(HLe," (BERT model)"),HLe.forEach(t),dvr=i(pe),GM=s(pe,"LI",{});var ULe=n(GM);obe=s(ULe,"STRONG",{});var ymt=n(obe);cvr=r(ymt,"big_bird"),ymt.forEach(t),mvr=r(ULe," \u2014 "),$O=s(ULe,"A",{href:!0});var wmt=n($O);fvr=r(wmt,"FlaxBigBirdForPreTraining"),wmt.forEach(t),gvr=r(ULe," (BigBird model)"),ULe.forEach(t),hvr=i(pe),OM=s(pe,"LI",{});var JLe=n(OM);rbe=s(JLe,"STRONG",{});var Amt=n(rbe);uvr=r(Amt,"electra"),Amt.forEach(t),pvr=r(JLe," \u2014 "),IO=s(JLe,"A",{href:!0});var Lmt=n(IO);_vr=r(Lmt,"FlaxElectraForPreTraining"),Lmt.forEach(t),bvr=r(JLe," (ELECTRA model)"),JLe.forEach(t),vvr=i(pe),XM=s(pe,"LI",{});var YLe=n(XM);tbe=s(YLe,"STRONG",{});var Bmt=n(tbe);Tvr=r(Bmt,"mbart"),Bmt.forEach(t),Fvr=r(YLe," \u2014 "),jO=s(YLe,"A",{href:!0});var kmt=n(jO);Cvr=r(kmt,"FlaxMBartForConditionalGeneration"),kmt.forEach(t),Mvr=r(YLe," (mBART model)"),YLe.forEach(t),Evr=i(pe),zM=s(pe,"LI",{});var KLe=n(zM);abe=s(KLe,"STRONG",{});var xmt=n(abe);yvr=r(xmt,"mt5"),xmt.forEach(t),wvr=r(KLe," \u2014 "),NO=s(KLe,"A",{href:!0});var Rmt=n(NO);Avr=r(Rmt,"FlaxMT5ForConditionalGeneration"),Rmt.forEach(t),Lvr=r(KLe," (mT5 model)"),KLe.forEach(t),Bvr=i(pe),VM=s(pe,"LI",{});var ZLe=n(VM);sbe=s(ZLe,"STRONG",{});var Smt=n(sbe);kvr=r(Smt,"roberta"),Smt.forEach(t),xvr=r(ZLe," \u2014 "),DO=s(ZLe,"A",{href:!0});var Pmt=n(DO);Rvr=r(Pmt,"FlaxRobertaForMaskedLM"),Pmt.forEach(t),Svr=r(ZLe," (RoBERTa model)"),ZLe.forEach(t),Pvr=i(pe),WM=s(pe,"LI",{});var e7e=n(WM);nbe=s(e7e,"STRONG",{});var $mt=n(nbe);$vr=r($mt,"roformer"),$mt.forEach(t),Ivr=r(e7e," \u2014 "),qO=s(e7e,"A",{href:!0});var Imt=n(qO);jvr=r(Imt,"FlaxRoFormerForMaskedLM"),Imt.forEach(t),Nvr=r(e7e," (RoFormer model)"),e7e.forEach(t),Dvr=i(pe),QM=s(pe,"LI",{});var o7e=n(QM);lbe=s(o7e,"STRONG",{});var jmt=n(lbe);qvr=r(jmt,"t5"),jmt.forEach(t),Gvr=r(o7e," \u2014 "),GO=s(o7e,"A",{href:!0});var Nmt=n(GO);Ovr=r(Nmt,"FlaxT5ForConditionalGeneration"),Nmt.forEach(t),Xvr=r(o7e," (T5 model)"),o7e.forEach(t),zvr=i(pe),HM=s(pe,"LI",{});var r7e=n(HM);ibe=s(r7e,"STRONG",{});var Dmt=n(ibe);Vvr=r(Dmt,"wav2vec2"),Dmt.forEach(t),Wvr=r(r7e," \u2014 "),OO=s(r7e,"A",{href:!0});var qmt=n(OO);Qvr=r(qmt,"FlaxWav2Vec2ForPreTraining"),qmt.forEach(t),Hvr=r(r7e," (Wav2Vec2 model)"),r7e.forEach(t),pe.forEach(t),Uvr=i(ya),dbe=s(ya,"P",{});var Gmt=n(dbe);Jvr=r(Gmt,"Examples:"),Gmt.forEach(t),Yvr=i(ya),f(R0.$$.fragment,ya),ya.forEach(t),si.forEach(t),N9e=i(d),Uc=s(d,"H2",{class:!0});var Qke=n(Uc);UM=s(Qke,"A",{id:!0,class:!0,href:!0});var Omt=n(UM);cbe=s(Omt,"SPAN",{});var Xmt=n(cbe);f(S0.$$.fragment,Xmt),Xmt.forEach(t),Omt.forEach(t),Kvr=i(Qke),mbe=s(Qke,"SPAN",{});var zmt=n(mbe);Zvr=r(zmt,"FlaxAutoModelForMaskedLM"),zmt.forEach(t),Qke.forEach(t),D9e=i(d),Br=s(d,"DIV",{class:!0});var li=n(Br);f(P0.$$.fragment,li),eTr=i(li),Jc=s(li,"P",{});var _V=n(Jc);oTr=r(_V,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),fbe=s(_V,"CODE",{});var Vmt=n(fbe);rTr=r(Vmt,"from_pretrained()"),Vmt.forEach(t),tTr=r(_V,"class method or the "),gbe=s(_V,"CODE",{});var Wmt=n(gbe);aTr=r(Wmt,"from_config()"),Wmt.forEach(t),sTr=r(_V,`class
method.`),_V.forEach(t),nTr=i(li),$0=s(li,"P",{});var Hke=n($0);lTr=r(Hke,"This class cannot be instantiated directly using "),hbe=s(Hke,"CODE",{});var Qmt=n(hbe);iTr=r(Qmt,"__init__()"),Qmt.forEach(t),dTr=r(Hke," (throws an error)."),Hke.forEach(t),cTr=i(li),Ct=s(li,"DIV",{class:!0});var ii=n(Ct);f(I0.$$.fragment,ii),mTr=i(ii),ube=s(ii,"P",{});var Hmt=n(ube);fTr=r(Hmt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Hmt.forEach(t),gTr=i(ii),Yc=s(ii,"P",{});var bV=n(Yc);hTr=r(bV,`Note:
Loading a model from its configuration file does `),pbe=s(bV,"STRONG",{});var Umt=n(pbe);uTr=r(Umt,"not"),Umt.forEach(t),pTr=r(bV,` load the model weights. It only affects the
model\u2019s configuration. Use `),_be=s(bV,"CODE",{});var Jmt=n(_be);_Tr=r(Jmt,"from_pretrained()"),Jmt.forEach(t),bTr=r(bV,"to load the model weights."),bV.forEach(t),vTr=i(ii),bbe=s(ii,"P",{});var Ymt=n(bbe);TTr=r(Ymt,"Examples:"),Ymt.forEach(t),FTr=i(ii),f(j0.$$.fragment,ii),ii.forEach(t),CTr=i(li),Bo=s(li,"DIV",{class:!0});var wa=n(Bo);f(N0.$$.fragment,wa),MTr=i(wa),vbe=s(wa,"P",{});var Kmt=n(vbe);ETr=r(Kmt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Kmt.forEach(t),yTr=i(wa),Es=s(wa,"P",{});var W3=n(Es);wTr=r(W3,"The model class to instantiate is selected based on the "),Tbe=s(W3,"CODE",{});var Zmt=n(Tbe);ATr=r(Zmt,"model_type"),Zmt.forEach(t),LTr=r(W3,` property of the config object (either
passed as an argument or loaded from `),Fbe=s(W3,"CODE",{});var eft=n(Fbe);BTr=r(eft,"pretrained_model_name_or_path"),eft.forEach(t),kTr=r(W3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cbe=s(W3,"CODE",{});var oft=n(Cbe);xTr=r(oft,"pretrained_model_name_or_path"),oft.forEach(t),RTr=r(W3,":"),W3.forEach(t),STr=i(wa),ve=s(wa,"UL",{});var Ze=n(ve);JM=s(Ze,"LI",{});var t7e=n(JM);Mbe=s(t7e,"STRONG",{});var rft=n(Mbe);PTr=r(rft,"albert"),rft.forEach(t),$Tr=r(t7e," \u2014 "),XO=s(t7e,"A",{href:!0});var tft=n(XO);ITr=r(tft,"FlaxAlbertForMaskedLM"),tft.forEach(t),jTr=r(t7e," (ALBERT model)"),t7e.forEach(t),NTr=i(Ze),YM=s(Ze,"LI",{});var a7e=n(YM);Ebe=s(a7e,"STRONG",{});var aft=n(Ebe);DTr=r(aft,"bart"),aft.forEach(t),qTr=r(a7e," \u2014 "),zO=s(a7e,"A",{href:!0});var sft=n(zO);GTr=r(sft,"FlaxBartForConditionalGeneration"),sft.forEach(t),OTr=r(a7e," (BART model)"),a7e.forEach(t),XTr=i(Ze),KM=s(Ze,"LI",{});var s7e=n(KM);ybe=s(s7e,"STRONG",{});var nft=n(ybe);zTr=r(nft,"bert"),nft.forEach(t),VTr=r(s7e," \u2014 "),VO=s(s7e,"A",{href:!0});var lft=n(VO);WTr=r(lft,"FlaxBertForMaskedLM"),lft.forEach(t),QTr=r(s7e," (BERT model)"),s7e.forEach(t),HTr=i(Ze),ZM=s(Ze,"LI",{});var n7e=n(ZM);wbe=s(n7e,"STRONG",{});var ift=n(wbe);UTr=r(ift,"big_bird"),ift.forEach(t),JTr=r(n7e," \u2014 "),WO=s(n7e,"A",{href:!0});var dft=n(WO);YTr=r(dft,"FlaxBigBirdForMaskedLM"),dft.forEach(t),KTr=r(n7e," (BigBird model)"),n7e.forEach(t),ZTr=i(Ze),eE=s(Ze,"LI",{});var l7e=n(eE);Abe=s(l7e,"STRONG",{});var cft=n(Abe);e1r=r(cft,"distilbert"),cft.forEach(t),o1r=r(l7e," \u2014 "),QO=s(l7e,"A",{href:!0});var mft=n(QO);r1r=r(mft,"FlaxDistilBertForMaskedLM"),mft.forEach(t),t1r=r(l7e," (DistilBERT model)"),l7e.forEach(t),a1r=i(Ze),oE=s(Ze,"LI",{});var i7e=n(oE);Lbe=s(i7e,"STRONG",{});var fft=n(Lbe);s1r=r(fft,"electra"),fft.forEach(t),n1r=r(i7e," \u2014 "),HO=s(i7e,"A",{href:!0});var gft=n(HO);l1r=r(gft,"FlaxElectraForMaskedLM"),gft.forEach(t),i1r=r(i7e," (ELECTRA model)"),i7e.forEach(t),d1r=i(Ze),rE=s(Ze,"LI",{});var d7e=n(rE);Bbe=s(d7e,"STRONG",{});var hft=n(Bbe);c1r=r(hft,"mbart"),hft.forEach(t),m1r=r(d7e," \u2014 "),UO=s(d7e,"A",{href:!0});var uft=n(UO);f1r=r(uft,"FlaxMBartForConditionalGeneration"),uft.forEach(t),g1r=r(d7e," (mBART model)"),d7e.forEach(t),h1r=i(Ze),tE=s(Ze,"LI",{});var c7e=n(tE);kbe=s(c7e,"STRONG",{});var pft=n(kbe);u1r=r(pft,"roberta"),pft.forEach(t),p1r=r(c7e," \u2014 "),JO=s(c7e,"A",{href:!0});var _ft=n(JO);_1r=r(_ft,"FlaxRobertaForMaskedLM"),_ft.forEach(t),b1r=r(c7e," (RoBERTa model)"),c7e.forEach(t),v1r=i(Ze),aE=s(Ze,"LI",{});var m7e=n(aE);xbe=s(m7e,"STRONG",{});var bft=n(xbe);T1r=r(bft,"roformer"),bft.forEach(t),F1r=r(m7e," \u2014 "),YO=s(m7e,"A",{href:!0});var vft=n(YO);C1r=r(vft,"FlaxRoFormerForMaskedLM"),vft.forEach(t),M1r=r(m7e," (RoFormer model)"),m7e.forEach(t),Ze.forEach(t),E1r=i(wa),Rbe=s(wa,"P",{});var Tft=n(Rbe);y1r=r(Tft,"Examples:"),Tft.forEach(t),w1r=i(wa),f(D0.$$.fragment,wa),wa.forEach(t),li.forEach(t),q9e=i(d),Kc=s(d,"H2",{class:!0});var Uke=n(Kc);sE=s(Uke,"A",{id:!0,class:!0,href:!0});var Fft=n(sE);Sbe=s(Fft,"SPAN",{});var Cft=n(Sbe);f(q0.$$.fragment,Cft),Cft.forEach(t),Fft.forEach(t),A1r=i(Uke),Pbe=s(Uke,"SPAN",{});var Mft=n(Pbe);L1r=r(Mft,"FlaxAutoModelForSeq2SeqLM"),Mft.forEach(t),Uke.forEach(t),G9e=i(d),kr=s(d,"DIV",{class:!0});var di=n(kr);f(G0.$$.fragment,di),B1r=i(di),Zc=s(di,"P",{});var vV=n(Zc);k1r=r(vV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),$be=s(vV,"CODE",{});var Eft=n($be);x1r=r(Eft,"from_pretrained()"),Eft.forEach(t),R1r=r(vV,"class method or the "),Ibe=s(vV,"CODE",{});var yft=n(Ibe);S1r=r(yft,"from_config()"),yft.forEach(t),P1r=r(vV,`class
method.`),vV.forEach(t),$1r=i(di),O0=s(di,"P",{});var Jke=n(O0);I1r=r(Jke,"This class cannot be instantiated directly using "),jbe=s(Jke,"CODE",{});var wft=n(jbe);j1r=r(wft,"__init__()"),wft.forEach(t),N1r=r(Jke," (throws an error)."),Jke.forEach(t),D1r=i(di),Mt=s(di,"DIV",{class:!0});var ci=n(Mt);f(X0.$$.fragment,ci),q1r=i(ci),Nbe=s(ci,"P",{});var Aft=n(Nbe);G1r=r(Aft,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Aft.forEach(t),O1r=i(ci),em=s(ci,"P",{});var TV=n(em);X1r=r(TV,`Note:
Loading a model from its configuration file does `),Dbe=s(TV,"STRONG",{});var Lft=n(Dbe);z1r=r(Lft,"not"),Lft.forEach(t),V1r=r(TV,` load the model weights. It only affects the
model\u2019s configuration. Use `),qbe=s(TV,"CODE",{});var Bft=n(qbe);W1r=r(Bft,"from_pretrained()"),Bft.forEach(t),Q1r=r(TV,"to load the model weights."),TV.forEach(t),H1r=i(ci),Gbe=s(ci,"P",{});var kft=n(Gbe);U1r=r(kft,"Examples:"),kft.forEach(t),J1r=i(ci),f(z0.$$.fragment,ci),ci.forEach(t),Y1r=i(di),ko=s(di,"DIV",{class:!0});var Aa=n(ko);f(V0.$$.fragment,Aa),K1r=i(Aa),Obe=s(Aa,"P",{});var xft=n(Obe);Z1r=r(xft,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),xft.forEach(t),eFr=i(Aa),ys=s(Aa,"P",{});var Q3=n(ys);oFr=r(Q3,"The model class to instantiate is selected based on the "),Xbe=s(Q3,"CODE",{});var Rft=n(Xbe);rFr=r(Rft,"model_type"),Rft.forEach(t),tFr=r(Q3,` property of the config object (either
passed as an argument or loaded from `),zbe=s(Q3,"CODE",{});var Sft=n(zbe);aFr=r(Sft,"pretrained_model_name_or_path"),Sft.forEach(t),sFr=r(Q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Vbe=s(Q3,"CODE",{});var Pft=n(Vbe);nFr=r(Pft,"pretrained_model_name_or_path"),Pft.forEach(t),lFr=r(Q3,":"),Q3.forEach(t),iFr=i(Aa),Te=s(Aa,"UL",{});var eo=n(Te);nE=s(eo,"LI",{});var f7e=n(nE);Wbe=s(f7e,"STRONG",{});var $ft=n(Wbe);dFr=r($ft,"bart"),$ft.forEach(t),cFr=r(f7e," \u2014 "),KO=s(f7e,"A",{href:!0});var Ift=n(KO);mFr=r(Ift,"FlaxBartForConditionalGeneration"),Ift.forEach(t),fFr=r(f7e," (BART model)"),f7e.forEach(t),gFr=i(eo),lE=s(eo,"LI",{});var g7e=n(lE);Qbe=s(g7e,"STRONG",{});var jft=n(Qbe);hFr=r(jft,"blenderbot"),jft.forEach(t),uFr=r(g7e," \u2014 "),ZO=s(g7e,"A",{href:!0});var Nft=n(ZO);pFr=r(Nft,"FlaxBlenderbotForConditionalGeneration"),Nft.forEach(t),_Fr=r(g7e," (Blenderbot model)"),g7e.forEach(t),bFr=i(eo),iE=s(eo,"LI",{});var h7e=n(iE);Hbe=s(h7e,"STRONG",{});var Dft=n(Hbe);vFr=r(Dft,"blenderbot-small"),Dft.forEach(t),TFr=r(h7e," \u2014 "),eX=s(h7e,"A",{href:!0});var qft=n(eX);FFr=r(qft,"FlaxBlenderbotSmallForConditionalGeneration"),qft.forEach(t),CFr=r(h7e," (BlenderbotSmall model)"),h7e.forEach(t),MFr=i(eo),dE=s(eo,"LI",{});var u7e=n(dE);Ube=s(u7e,"STRONG",{});var Gft=n(Ube);EFr=r(Gft,"encoder-decoder"),Gft.forEach(t),yFr=r(u7e," \u2014 "),oX=s(u7e,"A",{href:!0});var Oft=n(oX);wFr=r(Oft,"FlaxEncoderDecoderModel"),Oft.forEach(t),AFr=r(u7e," (Encoder decoder model)"),u7e.forEach(t),LFr=i(eo),cE=s(eo,"LI",{});var p7e=n(cE);Jbe=s(p7e,"STRONG",{});var Xft=n(Jbe);BFr=r(Xft,"marian"),Xft.forEach(t),kFr=r(p7e," \u2014 "),rX=s(p7e,"A",{href:!0});var zft=n(rX);xFr=r(zft,"FlaxMarianMTModel"),zft.forEach(t),RFr=r(p7e," (Marian model)"),p7e.forEach(t),SFr=i(eo),mE=s(eo,"LI",{});var _7e=n(mE);Ybe=s(_7e,"STRONG",{});var Vft=n(Ybe);PFr=r(Vft,"mbart"),Vft.forEach(t),$Fr=r(_7e," \u2014 "),tX=s(_7e,"A",{href:!0});var Wft=n(tX);IFr=r(Wft,"FlaxMBartForConditionalGeneration"),Wft.forEach(t),jFr=r(_7e," (mBART model)"),_7e.forEach(t),NFr=i(eo),fE=s(eo,"LI",{});var b7e=n(fE);Kbe=s(b7e,"STRONG",{});var Qft=n(Kbe);DFr=r(Qft,"mt5"),Qft.forEach(t),qFr=r(b7e," \u2014 "),aX=s(b7e,"A",{href:!0});var Hft=n(aX);GFr=r(Hft,"FlaxMT5ForConditionalGeneration"),Hft.forEach(t),OFr=r(b7e," (mT5 model)"),b7e.forEach(t),XFr=i(eo),gE=s(eo,"LI",{});var v7e=n(gE);Zbe=s(v7e,"STRONG",{});var Uft=n(Zbe);zFr=r(Uft,"pegasus"),Uft.forEach(t),VFr=r(v7e," \u2014 "),sX=s(v7e,"A",{href:!0});var Jft=n(sX);WFr=r(Jft,"FlaxPegasusForConditionalGeneration"),Jft.forEach(t),QFr=r(v7e," (Pegasus model)"),v7e.forEach(t),HFr=i(eo),hE=s(eo,"LI",{});var T7e=n(hE);e2e=s(T7e,"STRONG",{});var Yft=n(e2e);UFr=r(Yft,"t5"),Yft.forEach(t),JFr=r(T7e," \u2014 "),nX=s(T7e,"A",{href:!0});var Kft=n(nX);YFr=r(Kft,"FlaxT5ForConditionalGeneration"),Kft.forEach(t),KFr=r(T7e," (T5 model)"),T7e.forEach(t),eo.forEach(t),ZFr=i(Aa),o2e=s(Aa,"P",{});var Zft=n(o2e);eCr=r(Zft,"Examples:"),Zft.forEach(t),oCr=i(Aa),f(W0.$$.fragment,Aa),Aa.forEach(t),di.forEach(t),O9e=i(d),om=s(d,"H2",{class:!0});var Yke=n(om);uE=s(Yke,"A",{id:!0,class:!0,href:!0});var egt=n(uE);r2e=s(egt,"SPAN",{});var ogt=n(r2e);f(Q0.$$.fragment,ogt),ogt.forEach(t),egt.forEach(t),rCr=i(Yke),t2e=s(Yke,"SPAN",{});var rgt=n(t2e);tCr=r(rgt,"FlaxAutoModelForSequenceClassification"),rgt.forEach(t),Yke.forEach(t),X9e=i(d),xr=s(d,"DIV",{class:!0});var mi=n(xr);f(H0.$$.fragment,mi),aCr=i(mi),rm=s(mi,"P",{});var FV=n(rm);sCr=r(FV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),a2e=s(FV,"CODE",{});var tgt=n(a2e);nCr=r(tgt,"from_pretrained()"),tgt.forEach(t),lCr=r(FV,"class method or the "),s2e=s(FV,"CODE",{});var agt=n(s2e);iCr=r(agt,"from_config()"),agt.forEach(t),dCr=r(FV,`class
method.`),FV.forEach(t),cCr=i(mi),U0=s(mi,"P",{});var Kke=n(U0);mCr=r(Kke,"This class cannot be instantiated directly using "),n2e=s(Kke,"CODE",{});var sgt=n(n2e);fCr=r(sgt,"__init__()"),sgt.forEach(t),gCr=r(Kke," (throws an error)."),Kke.forEach(t),hCr=i(mi),Et=s(mi,"DIV",{class:!0});var fi=n(Et);f(J0.$$.fragment,fi),uCr=i(fi),l2e=s(fi,"P",{});var ngt=n(l2e);pCr=r(ngt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),ngt.forEach(t),_Cr=i(fi),tm=s(fi,"P",{});var CV=n(tm);bCr=r(CV,`Note:
Loading a model from its configuration file does `),i2e=s(CV,"STRONG",{});var lgt=n(i2e);vCr=r(lgt,"not"),lgt.forEach(t),TCr=r(CV,` load the model weights. It only affects the
model\u2019s configuration. Use `),d2e=s(CV,"CODE",{});var igt=n(d2e);FCr=r(igt,"from_pretrained()"),igt.forEach(t),CCr=r(CV,"to load the model weights."),CV.forEach(t),MCr=i(fi),c2e=s(fi,"P",{});var dgt=n(c2e);ECr=r(dgt,"Examples:"),dgt.forEach(t),yCr=i(fi),f(Y0.$$.fragment,fi),fi.forEach(t),wCr=i(mi),xo=s(mi,"DIV",{class:!0});var La=n(xo);f(K0.$$.fragment,La),ACr=i(La),m2e=s(La,"P",{});var cgt=n(m2e);LCr=r(cgt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),cgt.forEach(t),BCr=i(La),ws=s(La,"P",{});var H3=n(ws);kCr=r(H3,"The model class to instantiate is selected based on the "),f2e=s(H3,"CODE",{});var mgt=n(f2e);xCr=r(mgt,"model_type"),mgt.forEach(t),RCr=r(H3,` property of the config object (either
passed as an argument or loaded from `),g2e=s(H3,"CODE",{});var fgt=n(g2e);SCr=r(fgt,"pretrained_model_name_or_path"),fgt.forEach(t),PCr=r(H3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h2e=s(H3,"CODE",{});var ggt=n(h2e);$Cr=r(ggt,"pretrained_model_name_or_path"),ggt.forEach(t),ICr=r(H3,":"),H3.forEach(t),jCr=i(La),Fe=s(La,"UL",{});var oo=n(Fe);pE=s(oo,"LI",{});var F7e=n(pE);u2e=s(F7e,"STRONG",{});var hgt=n(u2e);NCr=r(hgt,"albert"),hgt.forEach(t),DCr=r(F7e," \u2014 "),lX=s(F7e,"A",{href:!0});var ugt=n(lX);qCr=r(ugt,"FlaxAlbertForSequenceClassification"),ugt.forEach(t),GCr=r(F7e," (ALBERT model)"),F7e.forEach(t),OCr=i(oo),_E=s(oo,"LI",{});var C7e=n(_E);p2e=s(C7e,"STRONG",{});var pgt=n(p2e);XCr=r(pgt,"bart"),pgt.forEach(t),zCr=r(C7e," \u2014 "),iX=s(C7e,"A",{href:!0});var _gt=n(iX);VCr=r(_gt,"FlaxBartForSequenceClassification"),_gt.forEach(t),WCr=r(C7e," (BART model)"),C7e.forEach(t),QCr=i(oo),bE=s(oo,"LI",{});var M7e=n(bE);_2e=s(M7e,"STRONG",{});var bgt=n(_2e);HCr=r(bgt,"bert"),bgt.forEach(t),UCr=r(M7e," \u2014 "),dX=s(M7e,"A",{href:!0});var vgt=n(dX);JCr=r(vgt,"FlaxBertForSequenceClassification"),vgt.forEach(t),YCr=r(M7e," (BERT model)"),M7e.forEach(t),KCr=i(oo),vE=s(oo,"LI",{});var E7e=n(vE);b2e=s(E7e,"STRONG",{});var Tgt=n(b2e);ZCr=r(Tgt,"big_bird"),Tgt.forEach(t),e4r=r(E7e," \u2014 "),cX=s(E7e,"A",{href:!0});var Fgt=n(cX);o4r=r(Fgt,"FlaxBigBirdForSequenceClassification"),Fgt.forEach(t),r4r=r(E7e," (BigBird model)"),E7e.forEach(t),t4r=i(oo),TE=s(oo,"LI",{});var y7e=n(TE);v2e=s(y7e,"STRONG",{});var Cgt=n(v2e);a4r=r(Cgt,"distilbert"),Cgt.forEach(t),s4r=r(y7e," \u2014 "),mX=s(y7e,"A",{href:!0});var Mgt=n(mX);n4r=r(Mgt,"FlaxDistilBertForSequenceClassification"),Mgt.forEach(t),l4r=r(y7e," (DistilBERT model)"),y7e.forEach(t),i4r=i(oo),FE=s(oo,"LI",{});var w7e=n(FE);T2e=s(w7e,"STRONG",{});var Egt=n(T2e);d4r=r(Egt,"electra"),Egt.forEach(t),c4r=r(w7e," \u2014 "),fX=s(w7e,"A",{href:!0});var ygt=n(fX);m4r=r(ygt,"FlaxElectraForSequenceClassification"),ygt.forEach(t),f4r=r(w7e," (ELECTRA model)"),w7e.forEach(t),g4r=i(oo),CE=s(oo,"LI",{});var A7e=n(CE);F2e=s(A7e,"STRONG",{});var wgt=n(F2e);h4r=r(wgt,"mbart"),wgt.forEach(t),u4r=r(A7e," \u2014 "),gX=s(A7e,"A",{href:!0});var Agt=n(gX);p4r=r(Agt,"FlaxMBartForSequenceClassification"),Agt.forEach(t),_4r=r(A7e," (mBART model)"),A7e.forEach(t),b4r=i(oo),ME=s(oo,"LI",{});var L7e=n(ME);C2e=s(L7e,"STRONG",{});var Lgt=n(C2e);v4r=r(Lgt,"roberta"),Lgt.forEach(t),T4r=r(L7e," \u2014 "),hX=s(L7e,"A",{href:!0});var Bgt=n(hX);F4r=r(Bgt,"FlaxRobertaForSequenceClassification"),Bgt.forEach(t),C4r=r(L7e," (RoBERTa model)"),L7e.forEach(t),M4r=i(oo),EE=s(oo,"LI",{});var B7e=n(EE);M2e=s(B7e,"STRONG",{});var kgt=n(M2e);E4r=r(kgt,"roformer"),kgt.forEach(t),y4r=r(B7e," \u2014 "),uX=s(B7e,"A",{href:!0});var xgt=n(uX);w4r=r(xgt,"FlaxRoFormerForSequenceClassification"),xgt.forEach(t),A4r=r(B7e," (RoFormer model)"),B7e.forEach(t),oo.forEach(t),L4r=i(La),E2e=s(La,"P",{});var Rgt=n(E2e);B4r=r(Rgt,"Examples:"),Rgt.forEach(t),k4r=i(La),f(Z0.$$.fragment,La),La.forEach(t),mi.forEach(t),z9e=i(d),am=s(d,"H2",{class:!0});var Zke=n(am);yE=s(Zke,"A",{id:!0,class:!0,href:!0});var Sgt=n(yE);y2e=s(Sgt,"SPAN",{});var Pgt=n(y2e);f(eL.$$.fragment,Pgt),Pgt.forEach(t),Sgt.forEach(t),x4r=i(Zke),w2e=s(Zke,"SPAN",{});var $gt=n(w2e);R4r=r($gt,"FlaxAutoModelForQuestionAnswering"),$gt.forEach(t),Zke.forEach(t),V9e=i(d),Rr=s(d,"DIV",{class:!0});var gi=n(Rr);f(oL.$$.fragment,gi),S4r=i(gi),sm=s(gi,"P",{});var MV=n(sm);P4r=r(MV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),A2e=s(MV,"CODE",{});var Igt=n(A2e);$4r=r(Igt,"from_pretrained()"),Igt.forEach(t),I4r=r(MV,"class method or the "),L2e=s(MV,"CODE",{});var jgt=n(L2e);j4r=r(jgt,"from_config()"),jgt.forEach(t),N4r=r(MV,`class
method.`),MV.forEach(t),D4r=i(gi),rL=s(gi,"P",{});var exe=n(rL);q4r=r(exe,"This class cannot be instantiated directly using "),B2e=s(exe,"CODE",{});var Ngt=n(B2e);G4r=r(Ngt,"__init__()"),Ngt.forEach(t),O4r=r(exe," (throws an error)."),exe.forEach(t),X4r=i(gi),yt=s(gi,"DIV",{class:!0});var hi=n(yt);f(tL.$$.fragment,hi),z4r=i(hi),k2e=s(hi,"P",{});var Dgt=n(k2e);V4r=r(Dgt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Dgt.forEach(t),W4r=i(hi),nm=s(hi,"P",{});var EV=n(nm);Q4r=r(EV,`Note:
Loading a model from its configuration file does `),x2e=s(EV,"STRONG",{});var qgt=n(x2e);H4r=r(qgt,"not"),qgt.forEach(t),U4r=r(EV,` load the model weights. It only affects the
model\u2019s configuration. Use `),R2e=s(EV,"CODE",{});var Ggt=n(R2e);J4r=r(Ggt,"from_pretrained()"),Ggt.forEach(t),Y4r=r(EV,"to load the model weights."),EV.forEach(t),K4r=i(hi),S2e=s(hi,"P",{});var Ogt=n(S2e);Z4r=r(Ogt,"Examples:"),Ogt.forEach(t),eMr=i(hi),f(aL.$$.fragment,hi),hi.forEach(t),oMr=i(gi),Ro=s(gi,"DIV",{class:!0});var Ba=n(Ro);f(sL.$$.fragment,Ba),rMr=i(Ba),P2e=s(Ba,"P",{});var Xgt=n(P2e);tMr=r(Xgt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Xgt.forEach(t),aMr=i(Ba),As=s(Ba,"P",{});var U3=n(As);sMr=r(U3,"The model class to instantiate is selected based on the "),$2e=s(U3,"CODE",{});var zgt=n($2e);nMr=r(zgt,"model_type"),zgt.forEach(t),lMr=r(U3,` property of the config object (either
passed as an argument or loaded from `),I2e=s(U3,"CODE",{});var Vgt=n(I2e);iMr=r(Vgt,"pretrained_model_name_or_path"),Vgt.forEach(t),dMr=r(U3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),j2e=s(U3,"CODE",{});var Wgt=n(j2e);cMr=r(Wgt,"pretrained_model_name_or_path"),Wgt.forEach(t),mMr=r(U3,":"),U3.forEach(t),fMr=i(Ba),Ce=s(Ba,"UL",{});var ro=n(Ce);wE=s(ro,"LI",{});var k7e=n(wE);N2e=s(k7e,"STRONG",{});var Qgt=n(N2e);gMr=r(Qgt,"albert"),Qgt.forEach(t),hMr=r(k7e," \u2014 "),pX=s(k7e,"A",{href:!0});var Hgt=n(pX);uMr=r(Hgt,"FlaxAlbertForQuestionAnswering"),Hgt.forEach(t),pMr=r(k7e," (ALBERT model)"),k7e.forEach(t),_Mr=i(ro),AE=s(ro,"LI",{});var x7e=n(AE);D2e=s(x7e,"STRONG",{});var Ugt=n(D2e);bMr=r(Ugt,"bart"),Ugt.forEach(t),vMr=r(x7e," \u2014 "),_X=s(x7e,"A",{href:!0});var Jgt=n(_X);TMr=r(Jgt,"FlaxBartForQuestionAnswering"),Jgt.forEach(t),FMr=r(x7e," (BART model)"),x7e.forEach(t),CMr=i(ro),LE=s(ro,"LI",{});var R7e=n(LE);q2e=s(R7e,"STRONG",{});var Ygt=n(q2e);MMr=r(Ygt,"bert"),Ygt.forEach(t),EMr=r(R7e," \u2014 "),bX=s(R7e,"A",{href:!0});var Kgt=n(bX);yMr=r(Kgt,"FlaxBertForQuestionAnswering"),Kgt.forEach(t),wMr=r(R7e," (BERT model)"),R7e.forEach(t),AMr=i(ro),BE=s(ro,"LI",{});var S7e=n(BE);G2e=s(S7e,"STRONG",{});var Zgt=n(G2e);LMr=r(Zgt,"big_bird"),Zgt.forEach(t),BMr=r(S7e," \u2014 "),vX=s(S7e,"A",{href:!0});var eht=n(vX);kMr=r(eht,"FlaxBigBirdForQuestionAnswering"),eht.forEach(t),xMr=r(S7e," (BigBird model)"),S7e.forEach(t),RMr=i(ro),kE=s(ro,"LI",{});var P7e=n(kE);O2e=s(P7e,"STRONG",{});var oht=n(O2e);SMr=r(oht,"distilbert"),oht.forEach(t),PMr=r(P7e," \u2014 "),TX=s(P7e,"A",{href:!0});var rht=n(TX);$Mr=r(rht,"FlaxDistilBertForQuestionAnswering"),rht.forEach(t),IMr=r(P7e," (DistilBERT model)"),P7e.forEach(t),jMr=i(ro),xE=s(ro,"LI",{});var $7e=n(xE);X2e=s($7e,"STRONG",{});var tht=n(X2e);NMr=r(tht,"electra"),tht.forEach(t),DMr=r($7e," \u2014 "),FX=s($7e,"A",{href:!0});var aht=n(FX);qMr=r(aht,"FlaxElectraForQuestionAnswering"),aht.forEach(t),GMr=r($7e," (ELECTRA model)"),$7e.forEach(t),OMr=i(ro),RE=s(ro,"LI",{});var I7e=n(RE);z2e=s(I7e,"STRONG",{});var sht=n(z2e);XMr=r(sht,"mbart"),sht.forEach(t),zMr=r(I7e," \u2014 "),CX=s(I7e,"A",{href:!0});var nht=n(CX);VMr=r(nht,"FlaxMBartForQuestionAnswering"),nht.forEach(t),WMr=r(I7e," (mBART model)"),I7e.forEach(t),QMr=i(ro),SE=s(ro,"LI",{});var j7e=n(SE);V2e=s(j7e,"STRONG",{});var lht=n(V2e);HMr=r(lht,"roberta"),lht.forEach(t),UMr=r(j7e," \u2014 "),MX=s(j7e,"A",{href:!0});var iht=n(MX);JMr=r(iht,"FlaxRobertaForQuestionAnswering"),iht.forEach(t),YMr=r(j7e," (RoBERTa model)"),j7e.forEach(t),KMr=i(ro),PE=s(ro,"LI",{});var N7e=n(PE);W2e=s(N7e,"STRONG",{});var dht=n(W2e);ZMr=r(dht,"roformer"),dht.forEach(t),eEr=r(N7e," \u2014 "),EX=s(N7e,"A",{href:!0});var cht=n(EX);oEr=r(cht,"FlaxRoFormerForQuestionAnswering"),cht.forEach(t),rEr=r(N7e," (RoFormer model)"),N7e.forEach(t),ro.forEach(t),tEr=i(Ba),Q2e=s(Ba,"P",{});var mht=n(Q2e);aEr=r(mht,"Examples:"),mht.forEach(t),sEr=i(Ba),f(nL.$$.fragment,Ba),Ba.forEach(t),gi.forEach(t),W9e=i(d),lm=s(d,"H2",{class:!0});var oxe=n(lm);$E=s(oxe,"A",{id:!0,class:!0,href:!0});var fht=n($E);H2e=s(fht,"SPAN",{});var ght=n(H2e);f(lL.$$.fragment,ght),ght.forEach(t),fht.forEach(t),nEr=i(oxe),U2e=s(oxe,"SPAN",{});var hht=n(U2e);lEr=r(hht,"FlaxAutoModelForTokenClassification"),hht.forEach(t),oxe.forEach(t),Q9e=i(d),Sr=s(d,"DIV",{class:!0});var ui=n(Sr);f(iL.$$.fragment,ui),iEr=i(ui),im=s(ui,"P",{});var yV=n(im);dEr=r(yV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),J2e=s(yV,"CODE",{});var uht=n(J2e);cEr=r(uht,"from_pretrained()"),uht.forEach(t),mEr=r(yV,"class method or the "),Y2e=s(yV,"CODE",{});var pht=n(Y2e);fEr=r(pht,"from_config()"),pht.forEach(t),gEr=r(yV,`class
method.`),yV.forEach(t),hEr=i(ui),dL=s(ui,"P",{});var rxe=n(dL);uEr=r(rxe,"This class cannot be instantiated directly using "),K2e=s(rxe,"CODE",{});var _ht=n(K2e);pEr=r(_ht,"__init__()"),_ht.forEach(t),_Er=r(rxe," (throws an error)."),rxe.forEach(t),bEr=i(ui),wt=s(ui,"DIV",{class:!0});var pi=n(wt);f(cL.$$.fragment,pi),vEr=i(pi),Z2e=s(pi,"P",{});var bht=n(Z2e);TEr=r(bht,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),bht.forEach(t),FEr=i(pi),dm=s(pi,"P",{});var wV=n(dm);CEr=r(wV,`Note:
Loading a model from its configuration file does `),eve=s(wV,"STRONG",{});var vht=n(eve);MEr=r(vht,"not"),vht.forEach(t),EEr=r(wV,` load the model weights. It only affects the
model\u2019s configuration. Use `),ove=s(wV,"CODE",{});var Tht=n(ove);yEr=r(Tht,"from_pretrained()"),Tht.forEach(t),wEr=r(wV,"to load the model weights."),wV.forEach(t),AEr=i(pi),rve=s(pi,"P",{});var Fht=n(rve);LEr=r(Fht,"Examples:"),Fht.forEach(t),BEr=i(pi),f(mL.$$.fragment,pi),pi.forEach(t),kEr=i(ui),So=s(ui,"DIV",{class:!0});var ka=n(So);f(fL.$$.fragment,ka),xEr=i(ka),tve=s(ka,"P",{});var Cht=n(tve);REr=r(Cht,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Cht.forEach(t),SEr=i(ka),Ls=s(ka,"P",{});var J3=n(Ls);PEr=r(J3,"The model class to instantiate is selected based on the "),ave=s(J3,"CODE",{});var Mht=n(ave);$Er=r(Mht,"model_type"),Mht.forEach(t),IEr=r(J3,` property of the config object (either
passed as an argument or loaded from `),sve=s(J3,"CODE",{});var Eht=n(sve);jEr=r(Eht,"pretrained_model_name_or_path"),Eht.forEach(t),NEr=r(J3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),nve=s(J3,"CODE",{});var yht=n(nve);DEr=r(yht,"pretrained_model_name_or_path"),yht.forEach(t),qEr=r(J3,":"),J3.forEach(t),GEr=i(ka),no=s(ka,"UL",{});var ta=n(no);IE=s(ta,"LI",{});var D7e=n(IE);lve=s(D7e,"STRONG",{});var wht=n(lve);OEr=r(wht,"albert"),wht.forEach(t),XEr=r(D7e," \u2014 "),yX=s(D7e,"A",{href:!0});var Aht=n(yX);zEr=r(Aht,"FlaxAlbertForTokenClassification"),Aht.forEach(t),VEr=r(D7e," (ALBERT model)"),D7e.forEach(t),WEr=i(ta),jE=s(ta,"LI",{});var q7e=n(jE);ive=s(q7e,"STRONG",{});var Lht=n(ive);QEr=r(Lht,"bert"),Lht.forEach(t),HEr=r(q7e," \u2014 "),wX=s(q7e,"A",{href:!0});var Bht=n(wX);UEr=r(Bht,"FlaxBertForTokenClassification"),Bht.forEach(t),JEr=r(q7e," (BERT model)"),q7e.forEach(t),YEr=i(ta),NE=s(ta,"LI",{});var G7e=n(NE);dve=s(G7e,"STRONG",{});var kht=n(dve);KEr=r(kht,"big_bird"),kht.forEach(t),ZEr=r(G7e," \u2014 "),AX=s(G7e,"A",{href:!0});var xht=n(AX);e3r=r(xht,"FlaxBigBirdForTokenClassification"),xht.forEach(t),o3r=r(G7e," (BigBird model)"),G7e.forEach(t),r3r=i(ta),DE=s(ta,"LI",{});var O7e=n(DE);cve=s(O7e,"STRONG",{});var Rht=n(cve);t3r=r(Rht,"distilbert"),Rht.forEach(t),a3r=r(O7e," \u2014 "),LX=s(O7e,"A",{href:!0});var Sht=n(LX);s3r=r(Sht,"FlaxDistilBertForTokenClassification"),Sht.forEach(t),n3r=r(O7e," (DistilBERT model)"),O7e.forEach(t),l3r=i(ta),qE=s(ta,"LI",{});var X7e=n(qE);mve=s(X7e,"STRONG",{});var Pht=n(mve);i3r=r(Pht,"electra"),Pht.forEach(t),d3r=r(X7e," \u2014 "),BX=s(X7e,"A",{href:!0});var $ht=n(BX);c3r=r($ht,"FlaxElectraForTokenClassification"),$ht.forEach(t),m3r=r(X7e," (ELECTRA model)"),X7e.forEach(t),f3r=i(ta),GE=s(ta,"LI",{});var z7e=n(GE);fve=s(z7e,"STRONG",{});var Iht=n(fve);g3r=r(Iht,"roberta"),Iht.forEach(t),h3r=r(z7e," \u2014 "),kX=s(z7e,"A",{href:!0});var jht=n(kX);u3r=r(jht,"FlaxRobertaForTokenClassification"),jht.forEach(t),p3r=r(z7e," (RoBERTa model)"),z7e.forEach(t),_3r=i(ta),OE=s(ta,"LI",{});var V7e=n(OE);gve=s(V7e,"STRONG",{});var Nht=n(gve);b3r=r(Nht,"roformer"),Nht.forEach(t),v3r=r(V7e," \u2014 "),xX=s(V7e,"A",{href:!0});var Dht=n(xX);T3r=r(Dht,"FlaxRoFormerForTokenClassification"),Dht.forEach(t),F3r=r(V7e," (RoFormer model)"),V7e.forEach(t),ta.forEach(t),C3r=i(ka),hve=s(ka,"P",{});var qht=n(hve);M3r=r(qht,"Examples:"),qht.forEach(t),E3r=i(ka),f(gL.$$.fragment,ka),ka.forEach(t),ui.forEach(t),H9e=i(d),cm=s(d,"H2",{class:!0});var txe=n(cm);XE=s(txe,"A",{id:!0,class:!0,href:!0});var Ght=n(XE);uve=s(Ght,"SPAN",{});var Oht=n(uve);f(hL.$$.fragment,Oht),Oht.forEach(t),Ght.forEach(t),y3r=i(txe),pve=s(txe,"SPAN",{});var Xht=n(pve);w3r=r(Xht,"FlaxAutoModelForMultipleChoice"),Xht.forEach(t),txe.forEach(t),U9e=i(d),Pr=s(d,"DIV",{class:!0});var _i=n(Pr);f(uL.$$.fragment,_i),A3r=i(_i),mm=s(_i,"P",{});var AV=n(mm);L3r=r(AV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),_ve=s(AV,"CODE",{});var zht=n(_ve);B3r=r(zht,"from_pretrained()"),zht.forEach(t),k3r=r(AV,"class method or the "),bve=s(AV,"CODE",{});var Vht=n(bve);x3r=r(Vht,"from_config()"),Vht.forEach(t),R3r=r(AV,`class
method.`),AV.forEach(t),S3r=i(_i),pL=s(_i,"P",{});var axe=n(pL);P3r=r(axe,"This class cannot be instantiated directly using "),vve=s(axe,"CODE",{});var Wht=n(vve);$3r=r(Wht,"__init__()"),Wht.forEach(t),I3r=r(axe," (throws an error)."),axe.forEach(t),j3r=i(_i),At=s(_i,"DIV",{class:!0});var bi=n(At);f(_L.$$.fragment,bi),N3r=i(bi),Tve=s(bi,"P",{});var Qht=n(Tve);D3r=r(Qht,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Qht.forEach(t),q3r=i(bi),fm=s(bi,"P",{});var LV=n(fm);G3r=r(LV,`Note:
Loading a model from its configuration file does `),Fve=s(LV,"STRONG",{});var Hht=n(Fve);O3r=r(Hht,"not"),Hht.forEach(t),X3r=r(LV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cve=s(LV,"CODE",{});var Uht=n(Cve);z3r=r(Uht,"from_pretrained()"),Uht.forEach(t),V3r=r(LV,"to load the model weights."),LV.forEach(t),W3r=i(bi),Mve=s(bi,"P",{});var Jht=n(Mve);Q3r=r(Jht,"Examples:"),Jht.forEach(t),H3r=i(bi),f(bL.$$.fragment,bi),bi.forEach(t),U3r=i(_i),Po=s(_i,"DIV",{class:!0});var xa=n(Po);f(vL.$$.fragment,xa),J3r=i(xa),Eve=s(xa,"P",{});var Yht=n(Eve);Y3r=r(Yht,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Yht.forEach(t),K3r=i(xa),Bs=s(xa,"P",{});var Y3=n(Bs);Z3r=r(Y3,"The model class to instantiate is selected based on the "),yve=s(Y3,"CODE",{});var Kht=n(yve);e5r=r(Kht,"model_type"),Kht.forEach(t),o5r=r(Y3,` property of the config object (either
passed as an argument or loaded from `),wve=s(Y3,"CODE",{});var Zht=n(wve);r5r=r(Zht,"pretrained_model_name_or_path"),Zht.forEach(t),t5r=r(Y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ave=s(Y3,"CODE",{});var eut=n(Ave);a5r=r(eut,"pretrained_model_name_or_path"),eut.forEach(t),s5r=r(Y3,":"),Y3.forEach(t),n5r=i(xa),lo=s(xa,"UL",{});var aa=n(lo);zE=s(aa,"LI",{});var W7e=n(zE);Lve=s(W7e,"STRONG",{});var out=n(Lve);l5r=r(out,"albert"),out.forEach(t),i5r=r(W7e," \u2014 "),RX=s(W7e,"A",{href:!0});var rut=n(RX);d5r=r(rut,"FlaxAlbertForMultipleChoice"),rut.forEach(t),c5r=r(W7e," (ALBERT model)"),W7e.forEach(t),m5r=i(aa),VE=s(aa,"LI",{});var Q7e=n(VE);Bve=s(Q7e,"STRONG",{});var tut=n(Bve);f5r=r(tut,"bert"),tut.forEach(t),g5r=r(Q7e," \u2014 "),SX=s(Q7e,"A",{href:!0});var aut=n(SX);h5r=r(aut,"FlaxBertForMultipleChoice"),aut.forEach(t),u5r=r(Q7e," (BERT model)"),Q7e.forEach(t),p5r=i(aa),WE=s(aa,"LI",{});var H7e=n(WE);kve=s(H7e,"STRONG",{});var sut=n(kve);_5r=r(sut,"big_bird"),sut.forEach(t),b5r=r(H7e," \u2014 "),PX=s(H7e,"A",{href:!0});var nut=n(PX);v5r=r(nut,"FlaxBigBirdForMultipleChoice"),nut.forEach(t),T5r=r(H7e," (BigBird model)"),H7e.forEach(t),F5r=i(aa),QE=s(aa,"LI",{});var U7e=n(QE);xve=s(U7e,"STRONG",{});var lut=n(xve);C5r=r(lut,"distilbert"),lut.forEach(t),M5r=r(U7e," \u2014 "),$X=s(U7e,"A",{href:!0});var iut=n($X);E5r=r(iut,"FlaxDistilBertForMultipleChoice"),iut.forEach(t),y5r=r(U7e," (DistilBERT model)"),U7e.forEach(t),w5r=i(aa),HE=s(aa,"LI",{});var J7e=n(HE);Rve=s(J7e,"STRONG",{});var dut=n(Rve);A5r=r(dut,"electra"),dut.forEach(t),L5r=r(J7e," \u2014 "),IX=s(J7e,"A",{href:!0});var cut=n(IX);B5r=r(cut,"FlaxElectraForMultipleChoice"),cut.forEach(t),k5r=r(J7e," (ELECTRA model)"),J7e.forEach(t),x5r=i(aa),UE=s(aa,"LI",{});var Y7e=n(UE);Sve=s(Y7e,"STRONG",{});var mut=n(Sve);R5r=r(mut,"roberta"),mut.forEach(t),S5r=r(Y7e," \u2014 "),jX=s(Y7e,"A",{href:!0});var fut=n(jX);P5r=r(fut,"FlaxRobertaForMultipleChoice"),fut.forEach(t),$5r=r(Y7e," (RoBERTa model)"),Y7e.forEach(t),I5r=i(aa),JE=s(aa,"LI",{});var K7e=n(JE);Pve=s(K7e,"STRONG",{});var gut=n(Pve);j5r=r(gut,"roformer"),gut.forEach(t),N5r=r(K7e," \u2014 "),NX=s(K7e,"A",{href:!0});var hut=n(NX);D5r=r(hut,"FlaxRoFormerForMultipleChoice"),hut.forEach(t),q5r=r(K7e," (RoFormer model)"),K7e.forEach(t),aa.forEach(t),G5r=i(xa),$ve=s(xa,"P",{});var uut=n($ve);O5r=r(uut,"Examples:"),uut.forEach(t),X5r=i(xa),f(TL.$$.fragment,xa),xa.forEach(t),_i.forEach(t),J9e=i(d),gm=s(d,"H2",{class:!0});var sxe=n(gm);YE=s(sxe,"A",{id:!0,class:!0,href:!0});var put=n(YE);Ive=s(put,"SPAN",{});var _ut=n(Ive);f(FL.$$.fragment,_ut),_ut.forEach(t),put.forEach(t),z5r=i(sxe),jve=s(sxe,"SPAN",{});var but=n(jve);V5r=r(but,"FlaxAutoModelForNextSentencePrediction"),but.forEach(t),sxe.forEach(t),Y9e=i(d),$r=s(d,"DIV",{class:!0});var vi=n($r);f(CL.$$.fragment,vi),W5r=i(vi),hm=s(vi,"P",{});var BV=n(hm);Q5r=r(BV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Nve=s(BV,"CODE",{});var vut=n(Nve);H5r=r(vut,"from_pretrained()"),vut.forEach(t),U5r=r(BV,"class method or the "),Dve=s(BV,"CODE",{});var Tut=n(Dve);J5r=r(Tut,"from_config()"),Tut.forEach(t),Y5r=r(BV,`class
method.`),BV.forEach(t),K5r=i(vi),ML=s(vi,"P",{});var nxe=n(ML);Z5r=r(nxe,"This class cannot be instantiated directly using "),qve=s(nxe,"CODE",{});var Fut=n(qve);eyr=r(Fut,"__init__()"),Fut.forEach(t),oyr=r(nxe," (throws an error)."),nxe.forEach(t),ryr=i(vi),Lt=s(vi,"DIV",{class:!0});var Ti=n(Lt);f(EL.$$.fragment,Ti),tyr=i(Ti),Gve=s(Ti,"P",{});var Cut=n(Gve);ayr=r(Cut,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Cut.forEach(t),syr=i(Ti),um=s(Ti,"P",{});var kV=n(um);nyr=r(kV,`Note:
Loading a model from its configuration file does `),Ove=s(kV,"STRONG",{});var Mut=n(Ove);lyr=r(Mut,"not"),Mut.forEach(t),iyr=r(kV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xve=s(kV,"CODE",{});var Eut=n(Xve);dyr=r(Eut,"from_pretrained()"),Eut.forEach(t),cyr=r(kV,"to load the model weights."),kV.forEach(t),myr=i(Ti),zve=s(Ti,"P",{});var yut=n(zve);fyr=r(yut,"Examples:"),yut.forEach(t),gyr=i(Ti),f(yL.$$.fragment,Ti),Ti.forEach(t),hyr=i(vi),$o=s(vi,"DIV",{class:!0});var Ra=n($o);f(wL.$$.fragment,Ra),uyr=i(Ra),Vve=s(Ra,"P",{});var wut=n(Vve);pyr=r(wut,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),wut.forEach(t),_yr=i(Ra),ks=s(Ra,"P",{});var K3=n(ks);byr=r(K3,"The model class to instantiate is selected based on the "),Wve=s(K3,"CODE",{});var Aut=n(Wve);vyr=r(Aut,"model_type"),Aut.forEach(t),Tyr=r(K3,` property of the config object (either
passed as an argument or loaded from `),Qve=s(K3,"CODE",{});var Lut=n(Qve);Fyr=r(Lut,"pretrained_model_name_or_path"),Lut.forEach(t),Cyr=r(K3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hve=s(K3,"CODE",{});var But=n(Hve);Myr=r(But,"pretrained_model_name_or_path"),But.forEach(t),Eyr=r(K3,":"),K3.forEach(t),yyr=i(Ra),Uve=s(Ra,"UL",{});var kut=n(Uve);KE=s(kut,"LI",{});var Z7e=n(KE);Jve=s(Z7e,"STRONG",{});var xut=n(Jve);wyr=r(xut,"bert"),xut.forEach(t),Ayr=r(Z7e," \u2014 "),DX=s(Z7e,"A",{href:!0});var Rut=n(DX);Lyr=r(Rut,"FlaxBertForNextSentencePrediction"),Rut.forEach(t),Byr=r(Z7e," (BERT model)"),Z7e.forEach(t),kut.forEach(t),kyr=i(Ra),Yve=s(Ra,"P",{});var Sut=n(Yve);xyr=r(Sut,"Examples:"),Sut.forEach(t),Ryr=i(Ra),f(AL.$$.fragment,Ra),Ra.forEach(t),vi.forEach(t),K9e=i(d),pm=s(d,"H2",{class:!0});var lxe=n(pm);ZE=s(lxe,"A",{id:!0,class:!0,href:!0});var Put=n(ZE);Kve=s(Put,"SPAN",{});var $ut=n(Kve);f(LL.$$.fragment,$ut),$ut.forEach(t),Put.forEach(t),Syr=i(lxe),Zve=s(lxe,"SPAN",{});var Iut=n(Zve);Pyr=r(Iut,"FlaxAutoModelForImageClassification"),Iut.forEach(t),lxe.forEach(t),Z9e=i(d),Ir=s(d,"DIV",{class:!0});var Fi=n(Ir);f(BL.$$.fragment,Fi),$yr=i(Fi),_m=s(Fi,"P",{});var xV=n(_m);Iyr=r(xV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),eTe=s(xV,"CODE",{});var jut=n(eTe);jyr=r(jut,"from_pretrained()"),jut.forEach(t),Nyr=r(xV,"class method or the "),oTe=s(xV,"CODE",{});var Nut=n(oTe);Dyr=r(Nut,"from_config()"),Nut.forEach(t),qyr=r(xV,`class
method.`),xV.forEach(t),Gyr=i(Fi),kL=s(Fi,"P",{});var ixe=n(kL);Oyr=r(ixe,"This class cannot be instantiated directly using "),rTe=s(ixe,"CODE",{});var Dut=n(rTe);Xyr=r(Dut,"__init__()"),Dut.forEach(t),zyr=r(ixe," (throws an error)."),ixe.forEach(t),Vyr=i(Fi),Bt=s(Fi,"DIV",{class:!0});var Ci=n(Bt);f(xL.$$.fragment,Ci),Wyr=i(Ci),tTe=s(Ci,"P",{});var qut=n(tTe);Qyr=r(qut,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qut.forEach(t),Hyr=i(Ci),bm=s(Ci,"P",{});var RV=n(bm);Uyr=r(RV,`Note:
Loading a model from its configuration file does `),aTe=s(RV,"STRONG",{});var Gut=n(aTe);Jyr=r(Gut,"not"),Gut.forEach(t),Yyr=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),sTe=s(RV,"CODE",{});var Out=n(sTe);Kyr=r(Out,"from_pretrained()"),Out.forEach(t),Zyr=r(RV,"to load the model weights."),RV.forEach(t),ewr=i(Ci),nTe=s(Ci,"P",{});var Xut=n(nTe);owr=r(Xut,"Examples:"),Xut.forEach(t),rwr=i(Ci),f(RL.$$.fragment,Ci),Ci.forEach(t),twr=i(Fi),Io=s(Fi,"DIV",{class:!0});var Sa=n(Io);f(SL.$$.fragment,Sa),awr=i(Sa),lTe=s(Sa,"P",{});var zut=n(lTe);swr=r(zut,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),zut.forEach(t),nwr=i(Sa),xs=s(Sa,"P",{});var Z3=n(xs);lwr=r(Z3,"The model class to instantiate is selected based on the "),iTe=s(Z3,"CODE",{});var Vut=n(iTe);iwr=r(Vut,"model_type"),Vut.forEach(t),dwr=r(Z3,` property of the config object (either
passed as an argument or loaded from `),dTe=s(Z3,"CODE",{});var Wut=n(dTe);cwr=r(Wut,"pretrained_model_name_or_path"),Wut.forEach(t),mwr=r(Z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cTe=s(Z3,"CODE",{});var Qut=n(cTe);fwr=r(Qut,"pretrained_model_name_or_path"),Qut.forEach(t),gwr=r(Z3,":"),Z3.forEach(t),hwr=i(Sa),PL=s(Sa,"UL",{});var dxe=n(PL);e3=s(dxe,"LI",{});var e8e=n(e3);mTe=s(e8e,"STRONG",{});var Hut=n(mTe);uwr=r(Hut,"beit"),Hut.forEach(t),pwr=r(e8e," \u2014 "),qX=s(e8e,"A",{href:!0});var Uut=n(qX);_wr=r(Uut,"FlaxBeitForImageClassification"),Uut.forEach(t),bwr=r(e8e," (BEiT model)"),e8e.forEach(t),vwr=i(dxe),o3=s(dxe,"LI",{});var o8e=n(o3);fTe=s(o8e,"STRONG",{});var Jut=n(fTe);Twr=r(Jut,"vit"),Jut.forEach(t),Fwr=r(o8e," \u2014 "),GX=s(o8e,"A",{href:!0});var Yut=n(GX);Cwr=r(Yut,"FlaxViTForImageClassification"),Yut.forEach(t),Mwr=r(o8e," (ViT model)"),o8e.forEach(t),dxe.forEach(t),Ewr=i(Sa),gTe=s(Sa,"P",{});var Kut=n(gTe);ywr=r(Kut,"Examples:"),Kut.forEach(t),wwr=i(Sa),f($L.$$.fragment,Sa),Sa.forEach(t),Fi.forEach(t),eBe=i(d),vm=s(d,"H2",{class:!0});var cxe=n(vm);r3=s(cxe,"A",{id:!0,class:!0,href:!0});var Zut=n(r3);hTe=s(Zut,"SPAN",{});var ept=n(hTe);f(IL.$$.fragment,ept),ept.forEach(t),Zut.forEach(t),Awr=i(cxe),uTe=s(cxe,"SPAN",{});var opt=n(uTe);Lwr=r(opt,"FlaxAutoModelForVision2Seq"),opt.forEach(t),cxe.forEach(t),oBe=i(d),jr=s(d,"DIV",{class:!0});var Mi=n(jr);f(jL.$$.fragment,Mi),Bwr=i(Mi),Tm=s(Mi,"P",{});var SV=n(Tm);kwr=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),pTe=s(SV,"CODE",{});var rpt=n(pTe);xwr=r(rpt,"from_pretrained()"),rpt.forEach(t),Rwr=r(SV,"class method or the "),_Te=s(SV,"CODE",{});var tpt=n(_Te);Swr=r(tpt,"from_config()"),tpt.forEach(t),Pwr=r(SV,`class
method.`),SV.forEach(t),$wr=i(Mi),NL=s(Mi,"P",{});var mxe=n(NL);Iwr=r(mxe,"This class cannot be instantiated directly using "),bTe=s(mxe,"CODE",{});var apt=n(bTe);jwr=r(apt,"__init__()"),apt.forEach(t),Nwr=r(mxe," (throws an error)."),mxe.forEach(t),Dwr=i(Mi),kt=s(Mi,"DIV",{class:!0});var Ei=n(kt);f(DL.$$.fragment,Ei),qwr=i(Ei),vTe=s(Ei,"P",{});var spt=n(vTe);Gwr=r(spt,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),spt.forEach(t),Owr=i(Ei),Fm=s(Ei,"P",{});var PV=n(Fm);Xwr=r(PV,`Note:
Loading a model from its configuration file does `),TTe=s(PV,"STRONG",{});var npt=n(TTe);zwr=r(npt,"not"),npt.forEach(t),Vwr=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),FTe=s(PV,"CODE",{});var lpt=n(FTe);Wwr=r(lpt,"from_pretrained()"),lpt.forEach(t),Qwr=r(PV,"to load the model weights."),PV.forEach(t),Hwr=i(Ei),CTe=s(Ei,"P",{});var ipt=n(CTe);Uwr=r(ipt,"Examples:"),ipt.forEach(t),Jwr=i(Ei),f(qL.$$.fragment,Ei),Ei.forEach(t),Ywr=i(Mi),jo=s(Mi,"DIV",{class:!0});var Pa=n(jo);f(GL.$$.fragment,Pa),Kwr=i(Pa),MTe=s(Pa,"P",{});var dpt=n(MTe);Zwr=r(dpt,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),dpt.forEach(t),eAr=i(Pa),Rs=s(Pa,"P",{});var e5=n(Rs);oAr=r(e5,"The model class to instantiate is selected based on the "),ETe=s(e5,"CODE",{});var cpt=n(ETe);rAr=r(cpt,"model_type"),cpt.forEach(t),tAr=r(e5,` property of the config object (either
passed as an argument or loaded from `),yTe=s(e5,"CODE",{});var mpt=n(yTe);aAr=r(mpt,"pretrained_model_name_or_path"),mpt.forEach(t),sAr=r(e5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),wTe=s(e5,"CODE",{});var fpt=n(wTe);nAr=r(fpt,"pretrained_model_name_or_path"),fpt.forEach(t),lAr=r(e5,":"),e5.forEach(t),iAr=i(Pa),ATe=s(Pa,"UL",{});var gpt=n(ATe);t3=s(gpt,"LI",{});var r8e=n(t3);LTe=s(r8e,"STRONG",{});var hpt=n(LTe);dAr=r(hpt,"vision-encoder-decoder"),hpt.forEach(t),cAr=r(r8e," \u2014 "),OX=s(r8e,"A",{href:!0});var upt=n(OX);mAr=r(upt,"FlaxVisionEncoderDecoderModel"),upt.forEach(t),fAr=r(r8e," (Vision Encoder decoder model)"),r8e.forEach(t),gpt.forEach(t),gAr=i(Pa),BTe=s(Pa,"P",{});var ppt=n(BTe);hAr=r(ppt,"Examples:"),ppt.forEach(t),uAr=i(Pa),f(OL.$$.fragment,Pa),Pa.forEach(t),Mi.forEach(t),this.h()},h(){c(J,"name","hf:doc:metadata"),c(J,"content",JSON.stringify(ypt)),c(fe,"id","auto-classes"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#auto-classes"),c(ie,"class","relative group"),c(Ss,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),c($s,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),c(Is,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),c(Ri,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(Am,"id","extending-the-auto-classes"),c(Am,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Am,"href","#extending-the-auto-classes"),c(Si,"class","relative group"),c(Bm,"id","transformers.AutoConfig"),c(Bm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bm,"href","#transformers.AutoConfig"),c(Pi,"class","relative group"),c(V7,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(W7,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),c(Q7,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),c(H7,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),c(U7,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),c(J7,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig"),c(Y7,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig"),c(K7,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(Z7,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c(e8,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),c(o8,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),c(r8,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),c(t8,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),c(a8,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),c(s8,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig"),c(n8,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),c(l8,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),c(i8,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config"),c(d8,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),c(c8,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),c(m8,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),c(f8,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),c(g8,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),c(h8,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),c(u8,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),c(p8,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),c(_8,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),c(b8,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),c(v8,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),c(T8,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(F8,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),c(C8,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),c(M8,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),c(E8,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(y8,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(w8,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(A8,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),c(L8,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),c(B8,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),c(k8,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),c(x8,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),c(R8,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),c(S8,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),c(P8,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),c($8,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(I8,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),c(j8,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),c(N8,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig"),c(D8,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),c(q8,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),c(G8,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),c(O8,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig"),c(X8,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig"),c(z8,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(V8,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(W8,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),c(Q8,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmConfig"),c(H8,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),c(U8,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),c(J8,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),c(Y8,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),c(K8,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),c(Z8,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),c(e9,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),c(o9,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig"),c(r9,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),c(t9,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(a9,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c(s9,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),c(n9,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(l9,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig"),c(i9,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),c(d9,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),c(c9,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),c(m9,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),c(f9,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(g9,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),c(h9,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig"),c(u9,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),c(p9,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),c(_9,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(b9,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),c(v9,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig"),c(T9,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(F9,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig"),c(C9,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig"),c(M9,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),c(E9,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),c(y9,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),c(w9,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),c(A9,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),c(L9,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig"),c(mo,"class","docstring"),c(hg,"class","docstring"),c(Go,"class","docstring"),c(ug,"id","transformers.AutoTokenizer"),c(ug,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ug,"href","#transformers.AutoTokenizer"),c(Ii,"class","relative group"),c(B9,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(k9,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),c(x9,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(R9,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),c(S9,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),c(P9,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),c($9,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(I9,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(j9,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(N9,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(D9,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),c(q9,"href","/docs/transformers/master/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),c(G9,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(O9,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizer"),c(X9,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),c(z9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(V9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(W9,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(Q9,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(H9,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),c(U9,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(J9,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),c(Y9,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(K9,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),c(Z9,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),c(eB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(oB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(rB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(tB,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),c(aB,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(sB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),c(nB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(lB,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),c(iB,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(dB,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(cB,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(mB,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c(fB,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),c(gB,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(hB,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(uB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),c(pB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(_B,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(bB,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),c(vB,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(TB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(FB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(CB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(MB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(EB,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizer"),c(yB,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizerFast"),c(wB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(AB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(LB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(BB,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(kB,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(xB,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(RB,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(SB,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),c(PB,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),c($B,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),c(IB,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),c(jB,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),c(NB,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(DB,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),c(qB,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(GB,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(OB,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(XB,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),c(zB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),c(VB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(WB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(QB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(HB,"href","/docs/transformers/master/en/model_doc/mluke#transformers.MLukeTokenizer"),c(UB,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(JB,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(YB,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(KB,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(ZB,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),c(ek,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),c(ok,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),c(rk,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),c(tk,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(ak,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(sk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverTokenizer"),c(nk,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),c(lk,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartTokenizer"),c(ik,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(dk,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(ck,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(mk,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),c(fk,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),c(gk,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(hk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),c(uk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(pk,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(_k,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(bk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(vk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(Tk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(Fk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(Ck,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Mk,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(Ek,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),c(yk,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(wk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(Ak,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(Lk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),c(Bk,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),c(kk,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),c(xk,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),c(Rk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(Sk,"href","/docs/transformers/master/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),c(Pk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizer"),c($k,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizerFast"),c(Ik,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),c(jk,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),c(Nk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),c(Dk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),c(qk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(Gk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(fo,"class","docstring"),c(Vg,"class","docstring"),c(Oo,"class","docstring"),c(Wg,"id","transformers.AutoFeatureExtractor"),c(Wg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wg,"href","#transformers.AutoFeatureExtractor"),c(ji,"class","relative group"),c(Ok,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(Xk,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(zk,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(Vk,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),c(Wk,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(Qk,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(Hk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Uk,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(Jk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),c(Yk,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),c(Kk,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),c(Zk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(ex,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(ox,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(rx,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(tx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(Le,"class","docstring"),c(dh,"class","docstring"),c(Xo,"class","docstring"),c(ch,"id","transformers.AutoProcessor"),c(ch,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ch,"href","#transformers.AutoProcessor"),c(Ni,"class","relative group"),c(ax,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(sx,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),c(nx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(lx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),c(ix,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(dx,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(cx,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),c(mx,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),c(fx,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Be,"class","docstring"),c(Th,"class","docstring"),c(zo,"class","docstring"),c(Fh,"id","transformers.AutoModel"),c(Fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fh,"href","#transformers.AutoModel"),c(qi,"class","relative group"),c(Nr,"class","docstring"),c(gx,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),c(hx,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),c(ux,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),c(px,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(_x,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),c(bx,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel"),c(vx,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(Tx,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(Fx,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),c(Cx,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),c(Mx,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),c(Ex,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),c(yx,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),c(wx,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel"),c(Ax,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),c(Lx,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),c(Bx,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model"),c(kx,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),c(xx,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),c(Rx,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),c(Sx,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(Px,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),c($x,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),c(Ix,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),c(jx,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),c(Nx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(Dx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),c(qx,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),c(Gx,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(Ox,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),c(Xx,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),c(zx,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),c(Vx,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(Wx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(Qx,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(Hx,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),c(Ux,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),c(Jx,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),c(Yx,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),c(Kx,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),c(Zx,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),c(eR,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),c(oR,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel"),c(rR,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),c(tR,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),c(aR,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),c(sR,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel"),c(nR,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),c(lR,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),c(iR,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),c(dR,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel"),c(cR,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel"),c(mR,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(fR,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel"),c(gR,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),c(hR,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),c(uR,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(pR,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),c(_R,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),c(bR,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),c(vR,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),c(TR,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel"),c(FR,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(CR,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),c(MR,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(ER,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinModel"),c(yR,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),c(wR,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),c(AR,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel"),c(LR,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),c(BR,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),c(kR,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel"),c(xR,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),c(RR,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),c(SR,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),c(PR,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel"),c($R,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(IR,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel"),c(jR,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel"),c(NR,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),c(DR,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),c(qR,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),c(GR,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),c(OR,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),c(XR,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel"),c(ke,"class","docstring"),c(Vo,"class","docstring"),c(Ku,"id","transformers.AutoModelForPreTraining"),c(Ku,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ku,"href","#transformers.AutoModelForPreTraining"),c(Xi,"class","relative group"),c(Dr,"class","docstring"),c(zR,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),c(VR,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(WR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),c(QR,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),c(HR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(UR,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(JR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(YR,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(KR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(ZR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),c(eS,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(oS,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),c(rS,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(tS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(aS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(sS,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(nS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(lS,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(iS,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(dS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),c(cS,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(mS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(fS,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(gS,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(hS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(uS,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(pS,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(_S,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(bS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(vS,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(TS,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),c(FS,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(CS,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),c(MS,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(ES,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(yS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(wS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(AS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(xe,"class","docstring"),c(Wo,"class","docstring"),c(Np,"id","transformers.AutoModelForCausalLM"),c(Np,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Np,"href","#transformers.AutoModelForCausalLM"),c(Wi,"class","relative group"),c(qr,"class","docstring"),c(LS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),c(BS,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),c(kS,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),c(xS,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),c(RS,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(SS,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(PS,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),c($S,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(IS,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(jS,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM"),c(NS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(DS,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c(qS,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(GS,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),c(OS,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),c(XS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),c(zS,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),c(VS,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(WS,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM"),c(QS,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(HS,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(US,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(JS,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(YS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(KS,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(ZS,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(eP,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),c(oP,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(rP,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM"),c(tP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(aP,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),c(sP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),c(nP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),c(lP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Re,"class","docstring"),c(Qo,"class","docstring"),c(T_,"id","transformers.AutoModelForMaskedLM"),c(T_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T_,"href","#transformers.AutoModelForMaskedLM"),c(Ui,"class","relative group"),c(Gr,"class","docstring"),c(iP,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(dP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(cP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),c(mP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),c(fP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(gP,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(hP,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(uP,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),c(pP,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(_P,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(bP,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(vP,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(TP,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(FP,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(CP,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(MP,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(EP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(yP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),c(wP,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(AP,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(LP,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),c(BP,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(kP,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(xP,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(RP,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(SP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(PP,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c($P,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(IP,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(jP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(NP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),c(DP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),c(qP,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM"),c(Se,"class","docstring"),c(Ho,"class","docstring"),c(rb,"id","transformers.AutoModelForSeq2SeqLM"),c(rb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rb,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ki,"class","relative group"),c(Or,"class","docstring"),c(GP,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(OP,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(XP,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(zP,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),c(VP,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),c(WP,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(QP,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(HP,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(UP,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),c(JP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(YP,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(KP,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(ZP,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),c(e$,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(o$,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(r$,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Pe,"class","docstring"),c(Uo,"class","docstring"),c(Tb,"id","transformers.AutoModelForSequenceClassification"),c(Tb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Tb,"href","#transformers.AutoModelForSequenceClassification"),c(od,"class","relative group"),c(Xr,"class","docstring"),c(t$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(a$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),c(s$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),c(n$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),c(l$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(i$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(d$,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(c$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(m$,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(f$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(g$,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),c(h$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(u$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(p$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(_$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(b$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(v$,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(T$,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(F$,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(C$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(M$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(E$,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(y$,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),c(w$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(A$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(L$,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),c(B$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(k$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(x$,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),c(R$,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),c(S$,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(P$,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),c($$,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(I$,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(j$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(N$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(D$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(q$,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(G$,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(O$,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),c(X$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(z$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),c(V$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),c(W$,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(Q$,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification"),c($e,"class","docstring"),c(Jo,"class","docstring"),c(g2,"id","transformers.AutoModelForMultipleChoice"),c(g2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g2,"href","#transformers.AutoModelForMultipleChoice"),c(ad,"class","relative group"),c(zr,"class","docstring"),c(H$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(U$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),c(J$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),c(Y$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(K$,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(Z$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(eI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(oI,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(rI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(tI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(aI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(sI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(nI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(lI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),c(iI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(dI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(cI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),c(mI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(fI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(gI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(hI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(uI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(pI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(_I,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),c(bI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),c(vI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(TI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice"),c(Ie,"class","docstring"),c(Yo,"class","docstring"),c(O2,"id","transformers.AutoModelForNextSentencePrediction"),c(O2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(O2,"href","#transformers.AutoModelForNextSentencePrediction"),c(ld,"class","relative group"),c(Vr,"class","docstring"),c(FI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(CI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(MI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),c(EI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(yI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(je,"class","docstring"),c(Ko,"class","docstring"),c(U2,"id","transformers.AutoModelForTokenClassification"),c(U2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(U2,"href","#transformers.AutoModelForTokenClassification"),c(cd,"class","relative group"),c(Wr,"class","docstring"),c(wI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(AI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),c(LI,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),c(BI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(kI,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),c(xI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(RI,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(SI,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),c(PI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c($I,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(II,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(jI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(NI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(DI,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(qI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(GI,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(OI,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(XI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(zI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),c(VI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(WI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(QI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),c(HI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(UI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(JI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(YI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(KI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(ZI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(ej,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),c(oj,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),c(rj,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(tj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification"),c(Ne,"class","docstring"),c(Zo,"class","docstring"),c(Bv,"id","transformers.AutoModelForQuestionAnswering"),c(Bv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bv,"href","#transformers.AutoModelForQuestionAnswering"),c(gd,"class","relative group"),c(Qr,"class","docstring"),c(aj,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(sj,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(nj,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(lj,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),c(ij,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(dj,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(cj,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(mj,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(fj,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(gj,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),c(hj,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(uj,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(pj,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(_j,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(bj,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(vj,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(Tj,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(Fj,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(Cj,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(Mj,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(Ej,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(yj,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(wj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),c(Aj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(Lj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(Bj,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),c(kj,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(xj,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(Rj,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(Sj,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Pj,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c($j,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(Ij,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(jj,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(Nj,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),c(Dj,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),c(qj,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(Gj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),c(De,"class","docstring"),c(er,"class","docstring"),c(uT,"id","transformers.AutoModelForTableQuestionAnswering"),c(uT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uT,"href","#transformers.AutoModelForTableQuestionAnswering"),c(pd,"class","relative group"),c(Hr,"class","docstring"),c(Oj,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(qe,"class","docstring"),c(or,"class","docstring"),c(bT,"id","transformers.AutoModelForImageClassification"),c(bT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bT,"href","#transformers.AutoModelForImageClassification"),c(vd,"class","relative group"),c(Ur,"class","docstring"),c(Xj,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),c(zj,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification"),c(Vj,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),c(Wj,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(Qj,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(Hj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(Uj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(Jj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(Yj,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),c(Kj,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(Zj,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification"),c(eN,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),c(Ge,"class","docstring"),c(rr,"class","docstring"),c(AT,"id","transformers.AutoModelForVision2Seq"),c(AT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(AT,"href","#transformers.AutoModelForVision2Seq"),c(Cd,"class","relative group"),c(Jr,"class","docstring"),c(oN,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),c(Oe,"class","docstring"),c(tr,"class","docstring"),c(kT,"id","transformers.AutoModelForAudioClassification"),c(kT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kT,"href","#transformers.AutoModelForAudioClassification"),c(yd,"class","relative group"),c(Yr,"class","docstring"),c(rN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(tN,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(aN,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),c(sN,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(nN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),c(lN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(iN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),c(Xe,"class","docstring"),c(ar,"class","docstring"),c(DT,"id","transformers.AutoModelForAudioFrameClassification"),c(DT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(DT,"href","#transformers.AutoModelForAudioFrameClassification"),c(Ld,"class","relative group"),c(Kr,"class","docstring"),c(dN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),c(cN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),c(mN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),c(ze,"class","docstring"),c(sr,"class","docstring"),c(zT,"id","transformers.AutoModelForCTC"),c(zT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zT,"href","#transformers.AutoModelForCTC"),c(Rd,"class","relative group"),c(Zr,"class","docstring"),c(fN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),c(gN,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),c(hN,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC"),c(uN,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(pN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),c(_N,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(bN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC"),c(Ve,"class","docstring"),c(nr,"class","docstring"),c(ZT,"id","transformers.AutoModelForSpeechSeq2Seq"),c(ZT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZT,"href","#transformers.AutoModelForSpeechSeq2Seq"),c($d,"class","relative group"),c(et,"class","docstring"),c(vN,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),c(TN,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(We,"class","docstring"),c(lr,"class","docstring"),c(t1,"id","transformers.AutoModelForAudioXVector"),c(t1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(t1,"href","#transformers.AutoModelForAudioXVector"),c(Nd,"class","relative group"),c(ot,"class","docstring"),c(FN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),c(CN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),c(MN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector"),c(Qe,"class","docstring"),c(ir,"class","docstring"),c(i1,"id","transformers.AutoModelForMaskedImageModeling"),c(i1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(i1,"href","#transformers.AutoModelForMaskedImageModeling"),c(Od,"class","relative group"),c(rt,"class","docstring"),c(EN,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),c(yN,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),c(wN,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),c(He,"class","docstring"),c(dr,"class","docstring"),c(g1,"id","transformers.AutoModelForObjectDetection"),c(g1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g1,"href","#transformers.AutoModelForObjectDetection"),c(Wd,"class","relative group"),c(tt,"class","docstring"),c(AN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Ue,"class","docstring"),c(cr,"class","docstring"),c(p1,"id","transformers.AutoModelForImageSegmentation"),c(p1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p1,"href","#transformers.AutoModelForImageSegmentation"),c(Ud,"class","relative group"),c(at,"class","docstring"),c(LN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),c(Je,"class","docstring"),c(mr,"class","docstring"),c(v1,"id","transformers.AutoModelForSemanticSegmentation"),c(v1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(v1,"href","#transformers.AutoModelForSemanticSegmentation"),c(Kd,"class","relative group"),c(st,"class","docstring"),c(BN,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),c(kN,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),c(Ye,"class","docstring"),c(fr,"class","docstring"),c(M1,"id","transformers.TFAutoModel"),c(M1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M1,"href","#transformers.TFAutoModel"),c(oc,"class","relative group"),c(nt,"class","docstring"),c(xN,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),c(RN,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),c(SN,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),c(PN,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c($N,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),c(IN,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),c(jN,"href","/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel"),c(NN,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),c(DN,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),c(qN,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),c(GN,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),c(ON,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(XN,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(zN,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),c(VN,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(WN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),c(QN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(HN,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),c(UN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),c(JN,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(YN,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),c(KN,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),c(ZN,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),c(eD,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),c(oD,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),c(rD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(tD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),c(aD,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),c(sD,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),c(nD,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),c(lD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),c(iD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),c(dD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),c(cD,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),c(mD,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),c(fD,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),c(gD,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),c(hD,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),c(uD,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(pD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),c(_D,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),c(bD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),c(go,"class","docstring"),c(gr,"class","docstring"),c(cF,"id","transformers.TFAutoModelForPreTraining"),c(cF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(cF,"href","#transformers.TFAutoModelForPreTraining"),c(ac,"class","relative group"),c(lt,"class","docstring"),c(vD,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(TD,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(FD,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),c(CD,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(MD,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(ED,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(yD,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(wD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(AD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(LD,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(BD,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(kD,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(xD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(RD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(SD,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(PD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c($D,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(ID,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(jD,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(ND,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(DD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(qD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(ho,"class","docstring"),c(hr,"class","docstring"),c(SF,"id","transformers.TFAutoModelForCausalLM"),c(SF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(SF,"href","#transformers.TFAutoModelForCausalLM"),c(lc,"class","relative group"),c(it,"class","docstring"),c(GD,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(OD,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(XD,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(zD,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),c(VD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(WD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(QD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(HD,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),c(UD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(JD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(uo,"class","docstring"),c(ur,"class","docstring"),c(zF,"id","transformers.TFAutoModelForImageClassification"),c(zF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zF,"href","#transformers.TFAutoModelForImageClassification"),c(cc,"class","relative group"),c(dt,"class","docstring"),c(YD,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),c(po,"class","docstring"),c(pr,"class","docstring"),c(WF,"id","transformers.TFAutoModelForMaskedLM"),c(WF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(WF,"href","#transformers.TFAutoModelForMaskedLM"),c(gc,"class","relative group"),c(ct,"class","docstring"),c(KD,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(ZD,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(eq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(oq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(rq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(tq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),c(aq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(sq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(nq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(lq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(iq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(dq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(cq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(mq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(fq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(gq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(hq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(uq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(pq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(_q,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),c(_o,"class","docstring"),c(_r,"class","docstring"),c(gC,"id","transformers.TFAutoModelForSeq2SeqLM"),c(gC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gC,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(pc,"class","relative group"),c(mt,"class","docstring"),c(bq,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(vq,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(Tq,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(Fq,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),c(Cq,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(Mq,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),c(Eq,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(yq,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(wq,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(Aq,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(bo,"class","docstring"),c(br,"class","docstring"),c(EC,"id","transformers.TFAutoModelForSequenceClassification"),c(EC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(EC,"href","#transformers.TFAutoModelForSequenceClassification"),c(vc,"class","relative group"),c(ft,"class","docstring"),c(Lq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(Bq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(kq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(xq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(Rq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(Sq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Pq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),c($q,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Iq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(jq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(Nq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(Dq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(qq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Gq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Oq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(Xq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(zq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(Vq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(Wq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Qq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(Hq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(Uq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),c(Jq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(Yq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),c(Kq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(vo,"class","docstring"),c(vr,"class","docstring"),c(JC,"id","transformers.TFAutoModelForMultipleChoice"),c(JC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(JC,"href","#transformers.TFAutoModelForMultipleChoice"),c(Cc,"class","relative group"),c(gt,"class","docstring"),c(Zq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(eG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(oG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(rG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(tG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(aG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(sG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(nG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(lG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(iG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(dG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(cG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(mG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(fG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(gG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(hG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),c(uG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(To,"class","docstring"),c(Tr,"class","docstring"),c(h4,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(h4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(h4,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c(yc,"class","relative group"),c(ht,"class","docstring"),c(pG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(Fo,"class","docstring"),c(Fr,"class","docstring"),c(p4,"id","transformers.TFAutoModelForTokenClassification"),c(p4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(p4,"href","#transformers.TFAutoModelForTokenClassification"),c(Lc,"class","relative group"),c(ut,"class","docstring"),c(_G,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(bG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(vG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(TG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(FG,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(CG,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),c(MG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(EG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(yG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(wG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(AG,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(LG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(BG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(kG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(xG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(RG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(SG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(PG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c($G,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),c(IG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(Co,"class","docstring"),c(Cr,"class","docstring"),c(j4,"id","transformers.TFAutoModelForQuestionAnswering"),c(j4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j4,"href","#transformers.TFAutoModelForQuestionAnswering"),c(xc,"class","relative group"),c(pt,"class","docstring"),c(jG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(NG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(DG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(qG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(GG,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(OG,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),c(XG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(zG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(VG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(WG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(QG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(HG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(UG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(JG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(YG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(KG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(ZG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(eO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),c(oO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(Mo,"class","docstring"),c(Mr,"class","docstring"),c(tM,"id","transformers.TFAutoModelForVision2Seq"),c(tM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tM,"href","#transformers.TFAutoModelForVision2Seq"),c(Pc,"class","relative group"),c(_t,"class","docstring"),c(rO,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),c(Eo,"class","docstring"),c(Er,"class","docstring"),c(sM,"id","transformers.TFAutoModelForSpeechSeq2Seq"),c(sM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sM,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),c(jc,"class","relative group"),c(bt,"class","docstring"),c(tO,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),c(yo,"class","docstring"),c(yr,"class","docstring"),c(lM,"id","transformers.FlaxAutoModel"),c(lM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lM,"href","#transformers.FlaxAutoModel"),c(qc,"class","relative group"),c(vt,"class","docstring"),c(aO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),c(sO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),c(nO,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),c(lO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),c(iO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),c(dO,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(cO,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),c(mO,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),c(fO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(gO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),c(hO,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(uO,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(pO,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(_O,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),c(bO,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),c(vO,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),c(TO,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(FO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(CO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel"),c(MO,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),c(EO,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),c(yO,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),c(wO,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(AO,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel"),c(wo,"class","docstring"),c(wr,"class","docstring"),c(RM,"id","transformers.FlaxAutoModelForCausalLM"),c(RM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(RM,"href","#transformers.FlaxAutoModelForCausalLM"),c(Xc,"class","relative group"),c(Tt,"class","docstring"),c(LO,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(BO,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(kO,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(xO,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),c(Ao,"class","docstring"),c(Ar,"class","docstring"),c(jM,"id","transformers.FlaxAutoModelForPreTraining"),c(jM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jM,"href","#transformers.FlaxAutoModelForPreTraining"),c(Wc,"class","relative group"),c(Ft,"class","docstring"),c(RO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(SO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(PO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c($O,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),c(IO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(jO,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(NO,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(DO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(qO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(GO,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(OO,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(UM,"id","transformers.FlaxAutoModelForMaskedLM"),c(UM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(UM,"href","#transformers.FlaxAutoModelForMaskedLM"),c(Uc,"class","relative group"),c(Ct,"class","docstring"),c(XO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(zO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(VO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(WO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),c(QO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(HO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(UO,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(JO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(YO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(sE,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(sE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sE,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(Kc,"class","relative group"),c(Mt,"class","docstring"),c(KO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(ZO,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(eX,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(oX,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),c(rX,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(tX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(aX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(sX,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(nX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(ko,"class","docstring"),c(kr,"class","docstring"),c(uE,"id","transformers.FlaxAutoModelForSequenceClassification"),c(uE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uE,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(om,"class","relative group"),c(Et,"class","docstring"),c(lX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(iX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(dX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(cX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),c(mX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(fX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(gX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(hX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(uX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),c(xo,"class","docstring"),c(xr,"class","docstring"),c(yE,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(yE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(yE,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(am,"class","relative group"),c(yt,"class","docstring"),c(pX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(_X,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(bX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(vX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),c(TX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(FX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(CX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(MX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(EX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),c(Ro,"class","docstring"),c(Rr,"class","docstring"),c($E,"id","transformers.FlaxAutoModelForTokenClassification"),c($E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($E,"href","#transformers.FlaxAutoModelForTokenClassification"),c(lm,"class","relative group"),c(wt,"class","docstring"),c(yX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(wX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(AX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),c(LX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(BX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(kX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(xX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),c(So,"class","docstring"),c(Sr,"class","docstring"),c(XE,"id","transformers.FlaxAutoModelForMultipleChoice"),c(XE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(XE,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(cm,"class","relative group"),c(At,"class","docstring"),c(RX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(SX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c(PX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),c($X,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(IX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(jX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(NX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),c(Po,"class","docstring"),c(Pr,"class","docstring"),c(YE,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(YE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(YE,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(gm,"class","relative group"),c(Lt,"class","docstring"),c(DX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c($o,"class","docstring"),c($r,"class","docstring"),c(ZE,"id","transformers.FlaxAutoModelForImageClassification"),c(ZE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ZE,"href","#transformers.FlaxAutoModelForImageClassification"),c(pm,"class","relative group"),c(Bt,"class","docstring"),c(qX,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(GX,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Io,"class","docstring"),c(Ir,"class","docstring"),c(r3,"id","transformers.FlaxAutoModelForVision2Seq"),c(r3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(r3,"href","#transformers.FlaxAutoModelForVision2Seq"),c(vm,"class","relative group"),c(kt,"class","docstring"),c(OX,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),c(jo,"class","docstring"),c(jr,"class","docstring")},m(d,_){e(document.head,J),b(d,Ae,_),b(d,ie,_),e(ie,fe),e(fe,to),g(ce,to,null),e(ie,_e),e(ie,Do),e(Do,wi),b(d,Mm,_),b(d,na,_),e(na,Ai),e(na,Li),e(Li,o5),e(na,Em),b(d,ye,_),b(d,io,_),e(io,Bi),e(io,Ss),e(Ss,r5),e(io,Ps),e(io,$s),e($s,t5),e(io,ki),e(io,Is),e(Is,a5),e(io,xi),b(d,ym,_),g($a,d,_),b(d,co,_),b(d,ge,_),e(ge,D7),e(ge,Ri),e(Ri,q7),e(ge,G7),b(d,qo,_),b(d,Ia,_),e(Ia,O7),e(Ia,wm),e(wm,X7),e(Ia,fxe),b(d,t8e,_),b(d,Si,_),e(Si,Am),e(Am,$V),g(s5,$V,null),e(Si,gxe),e(Si,IV),e(IV,hxe),b(d,a8e,_),b(d,js,_),e(js,uxe),e(js,jV),e(jV,pxe),e(js,_xe),e(js,NV),e(NV,bxe),e(js,vxe),b(d,s8e,_),g(n5,d,_),b(d,n8e,_),b(d,z7,_),e(z7,Txe),b(d,l8e,_),g(Lm,d,_),b(d,i8e,_),b(d,Pi,_),e(Pi,Bm),e(Bm,DV),g(l5,DV,null),e(Pi,Fxe),e(Pi,qV),e(qV,Cxe),b(d,d8e,_),b(d,Go,_),g(i5,Go,null),e(Go,Mxe),e(Go,d5),e(d5,Exe),e(d5,V7),e(V7,yxe),e(d5,wxe),e(Go,Axe),e(Go,c5),e(c5,Lxe),e(c5,GV),e(GV,Bxe),e(c5,kxe),e(Go,xxe),e(Go,mo),g(m5,mo,null),e(mo,Rxe),e(mo,OV),e(OV,Sxe),e(mo,Pxe),e(mo,$i),e($i,$xe),e($i,XV),e(XV,Ixe),e($i,jxe),e($i,zV),e(zV,Nxe),e($i,Dxe),e(mo,qxe),e(mo,v),e(v,km),e(km,VV),e(VV,Gxe),e(km,Oxe),e(km,W7),e(W7,Xxe),e(km,zxe),e(v,Vxe),e(v,xm),e(xm,WV),e(WV,Wxe),e(xm,Qxe),e(xm,Q7),e(Q7,Hxe),e(xm,Uxe),e(v,Jxe),e(v,Rm),e(Rm,QV),e(QV,Yxe),e(Rm,Kxe),e(Rm,H7),e(H7,Zxe),e(Rm,eRe),e(v,oRe),e(v,Sm),e(Sm,HV),e(HV,rRe),e(Sm,tRe),e(Sm,U7),e(U7,aRe),e(Sm,sRe),e(v,nRe),e(v,Pm),e(Pm,UV),e(UV,lRe),e(Pm,iRe),e(Pm,J7),e(J7,dRe),e(Pm,cRe),e(v,mRe),e(v,$m),e($m,JV),e(JV,fRe),e($m,gRe),e($m,Y7),e(Y7,hRe),e($m,uRe),e(v,pRe),e(v,Im),e(Im,YV),e(YV,_Re),e(Im,bRe),e(Im,K7),e(K7,vRe),e(Im,TRe),e(v,FRe),e(v,jm),e(jm,KV),e(KV,CRe),e(jm,MRe),e(jm,Z7),e(Z7,ERe),e(jm,yRe),e(v,wRe),e(v,Nm),e(Nm,ZV),e(ZV,ARe),e(Nm,LRe),e(Nm,e8),e(e8,BRe),e(Nm,kRe),e(v,xRe),e(v,Dm),e(Dm,eW),e(eW,RRe),e(Dm,SRe),e(Dm,o8),e(o8,PRe),e(Dm,$Re),e(v,IRe),e(v,qm),e(qm,oW),e(oW,jRe),e(qm,NRe),e(qm,r8),e(r8,DRe),e(qm,qRe),e(v,GRe),e(v,Gm),e(Gm,rW),e(rW,ORe),e(Gm,XRe),e(Gm,t8),e(t8,zRe),e(Gm,VRe),e(v,WRe),e(v,Om),e(Om,tW),e(tW,QRe),e(Om,HRe),e(Om,a8),e(a8,URe),e(Om,JRe),e(v,YRe),e(v,Xm),e(Xm,aW),e(aW,KRe),e(Xm,ZRe),e(Xm,s8),e(s8,eSe),e(Xm,oSe),e(v,rSe),e(v,zm),e(zm,sW),e(sW,tSe),e(zm,aSe),e(zm,n8),e(n8,sSe),e(zm,nSe),e(v,lSe),e(v,Vm),e(Vm,nW),e(nW,iSe),e(Vm,dSe),e(Vm,l8),e(l8,cSe),e(Vm,mSe),e(v,fSe),e(v,Wm),e(Wm,lW),e(lW,gSe),e(Wm,hSe),e(Wm,i8),e(i8,uSe),e(Wm,pSe),e(v,_Se),e(v,Qm),e(Qm,iW),e(iW,bSe),e(Qm,vSe),e(Qm,d8),e(d8,TSe),e(Qm,FSe),e(v,CSe),e(v,Hm),e(Hm,dW),e(dW,MSe),e(Hm,ESe),e(Hm,c8),e(c8,ySe),e(Hm,wSe),e(v,ASe),e(v,Um),e(Um,cW),e(cW,LSe),e(Um,BSe),e(Um,m8),e(m8,kSe),e(Um,xSe),e(v,RSe),e(v,Jm),e(Jm,mW),e(mW,SSe),e(Jm,PSe),e(Jm,f8),e(f8,$Se),e(Jm,ISe),e(v,jSe),e(v,Ym),e(Ym,fW),e(fW,NSe),e(Ym,DSe),e(Ym,g8),e(g8,qSe),e(Ym,GSe),e(v,OSe),e(v,Km),e(Km,gW),e(gW,XSe),e(Km,zSe),e(Km,h8),e(h8,VSe),e(Km,WSe),e(v,QSe),e(v,Zm),e(Zm,hW),e(hW,HSe),e(Zm,USe),e(Zm,u8),e(u8,JSe),e(Zm,YSe),e(v,KSe),e(v,ef),e(ef,uW),e(uW,ZSe),e(ef,ePe),e(ef,p8),e(p8,oPe),e(ef,rPe),e(v,tPe),e(v,of),e(of,pW),e(pW,aPe),e(of,sPe),e(of,_8),e(_8,nPe),e(of,lPe),e(v,iPe),e(v,rf),e(rf,_W),e(_W,dPe),e(rf,cPe),e(rf,b8),e(b8,mPe),e(rf,fPe),e(v,gPe),e(v,tf),e(tf,bW),e(bW,hPe),e(tf,uPe),e(tf,v8),e(v8,pPe),e(tf,_Pe),e(v,bPe),e(v,af),e(af,vW),e(vW,vPe),e(af,TPe),e(af,T8),e(T8,FPe),e(af,CPe),e(v,MPe),e(v,sf),e(sf,TW),e(TW,EPe),e(sf,yPe),e(sf,F8),e(F8,wPe),e(sf,APe),e(v,LPe),e(v,nf),e(nf,FW),e(FW,BPe),e(nf,kPe),e(nf,C8),e(C8,xPe),e(nf,RPe),e(v,SPe),e(v,lf),e(lf,CW),e(CW,PPe),e(lf,$Pe),e(lf,M8),e(M8,IPe),e(lf,jPe),e(v,NPe),e(v,df),e(df,MW),e(MW,DPe),e(df,qPe),e(df,E8),e(E8,GPe),e(df,OPe),e(v,XPe),e(v,cf),e(cf,EW),e(EW,zPe),e(cf,VPe),e(cf,y8),e(y8,WPe),e(cf,QPe),e(v,HPe),e(v,mf),e(mf,yW),e(yW,UPe),e(mf,JPe),e(mf,w8),e(w8,YPe),e(mf,KPe),e(v,ZPe),e(v,ff),e(ff,wW),e(wW,e$e),e(ff,o$e),e(ff,A8),e(A8,r$e),e(ff,t$e),e(v,a$e),e(v,gf),e(gf,AW),e(AW,s$e),e(gf,n$e),e(gf,L8),e(L8,l$e),e(gf,i$e),e(v,d$e),e(v,hf),e(hf,LW),e(LW,c$e),e(hf,m$e),e(hf,B8),e(B8,f$e),e(hf,g$e),e(v,h$e),e(v,uf),e(uf,BW),e(BW,u$e),e(uf,p$e),e(uf,k8),e(k8,_$e),e(uf,b$e),e(v,v$e),e(v,pf),e(pf,kW),e(kW,T$e),e(pf,F$e),e(pf,x8),e(x8,C$e),e(pf,M$e),e(v,E$e),e(v,_f),e(_f,xW),e(xW,y$e),e(_f,w$e),e(_f,R8),e(R8,A$e),e(_f,L$e),e(v,B$e),e(v,bf),e(bf,RW),e(RW,k$e),e(bf,x$e),e(bf,S8),e(S8,R$e),e(bf,S$e),e(v,P$e),e(v,vf),e(vf,SW),e(SW,$$e),e(vf,I$e),e(vf,P8),e(P8,j$e),e(vf,N$e),e(v,D$e),e(v,Tf),e(Tf,PW),e(PW,q$e),e(Tf,G$e),e(Tf,$8),e($8,O$e),e(Tf,X$e),e(v,z$e),e(v,Ff),e(Ff,$W),e($W,V$e),e(Ff,W$e),e(Ff,I8),e(I8,Q$e),e(Ff,H$e),e(v,U$e),e(v,Cf),e(Cf,IW),e(IW,J$e),e(Cf,Y$e),e(Cf,j8),e(j8,K$e),e(Cf,Z$e),e(v,eIe),e(v,Mf),e(Mf,jW),e(jW,oIe),e(Mf,rIe),e(Mf,N8),e(N8,tIe),e(Mf,aIe),e(v,sIe),e(v,Ef),e(Ef,NW),e(NW,nIe),e(Ef,lIe),e(Ef,D8),e(D8,iIe),e(Ef,dIe),e(v,cIe),e(v,yf),e(yf,DW),e(DW,mIe),e(yf,fIe),e(yf,q8),e(q8,gIe),e(yf,hIe),e(v,uIe),e(v,wf),e(wf,qW),e(qW,pIe),e(wf,_Ie),e(wf,G8),e(G8,bIe),e(wf,vIe),e(v,TIe),e(v,Af),e(Af,GW),e(GW,FIe),e(Af,CIe),e(Af,O8),e(O8,MIe),e(Af,EIe),e(v,yIe),e(v,Lf),e(Lf,OW),e(OW,wIe),e(Lf,AIe),e(Lf,X8),e(X8,LIe),e(Lf,BIe),e(v,kIe),e(v,Bf),e(Bf,XW),e(XW,xIe),e(Bf,RIe),e(Bf,z8),e(z8,SIe),e(Bf,PIe),e(v,$Ie),e(v,kf),e(kf,zW),e(zW,IIe),e(kf,jIe),e(kf,V8),e(V8,NIe),e(kf,DIe),e(v,qIe),e(v,xf),e(xf,VW),e(VW,GIe),e(xf,OIe),e(xf,W8),e(W8,XIe),e(xf,zIe),e(v,VIe),e(v,Rf),e(Rf,WW),e(WW,WIe),e(Rf,QIe),e(Rf,Q8),e(Q8,HIe),e(Rf,UIe),e(v,JIe),e(v,Sf),e(Sf,QW),e(QW,YIe),e(Sf,KIe),e(Sf,H8),e(H8,ZIe),e(Sf,eje),e(v,oje),e(v,Pf),e(Pf,HW),e(HW,rje),e(Pf,tje),e(Pf,U8),e(U8,aje),e(Pf,sje),e(v,nje),e(v,$f),e($f,UW),e(UW,lje),e($f,ije),e($f,J8),e(J8,dje),e($f,cje),e(v,mje),e(v,If),e(If,JW),e(JW,fje),e(If,gje),e(If,Y8),e(Y8,hje),e(If,uje),e(v,pje),e(v,jf),e(jf,YW),e(YW,_je),e(jf,bje),e(jf,K8),e(K8,vje),e(jf,Tje),e(v,Fje),e(v,Nf),e(Nf,KW),e(KW,Cje),e(Nf,Mje),e(Nf,Z8),e(Z8,Eje),e(Nf,yje),e(v,wje),e(v,Df),e(Df,ZW),e(ZW,Aje),e(Df,Lje),e(Df,e9),e(e9,Bje),e(Df,kje),e(v,xje),e(v,qf),e(qf,eQ),e(eQ,Rje),e(qf,Sje),e(qf,o9),e(o9,Pje),e(qf,$je),e(v,Ije),e(v,Gf),e(Gf,oQ),e(oQ,jje),e(Gf,Nje),e(Gf,r9),e(r9,Dje),e(Gf,qje),e(v,Gje),e(v,Of),e(Of,rQ),e(rQ,Oje),e(Of,Xje),e(Of,t9),e(t9,zje),e(Of,Vje),e(v,Wje),e(v,Xf),e(Xf,tQ),e(tQ,Qje),e(Xf,Hje),e(Xf,a9),e(a9,Uje),e(Xf,Jje),e(v,Yje),e(v,zf),e(zf,aQ),e(aQ,Kje),e(zf,Zje),e(zf,s9),e(s9,eNe),e(zf,oNe),e(v,rNe),e(v,Vf),e(Vf,sQ),e(sQ,tNe),e(Vf,aNe),e(Vf,n9),e(n9,sNe),e(Vf,nNe),e(v,lNe),e(v,Wf),e(Wf,nQ),e(nQ,iNe),e(Wf,dNe),e(Wf,l9),e(l9,cNe),e(Wf,mNe),e(v,fNe),e(v,Qf),e(Qf,lQ),e(lQ,gNe),e(Qf,hNe),e(Qf,i9),e(i9,uNe),e(Qf,pNe),e(v,_Ne),e(v,Hf),e(Hf,iQ),e(iQ,bNe),e(Hf,vNe),e(Hf,d9),e(d9,TNe),e(Hf,FNe),e(v,CNe),e(v,Uf),e(Uf,dQ),e(dQ,MNe),e(Uf,ENe),e(Uf,c9),e(c9,yNe),e(Uf,wNe),e(v,ANe),e(v,Jf),e(Jf,cQ),e(cQ,LNe),e(Jf,BNe),e(Jf,m9),e(m9,kNe),e(Jf,xNe),e(v,RNe),e(v,Yf),e(Yf,mQ),e(mQ,SNe),e(Yf,PNe),e(Yf,f9),e(f9,$Ne),e(Yf,INe),e(v,jNe),e(v,Kf),e(Kf,fQ),e(fQ,NNe),e(Kf,DNe),e(Kf,g9),e(g9,qNe),e(Kf,GNe),e(v,ONe),e(v,Zf),e(Zf,gQ),e(gQ,XNe),e(Zf,zNe),e(Zf,h9),e(h9,VNe),e(Zf,WNe),e(v,QNe),e(v,eg),e(eg,hQ),e(hQ,HNe),e(eg,UNe),e(eg,u9),e(u9,JNe),e(eg,YNe),e(v,KNe),e(v,og),e(og,uQ),e(uQ,ZNe),e(og,eDe),e(og,p9),e(p9,oDe),e(og,rDe),e(v,tDe),e(v,rg),e(rg,pQ),e(pQ,aDe),e(rg,sDe),e(rg,_9),e(_9,nDe),e(rg,lDe),e(v,iDe),e(v,tg),e(tg,_Q),e(_Q,dDe),e(tg,cDe),e(tg,b9),e(b9,mDe),e(tg,fDe),e(v,gDe),e(v,ag),e(ag,bQ),e(bQ,hDe),e(ag,uDe),e(ag,v9),e(v9,pDe),e(ag,_De),e(v,bDe),e(v,sg),e(sg,vQ),e(vQ,vDe),e(sg,TDe),e(sg,T9),e(T9,FDe),e(sg,CDe),e(v,MDe),e(v,ng),e(ng,TQ),e(TQ,EDe),e(ng,yDe),e(ng,F9),e(F9,wDe),e(ng,ADe),e(v,LDe),e(v,lg),e(lg,FQ),e(FQ,BDe),e(lg,kDe),e(lg,C9),e(C9,xDe),e(lg,RDe),e(v,SDe),e(v,ig),e(ig,CQ),e(CQ,PDe),e(ig,$De),e(ig,M9),e(M9,IDe),e(ig,jDe),e(v,NDe),e(v,dg),e(dg,MQ),e(MQ,DDe),e(dg,qDe),e(dg,E9),e(E9,GDe),e(dg,ODe),e(v,XDe),e(v,cg),e(cg,EQ),e(EQ,zDe),e(cg,VDe),e(cg,y9),e(y9,WDe),e(cg,QDe),e(v,HDe),e(v,mg),e(mg,yQ),e(yQ,UDe),e(mg,JDe),e(mg,w9),e(w9,YDe),e(mg,KDe),e(v,ZDe),e(v,fg),e(fg,wQ),e(wQ,eqe),e(fg,oqe),e(fg,A9),e(A9,rqe),e(fg,tqe),e(v,aqe),e(v,gg),e(gg,AQ),e(AQ,sqe),e(gg,nqe),e(gg,L9),e(L9,lqe),e(gg,iqe),e(mo,dqe),e(mo,LQ),e(LQ,cqe),e(mo,mqe),g(f5,mo,null),e(Go,fqe),e(Go,hg),g(g5,hg,null),e(hg,gqe),e(hg,BQ),e(BQ,hqe),b(d,c8e,_),b(d,Ii,_),e(Ii,ug),e(ug,kQ),g(h5,kQ,null),e(Ii,uqe),e(Ii,xQ),e(xQ,pqe),b(d,m8e,_),b(d,Oo,_),g(u5,Oo,null),e(Oo,_qe),e(Oo,p5),e(p5,bqe),e(p5,B9),e(B9,vqe),e(p5,Tqe),e(Oo,Fqe),e(Oo,_5),e(_5,Cqe),e(_5,RQ),e(RQ,Mqe),e(_5,Eqe),e(Oo,yqe),e(Oo,fo),g(b5,fo,null),e(fo,wqe),e(fo,SQ),e(SQ,Aqe),e(fo,Lqe),e(fo,ja),e(ja,Bqe),e(ja,PQ),e(PQ,kqe),e(ja,xqe),e(ja,$Q),e($Q,Rqe),e(ja,Sqe),e(ja,IQ),e(IQ,Pqe),e(ja,$qe),e(fo,Iqe),e(fo,M),e(M,Ns),e(Ns,jQ),e(jQ,jqe),e(Ns,Nqe),e(Ns,k9),e(k9,Dqe),e(Ns,qqe),e(Ns,x9),e(x9,Gqe),e(Ns,Oqe),e(M,Xqe),e(M,Ds),e(Ds,NQ),e(NQ,zqe),e(Ds,Vqe),e(Ds,R9),e(R9,Wqe),e(Ds,Qqe),e(Ds,S9),e(S9,Hqe),e(Ds,Uqe),e(M,Jqe),e(M,qs),e(qs,DQ),e(DQ,Yqe),e(qs,Kqe),e(qs,P9),e(P9,Zqe),e(qs,eGe),e(qs,$9),e($9,oGe),e(qs,rGe),e(M,tGe),e(M,pg),e(pg,qQ),e(qQ,aGe),e(pg,sGe),e(pg,I9),e(I9,nGe),e(pg,lGe),e(M,iGe),e(M,Gs),e(Gs,GQ),e(GQ,dGe),e(Gs,cGe),e(Gs,j9),e(j9,mGe),e(Gs,fGe),e(Gs,N9),e(N9,gGe),e(Gs,hGe),e(M,uGe),e(M,_g),e(_g,OQ),e(OQ,pGe),e(_g,_Ge),e(_g,D9),e(D9,bGe),e(_g,vGe),e(M,TGe),e(M,bg),e(bg,XQ),e(XQ,FGe),e(bg,CGe),e(bg,q9),e(q9,MGe),e(bg,EGe),e(M,yGe),e(M,vg),e(vg,zQ),e(zQ,wGe),e(vg,AGe),e(vg,G9),e(G9,LGe),e(vg,BGe),e(M,kGe),e(M,Os),e(Os,VQ),e(VQ,xGe),e(Os,RGe),e(Os,O9),e(O9,SGe),e(Os,PGe),e(Os,X9),e(X9,$Ge),e(Os,IGe),e(M,jGe),e(M,Xs),e(Xs,WQ),e(WQ,NGe),e(Xs,DGe),e(Xs,z9),e(z9,qGe),e(Xs,GGe),e(Xs,V9),e(V9,OGe),e(Xs,XGe),e(M,zGe),e(M,zs),e(zs,QQ),e(QQ,VGe),e(zs,WGe),e(zs,W9),e(W9,QGe),e(zs,HGe),e(zs,Q9),e(Q9,UGe),e(zs,JGe),e(M,YGe),e(M,Tg),e(Tg,HQ),e(HQ,KGe),e(Tg,ZGe),e(Tg,H9),e(H9,eOe),e(Tg,oOe),e(M,rOe),e(M,Fg),e(Fg,UQ),e(UQ,tOe),e(Fg,aOe),e(Fg,U9),e(U9,sOe),e(Fg,nOe),e(M,lOe),e(M,Vs),e(Vs,JQ),e(JQ,iOe),e(Vs,dOe),e(Vs,J9),e(J9,cOe),e(Vs,mOe),e(Vs,Y9),e(Y9,fOe),e(Vs,gOe),e(M,hOe),e(M,Cg),e(Cg,YQ),e(YQ,uOe),e(Cg,pOe),e(Cg,K9),e(K9,_Oe),e(Cg,bOe),e(M,vOe),e(M,Ws),e(Ws,KQ),e(KQ,TOe),e(Ws,FOe),e(Ws,Z9),e(Z9,COe),e(Ws,MOe),e(Ws,eB),e(eB,EOe),e(Ws,yOe),e(M,wOe),e(M,Qs),e(Qs,ZQ),e(ZQ,AOe),e(Qs,LOe),e(Qs,oB),e(oB,BOe),e(Qs,kOe),e(Qs,rB),e(rB,xOe),e(Qs,ROe),e(M,SOe),e(M,Hs),e(Hs,eH),e(eH,POe),e(Hs,$Oe),e(Hs,tB),e(tB,IOe),e(Hs,jOe),e(Hs,oH),e(oH,NOe),e(Hs,DOe),e(M,qOe),e(M,Mg),e(Mg,rH),e(rH,GOe),e(Mg,OOe),e(Mg,aB),e(aB,XOe),e(Mg,zOe),e(M,VOe),e(M,Us),e(Us,tH),e(tH,WOe),e(Us,QOe),e(Us,sB),e(sB,HOe),e(Us,UOe),e(Us,nB),e(nB,JOe),e(Us,YOe),e(M,KOe),e(M,Eg),e(Eg,aH),e(aH,ZOe),e(Eg,eXe),e(Eg,lB),e(lB,oXe),e(Eg,rXe),e(M,tXe),e(M,Js),e(Js,sH),e(sH,aXe),e(Js,sXe),e(Js,iB),e(iB,nXe),e(Js,lXe),e(Js,dB),e(dB,iXe),e(Js,dXe),e(M,cXe),e(M,Ys),e(Ys,nH),e(nH,mXe),e(Ys,fXe),e(Ys,cB),e(cB,gXe),e(Ys,hXe),e(Ys,mB),e(mB,uXe),e(Ys,pXe),e(M,_Xe),e(M,Ks),e(Ks,lH),e(lH,bXe),e(Ks,vXe),e(Ks,fB),e(fB,TXe),e(Ks,FXe),e(Ks,gB),e(gB,CXe),e(Ks,MXe),e(M,EXe),e(M,yg),e(yg,iH),e(iH,yXe),e(yg,wXe),e(yg,hB),e(hB,AXe),e(yg,LXe),e(M,BXe),e(M,Zs),e(Zs,dH),e(dH,kXe),e(Zs,xXe),e(Zs,uB),e(uB,RXe),e(Zs,SXe),e(Zs,pB),e(pB,PXe),e(Zs,$Xe),e(M,IXe),e(M,wg),e(wg,cH),e(cH,jXe),e(wg,NXe),e(wg,_B),e(_B,DXe),e(wg,qXe),e(M,GXe),e(M,en),e(en,mH),e(mH,OXe),e(en,XXe),e(en,bB),e(bB,zXe),e(en,VXe),e(en,vB),e(vB,WXe),e(en,QXe),e(M,HXe),e(M,on),e(on,fH),e(fH,UXe),e(on,JXe),e(on,TB),e(TB,YXe),e(on,KXe),e(on,FB),e(FB,ZXe),e(on,eze),e(M,oze),e(M,rn),e(rn,gH),e(gH,rze),e(rn,tze),e(rn,CB),e(CB,aze),e(rn,sze),e(rn,MB),e(MB,nze),e(rn,lze),e(M,ize),e(M,tn),e(tn,hH),e(hH,dze),e(tn,cze),e(tn,EB),e(EB,mze),e(tn,fze),e(tn,yB),e(yB,gze),e(tn,hze),e(M,uze),e(M,Ag),e(Ag,uH),e(uH,pze),e(Ag,_ze),e(Ag,wB),e(wB,bze),e(Ag,vze),e(M,Tze),e(M,an),e(an,pH),e(pH,Fze),e(an,Cze),e(an,AB),e(AB,Mze),e(an,Eze),e(an,LB),e(LB,yze),e(an,wze),e(M,Aze),e(M,sn),e(sn,_H),e(_H,Lze),e(sn,Bze),e(sn,BB),e(BB,kze),e(sn,xze),e(sn,kB),e(kB,Rze),e(sn,Sze),e(M,Pze),e(M,nn),e(nn,bH),e(bH,$ze),e(nn,Ize),e(nn,xB),e(xB,jze),e(nn,Nze),e(nn,RB),e(RB,Dze),e(nn,qze),e(M,Gze),e(M,ln),e(ln,vH),e(vH,Oze),e(ln,Xze),e(ln,SB),e(SB,zze),e(ln,Vze),e(ln,PB),e(PB,Wze),e(ln,Qze),e(M,Hze),e(M,dn),e(dn,TH),e(TH,Uze),e(dn,Jze),e(dn,$B),e($B,Yze),e(dn,Kze),e(dn,IB),e(IB,Zze),e(dn,eVe),e(M,oVe),e(M,cn),e(cn,FH),e(FH,rVe),e(cn,tVe),e(cn,jB),e(jB,aVe),e(cn,sVe),e(cn,NB),e(NB,nVe),e(cn,lVe),e(M,iVe),e(M,Lg),e(Lg,CH),e(CH,dVe),e(Lg,cVe),e(Lg,DB),e(DB,mVe),e(Lg,fVe),e(M,gVe),e(M,mn),e(mn,MH),e(MH,hVe),e(mn,uVe),e(mn,qB),e(qB,pVe),e(mn,_Ve),e(mn,GB),e(GB,bVe),e(mn,vVe),e(M,TVe),e(M,Bg),e(Bg,EH),e(EH,FVe),e(Bg,CVe),e(Bg,OB),e(OB,MVe),e(Bg,EVe),e(M,yVe),e(M,kg),e(kg,yH),e(yH,wVe),e(kg,AVe),e(kg,XB),e(XB,LVe),e(kg,BVe),e(M,kVe),e(M,fn),e(fn,wH),e(wH,xVe),e(fn,RVe),e(fn,zB),e(zB,SVe),e(fn,PVe),e(fn,VB),e(VB,$Ve),e(fn,IVe),e(M,jVe),e(M,gn),e(gn,AH),e(AH,NVe),e(gn,DVe),e(gn,WB),e(WB,qVe),e(gn,GVe),e(gn,QB),e(QB,OVe),e(gn,XVe),e(M,zVe),e(M,xg),e(xg,LH),e(LH,VVe),e(xg,WVe),e(xg,HB),e(HB,QVe),e(xg,HVe),e(M,UVe),e(M,hn),e(hn,BH),e(BH,JVe),e(hn,YVe),e(hn,UB),e(UB,KVe),e(hn,ZVe),e(hn,JB),e(JB,eWe),e(hn,oWe),e(M,rWe),e(M,un),e(un,kH),e(kH,tWe),e(un,aWe),e(un,YB),e(YB,sWe),e(un,nWe),e(un,KB),e(KB,lWe),e(un,iWe),e(M,dWe),e(M,pn),e(pn,xH),e(xH,cWe),e(pn,mWe),e(pn,ZB),e(ZB,fWe),e(pn,gWe),e(pn,ek),e(ek,hWe),e(pn,uWe),e(M,pWe),e(M,_n),e(_n,RH),e(RH,_We),e(_n,bWe),e(_n,ok),e(ok,vWe),e(_n,TWe),e(_n,rk),e(rk,FWe),e(_n,CWe),e(M,MWe),e(M,bn),e(bn,SH),e(SH,EWe),e(bn,yWe),e(bn,tk),e(tk,wWe),e(bn,AWe),e(bn,ak),e(ak,LWe),e(bn,BWe),e(M,kWe),e(M,Rg),e(Rg,PH),e(PH,xWe),e(Rg,RWe),e(Rg,sk),e(sk,SWe),e(Rg,PWe),e(M,$We),e(M,Sg),e(Sg,$H),e($H,IWe),e(Sg,jWe),e(Sg,nk),e(nk,NWe),e(Sg,DWe),e(M,qWe),e(M,Pg),e(Pg,IH),e(IH,GWe),e(Pg,OWe),e(Pg,lk),e(lk,XWe),e(Pg,zWe),e(M,VWe),e(M,$g),e($g,jH),e(jH,WWe),e($g,QWe),e($g,ik),e(ik,HWe),e($g,UWe),e(M,JWe),e(M,vn),e(vn,NH),e(NH,YWe),e(vn,KWe),e(vn,dk),e(dk,ZWe),e(vn,eQe),e(vn,ck),e(ck,oQe),e(vn,rQe),e(M,tQe),e(M,Ig),e(Ig,DH),e(DH,aQe),e(Ig,sQe),e(Ig,mk),e(mk,nQe),e(Ig,lQe),e(M,iQe),e(M,Tn),e(Tn,qH),e(qH,dQe),e(Tn,cQe),e(Tn,fk),e(fk,mQe),e(Tn,fQe),e(Tn,gk),e(gk,gQe),e(Tn,hQe),e(M,uQe),e(M,Fn),e(Fn,GH),e(GH,pQe),e(Fn,_Qe),e(Fn,hk),e(hk,bQe),e(Fn,vQe),e(Fn,uk),e(uk,TQe),e(Fn,FQe),e(M,CQe),e(M,Cn),e(Cn,OH),e(OH,MQe),e(Cn,EQe),e(Cn,pk),e(pk,yQe),e(Cn,wQe),e(Cn,_k),e(_k,AQe),e(Cn,LQe),e(M,BQe),e(M,Mn),e(Mn,XH),e(XH,kQe),e(Mn,xQe),e(Mn,bk),e(bk,RQe),e(Mn,SQe),e(Mn,vk),e(vk,PQe),e(Mn,$Qe),e(M,IQe),e(M,En),e(En,zH),e(zH,jQe),e(En,NQe),e(En,Tk),e(Tk,DQe),e(En,qQe),e(En,Fk),e(Fk,GQe),e(En,OQe),e(M,XQe),e(M,jg),e(jg,VH),e(VH,zQe),e(jg,VQe),e(jg,Ck),e(Ck,WQe),e(jg,QQe),e(M,HQe),e(M,Ng),e(Ng,WH),e(WH,UQe),e(Ng,JQe),e(Ng,Mk),e(Mk,YQe),e(Ng,KQe),e(M,ZQe),e(M,yn),e(yn,QH),e(QH,eHe),e(yn,oHe),e(yn,Ek),e(Ek,rHe),e(yn,tHe),e(yn,yk),e(yk,aHe),e(yn,sHe),e(M,nHe),e(M,wn),e(wn,HH),e(HH,lHe),e(wn,iHe),e(wn,wk),e(wk,dHe),e(wn,cHe),e(wn,Ak),e(Ak,mHe),e(wn,fHe),e(M,gHe),e(M,An),e(An,UH),e(UH,hHe),e(An,uHe),e(An,Lk),e(Lk,pHe),e(An,_He),e(An,Bk),e(Bk,bHe),e(An,vHe),e(M,THe),e(M,Dg),e(Dg,JH),e(JH,FHe),e(Dg,CHe),e(Dg,kk),e(kk,MHe),e(Dg,EHe),e(M,yHe),e(M,qg),e(qg,YH),e(YH,wHe),e(qg,AHe),e(qg,xk),e(xk,LHe),e(qg,BHe),e(M,kHe),e(M,Gg),e(Gg,KH),e(KH,xHe),e(Gg,RHe),e(Gg,Rk),e(Rk,SHe),e(Gg,PHe),e(M,$He),e(M,Og),e(Og,ZH),e(ZH,IHe),e(Og,jHe),e(Og,Sk),e(Sk,NHe),e(Og,DHe),e(M,qHe),e(M,Ln),e(Ln,eU),e(eU,GHe),e(Ln,OHe),e(Ln,Pk),e(Pk,XHe),e(Ln,zHe),e(Ln,$k),e($k,VHe),e(Ln,WHe),e(M,QHe),e(M,Xg),e(Xg,oU),e(oU,HHe),e(Xg,UHe),e(Xg,Ik),e(Ik,JHe),e(Xg,YHe),e(M,KHe),e(M,zg),e(zg,rU),e(rU,ZHe),e(zg,eUe),e(zg,jk),e(jk,oUe),e(zg,rUe),e(M,tUe),e(M,Bn),e(Bn,tU),e(tU,aUe),e(Bn,sUe),e(Bn,Nk),e(Nk,nUe),e(Bn,lUe),e(Bn,Dk),e(Dk,iUe),e(Bn,dUe),e(M,cUe),e(M,kn),e(kn,aU),e(aU,mUe),e(kn,fUe),e(kn,qk),e(qk,gUe),e(kn,hUe),e(kn,Gk),e(Gk,uUe),e(kn,pUe),e(fo,_Ue),e(fo,sU),e(sU,bUe),e(fo,vUe),g(v5,fo,null),e(Oo,TUe),e(Oo,Vg),g(T5,Vg,null),e(Vg,FUe),e(Vg,nU),e(nU,CUe),b(d,f8e,_),b(d,ji,_),e(ji,Wg),e(Wg,lU),g(F5,lU,null),e(ji,MUe),e(ji,iU),e(iU,EUe),b(d,g8e,_),b(d,Xo,_),g(C5,Xo,null),e(Xo,yUe),e(Xo,M5),e(M5,wUe),e(M5,Ok),e(Ok,AUe),e(M5,LUe),e(Xo,BUe),e(Xo,E5),e(E5,kUe),e(E5,dU),e(dU,xUe),e(E5,RUe),e(Xo,SUe),e(Xo,Le),g(y5,Le,null),e(Le,PUe),e(Le,cU),e(cU,$Ue),e(Le,IUe),e(Le,Na),e(Na,jUe),e(Na,mU),e(mU,NUe),e(Na,DUe),e(Na,fU),e(fU,qUe),e(Na,GUe),e(Na,gU),e(gU,OUe),e(Na,XUe),e(Le,zUe),e(Le,ne),e(ne,Qg),e(Qg,hU),e(hU,VUe),e(Qg,WUe),e(Qg,Xk),e(Xk,QUe),e(Qg,HUe),e(ne,UUe),e(ne,Hg),e(Hg,uU),e(uU,JUe),e(Hg,YUe),e(Hg,zk),e(zk,KUe),e(Hg,ZUe),e(ne,eJe),e(ne,Ug),e(Ug,pU),e(pU,oJe),e(Ug,rJe),e(Ug,Vk),e(Vk,tJe),e(Ug,aJe),e(ne,sJe),e(ne,Jg),e(Jg,_U),e(_U,nJe),e(Jg,lJe),e(Jg,Wk),e(Wk,iJe),e(Jg,dJe),e(ne,cJe),e(ne,Yg),e(Yg,bU),e(bU,mJe),e(Yg,fJe),e(Yg,Qk),e(Qk,gJe),e(Yg,hJe),e(ne,uJe),e(ne,Kg),e(Kg,vU),e(vU,pJe),e(Kg,_Je),e(Kg,Hk),e(Hk,bJe),e(Kg,vJe),e(ne,TJe),e(ne,Zg),e(Zg,TU),e(TU,FJe),e(Zg,CJe),e(Zg,Uk),e(Uk,MJe),e(Zg,EJe),e(ne,yJe),e(ne,eh),e(eh,FU),e(FU,wJe),e(eh,AJe),e(eh,Jk),e(Jk,LJe),e(eh,BJe),e(ne,kJe),e(ne,oh),e(oh,CU),e(CU,xJe),e(oh,RJe),e(oh,Yk),e(Yk,SJe),e(oh,PJe),e(ne,$Je),e(ne,rh),e(rh,MU),e(MU,IJe),e(rh,jJe),e(rh,Kk),e(Kk,NJe),e(rh,DJe),e(ne,qJe),e(ne,th),e(th,EU),e(EU,GJe),e(th,OJe),e(th,Zk),e(Zk,XJe),e(th,zJe),e(ne,VJe),e(ne,ah),e(ah,yU),e(yU,WJe),e(ah,QJe),e(ah,ex),e(ex,HJe),e(ah,UJe),e(ne,JJe),e(ne,sh),e(sh,wU),e(wU,YJe),e(sh,KJe),e(sh,ox),e(ox,ZJe),e(sh,eYe),e(ne,oYe),e(ne,nh),e(nh,AU),e(AU,rYe),e(nh,tYe),e(nh,rx),e(rx,aYe),e(nh,sYe),e(ne,nYe),e(ne,lh),e(lh,LU),e(LU,lYe),e(lh,iYe),e(lh,tx),e(tx,dYe),e(lh,cYe),e(Le,mYe),g(ih,Le,null),e(Le,fYe),e(Le,BU),e(BU,gYe),e(Le,hYe),g(w5,Le,null),e(Xo,uYe),e(Xo,dh),g(A5,dh,null),e(dh,pYe),e(dh,kU),e(kU,_Ye),b(d,h8e,_),b(d,Ni,_),e(Ni,ch),e(ch,xU),g(L5,xU,null),e(Ni,bYe),e(Ni,RU),e(RU,vYe),b(d,u8e,_),b(d,zo,_),g(B5,zo,null),e(zo,TYe),e(zo,k5),e(k5,FYe),e(k5,ax),e(ax,CYe),e(k5,MYe),e(zo,EYe),e(zo,x5),e(x5,yYe),e(x5,SU),e(SU,wYe),e(x5,AYe),e(zo,LYe),e(zo,Be),g(R5,Be,null),e(Be,BYe),e(Be,PU),e(PU,kYe),e(Be,xYe),e(Be,Di),e(Di,RYe),e(Di,$U),e($U,SYe),e(Di,PYe),e(Di,IU),e(IU,$Ye),e(Di,IYe),e(Be,jYe),e(Be,we),e(we,mh),e(mh,jU),e(jU,NYe),e(mh,DYe),e(mh,sx),e(sx,qYe),e(mh,GYe),e(we,OYe),e(we,fh),e(fh,NU),e(NU,XYe),e(fh,zYe),e(fh,nx),e(nx,VYe),e(fh,WYe),e(we,QYe),e(we,gh),e(gh,DU),e(DU,HYe),e(gh,UYe),e(gh,lx),e(lx,JYe),e(gh,YYe),e(we,KYe),e(we,hh),e(hh,qU),e(qU,ZYe),e(hh,eKe),e(hh,ix),e(ix,oKe),e(hh,rKe),e(we,tKe),e(we,uh),e(uh,GU),e(GU,aKe),e(uh,sKe),e(uh,dx),e(dx,nKe),e(uh,lKe),e(we,iKe),e(we,ph),e(ph,OU),e(OU,dKe),e(ph,cKe),e(ph,cx),e(cx,mKe),e(ph,fKe),e(we,gKe),e(we,_h),e(_h,XU),e(XU,hKe),e(_h,uKe),e(_h,mx),e(mx,pKe),e(_h,_Ke),e(we,bKe),e(we,bh),e(bh,zU),e(zU,vKe),e(bh,TKe),e(bh,fx),e(fx,FKe),e(bh,CKe),e(Be,MKe),g(vh,Be,null),e(Be,EKe),e(Be,VU),e(VU,yKe),e(Be,wKe),g(S5,Be,null),e(zo,AKe),e(zo,Th),g(P5,Th,null),e(Th,LKe),e(Th,WU),e(WU,BKe),b(d,p8e,_),b(d,qi,_),e(qi,Fh),e(Fh,QU),g($5,QU,null),e(qi,kKe),e(qi,HU),e(HU,xKe),b(d,_8e,_),b(d,Vo,_),g(I5,Vo,null),e(Vo,RKe),e(Vo,Gi),e(Gi,SKe),e(Gi,UU),e(UU,PKe),e(Gi,$Ke),e(Gi,JU),e(JU,IKe),e(Gi,jKe),e(Vo,NKe),e(Vo,j5),e(j5,DKe),e(j5,YU),e(YU,qKe),e(j5,GKe),e(Vo,OKe),e(Vo,Nr),g(N5,Nr,null),e(Nr,XKe),e(Nr,KU),e(KU,zKe),e(Nr,VKe),e(Nr,Oi),e(Oi,WKe),e(Oi,ZU),e(ZU,QKe),e(Oi,HKe),e(Oi,eJ),e(eJ,UKe),e(Oi,JKe),e(Nr,YKe),e(Nr,oJ),e(oJ,KKe),e(Nr,ZKe),g(D5,Nr,null),e(Vo,eZe),e(Vo,ke),g(q5,ke,null),e(ke,oZe),e(ke,rJ),e(rJ,rZe),e(ke,tZe),e(ke,Da),e(Da,aZe),e(Da,tJ),e(tJ,sZe),e(Da,nZe),e(Da,aJ),e(aJ,lZe),e(Da,iZe),e(Da,sJ),e(sJ,dZe),e(Da,cZe),e(ke,mZe),e(ke,F),e(F,Ch),e(Ch,nJ),e(nJ,fZe),e(Ch,gZe),e(Ch,gx),e(gx,hZe),e(Ch,uZe),e(F,pZe),e(F,Mh),e(Mh,lJ),e(lJ,_Ze),e(Mh,bZe),e(Mh,hx),e(hx,vZe),e(Mh,TZe),e(F,FZe),e(F,Eh),e(Eh,iJ),e(iJ,CZe),e(Eh,MZe),e(Eh,ux),e(ux,EZe),e(Eh,yZe),e(F,wZe),e(F,yh),e(yh,dJ),e(dJ,AZe),e(yh,LZe),e(yh,px),e(px,BZe),e(yh,kZe),e(F,xZe),e(F,wh),e(wh,cJ),e(cJ,RZe),e(wh,SZe),e(wh,_x),e(_x,PZe),e(wh,$Ze),e(F,IZe),e(F,Ah),e(Ah,mJ),e(mJ,jZe),e(Ah,NZe),e(Ah,bx),e(bx,DZe),e(Ah,qZe),e(F,GZe),e(F,Lh),e(Lh,fJ),e(fJ,OZe),e(Lh,XZe),e(Lh,vx),e(vx,zZe),e(Lh,VZe),e(F,WZe),e(F,Bh),e(Bh,gJ),e(gJ,QZe),e(Bh,HZe),e(Bh,Tx),e(Tx,UZe),e(Bh,JZe),e(F,YZe),e(F,kh),e(kh,hJ),e(hJ,KZe),e(kh,ZZe),e(kh,Fx),e(Fx,eeo),e(kh,oeo),e(F,reo),e(F,xh),e(xh,uJ),e(uJ,teo),e(xh,aeo),e(xh,Cx),e(Cx,seo),e(xh,neo),e(F,leo),e(F,Rh),e(Rh,pJ),e(pJ,ieo),e(Rh,deo),e(Rh,Mx),e(Mx,ceo),e(Rh,meo),e(F,feo),e(F,Sh),e(Sh,_J),e(_J,geo),e(Sh,heo),e(Sh,Ex),e(Ex,ueo),e(Sh,peo),e(F,_eo),e(F,Ph),e(Ph,bJ),e(bJ,beo),e(Ph,veo),e(Ph,yx),e(yx,Teo),e(Ph,Feo),e(F,Ceo),e(F,$h),e($h,vJ),e(vJ,Meo),e($h,Eeo),e($h,wx),e(wx,yeo),e($h,weo),e(F,Aeo),e(F,Ih),e(Ih,TJ),e(TJ,Leo),e(Ih,Beo),e(Ih,Ax),e(Ax,keo),e(Ih,xeo),e(F,Reo),e(F,jh),e(jh,FJ),e(FJ,Seo),e(jh,Peo),e(jh,Lx),e(Lx,$eo),e(jh,Ieo),e(F,jeo),e(F,Nh),e(Nh,CJ),e(CJ,Neo),e(Nh,Deo),e(Nh,Bx),e(Bx,qeo),e(Nh,Geo),e(F,Oeo),e(F,Dh),e(Dh,MJ),e(MJ,Xeo),e(Dh,zeo),e(Dh,kx),e(kx,Veo),e(Dh,Weo),e(F,Qeo),e(F,qh),e(qh,EJ),e(EJ,Heo),e(qh,Ueo),e(qh,xx),e(xx,Jeo),e(qh,Yeo),e(F,Keo),e(F,Gh),e(Gh,yJ),e(yJ,Zeo),e(Gh,eoo),e(Gh,Rx),e(Rx,ooo),e(Gh,roo),e(F,too),e(F,Oh),e(Oh,wJ),e(wJ,aoo),e(Oh,soo),e(Oh,Sx),e(Sx,noo),e(Oh,loo),e(F,ioo),e(F,Xh),e(Xh,AJ),e(AJ,doo),e(Xh,coo),e(Xh,Px),e(Px,moo),e(Xh,foo),e(F,goo),e(F,zh),e(zh,LJ),e(LJ,hoo),e(zh,uoo),e(zh,$x),e($x,poo),e(zh,_oo),e(F,boo),e(F,Vh),e(Vh,BJ),e(BJ,voo),e(Vh,Too),e(Vh,Ix),e(Ix,Foo),e(Vh,Coo),e(F,Moo),e(F,Wh),e(Wh,kJ),e(kJ,Eoo),e(Wh,yoo),e(Wh,jx),e(jx,woo),e(Wh,Aoo),e(F,Loo),e(F,xn),e(xn,xJ),e(xJ,Boo),e(xn,koo),e(xn,Nx),e(Nx,xoo),e(xn,Roo),e(xn,Dx),e(Dx,Soo),e(xn,Poo),e(F,$oo),e(F,Qh),e(Qh,RJ),e(RJ,Ioo),e(Qh,joo),e(Qh,qx),e(qx,Noo),e(Qh,Doo),e(F,qoo),e(F,Hh),e(Hh,SJ),e(SJ,Goo),e(Hh,Ooo),e(Hh,Gx),e(Gx,Xoo),e(Hh,zoo),e(F,Voo),e(F,Uh),e(Uh,PJ),e(PJ,Woo),e(Uh,Qoo),e(Uh,Ox),e(Ox,Hoo),e(Uh,Uoo),e(F,Joo),e(F,Jh),e(Jh,$J),e($J,Yoo),e(Jh,Koo),e(Jh,Xx),e(Xx,Zoo),e(Jh,ero),e(F,oro),e(F,Yh),e(Yh,IJ),e(IJ,rro),e(Yh,tro),e(Yh,zx),e(zx,aro),e(Yh,sro),e(F,nro),e(F,Kh),e(Kh,jJ),e(jJ,lro),e(Kh,iro),e(Kh,Vx),e(Vx,dro),e(Kh,cro),e(F,mro),e(F,Zh),e(Zh,NJ),e(NJ,fro),e(Zh,gro),e(Zh,Wx),e(Wx,hro),e(Zh,uro),e(F,pro),e(F,eu),e(eu,DJ),e(DJ,_ro),e(eu,bro),e(eu,Qx),e(Qx,vro),e(eu,Tro),e(F,Fro),e(F,ou),e(ou,qJ),e(qJ,Cro),e(ou,Mro),e(ou,Hx),e(Hx,Ero),e(ou,yro),e(F,wro),e(F,ru),e(ru,GJ),e(GJ,Aro),e(ru,Lro),e(ru,Ux),e(Ux,Bro),e(ru,kro),e(F,xro),e(F,tu),e(tu,OJ),e(OJ,Rro),e(tu,Sro),e(tu,Jx),e(Jx,Pro),e(tu,$ro),e(F,Iro),e(F,au),e(au,XJ),e(XJ,jro),e(au,Nro),e(au,Yx),e(Yx,Dro),e(au,qro),e(F,Gro),e(F,su),e(su,zJ),e(zJ,Oro),e(su,Xro),e(su,Kx),e(Kx,zro),e(su,Vro),e(F,Wro),e(F,nu),e(nu,VJ),e(VJ,Qro),e(nu,Hro),e(nu,Zx),e(Zx,Uro),e(nu,Jro),e(F,Yro),e(F,lu),e(lu,WJ),e(WJ,Kro),e(lu,Zro),e(lu,eR),e(eR,eto),e(lu,oto),e(F,rto),e(F,iu),e(iu,QJ),e(QJ,tto),e(iu,ato),e(iu,oR),e(oR,sto),e(iu,nto),e(F,lto),e(F,du),e(du,HJ),e(HJ,ito),e(du,dto),e(du,rR),e(rR,cto),e(du,mto),e(F,fto),e(F,cu),e(cu,UJ),e(UJ,gto),e(cu,hto),e(cu,tR),e(tR,uto),e(cu,pto),e(F,_to),e(F,mu),e(mu,JJ),e(JJ,bto),e(mu,vto),e(mu,aR),e(aR,Tto),e(mu,Fto),e(F,Cto),e(F,fu),e(fu,YJ),e(YJ,Mto),e(fu,Eto),e(fu,sR),e(sR,yto),e(fu,wto),e(F,Ato),e(F,gu),e(gu,KJ),e(KJ,Lto),e(gu,Bto),e(gu,nR),e(nR,kto),e(gu,xto),e(F,Rto),e(F,hu),e(hu,ZJ),e(ZJ,Sto),e(hu,Pto),e(hu,lR),e(lR,$to),e(hu,Ito),e(F,jto),e(F,uu),e(uu,eY),e(eY,Nto),e(uu,Dto),e(uu,iR),e(iR,qto),e(uu,Gto),e(F,Oto),e(F,pu),e(pu,oY),e(oY,Xto),e(pu,zto),e(pu,dR),e(dR,Vto),e(pu,Wto),e(F,Qto),e(F,_u),e(_u,rY),e(rY,Hto),e(_u,Uto),e(_u,cR),e(cR,Jto),e(_u,Yto),e(F,Kto),e(F,bu),e(bu,tY),e(tY,Zto),e(bu,eao),e(bu,mR),e(mR,oao),e(bu,rao),e(F,tao),e(F,vu),e(vu,aY),e(aY,aao),e(vu,sao),e(vu,fR),e(fR,nao),e(vu,lao),e(F,iao),e(F,Tu),e(Tu,sY),e(sY,dao),e(Tu,cao),e(Tu,gR),e(gR,mao),e(Tu,fao),e(F,gao),e(F,Fu),e(Fu,nY),e(nY,hao),e(Fu,uao),e(Fu,hR),e(hR,pao),e(Fu,_ao),e(F,bao),e(F,Cu),e(Cu,lY),e(lY,vao),e(Cu,Tao),e(Cu,uR),e(uR,Fao),e(Cu,Cao),e(F,Mao),e(F,Mu),e(Mu,iY),e(iY,Eao),e(Mu,yao),e(Mu,pR),e(pR,wao),e(Mu,Aao),e(F,Lao),e(F,Eu),e(Eu,dY),e(dY,Bao),e(Eu,kao),e(Eu,_R),e(_R,xao),e(Eu,Rao),e(F,Sao),e(F,yu),e(yu,cY),e(cY,Pao),e(yu,$ao),e(yu,bR),e(bR,Iao),e(yu,jao),e(F,Nao),e(F,wu),e(wu,mY),e(mY,Dao),e(wu,qao),e(wu,vR),e(vR,Gao),e(wu,Oao),e(F,Xao),e(F,Au),e(Au,fY),e(fY,zao),e(Au,Vao),e(Au,TR),e(TR,Wao),e(Au,Qao),e(F,Hao),e(F,Lu),e(Lu,gY),e(gY,Uao),e(Lu,Jao),e(Lu,FR),e(FR,Yao),e(Lu,Kao),e(F,Zao),e(F,Bu),e(Bu,hY),e(hY,eso),e(Bu,oso),e(Bu,CR),e(CR,rso),e(Bu,tso),e(F,aso),e(F,ku),e(ku,uY),e(uY,sso),e(ku,nso),e(ku,MR),e(MR,lso),e(ku,iso),e(F,dso),e(F,xu),e(xu,pY),e(pY,cso),e(xu,mso),e(xu,ER),e(ER,fso),e(xu,gso),e(F,hso),e(F,Ru),e(Ru,_Y),e(_Y,uso),e(Ru,pso),e(Ru,yR),e(yR,_so),e(Ru,bso),e(F,vso),e(F,Su),e(Su,bY),e(bY,Tso),e(Su,Fso),e(Su,wR),e(wR,Cso),e(Su,Mso),e(F,Eso),e(F,Pu),e(Pu,vY),e(vY,yso),e(Pu,wso),e(Pu,AR),e(AR,Aso),e(Pu,Lso),e(F,Bso),e(F,$u),e($u,TY),e(TY,kso),e($u,xso),e($u,LR),e(LR,Rso),e($u,Sso),e(F,Pso),e(F,Iu),e(Iu,FY),e(FY,$so),e(Iu,Iso),e(Iu,BR),e(BR,jso),e(Iu,Nso),e(F,Dso),e(F,ju),e(ju,CY),e(CY,qso),e(ju,Gso),e(ju,kR),e(kR,Oso),e(ju,Xso),e(F,zso),e(F,Nu),e(Nu,MY),e(MY,Vso),e(Nu,Wso),e(Nu,xR),e(xR,Qso),e(Nu,Hso),e(F,Uso),e(F,Du),e(Du,EY),e(EY,Jso),e(Du,Yso),e(Du,RR),e(RR,Kso),e(Du,Zso),e(F,eno),e(F,qu),e(qu,yY),e(yY,ono),e(qu,rno),e(qu,SR),e(SR,tno),e(qu,ano),e(F,sno),e(F,Gu),e(Gu,wY),e(wY,nno),e(Gu,lno),e(Gu,PR),e(PR,ino),e(Gu,dno),e(F,cno),e(F,Ou),e(Ou,AY),e(AY,mno),e(Ou,fno),e(Ou,$R),e($R,gno),e(Ou,hno),e(F,uno),e(F,Xu),e(Xu,LY),e(LY,pno),e(Xu,_no),e(Xu,IR),e(IR,bno),e(Xu,vno),e(F,Tno),e(F,zu),e(zu,BY),e(BY,Fno),e(zu,Cno),e(zu,jR),e(jR,Mno),e(zu,Eno),e(F,yno),e(F,Vu),e(Vu,kY),e(kY,wno),e(Vu,Ano),e(Vu,NR),e(NR,Lno),e(Vu,Bno),e(F,kno),e(F,Wu),e(Wu,xY),e(xY,xno),e(Wu,Rno),e(Wu,DR),e(DR,Sno),e(Wu,Pno),e(F,$no),e(F,Qu),e(Qu,RY),e(RY,Ino),e(Qu,jno),e(Qu,qR),e(qR,Nno),e(Qu,Dno),e(F,qno),e(F,Hu),e(Hu,SY),e(SY,Gno),e(Hu,Ono),e(Hu,GR),e(GR,Xno),e(Hu,zno),e(F,Vno),e(F,Uu),e(Uu,PY),e(PY,Wno),e(Uu,Qno),e(Uu,OR),e(OR,Hno),e(Uu,Uno),e(F,Jno),e(F,Ju),e(Ju,$Y),e($Y,Yno),e(Ju,Kno),e(Ju,XR),e(XR,Zno),e(Ju,elo),e(ke,olo),e(ke,Yu),e(Yu,rlo),e(Yu,IY),e(IY,tlo),e(Yu,alo),e(Yu,jY),e(jY,slo),e(ke,nlo),e(ke,NY),e(NY,llo),e(ke,ilo),g(G5,ke,null),b(d,b8e,_),b(d,Xi,_),e(Xi,Ku),e(Ku,DY),g(O5,DY,null),e(Xi,dlo),e(Xi,qY),e(qY,clo),b(d,v8e,_),b(d,Wo,_),g(X5,Wo,null),e(Wo,mlo),e(Wo,zi),e(zi,flo),e(zi,GY),e(GY,glo),e(zi,hlo),e(zi,OY),e(OY,ulo),e(zi,plo),e(Wo,_lo),e(Wo,z5),e(z5,blo),e(z5,XY),e(XY,vlo),e(z5,Tlo),e(Wo,Flo),e(Wo,Dr),g(V5,Dr,null),e(Dr,Clo),e(Dr,zY),e(zY,Mlo),e(Dr,Elo),e(Dr,Vi),e(Vi,ylo),e(Vi,VY),e(VY,wlo),e(Vi,Alo),e(Vi,WY),e(WY,Llo),e(Vi,Blo),e(Dr,klo),e(Dr,QY),e(QY,xlo),e(Dr,Rlo),g(W5,Dr,null),e(Wo,Slo),e(Wo,xe),g(Q5,xe,null),e(xe,Plo),e(xe,HY),e(HY,$lo),e(xe,Ilo),e(xe,qa),e(qa,jlo),e(qa,UY),e(UY,Nlo),e(qa,Dlo),e(qa,JY),e(JY,qlo),e(qa,Glo),e(qa,YY),e(YY,Olo),e(qa,Xlo),e(xe,zlo),e(xe,x),e(x,Zu),e(Zu,KY),e(KY,Vlo),e(Zu,Wlo),e(Zu,zR),e(zR,Qlo),e(Zu,Hlo),e(x,Ulo),e(x,ep),e(ep,ZY),e(ZY,Jlo),e(ep,Ylo),e(ep,VR),e(VR,Klo),e(ep,Zlo),e(x,eio),e(x,op),e(op,eK),e(eK,oio),e(op,rio),e(op,WR),e(WR,tio),e(op,aio),e(x,sio),e(x,rp),e(rp,oK),e(oK,nio),e(rp,lio),e(rp,QR),e(QR,iio),e(rp,dio),e(x,cio),e(x,tp),e(tp,rK),e(rK,mio),e(tp,fio),e(tp,HR),e(HR,gio),e(tp,hio),e(x,uio),e(x,ap),e(ap,tK),e(tK,pio),e(ap,_io),e(ap,UR),e(UR,bio),e(ap,vio),e(x,Tio),e(x,sp),e(sp,aK),e(aK,Fio),e(sp,Cio),e(sp,JR),e(JR,Mio),e(sp,Eio),e(x,yio),e(x,np),e(np,sK),e(sK,wio),e(np,Aio),e(np,YR),e(YR,Lio),e(np,Bio),e(x,kio),e(x,lp),e(lp,nK),e(nK,xio),e(lp,Rio),e(lp,KR),e(KR,Sio),e(lp,Pio),e(x,$io),e(x,ip),e(ip,lK),e(lK,Iio),e(ip,jio),e(ip,ZR),e(ZR,Nio),e(ip,Dio),e(x,qio),e(x,dp),e(dp,iK),e(iK,Gio),e(dp,Oio),e(dp,eS),e(eS,Xio),e(dp,zio),e(x,Vio),e(x,cp),e(cp,dK),e(dK,Wio),e(cp,Qio),e(cp,oS),e(oS,Hio),e(cp,Uio),e(x,Jio),e(x,mp),e(mp,cK),e(cK,Yio),e(mp,Kio),e(mp,rS),e(rS,Zio),e(mp,edo),e(x,odo),e(x,fp),e(fp,mK),e(mK,rdo),e(fp,tdo),e(fp,tS),e(tS,ado),e(fp,sdo),e(x,ndo),e(x,gp),e(gp,fK),e(fK,ldo),e(gp,ido),e(gp,aS),e(aS,ddo),e(gp,cdo),e(x,mdo),e(x,hp),e(hp,gK),e(gK,fdo),e(hp,gdo),e(hp,sS),e(sS,hdo),e(hp,udo),e(x,pdo),e(x,up),e(up,hK),e(hK,_do),e(up,bdo),e(up,nS),e(nS,vdo),e(up,Tdo),e(x,Fdo),e(x,pp),e(pp,uK),e(uK,Cdo),e(pp,Mdo),e(pp,lS),e(lS,Edo),e(pp,ydo),e(x,wdo),e(x,_p),e(_p,pK),e(pK,Ado),e(_p,Ldo),e(_p,iS),e(iS,Bdo),e(_p,kdo),e(x,xdo),e(x,bp),e(bp,_K),e(_K,Rdo),e(bp,Sdo),e(bp,dS),e(dS,Pdo),e(bp,$do),e(x,Ido),e(x,vp),e(vp,bK),e(bK,jdo),e(vp,Ndo),e(vp,cS),e(cS,Ddo),e(vp,qdo),e(x,Gdo),e(x,Tp),e(Tp,vK),e(vK,Odo),e(Tp,Xdo),e(Tp,mS),e(mS,zdo),e(Tp,Vdo),e(x,Wdo),e(x,Fp),e(Fp,TK),e(TK,Qdo),e(Fp,Hdo),e(Fp,fS),e(fS,Udo),e(Fp,Jdo),e(x,Ydo),e(x,Cp),e(Cp,FK),e(FK,Kdo),e(Cp,Zdo),e(Cp,gS),e(gS,eco),e(Cp,oco),e(x,rco),e(x,Mp),e(Mp,CK),e(CK,tco),e(Mp,aco),e(Mp,hS),e(hS,sco),e(Mp,nco),e(x,lco),e(x,Ep),e(Ep,MK),e(MK,ico),e(Ep,dco),e(Ep,uS),e(uS,cco),e(Ep,mco),e(x,fco),e(x,yp),e(yp,EK),e(EK,gco),e(yp,hco),e(yp,pS),e(pS,uco),e(yp,pco),e(x,_co),e(x,wp),e(wp,yK),e(yK,bco),e(wp,vco),e(wp,_S),e(_S,Tco),e(wp,Fco),e(x,Cco),e(x,Ap),e(Ap,wK),e(wK,Mco),e(Ap,Eco),e(Ap,bS),e(bS,yco),e(Ap,wco),e(x,Aco),e(x,Lp),e(Lp,AK),e(AK,Lco),e(Lp,Bco),e(Lp,vS),e(vS,kco),e(Lp,xco),e(x,Rco),e(x,Bp),e(Bp,LK),e(LK,Sco),e(Bp,Pco),e(Bp,TS),e(TS,$co),e(Bp,Ico),e(x,jco),e(x,kp),e(kp,BK),e(BK,Nco),e(kp,Dco),e(kp,FS),e(FS,qco),e(kp,Gco),e(x,Oco),e(x,xp),e(xp,kK),e(kK,Xco),e(xp,zco),e(xp,CS),e(CS,Vco),e(xp,Wco),e(x,Qco),e(x,Rp),e(Rp,xK),e(xK,Hco),e(Rp,Uco),e(Rp,MS),e(MS,Jco),e(Rp,Yco),e(x,Kco),e(x,Sp),e(Sp,RK),e(RK,Zco),e(Sp,emo),e(Sp,ES),e(ES,omo),e(Sp,rmo),e(x,tmo),e(x,Pp),e(Pp,SK),e(SK,amo),e(Pp,smo),e(Pp,yS),e(yS,nmo),e(Pp,lmo),e(x,imo),e(x,$p),e($p,PK),e(PK,dmo),e($p,cmo),e($p,wS),e(wS,mmo),e($p,fmo),e(x,gmo),e(x,Ip),e(Ip,$K),e($K,hmo),e(Ip,umo),e(Ip,AS),e(AS,pmo),e(Ip,_mo),e(xe,bmo),e(xe,jp),e(jp,vmo),e(jp,IK),e(IK,Tmo),e(jp,Fmo),e(jp,jK),e(jK,Cmo),e(xe,Mmo),e(xe,NK),e(NK,Emo),e(xe,ymo),g(H5,xe,null),b(d,T8e,_),b(d,Wi,_),e(Wi,Np),e(Np,DK),g(U5,DK,null),e(Wi,wmo),e(Wi,qK),e(qK,Amo),b(d,F8e,_),b(d,Qo,_),g(J5,Qo,null),e(Qo,Lmo),e(Qo,Qi),e(Qi,Bmo),e(Qi,GK),e(GK,kmo),e(Qi,xmo),e(Qi,OK),e(OK,Rmo),e(Qi,Smo),e(Qo,Pmo),e(Qo,Y5),e(Y5,$mo),e(Y5,XK),e(XK,Imo),e(Y5,jmo),e(Qo,Nmo),e(Qo,qr),g(K5,qr,null),e(qr,Dmo),e(qr,zK),e(zK,qmo),e(qr,Gmo),e(qr,Hi),e(Hi,Omo),e(Hi,VK),e(VK,Xmo),e(Hi,zmo),e(Hi,WK),e(WK,Vmo),e(Hi,Wmo),e(qr,Qmo),e(qr,QK),e(QK,Hmo),e(qr,Umo),g(Z5,qr,null),e(Qo,Jmo),e(Qo,Re),g(ey,Re,null),e(Re,Ymo),e(Re,HK),e(HK,Kmo),e(Re,Zmo),e(Re,Ga),e(Ga,efo),e(Ga,UK),e(UK,ofo),e(Ga,rfo),e(Ga,JK),e(JK,tfo),e(Ga,afo),e(Ga,YK),e(YK,sfo),e(Ga,nfo),e(Re,lfo),e(Re,$),e($,Dp),e(Dp,KK),e(KK,ifo),e(Dp,dfo),e(Dp,LS),e(LS,cfo),e(Dp,mfo),e($,ffo),e($,qp),e(qp,ZK),e(ZK,gfo),e(qp,hfo),e(qp,BS),e(BS,ufo),e(qp,pfo),e($,_fo),e($,Gp),e(Gp,eZ),e(eZ,bfo),e(Gp,vfo),e(Gp,kS),e(kS,Tfo),e(Gp,Ffo),e($,Cfo),e($,Op),e(Op,oZ),e(oZ,Mfo),e(Op,Efo),e(Op,xS),e(xS,yfo),e(Op,wfo),e($,Afo),e($,Xp),e(Xp,rZ),e(rZ,Lfo),e(Xp,Bfo),e(Xp,RS),e(RS,kfo),e(Xp,xfo),e($,Rfo),e($,zp),e(zp,tZ),e(tZ,Sfo),e(zp,Pfo),e(zp,SS),e(SS,$fo),e(zp,Ifo),e($,jfo),e($,Vp),e(Vp,aZ),e(aZ,Nfo),e(Vp,Dfo),e(Vp,PS),e(PS,qfo),e(Vp,Gfo),e($,Ofo),e($,Wp),e(Wp,sZ),e(sZ,Xfo),e(Wp,zfo),e(Wp,$S),e($S,Vfo),e(Wp,Wfo),e($,Qfo),e($,Qp),e(Qp,nZ),e(nZ,Hfo),e(Qp,Ufo),e(Qp,IS),e(IS,Jfo),e(Qp,Yfo),e($,Kfo),e($,Hp),e(Hp,lZ),e(lZ,Zfo),e(Hp,ego),e(Hp,jS),e(jS,ogo),e(Hp,rgo),e($,tgo),e($,Up),e(Up,iZ),e(iZ,ago),e(Up,sgo),e(Up,NS),e(NS,ngo),e(Up,lgo),e($,igo),e($,Jp),e(Jp,dZ),e(dZ,dgo),e(Jp,cgo),e(Jp,DS),e(DS,mgo),e(Jp,fgo),e($,ggo),e($,Yp),e(Yp,cZ),e(cZ,hgo),e(Yp,ugo),e(Yp,qS),e(qS,pgo),e(Yp,_go),e($,bgo),e($,Kp),e(Kp,mZ),e(mZ,vgo),e(Kp,Tgo),e(Kp,GS),e(GS,Fgo),e(Kp,Cgo),e($,Mgo),e($,Zp),e(Zp,fZ),e(fZ,Ego),e(Zp,ygo),e(Zp,OS),e(OS,wgo),e(Zp,Ago),e($,Lgo),e($,e_),e(e_,gZ),e(gZ,Bgo),e(e_,kgo),e(e_,XS),e(XS,xgo),e(e_,Rgo),e($,Sgo),e($,o_),e(o_,hZ),e(hZ,Pgo),e(o_,$go),e(o_,zS),e(zS,Igo),e(o_,jgo),e($,Ngo),e($,r_),e(r_,uZ),e(uZ,Dgo),e(r_,qgo),e(r_,VS),e(VS,Ggo),e(r_,Ogo),e($,Xgo),e($,t_),e(t_,pZ),e(pZ,zgo),e(t_,Vgo),e(t_,WS),e(WS,Wgo),e(t_,Qgo),e($,Hgo),e($,a_),e(a_,_Z),e(_Z,Ugo),e(a_,Jgo),e(a_,QS),e(QS,Ygo),e(a_,Kgo),e($,Zgo),e($,s_),e(s_,bZ),e(bZ,eho),e(s_,oho),e(s_,HS),e(HS,rho),e(s_,tho),e($,aho),e($,n_),e(n_,vZ),e(vZ,sho),e(n_,nho),e(n_,US),e(US,lho),e(n_,iho),e($,dho),e($,l_),e(l_,TZ),e(TZ,cho),e(l_,mho),e(l_,JS),e(JS,fho),e(l_,gho),e($,hho),e($,i_),e(i_,FZ),e(FZ,uho),e(i_,pho),e(i_,YS),e(YS,_ho),e(i_,bho),e($,vho),e($,d_),e(d_,CZ),e(CZ,Tho),e(d_,Fho),e(d_,KS),e(KS,Cho),e(d_,Mho),e($,Eho),e($,c_),e(c_,MZ),e(MZ,yho),e(c_,who),e(c_,ZS),e(ZS,Aho),e(c_,Lho),e($,Bho),e($,m_),e(m_,EZ),e(EZ,kho),e(m_,xho),e(m_,eP),e(eP,Rho),e(m_,Sho),e($,Pho),e($,f_),e(f_,yZ),e(yZ,$ho),e(f_,Iho),e(f_,oP),e(oP,jho),e(f_,Nho),e($,Dho),e($,g_),e(g_,wZ),e(wZ,qho),e(g_,Gho),e(g_,rP),e(rP,Oho),e(g_,Xho),e($,zho),e($,h_),e(h_,AZ),e(AZ,Vho),e(h_,Who),e(h_,tP),e(tP,Qho),e(h_,Hho),e($,Uho),e($,u_),e(u_,LZ),e(LZ,Jho),e(u_,Yho),e(u_,aP),e(aP,Kho),e(u_,Zho),e($,euo),e($,p_),e(p_,BZ),e(BZ,ouo),e(p_,ruo),e(p_,sP),e(sP,tuo),e(p_,auo),e($,suo),e($,__),e(__,kZ),e(kZ,nuo),e(__,luo),e(__,nP),e(nP,iuo),e(__,duo),e($,cuo),e($,b_),e(b_,xZ),e(xZ,muo),e(b_,fuo),e(b_,lP),e(lP,guo),e(b_,huo),e(Re,uuo),e(Re,v_),e(v_,puo),e(v_,RZ),e(RZ,_uo),e(v_,buo),e(v_,SZ),e(SZ,vuo),e(Re,Tuo),e(Re,PZ),e(PZ,Fuo),e(Re,Cuo),g(oy,Re,null),b(d,C8e,_),b(d,Ui,_),e(Ui,T_),e(T_,$Z),g(ry,$Z,null),e(Ui,Muo),e(Ui,IZ),e(IZ,Euo),b(d,M8e,_),b(d,Ho,_),g(ty,Ho,null),e(Ho,yuo),e(Ho,Ji),e(Ji,wuo),e(Ji,jZ),e(jZ,Auo),e(Ji,Luo),e(Ji,NZ),e(NZ,Buo),e(Ji,kuo),e(Ho,xuo),e(Ho,ay),e(ay,Ruo),e(ay,DZ),e(DZ,Suo),e(ay,Puo),e(Ho,$uo),e(Ho,Gr),g(sy,Gr,null),e(Gr,Iuo),e(Gr,qZ),e(qZ,juo),e(Gr,Nuo),e(Gr,Yi),e(Yi,Duo),e(Yi,GZ),e(GZ,quo),e(Yi,Guo),e(Yi,OZ),e(OZ,Ouo),e(Yi,Xuo),e(Gr,zuo),e(Gr,XZ),e(XZ,Vuo),e(Gr,Wuo),g(ny,Gr,null),e(Ho,Quo),e(Ho,Se),g(ly,Se,null),e(Se,Huo),e(Se,zZ),e(zZ,Uuo),e(Se,Juo),e(Se,Oa),e(Oa,Yuo),e(Oa,VZ),e(VZ,Kuo),e(Oa,Zuo),e(Oa,WZ),e(WZ,epo),e(Oa,opo),e(Oa,QZ),e(QZ,rpo),e(Oa,tpo),e(Se,apo),e(Se,I),e(I,F_),e(F_,HZ),e(HZ,spo),e(F_,npo),e(F_,iP),e(iP,lpo),e(F_,ipo),e(I,dpo),e(I,C_),e(C_,UZ),e(UZ,cpo),e(C_,mpo),e(C_,dP),e(dP,fpo),e(C_,gpo),e(I,hpo),e(I,M_),e(M_,JZ),e(JZ,upo),e(M_,ppo),e(M_,cP),e(cP,_po),e(M_,bpo),e(I,vpo),e(I,E_),e(E_,YZ),e(YZ,Tpo),e(E_,Fpo),e(E_,mP),e(mP,Cpo),e(E_,Mpo),e(I,Epo),e(I,y_),e(y_,KZ),e(KZ,ypo),e(y_,wpo),e(y_,fP),e(fP,Apo),e(y_,Lpo),e(I,Bpo),e(I,w_),e(w_,ZZ),e(ZZ,kpo),e(w_,xpo),e(w_,gP),e(gP,Rpo),e(w_,Spo),e(I,Ppo),e(I,A_),e(A_,eee),e(eee,$po),e(A_,Ipo),e(A_,hP),e(hP,jpo),e(A_,Npo),e(I,Dpo),e(I,L_),e(L_,oee),e(oee,qpo),e(L_,Gpo),e(L_,uP),e(uP,Opo),e(L_,Xpo),e(I,zpo),e(I,B_),e(B_,ree),e(ree,Vpo),e(B_,Wpo),e(B_,pP),e(pP,Qpo),e(B_,Hpo),e(I,Upo),e(I,k_),e(k_,tee),e(tee,Jpo),e(k_,Ypo),e(k_,_P),e(_P,Kpo),e(k_,Zpo),e(I,e_o),e(I,x_),e(x_,aee),e(aee,o_o),e(x_,r_o),e(x_,bP),e(bP,t_o),e(x_,a_o),e(I,s_o),e(I,R_),e(R_,see),e(see,n_o),e(R_,l_o),e(R_,vP),e(vP,i_o),e(R_,d_o),e(I,c_o),e(I,S_),e(S_,nee),e(nee,m_o),e(S_,f_o),e(S_,TP),e(TP,g_o),e(S_,h_o),e(I,u_o),e(I,P_),e(P_,lee),e(lee,p_o),e(P_,__o),e(P_,FP),e(FP,b_o),e(P_,v_o),e(I,T_o),e(I,$_),e($_,iee),e(iee,F_o),e($_,C_o),e($_,CP),e(CP,M_o),e($_,E_o),e(I,y_o),e(I,I_),e(I_,dee),e(dee,w_o),e(I_,A_o),e(I_,MP),e(MP,L_o),e(I_,B_o),e(I,k_o),e(I,j_),e(j_,cee),e(cee,x_o),e(j_,R_o),e(j_,EP),e(EP,S_o),e(j_,P_o),e(I,$_o),e(I,N_),e(N_,mee),e(mee,I_o),e(N_,j_o),e(N_,yP),e(yP,N_o),e(N_,D_o),e(I,q_o),e(I,D_),e(D_,fee),e(fee,G_o),e(D_,O_o),e(D_,wP),e(wP,X_o),e(D_,z_o),e(I,V_o),e(I,q_),e(q_,gee),e(gee,W_o),e(q_,Q_o),e(q_,AP),e(AP,H_o),e(q_,U_o),e(I,J_o),e(I,G_),e(G_,hee),e(hee,Y_o),e(G_,K_o),e(G_,LP),e(LP,Z_o),e(G_,ebo),e(I,obo),e(I,O_),e(O_,uee),e(uee,rbo),e(O_,tbo),e(O_,BP),e(BP,abo),e(O_,sbo),e(I,nbo),e(I,X_),e(X_,pee),e(pee,lbo),e(X_,ibo),e(X_,kP),e(kP,dbo),e(X_,cbo),e(I,mbo),e(I,z_),e(z_,_ee),e(_ee,fbo),e(z_,gbo),e(z_,xP),e(xP,hbo),e(z_,ubo),e(I,pbo),e(I,V_),e(V_,bee),e(bee,_bo),e(V_,bbo),e(V_,RP),e(RP,vbo),e(V_,Tbo),e(I,Fbo),e(I,W_),e(W_,vee),e(vee,Cbo),e(W_,Mbo),e(W_,SP),e(SP,Ebo),e(W_,ybo),e(I,wbo),e(I,Q_),e(Q_,Tee),e(Tee,Abo),e(Q_,Lbo),e(Q_,PP),e(PP,Bbo),e(Q_,kbo),e(I,xbo),e(I,H_),e(H_,Fee),e(Fee,Rbo),e(H_,Sbo),e(H_,$P),e($P,Pbo),e(H_,$bo),e(I,Ibo),e(I,U_),e(U_,Cee),e(Cee,jbo),e(U_,Nbo),e(U_,IP),e(IP,Dbo),e(U_,qbo),e(I,Gbo),e(I,J_),e(J_,Mee),e(Mee,Obo),e(J_,Xbo),e(J_,Eee),e(Eee,zbo),e(J_,Vbo),e(I,Wbo),e(I,Y_),e(Y_,yee),e(yee,Qbo),e(Y_,Hbo),e(Y_,jP),e(jP,Ubo),e(Y_,Jbo),e(I,Ybo),e(I,K_),e(K_,wee),e(wee,Kbo),e(K_,Zbo),e(K_,NP),e(NP,e2o),e(K_,o2o),e(I,r2o),e(I,Z_),e(Z_,Aee),e(Aee,t2o),e(Z_,a2o),e(Z_,DP),e(DP,s2o),e(Z_,n2o),e(I,l2o),e(I,eb),e(eb,Lee),e(Lee,i2o),e(eb,d2o),e(eb,qP),e(qP,c2o),e(eb,m2o),e(Se,f2o),e(Se,ob),e(ob,g2o),e(ob,Bee),e(Bee,h2o),e(ob,u2o),e(ob,kee),e(kee,p2o),e(Se,_2o),e(Se,xee),e(xee,b2o),e(Se,v2o),g(iy,Se,null),b(d,E8e,_),b(d,Ki,_),e(Ki,rb),e(rb,Ree),g(dy,Ree,null),e(Ki,T2o),e(Ki,See),e(See,F2o),b(d,y8e,_),b(d,Uo,_),g(cy,Uo,null),e(Uo,C2o),e(Uo,Zi),e(Zi,M2o),e(Zi,Pee),e(Pee,E2o),e(Zi,y2o),e(Zi,$ee),e($ee,w2o),e(Zi,A2o),e(Uo,L2o),e(Uo,my),e(my,B2o),e(my,Iee),e(Iee,k2o),e(my,x2o),e(Uo,R2o),e(Uo,Or),g(fy,Or,null),e(Or,S2o),e(Or,jee),e(jee,P2o),e(Or,$2o),e(Or,ed),e(ed,I2o),e(ed,Nee),e(Nee,j2o),e(ed,N2o),e(ed,Dee),e(Dee,D2o),e(ed,q2o),e(Or,G2o),e(Or,qee),e(qee,O2o),e(Or,X2o),g(gy,Or,null),e(Uo,z2o),e(Uo,Pe),g(hy,Pe,null),e(Pe,V2o),e(Pe,Gee),e(Gee,W2o),e(Pe,Q2o),e(Pe,Xa),e(Xa,H2o),e(Xa,Oee),e(Oee,U2o),e(Xa,J2o),e(Xa,Xee),e(Xee,Y2o),e(Xa,K2o),e(Xa,zee),e(zee,Z2o),e(Xa,evo),e(Pe,ovo),e(Pe,ae),e(ae,tb),e(tb,Vee),e(Vee,rvo),e(tb,tvo),e(tb,GP),e(GP,avo),e(tb,svo),e(ae,nvo),e(ae,ab),e(ab,Wee),e(Wee,lvo),e(ab,ivo),e(ab,OP),e(OP,dvo),e(ab,cvo),e(ae,mvo),e(ae,sb),e(sb,Qee),e(Qee,fvo),e(sb,gvo),e(sb,XP),e(XP,hvo),e(sb,uvo),e(ae,pvo),e(ae,nb),e(nb,Hee),e(Hee,_vo),e(nb,bvo),e(nb,zP),e(zP,vvo),e(nb,Tvo),e(ae,Fvo),e(ae,lb),e(lb,Uee),e(Uee,Cvo),e(lb,Mvo),e(lb,VP),e(VP,Evo),e(lb,yvo),e(ae,wvo),e(ae,ib),e(ib,Jee),e(Jee,Avo),e(ib,Lvo),e(ib,WP),e(WP,Bvo),e(ib,kvo),e(ae,xvo),e(ae,db),e(db,Yee),e(Yee,Rvo),e(db,Svo),e(db,QP),e(QP,Pvo),e(db,$vo),e(ae,Ivo),e(ae,cb),e(cb,Kee),e(Kee,jvo),e(cb,Nvo),e(cb,HP),e(HP,Dvo),e(cb,qvo),e(ae,Gvo),e(ae,mb),e(mb,Zee),e(Zee,Ovo),e(mb,Xvo),e(mb,UP),e(UP,zvo),e(mb,Vvo),e(ae,Wvo),e(ae,fb),e(fb,eoe),e(eoe,Qvo),e(fb,Hvo),e(fb,JP),e(JP,Uvo),e(fb,Jvo),e(ae,Yvo),e(ae,gb),e(gb,ooe),e(ooe,Kvo),e(gb,Zvo),e(gb,YP),e(YP,eTo),e(gb,oTo),e(ae,rTo),e(ae,hb),e(hb,roe),e(roe,tTo),e(hb,aTo),e(hb,KP),e(KP,sTo),e(hb,nTo),e(ae,lTo),e(ae,ub),e(ub,toe),e(toe,iTo),e(ub,dTo),e(ub,ZP),e(ZP,cTo),e(ub,mTo),e(ae,fTo),e(ae,pb),e(pb,aoe),e(aoe,gTo),e(pb,hTo),e(pb,e$),e(e$,uTo),e(pb,pTo),e(ae,_To),e(ae,_b),e(_b,soe),e(soe,bTo),e(_b,vTo),e(_b,o$),e(o$,TTo),e(_b,FTo),e(ae,CTo),e(ae,bb),e(bb,noe),e(noe,MTo),e(bb,ETo),e(bb,r$),e(r$,yTo),e(bb,wTo),e(Pe,ATo),e(Pe,vb),e(vb,LTo),e(vb,loe),e(loe,BTo),e(vb,kTo),e(vb,ioe),e(ioe,xTo),e(Pe,RTo),e(Pe,doe),e(doe,STo),e(Pe,PTo),g(uy,Pe,null),b(d,w8e,_),b(d,od,_),e(od,Tb),e(Tb,coe),g(py,coe,null),e(od,$To),e(od,moe),e(moe,ITo),b(d,A8e,_),b(d,Jo,_),g(_y,Jo,null),e(Jo,jTo),e(Jo,rd),e(rd,NTo),e(rd,foe),e(foe,DTo),e(rd,qTo),e(rd,goe),e(goe,GTo),e(rd,OTo),e(Jo,XTo),e(Jo,by),e(by,zTo),e(by,hoe),e(hoe,VTo),e(by,WTo),e(Jo,QTo),e(Jo,Xr),g(vy,Xr,null),e(Xr,HTo),e(Xr,uoe),e(uoe,UTo),e(Xr,JTo),e(Xr,td),e(td,YTo),e(td,poe),e(poe,KTo),e(td,ZTo),e(td,_oe),e(_oe,e1o),e(td,o1o),e(Xr,r1o),e(Xr,boe),e(boe,t1o),e(Xr,a1o),g(Ty,Xr,null),e(Jo,s1o),e(Jo,$e),g(Fy,$e,null),e($e,n1o),e($e,voe),e(voe,l1o),e($e,i1o),e($e,za),e(za,d1o),e(za,Toe),e(Toe,c1o),e(za,m1o),e(za,Foe),e(Foe,f1o),e(za,g1o),e(za,Coe),e(Coe,h1o),e(za,u1o),e($e,p1o),e($e,A),e(A,Fb),e(Fb,Moe),e(Moe,_1o),e(Fb,b1o),e(Fb,t$),e(t$,v1o),e(Fb,T1o),e(A,F1o),e(A,Cb),e(Cb,Eoe),e(Eoe,C1o),e(Cb,M1o),e(Cb,a$),e(a$,E1o),e(Cb,y1o),e(A,w1o),e(A,Mb),e(Mb,yoe),e(yoe,A1o),e(Mb,L1o),e(Mb,s$),e(s$,B1o),e(Mb,k1o),e(A,x1o),e(A,Eb),e(Eb,woe),e(woe,R1o),e(Eb,S1o),e(Eb,n$),e(n$,P1o),e(Eb,$1o),e(A,I1o),e(A,yb),e(yb,Aoe),e(Aoe,j1o),e(yb,N1o),e(yb,l$),e(l$,D1o),e(yb,q1o),e(A,G1o),e(A,wb),e(wb,Loe),e(Loe,O1o),e(wb,X1o),e(wb,i$),e(i$,z1o),e(wb,V1o),e(A,W1o),e(A,Ab),e(Ab,Boe),e(Boe,Q1o),e(Ab,H1o),e(Ab,d$),e(d$,U1o),e(Ab,J1o),e(A,Y1o),e(A,Lb),e(Lb,koe),e(koe,K1o),e(Lb,Z1o),e(Lb,c$),e(c$,eFo),e(Lb,oFo),e(A,rFo),e(A,Bb),e(Bb,xoe),e(xoe,tFo),e(Bb,aFo),e(Bb,m$),e(m$,sFo),e(Bb,nFo),e(A,lFo),e(A,kb),e(kb,Roe),e(Roe,iFo),e(kb,dFo),e(kb,f$),e(f$,cFo),e(kb,mFo),e(A,fFo),e(A,xb),e(xb,Soe),e(Soe,gFo),e(xb,hFo),e(xb,g$),e(g$,uFo),e(xb,pFo),e(A,_Fo),e(A,Rb),e(Rb,Poe),e(Poe,bFo),e(Rb,vFo),e(Rb,h$),e(h$,TFo),e(Rb,FFo),e(A,CFo),e(A,Sb),e(Sb,$oe),e($oe,MFo),e(Sb,EFo),e(Sb,u$),e(u$,yFo),e(Sb,wFo),e(A,AFo),e(A,Pb),e(Pb,Ioe),e(Ioe,LFo),e(Pb,BFo),e(Pb,p$),e(p$,kFo),e(Pb,xFo),e(A,RFo),e(A,$b),e($b,joe),e(joe,SFo),e($b,PFo),e($b,_$),e(_$,$Fo),e($b,IFo),e(A,jFo),e(A,Ib),e(Ib,Noe),e(Noe,NFo),e(Ib,DFo),e(Ib,b$),e(b$,qFo),e(Ib,GFo),e(A,OFo),e(A,jb),e(jb,Doe),e(Doe,XFo),e(jb,zFo),e(jb,v$),e(v$,VFo),e(jb,WFo),e(A,QFo),e(A,Nb),e(Nb,qoe),e(qoe,HFo),e(Nb,UFo),e(Nb,T$),e(T$,JFo),e(Nb,YFo),e(A,KFo),e(A,Db),e(Db,Goe),e(Goe,ZFo),e(Db,eCo),e(Db,F$),e(F$,oCo),e(Db,rCo),e(A,tCo),e(A,qb),e(qb,Ooe),e(Ooe,aCo),e(qb,sCo),e(qb,C$),e(C$,nCo),e(qb,lCo),e(A,iCo),e(A,Gb),e(Gb,Xoe),e(Xoe,dCo),e(Gb,cCo),e(Gb,M$),e(M$,mCo),e(Gb,fCo),e(A,gCo),e(A,Ob),e(Ob,zoe),e(zoe,hCo),e(Ob,uCo),e(Ob,E$),e(E$,pCo),e(Ob,_Co),e(A,bCo),e(A,Xb),e(Xb,Voe),e(Voe,vCo),e(Xb,TCo),e(Xb,y$),e(y$,FCo),e(Xb,CCo),e(A,MCo),e(A,zb),e(zb,Woe),e(Woe,ECo),e(zb,yCo),e(zb,w$),e(w$,wCo),e(zb,ACo),e(A,LCo),e(A,Vb),e(Vb,Qoe),e(Qoe,BCo),e(Vb,kCo),e(Vb,A$),e(A$,xCo),e(Vb,RCo),e(A,SCo),e(A,Wb),e(Wb,Hoe),e(Hoe,PCo),e(Wb,$Co),e(Wb,L$),e(L$,ICo),e(Wb,jCo),e(A,NCo),e(A,Qb),e(Qb,Uoe),e(Uoe,DCo),e(Qb,qCo),e(Qb,B$),e(B$,GCo),e(Qb,OCo),e(A,XCo),e(A,Hb),e(Hb,Joe),e(Joe,zCo),e(Hb,VCo),e(Hb,k$),e(k$,WCo),e(Hb,QCo),e(A,HCo),e(A,Ub),e(Ub,Yoe),e(Yoe,UCo),e(Ub,JCo),e(Ub,x$),e(x$,YCo),e(Ub,KCo),e(A,ZCo),e(A,Jb),e(Jb,Koe),e(Koe,e4o),e(Jb,o4o),e(Jb,R$),e(R$,r4o),e(Jb,t4o),e(A,a4o),e(A,Yb),e(Yb,Zoe),e(Zoe,s4o),e(Yb,n4o),e(Yb,S$),e(S$,l4o),e(Yb,i4o),e(A,d4o),e(A,Kb),e(Kb,ere),e(ere,c4o),e(Kb,m4o),e(Kb,P$),e(P$,f4o),e(Kb,g4o),e(A,h4o),e(A,Zb),e(Zb,ore),e(ore,u4o),e(Zb,p4o),e(Zb,$$),e($$,_4o),e(Zb,b4o),e(A,v4o),e(A,e2),e(e2,rre),e(rre,T4o),e(e2,F4o),e(e2,I$),e(I$,C4o),e(e2,M4o),e(A,E4o),e(A,o2),e(o2,tre),e(tre,y4o),e(o2,w4o),e(o2,j$),e(j$,A4o),e(o2,L4o),e(A,B4o),e(A,r2),e(r2,are),e(are,k4o),e(r2,x4o),e(r2,N$),e(N$,R4o),e(r2,S4o),e(A,P4o),e(A,t2),e(t2,sre),e(sre,$4o),e(t2,I4o),e(t2,D$),e(D$,j4o),e(t2,N4o),e(A,D4o),e(A,a2),e(a2,nre),e(nre,q4o),e(a2,G4o),e(a2,q$),e(q$,O4o),e(a2,X4o),e(A,z4o),e(A,s2),e(s2,lre),e(lre,V4o),e(s2,W4o),e(s2,G$),e(G$,Q4o),e(s2,H4o),e(A,U4o),e(A,n2),e(n2,ire),e(ire,J4o),e(n2,Y4o),e(n2,O$),e(O$,K4o),e(n2,Z4o),e(A,eMo),e(A,l2),e(l2,dre),e(dre,oMo),e(l2,rMo),e(l2,X$),e(X$,tMo),e(l2,aMo),e(A,sMo),e(A,i2),e(i2,cre),e(cre,nMo),e(i2,lMo),e(i2,z$),e(z$,iMo),e(i2,dMo),e(A,cMo),e(A,d2),e(d2,mre),e(mre,mMo),e(d2,fMo),e(d2,V$),e(V$,gMo),e(d2,hMo),e(A,uMo),e(A,c2),e(c2,fre),e(fre,pMo),e(c2,_Mo),e(c2,W$),e(W$,bMo),e(c2,vMo),e(A,TMo),e(A,m2),e(m2,gre),e(gre,FMo),e(m2,CMo),e(m2,Q$),e(Q$,MMo),e(m2,EMo),e($e,yMo),e($e,f2),e(f2,wMo),e(f2,hre),e(hre,AMo),e(f2,LMo),e(f2,ure),e(ure,BMo),e($e,kMo),e($e,pre),e(pre,xMo),e($e,RMo),g(Cy,$e,null),b(d,L8e,_),b(d,ad,_),e(ad,g2),e(g2,_re),g(My,_re,null),e(ad,SMo),e(ad,bre),e(bre,PMo),b(d,B8e,_),b(d,Yo,_),g(Ey,Yo,null),e(Yo,$Mo),e(Yo,sd),e(sd,IMo),e(sd,vre),e(vre,jMo),e(sd,NMo),e(sd,Tre),e(Tre,DMo),e(sd,qMo),e(Yo,GMo),e(Yo,yy),e(yy,OMo),e(yy,Fre),e(Fre,XMo),e(yy,zMo),e(Yo,VMo),e(Yo,zr),g(wy,zr,null),e(zr,WMo),e(zr,Cre),e(Cre,QMo),e(zr,HMo),e(zr,nd),e(nd,UMo),e(nd,Mre),e(Mre,JMo),e(nd,YMo),e(nd,Ere),e(Ere,KMo),e(nd,ZMo),e(zr,eEo),e(zr,yre),e(yre,oEo),e(zr,rEo),g(Ay,zr,null),e(Yo,tEo),e(Yo,Ie),g(Ly,Ie,null),e(Ie,aEo),e(Ie,wre),e(wre,sEo),e(Ie,nEo),e(Ie,Va),e(Va,lEo),e(Va,Are),e(Are,iEo),e(Va,dEo),e(Va,Lre),e(Lre,cEo),e(Va,mEo),e(Va,Bre),e(Bre,fEo),e(Va,gEo),e(Ie,hEo),e(Ie,G),e(G,h2),e(h2,kre),e(kre,uEo),e(h2,pEo),e(h2,H$),e(H$,_Eo),e(h2,bEo),e(G,vEo),e(G,u2),e(u2,xre),e(xre,TEo),e(u2,FEo),e(u2,U$),e(U$,CEo),e(u2,MEo),e(G,EEo),e(G,p2),e(p2,Rre),e(Rre,yEo),e(p2,wEo),e(p2,J$),e(J$,AEo),e(p2,LEo),e(G,BEo),e(G,_2),e(_2,Sre),e(Sre,kEo),e(_2,xEo),e(_2,Y$),e(Y$,REo),e(_2,SEo),e(G,PEo),e(G,b2),e(b2,Pre),e(Pre,$Eo),e(b2,IEo),e(b2,K$),e(K$,jEo),e(b2,NEo),e(G,DEo),e(G,v2),e(v2,$re),e($re,qEo),e(v2,GEo),e(v2,Z$),e(Z$,OEo),e(v2,XEo),e(G,zEo),e(G,T2),e(T2,Ire),e(Ire,VEo),e(T2,WEo),e(T2,eI),e(eI,QEo),e(T2,HEo),e(G,UEo),e(G,F2),e(F2,jre),e(jre,JEo),e(F2,YEo),e(F2,oI),e(oI,KEo),e(F2,ZEo),e(G,e3o),e(G,C2),e(C2,Nre),e(Nre,o3o),e(C2,r3o),e(C2,rI),e(rI,t3o),e(C2,a3o),e(G,s3o),e(G,M2),e(M2,Dre),e(Dre,n3o),e(M2,l3o),e(M2,tI),e(tI,i3o),e(M2,d3o),e(G,c3o),e(G,E2),e(E2,qre),e(qre,m3o),e(E2,f3o),e(E2,aI),e(aI,g3o),e(E2,h3o),e(G,u3o),e(G,y2),e(y2,Gre),e(Gre,p3o),e(y2,_3o),e(y2,sI),e(sI,b3o),e(y2,v3o),e(G,T3o),e(G,w2),e(w2,Ore),e(Ore,F3o),e(w2,C3o),e(w2,nI),e(nI,M3o),e(w2,E3o),e(G,y3o),e(G,A2),e(A2,Xre),e(Xre,w3o),e(A2,A3o),e(A2,lI),e(lI,L3o),e(A2,B3o),e(G,k3o),e(G,L2),e(L2,zre),e(zre,x3o),e(L2,R3o),e(L2,iI),e(iI,S3o),e(L2,P3o),e(G,$3o),e(G,B2),e(B2,Vre),e(Vre,I3o),e(B2,j3o),e(B2,dI),e(dI,N3o),e(B2,D3o),e(G,q3o),e(G,k2),e(k2,Wre),e(Wre,G3o),e(k2,O3o),e(k2,cI),e(cI,X3o),e(k2,z3o),e(G,V3o),e(G,x2),e(x2,Qre),e(Qre,W3o),e(x2,Q3o),e(x2,mI),e(mI,H3o),e(x2,U3o),e(G,J3o),e(G,R2),e(R2,Hre),e(Hre,Y3o),e(R2,K3o),e(R2,fI),e(fI,Z3o),e(R2,e5o),e(G,o5o),e(G,S2),e(S2,Ure),e(Ure,r5o),e(S2,t5o),e(S2,gI),e(gI,a5o),e(S2,s5o),e(G,n5o),e(G,P2),e(P2,Jre),e(Jre,l5o),e(P2,i5o),e(P2,hI),e(hI,d5o),e(P2,c5o),e(G,m5o),e(G,$2),e($2,Yre),e(Yre,f5o),e($2,g5o),e($2,uI),e(uI,h5o),e($2,u5o),e(G,p5o),e(G,I2),e(I2,Kre),e(Kre,_5o),e(I2,b5o),e(I2,pI),e(pI,v5o),e(I2,T5o),e(G,F5o),e(G,j2),e(j2,Zre),e(Zre,C5o),e(j2,M5o),e(j2,_I),e(_I,E5o),e(j2,y5o),e(G,w5o),e(G,N2),e(N2,ete),e(ete,A5o),e(N2,L5o),e(N2,bI),e(bI,B5o),e(N2,k5o),e(G,x5o),e(G,D2),e(D2,ote),e(ote,R5o),e(D2,S5o),e(D2,vI),e(vI,P5o),e(D2,$5o),e(G,I5o),e(G,q2),e(q2,rte),e(rte,j5o),e(q2,N5o),e(q2,TI),e(TI,D5o),e(q2,q5o),e(Ie,G5o),e(Ie,G2),e(G2,O5o),e(G2,tte),e(tte,X5o),e(G2,z5o),e(G2,ate),e(ate,V5o),e(Ie,W5o),e(Ie,ste),e(ste,Q5o),e(Ie,H5o),g(By,Ie,null),b(d,k8e,_),b(d,ld,_),e(ld,O2),e(O2,nte),g(ky,nte,null),e(ld,U5o),e(ld,lte),e(lte,J5o),b(d,x8e,_),b(d,Ko,_),g(xy,Ko,null),e(Ko,Y5o),e(Ko,id),e(id,K5o),e(id,ite),e(ite,Z5o),e(id,eyo),e(id,dte),e(dte,oyo),e(id,ryo),e(Ko,tyo),e(Ko,Ry),e(Ry,ayo),e(Ry,cte),e(cte,syo),e(Ry,nyo),e(Ko,lyo),e(Ko,Vr),g(Sy,Vr,null),e(Vr,iyo),e(Vr,mte),e(mte,dyo),e(Vr,cyo),e(Vr,dd),e(dd,myo),e(dd,fte),e(fte,fyo),e(dd,gyo),e(dd,gte),e(gte,hyo),e(dd,uyo),e(Vr,pyo),e(Vr,hte),e(hte,_yo),e(Vr,byo),g(Py,Vr,null),e(Ko,vyo),e(Ko,je),g($y,je,null),e(je,Tyo),e(je,ute),e(ute,Fyo),e(je,Cyo),e(je,Wa),e(Wa,Myo),e(Wa,pte),e(pte,Eyo),e(Wa,yyo),e(Wa,_te),e(_te,wyo),e(Wa,Ayo),e(Wa,bte),e(bte,Lyo),e(Wa,Byo),e(je,kyo),e(je,sa),e(sa,X2),e(X2,vte),e(vte,xyo),e(X2,Ryo),e(X2,FI),e(FI,Syo),e(X2,Pyo),e(sa,$yo),e(sa,z2),e(z2,Tte),e(Tte,Iyo),e(z2,jyo),e(z2,CI),e(CI,Nyo),e(z2,Dyo),e(sa,qyo),e(sa,V2),e(V2,Fte),e(Fte,Gyo),e(V2,Oyo),e(V2,MI),e(MI,Xyo),e(V2,zyo),e(sa,Vyo),e(sa,W2),e(W2,Cte),e(Cte,Wyo),e(W2,Qyo),e(W2,EI),e(EI,Hyo),e(W2,Uyo),e(sa,Jyo),e(sa,Q2),e(Q2,Mte),e(Mte,Yyo),e(Q2,Kyo),e(Q2,yI),e(yI,Zyo),e(Q2,ewo),e(je,owo),e(je,H2),e(H2,rwo),e(H2,Ete),e(Ete,two),e(H2,awo),e(H2,yte),e(yte,swo),e(je,nwo),e(je,wte),e(wte,lwo),e(je,iwo),g(Iy,je,null),b(d,R8e,_),b(d,cd,_),e(cd,U2),e(U2,Ate),g(jy,Ate,null),e(cd,dwo),e(cd,Lte),e(Lte,cwo),b(d,S8e,_),b(d,Zo,_),g(Ny,Zo,null),e(Zo,mwo),e(Zo,md),e(md,fwo),e(md,Bte),e(Bte,gwo),e(md,hwo),e(md,kte),e(kte,uwo),e(md,pwo),e(Zo,_wo),e(Zo,Dy),e(Dy,bwo),e(Dy,xte),e(xte,vwo),e(Dy,Two),e(Zo,Fwo),e(Zo,Wr),g(qy,Wr,null),e(Wr,Cwo),e(Wr,Rte),e(Rte,Mwo),e(Wr,Ewo),e(Wr,fd),e(fd,ywo),e(fd,Ste),e(Ste,wwo),e(fd,Awo),e(fd,Pte),e(Pte,Lwo),e(fd,Bwo),e(Wr,kwo),e(Wr,$te),e($te,xwo),e(Wr,Rwo),g(Gy,Wr,null),e(Zo,Swo),e(Zo,Ne),g(Oy,Ne,null),e(Ne,Pwo),e(Ne,Ite),e(Ite,$wo),e(Ne,Iwo),e(Ne,Qa),e(Qa,jwo),e(Qa,jte),e(jte,Nwo),e(Qa,Dwo),e(Qa,Nte),e(Nte,qwo),e(Qa,Gwo),e(Qa,Dte),e(Dte,Owo),e(Qa,Xwo),e(Ne,zwo),e(Ne,D),e(D,J2),e(J2,qte),e(qte,Vwo),e(J2,Wwo),e(J2,wI),e(wI,Qwo),e(J2,Hwo),e(D,Uwo),e(D,Y2),e(Y2,Gte),e(Gte,Jwo),e(Y2,Ywo),e(Y2,AI),e(AI,Kwo),e(Y2,Zwo),e(D,eAo),e(D,K2),e(K2,Ote),e(Ote,oAo),e(K2,rAo),e(K2,LI),e(LI,tAo),e(K2,aAo),e(D,sAo),e(D,Z2),e(Z2,Xte),e(Xte,nAo),e(Z2,lAo),e(Z2,BI),e(BI,iAo),e(Z2,dAo),e(D,cAo),e(D,ev),e(ev,zte),e(zte,mAo),e(ev,fAo),e(ev,kI),e(kI,gAo),e(ev,hAo),e(D,uAo),e(D,ov),e(ov,Vte),e(Vte,pAo),e(ov,_Ao),e(ov,xI),e(xI,bAo),e(ov,vAo),e(D,TAo),e(D,rv),e(rv,Wte),e(Wte,FAo),e(rv,CAo),e(rv,RI),e(RI,MAo),e(rv,EAo),e(D,yAo),e(D,tv),e(tv,Qte),e(Qte,wAo),e(tv,AAo),e(tv,SI),e(SI,LAo),e(tv,BAo),e(D,kAo),e(D,av),e(av,Hte),e(Hte,xAo),e(av,RAo),e(av,PI),e(PI,SAo),e(av,PAo),e(D,$Ao),e(D,sv),e(sv,Ute),e(Ute,IAo),e(sv,jAo),e(sv,$I),e($I,NAo),e(sv,DAo),e(D,qAo),e(D,nv),e(nv,Jte),e(Jte,GAo),e(nv,OAo),e(nv,II),e(II,XAo),e(nv,zAo),e(D,VAo),e(D,lv),e(lv,Yte),e(Yte,WAo),e(lv,QAo),e(lv,jI),e(jI,HAo),e(lv,UAo),e(D,JAo),e(D,iv),e(iv,Kte),e(Kte,YAo),e(iv,KAo),e(iv,NI),e(NI,ZAo),e(iv,e6o),e(D,o6o),e(D,dv),e(dv,Zte),e(Zte,r6o),e(dv,t6o),e(dv,DI),e(DI,a6o),e(dv,s6o),e(D,n6o),e(D,cv),e(cv,eae),e(eae,l6o),e(cv,i6o),e(cv,qI),e(qI,d6o),e(cv,c6o),e(D,m6o),e(D,mv),e(mv,oae),e(oae,f6o),e(mv,g6o),e(mv,GI),e(GI,h6o),e(mv,u6o),e(D,p6o),e(D,fv),e(fv,rae),e(rae,_6o),e(fv,b6o),e(fv,OI),e(OI,v6o),e(fv,T6o),e(D,F6o),e(D,gv),e(gv,tae),e(tae,C6o),e(gv,M6o),e(gv,XI),e(XI,E6o),e(gv,y6o),e(D,w6o),e(D,hv),e(hv,aae),e(aae,A6o),e(hv,L6o),e(hv,zI),e(zI,B6o),e(hv,k6o),e(D,x6o),e(D,uv),e(uv,sae),e(sae,R6o),e(uv,S6o),e(uv,VI),e(VI,P6o),e(uv,$6o),e(D,I6o),e(D,pv),e(pv,nae),e(nae,j6o),e(pv,N6o),e(pv,WI),e(WI,D6o),e(pv,q6o),e(D,G6o),e(D,_v),e(_v,lae),e(lae,O6o),e(_v,X6o),e(_v,QI),e(QI,z6o),e(_v,V6o),e(D,W6o),e(D,bv),e(bv,iae),e(iae,Q6o),e(bv,H6o),e(bv,HI),e(HI,U6o),e(bv,J6o),e(D,Y6o),e(D,vv),e(vv,dae),e(dae,K6o),e(vv,Z6o),e(vv,UI),e(UI,e0o),e(vv,o0o),e(D,r0o),e(D,Tv),e(Tv,cae),e(cae,t0o),e(Tv,a0o),e(Tv,JI),e(JI,s0o),e(Tv,n0o),e(D,l0o),e(D,Fv),e(Fv,mae),e(mae,i0o),e(Fv,d0o),e(Fv,YI),e(YI,c0o),e(Fv,m0o),e(D,f0o),e(D,Cv),e(Cv,fae),e(fae,g0o),e(Cv,h0o),e(Cv,KI),e(KI,u0o),e(Cv,p0o),e(D,_0o),e(D,Mv),e(Mv,gae),e(gae,b0o),e(Mv,v0o),e(Mv,ZI),e(ZI,T0o),e(Mv,F0o),e(D,C0o),e(D,Ev),e(Ev,hae),e(hae,M0o),e(Ev,E0o),e(Ev,ej),e(ej,y0o),e(Ev,w0o),e(D,A0o),e(D,yv),e(yv,uae),e(uae,L0o),e(yv,B0o),e(yv,oj),e(oj,k0o),e(yv,x0o),e(D,R0o),e(D,wv),e(wv,pae),e(pae,S0o),e(wv,P0o),e(wv,rj),e(rj,$0o),e(wv,I0o),e(D,j0o),e(D,Av),e(Av,_ae),e(_ae,N0o),e(Av,D0o),e(Av,tj),e(tj,q0o),e(Av,G0o),e(Ne,O0o),e(Ne,Lv),e(Lv,X0o),e(Lv,bae),e(bae,z0o),e(Lv,V0o),e(Lv,vae),e(vae,W0o),e(Ne,Q0o),e(Ne,Tae),e(Tae,H0o),e(Ne,U0o),g(Xy,Ne,null),b(d,P8e,_),b(d,gd,_),e(gd,Bv),e(Bv,Fae),g(zy,Fae,null),e(gd,J0o),e(gd,Cae),e(Cae,Y0o),b(d,$8e,_),b(d,er,_),g(Vy,er,null),e(er,K0o),e(er,hd),e(hd,Z0o),e(hd,Mae),e(Mae,eLo),e(hd,oLo),e(hd,Eae),e(Eae,rLo),e(hd,tLo),e(er,aLo),e(er,Wy),e(Wy,sLo),e(Wy,yae),e(yae,nLo),e(Wy,lLo),e(er,iLo),e(er,Qr),g(Qy,Qr,null),e(Qr,dLo),e(Qr,wae),e(wae,cLo),e(Qr,mLo),e(Qr,ud),e(ud,fLo),e(ud,Aae),e(Aae,gLo),e(ud,hLo),e(ud,Lae),e(Lae,uLo),e(ud,pLo),e(Qr,_Lo),e(Qr,Bae),e(Bae,bLo),e(Qr,vLo),g(Hy,Qr,null),e(er,TLo),e(er,De),g(Uy,De,null),e(De,FLo),e(De,kae),e(kae,CLo),e(De,MLo),e(De,Ha),e(Ha,ELo),e(Ha,xae),e(xae,yLo),e(Ha,wLo),e(Ha,Rae),e(Rae,ALo),e(Ha,LLo),e(Ha,Sae),e(Sae,BLo),e(Ha,kLo),e(De,xLo),e(De,R),e(R,kv),e(kv,Pae),e(Pae,RLo),e(kv,SLo),e(kv,aj),e(aj,PLo),e(kv,$Lo),e(R,ILo),e(R,xv),e(xv,$ae),e($ae,jLo),e(xv,NLo),e(xv,sj),e(sj,DLo),e(xv,qLo),e(R,GLo),e(R,Rv),e(Rv,Iae),e(Iae,OLo),e(Rv,XLo),e(Rv,nj),e(nj,zLo),e(Rv,VLo),e(R,WLo),e(R,Sv),e(Sv,jae),e(jae,QLo),e(Sv,HLo),e(Sv,lj),e(lj,ULo),e(Sv,JLo),e(R,YLo),e(R,Pv),e(Pv,Nae),e(Nae,KLo),e(Pv,ZLo),e(Pv,ij),e(ij,e7o),e(Pv,o7o),e(R,r7o),e(R,$v),e($v,Dae),e(Dae,t7o),e($v,a7o),e($v,dj),e(dj,s7o),e($v,n7o),e(R,l7o),e(R,Iv),e(Iv,qae),e(qae,i7o),e(Iv,d7o),e(Iv,cj),e(cj,c7o),e(Iv,m7o),e(R,f7o),e(R,jv),e(jv,Gae),e(Gae,g7o),e(jv,h7o),e(jv,mj),e(mj,u7o),e(jv,p7o),e(R,_7o),e(R,Nv),e(Nv,Oae),e(Oae,b7o),e(Nv,v7o),e(Nv,fj),e(fj,T7o),e(Nv,F7o),e(R,C7o),e(R,Dv),e(Dv,Xae),e(Xae,M7o),e(Dv,E7o),e(Dv,gj),e(gj,y7o),e(Dv,w7o),e(R,A7o),e(R,qv),e(qv,zae),e(zae,L7o),e(qv,B7o),e(qv,hj),e(hj,k7o),e(qv,x7o),e(R,R7o),e(R,Gv),e(Gv,Vae),e(Vae,S7o),e(Gv,P7o),e(Gv,uj),e(uj,$7o),e(Gv,I7o),e(R,j7o),e(R,Ov),e(Ov,Wae),e(Wae,N7o),e(Ov,D7o),e(Ov,pj),e(pj,q7o),e(Ov,G7o),e(R,O7o),e(R,Xv),e(Xv,Qae),e(Qae,X7o),e(Xv,z7o),e(Xv,_j),e(_j,V7o),e(Xv,W7o),e(R,Q7o),e(R,zv),e(zv,Hae),e(Hae,H7o),e(zv,U7o),e(zv,bj),e(bj,J7o),e(zv,Y7o),e(R,K7o),e(R,Vv),e(Vv,Uae),e(Uae,Z7o),e(Vv,e8o),e(Vv,vj),e(vj,o8o),e(Vv,r8o),e(R,t8o),e(R,Wv),e(Wv,Jae),e(Jae,a8o),e(Wv,s8o),e(Wv,Tj),e(Tj,n8o),e(Wv,l8o),e(R,i8o),e(R,Qv),e(Qv,Yae),e(Yae,d8o),e(Qv,c8o),e(Qv,Fj),e(Fj,m8o),e(Qv,f8o),e(R,g8o),e(R,Hv),e(Hv,Kae),e(Kae,h8o),e(Hv,u8o),e(Hv,Cj),e(Cj,p8o),e(Hv,_8o),e(R,b8o),e(R,Uv),e(Uv,Zae),e(Zae,v8o),e(Uv,T8o),e(Uv,Mj),e(Mj,F8o),e(Uv,C8o),e(R,M8o),e(R,Jv),e(Jv,ese),e(ese,E8o),e(Jv,y8o),e(Jv,Ej),e(Ej,w8o),e(Jv,A8o),e(R,L8o),e(R,Yv),e(Yv,ose),e(ose,B8o),e(Yv,k8o),e(Yv,yj),e(yj,x8o),e(Yv,R8o),e(R,S8o),e(R,Kv),e(Kv,rse),e(rse,P8o),e(Kv,$8o),e(Kv,wj),e(wj,I8o),e(Kv,j8o),e(R,N8o),e(R,Zv),e(Zv,tse),e(tse,D8o),e(Zv,q8o),e(Zv,Aj),e(Aj,G8o),e(Zv,O8o),e(R,X8o),e(R,eT),e(eT,ase),e(ase,z8o),e(eT,V8o),e(eT,Lj),e(Lj,W8o),e(eT,Q8o),e(R,H8o),e(R,oT),e(oT,sse),e(sse,U8o),e(oT,J8o),e(oT,Bj),e(Bj,Y8o),e(oT,K8o),e(R,Z8o),e(R,rT),e(rT,nse),e(nse,e9o),e(rT,o9o),e(rT,kj),e(kj,r9o),e(rT,t9o),e(R,a9o),e(R,tT),e(tT,lse),e(lse,s9o),e(tT,n9o),e(tT,xj),e(xj,l9o),e(tT,i9o),e(R,d9o),e(R,aT),e(aT,ise),e(ise,c9o),e(aT,m9o),e(aT,Rj),e(Rj,f9o),e(aT,g9o),e(R,h9o),e(R,sT),e(sT,dse),e(dse,u9o),e(sT,p9o),e(sT,Sj),e(Sj,_9o),e(sT,b9o),e(R,v9o),e(R,nT),e(nT,cse),e(cse,T9o),e(nT,F9o),e(nT,Pj),e(Pj,C9o),e(nT,M9o),e(R,E9o),e(R,lT),e(lT,mse),e(mse,y9o),e(lT,w9o),e(lT,$j),e($j,A9o),e(lT,L9o),e(R,B9o),e(R,iT),e(iT,fse),e(fse,k9o),e(iT,x9o),e(iT,Ij),e(Ij,R9o),e(iT,S9o),e(R,P9o),e(R,dT),e(dT,gse),e(gse,$9o),e(dT,I9o),e(dT,jj),e(jj,j9o),e(dT,N9o),e(R,D9o),e(R,cT),e(cT,hse),e(hse,q9o),e(cT,G9o),e(cT,Nj),e(Nj,O9o),e(cT,X9o),e(R,z9o),e(R,mT),e(mT,use),e(use,V9o),e(mT,W9o),e(mT,Dj),e(Dj,Q9o),e(mT,H9o),e(R,U9o),e(R,fT),e(fT,pse),e(pse,J9o),e(fT,Y9o),e(fT,qj),e(qj,K9o),e(fT,Z9o),e(R,eBo),e(R,gT),e(gT,_se),e(_se,oBo),e(gT,rBo),e(gT,Gj),e(Gj,tBo),e(gT,aBo),e(De,sBo),e(De,hT),e(hT,nBo),e(hT,bse),e(bse,lBo),e(hT,iBo),e(hT,vse),e(vse,dBo),e(De,cBo),e(De,Tse),e(Tse,mBo),e(De,fBo),g(Jy,De,null),b(d,I8e,_),b(d,pd,_),e(pd,uT),e(uT,Fse),g(Yy,Fse,null),e(pd,gBo),e(pd,Cse),e(Cse,hBo),b(d,j8e,_),b(d,or,_),g(Ky,or,null),e(or,uBo),e(or,_d),e(_d,pBo),e(_d,Mse),e(Mse,_Bo),e(_d,bBo),e(_d,Ese),e(Ese,vBo),e(_d,TBo),e(or,FBo),e(or,Zy),e(Zy,CBo),e(Zy,yse),e(yse,MBo),e(Zy,EBo),e(or,yBo),e(or,Hr),g(ew,Hr,null),e(Hr,wBo),e(Hr,wse),e(wse,ABo),e(Hr,LBo),e(Hr,bd),e(bd,BBo),e(bd,Ase),e(Ase,kBo),e(bd,xBo),e(bd,Lse),e(Lse,RBo),e(bd,SBo),e(Hr,PBo),e(Hr,Bse),e(Bse,$Bo),e(Hr,IBo),g(ow,Hr,null),e(or,jBo),e(or,qe),g(rw,qe,null),e(qe,NBo),e(qe,kse),e(kse,DBo),e(qe,qBo),e(qe,Ua),e(Ua,GBo),e(Ua,xse),e(xse,OBo),e(Ua,XBo),e(Ua,Rse),e(Rse,zBo),e(Ua,VBo),e(Ua,Sse),e(Sse,WBo),e(Ua,QBo),e(qe,HBo),e(qe,Pse),e(Pse,pT),e(pT,$se),e($se,UBo),e(pT,JBo),e(pT,Oj),e(Oj,YBo),e(pT,KBo),e(qe,ZBo),e(qe,_T),e(_T,eko),e(_T,Ise),e(Ise,oko),e(_T,rko),e(_T,jse),e(jse,tko),e(qe,ako),e(qe,Nse),e(Nse,sko),e(qe,nko),g(tw,qe,null),b(d,N8e,_),b(d,vd,_),e(vd,bT),e(bT,Dse),g(aw,Dse,null),e(vd,lko),e(vd,qse),e(qse,iko),b(d,D8e,_),b(d,rr,_),g(sw,rr,null),e(rr,dko),e(rr,Td),e(Td,cko),e(Td,Gse),e(Gse,mko),e(Td,fko),e(Td,Ose),e(Ose,gko),e(Td,hko),e(rr,uko),e(rr,nw),e(nw,pko),e(nw,Xse),e(Xse,_ko),e(nw,bko),e(rr,vko),e(rr,Ur),g(lw,Ur,null),e(Ur,Tko),e(Ur,zse),e(zse,Fko),e(Ur,Cko),e(Ur,Fd),e(Fd,Mko),e(Fd,Vse),e(Vse,Eko),e(Fd,yko),e(Fd,Wse),e(Wse,wko),e(Fd,Ako),e(Ur,Lko),e(Ur,Qse),e(Qse,Bko),e(Ur,kko),g(iw,Ur,null),e(rr,xko),e(rr,Ge),g(dw,Ge,null),e(Ge,Rko),e(Ge,Hse),e(Hse,Sko),e(Ge,Pko),e(Ge,Ja),e(Ja,$ko),e(Ja,Use),e(Use,Iko),e(Ja,jko),e(Ja,Jse),e(Jse,Nko),e(Ja,Dko),e(Ja,Yse),e(Yse,qko),e(Ja,Gko),e(Ge,Oko),e(Ge,be),e(be,vT),e(vT,Kse),e(Kse,Xko),e(vT,zko),e(vT,Xj),e(Xj,Vko),e(vT,Wko),e(be,Qko),e(be,TT),e(TT,Zse),e(Zse,Hko),e(TT,Uko),e(TT,zj),e(zj,Jko),e(TT,Yko),e(be,Kko),e(be,Rn),e(Rn,ene),e(ene,Zko),e(Rn,exo),e(Rn,Vj),e(Vj,oxo),e(Rn,rxo),e(Rn,Wj),e(Wj,txo),e(Rn,axo),e(be,sxo),e(be,FT),e(FT,one),e(one,nxo),e(FT,lxo),e(FT,Qj),e(Qj,ixo),e(FT,dxo),e(be,cxo),e(be,la),e(la,rne),e(rne,mxo),e(la,fxo),e(la,Hj),e(Hj,gxo),e(la,hxo),e(la,Uj),e(Uj,uxo),e(la,pxo),e(la,Jj),e(Jj,_xo),e(la,bxo),e(be,vxo),e(be,CT),e(CT,tne),e(tne,Txo),e(CT,Fxo),e(CT,Yj),e(Yj,Cxo),e(CT,Mxo),e(be,Exo),e(be,MT),e(MT,ane),e(ane,yxo),e(MT,wxo),e(MT,Kj),e(Kj,Axo),e(MT,Lxo),e(be,Bxo),e(be,ET),e(ET,sne),e(sne,kxo),e(ET,xxo),e(ET,Zj),e(Zj,Rxo),e(ET,Sxo),e(be,Pxo),e(be,yT),e(yT,nne),e(nne,$xo),e(yT,Ixo),e(yT,eN),e(eN,jxo),e(yT,Nxo),e(Ge,Dxo),e(Ge,wT),e(wT,qxo),e(wT,lne),e(lne,Gxo),e(wT,Oxo),e(wT,ine),e(ine,Xxo),e(Ge,zxo),e(Ge,dne),e(dne,Vxo),e(Ge,Wxo),g(cw,Ge,null),b(d,q8e,_),b(d,Cd,_),e(Cd,AT),e(AT,cne),g(mw,cne,null),e(Cd,Qxo),e(Cd,mne),e(mne,Hxo),b(d,G8e,_),b(d,tr,_),g(fw,tr,null),e(tr,Uxo),e(tr,Md),e(Md,Jxo),e(Md,fne),e(fne,Yxo),e(Md,Kxo),e(Md,gne),e(gne,Zxo),e(Md,eRo),e(tr,oRo),e(tr,gw),e(gw,rRo),e(gw,hne),e(hne,tRo),e(gw,aRo),e(tr,sRo),e(tr,Jr),g(hw,Jr,null),e(Jr,nRo),e(Jr,une),e(une,lRo),e(Jr,iRo),e(Jr,Ed),e(Ed,dRo),e(Ed,pne),e(pne,cRo),e(Ed,mRo),e(Ed,_ne),e(_ne,fRo),e(Ed,gRo),e(Jr,hRo),e(Jr,bne),e(bne,uRo),e(Jr,pRo),g(uw,Jr,null),e(tr,_Ro),e(tr,Oe),g(pw,Oe,null),e(Oe,bRo),e(Oe,vne),e(vne,vRo),e(Oe,TRo),e(Oe,Ya),e(Ya,FRo),e(Ya,Tne),e(Tne,CRo),e(Ya,MRo),e(Ya,Fne),e(Fne,ERo),e(Ya,yRo),e(Ya,Cne),e(Cne,wRo),e(Ya,ARo),e(Oe,LRo),e(Oe,Mne),e(Mne,LT),e(LT,Ene),e(Ene,BRo),e(LT,kRo),e(LT,oN),e(oN,xRo),e(LT,RRo),e(Oe,SRo),e(Oe,BT),e(BT,PRo),e(BT,yne),e(yne,$Ro),e(BT,IRo),e(BT,wne),e(wne,jRo),e(Oe,NRo),e(Oe,Ane),e(Ane,DRo),e(Oe,qRo),g(_w,Oe,null),b(d,O8e,_),b(d,yd,_),e(yd,kT),e(kT,Lne),g(bw,Lne,null),e(yd,GRo),e(yd,Bne),e(Bne,ORo),b(d,X8e,_),b(d,ar,_),g(vw,ar,null),e(ar,XRo),e(ar,wd),e(wd,zRo),e(wd,kne),e(kne,VRo),e(wd,WRo),e(wd,xne),e(xne,QRo),e(wd,HRo),e(ar,URo),e(ar,Tw),e(Tw,JRo),e(Tw,Rne),e(Rne,YRo),e(Tw,KRo),e(ar,ZRo),e(ar,Yr),g(Fw,Yr,null),e(Yr,eSo),e(Yr,Sne),e(Sne,oSo),e(Yr,rSo),e(Yr,Ad),e(Ad,tSo),e(Ad,Pne),e(Pne,aSo),e(Ad,sSo),e(Ad,$ne),e($ne,nSo),e(Ad,lSo),e(Yr,iSo),e(Yr,Ine),e(Ine,dSo),e(Yr,cSo),g(Cw,Yr,null),e(ar,mSo),e(ar,Xe),g(Mw,Xe,null),e(Xe,fSo),e(Xe,jne),e(jne,gSo),e(Xe,hSo),e(Xe,Ka),e(Ka,uSo),e(Ka,Nne),e(Nne,pSo),e(Ka,_So),e(Ka,Dne),e(Dne,bSo),e(Ka,vSo),e(Ka,qne),e(qne,TSo),e(Ka,FSo),e(Xe,CSo),e(Xe,ao),e(ao,xT),e(xT,Gne),e(Gne,MSo),e(xT,ESo),e(xT,rN),e(rN,ySo),e(xT,wSo),e(ao,ASo),e(ao,RT),e(RT,One),e(One,LSo),e(RT,BSo),e(RT,tN),e(tN,kSo),e(RT,xSo),e(ao,RSo),e(ao,ST),e(ST,Xne),e(Xne,SSo),e(ST,PSo),e(ST,aN),e(aN,$So),e(ST,ISo),e(ao,jSo),e(ao,PT),e(PT,zne),e(zne,NSo),e(PT,DSo),e(PT,sN),e(sN,qSo),e(PT,GSo),e(ao,OSo),e(ao,$T),e($T,Vne),e(Vne,XSo),e($T,zSo),e($T,nN),e(nN,VSo),e($T,WSo),e(ao,QSo),e(ao,IT),e(IT,Wne),e(Wne,HSo),e(IT,USo),e(IT,lN),e(lN,JSo),e(IT,YSo),e(ao,KSo),e(ao,jT),e(jT,Qne),e(Qne,ZSo),e(jT,ePo),e(jT,iN),e(iN,oPo),e(jT,rPo),e(Xe,tPo),e(Xe,NT),e(NT,aPo),e(NT,Hne),e(Hne,sPo),e(NT,nPo),e(NT,Une),e(Une,lPo),e(Xe,iPo),e(Xe,Jne),e(Jne,dPo),e(Xe,cPo),g(Ew,Xe,null),b(d,z8e,_),b(d,Ld,_),e(Ld,DT),e(DT,Yne),g(yw,Yne,null),e(Ld,mPo),e(Ld,Kne),e(Kne,fPo),b(d,V8e,_),b(d,sr,_),g(ww,sr,null),e(sr,gPo),e(sr,Bd),e(Bd,hPo),e(Bd,Zne),e(Zne,uPo),e(Bd,pPo),e(Bd,ele),e(ele,_Po),e(Bd,bPo),e(sr,vPo),e(sr,Aw),e(Aw,TPo),e(Aw,ole),e(ole,FPo),e(Aw,CPo),e(sr,MPo),e(sr,Kr),g(Lw,Kr,null),e(Kr,EPo),e(Kr,rle),e(rle,yPo),e(Kr,wPo),e(Kr,kd),e(kd,APo),e(kd,tle),e(tle,LPo),e(kd,BPo),e(kd,ale),e(ale,kPo),e(kd,xPo),e(Kr,RPo),e(Kr,sle),e(sle,SPo),e(Kr,PPo),g(Bw,Kr,null),e(sr,$Po),e(sr,ze),g(kw,ze,null),e(ze,IPo),e(ze,nle),e(nle,jPo),e(ze,NPo),e(ze,Za),e(Za,DPo),e(Za,lle),e(lle,qPo),e(Za,GPo),e(Za,ile),e(ile,OPo),e(Za,XPo),e(Za,dle),e(dle,zPo),e(Za,VPo),e(ze,WPo),e(ze,xd),e(xd,qT),e(qT,cle),e(cle,QPo),e(qT,HPo),e(qT,dN),e(dN,UPo),e(qT,JPo),e(xd,YPo),e(xd,GT),e(GT,mle),e(mle,KPo),e(GT,ZPo),e(GT,cN),e(cN,e$o),e(GT,o$o),e(xd,r$o),e(xd,OT),e(OT,fle),e(fle,t$o),e(OT,a$o),e(OT,mN),e(mN,s$o),e(OT,n$o),e(ze,l$o),e(ze,XT),e(XT,i$o),e(XT,gle),e(gle,d$o),e(XT,c$o),e(XT,hle),e(hle,m$o),e(ze,f$o),e(ze,ule),e(ule,g$o),e(ze,h$o),g(xw,ze,null),b(d,W8e,_),b(d,Rd,_),e(Rd,zT),e(zT,ple),g(Rw,ple,null),e(Rd,u$o),e(Rd,_le),e(_le,p$o),b(d,Q8e,_),b(d,nr,_),g(Sw,nr,null),e(nr,_$o),e(nr,Sd),e(Sd,b$o),e(Sd,ble),e(ble,v$o),e(Sd,T$o),e(Sd,vle),e(vle,F$o),e(Sd,C$o),e(nr,M$o),e(nr,Pw),e(Pw,E$o),e(Pw,Tle),e(Tle,y$o),e(Pw,w$o),e(nr,A$o),e(nr,Zr),g($w,Zr,null),e(Zr,L$o),e(Zr,Fle),e(Fle,B$o),e(Zr,k$o),e(Zr,Pd),e(Pd,x$o),e(Pd,Cle),e(Cle,R$o),e(Pd,S$o),e(Pd,Mle),e(Mle,P$o),e(Pd,$$o),e(Zr,I$o),e(Zr,Ele),e(Ele,j$o),e(Zr,N$o),g(Iw,Zr,null),e(nr,D$o),e(nr,Ve),g(jw,Ve,null),e(Ve,q$o),e(Ve,yle),e(yle,G$o),e(Ve,O$o),e(Ve,es),e(es,X$o),e(es,wle),e(wle,z$o),e(es,V$o),e(es,Ale),e(Ale,W$o),e(es,Q$o),e(es,Lle),e(Lle,H$o),e(es,U$o),e(Ve,J$o),e(Ve,so),e(so,VT),e(VT,Ble),e(Ble,Y$o),e(VT,K$o),e(VT,fN),e(fN,Z$o),e(VT,eIo),e(so,oIo),e(so,WT),e(WT,kle),e(kle,rIo),e(WT,tIo),e(WT,gN),e(gN,aIo),e(WT,sIo),e(so,nIo),e(so,QT),e(QT,xle),e(xle,lIo),e(QT,iIo),e(QT,hN),e(hN,dIo),e(QT,cIo),e(so,mIo),e(so,HT),e(HT,Rle),e(Rle,fIo),e(HT,gIo),e(HT,uN),e(uN,hIo),e(HT,uIo),e(so,pIo),e(so,UT),e(UT,Sle),e(Sle,_Io),e(UT,bIo),e(UT,pN),e(pN,vIo),e(UT,TIo),e(so,FIo),e(so,JT),e(JT,Ple),e(Ple,CIo),e(JT,MIo),e(JT,_N),e(_N,EIo),e(JT,yIo),e(so,wIo),e(so,YT),e(YT,$le),e($le,AIo),e(YT,LIo),e(YT,bN),e(bN,BIo),e(YT,kIo),e(Ve,xIo),e(Ve,KT),e(KT,RIo),e(KT,Ile),e(Ile,SIo),e(KT,PIo),e(KT,jle),e(jle,$Io),e(Ve,IIo),e(Ve,Nle),e(Nle,jIo),e(Ve,NIo),g(Nw,Ve,null),b(d,H8e,_),b(d,$d,_),e($d,ZT),e(ZT,Dle),g(Dw,Dle,null),e($d,DIo),e($d,qle),e(qle,qIo),b(d,U8e,_),b(d,lr,_),g(qw,lr,null),e(lr,GIo),e(lr,Id),e(Id,OIo),e(Id,Gle),e(Gle,XIo),e(Id,zIo),e(Id,Ole),e(Ole,VIo),e(Id,WIo),e(lr,QIo),e(lr,Gw),e(Gw,HIo),e(Gw,Xle),e(Xle,UIo),e(Gw,JIo),e(lr,YIo),e(lr,et),g(Ow,et,null),e(et,KIo),e(et,zle),e(zle,ZIo),e(et,ejo),e(et,jd),e(jd,ojo),e(jd,Vle),e(Vle,rjo),e(jd,tjo),e(jd,Wle),e(Wle,ajo),e(jd,sjo),e(et,njo),e(et,Qle),e(Qle,ljo),e(et,ijo),g(Xw,et,null),e(lr,djo),e(lr,We),g(zw,We,null),e(We,cjo),e(We,Hle),e(Hle,mjo),e(We,fjo),e(We,os),e(os,gjo),e(os,Ule),e(Ule,hjo),e(os,ujo),e(os,Jle),e(Jle,pjo),e(os,_jo),e(os,Yle),e(Yle,bjo),e(os,vjo),e(We,Tjo),e(We,Vw),e(Vw,e1),e(e1,Kle),e(Kle,Fjo),e(e1,Cjo),e(e1,vN),e(vN,Mjo),e(e1,Ejo),e(Vw,yjo),e(Vw,o1),e(o1,Zle),e(Zle,wjo),e(o1,Ajo),e(o1,TN),e(TN,Ljo),e(o1,Bjo),e(We,kjo),e(We,r1),e(r1,xjo),e(r1,eie),e(eie,Rjo),e(r1,Sjo),e(r1,oie),e(oie,Pjo),e(We,$jo),e(We,rie),e(rie,Ijo),e(We,jjo),g(Ww,We,null),b(d,J8e,_),b(d,Nd,_),e(Nd,t1),e(t1,tie),g(Qw,tie,null),e(Nd,Njo),e(Nd,aie),e(aie,Djo),b(d,Y8e,_),b(d,ir,_),g(Hw,ir,null),e(ir,qjo),e(ir,Dd),e(Dd,Gjo),e(Dd,sie),e(sie,Ojo),e(Dd,Xjo),e(Dd,nie),e(nie,zjo),e(Dd,Vjo),e(ir,Wjo),e(ir,Uw),e(Uw,Qjo),e(Uw,lie),e(lie,Hjo),e(Uw,Ujo),e(ir,Jjo),e(ir,ot),g(Jw,ot,null),e(ot,Yjo),e(ot,iie),e(iie,Kjo),e(ot,Zjo),e(ot,qd),e(qd,eNo),e(qd,die),e(die,oNo),e(qd,rNo),e(qd,cie),e(cie,tNo),e(qd,aNo),e(ot,sNo),e(ot,mie),e(mie,nNo),e(ot,lNo),g(Yw,ot,null),e(ir,iNo),e(ir,Qe),g(Kw,Qe,null),e(Qe,dNo),e(Qe,fie),e(fie,cNo),e(Qe,mNo),e(Qe,rs),e(rs,fNo),e(rs,gie),e(gie,gNo),e(rs,hNo),e(rs,hie),e(hie,uNo),e(rs,pNo),e(rs,uie),e(uie,_No),e(rs,bNo),e(Qe,vNo),e(Qe,Gd),e(Gd,a1),e(a1,pie),e(pie,TNo),e(a1,FNo),e(a1,FN),e(FN,CNo),e(a1,MNo),e(Gd,ENo),e(Gd,s1),e(s1,_ie),e(_ie,yNo),e(s1,wNo),e(s1,CN),e(CN,ANo),e(s1,LNo),e(Gd,BNo),e(Gd,n1),e(n1,bie),e(bie,kNo),e(n1,xNo),e(n1,MN),e(MN,RNo),e(n1,SNo),e(Qe,PNo),e(Qe,l1),e(l1,$No),e(l1,vie),e(vie,INo),e(l1,jNo),e(l1,Tie),e(Tie,NNo),e(Qe,DNo),e(Qe,Fie),e(Fie,qNo),e(Qe,GNo),g(Zw,Qe,null),b(d,K8e,_),b(d,Od,_),e(Od,i1),e(i1,Cie),g(eA,Cie,null),e(Od,ONo),e(Od,Mie),e(Mie,XNo),b(d,Z8e,_),b(d,dr,_),g(oA,dr,null),e(dr,zNo),e(dr,Xd),e(Xd,VNo),e(Xd,Eie),e(Eie,WNo),e(Xd,QNo),e(Xd,yie),e(yie,HNo),e(Xd,UNo),e(dr,JNo),e(dr,rA),e(rA,YNo),e(rA,wie),e(wie,KNo),e(rA,ZNo),e(dr,eDo),e(dr,rt),g(tA,rt,null),e(rt,oDo),e(rt,Aie),e(Aie,rDo),e(rt,tDo),e(rt,zd),e(zd,aDo),e(zd,Lie),e(Lie,sDo),e(zd,nDo),e(zd,Bie),e(Bie,lDo),e(zd,iDo),e(rt,dDo),e(rt,kie),e(kie,cDo),e(rt,mDo),g(aA,rt,null),e(dr,fDo),e(dr,He),g(sA,He,null),e(He,gDo),e(He,xie),e(xie,hDo),e(He,uDo),e(He,ts),e(ts,pDo),e(ts,Rie),e(Rie,_Do),e(ts,bDo),e(ts,Sie),e(Sie,vDo),e(ts,TDo),e(ts,Pie),e(Pie,FDo),e(ts,CDo),e(He,MDo),e(He,Vd),e(Vd,d1),e(d1,$ie),e($ie,EDo),e(d1,yDo),e(d1,EN),e(EN,wDo),e(d1,ADo),e(Vd,LDo),e(Vd,c1),e(c1,Iie),e(Iie,BDo),e(c1,kDo),e(c1,yN),e(yN,xDo),e(c1,RDo),e(Vd,SDo),e(Vd,m1),e(m1,jie),e(jie,PDo),e(m1,$Do),e(m1,wN),e(wN,IDo),e(m1,jDo),e(He,NDo),e(He,f1),e(f1,DDo),e(f1,Nie),e(Nie,qDo),e(f1,GDo),e(f1,Die),e(Die,ODo),e(He,XDo),e(He,qie),e(qie,zDo),e(He,VDo),g(nA,He,null),b(d,e9e,_),b(d,Wd,_),e(Wd,g1),e(g1,Gie),g(lA,Gie,null),e(Wd,WDo),e(Wd,Oie),e(Oie,QDo),b(d,o9e,_),b(d,cr,_),g(iA,cr,null),e(cr,HDo),e(cr,Qd),e(Qd,UDo),e(Qd,Xie),e(Xie,JDo),e(Qd,YDo),e(Qd,zie),e(zie,KDo),e(Qd,ZDo),e(cr,eqo),e(cr,dA),e(dA,oqo),e(dA,Vie),e(Vie,rqo),e(dA,tqo),e(cr,aqo),e(cr,tt),g(cA,tt,null),e(tt,sqo),e(tt,Wie),e(Wie,nqo),e(tt,lqo),e(tt,Hd),e(Hd,iqo),e(Hd,Qie),e(Qie,dqo),e(Hd,cqo),e(Hd,Hie),e(Hie,mqo),e(Hd,fqo),e(tt,gqo),e(tt,Uie),e(Uie,hqo),e(tt,uqo),g(mA,tt,null),e(cr,pqo),e(cr,Ue),g(fA,Ue,null),e(Ue,_qo),e(Ue,Jie),e(Jie,bqo),e(Ue,vqo),e(Ue,as),e(as,Tqo),e(as,Yie),e(Yie,Fqo),e(as,Cqo),e(as,Kie),e(Kie,Mqo),e(as,Eqo),e(as,Zie),e(Zie,yqo),e(as,wqo),e(Ue,Aqo),e(Ue,ede),e(ede,h1),e(h1,ode),e(ode,Lqo),e(h1,Bqo),e(h1,AN),e(AN,kqo),e(h1,xqo),e(Ue,Rqo),e(Ue,u1),e(u1,Sqo),e(u1,rde),e(rde,Pqo),e(u1,$qo),e(u1,tde),e(tde,Iqo),e(Ue,jqo),e(Ue,ade),e(ade,Nqo),e(Ue,Dqo),g(gA,Ue,null),b(d,r9e,_),b(d,Ud,_),e(Ud,p1),e(p1,sde),g(hA,sde,null),e(Ud,qqo),e(Ud,nde),e(nde,Gqo),b(d,t9e,_),b(d,mr,_),g(uA,mr,null),e(mr,Oqo),e(mr,Jd),e(Jd,Xqo),e(Jd,lde),e(lde,zqo),e(Jd,Vqo),e(Jd,ide),e(ide,Wqo),e(Jd,Qqo),e(mr,Hqo),e(mr,pA),e(pA,Uqo),e(pA,dde),e(dde,Jqo),e(pA,Yqo),e(mr,Kqo),e(mr,at),g(_A,at,null),e(at,Zqo),e(at,cde),e(cde,eGo),e(at,oGo),e(at,Yd),e(Yd,rGo),e(Yd,mde),e(mde,tGo),e(Yd,aGo),e(Yd,fde),e(fde,sGo),e(Yd,nGo),e(at,lGo),e(at,gde),e(gde,iGo),e(at,dGo),g(bA,at,null),e(mr,cGo),e(mr,Je),g(vA,Je,null),e(Je,mGo),e(Je,hde),e(hde,fGo),e(Je,gGo),e(Je,ss),e(ss,hGo),e(ss,ude),e(ude,uGo),e(ss,pGo),e(ss,pde),e(pde,_Go),e(ss,bGo),e(ss,_de),e(_de,vGo),e(ss,TGo),e(Je,FGo),e(Je,bde),e(bde,_1),e(_1,vde),e(vde,CGo),e(_1,MGo),e(_1,LN),e(LN,EGo),e(_1,yGo),e(Je,wGo),e(Je,b1),e(b1,AGo),e(b1,Tde),e(Tde,LGo),e(b1,BGo),e(b1,Fde),e(Fde,kGo),e(Je,xGo),e(Je,Cde),e(Cde,RGo),e(Je,SGo),g(TA,Je,null),b(d,a9e,_),b(d,Kd,_),e(Kd,v1),e(v1,Mde),g(FA,Mde,null),e(Kd,PGo),e(Kd,Ede),e(Ede,$Go),b(d,s9e,_),b(d,fr,_),g(CA,fr,null),e(fr,IGo),e(fr,Zd),e(Zd,jGo),e(Zd,yde),e(yde,NGo),e(Zd,DGo),e(Zd,wde),e(wde,qGo),e(Zd,GGo),e(fr,OGo),e(fr,MA),e(MA,XGo),e(MA,Ade),e(Ade,zGo),e(MA,VGo),e(fr,WGo),e(fr,st),g(EA,st,null),e(st,QGo),e(st,Lde),e(Lde,HGo),e(st,UGo),e(st,ec),e(ec,JGo),e(ec,Bde),e(Bde,YGo),e(ec,KGo),e(ec,kde),e(kde,ZGo),e(ec,eOo),e(st,oOo),e(st,xde),e(xde,rOo),e(st,tOo),g(yA,st,null),e(fr,aOo),e(fr,Ye),g(wA,Ye,null),e(Ye,sOo),e(Ye,Rde),e(Rde,nOo),e(Ye,lOo),e(Ye,ns),e(ns,iOo),e(ns,Sde),e(Sde,dOo),e(ns,cOo),e(ns,Pde),e(Pde,mOo),e(ns,fOo),e(ns,$de),e($de,gOo),e(ns,hOo),e(Ye,uOo),e(Ye,AA),e(AA,T1),e(T1,Ide),e(Ide,pOo),e(T1,_Oo),e(T1,BN),e(BN,bOo),e(T1,vOo),e(AA,TOo),e(AA,F1),e(F1,jde),e(jde,FOo),e(F1,COo),e(F1,kN),e(kN,MOo),e(F1,EOo),e(Ye,yOo),e(Ye,C1),e(C1,wOo),e(C1,Nde),e(Nde,AOo),e(C1,LOo),e(C1,Dde),e(Dde,BOo),e(Ye,kOo),e(Ye,qde),e(qde,xOo),e(Ye,ROo),g(LA,Ye,null),b(d,n9e,_),b(d,oc,_),e(oc,M1),e(M1,Gde),g(BA,Gde,null),e(oc,SOo),e(oc,Ode),e(Ode,POo),b(d,l9e,_),b(d,gr,_),g(kA,gr,null),e(gr,$Oo),e(gr,rc),e(rc,IOo),e(rc,Xde),e(Xde,jOo),e(rc,NOo),e(rc,zde),e(zde,DOo),e(rc,qOo),e(gr,GOo),e(gr,xA),e(xA,OOo),e(xA,Vde),e(Vde,XOo),e(xA,zOo),e(gr,VOo),e(gr,nt),g(RA,nt,null),e(nt,WOo),e(nt,Wde),e(Wde,QOo),e(nt,HOo),e(nt,tc),e(tc,UOo),e(tc,Qde),e(Qde,JOo),e(tc,YOo),e(tc,Hde),e(Hde,KOo),e(tc,ZOo),e(nt,eXo),e(nt,Ude),e(Ude,oXo),e(nt,rXo),g(SA,nt,null),e(gr,tXo),e(gr,go),g(PA,go,null),e(go,aXo),e(go,Jde),e(Jde,sXo),e(go,nXo),e(go,ls),e(ls,lXo),e(ls,Yde),e(Yde,iXo),e(ls,dXo),e(ls,Kde),e(Kde,cXo),e(ls,mXo),e(ls,Zde),e(Zde,fXo),e(ls,gXo),e(go,hXo),e(go,B),e(B,E1),e(E1,ece),e(ece,uXo),e(E1,pXo),e(E1,xN),e(xN,_Xo),e(E1,bXo),e(B,vXo),e(B,y1),e(y1,oce),e(oce,TXo),e(y1,FXo),e(y1,RN),e(RN,CXo),e(y1,MXo),e(B,EXo),e(B,w1),e(w1,rce),e(rce,yXo),e(w1,wXo),e(w1,SN),e(SN,AXo),e(w1,LXo),e(B,BXo),e(B,A1),e(A1,tce),e(tce,kXo),e(A1,xXo),e(A1,PN),e(PN,RXo),e(A1,SXo),e(B,PXo),e(B,L1),e(L1,ace),e(ace,$Xo),e(L1,IXo),e(L1,$N),e($N,jXo),e(L1,NXo),e(B,DXo),e(B,B1),e(B1,sce),e(sce,qXo),e(B1,GXo),e(B1,IN),e(IN,OXo),e(B1,XXo),e(B,zXo),e(B,k1),e(k1,nce),e(nce,VXo),e(k1,WXo),e(k1,jN),e(jN,QXo),e(k1,HXo),e(B,UXo),e(B,x1),e(x1,lce),e(lce,JXo),e(x1,YXo),e(x1,NN),e(NN,KXo),e(x1,ZXo),e(B,ezo),e(B,R1),e(R1,ice),e(ice,ozo),e(R1,rzo),e(R1,DN),e(DN,tzo),e(R1,azo),e(B,szo),e(B,S1),e(S1,dce),e(dce,nzo),e(S1,lzo),e(S1,qN),e(qN,izo),e(S1,dzo),e(B,czo),e(B,P1),e(P1,cce),e(cce,mzo),e(P1,fzo),e(P1,GN),e(GN,gzo),e(P1,hzo),e(B,uzo),e(B,$1),e($1,mce),e(mce,pzo),e($1,_zo),e($1,ON),e(ON,bzo),e($1,vzo),e(B,Tzo),e(B,I1),e(I1,fce),e(fce,Fzo),e(I1,Czo),e(I1,XN),e(XN,Mzo),e(I1,Ezo),e(B,yzo),e(B,j1),e(j1,gce),e(gce,wzo),e(j1,Azo),e(j1,zN),e(zN,Lzo),e(j1,Bzo),e(B,kzo),e(B,N1),e(N1,hce),e(hce,xzo),e(N1,Rzo),e(N1,VN),e(VN,Szo),e(N1,Pzo),e(B,$zo),e(B,Sn),e(Sn,uce),e(uce,Izo),e(Sn,jzo),e(Sn,WN),e(WN,Nzo),e(Sn,Dzo),e(Sn,QN),e(QN,qzo),e(Sn,Gzo),e(B,Ozo),e(B,D1),e(D1,pce),e(pce,Xzo),e(D1,zzo),e(D1,HN),e(HN,Vzo),e(D1,Wzo),e(B,Qzo),e(B,q1),e(q1,_ce),e(_ce,Hzo),e(q1,Uzo),e(q1,UN),e(UN,Jzo),e(q1,Yzo),e(B,Kzo),e(B,G1),e(G1,bce),e(bce,Zzo),e(G1,eVo),e(G1,JN),e(JN,oVo),e(G1,rVo),e(B,tVo),e(B,O1),e(O1,vce),e(vce,aVo),e(O1,sVo),e(O1,YN),e(YN,nVo),e(O1,lVo),e(B,iVo),e(B,X1),e(X1,Tce),e(Tce,dVo),e(X1,cVo),e(X1,KN),e(KN,mVo),e(X1,fVo),e(B,gVo),e(B,z1),e(z1,Fce),e(Fce,hVo),e(z1,uVo),e(z1,ZN),e(ZN,pVo),e(z1,_Vo),e(B,bVo),e(B,V1),e(V1,Cce),e(Cce,vVo),e(V1,TVo),e(V1,eD),e(eD,FVo),e(V1,CVo),e(B,MVo),e(B,W1),e(W1,Mce),e(Mce,EVo),e(W1,yVo),e(W1,oD),e(oD,wVo),e(W1,AVo),e(B,LVo),e(B,Q1),e(Q1,Ece),e(Ece,BVo),e(Q1,kVo),e(Q1,rD),e(rD,xVo),e(Q1,RVo),e(B,SVo),e(B,H1),e(H1,yce),e(yce,PVo),e(H1,$Vo),e(H1,tD),e(tD,IVo),e(H1,jVo),e(B,NVo),e(B,U1),e(U1,wce),e(wce,DVo),e(U1,qVo),e(U1,aD),e(aD,GVo),e(U1,OVo),e(B,XVo),e(B,J1),e(J1,Ace),e(Ace,zVo),e(J1,VVo),e(J1,sD),e(sD,WVo),e(J1,QVo),e(B,HVo),e(B,Y1),e(Y1,Lce),e(Lce,UVo),e(Y1,JVo),e(Y1,nD),e(nD,YVo),e(Y1,KVo),e(B,ZVo),e(B,K1),e(K1,Bce),e(Bce,eWo),e(K1,oWo),e(K1,lD),e(lD,rWo),e(K1,tWo),e(B,aWo),e(B,Z1),e(Z1,kce),e(kce,sWo),e(Z1,nWo),e(Z1,iD),e(iD,lWo),e(Z1,iWo),e(B,dWo),e(B,eF),e(eF,xce),e(xce,cWo),e(eF,mWo),e(eF,dD),e(dD,fWo),e(eF,gWo),e(B,hWo),e(B,oF),e(oF,Rce),e(Rce,uWo),e(oF,pWo),e(oF,cD),e(cD,_Wo),e(oF,bWo),e(B,vWo),e(B,rF),e(rF,Sce),e(Sce,TWo),e(rF,FWo),e(rF,mD),e(mD,CWo),e(rF,MWo),e(B,EWo),e(B,tF),e(tF,Pce),e(Pce,yWo),e(tF,wWo),e(tF,fD),e(fD,AWo),e(tF,LWo),e(B,BWo),e(B,aF),e(aF,$ce),e($ce,kWo),e(aF,xWo),e(aF,gD),e(gD,RWo),e(aF,SWo),e(B,PWo),e(B,sF),e(sF,Ice),e(Ice,$Wo),e(sF,IWo),e(sF,hD),e(hD,jWo),e(sF,NWo),e(B,DWo),e(B,nF),e(nF,jce),e(jce,qWo),e(nF,GWo),e(nF,uD),e(uD,OWo),e(nF,XWo),e(B,zWo),e(B,lF),e(lF,Nce),e(Nce,VWo),e(lF,WWo),e(lF,pD),e(pD,QWo),e(lF,HWo),e(B,UWo),e(B,iF),e(iF,Dce),e(Dce,JWo),e(iF,YWo),e(iF,_D),e(_D,KWo),e(iF,ZWo),e(B,eQo),e(B,dF),e(dF,qce),e(qce,oQo),e(dF,rQo),e(dF,bD),e(bD,tQo),e(dF,aQo),e(go,sQo),e(go,Gce),e(Gce,nQo),e(go,lQo),g($A,go,null),b(d,i9e,_),b(d,ac,_),e(ac,cF),e(cF,Oce),g(IA,Oce,null),e(ac,iQo),e(ac,Xce),e(Xce,dQo),b(d,d9e,_),b(d,hr,_),g(jA,hr,null),e(hr,cQo),e(hr,sc),e(sc,mQo),e(sc,zce),e(zce,fQo),e(sc,gQo),e(sc,Vce),e(Vce,hQo),e(sc,uQo),e(hr,pQo),e(hr,NA),e(NA,_Qo),e(NA,Wce),e(Wce,bQo),e(NA,vQo),e(hr,TQo),e(hr,lt),g(DA,lt,null),e(lt,FQo),e(lt,Qce),e(Qce,CQo),e(lt,MQo),e(lt,nc),e(nc,EQo),e(nc,Hce),e(Hce,yQo),e(nc,wQo),e(nc,Uce),e(Uce,AQo),e(nc,LQo),e(lt,BQo),e(lt,Jce),e(Jce,kQo),e(lt,xQo),g(qA,lt,null),e(hr,RQo),e(hr,ho),g(GA,ho,null),e(ho,SQo),e(ho,Yce),e(Yce,PQo),e(ho,$Qo),e(ho,is),e(is,IQo),e(is,Kce),e(Kce,jQo),e(is,NQo),e(is,Zce),e(Zce,DQo),e(is,qQo),e(is,eme),e(eme,GQo),e(is,OQo),e(ho,XQo),e(ho,H),e(H,mF),e(mF,ome),e(ome,zQo),e(mF,VQo),e(mF,vD),e(vD,WQo),e(mF,QQo),e(H,HQo),e(H,fF),e(fF,rme),e(rme,UQo),e(fF,JQo),e(fF,TD),e(TD,YQo),e(fF,KQo),e(H,ZQo),e(H,gF),e(gF,tme),e(tme,eHo),e(gF,oHo),e(gF,FD),e(FD,rHo),e(gF,tHo),e(H,aHo),e(H,hF),e(hF,ame),e(ame,sHo),e(hF,nHo),e(hF,CD),e(CD,lHo),e(hF,iHo),e(H,dHo),e(H,uF),e(uF,sme),e(sme,cHo),e(uF,mHo),e(uF,MD),e(MD,fHo),e(uF,gHo),e(H,hHo),e(H,pF),e(pF,nme),e(nme,uHo),e(pF,pHo),e(pF,ED),e(ED,_Ho),e(pF,bHo),e(H,vHo),e(H,_F),e(_F,lme),e(lme,THo),e(_F,FHo),e(_F,yD),e(yD,CHo),e(_F,MHo),e(H,EHo),e(H,bF),e(bF,ime),e(ime,yHo),e(bF,wHo),e(bF,wD),e(wD,AHo),e(bF,LHo),e(H,BHo),e(H,vF),e(vF,dme),e(dme,kHo),e(vF,xHo),e(vF,AD),e(AD,RHo),e(vF,SHo),e(H,PHo),e(H,TF),e(TF,cme),e(cme,$Ho),e(TF,IHo),e(TF,LD),e(LD,jHo),e(TF,NHo),e(H,DHo),e(H,FF),e(FF,mme),e(mme,qHo),e(FF,GHo),e(FF,BD),e(BD,OHo),e(FF,XHo),e(H,zHo),e(H,CF),e(CF,fme),e(fme,VHo),e(CF,WHo),e(CF,kD),e(kD,QHo),e(CF,HHo),e(H,UHo),e(H,MF),e(MF,gme),e(gme,JHo),e(MF,YHo),e(MF,xD),e(xD,KHo),e(MF,ZHo),e(H,eUo),e(H,EF),e(EF,hme),e(hme,oUo),e(EF,rUo),e(EF,RD),e(RD,tUo),e(EF,aUo),e(H,sUo),e(H,yF),e(yF,ume),e(ume,nUo),e(yF,lUo),e(yF,SD),e(SD,iUo),e(yF,dUo),e(H,cUo),e(H,wF),e(wF,pme),e(pme,mUo),e(wF,fUo),e(wF,PD),e(PD,gUo),e(wF,hUo),e(H,uUo),e(H,AF),e(AF,_me),e(_me,pUo),e(AF,_Uo),e(AF,$D),e($D,bUo),e(AF,vUo),e(H,TUo),e(H,LF),e(LF,bme),e(bme,FUo),e(LF,CUo),e(LF,ID),e(ID,MUo),e(LF,EUo),e(H,yUo),e(H,BF),e(BF,vme),e(vme,wUo),e(BF,AUo),e(BF,jD),e(jD,LUo),e(BF,BUo),e(H,kUo),e(H,kF),e(kF,Tme),e(Tme,xUo),e(kF,RUo),e(kF,ND),e(ND,SUo),e(kF,PUo),e(H,$Uo),e(H,xF),e(xF,Fme),e(Fme,IUo),e(xF,jUo),e(xF,DD),e(DD,NUo),e(xF,DUo),e(H,qUo),e(H,RF),e(RF,Cme),e(Cme,GUo),e(RF,OUo),e(RF,qD),e(qD,XUo),e(RF,zUo),e(ho,VUo),e(ho,Mme),e(Mme,WUo),e(ho,QUo),g(OA,ho,null),b(d,c9e,_),b(d,lc,_),e(lc,SF),e(SF,Eme),g(XA,Eme,null),e(lc,HUo),e(lc,yme),e(yme,UUo),b(d,m9e,_),b(d,ur,_),g(zA,ur,null),e(ur,JUo),e(ur,ic),e(ic,YUo),e(ic,wme),e(wme,KUo),e(ic,ZUo),e(ic,Ame),e(Ame,eJo),e(ic,oJo),e(ur,rJo),e(ur,VA),e(VA,tJo),e(VA,Lme),e(Lme,aJo),e(VA,sJo),e(ur,nJo),e(ur,it),g(WA,it,null),e(it,lJo),e(it,Bme),e(Bme,iJo),e(it,dJo),e(it,dc),e(dc,cJo),e(dc,kme),e(kme,mJo),e(dc,fJo),e(dc,xme),e(xme,gJo),e(dc,hJo),e(it,uJo),e(it,Rme),e(Rme,pJo),e(it,_Jo),g(QA,it,null),e(ur,bJo),e(ur,uo),g(HA,uo,null),e(uo,vJo),e(uo,Sme),e(Sme,TJo),e(uo,FJo),e(uo,ds),e(ds,CJo),e(ds,Pme),e(Pme,MJo),e(ds,EJo),e(ds,$me),e($me,yJo),e(ds,wJo),e(ds,Ime),e(Ime,AJo),e(ds,LJo),e(uo,BJo),e(uo,he),e(he,PF),e(PF,jme),e(jme,kJo),e(PF,xJo),e(PF,GD),e(GD,RJo),e(PF,SJo),e(he,PJo),e(he,$F),e($F,Nme),e(Nme,$Jo),e($F,IJo),e($F,OD),e(OD,jJo),e($F,NJo),e(he,DJo),e(he,IF),e(IF,Dme),e(Dme,qJo),e(IF,GJo),e(IF,XD),e(XD,OJo),e(IF,XJo),e(he,zJo),e(he,jF),e(jF,qme),e(qme,VJo),e(jF,WJo),e(jF,zD),e(zD,QJo),e(jF,HJo),e(he,UJo),e(he,NF),e(NF,Gme),e(Gme,JJo),e(NF,YJo),e(NF,VD),e(VD,KJo),e(NF,ZJo),e(he,eYo),e(he,DF),e(DF,Ome),e(Ome,oYo),e(DF,rYo),e(DF,WD),e(WD,tYo),e(DF,aYo),e(he,sYo),e(he,qF),e(qF,Xme),e(Xme,nYo),e(qF,lYo),e(qF,QD),e(QD,iYo),e(qF,dYo),e(he,cYo),e(he,GF),e(GF,zme),e(zme,mYo),e(GF,fYo),e(GF,HD),e(HD,gYo),e(GF,hYo),e(he,uYo),e(he,OF),e(OF,Vme),e(Vme,pYo),e(OF,_Yo),e(OF,UD),e(UD,bYo),e(OF,vYo),e(he,TYo),e(he,XF),e(XF,Wme),e(Wme,FYo),e(XF,CYo),e(XF,JD),e(JD,MYo),e(XF,EYo),e(uo,yYo),e(uo,Qme),e(Qme,wYo),e(uo,AYo),g(UA,uo,null),b(d,f9e,_),b(d,cc,_),e(cc,zF),e(zF,Hme),g(JA,Hme,null),e(cc,LYo),e(cc,Ume),e(Ume,BYo),b(d,g9e,_),b(d,pr,_),g(YA,pr,null),e(pr,kYo),e(pr,mc),e(mc,xYo),e(mc,Jme),e(Jme,RYo),e(mc,SYo),e(mc,Yme),e(Yme,PYo),e(mc,$Yo),e(pr,IYo),e(pr,KA),e(KA,jYo),e(KA,Kme),e(Kme,NYo),e(KA,DYo),e(pr,qYo),e(pr,dt),g(ZA,dt,null),e(dt,GYo),e(dt,Zme),e(Zme,OYo),e(dt,XYo),e(dt,fc),e(fc,zYo),e(fc,efe),e(efe,VYo),e(fc,WYo),e(fc,ofe),e(ofe,QYo),e(fc,HYo),e(dt,UYo),e(dt,rfe),e(rfe,JYo),e(dt,YYo),g(e6,dt,null),e(pr,KYo),e(pr,po),g(o6,po,null),e(po,ZYo),e(po,tfe),e(tfe,eKo),e(po,oKo),e(po,cs),e(cs,rKo),e(cs,afe),e(afe,tKo),e(cs,aKo),e(cs,sfe),e(sfe,sKo),e(cs,nKo),e(cs,nfe),e(nfe,lKo),e(cs,iKo),e(po,dKo),e(po,lfe),e(lfe,VF),e(VF,ife),e(ife,cKo),e(VF,mKo),e(VF,YD),e(YD,fKo),e(VF,gKo),e(po,hKo),e(po,dfe),e(dfe,uKo),e(po,pKo),g(r6,po,null),b(d,h9e,_),b(d,gc,_),e(gc,WF),e(WF,cfe),g(t6,cfe,null),e(gc,_Ko),e(gc,mfe),e(mfe,bKo),b(d,u9e,_),b(d,_r,_),g(a6,_r,null),e(_r,vKo),e(_r,hc),e(hc,TKo),e(hc,ffe),e(ffe,FKo),e(hc,CKo),e(hc,gfe),e(gfe,MKo),e(hc,EKo),e(_r,yKo),e(_r,s6),e(s6,wKo),e(s6,hfe),e(hfe,AKo),e(s6,LKo),e(_r,BKo),e(_r,ct),g(n6,ct,null),e(ct,kKo),e(ct,ufe),e(ufe,xKo),e(ct,RKo),e(ct,uc),e(uc,SKo),e(uc,pfe),e(pfe,PKo),e(uc,$Ko),e(uc,_fe),e(_fe,IKo),e(uc,jKo),e(ct,NKo),e(ct,bfe),e(bfe,DKo),e(ct,qKo),g(l6,ct,null),e(_r,GKo),e(_r,_o),g(i6,_o,null),e(_o,OKo),e(_o,vfe),e(vfe,XKo),e(_o,zKo),e(_o,ms),e(ms,VKo),e(ms,Tfe),e(Tfe,WKo),e(ms,QKo),e(ms,Ffe),e(Ffe,HKo),e(ms,UKo),e(ms,Cfe),e(Cfe,JKo),e(ms,YKo),e(_o,KKo),e(_o,Y),e(Y,QF),e(QF,Mfe),e(Mfe,ZKo),e(QF,eZo),e(QF,KD),e(KD,oZo),e(QF,rZo),e(Y,tZo),e(Y,HF),e(HF,Efe),e(Efe,aZo),e(HF,sZo),e(HF,ZD),e(ZD,nZo),e(HF,lZo),e(Y,iZo),e(Y,UF),e(UF,yfe),e(yfe,dZo),e(UF,cZo),e(UF,eq),e(eq,mZo),e(UF,fZo),e(Y,gZo),e(Y,JF),e(JF,wfe),e(wfe,hZo),e(JF,uZo),e(JF,oq),e(oq,pZo),e(JF,_Zo),e(Y,bZo),e(Y,YF),e(YF,Afe),e(Afe,vZo),e(YF,TZo),e(YF,rq),e(rq,FZo),e(YF,CZo),e(Y,MZo),e(Y,KF),e(KF,Lfe),e(Lfe,EZo),e(KF,yZo),e(KF,tq),e(tq,wZo),e(KF,AZo),e(Y,LZo),e(Y,ZF),e(ZF,Bfe),e(Bfe,BZo),e(ZF,kZo),e(ZF,aq),e(aq,xZo),e(ZF,RZo),e(Y,SZo),e(Y,eC),e(eC,kfe),e(kfe,PZo),e(eC,$Zo),e(eC,sq),e(sq,IZo),e(eC,jZo),e(Y,NZo),e(Y,oC),e(oC,xfe),e(xfe,DZo),e(oC,qZo),e(oC,nq),e(nq,GZo),e(oC,OZo),e(Y,XZo),e(Y,rC),e(rC,Rfe),e(Rfe,zZo),e(rC,VZo),e(rC,lq),e(lq,WZo),e(rC,QZo),e(Y,HZo),e(Y,tC),e(tC,Sfe),e(Sfe,UZo),e(tC,JZo),e(tC,iq),e(iq,YZo),e(tC,KZo),e(Y,ZZo),e(Y,aC),e(aC,Pfe),e(Pfe,eer),e(aC,oer),e(aC,dq),e(dq,rer),e(aC,ter),e(Y,aer),e(Y,sC),e(sC,$fe),e($fe,ser),e(sC,ner),e(sC,cq),e(cq,ler),e(sC,ier),e(Y,der),e(Y,nC),e(nC,Ife),e(Ife,cer),e(nC,mer),e(nC,mq),e(mq,fer),e(nC,ger),e(Y,her),e(Y,lC),e(lC,jfe),e(jfe,uer),e(lC,per),e(lC,fq),e(fq,_er),e(lC,ber),e(Y,ver),e(Y,iC),e(iC,Nfe),e(Nfe,Ter),e(iC,Fer),e(iC,gq),e(gq,Cer),e(iC,Mer),e(Y,Eer),e(Y,dC),e(dC,Dfe),e(Dfe,yer),e(dC,wer),e(dC,hq),e(hq,Aer),e(dC,Ler),e(Y,Ber),e(Y,cC),e(cC,qfe),e(qfe,ker),e(cC,xer),e(cC,uq),e(uq,Rer),e(cC,Ser),e(Y,Per),e(Y,mC),e(mC,Gfe),e(Gfe,$er),e(mC,Ier),e(mC,pq),e(pq,jer),e(mC,Ner),e(Y,Der),e(Y,fC),e(fC,Ofe),e(Ofe,qer),e(fC,Ger),e(fC,_q),e(_q,Oer),e(fC,Xer),e(_o,zer),e(_o,Xfe),e(Xfe,Ver),e(_o,Wer),g(d6,_o,null),b(d,p9e,_),b(d,pc,_),e(pc,gC),e(gC,zfe),g(c6,zfe,null),e(pc,Qer),e(pc,Vfe),e(Vfe,Her),b(d,_9e,_),b(d,br,_),g(m6,br,null),e(br,Uer),e(br,_c),e(_c,Jer),e(_c,Wfe),e(Wfe,Yer),e(_c,Ker),e(_c,Qfe),e(Qfe,Zer),e(_c,eor),e(br,oor),e(br,f6),e(f6,ror),e(f6,Hfe),e(Hfe,tor),e(f6,aor),e(br,sor),e(br,mt),g(g6,mt,null),e(mt,nor),e(mt,Ufe),e(Ufe,lor),e(mt,ior),e(mt,bc),e(bc,dor),e(bc,Jfe),e(Jfe,cor),e(bc,mor),e(bc,Yfe),e(Yfe,gor),e(bc,hor),e(mt,uor),e(mt,Kfe),e(Kfe,por),e(mt,_or),g(h6,mt,null),e(br,bor),e(br,bo),g(u6,bo,null),e(bo,vor),e(bo,Zfe),e(Zfe,Tor),e(bo,For),e(bo,fs),e(fs,Cor),e(fs,ege),e(ege,Mor),e(fs,Eor),e(fs,oge),e(oge,yor),e(fs,wor),e(fs,rge),e(rge,Aor),e(fs,Lor),e(bo,Bor),e(bo,ue),e(ue,hC),e(hC,tge),e(tge,kor),e(hC,xor),e(hC,bq),e(bq,Ror),e(hC,Sor),e(ue,Por),e(ue,uC),e(uC,age),e(age,$or),e(uC,Ior),e(uC,vq),e(vq,jor),e(uC,Nor),e(ue,Dor),e(ue,pC),e(pC,sge),e(sge,qor),e(pC,Gor),e(pC,Tq),e(Tq,Oor),e(pC,Xor),e(ue,zor),e(ue,_C),e(_C,nge),e(nge,Vor),e(_C,Wor),e(_C,Fq),e(Fq,Qor),e(_C,Hor),e(ue,Uor),e(ue,bC),e(bC,lge),e(lge,Jor),e(bC,Yor),e(bC,Cq),e(Cq,Kor),e(bC,Zor),e(ue,err),e(ue,vC),e(vC,ige),e(ige,orr),e(vC,rrr),e(vC,Mq),e(Mq,trr),e(vC,arr),e(ue,srr),e(ue,TC),e(TC,dge),e(dge,nrr),e(TC,lrr),e(TC,Eq),e(Eq,irr),e(TC,drr),e(ue,crr),e(ue,FC),e(FC,cge),e(cge,mrr),e(FC,frr),e(FC,yq),e(yq,grr),e(FC,hrr),e(ue,urr),e(ue,CC),e(CC,mge),e(mge,prr),e(CC,_rr),e(CC,wq),e(wq,brr),e(CC,vrr),e(ue,Trr),e(ue,MC),e(MC,fge),e(fge,Frr),e(MC,Crr),e(MC,Aq),e(Aq,Mrr),e(MC,Err),e(bo,yrr),e(bo,gge),e(gge,wrr),e(bo,Arr),g(p6,bo,null),b(d,b9e,_),b(d,vc,_),e(vc,EC),e(EC,hge),g(_6,hge,null),e(vc,Lrr),e(vc,uge),e(uge,Brr),b(d,v9e,_),b(d,vr,_),g(b6,vr,null),e(vr,krr),e(vr,Tc),e(Tc,xrr),e(Tc,pge),e(pge,Rrr),e(Tc,Srr),e(Tc,_ge),e(_ge,Prr),e(Tc,$rr),e(vr,Irr),e(vr,v6),e(v6,jrr),e(v6,bge),e(bge,Nrr),e(v6,Drr),e(vr,qrr),e(vr,ft),g(T6,ft,null),e(ft,Grr),e(ft,vge),e(vge,Orr),e(ft,Xrr),e(ft,Fc),e(Fc,zrr),e(Fc,Tge),e(Tge,Vrr),e(Fc,Wrr),e(Fc,Fge),e(Fge,Qrr),e(Fc,Hrr),e(ft,Urr),e(ft,Cge),e(Cge,Jrr),e(ft,Yrr),g(F6,ft,null),e(vr,Krr),e(vr,vo),g(C6,vo,null),e(vo,Zrr),e(vo,Mge),e(Mge,etr),e(vo,otr),e(vo,gs),e(gs,rtr),e(gs,Ege),e(Ege,ttr),e(gs,atr),e(gs,yge),e(yge,str),e(gs,ntr),e(gs,wge),e(wge,ltr),e(gs,itr),e(vo,dtr),e(vo,X),e(X,yC),e(yC,Age),e(Age,ctr),e(yC,mtr),e(yC,Lq),e(Lq,ftr),e(yC,gtr),e(X,htr),e(X,wC),e(wC,Lge),e(Lge,utr),e(wC,ptr),e(wC,Bq),e(Bq,_tr),e(wC,btr),e(X,vtr),e(X,AC),e(AC,Bge),e(Bge,Ttr),e(AC,Ftr),e(AC,kq),e(kq,Ctr),e(AC,Mtr),e(X,Etr),e(X,LC),e(LC,kge),e(kge,ytr),e(LC,wtr),e(LC,xq),e(xq,Atr),e(LC,Ltr),e(X,Btr),e(X,BC),e(BC,xge),e(xge,ktr),e(BC,xtr),e(BC,Rq),e(Rq,Rtr),e(BC,Str),e(X,Ptr),e(X,kC),e(kC,Rge),e(Rge,$tr),e(kC,Itr),e(kC,Sq),e(Sq,jtr),e(kC,Ntr),e(X,Dtr),e(X,xC),e(xC,Sge),e(Sge,qtr),e(xC,Gtr),e(xC,Pq),e(Pq,Otr),e(xC,Xtr),e(X,ztr),e(X,RC),e(RC,Pge),e(Pge,Vtr),e(RC,Wtr),e(RC,$q),e($q,Qtr),e(RC,Htr),e(X,Utr),e(X,SC),e(SC,$ge),e($ge,Jtr),e(SC,Ytr),e(SC,Iq),e(Iq,Ktr),e(SC,Ztr),e(X,ear),e(X,PC),e(PC,Ige),e(Ige,oar),e(PC,rar),e(PC,jq),e(jq,tar),e(PC,aar),e(X,sar),e(X,$C),e($C,jge),e(jge,nar),e($C,lar),e($C,Nq),e(Nq,iar),e($C,dar),e(X,car),e(X,IC),e(IC,Nge),e(Nge,mar),e(IC,far),e(IC,Dq),e(Dq,gar),e(IC,har),e(X,uar),e(X,jC),e(jC,Dge),e(Dge,par),e(jC,_ar),e(jC,qq),e(qq,bar),e(jC,Tar),e(X,Far),e(X,NC),e(NC,qge),e(qge,Car),e(NC,Mar),e(NC,Gq),e(Gq,Ear),e(NC,yar),e(X,war),e(X,DC),e(DC,Gge),e(Gge,Aar),e(DC,Lar),e(DC,Oq),e(Oq,Bar),e(DC,kar),e(X,xar),e(X,qC),e(qC,Oge),e(Oge,Rar),e(qC,Sar),e(qC,Xq),e(Xq,Par),e(qC,$ar),e(X,Iar),e(X,GC),e(GC,Xge),e(Xge,jar),e(GC,Nar),e(GC,zq),e(zq,Dar),e(GC,qar),e(X,Gar),e(X,OC),e(OC,zge),e(zge,Oar),e(OC,Xar),e(OC,Vq),e(Vq,zar),e(OC,Var),e(X,War),e(X,XC),e(XC,Vge),e(Vge,Qar),e(XC,Har),e(XC,Wq),e(Wq,Uar),e(XC,Jar),e(X,Yar),e(X,zC),e(zC,Wge),e(Wge,Kar),e(zC,Zar),e(zC,Qq),e(Qq,esr),e(zC,osr),e(X,rsr),e(X,VC),e(VC,Qge),e(Qge,tsr),e(VC,asr),e(VC,Hq),e(Hq,ssr),e(VC,nsr),e(X,lsr),e(X,WC),e(WC,Hge),e(Hge,isr),e(WC,dsr),e(WC,Uq),e(Uq,csr),e(WC,msr),e(X,fsr),e(X,QC),e(QC,Uge),e(Uge,gsr),e(QC,hsr),e(QC,Jq),e(Jq,usr),e(QC,psr),e(X,_sr),e(X,HC),e(HC,Jge),e(Jge,bsr),e(HC,vsr),e(HC,Yq),e(Yq,Tsr),e(HC,Fsr),e(X,Csr),e(X,UC),e(UC,Yge),e(Yge,Msr),e(UC,Esr),e(UC,Kq),e(Kq,ysr),e(UC,wsr),e(vo,Asr),e(vo,Kge),e(Kge,Lsr),e(vo,Bsr),g(M6,vo,null),b(d,T9e,_),b(d,Cc,_),e(Cc,JC),e(JC,Zge),g(E6,Zge,null),e(Cc,ksr),e(Cc,ehe),e(ehe,xsr),b(d,F9e,_),b(d,Tr,_),g(y6,Tr,null),e(Tr,Rsr),e(Tr,Mc),e(Mc,Ssr),e(Mc,ohe),e(ohe,Psr),e(Mc,$sr),e(Mc,rhe),e(rhe,Isr),e(Mc,jsr),e(Tr,Nsr),e(Tr,w6),e(w6,Dsr),e(w6,the),e(the,qsr),e(w6,Gsr),e(Tr,Osr),e(Tr,gt),g(A6,gt,null),e(gt,Xsr),e(gt,ahe),e(ahe,zsr),e(gt,Vsr),e(gt,Ec),e(Ec,Wsr),e(Ec,she),e(she,Qsr),e(Ec,Hsr),e(Ec,nhe),e(nhe,Usr),e(Ec,Jsr),e(gt,Ysr),e(gt,lhe),e(lhe,Ksr),e(gt,Zsr),g(L6,gt,null),e(Tr,enr),e(Tr,To),g(B6,To,null),e(To,onr),e(To,ihe),e(ihe,rnr),e(To,tnr),e(To,hs),e(hs,anr),e(hs,dhe),e(dhe,snr),e(hs,nnr),e(hs,che),e(che,lnr),e(hs,inr),e(hs,mhe),e(mhe,dnr),e(hs,cnr),e(To,mnr),e(To,te),e(te,YC),e(YC,fhe),e(fhe,fnr),e(YC,gnr),e(YC,Zq),e(Zq,hnr),e(YC,unr),e(te,pnr),e(te,KC),e(KC,ghe),e(ghe,_nr),e(KC,bnr),e(KC,eG),e(eG,vnr),e(KC,Tnr),e(te,Fnr),e(te,ZC),e(ZC,hhe),e(hhe,Cnr),e(ZC,Mnr),e(ZC,oG),e(oG,Enr),e(ZC,ynr),e(te,wnr),e(te,e4),e(e4,uhe),e(uhe,Anr),e(e4,Lnr),e(e4,rG),e(rG,Bnr),e(e4,knr),e(te,xnr),e(te,o4),e(o4,phe),e(phe,Rnr),e(o4,Snr),e(o4,tG),e(tG,Pnr),e(o4,$nr),e(te,Inr),e(te,r4),e(r4,_he),e(_he,jnr),e(r4,Nnr),e(r4,aG),e(aG,Dnr),e(r4,qnr),e(te,Gnr),e(te,t4),e(t4,bhe),e(bhe,Onr),e(t4,Xnr),e(t4,sG),e(sG,znr),e(t4,Vnr),e(te,Wnr),e(te,a4),e(a4,vhe),e(vhe,Qnr),e(a4,Hnr),e(a4,nG),e(nG,Unr),e(a4,Jnr),e(te,Ynr),e(te,s4),e(s4,The),e(The,Knr),e(s4,Znr),e(s4,lG),e(lG,elr),e(s4,olr),e(te,rlr),e(te,n4),e(n4,Fhe),e(Fhe,tlr),e(n4,alr),e(n4,iG),e(iG,slr),e(n4,nlr),e(te,llr),e(te,l4),e(l4,Che),e(Che,ilr),e(l4,dlr),e(l4,dG),e(dG,clr),e(l4,mlr),e(te,flr),e(te,i4),e(i4,Mhe),e(Mhe,glr),e(i4,hlr),e(i4,cG),e(cG,ulr),e(i4,plr),e(te,_lr),e(te,d4),e(d4,Ehe),e(Ehe,blr),e(d4,vlr),e(d4,mG),e(mG,Tlr),e(d4,Flr),e(te,Clr),e(te,c4),e(c4,yhe),e(yhe,Mlr),e(c4,Elr),e(c4,fG),e(fG,ylr),e(c4,wlr),e(te,Alr),e(te,m4),e(m4,whe),e(whe,Llr),e(m4,Blr),e(m4,gG),e(gG,klr),e(m4,xlr),e(te,Rlr),e(te,f4),e(f4,Ahe),e(Ahe,Slr),e(f4,Plr),e(f4,hG),e(hG,$lr),e(f4,Ilr),e(te,jlr),e(te,g4),e(g4,Lhe),e(Lhe,Nlr),e(g4,Dlr),e(g4,uG),e(uG,qlr),e(g4,Glr),e(To,Olr),e(To,Bhe),e(Bhe,Xlr),e(To,zlr),g(k6,To,null),b(d,C9e,_),b(d,yc,_),e(yc,h4),e(h4,khe),g(x6,khe,null),e(yc,Vlr),e(yc,xhe),e(xhe,Wlr),b(d,M9e,_),b(d,Fr,_),g(R6,Fr,null),e(Fr,Qlr),e(Fr,wc),e(wc,Hlr),e(wc,Rhe),e(Rhe,Ulr),e(wc,Jlr),e(wc,She),e(She,Ylr),e(wc,Klr),e(Fr,Zlr),e(Fr,S6),e(S6,eir),e(S6,Phe),e(Phe,oir),e(S6,rir),e(Fr,tir),e(Fr,ht),g(P6,ht,null),e(ht,air),e(ht,$he),e($he,sir),e(ht,nir),e(ht,Ac),e(Ac,lir),e(Ac,Ihe),e(Ihe,iir),e(Ac,dir),e(Ac,jhe),e(jhe,cir),e(Ac,mir),e(ht,fir),e(ht,Nhe),e(Nhe,gir),e(ht,hir),g($6,ht,null),e(Fr,uir),e(Fr,Fo),g(I6,Fo,null),e(Fo,pir),e(Fo,Dhe),e(Dhe,_ir),e(Fo,bir),e(Fo,us),e(us,vir),e(us,qhe),e(qhe,Tir),e(us,Fir),e(us,Ghe),e(Ghe,Cir),e(us,Mir),e(us,Ohe),e(Ohe,Eir),e(us,yir),e(Fo,wir),e(Fo,Xhe),e(Xhe,u4),e(u4,zhe),e(zhe,Air),e(u4,Lir),e(u4,pG),e(pG,Bir),e(u4,kir),e(Fo,xir),e(Fo,Vhe),e(Vhe,Rir),e(Fo,Sir),g(j6,Fo,null),b(d,E9e,_),b(d,Lc,_),e(Lc,p4),e(p4,Whe),g(N6,Whe,null),e(Lc,Pir),e(Lc,Qhe),e(Qhe,$ir),b(d,y9e,_),b(d,Cr,_),g(D6,Cr,null),e(Cr,Iir),e(Cr,Bc),e(Bc,jir),e(Bc,Hhe),e(Hhe,Nir),e(Bc,Dir),e(Bc,Uhe),e(Uhe,qir),e(Bc,Gir),e(Cr,Oir),e(Cr,q6),e(q6,Xir),e(q6,Jhe),e(Jhe,zir),e(q6,Vir),e(Cr,Wir),e(Cr,ut),g(G6,ut,null),e(ut,Qir),e(ut,Yhe),e(Yhe,Hir),e(ut,Uir),e(ut,kc),e(kc,Jir),e(kc,Khe),e(Khe,Yir),e(kc,Kir),e(kc,Zhe),e(Zhe,Zir),e(kc,edr),e(ut,odr),e(ut,eue),e(eue,rdr),e(ut,tdr),g(O6,ut,null),e(Cr,adr),e(Cr,Co),g(X6,Co,null),e(Co,sdr),e(Co,oue),e(oue,ndr),e(Co,ldr),e(Co,ps),e(ps,idr),e(ps,rue),e(rue,ddr),e(ps,cdr),e(ps,tue),e(tue,mdr),e(ps,fdr),e(ps,aue),e(aue,gdr),e(ps,hdr),e(Co,udr),e(Co,K),e(K,_4),e(_4,sue),e(sue,pdr),e(_4,_dr),e(_4,_G),e(_G,bdr),e(_4,vdr),e(K,Tdr),e(K,b4),e(b4,nue),e(nue,Fdr),e(b4,Cdr),e(b4,bG),e(bG,Mdr),e(b4,Edr),e(K,ydr),e(K,v4),e(v4,lue),e(lue,wdr),e(v4,Adr),e(v4,vG),e(vG,Ldr),e(v4,Bdr),e(K,kdr),e(K,T4),e(T4,iue),e(iue,xdr),e(T4,Rdr),e(T4,TG),e(TG,Sdr),e(T4,Pdr),e(K,$dr),e(K,F4),e(F4,due),e(due,Idr),e(F4,jdr),e(F4,FG),e(FG,Ndr),e(F4,Ddr),e(K,qdr),e(K,C4),e(C4,cue),e(cue,Gdr),e(C4,Odr),e(C4,CG),e(CG,Xdr),e(C4,zdr),e(K,Vdr),e(K,M4),e(M4,mue),e(mue,Wdr),e(M4,Qdr),e(M4,MG),e(MG,Hdr),e(M4,Udr),e(K,Jdr),e(K,E4),e(E4,fue),e(fue,Ydr),e(E4,Kdr),e(E4,EG),e(EG,Zdr),e(E4,ecr),e(K,ocr),e(K,y4),e(y4,gue),e(gue,rcr),e(y4,tcr),e(y4,yG),e(yG,acr),e(y4,scr),e(K,ncr),e(K,w4),e(w4,hue),e(hue,lcr),e(w4,icr),e(w4,wG),e(wG,dcr),e(w4,ccr),e(K,mcr),e(K,A4),e(A4,uue),e(uue,fcr),e(A4,gcr),e(A4,AG),e(AG,hcr),e(A4,ucr),e(K,pcr),e(K,L4),e(L4,pue),e(pue,_cr),e(L4,bcr),e(L4,LG),e(LG,vcr),e(L4,Tcr),e(K,Fcr),e(K,B4),e(B4,_ue),e(_ue,Ccr),e(B4,Mcr),e(B4,BG),e(BG,Ecr),e(B4,ycr),e(K,wcr),e(K,k4),e(k4,bue),e(bue,Acr),e(k4,Lcr),e(k4,kG),e(kG,Bcr),e(k4,kcr),e(K,xcr),e(K,x4),e(x4,vue),e(vue,Rcr),e(x4,Scr),e(x4,xG),e(xG,Pcr),e(x4,$cr),e(K,Icr),e(K,R4),e(R4,Tue),e(Tue,jcr),e(R4,Ncr),e(R4,RG),e(RG,Dcr),e(R4,qcr),e(K,Gcr),e(K,S4),e(S4,Fue),e(Fue,Ocr),e(S4,Xcr),e(S4,SG),e(SG,zcr),e(S4,Vcr),e(K,Wcr),e(K,P4),e(P4,Cue),e(Cue,Qcr),e(P4,Hcr),e(P4,PG),e(PG,Ucr),e(P4,Jcr),e(K,Ycr),e(K,$4),e($4,Mue),e(Mue,Kcr),e($4,Zcr),e($4,$G),e($G,emr),e($4,omr),e(K,rmr),e(K,I4),e(I4,Eue),e(Eue,tmr),e(I4,amr),e(I4,IG),e(IG,smr),e(I4,nmr),e(Co,lmr),e(Co,yue),e(yue,imr),e(Co,dmr),g(z6,Co,null),b(d,w9e,_),b(d,xc,_),e(xc,j4),e(j4,wue),g(V6,wue,null),e(xc,cmr),e(xc,Aue),e(Aue,mmr),b(d,A9e,_),b(d,Mr,_),g(W6,Mr,null),e(Mr,fmr),e(Mr,Rc),e(Rc,gmr),e(Rc,Lue),e(Lue,hmr),e(Rc,umr),e(Rc,Bue),e(Bue,pmr),e(Rc,_mr),e(Mr,bmr),e(Mr,Q6),e(Q6,vmr),e(Q6,kue),e(kue,Tmr),e(Q6,Fmr),e(Mr,Cmr),e(Mr,pt),g(H6,pt,null),e(pt,Mmr),e(pt,xue),e(xue,Emr),e(pt,ymr),e(pt,Sc),e(Sc,wmr),e(Sc,Rue),e(Rue,Amr),e(Sc,Lmr),e(Sc,Sue),e(Sue,Bmr),e(Sc,kmr),e(pt,xmr),e(pt,Pue),e(Pue,Rmr),e(pt,Smr),g(U6,pt,null),e(Mr,Pmr),e(Mr,Mo),g(J6,Mo,null),e(Mo,$mr),e(Mo,$ue),e($ue,Imr),e(Mo,jmr),e(Mo,_s),e(_s,Nmr),e(_s,Iue),e(Iue,Dmr),e(_s,qmr),e(_s,jue),e(jue,Gmr),e(_s,Omr),e(_s,Nue),e(Nue,Xmr),e(_s,zmr),e(Mo,Vmr),e(Mo,Z),e(Z,N4),e(N4,Due),e(Due,Wmr),e(N4,Qmr),e(N4,jG),e(jG,Hmr),e(N4,Umr),e(Z,Jmr),e(Z,D4),e(D4,que),e(que,Ymr),e(D4,Kmr),e(D4,NG),e(NG,Zmr),e(D4,efr),e(Z,ofr),e(Z,q4),e(q4,Gue),e(Gue,rfr),e(q4,tfr),e(q4,DG),e(DG,afr),e(q4,sfr),e(Z,nfr),e(Z,G4),e(G4,Oue),e(Oue,lfr),e(G4,ifr),e(G4,qG),e(qG,dfr),e(G4,cfr),e(Z,mfr),e(Z,O4),e(O4,Xue),e(Xue,ffr),e(O4,gfr),e(O4,GG),e(GG,hfr),e(O4,ufr),e(Z,pfr),e(Z,X4),e(X4,zue),e(zue,_fr),e(X4,bfr),e(X4,OG),e(OG,vfr),e(X4,Tfr),e(Z,Ffr),e(Z,z4),e(z4,Vue),e(Vue,Cfr),e(z4,Mfr),e(z4,XG),e(XG,Efr),e(z4,yfr),e(Z,wfr),e(Z,V4),e(V4,Wue),e(Wue,Afr),e(V4,Lfr),e(V4,zG),e(zG,Bfr),e(V4,kfr),e(Z,xfr),e(Z,W4),e(W4,Que),e(Que,Rfr),e(W4,Sfr),e(W4,VG),e(VG,Pfr),e(W4,$fr),e(Z,Ifr),e(Z,Q4),e(Q4,Hue),e(Hue,jfr),e(Q4,Nfr),e(Q4,WG),e(WG,Dfr),e(Q4,qfr),e(Z,Gfr),e(Z,H4),e(H4,Uue),e(Uue,Ofr),e(H4,Xfr),e(H4,QG),e(QG,zfr),e(H4,Vfr),e(Z,Wfr),e(Z,U4),e(U4,Jue),e(Jue,Qfr),e(U4,Hfr),e(U4,HG),e(HG,Ufr),e(U4,Jfr),e(Z,Yfr),e(Z,J4),e(J4,Yue),e(Yue,Kfr),e(J4,Zfr),e(J4,UG),e(UG,egr),e(J4,ogr),e(Z,rgr),e(Z,Y4),e(Y4,Kue),e(Kue,tgr),e(Y4,agr),e(Y4,JG),e(JG,sgr),e(Y4,ngr),e(Z,lgr),e(Z,K4),e(K4,Zue),e(Zue,igr),e(K4,dgr),e(K4,YG),e(YG,cgr),e(K4,mgr),e(Z,fgr),e(Z,Z4),e(Z4,epe),e(epe,ggr),e(Z4,hgr),e(Z4,KG),e(KG,ugr),e(Z4,pgr),e(Z,_gr),e(Z,eM),e(eM,ope),e(ope,bgr),e(eM,vgr),e(eM,ZG),e(ZG,Tgr),e(eM,Fgr),e(Z,Cgr),e(Z,oM),e(oM,rpe),e(rpe,Mgr),e(oM,Egr),e(oM,eO),e(eO,ygr),e(oM,wgr),e(Z,Agr),e(Z,rM),e(rM,tpe),e(tpe,Lgr),e(rM,Bgr),e(rM,oO),e(oO,kgr),e(rM,xgr),e(Mo,Rgr),e(Mo,ape),e(ape,Sgr),e(Mo,Pgr),g(Y6,Mo,null),b(d,L9e,_),b(d,Pc,_),e(Pc,tM),e(tM,spe),g(K6,spe,null),e(Pc,$gr),e(Pc,npe),e(npe,Igr),b(d,B9e,_),b(d,Er,_),g(Z6,Er,null),e(Er,jgr),e(Er,$c),e($c,Ngr),e($c,lpe),e(lpe,Dgr),e($c,qgr),e($c,ipe),e(ipe,Ggr),e($c,Ogr),e(Er,Xgr),e(Er,e0),e(e0,zgr),e(e0,dpe),e(dpe,Vgr),e(e0,Wgr),e(Er,Qgr),e(Er,_t),g(o0,_t,null),e(_t,Hgr),e(_t,cpe),e(cpe,Ugr),e(_t,Jgr),e(_t,Ic),e(Ic,Ygr),e(Ic,mpe),e(mpe,Kgr),e(Ic,Zgr),e(Ic,fpe),e(fpe,ehr),e(Ic,ohr),e(_t,rhr),e(_t,gpe),e(gpe,thr),e(_t,ahr),g(r0,_t,null),e(Er,shr),e(Er,Eo),g(t0,Eo,null),e(Eo,nhr),e(Eo,hpe),e(hpe,lhr),e(Eo,ihr),e(Eo,bs),e(bs,dhr),e(bs,upe),e(upe,chr),e(bs,mhr),e(bs,ppe),e(ppe,fhr),e(bs,ghr),e(bs,_pe),e(_pe,hhr),e(bs,uhr),e(Eo,phr),e(Eo,bpe),e(bpe,aM),e(aM,vpe),e(vpe,_hr),e(aM,bhr),e(aM,rO),e(rO,vhr),e(aM,Thr),e(Eo,Fhr),e(Eo,Tpe),e(Tpe,Chr),e(Eo,Mhr),g(a0,Eo,null),b(d,k9e,_),b(d,jc,_),e(jc,sM),e(sM,Fpe),g(s0,Fpe,null),e(jc,Ehr),e(jc,Cpe),e(Cpe,yhr),b(d,x9e,_),b(d,yr,_),g(n0,yr,null),e(yr,whr),e(yr,Nc),e(Nc,Ahr),e(Nc,Mpe),e(Mpe,Lhr),e(Nc,Bhr),e(Nc,Epe),e(Epe,khr),e(Nc,xhr),e(yr,Rhr),e(yr,l0),e(l0,Shr),e(l0,ype),e(ype,Phr),e(l0,$hr),e(yr,Ihr),e(yr,bt),g(i0,bt,null),e(bt,jhr),e(bt,wpe),e(wpe,Nhr),e(bt,Dhr),e(bt,Dc),e(Dc,qhr),e(Dc,Ape),e(Ape,Ghr),e(Dc,Ohr),e(Dc,Lpe),e(Lpe,Xhr),e(Dc,zhr),e(bt,Vhr),e(bt,Bpe),e(Bpe,Whr),e(bt,Qhr),g(d0,bt,null),e(yr,Hhr),e(yr,yo),g(c0,yo,null),e(yo,Uhr),e(yo,kpe),e(kpe,Jhr),e(yo,Yhr),e(yo,vs),e(vs,Khr),e(vs,xpe),e(xpe,Zhr),e(vs,eur),e(vs,Rpe),e(Rpe,our),e(vs,rur),e(vs,Spe),e(Spe,tur),e(vs,aur),e(yo,sur),e(yo,Ppe),e(Ppe,nM),e(nM,$pe),e($pe,nur),e(nM,lur),e(nM,tO),e(tO,iur),e(nM,dur),e(yo,cur),e(yo,Ipe),e(Ipe,mur),e(yo,fur),g(m0,yo,null),b(d,R9e,_),b(d,qc,_),e(qc,lM),e(lM,jpe),g(f0,jpe,null),e(qc,gur),e(qc,Npe),e(Npe,hur),b(d,S9e,_),b(d,wr,_),g(g0,wr,null),e(wr,uur),e(wr,Gc),e(Gc,pur),e(Gc,Dpe),e(Dpe,_ur),e(Gc,bur),e(Gc,qpe),e(qpe,vur),e(Gc,Tur),e(wr,Fur),e(wr,h0),e(h0,Cur),e(h0,Gpe),e(Gpe,Mur),e(h0,Eur),e(wr,yur),e(wr,vt),g(u0,vt,null),e(vt,wur),e(vt,Ope),e(Ope,Aur),e(vt,Lur),e(vt,Oc),e(Oc,Bur),e(Oc,Xpe),e(Xpe,kur),e(Oc,xur),e(Oc,zpe),e(zpe,Rur),e(Oc,Sur),e(vt,Pur),e(vt,Vpe),e(Vpe,$ur),e(vt,Iur),g(p0,vt,null),e(wr,jur),e(wr,wo),g(_0,wo,null),e(wo,Nur),e(wo,Wpe),e(Wpe,Dur),e(wo,qur),e(wo,Ts),e(Ts,Gur),e(Ts,Qpe),e(Qpe,Our),e(Ts,Xur),e(Ts,Hpe),e(Hpe,zur),e(Ts,Vur),e(Ts,Upe),e(Upe,Wur),e(Ts,Qur),e(wo,Hur),e(wo,V),e(V,iM),e(iM,Jpe),e(Jpe,Uur),e(iM,Jur),e(iM,aO),e(aO,Yur),e(iM,Kur),e(V,Zur),e(V,dM),e(dM,Ype),e(Ype,epr),e(dM,opr),e(dM,sO),e(sO,rpr),e(dM,tpr),e(V,apr),e(V,cM),e(cM,Kpe),e(Kpe,spr),e(cM,npr),e(cM,nO),e(nO,lpr),e(cM,ipr),e(V,dpr),e(V,mM),e(mM,Zpe),e(Zpe,cpr),e(mM,mpr),e(mM,lO),e(lO,fpr),e(mM,gpr),e(V,hpr),e(V,fM),e(fM,e_e),e(e_e,upr),e(fM,ppr),e(fM,iO),e(iO,_pr),e(fM,bpr),e(V,vpr),e(V,gM),e(gM,o_e),e(o_e,Tpr),e(gM,Fpr),e(gM,dO),e(dO,Cpr),e(gM,Mpr),e(V,Epr),e(V,hM),e(hM,r_e),e(r_e,ypr),e(hM,wpr),e(hM,cO),e(cO,Apr),e(hM,Lpr),e(V,Bpr),e(V,uM),e(uM,t_e),e(t_e,kpr),e(uM,xpr),e(uM,mO),e(mO,Rpr),e(uM,Spr),e(V,Ppr),e(V,pM),e(pM,a_e),e(a_e,$pr),e(pM,Ipr),e(pM,fO),e(fO,jpr),e(pM,Npr),e(V,Dpr),e(V,_M),e(_M,s_e),e(s_e,qpr),e(_M,Gpr),e(_M,gO),e(gO,Opr),e(_M,Xpr),e(V,zpr),e(V,bM),e(bM,n_e),e(n_e,Vpr),e(bM,Wpr),e(bM,hO),e(hO,Qpr),e(bM,Hpr),e(V,Upr),e(V,vM),e(vM,l_e),e(l_e,Jpr),e(vM,Ypr),e(vM,uO),e(uO,Kpr),e(vM,Zpr),e(V,e_r),e(V,TM),e(TM,i_e),e(i_e,o_r),e(TM,r_r),e(TM,pO),e(pO,t_r),e(TM,a_r),e(V,s_r),e(V,FM),e(FM,d_e),e(d_e,n_r),e(FM,l_r),e(FM,_O),e(_O,i_r),e(FM,d_r),e(V,c_r),e(V,CM),e(CM,c_e),e(c_e,m_r),e(CM,f_r),e(CM,bO),e(bO,g_r),e(CM,h_r),e(V,u_r),e(V,MM),e(MM,m_e),e(m_e,p_r),e(MM,__r),e(MM,vO),e(vO,b_r),e(MM,v_r),e(V,T_r),e(V,EM),e(EM,f_e),e(f_e,F_r),e(EM,C_r),e(EM,TO),e(TO,M_r),e(EM,E_r),e(V,y_r),e(V,yM),e(yM,g_e),e(g_e,w_r),e(yM,A_r),e(yM,FO),e(FO,L_r),e(yM,B_r),e(V,k_r),e(V,wM),e(wM,h_e),e(h_e,x_r),e(wM,R_r),e(wM,CO),e(CO,S_r),e(wM,P_r),e(V,$_r),e(V,AM),e(AM,u_e),e(u_e,I_r),e(AM,j_r),e(AM,MO),e(MO,N_r),e(AM,D_r),e(V,q_r),e(V,LM),e(LM,p_e),e(p_e,G_r),e(LM,O_r),e(LM,EO),e(EO,X_r),e(LM,z_r),e(V,V_r),e(V,BM),e(BM,__e),e(__e,W_r),e(BM,Q_r),e(BM,yO),e(yO,H_r),e(BM,U_r),e(V,J_r),e(V,kM),e(kM,b_e),e(b_e,Y_r),e(kM,K_r),e(kM,wO),e(wO,Z_r),e(kM,ebr),e(V,obr),e(V,xM),e(xM,v_e),e(v_e,rbr),e(xM,tbr),e(xM,AO),e(AO,abr),e(xM,sbr),e(wo,nbr),e(wo,T_e),e(T_e,lbr),e(wo,ibr),g(b0,wo,null),b(d,P9e,_),b(d,Xc,_),e(Xc,RM),e(RM,F_e),g(v0,F_e,null),e(Xc,dbr),e(Xc,C_e),e(C_e,cbr),b(d,$9e,_),b(d,Ar,_),g(T0,Ar,null),e(Ar,mbr),e(Ar,zc),e(zc,fbr),e(zc,M_e),e(M_e,gbr),e(zc,hbr),e(zc,E_e),e(E_e,ubr),e(zc,pbr),e(Ar,_br),e(Ar,F0),e(F0,bbr),e(F0,y_e),e(y_e,vbr),e(F0,Tbr),e(Ar,Fbr),e(Ar,Tt),g(C0,Tt,null),e(Tt,Cbr),e(Tt,w_e),e(w_e,Mbr),e(Tt,Ebr),e(Tt,Vc),e(Vc,ybr),e(Vc,A_e),e(A_e,wbr),e(Vc,Abr),e(Vc,L_e),e(L_e,Lbr),e(Vc,Bbr),e(Tt,kbr),e(Tt,B_e),e(B_e,xbr),e(Tt,Rbr),g(M0,Tt,null),e(Ar,Sbr),e(Ar,Ao),g(E0,Ao,null),e(Ao,Pbr),e(Ao,k_e),e(k_e,$br),e(Ao,Ibr),e(Ao,Fs),e(Fs,jbr),e(Fs,x_e),e(x_e,Nbr),e(Fs,Dbr),e(Fs,R_e),e(R_e,qbr),e(Fs,Gbr),e(Fs,S_e),e(S_e,Obr),e(Fs,Xbr),e(Ao,zbr),e(Ao,Cs),e(Cs,SM),e(SM,P_e),e(P_e,Vbr),e(SM,Wbr),e(SM,LO),e(LO,Qbr),e(SM,Hbr),e(Cs,Ubr),e(Cs,PM),e(PM,$_e),e($_e,Jbr),e(PM,Ybr),e(PM,BO),e(BO,Kbr),e(PM,Zbr),e(Cs,e2r),e(Cs,$M),e($M,I_e),e(I_e,o2r),e($M,r2r),e($M,kO),e(kO,t2r),e($M,a2r),e(Cs,s2r),e(Cs,IM),e(IM,j_e),e(j_e,n2r),e(IM,l2r),e(IM,xO),e(xO,i2r),e(IM,d2r),e(Ao,c2r),e(Ao,N_e),e(N_e,m2r),e(Ao,f2r),g(y0,Ao,null),b(d,I9e,_),b(d,Wc,_),e(Wc,jM),e(jM,D_e),g(w0,D_e,null),e(Wc,g2r),e(Wc,q_e),e(q_e,h2r),b(d,j9e,_),b(d,Lr,_),g(A0,Lr,null),e(Lr,u2r),e(Lr,Qc),e(Qc,p2r),e(Qc,G_e),e(G_e,_2r),e(Qc,b2r),e(Qc,O_e),e(O_e,v2r),e(Qc,T2r),e(Lr,F2r),e(Lr,L0),e(L0,C2r),e(L0,X_e),e(X_e,M2r),e(L0,E2r),e(Lr,y2r),e(Lr,Ft),g(B0,Ft,null),e(Ft,w2r),e(Ft,z_e),e(z_e,A2r),e(Ft,L2r),e(Ft,Hc),e(Hc,B2r),e(Hc,V_e),e(V_e,k2r),e(Hc,x2r),e(Hc,W_e),e(W_e,R2r),e(Hc,S2r),e(Ft,P2r),e(Ft,Q_e),e(Q_e,$2r),e(Ft,I2r),g(k0,Ft,null),e(Lr,j2r),e(Lr,Lo),g(x0,Lo,null),e(Lo,N2r),e(Lo,H_e),e(H_e,D2r),e(Lo,q2r),e(Lo,Ms),e(Ms,G2r),e(Ms,U_e),e(U_e,O2r),e(Ms,X2r),e(Ms,J_e),e(J_e,z2r),e(Ms,V2r),e(Ms,Y_e),e(Y_e,W2r),e(Ms,Q2r),e(Lo,H2r),e(Lo,me),e(me,NM),e(NM,K_e),e(K_e,U2r),e(NM,J2r),e(NM,RO),e(RO,Y2r),e(NM,K2r),e(me,Z2r),e(me,DM),e(DM,Z_e),e(Z_e,evr),e(DM,ovr),e(DM,SO),e(SO,rvr),e(DM,tvr),e(me,avr),e(me,qM),e(qM,ebe),e(ebe,svr),e(qM,nvr),e(qM,PO),e(PO,lvr),e(qM,ivr),e(me,dvr),e(me,GM),e(GM,obe),e(obe,cvr),e(GM,mvr),e(GM,$O),e($O,fvr),e(GM,gvr),e(me,hvr),e(me,OM),e(OM,rbe),e(rbe,uvr),e(OM,pvr),e(OM,IO),e(IO,_vr),e(OM,bvr),e(me,vvr),e(me,XM),e(XM,tbe),e(tbe,Tvr),e(XM,Fvr),e(XM,jO),e(jO,Cvr),e(XM,Mvr),e(me,Evr),e(me,zM),e(zM,abe),e(abe,yvr),e(zM,wvr),e(zM,NO),e(NO,Avr),e(zM,Lvr),e(me,Bvr),e(me,VM),e(VM,sbe),e(sbe,kvr),e(VM,xvr),e(VM,DO),e(DO,Rvr),e(VM,Svr),e(me,Pvr),e(me,WM),e(WM,nbe),e(nbe,$vr),e(WM,Ivr),e(WM,qO),e(qO,jvr),e(WM,Nvr),e(me,Dvr),e(me,QM),e(QM,lbe),e(lbe,qvr),e(QM,Gvr),e(QM,GO),e(GO,Ovr),e(QM,Xvr),e(me,zvr),e(me,HM),e(HM,ibe),e(ibe,Vvr),e(HM,Wvr),e(HM,OO),e(OO,Qvr),e(HM,Hvr),e(Lo,Uvr),e(Lo,dbe),e(dbe,Jvr),e(Lo,Yvr),g(R0,Lo,null),b(d,N9e,_),b(d,Uc,_),e(Uc,UM),e(UM,cbe),g(S0,cbe,null),e(Uc,Kvr),e(Uc,mbe),e(mbe,Zvr),b(d,D9e,_),b(d,Br,_),g(P0,Br,null),e(Br,eTr),e(Br,Jc),e(Jc,oTr),e(Jc,fbe),e(fbe,rTr),e(Jc,tTr),e(Jc,gbe),e(gbe,aTr),e(Jc,sTr),e(Br,nTr),e(Br,$0),e($0,lTr),e($0,hbe),e(hbe,iTr),e($0,dTr),e(Br,cTr),e(Br,Ct),g(I0,Ct,null),e(Ct,mTr),e(Ct,ube),e(ube,fTr),e(Ct,gTr),e(Ct,Yc),e(Yc,hTr),e(Yc,pbe),e(pbe,uTr),e(Yc,pTr),e(Yc,_be),e(_be,_Tr),e(Yc,bTr),e(Ct,vTr),e(Ct,bbe),e(bbe,TTr),e(Ct,FTr),g(j0,Ct,null),e(Br,CTr),e(Br,Bo),g(N0,Bo,null),e(Bo,MTr),e(Bo,vbe),e(vbe,ETr),e(Bo,yTr),e(Bo,Es),e(Es,wTr),e(Es,Tbe),e(Tbe,ATr),e(Es,LTr),e(Es,Fbe),e(Fbe,BTr),e(Es,kTr),e(Es,Cbe),e(Cbe,xTr),e(Es,RTr),e(Bo,STr),e(Bo,ve),e(ve,JM),e(JM,Mbe),e(Mbe,PTr),e(JM,$Tr),e(JM,XO),e(XO,ITr),e(JM,jTr),e(ve,NTr),e(ve,YM),e(YM,Ebe),e(Ebe,DTr),e(YM,qTr),e(YM,zO),e(zO,GTr),e(YM,OTr),e(ve,XTr),e(ve,KM),e(KM,ybe),e(ybe,zTr),e(KM,VTr),e(KM,VO),e(VO,WTr),e(KM,QTr),e(ve,HTr),e(ve,ZM),e(ZM,wbe),e(wbe,UTr),e(ZM,JTr),e(ZM,WO),e(WO,YTr),e(ZM,KTr),e(ve,ZTr),e(ve,eE),e(eE,Abe),e(Abe,e1r),e(eE,o1r),e(eE,QO),e(QO,r1r),e(eE,t1r),e(ve,a1r),e(ve,oE),e(oE,Lbe),e(Lbe,s1r),e(oE,n1r),e(oE,HO),e(HO,l1r),e(oE,i1r),e(ve,d1r),e(ve,rE),e(rE,Bbe),e(Bbe,c1r),e(rE,m1r),e(rE,UO),e(UO,f1r),e(rE,g1r),e(ve,h1r),e(ve,tE),e(tE,kbe),e(kbe,u1r),e(tE,p1r),e(tE,JO),e(JO,_1r),e(tE,b1r),e(ve,v1r),e(ve,aE),e(aE,xbe),e(xbe,T1r),e(aE,F1r),e(aE,YO),e(YO,C1r),e(aE,M1r),e(Bo,E1r),e(Bo,Rbe),e(Rbe,y1r),e(Bo,w1r),g(D0,Bo,null),b(d,q9e,_),b(d,Kc,_),e(Kc,sE),e(sE,Sbe),g(q0,Sbe,null),e(Kc,A1r),e(Kc,Pbe),e(Pbe,L1r),b(d,G9e,_),b(d,kr,_),g(G0,kr,null),e(kr,B1r),e(kr,Zc),e(Zc,k1r),e(Zc,$be),e($be,x1r),e(Zc,R1r),e(Zc,Ibe),e(Ibe,S1r),e(Zc,P1r),e(kr,$1r),e(kr,O0),e(O0,I1r),e(O0,jbe),e(jbe,j1r),e(O0,N1r),e(kr,D1r),e(kr,Mt),g(X0,Mt,null),e(Mt,q1r),e(Mt,Nbe),e(Nbe,G1r),e(Mt,O1r),e(Mt,em),e(em,X1r),e(em,Dbe),e(Dbe,z1r),e(em,V1r),e(em,qbe),e(qbe,W1r),e(em,Q1r),e(Mt,H1r),e(Mt,Gbe),e(Gbe,U1r),e(Mt,J1r),g(z0,Mt,null),e(kr,Y1r),e(kr,ko),g(V0,ko,null),e(ko,K1r),e(ko,Obe),e(Obe,Z1r),e(ko,eFr),e(ko,ys),e(ys,oFr),e(ys,Xbe),e(Xbe,rFr),e(ys,tFr),e(ys,zbe),e(zbe,aFr),e(ys,sFr),e(ys,Vbe),e(Vbe,nFr),e(ys,lFr),e(ko,iFr),e(ko,Te),e(Te,nE),e(nE,Wbe),e(Wbe,dFr),e(nE,cFr),e(nE,KO),e(KO,mFr),e(nE,fFr),e(Te,gFr),e(Te,lE),e(lE,Qbe),e(Qbe,hFr),e(lE,uFr),e(lE,ZO),e(ZO,pFr),e(lE,_Fr),e(Te,bFr),e(Te,iE),e(iE,Hbe),e(Hbe,vFr),e(iE,TFr),e(iE,eX),e(eX,FFr),e(iE,CFr),e(Te,MFr),e(Te,dE),e(dE,Ube),e(Ube,EFr),e(dE,yFr),e(dE,oX),e(oX,wFr),e(dE,AFr),e(Te,LFr),e(Te,cE),e(cE,Jbe),e(Jbe,BFr),e(cE,kFr),e(cE,rX),e(rX,xFr),e(cE,RFr),e(Te,SFr),e(Te,mE),e(mE,Ybe),e(Ybe,PFr),e(mE,$Fr),e(mE,tX),e(tX,IFr),e(mE,jFr),e(Te,NFr),e(Te,fE),e(fE,Kbe),e(Kbe,DFr),e(fE,qFr),e(fE,aX),e(aX,GFr),e(fE,OFr),e(Te,XFr),e(Te,gE),e(gE,Zbe),e(Zbe,zFr),e(gE,VFr),e(gE,sX),e(sX,WFr),e(gE,QFr),e(Te,HFr),e(Te,hE),e(hE,e2e),e(e2e,UFr),e(hE,JFr),e(hE,nX),e(nX,YFr),e(hE,KFr),e(ko,ZFr),e(ko,o2e),e(o2e,eCr),e(ko,oCr),g(W0,ko,null),b(d,O9e,_),b(d,om,_),e(om,uE),e(uE,r2e),g(Q0,r2e,null),e(om,rCr),e(om,t2e),e(t2e,tCr),b(d,X9e,_),b(d,xr,_),g(H0,xr,null),e(xr,aCr),e(xr,rm),e(rm,sCr),e(rm,a2e),e(a2e,nCr),e(rm,lCr),e(rm,s2e),e(s2e,iCr),e(rm,dCr),e(xr,cCr),e(xr,U0),e(U0,mCr),e(U0,n2e),e(n2e,fCr),e(U0,gCr),e(xr,hCr),e(xr,Et),g(J0,Et,null),e(Et,uCr),e(Et,l2e),e(l2e,pCr),e(Et,_Cr),e(Et,tm),e(tm,bCr),e(tm,i2e),e(i2e,vCr),e(tm,TCr),e(tm,d2e),e(d2e,FCr),e(tm,CCr),e(Et,MCr),e(Et,c2e),e(c2e,ECr),e(Et,yCr),g(Y0,Et,null),e(xr,wCr),e(xr,xo),g(K0,xo,null),e(xo,ACr),e(xo,m2e),e(m2e,LCr),e(xo,BCr),e(xo,ws),e(ws,kCr),e(ws,f2e),e(f2e,xCr),e(ws,RCr),e(ws,g2e),e(g2e,SCr),e(ws,PCr),e(ws,h2e),e(h2e,$Cr),e(ws,ICr),e(xo,jCr),e(xo,Fe),e(Fe,pE),e(pE,u2e),e(u2e,NCr),e(pE,DCr),e(pE,lX),e(lX,qCr),e(pE,GCr),e(Fe,OCr),e(Fe,_E),e(_E,p2e),e(p2e,XCr),e(_E,zCr),e(_E,iX),e(iX,VCr),e(_E,WCr),e(Fe,QCr),e(Fe,bE),e(bE,_2e),e(_2e,HCr),e(bE,UCr),e(bE,dX),e(dX,JCr),e(bE,YCr),e(Fe,KCr),e(Fe,vE),e(vE,b2e),e(b2e,ZCr),e(vE,e4r),e(vE,cX),e(cX,o4r),e(vE,r4r),e(Fe,t4r),e(Fe,TE),e(TE,v2e),e(v2e,a4r),e(TE,s4r),e(TE,mX),e(mX,n4r),e(TE,l4r),e(Fe,i4r),e(Fe,FE),e(FE,T2e),e(T2e,d4r),e(FE,c4r),e(FE,fX),e(fX,m4r),e(FE,f4r),e(Fe,g4r),e(Fe,CE),e(CE,F2e),e(F2e,h4r),e(CE,u4r),e(CE,gX),e(gX,p4r),e(CE,_4r),e(Fe,b4r),e(Fe,ME),e(ME,C2e),e(C2e,v4r),e(ME,T4r),e(ME,hX),e(hX,F4r),e(ME,C4r),e(Fe,M4r),e(Fe,EE),e(EE,M2e),e(M2e,E4r),e(EE,y4r),e(EE,uX),e(uX,w4r),e(EE,A4r),e(xo,L4r),e(xo,E2e),e(E2e,B4r),e(xo,k4r),g(Z0,xo,null),b(d,z9e,_),b(d,am,_),e(am,yE),e(yE,y2e),g(eL,y2e,null),e(am,x4r),e(am,w2e),e(w2e,R4r),b(d,V9e,_),b(d,Rr,_),g(oL,Rr,null),e(Rr,S4r),e(Rr,sm),e(sm,P4r),e(sm,A2e),e(A2e,$4r),e(sm,I4r),e(sm,L2e),e(L2e,j4r),e(sm,N4r),e(Rr,D4r),e(Rr,rL),e(rL,q4r),e(rL,B2e),e(B2e,G4r),e(rL,O4r),e(Rr,X4r),e(Rr,yt),g(tL,yt,null),e(yt,z4r),e(yt,k2e),e(k2e,V4r),e(yt,W4r),e(yt,nm),e(nm,Q4r),e(nm,x2e),e(x2e,H4r),e(nm,U4r),e(nm,R2e),e(R2e,J4r),e(nm,Y4r),e(yt,K4r),e(yt,S2e),e(S2e,Z4r),e(yt,eMr),g(aL,yt,null),e(Rr,oMr),e(Rr,Ro),g(sL,Ro,null),e(Ro,rMr),e(Ro,P2e),e(P2e,tMr),e(Ro,aMr),e(Ro,As),e(As,sMr),e(As,$2e),e($2e,nMr),e(As,lMr),e(As,I2e),e(I2e,iMr),e(As,dMr),e(As,j2e),e(j2e,cMr),e(As,mMr),e(Ro,fMr),e(Ro,Ce),e(Ce,wE),e(wE,N2e),e(N2e,gMr),e(wE,hMr),e(wE,pX),e(pX,uMr),e(wE,pMr),e(Ce,_Mr),e(Ce,AE),e(AE,D2e),e(D2e,bMr),e(AE,vMr),e(AE,_X),e(_X,TMr),e(AE,FMr),e(Ce,CMr),e(Ce,LE),e(LE,q2e),e(q2e,MMr),e(LE,EMr),e(LE,bX),e(bX,yMr),e(LE,wMr),e(Ce,AMr),e(Ce,BE),e(BE,G2e),e(G2e,LMr),e(BE,BMr),e(BE,vX),e(vX,kMr),e(BE,xMr),e(Ce,RMr),e(Ce,kE),e(kE,O2e),e(O2e,SMr),e(kE,PMr),e(kE,TX),e(TX,$Mr),e(kE,IMr),e(Ce,jMr),e(Ce,xE),e(xE,X2e),e(X2e,NMr),e(xE,DMr),e(xE,FX),e(FX,qMr),e(xE,GMr),e(Ce,OMr),e(Ce,RE),e(RE,z2e),e(z2e,XMr),e(RE,zMr),e(RE,CX),e(CX,VMr),e(RE,WMr),e(Ce,QMr),e(Ce,SE),e(SE,V2e),e(V2e,HMr),e(SE,UMr),e(SE,MX),e(MX,JMr),e(SE,YMr),e(Ce,KMr),e(Ce,PE),e(PE,W2e),e(W2e,ZMr),e(PE,eEr),e(PE,EX),e(EX,oEr),e(PE,rEr),e(Ro,tEr),e(Ro,Q2e),e(Q2e,aEr),e(Ro,sEr),g(nL,Ro,null),b(d,W9e,_),b(d,lm,_),e(lm,$E),e($E,H2e),g(lL,H2e,null),e(lm,nEr),e(lm,U2e),e(U2e,lEr),b(d,Q9e,_),b(d,Sr,_),g(iL,Sr,null),e(Sr,iEr),e(Sr,im),e(im,dEr),e(im,J2e),e(J2e,cEr),e(im,mEr),e(im,Y2e),e(Y2e,fEr),e(im,gEr),e(Sr,hEr),e(Sr,dL),e(dL,uEr),e(dL,K2e),e(K2e,pEr),e(dL,_Er),e(Sr,bEr),e(Sr,wt),g(cL,wt,null),e(wt,vEr),e(wt,Z2e),e(Z2e,TEr),e(wt,FEr),e(wt,dm),e(dm,CEr),e(dm,eve),e(eve,MEr),e(dm,EEr),e(dm,ove),e(ove,yEr),e(dm,wEr),e(wt,AEr),e(wt,rve),e(rve,LEr),e(wt,BEr),g(mL,wt,null),e(Sr,kEr),e(Sr,So),g(fL,So,null),e(So,xEr),e(So,tve),e(tve,REr),e(So,SEr),e(So,Ls),e(Ls,PEr),e(Ls,ave),e(ave,$Er),e(Ls,IEr),e(Ls,sve),e(sve,jEr),e(Ls,NEr),e(Ls,nve),e(nve,DEr),e(Ls,qEr),e(So,GEr),e(So,no),e(no,IE),e(IE,lve),e(lve,OEr),e(IE,XEr),e(IE,yX),e(yX,zEr),e(IE,VEr),e(no,WEr),e(no,jE),e(jE,ive),e(ive,QEr),e(jE,HEr),e(jE,wX),e(wX,UEr),e(jE,JEr),e(no,YEr),e(no,NE),e(NE,dve),e(dve,KEr),e(NE,ZEr),e(NE,AX),e(AX,e3r),e(NE,o3r),e(no,r3r),e(no,DE),e(DE,cve),e(cve,t3r),e(DE,a3r),e(DE,LX),e(LX,s3r),e(DE,n3r),e(no,l3r),e(no,qE),e(qE,mve),e(mve,i3r),e(qE,d3r),e(qE,BX),e(BX,c3r),e(qE,m3r),e(no,f3r),e(no,GE),e(GE,fve),e(fve,g3r),e(GE,h3r),e(GE,kX),e(kX,u3r),e(GE,p3r),e(no,_3r),e(no,OE),e(OE,gve),e(gve,b3r),e(OE,v3r),e(OE,xX),e(xX,T3r),e(OE,F3r),e(So,C3r),e(So,hve),e(hve,M3r),e(So,E3r),g(gL,So,null),b(d,H9e,_),b(d,cm,_),e(cm,XE),e(XE,uve),g(hL,uve,null),e(cm,y3r),e(cm,pve),e(pve,w3r),b(d,U9e,_),b(d,Pr,_),g(uL,Pr,null),e(Pr,A3r),e(Pr,mm),e(mm,L3r),e(mm,_ve),e(_ve,B3r),e(mm,k3r),e(mm,bve),e(bve,x3r),e(mm,R3r),e(Pr,S3r),e(Pr,pL),e(pL,P3r),e(pL,vve),e(vve,$3r),e(pL,I3r),e(Pr,j3r),e(Pr,At),g(_L,At,null),e(At,N3r),e(At,Tve),e(Tve,D3r),e(At,q3r),e(At,fm),e(fm,G3r),e(fm,Fve),e(Fve,O3r),e(fm,X3r),e(fm,Cve),e(Cve,z3r),e(fm,V3r),e(At,W3r),e(At,Mve),e(Mve,Q3r),e(At,H3r),g(bL,At,null),e(Pr,U3r),e(Pr,Po),g(vL,Po,null),e(Po,J3r),e(Po,Eve),e(Eve,Y3r),e(Po,K3r),e(Po,Bs),e(Bs,Z3r),e(Bs,yve),e(yve,e5r),e(Bs,o5r),e(Bs,wve),e(wve,r5r),e(Bs,t5r),e(Bs,Ave),e(Ave,a5r),e(Bs,s5r),e(Po,n5r),e(Po,lo),e(lo,zE),e(zE,Lve),e(Lve,l5r),e(zE,i5r),e(zE,RX),e(RX,d5r),e(zE,c5r),e(lo,m5r),e(lo,VE),e(VE,Bve),e(Bve,f5r),e(VE,g5r),e(VE,SX),e(SX,h5r),e(VE,u5r),e(lo,p5r),e(lo,WE),e(WE,kve),e(kve,_5r),e(WE,b5r),e(WE,PX),e(PX,v5r),e(WE,T5r),e(lo,F5r),e(lo,QE),e(QE,xve),e(xve,C5r),e(QE,M5r),e(QE,$X),e($X,E5r),e(QE,y5r),e(lo,w5r),e(lo,HE),e(HE,Rve),e(Rve,A5r),e(HE,L5r),e(HE,IX),e(IX,B5r),e(HE,k5r),e(lo,x5r),e(lo,UE),e(UE,Sve),e(Sve,R5r),e(UE,S5r),e(UE,jX),e(jX,P5r),e(UE,$5r),e(lo,I5r),e(lo,JE),e(JE,Pve),e(Pve,j5r),e(JE,N5r),e(JE,NX),e(NX,D5r),e(JE,q5r),e(Po,G5r),e(Po,$ve),e($ve,O5r),e(Po,X5r),g(TL,Po,null),b(d,J9e,_),b(d,gm,_),e(gm,YE),e(YE,Ive),g(FL,Ive,null),e(gm,z5r),e(gm,jve),e(jve,V5r),b(d,Y9e,_),b(d,$r,_),g(CL,$r,null),e($r,W5r),e($r,hm),e(hm,Q5r),e(hm,Nve),e(Nve,H5r),e(hm,U5r),e(hm,Dve),e(Dve,J5r),e(hm,Y5r),e($r,K5r),e($r,ML),e(ML,Z5r),e(ML,qve),e(qve,eyr),e(ML,oyr),e($r,ryr),e($r,Lt),g(EL,Lt,null),e(Lt,tyr),e(Lt,Gve),e(Gve,ayr),e(Lt,syr),e(Lt,um),e(um,nyr),e(um,Ove),e(Ove,lyr),e(um,iyr),e(um,Xve),e(Xve,dyr),e(um,cyr),e(Lt,myr),e(Lt,zve),e(zve,fyr),e(Lt,gyr),g(yL,Lt,null),e($r,hyr),e($r,$o),g(wL,$o,null),e($o,uyr),e($o,Vve),e(Vve,pyr),e($o,_yr),e($o,ks),e(ks,byr),e(ks,Wve),e(Wve,vyr),e(ks,Tyr),e(ks,Qve),e(Qve,Fyr),e(ks,Cyr),e(ks,Hve),e(Hve,Myr),e(ks,Eyr),e($o,yyr),e($o,Uve),e(Uve,KE),e(KE,Jve),e(Jve,wyr),e(KE,Ayr),e(KE,DX),e(DX,Lyr),e(KE,Byr),e($o,kyr),e($o,Yve),e(Yve,xyr),e($o,Ryr),g(AL,$o,null),b(d,K9e,_),b(d,pm,_),e(pm,ZE),e(ZE,Kve),g(LL,Kve,null),e(pm,Syr),e(pm,Zve),e(Zve,Pyr),b(d,Z9e,_),b(d,Ir,_),g(BL,Ir,null),e(Ir,$yr),e(Ir,_m),e(_m,Iyr),e(_m,eTe),e(eTe,jyr),e(_m,Nyr),e(_m,oTe),e(oTe,Dyr),e(_m,qyr),e(Ir,Gyr),e(Ir,kL),e(kL,Oyr),e(kL,rTe),e(rTe,Xyr),e(kL,zyr),e(Ir,Vyr),e(Ir,Bt),g(xL,Bt,null),e(Bt,Wyr),e(Bt,tTe),e(tTe,Qyr),e(Bt,Hyr),e(Bt,bm),e(bm,Uyr),e(bm,aTe),e(aTe,Jyr),e(bm,Yyr),e(bm,sTe),e(sTe,Kyr),e(bm,Zyr),e(Bt,ewr),e(Bt,nTe),e(nTe,owr),e(Bt,rwr),g(RL,Bt,null),e(Ir,twr),e(Ir,Io),g(SL,Io,null),e(Io,awr),e(Io,lTe),e(lTe,swr),e(Io,nwr),e(Io,xs),e(xs,lwr),e(xs,iTe),e(iTe,iwr),e(xs,dwr),e(xs,dTe),e(dTe,cwr),e(xs,mwr),e(xs,cTe),e(cTe,fwr),e(xs,gwr),e(Io,hwr),e(Io,PL),e(PL,e3),e(e3,mTe),e(mTe,uwr),e(e3,pwr),e(e3,qX),e(qX,_wr),e(e3,bwr),e(PL,vwr),e(PL,o3),e(o3,fTe),e(fTe,Twr),e(o3,Fwr),e(o3,GX),e(GX,Cwr),e(o3,Mwr),e(Io,Ewr),e(Io,gTe),e(gTe,ywr),e(Io,wwr),g($L,Io,null),b(d,eBe,_),b(d,vm,_),e(vm,r3),e(r3,hTe),g(IL,hTe,null),e(vm,Awr),e(vm,uTe),e(uTe,Lwr),b(d,oBe,_),b(d,jr,_),g(jL,jr,null),e(jr,Bwr),e(jr,Tm),e(Tm,kwr),e(Tm,pTe),e(pTe,xwr),e(Tm,Rwr),e(Tm,_Te),e(_Te,Swr),e(Tm,Pwr),e(jr,$wr),e(jr,NL),e(NL,Iwr),e(NL,bTe),e(bTe,jwr),e(NL,Nwr),e(jr,Dwr),e(jr,kt),g(DL,kt,null),e(kt,qwr),e(kt,vTe),e(vTe,Gwr),e(kt,Owr),e(kt,Fm),e(Fm,Xwr),e(Fm,TTe),e(TTe,zwr),e(Fm,Vwr),e(Fm,FTe),e(FTe,Wwr),e(Fm,Qwr),e(kt,Hwr),e(kt,CTe),e(CTe,Uwr),e(kt,Jwr),g(qL,kt,null),e(jr,Ywr),e(jr,jo),g(GL,jo,null),e(jo,Kwr),e(jo,MTe),e(MTe,Zwr),e(jo,eAr),e(jo,Rs),e(Rs,oAr),e(Rs,ETe),e(ETe,rAr),e(Rs,tAr),e(Rs,yTe),e(yTe,aAr),e(Rs,sAr),e(Rs,wTe),e(wTe,nAr),e(Rs,lAr),e(jo,iAr),e(jo,ATe),e(ATe,t3),e(t3,LTe),e(LTe,dAr),e(t3,cAr),e(t3,OX),e(OX,mAr),e(t3,fAr),e(jo,gAr),e(jo,BTe),e(BTe,hAr),e(jo,uAr),g(OL,jo,null),rBe=!0},p(d,[_]){const XL={};_&2&&(XL.$$scope={dirty:_,ctx:d}),Lm.$set(XL);const kTe={};_&2&&(kTe.$$scope={dirty:_,ctx:d}),ih.$set(kTe);const xTe={};_&2&&(xTe.$$scope={dirty:_,ctx:d}),vh.$set(xTe)},i(d){rBe||(h(ce.$$.fragment,d),h($a.$$.fragment,d),h(s5.$$.fragment,d),h(n5.$$.fragment,d),h(Lm.$$.fragment,d),h(l5.$$.fragment,d),h(i5.$$.fragment,d),h(m5.$$.fragment,d),h(f5.$$.fragment,d),h(g5.$$.fragment,d),h(h5.$$.fragment,d),h(u5.$$.fragment,d),h(b5.$$.fragment,d),h(v5.$$.fragment,d),h(T5.$$.fragment,d),h(F5.$$.fragment,d),h(C5.$$.fragment,d),h(y5.$$.fragment,d),h(ih.$$.fragment,d),h(w5.$$.fragment,d),h(A5.$$.fragment,d),h(L5.$$.fragment,d),h(B5.$$.fragment,d),h(R5.$$.fragment,d),h(vh.$$.fragment,d),h(S5.$$.fragment,d),h(P5.$$.fragment,d),h($5.$$.fragment,d),h(I5.$$.fragment,d),h(N5.$$.fragment,d),h(D5.$$.fragment,d),h(q5.$$.fragment,d),h(G5.$$.fragment,d),h(O5.$$.fragment,d),h(X5.$$.fragment,d),h(V5.$$.fragment,d),h(W5.$$.fragment,d),h(Q5.$$.fragment,d),h(H5.$$.fragment,d),h(U5.$$.fragment,d),h(J5.$$.fragment,d),h(K5.$$.fragment,d),h(Z5.$$.fragment,d),h(ey.$$.fragment,d),h(oy.$$.fragment,d),h(ry.$$.fragment,d),h(ty.$$.fragment,d),h(sy.$$.fragment,d),h(ny.$$.fragment,d),h(ly.$$.fragment,d),h(iy.$$.fragment,d),h(dy.$$.fragment,d),h(cy.$$.fragment,d),h(fy.$$.fragment,d),h(gy.$$.fragment,d),h(hy.$$.fragment,d),h(uy.$$.fragment,d),h(py.$$.fragment,d),h(_y.$$.fragment,d),h(vy.$$.fragment,d),h(Ty.$$.fragment,d),h(Fy.$$.fragment,d),h(Cy.$$.fragment,d),h(My.$$.fragment,d),h(Ey.$$.fragment,d),h(wy.$$.fragment,d),h(Ay.$$.fragment,d),h(Ly.$$.fragment,d),h(By.$$.fragment,d),h(ky.$$.fragment,d),h(xy.$$.fragment,d),h(Sy.$$.fragment,d),h(Py.$$.fragment,d),h($y.$$.fragment,d),h(Iy.$$.fragment,d),h(jy.$$.fragment,d),h(Ny.$$.fragment,d),h(qy.$$.fragment,d),h(Gy.$$.fragment,d),h(Oy.$$.fragment,d),h(Xy.$$.fragment,d),h(zy.$$.fragment,d),h(Vy.$$.fragment,d),h(Qy.$$.fragment,d),h(Hy.$$.fragment,d),h(Uy.$$.fragment,d),h(Jy.$$.fragment,d),h(Yy.$$.fragment,d),h(Ky.$$.fragment,d),h(ew.$$.fragment,d),h(ow.$$.fragment,d),h(rw.$$.fragment,d),h(tw.$$.fragment,d),h(aw.$$.fragment,d),h(sw.$$.fragment,d),h(lw.$$.fragment,d),h(iw.$$.fragment,d),h(dw.$$.fragment,d),h(cw.$$.fragment,d),h(mw.$$.fragment,d),h(fw.$$.fragment,d),h(hw.$$.fragment,d),h(uw.$$.fragment,d),h(pw.$$.fragment,d),h(_w.$$.fragment,d),h(bw.$$.fragment,d),h(vw.$$.fragment,d),h(Fw.$$.fragment,d),h(Cw.$$.fragment,d),h(Mw.$$.fragment,d),h(Ew.$$.fragment,d),h(yw.$$.fragment,d),h(ww.$$.fragment,d),h(Lw.$$.fragment,d),h(Bw.$$.fragment,d),h(kw.$$.fragment,d),h(xw.$$.fragment,d),h(Rw.$$.fragment,d),h(Sw.$$.fragment,d),h($w.$$.fragment,d),h(Iw.$$.fragment,d),h(jw.$$.fragment,d),h(Nw.$$.fragment,d),h(Dw.$$.fragment,d),h(qw.$$.fragment,d),h(Ow.$$.fragment,d),h(Xw.$$.fragment,d),h(zw.$$.fragment,d),h(Ww.$$.fragment,d),h(Qw.$$.fragment,d),h(Hw.$$.fragment,d),h(Jw.$$.fragment,d),h(Yw.$$.fragment,d),h(Kw.$$.fragment,d),h(Zw.$$.fragment,d),h(eA.$$.fragment,d),h(oA.$$.fragment,d),h(tA.$$.fragment,d),h(aA.$$.fragment,d),h(sA.$$.fragment,d),h(nA.$$.fragment,d),h(lA.$$.fragment,d),h(iA.$$.fragment,d),h(cA.$$.fragment,d),h(mA.$$.fragment,d),h(fA.$$.fragment,d),h(gA.$$.fragment,d),h(hA.$$.fragment,d),h(uA.$$.fragment,d),h(_A.$$.fragment,d),h(bA.$$.fragment,d),h(vA.$$.fragment,d),h(TA.$$.fragment,d),h(FA.$$.fragment,d),h(CA.$$.fragment,d),h(EA.$$.fragment,d),h(yA.$$.fragment,d),h(wA.$$.fragment,d),h(LA.$$.fragment,d),h(BA.$$.fragment,d),h(kA.$$.fragment,d),h(RA.$$.fragment,d),h(SA.$$.fragment,d),h(PA.$$.fragment,d),h($A.$$.fragment,d),h(IA.$$.fragment,d),h(jA.$$.fragment,d),h(DA.$$.fragment,d),h(qA.$$.fragment,d),h(GA.$$.fragment,d),h(OA.$$.fragment,d),h(XA.$$.fragment,d),h(zA.$$.fragment,d),h(WA.$$.fragment,d),h(QA.$$.fragment,d),h(HA.$$.fragment,d),h(UA.$$.fragment,d),h(JA.$$.fragment,d),h(YA.$$.fragment,d),h(ZA.$$.fragment,d),h(e6.$$.fragment,d),h(o6.$$.fragment,d),h(r6.$$.fragment,d),h(t6.$$.fragment,d),h(a6.$$.fragment,d),h(n6.$$.fragment,d),h(l6.$$.fragment,d),h(i6.$$.fragment,d),h(d6.$$.fragment,d),h(c6.$$.fragment,d),h(m6.$$.fragment,d),h(g6.$$.fragment,d),h(h6.$$.fragment,d),h(u6.$$.fragment,d),h(p6.$$.fragment,d),h(_6.$$.fragment,d),h(b6.$$.fragment,d),h(T6.$$.fragment,d),h(F6.$$.fragment,d),h(C6.$$.fragment,d),h(M6.$$.fragment,d),h(E6.$$.fragment,d),h(y6.$$.fragment,d),h(A6.$$.fragment,d),h(L6.$$.fragment,d),h(B6.$$.fragment,d),h(k6.$$.fragment,d),h(x6.$$.fragment,d),h(R6.$$.fragment,d),h(P6.$$.fragment,d),h($6.$$.fragment,d),h(I6.$$.fragment,d),h(j6.$$.fragment,d),h(N6.$$.fragment,d),h(D6.$$.fragment,d),h(G6.$$.fragment,d),h(O6.$$.fragment,d),h(X6.$$.fragment,d),h(z6.$$.fragment,d),h(V6.$$.fragment,d),h(W6.$$.fragment,d),h(H6.$$.fragment,d),h(U6.$$.fragment,d),h(J6.$$.fragment,d),h(Y6.$$.fragment,d),h(K6.$$.fragment,d),h(Z6.$$.fragment,d),h(o0.$$.fragment,d),h(r0.$$.fragment,d),h(t0.$$.fragment,d),h(a0.$$.fragment,d),h(s0.$$.fragment,d),h(n0.$$.fragment,d),h(i0.$$.fragment,d),h(d0.$$.fragment,d),h(c0.$$.fragment,d),h(m0.$$.fragment,d),h(f0.$$.fragment,d),h(g0.$$.fragment,d),h(u0.$$.fragment,d),h(p0.$$.fragment,d),h(_0.$$.fragment,d),h(b0.$$.fragment,d),h(v0.$$.fragment,d),h(T0.$$.fragment,d),h(C0.$$.fragment,d),h(M0.$$.fragment,d),h(E0.$$.fragment,d),h(y0.$$.fragment,d),h(w0.$$.fragment,d),h(A0.$$.fragment,d),h(B0.$$.fragment,d),h(k0.$$.fragment,d),h(x0.$$.fragment,d),h(R0.$$.fragment,d),h(S0.$$.fragment,d),h(P0.$$.fragment,d),h(I0.$$.fragment,d),h(j0.$$.fragment,d),h(N0.$$.fragment,d),h(D0.$$.fragment,d),h(q0.$$.fragment,d),h(G0.$$.fragment,d),h(X0.$$.fragment,d),h(z0.$$.fragment,d),h(V0.$$.fragment,d),h(W0.$$.fragment,d),h(Q0.$$.fragment,d),h(H0.$$.fragment,d),h(J0.$$.fragment,d),h(Y0.$$.fragment,d),h(K0.$$.fragment,d),h(Z0.$$.fragment,d),h(eL.$$.fragment,d),h(oL.$$.fragment,d),h(tL.$$.fragment,d),h(aL.$$.fragment,d),h(sL.$$.fragment,d),h(nL.$$.fragment,d),h(lL.$$.fragment,d),h(iL.$$.fragment,d),h(cL.$$.fragment,d),h(mL.$$.fragment,d),h(fL.$$.fragment,d),h(gL.$$.fragment,d),h(hL.$$.fragment,d),h(uL.$$.fragment,d),h(_L.$$.fragment,d),h(bL.$$.fragment,d),h(vL.$$.fragment,d),h(TL.$$.fragment,d),h(FL.$$.fragment,d),h(CL.$$.fragment,d),h(EL.$$.fragment,d),h(yL.$$.fragment,d),h(wL.$$.fragment,d),h(AL.$$.fragment,d),h(LL.$$.fragment,d),h(BL.$$.fragment,d),h(xL.$$.fragment,d),h(RL.$$.fragment,d),h(SL.$$.fragment,d),h($L.$$.fragment,d),h(IL.$$.fragment,d),h(jL.$$.fragment,d),h(DL.$$.fragment,d),h(qL.$$.fragment,d),h(GL.$$.fragment,d),h(OL.$$.fragment,d),rBe=!0)},o(d){u(ce.$$.fragment,d),u($a.$$.fragment,d),u(s5.$$.fragment,d),u(n5.$$.fragment,d),u(Lm.$$.fragment,d),u(l5.$$.fragment,d),u(i5.$$.fragment,d),u(m5.$$.fragment,d),u(f5.$$.fragment,d),u(g5.$$.fragment,d),u(h5.$$.fragment,d),u(u5.$$.fragment,d),u(b5.$$.fragment,d),u(v5.$$.fragment,d),u(T5.$$.fragment,d),u(F5.$$.fragment,d),u(C5.$$.fragment,d),u(y5.$$.fragment,d),u(ih.$$.fragment,d),u(w5.$$.fragment,d),u(A5.$$.fragment,d),u(L5.$$.fragment,d),u(B5.$$.fragment,d),u(R5.$$.fragment,d),u(vh.$$.fragment,d),u(S5.$$.fragment,d),u(P5.$$.fragment,d),u($5.$$.fragment,d),u(I5.$$.fragment,d),u(N5.$$.fragment,d),u(D5.$$.fragment,d),u(q5.$$.fragment,d),u(G5.$$.fragment,d),u(O5.$$.fragment,d),u(X5.$$.fragment,d),u(V5.$$.fragment,d),u(W5.$$.fragment,d),u(Q5.$$.fragment,d),u(H5.$$.fragment,d),u(U5.$$.fragment,d),u(J5.$$.fragment,d),u(K5.$$.fragment,d),u(Z5.$$.fragment,d),u(ey.$$.fragment,d),u(oy.$$.fragment,d),u(ry.$$.fragment,d),u(ty.$$.fragment,d),u(sy.$$.fragment,d),u(ny.$$.fragment,d),u(ly.$$.fragment,d),u(iy.$$.fragment,d),u(dy.$$.fragment,d),u(cy.$$.fragment,d),u(fy.$$.fragment,d),u(gy.$$.fragment,d),u(hy.$$.fragment,d),u(uy.$$.fragment,d),u(py.$$.fragment,d),u(_y.$$.fragment,d),u(vy.$$.fragment,d),u(Ty.$$.fragment,d),u(Fy.$$.fragment,d),u(Cy.$$.fragment,d),u(My.$$.fragment,d),u(Ey.$$.fragment,d),u(wy.$$.fragment,d),u(Ay.$$.fragment,d),u(Ly.$$.fragment,d),u(By.$$.fragment,d),u(ky.$$.fragment,d),u(xy.$$.fragment,d),u(Sy.$$.fragment,d),u(Py.$$.fragment,d),u($y.$$.fragment,d),u(Iy.$$.fragment,d),u(jy.$$.fragment,d),u(Ny.$$.fragment,d),u(qy.$$.fragment,d),u(Gy.$$.fragment,d),u(Oy.$$.fragment,d),u(Xy.$$.fragment,d),u(zy.$$.fragment,d),u(Vy.$$.fragment,d),u(Qy.$$.fragment,d),u(Hy.$$.fragment,d),u(Uy.$$.fragment,d),u(Jy.$$.fragment,d),u(Yy.$$.fragment,d),u(Ky.$$.fragment,d),u(ew.$$.fragment,d),u(ow.$$.fragment,d),u(rw.$$.fragment,d),u(tw.$$.fragment,d),u(aw.$$.fragment,d),u(sw.$$.fragment,d),u(lw.$$.fragment,d),u(iw.$$.fragment,d),u(dw.$$.fragment,d),u(cw.$$.fragment,d),u(mw.$$.fragment,d),u(fw.$$.fragment,d),u(hw.$$.fragment,d),u(uw.$$.fragment,d),u(pw.$$.fragment,d),u(_w.$$.fragment,d),u(bw.$$.fragment,d),u(vw.$$.fragment,d),u(Fw.$$.fragment,d),u(Cw.$$.fragment,d),u(Mw.$$.fragment,d),u(Ew.$$.fragment,d),u(yw.$$.fragment,d),u(ww.$$.fragment,d),u(Lw.$$.fragment,d),u(Bw.$$.fragment,d),u(kw.$$.fragment,d),u(xw.$$.fragment,d),u(Rw.$$.fragment,d),u(Sw.$$.fragment,d),u($w.$$.fragment,d),u(Iw.$$.fragment,d),u(jw.$$.fragment,d),u(Nw.$$.fragment,d),u(Dw.$$.fragment,d),u(qw.$$.fragment,d),u(Ow.$$.fragment,d),u(Xw.$$.fragment,d),u(zw.$$.fragment,d),u(Ww.$$.fragment,d),u(Qw.$$.fragment,d),u(Hw.$$.fragment,d),u(Jw.$$.fragment,d),u(Yw.$$.fragment,d),u(Kw.$$.fragment,d),u(Zw.$$.fragment,d),u(eA.$$.fragment,d),u(oA.$$.fragment,d),u(tA.$$.fragment,d),u(aA.$$.fragment,d),u(sA.$$.fragment,d),u(nA.$$.fragment,d),u(lA.$$.fragment,d),u(iA.$$.fragment,d),u(cA.$$.fragment,d),u(mA.$$.fragment,d),u(fA.$$.fragment,d),u(gA.$$.fragment,d),u(hA.$$.fragment,d),u(uA.$$.fragment,d),u(_A.$$.fragment,d),u(bA.$$.fragment,d),u(vA.$$.fragment,d),u(TA.$$.fragment,d),u(FA.$$.fragment,d),u(CA.$$.fragment,d),u(EA.$$.fragment,d),u(yA.$$.fragment,d),u(wA.$$.fragment,d),u(LA.$$.fragment,d),u(BA.$$.fragment,d),u(kA.$$.fragment,d),u(RA.$$.fragment,d),u(SA.$$.fragment,d),u(PA.$$.fragment,d),u($A.$$.fragment,d),u(IA.$$.fragment,d),u(jA.$$.fragment,d),u(DA.$$.fragment,d),u(qA.$$.fragment,d),u(GA.$$.fragment,d),u(OA.$$.fragment,d),u(XA.$$.fragment,d),u(zA.$$.fragment,d),u(WA.$$.fragment,d),u(QA.$$.fragment,d),u(HA.$$.fragment,d),u(UA.$$.fragment,d),u(JA.$$.fragment,d),u(YA.$$.fragment,d),u(ZA.$$.fragment,d),u(e6.$$.fragment,d),u(o6.$$.fragment,d),u(r6.$$.fragment,d),u(t6.$$.fragment,d),u(a6.$$.fragment,d),u(n6.$$.fragment,d),u(l6.$$.fragment,d),u(i6.$$.fragment,d),u(d6.$$.fragment,d),u(c6.$$.fragment,d),u(m6.$$.fragment,d),u(g6.$$.fragment,d),u(h6.$$.fragment,d),u(u6.$$.fragment,d),u(p6.$$.fragment,d),u(_6.$$.fragment,d),u(b6.$$.fragment,d),u(T6.$$.fragment,d),u(F6.$$.fragment,d),u(C6.$$.fragment,d),u(M6.$$.fragment,d),u(E6.$$.fragment,d),u(y6.$$.fragment,d),u(A6.$$.fragment,d),u(L6.$$.fragment,d),u(B6.$$.fragment,d),u(k6.$$.fragment,d),u(x6.$$.fragment,d),u(R6.$$.fragment,d),u(P6.$$.fragment,d),u($6.$$.fragment,d),u(I6.$$.fragment,d),u(j6.$$.fragment,d),u(N6.$$.fragment,d),u(D6.$$.fragment,d),u(G6.$$.fragment,d),u(O6.$$.fragment,d),u(X6.$$.fragment,d),u(z6.$$.fragment,d),u(V6.$$.fragment,d),u(W6.$$.fragment,d),u(H6.$$.fragment,d),u(U6.$$.fragment,d),u(J6.$$.fragment,d),u(Y6.$$.fragment,d),u(K6.$$.fragment,d),u(Z6.$$.fragment,d),u(o0.$$.fragment,d),u(r0.$$.fragment,d),u(t0.$$.fragment,d),u(a0.$$.fragment,d),u(s0.$$.fragment,d),u(n0.$$.fragment,d),u(i0.$$.fragment,d),u(d0.$$.fragment,d),u(c0.$$.fragment,d),u(m0.$$.fragment,d),u(f0.$$.fragment,d),u(g0.$$.fragment,d),u(u0.$$.fragment,d),u(p0.$$.fragment,d),u(_0.$$.fragment,d),u(b0.$$.fragment,d),u(v0.$$.fragment,d),u(T0.$$.fragment,d),u(C0.$$.fragment,d),u(M0.$$.fragment,d),u(E0.$$.fragment,d),u(y0.$$.fragment,d),u(w0.$$.fragment,d),u(A0.$$.fragment,d),u(B0.$$.fragment,d),u(k0.$$.fragment,d),u(x0.$$.fragment,d),u(R0.$$.fragment,d),u(S0.$$.fragment,d),u(P0.$$.fragment,d),u(I0.$$.fragment,d),u(j0.$$.fragment,d),u(N0.$$.fragment,d),u(D0.$$.fragment,d),u(q0.$$.fragment,d),u(G0.$$.fragment,d),u(X0.$$.fragment,d),u(z0.$$.fragment,d),u(V0.$$.fragment,d),u(W0.$$.fragment,d),u(Q0.$$.fragment,d),u(H0.$$.fragment,d),u(J0.$$.fragment,d),u(Y0.$$.fragment,d),u(K0.$$.fragment,d),u(Z0.$$.fragment,d),u(eL.$$.fragment,d),u(oL.$$.fragment,d),u(tL.$$.fragment,d),u(aL.$$.fragment,d),u(sL.$$.fragment,d),u(nL.$$.fragment,d),u(lL.$$.fragment,d),u(iL.$$.fragment,d),u(cL.$$.fragment,d),u(mL.$$.fragment,d),u(fL.$$.fragment,d),u(gL.$$.fragment,d),u(hL.$$.fragment,d),u(uL.$$.fragment,d),u(_L.$$.fragment,d),u(bL.$$.fragment,d),u(vL.$$.fragment,d),u(TL.$$.fragment,d),u(FL.$$.fragment,d),u(CL.$$.fragment,d),u(EL.$$.fragment,d),u(yL.$$.fragment,d),u(wL.$$.fragment,d),u(AL.$$.fragment,d),u(LL.$$.fragment,d),u(BL.$$.fragment,d),u(xL.$$.fragment,d),u(RL.$$.fragment,d),u(SL.$$.fragment,d),u($L.$$.fragment,d),u(IL.$$.fragment,d),u(jL.$$.fragment,d),u(DL.$$.fragment,d),u(qL.$$.fragment,d),u(GL.$$.fragment,d),u(OL.$$.fragment,d),rBe=!1},d(d){t(J),d&&t(Ae),d&&t(ie),p(ce),d&&t(Mm),d&&t(na),d&&t(ye),d&&t(io),d&&t(ym),p($a,d),d&&t(co),d&&t(ge),d&&t(qo),d&&t(Ia),d&&t(t8e),d&&t(Si),p(s5),d&&t(a8e),d&&t(js),d&&t(s8e),p(n5,d),d&&t(n8e),d&&t(z7),d&&t(l8e),p(Lm,d),d&&t(i8e),d&&t(Pi),p(l5),d&&t(d8e),d&&t(Go),p(i5),p(m5),p(f5),p(g5),d&&t(c8e),d&&t(Ii),p(h5),d&&t(m8e),d&&t(Oo),p(u5),p(b5),p(v5),p(T5),d&&t(f8e),d&&t(ji),p(F5),d&&t(g8e),d&&t(Xo),p(C5),p(y5),p(ih),p(w5),p(A5),d&&t(h8e),d&&t(Ni),p(L5),d&&t(u8e),d&&t(zo),p(B5),p(R5),p(vh),p(S5),p(P5),d&&t(p8e),d&&t(qi),p($5),d&&t(_8e),d&&t(Vo),p(I5),p(N5),p(D5),p(q5),p(G5),d&&t(b8e),d&&t(Xi),p(O5),d&&t(v8e),d&&t(Wo),p(X5),p(V5),p(W5),p(Q5),p(H5),d&&t(T8e),d&&t(Wi),p(U5),d&&t(F8e),d&&t(Qo),p(J5),p(K5),p(Z5),p(ey),p(oy),d&&t(C8e),d&&t(Ui),p(ry),d&&t(M8e),d&&t(Ho),p(ty),p(sy),p(ny),p(ly),p(iy),d&&t(E8e),d&&t(Ki),p(dy),d&&t(y8e),d&&t(Uo),p(cy),p(fy),p(gy),p(hy),p(uy),d&&t(w8e),d&&t(od),p(py),d&&t(A8e),d&&t(Jo),p(_y),p(vy),p(Ty),p(Fy),p(Cy),d&&t(L8e),d&&t(ad),p(My),d&&t(B8e),d&&t(Yo),p(Ey),p(wy),p(Ay),p(Ly),p(By),d&&t(k8e),d&&t(ld),p(ky),d&&t(x8e),d&&t(Ko),p(xy),p(Sy),p(Py),p($y),p(Iy),d&&t(R8e),d&&t(cd),p(jy),d&&t(S8e),d&&t(Zo),p(Ny),p(qy),p(Gy),p(Oy),p(Xy),d&&t(P8e),d&&t(gd),p(zy),d&&t($8e),d&&t(er),p(Vy),p(Qy),p(Hy),p(Uy),p(Jy),d&&t(I8e),d&&t(pd),p(Yy),d&&t(j8e),d&&t(or),p(Ky),p(ew),p(ow),p(rw),p(tw),d&&t(N8e),d&&t(vd),p(aw),d&&t(D8e),d&&t(rr),p(sw),p(lw),p(iw),p(dw),p(cw),d&&t(q8e),d&&t(Cd),p(mw),d&&t(G8e),d&&t(tr),p(fw),p(hw),p(uw),p(pw),p(_w),d&&t(O8e),d&&t(yd),p(bw),d&&t(X8e),d&&t(ar),p(vw),p(Fw),p(Cw),p(Mw),p(Ew),d&&t(z8e),d&&t(Ld),p(yw),d&&t(V8e),d&&t(sr),p(ww),p(Lw),p(Bw),p(kw),p(xw),d&&t(W8e),d&&t(Rd),p(Rw),d&&t(Q8e),d&&t(nr),p(Sw),p($w),p(Iw),p(jw),p(Nw),d&&t(H8e),d&&t($d),p(Dw),d&&t(U8e),d&&t(lr),p(qw),p(Ow),p(Xw),p(zw),p(Ww),d&&t(J8e),d&&t(Nd),p(Qw),d&&t(Y8e),d&&t(ir),p(Hw),p(Jw),p(Yw),p(Kw),p(Zw),d&&t(K8e),d&&t(Od),p(eA),d&&t(Z8e),d&&t(dr),p(oA),p(tA),p(aA),p(sA),p(nA),d&&t(e9e),d&&t(Wd),p(lA),d&&t(o9e),d&&t(cr),p(iA),p(cA),p(mA),p(fA),p(gA),d&&t(r9e),d&&t(Ud),p(hA),d&&t(t9e),d&&t(mr),p(uA),p(_A),p(bA),p(vA),p(TA),d&&t(a9e),d&&t(Kd),p(FA),d&&t(s9e),d&&t(fr),p(CA),p(EA),p(yA),p(wA),p(LA),d&&t(n9e),d&&t(oc),p(BA),d&&t(l9e),d&&t(gr),p(kA),p(RA),p(SA),p(PA),p($A),d&&t(i9e),d&&t(ac),p(IA),d&&t(d9e),d&&t(hr),p(jA),p(DA),p(qA),p(GA),p(OA),d&&t(c9e),d&&t(lc),p(XA),d&&t(m9e),d&&t(ur),p(zA),p(WA),p(QA),p(HA),p(UA),d&&t(f9e),d&&t(cc),p(JA),d&&t(g9e),d&&t(pr),p(YA),p(ZA),p(e6),p(o6),p(r6),d&&t(h9e),d&&t(gc),p(t6),d&&t(u9e),d&&t(_r),p(a6),p(n6),p(l6),p(i6),p(d6),d&&t(p9e),d&&t(pc),p(c6),d&&t(_9e),d&&t(br),p(m6),p(g6),p(h6),p(u6),p(p6),d&&t(b9e),d&&t(vc),p(_6),d&&t(v9e),d&&t(vr),p(b6),p(T6),p(F6),p(C6),p(M6),d&&t(T9e),d&&t(Cc),p(E6),d&&t(F9e),d&&t(Tr),p(y6),p(A6),p(L6),p(B6),p(k6),d&&t(C9e),d&&t(yc),p(x6),d&&t(M9e),d&&t(Fr),p(R6),p(P6),p($6),p(I6),p(j6),d&&t(E9e),d&&t(Lc),p(N6),d&&t(y9e),d&&t(Cr),p(D6),p(G6),p(O6),p(X6),p(z6),d&&t(w9e),d&&t(xc),p(V6),d&&t(A9e),d&&t(Mr),p(W6),p(H6),p(U6),p(J6),p(Y6),d&&t(L9e),d&&t(Pc),p(K6),d&&t(B9e),d&&t(Er),p(Z6),p(o0),p(r0),p(t0),p(a0),d&&t(k9e),d&&t(jc),p(s0),d&&t(x9e),d&&t(yr),p(n0),p(i0),p(d0),p(c0),p(m0),d&&t(R9e),d&&t(qc),p(f0),d&&t(S9e),d&&t(wr),p(g0),p(u0),p(p0),p(_0),p(b0),d&&t(P9e),d&&t(Xc),p(v0),d&&t($9e),d&&t(Ar),p(T0),p(C0),p(M0),p(E0),p(y0),d&&t(I9e),d&&t(Wc),p(w0),d&&t(j9e),d&&t(Lr),p(A0),p(B0),p(k0),p(x0),p(R0),d&&t(N9e),d&&t(Uc),p(S0),d&&t(D9e),d&&t(Br),p(P0),p(I0),p(j0),p(N0),p(D0),d&&t(q9e),d&&t(Kc),p(q0),d&&t(G9e),d&&t(kr),p(G0),p(X0),p(z0),p(V0),p(W0),d&&t(O9e),d&&t(om),p(Q0),d&&t(X9e),d&&t(xr),p(H0),p(J0),p(Y0),p(K0),p(Z0),d&&t(z9e),d&&t(am),p(eL),d&&t(V9e),d&&t(Rr),p(oL),p(tL),p(aL),p(sL),p(nL),d&&t(W9e),d&&t(lm),p(lL),d&&t(Q9e),d&&t(Sr),p(iL),p(cL),p(mL),p(fL),p(gL),d&&t(H9e),d&&t(cm),p(hL),d&&t(U9e),d&&t(Pr),p(uL),p(_L),p(bL),p(vL),p(TL),d&&t(J9e),d&&t(gm),p(FL),d&&t(Y9e),d&&t($r),p(CL),p(EL),p(yL),p(wL),p(AL),d&&t(K9e),d&&t(pm),p(LL),d&&t(Z9e),d&&t(Ir),p(BL),p(xL),p(RL),p(SL),p($L),d&&t(eBe),d&&t(vm),p(IL),d&&t(oBe),d&&t(jr),p(jL),p(DL),p(qL),p(GL),p(OL)}}}const ypt={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function wpt(yi,J,Ae){let{fw:ie}=J;return yi.$$set=fe=>{"fw"in fe&&Ae(0,ie=fe.fw)},[ie]}class Spt extends _pt{constructor(J){super();bpt(this,J,wpt,Ept,vpt,{fw:0})}}export{Spt as default,ypt as metadata};
