import{S as lc,i as pc,s as hc,e as o,k as h,w as g,t as i,U as mc,M as fc,c as a,d as t,m,a as n,x as _,h as l,V as cc,b as c,F as s,g as p,y as w,q as b,o as y,B as $}from"../chunks/vendor-ab4e3193.js";import{T as me}from"../chunks/Tip-b5c6375a.js";import{I as z}from"../chunks/IconCopyLink-d992940d.js";import{C as D}from"../chunks/CodeBlock-516df0c5.js";import"../chunks/CopyButton-204b56db.js";function dc(O){let f,v;return{c(){f=o("p"),v=i(`The models showcased here are close to fully feature complete, but do lack some features that are currently in
development. Namely, the ability to handle the past key values for decoder models is currently in the works.`)},l(d){f=a(d,"P",{});var u=n(f);v=l(u,`The models showcased here are close to fully feature complete, but do lack some features that are currently in
development. Namely, the ability to handle the past key values for decoder models is currently in the works.`),u.forEach(t)},m(d,u){p(d,f,u),s(f,v)},d(d){d&&t(f)}}}function uc(O){let f,v;return{c(){f=o("p"),v=i("The approach detailed here is bing deprecated. We recommend you follow the part above for an up to date approach.")},l(d){f=a(d,"P",{});var u=n(f);v=l(u,"The approach detailed here is bing deprecated. We recommend you follow the part above for an up to date approach."),u.forEach(t)},m(d,u){p(d,f,u),s(f,v)},d(d){d&&t(f)}}}function vc(O){let f,v;return{c(){f=o("p"),v=i(`Currently, inputs and outputs are always exported with dynamic sequence axes preventing some optimizations on the
ONNX Runtime. If you would like to see such support for fixed-length inputs/outputs, please open up an issue on
transformers.`)},l(d){f=a(d,"P",{});var u=n(f);v=l(u,`Currently, inputs and outputs are always exported with dynamic sequence axes preventing some optimizations on the
ONNX Runtime. If you would like to see such support for fixed-length inputs/outputs, please open up an issue on
transformers.`),u.forEach(t)},m(d,u){p(d,f,u),s(f,v)},d(d){d&&t(f)}}}function gc(O){let f,v,d,u,j;return{c(){f=o("p"),v=i("When quantization is enabled (see below), "),d=o("code"),u=i("convert_graph_to_onnx.py"),j=i(` script will enable optimizations on the
model because quantization would modify the underlying graph making it impossible for ONNX runtime to do the
optimizations afterwards.`)},l(E){f=a(E,"P",{});var N=n(f);v=l(N,"When quantization is enabled (see below), "),d=a(N,"CODE",{});var P=n(d);u=l(P,"convert_graph_to_onnx.py"),P.forEach(t),j=l(N,` script will enable optimizations on the
model because quantization would modify the underlying graph making it impossible for ONNX runtime to do the
optimizations afterwards.`),N.forEach(t)},m(E,N){p(E,f,N),s(f,v),s(f,d),s(d,u),s(f,j)},d(E){E&&t(f)}}}function _c(O){let f,v,d,u,j;return{c(){f=o("p"),v=i("For more information about the optimizations enabled by ONNXRuntime, please have a look at the "),d=o("a"),u=i("ONNXRuntime Github"),j=i("."),this.h()},l(E){f=a(E,"P",{});var N=n(f);v=l(N,"For more information about the optimizations enabled by ONNXRuntime, please have a look at the "),d=a(N,"A",{href:!0,rel:!0});var P=n(d);u=l(P,"ONNXRuntime Github"),P.forEach(t),j=l(N,"."),N.forEach(t),this.h()},h(){c(d,"href","https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers"),c(d,"rel","nofollow")},m(E,N){p(E,f,N),s(f,v),s(f,d),s(d,u),s(f,j)},d(E){E&&t(f)}}}function wc(O){let f,v,d,u,j,E,N,P;return{c(){f=o("p"),v=i("The quantization process will infer the parameter "),d=o("em"),u=i("scale"),j=i(" and "),E=o("em"),N=i("zero_point"),P=i(" from the neural network parameters")},l(q){f=a(q,"P",{});var T=n(f);v=l(T,"The quantization process will infer the parameter "),d=a(T,"EM",{});var A=n(d);u=l(A,"scale"),A.forEach(t),j=l(T," and "),E=a(T,"EM",{});var S=n(E);N=l(S,"zero_point"),S.forEach(t),P=l(T," from the neural network parameters"),T.forEach(t)},m(q,T){p(q,f,T),s(f,v),s(f,d),s(d,u),s(f,j),s(f,E),s(E,N),s(f,P)},d(q){q&&t(f)}}}function bc(O){let f,v;return{c(){f=o("p"),v=i("Quantization support requires ONNX Runtime >= 1.4.0")},l(d){f=a(d,"P",{});var u=n(f);v=l(u,"Quantization support requires ONNX Runtime >= 1.4.0"),u.forEach(t)},m(d,u){p(d,f,u),s(f,v)},d(d){d&&t(f)}}}function yc(O){let f,v,d,u,j,E,N,P;return{c(){f=o("p"),v=i(`When exporting quantized model you will end up with two different ONNX files. The one specified at the end of the
above command will contain the original ONNX model storing `),d=o("em"),u=i("float32"),j=i(" weights. The second one, with "),E=o("code"),N=i("-quantized"),P=i(`
suffix, will hold the quantized parameters.`)},l(q){f=a(q,"P",{});var T=n(f);v=l(T,`When exporting quantized model you will end up with two different ONNX files. The one specified at the end of the
above command will contain the original ONNX model storing `),d=a(T,"EM",{});var A=n(d);u=l(A,"float32"),A.forEach(t),j=l(T," weights. The second one, with "),E=a(T,"CODE",{});var S=n(E);N=l(S,"-quantized"),S.forEach(t),P=l(T,`
suffix, will hold the quantized parameters.`),T.forEach(t)},m(q,T){p(q,f,T),s(f,v),s(f,d),s(d,u),s(f,j),s(f,E),s(E,N),s(f,P)},d(q){q&&t(f)}}}function $c(O){let f,v;return{c(){f=o("p"),v=i(`This is the very beginning of our experiments with TorchScript and we are still exploring its capabilities with
variable-input-size models. It is a focus of interest to us and we will deepen our analysis in upcoming releases,
with more code examples, a more flexible implementation, and benchmarks comparing python-based codes with compiled
TorchScript.`)},l(d){f=a(d,"P",{});var u=n(f);v=l(u,`This is the very beginning of our experiments with TorchScript and we are still exploring its capabilities with
variable-input-size models. It is a focus of interest to us and we will deepen our analysis in upcoming releases,
with more code examples, a more flexible implementation, and benchmarks comparing python-based codes with compiled
TorchScript.`),u.forEach(t)},m(d,u){p(d,f,u),s(f,v)},d(d){d&&t(f)}}}function Ec(O){let f,v,d,u,j,E,N,P,q,T,A,S,_s,st,Or,ws,jr,ca,H,Pr,ot,zr,qr,at,Ar,Sr,da,fe,Xr,nt,Mr,Lr,ua,J,ce,bs,rt,Br,ys,Ir,va,de,Cr,$s,Rr,Dr,ga,Dt,Hr,_a,x,Es,Fr,Ur,xs,Gr,Wr,ks,Jr,Qr,Ns,Vr,Yr,Ts,Kr,Zr,Os,ei,ti,js,si,oi,Ps,ai,ni,zs,ri,ii,qs,li,pi,As,hi,mi,Ss,fi,ci,Xs,di,wa,Ht,ui,ba,ue,ya,Q,ve,Ms,it,vi,lt,gi,Ls,_i,wi,$a,Ft,bi,Ea,pt,xa,Ut,yi,ka,ht,Na,ge,$i,Bs,Ei,xi,Ta,_e,ki,Is,Ni,Ti,Oa,mt,ja,Gt,Oi,Pa,ft,za,we,ji,Cs,Pi,zi,qa,ct,Aa,V,be,Rs,dt,qi,Ds,Ai,Sa,Wt,Si,Xa,ut,Ma,Jt,Xi,La,Qt,Mi,Ba,Vt,Li,Ia,Yt,Bi,Ca,ye,Ii,Hs,Ci,Ri,Ra,L,Di,Fs,Hi,Fi,Us,Ui,Gi,Gs,Wi,Ji,Da,vt,Qi,gt,Vi,Ha,_t,Yi,wt,Ki,Fa,Y,$e,Ws,bt,Zi,Js,el,Ua,Ee,Ga,xe,tl,Qs,sl,ol,Wa,yt,Ja,Kt,al,Qa,F,Vs,nl,rl,Ys,il,ll,Ks,pl,Va,ke,Ya,Zt,hl,Ka,U,Zs,es,eo,ml,fl,cl,to,ts,so,dl,ul,vl,oo,Ne,ao,gl,_l,$t,wl,bl,Za,K,Te,no,Et,yl,ro,$l,en,Oe,El,io,xl,kl,tn,B,lo,Nl,Tl,po,Ol,jl,ho,Pl,zl,mo,ql,sn,je,Al,fo,Sl,Xl,on,Pe,an,ze,nn,Z,qe,co,xt,Ml,uo,Ll,rn,ss,Bl,ln,Ae,Il,vo,Cl,Rl,pn,Se,Dl,go,Hl,Fl,hn,X,Ul,_o,Gl,Wl,wo,Jl,Ql,bo,Vl,Yl,mn,ic='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>y</mi><mrow><mi>f</mi><mi>l</mi><mi>o</mi><mi>a</mi><mi>t</mi><mn>32</mn></mrow></msub><mo>=</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>\u2217</mo><msub><mi>x</mi><mrow><mi>i</mi><mi>n</mi><mi>t</mi><mn>8</mn></mrow></msub><mo>\u2212</mo><mi>z</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi mathvariant="normal">_</mi><mi>p</mi><mi>o</mi><mi>i</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">y_{float32} = scale * x_{int8} - zero\\_point</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10764em;">f</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">t</span><span class="mord mtight">32</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">sc</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\u2217</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span><span class="mord mathnormal mtight">t</span><span class="mord mtight">8</span></span></span></span></span><span class="vlist-s">\u200B</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">\u2212</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9695em;vertical-align:-0.31em;"></span><span class="mord mathnormal">zero</span><span class="mord" style="margin-right:0.02778em;">_</span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">in</span><span class="mord mathnormal">t</span></span></span></span></span>',fn,Xe,cn,os,Kl,dn,G,kt,Zl,yo,ep,tp,sp,$o,op,ap,Eo,np,un,I,rp,xo,ip,lp,ko,pp,hp,No,mp,fp,vn,as,cp,gn,Nt,_n,Me,wn,Le,bn,ee,Be,To,Tt,dp,Oo,up,yn,Ie,$n,Ce,vp,Ot,gp,_p,En,ns,wp,xn,rs,bp,kn,Re,jo,yp,$p,jt,Ep,Po,xp,kp,Nn,is,Np,Tn,te,De,zo,Pt,Tp,qo,Op,On,se,He,Ao,zt,jp,So,Pp,jn,W,zp,Xo,qp,Ap,Mo,Sp,Xp,Pn,C,Mp,Lo,Lp,Bp,Bo,Ip,Cp,Io,Rp,Dp,zn,Fe,Hp,Co,Fp,Up,qn,oe,Ue,Ro,qt,Gp,Do,Wp,An,ls,Jp,Sn,ps,Qp,Xn,hs,Ho,Vp,Mn,ms,Yp,Ln,fs,Kp,Bn,ae,Ge,Fo,At,Zp,Uo,eh,In,cs,th,Cn,ne,We,Go,St,sh,Wo,oh,Rn,M,ah,Jo,nh,rh,Qo,ih,lh,Vo,ph,hh,Yo,mh,Dn,Xt,Hn,re,Je,Ko,Mt,fh,Zo,ch,Fn,R,dh,ea,uh,vh,ta,gh,_h,sa,wh,bh,Un,Lt,Gn,ie,Qe,oa,Bt,yh,aa,$h,Wn,Ve,Eh,na,xh,kh,Jn,It,Qn;return E=new z({}),st=new z({}),rt=new z({}),ue=new me({props:{$$slots:{default:[dc]},$$scope:{ctx:O}}}),it=new z({}),pt=new D({props:{code:`python -m transformers.onnx --help

usage: Hugging Face ONNX Exporter tool [-h] -m MODEL -f {pytorch} [--features {default}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating where to store generated ONNX model.

optional arguments:
  -h, --help            show this help message and exit
  -m MODEL, --model MODEL
                        Model's name of path on disk to load.
  --features {default}  Export the model with some additional features.
  --opset OPSET         ONNX opset version to export the model with (default 12).
  --atol ATOL           Absolute difference tolerance when validating the model.,`,highlighted:`python -m transformers.onnx <span class="hljs-comment">--help</span>

usage: Hugging Face ONNX Exporter tool [-h] -m MODEL -f {pytorch} [<span class="hljs-comment">--features {default}] [--opset OPSET] [--atol ATOL] output</span>

positional arguments:
  output                Path indicating <span class="hljs-keyword">where</span> <span class="hljs-keyword">to</span> store generated ONNX model.

optional arguments:
  -h, <span class="hljs-comment">--help            show this help message and exit</span>
  -m MODEL, <span class="hljs-comment">--model MODEL</span>
                        Model&#x27;s <span class="hljs-built_in">name</span> <span class="hljs-keyword">of</span> path <span class="hljs-keyword">on</span> disk <span class="hljs-keyword">to</span> load.
  <span class="hljs-comment">--features {default}  Export the model with some additional features.</span>
  <span class="hljs-comment">--opset OPSET         ONNX opset version to export the model with (default 12).</span>
  <span class="hljs-comment">--atol ATOL           Absolute difference tolerance when validating the model.</span>`}}),ht=new D({props:{code:"python -m transformers.onnx --model=bert-base-cased onnx/bert-base-cased/,",highlighted:'python -m transformers.onnx --model=bert-base-cased onnx<span class="hljs-regexp">/bert-base-cased/</span>'}}),mt=new D({props:{code:`Validating ONNX model...
        -[\u2713] ONNX model outputs' name match reference model ({'pooler_output', 'last_hidden_state'}
- Validating ONNX Model output "last_hidden_state":
                -[\u2713] (2, 8, 768) matchs (2, 8, 768)
                -[\u2713] all values close (atol: 0.0001)
- Validating ONNX Model output "pooler_output":
                -[\u2713] (2, 768) matchs (2, 768)
                -[\u2713] all values close (atol: 0.0001)
All good, model saved at: onnx/bert-base-cased/model.onnx,`,highlighted:`Validating ONNX model...
        -[\u2713] ONNX model outputs&#x27; name match reference model ({<span class="hljs-symbol">&#x27;pooler_output</span>&#x27;, <span class="hljs-symbol">&#x27;last_hidden_state</span>&#x27;}
- Validating ONNX Model output <span class="hljs-string">&quot;last_hidden_state&quot;</span>:
                -[\u2713] (<span class="hljs-number">2</span>, <span class="hljs-number">8</span>, <span class="hljs-number">768</span>) matchs (<span class="hljs-number">2</span>, <span class="hljs-number">8</span>, <span class="hljs-number">768</span>)
                -[\u2713] <span class="hljs-keyword">all</span> values close (atol: <span class="hljs-number">0.0001</span>)
- Validating ONNX Model output <span class="hljs-string">&quot;pooler_output&quot;</span>:
                -[\u2713] (<span class="hljs-number">2</span>, <span class="hljs-number">768</span>) matchs (<span class="hljs-number">2</span>, <span class="hljs-number">768</span>)
                -[\u2713] <span class="hljs-keyword">all</span> values close (atol: <span class="hljs-number">0.0001</span>)
<span class="hljs-keyword">All</span> good, model saved <span class="hljs-keyword">at</span>: onnx/bert-base-cased/model.onnx`}}),ft=new D({props:{code:`import onnxruntime as ort

from transformers import BertTokenizerFast
tokenizer = BertTokenizerFast.from_pretrained("bert-base-cased")

ort_session = ort.InferenceSession("onnx/bert-base-cased/model.onnx")

inputs = tokenizer("Using BERT in ONNX!", return_tensors="np")
outputs = ort_session.run(["last_hidden_state", "pooler_output"], dict(inputs)),`,highlighted:`<span class="hljs-built_in">import</span> onnxruntime as ort

from transformers <span class="hljs-built_in">import</span> BertTokenizerFast
<span class="hljs-attr">tokenizer</span> = BertTokenizerFast.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-attr">ort_session</span> = ort.InferenceSession(<span class="hljs-string">&quot;onnx/bert-base-cased/model.onnx&quot;</span>)

<span class="hljs-attr">inputs</span> = tokenizer(<span class="hljs-string">&quot;Using BERT in ONNX!&quot;</span>, <span class="hljs-attr">return_tensors=&quot;np&quot;)</span>
<span class="hljs-attr">outputs</span> = ort_session.run([<span class="hljs-string">&quot;last_hidden_state&quot;</span>, <span class="hljs-string">&quot;pooler_output&quot;</span>], dict(inputs))`}}),ct=new D({props:{code:`from transformers.models.bert import BertOnnxConfig, BertConfig

config = BertConfig()
onnx_config = BertOnnxConfig(config)
output_keys = list(onnx_config.outputs.keys()),`,highlighted:`from transformers<span class="hljs-selector-class">.models</span><span class="hljs-selector-class">.bert</span> import BertOnnxConfig, BertConfig

config = <span class="hljs-built_in">BertConfig</span>()
onnx_config = <span class="hljs-built_in">BertOnnxConfig</span>(config)
output_keys = <span class="hljs-built_in">list</span>(onnx_config<span class="hljs-selector-class">.outputs</span><span class="hljs-selector-class">.keys</span>())`}}),dt=new z({}),ut=new D({props:{code:`class BertOnnxConfig(OnnxConfig):
    @property
    def inputs(self) -> Mapping[str, Mapping[int, str]]:
        return OrderedDict(
            [
                ("input_ids", {0: "batch", 1: "sequence"}),
                ("attention_mask", {0: "batch", 1: "sequence"}),
                ("token_type_ids", {0: "batch", 1: "sequence"}),
            ]
        )

    @property
    def outputs(self) -> Mapping[str, Mapping[int, str]]:
        return OrderedDict([("last_hidden_state", {0: "batch", 1: "sequence"}), ("pooler_output", {0: "batch"})]),`,highlighted:`<span class="hljs-keyword">class</span> <span class="hljs-title class_">BertOnnxConfig</span>(OnnxConfig):
    @property
    def inputs<span class="hljs-function"><span class="hljs-params">(self)</span> -&gt;</span> Mapping[str, Mapping[int, str]]:
        <span class="hljs-keyword">return</span> OrderedDict(
            [
                (<span class="hljs-string">&quot;input_ids&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
                (<span class="hljs-string">&quot;attention_mask&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
                (<span class="hljs-string">&quot;token_type_ids&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
            ]
        )

    @property
    def outputs<span class="hljs-function"><span class="hljs-params">(self)</span> -&gt;</span> Mapping[str, Mapping[int, str]]:
        <span class="hljs-keyword">return</span> OrderedDict([(<span class="hljs-string">&quot;last_hidden_state&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}), (<span class="hljs-string">&quot;pooler_output&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>})])`}}),bt=new z({}),Ee=new me({props:{$$slots:{default:[uc]},$$scope:{ctx:O}}}),yt=new D({props:{code:"python convert_graph_to_onnx.py --framework <pt, tf> --model bert-base-cased bert-base-cased.onnx,",highlighted:"python convert_graph_to_onnx.py --framework &lt;pt, tf&gt; --model bert-base-cased bert-base-cased.onnx"}}),ke=new me({props:{$$slots:{default:[vc]},$$scope:{ctx:O}}}),Et=new z({}),Pe=new me({props:{$$slots:{default:[gc]},$$scope:{ctx:O}}}),ze=new me({props:{$$slots:{default:[_c]},$$scope:{ctx:O}}}),xt=new z({}),Xe=new me({props:{$$slots:{default:[wc]},$$scope:{ctx:O}}}),Nt=new D({props:{code:"python convert_graph_to_onnx.py --framework <pt, tf> --model bert-base-cased --quantize bert-base-cased.onnx,",highlighted:"python convert_graph_to_onnx.py --framework &lt;pt, tf&gt; --model bert-base-cased --quantize bert-base-cased.onnx"}}),Me=new me({props:{$$slots:{default:[bc]},$$scope:{ctx:O}}}),Le=new me({props:{$$slots:{default:[yc]},$$scope:{ctx:O}}}),Tt=new z({}),Ie=new me({props:{$$slots:{default:[$c]},$$scope:{ctx:O}}}),Pt=new z({}),zt=new z({}),qt=new z({}),At=new z({}),St=new z({}),Xt=new D({props:{code:`from transformers import BertModel, BertTokenizer, BertConfig
import torch

enc = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenizing input text
text = "[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]"
tokenized_text = enc.tokenize(text)

# Masking one of the input tokens
masked_index = 8
tokenized_text[masked_index] = '[MASK]'
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]

# Creating a dummy input
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

# Initializing the model with the torchscript flag
# Flag set to True even though it is not necessary as this model does not have an LM Head.
config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,
    num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, torchscript=True)

# Instantiating the model
model = BertModel(config)

# The model needs to be in evaluation mode
model.eval()

# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag
model = BertModel.from_pretrained("bert-base-uncased", torchscript=True)

# Creating the trace
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, "traced_bert.pt"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch

enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-comment"># Tokenizing input text</span>
text = <span class="hljs-string">&quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;</span>
tokenized_text = enc.tokenize(text)

<span class="hljs-comment"># Masking one of the input tokens</span>
masked_index = <span class="hljs-number">8</span>
tokenized_text[masked_index] = <span class="hljs-string">&#x27;[MASK]&#x27;</span>
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]

<span class="hljs-comment"># Creating a dummy input</span>
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

<span class="hljs-comment"># Initializing the model with the torchscript flag</span>
<span class="hljs-comment"># Flag set to True even though it is not necessary as this model does not have an LM Head.</span>
config = BertConfig(vocab_size_or_config_json_file=<span class="hljs-number">32000</span>, hidden_size=<span class="hljs-number">768</span>,
    num_hidden_layers=<span class="hljs-number">12</span>, num_attention_heads=<span class="hljs-number">12</span>, intermediate_size=<span class="hljs-number">3072</span>, torchscript=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Instantiating the model</span>
model = BertModel(config)

<span class="hljs-comment"># The model needs to be in evaluation mode</span>
model.<span class="hljs-built_in">eval</span>()

<span class="hljs-comment"># If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag</span>
model = BertModel.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, torchscript=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Creating the trace</span>
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, <span class="hljs-string">&quot;traced_bert.pt&quot;</span>)`}}),Mt=new z({}),Lt=new D({props:{code:`loaded_model = torch.jit.load("traced_bert.pt")
loaded_model.eval()

all_encoder_layers, pooled_output = loaded_model(*dummy_input),`,highlighted:`loaded_model = torch.jit.load(<span class="hljs-string">&quot;traced_bert.pt&quot;</span>)
loaded_model.<span class="hljs-built_in">eval</span>()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)`}}),Bt=new z({}),It=new D({props:{code:"traced_model(tokens_tensor, segments_tensors),",highlighted:"traced_model(tokens_tensor, segments_tensors)"}}),{c(){f=o("meta"),v=h(),d=o("h1"),u=o("a"),j=o("span"),g(E.$$.fragment),N=h(),P=o("span"),q=i("Exporting transformers models"),T=h(),A=o("h2"),S=o("a"),_s=o("span"),g(st.$$.fragment),Or=h(),ws=o("span"),jr=i("ONNX / ONNXRuntime"),ca=h(),H=o("p"),Pr=i("Projects "),ot=o("a"),zr=i("ONNX (Open Neural Network eXchange)"),qr=i(" and "),at=o("a"),Ar=i("ONNXRuntime (ORT)"),Sr=i(` are part of an effort from leading industries in the AI field to provide a
unified and community-driven format to store and, by extension, efficiently execute neural network leveraging a variety
of hardware and dedicated optimizations.`),da=h(),fe=o("p"),Xr=i(`Starting from transformers v2.10.0 we partnered with ONNX Runtime to provide an easy export of transformers models to
the ONNX format. You can have a look at the effort by looking at our joint blog post `),nt=o("a"),Mr=i(`Accelerate your NLP pipelines
using Hugging Face Transformers and ONNX Runtime`),Lr=i("."),ua=h(),J=o("h3"),ce=o("a"),bs=o("span"),g(rt.$$.fragment),Br=h(),ys=o("span"),Ir=i("Configuration-based approach"),va=h(),de=o("p"),Cr=i("Transformers v4.9.0 introduces a new package: "),$s=o("code"),Rr=i("transformers.onnx"),Dr=i(`. This package allows converting checkpoints to an
ONNX graph by leveraging configuration objects. These configuration objects come ready made for a number of model
architectures, and are made to be easily extendable to other architectures.`),ga=h(),Dt=o("p"),Hr=i("Ready-made configurations include the following models:"),_a=h(),x=o("ul"),Es=o("li"),Fr=i("ALBERT"),Ur=h(),xs=o("li"),Gr=i("BART"),Wr=h(),ks=o("li"),Jr=i("BERT"),Qr=h(),Ns=o("li"),Vr=i("CamemBERT"),Yr=h(),Ts=o("li"),Kr=i("DistilBERT"),Zr=h(),Os=o("li"),ei=i("GPT Neo"),ti=h(),js=o("li"),si=i("LayoutLM"),oi=h(),Ps=o("li"),ai=i("Longformer"),ni=h(),zs=o("li"),ri=i("mBART"),ii=h(),qs=o("li"),li=i("OpenAI GPT-2"),pi=h(),As=o("li"),hi=i("RoBERTa"),mi=h(),Ss=o("li"),fi=i("T5"),ci=h(),Xs=o("li"),di=i("XLM-RoBERTa"),wa=h(),Ht=o("p"),ui=i(`This conversion is handled with the PyTorch version of models - it, therefore, requires PyTorch to be installed. If you
would like to be able to convert from TensorFlow, please let us know by opening an issue.`),ba=h(),g(ue.$$.fragment),ya=h(),Q=o("h4"),ve=o("a"),Ms=o("span"),g(it.$$.fragment),vi=h(),lt=o("span"),gi=i("Converting an ONNX model using the "),Ls=o("code"),_i=i("transformers.onnx"),wi=i(" package"),$a=h(),Ft=o("p"),bi=i("The package may be used as a Python module:"),Ea=h(),g(pt.$$.fragment),xa=h(),Ut=o("p"),yi=i("Exporting a checkpoint using a ready-made configuration can be done as follows:"),ka=h(),g(ht.$$.fragment),Na=h(),ge=o("p"),$i=i("This exports an ONNX graph of the mentioned checkpoint. Here it is "),Bs=o("em"),Ei=i("bert-base-cased"),xi=i(`, but it can be any model from the
hub, or a local path.`),Ta=h(),_e=o("p"),ki=i("It will be exported under "),Is=o("code"),Ni=i("onnx/bert-base-cased"),Ti=i(". You should see similar logs:"),Oa=h(),g(mt.$$.fragment),ja=h(),Gt=o("p"),Oi=i("This export can now be used in the ONNX inference runtime:"),Pa=h(),g(ft.$$.fragment),za=h(),we=o("p"),ji=i("The outputs used ("),Cs=o("code"),Pi=i('["last_hidden_state", "pooler_output"]'),zi=i(`) can be obtained by taking a look at the ONNX
configuration of each model. For example, for BERT:`),qa=h(),g(ct.$$.fragment),Aa=h(),V=o("h4"),be=o("a"),Rs=o("span"),g(dt.$$.fragment),qi=h(),Ds=o("span"),Ai=i("Implementing a custom configuration for an unsupported architecture"),Sa=h(),Wt=o("p"),Si=i(`Let\u2019s take a look at the changes necessary to add a custom configuration for an unsupported architecture. Firstly, we
will need a custom ONNX configuration object that details the model inputs and outputs. The BERT ONNX configuration is
visible below:`),Xa=h(),g(ut.$$.fragment),Ma=h(),Jt=o("p"),Xi=i("Let\u2019s understand what\u2019s happening here. This configuration has two properties: the inputs, and the outputs."),La=h(),Qt=o("p"),Mi=i(`The inputs return a dictionary, where each key corresponds to an expected input, and each value indicates the axis of
that input.`),Ba=h(),Vt=o("p"),Li=i(`For BERT, there are three necessary inputs. These three inputs are of similar shape, which is made up of two
dimensions: the batch is the first dimension, and the second is the sequence.`),Ia=h(),Yt=o("p"),Bi=i(`The outputs return a similar dictionary, where, once again, each key corresponds to an expected output, and each value
indicates the axis of that output.`),Ca=h(),ye=o("p"),Ii=i(`Once this is done, a single step remains: adding this configuration object to the initialisation of the model class,
and to the general `),Hs=o("code"),Ci=i("transformers"),Ri=i(" initialisation."),Ra=h(),L=o("p"),Di=i("An important fact to notice is the use of "),Fs=o("em"),Hi=i("OrderedDict"),Fi=i(` in both inputs and outputs properties. This is a requirements
as inputs are matched against their relative position within the `),Us=o("em"),Ui=i("PreTrainedModel.forward()"),Gi=i(` prototype and outputs are
match against there position in the returned `),Gs=o("em"),Wi=i("BaseModelOutputX"),Ji=i(" instance."),Da=h(),vt=o("p"),Qi=i("An example of such an addition is visible here, for the MBart model: "),gt=o("a"),Vi=i("Making MBART ONNX-convertible"),Ha=h(),_t=o("p"),Yi=i(`If you would like to contribute your addition to the library, we recommend you implement tests. An example of such
tests is visible here: `),wt=o("a"),Ki=i("Adding tests to the MBART ONNX conversion"),Fa=h(),Y=o("h3"),$e=o("a"),Ws=o("span"),g(bt.$$.fragment),Zi=h(),Js=o("span"),el=i("Graph conversion"),Ua=h(),g(Ee.$$.fragment),Ga=h(),xe=o("p"),tl=i("Exporting a model is done through the script "),Qs=o("em"),sl=i("convert_graph_to_onnx.py"),ol=i(` at the root of the transformers sources. The
following command shows how easy it is to export a BERT model from the library, simply run:`),Wa=h(),g(yt.$$.fragment),Ja=h(),Kt=o("p"),al=i("The conversion tool works for both PyTorch and Tensorflow models and ensures:"),Qa=h(),F=o("ul"),Vs=o("li"),nl=i("The model and its weights are correctly initialized from the Hugging Face model hub or a local checkpoint."),rl=h(),Ys=o("li"),il=i("The inputs and outputs are correctly generated to their ONNX counterpart."),ll=h(),Ks=o("li"),pl=i("The generated model can be correctly loaded through onnxruntime."),Va=h(),g(ke.$$.fragment),Ya=h(),Zt=o("p"),hl=i("Also, the conversion tool supports different options which let you tune the behavior of the generated model:"),Ka=h(),U=o("ul"),Zs=o("li"),es=o("p"),eo=o("strong"),ml=i("Change the target opset version of the generated model."),fl=i(` (More recent opset generally supports more operators and
enables faster inference)`),cl=h(),to=o("li"),ts=o("p"),so=o("strong"),dl=i("Export pipeline-specific prediction heads."),ul=i(` (Allow to export model along with its task-specific prediction
head(s))`),vl=h(),oo=o("li"),Ne=o("p"),ao=o("strong"),gl=i("Use the external data format (PyTorch only)."),_l=i(" (Lets you export model which size is above 2Gb ("),$t=o("a"),wl=i("More info"),bl=i("))"),Za=h(),K=o("h3"),Te=o("a"),no=o("span"),g(Et.$$.fragment),yl=h(),ro=o("span"),$l=i("Optimizations"),en=h(),Oe=o("p"),El=i(`ONNXRuntime includes some transformers-specific transformations to leverage optimized operations in the graph. Below
are some of the operators which can be enabled to speed up inference through ONNXRuntime (`),io=o("em"),xl=i("see note below"),kl=i("):"),tn=h(),B=o("ul"),lo=o("li"),Nl=i("Constant folding"),Tl=h(),po=o("li"),Ol=i("Attention Layer fusing"),jl=h(),ho=o("li"),Pl=i("Skip connection LayerNormalization fusing"),zl=h(),mo=o("li"),ql=i("FastGeLU approximation"),sn=h(),je=o("p"),Al=i(`Some of the optimizations performed by ONNX runtime can be hardware specific and thus lead to different performances if
used on another machine with a different hardware configuration than the one used for exporting the model. For this
reason, when using `),fo=o("code"),Sl=i("convert_graph_to_onnx.py"),Xl=i(` optimizations are not enabled, ensuring the model can be easily
exported to various hardware. Optimizations can then be enabled when loading the model through ONNX runtime for
inference.`),on=h(),g(Pe.$$.fragment),an=h(),g(ze.$$.fragment),nn=h(),Z=o("h3"),qe=o("a"),co=o("span"),g(xt.$$.fragment),Ml=h(),uo=o("span"),Ll=i("Quantization"),rn=h(),ss=o("p"),Bl=i("ONNX exporter supports generating a quantized version of the model to allow efficient inference."),ln=h(),Ae=o("p"),Il=i(`Quantization works by converting the memory representation of the parameters in the neural network to a compact integer
format. By default, weights of a neural network are stored as single-precision float (`),vo=o("em"),Cl=i("float32"),Rl=i(`) which can express a
wide-range of floating-point numbers with decent precision. These properties are especially interesting at training
where you want fine-grained representation.`),pn=h(),Se=o("p"),Dl=i(`On the other hand, after the training phase, it has been shown one can greatly reduce the range and the precision of
`),go=o("em"),Hl=i("float32"),Fl=i(" numbers without changing the performances of the neural network."),hn=h(),X=o("p"),Ul=i("More technically, "),_o=o("em"),Gl=i("float32"),Wl=i(` parameters are converted to a type requiring fewer bits to represent each number, thus
reducing the overall size of the model. Here, we are enabling `),wo=o("em"),Jl=i("float32"),Ql=i(" mapping to "),bo=o("em"),Vl=i("int8"),Yl=i(` values (a non-floating,
single byte, number representation) according to the following formula:
`),mn=new mc,fn=h(),g(Xe.$$.fragment),cn=h(),os=o("p"),Kl=i("Leveraging tiny-integers has numerous advantages when it comes to inference:"),dn=h(),G=o("ul"),kt=o("li"),Zl=i("Storing fewer bits instead of 32 bits for the "),yo=o("em"),ep=i("float32"),tp=i(" reduces the size of the model and makes it load faster."),sp=h(),$o=o("li"),op=i("Integer operations execute a magnitude faster on modern hardware"),ap=h(),Eo=o("li"),np=i("Integer operations require less power to do the computations"),un=h(),I=o("p"),rp=i("In order to convert a transformers model to ONNX IR with quantized weights you just need to specify "),xo=o("code"),ip=i("--quantize"),lp=i(` when
using `),ko=o("code"),pp=i("convert_graph_to_onnx.py"),hp=i(". Also, you can have a look at the "),No=o("code"),mp=i("quantize()"),fp=i(` utility-method in this same script
file.`),vn=h(),as=o("p"),cp=i("Example of quantized BERT model export:"),gn=h(),g(Nt.$$.fragment),_n=h(),g(Me.$$.fragment),wn=h(),g(Le.$$.fragment),bn=h(),ee=o("h2"),Be=o("a"),To=o("span"),g(Tt.$$.fragment),dp=h(),Oo=o("span"),up=i("TorchScript"),yn=h(),g(Ie.$$.fragment),$n=h(),Ce=o("p"),vp=i(`According to Pytorch\u2019s documentation: \u201CTorchScript is a way to create serializable and optimizable models from PyTorch
code\u201D. Pytorch\u2019s two modules `),Ot=o("a"),gp=i("JIT and TRACE"),_p=i(` allow the developer to export
their model to be re-used in other programs, such as efficiency-oriented C++ programs.`),En=h(),ns=o("p"),wp=i(`We have provided an interface that allows the export of \u{1F917} Transformers models to TorchScript so that they can be reused
in a different environment than a Pytorch-based python program. Here we explain how to export and use our models using
TorchScript.`),xn=h(),rs=o("p"),bp=i("Exporting a model requires two things:"),kn=h(),Re=o("ul"),jo=o("li"),yp=i("a forward pass with dummy inputs."),$p=h(),jt=o("li"),Ep=i("model instantiation with the "),Po=o("code"),xp=i("torchscript"),kp=i(" flag."),Nn=h(),is=o("p"),Np=i("These necessities imply several things developers should be careful about. These are detailed below."),Tn=h(),te=o("h3"),De=o("a"),zo=o("span"),g(Pt.$$.fragment),Tp=h(),qo=o("span"),Op=i("Implications"),On=h(),se=o("h3"),He=o("a"),Ao=o("span"),g(zt.$$.fragment),jp=h(),So=o("span"),Pp=i("TorchScript flag and tied weights"),jn=h(),W=o("p"),zp=i(`This flag is necessary because most of the language models in this repository have tied weights between their
`),Xo=o("code"),qp=i("Embedding"),Ap=i(" layer and their "),Mo=o("code"),Sp=i("Decoding"),Xp=i(` layer. TorchScript does not allow the export of models that have tied
weights, therefore it is necessary to untie and clone the weights beforehand.`),Pn=h(),C=o("p"),Mp=i("This implies that models instantiated with the "),Lo=o("code"),Lp=i("torchscript"),Bp=i(" flag have their "),Bo=o("code"),Ip=i("Embedding"),Cp=i(" layer and "),Io=o("code"),Rp=i("Decoding"),Dp=i(`
layer separate, which means that they should not be trained down the line. Training would de-synchronize the two
layers, leading to unexpected results.`),zn=h(),Fe=o("p"),Hp=i(`This is not the case for models that do not have a Language Model head, as those do not have tied weights. These models
can be safely exported without the `),Co=o("code"),Fp=i("torchscript"),Up=i(" flag."),qn=h(),oe=o("h3"),Ue=o("a"),Ro=o("span"),g(qt.$$.fragment),Gp=h(),Do=o("span"),Wp=i("Dummy inputs and standard lengths"),An=h(),ls=o("p"),Jp=i(`The dummy inputs are used to do a model forward pass. While the inputs\u2019 values are propagating through the layers,
Pytorch keeps track of the different operations executed on each tensor. These recorded operations are then used to
create the \u201Ctrace\u201D of the model.`),Sn=h(),ps=o("p"),Qp=i(`The trace is created relatively to the inputs\u2019 dimensions. It is therefore constrained by the dimensions of the dummy
input, and will not work for any other sequence length or batch size. When trying with a different size, an error such
as:`),Xn=h(),hs=o("p"),Ho=o("code"),Vp=i("The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2"),Mn=h(),ms=o("p"),Yp=i(`will be raised. It is therefore recommended to trace the model with a dummy input size at least as large as the largest
input that will be fed to the model during inference. Padding can be performed to fill the missing values. As the model
will have been traced with a large input size however, the dimensions of the different matrix will be large as well,
resulting in more calculations.`),Ln=h(),fs=o("p"),Kp=i(`It is recommended to be careful of the total number of operations done on each input and to follow performance closely
when exporting varying sequence-length models.`),Bn=h(),ae=o("h3"),Ge=o("a"),Fo=o("span"),g(At.$$.fragment),Zp=h(),Uo=o("span"),eh=i("Using TorchScript in Python"),In=h(),cs=o("p"),th=i("Below is an example, showing how to save, load models as well as how to use the trace for inference."),Cn=h(),ne=o("h4"),We=o("a"),Go=o("span"),g(St.$$.fragment),sh=h(),Wo=o("span"),oh=i("Saving a model"),Rn=h(),M=o("p"),ah=i("This snippet shows how to use TorchScript to export a "),Jo=o("code"),nh=i("BertModel"),rh=i(". Here the "),Qo=o("code"),ih=i("BertModel"),lh=i(` is instantiated according
to a `),Vo=o("code"),ph=i("BertConfig"),hh=i(" class and then saved to disk under the filename "),Yo=o("code"),mh=i("traced_bert.pt"),Dn=h(),g(Xt.$$.fragment),Hn=h(),re=o("h4"),Je=o("a"),Ko=o("span"),g(Mt.$$.fragment),fh=h(),Zo=o("span"),ch=i("Loading a model"),Fn=h(),R=o("p"),dh=i("This snippet shows how to load the "),ea=o("code"),uh=i("BertModel"),vh=i(" that was previously saved to disk under the name "),ta=o("code"),gh=i("traced_bert.pt"),_h=i(`.
We are re-using the previously initialised `),sa=o("code"),wh=i("dummy_input"),bh=i("."),Un=h(),g(Lt.$$.fragment),Gn=h(),ie=o("h4"),Qe=o("a"),oa=o("span"),g(Bt.$$.fragment),yh=h(),aa=o("span"),$h=i("Using a traced model for inference"),Wn=h(),Ve=o("p"),Eh=i("Using the traced model for inference is as simple as using its "),na=o("code"),xh=i("__call__"),kh=i(" dunder method:"),Jn=h(),g(It.$$.fragment),this.h()},l(e){const r=fc('[data-svelte="svelte-1phssyn"]',document.head);f=a(r,"META",{name:!0,content:!0}),r.forEach(t),v=m(e),d=a(e,"H1",{class:!0});var Ct=n(d);u=a(Ct,"A",{id:!0,class:!0,href:!0});var ra=n(u);j=a(ra,"SPAN",{});var ia=n(j);_(E.$$.fragment,ia),ia.forEach(t),ra.forEach(t),N=m(Ct),P=a(Ct,"SPAN",{});var la=n(P);q=l(la,"Exporting transformers models"),la.forEach(t),Ct.forEach(t),T=m(e),A=a(e,"H2",{class:!0});var Rt=n(A);S=a(Rt,"A",{id:!0,class:!0,href:!0});var pa=n(S);_s=a(pa,"SPAN",{});var ha=n(_s);_(st.$$.fragment,ha),ha.forEach(t),pa.forEach(t),Or=m(Rt),ws=a(Rt,"SPAN",{});var ma=n(ws);jr=l(ma,"ONNX / ONNXRuntime"),ma.forEach(t),Rt.forEach(t),ca=m(e),H=a(e,"P",{});var le=n(H);Pr=l(le,"Projects "),ot=a(le,"A",{href:!0,rel:!0});var Ph=n(ot);zr=l(Ph,"ONNX (Open Neural Network eXchange)"),Ph.forEach(t),qr=l(le," and "),at=a(le,"A",{href:!0,rel:!0});var zh=n(at);Ar=l(zh,"ONNXRuntime (ORT)"),zh.forEach(t),Sr=l(le,` are part of an effort from leading industries in the AI field to provide a
unified and community-driven format to store and, by extension, efficiently execute neural network leveraging a variety
of hardware and dedicated optimizations.`),le.forEach(t),da=m(e),fe=a(e,"P",{});var Vn=n(fe);Xr=l(Vn,`Starting from transformers v2.10.0 we partnered with ONNX Runtime to provide an easy export of transformers models to
the ONNX format. You can have a look at the effort by looking at our joint blog post `),nt=a(Vn,"A",{href:!0,rel:!0});var qh=n(nt);Mr=l(qh,`Accelerate your NLP pipelines
using Hugging Face Transformers and ONNX Runtime`),qh.forEach(t),Lr=l(Vn,"."),Vn.forEach(t),ua=m(e),J=a(e,"H3",{class:!0});var Yn=n(J);ce=a(Yn,"A",{id:!0,class:!0,href:!0});var Ah=n(ce);bs=a(Ah,"SPAN",{});var Sh=n(bs);_(rt.$$.fragment,Sh),Sh.forEach(t),Ah.forEach(t),Br=m(Yn),ys=a(Yn,"SPAN",{});var Xh=n(ys);Ir=l(Xh,"Configuration-based approach"),Xh.forEach(t),Yn.forEach(t),va=m(e),de=a(e,"P",{});var Kn=n(de);Cr=l(Kn,"Transformers v4.9.0 introduces a new package: "),$s=a(Kn,"CODE",{});var Mh=n($s);Rr=l(Mh,"transformers.onnx"),Mh.forEach(t),Dr=l(Kn,`. This package allows converting checkpoints to an
ONNX graph by leveraging configuration objects. These configuration objects come ready made for a number of model
architectures, and are made to be easily extendable to other architectures.`),Kn.forEach(t),ga=m(e),Dt=a(e,"P",{});var Lh=n(Dt);Hr=l(Lh,"Ready-made configurations include the following models:"),Lh.forEach(t),_a=m(e),x=a(e,"UL",{});var k=n(x);Es=a(k,"LI",{});var Bh=n(Es);Fr=l(Bh,"ALBERT"),Bh.forEach(t),Ur=m(k),xs=a(k,"LI",{});var Ih=n(xs);Gr=l(Ih,"BART"),Ih.forEach(t),Wr=m(k),ks=a(k,"LI",{});var Ch=n(ks);Jr=l(Ch,"BERT"),Ch.forEach(t),Qr=m(k),Ns=a(k,"LI",{});var Rh=n(Ns);Vr=l(Rh,"CamemBERT"),Rh.forEach(t),Yr=m(k),Ts=a(k,"LI",{});var Dh=n(Ts);Kr=l(Dh,"DistilBERT"),Dh.forEach(t),Zr=m(k),Os=a(k,"LI",{});var Hh=n(Os);ei=l(Hh,"GPT Neo"),Hh.forEach(t),ti=m(k),js=a(k,"LI",{});var Fh=n(js);si=l(Fh,"LayoutLM"),Fh.forEach(t),oi=m(k),Ps=a(k,"LI",{});var Uh=n(Ps);ai=l(Uh,"Longformer"),Uh.forEach(t),ni=m(k),zs=a(k,"LI",{});var Gh=n(zs);ri=l(Gh,"mBART"),Gh.forEach(t),ii=m(k),qs=a(k,"LI",{});var Wh=n(qs);li=l(Wh,"OpenAI GPT-2"),Wh.forEach(t),pi=m(k),As=a(k,"LI",{});var Jh=n(As);hi=l(Jh,"RoBERTa"),Jh.forEach(t),mi=m(k),Ss=a(k,"LI",{});var Qh=n(Ss);fi=l(Qh,"T5"),Qh.forEach(t),ci=m(k),Xs=a(k,"LI",{});var Vh=n(Xs);di=l(Vh,"XLM-RoBERTa"),Vh.forEach(t),k.forEach(t),wa=m(e),Ht=a(e,"P",{});var Yh=n(Ht);ui=l(Yh,`This conversion is handled with the PyTorch version of models - it, therefore, requires PyTorch to be installed. If you
would like to be able to convert from TensorFlow, please let us know by opening an issue.`),Yh.forEach(t),ba=m(e),_(ue.$$.fragment,e),ya=m(e),Q=a(e,"H4",{class:!0});var Zn=n(Q);ve=a(Zn,"A",{id:!0,class:!0,href:!0});var Kh=n(ve);Ms=a(Kh,"SPAN",{});var Zh=n(Ms);_(it.$$.fragment,Zh),Zh.forEach(t),Kh.forEach(t),vi=m(Zn),lt=a(Zn,"SPAN",{});var er=n(lt);gi=l(er,"Converting an ONNX model using the "),Ls=a(er,"CODE",{});var em=n(Ls);_i=l(em,"transformers.onnx"),em.forEach(t),wi=l(er," package"),er.forEach(t),Zn.forEach(t),$a=m(e),Ft=a(e,"P",{});var tm=n(Ft);bi=l(tm,"The package may be used as a Python module:"),tm.forEach(t),Ea=m(e),_(pt.$$.fragment,e),xa=m(e),Ut=a(e,"P",{});var sm=n(Ut);yi=l(sm,"Exporting a checkpoint using a ready-made configuration can be done as follows:"),sm.forEach(t),ka=m(e),_(ht.$$.fragment,e),Na=m(e),ge=a(e,"P",{});var tr=n(ge);$i=l(tr,"This exports an ONNX graph of the mentioned checkpoint. Here it is "),Bs=a(tr,"EM",{});var om=n(Bs);Ei=l(om,"bert-base-cased"),om.forEach(t),xi=l(tr,`, but it can be any model from the
hub, or a local path.`),tr.forEach(t),Ta=m(e),_e=a(e,"P",{});var sr=n(_e);ki=l(sr,"It will be exported under "),Is=a(sr,"CODE",{});var am=n(Is);Ni=l(am,"onnx/bert-base-cased"),am.forEach(t),Ti=l(sr,". You should see similar logs:"),sr.forEach(t),Oa=m(e),_(mt.$$.fragment,e),ja=m(e),Gt=a(e,"P",{});var nm=n(Gt);Oi=l(nm,"This export can now be used in the ONNX inference runtime:"),nm.forEach(t),Pa=m(e),_(ft.$$.fragment,e),za=m(e),we=a(e,"P",{});var or=n(we);ji=l(or,"The outputs used ("),Cs=a(or,"CODE",{});var rm=n(Cs);Pi=l(rm,'["last_hidden_state", "pooler_output"]'),rm.forEach(t),zi=l(or,`) can be obtained by taking a look at the ONNX
configuration of each model. For example, for BERT:`),or.forEach(t),qa=m(e),_(ct.$$.fragment,e),Aa=m(e),V=a(e,"H4",{class:!0});var ar=n(V);be=a(ar,"A",{id:!0,class:!0,href:!0});var im=n(be);Rs=a(im,"SPAN",{});var lm=n(Rs);_(dt.$$.fragment,lm),lm.forEach(t),im.forEach(t),qi=m(ar),Ds=a(ar,"SPAN",{});var pm=n(Ds);Ai=l(pm,"Implementing a custom configuration for an unsupported architecture"),pm.forEach(t),ar.forEach(t),Sa=m(e),Wt=a(e,"P",{});var hm=n(Wt);Si=l(hm,`Let\u2019s take a look at the changes necessary to add a custom configuration for an unsupported architecture. Firstly, we
will need a custom ONNX configuration object that details the model inputs and outputs. The BERT ONNX configuration is
visible below:`),hm.forEach(t),Xa=m(e),_(ut.$$.fragment,e),Ma=m(e),Jt=a(e,"P",{});var mm=n(Jt);Xi=l(mm,"Let\u2019s understand what\u2019s happening here. This configuration has two properties: the inputs, and the outputs."),mm.forEach(t),La=m(e),Qt=a(e,"P",{});var fm=n(Qt);Mi=l(fm,`The inputs return a dictionary, where each key corresponds to an expected input, and each value indicates the axis of
that input.`),fm.forEach(t),Ba=m(e),Vt=a(e,"P",{});var cm=n(Vt);Li=l(cm,`For BERT, there are three necessary inputs. These three inputs are of similar shape, which is made up of two
dimensions: the batch is the first dimension, and the second is the sequence.`),cm.forEach(t),Ia=m(e),Yt=a(e,"P",{});var dm=n(Yt);Bi=l(dm,`The outputs return a similar dictionary, where, once again, each key corresponds to an expected output, and each value
indicates the axis of that output.`),dm.forEach(t),Ca=m(e),ye=a(e,"P",{});var nr=n(ye);Ii=l(nr,`Once this is done, a single step remains: adding this configuration object to the initialisation of the model class,
and to the general `),Hs=a(nr,"CODE",{});var um=n(Hs);Ci=l(um,"transformers"),um.forEach(t),Ri=l(nr," initialisation."),nr.forEach(t),Ra=m(e),L=a(e,"P",{});var Ye=n(L);Di=l(Ye,"An important fact to notice is the use of "),Fs=a(Ye,"EM",{});var vm=n(Fs);Hi=l(vm,"OrderedDict"),vm.forEach(t),Fi=l(Ye,` in both inputs and outputs properties. This is a requirements
as inputs are matched against their relative position within the `),Us=a(Ye,"EM",{});var gm=n(Us);Ui=l(gm,"PreTrainedModel.forward()"),gm.forEach(t),Gi=l(Ye,` prototype and outputs are
match against there position in the returned `),Gs=a(Ye,"EM",{});var _m=n(Gs);Wi=l(_m,"BaseModelOutputX"),_m.forEach(t),Ji=l(Ye," instance."),Ye.forEach(t),Da=m(e),vt=a(e,"P",{});var Nh=n(vt);Qi=l(Nh,"An example of such an addition is visible here, for the MBart model: "),gt=a(Nh,"A",{href:!0,rel:!0});var wm=n(gt);Vi=l(wm,"Making MBART ONNX-convertible"),wm.forEach(t),Nh.forEach(t),Ha=m(e),_t=a(e,"P",{});var Th=n(_t);Yi=l(Th,`If you would like to contribute your addition to the library, we recommend you implement tests. An example of such
tests is visible here: `),wt=a(Th,"A",{href:!0,rel:!0});var bm=n(wt);Ki=l(bm,"Adding tests to the MBART ONNX conversion"),bm.forEach(t),Th.forEach(t),Fa=m(e),Y=a(e,"H3",{class:!0});var rr=n(Y);$e=a(rr,"A",{id:!0,class:!0,href:!0});var ym=n($e);Ws=a(ym,"SPAN",{});var $m=n(Ws);_(bt.$$.fragment,$m),$m.forEach(t),ym.forEach(t),Zi=m(rr),Js=a(rr,"SPAN",{});var Em=n(Js);el=l(Em,"Graph conversion"),Em.forEach(t),rr.forEach(t),Ua=m(e),_(Ee.$$.fragment,e),Ga=m(e),xe=a(e,"P",{});var ir=n(xe);tl=l(ir,"Exporting a model is done through the script "),Qs=a(ir,"EM",{});var xm=n(Qs);sl=l(xm,"convert_graph_to_onnx.py"),xm.forEach(t),ol=l(ir,` at the root of the transformers sources. The
following command shows how easy it is to export a BERT model from the library, simply run:`),ir.forEach(t),Wa=m(e),_(yt.$$.fragment,e),Ja=m(e),Kt=a(e,"P",{});var km=n(Kt);al=l(km,"The conversion tool works for both PyTorch and Tensorflow models and ensures:"),km.forEach(t),Qa=m(e),F=a(e,"UL",{});var ds=n(F);Vs=a(ds,"LI",{});var Nm=n(Vs);nl=l(Nm,"The model and its weights are correctly initialized from the Hugging Face model hub or a local checkpoint."),Nm.forEach(t),rl=m(ds),Ys=a(ds,"LI",{});var Tm=n(Ys);il=l(Tm,"The inputs and outputs are correctly generated to their ONNX counterpart."),Tm.forEach(t),ll=m(ds),Ks=a(ds,"LI",{});var Om=n(Ks);pl=l(Om,"The generated model can be correctly loaded through onnxruntime."),Om.forEach(t),ds.forEach(t),Va=m(e),_(ke.$$.fragment,e),Ya=m(e),Zt=a(e,"P",{});var jm=n(Zt);hl=l(jm,"Also, the conversion tool supports different options which let you tune the behavior of the generated model:"),jm.forEach(t),Ka=m(e),U=a(e,"UL",{});var us=n(U);Zs=a(us,"LI",{});var Pm=n(Zs);es=a(Pm,"P",{});var Oh=n(es);eo=a(Oh,"STRONG",{});var zm=n(eo);ml=l(zm,"Change the target opset version of the generated model."),zm.forEach(t),fl=l(Oh,` (More recent opset generally supports more operators and
enables faster inference)`),Oh.forEach(t),Pm.forEach(t),cl=m(us),to=a(us,"LI",{});var qm=n(to);ts=a(qm,"P",{});var jh=n(ts);so=a(jh,"STRONG",{});var Am=n(so);dl=l(Am,"Export pipeline-specific prediction heads."),Am.forEach(t),ul=l(jh,` (Allow to export model along with its task-specific prediction
head(s))`),jh.forEach(t),qm.forEach(t),vl=m(us),oo=a(us,"LI",{});var Sm=n(oo);Ne=a(Sm,"P",{});var fa=n(Ne);ao=a(fa,"STRONG",{});var Xm=n(ao);gl=l(Xm,"Use the external data format (PyTorch only)."),Xm.forEach(t),_l=l(fa," (Lets you export model which size is above 2Gb ("),$t=a(fa,"A",{href:!0,rel:!0});var Mm=n($t);wl=l(Mm,"More info"),Mm.forEach(t),bl=l(fa,"))"),fa.forEach(t),Sm.forEach(t),us.forEach(t),Za=m(e),K=a(e,"H3",{class:!0});var lr=n(K);Te=a(lr,"A",{id:!0,class:!0,href:!0});var Lm=n(Te);no=a(Lm,"SPAN",{});var Bm=n(no);_(Et.$$.fragment,Bm),Bm.forEach(t),Lm.forEach(t),yl=m(lr),ro=a(lr,"SPAN",{});var Im=n(ro);$l=l(Im,"Optimizations"),Im.forEach(t),lr.forEach(t),en=m(e),Oe=a(e,"P",{});var pr=n(Oe);El=l(pr,`ONNXRuntime includes some transformers-specific transformations to leverage optimized operations in the graph. Below
are some of the operators which can be enabled to speed up inference through ONNXRuntime (`),io=a(pr,"EM",{});var Cm=n(io);xl=l(Cm,"see note below"),Cm.forEach(t),kl=l(pr,"):"),pr.forEach(t),tn=m(e),B=a(e,"UL",{});var Ke=n(B);lo=a(Ke,"LI",{});var Rm=n(lo);Nl=l(Rm,"Constant folding"),Rm.forEach(t),Tl=m(Ke),po=a(Ke,"LI",{});var Dm=n(po);Ol=l(Dm,"Attention Layer fusing"),Dm.forEach(t),jl=m(Ke),ho=a(Ke,"LI",{});var Hm=n(ho);Pl=l(Hm,"Skip connection LayerNormalization fusing"),Hm.forEach(t),zl=m(Ke),mo=a(Ke,"LI",{});var Fm=n(mo);ql=l(Fm,"FastGeLU approximation"),Fm.forEach(t),Ke.forEach(t),sn=m(e),je=a(e,"P",{});var hr=n(je);Al=l(hr,`Some of the optimizations performed by ONNX runtime can be hardware specific and thus lead to different performances if
used on another machine with a different hardware configuration than the one used for exporting the model. For this
reason, when using `),fo=a(hr,"CODE",{});var Um=n(fo);Sl=l(Um,"convert_graph_to_onnx.py"),Um.forEach(t),Xl=l(hr,` optimizations are not enabled, ensuring the model can be easily
exported to various hardware. Optimizations can then be enabled when loading the model through ONNX runtime for
inference.`),hr.forEach(t),on=m(e),_(Pe.$$.fragment,e),an=m(e),_(ze.$$.fragment,e),nn=m(e),Z=a(e,"H3",{class:!0});var mr=n(Z);qe=a(mr,"A",{id:!0,class:!0,href:!0});var Gm=n(qe);co=a(Gm,"SPAN",{});var Wm=n(co);_(xt.$$.fragment,Wm),Wm.forEach(t),Gm.forEach(t),Ml=m(mr),uo=a(mr,"SPAN",{});var Jm=n(uo);Ll=l(Jm,"Quantization"),Jm.forEach(t),mr.forEach(t),rn=m(e),ss=a(e,"P",{});var Qm=n(ss);Bl=l(Qm,"ONNX exporter supports generating a quantized version of the model to allow efficient inference."),Qm.forEach(t),ln=m(e),Ae=a(e,"P",{});var fr=n(Ae);Il=l(fr,`Quantization works by converting the memory representation of the parameters in the neural network to a compact integer
format. By default, weights of a neural network are stored as single-precision float (`),vo=a(fr,"EM",{});var Vm=n(vo);Cl=l(Vm,"float32"),Vm.forEach(t),Rl=l(fr,`) which can express a
wide-range of floating-point numbers with decent precision. These properties are especially interesting at training
where you want fine-grained representation.`),fr.forEach(t),pn=m(e),Se=a(e,"P",{});var cr=n(Se);Dl=l(cr,`On the other hand, after the training phase, it has been shown one can greatly reduce the range and the precision of
`),go=a(cr,"EM",{});var Ym=n(go);Hl=l(Ym,"float32"),Ym.forEach(t),Fl=l(cr," numbers without changing the performances of the neural network."),cr.forEach(t),hn=m(e),X=a(e,"P",{});var pe=n(X);Ul=l(pe,"More technically, "),_o=a(pe,"EM",{});var Km=n(_o);Gl=l(Km,"float32"),Km.forEach(t),Wl=l(pe,` parameters are converted to a type requiring fewer bits to represent each number, thus
reducing the overall size of the model. Here, we are enabling `),wo=a(pe,"EM",{});var Zm=n(wo);Jl=l(Zm,"float32"),Zm.forEach(t),Ql=l(pe," mapping to "),bo=a(pe,"EM",{});var ef=n(bo);Vl=l(ef,"int8"),ef.forEach(t),Yl=l(pe,` values (a non-floating,
single byte, number representation) according to the following formula:
`),mn=cc(pe),pe.forEach(t),fn=m(e),_(Xe.$$.fragment,e),cn=m(e),os=a(e,"P",{});var tf=n(os);Kl=l(tf,"Leveraging tiny-integers has numerous advantages when it comes to inference:"),tf.forEach(t),dn=m(e),G=a(e,"UL",{});var vs=n(G);kt=a(vs,"LI",{});var dr=n(kt);Zl=l(dr,"Storing fewer bits instead of 32 bits for the "),yo=a(dr,"EM",{});var sf=n(yo);ep=l(sf,"float32"),sf.forEach(t),tp=l(dr," reduces the size of the model and makes it load faster."),dr.forEach(t),sp=m(vs),$o=a(vs,"LI",{});var of=n($o);op=l(of,"Integer operations execute a magnitude faster on modern hardware"),of.forEach(t),ap=m(vs),Eo=a(vs,"LI",{});var af=n(Eo);np=l(af,"Integer operations require less power to do the computations"),af.forEach(t),vs.forEach(t),un=m(e),I=a(e,"P",{});var Ze=n(I);rp=l(Ze,"In order to convert a transformers model to ONNX IR with quantized weights you just need to specify "),xo=a(Ze,"CODE",{});var nf=n(xo);ip=l(nf,"--quantize"),nf.forEach(t),lp=l(Ze,` when
using `),ko=a(Ze,"CODE",{});var rf=n(ko);pp=l(rf,"convert_graph_to_onnx.py"),rf.forEach(t),hp=l(Ze,". Also, you can have a look at the "),No=a(Ze,"CODE",{});var lf=n(No);mp=l(lf,"quantize()"),lf.forEach(t),fp=l(Ze,` utility-method in this same script
file.`),Ze.forEach(t),vn=m(e),as=a(e,"P",{});var pf=n(as);cp=l(pf,"Example of quantized BERT model export:"),pf.forEach(t),gn=m(e),_(Nt.$$.fragment,e),_n=m(e),_(Me.$$.fragment,e),wn=m(e),_(Le.$$.fragment,e),bn=m(e),ee=a(e,"H2",{class:!0});var ur=n(ee);Be=a(ur,"A",{id:!0,class:!0,href:!0});var hf=n(Be);To=a(hf,"SPAN",{});var mf=n(To);_(Tt.$$.fragment,mf),mf.forEach(t),hf.forEach(t),dp=m(ur),Oo=a(ur,"SPAN",{});var ff=n(Oo);up=l(ff,"TorchScript"),ff.forEach(t),ur.forEach(t),yn=m(e),_(Ie.$$.fragment,e),$n=m(e),Ce=a(e,"P",{});var vr=n(Ce);vp=l(vr,`According to Pytorch\u2019s documentation: \u201CTorchScript is a way to create serializable and optimizable models from PyTorch
code\u201D. Pytorch\u2019s two modules `),Ot=a(vr,"A",{href:!0,rel:!0});var cf=n(Ot);gp=l(cf,"JIT and TRACE"),cf.forEach(t),_p=l(vr,` allow the developer to export
their model to be re-used in other programs, such as efficiency-oriented C++ programs.`),vr.forEach(t),En=m(e),ns=a(e,"P",{});var df=n(ns);wp=l(df,`We have provided an interface that allows the export of \u{1F917} Transformers models to TorchScript so that they can be reused
in a different environment than a Pytorch-based python program. Here we explain how to export and use our models using
TorchScript.`),df.forEach(t),xn=m(e),rs=a(e,"P",{});var uf=n(rs);bp=l(uf,"Exporting a model requires two things:"),uf.forEach(t),kn=m(e),Re=a(e,"UL",{});var gr=n(Re);jo=a(gr,"LI",{});var vf=n(jo);yp=l(vf,"a forward pass with dummy inputs."),vf.forEach(t),$p=m(gr),jt=a(gr,"LI",{});var _r=n(jt);Ep=l(_r,"model instantiation with the "),Po=a(_r,"CODE",{});var gf=n(Po);xp=l(gf,"torchscript"),gf.forEach(t),kp=l(_r," flag."),_r.forEach(t),gr.forEach(t),Nn=m(e),is=a(e,"P",{});var _f=n(is);Np=l(_f,"These necessities imply several things developers should be careful about. These are detailed below."),_f.forEach(t),Tn=m(e),te=a(e,"H3",{class:!0});var wr=n(te);De=a(wr,"A",{id:!0,class:!0,href:!0});var wf=n(De);zo=a(wf,"SPAN",{});var bf=n(zo);_(Pt.$$.fragment,bf),bf.forEach(t),wf.forEach(t),Tp=m(wr),qo=a(wr,"SPAN",{});var yf=n(qo);Op=l(yf,"Implications"),yf.forEach(t),wr.forEach(t),On=m(e),se=a(e,"H3",{class:!0});var br=n(se);He=a(br,"A",{id:!0,class:!0,href:!0});var $f=n(He);Ao=a($f,"SPAN",{});var Ef=n(Ao);_(zt.$$.fragment,Ef),Ef.forEach(t),$f.forEach(t),jp=m(br),So=a(br,"SPAN",{});var xf=n(So);Pp=l(xf,"TorchScript flag and tied weights"),xf.forEach(t),br.forEach(t),jn=m(e),W=a(e,"P",{});var gs=n(W);zp=l(gs,`This flag is necessary because most of the language models in this repository have tied weights between their
`),Xo=a(gs,"CODE",{});var kf=n(Xo);qp=l(kf,"Embedding"),kf.forEach(t),Ap=l(gs," layer and their "),Mo=a(gs,"CODE",{});var Nf=n(Mo);Sp=l(Nf,"Decoding"),Nf.forEach(t),Xp=l(gs,` layer. TorchScript does not allow the export of models that have tied
weights, therefore it is necessary to untie and clone the weights beforehand.`),gs.forEach(t),Pn=m(e),C=a(e,"P",{});var et=n(C);Mp=l(et,"This implies that models instantiated with the "),Lo=a(et,"CODE",{});var Tf=n(Lo);Lp=l(Tf,"torchscript"),Tf.forEach(t),Bp=l(et," flag have their "),Bo=a(et,"CODE",{});var Of=n(Bo);Ip=l(Of,"Embedding"),Of.forEach(t),Cp=l(et," layer and "),Io=a(et,"CODE",{});var jf=n(Io);Rp=l(jf,"Decoding"),jf.forEach(t),Dp=l(et,`
layer separate, which means that they should not be trained down the line. Training would de-synchronize the two
layers, leading to unexpected results.`),et.forEach(t),zn=m(e),Fe=a(e,"P",{});var yr=n(Fe);Hp=l(yr,`This is not the case for models that do not have a Language Model head, as those do not have tied weights. These models
can be safely exported without the `),Co=a(yr,"CODE",{});var Pf=n(Co);Fp=l(Pf,"torchscript"),Pf.forEach(t),Up=l(yr," flag."),yr.forEach(t),qn=m(e),oe=a(e,"H3",{class:!0});var $r=n(oe);Ue=a($r,"A",{id:!0,class:!0,href:!0});var zf=n(Ue);Ro=a(zf,"SPAN",{});var qf=n(Ro);_(qt.$$.fragment,qf),qf.forEach(t),zf.forEach(t),Gp=m($r),Do=a($r,"SPAN",{});var Af=n(Do);Wp=l(Af,"Dummy inputs and standard lengths"),Af.forEach(t),$r.forEach(t),An=m(e),ls=a(e,"P",{});var Sf=n(ls);Jp=l(Sf,`The dummy inputs are used to do a model forward pass. While the inputs\u2019 values are propagating through the layers,
Pytorch keeps track of the different operations executed on each tensor. These recorded operations are then used to
create the \u201Ctrace\u201D of the model.`),Sf.forEach(t),Sn=m(e),ps=a(e,"P",{});var Xf=n(ps);Qp=l(Xf,`The trace is created relatively to the inputs\u2019 dimensions. It is therefore constrained by the dimensions of the dummy
input, and will not work for any other sequence length or batch size. When trying with a different size, an error such
as:`),Xf.forEach(t),Xn=m(e),hs=a(e,"P",{});var Mf=n(hs);Ho=a(Mf,"CODE",{});var Lf=n(Ho);Vp=l(Lf,"The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2"),Lf.forEach(t),Mf.forEach(t),Mn=m(e),ms=a(e,"P",{});var Bf=n(ms);Yp=l(Bf,`will be raised. It is therefore recommended to trace the model with a dummy input size at least as large as the largest
input that will be fed to the model during inference. Padding can be performed to fill the missing values. As the model
will have been traced with a large input size however, the dimensions of the different matrix will be large as well,
resulting in more calculations.`),Bf.forEach(t),Ln=m(e),fs=a(e,"P",{});var If=n(fs);Kp=l(If,`It is recommended to be careful of the total number of operations done on each input and to follow performance closely
when exporting varying sequence-length models.`),If.forEach(t),Bn=m(e),ae=a(e,"H3",{class:!0});var Er=n(ae);Ge=a(Er,"A",{id:!0,class:!0,href:!0});var Cf=n(Ge);Fo=a(Cf,"SPAN",{});var Rf=n(Fo);_(At.$$.fragment,Rf),Rf.forEach(t),Cf.forEach(t),Zp=m(Er),Uo=a(Er,"SPAN",{});var Df=n(Uo);eh=l(Df,"Using TorchScript in Python"),Df.forEach(t),Er.forEach(t),In=m(e),cs=a(e,"P",{});var Hf=n(cs);th=l(Hf,"Below is an example, showing how to save, load models as well as how to use the trace for inference."),Hf.forEach(t),Cn=m(e),ne=a(e,"H4",{class:!0});var xr=n(ne);We=a(xr,"A",{id:!0,class:!0,href:!0});var Ff=n(We);Go=a(Ff,"SPAN",{});var Uf=n(Go);_(St.$$.fragment,Uf),Uf.forEach(t),Ff.forEach(t),sh=m(xr),Wo=a(xr,"SPAN",{});var Gf=n(Wo);oh=l(Gf,"Saving a model"),Gf.forEach(t),xr.forEach(t),Rn=m(e),M=a(e,"P",{});var he=n(M);ah=l(he,"This snippet shows how to use TorchScript to export a "),Jo=a(he,"CODE",{});var Wf=n(Jo);nh=l(Wf,"BertModel"),Wf.forEach(t),rh=l(he,". Here the "),Qo=a(he,"CODE",{});var Jf=n(Qo);ih=l(Jf,"BertModel"),Jf.forEach(t),lh=l(he,` is instantiated according
to a `),Vo=a(he,"CODE",{});var Qf=n(Vo);ph=l(Qf,"BertConfig"),Qf.forEach(t),hh=l(he," class and then saved to disk under the filename "),Yo=a(he,"CODE",{});var Vf=n(Yo);mh=l(Vf,"traced_bert.pt"),Vf.forEach(t),he.forEach(t),Dn=m(e),_(Xt.$$.fragment,e),Hn=m(e),re=a(e,"H4",{class:!0});var kr=n(re);Je=a(kr,"A",{id:!0,class:!0,href:!0});var Yf=n(Je);Ko=a(Yf,"SPAN",{});var Kf=n(Ko);_(Mt.$$.fragment,Kf),Kf.forEach(t),Yf.forEach(t),fh=m(kr),Zo=a(kr,"SPAN",{});var Zf=n(Zo);ch=l(Zf,"Loading a model"),Zf.forEach(t),kr.forEach(t),Fn=m(e),R=a(e,"P",{});var tt=n(R);dh=l(tt,"This snippet shows how to load the "),ea=a(tt,"CODE",{});var ec=n(ea);uh=l(ec,"BertModel"),ec.forEach(t),vh=l(tt," that was previously saved to disk under the name "),ta=a(tt,"CODE",{});var tc=n(ta);gh=l(tc,"traced_bert.pt"),tc.forEach(t),_h=l(tt,`.
We are re-using the previously initialised `),sa=a(tt,"CODE",{});var sc=n(sa);wh=l(sc,"dummy_input"),sc.forEach(t),bh=l(tt,"."),tt.forEach(t),Un=m(e),_(Lt.$$.fragment,e),Gn=m(e),ie=a(e,"H4",{class:!0});var Nr=n(ie);Qe=a(Nr,"A",{id:!0,class:!0,href:!0});var oc=n(Qe);oa=a(oc,"SPAN",{});var ac=n(oa);_(Bt.$$.fragment,ac),ac.forEach(t),oc.forEach(t),yh=m(Nr),aa=a(Nr,"SPAN",{});var nc=n(aa);$h=l(nc,"Using a traced model for inference"),nc.forEach(t),Nr.forEach(t),Wn=m(e),Ve=a(e,"P",{});var Tr=n(Ve);Eh=l(Tr,"Using the traced model for inference is as simple as using its "),na=a(Tr,"CODE",{});var rc=n(na);xh=l(rc,"__call__"),rc.forEach(t),kh=l(Tr," dunder method:"),Tr.forEach(t),Jn=m(e),_(It.$$.fragment,e),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(xc)),c(u,"id","exporting-transformers-models"),c(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u,"href","#exporting-transformers-models"),c(d,"class","relative group"),c(S,"id","onnx-onnxruntime"),c(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(S,"href","#onnx-onnxruntime"),c(A,"class","relative group"),c(ot,"href","http://onnx.ai"),c(ot,"rel","nofollow"),c(at,"href","https://microsoft.github.io/onnxruntime/"),c(at,"rel","nofollow"),c(nt,"href","https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333"),c(nt,"rel","nofollow"),c(ce,"id","configurationbased-approach"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#configurationbased-approach"),c(J,"class","relative group"),c(ve,"id","converting-an-onnx-model-using-the-transformersonnx-package"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#converting-an-onnx-model-using-the-transformersonnx-package"),c(Q,"class","relative group"),c(be,"id","implementing-a-custom-configuration-for-an-unsupported-architecture"),c(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(be,"href","#implementing-a-custom-configuration-for-an-unsupported-architecture"),c(V,"class","relative group"),c(gt,"href","https://github.com/huggingface/transformers/pull/13049/commits/d097adcebd89a520f04352eb215a85916934204f"),c(gt,"rel","nofollow"),c(wt,"href","https://github.com/huggingface/transformers/pull/13049/commits/5d642f65abf45ceeb72bd855ca7bfe2506a58e6a"),c(wt,"rel","nofollow"),c($e,"id","graph-conversion"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#graph-conversion"),c(Y,"class","relative group"),c($t,"href","https://github.com/pytorch/pytorch/pull/33062"),c($t,"rel","nofollow"),c(Te,"id","optimizations"),c(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Te,"href","#optimizations"),c(K,"class","relative group"),c(qe,"id","quantization"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#quantization"),c(Z,"class","relative group"),mn.a=null,c(Be,"id","torchscript"),c(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Be,"href","#torchscript"),c(ee,"class","relative group"),c(Ot,"href","https://pytorch.org/docs/stable/jit.html"),c(Ot,"rel","nofollow"),c(De,"id","implications"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#implications"),c(te,"class","relative group"),c(He,"id","torchscript-flag-and-tied-weights"),c(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(He,"href","#torchscript-flag-and-tied-weights"),c(se,"class","relative group"),c(Ue,"id","dummy-inputs-and-standard-lengths"),c(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ue,"href","#dummy-inputs-and-standard-lengths"),c(oe,"class","relative group"),c(Ge,"id","using-torchscript-in-python"),c(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ge,"href","#using-torchscript-in-python"),c(ae,"class","relative group"),c(We,"id","saving-a-model"),c(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(We,"href","#saving-a-model"),c(ne,"class","relative group"),c(Je,"id","loading-a-model"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#loading-a-model"),c(re,"class","relative group"),c(Qe,"id","using-a-traced-model-for-inference"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#using-a-traced-model-for-inference"),c(ie,"class","relative group")},m(e,r){s(document.head,f),p(e,v,r),p(e,d,r),s(d,u),s(u,j),w(E,j,null),s(d,N),s(d,P),s(P,q),p(e,T,r),p(e,A,r),s(A,S),s(S,_s),w(st,_s,null),s(A,Or),s(A,ws),s(ws,jr),p(e,ca,r),p(e,H,r),s(H,Pr),s(H,ot),s(ot,zr),s(H,qr),s(H,at),s(at,Ar),s(H,Sr),p(e,da,r),p(e,fe,r),s(fe,Xr),s(fe,nt),s(nt,Mr),s(fe,Lr),p(e,ua,r),p(e,J,r),s(J,ce),s(ce,bs),w(rt,bs,null),s(J,Br),s(J,ys),s(ys,Ir),p(e,va,r),p(e,de,r),s(de,Cr),s(de,$s),s($s,Rr),s(de,Dr),p(e,ga,r),p(e,Dt,r),s(Dt,Hr),p(e,_a,r),p(e,x,r),s(x,Es),s(Es,Fr),s(x,Ur),s(x,xs),s(xs,Gr),s(x,Wr),s(x,ks),s(ks,Jr),s(x,Qr),s(x,Ns),s(Ns,Vr),s(x,Yr),s(x,Ts),s(Ts,Kr),s(x,Zr),s(x,Os),s(Os,ei),s(x,ti),s(x,js),s(js,si),s(x,oi),s(x,Ps),s(Ps,ai),s(x,ni),s(x,zs),s(zs,ri),s(x,ii),s(x,qs),s(qs,li),s(x,pi),s(x,As),s(As,hi),s(x,mi),s(x,Ss),s(Ss,fi),s(x,ci),s(x,Xs),s(Xs,di),p(e,wa,r),p(e,Ht,r),s(Ht,ui),p(e,ba,r),w(ue,e,r),p(e,ya,r),p(e,Q,r),s(Q,ve),s(ve,Ms),w(it,Ms,null),s(Q,vi),s(Q,lt),s(lt,gi),s(lt,Ls),s(Ls,_i),s(lt,wi),p(e,$a,r),p(e,Ft,r),s(Ft,bi),p(e,Ea,r),w(pt,e,r),p(e,xa,r),p(e,Ut,r),s(Ut,yi),p(e,ka,r),w(ht,e,r),p(e,Na,r),p(e,ge,r),s(ge,$i),s(ge,Bs),s(Bs,Ei),s(ge,xi),p(e,Ta,r),p(e,_e,r),s(_e,ki),s(_e,Is),s(Is,Ni),s(_e,Ti),p(e,Oa,r),w(mt,e,r),p(e,ja,r),p(e,Gt,r),s(Gt,Oi),p(e,Pa,r),w(ft,e,r),p(e,za,r),p(e,we,r),s(we,ji),s(we,Cs),s(Cs,Pi),s(we,zi),p(e,qa,r),w(ct,e,r),p(e,Aa,r),p(e,V,r),s(V,be),s(be,Rs),w(dt,Rs,null),s(V,qi),s(V,Ds),s(Ds,Ai),p(e,Sa,r),p(e,Wt,r),s(Wt,Si),p(e,Xa,r),w(ut,e,r),p(e,Ma,r),p(e,Jt,r),s(Jt,Xi),p(e,La,r),p(e,Qt,r),s(Qt,Mi),p(e,Ba,r),p(e,Vt,r),s(Vt,Li),p(e,Ia,r),p(e,Yt,r),s(Yt,Bi),p(e,Ca,r),p(e,ye,r),s(ye,Ii),s(ye,Hs),s(Hs,Ci),s(ye,Ri),p(e,Ra,r),p(e,L,r),s(L,Di),s(L,Fs),s(Fs,Hi),s(L,Fi),s(L,Us),s(Us,Ui),s(L,Gi),s(L,Gs),s(Gs,Wi),s(L,Ji),p(e,Da,r),p(e,vt,r),s(vt,Qi),s(vt,gt),s(gt,Vi),p(e,Ha,r),p(e,_t,r),s(_t,Yi),s(_t,wt),s(wt,Ki),p(e,Fa,r),p(e,Y,r),s(Y,$e),s($e,Ws),w(bt,Ws,null),s(Y,Zi),s(Y,Js),s(Js,el),p(e,Ua,r),w(Ee,e,r),p(e,Ga,r),p(e,xe,r),s(xe,tl),s(xe,Qs),s(Qs,sl),s(xe,ol),p(e,Wa,r),w(yt,e,r),p(e,Ja,r),p(e,Kt,r),s(Kt,al),p(e,Qa,r),p(e,F,r),s(F,Vs),s(Vs,nl),s(F,rl),s(F,Ys),s(Ys,il),s(F,ll),s(F,Ks),s(Ks,pl),p(e,Va,r),w(ke,e,r),p(e,Ya,r),p(e,Zt,r),s(Zt,hl),p(e,Ka,r),p(e,U,r),s(U,Zs),s(Zs,es),s(es,eo),s(eo,ml),s(es,fl),s(U,cl),s(U,to),s(to,ts),s(ts,so),s(so,dl),s(ts,ul),s(U,vl),s(U,oo),s(oo,Ne),s(Ne,ao),s(ao,gl),s(Ne,_l),s(Ne,$t),s($t,wl),s(Ne,bl),p(e,Za,r),p(e,K,r),s(K,Te),s(Te,no),w(Et,no,null),s(K,yl),s(K,ro),s(ro,$l),p(e,en,r),p(e,Oe,r),s(Oe,El),s(Oe,io),s(io,xl),s(Oe,kl),p(e,tn,r),p(e,B,r),s(B,lo),s(lo,Nl),s(B,Tl),s(B,po),s(po,Ol),s(B,jl),s(B,ho),s(ho,Pl),s(B,zl),s(B,mo),s(mo,ql),p(e,sn,r),p(e,je,r),s(je,Al),s(je,fo),s(fo,Sl),s(je,Xl),p(e,on,r),w(Pe,e,r),p(e,an,r),w(ze,e,r),p(e,nn,r),p(e,Z,r),s(Z,qe),s(qe,co),w(xt,co,null),s(Z,Ml),s(Z,uo),s(uo,Ll),p(e,rn,r),p(e,ss,r),s(ss,Bl),p(e,ln,r),p(e,Ae,r),s(Ae,Il),s(Ae,vo),s(vo,Cl),s(Ae,Rl),p(e,pn,r),p(e,Se,r),s(Se,Dl),s(Se,go),s(go,Hl),s(Se,Fl),p(e,hn,r),p(e,X,r),s(X,Ul),s(X,_o),s(_o,Gl),s(X,Wl),s(X,wo),s(wo,Jl),s(X,Ql),s(X,bo),s(bo,Vl),s(X,Yl),mn.m(ic,X),p(e,fn,r),w(Xe,e,r),p(e,cn,r),p(e,os,r),s(os,Kl),p(e,dn,r),p(e,G,r),s(G,kt),s(kt,Zl),s(kt,yo),s(yo,ep),s(kt,tp),s(G,sp),s(G,$o),s($o,op),s(G,ap),s(G,Eo),s(Eo,np),p(e,un,r),p(e,I,r),s(I,rp),s(I,xo),s(xo,ip),s(I,lp),s(I,ko),s(ko,pp),s(I,hp),s(I,No),s(No,mp),s(I,fp),p(e,vn,r),p(e,as,r),s(as,cp),p(e,gn,r),w(Nt,e,r),p(e,_n,r),w(Me,e,r),p(e,wn,r),w(Le,e,r),p(e,bn,r),p(e,ee,r),s(ee,Be),s(Be,To),w(Tt,To,null),s(ee,dp),s(ee,Oo),s(Oo,up),p(e,yn,r),w(Ie,e,r),p(e,$n,r),p(e,Ce,r),s(Ce,vp),s(Ce,Ot),s(Ot,gp),s(Ce,_p),p(e,En,r),p(e,ns,r),s(ns,wp),p(e,xn,r),p(e,rs,r),s(rs,bp),p(e,kn,r),p(e,Re,r),s(Re,jo),s(jo,yp),s(Re,$p),s(Re,jt),s(jt,Ep),s(jt,Po),s(Po,xp),s(jt,kp),p(e,Nn,r),p(e,is,r),s(is,Np),p(e,Tn,r),p(e,te,r),s(te,De),s(De,zo),w(Pt,zo,null),s(te,Tp),s(te,qo),s(qo,Op),p(e,On,r),p(e,se,r),s(se,He),s(He,Ao),w(zt,Ao,null),s(se,jp),s(se,So),s(So,Pp),p(e,jn,r),p(e,W,r),s(W,zp),s(W,Xo),s(Xo,qp),s(W,Ap),s(W,Mo),s(Mo,Sp),s(W,Xp),p(e,Pn,r),p(e,C,r),s(C,Mp),s(C,Lo),s(Lo,Lp),s(C,Bp),s(C,Bo),s(Bo,Ip),s(C,Cp),s(C,Io),s(Io,Rp),s(C,Dp),p(e,zn,r),p(e,Fe,r),s(Fe,Hp),s(Fe,Co),s(Co,Fp),s(Fe,Up),p(e,qn,r),p(e,oe,r),s(oe,Ue),s(Ue,Ro),w(qt,Ro,null),s(oe,Gp),s(oe,Do),s(Do,Wp),p(e,An,r),p(e,ls,r),s(ls,Jp),p(e,Sn,r),p(e,ps,r),s(ps,Qp),p(e,Xn,r),p(e,hs,r),s(hs,Ho),s(Ho,Vp),p(e,Mn,r),p(e,ms,r),s(ms,Yp),p(e,Ln,r),p(e,fs,r),s(fs,Kp),p(e,Bn,r),p(e,ae,r),s(ae,Ge),s(Ge,Fo),w(At,Fo,null),s(ae,Zp),s(ae,Uo),s(Uo,eh),p(e,In,r),p(e,cs,r),s(cs,th),p(e,Cn,r),p(e,ne,r),s(ne,We),s(We,Go),w(St,Go,null),s(ne,sh),s(ne,Wo),s(Wo,oh),p(e,Rn,r),p(e,M,r),s(M,ah),s(M,Jo),s(Jo,nh),s(M,rh),s(M,Qo),s(Qo,ih),s(M,lh),s(M,Vo),s(Vo,ph),s(M,hh),s(M,Yo),s(Yo,mh),p(e,Dn,r),w(Xt,e,r),p(e,Hn,r),p(e,re,r),s(re,Je),s(Je,Ko),w(Mt,Ko,null),s(re,fh),s(re,Zo),s(Zo,ch),p(e,Fn,r),p(e,R,r),s(R,dh),s(R,ea),s(ea,uh),s(R,vh),s(R,ta),s(ta,gh),s(R,_h),s(R,sa),s(sa,wh),s(R,bh),p(e,Un,r),w(Lt,e,r),p(e,Gn,r),p(e,ie,r),s(ie,Qe),s(Qe,oa),w(Bt,oa,null),s(ie,yh),s(ie,aa),s(aa,$h),p(e,Wn,r),p(e,Ve,r),s(Ve,Eh),s(Ve,na),s(na,xh),s(Ve,kh),p(e,Jn,r),w(It,e,r),Qn=!0},p(e,[r]){const Ct={};r&2&&(Ct.$$scope={dirty:r,ctx:e}),ue.$set(Ct);const ra={};r&2&&(ra.$$scope={dirty:r,ctx:e}),Ee.$set(ra);const ia={};r&2&&(ia.$$scope={dirty:r,ctx:e}),ke.$set(ia);const la={};r&2&&(la.$$scope={dirty:r,ctx:e}),Pe.$set(la);const Rt={};r&2&&(Rt.$$scope={dirty:r,ctx:e}),ze.$set(Rt);const pa={};r&2&&(pa.$$scope={dirty:r,ctx:e}),Xe.$set(pa);const ha={};r&2&&(ha.$$scope={dirty:r,ctx:e}),Me.$set(ha);const ma={};r&2&&(ma.$$scope={dirty:r,ctx:e}),Le.$set(ma);const le={};r&2&&(le.$$scope={dirty:r,ctx:e}),Ie.$set(le)},i(e){Qn||(b(E.$$.fragment,e),b(st.$$.fragment,e),b(rt.$$.fragment,e),b(ue.$$.fragment,e),b(it.$$.fragment,e),b(pt.$$.fragment,e),b(ht.$$.fragment,e),b(mt.$$.fragment,e),b(ft.$$.fragment,e),b(ct.$$.fragment,e),b(dt.$$.fragment,e),b(ut.$$.fragment,e),b(bt.$$.fragment,e),b(Ee.$$.fragment,e),b(yt.$$.fragment,e),b(ke.$$.fragment,e),b(Et.$$.fragment,e),b(Pe.$$.fragment,e),b(ze.$$.fragment,e),b(xt.$$.fragment,e),b(Xe.$$.fragment,e),b(Nt.$$.fragment,e),b(Me.$$.fragment,e),b(Le.$$.fragment,e),b(Tt.$$.fragment,e),b(Ie.$$.fragment,e),b(Pt.$$.fragment,e),b(zt.$$.fragment,e),b(qt.$$.fragment,e),b(At.$$.fragment,e),b(St.$$.fragment,e),b(Xt.$$.fragment,e),b(Mt.$$.fragment,e),b(Lt.$$.fragment,e),b(Bt.$$.fragment,e),b(It.$$.fragment,e),Qn=!0)},o(e){y(E.$$.fragment,e),y(st.$$.fragment,e),y(rt.$$.fragment,e),y(ue.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(ht.$$.fragment,e),y(mt.$$.fragment,e),y(ft.$$.fragment,e),y(ct.$$.fragment,e),y(dt.$$.fragment,e),y(ut.$$.fragment,e),y(bt.$$.fragment,e),y(Ee.$$.fragment,e),y(yt.$$.fragment,e),y(ke.$$.fragment,e),y(Et.$$.fragment,e),y(Pe.$$.fragment,e),y(ze.$$.fragment,e),y(xt.$$.fragment,e),y(Xe.$$.fragment,e),y(Nt.$$.fragment,e),y(Me.$$.fragment,e),y(Le.$$.fragment,e),y(Tt.$$.fragment,e),y(Ie.$$.fragment,e),y(Pt.$$.fragment,e),y(zt.$$.fragment,e),y(qt.$$.fragment,e),y(At.$$.fragment,e),y(St.$$.fragment,e),y(Xt.$$.fragment,e),y(Mt.$$.fragment,e),y(Lt.$$.fragment,e),y(Bt.$$.fragment,e),y(It.$$.fragment,e),Qn=!1},d(e){t(f),e&&t(v),e&&t(d),$(E),e&&t(T),e&&t(A),$(st),e&&t(ca),e&&t(H),e&&t(da),e&&t(fe),e&&t(ua),e&&t(J),$(rt),e&&t(va),e&&t(de),e&&t(ga),e&&t(Dt),e&&t(_a),e&&t(x),e&&t(wa),e&&t(Ht),e&&t(ba),$(ue,e),e&&t(ya),e&&t(Q),$(it),e&&t($a),e&&t(Ft),e&&t(Ea),$(pt,e),e&&t(xa),e&&t(Ut),e&&t(ka),$(ht,e),e&&t(Na),e&&t(ge),e&&t(Ta),e&&t(_e),e&&t(Oa),$(mt,e),e&&t(ja),e&&t(Gt),e&&t(Pa),$(ft,e),e&&t(za),e&&t(we),e&&t(qa),$(ct,e),e&&t(Aa),e&&t(V),$(dt),e&&t(Sa),e&&t(Wt),e&&t(Xa),$(ut,e),e&&t(Ma),e&&t(Jt),e&&t(La),e&&t(Qt),e&&t(Ba),e&&t(Vt),e&&t(Ia),e&&t(Yt),e&&t(Ca),e&&t(ye),e&&t(Ra),e&&t(L),e&&t(Da),e&&t(vt),e&&t(Ha),e&&t(_t),e&&t(Fa),e&&t(Y),$(bt),e&&t(Ua),$(Ee,e),e&&t(Ga),e&&t(xe),e&&t(Wa),$(yt,e),e&&t(Ja),e&&t(Kt),e&&t(Qa),e&&t(F),e&&t(Va),$(ke,e),e&&t(Ya),e&&t(Zt),e&&t(Ka),e&&t(U),e&&t(Za),e&&t(K),$(Et),e&&t(en),e&&t(Oe),e&&t(tn),e&&t(B),e&&t(sn),e&&t(je),e&&t(on),$(Pe,e),e&&t(an),$(ze,e),e&&t(nn),e&&t(Z),$(xt),e&&t(rn),e&&t(ss),e&&t(ln),e&&t(Ae),e&&t(pn),e&&t(Se),e&&t(hn),e&&t(X),e&&t(fn),$(Xe,e),e&&t(cn),e&&t(os),e&&t(dn),e&&t(G),e&&t(un),e&&t(I),e&&t(vn),e&&t(as),e&&t(gn),$(Nt,e),e&&t(_n),$(Me,e),e&&t(wn),$(Le,e),e&&t(bn),e&&t(ee),$(Tt),e&&t(yn),$(Ie,e),e&&t($n),e&&t(Ce),e&&t(En),e&&t(ns),e&&t(xn),e&&t(rs),e&&t(kn),e&&t(Re),e&&t(Nn),e&&t(is),e&&t(Tn),e&&t(te),$(Pt),e&&t(On),e&&t(se),$(zt),e&&t(jn),e&&t(W),e&&t(Pn),e&&t(C),e&&t(zn),e&&t(Fe),e&&t(qn),e&&t(oe),$(qt),e&&t(An),e&&t(ls),e&&t(Sn),e&&t(ps),e&&t(Xn),e&&t(hs),e&&t(Mn),e&&t(ms),e&&t(Ln),e&&t(fs),e&&t(Bn),e&&t(ae),$(At),e&&t(In),e&&t(cs),e&&t(Cn),e&&t(ne),$(St),e&&t(Rn),e&&t(M),e&&t(Dn),$(Xt,e),e&&t(Hn),e&&t(re),$(Mt),e&&t(Fn),e&&t(R),e&&t(Un),$(Lt,e),e&&t(Gn),e&&t(ie),$(Bt),e&&t(Wn),e&&t(Ve),e&&t(Jn),$(It,e)}}}const xc={local:"exporting-transformers-models",sections:[{local:"onnx-onnxruntime",sections:[{local:"configurationbased-approach",sections:[{local:"converting-an-onnx-model-using-the-transformersonnx-package",title:"Converting an ONNX model using the `transformers.onnx` package"},{local:"implementing-a-custom-configuration-for-an-unsupported-architecture",title:"Implementing a custom configuration for an unsupported architecture"}],title:"Configuration-based approach"},{local:"graph-conversion",title:"Graph conversion"},{local:"optimizations",title:"Optimizations"},{local:"quantization",title:"Quantization"}],title:"ONNX / ONNXRuntime"},{local:"torchscript",sections:[{local:"implications",title:"Implications"},{local:"torchscript-flag-and-tied-weights",title:"TorchScript flag and tied weights"},{local:"dummy-inputs-and-standard-lengths",title:"Dummy inputs and standard lengths"},{local:"using-torchscript-in-python",sections:[{local:"saving-a-model",title:"Saving a model"},{local:"loading-a-model",title:"Loading a model"},{local:"using-a-traced-model-for-inference",title:"Using a traced model for inference"}],title:"Using TorchScript in Python"}],title:"TorchScript"}],title:"Exporting transformers models"};function kc(O,f,v){let{fw:d}=f;return O.$$set=u=>{"fw"in u&&v(0,d=u.fw)},[d]}class zc extends lc{constructor(f){super();pc(this,f,kc,Ec,hc,{fw:0})}}export{zc as default,xc as metadata};
