import{S as jt,i as $t,s as bt,e as r,k as h,w as N,t as i,M as Pt,c as n,d as t,m,a as o,x as B,h as l,b as f,F as s,g as p,y as L,L as Et,q as x,o as O,B as J}from"../chunks/vendor-ab4e3193.js";import{I as at}from"../chunks/IconCopyLink-d992940d.js";import{C as $e}from"../chunks/CodeBlock-516df0c5.js";import"../chunks/CopyButton-204b56db.js";function St(be){let d,W,c,k,X,$,Pe,Y,Ee,ne,g,Se,C,Ae,qe,b,Fe,Ne,oe,H,Be,ie,P,le,U,Le,pe,v,_,Z,E,xe,ee,Oe,fe,u,Je,K,We,Ce,te,He,Ue,he,S,me,T,Ke,M,Me,Ie,ce,y,w,se,A,De,ae,Ge,ke,I,Qe,de,q,ge,z,Re,D,Ve,Xe,re,Ye,Ze,ue,F,ze,j,et,G,tt,st,ve;return $=new at({}),P=new $e({props:{code:`from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])

tokenizer.pre_tokenizer = Whitespace()
files = [...]
tokenizer.train(files, trainer),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer.pre_tokenizer = Whitespace()</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">files = [...]</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer.train(files, trainer)</span>`}}),E=new at({}),S=new $e({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)</span>`}}),A=new at({}),q=new $e({props:{code:'tokenizer.save("tokenizer.json"),',highlighted:'<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)</span>'}}),F=new $e({props:{code:`from transformers import PreTrainedTokenizerFast

fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file="tokenizer.json"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)</span>`}}),{c(){d=r("meta"),W=h(),c=r("h1"),k=r("a"),X=r("span"),N($.$$.fragment),Pe=h(),Y=r("span"),Ee=i("Using tokenizers from \u{1F917} Tokenizers"),ne=h(),g=r("p"),Se=i("The "),C=r("a"),Ae=i("PreTrainedTokenizerFast"),qe=i(" depends on the "),b=r("a"),Fe=i("tokenizers"),Ne=i(` library. The tokenizers obtained from the \u{1F917} Tokenizers library can be
loaded very simply into \u{1F917} Transformers.`),oe=h(),H=r("p"),Be=i("Before getting in the specifics, let\u2019s first start by creating a dummy tokenizer in a few lines:"),ie=h(),N(P.$$.fragment),le=h(),U=r("p"),Le=i(`We now have a tokenizer trained on the files we defined. We can either continue using it in that runtime, or save it to
a JSON file for future re-use.`),pe=h(),v=r("h2"),_=r("a"),Z=r("span"),N(E.$$.fragment),xe=h(),ee=r("span"),Oe=i("Loading directly from the tokenizer object"),fe=h(),u=r("p"),Je=i(`Let\u2019s see how to leverage this tokenizer object in the \u{1F917} Transformers library. The
`),K=r("a"),We=i("PreTrainedTokenizerFast"),Ce=i(` class allows for easy instantiation, by accepting the instantiated
`),te=r("em"),He=i("tokenizer"),Ue=i(" object as an argument:"),he=h(),N(S.$$.fragment),me=h(),T=r("p"),Ke=i("This object can now be used with all the methods shared by the \u{1F917} Transformers tokenizers! Head to "),M=r("a"),Me=i(`the tokenizer
page`),Ie=i(" for more information."),ce=h(),y=r("h2"),w=r("a"),se=r("span"),N(A.$$.fragment),De=h(),ae=r("span"),Ge=i("Loading from a JSON file"),ke=h(),I=r("p"),Qe=i("In order to load a tokenizer from a JSON file, let\u2019s first start by saving our tokenizer:"),de=h(),N(q.$$.fragment),ge=h(),z=r("p"),Re=i("The path to which we saved this file can be passed to the "),D=r("a"),Ve=i("PreTrainedTokenizerFast"),Xe=i(` initialization
method using the `),re=r("code"),Ye=i("tokenizer_file"),Ze=i(" parameter:"),ue=h(),N(F.$$.fragment),ze=h(),j=r("p"),et=i("This object can now be used with all the methods shared by the \u{1F917} Transformers tokenizers! Head to "),G=r("a"),tt=i(`the tokenizer
page`),st=i(" for more information."),this.h()},l(e){const a=Pt('[data-svelte="svelte-1phssyn"]',document.head);d=n(a,"META",{name:!0,content:!0}),a.forEach(t),W=m(e),c=n(e,"H1",{class:!0});var ye=o(c);k=n(ye,"A",{id:!0,class:!0,href:!0});var rt=o(k);X=n(rt,"SPAN",{});var nt=o(X);B($.$$.fragment,nt),nt.forEach(t),rt.forEach(t),Pe=m(ye),Y=n(ye,"SPAN",{});var ot=o(Y);Ee=l(ot,"Using tokenizers from \u{1F917} Tokenizers"),ot.forEach(t),ye.forEach(t),ne=m(e),g=n(e,"P",{});var Q=o(g);Se=l(Q,"The "),C=n(Q,"A",{href:!0});var it=o(C);Ae=l(it,"PreTrainedTokenizerFast"),it.forEach(t),qe=l(Q," depends on the "),b=n(Q,"A",{href:!0,rel:!0});var lt=o(b);Fe=l(lt,"tokenizers"),lt.forEach(t),Ne=l(Q,` library. The tokenizers obtained from the \u{1F917} Tokenizers library can be
loaded very simply into \u{1F917} Transformers.`),Q.forEach(t),oe=m(e),H=n(e,"P",{});var pt=o(H);Be=l(pt,"Before getting in the specifics, let\u2019s first start by creating a dummy tokenizer in a few lines:"),pt.forEach(t),ie=m(e),B(P.$$.fragment,e),le=m(e),U=n(e,"P",{});var ft=o(U);Le=l(ft,`We now have a tokenizer trained on the files we defined. We can either continue using it in that runtime, or save it to
a JSON file for future re-use.`),ft.forEach(t),pe=m(e),v=n(e,"H2",{class:!0});var _e=o(v);_=n(_e,"A",{id:!0,class:!0,href:!0});var ht=o(_);Z=n(ht,"SPAN",{});var mt=o(Z);B(E.$$.fragment,mt),mt.forEach(t),ht.forEach(t),xe=m(_e),ee=n(_e,"SPAN",{});var ct=o(ee);Oe=l(ct,"Loading directly from the tokenizer object"),ct.forEach(t),_e.forEach(t),fe=m(e),u=n(e,"P",{});var R=o(u);Je=l(R,`Let\u2019s see how to leverage this tokenizer object in the \u{1F917} Transformers library. The
`),K=n(R,"A",{href:!0});var kt=o(K);We=l(kt,"PreTrainedTokenizerFast"),kt.forEach(t),Ce=l(R,` class allows for easy instantiation, by accepting the instantiated
`),te=n(R,"EM",{});var dt=o(te);He=l(dt,"tokenizer"),dt.forEach(t),Ue=l(R," object as an argument:"),R.forEach(t),he=m(e),B(S.$$.fragment,e),me=m(e),T=n(e,"P",{});var Te=o(T);Ke=l(Te,"This object can now be used with all the methods shared by the \u{1F917} Transformers tokenizers! Head to "),M=n(Te,"A",{href:!0});var gt=o(M);Me=l(gt,`the tokenizer
page`),gt.forEach(t),Ie=l(Te," for more information."),Te.forEach(t),ce=m(e),y=n(e,"H2",{class:!0});var we=o(y);w=n(we,"A",{id:!0,class:!0,href:!0});var ut=o(w);se=n(ut,"SPAN",{});var zt=o(se);B(A.$$.fragment,zt),zt.forEach(t),ut.forEach(t),De=m(we),ae=n(we,"SPAN",{});var vt=o(ae);Ge=l(vt,"Loading from a JSON file"),vt.forEach(t),we.forEach(t),ke=m(e),I=n(e,"P",{});var yt=o(I);Qe=l(yt,"In order to load a tokenizer from a JSON file, let\u2019s first start by saving our tokenizer:"),yt.forEach(t),de=m(e),B(q.$$.fragment,e),ge=m(e),z=n(e,"P",{});var V=o(z);Re=l(V,"The path to which we saved this file can be passed to the "),D=n(V,"A",{href:!0});var _t=o(D);Ve=l(_t,"PreTrainedTokenizerFast"),_t.forEach(t),Xe=l(V,` initialization
method using the `),re=n(V,"CODE",{});var Tt=o(re);Ye=l(Tt,"tokenizer_file"),Tt.forEach(t),Ze=l(V," parameter:"),V.forEach(t),ue=m(e),B(F.$$.fragment,e),ze=m(e),j=n(e,"P",{});var je=o(j);et=l(je,"This object can now be used with all the methods shared by the \u{1F917} Transformers tokenizers! Head to "),G=n(je,"A",{href:!0});var wt=o(G);tt=l(wt,`the tokenizer
page`),wt.forEach(t),st=l(je," for more information."),je.forEach(t),this.h()},h(){f(d,"name","hf:doc:metadata"),f(d,"content",JSON.stringify(At)),f(k,"id","using-tokenizers-from-tokenizers"),f(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(k,"href","#using-tokenizers-from-tokenizers"),f(c,"class","relative group"),f(C,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),f(b,"href","https://huggingface.co/docs/tokenizers"),f(b,"rel","nofollow"),f(_,"id","loading-directly-from-the-tokenizer-object"),f(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(_,"href","#loading-directly-from-the-tokenizer-object"),f(v,"class","relative group"),f(K,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),f(M,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer"),f(w,"id","loading-from-a-json-file"),f(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(w,"href","#loading-from-a-json-file"),f(y,"class","relative group"),f(D,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),f(G,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer")},m(e,a){s(document.head,d),p(e,W,a),p(e,c,a),s(c,k),s(k,X),L($,X,null),s(c,Pe),s(c,Y),s(Y,Ee),p(e,ne,a),p(e,g,a),s(g,Se),s(g,C),s(C,Ae),s(g,qe),s(g,b),s(b,Fe),s(g,Ne),p(e,oe,a),p(e,H,a),s(H,Be),p(e,ie,a),L(P,e,a),p(e,le,a),p(e,U,a),s(U,Le),p(e,pe,a),p(e,v,a),s(v,_),s(_,Z),L(E,Z,null),s(v,xe),s(v,ee),s(ee,Oe),p(e,fe,a),p(e,u,a),s(u,Je),s(u,K),s(K,We),s(u,Ce),s(u,te),s(te,He),s(u,Ue),p(e,he,a),L(S,e,a),p(e,me,a),p(e,T,a),s(T,Ke),s(T,M),s(M,Me),s(T,Ie),p(e,ce,a),p(e,y,a),s(y,w),s(w,se),L(A,se,null),s(y,De),s(y,ae),s(ae,Ge),p(e,ke,a),p(e,I,a),s(I,Qe),p(e,de,a),L(q,e,a),p(e,ge,a),p(e,z,a),s(z,Re),s(z,D),s(D,Ve),s(z,Xe),s(z,re),s(re,Ye),s(z,Ze),p(e,ue,a),L(F,e,a),p(e,ze,a),p(e,j,a),s(j,et),s(j,G),s(G,tt),s(j,st),ve=!0},p:Et,i(e){ve||(x($.$$.fragment,e),x(P.$$.fragment,e),x(E.$$.fragment,e),x(S.$$.fragment,e),x(A.$$.fragment,e),x(q.$$.fragment,e),x(F.$$.fragment,e),ve=!0)},o(e){O($.$$.fragment,e),O(P.$$.fragment,e),O(E.$$.fragment,e),O(S.$$.fragment,e),O(A.$$.fragment,e),O(q.$$.fragment,e),O(F.$$.fragment,e),ve=!1},d(e){t(d),e&&t(W),e&&t(c),J($),e&&t(ne),e&&t(g),e&&t(oe),e&&t(H),e&&t(ie),J(P,e),e&&t(le),e&&t(U),e&&t(pe),e&&t(v),J(E),e&&t(fe),e&&t(u),e&&t(he),J(S,e),e&&t(me),e&&t(T),e&&t(ce),e&&t(y),J(A),e&&t(ke),e&&t(I),e&&t(de),J(q,e),e&&t(ge),e&&t(z),e&&t(ue),J(F,e),e&&t(ze),e&&t(j)}}}const At={local:"using-tokenizers-from-tokenizers",sections:[{local:"loading-directly-from-the-tokenizer-object",title:"Loading directly from the tokenizer object"},{local:"loading-from-a-json-file",title:"Loading from a JSON file"}],title:"Using tokenizers from \u{1F917} Tokenizers"};function qt(be,d,W){let{fw:c}=d;return be.$$set=k=>{"fw"in k&&W(0,c=k.fw)},[c]}class xt extends jt{constructor(d){super();$t(this,d,qt,St,bt,{fw:0})}}export{xt as default,At as metadata};
