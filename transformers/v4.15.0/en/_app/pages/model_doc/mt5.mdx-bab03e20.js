import{S as ap,i as ip,s as lp,e as n,k as d,w as f,t as a,L as dp,c as r,d as s,m as p,a as o,x as h,h as i,b as l,J as e,g as c,y as u,K as pp,q as g,o as _,B as v}from"../../chunks/vendor-b1433968.js";import{D as T}from"../../chunks/Docstring-ff504c58.js";import{C as Ce}from"../../chunks/CodeBlock-a320dbd7.js";import{I as P}from"../../chunks/IconCopyLink-7029626d.js";import"../../chunks/CopyButton-f65cb278.js";function mp(Gr){let D,Wt,C,A,ws,Ae,Or,ys,Vr,En,U,re,bs,Se,Ur,zs,Wr,Mn,oe,Br,Le,Hr,Rr,xn,Bt,Xr,qn,Ht,Es,Kr,jn,ae,Jr,De,Qr,Yr,Fn,Rt,Zr,Pn,y,Ms,xs,Ne,eo,to,qs,js,Ie,so,no,Fs,Ps,Ge,ro,oo,Cs,As,Oe,ao,io,Ss,Xt,Ve,lo,po,Cn,N,mo,Ue,co,fo,We,ho,uo,An,W,ie,Ls,Be,go,Ds,_o,Sn,S,He,vo,L,ko,Kt,To,$o,Jt,wo,yo,Re,bo,zo,Eo,B,Mo,Qt,xo,qo,Yt,jo,Fo,Ln,H,le,Ns,Xe,Po,Is,Co,Dn,k,Ke,Ao,Je,So,Qe,Lo,Do,No,Ye,Io,Zt,Go,Oo,Vo,R,Uo,Gs,Wo,Bo,Os,Ho,Ro,Xo,I,Ze,Ko,Vs,Jo,Qo,et,es,Yo,Us,Zo,ea,ts,ta,Ws,sa,na,de,tt,ra,Bs,oa,aa,pe,st,ia,Hs,la,da,me,nt,pa,rt,ma,Rs,ca,fa,Nn,ce,ha,ss,ua,ga,In,X,fe,Xs,ot,_a,Ks,va,Gn,w,at,ka,K,Ta,Js,$a,wa,it,ya,ba,za,lt,Ea,ns,Ma,xa,qa,G,dt,ja,Qs,Fa,Pa,pt,rs,Ca,Ys,Aa,Sa,os,La,Zs,Da,Na,he,mt,Ia,en,Ga,On,ue,Oa,as,Va,Ua,Vn,J,ge,tn,ct,Wa,sn,Ba,Un,b,ft,Ha,ht,Ra,is,Xa,Ka,Ja,nn,Qa,Ya,ut,Wn,Q,_e,rn,gt,Za,on,ei,Bn,z,_t,ti,vt,si,ls,ni,ri,oi,an,ai,ii,kt,Hn,Y,ve,ln,Tt,li,dn,di,Rn,E,$t,pi,wt,mi,ds,ci,fi,hi,pn,ui,gi,yt,Xn,Z,ke,mn,bt,_i,cn,vi,Kn,M,zt,ki,Et,Ti,ps,$i,wi,yi,fn,bi,zi,Mt,Jn,ee,Te,hn,xt,Ei,un,Mi,Qn,x,qt,xi,jt,qi,ms,ji,Fi,Pi,gn,Ci,Ai,Ft,Yn,te,$e,_n,Pt,Si,vn,Li,Zn,q,Ct,Di,At,Ni,cs,Ii,Gi,Oi,kn,Vi,Ui,St,er,se,we,Tn,Lt,Wi,$n,Bi,tr,j,Dt,Hi,Nt,Ri,fs,Xi,Ki,Ji,wn,Qi,Yi,It,sr,ne,ye,yn,Gt,Zi,bn,el,nr,F,Ot,tl,Vt,sl,hs,nl,rl,ol,zn,al,il,Ut,rr;return Ae=new P({}),Se=new P({}),Be=new P({}),He=new T({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/configuration_mt5.py#L24",parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.15.0/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/v4.15.0/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not
set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),Xe=new P({}),Ke=new T({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5.py#L53",parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for SentencePiece</a> can be used, among other things, to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"}]}}),Ze=new T({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5.py#L220",parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),tt=new T({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5.py#L281"}}),st=new T({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5.py#L198",parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),nt=new T({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5.py#L160",parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ot=new P({}),at=new T({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5_fast.py#L63",parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see <a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}]}}),dt=new T({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5_fast.py#L164",parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new T({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/t5/tokenization_t5_fast.py#L190",parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new P({}),ft=new T({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_mt5.py#L28"}}),ut=new Ce({props:{code:`from transformers import MT5Model, T5Tokenizer
model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),gt=new P({}),_t=new T({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_mt5.py#L61"}}),kt=new Ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs,labels=labels["input_ids"])
loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs,labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),Tt=new P({}),$t=new T({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_mt5.py#L92"}}),yt=new Ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer
model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),bt=new P({}),zt=new T({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),Mt=new Ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer
model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),xt=new P({}),qt=new T({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_tf_mt5.py#L52"}}),Ft=new Ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer
model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs,labels=labels["input_ids"])
loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs,labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),Pt=new P({}),Ct=new T({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_tf_mt5.py#L77"}}),St=new Ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer
model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),Lt=new P({}),Dt=new T({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_flax_mt5.py#L28"}}),It=new Ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),Gt=new P({}),Ot=new T({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/mt5/modeling_flax_mt5.py#L55"}}),Ut=new Ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){D=n("meta"),Wt=d(),C=n("h1"),A=n("a"),ws=n("span"),f(Ae.$$.fragment),Or=d(),ys=n("span"),Vr=a("mT5"),En=d(),U=n("h2"),re=n("a"),bs=n("span"),f(Se.$$.fragment),Ur=d(),zs=n("span"),Wr=a("Overview"),Mn=d(),oe=n("p"),Br=a("The mT5 model was presented in "),Le=n("a"),Hr=a("mT5: A massively multilingual pre-trained text-to-text transformer"),Rr=a(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),xn=d(),Bt=n("p"),Xr=a("The abstract from the paper is the following:"),qn=d(),Ht=n("p"),Es=n("em"),Kr=a(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),jn=d(),ae=n("p"),Jr=a("Note: mT5 was only pre-trained on "),De=n("a"),Qr=a("mC4"),Yr=a(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Fn=d(),Rt=n("p"),Zr=a("Google has released the following variants:"),Pn=d(),y=n("ul"),Ms=n("li"),xs=n("p"),Ne=n("a"),eo=a("google/mt5-small"),to=d(),qs=n("li"),js=n("p"),Ie=n("a"),so=a("google/mt5-base"),no=d(),Fs=n("li"),Ps=n("p"),Ge=n("a"),ro=a("google/mt5-large"),oo=d(),Cs=n("li"),As=n("p"),Oe=n("a"),ao=a("google/mt5-xl"),io=d(),Ss=n("li"),Xt=n("p"),Ve=n("a"),lo=a("google/mt5-xxl"),po=a("."),Cn=d(),N=n("p"),mo=a("This model was contributed by "),Ue=n("a"),co=a("patrickvonplaten"),fo=a(`. The original code can be
found `),We=n("a"),ho=a("here"),uo=a("."),An=d(),W=n("h2"),ie=n("a"),Ls=n("span"),f(Be.$$.fragment),go=d(),Ds=n("span"),_o=a("MT5Config"),Sn=d(),S=n("div"),f(He.$$.fragment),vo=d(),L=n("p"),ko=a("This is the configuration class to store the configuration of a "),Kt=n("a"),To=a("MT5Model"),$o=a(` or a
`),Jt=n("a"),wo=a("TFMT5Model"),yo=a(`. It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 `),Re=n("a"),bo=a("google/mt5-small"),zo=a(" architecture."),Eo=d(),B=n("p"),Mo=a("Configuration objects inherit from "),Qt=n("a"),xo=a("PretrainedConfig"),qo=a(` and can be used to control the model
outputs. Read the documentation from `),Yt=n("a"),jo=a("PretrainedConfig"),Fo=a(" for more information."),Ln=d(),H=n("h2"),le=n("a"),Ns=n("span"),f(Xe.$$.fragment),Po=d(),Is=n("span"),Co=a("MT5Tokenizer"),Dn=d(),k=n("div"),f(Ke.$$.fragment),Ao=d(),Je=n("p"),So=a("Construct a T5 tokenizer. Based on "),Qe=n("a"),Lo=a("SentencePiece"),Do=a("."),No=d(),Ye=n("p"),Io=a("This tokenizer inherits from "),Zt=n("a"),Go=a("PreTrainedTokenizer"),Oo=a(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Vo=d(),R=n("p"),Uo=a(`Attributes:
sp_model (`),Gs=n("code"),Wo=a("SentencePieceProcessor"),Bo=a(`):
The `),Os=n("em"),Ho=a("SentencePiece"),Ro=a(" processor that is used for every conversion (string, tokens and IDs)."),Xo=d(),I=n("div"),f(Ze.$$.fragment),Ko=d(),Vs=n("p"),Jo=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Qo=d(),et=n("ul"),es=n("li"),Yo=a("single sequence: "),Us=n("code"),Zo=a("X </s>"),ea=d(),ts=n("li"),ta=a("pair of sequences: "),Ws=n("code"),sa=a("A </s> B </s>"),na=d(),de=n("div"),f(tt.$$.fragment),ra=d(),Bs=n("p"),oa=a("Converts a sequence of tokens (string) in a single string."),aa=d(),pe=n("div"),f(st.$$.fragment),ia=d(),Hs=n("p"),la=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),da=d(),me=n("div"),f(nt.$$.fragment),pa=d(),rt=n("p"),ma=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Rs=n("code"),ca=a("prepare_for_model"),fa=a(" method."),Nn=d(),ce=n("p"),ha=a("See "),ss=n("a"),ua=a("T5Tokenizer"),ga=a(" for all details."),In=d(),X=n("h2"),fe=n("a"),Xs=n("span"),f(ot.$$.fragment),_a=d(),Ks=n("span"),va=a("MT5TokenizerFast"),Gn=d(),w=n("div"),f(at.$$.fragment),ka=d(),K=n("p"),Ta=a("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Js=n("em"),$a=a("tokenizers"),wa=a(" library). Based on "),it=n("a"),ya=a("Unigram"),ba=a("."),za=d(),lt=n("p"),Ea=a("This tokenizer inherits from "),ns=n("a"),Ma=a("PreTrainedTokenizerFast"),xa=a(` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),qa=d(),G=n("div"),f(dt.$$.fragment),ja=d(),Qs=n("p"),Fa=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Pa=d(),pt=n("ul"),rs=n("li"),Ca=a("single sequence: "),Ys=n("code"),Aa=a("X </s>"),Sa=d(),os=n("li"),La=a("pair of sequences: "),Zs=n("code"),Da=a("A </s> B </s>"),Na=d(),he=n("div"),f(mt.$$.fragment),Ia=d(),en=n("p"),Ga=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),On=d(),ue=n("p"),Oa=a("See "),as=n("a"),Va=a("T5TokenizerFast"),Ua=a(" for all details."),Vn=d(),J=n("h2"),ge=n("a"),tn=n("span"),f(ct.$$.fragment),Wa=d(),sn=n("span"),Ba=a("MT5Model"),Un=d(),b=n("div"),f(ft.$$.fragment),Ha=d(),ht=n("p"),Ra=a("This class overrides "),is=n("a"),Xa=a("T5Model"),Ka=a(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ja=d(),nn=n("p"),Qa=a("Examples:"),Ya=d(),f(ut.$$.fragment),Wn=d(),Q=n("h2"),_e=n("a"),rn=n("span"),f(gt.$$.fragment),Za=d(),on=n("span"),ei=a("MT5ForConditionalGeneration"),Bn=d(),z=n("div"),f(_t.$$.fragment),ti=d(),vt=n("p"),si=a("This class overrides "),ls=n("a"),ni=a("T5ForConditionalGeneration"),ri=a(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),oi=d(),an=n("p"),ai=a("Examples:"),ii=d(),f(kt.$$.fragment),Hn=d(),Y=n("h2"),ve=n("a"),ln=n("span"),f(Tt.$$.fragment),li=d(),dn=n("span"),di=a("MT5EncoderModel"),Rn=d(),E=n("div"),f($t.$$.fragment),pi=d(),wt=n("p"),mi=a("This class overrides "),ds=n("a"),ci=a("T5EncoderModel"),fi=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),hi=d(),pn=n("p"),ui=a("Examples:"),gi=d(),f(yt.$$.fragment),Xn=d(),Z=n("h2"),ke=n("a"),mn=n("span"),f(bt.$$.fragment),_i=d(),cn=n("span"),vi=a("TFMT5Model"),Kn=d(),M=n("div"),f(zt.$$.fragment),ki=d(),Et=n("p"),Ti=a("This class overrides "),ps=n("a"),$i=a("TFT5Model"),wi=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),yi=d(),fn=n("p"),bi=a("Examples:"),zi=d(),f(Mt.$$.fragment),Jn=d(),ee=n("h2"),Te=n("a"),hn=n("span"),f(xt.$$.fragment),Ei=d(),un=n("span"),Mi=a("TFMT5ForConditionalGeneration"),Qn=d(),x=n("div"),f(qt.$$.fragment),xi=d(),jt=n("p"),qi=a("This class overrides "),ms=n("a"),ji=a("TFT5ForConditionalGeneration"),Fi=a(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Pi=d(),gn=n("p"),Ci=a("Examples:"),Ai=d(),f(Ft.$$.fragment),Yn=d(),te=n("h2"),$e=n("a"),_n=n("span"),f(Pt.$$.fragment),Si=d(),vn=n("span"),Li=a("TFMT5EncoderModel"),Zn=d(),q=n("div"),f(Ct.$$.fragment),Di=d(),At=n("p"),Ni=a("This class overrides "),cs=n("a"),Ii=a("TFT5EncoderModel"),Gi=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Oi=d(),kn=n("p"),Vi=a("Examples:"),Ui=d(),f(St.$$.fragment),er=d(),se=n("h2"),we=n("a"),Tn=n("span"),f(Lt.$$.fragment),Wi=d(),$n=n("span"),Bi=a("FlaxMT5Model"),tr=d(),j=n("div"),f(Dt.$$.fragment),Hi=d(),Nt=n("p"),Ri=a("This class overrides "),fs=n("a"),Xi=a("FlaxT5Model"),Ki=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ji=d(),wn=n("p"),Qi=a("Examples:"),Yi=d(),f(It.$$.fragment),sr=d(),ne=n("h2"),ye=n("a"),yn=n("span"),f(Gt.$$.fragment),Zi=d(),bn=n("span"),el=a("FlaxMT5ForConditionalGeneration"),nr=d(),F=n("div"),f(Ot.$$.fragment),tl=d(),Vt=n("p"),sl=a("This class overrides "),hs=n("a"),nl=a("FlaxT5ForConditionalGeneration"),rl=a(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),ol=d(),zn=n("p"),al=a("Examples:"),il=d(),f(Ut.$$.fragment),this.h()},l(t){const m=dp('[data-svelte="svelte-1phssyn"]',document.head);D=r(m,"META",{name:!0,content:!0}),m.forEach(s),Wt=p(t),C=r(t,"H1",{class:!0});var or=o(C);A=r(or,"A",{id:!0,class:!0,href:!0});var fl=o(A);ws=r(fl,"SPAN",{});var hl=o(ws);h(Ae.$$.fragment,hl),hl.forEach(s),fl.forEach(s),Or=p(or),ys=r(or,"SPAN",{});var ul=o(ys);Vr=i(ul,"mT5"),ul.forEach(s),or.forEach(s),En=p(t),U=r(t,"H2",{class:!0});var ar=o(U);re=r(ar,"A",{id:!0,class:!0,href:!0});var gl=o(re);bs=r(gl,"SPAN",{});var _l=o(bs);h(Se.$$.fragment,_l),_l.forEach(s),gl.forEach(s),Ur=p(ar),zs=r(ar,"SPAN",{});var vl=o(zs);Wr=i(vl,"Overview"),vl.forEach(s),ar.forEach(s),Mn=p(t),oe=r(t,"P",{});var ir=o(oe);Br=i(ir,"The mT5 model was presented in "),Le=r(ir,"A",{href:!0,rel:!0});var kl=o(Le);Hr=i(kl,"mT5: A massively multilingual pre-trained text-to-text transformer"),kl.forEach(s),Rr=i(ir,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),ir.forEach(s),xn=p(t),Bt=r(t,"P",{});var Tl=o(Bt);Xr=i(Tl,"The abstract from the paper is the following:"),Tl.forEach(s),qn=p(t),Ht=r(t,"P",{});var $l=o(Ht);Es=r($l,"EM",{});var wl=o(Es);Kr=i(wl,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),wl.forEach(s),$l.forEach(s),jn=p(t),ae=r(t,"P",{});var lr=o(ae);Jr=i(lr,"Note: mT5 was only pre-trained on "),De=r(lr,"A",{href:!0,rel:!0});var yl=o(De);Qr=i(yl,"mC4"),yl.forEach(s),Yr=i(lr,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),lr.forEach(s),Fn=p(t),Rt=r(t,"P",{});var bl=o(Rt);Zr=i(bl,"Google has released the following variants:"),bl.forEach(s),Pn=p(t),y=r(t,"UL",{});var O=o(y);Ms=r(O,"LI",{});var zl=o(Ms);xs=r(zl,"P",{});var El=o(xs);Ne=r(El,"A",{href:!0,rel:!0});var Ml=o(Ne);eo=i(Ml,"google/mt5-small"),Ml.forEach(s),El.forEach(s),zl.forEach(s),to=p(O),qs=r(O,"LI",{});var xl=o(qs);js=r(xl,"P",{});var ql=o(js);Ie=r(ql,"A",{href:!0,rel:!0});var jl=o(Ie);so=i(jl,"google/mt5-base"),jl.forEach(s),ql.forEach(s),xl.forEach(s),no=p(O),Fs=r(O,"LI",{});var Fl=o(Fs);Ps=r(Fl,"P",{});var Pl=o(Ps);Ge=r(Pl,"A",{href:!0,rel:!0});var Cl=o(Ge);ro=i(Cl,"google/mt5-large"),Cl.forEach(s),Pl.forEach(s),Fl.forEach(s),oo=p(O),Cs=r(O,"LI",{});var Al=o(Cs);As=r(Al,"P",{});var Sl=o(As);Oe=r(Sl,"A",{href:!0,rel:!0});var Ll=o(Oe);ao=i(Ll,"google/mt5-xl"),Ll.forEach(s),Sl.forEach(s),Al.forEach(s),io=p(O),Ss=r(O,"LI",{});var Dl=o(Ss);Xt=r(Dl,"P",{});var ll=o(Xt);Ve=r(ll,"A",{href:!0,rel:!0});var Nl=o(Ve);lo=i(Nl,"google/mt5-xxl"),Nl.forEach(s),po=i(ll,"."),ll.forEach(s),Dl.forEach(s),O.forEach(s),Cn=p(t),N=r(t,"P",{});var us=o(N);mo=i(us,"This model was contributed by "),Ue=r(us,"A",{href:!0,rel:!0});var Il=o(Ue);co=i(Il,"patrickvonplaten"),Il.forEach(s),fo=i(us,`. The original code can be
found `),We=r(us,"A",{href:!0,rel:!0});var Gl=o(We);ho=i(Gl,"here"),Gl.forEach(s),uo=i(us,"."),us.forEach(s),An=p(t),W=r(t,"H2",{class:!0});var dr=o(W);ie=r(dr,"A",{id:!0,class:!0,href:!0});var Ol=o(ie);Ls=r(Ol,"SPAN",{});var Vl=o(Ls);h(Be.$$.fragment,Vl),Vl.forEach(s),Ol.forEach(s),go=p(dr),Ds=r(dr,"SPAN",{});var Ul=o(Ds);_o=i(Ul,"MT5Config"),Ul.forEach(s),dr.forEach(s),Sn=p(t),S=r(t,"DIV",{class:!0});var gs=o(S);h(He.$$.fragment,gs),vo=p(gs),L=r(gs,"P",{});var be=o(L);ko=i(be,"This is the configuration class to store the configuration of a "),Kt=r(be,"A",{href:!0});var Wl=o(Kt);To=i(Wl,"MT5Model"),Wl.forEach(s),$o=i(be,` or a
`),Jt=r(be,"A",{href:!0});var Bl=o(Jt);wo=i(Bl,"TFMT5Model"),Bl.forEach(s),yo=i(be,`. It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 `),Re=r(be,"A",{href:!0,rel:!0});var Hl=o(Re);bo=i(Hl,"google/mt5-small"),Hl.forEach(s),zo=i(be," architecture."),be.forEach(s),Eo=p(gs),B=r(gs,"P",{});var _s=o(B);Mo=i(_s,"Configuration objects inherit from "),Qt=r(_s,"A",{href:!0});var Rl=o(Qt);xo=i(Rl,"PretrainedConfig"),Rl.forEach(s),qo=i(_s,` and can be used to control the model
outputs. Read the documentation from `),Yt=r(_s,"A",{href:!0});var Xl=o(Yt);jo=i(Xl,"PretrainedConfig"),Xl.forEach(s),Fo=i(_s," for more information."),_s.forEach(s),gs.forEach(s),Ln=p(t),H=r(t,"H2",{class:!0});var pr=o(H);le=r(pr,"A",{id:!0,class:!0,href:!0});var Kl=o(le);Ns=r(Kl,"SPAN",{});var Jl=o(Ns);h(Xe.$$.fragment,Jl),Jl.forEach(s),Kl.forEach(s),Po=p(pr),Is=r(pr,"SPAN",{});var Ql=o(Is);Co=i(Ql,"MT5Tokenizer"),Ql.forEach(s),pr.forEach(s),Dn=p(t),k=r(t,"DIV",{class:!0});var $=o(k);h(Ke.$$.fragment,$),Ao=p($),Je=r($,"P",{});var mr=o(Je);So=i(mr,"Construct a T5 tokenizer. Based on "),Qe=r(mr,"A",{href:!0,rel:!0});var Yl=o(Qe);Lo=i(Yl,"SentencePiece"),Yl.forEach(s),Do=i(mr,"."),mr.forEach(s),No=p($),Ye=r($,"P",{});var cr=o(Ye);Io=i(cr,"This tokenizer inherits from "),Zt=r(cr,"A",{href:!0});var Zl=o(Zt);Go=i(Zl,"PreTrainedTokenizer"),Zl.forEach(s),Oo=i(cr,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),cr.forEach(s),Vo=p($),R=r($,"P",{});var vs=o(R);Uo=i(vs,`Attributes:
sp_model (`),Gs=r(vs,"CODE",{});var ed=o(Gs);Wo=i(ed,"SentencePieceProcessor"),ed.forEach(s),Bo=i(vs,`):
The `),Os=r(vs,"EM",{});var td=o(Os);Ho=i(td,"SentencePiece"),td.forEach(s),Ro=i(vs," processor that is used for every conversion (string, tokens and IDs)."),vs.forEach(s),Xo=p($),I=r($,"DIV",{class:!0});var ks=o(I);h(Ze.$$.fragment,ks),Ko=p(ks),Vs=r(ks,"P",{});var sd=o(Vs);Jo=i(sd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),sd.forEach(s),Qo=p(ks),et=r(ks,"UL",{});var fr=o(et);es=r(fr,"LI",{});var dl=o(es);Yo=i(dl,"single sequence: "),Us=r(dl,"CODE",{});var nd=o(Us);Zo=i(nd,"X </s>"),nd.forEach(s),dl.forEach(s),ea=p(fr),ts=r(fr,"LI",{});var pl=o(ts);ta=i(pl,"pair of sequences: "),Ws=r(pl,"CODE",{});var rd=o(Ws);sa=i(rd,"A </s> B </s>"),rd.forEach(s),pl.forEach(s),fr.forEach(s),ks.forEach(s),na=p($),de=r($,"DIV",{class:!0});var hr=o(de);h(tt.$$.fragment,hr),ra=p(hr),Bs=r(hr,"P",{});var od=o(Bs);oa=i(od,"Converts a sequence of tokens (string) in a single string."),od.forEach(s),hr.forEach(s),aa=p($),pe=r($,"DIV",{class:!0});var ur=o(pe);h(st.$$.fragment,ur),ia=p(ur),Hs=r(ur,"P",{});var ad=o(Hs);la=i(ad,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ad.forEach(s),ur.forEach(s),da=p($),me=r($,"DIV",{class:!0});var gr=o(me);h(nt.$$.fragment,gr),pa=p(gr),rt=r(gr,"P",{});var _r=o(rt);ma=i(_r,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Rs=r(_r,"CODE",{});var id=o(Rs);ca=i(id,"prepare_for_model"),id.forEach(s),fa=i(_r," method."),_r.forEach(s),gr.forEach(s),$.forEach(s),Nn=p(t),ce=r(t,"P",{});var vr=o(ce);ha=i(vr,"See "),ss=r(vr,"A",{href:!0});var ld=o(ss);ua=i(ld,"T5Tokenizer"),ld.forEach(s),ga=i(vr," for all details."),vr.forEach(s),In=p(t),X=r(t,"H2",{class:!0});var kr=o(X);fe=r(kr,"A",{id:!0,class:!0,href:!0});var dd=o(fe);Xs=r(dd,"SPAN",{});var pd=o(Xs);h(ot.$$.fragment,pd),pd.forEach(s),dd.forEach(s),_a=p(kr),Ks=r(kr,"SPAN",{});var md=o(Ks);va=i(md,"MT5TokenizerFast"),md.forEach(s),kr.forEach(s),Gn=p(t),w=r(t,"DIV",{class:!0});var V=o(w);h(at.$$.fragment,V),ka=p(V),K=r(V,"P",{});var Ts=o(K);Ta=i(Ts,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Js=r(Ts,"EM",{});var cd=o(Js);$a=i(cd,"tokenizers"),cd.forEach(s),wa=i(Ts," library). Based on "),it=r(Ts,"A",{href:!0,rel:!0});var fd=o(it);ya=i(fd,"Unigram"),fd.forEach(s),ba=i(Ts,"."),Ts.forEach(s),za=p(V),lt=r(V,"P",{});var Tr=o(lt);Ea=i(Tr,"This tokenizer inherits from "),ns=r(Tr,"A",{href:!0});var hd=o(ns);Ma=i(hd,"PreTrainedTokenizerFast"),hd.forEach(s),xa=i(Tr,` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),Tr.forEach(s),qa=p(V),G=r(V,"DIV",{class:!0});var $s=o(G);h(dt.$$.fragment,$s),ja=p($s),Qs=r($s,"P",{});var ud=o(Qs);Fa=i(ud,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),ud.forEach(s),Pa=p($s),pt=r($s,"UL",{});var $r=o(pt);rs=r($r,"LI",{});var ml=o(rs);Ca=i(ml,"single sequence: "),Ys=r(ml,"CODE",{});var gd=o(Ys);Aa=i(gd,"X </s>"),gd.forEach(s),ml.forEach(s),Sa=p($r),os=r($r,"LI",{});var cl=o(os);La=i(cl,"pair of sequences: "),Zs=r(cl,"CODE",{});var _d=o(Zs);Da=i(_d,"A </s> B </s>"),_d.forEach(s),cl.forEach(s),$r.forEach(s),$s.forEach(s),Na=p(V),he=r(V,"DIV",{class:!0});var wr=o(he);h(mt.$$.fragment,wr),Ia=p(wr),en=r(wr,"P",{});var vd=o(en);Ga=i(vd,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),vd.forEach(s),wr.forEach(s),V.forEach(s),On=p(t),ue=r(t,"P",{});var yr=o(ue);Oa=i(yr,"See "),as=r(yr,"A",{href:!0});var kd=o(as);Va=i(kd,"T5TokenizerFast"),kd.forEach(s),Ua=i(yr," for all details."),yr.forEach(s),Vn=p(t),J=r(t,"H2",{class:!0});var br=o(J);ge=r(br,"A",{id:!0,class:!0,href:!0});var Td=o(ge);tn=r(Td,"SPAN",{});var $d=o(tn);h(ct.$$.fragment,$d),$d.forEach(s),Td.forEach(s),Wa=p(br),sn=r(br,"SPAN",{});var wd=o(sn);Ba=i(wd,"MT5Model"),wd.forEach(s),br.forEach(s),Un=p(t),b=r(t,"DIV",{class:!0});var ze=o(b);h(ft.$$.fragment,ze),Ha=p(ze),ht=r(ze,"P",{});var zr=o(ht);Ra=i(zr,"This class overrides "),is=r(zr,"A",{href:!0});var yd=o(is);Xa=i(yd,"T5Model"),yd.forEach(s),Ka=i(zr,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),zr.forEach(s),Ja=p(ze),nn=r(ze,"P",{});var bd=o(nn);Qa=i(bd,"Examples:"),bd.forEach(s),Ya=p(ze),h(ut.$$.fragment,ze),ze.forEach(s),Wn=p(t),Q=r(t,"H2",{class:!0});var Er=o(Q);_e=r(Er,"A",{id:!0,class:!0,href:!0});var zd=o(_e);rn=r(zd,"SPAN",{});var Ed=o(rn);h(gt.$$.fragment,Ed),Ed.forEach(s),zd.forEach(s),Za=p(Er),on=r(Er,"SPAN",{});var Md=o(on);ei=i(Md,"MT5ForConditionalGeneration"),Md.forEach(s),Er.forEach(s),Bn=p(t),z=r(t,"DIV",{class:!0});var Ee=o(z);h(_t.$$.fragment,Ee),ti=p(Ee),vt=r(Ee,"P",{});var Mr=o(vt);si=i(Mr,"This class overrides "),ls=r(Mr,"A",{href:!0});var xd=o(ls);ni=i(xd,"T5ForConditionalGeneration"),xd.forEach(s),ri=i(Mr,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Mr.forEach(s),oi=p(Ee),an=r(Ee,"P",{});var qd=o(an);ai=i(qd,"Examples:"),qd.forEach(s),ii=p(Ee),h(kt.$$.fragment,Ee),Ee.forEach(s),Hn=p(t),Y=r(t,"H2",{class:!0});var xr=o(Y);ve=r(xr,"A",{id:!0,class:!0,href:!0});var jd=o(ve);ln=r(jd,"SPAN",{});var Fd=o(ln);h(Tt.$$.fragment,Fd),Fd.forEach(s),jd.forEach(s),li=p(xr),dn=r(xr,"SPAN",{});var Pd=o(dn);di=i(Pd,"MT5EncoderModel"),Pd.forEach(s),xr.forEach(s),Rn=p(t),E=r(t,"DIV",{class:!0});var Me=o(E);h($t.$$.fragment,Me),pi=p(Me),wt=r(Me,"P",{});var qr=o(wt);mi=i(qr,"This class overrides "),ds=r(qr,"A",{href:!0});var Cd=o(ds);ci=i(Cd,"T5EncoderModel"),Cd.forEach(s),fi=i(qr,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),qr.forEach(s),hi=p(Me),pn=r(Me,"P",{});var Ad=o(pn);ui=i(Ad,"Examples:"),Ad.forEach(s),gi=p(Me),h(yt.$$.fragment,Me),Me.forEach(s),Xn=p(t),Z=r(t,"H2",{class:!0});var jr=o(Z);ke=r(jr,"A",{id:!0,class:!0,href:!0});var Sd=o(ke);mn=r(Sd,"SPAN",{});var Ld=o(mn);h(bt.$$.fragment,Ld),Ld.forEach(s),Sd.forEach(s),_i=p(jr),cn=r(jr,"SPAN",{});var Dd=o(cn);vi=i(Dd,"TFMT5Model"),Dd.forEach(s),jr.forEach(s),Kn=p(t),M=r(t,"DIV",{class:!0});var xe=o(M);h(zt.$$.fragment,xe),ki=p(xe),Et=r(xe,"P",{});var Fr=o(Et);Ti=i(Fr,"This class overrides "),ps=r(Fr,"A",{href:!0});var Nd=o(ps);$i=i(Nd,"TFT5Model"),Nd.forEach(s),wi=i(Fr,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Fr.forEach(s),yi=p(xe),fn=r(xe,"P",{});var Id=o(fn);bi=i(Id,"Examples:"),Id.forEach(s),zi=p(xe),h(Mt.$$.fragment,xe),xe.forEach(s),Jn=p(t),ee=r(t,"H2",{class:!0});var Pr=o(ee);Te=r(Pr,"A",{id:!0,class:!0,href:!0});var Gd=o(Te);hn=r(Gd,"SPAN",{});var Od=o(hn);h(xt.$$.fragment,Od),Od.forEach(s),Gd.forEach(s),Ei=p(Pr),un=r(Pr,"SPAN",{});var Vd=o(un);Mi=i(Vd,"TFMT5ForConditionalGeneration"),Vd.forEach(s),Pr.forEach(s),Qn=p(t),x=r(t,"DIV",{class:!0});var qe=o(x);h(qt.$$.fragment,qe),xi=p(qe),jt=r(qe,"P",{});var Cr=o(jt);qi=i(Cr,"This class overrides "),ms=r(Cr,"A",{href:!0});var Ud=o(ms);ji=i(Ud,"TFT5ForConditionalGeneration"),Ud.forEach(s),Fi=i(Cr,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Cr.forEach(s),Pi=p(qe),gn=r(qe,"P",{});var Wd=o(gn);Ci=i(Wd,"Examples:"),Wd.forEach(s),Ai=p(qe),h(Ft.$$.fragment,qe),qe.forEach(s),Yn=p(t),te=r(t,"H2",{class:!0});var Ar=o(te);$e=r(Ar,"A",{id:!0,class:!0,href:!0});var Bd=o($e);_n=r(Bd,"SPAN",{});var Hd=o(_n);h(Pt.$$.fragment,Hd),Hd.forEach(s),Bd.forEach(s),Si=p(Ar),vn=r(Ar,"SPAN",{});var Rd=o(vn);Li=i(Rd,"TFMT5EncoderModel"),Rd.forEach(s),Ar.forEach(s),Zn=p(t),q=r(t,"DIV",{class:!0});var je=o(q);h(Ct.$$.fragment,je),Di=p(je),At=r(je,"P",{});var Sr=o(At);Ni=i(Sr,"This class overrides "),cs=r(Sr,"A",{href:!0});var Xd=o(cs);Ii=i(Xd,"TFT5EncoderModel"),Xd.forEach(s),Gi=i(Sr,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Sr.forEach(s),Oi=p(je),kn=r(je,"P",{});var Kd=o(kn);Vi=i(Kd,"Examples:"),Kd.forEach(s),Ui=p(je),h(St.$$.fragment,je),je.forEach(s),er=p(t),se=r(t,"H2",{class:!0});var Lr=o(se);we=r(Lr,"A",{id:!0,class:!0,href:!0});var Jd=o(we);Tn=r(Jd,"SPAN",{});var Qd=o(Tn);h(Lt.$$.fragment,Qd),Qd.forEach(s),Jd.forEach(s),Wi=p(Lr),$n=r(Lr,"SPAN",{});var Yd=o($n);Bi=i(Yd,"FlaxMT5Model"),Yd.forEach(s),Lr.forEach(s),tr=p(t),j=r(t,"DIV",{class:!0});var Fe=o(j);h(Dt.$$.fragment,Fe),Hi=p(Fe),Nt=r(Fe,"P",{});var Dr=o(Nt);Ri=i(Dr,"This class overrides "),fs=r(Dr,"A",{href:!0});var Zd=o(fs);Xi=i(Zd,"FlaxT5Model"),Zd.forEach(s),Ki=i(Dr,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Dr.forEach(s),Ji=p(Fe),wn=r(Fe,"P",{});var ep=o(wn);Qi=i(ep,"Examples:"),ep.forEach(s),Yi=p(Fe),h(It.$$.fragment,Fe),Fe.forEach(s),sr=p(t),ne=r(t,"H2",{class:!0});var Nr=o(ne);ye=r(Nr,"A",{id:!0,class:!0,href:!0});var tp=o(ye);yn=r(tp,"SPAN",{});var sp=o(yn);h(Gt.$$.fragment,sp),sp.forEach(s),tp.forEach(s),Zi=p(Nr),bn=r(Nr,"SPAN",{});var np=o(bn);el=i(np,"FlaxMT5ForConditionalGeneration"),np.forEach(s),Nr.forEach(s),nr=p(t),F=r(t,"DIV",{class:!0});var Pe=o(F);h(Ot.$$.fragment,Pe),tl=p(Pe),Vt=r(Pe,"P",{});var Ir=o(Vt);sl=i(Ir,"This class overrides "),hs=r(Ir,"A",{href:!0});var rp=o(hs);nl=i(rp,"FlaxT5ForConditionalGeneration"),rp.forEach(s),rl=i(Ir,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Ir.forEach(s),ol=p(Pe),zn=r(Pe,"P",{});var op=o(zn);al=i(op,"Examples:"),op.forEach(s),il=p(Pe),h(Ut.$$.fragment,Pe),Pe.forEach(s),this.h()},h(){l(D,"name","hf:doc:metadata"),l(D,"content",JSON.stringify(cp)),l(A,"id","mt5"),l(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(A,"href","#mt5"),l(C,"class","relative group"),l(re,"id","overview"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#overview"),l(U,"class","relative group"),l(Le,"href","https://arxiv.org/abs/2010.11934"),l(Le,"rel","nofollow"),l(De,"href","https://huggingface.co/datasets/mc4"),l(De,"rel","nofollow"),l(Ne,"href","https://huggingface.co/google/mt5-small"),l(Ne,"rel","nofollow"),l(Ie,"href","https://huggingface.co/google/mt5-base"),l(Ie,"rel","nofollow"),l(Ge,"href","https://huggingface.co/google/mt5-large"),l(Ge,"rel","nofollow"),l(Oe,"href","https://huggingface.co/google/mt5-xl"),l(Oe,"rel","nofollow"),l(Ve,"href","https://huggingface.co/google/mt5-xxl"),l(Ve,"rel","nofollow"),l(Ue,"href","https://huggingface.co/patrickvonplaten"),l(Ue,"rel","nofollow"),l(We,"href","https://github.com/google-research/multilingual-t5"),l(We,"rel","nofollow"),l(ie,"id","transformers.MT5Config"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.MT5Config"),l(W,"class","relative group"),l(Kt,"href","/docs/transformers/v4.15.0/en/model_doc/mt5#transformers.MT5Model"),l(Jt,"href","/docs/transformers/v4.15.0/en/model_doc/mt5#transformers.TFMT5Model"),l(Re,"href","https://huggingface.co/google/mt5-small"),l(Re,"rel","nofollow"),l(Qt,"href","/docs/transformers/v4.15.0/en/main_classes/configuration#transformers.PretrainedConfig"),l(Yt,"href","/docs/transformers/v4.15.0/en/main_classes/configuration#transformers.PretrainedConfig"),l(S,"class","docstring"),l(le,"id","transformers.T5Tokenizer"),l(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(le,"href","#transformers.T5Tokenizer"),l(H,"class","relative group"),l(Qe,"href","https://github.com/google/sentencepiece"),l(Qe,"rel","nofollow"),l(Zt,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(I,"class","docstring"),l(de,"class","docstring"),l(pe,"class","docstring"),l(me,"class","docstring"),l(k,"class","docstring"),l(ss,"href","/docs/transformers/v4.15.0/en/model_doc/mt5#transformers.T5Tokenizer"),l(fe,"id","transformers.T5TokenizerFast"),l(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(fe,"href","#transformers.T5TokenizerFast"),l(X,"class","relative group"),l(it,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),l(it,"rel","nofollow"),l(ns,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),l(G,"class","docstring"),l(he,"class","docstring"),l(w,"class","docstring"),l(as,"href","/docs/transformers/v4.15.0/en/model_doc/mt5#transformers.T5TokenizerFast"),l(ge,"id","transformers.MT5Model"),l(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ge,"href","#transformers.MT5Model"),l(J,"class","relative group"),l(is,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.T5Model"),l(b,"class","docstring"),l(_e,"id","transformers.MT5ForConditionalGeneration"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.MT5ForConditionalGeneration"),l(Q,"class","relative group"),l(ls,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.T5ForConditionalGeneration"),l(z,"class","docstring"),l(ve,"id","transformers.MT5EncoderModel"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#transformers.MT5EncoderModel"),l(Y,"class","relative group"),l(ds,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.T5EncoderModel"),l(E,"class","docstring"),l(ke,"id","transformers.TFMT5Model"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.TFMT5Model"),l(Z,"class","relative group"),l(ps,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.TFT5Model"),l(M,"class","docstring"),l(Te,"id","transformers.TFMT5ForConditionalGeneration"),l(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Te,"href","#transformers.TFMT5ForConditionalGeneration"),l(ee,"class","relative group"),l(ms,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),l(x,"class","docstring"),l($e,"id","transformers.TFMT5EncoderModel"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#transformers.TFMT5EncoderModel"),l(te,"class","relative group"),l(cs,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.TFT5EncoderModel"),l(q,"class","docstring"),l(we,"id","transformers.FlaxMT5Model"),l(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(we,"href","#transformers.FlaxMT5Model"),l(se,"class","relative group"),l(fs,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.FlaxT5Model"),l(j,"class","docstring"),l(ye,"id","transformers.FlaxMT5ForConditionalGeneration"),l(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ye,"href","#transformers.FlaxMT5ForConditionalGeneration"),l(ne,"class","relative group"),l(hs,"href","/docs/transformers/v4.15.0/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),l(F,"class","docstring")},m(t,m){e(document.head,D),c(t,Wt,m),c(t,C,m),e(C,A),e(A,ws),u(Ae,ws,null),e(C,Or),e(C,ys),e(ys,Vr),c(t,En,m),c(t,U,m),e(U,re),e(re,bs),u(Se,bs,null),e(U,Ur),e(U,zs),e(zs,Wr),c(t,Mn,m),c(t,oe,m),e(oe,Br),e(oe,Le),e(Le,Hr),e(oe,Rr),c(t,xn,m),c(t,Bt,m),e(Bt,Xr),c(t,qn,m),c(t,Ht,m),e(Ht,Es),e(Es,Kr),c(t,jn,m),c(t,ae,m),e(ae,Jr),e(ae,De),e(De,Qr),e(ae,Yr),c(t,Fn,m),c(t,Rt,m),e(Rt,Zr),c(t,Pn,m),c(t,y,m),e(y,Ms),e(Ms,xs),e(xs,Ne),e(Ne,eo),e(y,to),e(y,qs),e(qs,js),e(js,Ie),e(Ie,so),e(y,no),e(y,Fs),e(Fs,Ps),e(Ps,Ge),e(Ge,ro),e(y,oo),e(y,Cs),e(Cs,As),e(As,Oe),e(Oe,ao),e(y,io),e(y,Ss),e(Ss,Xt),e(Xt,Ve),e(Ve,lo),e(Xt,po),c(t,Cn,m),c(t,N,m),e(N,mo),e(N,Ue),e(Ue,co),e(N,fo),e(N,We),e(We,ho),e(N,uo),c(t,An,m),c(t,W,m),e(W,ie),e(ie,Ls),u(Be,Ls,null),e(W,go),e(W,Ds),e(Ds,_o),c(t,Sn,m),c(t,S,m),u(He,S,null),e(S,vo),e(S,L),e(L,ko),e(L,Kt),e(Kt,To),e(L,$o),e(L,Jt),e(Jt,wo),e(L,yo),e(L,Re),e(Re,bo),e(L,zo),e(S,Eo),e(S,B),e(B,Mo),e(B,Qt),e(Qt,xo),e(B,qo),e(B,Yt),e(Yt,jo),e(B,Fo),c(t,Ln,m),c(t,H,m),e(H,le),e(le,Ns),u(Xe,Ns,null),e(H,Po),e(H,Is),e(Is,Co),c(t,Dn,m),c(t,k,m),u(Ke,k,null),e(k,Ao),e(k,Je),e(Je,So),e(Je,Qe),e(Qe,Lo),e(Je,Do),e(k,No),e(k,Ye),e(Ye,Io),e(Ye,Zt),e(Zt,Go),e(Ye,Oo),e(k,Vo),e(k,R),e(R,Uo),e(R,Gs),e(Gs,Wo),e(R,Bo),e(R,Os),e(Os,Ho),e(R,Ro),e(k,Xo),e(k,I),u(Ze,I,null),e(I,Ko),e(I,Vs),e(Vs,Jo),e(I,Qo),e(I,et),e(et,es),e(es,Yo),e(es,Us),e(Us,Zo),e(et,ea),e(et,ts),e(ts,ta),e(ts,Ws),e(Ws,sa),e(k,na),e(k,de),u(tt,de,null),e(de,ra),e(de,Bs),e(Bs,oa),e(k,aa),e(k,pe),u(st,pe,null),e(pe,ia),e(pe,Hs),e(Hs,la),e(k,da),e(k,me),u(nt,me,null),e(me,pa),e(me,rt),e(rt,ma),e(rt,Rs),e(Rs,ca),e(rt,fa),c(t,Nn,m),c(t,ce,m),e(ce,ha),e(ce,ss),e(ss,ua),e(ce,ga),c(t,In,m),c(t,X,m),e(X,fe),e(fe,Xs),u(ot,Xs,null),e(X,_a),e(X,Ks),e(Ks,va),c(t,Gn,m),c(t,w,m),u(at,w,null),e(w,ka),e(w,K),e(K,Ta),e(K,Js),e(Js,$a),e(K,wa),e(K,it),e(it,ya),e(K,ba),e(w,za),e(w,lt),e(lt,Ea),e(lt,ns),e(ns,Ma),e(lt,xa),e(w,qa),e(w,G),u(dt,G,null),e(G,ja),e(G,Qs),e(Qs,Fa),e(G,Pa),e(G,pt),e(pt,rs),e(rs,Ca),e(rs,Ys),e(Ys,Aa),e(pt,Sa),e(pt,os),e(os,La),e(os,Zs),e(Zs,Da),e(w,Na),e(w,he),u(mt,he,null),e(he,Ia),e(he,en),e(en,Ga),c(t,On,m),c(t,ue,m),e(ue,Oa),e(ue,as),e(as,Va),e(ue,Ua),c(t,Vn,m),c(t,J,m),e(J,ge),e(ge,tn),u(ct,tn,null),e(J,Wa),e(J,sn),e(sn,Ba),c(t,Un,m),c(t,b,m),u(ft,b,null),e(b,Ha),e(b,ht),e(ht,Ra),e(ht,is),e(is,Xa),e(ht,Ka),e(b,Ja),e(b,nn),e(nn,Qa),e(b,Ya),u(ut,b,null),c(t,Wn,m),c(t,Q,m),e(Q,_e),e(_e,rn),u(gt,rn,null),e(Q,Za),e(Q,on),e(on,ei),c(t,Bn,m),c(t,z,m),u(_t,z,null),e(z,ti),e(z,vt),e(vt,si),e(vt,ls),e(ls,ni),e(vt,ri),e(z,oi),e(z,an),e(an,ai),e(z,ii),u(kt,z,null),c(t,Hn,m),c(t,Y,m),e(Y,ve),e(ve,ln),u(Tt,ln,null),e(Y,li),e(Y,dn),e(dn,di),c(t,Rn,m),c(t,E,m),u($t,E,null),e(E,pi),e(E,wt),e(wt,mi),e(wt,ds),e(ds,ci),e(wt,fi),e(E,hi),e(E,pn),e(pn,ui),e(E,gi),u(yt,E,null),c(t,Xn,m),c(t,Z,m),e(Z,ke),e(ke,mn),u(bt,mn,null),e(Z,_i),e(Z,cn),e(cn,vi),c(t,Kn,m),c(t,M,m),u(zt,M,null),e(M,ki),e(M,Et),e(Et,Ti),e(Et,ps),e(ps,$i),e(Et,wi),e(M,yi),e(M,fn),e(fn,bi),e(M,zi),u(Mt,M,null),c(t,Jn,m),c(t,ee,m),e(ee,Te),e(Te,hn),u(xt,hn,null),e(ee,Ei),e(ee,un),e(un,Mi),c(t,Qn,m),c(t,x,m),u(qt,x,null),e(x,xi),e(x,jt),e(jt,qi),e(jt,ms),e(ms,ji),e(jt,Fi),e(x,Pi),e(x,gn),e(gn,Ci),e(x,Ai),u(Ft,x,null),c(t,Yn,m),c(t,te,m),e(te,$e),e($e,_n),u(Pt,_n,null),e(te,Si),e(te,vn),e(vn,Li),c(t,Zn,m),c(t,q,m),u(Ct,q,null),e(q,Di),e(q,At),e(At,Ni),e(At,cs),e(cs,Ii),e(At,Gi),e(q,Oi),e(q,kn),e(kn,Vi),e(q,Ui),u(St,q,null),c(t,er,m),c(t,se,m),e(se,we),e(we,Tn),u(Lt,Tn,null),e(se,Wi),e(se,$n),e($n,Bi),c(t,tr,m),c(t,j,m),u(Dt,j,null),e(j,Hi),e(j,Nt),e(Nt,Ri),e(Nt,fs),e(fs,Xi),e(Nt,Ki),e(j,Ji),e(j,wn),e(wn,Qi),e(j,Yi),u(It,j,null),c(t,sr,m),c(t,ne,m),e(ne,ye),e(ye,yn),u(Gt,yn,null),e(ne,Zi),e(ne,bn),e(bn,el),c(t,nr,m),c(t,F,m),u(Ot,F,null),e(F,tl),e(F,Vt),e(Vt,sl),e(Vt,hs),e(hs,nl),e(Vt,rl),e(F,ol),e(F,zn),e(zn,al),e(F,il),u(Ut,F,null),rr=!0},p:pp,i(t){rr||(g(Ae.$$.fragment,t),g(Se.$$.fragment,t),g(Be.$$.fragment,t),g(He.$$.fragment,t),g(Xe.$$.fragment,t),g(Ke.$$.fragment,t),g(Ze.$$.fragment,t),g(tt.$$.fragment,t),g(st.$$.fragment,t),g(nt.$$.fragment,t),g(ot.$$.fragment,t),g(at.$$.fragment,t),g(dt.$$.fragment,t),g(mt.$$.fragment,t),g(ct.$$.fragment,t),g(ft.$$.fragment,t),g(ut.$$.fragment,t),g(gt.$$.fragment,t),g(_t.$$.fragment,t),g(kt.$$.fragment,t),g(Tt.$$.fragment,t),g($t.$$.fragment,t),g(yt.$$.fragment,t),g(bt.$$.fragment,t),g(zt.$$.fragment,t),g(Mt.$$.fragment,t),g(xt.$$.fragment,t),g(qt.$$.fragment,t),g(Ft.$$.fragment,t),g(Pt.$$.fragment,t),g(Ct.$$.fragment,t),g(St.$$.fragment,t),g(Lt.$$.fragment,t),g(Dt.$$.fragment,t),g(It.$$.fragment,t),g(Gt.$$.fragment,t),g(Ot.$$.fragment,t),g(Ut.$$.fragment,t),rr=!0)},o(t){_(Ae.$$.fragment,t),_(Se.$$.fragment,t),_(Be.$$.fragment,t),_(He.$$.fragment,t),_(Xe.$$.fragment,t),_(Ke.$$.fragment,t),_(Ze.$$.fragment,t),_(tt.$$.fragment,t),_(st.$$.fragment,t),_(nt.$$.fragment,t),_(ot.$$.fragment,t),_(at.$$.fragment,t),_(dt.$$.fragment,t),_(mt.$$.fragment,t),_(ct.$$.fragment,t),_(ft.$$.fragment,t),_(ut.$$.fragment,t),_(gt.$$.fragment,t),_(_t.$$.fragment,t),_(kt.$$.fragment,t),_(Tt.$$.fragment,t),_($t.$$.fragment,t),_(yt.$$.fragment,t),_(bt.$$.fragment,t),_(zt.$$.fragment,t),_(Mt.$$.fragment,t),_(xt.$$.fragment,t),_(qt.$$.fragment,t),_(Ft.$$.fragment,t),_(Pt.$$.fragment,t),_(Ct.$$.fragment,t),_(St.$$.fragment,t),_(Lt.$$.fragment,t),_(Dt.$$.fragment,t),_(It.$$.fragment,t),_(Gt.$$.fragment,t),_(Ot.$$.fragment,t),_(Ut.$$.fragment,t),rr=!1},d(t){s(D),t&&s(Wt),t&&s(C),v(Ae),t&&s(En),t&&s(U),v(Se),t&&s(Mn),t&&s(oe),t&&s(xn),t&&s(Bt),t&&s(qn),t&&s(Ht),t&&s(jn),t&&s(ae),t&&s(Fn),t&&s(Rt),t&&s(Pn),t&&s(y),t&&s(Cn),t&&s(N),t&&s(An),t&&s(W),v(Be),t&&s(Sn),t&&s(S),v(He),t&&s(Ln),t&&s(H),v(Xe),t&&s(Dn),t&&s(k),v(Ke),v(Ze),v(tt),v(st),v(nt),t&&s(Nn),t&&s(ce),t&&s(In),t&&s(X),v(ot),t&&s(Gn),t&&s(w),v(at),v(dt),v(mt),t&&s(On),t&&s(ue),t&&s(Vn),t&&s(J),v(ct),t&&s(Un),t&&s(b),v(ft),v(ut),t&&s(Wn),t&&s(Q),v(gt),t&&s(Bn),t&&s(z),v(_t),v(kt),t&&s(Hn),t&&s(Y),v(Tt),t&&s(Rn),t&&s(E),v($t),v(yt),t&&s(Xn),t&&s(Z),v(bt),t&&s(Kn),t&&s(M),v(zt),v(Mt),t&&s(Jn),t&&s(ee),v(xt),t&&s(Qn),t&&s(x),v(qt),v(Ft),t&&s(Yn),t&&s(te),v(Pt),t&&s(Zn),t&&s(q),v(Ct),v(St),t&&s(er),t&&s(se),v(Lt),t&&s(tr),t&&s(j),v(Dt),v(It),t&&s(sr),t&&s(ne),v(Gt),t&&s(nr),t&&s(F),v(Ot),v(Ut)}}}const cp={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"}],title:"mT5"};function fp(Gr,D,Wt){let{fw:C}=D;return Gr.$$set=A=>{"fw"in A&&Wt(0,C=A.fw)},[C]}class kp extends ap{constructor(D){super();ip(this,D,fp,mp,lp,{fw:0})}}export{kp as default,cp as metadata};
