import{S as hn,i as dn,s as mn,e as o,k as l,w as v,t as a,M as fn,c as s,d as n,m as p,a as r,x as T,h as i,b as c,F as t,g as m,y as w,L as un,q as y,o as E,B as P}from"../../chunks/vendor-ab4e3193.js";import{D as ae}from"../../chunks/Docstring-b69c0bd4.js";import{C as gn}from"../../chunks/CodeBlock-516df0c5.js";import{I as Mt}from"../../chunks/IconCopyLink-d992940d.js";import"../../chunks/CopyButton-204b56db.js";function _n(Fe){let _,J,u,g,ie,j,We,le,Ue,we,$,z,pe,N,Xe,ce,He,ye,x,Ge,I,Qe,Ye,Ee,K,Je,Pe,Z,he,Ke,$e,ee,Ze,qe,M,ze,k,et,C,tt,nt,O,ot,st,xe,q,A,de,S,rt,me,at,Ae,d,V,it,fe,lt,pt,F,ct,te,ht,dt,mt,L,W,ft,ue,ut,gt,b,U,_t,ge,kt,bt,X,ne,vt,_e,Tt,wt,oe,yt,ke,Et,Pt,D,H,$t,be,qt,zt,B,G,xt,ve,At,Lt,R,Q,Dt,Y,Bt,Te,Rt,jt,Le;return j=new Mt({}),N=new Mt({}),M=new gn({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

phobert = AutoModel.from_pretrained("vinai/phobert-base")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!
line = "T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 ."

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = phobert(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# phobert = TFAutoModel.from_pretrained("vinai/phobert-base"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">phobert = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">line = <span class="hljs-string">&quot;T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 .&quot;</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">input_ids = torch.tensor([tokenizer.encode(line)])</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">with</span> torch.no_grad():</span>
<span class="hljs-meta">...</span> <span class="language-python">    features = phobert(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># With TensorFlow 2.0+:</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># from transformers import TFAutoModel</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># phobert = TFAutoModel.from_pretrained(&quot;vinai/phobert-base&quot;)</span></span>`}}),S=new Mt({}),V=new ae({props:{name:"class transformers.PhobertTokenizer",anchor:"transformers.PhobertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/phobert/tokenization_phobert.py#L68",parametersDescription:[{anchor:"transformers.PhobertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.PhobertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>st</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.PhobertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.PhobertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.PhobertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.PhobertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.PhobertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.PhobertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}]}}),W=new ae({props:{name:"add_from_file",anchor:"transformers.PhobertTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/phobert/tokenization_phobert.py#L341"}}),U=new ae({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/phobert/tokenization_phobert.py#L164",parametersDescription:[{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),H=new ae({props:{name:"convert_tokens_to_string",anchor:"transformers.PhobertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/phobert/tokenization_phobert.py#L311"}}),G=new ae({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/phobert/tokenization_phobert.py#L218",parametersDescription:[{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Q=new ae({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/phobert/tokenization_phobert.py#L190",parametersDescription:[{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){_=o("meta"),J=l(),u=o("h1"),g=o("a"),ie=o("span"),v(j.$$.fragment),We=l(),le=o("span"),Ue=a("PhoBERT"),we=l(),$=o("h2"),z=o("a"),pe=o("span"),v(N.$$.fragment),Xe=l(),ce=o("span"),He=a("Overview"),ye=l(),x=o("p"),Ge=a("The PhoBERT model was proposed in "),I=o("a"),Qe=a("PhoBERT: Pre-trained language models for Vietnamese"),Ye=a(" by Dat Quoc Nguyen, Anh Tuan Nguyen."),Ee=l(),K=o("p"),Je=a("The abstract from the paper is the following:"),Pe=l(),Z=o("p"),he=o("em"),Ke=a(`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),$e=l(),ee=o("p"),Ze=a("Example of use:"),qe=l(),v(M.$$.fragment),ze=l(),k=o("p"),et=a("This model was contributed by "),C=o("a"),tt=a("dqnguyen"),nt=a(". The original code can be found "),O=o("a"),ot=a("here"),st=a("."),xe=l(),q=o("h2"),A=o("a"),de=o("span"),v(S.$$.fragment),rt=l(),me=o("span"),at=a("PhobertTokenizer"),Ae=l(),d=o("div"),v(V.$$.fragment),it=l(),fe=o("p"),lt=a("Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),pt=l(),F=o("p"),ct=a("This tokenizer inherits from "),te=o("a"),ht=a("PreTrainedTokenizer"),dt=a(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),mt=l(),L=o("div"),v(W.$$.fragment),ft=l(),ue=o("p"),ut=a("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),gt=l(),b=o("div"),v(U.$$.fragment),_t=l(),ge=o("p"),kt=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),bt=l(),X=o("ul"),ne=o("li"),vt=a("single sequence: "),_e=o("code"),Tt=a("<s> X </s>"),wt=l(),oe=o("li"),yt=a("pair of sequences: "),ke=o("code"),Et=a("<s> A </s></s> B </s>"),Pt=l(),D=o("div"),v(H.$$.fragment),$t=l(),be=o("p"),qt=a("Converts a sequence of tokens (string) in a single string."),zt=l(),B=o("div"),v(G.$$.fragment),xt=l(),ve=o("p"),At=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Lt=l(),R=o("div"),v(Q.$$.fragment),Dt=l(),Y=o("p"),Bt=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Te=o("code"),Rt=a("prepare_for_model"),jt=a(" method."),this.h()},l(e){const h=fn('[data-svelte="svelte-1phssyn"]',document.head);_=s(h,"META",{name:!0,content:!0}),h.forEach(n),J=p(e),u=s(e,"H1",{class:!0});var De=r(u);g=s(De,"A",{id:!0,class:!0,href:!0});var Ct=r(g);ie=s(Ct,"SPAN",{});var Ot=r(ie);T(j.$$.fragment,Ot),Ot.forEach(n),Ct.forEach(n),We=p(De),le=s(De,"SPAN",{});var St=r(le);Ue=i(St,"PhoBERT"),St.forEach(n),De.forEach(n),we=p(e),$=s(e,"H2",{class:!0});var Be=r($);z=s(Be,"A",{id:!0,class:!0,href:!0});var Vt=r(z);pe=s(Vt,"SPAN",{});var Ft=r(pe);T(N.$$.fragment,Ft),Ft.forEach(n),Vt.forEach(n),Xe=p(Be),ce=s(Be,"SPAN",{});var Wt=r(ce);He=i(Wt,"Overview"),Wt.forEach(n),Be.forEach(n),ye=p(e),x=s(e,"P",{});var Re=r(x);Ge=i(Re,"The PhoBERT model was proposed in "),I=s(Re,"A",{href:!0,rel:!0});var Ut=r(I);Qe=i(Ut,"PhoBERT: Pre-trained language models for Vietnamese"),Ut.forEach(n),Ye=i(Re," by Dat Quoc Nguyen, Anh Tuan Nguyen."),Re.forEach(n),Ee=p(e),K=s(e,"P",{});var Xt=r(K);Je=i(Xt,"The abstract from the paper is the following:"),Xt.forEach(n),Pe=p(e),Z=s(e,"P",{});var Ht=r(Z);he=s(Ht,"EM",{});var Gt=r(he);Ke=i(Gt,`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),Gt.forEach(n),Ht.forEach(n),$e=p(e),ee=s(e,"P",{});var Qt=r(ee);Ze=i(Qt,"Example of use:"),Qt.forEach(n),qe=p(e),T(M.$$.fragment,e),ze=p(e),k=s(e,"P",{});var se=r(k);et=i(se,"This model was contributed by "),C=s(se,"A",{href:!0,rel:!0});var Yt=r(C);tt=i(Yt,"dqnguyen"),Yt.forEach(n),nt=i(se,". The original code can be found "),O=s(se,"A",{href:!0,rel:!0});var Jt=r(O);ot=i(Jt,"here"),Jt.forEach(n),st=i(se,"."),se.forEach(n),xe=p(e),q=s(e,"H2",{class:!0});var je=r(q);A=s(je,"A",{id:!0,class:!0,href:!0});var Kt=r(A);de=s(Kt,"SPAN",{});var Zt=r(de);T(S.$$.fragment,Zt),Zt.forEach(n),Kt.forEach(n),rt=p(je),me=s(je,"SPAN",{});var en=r(me);at=i(en,"PhobertTokenizer"),en.forEach(n),je.forEach(n),Ae=p(e),d=s(e,"DIV",{class:!0});var f=r(d);T(V.$$.fragment,f),it=p(f),fe=s(f,"P",{});var tn=r(fe);lt=i(tn,"Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),tn.forEach(n),pt=p(f),F=s(f,"P",{});var Ne=r(F);ct=i(Ne,"This tokenizer inherits from "),te=s(Ne,"A",{href:!0});var nn=r(te);ht=i(nn,"PreTrainedTokenizer"),nn.forEach(n),dt=i(Ne,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Ne.forEach(n),mt=p(f),L=s(f,"DIV",{class:!0});var Ie=r(L);T(W.$$.fragment,Ie),ft=p(Ie),ue=s(Ie,"P",{});var on=r(ue);ut=i(on,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),on.forEach(n),Ie.forEach(n),gt=p(f),b=s(f,"DIV",{class:!0});var re=r(b);T(U.$$.fragment,re),_t=p(re),ge=s(re,"P",{});var sn=r(ge);kt=i(sn,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),sn.forEach(n),bt=p(re),X=s(re,"UL",{});var Me=r(X);ne=s(Me,"LI",{});var Nt=r(ne);vt=i(Nt,"single sequence: "),_e=s(Nt,"CODE",{});var rn=r(_e);Tt=i(rn,"<s> X </s>"),rn.forEach(n),Nt.forEach(n),wt=p(Me),oe=s(Me,"LI",{});var It=r(oe);yt=i(It,"pair of sequences: "),ke=s(It,"CODE",{});var an=r(ke);Et=i(an,"<s> A </s></s> B </s>"),an.forEach(n),It.forEach(n),Me.forEach(n),re.forEach(n),Pt=p(f),D=s(f,"DIV",{class:!0});var Ce=r(D);T(H.$$.fragment,Ce),$t=p(Ce),be=s(Ce,"P",{});var ln=r(be);qt=i(ln,"Converts a sequence of tokens (string) in a single string."),ln.forEach(n),Ce.forEach(n),zt=p(f),B=s(f,"DIV",{class:!0});var Oe=r(B);T(G.$$.fragment,Oe),xt=p(Oe),ve=s(Oe,"P",{});var pn=r(ve);At=i(pn,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),pn.forEach(n),Oe.forEach(n),Lt=p(f),R=s(f,"DIV",{class:!0});var Se=r(R);T(Q.$$.fragment,Se),Dt=p(Se),Y=s(Se,"P",{});var Ve=r(Y);Bt=i(Ve,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Te=s(Ve,"CODE",{});var cn=r(Te);Rt=i(cn,"prepare_for_model"),cn.forEach(n),jt=i(Ve," method."),Ve.forEach(n),Se.forEach(n),f.forEach(n),this.h()},h(){c(_,"name","hf:doc:metadata"),c(_,"content",JSON.stringify(kn)),c(g,"id","phobert"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#phobert"),c(u,"class","relative group"),c(z,"id","overview"),c(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z,"href","#overview"),c($,"class","relative group"),c(I,"href","https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf"),c(I,"rel","nofollow"),c(C,"href","https://huggingface.co/dqnguyen"),c(C,"rel","nofollow"),c(O,"href","https://github.com/VinAIResearch/PhoBERT"),c(O,"rel","nofollow"),c(A,"id","transformers.PhobertTokenizer"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#transformers.PhobertTokenizer"),c(q,"class","relative group"),c(te,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(L,"class","docstring"),c(b,"class","docstring"),c(D,"class","docstring"),c(B,"class","docstring"),c(R,"class","docstring"),c(d,"class","docstring")},m(e,h){t(document.head,_),m(e,J,h),m(e,u,h),t(u,g),t(g,ie),w(j,ie,null),t(u,We),t(u,le),t(le,Ue),m(e,we,h),m(e,$,h),t($,z),t(z,pe),w(N,pe,null),t($,Xe),t($,ce),t(ce,He),m(e,ye,h),m(e,x,h),t(x,Ge),t(x,I),t(I,Qe),t(x,Ye),m(e,Ee,h),m(e,K,h),t(K,Je),m(e,Pe,h),m(e,Z,h),t(Z,he),t(he,Ke),m(e,$e,h),m(e,ee,h),t(ee,Ze),m(e,qe,h),w(M,e,h),m(e,ze,h),m(e,k,h),t(k,et),t(k,C),t(C,tt),t(k,nt),t(k,O),t(O,ot),t(k,st),m(e,xe,h),m(e,q,h),t(q,A),t(A,de),w(S,de,null),t(q,rt),t(q,me),t(me,at),m(e,Ae,h),m(e,d,h),w(V,d,null),t(d,it),t(d,fe),t(fe,lt),t(d,pt),t(d,F),t(F,ct),t(F,te),t(te,ht),t(F,dt),t(d,mt),t(d,L),w(W,L,null),t(L,ft),t(L,ue),t(ue,ut),t(d,gt),t(d,b),w(U,b,null),t(b,_t),t(b,ge),t(ge,kt),t(b,bt),t(b,X),t(X,ne),t(ne,vt),t(ne,_e),t(_e,Tt),t(X,wt),t(X,oe),t(oe,yt),t(oe,ke),t(ke,Et),t(d,Pt),t(d,D),w(H,D,null),t(D,$t),t(D,be),t(be,qt),t(d,zt),t(d,B),w(G,B,null),t(B,xt),t(B,ve),t(ve,At),t(d,Lt),t(d,R),w(Q,R,null),t(R,Dt),t(R,Y),t(Y,Bt),t(Y,Te),t(Te,Rt),t(Y,jt),Le=!0},p:un,i(e){Le||(y(j.$$.fragment,e),y(N.$$.fragment,e),y(M.$$.fragment,e),y(S.$$.fragment,e),y(V.$$.fragment,e),y(W.$$.fragment,e),y(U.$$.fragment,e),y(H.$$.fragment,e),y(G.$$.fragment,e),y(Q.$$.fragment,e),Le=!0)},o(e){E(j.$$.fragment,e),E(N.$$.fragment,e),E(M.$$.fragment,e),E(S.$$.fragment,e),E(V.$$.fragment,e),E(W.$$.fragment,e),E(U.$$.fragment,e),E(H.$$.fragment,e),E(G.$$.fragment,e),E(Q.$$.fragment,e),Le=!1},d(e){n(_),e&&n(J),e&&n(u),P(j),e&&n(we),e&&n($),P(N),e&&n(ye),e&&n(x),e&&n(Ee),e&&n(K),e&&n(Pe),e&&n(Z),e&&n($e),e&&n(ee),e&&n(qe),P(M,e),e&&n(ze),e&&n(k),e&&n(xe),e&&n(q),P(S),e&&n(Ae),e&&n(d),P(V),P(W),P(U),P(H),P(G),P(Q)}}}const kn={local:"phobert",sections:[{local:"overview",title:"Overview"},{local:"transformers.PhobertTokenizer",title:"PhobertTokenizer"}],title:"PhoBERT"};function bn(Fe,_,J){let{fw:u}=_;return Fe.$$set=g=>{"fw"in g&&J(0,u=g.fw)},[u]}class Pn extends hn{constructor(_){super();dn(this,_,bn,_n,mn,{fw:0})}}export{Pn as default,kn as metadata};
