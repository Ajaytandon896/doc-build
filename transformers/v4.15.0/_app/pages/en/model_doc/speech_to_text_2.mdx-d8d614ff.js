import{S as _i,i as gi,s as vi,e as a,k as l,w as m,t as r,L as Ti,c as n,d as o,m as d,a as c,x as u,h as s,b as i,J as e,g as p,y as _,q as g,o as v,B as T}from"../../../chunks/vendor-b1433968.js";import{T as ui}from"../../../chunks/Tip-c3840994.js";import{D as W}from"../../../chunks/Docstring-ff504c58.js";import{C as Ar}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as gt}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function xi($e){let f,A,x,S,F,w,B,D;return{c(){f=a("p"),A=r(`This class method is simply calling AutoFeatureExtractor\u2019s
`),x=a("code"),S=r("from_pretrained"),F=r(` and Speech2Text2Tokenizer\u2019s
`),w=a("code"),B=r("from_pretrained"),D=r(`. Please refer to the
docstrings of the methods above for more information.`)},l(z){f=n(z,"P",{});var b=c(f);A=s(b,`This class method is simply calling AutoFeatureExtractor\u2019s
`),x=n(b,"CODE",{});var C=c(x);S=s(C,"from_pretrained"),C.forEach(o),F=s(b,` and Speech2Text2Tokenizer\u2019s
`),w=n(b,"CODE",{});var j=c(w);B=s(j,"from_pretrained"),j.forEach(o),D=s(b,`. Please refer to the
docstrings of the methods above for more information.`),b.forEach(o)},m(z,b){p(z,f,b),e(f,A),e(f,x),e(x,S),e(f,F),e(f,w),e(w,B),e(f,D)},d(z){z&&o(f)}}}function ki($e){let f,A,x,S,F,w,B,D;return{c(){f=a("p"),A=r("This class method is simply calling "),x=a("code"),S=r("save_pretrained"),F=r(` and
`),w=a("code"),B=r("save_pretrained"),D=r(`. Please refer to the
docstrings of the methods above for more information.`)},l(z){f=n(z,"P",{});var b=c(f);A=s(b,"This class method is simply calling "),x=n(b,"CODE",{});var C=c(x);S=s(C,"save_pretrained"),C.forEach(o),F=s(b,` and
`),w=n(b,"CODE",{});var j=c(w);B=s(j,"save_pretrained"),j.forEach(o),D=s(b,`. Please refer to the
docstrings of the methods above for more information.`),b.forEach(o)},m(z,b){p(z,f,b),e(f,A),e(f,x),e(x,S),e(f,F),e(f,w),e(w,B),e(f,D)},d(z){z&&o(f)}}}function bi($e){let f,A,x,S,F,w,B,D,z,b,C,j,so,Ee,jr,ao,Mr,Wo,H,Lr,vt,Fr,Dr,Pe,Ir,Wr,Vo,y,Vr,no,Nr,Br,co,Or,Ur,Tt,Rr,Hr,xt,Jr,Gr,kt,Kr,Qr,io,Xr,Yr,No,ie,Zr,ze,es,ts,Bo,le,os,Ce,rs,ss,Oo,bt,as,Uo,J,qe,ns,Ae,cs,is,ls,je,ds,St,hs,ps,fs,Me,ms,wt,us,Le,_s,gs,Ro,ee,de,lo,Fe,vs,ho,Ts,Ho,G,xs,yt,ks,bs,$t,Ss,ws,Jo,P,ys,Et,$s,Es,Pt,Ps,zs,zt,Cs,qs,Ct,As,js,qt,Ms,Ls,Go,At,po,Fs,Ko,De,Qo,jt,Ie,fo,Ds,Is,mo,Ws,Xo,We,Yo,he,Vs,Ve,Ns,Bs,Zo,te,pe,uo,Ne,Os,_o,Us,er,q,Be,Rs,oe,Hs,Mt,Js,Gs,Oe,Ks,Qs,Xs,re,Ys,Lt,Zs,ea,Ft,ta,oa,ra,go,sa,aa,Ue,tr,se,fe,vo,Re,na,To,ca,or,E,He,ia,xo,la,da,Je,ha,Dt,pa,fa,ma,me,Ge,ua,ko,_a,ga,K,Ke,va,bo,Ta,xa,Qe,ka,So,ba,Sa,wa,wo,rr,ae,ue,yo,Xe,ya,$o,$a,sr,k,Ye,Ea,Eo,Pa,za,M,It,Ca,qa,Wt,Aa,ja,Vt,Ma,La,Ze,Po,Fa,Da,Ia,Nt,Wa,Va,Na,_e,et,Ba,O,Oa,zo,Ua,Ra,Bt,Ha,Ja,tt,Co,Ga,Ka,Qa,Xa,Q,ot,Ya,rt,Za,Ot,en,tn,on,ge,rn,X,st,sn,ne,an,qo,nn,cn,Ut,ln,dn,hn,ve,pn,Te,at,fn,nt,mn,Rt,un,_n,gn,xe,ct,vn,it,Tn,Ht,xn,kn,bn,ke,lt,Sn,Ao,wn,ar,ce,be,jo,dt,yn,Mo,$n,nr,I,ht,En,U,Pn,Jt,zn,Cn,Lo,qn,An,Fo,jn,Mn,Ln,pt,Fn,ft,Dn,In,Wn,Y,mt,Vn,Do,Nn,Bn,ut,cr;return w=new gt({}),Ee=new gt({}),Fe=new gt({}),De=new Ar({props:{code:`import torch
from transformers import Speech2Text2Processor, SpeechEncoderDecoderModel
from datasets import load_dataset
import soundfile as sf

model = SpeechEncoderDecoderModel.from_pretrained("facebook/s2t-wav2vec2-large-en-de")
processor = Speech2Text2Processor.from_pretrained("facebook/s2t-wav2vec2-large-en-de")

def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(input_ids=inputs["input_values"], attention_mask=inputs["attention_mask"])

transcription = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2Text2Processor, SpeechEncoderDecoderModel</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = SpeechEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">processor = Speech2Text2Processor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):</span>
<span class="hljs-meta">...</span> <span class="language-python">    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])</span>
<span class="hljs-meta">...</span> <span class="language-python">    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech</span>
<span class="hljs-meta">...</span> <span class="language-python">    <span class="hljs-keyword">return</span> batch</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">ds = ds.<span class="hljs-built_in">map</span>(map_to_array)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">inputs = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16_000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">generated_ids = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_values&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>])</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">transcription = processor.batch_decode(generated_ids)</span>`}}),We=new Ar({props:{code:`from datasets import load_dataset
from transformers import pipeline

librispeech_en = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
asr = pipeline("automatic-speech-recognition", model="facebook/s2t-wav2vec2-large-en-de", feature_extractor="facebook/s2t-wav2vec2-large-en-de")

translation_de = asr(librispeech_en[0]["file"]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">librispeech_en = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">asr = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>, feature_extractor=<span class="hljs-string">&quot;facebook/s2t-wav2vec2-large-en-de&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">translation_de = asr(librispeech_en[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;file&quot;</span>])</span>`}}),Ne=new gt({}),Be=new W({props:{name:"class transformers.Speech2Text2Config",anchor:"transformers.Speech2Text2Config",parameters:[{name:"vocab_size",val:" = 10000"},{name:"decoder_layers",val:" = 6"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 4"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 256"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"classifier_dropout",val:" = 0.0"},{name:"scale_embedding",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"max_source_positions",val:" = 6000"},{name:"max_target_positions",val:" = 1024"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/configuration_speech_to_text_2.py#L29",parametersDescription:[{anchor:"transformers.Speech2Text2Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the Speech2Text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.15.0/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a>`,name:"vocab_size"},{anchor:"transformers.Speech2Text2Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.Speech2Text2Config.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.Speech2Text2Config.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.Speech2Text2Config.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.Speech2Text2Config.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.Speech2Text2Config.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, and pooler.`,name:"dropout"},{anchor:"transformers.Speech2Text2Config.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Speech2Text2Config.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.Speech2Text2Config.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for classifier.`,name:"classifier_dropout"},{anchor:"transformers.Speech2Text2Config.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
<a href="https://arxiv.org/abs/1909.11556%3E%60" rel="nofollow">https://arxiv.org/abs/1909.11556&gt;\`</a>__ for more details. decoder_layerdrop: (<code>float</code>, <em>optional</em>, defaults to 0.0):
The LayerDrop probability for the decoder. See the [LayerDrop paper](see
<a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>) for more details.`,name:"init_std"},{anchor:"transformers.Speech2Text2Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.Speech2Text2Config.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 6000) &#x2014;
The maximum sequence length of log-mel filter-bank features that this model might ever be used with.
max_target_positions &#x2014; (<code>int</code>, <em>optional</em>, defaults to 1024):
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_source_positions"}]}}),Ue=new Ar({props:{code:`from transformers import Speech2Text2ForCausalLM, Speech2Text2Config

# Initializing a Speech2Text2 s2t_transformer_s style configuration
configuration = Speech2Text2Config()

# Initializing a model from the s2t_transformer_s style configuration
model = Speech2Text2ForCausalLM(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2Text2ForCausalLM, Speech2Text2Config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Speech2Text2 s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Speech2Text2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2Text2ForCausalLM(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Re=new gt({}),He=new W({props:{name:"class transformers.Speech2Text2Tokenizer",anchor:"transformers.Speech2Text2Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"do_lower_case",val:" = False"},{name:"merges_file",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py#L67",parametersDescription:[{anchor:"transformers.Speech2Text2Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.Speech2Text2Tokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sentence token.`,name:"bos_token"},{anchor:"transformers.Speech2Text2Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sentence token.`,name:"eos_token"},{anchor:"transformers.Speech2Text2Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.Speech2Text2Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.</p>
<p>**kwargs &#x2014;
Additional keyword arguments passed along to <a href="/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>`,name:"pad_token"}]}}),Ge=new W({props:{name:"batch\\_decode",anchor:"transformers.PreTrainedTokenizerBase.batch_decode",parameters:[{name:"sequences",val:": typing.Union[typing.List[int], typing.List[typing.List[int]], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]"},{name:"skip_special_tokens",val:": bool = False"},{name:"clean_up_tokenization_spaces",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/tokenization_utils_base.py#L3178",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.sequences",description:`<strong>sequences</strong> (<code>Union[List[int], List[List[int]], np.ndarray, torch.Tensor, tf.Tensor]</code>) &#x2014;
List of tokenized input ids. Can be obtained using the <code>__call__</code> method.`,name:"sequences"},{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"},{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean up the tokenization spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.PreTrainedTokenizerBase.batch_decode.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the underlying model specific decode method.`,name:"kwargs"}],returnDescription:`
<p>The list of decoded sentences.</p>
`,returnType:`
<p><code>List[str]</code></p>
`}}),Ke=new W({props:{name:"decode",anchor:"transformers.PreTrainedTokenizerBase.decode",parameters:[{name:"token_ids",val:": typing.Union[int, typing.List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')]"},{name:"skip_special_tokens",val:": bool = False"},{name:"clean_up_tokenization_spaces",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/tokenization_utils_base.py#L3211",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.decode.token_ids",description:`<strong>token_ids</strong> (<code>Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]</code>) &#x2014;
List of tokenized input ids. Can be obtained using the <code>__call__</code> method.`,name:"token_ids"},{anchor:"transformers.PreTrainedTokenizerBase.decode.skip_special_tokens",description:`<strong>skip_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to remove special tokens in the decoding.`,name:"skip_special_tokens"},{anchor:"transformers.PreTrainedTokenizerBase.decode.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean up the tokenization spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.PreTrainedTokenizerBase.decode.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the underlying model specific decode method.`,name:"kwargs"}],returnDescription:`
<p>The decoded sentence.</p>
`,returnType:`
<p><code>str</code></p>
`}}),Xe=new gt({}),Ye=new W({props:{name:"class transformers.Speech2Text2Processor",anchor:"transformers.Speech2Text2Processor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L25",parametersDescription:[{anchor:"transformers.Speech2Text2Processor.feature_extractor",description:`<strong>feature_extractor</strong> (<code>AutoFeatureExtractor</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.15.0/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a>. The feature extractor is a required input.`,name:"feature_extractor"},{anchor:"transformers.Speech2Text2Processor.tokenizer",description:`<strong>tokenizer</strong> (<code>Speech2Text2Tokenizer</code>) &#x2014;
An instance of <a href="/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer">Speech2Text2Tokenizer</a>. The tokenizer is a required input.`,name:"tokenizer"}]}}),et=new W({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.Speech2Text2Processor.__call__",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L114"}}),ot=new W({props:{name:"from\\_pretrained",anchor:"transformers.Speech2Text2Processor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L79",parametersDescription:[{anchor:"transformers.Speech2Text2Processor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<code>save_pretrained</code> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.
**kwargs &#x2014;
Additional keyword arguments passed along to both <code>PreTrainedFeatureExtractor</code> and
<a href="/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a></li>
</ul>`,name:"pretrained_model_name_or_path"}]}}),ge=new ui({props:{$$slots:{default:[xi]},$$scope:{ctx:$e}}}),st=new W({props:{name:"save\\_pretrained",anchor:"transformers.Speech2Text2Processor.save_pretrained",parameters:[{name:"save_directory",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L56",parametersDescription:[{anchor:"transformers.Speech2Text2Processor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"}]}}),ve=new ui({props:{$$slots:{default:[ki]},$$scope:{ctx:$e}}}),at=new W({props:{name:"batch\\_decode",anchor:"transformers.Speech2Text2Processor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L124"}}),ct=new W({props:{name:"decode",anchor:"transformers.Speech2Text2Processor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L132"}}),lt=new W({props:{name:"as\\_target\\_processor",anchor:"transformers.Speech2Text2Processor.as_target_processor",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/processing_speech_to_text_2.py#L140"}}),dt=new gt({}),ht=new W({props:{name:"class transformers.Speech2Text2ForCausalLM",anchor:"transformers.Speech2Text2ForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py#L742",parametersDescription:[{anchor:"transformers.Speech2Text2ForCausalLM.config",description:`<strong>config</strong> ([<em>Speech2Text2Config</em>]) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
[<em>~PreTrainedModel.from_pretrained</em>] method to load the model weights.`,name:"config"}]}}),mt=new W({props:{name:"forward",anchor:"transformers.Speech2Text2ForCausalLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py#L773",parametersDescription:[{anchor:"transformers.Speech2Text2ForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer">Speech2Text2Tokenizer</a>. See
<a href="/docs/transformers/v4.15.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.15.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a>
for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention
if the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used
in the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2
tensors of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional
tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>. The two
additional tensors are only required when the model is used as a decoder in a Sequence to Sequence
model.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the
cross-attention blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential
decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more detail.`,name:"output_hidden_states"},{anchor:"transformers.Speech2Text2ForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"
>Speech2Text2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the
cached key, value states of the self-attention and the cross-attention layers if model is used in
encoder-decoder setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ut=new Ar({props:{code:`from transformers import SpeechEncoderDecoderModel, Speech2Text2ForCausalLM, Wav2Vec2Model, Speech2Text2Config, Wav2Vec2Config

encoder = Wav2Vec2Model(Wav2Vec2Config())
decoder = Speech2Text2ForCausalLM(Speech2Text2Config())

model = SpeechEncoderDecoderModel(encoder=encoder, decoder=decoder),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SpeechEncoderDecoderModel, Speech2Text2ForCausalLM, Wav2Vec2Model, Speech2Text2Config, Wav2Vec2Config

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder = Wav2Vec2Model(Wav2Vec2Config())
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder = Speech2Text2ForCausalLM(Speech2Text2Config())

<span class="hljs-comment"># init speech2text model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SpeechEncoderDecoderModel(encoder=encoder, decoder=decoder)`}}),{c(){f=a("meta"),A=l(),x=a("h1"),S=a("a"),F=a("span"),m(w.$$.fragment),B=l(),D=a("span"),z=r("Speech2Text2"),b=l(),C=a("h2"),j=a("a"),so=a("span"),m(Ee.$$.fragment),jr=l(),ao=a("span"),Mr=r("Overview"),Wo=l(),H=a("p"),Lr=r("The Speech2Text2 model is used together with "),vt=a("a"),Fr=r("Wav2Vec2"),Dr=r(` for Speech Translation models proposed in
`),Pe=a("a"),Ir=r("Large-Scale Self- and Semi-Supervised Learning for Speech Translation"),Wr=r(` by
Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.`),Vo=l(),y=a("p"),Vr=r("Speech2Text2 is a "),no=a("em"),Nr=r("decoder-only"),Br=r(" transformer model that can be used with any speech "),co=a("em"),Or=r("encoder-only"),Ur=r(`, such as
`),Tt=a("a"),Rr=r("Wav2Vec2"),Hr=r(" or "),xt=a("a"),Jr=r("HuBERT"),Gr=r(` for Speech-to-Text tasks. Please refer to the
`),kt=a("a"),Kr=r("SpeechEncoderDecoder"),Qr=r(" class on how to combine Speech2Text2 with any speech "),io=a("em"),Xr=r("encoder-only"),Yr=r(`
model.`),No=l(),ie=a("p"),Zr=r("This model was contributed by "),ze=a("a"),es=r("Patrick von Platen"),ts=r("."),Bo=l(),le=a("p"),os=r("The original code can be found "),Ce=a("a"),rs=r("here"),ss=r("."),Oo=l(),bt=a("p"),as=r("Tips:"),Uo=l(),J=a("ul"),qe=a("li"),ns=r(`Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dataset. For more information, see
the `),Ae=a("a"),cs=r("official models"),is=r(" ."),ls=l(),je=a("li"),ds=r("Speech2Text2 is always used within the "),St=a("a"),hs=r("SpeechEncoderDecoder"),ps=r(" framework."),fs=l(),Me=a("li"),ms=r("Speech2Text2\u2019s tokenizer is based on "),wt=a("em"),us=r("fastBPE <"),Le=a("a"),_s=r("https://github.com/glample/fastBPE>"),gs=r("."),Ro=l(),ee=a("h2"),de=a("a"),lo=a("span"),m(Fe.$$.fragment),vs=l(),ho=a("span"),Ts=r("Inference"),Ho=l(),G=a("p"),xs=r("Speech2Text2\u2019s "),yt=a("a"),ks=r("SpeechEncoderDecoderModel"),bs=r(` model accepts raw waveform input values from speech and
makes use of `),$t=a("a"),Ss=r("generate()"),ws=r(` to translate the input speech
autoregressively to the target language.`),Jo=l(),P=a("p"),ys=r("The "),Et=a("a"),$s=r("Wav2Vec2FeatureExtractor"),Es=r(` class is responsible for preprocessing the input speech and
`),Pt=a("a"),Ps=r("Speech2Text2Tokenizer"),zs=r(` decodes the generated target tokens to the target string. The
`),zt=a("a"),Cs=r("Speech2Text2Processor"),qs=r(" wraps "),Ct=a("a"),As=r("Wav2Vec2FeatureExtractor"),js=r(` and
`),qt=a("a"),Ms=r("Speech2Text2Tokenizer"),Ls=r(` into a single instance to both extract the input features and decode the
predicted token ids.`),Go=l(),At=a("ul"),po=a("li"),Fs=r("Step-by-step Speech Translation"),Ko=l(),m(De.$$.fragment),Qo=l(),jt=a("ul"),Ie=a("li"),fo=a("p"),Ds=r("Speech Translation via Pipelines"),Is=l(),mo=a("p"),Ws=r("The automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code"),Xo=l(),m(We.$$.fragment),Yo=l(),he=a("p"),Vs=r("See "),Ve=a("a"),Ns=r("model hub"),Bs=r(" to look for Speech2Text2 checkpoints."),Zo=l(),te=a("h2"),pe=a("a"),uo=a("span"),m(Ne.$$.fragment),Os=l(),_o=a("span"),Us=r("Speech2Text2Config"),er=l(),q=a("div"),m(Be.$$.fragment),Rs=l(),oe=a("p"),Hs=r("This is the configuration class to store the configuration of a "),Mt=a("a"),Js=r("Speech2Text2ForCausalLM"),Gs=r(`. It
is used to instantiate an Speech2Text2 model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text2
`),Oe=a("a"),Ks=r("facebook/s2t-small-librispeech-asr"),Qs=r(" architecture."),Xs=l(),re=a("p"),Ys=r("Configuration objects inherit from "),Lt=a("a"),Zs=r("PretrainedConfig"),ea=r(` and can be used to control the model
outputs. Read the documentation from `),Ft=a("a"),ta=r("PretrainedConfig"),oa=r(" for more information."),ra=l(),go=a("p"),sa=r("Example:"),aa=l(),m(Ue.$$.fragment),tr=l(),se=a("h2"),fe=a("a"),vo=a("span"),m(Re.$$.fragment),na=l(),To=a("span"),ca=r("Speech2TextTokenizer"),or=l(),E=a("div"),m(He.$$.fragment),ia=l(),xo=a("p"),la=r("Constructs a Speech2Text2Tokenizer."),da=l(),Je=a("p"),ha=r("This tokenizer inherits from "),Dt=a("a"),pa=r("PreTrainedTokenizer"),fa=r(` which contains some of the main methods.
Users should refer to the superclass for more information regarding such methods.`),ma=l(),me=a("div"),m(Ge.$$.fragment),ua=l(),ko=a("p"),_a=r("Convert a list of lists of token ids into a list of strings by calling decode."),ga=l(),K=a("div"),m(Ke.$$.fragment),va=l(),bo=a("p"),Ta=r(`Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.`),xa=l(),Qe=a("p"),ka=r("Similar to doing "),So=a("code"),ba=r("self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))"),Sa=r("."),wa=l(),wo=a("div"),rr=l(),ae=a("h2"),ue=a("a"),yo=a("span"),m(Xe.$$.fragment),ya=l(),$o=a("span"),$a=r("Speech2Text2Processor"),sr=l(),k=a("div"),m(Ye.$$.fragment),Ea=l(),Eo=a("p"),Pa=r(`Constructs a Speech2Text2 processor which wraps a Speech2Text2 feature extractor and a Speech2Text2 tokenizer into
a single processor.`),za=l(),M=a("p"),It=a("a"),Ca=r("Speech2Text2Processor"),qa=r(` offers all the functionalities of
`),Wt=a("a"),Aa=r("AutoFeatureExtractor"),ja=r(" and "),Vt=a("a"),Ma=r("Speech2Text2Tokenizer"),La=r(`. See the
`),Ze=a("a"),Po=a("strong"),Fa=r("call"),Da=r("()"),Ia=r(" and "),Nt=a("a"),Wa=r("decode()"),Va=r(` for
more information.`),Na=l(),_e=a("div"),m(et.$$.fragment),Ba=l(),O=a("p"),Oa=r(`When used in normal mode, this method forwards all its arguments to AutoFeatureExtractor\u2019s
`),zo=a("code"),Ua=r("__call__()"),Ra=r(` and returns its output. If used in the context
`),Bt=a("a"),Ha=r("as_target_processor()"),Ja=r(` this method forwards all its arguments to
Speech2Text2Tokenizer\u2019s `),tt=a("a"),Co=a("strong"),Ga=r("call"),Ka=r("()"),Qa=r(`. Please refer to the doctsring of
the above two methods for more information.`),Xa=l(),Q=a("div"),m(ot.$$.fragment),Ya=l(),rt=a("p"),Za=r("Instantiate a "),Ot=a("a"),en=r("Speech2Text2Processor"),tn=r(" from a pretrained Speech2Text2 processor."),on=l(),m(ge.$$.fragment),rn=l(),X=a("div"),m(st.$$.fragment),sn=l(),ne=a("p"),an=r(`Save a Speech2Text2 feature extractor object and Speech2Text2 tokenizer object to the directory
`),qo=a("code"),nn=r("save_directory"),cn=r(`, so that it can be re-loaded using the
`),Ut=a("a"),ln=r("from_pretrained()"),dn=r(" class method."),hn=l(),m(ve.$$.fragment),pn=l(),Te=a("div"),m(at.$$.fragment),fn=l(),nt=a("p"),mn=r(`This method forwards all its arguments to Speech2Text2Tokenizer\u2019s
`),Rt=a("a"),un=r("batch_decode()"),_n=r(`. Please refer to the docstring of this method for more
information.`),gn=l(),xe=a("div"),m(ct.$$.fragment),vn=l(),it=a("p"),Tn=r(`This method forwards all its arguments to Speech2Text2Tokenizer\u2019s
`),Ht=a("a"),xn=r("decode()"),kn=r(`. Please refer to the docstring of this method for more
information.`),bn=l(),ke=a("div"),m(lt.$$.fragment),Sn=l(),Ao=a("p"),wn=r(`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text2.`),ar=l(),ce=a("h2"),be=a("a"),jo=a("span"),m(dt.$$.fragment),yn=l(),Mo=a("span"),$n=r("Speech2Text2ForCausalLM"),nr=l(),I=a("div"),m(ht.$$.fragment),En=l(),U=a("p"),Pn=r("The Speech2Text2 Decoder with a language modeling head. Can be used as the decoder part of "),Jt=a("a"),zn=r("EncoderDecoderModel"),Cn=r(" and "),Lo=a("code"),qn=r("SpeechEncoderDecoder"),An=r(`.
This model inherits from [`),Fo=a("em"),jn=r("PreTrainedModel"),Mn=r(`]. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Ln=l(),pt=a("p"),Fn=r("This model is also a PyTorch "),ft=a("a"),Dn=r("torch.nn.Module"),In=r(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Wn=l(),Y=a("div"),m(mt.$$.fragment),Vn=l(),Do=a("p"),Nn=r("Example:"),Bn=l(),m(ut.$$.fragment),this.h()},l(t){const h=Ti('[data-svelte="svelte-1phssyn"]',document.head);f=n(h,"META",{name:!0,content:!0}),h.forEach(o),A=d(t),x=n(t,"H1",{class:!0});var _t=c(x);S=n(_t,"A",{id:!0,class:!0,href:!0});var Io=c(S);F=n(Io,"SPAN",{});var Hn=c(F);u(w.$$.fragment,Hn),Hn.forEach(o),Io.forEach(o),B=d(_t),D=n(_t,"SPAN",{});var Jn=c(D);z=s(Jn,"Speech2Text2"),Jn.forEach(o),_t.forEach(o),b=d(t),C=n(t,"H2",{class:!0});var ir=c(C);j=n(ir,"A",{id:!0,class:!0,href:!0});var Gn=c(j);so=n(Gn,"SPAN",{});var Kn=c(so);u(Ee.$$.fragment,Kn),Kn.forEach(o),Gn.forEach(o),jr=d(ir),ao=n(ir,"SPAN",{});var Qn=c(ao);Mr=s(Qn,"Overview"),Qn.forEach(o),ir.forEach(o),Wo=d(t),H=n(t,"P",{});var Gt=c(H);Lr=s(Gt,"The Speech2Text2 model is used together with "),vt=n(Gt,"A",{href:!0});var Xn=c(vt);Fr=s(Xn,"Wav2Vec2"),Xn.forEach(o),Dr=s(Gt,` for Speech Translation models proposed in
`),Pe=n(Gt,"A",{href:!0,rel:!0});var Yn=c(Pe);Ir=s(Yn,"Large-Scale Self- and Semi-Supervised Learning for Speech Translation"),Yn.forEach(o),Wr=s(Gt,` by
Changhan Wang, Anne Wu, Juan Pino, Alexei Baevski, Michael Auli, Alexis Conneau.`),Gt.forEach(o),Vo=d(t),y=n(t,"P",{});var L=c(y);Vr=s(L,"Speech2Text2 is a "),no=n(L,"EM",{});var Zn=c(no);Nr=s(Zn,"decoder-only"),Zn.forEach(o),Br=s(L," transformer model that can be used with any speech "),co=n(L,"EM",{});var ec=c(co);Or=s(ec,"encoder-only"),ec.forEach(o),Ur=s(L,`, such as
`),Tt=n(L,"A",{href:!0});var tc=c(Tt);Rr=s(tc,"Wav2Vec2"),tc.forEach(o),Hr=s(L," or "),xt=n(L,"A",{href:!0});var oc=c(xt);Jr=s(oc,"HuBERT"),oc.forEach(o),Gr=s(L,` for Speech-to-Text tasks. Please refer to the
`),kt=n(L,"A",{href:!0});var rc=c(kt);Kr=s(rc,"SpeechEncoderDecoder"),rc.forEach(o),Qr=s(L," class on how to combine Speech2Text2 with any speech "),io=n(L,"EM",{});var sc=c(io);Xr=s(sc,"encoder-only"),sc.forEach(o),Yr=s(L,`
model.`),L.forEach(o),No=d(t),ie=n(t,"P",{});var lr=c(ie);Zr=s(lr,"This model was contributed by "),ze=n(lr,"A",{href:!0,rel:!0});var ac=c(ze);es=s(ac,"Patrick von Platen"),ac.forEach(o),ts=s(lr,"."),lr.forEach(o),Bo=d(t),le=n(t,"P",{});var dr=c(le);os=s(dr,"The original code can be found "),Ce=n(dr,"A",{href:!0,rel:!0});var nc=c(Ce);rs=s(nc,"here"),nc.forEach(o),ss=s(dr,"."),dr.forEach(o),Oo=d(t),bt=n(t,"P",{});var cc=c(bt);as=s(cc,"Tips:"),cc.forEach(o),Uo=d(t),J=n(t,"UL",{});var Kt=c(J);qe=n(Kt,"LI",{});var hr=c(qe);ns=s(hr,`Speech2Text2 achieves state-of-the-art results on the CoVoST Speech Translation dataset. For more information, see
the `),Ae=n(hr,"A",{href:!0,rel:!0});var ic=c(Ae);cs=s(ic,"official models"),ic.forEach(o),is=s(hr," ."),hr.forEach(o),ls=d(Kt),je=n(Kt,"LI",{});var pr=c(je);ds=s(pr,"Speech2Text2 is always used within the "),St=n(pr,"A",{href:!0});var lc=c(St);hs=s(lc,"SpeechEncoderDecoder"),lc.forEach(o),ps=s(pr," framework."),pr.forEach(o),fs=d(Kt),Me=n(Kt,"LI",{});var fr=c(Me);ms=s(fr,"Speech2Text2\u2019s tokenizer is based on "),wt=n(fr,"EM",{});var On=c(wt);us=s(On,"fastBPE <"),Le=n(On,"A",{href:!0,rel:!0});var dc=c(Le);_s=s(dc,"https://github.com/glample/fastBPE>"),dc.forEach(o),On.forEach(o),gs=s(fr,"."),fr.forEach(o),Kt.forEach(o),Ro=d(t),ee=n(t,"H2",{class:!0});var mr=c(ee);de=n(mr,"A",{id:!0,class:!0,href:!0});var hc=c(de);lo=n(hc,"SPAN",{});var pc=c(lo);u(Fe.$$.fragment,pc),pc.forEach(o),hc.forEach(o),vs=d(mr),ho=n(mr,"SPAN",{});var fc=c(ho);Ts=s(fc,"Inference"),fc.forEach(o),mr.forEach(o),Ho=d(t),G=n(t,"P",{});var Qt=c(G);xs=s(Qt,"Speech2Text2\u2019s "),yt=n(Qt,"A",{href:!0});var mc=c(yt);ks=s(mc,"SpeechEncoderDecoderModel"),mc.forEach(o),bs=s(Qt,` model accepts raw waveform input values from speech and
makes use of `),$t=n(Qt,"A",{href:!0});var uc=c($t);Ss=s(uc,"generate()"),uc.forEach(o),ws=s(Qt,` to translate the input speech
autoregressively to the target language.`),Qt.forEach(o),Jo=d(t),P=n(t,"P",{});var V=c(P);ys=s(V,"The "),Et=n(V,"A",{href:!0});var _c=c(Et);$s=s(_c,"Wav2Vec2FeatureExtractor"),_c.forEach(o),Es=s(V,` class is responsible for preprocessing the input speech and
`),Pt=n(V,"A",{href:!0});var gc=c(Pt);Ps=s(gc,"Speech2Text2Tokenizer"),gc.forEach(o),zs=s(V,` decodes the generated target tokens to the target string. The
`),zt=n(V,"A",{href:!0});var vc=c(zt);Cs=s(vc,"Speech2Text2Processor"),vc.forEach(o),qs=s(V," wraps "),Ct=n(V,"A",{href:!0});var Tc=c(Ct);As=s(Tc,"Wav2Vec2FeatureExtractor"),Tc.forEach(o),js=s(V,` and
`),qt=n(V,"A",{href:!0});var xc=c(qt);Ms=s(xc,"Speech2Text2Tokenizer"),xc.forEach(o),Ls=s(V,` into a single instance to both extract the input features and decode the
predicted token ids.`),V.forEach(o),Go=d(t),At=n(t,"UL",{});var kc=c(At);po=n(kc,"LI",{});var bc=c(po);Fs=s(bc,"Step-by-step Speech Translation"),bc.forEach(o),kc.forEach(o),Ko=d(t),u(De.$$.fragment,t),Qo=d(t),jt=n(t,"UL",{});var Sc=c(jt);Ie=n(Sc,"LI",{});var ur=c(Ie);fo=n(ur,"P",{});var wc=c(fo);Ds=s(wc,"Speech Translation via Pipelines"),wc.forEach(o),Is=d(ur),mo=n(ur,"P",{});var yc=c(mo);Ws=s(yc,"The automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code"),yc.forEach(o),ur.forEach(o),Sc.forEach(o),Xo=d(t),u(We.$$.fragment,t),Yo=d(t),he=n(t,"P",{});var _r=c(he);Vs=s(_r,"See "),Ve=n(_r,"A",{href:!0,rel:!0});var $c=c(Ve);Ns=s($c,"model hub"),$c.forEach(o),Bs=s(_r," to look for Speech2Text2 checkpoints."),_r.forEach(o),Zo=d(t),te=n(t,"H2",{class:!0});var gr=c(te);pe=n(gr,"A",{id:!0,class:!0,href:!0});var Ec=c(pe);uo=n(Ec,"SPAN",{});var Pc=c(uo);u(Ne.$$.fragment,Pc),Pc.forEach(o),Ec.forEach(o),Os=d(gr),_o=n(gr,"SPAN",{});var zc=c(_o);Us=s(zc,"Speech2Text2Config"),zc.forEach(o),gr.forEach(o),er=d(t),q=n(t,"DIV",{class:!0});var Z=c(q);u(Be.$$.fragment,Z),Rs=d(Z),oe=n(Z,"P",{});var Xt=c(oe);Hs=s(Xt,"This is the configuration class to store the configuration of a "),Mt=n(Xt,"A",{href:!0});var Cc=c(Mt);Js=s(Cc,"Speech2Text2ForCausalLM"),Cc.forEach(o),Gs=s(Xt,`. It
is used to instantiate an Speech2Text2 model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text2
`),Oe=n(Xt,"A",{href:!0,rel:!0});var qc=c(Oe);Ks=s(qc,"facebook/s2t-small-librispeech-asr"),qc.forEach(o),Qs=s(Xt," architecture."),Xt.forEach(o),Xs=d(Z),re=n(Z,"P",{});var Yt=c(re);Ys=s(Yt,"Configuration objects inherit from "),Lt=n(Yt,"A",{href:!0});var Ac=c(Lt);Zs=s(Ac,"PretrainedConfig"),Ac.forEach(o),ea=s(Yt,` and can be used to control the model
outputs. Read the documentation from `),Ft=n(Yt,"A",{href:!0});var jc=c(Ft);ta=s(jc,"PretrainedConfig"),jc.forEach(o),oa=s(Yt," for more information."),Yt.forEach(o),ra=d(Z),go=n(Z,"P",{});var Mc=c(go);sa=s(Mc,"Example:"),Mc.forEach(o),aa=d(Z),u(Ue.$$.fragment,Z),Z.forEach(o),tr=d(t),se=n(t,"H2",{class:!0});var vr=c(se);fe=n(vr,"A",{id:!0,class:!0,href:!0});var Lc=c(fe);vo=n(Lc,"SPAN",{});var Fc=c(vo);u(Re.$$.fragment,Fc),Fc.forEach(o),Lc.forEach(o),na=d(vr),To=n(vr,"SPAN",{});var Dc=c(To);ca=s(Dc,"Speech2TextTokenizer"),Dc.forEach(o),vr.forEach(o),or=d(t),E=n(t,"DIV",{class:!0});var N=c(E);u(He.$$.fragment,N),ia=d(N),xo=n(N,"P",{});var Ic=c(xo);la=s(Ic,"Constructs a Speech2Text2Tokenizer."),Ic.forEach(o),da=d(N),Je=n(N,"P",{});var Tr=c(Je);ha=s(Tr,"This tokenizer inherits from "),Dt=n(Tr,"A",{href:!0});var Wc=c(Dt);pa=s(Wc,"PreTrainedTokenizer"),Wc.forEach(o),fa=s(Tr,` which contains some of the main methods.
Users should refer to the superclass for more information regarding such methods.`),Tr.forEach(o),ma=d(N),me=n(N,"DIV",{class:!0});var xr=c(me);u(Ge.$$.fragment,xr),ua=d(xr),ko=n(xr,"P",{});var Vc=c(ko);_a=s(Vc,"Convert a list of lists of token ids into a list of strings by calling decode."),Vc.forEach(o),xr.forEach(o),ga=d(N),K=n(N,"DIV",{class:!0});var Zt=c(K);u(Ke.$$.fragment,Zt),va=d(Zt),bo=n(Zt,"P",{});var Nc=c(bo);Ta=s(Nc,`Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special
tokens and clean up tokenization spaces.`),Nc.forEach(o),xa=d(Zt),Qe=n(Zt,"P",{});var kr=c(Qe);ka=s(kr,"Similar to doing "),So=n(kr,"CODE",{});var Bc=c(So);ba=s(Bc,"self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))"),Bc.forEach(o),Sa=s(kr,"."),kr.forEach(o),Zt.forEach(o),wa=d(N),wo=n(N,"DIV",{class:!0}),c(wo).forEach(o),N.forEach(o),rr=d(t),ae=n(t,"H2",{class:!0});var br=c(ae);ue=n(br,"A",{id:!0,class:!0,href:!0});var Oc=c(ue);yo=n(Oc,"SPAN",{});var Uc=c(yo);u(Xe.$$.fragment,Uc),Uc.forEach(o),Oc.forEach(o),ya=d(br),$o=n(br,"SPAN",{});var Rc=c($o);$a=s(Rc,"Speech2Text2Processor"),Rc.forEach(o),br.forEach(o),sr=d(t),k=n(t,"DIV",{class:!0});var $=c(k);u(Ye.$$.fragment,$),Ea=d($),Eo=n($,"P",{});var Hc=c(Eo);Pa=s(Hc,`Constructs a Speech2Text2 processor which wraps a Speech2Text2 feature extractor and a Speech2Text2 tokenizer into
a single processor.`),Hc.forEach(o),za=d($),M=n($,"P",{});var R=c(M);It=n(R,"A",{href:!0});var Jc=c(It);Ca=s(Jc,"Speech2Text2Processor"),Jc.forEach(o),qa=s(R,` offers all the functionalities of
`),Wt=n(R,"A",{href:!0});var Gc=c(Wt);Aa=s(Gc,"AutoFeatureExtractor"),Gc.forEach(o),ja=s(R," and "),Vt=n(R,"A",{href:!0});var Kc=c(Vt);Ma=s(Kc,"Speech2Text2Tokenizer"),Kc.forEach(o),La=s(R,`. See the
`),Ze=n(R,"A",{href:!0});var Un=c(Ze);Po=n(Un,"STRONG",{});var Qc=c(Po);Fa=s(Qc,"call"),Qc.forEach(o),Da=s(Un,"()"),Un.forEach(o),Ia=s(R," and "),Nt=n(R,"A",{href:!0});var Xc=c(Nt);Wa=s(Xc,"decode()"),Xc.forEach(o),Va=s(R,` for
more information.`),R.forEach(o),Na=d($),_e=n($,"DIV",{class:!0});var Sr=c(_e);u(et.$$.fragment,Sr),Ba=d(Sr),O=n(Sr,"P",{});var Se=c(O);Oa=s(Se,`When used in normal mode, this method forwards all its arguments to AutoFeatureExtractor\u2019s
`),zo=n(Se,"CODE",{});var Yc=c(zo);Ua=s(Yc,"__call__()"),Yc.forEach(o),Ra=s(Se,` and returns its output. If used in the context
`),Bt=n(Se,"A",{href:!0});var Zc=c(Bt);Ha=s(Zc,"as_target_processor()"),Zc.forEach(o),Ja=s(Se,` this method forwards all its arguments to
Speech2Text2Tokenizer\u2019s `),tt=n(Se,"A",{href:!0});var Rn=c(tt);Co=n(Rn,"STRONG",{});var ei=c(Co);Ga=s(ei,"call"),ei.forEach(o),Ka=s(Rn,"()"),Rn.forEach(o),Qa=s(Se,`. Please refer to the doctsring of
the above two methods for more information.`),Se.forEach(o),Sr.forEach(o),Xa=d($),Q=n($,"DIV",{class:!0});var eo=c(Q);u(ot.$$.fragment,eo),Ya=d(eo),rt=n(eo,"P",{});var wr=c(rt);Za=s(wr,"Instantiate a "),Ot=n(wr,"A",{href:!0});var ti=c(Ot);en=s(ti,"Speech2Text2Processor"),ti.forEach(o),tn=s(wr," from a pretrained Speech2Text2 processor."),wr.forEach(o),on=d(eo),u(ge.$$.fragment,eo),eo.forEach(o),rn=d($),X=n($,"DIV",{class:!0});var to=c(X);u(st.$$.fragment,to),sn=d(to),ne=n(to,"P",{});var oo=c(ne);an=s(oo,`Save a Speech2Text2 feature extractor object and Speech2Text2 tokenizer object to the directory
`),qo=n(oo,"CODE",{});var oi=c(qo);nn=s(oi,"save_directory"),oi.forEach(o),cn=s(oo,`, so that it can be re-loaded using the
`),Ut=n(oo,"A",{href:!0});var ri=c(Ut);ln=s(ri,"from_pretrained()"),ri.forEach(o),dn=s(oo," class method."),oo.forEach(o),hn=d(to),u(ve.$$.fragment,to),to.forEach(o),pn=d($),Te=n($,"DIV",{class:!0});var yr=c(Te);u(at.$$.fragment,yr),fn=d(yr),nt=n(yr,"P",{});var $r=c(nt);mn=s($r,`This method forwards all its arguments to Speech2Text2Tokenizer\u2019s
`),Rt=n($r,"A",{href:!0});var si=c(Rt);un=s(si,"batch_decode()"),si.forEach(o),_n=s($r,`. Please refer to the docstring of this method for more
information.`),$r.forEach(o),yr.forEach(o),gn=d($),xe=n($,"DIV",{class:!0});var Er=c(xe);u(ct.$$.fragment,Er),vn=d(Er),it=n(Er,"P",{});var Pr=c(it);Tn=s(Pr,`This method forwards all its arguments to Speech2Text2Tokenizer\u2019s
`),Ht=n(Pr,"A",{href:!0});var ai=c(Ht);xn=s(ai,"decode()"),ai.forEach(o),kn=s(Pr,`. Please refer to the docstring of this method for more
information.`),Pr.forEach(o),Er.forEach(o),bn=d($),ke=n($,"DIV",{class:!0});var zr=c(ke);u(lt.$$.fragment,zr),Sn=d(zr),Ao=n(zr,"P",{});var ni=c(Ao);wn=s(ni,`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text2.`),ni.forEach(o),zr.forEach(o),$.forEach(o),ar=d(t),ce=n(t,"H2",{class:!0});var Cr=c(ce);be=n(Cr,"A",{id:!0,class:!0,href:!0});var ci=c(be);jo=n(ci,"SPAN",{});var ii=c(jo);u(dt.$$.fragment,ii),ii.forEach(o),ci.forEach(o),yn=d(Cr),Mo=n(Cr,"SPAN",{});var li=c(Mo);$n=s(li,"Speech2Text2ForCausalLM"),li.forEach(o),Cr.forEach(o),nr=d(t),I=n(t,"DIV",{class:!0});var we=c(I);u(ht.$$.fragment,we),En=d(we),U=n(we,"P",{});var ye=c(U);Pn=s(ye,"The Speech2Text2 Decoder with a language modeling head. Can be used as the decoder part of "),Jt=n(ye,"A",{href:!0});var di=c(Jt);zn=s(di,"EncoderDecoderModel"),di.forEach(o),Cn=s(ye," and "),Lo=n(ye,"CODE",{});var hi=c(Lo);qn=s(hi,"SpeechEncoderDecoder"),hi.forEach(o),An=s(ye,`.
This model inherits from [`),Fo=n(ye,"EM",{});var pi=c(Fo);jn=s(pi,"PreTrainedModel"),pi.forEach(o),Mn=s(ye,`]. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ye.forEach(o),Ln=d(we),pt=n(we,"P",{});var qr=c(pt);Fn=s(qr,"This model is also a PyTorch "),ft=n(qr,"A",{href:!0,rel:!0});var fi=c(ft);Dn=s(fi,"torch.nn.Module"),fi.forEach(o),In=s(qr,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),qr.forEach(o),Wn=d(we),Y=n(we,"DIV",{class:!0});var ro=c(Y);u(mt.$$.fragment,ro),Vn=d(ro),Do=n(ro,"P",{});var mi=c(Do);Nn=s(mi,"Example:"),mi.forEach(o),Bn=d(ro),u(ut.$$.fragment,ro),ro.forEach(o),we.forEach(o),this.h()},h(){i(f,"name","hf:doc:metadata"),i(f,"content",JSON.stringify(Si)),i(S,"id","speech2text2"),i(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(S,"href","#speech2text2"),i(x,"class","relative group"),i(j,"id","overview"),i(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(j,"href","#overview"),i(C,"class","relative group"),i(vt,"href","/docs/transformers/v4.15.0/en/wav2vec2"),i(Pe,"href","https://arxiv.org/abs/2104.06678"),i(Pe,"rel","nofollow"),i(Tt,"href","/docs/transformers/v4.15.0/en/wav2vec2"),i(xt,"href","/docs/transformers/v4.15.0/en/hubert"),i(kt,"href","/docs/transformers/v4.15.0/en/speechencoderdecoder"),i(ze,"href","https://huggingface.co/patrickvonplaten"),i(ze,"rel","nofollow"),i(Ce,"href","https://github.com/pytorch/fairseq/blob/1f7ef9ed1e1061f8c7f88f8b94c7186834398690/fairseq/models/wav2vec/wav2vec2_asr.py#L266"),i(Ce,"rel","nofollow"),i(Ae,"href","https://huggingface.co/models?other=speech2text2"),i(Ae,"rel","nofollow"),i(St,"href","/docs/transformers/v4.15.0/en/speechencoderdecoder"),i(Le,"href","https://github.com/glample/fastBPE%3E"),i(Le,"rel","nofollow"),i(de,"id","inference"),i(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(de,"href","#inference"),i(ee,"class","relative group"),i(yt,"href","/docs/transformers/v4.15.0/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderModel"),i($t,"href","/docs/transformers/v4.15.0/en/main_classes/model#transformers.generation_utils.GenerationMixin.generate"),i(Et,"href","/docs/transformers/v4.15.0/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),i(Pt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),i(zt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),i(Ct,"href","/docs/transformers/v4.15.0/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),i(qt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),i(Ve,"href","https://huggingface.co/models?filter=speech2text2"),i(Ve,"rel","nofollow"),i(pe,"id","transformers.Speech2Text2Config"),i(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(pe,"href","#transformers.Speech2Text2Config"),i(te,"class","relative group"),i(Mt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),i(Oe,"href","https://huggingface.co/facebook/s2t-small-librispeech-asr"),i(Oe,"rel","nofollow"),i(Lt,"href","/docs/transformers/v4.15.0/en/main_classes/configuration#transformers.PretrainedConfig"),i(Ft,"href","/docs/transformers/v4.15.0/en/main_classes/configuration#transformers.PretrainedConfig"),i(q,"class","docstring"),i(fe,"id","transformers.Speech2Text2Tokenizer"),i(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(fe,"href","#transformers.Speech2Text2Tokenizer"),i(se,"class","relative group"),i(Dt,"href","/docs/transformers/v4.15.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(me,"class","docstring"),i(K,"class","docstring"),i(wo,"class","docstring"),i(E,"class","docstring"),i(ue,"id","transformers.Speech2Text2Processor"),i(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ue,"href","#transformers.Speech2Text2Processor"),i(ae,"class","relative group"),i(It,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),i(Wt,"href","/docs/transformers/v4.15.0/en/model_doc/auto#transformers.AutoFeatureExtractor"),i(Vt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),i(Ze,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.__call__"),i(Nt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.decode"),i(Bt,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.as_target_processor"),i(tt,"href","/docs/transformers/v4.15.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"),i(_e,"class","docstring"),i(Ot,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),i(Q,"class","docstring"),i(Ut,"href","/docs/transformers/v4.15.0/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor.from_pretrained"),i(X,"class","docstring"),i(Rt,"href","/docs/transformers/v4.15.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),i(Te,"class","docstring"),i(Ht,"href","/docs/transformers/v4.15.0/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),i(xe,"class","docstring"),i(ke,"class","docstring"),i(k,"class","docstring"),i(be,"id","transformers.Speech2Text2ForCausalLM"),i(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(be,"href","#transformers.Speech2Text2ForCausalLM"),i(ce,"class","relative group"),i(Jt,"href","/docs/transformers/v4.15.0/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),i(ft,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(ft,"rel","nofollow"),i(Y,"class","docstring"),i(I,"class","docstring")},m(t,h){e(document.head,f),p(t,A,h),p(t,x,h),e(x,S),e(S,F),_(w,F,null),e(x,B),e(x,D),e(D,z),p(t,b,h),p(t,C,h),e(C,j),e(j,so),_(Ee,so,null),e(C,jr),e(C,ao),e(ao,Mr),p(t,Wo,h),p(t,H,h),e(H,Lr),e(H,vt),e(vt,Fr),e(H,Dr),e(H,Pe),e(Pe,Ir),e(H,Wr),p(t,Vo,h),p(t,y,h),e(y,Vr),e(y,no),e(no,Nr),e(y,Br),e(y,co),e(co,Or),e(y,Ur),e(y,Tt),e(Tt,Rr),e(y,Hr),e(y,xt),e(xt,Jr),e(y,Gr),e(y,kt),e(kt,Kr),e(y,Qr),e(y,io),e(io,Xr),e(y,Yr),p(t,No,h),p(t,ie,h),e(ie,Zr),e(ie,ze),e(ze,es),e(ie,ts),p(t,Bo,h),p(t,le,h),e(le,os),e(le,Ce),e(Ce,rs),e(le,ss),p(t,Oo,h),p(t,bt,h),e(bt,as),p(t,Uo,h),p(t,J,h),e(J,qe),e(qe,ns),e(qe,Ae),e(Ae,cs),e(qe,is),e(J,ls),e(J,je),e(je,ds),e(je,St),e(St,hs),e(je,ps),e(J,fs),e(J,Me),e(Me,ms),e(Me,wt),e(wt,us),e(wt,Le),e(Le,_s),e(Me,gs),p(t,Ro,h),p(t,ee,h),e(ee,de),e(de,lo),_(Fe,lo,null),e(ee,vs),e(ee,ho),e(ho,Ts),p(t,Ho,h),p(t,G,h),e(G,xs),e(G,yt),e(yt,ks),e(G,bs),e(G,$t),e($t,Ss),e(G,ws),p(t,Jo,h),p(t,P,h),e(P,ys),e(P,Et),e(Et,$s),e(P,Es),e(P,Pt),e(Pt,Ps),e(P,zs),e(P,zt),e(zt,Cs),e(P,qs),e(P,Ct),e(Ct,As),e(P,js),e(P,qt),e(qt,Ms),e(P,Ls),p(t,Go,h),p(t,At,h),e(At,po),e(po,Fs),p(t,Ko,h),_(De,t,h),p(t,Qo,h),p(t,jt,h),e(jt,Ie),e(Ie,fo),e(fo,Ds),e(Ie,Is),e(Ie,mo),e(mo,Ws),p(t,Xo,h),_(We,t,h),p(t,Yo,h),p(t,he,h),e(he,Vs),e(he,Ve),e(Ve,Ns),e(he,Bs),p(t,Zo,h),p(t,te,h),e(te,pe),e(pe,uo),_(Ne,uo,null),e(te,Os),e(te,_o),e(_o,Us),p(t,er,h),p(t,q,h),_(Be,q,null),e(q,Rs),e(q,oe),e(oe,Hs),e(oe,Mt),e(Mt,Js),e(oe,Gs),e(oe,Oe),e(Oe,Ks),e(oe,Qs),e(q,Xs),e(q,re),e(re,Ys),e(re,Lt),e(Lt,Zs),e(re,ea),e(re,Ft),e(Ft,ta),e(re,oa),e(q,ra),e(q,go),e(go,sa),e(q,aa),_(Ue,q,null),p(t,tr,h),p(t,se,h),e(se,fe),e(fe,vo),_(Re,vo,null),e(se,na),e(se,To),e(To,ca),p(t,or,h),p(t,E,h),_(He,E,null),e(E,ia),e(E,xo),e(xo,la),e(E,da),e(E,Je),e(Je,ha),e(Je,Dt),e(Dt,pa),e(Je,fa),e(E,ma),e(E,me),_(Ge,me,null),e(me,ua),e(me,ko),e(ko,_a),e(E,ga),e(E,K),_(Ke,K,null),e(K,va),e(K,bo),e(bo,Ta),e(K,xa),e(K,Qe),e(Qe,ka),e(Qe,So),e(So,ba),e(Qe,Sa),e(E,wa),e(E,wo),p(t,rr,h),p(t,ae,h),e(ae,ue),e(ue,yo),_(Xe,yo,null),e(ae,ya),e(ae,$o),e($o,$a),p(t,sr,h),p(t,k,h),_(Ye,k,null),e(k,Ea),e(k,Eo),e(Eo,Pa),e(k,za),e(k,M),e(M,It),e(It,Ca),e(M,qa),e(M,Wt),e(Wt,Aa),e(M,ja),e(M,Vt),e(Vt,Ma),e(M,La),e(M,Ze),e(Ze,Po),e(Po,Fa),e(Ze,Da),e(M,Ia),e(M,Nt),e(Nt,Wa),e(M,Va),e(k,Na),e(k,_e),_(et,_e,null),e(_e,Ba),e(_e,O),e(O,Oa),e(O,zo),e(zo,Ua),e(O,Ra),e(O,Bt),e(Bt,Ha),e(O,Ja),e(O,tt),e(tt,Co),e(Co,Ga),e(tt,Ka),e(O,Qa),e(k,Xa),e(k,Q),_(ot,Q,null),e(Q,Ya),e(Q,rt),e(rt,Za),e(rt,Ot),e(Ot,en),e(rt,tn),e(Q,on),_(ge,Q,null),e(k,rn),e(k,X),_(st,X,null),e(X,sn),e(X,ne),e(ne,an),e(ne,qo),e(qo,nn),e(ne,cn),e(ne,Ut),e(Ut,ln),e(ne,dn),e(X,hn),_(ve,X,null),e(k,pn),e(k,Te),_(at,Te,null),e(Te,fn),e(Te,nt),e(nt,mn),e(nt,Rt),e(Rt,un),e(nt,_n),e(k,gn),e(k,xe),_(ct,xe,null),e(xe,vn),e(xe,it),e(it,Tn),e(it,Ht),e(Ht,xn),e(it,kn),e(k,bn),e(k,ke),_(lt,ke,null),e(ke,Sn),e(ke,Ao),e(Ao,wn),p(t,ar,h),p(t,ce,h),e(ce,be),e(be,jo),_(dt,jo,null),e(ce,yn),e(ce,Mo),e(Mo,$n),p(t,nr,h),p(t,I,h),_(ht,I,null),e(I,En),e(I,U),e(U,Pn),e(U,Jt),e(Jt,zn),e(U,Cn),e(U,Lo),e(Lo,qn),e(U,An),e(U,Fo),e(Fo,jn),e(U,Mn),e(I,Ln),e(I,pt),e(pt,Fn),e(pt,ft),e(ft,Dn),e(pt,In),e(I,Wn),e(I,Y),_(mt,Y,null),e(Y,Vn),e(Y,Do),e(Do,Nn),e(Y,Bn),_(ut,Y,null),cr=!0},p(t,[h]){const _t={};h&2&&(_t.$$scope={dirty:h,ctx:t}),ge.$set(_t);const Io={};h&2&&(Io.$$scope={dirty:h,ctx:t}),ve.$set(Io)},i(t){cr||(g(w.$$.fragment,t),g(Ee.$$.fragment,t),g(Fe.$$.fragment,t),g(De.$$.fragment,t),g(We.$$.fragment,t),g(Ne.$$.fragment,t),g(Be.$$.fragment,t),g(Ue.$$.fragment,t),g(Re.$$.fragment,t),g(He.$$.fragment,t),g(Ge.$$.fragment,t),g(Ke.$$.fragment,t),g(Xe.$$.fragment,t),g(Ye.$$.fragment,t),g(et.$$.fragment,t),g(ot.$$.fragment,t),g(ge.$$.fragment,t),g(st.$$.fragment,t),g(ve.$$.fragment,t),g(at.$$.fragment,t),g(ct.$$.fragment,t),g(lt.$$.fragment,t),g(dt.$$.fragment,t),g(ht.$$.fragment,t),g(mt.$$.fragment,t),g(ut.$$.fragment,t),cr=!0)},o(t){v(w.$$.fragment,t),v(Ee.$$.fragment,t),v(Fe.$$.fragment,t),v(De.$$.fragment,t),v(We.$$.fragment,t),v(Ne.$$.fragment,t),v(Be.$$.fragment,t),v(Ue.$$.fragment,t),v(Re.$$.fragment,t),v(He.$$.fragment,t),v(Ge.$$.fragment,t),v(Ke.$$.fragment,t),v(Xe.$$.fragment,t),v(Ye.$$.fragment,t),v(et.$$.fragment,t),v(ot.$$.fragment,t),v(ge.$$.fragment,t),v(st.$$.fragment,t),v(ve.$$.fragment,t),v(at.$$.fragment,t),v(ct.$$.fragment,t),v(lt.$$.fragment,t),v(dt.$$.fragment,t),v(ht.$$.fragment,t),v(mt.$$.fragment,t),v(ut.$$.fragment,t),cr=!1},d(t){o(f),t&&o(A),t&&o(x),T(w),t&&o(b),t&&o(C),T(Ee),t&&o(Wo),t&&o(H),t&&o(Vo),t&&o(y),t&&o(No),t&&o(ie),t&&o(Bo),t&&o(le),t&&o(Oo),t&&o(bt),t&&o(Uo),t&&o(J),t&&o(Ro),t&&o(ee),T(Fe),t&&o(Ho),t&&o(G),t&&o(Jo),t&&o(P),t&&o(Go),t&&o(At),t&&o(Ko),T(De,t),t&&o(Qo),t&&o(jt),t&&o(Xo),T(We,t),t&&o(Yo),t&&o(he),t&&o(Zo),t&&o(te),T(Ne),t&&o(er),t&&o(q),T(Be),T(Ue),t&&o(tr),t&&o(se),T(Re),t&&o(or),t&&o(E),T(He),T(Ge),T(Ke),t&&o(rr),t&&o(ae),T(Xe),t&&o(sr),t&&o(k),T(Ye),T(et),T(ot),T(ge),T(st),T(ve),T(at),T(ct),T(lt),t&&o(ar),t&&o(ce),T(dt),t&&o(nr),t&&o(I),T(ht),T(mt),T(ut)}}}const Si={local:"speech2text2",sections:[{local:"overview",title:"Overview"},{local:"inference",title:"Inference"},{local:"transformers.Speech2Text2Config",title:"Speech2Text2Config"},{local:"transformers.Speech2Text2Tokenizer",title:"Speech2TextTokenizer"},{local:"transformers.Speech2Text2Processor",title:"Speech2Text2Processor"},{local:"transformers.Speech2Text2ForCausalLM",title:"Speech2Text2ForCausalLM"}],title:"Speech2Text2"};function wi($e,f,A){let{fw:x}=f;return $e.$$set=S=>{"fw"in S&&A(0,x=S.fw)},[x]}class qi extends _i{constructor(f){super();gi(this,f,wi,bi,vi,{fw:0})}}export{qi as default,Si as metadata};
