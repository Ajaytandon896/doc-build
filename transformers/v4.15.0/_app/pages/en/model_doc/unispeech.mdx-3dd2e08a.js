import{S as vi,i as bi,s as wi,e as n,k as l,w as _,t as r,L as yi,c as a,d as o,m as d,a as s,x as v,h as i,b as c,J as e,g as m,y as b,q as w,o as y,B as S}from"../../../chunks/vendor-b1433968.js";import{T as vn}from"../../../chunks/Tip-c3840994.js";import{D as W}from"../../../chunks/Docstring-ff504c58.js";import{C as To}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as Te}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function Si(A){let h,k,u,T,U;return{c(){h=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){m(f,h,g),e(h,k),e(h,u),e(u,T),e(h,U)},d(f){f&&o(h)}}}function Ti(A){let h,k,u,T,U;return{c(){h=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){m(f,h,g),e(h,k),e(h,u),e(u,T),e(h,U)},d(f){f&&o(h)}}}function ki(A){let h,k,u,T,U;return{c(){h=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){m(f,h,g),e(h,k),e(h,u),e(u,T),e(h,U)},d(f){f&&o(h)}}}function Ui(A){let h,k,u,T,U;return{c(){h=n("p"),k=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),u=n("code"),T=r("Module"),U=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);k=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(o),U=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(o)},m(f,g){m(f,h,g),e(h,k),e(h,u),e(u,T),e(h,U)},d(f){f&&o(h)}}}function $i(A){let h,k,u,T,U,f,g,$,bn,ko,R,de,Ht,ke,wn,Kt,yn,Uo,pe,Sn,Ue,Tn,kn,$o,yt,Un,Co,St,Yt,$n,jo,Tt,Cn,xo,he,$e,jn,kt,xn,Fn,qn,Ce,Pn,Ut,En,Mn,Fo,O,zn,je,Dn,An,xe,Wn,On,qo,Q,me,Rt,Fe,Ln,Qt,Nn,Po,j,qe,Vn,X,In,$t,Bn,Hn,Pe,Kn,Yn,Rn,Z,Qn,Ct,Xn,Zn,jt,Jn,Gn,ea,Xt,ta,oa,Ee,Eo,J,ue,Zt,Me,na,Jt,aa,Mo,G,ze,sa,De,ra,Gt,ia,ca,zo,ee,Ae,la,We,da,eo,pa,ha,Do,te,fe,to,Oe,ma,oo,ua,Ao,x,Le,fa,Ne,ga,Ve,_a,va,ba,Ie,wa,xt,ya,Sa,Ta,Be,ka,He,Ua,$a,Ca,P,Ke,ja,oe,xa,Ft,Fa,qa,no,Pa,Ea,Ma,ge,za,ao,Da,Aa,Ye,Wo,ne,_e,so,Re,Wa,ro,Oa,Oo,F,Qe,La,ae,Na,io,Va,Ia,Xe,Ba,Ha,Ka,Ze,Ya,qt,Ra,Qa,Xa,Je,Za,Ge,Ja,Ga,es,E,et,ts,se,os,Pt,ns,as,co,ss,rs,is,ve,cs,lo,ls,ds,tt,Lo,re,be,po,ot,ps,ho,hs,No,C,nt,ms,mo,us,fs,at,gs,st,_s,vs,bs,rt,ws,Et,ys,Ss,Ts,it,ks,ct,Us,$s,Cs,M,lt,js,ie,xs,Mt,Fs,qs,uo,Ps,Es,Ms,we,zs,fo,Ds,As,dt,Vo,ce,ye,go,pt,Ws,_o,Os,Io,q,ht,Ls,mt,Ns,ut,Vs,Is,Bs,ft,Hs,zt,Ks,Ys,Rs,gt,Qs,_t,Xs,Zs,Js,z,vt,Gs,le,er,Dt,tr,or,vo,nr,ar,sr,Se,rr,bo,ir,cr,bt,Bo;return f=new Te({}),ke=new Te({}),Fe=new Te({}),qe=new W({props:{name:"class transformers.UniSpeechConfig",anchor:"transformers.UniSpeechConfig",parameters:[{name:"vocab_size",val:" = 32"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"feat_proj_dropout",val:" = 0.0"},{name:"feat_quantizer_dropout",val:" = 0.0"},{name:"final_dropout",val:" = 0.1"},{name:"layerdrop",val:" = 0.1"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"feat_extract_norm",val:" = 'group'"},{name:"feat_extract_activation",val:" = 'gelu'"},{name:"conv_dim",val:" = (512, 512, 512, 512, 512, 512, 512)"},{name:"conv_stride",val:" = (5, 2, 2, 2, 2, 2, 2)"},{name:"conv_kernel",val:" = (10, 3, 3, 3, 3, 2, 2)"},{name:"conv_bias",val:" = False"},{name:"num_conv_pos_embeddings",val:" = 128"},{name:"num_conv_pos_embedding_groups",val:" = 16"},{name:"do_stable_layer_norm",val:" = False"},{name:"apply_spec_augment",val:" = True"},{name:"mask_time_prob",val:" = 0.05"},{name:"mask_time_length",val:" = 10"},{name:"mask_time_min_masks",val:" = 2"},{name:"mask_feature_prob",val:" = 0.0"},{name:"mask_feature_length",val:" = 10"},{name:"mask_feature_min_masks",val:" = 0"},{name:"num_codevectors_per_group",val:" = 320"},{name:"num_codevector_groups",val:" = 2"},{name:"contrastive_logits_temperature",val:" = 0.1"},{name:"num_negatives",val:" = 100"},{name:"codevector_dim",val:" = 256"},{name:"proj_codevector_dim",val:" = 256"},{name:"diversity_loss_weight",val:" = 0.1"},{name:"ctc_loss_reduction",val:" = 'mean'"},{name:"ctc_zero_infinity",val:" = False"},{name:"use_weighted_layer_sum",val:" = False"},{name:"classifier_proj_size",val:" = 256"},{name:"num_ctc_classes",val:" = 80"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"replace_prob",val:" = 0.5"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/configuration_unispeech.py#L29",parametersDescription:[{anchor:"transformers.UniSpeechConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Vocabulary size of the UniSpeech model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a>. Vocabulary size of the
model. Defines the different tokens that can be represented by the <em>inputs_ids</em> passed to the forward
method of <a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a>.`,name:"vocab_size"},{anchor:"transformers.UniSpeechConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.UniSpeechConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.UniSpeechConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.UniSpeechConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.UniSpeechConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.UniSpeechConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.UniSpeechConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.UniSpeechConfig.final_dropout",description:`<strong>final_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the final projection layer of <a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"final_dropout"},{anchor:"transformers.UniSpeechConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.UniSpeechConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.UniSpeechConfig.feat_extract_norm",description:`<strong>feat_extract_norm</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;group&quot;</code>) &#x2014;
The norm to be applied to 1D convolutional layers in feature extractor. One of <code>&quot;group&quot;</code> for group
normalization of only the first 1D convolutional layer or <code>&quot;layer&quot;</code> for layer normalization of all 1D
convolutional layers.`,name:"feat_extract_norm"},{anchor:"transformers.UniSpeechConfig.feat_proj_dropout",description:`<strong>feat_proj_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for output of the feature extractor.`,name:"feat_proj_dropout"},{anchor:"transformers.UniSpeechConfig.feat_extract_activation",description:"<strong>feat_extract_activation</strong> (<code>str, </code>optional<code>, defaults to </code>&#x201C;gelu&#x201D;<code>) -- The non-linear activation function (function or string) in the 1D convolutional layers of the feature extractor. If string, </code>&#x201C;gelu&#x201D;<code>, </code>&#x201C;relu&#x201D;<code>, </code>&#x201C;selu&#x201D;<code>and</code>&#x201C;gelu_new&#x201D;` are supported.",name:"feat_extract_activation"},{anchor:"transformers.UniSpeechConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (obj &#x2014;<em>float</em>, <em>optional</em>, defaults to 0.0):
The dropout probabilitiy for quantized feature extractor states.`,name:"feat_quantizer_dropout"},{anchor:"transformers.UniSpeechConfig.conv_dim",description:`<strong>conv_dim</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(512, 512, 512, 512, 512, 512, 512)</code>) &#x2014;
A tuple of integers defining the number of input and output channels of each 1D convolutional layer in the
feature extractor. The length of <em>conv_dim</em> defines the number of 1D convolutional layers.`,name:"conv_dim"},{anchor:"transformers.UniSpeechConfig.conv_stride",description:`<strong>conv_stride</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(5, 2, 2, 2, 2, 2, 2)</code>) &#x2014;
A tuple of integers defining the stride of each 1D convolutional layer in the feature extractor. The length
of <em>conv_stride</em> defines the number of convolutional layers and has to match the the length of <em>conv_dim</em>.`,name:"conv_stride"},{anchor:"transformers.UniSpeechConfig.conv_kernel",description:`<strong>conv_kernel</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(10, 3, 3, 3, 3, 3, 3)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the feature extractor. The
length of <em>conv_kernel</em> defines the number of convolutional layers and has to match the the length of
<em>conv_dim</em>.`,name:"conv_kernel"},{anchor:"transformers.UniSpeechConfig.conv_bias",description:`<strong>conv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the 1D convolutional layers have a bias.`,name:"conv_bias"},{anchor:"transformers.UniSpeechConfig.num_conv_pos_embeddings",description:`<strong>num_conv_pos_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of convolutional positional embeddings. Defines the kernel size of 1D convolutional positional
embeddings layer.`,name:"num_conv_pos_embeddings"},{anchor:"transformers.UniSpeechConfig.num_conv_pos_embedding_groups",description:`<strong>num_conv_pos_embedding_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of groups of 1D convolutional positional embeddings layer.`,name:"num_conv_pos_embedding_groups"},{anchor:"transformers.UniSpeechConfig.do_stable_layer_norm",description:`<strong>do_stable_layer_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply <em>stable</em> layer norm architecture of the Transformer encoder. <code>do_stable_layer_norm is True</code> corresponds to applying layer norm before the attention layer, whereas <code>do_stable_layer_norm is False</code> corresponds to applying layer norm after the attention layer.`,name:"do_stable_layer_norm"},{anchor:"transformers.UniSpeechConfig.apply_spec_augment",description:`<strong>apply_spec_augment</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply <em>SpecAugment</em> data augmentation to the outputs of the feature extractor. For reference see
<a href="https://arxiv.org/abs/1904.08779" rel="nofollow">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</a>.`,name:"apply_spec_augment"},{anchor:"transformers.UniSpeechConfig.mask_time_prob",description:`<strong>mask_time_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking
procecure generates &#x201D;mask_time_prob<em>len(time_axis)/mask_time_length&#x201D; independent masks over the axis. If
reasoning from the propability of each feature vector to be chosen as the start of the vector span to be
masked, </em>mask_time_prob<em> should be \`prob_vector_start</em>mask_time_length<code>. Note that overlap may decrease the actual percentage of masked vectors. This is only relevant if </code>apply_spec_augment is True\`.`,name:"mask_time_prob"},{anchor:"transformers.UniSpeechConfig.mask_time_length",description:`<strong>mask_time_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the time axis.`,name:"mask_time_length"},{anchor:"transformers.UniSpeechConfig.mask_time_min_masks",description:`<strong>mask_time_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 2), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the time axis, each time
step, irrespectively of <code>mask_feature_prob</code>. Only relevant if
&#x201D;mask_time_prob*len(time_axis)/mask_time_length &lt; mask_time_min_masks&#x201D;`,name:"mask_time_min_masks"},{anchor:"transformers.UniSpeechConfig.mask_feature_prob",description:`<strong>mask_feature_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The
masking procecure generates &#x201D;mask_feature_prob<em>len(feature_axis)/mask_time_length&#x201D; independent masks over
the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector
span to be masked, </em>mask_feature_prob<em> should be \`prob_vector_start</em>mask_feature_length<code>. Note that overlap may decrease the actual percentage of masked vectors. This is only relevant if </code>apply_spec_augment is True\`.`,name:"mask_feature_prob"},{anchor:"transformers.UniSpeechConfig.mask_feature_length",description:`<strong>mask_feature_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the feature axis.`,name:"mask_feature_length"},{anchor:"transformers.UniSpeechConfig.mask_feature_min_masks",description:`<strong>mask_feature_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 0), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the feature axis, each time
step, irrespectively of <code>mask_feature_prob</code>. Only relevant if
&#x201D;mask_feature_prob*len(feature_axis)/mask_feature_length &lt; mask_feature_min_masks&#x201D;`,name:"mask_feature_min_masks"},{anchor:"transformers.UniSpeechConfig.num_codevectors_per_group",description:`<strong>num_codevectors_per_group</strong> (<code>int</code>, <em>optional</em>, defaults to 320) &#x2014;
Number of entries in each quantization codebook (group).`,name:"num_codevectors_per_group"},{anchor:"transformers.UniSpeechConfig.num_codevector_groups",description:`<strong>num_codevector_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of codevector groups for product codevector quantization.`,name:"num_codevector_groups"},{anchor:"transformers.UniSpeechConfig.contrastive_logits_temperature",description:`<strong>contrastive_logits_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The temperature <em>kappa</em> in the contrastive loss.`,name:"contrastive_logits_temperature"},{anchor:"transformers.UniSpeechConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for the output of the feature extractor that&#x2019;s used by the quantizer.`,name:"feat_quantizer_dropout"},{anchor:"transformers.UniSpeechConfig.num_negatives",description:`<strong>num_negatives</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Number of negative samples for the contrastive loss.`,name:"num_negatives"},{anchor:"transformers.UniSpeechConfig.codevector_dim",description:`<strong>codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the quantized feature vectors.`,name:"codevector_dim"},{anchor:"transformers.UniSpeechConfig.proj_codevector_dim",description:`<strong>proj_codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the final projection of both the quantized and the transformer features.`,name:"proj_codevector_dim"},{anchor:"transformers.UniSpeechConfig.diversity_loss_weight",description:`<strong>diversity_loss_weight</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The weight of the codebook diversity loss component.`,name:"diversity_loss_weight"},{anchor:"transformers.UniSpeechConfig.ctc_loss_reduction",description:`<strong>ctc_loss_reduction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Specifies the reduction to apply to the output of <code>torch.nn.CTCLoss</code>. Only relevant when training an
instance of <a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"ctc_loss_reduction"},{anchor:"transformers.UniSpeechConfig.ctc_zero_infinity",description:`<strong>ctc_zero_infinity</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to zero infinite losses and the associated gradients of <code>torch.nn.CTCLoss</code>. Infinite losses
mainly occur when the inputs are too short to be aligned to the targets. Only relevant when training an
instance of <a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"ctc_zero_infinity"},{anchor:"transformers.UniSpeechConfig.use_weighted_layer_sum",description:`<strong>use_weighted_layer_sum</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a weighted average of layer outputs with learned weights. Only relevant when using an
instance of <a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a>.`,name:"use_weighted_layer_sum"},{anchor:"transformers.UniSpeechConfig.classifier_proj_size",description:`<strong>classifier_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the projection before token mean-pooling for classification.`,name:"classifier_proj_size"},{anchor:"transformers.UniSpeechConfig.replace_prob",description:`<strong>replace_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
Propability that transformer feature is replaced by quantized feature for pretraining.`,name:"replace_prob"}]}}),Ee=new To({props:{code:`from transformers import UniSpeechModel, UniSpeechConfig

# Initializing a UniSpeech facebook/unispeech-base-960h style configuration
configuration = UniSpeechConfig()

# Initializing a model from the facebook/unispeech-base-960h style configuration
model = UniSpeechModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UniSpeechModel, UniSpeechConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a UniSpeech facebook/unispeech-base-960h style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = UniSpeechConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the facebook/unispeech-base-960h style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Me=new Te({}),ze=new W({props:{name:"class transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"extract_features",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L61",parametersDescription:[{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.extract_features",description:`<strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) &#x2014;
Sequence of extracted feature vectors of the last convolutional layer of the model.`,name:"extract_features"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ae=new W({props:{name:"class transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput",anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"projected_states",val:": FloatTensor = None"},{name:"projected_quantized_states",val:": FloatTensor = None"},{name:"codevector_perplexity",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L89",parametersDescription:[{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when model is in train mode, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a href="https://arxiv.org/pdf/2006.11477.pdf" rel="nofollow">official
paper</a> . (classification) loss.`,name:"loss"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.projected_states",description:`<strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.`,name:"projected_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.projected_quantized_states",description:`<strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.`,name:"projected_quantized_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Oe=new Te({}),Le=new W({props:{name:"class transformers.UniSpeechModel",anchor:"transformers.UniSpeechModel",parameters:[{name:"config",val:": UniSpeechConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1046",parametersDescription:[{anchor:"transformers.UniSpeechModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ke=new W({props:{name:"forward",anchor:"transformers.UniSpeechModel.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"mask_time_indices",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1110",parametersDescription:[{anchor:"transformers.UniSpeechModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has
<code>config.return_attention_mask == True</code>. For all models whose processor has
<code>config.return_attention_mask == False</code>, <code>attention_mask</code> should <strong>not</strong> be passed to avoid
degraded performance when doing batched inference. For such models <code>input_values</code> should simply be
padded with 0 and passed without <code>attention_mask</code>. Be aware that these models also yield slightly
different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) \u2014 Sequence of extracted feature vectors of the last convolutional layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new vn({props:{$$slots:{default:[Si]},$$scope:{ctx:A}}}),Ye=new To({props:{code:`from transformers import Wav2Vec2Processor, UniSpeechModel
from datasets import load_dataset

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
sampling_rate = dataset.features["audio"].sampling_rate

processor = Wav2Vec2Processor.from_pretrained('microsoft/unispeech-large-1500h-cv')
model = UniSpeechModel.from_pretrained('microsoft/unispeech-large-1500h-cv')

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, UniSpeechModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechModel.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Re=new Te({}),Qe=new W({props:{name:"class transformers.UniSpeechForCTC",anchor:"transformers.UniSpeechForCTC",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1340",parametersDescription:[{anchor:"transformers.UniSpeechForCTC.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),et=new W({props:{name:"forward",anchor:"transformers.UniSpeechForCTC.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1366",parametersDescription:[{anchor:"transformers.UniSpeechForCTC.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechForCTC.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has
<code>config.return_attention_mask == True</code>. For all models whose processor has
<code>config.return_attention_mask == False</code>, <code>attention_mask</code> should <strong>not</strong> be passed to avoid
degraded performance when doing batched inference. For such models <code>input_values</code> should simply be
padded with 0 and passed without <code>attention_mask</code>. Be aware that these models also yield slightly
different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechForCTC.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechForCTC.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechForCTC.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UniSpeechForCTC.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_length)</code>, <em>optional</em>) &#x2014;
Labels for connectionist temporal classification. Note that <code>target_length</code> has to be smaller or equal to
the sequence length of the output logits. Indices are selected in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>transformers.modeling_outputs.CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new vn({props:{$$slots:{default:[Ti]},$$scope:{ctx:A}}}),tt=new To({props:{code:`from transformers import Wav2Vec2Processor, UniSpeechForCTC
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
sampling_rate = dataset.features["audio"].sampling_rate

processor = Wav2Vec2Processor.from_pretrained('microsoft/unispeech-large-1500h-cv')
model = UniSpeechForCTC.from_pretrained('microsoft/unispeech-large-1500h-cv')

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
logits = model(**inputs).logits
predicted_ids = torch.argmax(logits, dim=-1)

# transcribe speech
transcription = processor.batch_decode(predicted_ids)

# compute loss
with processor.as_target_processor():
    inputs["labels"] = processor(dataset[0]["text"], return_tensors="pt").input_ids

loss = model(**inputs).loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, UniSpeechForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForCTC.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># transcribe speech</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss`}}),ot=new Te({}),nt=new W({props:{name:"class transformers.UniSpeechForSequenceClassification",anchor:"transformers.UniSpeechForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1452",parametersDescription:[{anchor:"transformers.UniSpeechForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),lt=new W({props:{name:"forward",anchor:"transformers.UniSpeechForSequenceClassification.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1481",parametersDescription:[{anchor:"transformers.UniSpeechForSequenceClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has
<code>config.return_attention_mask == True</code>. For all models whose processor has
<code>config.return_attention_mask == False</code>, <code>attention_mask</code> should <strong>not</strong> be passed to avoid
degraded performance when doing batched inference. For such models <code>input_values</code> should simply be
padded with 0 and passed without <code>attention_mask</code>. Be aware that these models also yield slightly
different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new vn({props:{$$slots:{default:[ki]},$$scope:{ctx:A}}}),dt=new To({props:{code:`from transformers import Wav2Vec2FeatureExtractor, UniSpeechForSequenceClassification
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
sampling_rate = dataset.features["audio"].sampling_rate

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/unispeech-large-1500h-cv')
model = UniSpeechForSequenceClassification.from_pretrained('microsoft/unispeech-large-1500h-cv')

# audio file is decoded on the fly
inputs = feature_extractor(dataset[0]["audio"]["array"], return_tensors="pt")
logits = model(**inputs).logits >>> predicted_class_ids = torch.argmax(logits, dim=-1)
predicted_label = model.config.id2label[predicted_class_ids]

# compute loss - target_label is e.g. "down"
target_label = model.config.id2label[0]
inputs["labels"] = torch.tensor([model.config.label2id[target_label]])
loss = model(**inputs).loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2FeatureExtractor, UniSpeechForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits &gt;&gt;&gt; predicted_class_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss - target_label is e.g. &quot;down&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_label = model.config.id2label[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([model.config.label2id[target_label]])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss`}}),pt=new Te({}),ht=new W({props:{name:"class transformers.UniSpeechForPreTraining",anchor:"transformers.UniSpeechForPreTraining",parameters:[{name:"config",val:": UniSpeechConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1169",parametersDescription:[{anchor:"transformers.UniSpeechForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),vt=new W({props:{name:"forward",anchor:"transformers.UniSpeechForPreTraining.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/unispeech/modeling_unispeech.py#L1219",parametersDescription:[{anchor:"transformers.UniSpeechForPreTraining.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<div class="course-tip course-tip-orange bg-gradient-to-br dark:bg-gradient-to-r before:border-orange-500 dark:before:border-orange-800 from-orange-50 dark:from-gray-900 to-white dark:to-gray-950 border border-orange-50 text-orange-700 dark:text-gray-400">
						
<p><code>attention_mask</code> should only be passed if the corresponding processor has
<code>config.return_attention_mask == True</code>. For all models whose processor has
<code>config.return_attention_mask == False</code>, <code>attention_mask</code> should <strong>not</strong> be passed to avoid
degraded performance when doing batched inference. For such models <code>input_values</code> should simply be
padded with 0 and passed without <code>attention_mask</code>. Be aware that these models also yield slightly
different results depending on whether <code>input_values</code> is padded or not.</p>

					</div>`,name:"attention_mask"},{anchor:"transformers.UniSpeechForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UniSpeechForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.UniSpeechForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.15.0/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UniSpeechForPreTraining.forward.mask_time_indices",description:`<strong>mask_time_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict
masked extracted features in <em>config.proj_codevector_dim</em> space.`,name:"mask_time_indices"},{anchor:"transformers.UniSpeechForPreTraining.forward.sampled_negative_indices",description:`<strong>sampled_negative_indices</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length, num_negatives)</code>, <em>optional</em>) &#x2014;
Indices indicating which quantized target vectors are used as negative sampled vectors in contrastive loss.
Required input for pre-training.`,name:"sampled_negative_indices"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when model is in train mode, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a
  href="https://arxiv.org/pdf/2006.11477.pdf"
  rel="nofollow"
>official
paper</a> . (classification) loss.</p>
</li>
<li>
<p><strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) \u2014 Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.</p>
</li>
<li>
<p><strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) \u2014 Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput"
>transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new vn({props:{$$slots:{default:[Ui]},$$scope:{ctx:A}}}),bt=new To({props:{code:`import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining
from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices
from datasets import load_dataset
import soundfile as sf

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("patrickvonplaten/wav2vec2-base")
model = Wav2Vec2ForPreTraining.from_pretrained("patrickvonplaten/wav2vec2-base")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

input_values = feature_extractor(ds["speech"][0], return_tensors="pt").input_values  # Batch size 1

# compute masked indices
batch_size, raw_sequence_length = input_values.shape
sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)
mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=0.2, mask_length=2)

with torch.no_grad():
    outputs = model(input_values, mask_time_indices=mask_time_indices)

# compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
cosine_sim = torch.cosine_similarity(
    outputs.projected_states, outputs.projected_quantized_states, dim=-1
)

# show that cosine similarity is much higher than random
assert cosine_sim[mask_time_indices].mean() > 0.5

# for contrastive loss training model should be put into train mode
model.train()
loss = model(input_values, mask_time_indices=mask_time_indices).loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.wav2vec2.modeling_wav2vec2 <span class="hljs-keyword">import</span> _compute_mask_indices
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/wav2vec2-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ForPreTraining.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/wav2vec2-base&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch


<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;patrickvonplaten/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_values = feature_extractor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_values  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute masked indices</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size, raw_sequence_length = input_values.shape
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=<span class="hljs-number">0.2</span>, mask_length=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(input_values, mask_time_indices=mask_time_indices)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>cosine_sim = torch.cosine_similarity(
<span class="hljs-meta">... </span>    outputs.projected_states, outputs.projected_quantized_states, dim=-<span class="hljs-number">1</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># show that cosine similarity is much higher than random</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> cosine_sim[mask_time_indices].mean() &gt; <span class="hljs-number">0.5</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># for contrastive loss training model should be put into train mode</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(input_values, mask_time_indices=mask_time_indices).loss`}}),{c(){h=n("meta"),k=l(),u=n("h1"),T=n("a"),U=n("span"),_(f.$$.fragment),g=l(),$=n("span"),bn=r("UniSpeech"),ko=l(),R=n("h2"),de=n("a"),Ht=n("span"),_(ke.$$.fragment),wn=l(),Kt=n("span"),yn=r("Overview"),Uo=l(),pe=n("p"),Sn=r("The UniSpeech model was proposed in "),Ue=n("a"),Tn=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),kn=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
Zeng, Xuedong Huang .`),$o=l(),yt=n("p"),Un=r("The abstract from the paper is the following:"),Co=l(),St=n("p"),Yt=n("em"),$n=r(`In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.`),jo=l(),Tt=n("p"),Cn=r("Tips:"),xo=l(),he=n("ul"),$e=n("li"),jn=r(`UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please
use `),kt=n("a"),xn=r("Wav2Vec2Processor"),Fn=r(" for the feature extraction."),qn=l(),Ce=n("li"),Pn=r(`UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be
decoded using `),Ut=n("a"),En=r("Wav2Vec2CTCTokenizer"),Mn=r("."),Fo=l(),O=n("p"),zn=r("This model was contributed by "),je=n("a"),Dn=r("patrickvonplaten"),An=r(`. The Authors\u2019 code can be
found `),xe=n("a"),Wn=r("here"),On=r("."),qo=l(),Q=n("h2"),me=n("a"),Rt=n("span"),_(Fe.$$.fragment),Ln=l(),Qt=n("span"),Nn=r("UniSpeechConfig"),Po=l(),j=n("div"),_(qe.$$.fragment),Vn=l(),X=n("p"),In=r("This is the configuration class to store the configuration of a "),$t=n("a"),Bn=r("UniSpeechModel"),Hn=r(`. It is used
to instantiate an UniSpeech model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the UniSpeech
`),Pe=n("a"),Kn=r("facebook/unispeech-base-960h"),Yn=r(" architecture."),Rn=l(),Z=n("p"),Qn=r("Configuration objects inherit from "),Ct=n("a"),Xn=r("PretrainedConfig"),Zn=r(` and can be used to control the model
outputs. Read the documentation from `),jt=n("a"),Jn=r("PretrainedConfig"),Gn=r(" for more information."),ea=l(),Xt=n("p"),ta=r("Example:"),oa=l(),_(Ee.$$.fragment),Eo=l(),J=n("h2"),ue=n("a"),Zt=n("span"),_(Me.$$.fragment),na=l(),Jt=n("span"),aa=r("UniSpeech specific outputs"),Mo=l(),G=n("div"),_(ze.$$.fragment),sa=l(),De=n("p"),ra=r("Output type of "),Gt=n("code"),ia=r("UniSpeechBaseModelOutput"),ca=r(", with potential hidden states and attentions."),zo=l(),ee=n("div"),_(Ae.$$.fragment),la=l(),We=n("p"),da=r("Output type of "),eo=n("code"),pa=r("UniSpeechForPreTrainingOutput"),ha=r(", with potential hidden states and attentions."),Do=l(),te=n("h2"),fe=n("a"),to=n("span"),_(Oe.$$.fragment),ma=l(),oo=n("span"),ua=r("UniSpeechModel"),Ao=l(),x=n("div"),_(Le.$$.fragment),fa=l(),Ne=n("p"),ga=r(`The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.
UniSpeech was proposed in `),Ve=n("a"),_a=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),va=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),ba=l(),Ie=n("p"),wa=r("This model inherits from "),xt=n("a"),ya=r("PreTrainedModel"),Sa=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Ta=l(),Be=n("p"),ka=r("This model is a PyTorch "),He=n("a"),Ua=r("torch.nn.Module"),$a=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Ca=l(),P=n("div"),_(Ke.$$.fragment),ja=l(),oe=n("p"),xa=r("The "),Ft=n("a"),Fa=r("UniSpeechModel"),qa=r(" forward method, overrides the "),no=n("code"),Pa=r("__call__"),Ea=r(" special method."),Ma=l(),_(ge.$$.fragment),za=l(),ao=n("p"),Da=r("Example:"),Aa=l(),_(Ye.$$.fragment),Wo=l(),ne=n("h2"),_e=n("a"),so=n("span"),_(Re.$$.fragment),Wa=l(),ro=n("span"),Oa=r("UniSpeechForCTC"),Oo=l(),F=n("div"),_(Qe.$$.fragment),La=l(),ae=n("p"),Na=r("UniSpeech Model with a "),io=n("code"),Va=r("language modeling"),Ia=r(` head on top for Connectionist Temporal Classification (CTC).
UniSpeech was proposed in `),Xe=n("a"),Ba=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Ha=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Ka=l(),Ze=n("p"),Ya=r("This model inherits from "),qt=n("a"),Ra=r("PreTrainedModel"),Qa=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Xa=l(),Je=n("p"),Za=r("This model is a PyTorch "),Ge=n("a"),Ja=r("torch.nn.Module"),Ga=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),es=l(),E=n("div"),_(et.$$.fragment),ts=l(),se=n("p"),os=r("The "),Pt=n("a"),ns=r("UniSpeechForCTC"),as=r(" forward method, overrides the "),co=n("code"),ss=r("__call__"),rs=r(" special method."),is=l(),_(ve.$$.fragment),cs=l(),lo=n("p"),ls=r("Example:"),ds=l(),_(tt.$$.fragment),Lo=l(),re=n("h2"),be=n("a"),po=n("span"),_(ot.$$.fragment),ps=l(),ho=n("span"),hs=r("UniSpeechForSequenceClassification"),No=l(),C=n("div"),_(nt.$$.fragment),ms=l(),mo=n("p"),us=r(`UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`),fs=l(),at=n("p"),gs=r("UniSpeech was proposed in "),st=n("a"),_s=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),vs=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),bs=l(),rt=n("p"),ws=r("This model inherits from "),Et=n("a"),ys=r("PreTrainedModel"),Ss=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Ts=l(),it=n("p"),ks=r("This model is a PyTorch "),ct=n("a"),Us=r("torch.nn.Module"),$s=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Cs=l(),M=n("div"),_(lt.$$.fragment),js=l(),ie=n("p"),xs=r("The "),Mt=n("a"),Fs=r("UniSpeechForSequenceClassification"),qs=r(" forward method, overrides the "),uo=n("code"),Ps=r("__call__"),Es=r(" special method."),Ms=l(),_(we.$$.fragment),zs=l(),fo=n("p"),Ds=r("Example:"),As=l(),_(dt.$$.fragment),Vo=l(),ce=n("h2"),ye=n("a"),go=n("span"),_(pt.$$.fragment),Ws=l(),_o=n("span"),Os=r("UniSpeechForPreTraining"),Io=l(),q=n("div"),_(ht.$$.fragment),Ls=l(),mt=n("p"),Ns=r(`UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
UniSpeech was proposed in `),ut=n("a"),Vs=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Is=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Bs=l(),ft=n("p"),Hs=r("This model inherits from "),zt=n("a"),Ks=r("PreTrainedModel"),Ys=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Rs=l(),gt=n("p"),Qs=r("This model is a PyTorch "),_t=n("a"),Xs=r("torch.nn.Module"),Zs=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Js=l(),z=n("div"),_(vt.$$.fragment),Gs=l(),le=n("p"),er=r("The "),Dt=n("a"),tr=r("UniSpeechForPreTraining"),or=r(" forward method, overrides the "),vo=n("code"),nr=r("__call__"),ar=r(" special method."),sr=l(),_(Se.$$.fragment),rr=l(),bo=n("p"),ir=r("Example:"),cr=l(),_(bt.$$.fragment),this.h()},l(t){const p=yi('[data-svelte="svelte-1phssyn"]',document.head);h=a(p,"META",{name:!0,content:!0}),p.forEach(o),k=d(t),u=a(t,"H1",{class:!0});var wt=s(u);T=a(wt,"A",{id:!0,class:!0,href:!0});var wo=s(T);U=a(wo,"SPAN",{});var yo=s(U);v(f.$$.fragment,yo),yo.forEach(o),wo.forEach(o),g=d(wt),$=a(wt,"SPAN",{});var So=s($);bn=i(So,"UniSpeech"),So.forEach(o),wt.forEach(o),ko=d(t),R=a(t,"H2",{class:!0});var Ho=s(R);de=a(Ho,"A",{id:!0,class:!0,href:!0});var lr=s(de);Ht=a(lr,"SPAN",{});var dr=s(Ht);v(ke.$$.fragment,dr),dr.forEach(o),lr.forEach(o),wn=d(Ho),Kt=a(Ho,"SPAN",{});var pr=s(Kt);yn=i(pr,"Overview"),pr.forEach(o),Ho.forEach(o),Uo=d(t),pe=a(t,"P",{});var Ko=s(pe);Sn=i(Ko,"The UniSpeech model was proposed in "),Ue=a(Ko,"A",{href:!0,rel:!0});var hr=s(Ue);Tn=i(hr,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),hr.forEach(o),kn=i(Ko,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
Zeng, Xuedong Huang .`),Ko.forEach(o),$o=d(t),yt=a(t,"P",{});var mr=s(yt);Un=i(mr,"The abstract from the paper is the following:"),mr.forEach(o),Co=d(t),St=a(t,"P",{});var ur=s(St);Yt=a(ur,"EM",{});var fr=s(Yt);$n=i(fr,`In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.`),fr.forEach(o),ur.forEach(o),jo=d(t),Tt=a(t,"P",{});var gr=s(Tt);Cn=i(gr,"Tips:"),gr.forEach(o),xo=d(t),he=a(t,"UL",{});var Yo=s(he);$e=a(Yo,"LI",{});var Ro=s($e);jn=i(Ro,`UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please
use `),kt=a(Ro,"A",{href:!0});var _r=s(kt);xn=i(_r,"Wav2Vec2Processor"),_r.forEach(o),Fn=i(Ro," for the feature extraction."),Ro.forEach(o),qn=d(Yo),Ce=a(Yo,"LI",{});var Qo=s(Ce);Pn=i(Qo,`UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be
decoded using `),Ut=a(Qo,"A",{href:!0});var vr=s(Ut);En=i(vr,"Wav2Vec2CTCTokenizer"),vr.forEach(o),Mn=i(Qo,"."),Qo.forEach(o),Yo.forEach(o),Fo=d(t),O=a(t,"P",{});var At=s(O);zn=i(At,"This model was contributed by "),je=a(At,"A",{href:!0,rel:!0});var br=s(je);Dn=i(br,"patrickvonplaten"),br.forEach(o),An=i(At,`. The Authors\u2019 code can be
found `),xe=a(At,"A",{href:!0,rel:!0});var wr=s(xe);Wn=i(wr,"here"),wr.forEach(o),On=i(At,"."),At.forEach(o),qo=d(t),Q=a(t,"H2",{class:!0});var Xo=s(Q);me=a(Xo,"A",{id:!0,class:!0,href:!0});var yr=s(me);Rt=a(yr,"SPAN",{});var Sr=s(Rt);v(Fe.$$.fragment,Sr),Sr.forEach(o),yr.forEach(o),Ln=d(Xo),Qt=a(Xo,"SPAN",{});var Tr=s(Qt);Nn=i(Tr,"UniSpeechConfig"),Tr.forEach(o),Xo.forEach(o),Po=d(t),j=a(t,"DIV",{class:!0});var L=s(j);v(qe.$$.fragment,L),Vn=d(L),X=a(L,"P",{});var Wt=s(X);In=i(Wt,"This is the configuration class to store the configuration of a "),$t=a(Wt,"A",{href:!0});var kr=s($t);Bn=i(kr,"UniSpeechModel"),kr.forEach(o),Hn=i(Wt,`. It is used
to instantiate an UniSpeech model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the UniSpeech
`),Pe=a(Wt,"A",{href:!0,rel:!0});var Ur=s(Pe);Kn=i(Ur,"facebook/unispeech-base-960h"),Ur.forEach(o),Yn=i(Wt," architecture."),Wt.forEach(o),Rn=d(L),Z=a(L,"P",{});var Ot=s(Z);Qn=i(Ot,"Configuration objects inherit from "),Ct=a(Ot,"A",{href:!0});var $r=s(Ct);Xn=i($r,"PretrainedConfig"),$r.forEach(o),Zn=i(Ot,` and can be used to control the model
outputs. Read the documentation from `),jt=a(Ot,"A",{href:!0});var Cr=s(jt);Jn=i(Cr,"PretrainedConfig"),Cr.forEach(o),Gn=i(Ot," for more information."),Ot.forEach(o),ea=d(L),Xt=a(L,"P",{});var jr=s(Xt);ta=i(jr,"Example:"),jr.forEach(o),oa=d(L),v(Ee.$$.fragment,L),L.forEach(o),Eo=d(t),J=a(t,"H2",{class:!0});var Zo=s(J);ue=a(Zo,"A",{id:!0,class:!0,href:!0});var xr=s(ue);Zt=a(xr,"SPAN",{});var Fr=s(Zt);v(Me.$$.fragment,Fr),Fr.forEach(o),xr.forEach(o),na=d(Zo),Jt=a(Zo,"SPAN",{});var qr=s(Jt);aa=i(qr,"UniSpeech specific outputs"),qr.forEach(o),Zo.forEach(o),Mo=d(t),G=a(t,"DIV",{class:!0});var Jo=s(G);v(ze.$$.fragment,Jo),sa=d(Jo),De=a(Jo,"P",{});var Go=s(De);ra=i(Go,"Output type of "),Gt=a(Go,"CODE",{});var Pr=s(Gt);ia=i(Pr,"UniSpeechBaseModelOutput"),Pr.forEach(o),ca=i(Go,", with potential hidden states and attentions."),Go.forEach(o),Jo.forEach(o),zo=d(t),ee=a(t,"DIV",{class:!0});var en=s(ee);v(Ae.$$.fragment,en),la=d(en),We=a(en,"P",{});var tn=s(We);da=i(tn,"Output type of "),eo=a(tn,"CODE",{});var Er=s(eo);pa=i(Er,"UniSpeechForPreTrainingOutput"),Er.forEach(o),ha=i(tn,", with potential hidden states and attentions."),tn.forEach(o),en.forEach(o),Do=d(t),te=a(t,"H2",{class:!0});var on=s(te);fe=a(on,"A",{id:!0,class:!0,href:!0});var Mr=s(fe);to=a(Mr,"SPAN",{});var zr=s(to);v(Oe.$$.fragment,zr),zr.forEach(o),Mr.forEach(o),ma=d(on),oo=a(on,"SPAN",{});var Dr=s(oo);ua=i(Dr,"UniSpeechModel"),Dr.forEach(o),on.forEach(o),Ao=d(t),x=a(t,"DIV",{class:!0});var N=s(x);v(Le.$$.fragment,N),fa=d(N),Ne=a(N,"P",{});var nn=s(Ne);ga=i(nn,`The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.
UniSpeech was proposed in `),Ve=a(nn,"A",{href:!0,rel:!0});var Ar=s(Ve);_a=i(Ar,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Ar.forEach(o),va=i(nn,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),nn.forEach(o),ba=d(N),Ie=a(N,"P",{});var an=s(Ie);wa=i(an,"This model inherits from "),xt=a(an,"A",{href:!0});var Wr=s(xt);ya=i(Wr,"PreTrainedModel"),Wr.forEach(o),Sa=i(an,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),an.forEach(o),Ta=d(N),Be=a(N,"P",{});var sn=s(Be);ka=i(sn,"This model is a PyTorch "),He=a(sn,"A",{href:!0,rel:!0});var Or=s(He);Ua=i(Or,"torch.nn.Module"),Or.forEach(o),$a=i(sn,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),sn.forEach(o),Ca=d(N),P=a(N,"DIV",{class:!0});var V=s(P);v(Ke.$$.fragment,V),ja=d(V),oe=a(V,"P",{});var Lt=s(oe);xa=i(Lt,"The "),Ft=a(Lt,"A",{href:!0});var Lr=s(Ft);Fa=i(Lr,"UniSpeechModel"),Lr.forEach(o),qa=i(Lt," forward method, overrides the "),no=a(Lt,"CODE",{});var Nr=s(no);Pa=i(Nr,"__call__"),Nr.forEach(o),Ea=i(Lt," special method."),Lt.forEach(o),Ma=d(V),v(ge.$$.fragment,V),za=d(V),ao=a(V,"P",{});var Vr=s(ao);Da=i(Vr,"Example:"),Vr.forEach(o),Aa=d(V),v(Ye.$$.fragment,V),V.forEach(o),N.forEach(o),Wo=d(t),ne=a(t,"H2",{class:!0});var rn=s(ne);_e=a(rn,"A",{id:!0,class:!0,href:!0});var Ir=s(_e);so=a(Ir,"SPAN",{});var Br=s(so);v(Re.$$.fragment,Br),Br.forEach(o),Ir.forEach(o),Wa=d(rn),ro=a(rn,"SPAN",{});var Hr=s(ro);Oa=i(Hr,"UniSpeechForCTC"),Hr.forEach(o),rn.forEach(o),Oo=d(t),F=a(t,"DIV",{class:!0});var I=s(F);v(Qe.$$.fragment,I),La=d(I),ae=a(I,"P",{});var Nt=s(ae);Na=i(Nt,"UniSpeech Model with a "),io=a(Nt,"CODE",{});var Kr=s(io);Va=i(Kr,"language modeling"),Kr.forEach(o),Ia=i(Nt,` head on top for Connectionist Temporal Classification (CTC).
UniSpeech was proposed in `),Xe=a(Nt,"A",{href:!0,rel:!0});var Yr=s(Xe);Ba=i(Yr,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Yr.forEach(o),Ha=i(Nt,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Nt.forEach(o),Ka=d(I),Ze=a(I,"P",{});var cn=s(Ze);Ya=i(cn,"This model inherits from "),qt=a(cn,"A",{href:!0});var Rr=s(qt);Ra=i(Rr,"PreTrainedModel"),Rr.forEach(o),Qa=i(cn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),cn.forEach(o),Xa=d(I),Je=a(I,"P",{});var ln=s(Je);Za=i(ln,"This model is a PyTorch "),Ge=a(ln,"A",{href:!0,rel:!0});var Qr=s(Ge);Ja=i(Qr,"torch.nn.Module"),Qr.forEach(o),Ga=i(ln,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ln.forEach(o),es=d(I),E=a(I,"DIV",{class:!0});var B=s(E);v(et.$$.fragment,B),ts=d(B),se=a(B,"P",{});var Vt=s(se);os=i(Vt,"The "),Pt=a(Vt,"A",{href:!0});var Xr=s(Pt);ns=i(Xr,"UniSpeechForCTC"),Xr.forEach(o),as=i(Vt," forward method, overrides the "),co=a(Vt,"CODE",{});var Zr=s(co);ss=i(Zr,"__call__"),Zr.forEach(o),rs=i(Vt," special method."),Vt.forEach(o),is=d(B),v(ve.$$.fragment,B),cs=d(B),lo=a(B,"P",{});var Jr=s(lo);ls=i(Jr,"Example:"),Jr.forEach(o),ds=d(B),v(tt.$$.fragment,B),B.forEach(o),I.forEach(o),Lo=d(t),re=a(t,"H2",{class:!0});var dn=s(re);be=a(dn,"A",{id:!0,class:!0,href:!0});var Gr=s(be);po=a(Gr,"SPAN",{});var ei=s(po);v(ot.$$.fragment,ei),ei.forEach(o),Gr.forEach(o),ps=d(dn),ho=a(dn,"SPAN",{});var ti=s(ho);hs=i(ti,"UniSpeechForSequenceClassification"),ti.forEach(o),dn.forEach(o),No=d(t),C=a(t,"DIV",{class:!0});var D=s(C);v(nt.$$.fragment,D),ms=d(D),mo=a(D,"P",{});var oi=s(mo);us=i(oi,`UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`),oi.forEach(o),fs=d(D),at=a(D,"P",{});var pn=s(at);gs=i(pn,"UniSpeech was proposed in "),st=a(pn,"A",{href:!0,rel:!0});var ni=s(st);_s=i(ni,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),ni.forEach(o),vs=i(pn,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),pn.forEach(o),bs=d(D),rt=a(D,"P",{});var hn=s(rt);ws=i(hn,"This model inherits from "),Et=a(hn,"A",{href:!0});var ai=s(Et);ys=i(ai,"PreTrainedModel"),ai.forEach(o),Ss=i(hn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),hn.forEach(o),Ts=d(D),it=a(D,"P",{});var mn=s(it);ks=i(mn,"This model is a PyTorch "),ct=a(mn,"A",{href:!0,rel:!0});var si=s(ct);Us=i(si,"torch.nn.Module"),si.forEach(o),$s=i(mn,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),mn.forEach(o),Cs=d(D),M=a(D,"DIV",{class:!0});var H=s(M);v(lt.$$.fragment,H),js=d(H),ie=a(H,"P",{});var It=s(ie);xs=i(It,"The "),Mt=a(It,"A",{href:!0});var ri=s(Mt);Fs=i(ri,"UniSpeechForSequenceClassification"),ri.forEach(o),qs=i(It," forward method, overrides the "),uo=a(It,"CODE",{});var ii=s(uo);Ps=i(ii,"__call__"),ii.forEach(o),Es=i(It," special method."),It.forEach(o),Ms=d(H),v(we.$$.fragment,H),zs=d(H),fo=a(H,"P",{});var ci=s(fo);Ds=i(ci,"Example:"),ci.forEach(o),As=d(H),v(dt.$$.fragment,H),H.forEach(o),D.forEach(o),Vo=d(t),ce=a(t,"H2",{class:!0});var un=s(ce);ye=a(un,"A",{id:!0,class:!0,href:!0});var li=s(ye);go=a(li,"SPAN",{});var di=s(go);v(pt.$$.fragment,di),di.forEach(o),li.forEach(o),Ws=d(un),_o=a(un,"SPAN",{});var pi=s(_o);Os=i(pi,"UniSpeechForPreTraining"),pi.forEach(o),un.forEach(o),Io=d(t),q=a(t,"DIV",{class:!0});var K=s(q);v(ht.$$.fragment,K),Ls=d(K),mt=a(K,"P",{});var fn=s(mt);Ns=i(fn,`UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
UniSpeech was proposed in `),ut=a(fn,"A",{href:!0,rel:!0});var hi=s(ut);Vs=i(hi,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),hi.forEach(o),Is=i(fn,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),fn.forEach(o),Bs=d(K),ft=a(K,"P",{});var gn=s(ft);Hs=i(gn,"This model inherits from "),zt=a(gn,"A",{href:!0});var mi=s(zt);Ks=i(mi,"PreTrainedModel"),mi.forEach(o),Ys=i(gn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),gn.forEach(o),Rs=d(K),gt=a(K,"P",{});var _n=s(gt);Qs=i(_n,"This model is a PyTorch "),_t=a(_n,"A",{href:!0,rel:!0});var ui=s(_t);Xs=i(ui,"torch.nn.Module"),ui.forEach(o),Zs=i(_n,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_n.forEach(o),Js=d(K),z=a(K,"DIV",{class:!0});var Y=s(z);v(vt.$$.fragment,Y),Gs=d(Y),le=a(Y,"P",{});var Bt=s(le);er=i(Bt,"The "),Dt=a(Bt,"A",{href:!0});var fi=s(Dt);tr=i(fi,"UniSpeechForPreTraining"),fi.forEach(o),or=i(Bt," forward method, overrides the "),vo=a(Bt,"CODE",{});var gi=s(vo);nr=i(gi,"__call__"),gi.forEach(o),ar=i(Bt," special method."),Bt.forEach(o),sr=d(Y),v(Se.$$.fragment,Y),rr=d(Y),bo=a(Y,"P",{});var _i=s(bo);ir=i(_i,"Example:"),_i.forEach(o),cr=d(Y),v(bt.$$.fragment,Y),Y.forEach(o),K.forEach(o),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Ci)),c(T,"id","unispeech"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#unispeech"),c(u,"class","relative group"),c(de,"id","overview"),c(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(de,"href","#overview"),c(R,"class","relative group"),c(Ue,"href","https://arxiv.org/abs/2101.07597"),c(Ue,"rel","nofollow"),c(kt,"href","/docs/transformers/v4.15.0/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Ut,"href","/docs/transformers/v4.15.0/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(je,"href","https://huggingface.co/patrickvonplaten"),c(je,"rel","nofollow"),c(xe,"href","https://github.com/microsoft/UniSpeech/tree/main/UniSpeech"),c(xe,"rel","nofollow"),c(me,"id","transformers.UniSpeechConfig"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#transformers.UniSpeechConfig"),c(Q,"class","relative group"),c($t,"href","/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechModel"),c(Pe,"href","https://huggingface.co/facebook/unispeech-base-960h"),c(Pe,"rel","nofollow"),c(Ct,"href","/docs/transformers/v4.15.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(jt,"href","/docs/transformers/v4.15.0/en/main_classes/configuration#transformers.PretrainedConfig"),c(j,"class","docstring"),c(ue,"id","transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"),c(J,"class","relative group"),c(G,"class","docstring"),c(ee,"class","docstring"),c(fe,"id","transformers.UniSpeechModel"),c(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(fe,"href","#transformers.UniSpeechModel"),c(te,"class","relative group"),c(Ve,"href","https://arxiv.org/abs/2101.07597"),c(Ve,"rel","nofollow"),c(xt,"href","/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel"),c(He,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(He,"rel","nofollow"),c(Ft,"href","/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechModel"),c(P,"class","docstring"),c(x,"class","docstring"),c(_e,"id","transformers.UniSpeechForCTC"),c(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_e,"href","#transformers.UniSpeechForCTC"),c(ne,"class","relative group"),c(Xe,"href","https://arxiv.org/abs/2101.07597"),c(Xe,"rel","nofollow"),c(qt,"href","/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel"),c(Ge,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ge,"rel","nofollow"),c(Pt,"href","/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(E,"class","docstring"),c(F,"class","docstring"),c(be,"id","transformers.UniSpeechForSequenceClassification"),c(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(be,"href","#transformers.UniSpeechForSequenceClassification"),c(re,"class","relative group"),c(st,"href","https://arxiv.org/abs/2101.07597"),c(st,"rel","nofollow"),c(Et,"href","/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel"),c(ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ct,"rel","nofollow"),c(Mt,"href","/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(M,"class","docstring"),c(C,"class","docstring"),c(ye,"id","transformers.UniSpeechForPreTraining"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#transformers.UniSpeechForPreTraining"),c(ce,"class","relative group"),c(ut,"href","https://arxiv.org/abs/2101.07597"),c(ut,"rel","nofollow"),c(zt,"href","/docs/transformers/v4.15.0/en/main_classes/model#transformers.PreTrainedModel"),c(_t,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(_t,"rel","nofollow"),c(Dt,"href","/docs/transformers/v4.15.0/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(z,"class","docstring"),c(q,"class","docstring")},m(t,p){e(document.head,h),m(t,k,p),m(t,u,p),e(u,T),e(T,U),b(f,U,null),e(u,g),e(u,$),e($,bn),m(t,ko,p),m(t,R,p),e(R,de),e(de,Ht),b(ke,Ht,null),e(R,wn),e(R,Kt),e(Kt,yn),m(t,Uo,p),m(t,pe,p),e(pe,Sn),e(pe,Ue),e(Ue,Tn),e(pe,kn),m(t,$o,p),m(t,yt,p),e(yt,Un),m(t,Co,p),m(t,St,p),e(St,Yt),e(Yt,$n),m(t,jo,p),m(t,Tt,p),e(Tt,Cn),m(t,xo,p),m(t,he,p),e(he,$e),e($e,jn),e($e,kt),e(kt,xn),e($e,Fn),e(he,qn),e(he,Ce),e(Ce,Pn),e(Ce,Ut),e(Ut,En),e(Ce,Mn),m(t,Fo,p),m(t,O,p),e(O,zn),e(O,je),e(je,Dn),e(O,An),e(O,xe),e(xe,Wn),e(O,On),m(t,qo,p),m(t,Q,p),e(Q,me),e(me,Rt),b(Fe,Rt,null),e(Q,Ln),e(Q,Qt),e(Qt,Nn),m(t,Po,p),m(t,j,p),b(qe,j,null),e(j,Vn),e(j,X),e(X,In),e(X,$t),e($t,Bn),e(X,Hn),e(X,Pe),e(Pe,Kn),e(X,Yn),e(j,Rn),e(j,Z),e(Z,Qn),e(Z,Ct),e(Ct,Xn),e(Z,Zn),e(Z,jt),e(jt,Jn),e(Z,Gn),e(j,ea),e(j,Xt),e(Xt,ta),e(j,oa),b(Ee,j,null),m(t,Eo,p),m(t,J,p),e(J,ue),e(ue,Zt),b(Me,Zt,null),e(J,na),e(J,Jt),e(Jt,aa),m(t,Mo,p),m(t,G,p),b(ze,G,null),e(G,sa),e(G,De),e(De,ra),e(De,Gt),e(Gt,ia),e(De,ca),m(t,zo,p),m(t,ee,p),b(Ae,ee,null),e(ee,la),e(ee,We),e(We,da),e(We,eo),e(eo,pa),e(We,ha),m(t,Do,p),m(t,te,p),e(te,fe),e(fe,to),b(Oe,to,null),e(te,ma),e(te,oo),e(oo,ua),m(t,Ao,p),m(t,x,p),b(Le,x,null),e(x,fa),e(x,Ne),e(Ne,ga),e(Ne,Ve),e(Ve,_a),e(Ne,va),e(x,ba),e(x,Ie),e(Ie,wa),e(Ie,xt),e(xt,ya),e(Ie,Sa),e(x,Ta),e(x,Be),e(Be,ka),e(Be,He),e(He,Ua),e(Be,$a),e(x,Ca),e(x,P),b(Ke,P,null),e(P,ja),e(P,oe),e(oe,xa),e(oe,Ft),e(Ft,Fa),e(oe,qa),e(oe,no),e(no,Pa),e(oe,Ea),e(P,Ma),b(ge,P,null),e(P,za),e(P,ao),e(ao,Da),e(P,Aa),b(Ye,P,null),m(t,Wo,p),m(t,ne,p),e(ne,_e),e(_e,so),b(Re,so,null),e(ne,Wa),e(ne,ro),e(ro,Oa),m(t,Oo,p),m(t,F,p),b(Qe,F,null),e(F,La),e(F,ae),e(ae,Na),e(ae,io),e(io,Va),e(ae,Ia),e(ae,Xe),e(Xe,Ba),e(ae,Ha),e(F,Ka),e(F,Ze),e(Ze,Ya),e(Ze,qt),e(qt,Ra),e(Ze,Qa),e(F,Xa),e(F,Je),e(Je,Za),e(Je,Ge),e(Ge,Ja),e(Je,Ga),e(F,es),e(F,E),b(et,E,null),e(E,ts),e(E,se),e(se,os),e(se,Pt),e(Pt,ns),e(se,as),e(se,co),e(co,ss),e(se,rs),e(E,is),b(ve,E,null),e(E,cs),e(E,lo),e(lo,ls),e(E,ds),b(tt,E,null),m(t,Lo,p),m(t,re,p),e(re,be),e(be,po),b(ot,po,null),e(re,ps),e(re,ho),e(ho,hs),m(t,No,p),m(t,C,p),b(nt,C,null),e(C,ms),e(C,mo),e(mo,us),e(C,fs),e(C,at),e(at,gs),e(at,st),e(st,_s),e(at,vs),e(C,bs),e(C,rt),e(rt,ws),e(rt,Et),e(Et,ys),e(rt,Ss),e(C,Ts),e(C,it),e(it,ks),e(it,ct),e(ct,Us),e(it,$s),e(C,Cs),e(C,M),b(lt,M,null),e(M,js),e(M,ie),e(ie,xs),e(ie,Mt),e(Mt,Fs),e(ie,qs),e(ie,uo),e(uo,Ps),e(ie,Es),e(M,Ms),b(we,M,null),e(M,zs),e(M,fo),e(fo,Ds),e(M,As),b(dt,M,null),m(t,Vo,p),m(t,ce,p),e(ce,ye),e(ye,go),b(pt,go,null),e(ce,Ws),e(ce,_o),e(_o,Os),m(t,Io,p),m(t,q,p),b(ht,q,null),e(q,Ls),e(q,mt),e(mt,Ns),e(mt,ut),e(ut,Vs),e(mt,Is),e(q,Bs),e(q,ft),e(ft,Hs),e(ft,zt),e(zt,Ks),e(ft,Ys),e(q,Rs),e(q,gt),e(gt,Qs),e(gt,_t),e(_t,Xs),e(gt,Zs),e(q,Js),e(q,z),b(vt,z,null),e(z,Gs),e(z,le),e(le,er),e(le,Dt),e(Dt,tr),e(le,or),e(le,vo),e(vo,nr),e(le,ar),e(z,sr),b(Se,z,null),e(z,rr),e(z,bo),e(bo,ir),e(z,cr),b(bt,z,null),Bo=!0},p(t,[p]){const wt={};p&2&&(wt.$$scope={dirty:p,ctx:t}),ge.$set(wt);const wo={};p&2&&(wo.$$scope={dirty:p,ctx:t}),ve.$set(wo);const yo={};p&2&&(yo.$$scope={dirty:p,ctx:t}),we.$set(yo);const So={};p&2&&(So.$$scope={dirty:p,ctx:t}),Se.$set(So)},i(t){Bo||(w(f.$$.fragment,t),w(ke.$$.fragment,t),w(Fe.$$.fragment,t),w(qe.$$.fragment,t),w(Ee.$$.fragment,t),w(Me.$$.fragment,t),w(ze.$$.fragment,t),w(Ae.$$.fragment,t),w(Oe.$$.fragment,t),w(Le.$$.fragment,t),w(Ke.$$.fragment,t),w(ge.$$.fragment,t),w(Ye.$$.fragment,t),w(Re.$$.fragment,t),w(Qe.$$.fragment,t),w(et.$$.fragment,t),w(ve.$$.fragment,t),w(tt.$$.fragment,t),w(ot.$$.fragment,t),w(nt.$$.fragment,t),w(lt.$$.fragment,t),w(we.$$.fragment,t),w(dt.$$.fragment,t),w(pt.$$.fragment,t),w(ht.$$.fragment,t),w(vt.$$.fragment,t),w(Se.$$.fragment,t),w(bt.$$.fragment,t),Bo=!0)},o(t){y(f.$$.fragment,t),y(ke.$$.fragment,t),y(Fe.$$.fragment,t),y(qe.$$.fragment,t),y(Ee.$$.fragment,t),y(Me.$$.fragment,t),y(ze.$$.fragment,t),y(Ae.$$.fragment,t),y(Oe.$$.fragment,t),y(Le.$$.fragment,t),y(Ke.$$.fragment,t),y(ge.$$.fragment,t),y(Ye.$$.fragment,t),y(Re.$$.fragment,t),y(Qe.$$.fragment,t),y(et.$$.fragment,t),y(ve.$$.fragment,t),y(tt.$$.fragment,t),y(ot.$$.fragment,t),y(nt.$$.fragment,t),y(lt.$$.fragment,t),y(we.$$.fragment,t),y(dt.$$.fragment,t),y(pt.$$.fragment,t),y(ht.$$.fragment,t),y(vt.$$.fragment,t),y(Se.$$.fragment,t),y(bt.$$.fragment,t),Bo=!1},d(t){o(h),t&&o(k),t&&o(u),S(f),t&&o(ko),t&&o(R),S(ke),t&&o(Uo),t&&o(pe),t&&o($o),t&&o(yt),t&&o(Co),t&&o(St),t&&o(jo),t&&o(Tt),t&&o(xo),t&&o(he),t&&o(Fo),t&&o(O),t&&o(qo),t&&o(Q),S(Fe),t&&o(Po),t&&o(j),S(qe),S(Ee),t&&o(Eo),t&&o(J),S(Me),t&&o(Mo),t&&o(G),S(ze),t&&o(zo),t&&o(ee),S(Ae),t&&o(Do),t&&o(te),S(Oe),t&&o(Ao),t&&o(x),S(Le),S(Ke),S(ge),S(Ye),t&&o(Wo),t&&o(ne),S(Re),t&&o(Oo),t&&o(F),S(Qe),S(et),S(ve),S(tt),t&&o(Lo),t&&o(re),S(ot),t&&o(No),t&&o(C),S(nt),S(lt),S(we),S(dt),t&&o(Vo),t&&o(ce),S(pt),t&&o(Io),t&&o(q),S(ht),S(vt),S(Se),S(bt)}}}const Ci={local:"unispeech",sections:[{local:"overview",title:"Overview"},{local:"transformers.UniSpeechConfig",title:"UniSpeechConfig"},{local:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",title:"UniSpeech specific outputs"},{local:"transformers.UniSpeechModel",title:"UniSpeechModel"},{local:"transformers.UniSpeechForCTC",title:"UniSpeechForCTC"},{local:"transformers.UniSpeechForSequenceClassification",title:"UniSpeechForSequenceClassification"},{local:"transformers.UniSpeechForPreTraining",title:"UniSpeechForPreTraining"}],title:"UniSpeech"};function ji(A,h,k){let{fw:u}=h;return A.$$set=T=>{"fw"in T&&k(0,u=T.fw)},[u]}class zi extends vi{constructor(h){super();bi(this,h,ji,$i,wi,{fw:0})}}export{zi as default,Ci as metadata};
