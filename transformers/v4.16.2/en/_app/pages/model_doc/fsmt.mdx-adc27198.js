import{S as Ur,i as Hr,s as Vr,e as n,k as d,w as u,t as r,M as Qr,c as s,d as o,m as l,a,x as _,h as i,b as c,F as e,g as m,y as g,q as k,o as v,B as T}from"../../chunks/vendor-ab4e3193.js";import{T as Rr}from"../../chunks/Tip-b5c6375a.js";import{D as U}from"../../chunks/Docstring-b69c0bd4.js";import{C as Qo}from"../../chunks/CodeBlock-516df0c5.js";import{I as tt}from"../../chunks/IconCopyLink-d992940d.js";import"../../chunks/CopyButton-204b56db.js";function Xr(ke){let f,q,p,y,P;return{c(){f=n("p"),q=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=n("code"),y=r("Module"),P=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(M){f=s(M,"P",{});var w=a(f);q=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=s(w,"CODE",{});var A=a(p);y=i(A,"Module"),A.forEach(o),P=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(o)},m(M,w){m(M,f,w),e(f,q),e(f,p),e(p,y),e(f,P)},d(M){M&&o(f)}}}function Kr(ke){let f,q,p,y,P;return{c(){f=n("p"),q=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),p=n("code"),y=r("Module"),P=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(M){f=s(M,"P",{});var w=a(f);q=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),p=s(w,"CODE",{});var A=a(p);y=i(A,"Module"),A.forEach(o),P=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(o)},m(M,w){m(M,f,w),e(f,q),e(f,p),e(p,y),e(f,P)},d(M){M&&o(f)}}}function Yr(ke){let f,q,p,y,P,M,w,A,Xo,fo,H,Ft,Ko,Yo,ve,Jo,Zo,po,V,ae,St,Te,en,$t,tn,uo,re,on,be,nn,sn,_o,ot,an,go,nt,zt,rn,ko,L,dn,ye,ln,cn,Me,hn,mn,vo,Q,ie,qt,we,fn,Et,pn,To,st,X,un,at,_n,gn,rt,kn,vn,bo,K,de,xt,Fe,Tn,Ct,bn,yo,F,Se,yn,$e,Mn,it,wn,Fn,Sn,Y,$n,dt,zn,qn,lt,En,xn,Cn,Pt,Pn,In,ze,An,le,qe,Dn,J,Nn,It,Ln,jn,At,On,Gn,Mo,Z,ce,Dt,Ee,Bn,Nt,Wn,wo,b,xe,Rn,Lt,Un,Hn,D,jt,Vn,Qn,Ot,Xn,Kn,N,Yn,Gt,Jn,Zn,Bt,es,ts,Wt,os,ns,ss,Ce,as,Rt,rs,is,ds,Pe,ls,ct,cs,hs,ms,j,Ie,fs,Ut,ps,us,Ae,ht,_s,Ht,gs,ks,mt,vs,Vt,Ts,bs,he,De,ys,Ne,Ms,Qt,ws,Fs,Ss,E,Le,$s,Xt,zs,qs,je,Es,ee,xs,Kt,Cs,Ps,Yt,Is,As,Ds,Jt,Ns,Ls,Zt,Fo,te,me,eo,Oe,js,to,Os,So,$,Ge,Gs,oo,Bs,Ws,Be,Rs,ft,Us,Hs,Vs,We,Qs,Re,Xs,Ks,Ys,x,Ue,Js,oe,Zs,pt,ea,ta,no,oa,na,sa,fe,aa,so,ra,ia,He,$o,ne,pe,ao,Ve,da,ro,la,zo,z,Qe,ca,io,ha,ma,Xe,fa,ut,pa,ua,_a,Ke,ga,Ye,ka,va,Ta,C,Je,ba,se,ya,_t,Ma,wa,lo,Fa,Sa,$a,ue,za,co,qa,Ea,Ze,qo;return M=new tt({}),Te=new tt({}),we=new tt({}),Fe=new tt({}),Se=new U({props:{name:"class transformers.FSMTConfig",anchor:"transformers.FSMTConfig",parameters:[{name:"langs",val:" = ['en', 'de']"},{name:"src_vocab_size",val:" = 42024"},{name:"tgt_vocab_size",val:" = 42024"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 1024"},{name:"max_length",val:" = 200"},{name:"max_position_embeddings",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"encoder_layers",val:" = 12"},{name:"encoder_attention_heads",val:" = 16"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"decoder_layers",val:" = 12"},{name:"decoder_attention_heads",val:" = 16"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"is_encoder_decoder",val:" = True"},{name:"scale_embedding",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"num_beams",val:" = 5"},{name:"length_penalty",val:" = 1.0"},{name:"early_stopping",val:" = False"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"forced_eos_token_id",val:" = 2"},{name:"**common_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/configuration_fsmt.py#L41",parametersDescription:[{anchor:"transformers.FSMTConfig.langs",description:`<strong>langs</strong> (<code>List[str]</code>) &#x2014;
A list with source language and target_language (e.g., [&#x2018;en&#x2019;, &#x2018;ru&#x2019;]).`,name:"langs"},{anchor:"transformers.FSMTConfig.src_vocab_size",description:`<strong>src_vocab_size</strong> (<code>int</code>) &#x2014;
Vocabulary size of the encoder. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed to the forward method in the encoder.`,name:"src_vocab_size"},{anchor:"transformers.FSMTConfig.tgt_vocab_size",description:`<strong>tgt_vocab_size</strong> (<code>int</code>) &#x2014;
Vocabulary size of the decoder. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed to the forward method in the decoder.`,name:"tgt_vocab_size"},{anchor:"transformers.FSMTConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.FSMTConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.FSMTConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.FSMTConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FSMTConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FSMTConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.FSMTConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.FSMTConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.FSMTConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.FSMTConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FSMTConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.FSMTConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FSMTConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.FSMTConfig.scale_embedding",description:`<strong>scale_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Scale embeddings by diving by sqrt(d_model).`,name:"scale_embedding"},{anchor:"transformers.FSMTConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.FSMTConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.FSMTConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.FSMTConfig.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
This model starts decoding with <code>eos_token_id</code>
encoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
Google &#x201C;layerdrop arxiv&#x201D;, as its not explainable in one line.
decoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
Google &#x201C;layerdrop arxiv&#x201D;, as its not explainable in one line.`,name:"decoder_start_token_id"},{anchor:"transformers.FSMTConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.FSMTConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie input and output embeddings.`,name:"tie_word_embeddings"},{anchor:"transformers.FSMTConfig.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Number of beams for beam search that will be used by default in the <code>generate</code> method of the model. 1 means
no beam search.`,name:"num_beams"},{anchor:"transformers.FSMTConfig.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
Exponential penalty to the length that will be used by default in the <code>generate</code> method of the model.`,name:"length_penalty"},{anchor:"transformers.FSMTConfig.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Flag that will be used by default in the <code>generate</code> method of the model. Whether to stop the beam search
when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.FSMTConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.FSMTConfig.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached. Usually set to
<code>eos_token_id</code>.`,name:"forced_eos_token_id"}]}}),ze=new Qo({props:{code:`from transformers import FSMTConfig, FSMTModel

config = FSMTConfig.from_pretrained("facebook/wmt19-en-ru")
model = FSMTModel(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTConfig, FSMTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>config = FSMTConfig.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-en-ru&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTModel(config)`}}),qe=new U({props:{name:"to_dict",anchor:"transformers.FSMTConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/configuration_fsmt.py#L209",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Ee=new tt({}),xe=new U({props:{name:"class transformers.FSMTTokenizer",anchor:"transformers.FSMTTokenizer",parameters:[{name:"langs",val:" = None"},{name:"src_vocab_file",val:" = None"},{name:"tgt_vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"sep_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/tokenization_fsmt.py#L137",parametersDescription:[{anchor:"transformers.FSMTTokenizer.langs",description:`<strong>langs</strong> (<code>List[str]</code>) &#x2014;
A list of two languages to translate from and to, for instance <code>[&quot;en&quot;, &quot;ru&quot;]</code>.`,name:"langs"},{anchor:"transformers.FSMTTokenizer.src_vocab_file",description:`<strong>src_vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary for the source language.`,name:"src_vocab_file"},{anchor:"transformers.FSMTTokenizer.tgt_vocab_file",description:`<strong>tgt_vocab_file</strong> (<code>st</code>) &#x2014;
File containing the vocabulary for the target language.`,name:"tgt_vocab_file"},{anchor:"transformers.FSMTTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
File containing the merges.`,name:"merges_file"},{anchor:"transformers.FSMTTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.FSMTTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.FSMTTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.FSMTTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.FSMTTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"}]}}),Ie=new U({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/tokenization_fsmt.py#L397",parametersDescription:[{anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),De=new U({props:{name:"get_special_tokens_mask",anchor:"transformers.FSMTTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/tokenization_fsmt.py#L423",parametersDescription:[{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Le=new U({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/tokenization_fsmt.py#L451",parametersDescription:[{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),je=new Qo({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Oe=new tt({}),Ge=new U({props:{name:"class transformers.FSMTModel",anchor:"transformers.FSMTModel",parameters:[{name:"config",val:": FSMTConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/modeling_fsmt.py#L989",parametersDescription:[{anchor:"transformers.FSMTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ue=new U({props:{name:"forward",anchor:"transformers.FSMTModel.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/modeling_fsmt.py#L1003",parametersDescription:[{anchor:"transformers.FSMTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>IIndices can be obtained using <code>FSTMTokenizer</code>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FSMTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FSMTModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>FSMT uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.FSMTModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FSMTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FSMTModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.FSMTModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.FSMTModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code> is a sequence of hidden-states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.FSMTModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.FSMTModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.FSMTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FSMTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FSMTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTConfig"
>FSMTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new Rr({props:{$$slots:{default:[Xr]},$$scope:{ctx:ke}}}),He=new Qo({props:{code:`from transformers import FSMTTokenizer, FSMTModel
import torch

tokenizer = FSMTTokenizer.from_pretrained("facebook/wmt19-ru-en")
model = FSMTModel.from_pretrained("facebook/wmt19-ru-en")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTTokenizer, FSMTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FSMTTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTModel.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ve=new tt({}),Qe=new U({props:{name:"class transformers.FSMTForConditionalGeneration",anchor:"transformers.FSMTForConditionalGeneration",parameters:[{name:"config",val:": FSMTConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/modeling_fsmt.py#L1113",parametersDescription:[{anchor:"transformers.FSMTForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Je=new U({props:{name:"forward",anchor:"transformers.FSMTForConditionalGeneration.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/fsmt/modeling_fsmt.py#L1129",parametersDescription:[{anchor:"transformers.FSMTForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>IIndices can be obtained using <code>FSTMTokenizer</code>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FSMTForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a>. See <a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.16.2/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>FSMT uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code> is a sequence of hidden-states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.FSMTForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.FSMTForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.FSMTForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FSMTForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FSMTForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FSMTForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTConfig"
>FSMTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ue=new Rr({props:{$$slots:{default:[Kr]},$$scope:{ctx:ke}}}),Ze=new Qo({props:{code:`from transformers import FSMTTokenizer, FSMTForConditionalGeneration

mname = "facebook/wmt19-ru-en"
model = FSMTForConditionalGeneration.from_pretrained(mname)
tokenizer = FSMTTokenizer.from_pretrained(mname)

src_text = "\u041C\u0430\u0448\u0438\u043D\u043D\u043E\u0435 \u043E\u0431\u0443\u0447\u0435\u043D\u0438\u0435 - \u044D\u0442\u043E \u0437\u0434\u043E\u0440\u043E\u0432\u043E, \u043D\u0435 \u0442\u0430\u043A \u043B\u0438?"
input_ids = tokenizer(src_text, return_tensors="pt")
outputs = model.generate(input_ids, num_beams=5, num_return_sequences=3)
tokenizer.decode(outputs[0], skip_special_tokens=True),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTTokenizer, FSMTForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>mname = <span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTForConditionalGeneration.from_pretrained(mname)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FSMTTokenizer.from_pretrained(mname)

<span class="hljs-meta">&gt;&gt;&gt; </span>src_text = <span class="hljs-string">&quot;\u041C\u0430\u0448\u0438\u043D\u043D\u043E\u0435 \u043E\u0431\u0443\u0447\u0435\u043D\u0438\u0435 - \u044D\u0442\u043E \u0437\u0434\u043E\u0440\u043E\u0432\u043E, \u043D\u0435 \u0442\u0430\u043A \u043B\u0438?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(src_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&quot;Machine learning is great, isn&#x27;t it?&quot;</span>`}}),{c(){f=n("meta"),q=d(),p=n("h1"),y=n("a"),P=n("span"),u(M.$$.fragment),w=d(),A=n("span"),Xo=r("FSMT"),fo=d(),H=n("p"),Ft=n("strong"),Ko=r("DISCLAIMER:"),Yo=r(" If you see something strange, file a "),ve=n("a"),Jo=r("Github Issue"),Zo=r(` and assign
@stas00.`),po=d(),V=n("h2"),ae=n("a"),St=n("span"),u(Te.$$.fragment),en=d(),$t=n("span"),tn=r("Overview"),uo=d(),re=n("p"),on=r("FSMT (FairSeq MachineTranslation) models were introduced in "),be=n("a"),nn=r("Facebook FAIR\u2019s WMT19 News Translation Task Submission"),sn=r(" by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov."),_o=d(),ot=n("p"),an=r("The abstract of the paper is the following:"),go=d(),nt=n("p"),zt=n("em"),rn=r(`This paper describes Facebook FAIR\u2019s submission to the WMT19 shared news translation task. We participate in two
language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from
last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling
toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,
as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific
data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the
human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.
This system improves upon our WMT\u201918 submission by 4.5 BLEU points.`),ko=d(),L=n("p"),dn=r("This model was contributed by "),ye=n("a"),ln=r("stas"),cn=r(`. The original code can be found
`),Me=n("a"),hn=r("here"),mn=r("."),vo=d(),Q=n("h2"),ie=n("a"),qt=n("span"),u(we.$$.fragment),fn=d(),Et=n("span"),pn=r("Implementation Notes"),To=d(),st=n("ul"),X=n("li"),un=r(`FSMT uses source and target vocabulary pairs that aren\u2019t combined into one. It doesn\u2019t share embeddings tokens
either. Its tokenizer is very similar to `),at=n("a"),_n=r("XLMTokenizer"),gn=r(` and the main model is derived from
`),rt=n("a"),kn=r("BartModel"),vn=r("."),bo=d(),K=n("h2"),de=n("a"),xt=n("span"),u(Fe.$$.fragment),Tn=d(),Ct=n("span"),bn=r("FSMTConfig"),yo=d(),F=n("div"),u(Se.$$.fragment),yn=d(),$e=n("p"),Mn=r("This is the configuration class to store the configuration of a "),it=n("a"),wn=r("FSMTModel"),Fn=r(`. It is used to instantiate a FSMT
model according to the specified arguments, defining the model architecture.`),Sn=d(),Y=n("p"),$n=r("Configuration objects inherit from "),dt=n("a"),zn=r("PretrainedConfig"),qn=r(` and can be used to control the model outputs. Read the
documentation from `),lt=n("a"),En=r("PretrainedConfig"),xn=r(" for more information."),Cn=d(),Pt=n("p"),Pn=r("Examples:"),In=d(),u(ze.$$.fragment),An=d(),le=n("div"),u(qe.$$.fragment),Dn=d(),J=n("p"),Nn=r("Serializes this instance to a Python dictionary. Override the default "),It=n("em"),Ln=r("to_dict()"),jn=r(" from "),At=n("em"),On=r("PretrainedConfig"),Gn=r("."),Mo=d(),Z=n("h2"),ce=n("a"),Dt=n("span"),u(Ee.$$.fragment),Bn=d(),Nt=n("span"),Wn=r("FSMTTokenizer"),wo=d(),b=n("div"),u(xe.$$.fragment),Rn=d(),Lt=n("p"),Un=r("Construct an FAIRSEQ Transformer tokenizer. Based on Byte-Pair Encoding. The tokenization process is the following:"),Hn=d(),D=n("ul"),jt=n("li"),Vn=r("Moses preprocessing and tokenization."),Qn=d(),Ot=n("li"),Xn=r("Normalizing all inputs text."),Kn=d(),N=n("li"),Yn=r("The arguments "),Gt=n("code"),Jn=r("special_tokens"),Zn=r(" and the function "),Bt=n("code"),es=r("set_special_tokens"),ts=r(`, can be used to add additional symbols (like
\u201D`),Wt=n("strong"),os=r("classify"),ns=r("\u201D) to a vocabulary."),ss=d(),Ce=n("li"),as=r("The argument "),Rt=n("code"),rs=r("langs"),is=r(" defines a pair of languages."),ds=d(),Pe=n("p"),ls=r("This tokenizer inherits from "),ct=n("a"),cs=r("PreTrainedTokenizer"),hs=r(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),ms=d(),j=n("div"),u(Ie.$$.fragment),fs=d(),Ut=n("p"),ps=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A FAIRSEQ Transformer sequence has the following format:`),us=d(),Ae=n("ul"),ht=n("li"),_s=r("single sequence: "),Ht=n("code"),gs=r("<s> X </s>"),ks=d(),mt=n("li"),vs=r("pair of sequences: "),Vt=n("code"),Ts=r("<s> A </s> B </s>"),bs=d(),he=n("div"),u(De.$$.fragment),ys=d(),Ne=n("p"),Ms=r(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Qt=n("code"),ws=r("prepare_for_model"),Fs=r(" method."),Ss=d(),E=n("div"),u(Le.$$.fragment),$s=d(),Xt=n("p"),zs=r(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ
Transformer sequence pair mask has the following format:`),qs=d(),u(je.$$.fragment),Es=d(),ee=n("p"),xs=r("If "),Kt=n("code"),Cs=r("token_ids_1"),Ps=r(" is "),Yt=n("code"),Is=r("None"),As=r(", this method only returns the first portion of the mask (0s)."),Ds=d(),Jt=n("p"),Ns=r(`Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An
FAIRSEQ_TRANSFORMER sequence pair mask has the following format:`),Ls=d(),Zt=n("div"),Fo=d(),te=n("h2"),me=n("a"),eo=n("span"),u(Oe.$$.fragment),js=d(),to=n("span"),Os=r("FSMTModel"),So=d(),$=n("div"),u(Ge.$$.fragment),Gs=d(),oo=n("p"),Bs=r("The bare FSMT Model outputting raw hidden-states without any specific head on top."),Ws=d(),Be=n("p"),Rs=r("This model inherits from "),ft=n("a"),Us=r("PreTrainedModel"),Hs=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vs=d(),We=n("p"),Qs=r("This model is also a PyTorch "),Re=n("a"),Xs=r("torch.nn.Module"),Ks=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ys=d(),x=n("div"),u(Ue.$$.fragment),Js=d(),oe=n("p"),Zs=r("The "),pt=n("a"),ea=r("FSMTModel"),ta=r(" forward method, overrides the "),no=n("code"),oa=r("__call__"),na=r(" special method."),sa=d(),u(fe.$$.fragment),aa=d(),so=n("p"),ra=r("Example:"),ia=d(),u(He.$$.fragment),$o=d(),ne=n("h2"),pe=n("a"),ao=n("span"),u(Ve.$$.fragment),da=d(),ro=n("span"),la=r("FSMTForConditionalGeneration"),zo=d(),z=n("div"),u(Qe.$$.fragment),ca=d(),io=n("p"),ha=r("The FSMT Model with a language modeling head. Can be used for summarization."),ma=d(),Xe=n("p"),fa=r("This model inherits from "),ut=n("a"),pa=r("PreTrainedModel"),ua=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_a=d(),Ke=n("p"),ga=r("This model is also a PyTorch "),Ye=n("a"),ka=r("torch.nn.Module"),va=r(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ta=d(),C=n("div"),u(Je.$$.fragment),ba=d(),se=n("p"),ya=r("The "),_t=n("a"),Ma=r("FSMTForConditionalGeneration"),wa=r(" forward method, overrides the "),lo=n("code"),Fa=r("__call__"),Sa=r(" special method."),$a=d(),u(ue.$$.fragment),za=d(),co=n("p"),qa=r("Translation example::"),Ea=d(),u(Ze.$$.fragment),this.h()},l(t){const h=Qr('[data-svelte="svelte-1phssyn"]',document.head);f=s(h,"META",{name:!0,content:!0}),h.forEach(o),q=l(t),p=s(t,"H1",{class:!0});var et=a(p);y=s(et,"A",{id:!0,class:!0,href:!0});var ho=a(y);P=s(ho,"SPAN",{});var Pa=a(P);_(M.$$.fragment,Pa),Pa.forEach(o),ho.forEach(o),w=l(et),A=s(et,"SPAN",{});var Ia=a(A);Xo=i(Ia,"FSMT"),Ia.forEach(o),et.forEach(o),fo=l(t),H=s(t,"P",{});var mo=a(H);Ft=s(mo,"STRONG",{});var Aa=a(Ft);Ko=i(Aa,"DISCLAIMER:"),Aa.forEach(o),Yo=i(mo," If you see something strange, file a "),ve=s(mo,"A",{href:!0,rel:!0});var Da=a(ve);Jo=i(Da,"Github Issue"),Da.forEach(o),Zo=i(mo,` and assign
@stas00.`),mo.forEach(o),po=l(t),V=s(t,"H2",{class:!0});var Eo=a(V);ae=s(Eo,"A",{id:!0,class:!0,href:!0});var Na=a(ae);St=s(Na,"SPAN",{});var La=a(St);_(Te.$$.fragment,La),La.forEach(o),Na.forEach(o),en=l(Eo),$t=s(Eo,"SPAN",{});var ja=a($t);tn=i(ja,"Overview"),ja.forEach(o),Eo.forEach(o),uo=l(t),re=s(t,"P",{});var xo=a(re);on=i(xo,"FSMT (FairSeq MachineTranslation) models were introduced in "),be=s(xo,"A",{href:!0,rel:!0});var Oa=a(be);nn=i(Oa,"Facebook FAIR\u2019s WMT19 News Translation Task Submission"),Oa.forEach(o),sn=i(xo," by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov."),xo.forEach(o),_o=l(t),ot=s(t,"P",{});var Ga=a(ot);an=i(Ga,"The abstract of the paper is the following:"),Ga.forEach(o),go=l(t),nt=s(t,"P",{});var Ba=a(nt);zt=s(Ba,"EM",{});var Wa=a(zt);rn=i(Wa,`This paper describes Facebook FAIR\u2019s submission to the WMT19 shared news translation task. We participate in two
language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from
last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling
toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,
as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific
data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the
human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.
This system improves upon our WMT\u201918 submission by 4.5 BLEU points.`),Wa.forEach(o),Ba.forEach(o),ko=l(t),L=s(t,"P",{});var gt=a(L);dn=i(gt,"This model was contributed by "),ye=s(gt,"A",{href:!0,rel:!0});var Ra=a(ye);ln=i(Ra,"stas"),Ra.forEach(o),cn=i(gt,`. The original code can be found
`),Me=s(gt,"A",{href:!0,rel:!0});var Ua=a(Me);hn=i(Ua,"here"),Ua.forEach(o),mn=i(gt,"."),gt.forEach(o),vo=l(t),Q=s(t,"H2",{class:!0});var Co=a(Q);ie=s(Co,"A",{id:!0,class:!0,href:!0});var Ha=a(ie);qt=s(Ha,"SPAN",{});var Va=a(qt);_(we.$$.fragment,Va),Va.forEach(o),Ha.forEach(o),fn=l(Co),Et=s(Co,"SPAN",{});var Qa=a(Et);pn=i(Qa,"Implementation Notes"),Qa.forEach(o),Co.forEach(o),To=l(t),st=s(t,"UL",{});var Xa=a(st);X=s(Xa,"LI",{});var kt=a(X);un=i(kt,`FSMT uses source and target vocabulary pairs that aren\u2019t combined into one. It doesn\u2019t share embeddings tokens
either. Its tokenizer is very similar to `),at=s(kt,"A",{href:!0});var Ka=a(at);_n=i(Ka,"XLMTokenizer"),Ka.forEach(o),gn=i(kt,` and the main model is derived from
`),rt=s(kt,"A",{href:!0});var Ya=a(rt);kn=i(Ya,"BartModel"),Ya.forEach(o),vn=i(kt,"."),kt.forEach(o),Xa.forEach(o),bo=l(t),K=s(t,"H2",{class:!0});var Po=a(K);de=s(Po,"A",{id:!0,class:!0,href:!0});var Ja=a(de);xt=s(Ja,"SPAN",{});var Za=a(xt);_(Fe.$$.fragment,Za),Za.forEach(o),Ja.forEach(o),Tn=l(Po),Ct=s(Po,"SPAN",{});var er=a(Ct);bn=i(er,"FSMTConfig"),er.forEach(o),Po.forEach(o),yo=l(t),F=s(t,"DIV",{class:!0});var I=a(F);_(Se.$$.fragment,I),yn=l(I),$e=s(I,"P",{});var Io=a($e);Mn=i(Io,"This is the configuration class to store the configuration of a "),it=s(Io,"A",{href:!0});var tr=a(it);wn=i(tr,"FSMTModel"),tr.forEach(o),Fn=i(Io,`. It is used to instantiate a FSMT
model according to the specified arguments, defining the model architecture.`),Io.forEach(o),Sn=l(I),Y=s(I,"P",{});var vt=a(Y);$n=i(vt,"Configuration objects inherit from "),dt=s(vt,"A",{href:!0});var or=a(dt);zn=i(or,"PretrainedConfig"),or.forEach(o),qn=i(vt,` and can be used to control the model outputs. Read the
documentation from `),lt=s(vt,"A",{href:!0});var nr=a(lt);En=i(nr,"PretrainedConfig"),nr.forEach(o),xn=i(vt," for more information."),vt.forEach(o),Cn=l(I),Pt=s(I,"P",{});var sr=a(Pt);Pn=i(sr,"Examples:"),sr.forEach(o),In=l(I),_(ze.$$.fragment,I),An=l(I),le=s(I,"DIV",{class:!0});var Ao=a(le);_(qe.$$.fragment,Ao),Dn=l(Ao),J=s(Ao,"P",{});var Tt=a(J);Nn=i(Tt,"Serializes this instance to a Python dictionary. Override the default "),It=s(Tt,"EM",{});var ar=a(It);Ln=i(ar,"to_dict()"),ar.forEach(o),jn=i(Tt," from "),At=s(Tt,"EM",{});var rr=a(At);On=i(rr,"PretrainedConfig"),rr.forEach(o),Gn=i(Tt,"."),Tt.forEach(o),Ao.forEach(o),I.forEach(o),Mo=l(t),Z=s(t,"H2",{class:!0});var Do=a(Z);ce=s(Do,"A",{id:!0,class:!0,href:!0});var ir=a(ce);Dt=s(ir,"SPAN",{});var dr=a(Dt);_(Ee.$$.fragment,dr),dr.forEach(o),ir.forEach(o),Bn=l(Do),Nt=s(Do,"SPAN",{});var lr=a(Nt);Wn=i(lr,"FSMTTokenizer"),lr.forEach(o),Do.forEach(o),wo=l(t),b=s(t,"DIV",{class:!0});var S=a(b);_(xe.$$.fragment,S),Rn=l(S),Lt=s(S,"P",{});var cr=a(Lt);Un=i(cr,"Construct an FAIRSEQ Transformer tokenizer. Based on Byte-Pair Encoding. The tokenization process is the following:"),cr.forEach(o),Hn=l(S),D=s(S,"UL",{});var _e=a(D);jt=s(_e,"LI",{});var hr=a(jt);Vn=i(hr,"Moses preprocessing and tokenization."),hr.forEach(o),Qn=l(_e),Ot=s(_e,"LI",{});var mr=a(Ot);Xn=i(mr,"Normalizing all inputs text."),mr.forEach(o),Kn=l(_e),N=s(_e,"LI",{});var ge=a(N);Yn=i(ge,"The arguments "),Gt=s(ge,"CODE",{});var fr=a(Gt);Jn=i(fr,"special_tokens"),fr.forEach(o),Zn=i(ge," and the function "),Bt=s(ge,"CODE",{});var pr=a(Bt);es=i(pr,"set_special_tokens"),pr.forEach(o),ts=i(ge,`, can be used to add additional symbols (like
\u201D`),Wt=s(ge,"STRONG",{});var ur=a(Wt);os=i(ur,"classify"),ur.forEach(o),ns=i(ge,"\u201D) to a vocabulary."),ge.forEach(o),ss=l(_e),Ce=s(_e,"LI",{});var No=a(Ce);as=i(No,"The argument "),Rt=s(No,"CODE",{});var _r=a(Rt);rs=i(_r,"langs"),_r.forEach(o),is=i(No," defines a pair of languages."),No.forEach(o),_e.forEach(o),ds=l(S),Pe=s(S,"P",{});var Lo=a(Pe);ls=i(Lo,"This tokenizer inherits from "),ct=s(Lo,"A",{href:!0});var gr=a(ct);cs=i(gr,"PreTrainedTokenizer"),gr.forEach(o),hs=i(Lo,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Lo.forEach(o),ms=l(S),j=s(S,"DIV",{class:!0});var bt=a(j);_(Ie.$$.fragment,bt),fs=l(bt),Ut=s(bt,"P",{});var kr=a(Ut);ps=i(kr,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A FAIRSEQ Transformer sequence has the following format:`),kr.forEach(o),us=l(bt),Ae=s(bt,"UL",{});var jo=a(Ae);ht=s(jo,"LI",{});var xa=a(ht);_s=i(xa,"single sequence: "),Ht=s(xa,"CODE",{});var vr=a(Ht);gs=i(vr,"<s> X </s>"),vr.forEach(o),xa.forEach(o),ks=l(jo),mt=s(jo,"LI",{});var Ca=a(mt);vs=i(Ca,"pair of sequences: "),Vt=s(Ca,"CODE",{});var Tr=a(Vt);Ts=i(Tr,"<s> A </s> B </s>"),Tr.forEach(o),Ca.forEach(o),jo.forEach(o),bt.forEach(o),bs=l(S),he=s(S,"DIV",{class:!0});var Oo=a(he);_(De.$$.fragment,Oo),ys=l(Oo),Ne=s(Oo,"P",{});var Go=a(Ne);Ms=i(Go,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Qt=s(Go,"CODE",{});var br=a(Qt);ws=i(br,"prepare_for_model"),br.forEach(o),Fs=i(Go," method."),Go.forEach(o),Oo.forEach(o),Ss=l(S),E=s(S,"DIV",{class:!0});var O=a(E);_(Le.$$.fragment,O),$s=l(O),Xt=s(O,"P",{});var yr=a(Xt);zs=i(yr,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ
Transformer sequence pair mask has the following format:`),yr.forEach(o),qs=l(O),_(je.$$.fragment,O),Es=l(O),ee=s(O,"P",{});var yt=a(ee);xs=i(yt,"If "),Kt=s(yt,"CODE",{});var Mr=a(Kt);Cs=i(Mr,"token_ids_1"),Mr.forEach(o),Ps=i(yt," is "),Yt=s(yt,"CODE",{});var wr=a(Yt);Is=i(wr,"None"),wr.forEach(o),As=i(yt,", this method only returns the first portion of the mask (0s)."),yt.forEach(o),Ds=l(O),Jt=s(O,"P",{});var Fr=a(Jt);Ns=i(Fr,`Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An
FAIRSEQ_TRANSFORMER sequence pair mask has the following format:`),Fr.forEach(o),O.forEach(o),Ls=l(S),Zt=s(S,"DIV",{class:!0}),a(Zt).forEach(o),S.forEach(o),Fo=l(t),te=s(t,"H2",{class:!0});var Bo=a(te);me=s(Bo,"A",{id:!0,class:!0,href:!0});var Sr=a(me);eo=s(Sr,"SPAN",{});var $r=a(eo);_(Oe.$$.fragment,$r),$r.forEach(o),Sr.forEach(o),js=l(Bo),to=s(Bo,"SPAN",{});var zr=a(to);Os=i(zr,"FSMTModel"),zr.forEach(o),Bo.forEach(o),So=l(t),$=s(t,"DIV",{class:!0});var G=a($);_(Ge.$$.fragment,G),Gs=l(G),oo=s(G,"P",{});var qr=a(oo);Bs=i(qr,"The bare FSMT Model outputting raw hidden-states without any specific head on top."),qr.forEach(o),Ws=l(G),Be=s(G,"P",{});var Wo=a(Be);Rs=i(Wo,"This model inherits from "),ft=s(Wo,"A",{href:!0});var Er=a(ft);Us=i(Er,"PreTrainedModel"),Er.forEach(o),Hs=i(Wo,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wo.forEach(o),Vs=l(G),We=s(G,"P",{});var Ro=a(We);Qs=i(Ro,"This model is also a PyTorch "),Re=s(Ro,"A",{href:!0,rel:!0});var xr=a(Re);Xs=i(xr,"torch.nn.Module"),xr.forEach(o),Ks=i(Ro,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ro.forEach(o),Ys=l(G),x=s(G,"DIV",{class:!0});var B=a(x);_(Ue.$$.fragment,B),Js=l(B),oe=s(B,"P",{});var Mt=a(oe);Zs=i(Mt,"The "),pt=s(Mt,"A",{href:!0});var Cr=a(pt);ea=i(Cr,"FSMTModel"),Cr.forEach(o),ta=i(Mt," forward method, overrides the "),no=s(Mt,"CODE",{});var Pr=a(no);oa=i(Pr,"__call__"),Pr.forEach(o),na=i(Mt," special method."),Mt.forEach(o),sa=l(B),_(fe.$$.fragment,B),aa=l(B),so=s(B,"P",{});var Ir=a(so);ra=i(Ir,"Example:"),Ir.forEach(o),ia=l(B),_(He.$$.fragment,B),B.forEach(o),G.forEach(o),$o=l(t),ne=s(t,"H2",{class:!0});var Uo=a(ne);pe=s(Uo,"A",{id:!0,class:!0,href:!0});var Ar=a(pe);ao=s(Ar,"SPAN",{});var Dr=a(ao);_(Ve.$$.fragment,Dr),Dr.forEach(o),Ar.forEach(o),da=l(Uo),ro=s(Uo,"SPAN",{});var Nr=a(ro);la=i(Nr,"FSMTForConditionalGeneration"),Nr.forEach(o),Uo.forEach(o),zo=l(t),z=s(t,"DIV",{class:!0});var W=a(z);_(Qe.$$.fragment,W),ca=l(W),io=s(W,"P",{});var Lr=a(io);ha=i(Lr,"The FSMT Model with a language modeling head. Can be used for summarization."),Lr.forEach(o),ma=l(W),Xe=s(W,"P",{});var Ho=a(Xe);fa=i(Ho,"This model inherits from "),ut=s(Ho,"A",{href:!0});var jr=a(ut);pa=i(jr,"PreTrainedModel"),jr.forEach(o),ua=i(Ho,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ho.forEach(o),_a=l(W),Ke=s(W,"P",{});var Vo=a(Ke);ga=i(Vo,"This model is also a PyTorch "),Ye=s(Vo,"A",{href:!0,rel:!0});var Or=a(Ye);ka=i(Or,"torch.nn.Module"),Or.forEach(o),va=i(Vo,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vo.forEach(o),Ta=l(W),C=s(W,"DIV",{class:!0});var R=a(C);_(Je.$$.fragment,R),ba=l(R),se=s(R,"P",{});var wt=a(se);ya=i(wt,"The "),_t=s(wt,"A",{href:!0});var Gr=a(_t);Ma=i(Gr,"FSMTForConditionalGeneration"),Gr.forEach(o),wa=i(wt," forward method, overrides the "),lo=s(wt,"CODE",{});var Br=a(lo);Fa=i(Br,"__call__"),Br.forEach(o),Sa=i(wt," special method."),wt.forEach(o),$a=l(R),_(ue.$$.fragment,R),za=l(R),co=s(R,"P",{});var Wr=a(co);qa=i(Wr,"Translation example::"),Wr.forEach(o),Ea=l(R),_(Ze.$$.fragment,R),R.forEach(o),W.forEach(o),this.h()},h(){c(f,"name","hf:doc:metadata"),c(f,"content",JSON.stringify(Jr)),c(y,"id","fsmt"),c(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(y,"href","#fsmt"),c(p,"class","relative group"),c(ve,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),c(ve,"rel","nofollow"),c(ae,"id","overview"),c(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ae,"href","#overview"),c(V,"class","relative group"),c(be,"href","https://arxiv.org/abs/1907.06616"),c(be,"rel","nofollow"),c(ye,"href","https://huggingface.co/stas"),c(ye,"rel","nofollow"),c(Me,"href","https://github.com/pytorch/fairseq/tree/master/examples/wmt19"),c(Me,"rel","nofollow"),c(ie,"id","implementation-notes"),c(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ie,"href","#implementation-notes"),c(Q,"class","relative group"),c(at,"href","/docs/transformers/v4.16.2/en/model_doc/xlm#transformers.XLMTokenizer"),c(rt,"href","/docs/transformers/v4.16.2/en/model_doc/bart#transformers.BartModel"),c(de,"id","transformers.FSMTConfig"),c(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(de,"href","#transformers.FSMTConfig"),c(K,"class","relative group"),c(it,"href","/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTModel"),c(dt,"href","/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig"),c(lt,"href","/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig"),c(le,"class","docstring"),c(F,"class","docstring"),c(ce,"id","transformers.FSMTTokenizer"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#transformers.FSMTTokenizer"),c(Z,"class","relative group"),c(ct,"href","/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(j,"class","docstring"),c(he,"class","docstring"),c(E,"class","docstring"),c(Zt,"class","docstring"),c(b,"class","docstring"),c(me,"id","transformers.FSMTModel"),c(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(me,"href","#transformers.FSMTModel"),c(te,"class","relative group"),c(ft,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(Re,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Re,"rel","nofollow"),c(pt,"href","/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTModel"),c(x,"class","docstring"),c($,"class","docstring"),c(pe,"id","transformers.FSMTForConditionalGeneration"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#transformers.FSMTForConditionalGeneration"),c(ne,"class","relative group"),c(ut,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(Ye,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ye,"rel","nofollow"),c(_t,"href","/docs/transformers/v4.16.2/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(C,"class","docstring"),c(z,"class","docstring")},m(t,h){e(document.head,f),m(t,q,h),m(t,p,h),e(p,y),e(y,P),g(M,P,null),e(p,w),e(p,A),e(A,Xo),m(t,fo,h),m(t,H,h),e(H,Ft),e(Ft,Ko),e(H,Yo),e(H,ve),e(ve,Jo),e(H,Zo),m(t,po,h),m(t,V,h),e(V,ae),e(ae,St),g(Te,St,null),e(V,en),e(V,$t),e($t,tn),m(t,uo,h),m(t,re,h),e(re,on),e(re,be),e(be,nn),e(re,sn),m(t,_o,h),m(t,ot,h),e(ot,an),m(t,go,h),m(t,nt,h),e(nt,zt),e(zt,rn),m(t,ko,h),m(t,L,h),e(L,dn),e(L,ye),e(ye,ln),e(L,cn),e(L,Me),e(Me,hn),e(L,mn),m(t,vo,h),m(t,Q,h),e(Q,ie),e(ie,qt),g(we,qt,null),e(Q,fn),e(Q,Et),e(Et,pn),m(t,To,h),m(t,st,h),e(st,X),e(X,un),e(X,at),e(at,_n),e(X,gn),e(X,rt),e(rt,kn),e(X,vn),m(t,bo,h),m(t,K,h),e(K,de),e(de,xt),g(Fe,xt,null),e(K,Tn),e(K,Ct),e(Ct,bn),m(t,yo,h),m(t,F,h),g(Se,F,null),e(F,yn),e(F,$e),e($e,Mn),e($e,it),e(it,wn),e($e,Fn),e(F,Sn),e(F,Y),e(Y,$n),e(Y,dt),e(dt,zn),e(Y,qn),e(Y,lt),e(lt,En),e(Y,xn),e(F,Cn),e(F,Pt),e(Pt,Pn),e(F,In),g(ze,F,null),e(F,An),e(F,le),g(qe,le,null),e(le,Dn),e(le,J),e(J,Nn),e(J,It),e(It,Ln),e(J,jn),e(J,At),e(At,On),e(J,Gn),m(t,Mo,h),m(t,Z,h),e(Z,ce),e(ce,Dt),g(Ee,Dt,null),e(Z,Bn),e(Z,Nt),e(Nt,Wn),m(t,wo,h),m(t,b,h),g(xe,b,null),e(b,Rn),e(b,Lt),e(Lt,Un),e(b,Hn),e(b,D),e(D,jt),e(jt,Vn),e(D,Qn),e(D,Ot),e(Ot,Xn),e(D,Kn),e(D,N),e(N,Yn),e(N,Gt),e(Gt,Jn),e(N,Zn),e(N,Bt),e(Bt,es),e(N,ts),e(N,Wt),e(Wt,os),e(N,ns),e(D,ss),e(D,Ce),e(Ce,as),e(Ce,Rt),e(Rt,rs),e(Ce,is),e(b,ds),e(b,Pe),e(Pe,ls),e(Pe,ct),e(ct,cs),e(Pe,hs),e(b,ms),e(b,j),g(Ie,j,null),e(j,fs),e(j,Ut),e(Ut,ps),e(j,us),e(j,Ae),e(Ae,ht),e(ht,_s),e(ht,Ht),e(Ht,gs),e(Ae,ks),e(Ae,mt),e(mt,vs),e(mt,Vt),e(Vt,Ts),e(b,bs),e(b,he),g(De,he,null),e(he,ys),e(he,Ne),e(Ne,Ms),e(Ne,Qt),e(Qt,ws),e(Ne,Fs),e(b,Ss),e(b,E),g(Le,E,null),e(E,$s),e(E,Xt),e(Xt,zs),e(E,qs),g(je,E,null),e(E,Es),e(E,ee),e(ee,xs),e(ee,Kt),e(Kt,Cs),e(ee,Ps),e(ee,Yt),e(Yt,Is),e(ee,As),e(E,Ds),e(E,Jt),e(Jt,Ns),e(b,Ls),e(b,Zt),m(t,Fo,h),m(t,te,h),e(te,me),e(me,eo),g(Oe,eo,null),e(te,js),e(te,to),e(to,Os),m(t,So,h),m(t,$,h),g(Ge,$,null),e($,Gs),e($,oo),e(oo,Bs),e($,Ws),e($,Be),e(Be,Rs),e(Be,ft),e(ft,Us),e(Be,Hs),e($,Vs),e($,We),e(We,Qs),e(We,Re),e(Re,Xs),e(We,Ks),e($,Ys),e($,x),g(Ue,x,null),e(x,Js),e(x,oe),e(oe,Zs),e(oe,pt),e(pt,ea),e(oe,ta),e(oe,no),e(no,oa),e(oe,na),e(x,sa),g(fe,x,null),e(x,aa),e(x,so),e(so,ra),e(x,ia),g(He,x,null),m(t,$o,h),m(t,ne,h),e(ne,pe),e(pe,ao),g(Ve,ao,null),e(ne,da),e(ne,ro),e(ro,la),m(t,zo,h),m(t,z,h),g(Qe,z,null),e(z,ca),e(z,io),e(io,ha),e(z,ma),e(z,Xe),e(Xe,fa),e(Xe,ut),e(ut,pa),e(Xe,ua),e(z,_a),e(z,Ke),e(Ke,ga),e(Ke,Ye),e(Ye,ka),e(Ke,va),e(z,Ta),e(z,C),g(Je,C,null),e(C,ba),e(C,se),e(se,ya),e(se,_t),e(_t,Ma),e(se,wa),e(se,lo),e(lo,Fa),e(se,Sa),e(C,$a),g(ue,C,null),e(C,za),e(C,co),e(co,qa),e(C,Ea),g(Ze,C,null),qo=!0},p(t,[h]){const et={};h&2&&(et.$$scope={dirty:h,ctx:t}),fe.$set(et);const ho={};h&2&&(ho.$$scope={dirty:h,ctx:t}),ue.$set(ho)},i(t){qo||(k(M.$$.fragment,t),k(Te.$$.fragment,t),k(we.$$.fragment,t),k(Fe.$$.fragment,t),k(Se.$$.fragment,t),k(ze.$$.fragment,t),k(qe.$$.fragment,t),k(Ee.$$.fragment,t),k(xe.$$.fragment,t),k(Ie.$$.fragment,t),k(De.$$.fragment,t),k(Le.$$.fragment,t),k(je.$$.fragment,t),k(Oe.$$.fragment,t),k(Ge.$$.fragment,t),k(Ue.$$.fragment,t),k(fe.$$.fragment,t),k(He.$$.fragment,t),k(Ve.$$.fragment,t),k(Qe.$$.fragment,t),k(Je.$$.fragment,t),k(ue.$$.fragment,t),k(Ze.$$.fragment,t),qo=!0)},o(t){v(M.$$.fragment,t),v(Te.$$.fragment,t),v(we.$$.fragment,t),v(Fe.$$.fragment,t),v(Se.$$.fragment,t),v(ze.$$.fragment,t),v(qe.$$.fragment,t),v(Ee.$$.fragment,t),v(xe.$$.fragment,t),v(Ie.$$.fragment,t),v(De.$$.fragment,t),v(Le.$$.fragment,t),v(je.$$.fragment,t),v(Oe.$$.fragment,t),v(Ge.$$.fragment,t),v(Ue.$$.fragment,t),v(fe.$$.fragment,t),v(He.$$.fragment,t),v(Ve.$$.fragment,t),v(Qe.$$.fragment,t),v(Je.$$.fragment,t),v(ue.$$.fragment,t),v(Ze.$$.fragment,t),qo=!1},d(t){o(f),t&&o(q),t&&o(p),T(M),t&&o(fo),t&&o(H),t&&o(po),t&&o(V),T(Te),t&&o(uo),t&&o(re),t&&o(_o),t&&o(ot),t&&o(go),t&&o(nt),t&&o(ko),t&&o(L),t&&o(vo),t&&o(Q),T(we),t&&o(To),t&&o(st),t&&o(bo),t&&o(K),T(Fe),t&&o(yo),t&&o(F),T(Se),T(ze),T(qe),t&&o(Mo),t&&o(Z),T(Ee),t&&o(wo),t&&o(b),T(xe),T(Ie),T(De),T(Le),T(je),t&&o(Fo),t&&o(te),T(Oe),t&&o(So),t&&o($),T(Ge),T(Ue),T(fe),T(He),t&&o($o),t&&o(ne),T(Ve),t&&o(zo),t&&o(z),T(Qe),T(Je),T(ue),T(Ze)}}}const Jr={local:"fsmt",sections:[{local:"overview",title:"Overview"},{local:"implementation-notes",title:"Implementation Notes"},{local:"transformers.FSMTConfig",title:"FSMTConfig"},{local:"transformers.FSMTTokenizer",title:"FSMTTokenizer"},{local:"transformers.FSMTModel",title:"FSMTModel"},{local:"transformers.FSMTForConditionalGeneration",title:"FSMTForConditionalGeneration"}],title:"FSMT"};function Zr(ke,f,q){let{fw:p}=f;return ke.$$set=y=>{"fw"in y&&q(0,p=y.fw)},[p]}class ri extends Ur{constructor(f){super();Hr(this,f,Zr,Yr,Vr,{fw:0})}}export{ri as default,Jr as metadata};
