import{S as Nu,i as Uu,s as Hu,e as r,k as d,w as $,t as n,L as Qu,c as s,d as t,m,a,x as F,h as i,b as c,J as e,g as h,y as P,q as M,o as q,B as L}from"../../chunks/vendor-b1433968.js";import{T as zs}from"../../chunks/Tip-c3840994.js";import{D as Fe}from"../../chunks/Docstring-ff504c58.js";import{I as Pe}from"../../chunks/IconCopyLink-7029626d.js";function Vu(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue;return{c(){b=r("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),A=d(),_=r("ul"),z=r("li"),G=n("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=r("li"),de=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=r("p"),K=n("This second option is useful when using "),x=r("code"),X=n("tf.keras.Model.fit"),me=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r("code"),ce=n("model(inputs)"),se=n("."),H=d(),E=r("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=r("ul"),k=r("li"),ee=n("a single Tensor with "),O=r("code"),ae=n("input_ids"),te=n(" only and nothing else: "),S=r("code"),he=n("model(inputs_ids)"),ne=d(),v=r("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),oe=n("model([input_ids, attention_mask])"),J=n(" or "),N=r("code"),re=n("model([input_ids, attention_mask, token_type_ids])"),pe=d(),C=r("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=r("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){b=s(l,"P",{});var p=a(b);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),A=m(l),_=s(l,"UL",{});var W=a(_);z=s(W,"LI",{});var ke=a(z);G=i(ke,"having all inputs as keyword arguments (like PyTorch models), or"),ke.forEach(t),le=m(W),D=s(W,"LI",{});var Ee=a(D);de=i(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),W.forEach(t),Q=m(l),u=s(l,"P",{});var y=a(u);K=i(y,"This second option is useful when using "),x=s(y,"CODE",{});var Te=a(x);X=i(Te,"tf.keras.Model.fit"),Te.forEach(t),me=i(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=s(y,"CODE",{});var ge=a(I);ce=i(ge,"model(inputs)"),ge.forEach(t),se=i(y,"."),y.forEach(t),H=m(l),E=s(l,"P",{});var ve=a(E);j=i(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=s(l,"UL",{});var T=a(g);k=s(T,"LI",{});var R=a(k);ee=i(R,"a single Tensor with "),O=s(R,"CODE",{});var Ce=a(O);ae=i(Ce,"input_ids"),Ce.forEach(t),te=i(R," only and nothing else: "),S=s(R,"CODE",{});var be=a(S);he=i(be,"model(inputs_ids)"),be.forEach(t),R.forEach(t),ne=m(T),v=s(T,"LI",{});var U=a(v);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s(U,"CODE",{});var we=a(B);oe=i(we,"model([input_ids, attention_mask])"),we.forEach(t),J=i(U," or "),N=s(U,"CODE",{});var _e=a(N);re=i(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),U.forEach(t),pe=m(T),C=s(T,"LI",{});var Y=a(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=s(Y,"CODE",{});var ye=a(w);ue=i(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,b,p),e(b,Z),h(l,A,p),h(l,_,p),e(_,z),e(z,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,K),e(u,x),e(x,X),e(u,me),e(u,I),e(I,ce),e(u,se),h(l,H,p),h(l,E,p),e(E,j),h(l,V,p),h(l,g,p),e(g,k),e(k,ee),e(k,O),e(O,ae),e(k,te),e(k,S),e(S,he),e(g,ne),e(g,v),e(v,fe),e(v,B),e(B,oe),e(v,J),e(v,N),e(N,re),e(g,pe),e(g,C),e(C,ie),e(C,w),e(w,ue)},d(l){l&&t(b),l&&t(A),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(E),l&&t(V),l&&t(g)}}}function Wu(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue;return{c(){b=r("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),A=d(),_=r("ul"),z=r("li"),G=n("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=r("li"),de=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=r("p"),K=n("This second option is useful when using "),x=r("code"),X=n("tf.keras.Model.fit"),me=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r("code"),ce=n("model(inputs)"),se=n("."),H=d(),E=r("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=r("ul"),k=r("li"),ee=n("a single Tensor with "),O=r("code"),ae=n("input_ids"),te=n(" only and nothing else: "),S=r("code"),he=n("model(inputs_ids)"),ne=d(),v=r("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),oe=n("model([input_ids, attention_mask])"),J=n(" or "),N=r("code"),re=n("model([input_ids, attention_mask, token_type_ids])"),pe=d(),C=r("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=r("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){b=s(l,"P",{});var p=a(b);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),A=m(l),_=s(l,"UL",{});var W=a(_);z=s(W,"LI",{});var ke=a(z);G=i(ke,"having all inputs as keyword arguments (like PyTorch models), or"),ke.forEach(t),le=m(W),D=s(W,"LI",{});var Ee=a(D);de=i(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),W.forEach(t),Q=m(l),u=s(l,"P",{});var y=a(u);K=i(y,"This second option is useful when using "),x=s(y,"CODE",{});var Te=a(x);X=i(Te,"tf.keras.Model.fit"),Te.forEach(t),me=i(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=s(y,"CODE",{});var ge=a(I);ce=i(ge,"model(inputs)"),ge.forEach(t),se=i(y,"."),y.forEach(t),H=m(l),E=s(l,"P",{});var ve=a(E);j=i(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=s(l,"UL",{});var T=a(g);k=s(T,"LI",{});var R=a(k);ee=i(R,"a single Tensor with "),O=s(R,"CODE",{});var Ce=a(O);ae=i(Ce,"input_ids"),Ce.forEach(t),te=i(R," only and nothing else: "),S=s(R,"CODE",{});var be=a(S);he=i(be,"model(inputs_ids)"),be.forEach(t),R.forEach(t),ne=m(T),v=s(T,"LI",{});var U=a(v);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s(U,"CODE",{});var we=a(B);oe=i(we,"model([input_ids, attention_mask])"),we.forEach(t),J=i(U," or "),N=s(U,"CODE",{});var _e=a(N);re=i(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),U.forEach(t),pe=m(T),C=s(T,"LI",{});var Y=a(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=s(Y,"CODE",{});var ye=a(w);ue=i(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,b,p),e(b,Z),h(l,A,p),h(l,_,p),e(_,z),e(z,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,K),e(u,x),e(x,X),e(u,me),e(u,I),e(I,ce),e(u,se),h(l,H,p),h(l,E,p),e(E,j),h(l,V,p),h(l,g,p),e(g,k),e(k,ee),e(k,O),e(O,ae),e(k,te),e(k,S),e(S,he),e(g,ne),e(g,v),e(v,fe),e(v,B),e(B,oe),e(v,J),e(v,N),e(N,re),e(g,pe),e(g,C),e(C,ie),e(C,w),e(w,ue)},d(l){l&&t(b),l&&t(A),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(E),l&&t(V),l&&t(g)}}}function Ku(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue;return{c(){b=r("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),A=d(),_=r("ul"),z=r("li"),G=n("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=r("li"),de=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=r("p"),K=n("This second option is useful when using "),x=r("code"),X=n("tf.keras.Model.fit"),me=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r("code"),ce=n("model(inputs)"),se=n("."),H=d(),E=r("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=r("ul"),k=r("li"),ee=n("a single Tensor with "),O=r("code"),ae=n("input_ids"),te=n(" only and nothing else: "),S=r("code"),he=n("model(inputs_ids)"),ne=d(),v=r("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),oe=n("model([input_ids, attention_mask])"),J=n(" or "),N=r("code"),re=n("model([input_ids, attention_mask, token_type_ids])"),pe=d(),C=r("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=r("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){b=s(l,"P",{});var p=a(b);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),A=m(l),_=s(l,"UL",{});var W=a(_);z=s(W,"LI",{});var ke=a(z);G=i(ke,"having all inputs as keyword arguments (like PyTorch models), or"),ke.forEach(t),le=m(W),D=s(W,"LI",{});var Ee=a(D);de=i(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),W.forEach(t),Q=m(l),u=s(l,"P",{});var y=a(u);K=i(y,"This second option is useful when using "),x=s(y,"CODE",{});var Te=a(x);X=i(Te,"tf.keras.Model.fit"),Te.forEach(t),me=i(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=s(y,"CODE",{});var ge=a(I);ce=i(ge,"model(inputs)"),ge.forEach(t),se=i(y,"."),y.forEach(t),H=m(l),E=s(l,"P",{});var ve=a(E);j=i(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=s(l,"UL",{});var T=a(g);k=s(T,"LI",{});var R=a(k);ee=i(R,"a single Tensor with "),O=s(R,"CODE",{});var Ce=a(O);ae=i(Ce,"input_ids"),Ce.forEach(t),te=i(R," only and nothing else: "),S=s(R,"CODE",{});var be=a(S);he=i(be,"model(inputs_ids)"),be.forEach(t),R.forEach(t),ne=m(T),v=s(T,"LI",{});var U=a(v);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s(U,"CODE",{});var we=a(B);oe=i(we,"model([input_ids, attention_mask])"),we.forEach(t),J=i(U," or "),N=s(U,"CODE",{});var _e=a(N);re=i(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),U.forEach(t),pe=m(T),C=s(T,"LI",{});var Y=a(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=s(Y,"CODE",{});var ye=a(w);ue=i(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,b,p),e(b,Z),h(l,A,p),h(l,_,p),e(_,z),e(z,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,K),e(u,x),e(x,X),e(u,me),e(u,I),e(I,ce),e(u,se),h(l,H,p),h(l,E,p),e(E,j),h(l,V,p),h(l,g,p),e(g,k),e(k,ee),e(k,O),e(O,ae),e(k,te),e(k,S),e(S,he),e(g,ne),e(g,v),e(v,fe),e(v,B),e(B,oe),e(v,J),e(v,N),e(N,re),e(g,pe),e(g,C),e(C,ie),e(C,w),e(w,ue)},d(l){l&&t(b),l&&t(A),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(E),l&&t(V),l&&t(g)}}}function Gu(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue;return{c(){b=r("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),A=d(),_=r("ul"),z=r("li"),G=n("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=r("li"),de=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=r("p"),K=n("This second option is useful when using "),x=r("code"),X=n("tf.keras.Model.fit"),me=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r("code"),ce=n("model(inputs)"),se=n("."),H=d(),E=r("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=r("ul"),k=r("li"),ee=n("a single Tensor with "),O=r("code"),ae=n("input_ids"),te=n(" only and nothing else: "),S=r("code"),he=n("model(inputs_ids)"),ne=d(),v=r("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),oe=n("model([input_ids, attention_mask])"),J=n(" or "),N=r("code"),re=n("model([input_ids, attention_mask, token_type_ids])"),pe=d(),C=r("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=r("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){b=s(l,"P",{});var p=a(b);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),A=m(l),_=s(l,"UL",{});var W=a(_);z=s(W,"LI",{});var ke=a(z);G=i(ke,"having all inputs as keyword arguments (like PyTorch models), or"),ke.forEach(t),le=m(W),D=s(W,"LI",{});var Ee=a(D);de=i(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),W.forEach(t),Q=m(l),u=s(l,"P",{});var y=a(u);K=i(y,"This second option is useful when using "),x=s(y,"CODE",{});var Te=a(x);X=i(Te,"tf.keras.Model.fit"),Te.forEach(t),me=i(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=s(y,"CODE",{});var ge=a(I);ce=i(ge,"model(inputs)"),ge.forEach(t),se=i(y,"."),y.forEach(t),H=m(l),E=s(l,"P",{});var ve=a(E);j=i(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=s(l,"UL",{});var T=a(g);k=s(T,"LI",{});var R=a(k);ee=i(R,"a single Tensor with "),O=s(R,"CODE",{});var Ce=a(O);ae=i(Ce,"input_ids"),Ce.forEach(t),te=i(R," only and nothing else: "),S=s(R,"CODE",{});var be=a(S);he=i(be,"model(inputs_ids)"),be.forEach(t),R.forEach(t),ne=m(T),v=s(T,"LI",{});var U=a(v);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s(U,"CODE",{});var we=a(B);oe=i(we,"model([input_ids, attention_mask])"),we.forEach(t),J=i(U," or "),N=s(U,"CODE",{});var _e=a(N);re=i(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),U.forEach(t),pe=m(T),C=s(T,"LI",{});var Y=a(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=s(Y,"CODE",{});var ye=a(w);ue=i(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,b,p),e(b,Z),h(l,A,p),h(l,_,p),e(_,z),e(z,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,K),e(u,x),e(x,X),e(u,me),e(u,I),e(I,ce),e(u,se),h(l,H,p),h(l,E,p),e(E,j),h(l,V,p),h(l,g,p),e(g,k),e(k,ee),e(k,O),e(O,ae),e(k,te),e(k,S),e(S,he),e(g,ne),e(g,v),e(v,fe),e(v,B),e(B,oe),e(v,J),e(v,N),e(N,re),e(g,pe),e(g,C),e(C,ie),e(C,w),e(w,ue)},d(l){l&&t(b),l&&t(A),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(E),l&&t(V),l&&t(g)}}}function Xu(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue;return{c(){b=r("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),A=d(),_=r("ul"),z=r("li"),G=n("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=r("li"),de=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=r("p"),K=n("This second option is useful when using "),x=r("code"),X=n("tf.keras.Model.fit"),me=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r("code"),ce=n("model(inputs)"),se=n("."),H=d(),E=r("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=r("ul"),k=r("li"),ee=n("a single Tensor with "),O=r("code"),ae=n("input_ids"),te=n(" only and nothing else: "),S=r("code"),he=n("model(inputs_ids)"),ne=d(),v=r("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),oe=n("model([input_ids, attention_mask])"),J=n(" or "),N=r("code"),re=n("model([input_ids, attention_mask, token_type_ids])"),pe=d(),C=r("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=r("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){b=s(l,"P",{});var p=a(b);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),A=m(l),_=s(l,"UL",{});var W=a(_);z=s(W,"LI",{});var ke=a(z);G=i(ke,"having all inputs as keyword arguments (like PyTorch models), or"),ke.forEach(t),le=m(W),D=s(W,"LI",{});var Ee=a(D);de=i(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),W.forEach(t),Q=m(l),u=s(l,"P",{});var y=a(u);K=i(y,"This second option is useful when using "),x=s(y,"CODE",{});var Te=a(x);X=i(Te,"tf.keras.Model.fit"),Te.forEach(t),me=i(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=s(y,"CODE",{});var ge=a(I);ce=i(ge,"model(inputs)"),ge.forEach(t),se=i(y,"."),y.forEach(t),H=m(l),E=s(l,"P",{});var ve=a(E);j=i(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=s(l,"UL",{});var T=a(g);k=s(T,"LI",{});var R=a(k);ee=i(R,"a single Tensor with "),O=s(R,"CODE",{});var Ce=a(O);ae=i(Ce,"input_ids"),Ce.forEach(t),te=i(R," only and nothing else: "),S=s(R,"CODE",{});var be=a(S);he=i(be,"model(inputs_ids)"),be.forEach(t),R.forEach(t),ne=m(T),v=s(T,"LI",{});var U=a(v);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s(U,"CODE",{});var we=a(B);oe=i(we,"model([input_ids, attention_mask])"),we.forEach(t),J=i(U," or "),N=s(U,"CODE",{});var _e=a(N);re=i(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),U.forEach(t),pe=m(T),C=s(T,"LI",{});var Y=a(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=s(Y,"CODE",{});var ye=a(w);ue=i(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,b,p),e(b,Z),h(l,A,p),h(l,_,p),e(_,z),e(z,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,K),e(u,x),e(x,X),e(u,me),e(u,I),e(I,ce),e(u,se),h(l,H,p),h(l,E,p),e(E,j),h(l,V,p),h(l,g,p),e(g,k),e(k,ee),e(k,O),e(O,ae),e(k,te),e(k,S),e(S,he),e(g,ne),e(g,v),e(v,fe),e(v,B),e(B,oe),e(v,J),e(v,N),e(N,re),e(g,pe),e(g,C),e(C,ie),e(C,w),e(w,ue)},d(l){l&&t(b),l&&t(A),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(E),l&&t(V),l&&t(g)}}}function ju(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue;return{c(){b=r("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),A=d(),_=r("ul"),z=r("li"),G=n("having all inputs as keyword arguments (like PyTorch models), or"),le=d(),D=r("li"),de=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=d(),u=r("p"),K=n("This second option is useful when using "),x=r("code"),X=n("tf.keras.Model.fit"),me=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=r("code"),ce=n("model(inputs)"),se=n("."),H=d(),E=r("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=d(),g=r("ul"),k=r("li"),ee=n("a single Tensor with "),O=r("code"),ae=n("input_ids"),te=n(" only and nothing else: "),S=r("code"),he=n("model(inputs_ids)"),ne=d(),v=r("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r("code"),oe=n("model([input_ids, attention_mask])"),J=n(" or "),N=r("code"),re=n("model([input_ids, attention_mask, token_type_ids])"),pe=d(),C=r("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=r("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){b=s(l,"P",{});var p=a(b);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),A=m(l),_=s(l,"UL",{});var W=a(_);z=s(W,"LI",{});var ke=a(z);G=i(ke,"having all inputs as keyword arguments (like PyTorch models), or"),ke.forEach(t),le=m(W),D=s(W,"LI",{});var Ee=a(D);de=i(Ee,"having all inputs as a list, tuple or dict in the first positional arguments."),Ee.forEach(t),W.forEach(t),Q=m(l),u=s(l,"P",{});var y=a(u);K=i(y,"This second option is useful when using "),x=s(y,"CODE",{});var Te=a(x);X=i(Te,"tf.keras.Model.fit"),Te.forEach(t),me=i(y,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=s(y,"CODE",{});var ge=a(I);ce=i(ge,"model(inputs)"),ge.forEach(t),se=i(y,"."),y.forEach(t),H=m(l),E=s(l,"P",{});var ve=a(E);j=i(ve,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ve.forEach(t),V=m(l),g=s(l,"UL",{});var T=a(g);k=s(T,"LI",{});var R=a(k);ee=i(R,"a single Tensor with "),O=s(R,"CODE",{});var Ce=a(O);ae=i(Ce,"input_ids"),Ce.forEach(t),te=i(R," only and nothing else: "),S=s(R,"CODE",{});var be=a(S);he=i(be,"model(inputs_ids)"),be.forEach(t),R.forEach(t),ne=m(T),v=s(T,"LI",{});var U=a(v);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s(U,"CODE",{});var we=a(B);oe=i(we,"model([input_ids, attention_mask])"),we.forEach(t),J=i(U," or "),N=s(U,"CODE",{});var _e=a(N);re=i(_e,"model([input_ids, attention_mask, token_type_ids])"),_e.forEach(t),U.forEach(t),pe=m(T),C=s(T,"LI",{});var Y=a(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),w=s(Y,"CODE",{});var ye=a(w);ue=i(ye,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),ye.forEach(t),Y.forEach(t),T.forEach(t)},m(l,p){h(l,b,p),e(b,Z),h(l,A,p),h(l,_,p),e(_,z),e(z,G),e(_,le),e(_,D),e(D,de),h(l,Q,p),h(l,u,p),e(u,K),e(u,x),e(x,X),e(u,me),e(u,I),e(I,ce),e(u,se),h(l,H,p),h(l,E,p),e(E,j),h(l,V,p),h(l,g,p),e(g,k),e(k,ee),e(k,O),e(O,ae),e(k,te),e(k,S),e(S,he),e(g,ne),e(g,v),e(v,fe),e(v,B),e(B,oe),e(v,J),e(v,N),e(N,re),e(g,pe),e(g,C),e(C,ie),e(C,w),e(w,ue)},d(l){l&&t(b),l&&t(A),l&&t(_),l&&t(Q),l&&t(u),l&&t(H),l&&t(E),l&&t(V),l&&t(g)}}}function Ju(xe){let b,Z,A,_,z,G,le,D,de,Q,u,K,x,X,me,I,ce,se,H,E,j,V,g,k,ee,O,ae,te,S,he,ne,v,fe,B,oe,J,N,re,pe,C,ie,w,ue,l,p,W,ke,Ee,y,Te,ge,ve,T,R,Ce,be,U,we,_e,Y,ye,Zt,Si,Wr,Bi,Oi,Qa,mt,Ft,Ds,eo,Ni,xs,Ui,Va,$e,to,Hi,Ye,Qi,Kr,Vi,Wi,Gr,Ki,Gi,oo,Xi,ji,Ji,ro,Yi,Xr,Zi,el,tl,ct,ol,Is,rl,sl,Ss,al,nl,il,Ze,so,ll,Bs,dl,ml,ao,jr,cl,Os,hl,fl,Jr,pl,Ns,ul,gl,Pt,no,_l,io,vl,Us,kl,bl,Tl,Mt,lo,wl,Hs,El,Cl,Qs,Wa,ht,qt,Vs,mo,yl,Ws,$l,Ka,Ie,co,Fl,Ve,Pl,Ks,Ml,ql,Yr,Ll,Rl,Zr,Al,zl,ho,Dl,xl,Il,fo,Sl,es,Bl,Ol,Nl,et,po,Ul,Gs,Hl,Ql,uo,ts,Vl,Xs,Wl,Kl,os,Gl,js,Xl,jl,Lt,go,Jl,Js,Yl,Ga,ft,Rt,Ys,_o,Zl,Zs,ed,Xa,Se,vo,td,ea,od,rd,ko,sd,rs,ad,nd,id,bo,ld,To,dd,md,cd,wo,hd,ss,fd,pd,ja,pt,At,ta,Eo,ud,oa,gd,Ja,Be,Co,_d,yo,vd,ra,kd,bd,Td,$o,wd,as,Ed,Cd,yd,Fo,$d,Po,Fd,Pd,Md,Mo,qd,ns,Ld,Rd,Ya,ut,zt,sa,qo,Ad,aa,zd,Za,Oe,Lo,Dd,Ro,xd,na,Id,Sd,Bd,Ao,Od,is,Nd,Ud,Hd,zo,Qd,Do,Vd,Wd,Kd,xo,Gd,ls,Xd,jd,en,gt,Dt,ia,Io,Jd,la,Yd,tn,Ne,So,Zd,da,em,tm,Bo,om,ds,rm,sm,am,Oo,nm,No,im,lm,dm,Uo,mm,ms,cm,hm,on,_t,xt,ma,Ho,fm,ca,pm,rn,Ue,Qo,um,ha,gm,_m,Vo,vm,cs,km,bm,Tm,Wo,wm,Ko,Em,Cm,ym,Go,$m,hs,Fm,Pm,sn,vt,It,fa,Xo,Mm,pa,qm,an,He,jo,Lm,ua,Rm,Am,Jo,zm,fs,Dm,xm,Im,Yo,Sm,Zo,Bm,Om,Nm,er,Um,ps,Hm,Qm,nn,kt,St,ga,tr,Vm,_a,Wm,ln,Qe,or,Km,Bt,Gm,va,Xm,jm,ka,Jm,Ym,rr,Zm,us,ec,tc,oc,sr,rc,ar,sc,ac,nc,nr,ic,gs,lc,dc,dn,bt,Ot,ba,ir,mc,Ta,cc,mn,Me,lr,hc,wa,fc,pc,dr,uc,_s,gc,_c,vc,mr,kc,cr,bc,Tc,wc,Nt,Ec,hr,Cc,vs,yc,$c,cn,Tt,Ut,Ea,fr,Fc,Ca,Pc,hn,qe,pr,Mc,ur,qc,ya,Lc,Rc,Ac,gr,zc,ks,Dc,xc,Ic,_r,Sc,vr,Bc,Oc,Nc,Ht,Uc,kr,Hc,bs,Qc,Vc,fn,wt,Qt,$a,br,Wc,Fa,Kc,pn,Le,Tr,Gc,Pa,Xc,jc,wr,Jc,Ts,Yc,Zc,eh,Er,th,Cr,oh,rh,sh,Vt,ah,yr,nh,ws,ih,lh,un,Et,Wt,Ma,$r,dh,qa,mh,gn,Re,Fr,ch,La,hh,fh,Pr,ph,Es,uh,gh,_h,Mr,vh,qr,kh,bh,Th,Kt,wh,Lr,Eh,Cs,Ch,yh,_n,Ct,Gt,Ra,Rr,$h,Aa,Fh,vn,Ae,Ar,Ph,za,Mh,qh,zr,Lh,ys,Rh,Ah,zh,Dr,Dh,xr,xh,Ih,Sh,Xt,Bh,Ir,Oh,$s,Nh,Uh,kn,yt,jt,Da,Sr,Hh,xa,Qh,bn,ze,Br,Vh,$t,Wh,Ia,Kh,Gh,Sa,Xh,jh,Jh,Or,Yh,Fs,Zh,ef,tf,Nr,of,Ur,rf,sf,af,Jt,nf,Hr,lf,Ps,df,mf,Tn;return G=new Pe({}),X=new Pe({}),R=new Pe({}),Y=new Fe({props:{name:"class transformers.CamembertConfig",anchor:"transformers.CamembertConfig",parameters:[{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/configuration_camembert.py#L35"}}),eo=new Pe({}),to=new Fe({props:{name:"class transformers.CamembertTokenizer",anchor:"transformers.CamembertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<s>NOTUSED', '</s>NOTUSED']"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert.py#L45",parametersDescription:[{anchor:"transformers.CamembertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.CamembertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.CamembertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.CamembertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.CamembertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.CamembertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.CamembertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.CamembertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.CamembertTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.CamembertTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"}]}}),so=new Fe({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert.py#L161",parametersDescription:[{anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),no=new Fe({props:{name:"get_special_tokens_mask",anchor:"transformers.CamembertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert.py#L187",parametersDescription:[{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lo=new Fe({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert.py#L214",parametersDescription:[{anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mo=new Pe({}),co=new Fe({props:{name:"class transformers.CamembertTokenizerFast",anchor:"transformers.CamembertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<s>NOTUSED', '</s>NOTUSED']"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert_fast.py#L54",parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.CamembertTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.CamembertTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.CamembertTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.CamembertTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.CamembertTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.CamembertTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.CamembertTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.CamembertTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}]}}),po=new Fe({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert_fast.py#L146",parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),go=new Fe({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/tokenization_camembert_fast.py#L172",parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),_o=new Pe({}),vo=new Fe({props:{name:"class transformers.CamembertModel",anchor:"transformers.CamembertModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L64",parametersDescription:[{anchor:"transformers.CamembertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Eo=new Pe({}),Co=new Fe({props:{name:"class transformers.CamembertForCausalLM",anchor:"transformers.CamembertForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L153",parametersDescription:[{anchor:"transformers.CamembertForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qo=new Pe({}),Lo=new Fe({props:{name:"class transformers.CamembertForMaskedLM",anchor:"transformers.CamembertForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L77",parametersDescription:[{anchor:"transformers.CamembertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Io=new Pe({}),So=new Fe({props:{name:"class transformers.CamembertForSequenceClassification",anchor:"transformers.CamembertForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L93",parametersDescription:[{anchor:"transformers.CamembertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ho=new Pe({}),Qo=new Fe({props:{name:"class transformers.CamembertForMultipleChoice",anchor:"transformers.CamembertForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L109",parametersDescription:[{anchor:"transformers.CamembertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Xo=new Pe({}),jo=new Fe({props:{name:"class transformers.CamembertForTokenClassification",anchor:"transformers.CamembertForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L125",parametersDescription:[{anchor:"transformers.CamembertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),tr=new Pe({}),or=new Fe({props:{name:"class transformers.CamembertForQuestionAnswering",anchor:"transformers.CamembertForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_camembert.py#L141",parametersDescription:[{anchor:"transformers.CamembertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ir=new Pe({}),lr=new Fe({props:{name:"class transformers.TFCamembertModel",anchor:"transformers.TFCamembertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_tf_camembert.py#L80",parametersDescription:[{anchor:"transformers.TFCamembertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Nt=new zs({props:{$$slots:{default:[Vu]},$$scope:{ctx:xe}}}),fr=new Pe({}),pr=new Fe({props:{name:"class transformers.TFCamembertForMaskedLM",anchor:"transformers.TFCamembertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_tf_camembert.py#L93",parametersDescription:[{anchor:"transformers.TFCamembertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ht=new zs({props:{$$slots:{default:[Wu]},$$scope:{ctx:xe}}}),br=new Pe({}),Tr=new Fe({props:{name:"class transformers.TFCamembertForSequenceClassification",anchor:"transformers.TFCamembertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_tf_camembert.py#L109",parametersDescription:[{anchor:"transformers.TFCamembertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Vt=new zs({props:{$$slots:{default:[Ku]},$$scope:{ctx:xe}}}),$r=new Pe({}),Fr=new Fe({props:{name:"class transformers.TFCamembertForMultipleChoice",anchor:"transformers.TFCamembertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_tf_camembert.py#L141",parametersDescription:[{anchor:"transformers.TFCamembertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Kt=new zs({props:{$$slots:{default:[Gu]},$$scope:{ctx:xe}}}),Rr=new Pe({}),Ar=new Fe({props:{name:"class transformers.TFCamembertForTokenClassification",anchor:"transformers.TFCamembertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_tf_camembert.py#L125",parametersDescription:[{anchor:"transformers.TFCamembertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Xt=new zs({props:{$$slots:{default:[Xu]},$$scope:{ctx:xe}}}),Sr=new Pe({}),Br=new Fe({props:{name:"class transformers.TFCamembertForQuestionAnswering",anchor:"transformers.TFCamembertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/models/camembert/modeling_tf_camembert.py#L157",parametersDescription:[{anchor:"transformers.TFCamembertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.16.2/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Jt=new zs({props:{$$slots:{default:[ju]},$$scope:{ctx:xe}}}),{c(){b=r("meta"),Z=d(),A=r("h1"),_=r("a"),z=r("span"),$(G.$$.fragment),le=d(),D=r("span"),de=n("CamemBERT"),Q=d(),u=r("h2"),K=r("a"),x=r("span"),$(X.$$.fragment),me=d(),I=r("span"),ce=n("Overview"),se=d(),H=r("p"),E=n("The CamemBERT model was proposed in "),j=r("a"),V=n("CamemBERT: a Tasty French Language Model"),g=n(` by
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\xE1rez, Yoann Dupont, Laurent Romary, \xC9ric Villemonte de la
Clergerie, Djam\xE9 Seddah, and Beno\xEEt Sagot. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a model
trained on 138GB of French text.`),k=d(),ee=r("p"),O=n("The abstract from the paper is the following:"),ae=d(),te=r("p"),S=r("em"),he=n(`Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available
models have either been trained on English data or on the concatenation of data in multiple languages. This makes
practical use of such models \u2014in all languages except English\u2014 very limited. Aiming to address this issue for French,
we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the
performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,
dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art
for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and
downstream applications for French NLP.`),ne=d(),v=r("p"),fe=n("Tips:"),B=d(),oe=r("ul"),J=r("li"),N=n("This implementation is the same as RoBERTa. Refer to the "),re=r("a"),pe=n("documentation of RoBERTa"),C=n(` for usage examples
as well as the information relative to the inputs and outputs.`),ie=d(),w=r("p"),ue=n("This model was contributed by "),l=r("a"),p=n("camembert"),W=n(". The original code can be found "),ke=r("a"),Ee=n("here"),y=n("."),Te=d(),ge=r("h2"),ve=r("a"),T=r("span"),$(R.$$.fragment),Ce=d(),be=r("span"),U=n("CamembertConfig"),we=d(),_e=r("div"),$(Y.$$.fragment),ye=d(),Zt=r("p"),Si=n("This class overrides "),Wr=r("a"),Bi=n("RobertaConfig"),Oi=n(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Qa=d(),mt=r("h2"),Ft=r("a"),Ds=r("span"),$(eo.$$.fragment),Ni=d(),xs=r("span"),Ui=n("CamembertTokenizer"),Va=d(),$e=r("div"),$(to.$$.fragment),Hi=d(),Ye=r("p"),Qi=n("Adapted from "),Kr=r("a"),Vi=n("RobertaTokenizer"),Wi=n(" and "),Gr=r("a"),Ki=n("XLNetTokenizer"),Gi=n(`. Construct a CamemBERT tokenizer. Based on
`),oo=r("a"),Xi=n("SentencePiece"),ji=n("."),Ji=d(),ro=r("p"),Yi=n("This tokenizer inherits from "),Xr=r("a"),Zi=n("PreTrainedTokenizer"),el=n(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),tl=d(),ct=r("p"),ol=n(`Attributes:
sp_model (`),Is=r("code"),rl=n("SentencePieceProcessor"),sl=n(`):
The `),Ss=r("em"),al=n("SentencePiece"),nl=n(" processor that is used for every conversion (string, tokens and IDs)."),il=d(),Ze=r("div"),$(so.$$.fragment),ll=d(),Bs=r("p"),dl=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),ml=d(),ao=r("ul"),jr=r("li"),cl=n("single sequence: "),Os=r("code"),hl=n("<s> X </s>"),fl=d(),Jr=r("li"),pl=n("pair of sequences: "),Ns=r("code"),ul=n("<s> A </s></s> B </s>"),gl=d(),Pt=r("div"),$(no.$$.fragment),_l=d(),io=r("p"),vl=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Us=r("code"),kl=n("prepare_for_model"),bl=n(" method."),Tl=d(),Mt=r("div"),$(lo.$$.fragment),wl=d(),Hs=r("p"),El=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Cl=d(),Qs=r("div"),Wa=d(),ht=r("h2"),qt=r("a"),Vs=r("span"),$(mo.$$.fragment),yl=d(),Ws=r("span"),$l=n("CamembertTokenizerFast"),Ka=d(),Ie=r("div"),$(co.$$.fragment),Fl=d(),Ve=r("p"),Pl=n("Construct a \u201Cfast\u201D CamemBERT tokenizer (backed by HuggingFace\u2019s "),Ks=r("em"),Ml=n("tokenizers"),ql=n(` library). Adapted from
`),Yr=r("a"),Ll=n("RobertaTokenizer"),Rl=n(" and "),Zr=r("a"),Al=n("XLNetTokenizer"),zl=n(`. Based on
`),ho=r("a"),Dl=n("BPE"),xl=n("."),Il=d(),fo=r("p"),Sl=n("This tokenizer inherits from "),es=r("a"),Bl=n("PreTrainedTokenizerFast"),Ol=n(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Nl=d(),et=r("div"),$(po.$$.fragment),Ul=d(),Gs=r("p"),Hl=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Ql=d(),uo=r("ul"),ts=r("li"),Vl=n("single sequence: "),Xs=r("code"),Wl=n("<s> X </s>"),Kl=d(),os=r("li"),Gl=n("pair of sequences: "),js=r("code"),Xl=n("<s> A </s></s> B </s>"),jl=d(),Lt=r("div"),$(go.$$.fragment),Jl=d(),Js=r("p"),Yl=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Ga=d(),ft=r("h2"),Rt=r("a"),Ys=r("span"),$(_o.$$.fragment),Zl=d(),Zs=r("span"),ed=n("CamembertModel"),Xa=d(),Se=r("div"),$(vo.$$.fragment),td=d(),ea=r("p"),od=n("The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),rd=d(),ko=r("p"),sd=n("This model inherits from "),rs=r("a"),ad=n("PreTrainedModel"),nd=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),id=d(),bo=r("p"),ld=n("This model is also a PyTorch "),To=r("a"),dd=n("torch.nn.Module"),md=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),cd=d(),wo=r("p"),hd=n("This class overrides "),ss=r("a"),fd=n("RobertaModel"),pd=n(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ja=d(),pt=r("h2"),At=r("a"),ta=r("span"),$(Eo.$$.fragment),ud=d(),oa=r("span"),gd=n("CamembertForCausalLM"),Ja=d(),Be=r("div"),$(Co.$$.fragment),_d=d(),yo=r("p"),vd=n("CamemBERT Model with a "),ra=r("code"),kd=n("language modeling"),bd=n(" head on top for CLM fine-tuning."),Td=d(),$o=r("p"),wd=n("This model inherits from "),as=r("a"),Ed=n("PreTrainedModel"),Cd=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yd=d(),Fo=r("p"),$d=n("This model is also a PyTorch "),Po=r("a"),Fd=n("torch.nn.Module"),Pd=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Md=d(),Mo=r("p"),qd=n("This class overrides "),ns=r("a"),Ld=n("RobertaForCausalLM"),Rd=n(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ya=d(),ut=r("h2"),zt=r("a"),sa=r("span"),$(qo.$$.fragment),Ad=d(),aa=r("span"),zd=n("CamembertForMaskedLM"),Za=d(),Oe=r("div"),$(Lo.$$.fragment),Dd=d(),Ro=r("p"),xd=n("CamemBERT Model with a "),na=r("code"),Id=n("language modeling"),Sd=n(" head on top."),Bd=d(),Ao=r("p"),Od=n("This model inherits from "),is=r("a"),Nd=n("PreTrainedModel"),Ud=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hd=d(),zo=r("p"),Qd=n("This model is also a PyTorch "),Do=r("a"),Vd=n("torch.nn.Module"),Wd=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Kd=d(),xo=r("p"),Gd=n("This class overrides "),ls=r("a"),Xd=n("RobertaForMaskedLM"),jd=n(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),en=d(),gt=r("h2"),Dt=r("a"),ia=r("span"),$(Io.$$.fragment),Jd=d(),la=r("span"),Yd=n("CamembertForSequenceClassification"),tn=d(),Ne=r("div"),$(So.$$.fragment),Zd=d(),da=r("p"),em=n(`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),tm=d(),Bo=r("p"),om=n("This model inherits from "),ds=r("a"),rm=n("PreTrainedModel"),sm=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),am=d(),Oo=r("p"),nm=n("This model is also a PyTorch "),No=r("a"),im=n("torch.nn.Module"),lm=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),dm=d(),Uo=r("p"),mm=n("This class overrides "),ms=r("a"),cm=n("RobertaForSequenceClassification"),hm=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),on=d(),_t=r("h2"),xt=r("a"),ma=r("span"),$(Ho.$$.fragment),fm=d(),ca=r("span"),pm=n("CamembertForMultipleChoice"),rn=d(),Ue=r("div"),$(Qo.$$.fragment),um=d(),ha=r("p"),gm=n(`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),_m=d(),Vo=r("p"),vm=n("This model inherits from "),cs=r("a"),km=n("PreTrainedModel"),bm=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tm=d(),Wo=r("p"),wm=n("This model is also a PyTorch "),Ko=r("a"),Em=n("torch.nn.Module"),Cm=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ym=d(),Go=r("p"),$m=n("This class overrides "),hs=r("a"),Fm=n("RobertaForMultipleChoice"),Pm=n(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),sn=d(),vt=r("h2"),It=r("a"),fa=r("span"),$(Xo.$$.fragment),Mm=d(),pa=r("span"),qm=n("CamembertForTokenClassification"),an=d(),He=r("div"),$(jo.$$.fragment),Lm=d(),ua=r("p"),Rm=n(`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Am=d(),Jo=r("p"),zm=n("This model inherits from "),fs=r("a"),Dm=n("PreTrainedModel"),xm=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Im=d(),Yo=r("p"),Sm=n("This model is also a PyTorch "),Zo=r("a"),Bm=n("torch.nn.Module"),Om=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Nm=d(),er=r("p"),Um=n("This class overrides "),ps=r("a"),Hm=n("RobertaForTokenClassification"),Qm=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),nn=d(),kt=r("h2"),St=r("a"),ga=r("span"),$(tr.$$.fragment),Vm=d(),_a=r("span"),Wm=n("CamembertForQuestionAnswering"),ln=d(),Qe=r("div"),$(or.$$.fragment),Km=d(),Bt=r("p"),Gm=n(`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),va=r("code"),Xm=n("span start logits"),jm=n(" and "),ka=r("code"),Jm=n("span end logits"),Ym=d(),rr=r("p"),Zm=n("This model inherits from "),us=r("a"),ec=n("PreTrainedModel"),tc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),oc=d(),sr=r("p"),rc=n("This model is also a PyTorch "),ar=r("a"),sc=n("torch.nn.Module"),ac=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),nc=d(),nr=r("p"),ic=n("This class overrides "),gs=r("a"),lc=n("RobertaForQuestionAnswering"),dc=n(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),dn=d(),bt=r("h2"),Ot=r("a"),ba=r("span"),$(ir.$$.fragment),mc=d(),Ta=r("span"),cc=n("TFCamembertModel"),mn=d(),Me=r("div"),$(lr.$$.fragment),hc=d(),wa=r("p"),fc=n("The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),pc=d(),dr=r("p"),uc=n("This model inherits from "),_s=r("a"),gc=n("TFPreTrainedModel"),_c=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vc=d(),mr=r("p"),kc=n("This model is also a "),cr=r("a"),bc=n("tf.keras.Model"),Tc=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wc=d(),$(Nt.$$.fragment),Ec=d(),hr=r("p"),Cc=n("This class overrides "),vs=r("a"),yc=n("TFRobertaModel"),$c=n(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),cn=d(),Tt=r("h2"),Ut=r("a"),Ea=r("span"),$(fr.$$.fragment),Fc=d(),Ca=r("span"),Pc=n("TFCamembertForMaskedLM"),hn=d(),qe=r("div"),$(pr.$$.fragment),Mc=d(),ur=r("p"),qc=n("CamemBERT Model with a "),ya=r("code"),Lc=n("language modeling"),Rc=n(" head on top."),Ac=d(),gr=r("p"),zc=n("This model inherits from "),ks=r("a"),Dc=n("TFPreTrainedModel"),xc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ic=d(),_r=r("p"),Sc=n("This model is also a "),vr=r("a"),Bc=n("tf.keras.Model"),Oc=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Nc=d(),$(Ht.$$.fragment),Uc=d(),kr=r("p"),Hc=n("This class overrides "),bs=r("a"),Qc=n("TFRobertaForMaskedLM"),Vc=n(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),fn=d(),wt=r("h2"),Qt=r("a"),$a=r("span"),$(br.$$.fragment),Wc=d(),Fa=r("span"),Kc=n("TFCamembertForSequenceClassification"),pn=d(),Le=r("div"),$(Tr.$$.fragment),Gc=d(),Pa=r("p"),Xc=n(`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),jc=d(),wr=r("p"),Jc=n("This model inherits from "),Ts=r("a"),Yc=n("TFPreTrainedModel"),Zc=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eh=d(),Er=r("p"),th=n("This model is also a "),Cr=r("a"),oh=n("tf.keras.Model"),rh=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),sh=d(),$(Vt.$$.fragment),ah=d(),yr=r("p"),nh=n("This class overrides "),ws=r("a"),ih=n("TFRobertaForSequenceClassification"),lh=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),un=d(),Et=r("h2"),Wt=r("a"),Ma=r("span"),$($r.$$.fragment),dh=d(),qa=r("span"),mh=n("TFCamembertForMultipleChoice"),gn=d(),Re=r("div"),$(Fr.$$.fragment),ch=d(),La=r("p"),hh=n(`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),fh=d(),Pr=r("p"),ph=n("This model inherits from "),Es=r("a"),uh=n("TFPreTrainedModel"),gh=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_h=d(),Mr=r("p"),vh=n("This model is also a "),qr=r("a"),kh=n("tf.keras.Model"),bh=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Th=d(),$(Kt.$$.fragment),wh=d(),Lr=r("p"),Eh=n("This class overrides "),Cs=r("a"),Ch=n("TFRobertaForMultipleChoice"),yh=n(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),_n=d(),Ct=r("h2"),Gt=r("a"),Ra=r("span"),$(Rr.$$.fragment),$h=d(),Aa=r("span"),Fh=n("TFCamembertForTokenClassification"),vn=d(),Ae=r("div"),$(Ar.$$.fragment),Ph=d(),za=r("p"),Mh=n(`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),qh=d(),zr=r("p"),Lh=n("This model inherits from "),ys=r("a"),Rh=n("TFPreTrainedModel"),Ah=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),zh=d(),Dr=r("p"),Dh=n("This model is also a "),xr=r("a"),xh=n("tf.keras.Model"),Ih=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Sh=d(),$(Xt.$$.fragment),Bh=d(),Ir=r("p"),Oh=n("This class overrides "),$s=r("a"),Nh=n("TFRobertaForTokenClassification"),Uh=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),kn=d(),yt=r("h2"),jt=r("a"),Da=r("span"),$(Sr.$$.fragment),Hh=d(),xa=r("span"),Qh=n("TFCamembertForQuestionAnswering"),bn=d(),ze=r("div"),$(Br.$$.fragment),Vh=d(),$t=r("p"),Wh=n(`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Ia=r("code"),Kh=n("span start logits"),Gh=n(" and "),Sa=r("code"),Xh=n("span end logits"),jh=n(")."),Jh=d(),Or=r("p"),Yh=n("This model inherits from "),Fs=r("a"),Zh=n("TFPreTrainedModel"),ef=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tf=d(),Nr=r("p"),of=n("This model is also a "),Ur=r("a"),rf=n("tf.keras.Model"),sf=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),af=d(),$(Jt.$$.fragment),nf=d(),Hr=r("p"),lf=n("This class overrides "),Ps=r("a"),df=n("TFRobertaForQuestionAnswering"),mf=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),this.h()},l(o){const f=Qu('[data-svelte="svelte-1phssyn"]',document.head);b=s(f,"META",{name:!0,content:!0}),f.forEach(t),Z=m(o),A=s(o,"H1",{class:!0});var Qr=a(A);_=s(Qr,"A",{id:!0,class:!0,href:!0});var Ba=a(_);z=s(Ba,"SPAN",{});var Oa=a(z);F(G.$$.fragment,Oa),Oa.forEach(t),Ba.forEach(t),le=m(Qr),D=s(Qr,"SPAN",{});var Na=a(D);de=i(Na,"CamemBERT"),Na.forEach(t),Qr.forEach(t),Q=m(o),u=s(o,"H2",{class:!0});var Vr=a(u);K=s(Vr,"A",{id:!0,class:!0,href:!0});var Ua=a(K);x=s(Ua,"SPAN",{});var uf=a(x);F(X.$$.fragment,uf),uf.forEach(t),Ua.forEach(t),me=m(Vr),I=s(Vr,"SPAN",{});var gf=a(I);ce=i(gf,"Overview"),gf.forEach(t),Vr.forEach(t),se=m(o),H=s(o,"P",{});var wn=a(H);E=i(wn,"The CamemBERT model was proposed in "),j=s(wn,"A",{href:!0,rel:!0});var _f=a(j);V=i(_f,"CamemBERT: a Tasty French Language Model"),_f.forEach(t),g=i(wn,` by
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\xE1rez, Yoann Dupont, Laurent Romary, \xC9ric Villemonte de la
Clergerie, Djam\xE9 Seddah, and Beno\xEEt Sagot. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a model
trained on 138GB of French text.`),wn.forEach(t),k=m(o),ee=s(o,"P",{});var vf=a(ee);O=i(vf,"The abstract from the paper is the following:"),vf.forEach(t),ae=m(o),te=s(o,"P",{});var kf=a(te);S=s(kf,"EM",{});var bf=a(S);he=i(bf,`Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available
models have either been trained on English data or on the concatenation of data in multiple languages. This makes
practical use of such models \u2014in all languages except English\u2014 very limited. Aiming to address this issue for French,
we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the
performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,
dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art
for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and
downstream applications for French NLP.`),bf.forEach(t),kf.forEach(t),ne=m(o),v=s(o,"P",{});var Tf=a(v);fe=i(Tf,"Tips:"),Tf.forEach(t),B=m(o),oe=s(o,"UL",{});var wf=a(oe);J=s(wf,"LI",{});var En=a(J);N=i(En,"This implementation is the same as RoBERTa. Refer to the "),re=s(En,"A",{href:!0});var Ef=a(re);pe=i(Ef,"documentation of RoBERTa"),Ef.forEach(t),C=i(En,` for usage examples
as well as the information relative to the inputs and outputs.`),En.forEach(t),wf.forEach(t),ie=m(o),w=s(o,"P",{});var Ms=a(w);ue=i(Ms,"This model was contributed by "),l=s(Ms,"A",{href:!0,rel:!0});var Cf=a(l);p=i(Cf,"camembert"),Cf.forEach(t),W=i(Ms,". The original code can be found "),ke=s(Ms,"A",{href:!0,rel:!0});var yf=a(ke);Ee=i(yf,"here"),yf.forEach(t),y=i(Ms,"."),Ms.forEach(t),Te=m(o),ge=s(o,"H2",{class:!0});var Cn=a(ge);ve=s(Cn,"A",{id:!0,class:!0,href:!0});var $f=a(ve);T=s($f,"SPAN",{});var Ff=a(T);F(R.$$.fragment,Ff),Ff.forEach(t),$f.forEach(t),Ce=m(Cn),be=s(Cn,"SPAN",{});var Pf=a(be);U=i(Pf,"CamembertConfig"),Pf.forEach(t),Cn.forEach(t),we=m(o),_e=s(o,"DIV",{class:!0});var yn=a(_e);F(Y.$$.fragment,yn),ye=m(yn),Zt=s(yn,"P",{});var $n=a(Zt);Si=i($n,"This class overrides "),Wr=s($n,"A",{href:!0});var Mf=a(Wr);Bi=i(Mf,"RobertaConfig"),Mf.forEach(t),Oi=i($n,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),$n.forEach(t),yn.forEach(t),Qa=m(o),mt=s(o,"H2",{class:!0});var Fn=a(mt);Ft=s(Fn,"A",{id:!0,class:!0,href:!0});var qf=a(Ft);Ds=s(qf,"SPAN",{});var Lf=a(Ds);F(eo.$$.fragment,Lf),Lf.forEach(t),qf.forEach(t),Ni=m(Fn),xs=s(Fn,"SPAN",{});var Rf=a(xs);Ui=i(Rf,"CamembertTokenizer"),Rf.forEach(t),Fn.forEach(t),Va=m(o),$e=s(o,"DIV",{class:!0});var De=a($e);F(to.$$.fragment,De),Hi=m(De),Ye=s(De,"P",{});var Yt=a(Ye);Qi=i(Yt,"Adapted from "),Kr=s(Yt,"A",{href:!0});var Af=a(Kr);Vi=i(Af,"RobertaTokenizer"),Af.forEach(t),Wi=i(Yt," and "),Gr=s(Yt,"A",{href:!0});var zf=a(Gr);Ki=i(zf,"XLNetTokenizer"),zf.forEach(t),Gi=i(Yt,`. Construct a CamemBERT tokenizer. Based on
`),oo=s(Yt,"A",{href:!0,rel:!0});var Df=a(oo);Xi=i(Df,"SentencePiece"),Df.forEach(t),ji=i(Yt,"."),Yt.forEach(t),Ji=m(De),ro=s(De,"P",{});var Pn=a(ro);Yi=i(Pn,"This tokenizer inherits from "),Xr=s(Pn,"A",{href:!0});var xf=a(Xr);Zi=i(xf,"PreTrainedTokenizer"),xf.forEach(t),el=i(Pn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Pn.forEach(t),tl=m(De),ct=s(De,"P",{});var qs=a(ct);ol=i(qs,`Attributes:
sp_model (`),Is=s(qs,"CODE",{});var If=a(Is);rl=i(If,"SentencePieceProcessor"),If.forEach(t),sl=i(qs,`):
The `),Ss=s(qs,"EM",{});var Sf=a(Ss);al=i(Sf,"SentencePiece"),Sf.forEach(t),nl=i(qs," processor that is used for every conversion (string, tokens and IDs)."),qs.forEach(t),il=m(De),Ze=s(De,"DIV",{class:!0});var Ls=a(Ze);F(so.$$.fragment,Ls),ll=m(Ls),Bs=s(Ls,"P",{});var Bf=a(Bs);dl=i(Bf,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Bf.forEach(t),ml=m(Ls),ao=s(Ls,"UL",{});var Mn=a(ao);jr=s(Mn,"LI",{});var cf=a(jr);cl=i(cf,"single sequence: "),Os=s(cf,"CODE",{});var Of=a(Os);hl=i(Of,"<s> X </s>"),Of.forEach(t),cf.forEach(t),fl=m(Mn),Jr=s(Mn,"LI",{});var hf=a(Jr);pl=i(hf,"pair of sequences: "),Ns=s(hf,"CODE",{});var Nf=a(Ns);ul=i(Nf,"<s> A </s></s> B </s>"),Nf.forEach(t),hf.forEach(t),Mn.forEach(t),Ls.forEach(t),gl=m(De),Pt=s(De,"DIV",{class:!0});var qn=a(Pt);F(no.$$.fragment,qn),_l=m(qn),io=s(qn,"P",{});var Ln=a(io);vl=i(Ln,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Us=s(Ln,"CODE",{});var Uf=a(Us);kl=i(Uf,"prepare_for_model"),Uf.forEach(t),bl=i(Ln," method."),Ln.forEach(t),qn.forEach(t),Tl=m(De),Mt=s(De,"DIV",{class:!0});var Rn=a(Mt);F(lo.$$.fragment,Rn),wl=m(Rn),Hs=s(Rn,"P",{});var Hf=a(Hs);El=i(Hf,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Hf.forEach(t),Rn.forEach(t),Cl=m(De),Qs=s(De,"DIV",{class:!0}),a(Qs).forEach(t),De.forEach(t),Wa=m(o),ht=s(o,"H2",{class:!0});var An=a(ht);qt=s(An,"A",{id:!0,class:!0,href:!0});var Qf=a(qt);Vs=s(Qf,"SPAN",{});var Vf=a(Vs);F(mo.$$.fragment,Vf),Vf.forEach(t),Qf.forEach(t),yl=m(An),Ws=s(An,"SPAN",{});var Wf=a(Ws);$l=i(Wf,"CamembertTokenizerFast"),Wf.forEach(t),An.forEach(t),Ka=m(o),Ie=s(o,"DIV",{class:!0});var tt=a(Ie);F(co.$$.fragment,tt),Fl=m(tt),Ve=s(tt,"P",{});var ot=a(Ve);Pl=i(ot,"Construct a \u201Cfast\u201D CamemBERT tokenizer (backed by HuggingFace\u2019s "),Ks=s(ot,"EM",{});var Kf=a(Ks);Ml=i(Kf,"tokenizers"),Kf.forEach(t),ql=i(ot,` library). Adapted from
`),Yr=s(ot,"A",{href:!0});var Gf=a(Yr);Ll=i(Gf,"RobertaTokenizer"),Gf.forEach(t),Rl=i(ot," and "),Zr=s(ot,"A",{href:!0});var Xf=a(Zr);Al=i(Xf,"XLNetTokenizer"),Xf.forEach(t),zl=i(ot,`. Based on
`),ho=s(ot,"A",{href:!0,rel:!0});var jf=a(ho);Dl=i(jf,"BPE"),jf.forEach(t),xl=i(ot,"."),ot.forEach(t),Il=m(tt),fo=s(tt,"P",{});var zn=a(fo);Sl=i(zn,"This tokenizer inherits from "),es=s(zn,"A",{href:!0});var Jf=a(es);Bl=i(Jf,"PreTrainedTokenizerFast"),Jf.forEach(t),Ol=i(zn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),zn.forEach(t),Nl=m(tt),et=s(tt,"DIV",{class:!0});var Rs=a(et);F(po.$$.fragment,Rs),Ul=m(Rs),Gs=s(Rs,"P",{});var Yf=a(Gs);Hl=i(Yf,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Yf.forEach(t),Ql=m(Rs),uo=s(Rs,"UL",{});var Dn=a(uo);ts=s(Dn,"LI",{});var ff=a(ts);Vl=i(ff,"single sequence: "),Xs=s(ff,"CODE",{});var Zf=a(Xs);Wl=i(Zf,"<s> X </s>"),Zf.forEach(t),ff.forEach(t),Kl=m(Dn),os=s(Dn,"LI",{});var pf=a(os);Gl=i(pf,"pair of sequences: "),js=s(pf,"CODE",{});var ep=a(js);Xl=i(ep,"<s> A </s></s> B </s>"),ep.forEach(t),pf.forEach(t),Dn.forEach(t),Rs.forEach(t),jl=m(tt),Lt=s(tt,"DIV",{class:!0});var xn=a(Lt);F(go.$$.fragment,xn),Jl=m(xn),Js=s(xn,"P",{});var tp=a(Js);Yl=i(tp,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),tp.forEach(t),xn.forEach(t),tt.forEach(t),Ga=m(o),ft=s(o,"H2",{class:!0});var In=a(ft);Rt=s(In,"A",{id:!0,class:!0,href:!0});var op=a(Rt);Ys=s(op,"SPAN",{});var rp=a(Ys);F(_o.$$.fragment,rp),rp.forEach(t),op.forEach(t),Zl=m(In),Zs=s(In,"SPAN",{});var sp=a(Zs);ed=i(sp,"CamembertModel"),sp.forEach(t),In.forEach(t),Xa=m(o),Se=s(o,"DIV",{class:!0});var rt=a(Se);F(vo.$$.fragment,rt),td=m(rt),ea=s(rt,"P",{});var ap=a(ea);od=i(ap,"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),ap.forEach(t),rd=m(rt),ko=s(rt,"P",{});var Sn=a(ko);sd=i(Sn,"This model inherits from "),rs=s(Sn,"A",{href:!0});var np=a(rs);ad=i(np,"PreTrainedModel"),np.forEach(t),nd=i(Sn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sn.forEach(t),id=m(rt),bo=s(rt,"P",{});var Bn=a(bo);ld=i(Bn,"This model is also a PyTorch "),To=s(Bn,"A",{href:!0,rel:!0});var ip=a(To);dd=i(ip,"torch.nn.Module"),ip.forEach(t),md=i(Bn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bn.forEach(t),cd=m(rt),wo=s(rt,"P",{});var On=a(wo);hd=i(On,"This class overrides "),ss=s(On,"A",{href:!0});var lp=a(ss);fd=i(lp,"RobertaModel"),lp.forEach(t),pd=i(On,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),On.forEach(t),rt.forEach(t),ja=m(o),pt=s(o,"H2",{class:!0});var Nn=a(pt);At=s(Nn,"A",{id:!0,class:!0,href:!0});var dp=a(At);ta=s(dp,"SPAN",{});var mp=a(ta);F(Eo.$$.fragment,mp),mp.forEach(t),dp.forEach(t),ud=m(Nn),oa=s(Nn,"SPAN",{});var cp=a(oa);gd=i(cp,"CamembertForCausalLM"),cp.forEach(t),Nn.forEach(t),Ja=m(o),Be=s(o,"DIV",{class:!0});var st=a(Be);F(Co.$$.fragment,st),_d=m(st),yo=s(st,"P",{});var Un=a(yo);vd=i(Un,"CamemBERT Model with a "),ra=s(Un,"CODE",{});var hp=a(ra);kd=i(hp,"language modeling"),hp.forEach(t),bd=i(Un," head on top for CLM fine-tuning."),Un.forEach(t),Td=m(st),$o=s(st,"P",{});var Hn=a($o);wd=i(Hn,"This model inherits from "),as=s(Hn,"A",{href:!0});var fp=a(as);Ed=i(fp,"PreTrainedModel"),fp.forEach(t),Cd=i(Hn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hn.forEach(t),yd=m(st),Fo=s(st,"P",{});var Qn=a(Fo);$d=i(Qn,"This model is also a PyTorch "),Po=s(Qn,"A",{href:!0,rel:!0});var pp=a(Po);Fd=i(pp,"torch.nn.Module"),pp.forEach(t),Pd=i(Qn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qn.forEach(t),Md=m(st),Mo=s(st,"P",{});var Vn=a(Mo);qd=i(Vn,"This class overrides "),ns=s(Vn,"A",{href:!0});var up=a(ns);Ld=i(up,"RobertaForCausalLM"),up.forEach(t),Rd=i(Vn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Vn.forEach(t),st.forEach(t),Ya=m(o),ut=s(o,"H2",{class:!0});var Wn=a(ut);zt=s(Wn,"A",{id:!0,class:!0,href:!0});var gp=a(zt);sa=s(gp,"SPAN",{});var _p=a(sa);F(qo.$$.fragment,_p),_p.forEach(t),gp.forEach(t),Ad=m(Wn),aa=s(Wn,"SPAN",{});var vp=a(aa);zd=i(vp,"CamembertForMaskedLM"),vp.forEach(t),Wn.forEach(t),Za=m(o),Oe=s(o,"DIV",{class:!0});var at=a(Oe);F(Lo.$$.fragment,at),Dd=m(at),Ro=s(at,"P",{});var Kn=a(Ro);xd=i(Kn,"CamemBERT Model with a "),na=s(Kn,"CODE",{});var kp=a(na);Id=i(kp,"language modeling"),kp.forEach(t),Sd=i(Kn," head on top."),Kn.forEach(t),Bd=m(at),Ao=s(at,"P",{});var Gn=a(Ao);Od=i(Gn,"This model inherits from "),is=s(Gn,"A",{href:!0});var bp=a(is);Nd=i(bp,"PreTrainedModel"),bp.forEach(t),Ud=i(Gn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Gn.forEach(t),Hd=m(at),zo=s(at,"P",{});var Xn=a(zo);Qd=i(Xn,"This model is also a PyTorch "),Do=s(Xn,"A",{href:!0,rel:!0});var Tp=a(Do);Vd=i(Tp,"torch.nn.Module"),Tp.forEach(t),Wd=i(Xn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xn.forEach(t),Kd=m(at),xo=s(at,"P",{});var jn=a(xo);Gd=i(jn,"This class overrides "),ls=s(jn,"A",{href:!0});var wp=a(ls);Xd=i(wp,"RobertaForMaskedLM"),wp.forEach(t),jd=i(jn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),jn.forEach(t),at.forEach(t),en=m(o),gt=s(o,"H2",{class:!0});var Jn=a(gt);Dt=s(Jn,"A",{id:!0,class:!0,href:!0});var Ep=a(Dt);ia=s(Ep,"SPAN",{});var Cp=a(ia);F(Io.$$.fragment,Cp),Cp.forEach(t),Ep.forEach(t),Jd=m(Jn),la=s(Jn,"SPAN",{});var yp=a(la);Yd=i(yp,"CamembertForSequenceClassification"),yp.forEach(t),Jn.forEach(t),tn=m(o),Ne=s(o,"DIV",{class:!0});var nt=a(Ne);F(So.$$.fragment,nt),Zd=m(nt),da=s(nt,"P",{});var $p=a(da);em=i($p,`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),$p.forEach(t),tm=m(nt),Bo=s(nt,"P",{});var Yn=a(Bo);om=i(Yn,"This model inherits from "),ds=s(Yn,"A",{href:!0});var Fp=a(ds);rm=i(Fp,"PreTrainedModel"),Fp.forEach(t),sm=i(Yn,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yn.forEach(t),am=m(nt),Oo=s(nt,"P",{});var Zn=a(Oo);nm=i(Zn,"This model is also a PyTorch "),No=s(Zn,"A",{href:!0,rel:!0});var Pp=a(No);im=i(Pp,"torch.nn.Module"),Pp.forEach(t),lm=i(Zn,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Zn.forEach(t),dm=m(nt),Uo=s(nt,"P",{});var ei=a(Uo);mm=i(ei,"This class overrides "),ms=s(ei,"A",{href:!0});var Mp=a(ms);cm=i(Mp,"RobertaForSequenceClassification"),Mp.forEach(t),hm=i(ei,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ei.forEach(t),nt.forEach(t),on=m(o),_t=s(o,"H2",{class:!0});var ti=a(_t);xt=s(ti,"A",{id:!0,class:!0,href:!0});var qp=a(xt);ma=s(qp,"SPAN",{});var Lp=a(ma);F(Ho.$$.fragment,Lp),Lp.forEach(t),qp.forEach(t),fm=m(ti),ca=s(ti,"SPAN",{});var Rp=a(ca);pm=i(Rp,"CamembertForMultipleChoice"),Rp.forEach(t),ti.forEach(t),rn=m(o),Ue=s(o,"DIV",{class:!0});var it=a(Ue);F(Qo.$$.fragment,it),um=m(it),ha=s(it,"P",{});var Ap=a(ha);gm=i(Ap,`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Ap.forEach(t),_m=m(it),Vo=s(it,"P",{});var oi=a(Vo);vm=i(oi,"This model inherits from "),cs=s(oi,"A",{href:!0});var zp=a(cs);km=i(zp,"PreTrainedModel"),zp.forEach(t),bm=i(oi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),oi.forEach(t),Tm=m(it),Wo=s(it,"P",{});var ri=a(Wo);wm=i(ri,"This model is also a PyTorch "),Ko=s(ri,"A",{href:!0,rel:!0});var Dp=a(Ko);Em=i(Dp,"torch.nn.Module"),Dp.forEach(t),Cm=i(ri,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ri.forEach(t),ym=m(it),Go=s(it,"P",{});var si=a(Go);$m=i(si,"This class overrides "),hs=s(si,"A",{href:!0});var xp=a(hs);Fm=i(xp,"RobertaForMultipleChoice"),xp.forEach(t),Pm=i(si,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),si.forEach(t),it.forEach(t),sn=m(o),vt=s(o,"H2",{class:!0});var ai=a(vt);It=s(ai,"A",{id:!0,class:!0,href:!0});var Ip=a(It);fa=s(Ip,"SPAN",{});var Sp=a(fa);F(Xo.$$.fragment,Sp),Sp.forEach(t),Ip.forEach(t),Mm=m(ai),pa=s(ai,"SPAN",{});var Bp=a(pa);qm=i(Bp,"CamembertForTokenClassification"),Bp.forEach(t),ai.forEach(t),an=m(o),He=s(o,"DIV",{class:!0});var lt=a(He);F(jo.$$.fragment,lt),Lm=m(lt),ua=s(lt,"P",{});var Op=a(ua);Rm=i(Op,`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Op.forEach(t),Am=m(lt),Jo=s(lt,"P",{});var ni=a(Jo);zm=i(ni,"This model inherits from "),fs=s(ni,"A",{href:!0});var Np=a(fs);Dm=i(Np,"PreTrainedModel"),Np.forEach(t),xm=i(ni,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ni.forEach(t),Im=m(lt),Yo=s(lt,"P",{});var ii=a(Yo);Sm=i(ii,"This model is also a PyTorch "),Zo=s(ii,"A",{href:!0,rel:!0});var Up=a(Zo);Bm=i(Up,"torch.nn.Module"),Up.forEach(t),Om=i(ii,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ii.forEach(t),Nm=m(lt),er=s(lt,"P",{});var li=a(er);Um=i(li,"This class overrides "),ps=s(li,"A",{href:!0});var Hp=a(ps);Hm=i(Hp,"RobertaForTokenClassification"),Hp.forEach(t),Qm=i(li,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),li.forEach(t),lt.forEach(t),nn=m(o),kt=s(o,"H2",{class:!0});var di=a(kt);St=s(di,"A",{id:!0,class:!0,href:!0});var Qp=a(St);ga=s(Qp,"SPAN",{});var Vp=a(ga);F(tr.$$.fragment,Vp),Vp.forEach(t),Qp.forEach(t),Vm=m(di),_a=s(di,"SPAN",{});var Wp=a(_a);Wm=i(Wp,"CamembertForQuestionAnswering"),Wp.forEach(t),di.forEach(t),ln=m(o),Qe=s(o,"DIV",{class:!0});var dt=a(Qe);F(or.$$.fragment,dt),Km=m(dt),Bt=s(dt,"P",{});var Ha=a(Bt);Gm=i(Ha,`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),va=s(Ha,"CODE",{});var Kp=a(va);Xm=i(Kp,"span start logits"),Kp.forEach(t),jm=i(Ha," and "),ka=s(Ha,"CODE",{});var Gp=a(ka);Jm=i(Gp,"span end logits"),Gp.forEach(t),Ha.forEach(t),Ym=m(dt),rr=s(dt,"P",{});var mi=a(rr);Zm=i(mi,"This model inherits from "),us=s(mi,"A",{href:!0});var Xp=a(us);ec=i(Xp,"PreTrainedModel"),Xp.forEach(t),tc=i(mi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mi.forEach(t),oc=m(dt),sr=s(dt,"P",{});var ci=a(sr);rc=i(ci,"This model is also a PyTorch "),ar=s(ci,"A",{href:!0,rel:!0});var jp=a(ar);sc=i(jp,"torch.nn.Module"),jp.forEach(t),ac=i(ci,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ci.forEach(t),nc=m(dt),nr=s(dt,"P",{});var hi=a(nr);ic=i(hi,"This class overrides "),gs=s(hi,"A",{href:!0});var Jp=a(gs);lc=i(Jp,"RobertaForQuestionAnswering"),Jp.forEach(t),dc=i(hi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),hi.forEach(t),dt.forEach(t),dn=m(o),bt=s(o,"H2",{class:!0});var fi=a(bt);Ot=s(fi,"A",{id:!0,class:!0,href:!0});var Yp=a(Ot);ba=s(Yp,"SPAN",{});var Zp=a(ba);F(ir.$$.fragment,Zp),Zp.forEach(t),Yp.forEach(t),mc=m(fi),Ta=s(fi,"SPAN",{});var eu=a(Ta);cc=i(eu,"TFCamembertModel"),eu.forEach(t),fi.forEach(t),mn=m(o),Me=s(o,"DIV",{class:!0});var We=a(Me);F(lr.$$.fragment,We),hc=m(We),wa=s(We,"P",{});var tu=a(wa);fc=i(tu,"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),tu.forEach(t),pc=m(We),dr=s(We,"P",{});var pi=a(dr);uc=i(pi,"This model inherits from "),_s=s(pi,"A",{href:!0});var ou=a(_s);gc=i(ou,"TFPreTrainedModel"),ou.forEach(t),_c=i(pi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),pi.forEach(t),vc=m(We),mr=s(We,"P",{});var ui=a(mr);kc=i(ui,"This model is also a "),cr=s(ui,"A",{href:!0,rel:!0});var ru=a(cr);bc=i(ru,"tf.keras.Model"),ru.forEach(t),Tc=i(ui,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ui.forEach(t),wc=m(We),F(Nt.$$.fragment,We),Ec=m(We),hr=s(We,"P",{});var gi=a(hr);Cc=i(gi,"This class overrides "),vs=s(gi,"A",{href:!0});var su=a(vs);yc=i(su,"TFRobertaModel"),su.forEach(t),$c=i(gi,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),gi.forEach(t),We.forEach(t),cn=m(o),Tt=s(o,"H2",{class:!0});var _i=a(Tt);Ut=s(_i,"A",{id:!0,class:!0,href:!0});var au=a(Ut);Ea=s(au,"SPAN",{});var nu=a(Ea);F(fr.$$.fragment,nu),nu.forEach(t),au.forEach(t),Fc=m(_i),Ca=s(_i,"SPAN",{});var iu=a(Ca);Pc=i(iu,"TFCamembertForMaskedLM"),iu.forEach(t),_i.forEach(t),hn=m(o),qe=s(o,"DIV",{class:!0});var Ke=a(qe);F(pr.$$.fragment,Ke),Mc=m(Ke),ur=s(Ke,"P",{});var vi=a(ur);qc=i(vi,"CamemBERT Model with a "),ya=s(vi,"CODE",{});var lu=a(ya);Lc=i(lu,"language modeling"),lu.forEach(t),Rc=i(vi," head on top."),vi.forEach(t),Ac=m(Ke),gr=s(Ke,"P",{});var ki=a(gr);zc=i(ki,"This model inherits from "),ks=s(ki,"A",{href:!0});var du=a(ks);Dc=i(du,"TFPreTrainedModel"),du.forEach(t),xc=i(ki,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ki.forEach(t),Ic=m(Ke),_r=s(Ke,"P",{});var bi=a(_r);Sc=i(bi,"This model is also a "),vr=s(bi,"A",{href:!0,rel:!0});var mu=a(vr);Bc=i(mu,"tf.keras.Model"),mu.forEach(t),Oc=i(bi,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),bi.forEach(t),Nc=m(Ke),F(Ht.$$.fragment,Ke),Uc=m(Ke),kr=s(Ke,"P",{});var Ti=a(kr);Hc=i(Ti,"This class overrides "),bs=s(Ti,"A",{href:!0});var cu=a(bs);Qc=i(cu,"TFRobertaForMaskedLM"),cu.forEach(t),Vc=i(Ti,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ti.forEach(t),Ke.forEach(t),fn=m(o),wt=s(o,"H2",{class:!0});var wi=a(wt);Qt=s(wi,"A",{id:!0,class:!0,href:!0});var hu=a(Qt);$a=s(hu,"SPAN",{});var fu=a($a);F(br.$$.fragment,fu),fu.forEach(t),hu.forEach(t),Wc=m(wi),Fa=s(wi,"SPAN",{});var pu=a(Fa);Kc=i(pu,"TFCamembertForSequenceClassification"),pu.forEach(t),wi.forEach(t),pn=m(o),Le=s(o,"DIV",{class:!0});var Ge=a(Le);F(Tr.$$.fragment,Ge),Gc=m(Ge),Pa=s(Ge,"P",{});var uu=a(Pa);Xc=i(uu,`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),uu.forEach(t),jc=m(Ge),wr=s(Ge,"P",{});var Ei=a(wr);Jc=i(Ei,"This model inherits from "),Ts=s(Ei,"A",{href:!0});var gu=a(Ts);Yc=i(gu,"TFPreTrainedModel"),gu.forEach(t),Zc=i(Ei,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ei.forEach(t),eh=m(Ge),Er=s(Ge,"P",{});var Ci=a(Er);th=i(Ci,"This model is also a "),Cr=s(Ci,"A",{href:!0,rel:!0});var _u=a(Cr);oh=i(_u,"tf.keras.Model"),_u.forEach(t),rh=i(Ci,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ci.forEach(t),sh=m(Ge),F(Vt.$$.fragment,Ge),ah=m(Ge),yr=s(Ge,"P",{});var yi=a(yr);nh=i(yi,"This class overrides "),ws=s(yi,"A",{href:!0});var vu=a(ws);ih=i(vu,"TFRobertaForSequenceClassification"),vu.forEach(t),lh=i(yi,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),yi.forEach(t),Ge.forEach(t),un=m(o),Et=s(o,"H2",{class:!0});var $i=a(Et);Wt=s($i,"A",{id:!0,class:!0,href:!0});var ku=a(Wt);Ma=s(ku,"SPAN",{});var bu=a(Ma);F($r.$$.fragment,bu),bu.forEach(t),ku.forEach(t),dh=m($i),qa=s($i,"SPAN",{});var Tu=a(qa);mh=i(Tu,"TFCamembertForMultipleChoice"),Tu.forEach(t),$i.forEach(t),gn=m(o),Re=s(o,"DIV",{class:!0});var Xe=a(Re);F(Fr.$$.fragment,Xe),ch=m(Xe),La=s(Xe,"P",{});var wu=a(La);hh=i(wu,`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),wu.forEach(t),fh=m(Xe),Pr=s(Xe,"P",{});var Fi=a(Pr);ph=i(Fi,"This model inherits from "),Es=s(Fi,"A",{href:!0});var Eu=a(Es);uh=i(Eu,"TFPreTrainedModel"),Eu.forEach(t),gh=i(Fi,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Fi.forEach(t),_h=m(Xe),Mr=s(Xe,"P",{});var Pi=a(Mr);vh=i(Pi,"This model is also a "),qr=s(Pi,"A",{href:!0,rel:!0});var Cu=a(qr);kh=i(Cu,"tf.keras.Model"),Cu.forEach(t),bh=i(Pi,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pi.forEach(t),Th=m(Xe),F(Kt.$$.fragment,Xe),wh=m(Xe),Lr=s(Xe,"P",{});var Mi=a(Lr);Eh=i(Mi,"This class overrides "),Cs=s(Mi,"A",{href:!0});var yu=a(Cs);Ch=i(yu,"TFRobertaForMultipleChoice"),yu.forEach(t),yh=i(Mi,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Mi.forEach(t),Xe.forEach(t),_n=m(o),Ct=s(o,"H2",{class:!0});var qi=a(Ct);Gt=s(qi,"A",{id:!0,class:!0,href:!0});var $u=a(Gt);Ra=s($u,"SPAN",{});var Fu=a(Ra);F(Rr.$$.fragment,Fu),Fu.forEach(t),$u.forEach(t),$h=m(qi),Aa=s(qi,"SPAN",{});var Pu=a(Aa);Fh=i(Pu,"TFCamembertForTokenClassification"),Pu.forEach(t),qi.forEach(t),vn=m(o),Ae=s(o,"DIV",{class:!0});var je=a(Ae);F(Ar.$$.fragment,je),Ph=m(je),za=s(je,"P",{});var Mu=a(za);Mh=i(Mu,`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Mu.forEach(t),qh=m(je),zr=s(je,"P",{});var Li=a(zr);Lh=i(Li,"This model inherits from "),ys=s(Li,"A",{href:!0});var qu=a(ys);Rh=i(qu,"TFPreTrainedModel"),qu.forEach(t),Ah=i(Li,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Li.forEach(t),zh=m(je),Dr=s(je,"P",{});var Ri=a(Dr);Dh=i(Ri,"This model is also a "),xr=s(Ri,"A",{href:!0,rel:!0});var Lu=a(xr);xh=i(Lu,"tf.keras.Model"),Lu.forEach(t),Ih=i(Ri,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ri.forEach(t),Sh=m(je),F(Xt.$$.fragment,je),Bh=m(je),Ir=s(je,"P",{});var Ai=a(Ir);Oh=i(Ai,"This class overrides "),$s=s(Ai,"A",{href:!0});var Ru=a($s);Nh=i(Ru,"TFRobertaForTokenClassification"),Ru.forEach(t),Uh=i(Ai,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ai.forEach(t),je.forEach(t),kn=m(o),yt=s(o,"H2",{class:!0});var zi=a(yt);jt=s(zi,"A",{id:!0,class:!0,href:!0});var Au=a(jt);Da=s(Au,"SPAN",{});var zu=a(Da);F(Sr.$$.fragment,zu),zu.forEach(t),Au.forEach(t),Hh=m(zi),xa=s(zi,"SPAN",{});var Du=a(xa);Qh=i(Du,"TFCamembertForQuestionAnswering"),Du.forEach(t),zi.forEach(t),bn=m(o),ze=s(o,"DIV",{class:!0});var Je=a(ze);F(Br.$$.fragment,Je),Vh=m(Je),$t=s(Je,"P",{});var As=a($t);Wh=i(As,`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Ia=s(As,"CODE",{});var xu=a(Ia);Kh=i(xu,"span start logits"),xu.forEach(t),Gh=i(As," and "),Sa=s(As,"CODE",{});var Iu=a(Sa);Xh=i(Iu,"span end logits"),Iu.forEach(t),jh=i(As,")."),As.forEach(t),Jh=m(Je),Or=s(Je,"P",{});var Di=a(Or);Yh=i(Di,"This model inherits from "),Fs=s(Di,"A",{href:!0});var Su=a(Fs);Zh=i(Su,"TFPreTrainedModel"),Su.forEach(t),ef=i(Di,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Di.forEach(t),tf=m(Je),Nr=s(Je,"P",{});var xi=a(Nr);of=i(xi,"This model is also a "),Ur=s(xi,"A",{href:!0,rel:!0});var Bu=a(Ur);rf=i(Bu,"tf.keras.Model"),Bu.forEach(t),sf=i(xi,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),xi.forEach(t),af=m(Je),F(Jt.$$.fragment,Je),nf=m(Je),Hr=s(Je,"P",{});var Ii=a(Hr);lf=i(Ii,"This class overrides "),Ps=s(Ii,"A",{href:!0});var Ou=a(Ps);df=i(Ou,"TFRobertaForQuestionAnswering"),Ou.forEach(t),mf=i(Ii,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ii.forEach(t),Je.forEach(t),this.h()},h(){c(b,"name","hf:doc:metadata"),c(b,"content",JSON.stringify(Yu)),c(_,"id","camembert"),c(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_,"href","#camembert"),c(A,"class","relative group"),c(K,"id","overview"),c(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(K,"href","#overview"),c(u,"class","relative group"),c(j,"href","https://arxiv.org/abs/1911.03894"),c(j,"rel","nofollow"),c(re,"href","roberta"),c(l,"href","https://huggingface.co/camembert"),c(l,"rel","nofollow"),c(ke,"href","https://camembert-model.fr/"),c(ke,"rel","nofollow"),c(ve,"id","transformers.CamembertConfig"),c(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ve,"href","#transformers.CamembertConfig"),c(ge,"class","relative group"),c(Wr,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaConfig"),c(_e,"class","docstring"),c(Ft,"id","transformers.CamembertTokenizer"),c(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ft,"href","#transformers.CamembertTokenizer"),c(mt,"class","relative group"),c(Kr,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Gr,"href","/docs/transformers/v4.16.2/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(oo,"href","https://github.com/google/sentencepiece"),c(oo,"rel","nofollow"),c(Xr,"href","/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Ze,"class","docstring"),c(Pt,"class","docstring"),c(Mt,"class","docstring"),c(Qs,"class","docstring"),c($e,"class","docstring"),c(qt,"id","transformers.CamembertTokenizerFast"),c(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qt,"href","#transformers.CamembertTokenizerFast"),c(ht,"class","relative group"),c(Yr,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaTokenizer"),c(Zr,"href","/docs/transformers/v4.16.2/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(ho,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models"),c(ho,"rel","nofollow"),c(es,"href","/docs/transformers/v4.16.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(et,"class","docstring"),c(Lt,"class","docstring"),c(Ie,"class","docstring"),c(Rt,"id","transformers.CamembertModel"),c(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rt,"href","#transformers.CamembertModel"),c(ft,"class","relative group"),c(rs,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(To,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(To,"rel","nofollow"),c(ss,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaModel"),c(Se,"class","docstring"),c(At,"id","transformers.CamembertForCausalLM"),c(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(At,"href","#transformers.CamembertForCausalLM"),c(pt,"class","relative group"),c(as,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(Po,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Po,"rel","nofollow"),c(ns,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(Be,"class","docstring"),c(zt,"id","transformers.CamembertForMaskedLM"),c(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zt,"href","#transformers.CamembertForMaskedLM"),c(ut,"class","relative group"),c(is,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(Do,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Do,"rel","nofollow"),c(ls,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(Oe,"class","docstring"),c(Dt,"id","transformers.CamembertForSequenceClassification"),c(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Dt,"href","#transformers.CamembertForSequenceClassification"),c(gt,"class","relative group"),c(ds,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(No,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(No,"rel","nofollow"),c(ms,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(Ne,"class","docstring"),c(xt,"id","transformers.CamembertForMultipleChoice"),c(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(xt,"href","#transformers.CamembertForMultipleChoice"),c(_t,"class","relative group"),c(cs,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(Ko,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ko,"rel","nofollow"),c(hs,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(Ue,"class","docstring"),c(It,"id","transformers.CamembertForTokenClassification"),c(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(It,"href","#transformers.CamembertForTokenClassification"),c(vt,"class","relative group"),c(fs,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(Zo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Zo,"rel","nofollow"),c(ps,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(He,"class","docstring"),c(St,"id","transformers.CamembertForQuestionAnswering"),c(St,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(St,"href","#transformers.CamembertForQuestionAnswering"),c(kt,"class","relative group"),c(us,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.PreTrainedModel"),c(ar,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ar,"rel","nofollow"),c(gs,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(Qe,"class","docstring"),c(Ot,"id","transformers.TFCamembertModel"),c(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ot,"href","#transformers.TFCamembertModel"),c(bt,"class","relative group"),c(_s,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),c(cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(cr,"rel","nofollow"),c(vs,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.TFRobertaModel"),c(Me,"class","docstring"),c(Ut,"id","transformers.TFCamembertForMaskedLM"),c(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ut,"href","#transformers.TFCamembertForMaskedLM"),c(Tt,"class","relative group"),c(ks,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),c(vr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(vr,"rel","nofollow"),c(bs,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(qe,"class","docstring"),c(Qt,"id","transformers.TFCamembertForSequenceClassification"),c(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qt,"href","#transformers.TFCamembertForSequenceClassification"),c(wt,"class","relative group"),c(Ts,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),c(Cr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Cr,"rel","nofollow"),c(ws,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(Le,"class","docstring"),c(Wt,"id","transformers.TFCamembertForMultipleChoice"),c(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Wt,"href","#transformers.TFCamembertForMultipleChoice"),c(Et,"class","relative group"),c(Es,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),c(qr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(qr,"rel","nofollow"),c(Cs,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(Re,"class","docstring"),c(Gt,"id","transformers.TFCamembertForTokenClassification"),c(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gt,"href","#transformers.TFCamembertForTokenClassification"),c(Ct,"class","relative group"),c(ys,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),c(xr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(xr,"rel","nofollow"),c($s,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(Ae,"class","docstring"),c(jt,"id","transformers.TFCamembertForQuestionAnswering"),c(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(jt,"href","#transformers.TFCamembertForQuestionAnswering"),c(yt,"class","relative group"),c(Fs,"href","/docs/transformers/v4.16.2/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ur,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ur,"rel","nofollow"),c(Ps,"href","/docs/transformers/v4.16.2/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(ze,"class","docstring")},m(o,f){e(document.head,b),h(o,Z,f),h(o,A,f),e(A,_),e(_,z),P(G,z,null),e(A,le),e(A,D),e(D,de),h(o,Q,f),h(o,u,f),e(u,K),e(K,x),P(X,x,null),e(u,me),e(u,I),e(I,ce),h(o,se,f),h(o,H,f),e(H,E),e(H,j),e(j,V),e(H,g),h(o,k,f),h(o,ee,f),e(ee,O),h(o,ae,f),h(o,te,f),e(te,S),e(S,he),h(o,ne,f),h(o,v,f),e(v,fe),h(o,B,f),h(o,oe,f),e(oe,J),e(J,N),e(J,re),e(re,pe),e(J,C),h(o,ie,f),h(o,w,f),e(w,ue),e(w,l),e(l,p),e(w,W),e(w,ke),e(ke,Ee),e(w,y),h(o,Te,f),h(o,ge,f),e(ge,ve),e(ve,T),P(R,T,null),e(ge,Ce),e(ge,be),e(be,U),h(o,we,f),h(o,_e,f),P(Y,_e,null),e(_e,ye),e(_e,Zt),e(Zt,Si),e(Zt,Wr),e(Wr,Bi),e(Zt,Oi),h(o,Qa,f),h(o,mt,f),e(mt,Ft),e(Ft,Ds),P(eo,Ds,null),e(mt,Ni),e(mt,xs),e(xs,Ui),h(o,Va,f),h(o,$e,f),P(to,$e,null),e($e,Hi),e($e,Ye),e(Ye,Qi),e(Ye,Kr),e(Kr,Vi),e(Ye,Wi),e(Ye,Gr),e(Gr,Ki),e(Ye,Gi),e(Ye,oo),e(oo,Xi),e(Ye,ji),e($e,Ji),e($e,ro),e(ro,Yi),e(ro,Xr),e(Xr,Zi),e(ro,el),e($e,tl),e($e,ct),e(ct,ol),e(ct,Is),e(Is,rl),e(ct,sl),e(ct,Ss),e(Ss,al),e(ct,nl),e($e,il),e($e,Ze),P(so,Ze,null),e(Ze,ll),e(Ze,Bs),e(Bs,dl),e(Ze,ml),e(Ze,ao),e(ao,jr),e(jr,cl),e(jr,Os),e(Os,hl),e(ao,fl),e(ao,Jr),e(Jr,pl),e(Jr,Ns),e(Ns,ul),e($e,gl),e($e,Pt),P(no,Pt,null),e(Pt,_l),e(Pt,io),e(io,vl),e(io,Us),e(Us,kl),e(io,bl),e($e,Tl),e($e,Mt),P(lo,Mt,null),e(Mt,wl),e(Mt,Hs),e(Hs,El),e($e,Cl),e($e,Qs),h(o,Wa,f),h(o,ht,f),e(ht,qt),e(qt,Vs),P(mo,Vs,null),e(ht,yl),e(ht,Ws),e(Ws,$l),h(o,Ka,f),h(o,Ie,f),P(co,Ie,null),e(Ie,Fl),e(Ie,Ve),e(Ve,Pl),e(Ve,Ks),e(Ks,Ml),e(Ve,ql),e(Ve,Yr),e(Yr,Ll),e(Ve,Rl),e(Ve,Zr),e(Zr,Al),e(Ve,zl),e(Ve,ho),e(ho,Dl),e(Ve,xl),e(Ie,Il),e(Ie,fo),e(fo,Sl),e(fo,es),e(es,Bl),e(fo,Ol),e(Ie,Nl),e(Ie,et),P(po,et,null),e(et,Ul),e(et,Gs),e(Gs,Hl),e(et,Ql),e(et,uo),e(uo,ts),e(ts,Vl),e(ts,Xs),e(Xs,Wl),e(uo,Kl),e(uo,os),e(os,Gl),e(os,js),e(js,Xl),e(Ie,jl),e(Ie,Lt),P(go,Lt,null),e(Lt,Jl),e(Lt,Js),e(Js,Yl),h(o,Ga,f),h(o,ft,f),e(ft,Rt),e(Rt,Ys),P(_o,Ys,null),e(ft,Zl),e(ft,Zs),e(Zs,ed),h(o,Xa,f),h(o,Se,f),P(vo,Se,null),e(Se,td),e(Se,ea),e(ea,od),e(Se,rd),e(Se,ko),e(ko,sd),e(ko,rs),e(rs,ad),e(ko,nd),e(Se,id),e(Se,bo),e(bo,ld),e(bo,To),e(To,dd),e(bo,md),e(Se,cd),e(Se,wo),e(wo,hd),e(wo,ss),e(ss,fd),e(wo,pd),h(o,ja,f),h(o,pt,f),e(pt,At),e(At,ta),P(Eo,ta,null),e(pt,ud),e(pt,oa),e(oa,gd),h(o,Ja,f),h(o,Be,f),P(Co,Be,null),e(Be,_d),e(Be,yo),e(yo,vd),e(yo,ra),e(ra,kd),e(yo,bd),e(Be,Td),e(Be,$o),e($o,wd),e($o,as),e(as,Ed),e($o,Cd),e(Be,yd),e(Be,Fo),e(Fo,$d),e(Fo,Po),e(Po,Fd),e(Fo,Pd),e(Be,Md),e(Be,Mo),e(Mo,qd),e(Mo,ns),e(ns,Ld),e(Mo,Rd),h(o,Ya,f),h(o,ut,f),e(ut,zt),e(zt,sa),P(qo,sa,null),e(ut,Ad),e(ut,aa),e(aa,zd),h(o,Za,f),h(o,Oe,f),P(Lo,Oe,null),e(Oe,Dd),e(Oe,Ro),e(Ro,xd),e(Ro,na),e(na,Id),e(Ro,Sd),e(Oe,Bd),e(Oe,Ao),e(Ao,Od),e(Ao,is),e(is,Nd),e(Ao,Ud),e(Oe,Hd),e(Oe,zo),e(zo,Qd),e(zo,Do),e(Do,Vd),e(zo,Wd),e(Oe,Kd),e(Oe,xo),e(xo,Gd),e(xo,ls),e(ls,Xd),e(xo,jd),h(o,en,f),h(o,gt,f),e(gt,Dt),e(Dt,ia),P(Io,ia,null),e(gt,Jd),e(gt,la),e(la,Yd),h(o,tn,f),h(o,Ne,f),P(So,Ne,null),e(Ne,Zd),e(Ne,da),e(da,em),e(Ne,tm),e(Ne,Bo),e(Bo,om),e(Bo,ds),e(ds,rm),e(Bo,sm),e(Ne,am),e(Ne,Oo),e(Oo,nm),e(Oo,No),e(No,im),e(Oo,lm),e(Ne,dm),e(Ne,Uo),e(Uo,mm),e(Uo,ms),e(ms,cm),e(Uo,hm),h(o,on,f),h(o,_t,f),e(_t,xt),e(xt,ma),P(Ho,ma,null),e(_t,fm),e(_t,ca),e(ca,pm),h(o,rn,f),h(o,Ue,f),P(Qo,Ue,null),e(Ue,um),e(Ue,ha),e(ha,gm),e(Ue,_m),e(Ue,Vo),e(Vo,vm),e(Vo,cs),e(cs,km),e(Vo,bm),e(Ue,Tm),e(Ue,Wo),e(Wo,wm),e(Wo,Ko),e(Ko,Em),e(Wo,Cm),e(Ue,ym),e(Ue,Go),e(Go,$m),e(Go,hs),e(hs,Fm),e(Go,Pm),h(o,sn,f),h(o,vt,f),e(vt,It),e(It,fa),P(Xo,fa,null),e(vt,Mm),e(vt,pa),e(pa,qm),h(o,an,f),h(o,He,f),P(jo,He,null),e(He,Lm),e(He,ua),e(ua,Rm),e(He,Am),e(He,Jo),e(Jo,zm),e(Jo,fs),e(fs,Dm),e(Jo,xm),e(He,Im),e(He,Yo),e(Yo,Sm),e(Yo,Zo),e(Zo,Bm),e(Yo,Om),e(He,Nm),e(He,er),e(er,Um),e(er,ps),e(ps,Hm),e(er,Qm),h(o,nn,f),h(o,kt,f),e(kt,St),e(St,ga),P(tr,ga,null),e(kt,Vm),e(kt,_a),e(_a,Wm),h(o,ln,f),h(o,Qe,f),P(or,Qe,null),e(Qe,Km),e(Qe,Bt),e(Bt,Gm),e(Bt,va),e(va,Xm),e(Bt,jm),e(Bt,ka),e(ka,Jm),e(Qe,Ym),e(Qe,rr),e(rr,Zm),e(rr,us),e(us,ec),e(rr,tc),e(Qe,oc),e(Qe,sr),e(sr,rc),e(sr,ar),e(ar,sc),e(sr,ac),e(Qe,nc),e(Qe,nr),e(nr,ic),e(nr,gs),e(gs,lc),e(nr,dc),h(o,dn,f),h(o,bt,f),e(bt,Ot),e(Ot,ba),P(ir,ba,null),e(bt,mc),e(bt,Ta),e(Ta,cc),h(o,mn,f),h(o,Me,f),P(lr,Me,null),e(Me,hc),e(Me,wa),e(wa,fc),e(Me,pc),e(Me,dr),e(dr,uc),e(dr,_s),e(_s,gc),e(dr,_c),e(Me,vc),e(Me,mr),e(mr,kc),e(mr,cr),e(cr,bc),e(mr,Tc),e(Me,wc),P(Nt,Me,null),e(Me,Ec),e(Me,hr),e(hr,Cc),e(hr,vs),e(vs,yc),e(hr,$c),h(o,cn,f),h(o,Tt,f),e(Tt,Ut),e(Ut,Ea),P(fr,Ea,null),e(Tt,Fc),e(Tt,Ca),e(Ca,Pc),h(o,hn,f),h(o,qe,f),P(pr,qe,null),e(qe,Mc),e(qe,ur),e(ur,qc),e(ur,ya),e(ya,Lc),e(ur,Rc),e(qe,Ac),e(qe,gr),e(gr,zc),e(gr,ks),e(ks,Dc),e(gr,xc),e(qe,Ic),e(qe,_r),e(_r,Sc),e(_r,vr),e(vr,Bc),e(_r,Oc),e(qe,Nc),P(Ht,qe,null),e(qe,Uc),e(qe,kr),e(kr,Hc),e(kr,bs),e(bs,Qc),e(kr,Vc),h(o,fn,f),h(o,wt,f),e(wt,Qt),e(Qt,$a),P(br,$a,null),e(wt,Wc),e(wt,Fa),e(Fa,Kc),h(o,pn,f),h(o,Le,f),P(Tr,Le,null),e(Le,Gc),e(Le,Pa),e(Pa,Xc),e(Le,jc),e(Le,wr),e(wr,Jc),e(wr,Ts),e(Ts,Yc),e(wr,Zc),e(Le,eh),e(Le,Er),e(Er,th),e(Er,Cr),e(Cr,oh),e(Er,rh),e(Le,sh),P(Vt,Le,null),e(Le,ah),e(Le,yr),e(yr,nh),e(yr,ws),e(ws,ih),e(yr,lh),h(o,un,f),h(o,Et,f),e(Et,Wt),e(Wt,Ma),P($r,Ma,null),e(Et,dh),e(Et,qa),e(qa,mh),h(o,gn,f),h(o,Re,f),P(Fr,Re,null),e(Re,ch),e(Re,La),e(La,hh),e(Re,fh),e(Re,Pr),e(Pr,ph),e(Pr,Es),e(Es,uh),e(Pr,gh),e(Re,_h),e(Re,Mr),e(Mr,vh),e(Mr,qr),e(qr,kh),e(Mr,bh),e(Re,Th),P(Kt,Re,null),e(Re,wh),e(Re,Lr),e(Lr,Eh),e(Lr,Cs),e(Cs,Ch),e(Lr,yh),h(o,_n,f),h(o,Ct,f),e(Ct,Gt),e(Gt,Ra),P(Rr,Ra,null),e(Ct,$h),e(Ct,Aa),e(Aa,Fh),h(o,vn,f),h(o,Ae,f),P(Ar,Ae,null),e(Ae,Ph),e(Ae,za),e(za,Mh),e(Ae,qh),e(Ae,zr),e(zr,Lh),e(zr,ys),e(ys,Rh),e(zr,Ah),e(Ae,zh),e(Ae,Dr),e(Dr,Dh),e(Dr,xr),e(xr,xh),e(Dr,Ih),e(Ae,Sh),P(Xt,Ae,null),e(Ae,Bh),e(Ae,Ir),e(Ir,Oh),e(Ir,$s),e($s,Nh),e(Ir,Uh),h(o,kn,f),h(o,yt,f),e(yt,jt),e(jt,Da),P(Sr,Da,null),e(yt,Hh),e(yt,xa),e(xa,Qh),h(o,bn,f),h(o,ze,f),P(Br,ze,null),e(ze,Vh),e(ze,$t),e($t,Wh),e($t,Ia),e(Ia,Kh),e($t,Gh),e($t,Sa),e(Sa,Xh),e($t,jh),e(ze,Jh),e(ze,Or),e(Or,Yh),e(Or,Fs),e(Fs,Zh),e(Or,ef),e(ze,tf),e(ze,Nr),e(Nr,of),e(Nr,Ur),e(Ur,rf),e(Nr,sf),e(ze,af),P(Jt,ze,null),e(ze,nf),e(ze,Hr),e(Hr,lf),e(Hr,Ps),e(Ps,df),e(Hr,mf),Tn=!0},p(o,[f]){const Qr={};f&2&&(Qr.$$scope={dirty:f,ctx:o}),Nt.$set(Qr);const Ba={};f&2&&(Ba.$$scope={dirty:f,ctx:o}),Ht.$set(Ba);const Oa={};f&2&&(Oa.$$scope={dirty:f,ctx:o}),Vt.$set(Oa);const Na={};f&2&&(Na.$$scope={dirty:f,ctx:o}),Kt.$set(Na);const Vr={};f&2&&(Vr.$$scope={dirty:f,ctx:o}),Xt.$set(Vr);const Ua={};f&2&&(Ua.$$scope={dirty:f,ctx:o}),Jt.$set(Ua)},i(o){Tn||(M(G.$$.fragment,o),M(X.$$.fragment,o),M(R.$$.fragment,o),M(Y.$$.fragment,o),M(eo.$$.fragment,o),M(to.$$.fragment,o),M(so.$$.fragment,o),M(no.$$.fragment,o),M(lo.$$.fragment,o),M(mo.$$.fragment,o),M(co.$$.fragment,o),M(po.$$.fragment,o),M(go.$$.fragment,o),M(_o.$$.fragment,o),M(vo.$$.fragment,o),M(Eo.$$.fragment,o),M(Co.$$.fragment,o),M(qo.$$.fragment,o),M(Lo.$$.fragment,o),M(Io.$$.fragment,o),M(So.$$.fragment,o),M(Ho.$$.fragment,o),M(Qo.$$.fragment,o),M(Xo.$$.fragment,o),M(jo.$$.fragment,o),M(tr.$$.fragment,o),M(or.$$.fragment,o),M(ir.$$.fragment,o),M(lr.$$.fragment,o),M(Nt.$$.fragment,o),M(fr.$$.fragment,o),M(pr.$$.fragment,o),M(Ht.$$.fragment,o),M(br.$$.fragment,o),M(Tr.$$.fragment,o),M(Vt.$$.fragment,o),M($r.$$.fragment,o),M(Fr.$$.fragment,o),M(Kt.$$.fragment,o),M(Rr.$$.fragment,o),M(Ar.$$.fragment,o),M(Xt.$$.fragment,o),M(Sr.$$.fragment,o),M(Br.$$.fragment,o),M(Jt.$$.fragment,o),Tn=!0)},o(o){q(G.$$.fragment,o),q(X.$$.fragment,o),q(R.$$.fragment,o),q(Y.$$.fragment,o),q(eo.$$.fragment,o),q(to.$$.fragment,o),q(so.$$.fragment,o),q(no.$$.fragment,o),q(lo.$$.fragment,o),q(mo.$$.fragment,o),q(co.$$.fragment,o),q(po.$$.fragment,o),q(go.$$.fragment,o),q(_o.$$.fragment,o),q(vo.$$.fragment,o),q(Eo.$$.fragment,o),q(Co.$$.fragment,o),q(qo.$$.fragment,o),q(Lo.$$.fragment,o),q(Io.$$.fragment,o),q(So.$$.fragment,o),q(Ho.$$.fragment,o),q(Qo.$$.fragment,o),q(Xo.$$.fragment,o),q(jo.$$.fragment,o),q(tr.$$.fragment,o),q(or.$$.fragment,o),q(ir.$$.fragment,o),q(lr.$$.fragment,o),q(Nt.$$.fragment,o),q(fr.$$.fragment,o),q(pr.$$.fragment,o),q(Ht.$$.fragment,o),q(br.$$.fragment,o),q(Tr.$$.fragment,o),q(Vt.$$.fragment,o),q($r.$$.fragment,o),q(Fr.$$.fragment,o),q(Kt.$$.fragment,o),q(Rr.$$.fragment,o),q(Ar.$$.fragment,o),q(Xt.$$.fragment,o),q(Sr.$$.fragment,o),q(Br.$$.fragment,o),q(Jt.$$.fragment,o),Tn=!1},d(o){t(b),o&&t(Z),o&&t(A),L(G),o&&t(Q),o&&t(u),L(X),o&&t(se),o&&t(H),o&&t(k),o&&t(ee),o&&t(ae),o&&t(te),o&&t(ne),o&&t(v),o&&t(B),o&&t(oe),o&&t(ie),o&&t(w),o&&t(Te),o&&t(ge),L(R),o&&t(we),o&&t(_e),L(Y),o&&t(Qa),o&&t(mt),L(eo),o&&t(Va),o&&t($e),L(to),L(so),L(no),L(lo),o&&t(Wa),o&&t(ht),L(mo),o&&t(Ka),o&&t(Ie),L(co),L(po),L(go),o&&t(Ga),o&&t(ft),L(_o),o&&t(Xa),o&&t(Se),L(vo),o&&t(ja),o&&t(pt),L(Eo),o&&t(Ja),o&&t(Be),L(Co),o&&t(Ya),o&&t(ut),L(qo),o&&t(Za),o&&t(Oe),L(Lo),o&&t(en),o&&t(gt),L(Io),o&&t(tn),o&&t(Ne),L(So),o&&t(on),o&&t(_t),L(Ho),o&&t(rn),o&&t(Ue),L(Qo),o&&t(sn),o&&t(vt),L(Xo),o&&t(an),o&&t(He),L(jo),o&&t(nn),o&&t(kt),L(tr),o&&t(ln),o&&t(Qe),L(or),o&&t(dn),o&&t(bt),L(ir),o&&t(mn),o&&t(Me),L(lr),L(Nt),o&&t(cn),o&&t(Tt),L(fr),o&&t(hn),o&&t(qe),L(pr),L(Ht),o&&t(fn),o&&t(wt),L(br),o&&t(pn),o&&t(Le),L(Tr),L(Vt),o&&t(un),o&&t(Et),L($r),o&&t(gn),o&&t(Re),L(Fr),L(Kt),o&&t(_n),o&&t(Ct),L(Rr),o&&t(vn),o&&t(Ae),L(Ar),L(Xt),o&&t(kn),o&&t(yt),L(Sr),o&&t(bn),o&&t(ze),L(Br),L(Jt)}}}const Yu={local:"camembert",sections:[{local:"overview",title:"Overview"},{local:"transformers.CamembertConfig",title:"CamembertConfig"},{local:"transformers.CamembertTokenizer",title:"CamembertTokenizer"},{local:"transformers.CamembertTokenizerFast",title:"CamembertTokenizerFast"},{local:"transformers.CamembertModel",title:"CamembertModel"},{local:"transformers.CamembertForCausalLM",title:"CamembertForCausalLM"},{local:"transformers.CamembertForMaskedLM",title:"CamembertForMaskedLM"},{local:"transformers.CamembertForSequenceClassification",title:"CamembertForSequenceClassification"},{local:"transformers.CamembertForMultipleChoice",title:"CamembertForMultipleChoice"},{local:"transformers.CamembertForTokenClassification",title:"CamembertForTokenClassification"},{local:"transformers.CamembertForQuestionAnswering",title:"CamembertForQuestionAnswering"},{local:"transformers.TFCamembertModel",title:"TFCamembertModel"},{local:"transformers.TFCamembertForMaskedLM",title:"TFCamembertForMaskedLM"},{local:"transformers.TFCamembertForSequenceClassification",title:"TFCamembertForSequenceClassification"},{local:"transformers.TFCamembertForMultipleChoice",title:"TFCamembertForMultipleChoice"},{local:"transformers.TFCamembertForTokenClassification",title:"TFCamembertForTokenClassification"},{local:"transformers.TFCamembertForQuestionAnswering",title:"TFCamembertForQuestionAnswering"}],title:"CamemBERT"};function Zu(xe,b,Z){let{fw:A}=b;return xe.$$set=_=>{"fw"in _&&Z(0,A=_.fw)},[A]}class sg extends Nu{constructor(b){super();Uu(this,b,Zu,Ju,Hu,{fw:0})}}export{sg as default,Yu as metadata};
