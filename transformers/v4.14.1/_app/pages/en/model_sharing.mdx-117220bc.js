import{S as Qm,i as Km,s as Vm,e as i,k as u,w as h,t as r,L as eh,c as n,d as t,m as p,a as f,x as c,h as l,b as m,M as qu,J as o,g as s,y as d,q as y,o as v,B as w}from"../../chunks/vendor-b1433968.js";import{T as Zm}from"../../chunks/Tip-c3840994.js";import{I as F}from"../../chunks/IconCopyLink-7029626d.js";import{C as g}from"../../chunks/CodeBlock-a320dbd7.js";import"../../chunks/CopyButton-f65cb278.js";function th(Je){let $,A,_,b,I,k,T,x;return{c(){$=i("p"),A=r("You will need to create an account on "),_=i("a"),b=r("huggingface.co"),I=r(" for this."),k=u(),T=i("p"),x=r("Optionally, you can join an existing organization or create a new one."),this.h()},l(P){$=n(P,"P",{});var z=f($);A=l(z,"You will need to create an account on "),_=n(z,"A",{href:!0,rel:!0});var Y=f(_);b=l(Y,"huggingface.co"),Y.forEach(t),I=l(z," for this."),z.forEach(t),k=p(P),T=n(P,"P",{});var Qt=f(T);x=l(Qt,"Optionally, you can join an existing organization or create a new one."),Qt.forEach(t),this.h()},h(){m(_,"href","https://huggingface.co/join"),m(_,"rel","nofollow")},m(P,z){s(P,$,z),o($,A),o($,_),o(_,b),o($,I),s(P,k,z),s(P,T,z),o(T,x)},d(P){P&&t($),P&&t(k),P&&t(T)}}}function oh(Je){let $,A,_,b,I;return{c(){$=i("p"),A=r("Model cards used to live in the \u{1F917} Transformers repo under "),_=i("em"),b=r("model_cards/"),I=r(`, but for consistency and scalability we
migrated every model card from the repo to its corresponding huggingface.co model repo.`)},l(k){$=n(k,"P",{});var T=f($);A=l(T,"Model cards used to live in the \u{1F917} Transformers repo under "),_=n(T,"EM",{});var x=f(_);b=l(x,"model_cards/"),x.forEach(t),I=l(T,`, but for consistency and scalability we
migrated every model card from the repo to its corresponding huggingface.co model repo.`),T.forEach(t)},m(k,T){s(k,$,T),o($,A),o($,_),o(_,b),o($,I)},d(k){k&&t($)}}}function ah(Je){let $,A,_,b,I,k,T,x,P,z,Y,Qt,Xe,xs,Os,tr,M,Fu,or,ue,ar,j,Ns,Kt,Ys,Ls,Ho,Bs,Us,Wo,Rs,Hs,Ze,Ws,Gs,rr,G,pe,Go,Qe,Js,Jo,Xs,lr,me,Zs,Xo,Qs,Ks,sr,Vt,Vs,ir,L,Zo,ei,ti,Qo,oi,ai,Ko,ri,nr,he,li,Vo,si,ii,fr,eo,ni,ur,Ke,pr,J,ce,ea,Ve,fi,ta,ui,mr,X,de,oa,et,pi,aa,mi,hr,to,hi,cr,tt,dr,ye,ci,ra,di,yi,yr,O,vi,oo,wi,ot,gi,$i,la,_i,bi,sa,ki,Ei,vr,Z,ve,ia,at,Ti,na,Pi,wr,C,Iu,gr,we,Ai,fa,ji,qi,$r,rt,_r,ge,Fi,ua,Ii,zi,br,$e,Mi,pa,Ci,Si,kr,lt,Er,_e,Di,ma,xi,Oi,Tr,st,Pr,be,Ni,ha,Yi,Li,Ar,ke,Bi,ca,Ui,Ri,jr,Q,Ee,da,it,Hi,ya,Wi,qr,ao,Gi,Fr,nt,Ir,Te,Ji,va,Xi,Zi,zr,ft,Mr,ro,Qi,Cr,lo,Ki,Sr,ut,Dr,B,Vi,wa,en,tn,ga,on,an,xr,pt,Or,so,rn,Nr,mt,Yr,K,Pe,$a,ht,ln,_a,sn,Lr,S,zu,Br,V,Ae,ba,ct,nn,ka,fn,Ur,io,un,Rr,je,pn,dt,mn,hn,Hr,qe,cn,Ea,dn,yn,Wr,Fe,vn,Ta,wn,gn,Gr,yt,Jr,no,$n,Xr,vt,Zr,Ie,_n,Pa,bn,kn,Qr,wt,Kr,fo,En,Vr,gt,el,uo,Tn,tl,$t,ol,po,Pn,al,ze,An,_t,jn,qn,rl,Me,Fn,bt,In,zn,ll,ee,Ce,Aa,kt,Mn,ja,Cn,sl,N,Sn,qa,Dn,xn,Et,On,Nn,Tt,Yn,Ln,il,Se,Bn,mo,Un,Rn,nl,Pt,fl,De,Hn,ho,Wn,Gn,ul,At,pl,co,Jn,ml,yo,Xn,hl,jt,cl,vo,Zn,dl,qt,yl,wo,Qn,vl,te,xe,Fa,Ft,Kn,Ia,Vn,wl,go,ef,gl,E,oe,tf,za,of,af,$o,rf,lf,sf,It,nf,Ma,ff,uf,pf,zt,mf,Ca,hf,cf,df,ae,yf,Sa,vf,wf,_o,gf,$f,_f,re,bf,Da,kf,Ef,bo,Tf,Pf,Af,D,jf,xa,qf,Ff,Oa,If,zf,Na,Mf,Cf,ko,Sf,Df,xf,le,Of,Ya,Nf,Yf,Eo,Lf,Bf,$l,To,Uf,_l,se,Oe,La,Mt,Rf,Ba,Hf,bl,Po,Wf,kl,Ct,El,Ao,Gf,Tl,St,Pl,Ne,Jf,Ua,Xf,Zf,Al,Dt,jl,jo,Qf,ql,xt,Fl,qo,Kf,Il,Ot,zl,Fo,Vf,Ml,ie,Ye,Ra,Nt,eu,Ha,tu,Cl,Le,ou,Yt,au,ru,Sl,Be,Dl,Io,lu,xl,ne,Ue,Wa,Lt,su,Ga,iu,Ol,zo,nu,Nl,Mo,fu,Yl,Bt,Ll,U,uu,Ja,pu,mu,Xa,hu,cu,Bl,Ut,Ul,fe,Re,Za,Rt,du,Qa,yu,Rl,Co,vu,Hl,He,wu,Ka,gu,$u,Wl,Ht,Gl,R,_u,Wt,bu,ku,Va,Eu,Tu,Jl,Gt,Xl,So,Pu,Zl,Jt,Ql,Do,Au,Kl,Xt,Vl;return k=new F({}),ue=new Zm({props:{$$slots:{default:[th]},$$scope:{ctx:Je}}}),Qe=new F({}),Ke=new g({props:{code:`model = AutoModel.from_pretrained(
  "julien-c/EsperBERTo-small",
  revision="v2.0.1" # tag name, or branch name, or commit hash
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = AutoModel.from_pretrained(</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">  <span class="hljs-string">&quot;julien-c/EsperBERTo-small&quot;</span>,</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">  revision=<span class="hljs-string">&quot;v2.0.1&quot;</span> <span class="hljs-comment"># tag name, or branch name, or commit hash</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">)</span>`}}),Ve=new F({}),et=new F({}),tt=new g({props:{code:"transformers-cli login,",highlighted:"transformers-cli login"}}),at=new F({}),rt=new g({props:{code:'finetuned_model.push_to_hub("my-awesome-model"),',highlighted:'finetuned_model.push_to_hub(<span class="hljs-string">&quot;my-awesome-model&quot;</span>)'}}),lt=new g({props:{code:`from transformers import AutoModel

model = AutoModel.from_pretrained("your_username/my-awesome-model"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">&quot;your_username/my-awesome-model&quot;</span>)`}}),st=new g({props:{code:'finetuned_model.save_pretrained(save_directory, push_to_hub=True, repo_name="my-awesome-model"),',highlighted:'finetuned_model.save_pretrained(save_directory, push_to_hub=<span class="hljs-literal">True</span>, repo_name=<span class="hljs-string">&quot;my-awesome-model&quot;</span>)'}}),it=new F({}),nt=new g({props:{code:'tokenizer.push_to_hub("my-awesome-model"),',highlighted:'tokenizer.push_to_hub(<span class="hljs-string">&quot;my-awesome-model&quot;</span>)'}}),ft=new g({props:{code:"tokenizer.push_to_hub(repo_url=my_repo_url),",highlighted:"tokenizer.push_to_hub(repo_url=my_repo_url)"}}),ut=new g({props:{code:`from transformers import AutoModel

model = AutoModel.from_pretrained(save_directory, from_tf=True),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(save_directory, from_tf=<span class="hljs-literal">True</span>)`}}),pt=new g({props:{code:'model.push_to_hub("my-awesome-model"),',highlighted:'model.push_to_hub(<span class="hljs-string">&quot;my-awesome-model&quot;</span>)'}}),mt=new g({props:{code:"model.push_to_hub(repo_url=my_repo_url),",highlighted:"model.push_to_hub(repo_url=my_repo_url)"}}),ht=new F({}),ct=new F({}),yt=new g({props:{code:"transformers-cli login,",highlighted:"transformers-cli login"}}),vt=new g({props:{code:"transformers-cli repo create your-model-name,",highlighted:"transformers-cli repo create your-model-name"}}),wt=new g({props:{code:"transformers-cli repo create your-model-name --organization your-org-name,",highlighted:"transformers-cli repo create your-model-name --organization your-org-name"}}),gt=new g({props:{code:`# Make sure you have git-lfs installed
# (https://git-lfs.github.com/)
git lfs install

git clone https://huggingface.co/username/your-model-name,`,highlighted:`<span class="hljs-comment"># Make sure you have git-lfs installed</span>
<span class="hljs-comment"># (https://git-lfs.github.com/)</span>
git lfs install

git <span class="hljs-built_in">clone</span> https://huggingface.co/username/your-model-name`}}),$t=new g({props:{code:`# Commit as usual
cd your-model-name
echo "hello" >> README.md
git add . && git commit -m "Update from $USER",`,highlighted:`<span class="hljs-comment"># Commit as usual</span>
<span class="hljs-built_in">cd</span> your-model-name
<span class="hljs-built_in">echo</span> <span class="hljs-string">&quot;hello&quot;</span> &gt;&gt; README.md
git add . &amp;&amp; git commit -m <span class="hljs-string">&quot;Update from <span class="hljs-variable">$USER</span>&quot;</span>`}}),kt=new F({}),Pt=new g({props:{code:"from transformers import TFDistilBertForSequenceClassification,",highlighted:'<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDistilBertForSequenceClassification</span>'}}),At=new g({props:{code:"from transformers import DistilBertForSequenceClassification,",highlighted:'<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertForSequenceClassification</span>'}}),jt=new g({props:{code:`tf_model = TFDistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_pt=True)
tf_model.save_pretrained("path/to/awesome-name-you-picked"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tf_model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;path/to/awesome-name-you-picked&quot;</span>, from_pt=<span class="hljs-literal">True</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tf_model.save_pretrained(<span class="hljs-string">&quot;path/to/awesome-name-you-picked&quot;</span>)</span>`}}),qt=new g({props:{code:`pt_model = DistilBertForSequenceClassification.from_pretrained("path/to/awesome-name-you-picked", from_tf=True)
pt_model.save_pretrained("path/to/awesome-name-you-picked"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pt_model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;path/to/awesome-name-you-picked&quot;</span>, from_tf=<span class="hljs-literal">True</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pt_model.save_pretrained(<span class="hljs-string">&quot;path/to/awesome-name-you-picked&quot;</span>)</span>`}}),Ft=new F({}),Mt=new F({}),Ct=new g({props:{code:`model.save_pretrained("path/to/repo/clone/your-model-name")
tokenizer.save_pretrained("path/to/repo/clone/your-model-name"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model.save_pretrained(<span class="hljs-string">&quot;path/to/repo/clone/your-model-name&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer.save_pretrained(<span class="hljs-string">&quot;path/to/repo/clone/your-model-name&quot;</span>)</span>`}}),St=new g({props:{code:`trainer.save_model("path/to/awesome-name-you-picked")
tokenizer.save_pretrained("path/to/repo/clone/your-model-name"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">trainer.save_model(<span class="hljs-string">&quot;path/to/awesome-name-you-picked&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer.save_pretrained(<span class="hljs-string">&quot;path/to/repo/clone/your-model-name&quot;</span>)</span>`}}),Dt=new g({props:{code:`git add --all
git status,`,highlighted:`git add --all
git status`}}),xt=new g({props:{code:'git commit -m "First version of the your-model-name model and tokenizer.",',highlighted:'git commit -m <span class="hljs-string">&quot;First version of the your-model-name model and tokenizer.&quot;</span>'}}),Ot=new g({props:{code:"git push,",highlighted:"git push"}}),Nt=new F({}),Be=new Zm({props:{$$slots:{default:[oh]},$$scope:{ctx:Je}}}),Lt=new F({}),Bt=new g({props:{code:`tokenizer = AutoTokenizer.from_pretrained("namespace/awesome-name-you-picked")
model = AutoModel.from_pretrained("namespace/awesome-name-you-picked"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;namespace/awesome-name-you-picked&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;namespace/awesome-name-you-picked&quot;</span>)</span>`}}),Ut=new g({props:{code:`tokenizer = AutoTokenizer.from_pretrained(
  "julien-c/EsperBERTo-small",
  revision="v2.0.1" # tag name, or branch name, or commit hash
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">  <span class="hljs-string">&quot;julien-c/EsperBERTo-small&quot;</span>,</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">  revision=<span class="hljs-string">&quot;v2.0.1&quot;</span> <span class="hljs-comment"># tag name, or branch name, or commit hash</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">)</span>`}}),Rt=new F({}),Ht=new g({props:{code:"sudo apt-get install git-lfs,",highlighted:"sudo apt-get install git-lfs"}}),Gt=new g({props:{code:`transformers-cli login
transformers-cli repo create your-model-name,`,highlighted:`transformers-cli login
transformers-cli repo create your-model-name`}}),Jt=new g({props:{code:`git lfs install

git clone https://username:password@huggingface.co/username/your-model-name
# Alternatively if you have a token,
# you can use it instead of your password
git clone https://username:token@huggingface.co/username/your-model-name

cd your-model-name
git config --global user.email "email@example.com"
# Tip: using the same email than for your huggingface.co account will link your commits to your profile
git config --global user.name "Your name",`,highlighted:`git lfs install

git <span class="hljs-built_in">clone</span> https://username:password@huggingface.co/username/your-model-name
<span class="hljs-comment"># Alternatively if you have a token,</span>
<span class="hljs-comment"># you can use it instead of your password</span>
git <span class="hljs-built_in">clone</span> https://username:token@huggingface.co/username/your-model-name

<span class="hljs-built_in">cd</span> your-model-name
git config --global user.email <span class="hljs-string">&quot;email@example.com&quot;</span>
<span class="hljs-comment"># Tip: using the same email than for your huggingface.co account will link your commits to your profile</span>
git config --global user.name <span class="hljs-string">&quot;Your name&quot;</span>`}}),Xt=new g({props:{code:`git add .
git commit -m "Initial commit"
git push,`,highlighted:`git add .
git commit -m <span class="hljs-string">&quot;Initial commit&quot;</span>
git push`}}),{c(){$=i("meta"),A=u(),_=i("h1"),b=i("a"),I=i("span"),h(k.$$.fragment),T=u(),x=i("span"),P=r("Model sharing and uploading"),z=u(),Y=i("p"),Qt=r(`In this page, we will show you how to share a model you have trained or fine-tuned on new data with the community on
the `),Xe=i("a"),xs=r("model hub"),Os=r("."),tr=u(),M=i("iframe"),or=u(),h(ue.$$.fragment),ar=u(),j=i("p"),Ns=r("We have seen in the "),Kt=i("a"),Ys=r("training tutorial"),Ls=r(`: how to fine-tune a model on a given task. You have probably
done something similar on your task, either using the model directly in your own training loop or using the
`),Ho=i("code"),Bs=r("Trainer"),Us=r("/"),Wo=i("code"),Rs=r("TFTrainer"),Hs=r(` class. Let\u2019s see how you can share the result on the
`),Ze=i("a"),Ws=r("model hub"),Gs=r("."),rr=u(),G=i("h2"),pe=i("a"),Go=i("span"),h(Qe.$$.fragment),Js=u(),Jo=i("span"),Xs=r("Model versioning"),lr=u(),me=i("p"),Zs=r(`Since version v3.5.0, the model hub has built-in model versioning based on git and git-lfs. It is based on the paradigm
that one model `),Xo=i("em"),Qs=r("is"),Ks=r(" one repo."),sr=u(),Vt=i("p"),Vs=r("This allows:"),ir=u(),L=i("ul"),Zo=i("li"),ei=r("built-in versioning"),ti=u(),Qo=i("li"),oi=r("access control"),ai=u(),Ko=i("li"),ri=r("scalability"),nr=u(),he=i("p"),li=r("This is built around "),Vo=i("em"),si=r("revisions"),ii=r(`, which is a way to pin a specific version of a model, using a commit hash, tag or
branch.`),fr=u(),eo=i("p"),ni=r("For instance:"),ur=u(),h(Ke.$$.fragment),pr=u(),J=i("h2"),ce=i("a"),ea=i("span"),h(Ve.$$.fragment),fi=u(),ta=i("span"),ui=r("Push your model from Python"),mr=u(),X=i("h3"),de=i("a"),oa=i("span"),h(et.$$.fragment),pi=u(),aa=i("span"),mi=r("Preparation"),hr=u(),to=i("p"),hi=r(`The first step is to make sure your credentials to the hub are stored somewhere. This can be done in two ways. If you
have access to a terminal, you cam just run the following command in the virtual environment where you installed \u{1F917}
Transformers:`),cr=u(),h(tt.$$.fragment),dr=u(),ye=i("p"),ci=r("It will store your access token in the Hugging Face cache folder (by default "),ra=i("code"),di=r("~/.cache/"),yi=r(")."),yr=u(),O=i("p"),vi=r(`If you don\u2019t have an easy access to a terminal (for instance in a Colab session), you can find a token linked to your
account by going on `),oo=i("em"),wi=r("huggingface.co <"),ot=i("a"),gi=r("https://huggingface.co/>"),$i=r(`, click on your avatar on the top left corner, then on
`),la=i("em"),_i=r("Edit profile"),bi=r(" on the left, just beneath your profile picture. In the submenu "),sa=i("em"),ki=r("API Tokens"),Ei=r(`, you will find your API
token that you can just copy.`),vr=u(),Z=i("h3"),ve=i("a"),ia=i("span"),h(at.$$.fragment),Ti=u(),na=i("span"),Pi=r("Directly push your model to the hub"),wr=u(),C=i("iframe"),gr=u(),we=i("p"),Ai=r(`Once you have an API token (either stored in the cache or copied and pasted in your notebook), you can directly push a
finetuned model you saved in `),fa=i("code"),ji=r("save_directory"),qi=r(" by calling:"),$r=u(),h(rt.$$.fragment),_r=u(),ge=i("p"),Fi=r("If you have your API token not stored in the cache, you will need to pass it with "),ua=i("code"),Ii=r("use_auth_token=your_token"),zi=r(`.
This is also be the case for all the examples below, so we won\u2019t mention it again.`),br=u(),$e=i("p"),Mi=r("This will create a repository in your namespace name "),pa=i("code"),Ci=r("my-awesome-model"),Si=r(", so anyone can now run:"),kr=u(),h(lt.$$.fragment),Er=u(),_e=i("p"),Di=r("Even better, you can combine this push to the hub with the call to "),ma=i("code"),xi=r("save_pretrained"),Oi=r(":"),Tr=u(),h(st.$$.fragment),Pr=u(),be=i("p"),Ni=r("If you are a premium user and want your model to be private, just add "),ha=i("code"),Yi=r("private=True"),Li=r(" to this call."),Ar=u(),ke=i("p"),Bi=r(`If you are a member of an organization and want to push it inside the namespace of the organization instead of yours,
just add `),ca=i("code"),Ui=r("organization=my_amazing_org"),Ri=r("."),jr=u(),Q=i("h3"),Ee=i("a"),da=i("span"),h(it.$$.fragment),Hi=u(),ya=i("span"),Wi=r("Add new files to your model repo"),qr=u(),ao=i("p"),Gi=r(`Once you have pushed your model to the hub, you might want to add the tokenizer, or a version of your model for another
framework (TensorFlow, PyTorch, Flax). This is super easy to do! Let\u2019s begin with the tokenizer. You can add it to the
repo you created before like this`),Fr=u(),h(nt.$$.fragment),Ir=u(),Te=i("p"),Ji=r("If you know its URL (it should be "),va=i("code"),Xi=r("https://huggingface.co/username/repo_name"),Zi=r("), you can also do:"),zr=u(),h(ft.$$.fragment),Mr=u(),ro=i("p"),Qi=r("And that\u2019s all there is to it! It\u2019s also a very easy way to fix a mistake if one of the files online had a bug."),Cr=u(),lo=i("p"),Ki=r(`To add a model for another backend, it\u2019s also super easy. Let\u2019s say you have fine-tuned a TensorFlow model and want to
add the pytorch model files to your model repo, so that anyone in the community can use it. The following allows you to
directly create a PyTorch version of your TensorFlow model:`),Sr=u(),h(ut.$$.fragment),Dr=u(),B=i("p"),Vi=r("You can also replace "),wa=i("code"),en=r("save_directory"),tn=r(" by the identifier of your model ("),ga=i("code"),on=r("username/repo_name"),an=r(`) if you don\u2019t
have a local save of it anymore. Then, just do the same as before:`),xr=u(),h(pt.$$.fragment),Or=u(),so=i("p"),rn=r("or"),Nr=u(),h(mt.$$.fragment),Yr=u(),K=i("h2"),Pe=i("a"),$a=i("span"),h(ht.$$.fragment),ln=u(),_a=i("span"),sn=r("Use your terminal and git"),Lr=u(),S=i("iframe"),Br=u(),V=i("h3"),Ae=i("a"),ba=i("span"),h(ct.$$.fragment),nn=u(),ka=i("span"),fn=r("Basic steps"),Ur=u(),io=i("p"),un=r(`In order to upload a model, you\u2019ll need to first create a git repo. This repo will live on the model hub, allowing
users to clone it and you (and your organization members) to push to it.`),Rr=u(),je=i("p"),pn=r("You can create a model repo directly from "),dt=i("a"),mn=r("the /new page on the website"),hn=r("."),Hr=u(),qe=i("p"),cn=r("Alternatively, you can use the "),Ea=i("code"),dn=r("transformers-cli"),yn=r(". The next steps describe that process:"),Wr=u(),Fe=i("p"),vn=r(`Go to a terminal and run the following command. It should be in the virtual environment where you installed \u{1F917}
Transformers, since that command `),Ta=i("code"),wn=r("transformers-cli"),gn=r(" comes from the library."),Gr=u(),h(yt.$$.fragment),Jr=u(),no=i("p"),$n=r("Once you are logged in with your model hub credentials, you can start building your repositories. To create a repo:"),Xr=u(),h(vt.$$.fragment),Zr=u(),Ie=i("p"),_n=r("If you want to create a repo under a specific organization, you should add a "),Pa=i("em"),bn=r("\u2014organization"),kn=r(" flag:"),Qr=u(),h(wt.$$.fragment),Kr=u(),fo=i("p"),En=r("This creates a repo on the model hub, which can be cloned."),Vr=u(),h(gt.$$.fragment),el=u(),uo=i("p"),Tn=r(`When you have your local clone of your repo and lfs installed, you can then add/remove from that clone as you would
with any other git repo.`),tl=u(),h($t.$$.fragment),ol=u(),po=i("p"),Pn=r(`We are intentionally not wrapping git too much, so that you can go on with the workflow you\u2019re used to and the tools
you already know.`),al=u(),ze=i("p"),An=r(`The only learning curve you might have compared to regular git is the one for git-lfs. The documentation at
`),_t=i("a"),jn=r("git-lfs.github.com"),qn=r(` is decent, but we\u2019ll work on a tutorial with some tips and tricks
in the coming weeks!`),rl=u(),Me=i("p"),Fn=r("Additionally, if you want to change multiple repos at once, the "),bt=i("a"),In=r("change_config.py script"),zn=r(" can probably save you some time."),ll=u(),ee=i("h3"),Ce=i("a"),Aa=i("span"),h(kt.$$.fragment),Mn=u(),ja=i("span"),Cn=r("Make your model work on all frameworks"),sl=u(),N=i("p"),Sn=r(`You probably have your favorite framework, but so will other users! That\u2019s why it\u2019s best to upload your model with both
PyTorch `),qa=i("em"),Dn=r("and"),xn=r(` TensorFlow checkpoints to make it easier to use (if you skip this step, users will still be able to load
your model in another framework, but it will be slower, as it will have to be converted on the fly). Don\u2019t worry, it\u2019s
super easy to do (and in a future version, it might all be automatic). You will need to install both PyTorch and
TensorFlow for this step, but you don\u2019t need to worry about the GPU, so it should be very easy. Check the `),Et=i("a"),On=r(`TensorFlow
installation page`),Nn=r(" and/or the "),Tt=i("a"),Yn=r(`PyTorch
installation page`),Ln=r(" to see how."),il=u(),Se=i("p"),Bn=r(`First check that your model class exists in the other framework, that is try to import the same model by either adding
or removing TF. For instance, if you trained a `),mo=i("a"),Un=r("DistilBertForSequenceClassification"),Rn=r(", try to type"),nl=u(),h(Pt.$$.fragment),fl=u(),De=i("p"),Hn=r("and if you trained a "),ho=i("a"),Wn=r("TFDistilBertForSequenceClassification"),Gn=r(", try to type"),ul=u(),h(At.$$.fragment),pl=u(),co=i("p"),Jn=r(`This will give back an error if your model does not exist in the other framework (something that should be pretty rare
since we\u2019re aiming for full parity between the two frameworks). In this case, skip this and go to the next step.`),ml=u(),yo=i("p"),Xn=r(`Now, if you trained your model in PyTorch and have to create a TensorFlow version, adapt the following code to your
model class:`),hl=u(),h(jt.$$.fragment),cl=u(),vo=i("p"),Zn=r(`and if you trained your model in TensorFlow and have to create a PyTorch version, adapt the following code to your
model class:`),dl=u(),h(qt.$$.fragment),yl=u(),wo=i("p"),Qn=r("That\u2019s all there is to it!"),vl=u(),te=i("h3"),xe=i("a"),Fa=i("span"),h(Ft.$$.fragment),Kn=u(),Ia=i("span"),Vn=r("Check the directory before pushing to the model hub."),wl=u(),go=i("p"),ef=r("Make sure there are no garbage files in the directory you\u2019ll upload. It should only have:"),gl=u(),E=i("ul"),oe=i("li"),tf=r("a "),za=i("em"),of=r("config.json"),af=r(" file, which saves the "),$o=i("a"),rf=r("configuration"),lf=r(" of your model ;"),sf=u(),It=i("li"),nf=r("a "),Ma=i("em"),ff=r("pytorch_model.bin"),uf=r(" file, which is the PyTorch checkpoint (unless you can\u2019t have it for some reason) ;"),pf=u(),zt=i("li"),mf=r("a "),Ca=i("em"),hf=r("tf_model.h5"),cf=r(" file, which is the TensorFlow checkpoint (unless you can\u2019t have it for some reason) ;"),df=u(),ae=i("li"),yf=r("a "),Sa=i("em"),vf=r("special_tokens_map.json"),wf=r(", which is part of your "),_o=i("a"),gf=r("tokenizer"),$f=r(" save;"),_f=u(),re=i("li"),bf=r("a "),Da=i("em"),kf=r("tokenizer_config.json"),Ef=r(", which is part of your "),bo=i("a"),Tf=r("tokenizer"),Pf=r(" save;"),Af=u(),D=i("li"),jf=r("files named "),xa=i("em"),qf=r("vocab.json"),Ff=r(", "),Oa=i("em"),If=r("vocab.txt"),zf=r(", "),Na=i("em"),Mf=r("merges.txt"),Cf=r(`, or similar, which contain the vocabulary of your tokenizer, part
of your `),ko=i("a"),Sf=r("tokenizer"),Df=r(" save;"),xf=u(),le=i("li"),Of=r("maybe a "),Ya=i("em"),Nf=r("added_tokens.json"),Yf=r(", which is part of your "),Eo=i("a"),Lf=r("tokenizer"),Bf=r(" save."),$l=u(),To=i("p"),Uf=r("Other files can safely be deleted."),_l=u(),se=i("h2"),Oe=i("a"),La=i("span"),h(Mt.$$.fragment),Rf=u(),Ba=i("span"),Hf=r("Uploading your files"),bl=u(),Po=i("p"),Wf=r(`Once the repo is cloned, you can add the model, configuration and tokenizer files. For instance, saving the model and
tokenizer files:`),kl=u(),h(Ct.$$.fragment),El=u(),Ao=i("p"),Gf=r("Or, if you\u2019re using the Trainer API"),Tl=u(),h(St.$$.fragment),Pl=u(),Ne=i("p"),Jf=r("You can then add these files to the staging environment and verify that they have been correctly staged with the "),Ua=i("code"),Xf=r("git status"),Zf=r(" command:"),Al=u(),h(Dt.$$.fragment),jl=u(),jo=i("p"),Qf=r("Finally, the files should be committed:"),ql=u(),h(xt.$$.fragment),Fl=u(),qo=i("p"),Kf=r("And pushed to the remote:"),Il=u(),h(Ot.$$.fragment),zl=u(),Fo=i("p"),Vf=r("This will upload the folder containing the weights, tokenizer and configuration we have just prepared."),Ml=u(),ie=i("h3"),Ye=i("a"),Ra=i("span"),h(Nt.$$.fragment),eu=u(),Ha=i("span"),tu=r("Add a model card"),Cl=u(),Le=i("p"),ou=r(`To make sure everyone knows what your model can do, what its limitations, potential bias or ethical considerations are,
please add a README.md model card to your model repo. You can just create it, or there\u2019s also a convenient button
titled \u201CAdd a README.md\u201D on your model page. A model card documentation can be found `),Yt=i("a"),au=r("here"),ru=r(` (meta-suggestions are welcome). model card template (meta-suggestions
are welcome).`),Sl=u(),h(Be.$$.fragment),Dl=u(),Io=i("p"),lu=r(`If your model is fine-tuned from another model coming from the model hub (all \u{1F917} Transformers pretrained models do),
don\u2019t forget to link to its model card so that people can fully trace how your model was built.`),xl=u(),ne=i("h3"),Ue=i("a"),Wa=i("span"),h(Lt.$$.fragment),su=u(),Ga=i("span"),iu=r("Using your model"),Ol=u(),zo=i("p"),nu=r("Your model now has a page on huggingface.co/models \u{1F525}"),Nl=u(),Mo=i("p"),fu=r("Anyone can load it from code:"),Yl=u(),h(Bt.$$.fragment),Ll=u(),U=i("p"),uu=r("You may specify a revision by using the "),Ja=i("code"),pu=r("revision"),mu=r(" flag in the "),Xa=i("code"),hu=r("from_pretrained"),cu=r(" method:"),Bl=u(),h(Ut.$$.fragment),Ul=u(),fe=i("h2"),Re=i("a"),Za=i("span"),h(Rt.$$.fragment),du=u(),Qa=i("span"),yu=r("Workflow in a Colab notebook"),Rl=u(),Co=i("p"),vu=r(`If you\u2019re in a Colab notebook (or similar) with no direct access to a terminal, here is the workflow you can use to
upload your model. You can execute each one of them in a cell by adding a ! at the beginning.`),Hl=u(),He=i("p"),wu=r("First you need to install "),Ka=i("em"),gu=r("git-lfs"),$u=r(" in the environment used by the notebook:"),Wl=u(),h(Ht.$$.fragment),Gl=u(),R=i("p"),_u=r("Then you can use either create a repo directly from "),Wt=i("a"),bu=r("huggingface.co"),ku=r(` , or use the
`),Va=i("code"),Eu=r("transformers-cli"),Tu=r(" to create it:"),Jl=u(),h(Gt.$$.fragment),Xl=u(),So=i("p"),Pu=r("Once it\u2019s created, you can clone it and configure it (replace username by your username on huggingface.co):"),Zl=u(),h(Jt.$$.fragment),Ql=u(),Do=i("p"),Au=r(`Once you\u2019ve saved your model inside, and your clone is setup with the right remote URL, you can add it and push it with
usual git commands.`),Kl=u(),h(Xt.$$.fragment),this.h()},l(e){const a=eh('[data-svelte="svelte-1phssyn"]',document.head);$=n(a,"META",{name:!0,content:!0}),a.forEach(t),A=p(e),_=n(e,"H1",{class:!0});var Zt=f(_);b=n(Zt,"A",{id:!0,class:!0,href:!0});var er=f(b);I=n(er,"SPAN",{});var Mu=f(I);c(k.$$.fragment,Mu),Mu.forEach(t),er.forEach(t),T=p(Zt),x=n(Zt,"SPAN",{});var Cu=f(x);P=l(Cu,"Model sharing and uploading"),Cu.forEach(t),Zt.forEach(t),z=p(e),Y=n(e,"P",{});var es=f(Y);Qt=l(es,`In this page, we will show you how to share a model you have trained or fine-tuned on new data with the community on
the `),Xe=n(es,"A",{href:!0,rel:!0});var Su=f(Xe);xs=l(Su,"model hub"),Su.forEach(t),Os=l(es,"."),es.forEach(t),tr=p(e),M=n(e,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),f(M).forEach(t),or=p(e),c(ue.$$.fragment,e),ar=p(e),j=n(e,"P",{});var H=f(j);Ns=l(H,"We have seen in the "),Kt=n(H,"A",{href:!0});var Du=f(Kt);Ys=l(Du,"training tutorial"),Du.forEach(t),Ls=l(H,`: how to fine-tune a model on a given task. You have probably
done something similar on your task, either using the model directly in your own training loop or using the
`),Ho=n(H,"CODE",{});var xu=f(Ho);Bs=l(xu,"Trainer"),xu.forEach(t),Us=l(H,"/"),Wo=n(H,"CODE",{});var Ou=f(Wo);Rs=l(Ou,"TFTrainer"),Ou.forEach(t),Hs=l(H,` class. Let\u2019s see how you can share the result on the
`),Ze=n(H,"A",{href:!0,rel:!0});var Nu=f(Ze);Ws=l(Nu,"model hub"),Nu.forEach(t),Gs=l(H,"."),H.forEach(t),rr=p(e),G=n(e,"H2",{class:!0});var ts=f(G);pe=n(ts,"A",{id:!0,class:!0,href:!0});var Yu=f(pe);Go=n(Yu,"SPAN",{});var Lu=f(Go);c(Qe.$$.fragment,Lu),Lu.forEach(t),Yu.forEach(t),Js=p(ts),Jo=n(ts,"SPAN",{});var Bu=f(Jo);Xs=l(Bu,"Model versioning"),Bu.forEach(t),ts.forEach(t),lr=p(e),me=n(e,"P",{});var os=f(me);Zs=l(os,`Since version v3.5.0, the model hub has built-in model versioning based on git and git-lfs. It is based on the paradigm
that one model `),Xo=n(os,"EM",{});var Uu=f(Xo);Qs=l(Uu,"is"),Uu.forEach(t),Ks=l(os," one repo."),os.forEach(t),sr=p(e),Vt=n(e,"P",{});var Ru=f(Vt);Vs=l(Ru,"This allows:"),Ru.forEach(t),ir=p(e),L=n(e,"UL",{});var xo=f(L);Zo=n(xo,"LI",{});var Hu=f(Zo);ei=l(Hu,"built-in versioning"),Hu.forEach(t),ti=p(xo),Qo=n(xo,"LI",{});var Wu=f(Qo);oi=l(Wu,"access control"),Wu.forEach(t),ai=p(xo),Ko=n(xo,"LI",{});var Gu=f(Ko);ri=l(Gu,"scalability"),Gu.forEach(t),xo.forEach(t),nr=p(e),he=n(e,"P",{});var as=f(he);li=l(as,"This is built around "),Vo=n(as,"EM",{});var Ju=f(Vo);si=l(Ju,"revisions"),Ju.forEach(t),ii=l(as,`, which is a way to pin a specific version of a model, using a commit hash, tag or
branch.`),as.forEach(t),fr=p(e),eo=n(e,"P",{});var Xu=f(eo);ni=l(Xu,"For instance:"),Xu.forEach(t),ur=p(e),c(Ke.$$.fragment,e),pr=p(e),J=n(e,"H2",{class:!0});var rs=f(J);ce=n(rs,"A",{id:!0,class:!0,href:!0});var Zu=f(ce);ea=n(Zu,"SPAN",{});var Qu=f(ea);c(Ve.$$.fragment,Qu),Qu.forEach(t),Zu.forEach(t),fi=p(rs),ta=n(rs,"SPAN",{});var Ku=f(ta);ui=l(Ku,"Push your model from Python"),Ku.forEach(t),rs.forEach(t),mr=p(e),X=n(e,"H3",{class:!0});var ls=f(X);de=n(ls,"A",{id:!0,class:!0,href:!0});var Vu=f(de);oa=n(Vu,"SPAN",{});var ep=f(oa);c(et.$$.fragment,ep),ep.forEach(t),Vu.forEach(t),pi=p(ls),aa=n(ls,"SPAN",{});var tp=f(aa);mi=l(tp,"Preparation"),tp.forEach(t),ls.forEach(t),hr=p(e),to=n(e,"P",{});var op=f(to);hi=l(op,`The first step is to make sure your credentials to the hub are stored somewhere. This can be done in two ways. If you
have access to a terminal, you cam just run the following command in the virtual environment where you installed \u{1F917}
Transformers:`),op.forEach(t),cr=p(e),c(tt.$$.fragment,e),dr=p(e),ye=n(e,"P",{});var ss=f(ye);ci=l(ss,"It will store your access token in the Hugging Face cache folder (by default "),ra=n(ss,"CODE",{});var ap=f(ra);di=l(ap,"~/.cache/"),ap.forEach(t),yi=l(ss,")."),ss.forEach(t),yr=p(e),O=n(e,"P",{});var We=f(O);vi=l(We,`If you don\u2019t have an easy access to a terminal (for instance in a Colab session), you can find a token linked to your
account by going on `),oo=n(We,"EM",{});var ju=f(oo);wi=l(ju,"huggingface.co <"),ot=n(ju,"A",{href:!0,rel:!0});var rp=f(ot);gi=l(rp,"https://huggingface.co/>"),rp.forEach(t),ju.forEach(t),$i=l(We,`, click on your avatar on the top left corner, then on
`),la=n(We,"EM",{});var lp=f(la);_i=l(lp,"Edit profile"),lp.forEach(t),bi=l(We," on the left, just beneath your profile picture. In the submenu "),sa=n(We,"EM",{});var sp=f(sa);ki=l(sp,"API Tokens"),sp.forEach(t),Ei=l(We,`, you will find your API
token that you can just copy.`),We.forEach(t),vr=p(e),Z=n(e,"H3",{class:!0});var is=f(Z);ve=n(is,"A",{id:!0,class:!0,href:!0});var ip=f(ve);ia=n(ip,"SPAN",{});var np=f(ia);c(at.$$.fragment,np),np.forEach(t),ip.forEach(t),Ti=p(is),na=n(is,"SPAN",{});var fp=f(na);Pi=l(fp,"Directly push your model to the hub"),fp.forEach(t),is.forEach(t),wr=p(e),C=n(e,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),f(C).forEach(t),gr=p(e),we=n(e,"P",{});var ns=f(we);Ai=l(ns,`Once you have an API token (either stored in the cache or copied and pasted in your notebook), you can directly push a
finetuned model you saved in `),fa=n(ns,"CODE",{});var up=f(fa);ji=l(up,"save_directory"),up.forEach(t),qi=l(ns," by calling:"),ns.forEach(t),$r=p(e),c(rt.$$.fragment,e),_r=p(e),ge=n(e,"P",{});var fs=f(ge);Fi=l(fs,"If you have your API token not stored in the cache, you will need to pass it with "),ua=n(fs,"CODE",{});var pp=f(ua);Ii=l(pp,"use_auth_token=your_token"),pp.forEach(t),zi=l(fs,`.
This is also be the case for all the examples below, so we won\u2019t mention it again.`),fs.forEach(t),br=p(e),$e=n(e,"P",{});var us=f($e);Mi=l(us,"This will create a repository in your namespace name "),pa=n(us,"CODE",{});var mp=f(pa);Ci=l(mp,"my-awesome-model"),mp.forEach(t),Si=l(us,", so anyone can now run:"),us.forEach(t),kr=p(e),c(lt.$$.fragment,e),Er=p(e),_e=n(e,"P",{});var ps=f(_e);Di=l(ps,"Even better, you can combine this push to the hub with the call to "),ma=n(ps,"CODE",{});var hp=f(ma);xi=l(hp,"save_pretrained"),hp.forEach(t),Oi=l(ps,":"),ps.forEach(t),Tr=p(e),c(st.$$.fragment,e),Pr=p(e),be=n(e,"P",{});var ms=f(be);Ni=l(ms,"If you are a premium user and want your model to be private, just add "),ha=n(ms,"CODE",{});var cp=f(ha);Yi=l(cp,"private=True"),cp.forEach(t),Li=l(ms," to this call."),ms.forEach(t),Ar=p(e),ke=n(e,"P",{});var hs=f(ke);Bi=l(hs,`If you are a member of an organization and want to push it inside the namespace of the organization instead of yours,
just add `),ca=n(hs,"CODE",{});var dp=f(ca);Ui=l(dp,"organization=my_amazing_org"),dp.forEach(t),Ri=l(hs,"."),hs.forEach(t),jr=p(e),Q=n(e,"H3",{class:!0});var cs=f(Q);Ee=n(cs,"A",{id:!0,class:!0,href:!0});var yp=f(Ee);da=n(yp,"SPAN",{});var vp=f(da);c(it.$$.fragment,vp),vp.forEach(t),yp.forEach(t),Hi=p(cs),ya=n(cs,"SPAN",{});var wp=f(ya);Wi=l(wp,"Add new files to your model repo"),wp.forEach(t),cs.forEach(t),qr=p(e),ao=n(e,"P",{});var gp=f(ao);Gi=l(gp,`Once you have pushed your model to the hub, you might want to add the tokenizer, or a version of your model for another
framework (TensorFlow, PyTorch, Flax). This is super easy to do! Let\u2019s begin with the tokenizer. You can add it to the
repo you created before like this`),gp.forEach(t),Fr=p(e),c(nt.$$.fragment,e),Ir=p(e),Te=n(e,"P",{});var ds=f(Te);Ji=l(ds,"If you know its URL (it should be "),va=n(ds,"CODE",{});var $p=f(va);Xi=l($p,"https://huggingface.co/username/repo_name"),$p.forEach(t),Zi=l(ds,"), you can also do:"),ds.forEach(t),zr=p(e),c(ft.$$.fragment,e),Mr=p(e),ro=n(e,"P",{});var _p=f(ro);Qi=l(_p,"And that\u2019s all there is to it! It\u2019s also a very easy way to fix a mistake if one of the files online had a bug."),_p.forEach(t),Cr=p(e),lo=n(e,"P",{});var bp=f(lo);Ki=l(bp,`To add a model for another backend, it\u2019s also super easy. Let\u2019s say you have fine-tuned a TensorFlow model and want to
add the pytorch model files to your model repo, so that anyone in the community can use it. The following allows you to
directly create a PyTorch version of your TensorFlow model:`),bp.forEach(t),Sr=p(e),c(ut.$$.fragment,e),Dr=p(e),B=n(e,"P",{});var Oo=f(B);Vi=l(Oo,"You can also replace "),wa=n(Oo,"CODE",{});var kp=f(wa);en=l(kp,"save_directory"),kp.forEach(t),tn=l(Oo," by the identifier of your model ("),ga=n(Oo,"CODE",{});var Ep=f(ga);on=l(Ep,"username/repo_name"),Ep.forEach(t),an=l(Oo,`) if you don\u2019t
have a local save of it anymore. Then, just do the same as before:`),Oo.forEach(t),xr=p(e),c(pt.$$.fragment,e),Or=p(e),so=n(e,"P",{});var Tp=f(so);rn=l(Tp,"or"),Tp.forEach(t),Nr=p(e),c(mt.$$.fragment,e),Yr=p(e),K=n(e,"H2",{class:!0});var ys=f(K);Pe=n(ys,"A",{id:!0,class:!0,href:!0});var Pp=f(Pe);$a=n(Pp,"SPAN",{});var Ap=f($a);c(ht.$$.fragment,Ap),Ap.forEach(t),Pp.forEach(t),ln=p(ys),_a=n(ys,"SPAN",{});var jp=f(_a);sn=l(jp,"Use your terminal and git"),jp.forEach(t),ys.forEach(t),Lr=p(e),S=n(e,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),f(S).forEach(t),Br=p(e),V=n(e,"H3",{class:!0});var vs=f(V);Ae=n(vs,"A",{id:!0,class:!0,href:!0});var qp=f(Ae);ba=n(qp,"SPAN",{});var Fp=f(ba);c(ct.$$.fragment,Fp),Fp.forEach(t),qp.forEach(t),nn=p(vs),ka=n(vs,"SPAN",{});var Ip=f(ka);fn=l(Ip,"Basic steps"),Ip.forEach(t),vs.forEach(t),Ur=p(e),io=n(e,"P",{});var zp=f(io);un=l(zp,`In order to upload a model, you\u2019ll need to first create a git repo. This repo will live on the model hub, allowing
users to clone it and you (and your organization members) to push to it.`),zp.forEach(t),Rr=p(e),je=n(e,"P",{});var ws=f(je);pn=l(ws,"You can create a model repo directly from "),dt=n(ws,"A",{href:!0,rel:!0});var Mp=f(dt);mn=l(Mp,"the /new page on the website"),Mp.forEach(t),hn=l(ws,"."),ws.forEach(t),Hr=p(e),qe=n(e,"P",{});var gs=f(qe);cn=l(gs,"Alternatively, you can use the "),Ea=n(gs,"CODE",{});var Cp=f(Ea);dn=l(Cp,"transformers-cli"),Cp.forEach(t),yn=l(gs,". The next steps describe that process:"),gs.forEach(t),Wr=p(e),Fe=n(e,"P",{});var $s=f(Fe);vn=l($s,`Go to a terminal and run the following command. It should be in the virtual environment where you installed \u{1F917}
Transformers, since that command `),Ta=n($s,"CODE",{});var Sp=f(Ta);wn=l(Sp,"transformers-cli"),Sp.forEach(t),gn=l($s," comes from the library."),$s.forEach(t),Gr=p(e),c(yt.$$.fragment,e),Jr=p(e),no=n(e,"P",{});var Dp=f(no);$n=l(Dp,"Once you are logged in with your model hub credentials, you can start building your repositories. To create a repo:"),Dp.forEach(t),Xr=p(e),c(vt.$$.fragment,e),Zr=p(e),Ie=n(e,"P",{});var _s=f(Ie);_n=l(_s,"If you want to create a repo under a specific organization, you should add a "),Pa=n(_s,"EM",{});var xp=f(Pa);bn=l(xp,"\u2014organization"),xp.forEach(t),kn=l(_s," flag:"),_s.forEach(t),Qr=p(e),c(wt.$$.fragment,e),Kr=p(e),fo=n(e,"P",{});var Op=f(fo);En=l(Op,"This creates a repo on the model hub, which can be cloned."),Op.forEach(t),Vr=p(e),c(gt.$$.fragment,e),el=p(e),uo=n(e,"P",{});var Np=f(uo);Tn=l(Np,`When you have your local clone of your repo and lfs installed, you can then add/remove from that clone as you would
with any other git repo.`),Np.forEach(t),tl=p(e),c($t.$$.fragment,e),ol=p(e),po=n(e,"P",{});var Yp=f(po);Pn=l(Yp,`We are intentionally not wrapping git too much, so that you can go on with the workflow you\u2019re used to and the tools
you already know.`),Yp.forEach(t),al=p(e),ze=n(e,"P",{});var bs=f(ze);An=l(bs,`The only learning curve you might have compared to regular git is the one for git-lfs. The documentation at
`),_t=n(bs,"A",{href:!0,rel:!0});var Lp=f(_t);jn=l(Lp,"git-lfs.github.com"),Lp.forEach(t),qn=l(bs,` is decent, but we\u2019ll work on a tutorial with some tips and tricks
in the coming weeks!`),bs.forEach(t),rl=p(e),Me=n(e,"P",{});var ks=f(Me);Fn=l(ks,"Additionally, if you want to change multiple repos at once, the "),bt=n(ks,"A",{href:!0,rel:!0});var Bp=f(bt);In=l(Bp,"change_config.py script"),Bp.forEach(t),zn=l(ks," can probably save you some time."),ks.forEach(t),ll=p(e),ee=n(e,"H3",{class:!0});var Es=f(ee);Ce=n(Es,"A",{id:!0,class:!0,href:!0});var Up=f(Ce);Aa=n(Up,"SPAN",{});var Rp=f(Aa);c(kt.$$.fragment,Rp),Rp.forEach(t),Up.forEach(t),Mn=p(Es),ja=n(Es,"SPAN",{});var Hp=f(ja);Cn=l(Hp,"Make your model work on all frameworks"),Hp.forEach(t),Es.forEach(t),sl=p(e),N=n(e,"P",{});var Ge=f(N);Sn=l(Ge,`You probably have your favorite framework, but so will other users! That\u2019s why it\u2019s best to upload your model with both
PyTorch `),qa=n(Ge,"EM",{});var Wp=f(qa);Dn=l(Wp,"and"),Wp.forEach(t),xn=l(Ge,` TensorFlow checkpoints to make it easier to use (if you skip this step, users will still be able to load
your model in another framework, but it will be slower, as it will have to be converted on the fly). Don\u2019t worry, it\u2019s
super easy to do (and in a future version, it might all be automatic). You will need to install both PyTorch and
TensorFlow for this step, but you don\u2019t need to worry about the GPU, so it should be very easy. Check the `),Et=n(Ge,"A",{href:!0,rel:!0});var Gp=f(Et);On=l(Gp,`TensorFlow
installation page`),Gp.forEach(t),Nn=l(Ge," and/or the "),Tt=n(Ge,"A",{href:!0,rel:!0});var Jp=f(Tt);Yn=l(Jp,`PyTorch
installation page`),Jp.forEach(t),Ln=l(Ge," to see how."),Ge.forEach(t),il=p(e),Se=n(e,"P",{});var Ts=f(Se);Bn=l(Ts,`First check that your model class exists in the other framework, that is try to import the same model by either adding
or removing TF. For instance, if you trained a `),mo=n(Ts,"A",{href:!0});var Xp=f(mo);Un=l(Xp,"DistilBertForSequenceClassification"),Xp.forEach(t),Rn=l(Ts,", try to type"),Ts.forEach(t),nl=p(e),c(Pt.$$.fragment,e),fl=p(e),De=n(e,"P",{});var Ps=f(De);Hn=l(Ps,"and if you trained a "),ho=n(Ps,"A",{href:!0});var Zp=f(ho);Wn=l(Zp,"TFDistilBertForSequenceClassification"),Zp.forEach(t),Gn=l(Ps,", try to type"),Ps.forEach(t),ul=p(e),c(At.$$.fragment,e),pl=p(e),co=n(e,"P",{});var Qp=f(co);Jn=l(Qp,`This will give back an error if your model does not exist in the other framework (something that should be pretty rare
since we\u2019re aiming for full parity between the two frameworks). In this case, skip this and go to the next step.`),Qp.forEach(t),ml=p(e),yo=n(e,"P",{});var Kp=f(yo);Xn=l(Kp,`Now, if you trained your model in PyTorch and have to create a TensorFlow version, adapt the following code to your
model class:`),Kp.forEach(t),hl=p(e),c(jt.$$.fragment,e),cl=p(e),vo=n(e,"P",{});var Vp=f(vo);Zn=l(Vp,`and if you trained your model in TensorFlow and have to create a PyTorch version, adapt the following code to your
model class:`),Vp.forEach(t),dl=p(e),c(qt.$$.fragment,e),yl=p(e),wo=n(e,"P",{});var em=f(wo);Qn=l(em,"That\u2019s all there is to it!"),em.forEach(t),vl=p(e),te=n(e,"H3",{class:!0});var As=f(te);xe=n(As,"A",{id:!0,class:!0,href:!0});var tm=f(xe);Fa=n(tm,"SPAN",{});var om=f(Fa);c(Ft.$$.fragment,om),om.forEach(t),tm.forEach(t),Kn=p(As),Ia=n(As,"SPAN",{});var am=f(Ia);Vn=l(am,"Check the directory before pushing to the model hub."),am.forEach(t),As.forEach(t),wl=p(e),go=n(e,"P",{});var rm=f(go);ef=l(rm,"Make sure there are no garbage files in the directory you\u2019ll upload. It should only have:"),rm.forEach(t),gl=p(e),E=n(e,"UL",{});var q=f(E);oe=n(q,"LI",{});var No=f(oe);tf=l(No,"a "),za=n(No,"EM",{});var lm=f(za);of=l(lm,"config.json"),lm.forEach(t),af=l(No," file, which saves the "),$o=n(No,"A",{href:!0});var sm=f($o);rf=l(sm,"configuration"),sm.forEach(t),lf=l(No," of your model ;"),No.forEach(t),sf=p(q),It=n(q,"LI",{});var js=f(It);nf=l(js,"a "),Ma=n(js,"EM",{});var im=f(Ma);ff=l(im,"pytorch_model.bin"),im.forEach(t),uf=l(js," file, which is the PyTorch checkpoint (unless you can\u2019t have it for some reason) ;"),js.forEach(t),pf=p(q),zt=n(q,"LI",{});var qs=f(zt);mf=l(qs,"a "),Ca=n(qs,"EM",{});var nm=f(Ca);hf=l(nm,"tf_model.h5"),nm.forEach(t),cf=l(qs," file, which is the TensorFlow checkpoint (unless you can\u2019t have it for some reason) ;"),qs.forEach(t),df=p(q),ae=n(q,"LI",{});var Yo=f(ae);yf=l(Yo,"a "),Sa=n(Yo,"EM",{});var fm=f(Sa);vf=l(fm,"special_tokens_map.json"),fm.forEach(t),wf=l(Yo,", which is part of your "),_o=n(Yo,"A",{href:!0});var um=f(_o);gf=l(um,"tokenizer"),um.forEach(t),$f=l(Yo," save;"),Yo.forEach(t),_f=p(q),re=n(q,"LI",{});var Lo=f(re);bf=l(Lo,"a "),Da=n(Lo,"EM",{});var pm=f(Da);kf=l(pm,"tokenizer_config.json"),pm.forEach(t),Ef=l(Lo,", which is part of your "),bo=n(Lo,"A",{href:!0});var mm=f(bo);Tf=l(mm,"tokenizer"),mm.forEach(t),Pf=l(Lo," save;"),Lo.forEach(t),Af=p(q),D=n(q,"LI",{});var W=f(D);jf=l(W,"files named "),xa=n(W,"EM",{});var hm=f(xa);qf=l(hm,"vocab.json"),hm.forEach(t),Ff=l(W,", "),Oa=n(W,"EM",{});var cm=f(Oa);If=l(cm,"vocab.txt"),cm.forEach(t),zf=l(W,", "),Na=n(W,"EM",{});var dm=f(Na);Mf=l(dm,"merges.txt"),dm.forEach(t),Cf=l(W,`, or similar, which contain the vocabulary of your tokenizer, part
of your `),ko=n(W,"A",{href:!0});var ym=f(ko);Sf=l(ym,"tokenizer"),ym.forEach(t),Df=l(W," save;"),W.forEach(t),xf=p(q),le=n(q,"LI",{});var Bo=f(le);Of=l(Bo,"maybe a "),Ya=n(Bo,"EM",{});var vm=f(Ya);Nf=l(vm,"added_tokens.json"),vm.forEach(t),Yf=l(Bo,", which is part of your "),Eo=n(Bo,"A",{href:!0});var wm=f(Eo);Lf=l(wm,"tokenizer"),wm.forEach(t),Bf=l(Bo," save."),Bo.forEach(t),q.forEach(t),$l=p(e),To=n(e,"P",{});var gm=f(To);Uf=l(gm,"Other files can safely be deleted."),gm.forEach(t),_l=p(e),se=n(e,"H2",{class:!0});var Fs=f(se);Oe=n(Fs,"A",{id:!0,class:!0,href:!0});var $m=f(Oe);La=n($m,"SPAN",{});var _m=f(La);c(Mt.$$.fragment,_m),_m.forEach(t),$m.forEach(t),Rf=p(Fs),Ba=n(Fs,"SPAN",{});var bm=f(Ba);Hf=l(bm,"Uploading your files"),bm.forEach(t),Fs.forEach(t),bl=p(e),Po=n(e,"P",{});var km=f(Po);Wf=l(km,`Once the repo is cloned, you can add the model, configuration and tokenizer files. For instance, saving the model and
tokenizer files:`),km.forEach(t),kl=p(e),c(Ct.$$.fragment,e),El=p(e),Ao=n(e,"P",{});var Em=f(Ao);Gf=l(Em,"Or, if you\u2019re using the Trainer API"),Em.forEach(t),Tl=p(e),c(St.$$.fragment,e),Pl=p(e),Ne=n(e,"P",{});var Is=f(Ne);Jf=l(Is,"You can then add these files to the staging environment and verify that they have been correctly staged with the "),Ua=n(Is,"CODE",{});var Tm=f(Ua);Xf=l(Tm,"git status"),Tm.forEach(t),Zf=l(Is," command:"),Is.forEach(t),Al=p(e),c(Dt.$$.fragment,e),jl=p(e),jo=n(e,"P",{});var Pm=f(jo);Qf=l(Pm,"Finally, the files should be committed:"),Pm.forEach(t),ql=p(e),c(xt.$$.fragment,e),Fl=p(e),qo=n(e,"P",{});var Am=f(qo);Kf=l(Am,"And pushed to the remote:"),Am.forEach(t),Il=p(e),c(Ot.$$.fragment,e),zl=p(e),Fo=n(e,"P",{});var jm=f(Fo);Vf=l(jm,"This will upload the folder containing the weights, tokenizer and configuration we have just prepared."),jm.forEach(t),Ml=p(e),ie=n(e,"H3",{class:!0});var zs=f(ie);Ye=n(zs,"A",{id:!0,class:!0,href:!0});var qm=f(Ye);Ra=n(qm,"SPAN",{});var Fm=f(Ra);c(Nt.$$.fragment,Fm),Fm.forEach(t),qm.forEach(t),eu=p(zs),Ha=n(zs,"SPAN",{});var Im=f(Ha);tu=l(Im,"Add a model card"),Im.forEach(t),zs.forEach(t),Cl=p(e),Le=n(e,"P",{});var Ms=f(Le);ou=l(Ms,`To make sure everyone knows what your model can do, what its limitations, potential bias or ethical considerations are,
please add a README.md model card to your model repo. You can just create it, or there\u2019s also a convenient button
titled \u201CAdd a README.md\u201D on your model page. A model card documentation can be found `),Yt=n(Ms,"A",{href:!0,rel:!0});var zm=f(Yt);au=l(zm,"here"),zm.forEach(t),ru=l(Ms,` (meta-suggestions are welcome). model card template (meta-suggestions
are welcome).`),Ms.forEach(t),Sl=p(e),c(Be.$$.fragment,e),Dl=p(e),Io=n(e,"P",{});var Mm=f(Io);lu=l(Mm,`If your model is fine-tuned from another model coming from the model hub (all \u{1F917} Transformers pretrained models do),
don\u2019t forget to link to its model card so that people can fully trace how your model was built.`),Mm.forEach(t),xl=p(e),ne=n(e,"H3",{class:!0});var Cs=f(ne);Ue=n(Cs,"A",{id:!0,class:!0,href:!0});var Cm=f(Ue);Wa=n(Cm,"SPAN",{});var Sm=f(Wa);c(Lt.$$.fragment,Sm),Sm.forEach(t),Cm.forEach(t),su=p(Cs),Ga=n(Cs,"SPAN",{});var Dm=f(Ga);iu=l(Dm,"Using your model"),Dm.forEach(t),Cs.forEach(t),Ol=p(e),zo=n(e,"P",{});var xm=f(zo);nu=l(xm,"Your model now has a page on huggingface.co/models \u{1F525}"),xm.forEach(t),Nl=p(e),Mo=n(e,"P",{});var Om=f(Mo);fu=l(Om,"Anyone can load it from code:"),Om.forEach(t),Yl=p(e),c(Bt.$$.fragment,e),Ll=p(e),U=n(e,"P",{});var Uo=f(U);uu=l(Uo,"You may specify a revision by using the "),Ja=n(Uo,"CODE",{});var Nm=f(Ja);pu=l(Nm,"revision"),Nm.forEach(t),mu=l(Uo," flag in the "),Xa=n(Uo,"CODE",{});var Ym=f(Xa);hu=l(Ym,"from_pretrained"),Ym.forEach(t),cu=l(Uo," method:"),Uo.forEach(t),Bl=p(e),c(Ut.$$.fragment,e),Ul=p(e),fe=n(e,"H2",{class:!0});var Ss=f(fe);Re=n(Ss,"A",{id:!0,class:!0,href:!0});var Lm=f(Re);Za=n(Lm,"SPAN",{});var Bm=f(Za);c(Rt.$$.fragment,Bm),Bm.forEach(t),Lm.forEach(t),du=p(Ss),Qa=n(Ss,"SPAN",{});var Um=f(Qa);yu=l(Um,"Workflow in a Colab notebook"),Um.forEach(t),Ss.forEach(t),Rl=p(e),Co=n(e,"P",{});var Rm=f(Co);vu=l(Rm,`If you\u2019re in a Colab notebook (or similar) with no direct access to a terminal, here is the workflow you can use to
upload your model. You can execute each one of them in a cell by adding a ! at the beginning.`),Rm.forEach(t),Hl=p(e),He=n(e,"P",{});var Ds=f(He);wu=l(Ds,"First you need to install "),Ka=n(Ds,"EM",{});var Hm=f(Ka);gu=l(Hm,"git-lfs"),Hm.forEach(t),$u=l(Ds," in the environment used by the notebook:"),Ds.forEach(t),Wl=p(e),c(Ht.$$.fragment,e),Gl=p(e),R=n(e,"P",{});var Ro=f(R);_u=l(Ro,"Then you can use either create a repo directly from "),Wt=n(Ro,"A",{href:!0,rel:!0});var Wm=f(Wt);bu=l(Wm,"huggingface.co"),Wm.forEach(t),ku=l(Ro,` , or use the
`),Va=n(Ro,"CODE",{});var Gm=f(Va);Eu=l(Gm,"transformers-cli"),Gm.forEach(t),Tu=l(Ro," to create it:"),Ro.forEach(t),Jl=p(e),c(Gt.$$.fragment,e),Xl=p(e),So=n(e,"P",{});var Jm=f(So);Pu=l(Jm,"Once it\u2019s created, you can clone it and configure it (replace username by your username on huggingface.co):"),Jm.forEach(t),Zl=p(e),c(Jt.$$.fragment,e),Ql=p(e),Do=n(e,"P",{});var Xm=f(Do);Au=l(Xm,`Once you\u2019ve saved your model inside, and your clone is setup with the right remote URL, you can add it and push it with
usual git commands.`),Xm.forEach(t),Kl=p(e),c(Xt.$$.fragment,e),this.h()},h(){m($,"name","hf:doc:metadata"),m($,"content",JSON.stringify(rh)),m(b,"id","model-sharing-and-uploading"),m(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(b,"href","#model-sharing-and-uploading"),m(_,"class","relative group"),m(Xe,"href","https://huggingface.co/models"),m(Xe,"rel","nofollow"),m(M,"width","560"),m(M,"height","315"),qu(M.src,Fu="https://www.youtube.com/embed/XvSGPZFEjDY")||m(M,"src",Fu),m(M,"title","YouTube video player"),m(M,"frameborder","0"),m(M,"allow",`accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;
picture-in-picture`),M.allowFullscreen=!0,m(Kt,"href","/docs/transformers/v4.14.1/en/training"),m(Ze,"href","https://huggingface.co/models"),m(Ze,"rel","nofollow"),m(pe,"id","model-versioning"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#model-versioning"),m(G,"class","relative group"),m(ce,"id","push-your-model-from-python"),m(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ce,"href","#push-your-model-from-python"),m(J,"class","relative group"),m(de,"id","preparation"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#preparation"),m(X,"class","relative group"),m(ot,"href","https://huggingface.co/%3E"),m(ot,"rel","nofollow"),m(ve,"id","directly-push-your-model-to-the-hub"),m(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ve,"href","#directly-push-your-model-to-the-hub"),m(Z,"class","relative group"),m(C,"width","560"),m(C,"height","315"),qu(C.src,Iu="https://www.youtube.com/embed/Z1-XMy-GNLQ")||m(C,"src",Iu),m(C,"title","YouTube video player"),m(C,"frameborder","0"),m(C,"allow",`accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;
picture-in-picture`),C.allowFullscreen=!0,m(Ee,"id","add-new-files-to-your-model-repo"),m(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ee,"href","#add-new-files-to-your-model-repo"),m(Q,"class","relative group"),m(Pe,"id","use-your-terminal-and-git"),m(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Pe,"href","#use-your-terminal-and-git"),m(K,"class","relative group"),m(S,"width","560"),m(S,"height","315"),qu(S.src,zu="https://www.youtube.com/embed/rkCly_cbMBk")||m(S,"src",zu),m(S,"title","YouTube video player"),m(S,"frameborder","0"),m(S,"allow",`accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope;
picture-in-picture`),S.allowFullscreen=!0,m(Ae,"id","basic-steps"),m(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ae,"href","#basic-steps"),m(V,"class","relative group"),m(dt,"href","https://huggingface.co/new"),m(dt,"rel","nofollow"),m(_t,"href","https://git-lfs.github.com/"),m(_t,"rel","nofollow"),m(bt,"href","https://github.com/huggingface/efficient_scripts/blob/main/change_config.py"),m(bt,"rel","nofollow"),m(Ce,"id","make-your-model-work-on-all-frameworks"),m(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ce,"href","#make-your-model-work-on-all-frameworks"),m(ee,"class","relative group"),m(Et,"href","https://www.tensorflow.org/install/pip#tensorflow-2.0-rc-is-available"),m(Et,"rel","nofollow"),m(Tt,"href","https://pytorch.org/get-started/locally/#start-locally"),m(Tt,"rel","nofollow"),m(mo,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),m(ho,"href","/docs/transformers/v4.14.1/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),m(xe,"id","check-the-directory-before-pushing-to-the-model-hub"),m(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(xe,"href","#check-the-directory-before-pushing-to-the-model-hub"),m(te,"class","relative group"),m($o,"href","/docs/transformers/v4.14.1/en/main_classes/configuration"),m(_o,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer"),m(bo,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer"),m(ko,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer"),m(Eo,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer"),m(Oe,"id","uploading-your-files"),m(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Oe,"href","#uploading-your-files"),m(se,"class","relative group"),m(Ye,"id","add-a-model-card"),m(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ye,"href","#add-a-model-card"),m(ie,"class","relative group"),m(Yt,"href","https://huggingface.co/docs/hub/model-repos"),m(Yt,"rel","nofollow"),m(Ue,"id","using-your-model"),m(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ue,"href","#using-your-model"),m(ne,"class","relative group"),m(Re,"id","workflow-in-a-colab-notebook"),m(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Re,"href","#workflow-in-a-colab-notebook"),m(fe,"class","relative group"),m(Wt,"href","https://huggingface.co/"),m(Wt,"rel","nofollow")},m(e,a){o(document.head,$),s(e,A,a),s(e,_,a),o(_,b),o(b,I),d(k,I,null),o(_,T),o(_,x),o(x,P),s(e,z,a),s(e,Y,a),o(Y,Qt),o(Y,Xe),o(Xe,xs),o(Y,Os),s(e,tr,a),s(e,M,a),s(e,or,a),d(ue,e,a),s(e,ar,a),s(e,j,a),o(j,Ns),o(j,Kt),o(Kt,Ys),o(j,Ls),o(j,Ho),o(Ho,Bs),o(j,Us),o(j,Wo),o(Wo,Rs),o(j,Hs),o(j,Ze),o(Ze,Ws),o(j,Gs),s(e,rr,a),s(e,G,a),o(G,pe),o(pe,Go),d(Qe,Go,null),o(G,Js),o(G,Jo),o(Jo,Xs),s(e,lr,a),s(e,me,a),o(me,Zs),o(me,Xo),o(Xo,Qs),o(me,Ks),s(e,sr,a),s(e,Vt,a),o(Vt,Vs),s(e,ir,a),s(e,L,a),o(L,Zo),o(Zo,ei),o(L,ti),o(L,Qo),o(Qo,oi),o(L,ai),o(L,Ko),o(Ko,ri),s(e,nr,a),s(e,he,a),o(he,li),o(he,Vo),o(Vo,si),o(he,ii),s(e,fr,a),s(e,eo,a),o(eo,ni),s(e,ur,a),d(Ke,e,a),s(e,pr,a),s(e,J,a),o(J,ce),o(ce,ea),d(Ve,ea,null),o(J,fi),o(J,ta),o(ta,ui),s(e,mr,a),s(e,X,a),o(X,de),o(de,oa),d(et,oa,null),o(X,pi),o(X,aa),o(aa,mi),s(e,hr,a),s(e,to,a),o(to,hi),s(e,cr,a),d(tt,e,a),s(e,dr,a),s(e,ye,a),o(ye,ci),o(ye,ra),o(ra,di),o(ye,yi),s(e,yr,a),s(e,O,a),o(O,vi),o(O,oo),o(oo,wi),o(oo,ot),o(ot,gi),o(O,$i),o(O,la),o(la,_i),o(O,bi),o(O,sa),o(sa,ki),o(O,Ei),s(e,vr,a),s(e,Z,a),o(Z,ve),o(ve,ia),d(at,ia,null),o(Z,Ti),o(Z,na),o(na,Pi),s(e,wr,a),s(e,C,a),s(e,gr,a),s(e,we,a),o(we,Ai),o(we,fa),o(fa,ji),o(we,qi),s(e,$r,a),d(rt,e,a),s(e,_r,a),s(e,ge,a),o(ge,Fi),o(ge,ua),o(ua,Ii),o(ge,zi),s(e,br,a),s(e,$e,a),o($e,Mi),o($e,pa),o(pa,Ci),o($e,Si),s(e,kr,a),d(lt,e,a),s(e,Er,a),s(e,_e,a),o(_e,Di),o(_e,ma),o(ma,xi),o(_e,Oi),s(e,Tr,a),d(st,e,a),s(e,Pr,a),s(e,be,a),o(be,Ni),o(be,ha),o(ha,Yi),o(be,Li),s(e,Ar,a),s(e,ke,a),o(ke,Bi),o(ke,ca),o(ca,Ui),o(ke,Ri),s(e,jr,a),s(e,Q,a),o(Q,Ee),o(Ee,da),d(it,da,null),o(Q,Hi),o(Q,ya),o(ya,Wi),s(e,qr,a),s(e,ao,a),o(ao,Gi),s(e,Fr,a),d(nt,e,a),s(e,Ir,a),s(e,Te,a),o(Te,Ji),o(Te,va),o(va,Xi),o(Te,Zi),s(e,zr,a),d(ft,e,a),s(e,Mr,a),s(e,ro,a),o(ro,Qi),s(e,Cr,a),s(e,lo,a),o(lo,Ki),s(e,Sr,a),d(ut,e,a),s(e,Dr,a),s(e,B,a),o(B,Vi),o(B,wa),o(wa,en),o(B,tn),o(B,ga),o(ga,on),o(B,an),s(e,xr,a),d(pt,e,a),s(e,Or,a),s(e,so,a),o(so,rn),s(e,Nr,a),d(mt,e,a),s(e,Yr,a),s(e,K,a),o(K,Pe),o(Pe,$a),d(ht,$a,null),o(K,ln),o(K,_a),o(_a,sn),s(e,Lr,a),s(e,S,a),s(e,Br,a),s(e,V,a),o(V,Ae),o(Ae,ba),d(ct,ba,null),o(V,nn),o(V,ka),o(ka,fn),s(e,Ur,a),s(e,io,a),o(io,un),s(e,Rr,a),s(e,je,a),o(je,pn),o(je,dt),o(dt,mn),o(je,hn),s(e,Hr,a),s(e,qe,a),o(qe,cn),o(qe,Ea),o(Ea,dn),o(qe,yn),s(e,Wr,a),s(e,Fe,a),o(Fe,vn),o(Fe,Ta),o(Ta,wn),o(Fe,gn),s(e,Gr,a),d(yt,e,a),s(e,Jr,a),s(e,no,a),o(no,$n),s(e,Xr,a),d(vt,e,a),s(e,Zr,a),s(e,Ie,a),o(Ie,_n),o(Ie,Pa),o(Pa,bn),o(Ie,kn),s(e,Qr,a),d(wt,e,a),s(e,Kr,a),s(e,fo,a),o(fo,En),s(e,Vr,a),d(gt,e,a),s(e,el,a),s(e,uo,a),o(uo,Tn),s(e,tl,a),d($t,e,a),s(e,ol,a),s(e,po,a),o(po,Pn),s(e,al,a),s(e,ze,a),o(ze,An),o(ze,_t),o(_t,jn),o(ze,qn),s(e,rl,a),s(e,Me,a),o(Me,Fn),o(Me,bt),o(bt,In),o(Me,zn),s(e,ll,a),s(e,ee,a),o(ee,Ce),o(Ce,Aa),d(kt,Aa,null),o(ee,Mn),o(ee,ja),o(ja,Cn),s(e,sl,a),s(e,N,a),o(N,Sn),o(N,qa),o(qa,Dn),o(N,xn),o(N,Et),o(Et,On),o(N,Nn),o(N,Tt),o(Tt,Yn),o(N,Ln),s(e,il,a),s(e,Se,a),o(Se,Bn),o(Se,mo),o(mo,Un),o(Se,Rn),s(e,nl,a),d(Pt,e,a),s(e,fl,a),s(e,De,a),o(De,Hn),o(De,ho),o(ho,Wn),o(De,Gn),s(e,ul,a),d(At,e,a),s(e,pl,a),s(e,co,a),o(co,Jn),s(e,ml,a),s(e,yo,a),o(yo,Xn),s(e,hl,a),d(jt,e,a),s(e,cl,a),s(e,vo,a),o(vo,Zn),s(e,dl,a),d(qt,e,a),s(e,yl,a),s(e,wo,a),o(wo,Qn),s(e,vl,a),s(e,te,a),o(te,xe),o(xe,Fa),d(Ft,Fa,null),o(te,Kn),o(te,Ia),o(Ia,Vn),s(e,wl,a),s(e,go,a),o(go,ef),s(e,gl,a),s(e,E,a),o(E,oe),o(oe,tf),o(oe,za),o(za,of),o(oe,af),o(oe,$o),o($o,rf),o(oe,lf),o(E,sf),o(E,It),o(It,nf),o(It,Ma),o(Ma,ff),o(It,uf),o(E,pf),o(E,zt),o(zt,mf),o(zt,Ca),o(Ca,hf),o(zt,cf),o(E,df),o(E,ae),o(ae,yf),o(ae,Sa),o(Sa,vf),o(ae,wf),o(ae,_o),o(_o,gf),o(ae,$f),o(E,_f),o(E,re),o(re,bf),o(re,Da),o(Da,kf),o(re,Ef),o(re,bo),o(bo,Tf),o(re,Pf),o(E,Af),o(E,D),o(D,jf),o(D,xa),o(xa,qf),o(D,Ff),o(D,Oa),o(Oa,If),o(D,zf),o(D,Na),o(Na,Mf),o(D,Cf),o(D,ko),o(ko,Sf),o(D,Df),o(E,xf),o(E,le),o(le,Of),o(le,Ya),o(Ya,Nf),o(le,Yf),o(le,Eo),o(Eo,Lf),o(le,Bf),s(e,$l,a),s(e,To,a),o(To,Uf),s(e,_l,a),s(e,se,a),o(se,Oe),o(Oe,La),d(Mt,La,null),o(se,Rf),o(se,Ba),o(Ba,Hf),s(e,bl,a),s(e,Po,a),o(Po,Wf),s(e,kl,a),d(Ct,e,a),s(e,El,a),s(e,Ao,a),o(Ao,Gf),s(e,Tl,a),d(St,e,a),s(e,Pl,a),s(e,Ne,a),o(Ne,Jf),o(Ne,Ua),o(Ua,Xf),o(Ne,Zf),s(e,Al,a),d(Dt,e,a),s(e,jl,a),s(e,jo,a),o(jo,Qf),s(e,ql,a),d(xt,e,a),s(e,Fl,a),s(e,qo,a),o(qo,Kf),s(e,Il,a),d(Ot,e,a),s(e,zl,a),s(e,Fo,a),o(Fo,Vf),s(e,Ml,a),s(e,ie,a),o(ie,Ye),o(Ye,Ra),d(Nt,Ra,null),o(ie,eu),o(ie,Ha),o(Ha,tu),s(e,Cl,a),s(e,Le,a),o(Le,ou),o(Le,Yt),o(Yt,au),o(Le,ru),s(e,Sl,a),d(Be,e,a),s(e,Dl,a),s(e,Io,a),o(Io,lu),s(e,xl,a),s(e,ne,a),o(ne,Ue),o(Ue,Wa),d(Lt,Wa,null),o(ne,su),o(ne,Ga),o(Ga,iu),s(e,Ol,a),s(e,zo,a),o(zo,nu),s(e,Nl,a),s(e,Mo,a),o(Mo,fu),s(e,Yl,a),d(Bt,e,a),s(e,Ll,a),s(e,U,a),o(U,uu),o(U,Ja),o(Ja,pu),o(U,mu),o(U,Xa),o(Xa,hu),o(U,cu),s(e,Bl,a),d(Ut,e,a),s(e,Ul,a),s(e,fe,a),o(fe,Re),o(Re,Za),d(Rt,Za,null),o(fe,du),o(fe,Qa),o(Qa,yu),s(e,Rl,a),s(e,Co,a),o(Co,vu),s(e,Hl,a),s(e,He,a),o(He,wu),o(He,Ka),o(Ka,gu),o(He,$u),s(e,Wl,a),d(Ht,e,a),s(e,Gl,a),s(e,R,a),o(R,_u),o(R,Wt),o(Wt,bu),o(R,ku),o(R,Va),o(Va,Eu),o(R,Tu),s(e,Jl,a),d(Gt,e,a),s(e,Xl,a),s(e,So,a),o(So,Pu),s(e,Zl,a),d(Jt,e,a),s(e,Ql,a),s(e,Do,a),o(Do,Au),s(e,Kl,a),d(Xt,e,a),Vl=!0},p(e,[a]){const Zt={};a&2&&(Zt.$$scope={dirty:a,ctx:e}),ue.$set(Zt);const er={};a&2&&(er.$$scope={dirty:a,ctx:e}),Be.$set(er)},i(e){Vl||(y(k.$$.fragment,e),y(ue.$$.fragment,e),y(Qe.$$.fragment,e),y(Ke.$$.fragment,e),y(Ve.$$.fragment,e),y(et.$$.fragment,e),y(tt.$$.fragment,e),y(at.$$.fragment,e),y(rt.$$.fragment,e),y(lt.$$.fragment,e),y(st.$$.fragment,e),y(it.$$.fragment,e),y(nt.$$.fragment,e),y(ft.$$.fragment,e),y(ut.$$.fragment,e),y(pt.$$.fragment,e),y(mt.$$.fragment,e),y(ht.$$.fragment,e),y(ct.$$.fragment,e),y(yt.$$.fragment,e),y(vt.$$.fragment,e),y(wt.$$.fragment,e),y(gt.$$.fragment,e),y($t.$$.fragment,e),y(kt.$$.fragment,e),y(Pt.$$.fragment,e),y(At.$$.fragment,e),y(jt.$$.fragment,e),y(qt.$$.fragment,e),y(Ft.$$.fragment,e),y(Mt.$$.fragment,e),y(Ct.$$.fragment,e),y(St.$$.fragment,e),y(Dt.$$.fragment,e),y(xt.$$.fragment,e),y(Ot.$$.fragment,e),y(Nt.$$.fragment,e),y(Be.$$.fragment,e),y(Lt.$$.fragment,e),y(Bt.$$.fragment,e),y(Ut.$$.fragment,e),y(Rt.$$.fragment,e),y(Ht.$$.fragment,e),y(Gt.$$.fragment,e),y(Jt.$$.fragment,e),y(Xt.$$.fragment,e),Vl=!0)},o(e){v(k.$$.fragment,e),v(ue.$$.fragment,e),v(Qe.$$.fragment,e),v(Ke.$$.fragment,e),v(Ve.$$.fragment,e),v(et.$$.fragment,e),v(tt.$$.fragment,e),v(at.$$.fragment,e),v(rt.$$.fragment,e),v(lt.$$.fragment,e),v(st.$$.fragment,e),v(it.$$.fragment,e),v(nt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(pt.$$.fragment,e),v(mt.$$.fragment,e),v(ht.$$.fragment,e),v(ct.$$.fragment,e),v(yt.$$.fragment,e),v(vt.$$.fragment,e),v(wt.$$.fragment,e),v(gt.$$.fragment,e),v($t.$$.fragment,e),v(kt.$$.fragment,e),v(Pt.$$.fragment,e),v(At.$$.fragment,e),v(jt.$$.fragment,e),v(qt.$$.fragment,e),v(Ft.$$.fragment,e),v(Mt.$$.fragment,e),v(Ct.$$.fragment,e),v(St.$$.fragment,e),v(Dt.$$.fragment,e),v(xt.$$.fragment,e),v(Ot.$$.fragment,e),v(Nt.$$.fragment,e),v(Be.$$.fragment,e),v(Lt.$$.fragment,e),v(Bt.$$.fragment,e),v(Ut.$$.fragment,e),v(Rt.$$.fragment,e),v(Ht.$$.fragment,e),v(Gt.$$.fragment,e),v(Jt.$$.fragment,e),v(Xt.$$.fragment,e),Vl=!1},d(e){t($),e&&t(A),e&&t(_),w(k),e&&t(z),e&&t(Y),e&&t(tr),e&&t(M),e&&t(or),w(ue,e),e&&t(ar),e&&t(j),e&&t(rr),e&&t(G),w(Qe),e&&t(lr),e&&t(me),e&&t(sr),e&&t(Vt),e&&t(ir),e&&t(L),e&&t(nr),e&&t(he),e&&t(fr),e&&t(eo),e&&t(ur),w(Ke,e),e&&t(pr),e&&t(J),w(Ve),e&&t(mr),e&&t(X),w(et),e&&t(hr),e&&t(to),e&&t(cr),w(tt,e),e&&t(dr),e&&t(ye),e&&t(yr),e&&t(O),e&&t(vr),e&&t(Z),w(at),e&&t(wr),e&&t(C),e&&t(gr),e&&t(we),e&&t($r),w(rt,e),e&&t(_r),e&&t(ge),e&&t(br),e&&t($e),e&&t(kr),w(lt,e),e&&t(Er),e&&t(_e),e&&t(Tr),w(st,e),e&&t(Pr),e&&t(be),e&&t(Ar),e&&t(ke),e&&t(jr),e&&t(Q),w(it),e&&t(qr),e&&t(ao),e&&t(Fr),w(nt,e),e&&t(Ir),e&&t(Te),e&&t(zr),w(ft,e),e&&t(Mr),e&&t(ro),e&&t(Cr),e&&t(lo),e&&t(Sr),w(ut,e),e&&t(Dr),e&&t(B),e&&t(xr),w(pt,e),e&&t(Or),e&&t(so),e&&t(Nr),w(mt,e),e&&t(Yr),e&&t(K),w(ht),e&&t(Lr),e&&t(S),e&&t(Br),e&&t(V),w(ct),e&&t(Ur),e&&t(io),e&&t(Rr),e&&t(je),e&&t(Hr),e&&t(qe),e&&t(Wr),e&&t(Fe),e&&t(Gr),w(yt,e),e&&t(Jr),e&&t(no),e&&t(Xr),w(vt,e),e&&t(Zr),e&&t(Ie),e&&t(Qr),w(wt,e),e&&t(Kr),e&&t(fo),e&&t(Vr),w(gt,e),e&&t(el),e&&t(uo),e&&t(tl),w($t,e),e&&t(ol),e&&t(po),e&&t(al),e&&t(ze),e&&t(rl),e&&t(Me),e&&t(ll),e&&t(ee),w(kt),e&&t(sl),e&&t(N),e&&t(il),e&&t(Se),e&&t(nl),w(Pt,e),e&&t(fl),e&&t(De),e&&t(ul),w(At,e),e&&t(pl),e&&t(co),e&&t(ml),e&&t(yo),e&&t(hl),w(jt,e),e&&t(cl),e&&t(vo),e&&t(dl),w(qt,e),e&&t(yl),e&&t(wo),e&&t(vl),e&&t(te),w(Ft),e&&t(wl),e&&t(go),e&&t(gl),e&&t(E),e&&t($l),e&&t(To),e&&t(_l),e&&t(se),w(Mt),e&&t(bl),e&&t(Po),e&&t(kl),w(Ct,e),e&&t(El),e&&t(Ao),e&&t(Tl),w(St,e),e&&t(Pl),e&&t(Ne),e&&t(Al),w(Dt,e),e&&t(jl),e&&t(jo),e&&t(ql),w(xt,e),e&&t(Fl),e&&t(qo),e&&t(Il),w(Ot,e),e&&t(zl),e&&t(Fo),e&&t(Ml),e&&t(ie),w(Nt),e&&t(Cl),e&&t(Le),e&&t(Sl),w(Be,e),e&&t(Dl),e&&t(Io),e&&t(xl),e&&t(ne),w(Lt),e&&t(Ol),e&&t(zo),e&&t(Nl),e&&t(Mo),e&&t(Yl),w(Bt,e),e&&t(Ll),e&&t(U),e&&t(Bl),w(Ut,e),e&&t(Ul),e&&t(fe),w(Rt),e&&t(Rl),e&&t(Co),e&&t(Hl),e&&t(He),e&&t(Wl),w(Ht,e),e&&t(Gl),e&&t(R),e&&t(Jl),w(Gt,e),e&&t(Xl),e&&t(So),e&&t(Zl),w(Jt,e),e&&t(Ql),e&&t(Do),e&&t(Kl),w(Xt,e)}}}const rh={local:"model-sharing-and-uploading",sections:[{local:"model-versioning",title:"Model versioning"},{local:"push-your-model-from-python",sections:[{local:"preparation",title:"Preparation"},{local:"directly-push-your-model-to-the-hub",title:"Directly push your model to the hub"},{local:"add-new-files-to-your-model-repo",title:"Add new files to your model repo"}],title:"Push your model from Python"},{local:"use-your-terminal-and-git",sections:[{local:"basic-steps",title:"Basic steps"},{local:"make-your-model-work-on-all-frameworks",title:"Make your model work on all frameworks"},{local:"check-the-directory-before-pushing-to-the-model-hub",title:"Check the directory before pushing to the model hub."}],title:"Use your terminal and git"},{local:"uploading-your-files",sections:[{local:"add-a-model-card",title:"Add a model card"},{local:"using-your-model",title:"Using your model"}],title:"Uploading your files"},{local:"workflow-in-a-colab-notebook",title:"Workflow in a Colab notebook"}],title:"Model sharing and uploading"};function lh(Je,$,A){let{fw:_}=$;return Je.$$set=b=>{"fw"in b&&A(0,_=b.fw)},[_]}class ph extends Qm{constructor($){super();Km(this,$,lh,ah,Vm,{fw:0})}}export{ph as default,rh as metadata};
