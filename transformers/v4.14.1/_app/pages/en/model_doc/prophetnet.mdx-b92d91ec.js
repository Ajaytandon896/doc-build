import{S as Zh,i as el,s as tl,e as n,k as c,w as f,t as s,L as ol,c as r,d as o,m as h,a as d,x as _,h as a,b as i,J as e,g as p,y as g,q as v,o as k,B as T}from"../../../chunks/vendor-b1433968.js";import{T as Yn}from"../../../chunks/Tip-c3840994.js";import{D as q}from"../../../chunks/Docstring-ff504c58.js";import{C as Uo}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as ee}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function nl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),y=s("Module"),N=s(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(b,"CODE",{});var z=d(m);y=a(z,"Module"),z.forEach(o),N=a(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function rl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),y=s("Module"),N=s(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(b,"CODE",{});var z=d(m);y=a(z,"Module"),z.forEach(o),N=a(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function sl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),y=s("Module"),N=s(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(b,"CODE",{});var z=d(m);y=a(z,"Module"),z.forEach(o),N=a(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function al(I){let u,P,m,y,N;return{c(){u=n("p"),P=s(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),y=s("Module"),N=s(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(b,"CODE",{});var z=d(m);y=a(z,"Module"),z.forEach(o),N=a(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function dl(I){let u,P,m,y,N;return{c(){u=n("p"),P=s(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),y=s("Module"),N=s(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(w){u=r(w,"P",{});var b=d(u);P=a(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=r(b,"CODE",{});var z=d(m);y=a(z,"Module"),z.forEach(o),N=a(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function il(I){let u,P,m,y,N,w,b,z,es,Kn,te,Ro,ts,os,Qe,ns,rs,Qn,oe,Ee,Jo,Xe,ss,Yo,as,Xn,Me,ds,Ze,is,cs,Zn,ao,hs,er,io,ls,tr,co,Ko,ps,or,Ce,us,et,ms,fs,nr,ne,Se,Qo,tt,_s,Xo,gs,rr,H,ot,vs,nt,ks,ho,Ts,bs,ws,re,ys,lo,Ps,Ns,po,zs,qs,sr,se,De,Zo,rt,$s,en,Fs,ar,$,st,xs,tn,Es,Ms,at,Cs,uo,Ss,Ds,Os,V,dt,Ls,on,js,As,it,mo,Is,nn,Bs,Gs,fo,Ws,rn,Hs,Vs,Oe,ct,Us,sn,Rs,Js,B,ht,Ys,an,Ks,Qs,lt,Xs,ae,Zs,dn,ea,ta,cn,oa,na,ra,Le,pt,sa,ut,aa,hn,da,ia,dr,de,je,ln,mt,ca,pn,ha,ir,ie,ft,la,un,pa,cr,ce,_t,ua,mn,ma,hr,he,gt,fa,fn,_a,lr,le,vt,ga,_n,va,pr,pe,Ae,gn,kt,ka,vn,Ta,ur,E,Tt,ba,bt,wa,_o,ya,Pa,Na,ue,za,wt,qa,$a,kn,Fa,xa,Ea,yt,Ma,Pt,Ca,Sa,Da,S,Nt,Oa,me,La,go,ja,Aa,Tn,Ia,Ba,Ga,Ie,Wa,bn,Ha,Va,zt,mr,fe,Be,wn,qt,Ua,yn,Ra,fr,F,$t,Ja,Ft,Ya,vo,Ka,Qa,Xa,_e,Za,xt,ed,td,Pn,od,nd,rd,Et,sd,Mt,ad,dd,id,ge,cd,ve,hd,Nn,ld,pd,zn,ud,md,fd,ko,_d,gd,vd,D,Ct,kd,ke,Td,To,bd,wd,qn,yd,Pd,Nd,Ge,zd,$n,qd,$d,St,_r,Te,We,Fn,Dt,Fd,xn,xd,gr,x,Ot,Ed,Lt,Md,bo,Cd,Sd,Dd,be,Od,jt,Ld,jd,En,Ad,Id,Bd,At,Gd,It,Wd,Hd,Vd,we,Ud,ye,Rd,Mn,Jd,Yd,Cn,Kd,Qd,Xd,wo,Zd,ei,ti,O,Bt,oi,Pe,ni,yo,ri,si,Sn,ai,di,ii,He,ci,Dn,hi,li,Gt,vr,Ne,Ve,On,Wt,pi,Ln,ui,kr,M,Ht,mi,Vt,fi,Po,_i,gi,vi,ze,ki,Ut,Ti,bi,jn,wi,yi,Pi,Rt,Ni,Jt,zi,qi,$i,L,Yt,Fi,qe,xi,No,Ei,Mi,An,Ci,Si,Di,Ue,Oi,In,Li,ji,Kt,Tr,$e,Re,Bn,Qt,Ai,Gn,Ii,br,C,Xt,Bi,Zt,Gi,zo,Wi,Hi,Vi,Fe,Ui,eo,Ri,Ji,Wn,Yi,Ki,Qi,to,Xi,oo,Zi,ec,tc,j,no,oc,xe,nc,qo,rc,sc,Hn,ac,dc,ic,Je,cc,Vn,hc,lc,ro,wr;return w=new ee({}),Xe=new ee({}),tt=new ee({}),ot=new q({props:{name:"class transformers.ProphetNetConfig",anchor:"transformers.ProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/configuration_prophetnet.py#L29",parametersDescription:[{anchor:"transformers.ProphetNetConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.ProphetNetConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.ProphetNetConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the ProphetNET model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a>.`,name:"vocab_size"},{anchor:"transformers.ProphetNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ProphetNetConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_encoder_layers",description:`<strong>num_encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"num_encoder_layers"},{anchor:"transformers.ProphetNetConfig.num_encoder_attention_heads",description:`<strong>num_encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_encoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the <code>intermediate</code> (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"num_decoder_layers"},{anchor:"transformers.ProphetNetConfig.num_decoder_attention_heads",description:`<strong>num_decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_decoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.ProphetNetConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.ProphetNetConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.ProphetNetConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.ProphetNetConfig.add_cross_attention",description:`<strong>add_cross_attention</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether cross-attention layers should be added to the model.`,name:"add_cross_attention"},{anchor:"transformers.ProphetNetConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.ProphetNetConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.ProphetNetConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.ProphetNetConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.ProphetNetConfig.ngram",description:`<strong>ngram</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of future tokens to predict. Set to 1 to be same as traditional Language model to predict next first
token.`,name:"ngram"},{anchor:"transformers.ProphetNetConfig.num_buckets",description:`<strong>num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer. This is for relative position calculation. See the
[T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"num_buckets"},{anchor:"transformers.ProphetNetConfig.relative_max_distance",description:`<strong>relative_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Relative distances greater than this number will be put into the last same bucket. This is for relative
position calculation. See the [T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"relative_max_distance"},{anchor:"transformers.ProphetNetConfig.disable_ngram_loss",description:`<strong>disable_ngram_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether be trained predicting only the next first token.`,name:"disable_ngram_loss"},{anchor:"transformers.ProphetNetConfig.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Controls the <code>epsilon</code> parameter value for label smoothing in the loss calculation. If set to 0, no label
smoothing is performed.`,name:"eps"},{anchor:"transformers.ProphetNetConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),rt=new ee({}),st=new q({props:{name:"class transformers.ProphetNetTokenizer",anchor:"transformers.ProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"x_sep_token",val:" = '[X_SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/tokenization_prophetnet.py#L55",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.ProphetNetTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.ProphetNetTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.ProphetNetTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.ProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.ProphetNetTokenizer.x_sep_token",description:`<strong>x_sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[X_SEP]&quot;</code>) &#x2014;
Special second separator token, which can be generated by
<a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a>. It is used to separate bullet-point like
sentences in summarization, <em>e.g.</em>.`,name:"x_sep_token"},{anchor:"transformers.ProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.ProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.ProphetNetTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip<em>accents &#x2014; (<code>bool</code>, _optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),dt=new q({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/tokenization_prophetnet.py#L263",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new q({props:{name:"convert_tokens_to_string",anchor:"transformers.ProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/tokenization_prophetnet.py#L182"}}),ht=new q({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/tokenization_prophetnet.py#L214",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given
sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lt=new Uo({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),pt=new q({props:{name:"get_special_tokens_mask",anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/tokenization_prophetnet.py#L187",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new ee({}),ft=new q({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L257",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.`,name:"encoder_attentions"}]}}),_t=new q({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L341",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),gt=new q({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L426",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),vt=new q({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L486",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),kt=new ee({}),Tt=new q({props:{name:"class transformers.ProphetNetModel",anchor:"transformers.ProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1755",parametersDescription:[{anchor:"transformers.ProphetNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Nt=new q({props:{name:"forward",anchor:"transformers.ProphetNetModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1787",parametersDescription:[{anchor:"transformers.ProphetNetModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>,
<em>optional</em>) is a sequence of hidden-states at the output of the last layer of the encoder. Used in the
cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>ProphetNetSeq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>ProphetNetSeq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new Yn({props:{$$slots:{default:[nl]},$$scope:{ctx:I}}}),zt=new Uo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetModel

tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')
model = ProphetNetModel.from_pretrained('microsoft/prophetnet-large-uncased')

input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetModel.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),qt=new ee({}),$t=new q({props:{name:"class transformers.ProphetNetEncoder",anchor:"transformers.ProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1249",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ct=new q({props:{name:"forward",anchor:"transformers.ProphetNetEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1279",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ge=new Yn({props:{$$slots:{default:[rl]},$$scope:{ctx:I}}}),St=new Uo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetEncoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')
model = ProphetNetEncoder.from_pretrained('patrickvonplaten/prophetnet-large-uncased-standalone')
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetEncoder.from_pretrained(<span class="hljs-string">&#x27;patrickvonplaten/prophetnet-large-uncased-standalone&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Dt=new ee({}),Ot=new q({props:{name:"class transformers.ProphetNetDecoder",anchor:"transformers.ProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1388",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Bt=new q({props:{name:"forward",anchor:"transformers.ProphetNetDecoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1425",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetDecoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetDecoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetDecoder.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>ProphetNetDecoderModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>ProphetNetDecoderModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),He=new Yn({props:{$$slots:{default:[sl]},$$scope:{ctx:I}}}),Gt=new Uo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetDecoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')
model = ProphetNetDecoder.from_pretrained('microsoft/prophetnet-large-uncased', add_cross_attention=False)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetDecoder.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>, add_cross_attention=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Wt=new ee({}),Ht=new q({props:{name:"class transformers.ProphetNetForConditionalGeneration",anchor:"transformers.ProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1879",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Yt=new q({props:{name:"forward",anchor:"transformers.ProphetNetForConditionalGeneration.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L1900",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>,
<em>optional</em>) is a sequence of hidden-states at the output of the last layer of the encoder. Used in the
cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>ProphetNetSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>ProphetNetSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ue=new Yn({props:{$$slots:{default:[al]},$$scope:{ctx:I}}}),Kt=new Uo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration

tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')
model = ProphetNetForConditionalGeneration.from_pretrained('microsoft/prophetnet-large-uncased')

input_ids = tokenizer("Studies have been shown that owning a dog is good for you", return_tensors="pt").input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),Qt=new ee({}),Xt=new q({props:{name:"class transformers.ProphetNetForCausalLM",anchor:"transformers.ProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L2085",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),no=new q({props:{name:"forward",anchor:"transformers.ProphetNetForCausalLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/prophetnet/modeling_prophetnet.py#L2120",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.ProphetNetForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>ProphetNetDecoderLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>ProphetNetDecoderLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Je=new Yn({props:{$$slots:{default:[dl]},$$scope:{ctx:I}}}),ro=new Uo({props:{code:`from transformers import ProphetNetTokenizer, ProphetNetForCausalLM
import torch

tokenizer = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')
model = ProphetNetForCausalLM.from_pretrained('microsoft/prophetnet-large-uncased')
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
import torch

tokenizer_enc = BertTokenizer.from_pretrained('bert-large-uncased')
tokenizer_dec = ProphetNetTokenizer.from_pretrained('microsoft/prophetnet-large-uncased')
model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-large-uncased", "microsoft/prophetnet-large-uncased")

ARTICLE = (
"the us state department said wednesday it had received no "
"formal word from bolivia that it was expelling the us ambassador there "
"but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec("us rejects charges against its ambassador in bolivia", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&#x27;microsoft/prophetnet-large-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>, <span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span><span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span><span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span><span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(<span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){u=n("meta"),P=c(),m=n("h1"),y=n("a"),N=n("span"),f(w.$$.fragment),b=c(),z=n("span"),es=s("ProphetNet"),Kn=c(),te=n("p"),Ro=n("strong"),ts=s("DISCLAIMER:"),os=s(" If you see something strange, file a "),Qe=n("a"),ns=s("Github Issue"),rs=s(` and assign
@patrickvonplaten`),Qn=c(),oe=n("h2"),Ee=n("a"),Jo=n("span"),f(Xe.$$.fragment),ss=c(),Yo=n("span"),as=s("Overview"),Xn=c(),Me=n("p"),ds=s("The ProphetNet model was proposed in "),Ze=n("a"),is=s("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),cs=s(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Zn=c(),ao=n("p"),hs=s(`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),er=c(),io=n("p"),ls=s("The abstract from the paper is the following:"),tr=c(),co=n("p"),Ko=n("em"),ps=s(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),or=c(),Ce=n("p"),us=s("The Authors\u2019 code can be found "),et=n("a"),ms=s("here"),fs=s("."),nr=c(),ne=n("h2"),Se=n("a"),Qo=n("span"),f(tt.$$.fragment),_s=c(),Xo=n("span"),gs=s("ProphetNetConfig"),rr=c(),H=n("div"),f(ot.$$.fragment),vs=c(),nt=n("p"),ks=s("This is the configuration class to store the configuration of a "),ho=n("a"),Ts=s("ProphetNetModel"),bs=s(`. It is used
to instantiate a ProphetNet model according to the specified arguments, defining the model architecture.`),ws=c(),re=n("p"),ys=s("Configuration objects inherit from "),lo=n("a"),Ps=s("PretrainedConfig"),Ns=s(` and can be used to control the model
outputs. Read the documentation from `),po=n("a"),zs=s("PretrainedConfig"),qs=s(" for more information."),sr=c(),se=n("h2"),De=n("a"),Zo=n("span"),f(rt.$$.fragment),$s=c(),en=n("span"),Fs=s("ProphetNetTokenizer"),ar=c(),$=n("div"),f(st.$$.fragment),xs=c(),tn=n("p"),Es=s("Construct a ProphetNetTokenizer. Based on WordPiece."),Ms=c(),at=n("p"),Cs=s("This tokenizer inherits from "),uo=n("a"),Ss=s("PreTrainedTokenizer"),Ds=s(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Os=c(),V=n("div"),f(dt.$$.fragment),Ls=c(),on=n("p"),js=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),As=c(),it=n("ul"),mo=n("li"),Is=s("single sequence: "),nn=n("code"),Bs=s("[CLS] X [SEP]"),Gs=c(),fo=n("li"),Ws=s("pair of sequences: "),rn=n("code"),Hs=s("[CLS] A [SEP] B [SEP]"),Vs=c(),Oe=n("div"),f(ct.$$.fragment),Us=c(),sn=n("p"),Rs=s("Converts a sequence of tokens (string) in a single string."),Js=c(),B=n("div"),f(ht.$$.fragment),Ys=c(),an=n("p"),Ks=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Qs=c(),f(lt.$$.fragment),Xs=c(),ae=n("p"),Zs=s("If "),dn=n("code"),ea=s("token_ids_1"),ta=s(" is "),cn=n("code"),oa=s("None"),na=s(", this method only returns the first portion of the mask (0s)."),ra=c(),Le=n("div"),f(pt.$$.fragment),sa=c(),ut=n("p"),aa=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),hn=n("code"),da=s("prepare_for_model"),ia=s(" method."),dr=c(),de=n("h2"),je=n("a"),ln=n("span"),f(mt.$$.fragment),ca=c(),pn=n("span"),ha=s("ProphetNet specific outputs"),ir=c(),ie=n("div"),f(ft.$$.fragment),la=c(),un=n("p"),pa=s("Base class for sequence-to-sequence language models outputs."),cr=c(),ce=n("div"),f(_t.$$.fragment),ua=c(),mn=n("p"),ma=s(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),hr=c(),he=n("div"),f(gt.$$.fragment),fa=c(),fn=n("p"),_a=s("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),lr=c(),le=n("div"),f(vt.$$.fragment),ga=c(),_n=n("p"),va=s("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),pr=c(),pe=n("h2"),Ae=n("a"),gn=n("span"),f(kt.$$.fragment),ka=c(),vn=n("span"),Ta=s("ProphetNetModel"),ur=c(),E=n("div"),f(Tt.$$.fragment),ba=c(),bt=n("p"),wa=s(`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=n("a"),ya=s("PreTrainedModel"),Pa=s(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Na=c(),ue=n("p"),za=s("Original ProphetNet code can be found at <"),wt=n("a"),qa=s("https://github.com/microsoft/ProphetNet>"),$a=s(` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),kn=n("code"),Fa=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),xa=s("."),Ea=c(),yt=n("p"),Ma=s("This model is a PyTorch "),Pt=n("a"),Ca=s("torch.nn.Module"),Sa=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Da=c(),S=n("div"),f(Nt.$$.fragment),Oa=c(),me=n("p"),La=s("The "),go=n("a"),ja=s("ProphetNetModel"),Aa=s(" forward method, overrides the "),Tn=n("code"),Ia=s("__call__"),Ba=s(" special method."),Ga=c(),f(Ie.$$.fragment),Wa=c(),bn=n("p"),Ha=s("Example:"),Va=c(),f(zt.$$.fragment),mr=c(),fe=n("h2"),Be=n("a"),wn=n("span"),f(qt.$$.fragment),Ua=c(),yn=n("span"),Ra=s("ProphetNetEncoder"),fr=c(),F=n("div"),f($t.$$.fragment),Ja=c(),Ft=n("p"),Ya=s(`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=n("a"),Ka=s("PreTrainedModel"),Qa=s(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Xa=c(),_e=n("p"),Za=s("Original ProphetNet code can be found at <"),xt=n("a"),ed=s("https://github.com/microsoft/ProphetNet>"),td=s(` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Pn=n("code"),od=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),nd=s("."),rd=c(),Et=n("p"),sd=s("This model is a PyTorch "),Mt=n("a"),ad=s("torch.nn.Module"),dd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),id=c(),ge=n("p"),cd=s("word"),ve=n("em"),hd=s("embeddings  ("),Nn=n("code"),ld=s("torch.nn.Embeddings"),pd=s(" of shape "),zn=n("code"),ud=s("(config.vocab_size, config.hidden_size)"),md=s(", _optional"),fd=s(`):
The word embedding parameters. This can be used to initialize `),ko=n("a"),_d=s("ProphetNetEncoder"),gd=s(` with
pre-defined word embeddings instead of randomly initialized word embeddings.`),vd=c(),D=n("div"),f(Ct.$$.fragment),kd=c(),ke=n("p"),Td=s("The "),To=n("a"),bd=s("ProphetNetEncoder"),wd=s(" forward method, overrides the "),qn=n("code"),yd=s("__call__"),Pd=s(" special method."),Nd=c(),f(Ge.$$.fragment),zd=c(),$n=n("p"),qd=s("Example:"),$d=c(),f(St.$$.fragment),_r=c(),Te=n("h2"),We=n("a"),Fn=n("span"),f(Dt.$$.fragment),Fd=c(),xn=n("span"),xd=s("ProphetNetDecoder"),gr=c(),x=n("div"),f(Ot.$$.fragment),Ed=c(),Lt=n("p"),Md=s(`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=n("a"),Cd=s("PreTrainedModel"),Sd=s(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Dd=c(),be=n("p"),Od=s("Original ProphetNet code can be found at <"),jt=n("a"),Ld=s("https://github.com/microsoft/ProphetNet>"),jd=s(` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),En=n("code"),Ad=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Id=s("."),Bd=c(),At=n("p"),Gd=s("This model is a PyTorch "),It=n("a"),Wd=s("torch.nn.Module"),Hd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Vd=c(),we=n("p"),Ud=s("word"),ye=n("em"),Rd=s("embeddings  ("),Mn=n("code"),Jd=s("torch.nn.Embeddings"),Yd=s(" of shape "),Cn=n("code"),Kd=s("(config.vocab_size, config.hidden_size)"),Qd=s(", _optional"),Xd=s(`):
The word embedding parameters. This can be used to initialize `),wo=n("a"),Zd=s("ProphetNetEncoder"),ei=s(` with
pre-defined word embeddings instead of randomly initialized word embeddings.`),ti=c(),O=n("div"),f(Bt.$$.fragment),oi=c(),Pe=n("p"),ni=s("The "),yo=n("a"),ri=s("ProphetNetDecoder"),si=s(" forward method, overrides the "),Sn=n("code"),ai=s("__call__"),di=s(" special method."),ii=c(),f(He.$$.fragment),ci=c(),Dn=n("p"),hi=s("Example:"),li=c(),f(Gt.$$.fragment),vr=c(),Ne=n("h2"),Ve=n("a"),On=n("span"),f(Wt.$$.fragment),pi=c(),Ln=n("span"),ui=s("ProphetNetForConditionalGeneration"),kr=c(),M=n("div"),f(Ht.$$.fragment),mi=c(),Vt=n("p"),fi=s(`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=n("a"),_i=s("PreTrainedModel"),gi=s(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),vi=c(),ze=n("p"),ki=s("Original ProphetNet code can be found at <"),Ut=n("a"),Ti=s("https://github.com/microsoft/ProphetNet>"),bi=s(` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),jn=n("code"),wi=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),yi=s("."),Pi=c(),Rt=n("p"),Ni=s("This model is a PyTorch "),Jt=n("a"),zi=s("torch.nn.Module"),qi=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),$i=c(),L=n("div"),f(Yt.$$.fragment),Fi=c(),qe=n("p"),xi=s("The "),No=n("a"),Ei=s("ProphetNetForConditionalGeneration"),Mi=s(" forward method, overrides the "),An=n("code"),Ci=s("__call__"),Si=s(" special method."),Di=c(),f(Ue.$$.fragment),Oi=c(),In=n("p"),Li=s("Example:"),ji=c(),f(Kt.$$.fragment),Tr=c(),$e=n("h2"),Re=n("a"),Bn=n("span"),f(Qt.$$.fragment),Ai=c(),Gn=n("span"),Ii=s("ProphetNetForCausalLM"),br=c(),C=n("div"),f(Xt.$$.fragment),Bi=c(),Zt=n("p"),Gi=s(`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),zo=n("a"),Wi=s("PreTrainedModel"),Hi=s(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Vi=c(),Fe=n("p"),Ui=s("Original ProphetNet code can be found at <"),eo=n("a"),Ri=s("https://github.com/microsoft/ProphetNet>"),Ji=s(` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Wn=n("code"),Yi=s("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Ki=s("."),Qi=c(),to=n("p"),Xi=s("This model is a PyTorch "),oo=n("a"),Zi=s("torch.nn.Module"),ec=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),tc=c(),j=n("div"),f(no.$$.fragment),oc=c(),xe=n("p"),nc=s("The "),qo=n("a"),rc=s("ProphetNetForCausalLM"),sc=s(" forward method, overrides the "),Hn=n("code"),ac=s("__call__"),dc=s(" special method."),ic=c(),f(Je.$$.fragment),cc=c(),Vn=n("p"),hc=s("Example:"),lc=c(),f(ro.$$.fragment),this.h()},l(t){const l=ol('[data-svelte="svelte-1phssyn"]',document.head);u=r(l,"META",{name:!0,content:!0}),l.forEach(o),P=h(t),m=r(t,"H1",{class:!0});var so=d(m);y=r(so,"A",{id:!0,class:!0,href:!0});var Un=d(y);N=r(Un,"SPAN",{});var Rn=d(N);_(w.$$.fragment,Rn),Rn.forEach(o),Un.forEach(o),b=h(so),z=r(so,"SPAN",{});var Jn=d(z);es=a(Jn,"ProphetNet"),Jn.forEach(o),so.forEach(o),Kn=h(t),te=r(t,"P",{});var Ye=d(te);Ro=r(Ye,"STRONG",{});var mc=d(Ro);ts=a(mc,"DISCLAIMER:"),mc.forEach(o),os=a(Ye," If you see something strange, file a "),Qe=r(Ye,"A",{href:!0,rel:!0});var fc=d(Qe);ns=a(fc,"Github Issue"),fc.forEach(o),rs=a(Ye,` and assign
@patrickvonplaten`),Ye.forEach(o),Qn=h(t),oe=r(t,"H2",{class:!0});var yr=d(oe);Ee=r(yr,"A",{id:!0,class:!0,href:!0});var _c=d(Ee);Jo=r(_c,"SPAN",{});var gc=d(Jo);_(Xe.$$.fragment,gc),gc.forEach(o),_c.forEach(o),ss=h(yr),Yo=r(yr,"SPAN",{});var vc=d(Yo);as=a(vc,"Overview"),vc.forEach(o),yr.forEach(o),Xn=h(t),Me=r(t,"P",{});var Pr=d(Me);ds=a(Pr,"The ProphetNet model was proposed in "),Ze=r(Pr,"A",{href:!0,rel:!0});var kc=d(Ze);is=a(kc,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),kc.forEach(o),cs=a(Pr,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Pr.forEach(o),Zn=h(t),ao=r(t,"P",{});var Tc=d(ao);hs=a(Tc,`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),Tc.forEach(o),er=h(t),io=r(t,"P",{});var bc=d(io);ls=a(bc,"The abstract from the paper is the following:"),bc.forEach(o),tr=h(t),co=r(t,"P",{});var wc=d(co);Ko=r(wc,"EM",{});var yc=d(Ko);ps=a(yc,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),yc.forEach(o),wc.forEach(o),or=h(t),Ce=r(t,"P",{});var Nr=d(Ce);us=a(Nr,"The Authors\u2019 code can be found "),et=r(Nr,"A",{href:!0,rel:!0});var Pc=d(et);ms=a(Pc,"here"),Pc.forEach(o),fs=a(Nr,"."),Nr.forEach(o),nr=h(t),ne=r(t,"H2",{class:!0});var zr=d(ne);Se=r(zr,"A",{id:!0,class:!0,href:!0});var Nc=d(Se);Qo=r(Nc,"SPAN",{});var zc=d(Qo);_(tt.$$.fragment,zc),zc.forEach(o),Nc.forEach(o),_s=h(zr),Xo=r(zr,"SPAN",{});var qc=d(Xo);gs=a(qc,"ProphetNetConfig"),qc.forEach(o),zr.forEach(o),rr=h(t),H=r(t,"DIV",{class:!0});var $o=d(H);_(ot.$$.fragment,$o),vs=h($o),nt=r($o,"P",{});var qr=d(nt);ks=a(qr,"This is the configuration class to store the configuration of a "),ho=r(qr,"A",{href:!0});var $c=d(ho);Ts=a($c,"ProphetNetModel"),$c.forEach(o),bs=a(qr,`. It is used
to instantiate a ProphetNet model according to the specified arguments, defining the model architecture.`),qr.forEach(o),ws=h($o),re=r($o,"P",{});var Fo=d(re);ys=a(Fo,"Configuration objects inherit from "),lo=r(Fo,"A",{href:!0});var Fc=d(lo);Ps=a(Fc,"PretrainedConfig"),Fc.forEach(o),Ns=a(Fo,` and can be used to control the model
outputs. Read the documentation from `),po=r(Fo,"A",{href:!0});var xc=d(po);zs=a(xc,"PretrainedConfig"),xc.forEach(o),qs=a(Fo," for more information."),Fo.forEach(o),$o.forEach(o),sr=h(t),se=r(t,"H2",{class:!0});var $r=d(se);De=r($r,"A",{id:!0,class:!0,href:!0});var Ec=d(De);Zo=r(Ec,"SPAN",{});var Mc=d(Zo);_(rt.$$.fragment,Mc),Mc.forEach(o),Ec.forEach(o),$s=h($r),en=r($r,"SPAN",{});var Cc=d(en);Fs=a(Cc,"ProphetNetTokenizer"),Cc.forEach(o),$r.forEach(o),ar=h(t),$=r(t,"DIV",{class:!0});var A=d($);_(st.$$.fragment,A),xs=h(A),tn=r(A,"P",{});var Sc=d(tn);Es=a(Sc,"Construct a ProphetNetTokenizer. Based on WordPiece."),Sc.forEach(o),Ms=h(A),at=r(A,"P",{});var Fr=d(at);Cs=a(Fr,"This tokenizer inherits from "),uo=r(Fr,"A",{href:!0});var Dc=d(uo);Ss=a(Dc,"PreTrainedTokenizer"),Dc.forEach(o),Ds=a(Fr,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Fr.forEach(o),Os=h(A),V=r(A,"DIV",{class:!0});var xo=d(V);_(dt.$$.fragment,xo),Ls=h(xo),on=r(xo,"P",{});var Oc=d(on);js=a(Oc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Oc.forEach(o),As=h(xo),it=r(xo,"UL",{});var xr=d(it);mo=r(xr,"LI",{});var pc=d(mo);Is=a(pc,"single sequence: "),nn=r(pc,"CODE",{});var Lc=d(nn);Bs=a(Lc,"[CLS] X [SEP]"),Lc.forEach(o),pc.forEach(o),Gs=h(xr),fo=r(xr,"LI",{});var uc=d(fo);Ws=a(uc,"pair of sequences: "),rn=r(uc,"CODE",{});var jc=d(rn);Hs=a(jc,"[CLS] A [SEP] B [SEP]"),jc.forEach(o),uc.forEach(o),xr.forEach(o),xo.forEach(o),Vs=h(A),Oe=r(A,"DIV",{class:!0});var Er=d(Oe);_(ct.$$.fragment,Er),Us=h(Er),sn=r(Er,"P",{});var Ac=d(sn);Rs=a(Ac,"Converts a sequence of tokens (string) in a single string."),Ac.forEach(o),Er.forEach(o),Js=h(A),B=r(A,"DIV",{class:!0});var Ke=d(B);_(ht.$$.fragment,Ke),Ys=h(Ke),an=r(Ke,"P",{});var Ic=d(an);Ks=a(Ic,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Ic.forEach(o),Qs=h(Ke),_(lt.$$.fragment,Ke),Xs=h(Ke),ae=r(Ke,"P",{});var Eo=d(ae);Zs=a(Eo,"If "),dn=r(Eo,"CODE",{});var Bc=d(dn);ea=a(Bc,"token_ids_1"),Bc.forEach(o),ta=a(Eo," is "),cn=r(Eo,"CODE",{});var Gc=d(cn);oa=a(Gc,"None"),Gc.forEach(o),na=a(Eo,", this method only returns the first portion of the mask (0s)."),Eo.forEach(o),Ke.forEach(o),ra=h(A),Le=r(A,"DIV",{class:!0});var Mr=d(Le);_(pt.$$.fragment,Mr),sa=h(Mr),ut=r(Mr,"P",{});var Cr=d(ut);aa=a(Cr,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),hn=r(Cr,"CODE",{});var Wc=d(hn);da=a(Wc,"prepare_for_model"),Wc.forEach(o),ia=a(Cr," method."),Cr.forEach(o),Mr.forEach(o),A.forEach(o),dr=h(t),de=r(t,"H2",{class:!0});var Sr=d(de);je=r(Sr,"A",{id:!0,class:!0,href:!0});var Hc=d(je);ln=r(Hc,"SPAN",{});var Vc=d(ln);_(mt.$$.fragment,Vc),Vc.forEach(o),Hc.forEach(o),ca=h(Sr),pn=r(Sr,"SPAN",{});var Uc=d(pn);ha=a(Uc,"ProphetNet specific outputs"),Uc.forEach(o),Sr.forEach(o),ir=h(t),ie=r(t,"DIV",{class:!0});var Dr=d(ie);_(ft.$$.fragment,Dr),la=h(Dr),un=r(Dr,"P",{});var Rc=d(un);pa=a(Rc,"Base class for sequence-to-sequence language models outputs."),Rc.forEach(o),Dr.forEach(o),cr=h(t),ce=r(t,"DIV",{class:!0});var Or=d(ce);_(_t.$$.fragment,Or),ua=h(Or),mn=r(Or,"P",{});var Jc=d(mn);ma=a(Jc,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Jc.forEach(o),Or.forEach(o),hr=h(t),he=r(t,"DIV",{class:!0});var Lr=d(he);_(gt.$$.fragment,Lr),fa=h(Lr),fn=r(Lr,"P",{});var Yc=d(fn);_a=a(Yc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Yc.forEach(o),Lr.forEach(o),lr=h(t),le=r(t,"DIV",{class:!0});var jr=d(le);_(vt.$$.fragment,jr),ga=h(jr),_n=r(jr,"P",{});var Kc=d(_n);va=a(Kc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Kc.forEach(o),jr.forEach(o),pr=h(t),pe=r(t,"H2",{class:!0});var Ar=d(pe);Ae=r(Ar,"A",{id:!0,class:!0,href:!0});var Qc=d(Ae);gn=r(Qc,"SPAN",{});var Xc=d(gn);_(kt.$$.fragment,Xc),Xc.forEach(o),Qc.forEach(o),ka=h(Ar),vn=r(Ar,"SPAN",{});var Zc=d(vn);Ta=a(Zc,"ProphetNetModel"),Zc.forEach(o),Ar.forEach(o),ur=h(t),E=r(t,"DIV",{class:!0});var U=d(E);_(Tt.$$.fragment,U),ba=h(U),bt=r(U,"P",{});var Ir=d(bt);wa=a(Ir,`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=r(Ir,"A",{href:!0});var eh=d(_o);ya=a(eh,"PreTrainedModel"),eh.forEach(o),Pa=a(Ir,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Ir.forEach(o),Na=h(U),ue=r(U,"P",{});var Mo=d(ue);za=a(Mo,"Original ProphetNet code can be found at <"),wt=r(Mo,"A",{href:!0,rel:!0});var th=d(wt);qa=a(th,"https://github.com/microsoft/ProphetNet>"),th.forEach(o),$a=a(Mo,` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),kn=r(Mo,"CODE",{});var oh=d(kn);Fa=a(oh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),oh.forEach(o),xa=a(Mo,"."),Mo.forEach(o),Ea=h(U),yt=r(U,"P",{});var Br=d(yt);Ma=a(Br,"This model is a PyTorch "),Pt=r(Br,"A",{href:!0,rel:!0});var nh=d(Pt);Ca=a(nh,"torch.nn.Module"),nh.forEach(o),Sa=a(Br,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Br.forEach(o),Da=h(U),S=r(U,"DIV",{class:!0});var R=d(S);_(Nt.$$.fragment,R),Oa=h(R),me=r(R,"P",{});var Co=d(me);La=a(Co,"The "),go=r(Co,"A",{href:!0});var rh=d(go);ja=a(rh,"ProphetNetModel"),rh.forEach(o),Aa=a(Co," forward method, overrides the "),Tn=r(Co,"CODE",{});var sh=d(Tn);Ia=a(sh,"__call__"),sh.forEach(o),Ba=a(Co," special method."),Co.forEach(o),Ga=h(R),_(Ie.$$.fragment,R),Wa=h(R),bn=r(R,"P",{});var ah=d(bn);Ha=a(ah,"Example:"),ah.forEach(o),Va=h(R),_(zt.$$.fragment,R),R.forEach(o),U.forEach(o),mr=h(t),fe=r(t,"H2",{class:!0});var Gr=d(fe);Be=r(Gr,"A",{id:!0,class:!0,href:!0});var dh=d(Be);wn=r(dh,"SPAN",{});var ih=d(wn);_(qt.$$.fragment,ih),ih.forEach(o),dh.forEach(o),Ua=h(Gr),yn=r(Gr,"SPAN",{});var ch=d(yn);Ra=a(ch,"ProphetNetEncoder"),ch.forEach(o),Gr.forEach(o),fr=h(t),F=r(t,"DIV",{class:!0});var G=d(F);_($t.$$.fragment,G),Ja=h(G),Ft=r(G,"P",{});var Wr=d(Ft);Ya=a(Wr,`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=r(Wr,"A",{href:!0});var hh=d(vo);Ka=a(hh,"PreTrainedModel"),hh.forEach(o),Qa=a(Wr,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Wr.forEach(o),Xa=h(G),_e=r(G,"P",{});var So=d(_e);Za=a(So,"Original ProphetNet code can be found at <"),xt=r(So,"A",{href:!0,rel:!0});var lh=d(xt);ed=a(lh,"https://github.com/microsoft/ProphetNet>"),lh.forEach(o),td=a(So,` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Pn=r(So,"CODE",{});var ph=d(Pn);od=a(ph,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),ph.forEach(o),nd=a(So,"."),So.forEach(o),rd=h(G),Et=r(G,"P",{});var Hr=d(Et);sd=a(Hr,"This model is a PyTorch "),Mt=r(Hr,"A",{href:!0,rel:!0});var uh=d(Mt);ad=a(uh,"torch.nn.Module"),uh.forEach(o),dd=a(Hr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Hr.forEach(o),id=h(G),ge=r(G,"P",{});var Do=d(ge);cd=a(Do,"word"),ve=r(Do,"EM",{});var Oo=d(ve);hd=a(Oo,"embeddings  ("),Nn=r(Oo,"CODE",{});var mh=d(Nn);ld=a(mh,"torch.nn.Embeddings"),mh.forEach(o),pd=a(Oo," of shape "),zn=r(Oo,"CODE",{});var fh=d(zn);ud=a(fh,"(config.vocab_size, config.hidden_size)"),fh.forEach(o),md=a(Oo,", _optional"),Oo.forEach(o),fd=a(Do,`):
The word embedding parameters. This can be used to initialize `),ko=r(Do,"A",{href:!0});var _h=d(ko);_d=a(_h,"ProphetNetEncoder"),_h.forEach(o),gd=a(Do,` with
pre-defined word embeddings instead of randomly initialized word embeddings.`),Do.forEach(o),vd=h(G),D=r(G,"DIV",{class:!0});var J=d(D);_(Ct.$$.fragment,J),kd=h(J),ke=r(J,"P",{});var Lo=d(ke);Td=a(Lo,"The "),To=r(Lo,"A",{href:!0});var gh=d(To);bd=a(gh,"ProphetNetEncoder"),gh.forEach(o),wd=a(Lo," forward method, overrides the "),qn=r(Lo,"CODE",{});var vh=d(qn);yd=a(vh,"__call__"),vh.forEach(o),Pd=a(Lo," special method."),Lo.forEach(o),Nd=h(J),_(Ge.$$.fragment,J),zd=h(J),$n=r(J,"P",{});var kh=d($n);qd=a(kh,"Example:"),kh.forEach(o),$d=h(J),_(St.$$.fragment,J),J.forEach(o),G.forEach(o),_r=h(t),Te=r(t,"H2",{class:!0});var Vr=d(Te);We=r(Vr,"A",{id:!0,class:!0,href:!0});var Th=d(We);Fn=r(Th,"SPAN",{});var bh=d(Fn);_(Dt.$$.fragment,bh),bh.forEach(o),Th.forEach(o),Fd=h(Vr),xn=r(Vr,"SPAN",{});var wh=d(xn);xd=a(wh,"ProphetNetDecoder"),wh.forEach(o),Vr.forEach(o),gr=h(t),x=r(t,"DIV",{class:!0});var W=d(x);_(Ot.$$.fragment,W),Ed=h(W),Lt=r(W,"P",{});var Ur=d(Lt);Md=a(Ur,`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=r(Ur,"A",{href:!0});var yh=d(bo);Cd=a(yh,"PreTrainedModel"),yh.forEach(o),Sd=a(Ur,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Ur.forEach(o),Dd=h(W),be=r(W,"P",{});var jo=d(be);Od=a(jo,"Original ProphetNet code can be found at <"),jt=r(jo,"A",{href:!0,rel:!0});var Ph=d(jt);Ld=a(Ph,"https://github.com/microsoft/ProphetNet>"),Ph.forEach(o),jd=a(jo,` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),En=r(jo,"CODE",{});var Nh=d(En);Ad=a(Nh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Nh.forEach(o),Id=a(jo,"."),jo.forEach(o),Bd=h(W),At=r(W,"P",{});var Rr=d(At);Gd=a(Rr,"This model is a PyTorch "),It=r(Rr,"A",{href:!0,rel:!0});var zh=d(It);Wd=a(zh,"torch.nn.Module"),zh.forEach(o),Hd=a(Rr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Rr.forEach(o),Vd=h(W),we=r(W,"P",{});var Ao=d(we);Ud=a(Ao,"word"),ye=r(Ao,"EM",{});var Io=d(ye);Rd=a(Io,"embeddings  ("),Mn=r(Io,"CODE",{});var qh=d(Mn);Jd=a(qh,"torch.nn.Embeddings"),qh.forEach(o),Yd=a(Io," of shape "),Cn=r(Io,"CODE",{});var $h=d(Cn);Kd=a($h,"(config.vocab_size, config.hidden_size)"),$h.forEach(o),Qd=a(Io,", _optional"),Io.forEach(o),Xd=a(Ao,`):
The word embedding parameters. This can be used to initialize `),wo=r(Ao,"A",{href:!0});var Fh=d(wo);Zd=a(Fh,"ProphetNetEncoder"),Fh.forEach(o),ei=a(Ao,` with
pre-defined word embeddings instead of randomly initialized word embeddings.`),Ao.forEach(o),ti=h(W),O=r(W,"DIV",{class:!0});var Y=d(O);_(Bt.$$.fragment,Y),oi=h(Y),Pe=r(Y,"P",{});var Bo=d(Pe);ni=a(Bo,"The "),yo=r(Bo,"A",{href:!0});var xh=d(yo);ri=a(xh,"ProphetNetDecoder"),xh.forEach(o),si=a(Bo," forward method, overrides the "),Sn=r(Bo,"CODE",{});var Eh=d(Sn);ai=a(Eh,"__call__"),Eh.forEach(o),di=a(Bo," special method."),Bo.forEach(o),ii=h(Y),_(He.$$.fragment,Y),ci=h(Y),Dn=r(Y,"P",{});var Mh=d(Dn);hi=a(Mh,"Example:"),Mh.forEach(o),li=h(Y),_(Gt.$$.fragment,Y),Y.forEach(o),W.forEach(o),vr=h(t),Ne=r(t,"H2",{class:!0});var Jr=d(Ne);Ve=r(Jr,"A",{id:!0,class:!0,href:!0});var Ch=d(Ve);On=r(Ch,"SPAN",{});var Sh=d(On);_(Wt.$$.fragment,Sh),Sh.forEach(o),Ch.forEach(o),pi=h(Jr),Ln=r(Jr,"SPAN",{});var Dh=d(Ln);ui=a(Dh,"ProphetNetForConditionalGeneration"),Dh.forEach(o),Jr.forEach(o),kr=h(t),M=r(t,"DIV",{class:!0});var K=d(M);_(Ht.$$.fragment,K),mi=h(K),Vt=r(K,"P",{});var Yr=d(Vt);fi=a(Yr,`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=r(Yr,"A",{href:!0});var Oh=d(Po);_i=a(Oh,"PreTrainedModel"),Oh.forEach(o),gi=a(Yr,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Yr.forEach(o),vi=h(K),ze=r(K,"P",{});var Go=d(ze);ki=a(Go,"Original ProphetNet code can be found at <"),Ut=r(Go,"A",{href:!0,rel:!0});var Lh=d(Ut);Ti=a(Lh,"https://github.com/microsoft/ProphetNet>"),Lh.forEach(o),bi=a(Go,` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),jn=r(Go,"CODE",{});var jh=d(jn);wi=a(jh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jh.forEach(o),yi=a(Go,"."),Go.forEach(o),Pi=h(K),Rt=r(K,"P",{});var Kr=d(Rt);Ni=a(Kr,"This model is a PyTorch "),Jt=r(Kr,"A",{href:!0,rel:!0});var Ah=d(Jt);zi=a(Ah,"torch.nn.Module"),Ah.forEach(o),qi=a(Kr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Kr.forEach(o),$i=h(K),L=r(K,"DIV",{class:!0});var Q=d(L);_(Yt.$$.fragment,Q),Fi=h(Q),qe=r(Q,"P",{});var Wo=d(qe);xi=a(Wo,"The "),No=r(Wo,"A",{href:!0});var Ih=d(No);Ei=a(Ih,"ProphetNetForConditionalGeneration"),Ih.forEach(o),Mi=a(Wo," forward method, overrides the "),An=r(Wo,"CODE",{});var Bh=d(An);Ci=a(Bh,"__call__"),Bh.forEach(o),Si=a(Wo," special method."),Wo.forEach(o),Di=h(Q),_(Ue.$$.fragment,Q),Oi=h(Q),In=r(Q,"P",{});var Gh=d(In);Li=a(Gh,"Example:"),Gh.forEach(o),ji=h(Q),_(Kt.$$.fragment,Q),Q.forEach(o),K.forEach(o),Tr=h(t),$e=r(t,"H2",{class:!0});var Qr=d($e);Re=r(Qr,"A",{id:!0,class:!0,href:!0});var Wh=d(Re);Bn=r(Wh,"SPAN",{});var Hh=d(Bn);_(Qt.$$.fragment,Hh),Hh.forEach(o),Wh.forEach(o),Ai=h(Qr),Gn=r(Qr,"SPAN",{});var Vh=d(Gn);Ii=a(Vh,"ProphetNetForCausalLM"),Vh.forEach(o),Qr.forEach(o),br=h(t),C=r(t,"DIV",{class:!0});var X=d(C);_(Xt.$$.fragment,X),Bi=h(X),Zt=r(X,"P",{});var Xr=d(Zt);Gi=a(Xr,`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),zo=r(Xr,"A",{href:!0});var Uh=d(zo);Wi=a(Uh,"PreTrainedModel"),Uh.forEach(o),Hi=a(Xr,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Xr.forEach(o),Vi=h(X),Fe=r(X,"P",{});var Ho=d(Fe);Ui=a(Ho,"Original ProphetNet code can be found at <"),eo=r(Ho,"A",{href:!0,rel:!0});var Rh=d(eo);Ri=a(Rh,"https://github.com/microsoft/ProphetNet>"),Rh.forEach(o),Ji=a(Ho,` . Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Wn=r(Ho,"CODE",{});var Jh=d(Wn);Yi=a(Jh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Jh.forEach(o),Ki=a(Ho,"."),Ho.forEach(o),Qi=h(X),to=r(X,"P",{});var Zr=d(to);Xi=a(Zr,"This model is a PyTorch "),oo=r(Zr,"A",{href:!0,rel:!0});var Yh=d(oo);Zi=a(Yh,"torch.nn.Module"),Yh.forEach(o),ec=a(Zr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Zr.forEach(o),tc=h(X),j=r(X,"DIV",{class:!0});var Z=d(j);_(no.$$.fragment,Z),oc=h(Z),xe=r(Z,"P",{});var Vo=d(xe);nc=a(Vo,"The "),qo=r(Vo,"A",{href:!0});var Kh=d(qo);rc=a(Kh,"ProphetNetForCausalLM"),Kh.forEach(o),sc=a(Vo," forward method, overrides the "),Hn=r(Vo,"CODE",{});var Qh=d(Hn);ac=a(Qh,"__call__"),Qh.forEach(o),dc=a(Vo," special method."),Vo.forEach(o),ic=h(Z),_(Je.$$.fragment,Z),cc=h(Z),Vn=r(Z,"P",{});var Xh=d(Vn);hc=a(Xh,"Example:"),Xh.forEach(o),lc=h(Z),_(ro.$$.fragment,Z),Z.forEach(o),X.forEach(o),this.h()},h(){i(u,"name","hf:doc:metadata"),i(u,"content",JSON.stringify(cl)),i(y,"id","prophetnet"),i(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(y,"href","#prophetnet"),i(m,"class","relative group"),i(Qe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(Qe,"rel","nofollow"),i(Ee,"id","overview"),i(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ee,"href","#overview"),i(oe,"class","relative group"),i(Ze,"href","https://arxiv.org/abs/2001.04063"),i(Ze,"rel","nofollow"),i(et,"href","https://github.com/microsoft/ProphetNet"),i(et,"rel","nofollow"),i(Se,"id","transformers.ProphetNetConfig"),i(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Se,"href","#transformers.ProphetNetConfig"),i(ne,"class","relative group"),i(ho,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(lo,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),i(po,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),i(H,"class","docstring"),i(De,"id","transformers.ProphetNetTokenizer"),i(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(De,"href","#transformers.ProphetNetTokenizer"),i(se,"class","relative group"),i(uo,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(V,"class","docstring"),i(Oe,"class","docstring"),i(B,"class","docstring"),i(Le,"class","docstring"),i($,"class","docstring"),i(je,"id","transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(je,"href","#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(de,"class","relative group"),i(ie,"class","docstring"),i(ce,"class","docstring"),i(he,"class","docstring"),i(le,"class","docstring"),i(Ae,"id","transformers.ProphetNetModel"),i(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ae,"href","#transformers.ProphetNetModel"),i(pe,"class","relative group"),i(_o,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),i(wt,"href","https://github.com/microsoft/ProphetNet%3E"),i(wt,"rel","nofollow"),i(Pt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Pt,"rel","nofollow"),i(go,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(S,"class","docstring"),i(E,"class","docstring"),i(Be,"id","transformers.ProphetNetEncoder"),i(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Be,"href","#transformers.ProphetNetEncoder"),i(fe,"class","relative group"),i(vo,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),i(xt,"href","https://github.com/microsoft/ProphetNet%3E"),i(xt,"rel","nofollow"),i(Mt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Mt,"rel","nofollow"),i(ko,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(To,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(D,"class","docstring"),i(F,"class","docstring"),i(We,"id","transformers.ProphetNetDecoder"),i(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(We,"href","#transformers.ProphetNetDecoder"),i(Te,"class","relative group"),i(bo,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),i(jt,"href","https://github.com/microsoft/ProphetNet%3E"),i(jt,"rel","nofollow"),i(It,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(It,"rel","nofollow"),i(wo,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(yo,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(O,"class","docstring"),i(x,"class","docstring"),i(Ve,"id","transformers.ProphetNetForConditionalGeneration"),i(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ve,"href","#transformers.ProphetNetForConditionalGeneration"),i(Ne,"class","relative group"),i(Po,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),i(Ut,"href","https://github.com/microsoft/ProphetNet%3E"),i(Ut,"rel","nofollow"),i(Jt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Jt,"rel","nofollow"),i(No,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(L,"class","docstring"),i(M,"class","docstring"),i(Re,"id","transformers.ProphetNetForCausalLM"),i(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Re,"href","#transformers.ProphetNetForCausalLM"),i($e,"class","relative group"),i(zo,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),i(eo,"href","https://github.com/microsoft/ProphetNet%3E"),i(eo,"rel","nofollow"),i(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(oo,"rel","nofollow"),i(qo,"href","/docs/transformers/v4.14.1/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(j,"class","docstring"),i(C,"class","docstring")},m(t,l){e(document.head,u),p(t,P,l),p(t,m,l),e(m,y),e(y,N),g(w,N,null),e(m,b),e(m,z),e(z,es),p(t,Kn,l),p(t,te,l),e(te,Ro),e(Ro,ts),e(te,os),e(te,Qe),e(Qe,ns),e(te,rs),p(t,Qn,l),p(t,oe,l),e(oe,Ee),e(Ee,Jo),g(Xe,Jo,null),e(oe,ss),e(oe,Yo),e(Yo,as),p(t,Xn,l),p(t,Me,l),e(Me,ds),e(Me,Ze),e(Ze,is),e(Me,cs),p(t,Zn,l),p(t,ao,l),e(ao,hs),p(t,er,l),p(t,io,l),e(io,ls),p(t,tr,l),p(t,co,l),e(co,Ko),e(Ko,ps),p(t,or,l),p(t,Ce,l),e(Ce,us),e(Ce,et),e(et,ms),e(Ce,fs),p(t,nr,l),p(t,ne,l),e(ne,Se),e(Se,Qo),g(tt,Qo,null),e(ne,_s),e(ne,Xo),e(Xo,gs),p(t,rr,l),p(t,H,l),g(ot,H,null),e(H,vs),e(H,nt),e(nt,ks),e(nt,ho),e(ho,Ts),e(nt,bs),e(H,ws),e(H,re),e(re,ys),e(re,lo),e(lo,Ps),e(re,Ns),e(re,po),e(po,zs),e(re,qs),p(t,sr,l),p(t,se,l),e(se,De),e(De,Zo),g(rt,Zo,null),e(se,$s),e(se,en),e(en,Fs),p(t,ar,l),p(t,$,l),g(st,$,null),e($,xs),e($,tn),e(tn,Es),e($,Ms),e($,at),e(at,Cs),e(at,uo),e(uo,Ss),e(at,Ds),e($,Os),e($,V),g(dt,V,null),e(V,Ls),e(V,on),e(on,js),e(V,As),e(V,it),e(it,mo),e(mo,Is),e(mo,nn),e(nn,Bs),e(it,Gs),e(it,fo),e(fo,Ws),e(fo,rn),e(rn,Hs),e($,Vs),e($,Oe),g(ct,Oe,null),e(Oe,Us),e(Oe,sn),e(sn,Rs),e($,Js),e($,B),g(ht,B,null),e(B,Ys),e(B,an),e(an,Ks),e(B,Qs),g(lt,B,null),e(B,Xs),e(B,ae),e(ae,Zs),e(ae,dn),e(dn,ea),e(ae,ta),e(ae,cn),e(cn,oa),e(ae,na),e($,ra),e($,Le),g(pt,Le,null),e(Le,sa),e(Le,ut),e(ut,aa),e(ut,hn),e(hn,da),e(ut,ia),p(t,dr,l),p(t,de,l),e(de,je),e(je,ln),g(mt,ln,null),e(de,ca),e(de,pn),e(pn,ha),p(t,ir,l),p(t,ie,l),g(ft,ie,null),e(ie,la),e(ie,un),e(un,pa),p(t,cr,l),p(t,ce,l),g(_t,ce,null),e(ce,ua),e(ce,mn),e(mn,ma),p(t,hr,l),p(t,he,l),g(gt,he,null),e(he,fa),e(he,fn),e(fn,_a),p(t,lr,l),p(t,le,l),g(vt,le,null),e(le,ga),e(le,_n),e(_n,va),p(t,pr,l),p(t,pe,l),e(pe,Ae),e(Ae,gn),g(kt,gn,null),e(pe,ka),e(pe,vn),e(vn,Ta),p(t,ur,l),p(t,E,l),g(Tt,E,null),e(E,ba),e(E,bt),e(bt,wa),e(bt,_o),e(_o,ya),e(bt,Pa),e(E,Na),e(E,ue),e(ue,za),e(ue,wt),e(wt,qa),e(ue,$a),e(ue,kn),e(kn,Fa),e(ue,xa),e(E,Ea),e(E,yt),e(yt,Ma),e(yt,Pt),e(Pt,Ca),e(yt,Sa),e(E,Da),e(E,S),g(Nt,S,null),e(S,Oa),e(S,me),e(me,La),e(me,go),e(go,ja),e(me,Aa),e(me,Tn),e(Tn,Ia),e(me,Ba),e(S,Ga),g(Ie,S,null),e(S,Wa),e(S,bn),e(bn,Ha),e(S,Va),g(zt,S,null),p(t,mr,l),p(t,fe,l),e(fe,Be),e(Be,wn),g(qt,wn,null),e(fe,Ua),e(fe,yn),e(yn,Ra),p(t,fr,l),p(t,F,l),g($t,F,null),e(F,Ja),e(F,Ft),e(Ft,Ya),e(Ft,vo),e(vo,Ka),e(Ft,Qa),e(F,Xa),e(F,_e),e(_e,Za),e(_e,xt),e(xt,ed),e(_e,td),e(_e,Pn),e(Pn,od),e(_e,nd),e(F,rd),e(F,Et),e(Et,sd),e(Et,Mt),e(Mt,ad),e(Et,dd),e(F,id),e(F,ge),e(ge,cd),e(ge,ve),e(ve,hd),e(ve,Nn),e(Nn,ld),e(ve,pd),e(ve,zn),e(zn,ud),e(ve,md),e(ge,fd),e(ge,ko),e(ko,_d),e(ge,gd),e(F,vd),e(F,D),g(Ct,D,null),e(D,kd),e(D,ke),e(ke,Td),e(ke,To),e(To,bd),e(ke,wd),e(ke,qn),e(qn,yd),e(ke,Pd),e(D,Nd),g(Ge,D,null),e(D,zd),e(D,$n),e($n,qd),e(D,$d),g(St,D,null),p(t,_r,l),p(t,Te,l),e(Te,We),e(We,Fn),g(Dt,Fn,null),e(Te,Fd),e(Te,xn),e(xn,xd),p(t,gr,l),p(t,x,l),g(Ot,x,null),e(x,Ed),e(x,Lt),e(Lt,Md),e(Lt,bo),e(bo,Cd),e(Lt,Sd),e(x,Dd),e(x,be),e(be,Od),e(be,jt),e(jt,Ld),e(be,jd),e(be,En),e(En,Ad),e(be,Id),e(x,Bd),e(x,At),e(At,Gd),e(At,It),e(It,Wd),e(At,Hd),e(x,Vd),e(x,we),e(we,Ud),e(we,ye),e(ye,Rd),e(ye,Mn),e(Mn,Jd),e(ye,Yd),e(ye,Cn),e(Cn,Kd),e(ye,Qd),e(we,Xd),e(we,wo),e(wo,Zd),e(we,ei),e(x,ti),e(x,O),g(Bt,O,null),e(O,oi),e(O,Pe),e(Pe,ni),e(Pe,yo),e(yo,ri),e(Pe,si),e(Pe,Sn),e(Sn,ai),e(Pe,di),e(O,ii),g(He,O,null),e(O,ci),e(O,Dn),e(Dn,hi),e(O,li),g(Gt,O,null),p(t,vr,l),p(t,Ne,l),e(Ne,Ve),e(Ve,On),g(Wt,On,null),e(Ne,pi),e(Ne,Ln),e(Ln,ui),p(t,kr,l),p(t,M,l),g(Ht,M,null),e(M,mi),e(M,Vt),e(Vt,fi),e(Vt,Po),e(Po,_i),e(Vt,gi),e(M,vi),e(M,ze),e(ze,ki),e(ze,Ut),e(Ut,Ti),e(ze,bi),e(ze,jn),e(jn,wi),e(ze,yi),e(M,Pi),e(M,Rt),e(Rt,Ni),e(Rt,Jt),e(Jt,zi),e(Rt,qi),e(M,$i),e(M,L),g(Yt,L,null),e(L,Fi),e(L,qe),e(qe,xi),e(qe,No),e(No,Ei),e(qe,Mi),e(qe,An),e(An,Ci),e(qe,Si),e(L,Di),g(Ue,L,null),e(L,Oi),e(L,In),e(In,Li),e(L,ji),g(Kt,L,null),p(t,Tr,l),p(t,$e,l),e($e,Re),e(Re,Bn),g(Qt,Bn,null),e($e,Ai),e($e,Gn),e(Gn,Ii),p(t,br,l),p(t,C,l),g(Xt,C,null),e(C,Bi),e(C,Zt),e(Zt,Gi),e(Zt,zo),e(zo,Wi),e(Zt,Hi),e(C,Vi),e(C,Fe),e(Fe,Ui),e(Fe,eo),e(eo,Ri),e(Fe,Ji),e(Fe,Wn),e(Wn,Yi),e(Fe,Ki),e(C,Qi),e(C,to),e(to,Xi),e(to,oo),e(oo,Zi),e(to,ec),e(C,tc),e(C,j),g(no,j,null),e(j,oc),e(j,xe),e(xe,nc),e(xe,qo),e(qo,rc),e(xe,sc),e(xe,Hn),e(Hn,ac),e(xe,dc),e(j,ic),g(Je,j,null),e(j,cc),e(j,Vn),e(Vn,hc),e(j,lc),g(ro,j,null),wr=!0},p(t,[l]){const so={};l&2&&(so.$$scope={dirty:l,ctx:t}),Ie.$set(so);const Un={};l&2&&(Un.$$scope={dirty:l,ctx:t}),Ge.$set(Un);const Rn={};l&2&&(Rn.$$scope={dirty:l,ctx:t}),He.$set(Rn);const Jn={};l&2&&(Jn.$$scope={dirty:l,ctx:t}),Ue.$set(Jn);const Ye={};l&2&&(Ye.$$scope={dirty:l,ctx:t}),Je.$set(Ye)},i(t){wr||(v(w.$$.fragment,t),v(Xe.$$.fragment,t),v(tt.$$.fragment,t),v(ot.$$.fragment,t),v(rt.$$.fragment,t),v(st.$$.fragment,t),v(dt.$$.fragment,t),v(ct.$$.fragment,t),v(ht.$$.fragment,t),v(lt.$$.fragment,t),v(pt.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(_t.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(Nt.$$.fragment,t),v(Ie.$$.fragment,t),v(zt.$$.fragment,t),v(qt.$$.fragment,t),v($t.$$.fragment,t),v(Ct.$$.fragment,t),v(Ge.$$.fragment,t),v(St.$$.fragment,t),v(Dt.$$.fragment,t),v(Ot.$$.fragment,t),v(Bt.$$.fragment,t),v(He.$$.fragment,t),v(Gt.$$.fragment,t),v(Wt.$$.fragment,t),v(Ht.$$.fragment,t),v(Yt.$$.fragment,t),v(Ue.$$.fragment,t),v(Kt.$$.fragment,t),v(Qt.$$.fragment,t),v(Xt.$$.fragment,t),v(no.$$.fragment,t),v(Je.$$.fragment,t),v(ro.$$.fragment,t),wr=!0)},o(t){k(w.$$.fragment,t),k(Xe.$$.fragment,t),k(tt.$$.fragment,t),k(ot.$$.fragment,t),k(rt.$$.fragment,t),k(st.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(ht.$$.fragment,t),k(lt.$$.fragment,t),k(pt.$$.fragment,t),k(mt.$$.fragment,t),k(ft.$$.fragment,t),k(_t.$$.fragment,t),k(gt.$$.fragment,t),k(vt.$$.fragment,t),k(kt.$$.fragment,t),k(Tt.$$.fragment,t),k(Nt.$$.fragment,t),k(Ie.$$.fragment,t),k(zt.$$.fragment,t),k(qt.$$.fragment,t),k($t.$$.fragment,t),k(Ct.$$.fragment,t),k(Ge.$$.fragment,t),k(St.$$.fragment,t),k(Dt.$$.fragment,t),k(Ot.$$.fragment,t),k(Bt.$$.fragment,t),k(He.$$.fragment,t),k(Gt.$$.fragment,t),k(Wt.$$.fragment,t),k(Ht.$$.fragment,t),k(Yt.$$.fragment,t),k(Ue.$$.fragment,t),k(Kt.$$.fragment,t),k(Qt.$$.fragment,t),k(Xt.$$.fragment,t),k(no.$$.fragment,t),k(Je.$$.fragment,t),k(ro.$$.fragment,t),wr=!1},d(t){o(u),t&&o(P),t&&o(m),T(w),t&&o(Kn),t&&o(te),t&&o(Qn),t&&o(oe),T(Xe),t&&o(Xn),t&&o(Me),t&&o(Zn),t&&o(ao),t&&o(er),t&&o(io),t&&o(tr),t&&o(co),t&&o(or),t&&o(Ce),t&&o(nr),t&&o(ne),T(tt),t&&o(rr),t&&o(H),T(ot),t&&o(sr),t&&o(se),T(rt),t&&o(ar),t&&o($),T(st),T(dt),T(ct),T(ht),T(lt),T(pt),t&&o(dr),t&&o(de),T(mt),t&&o(ir),t&&o(ie),T(ft),t&&o(cr),t&&o(ce),T(_t),t&&o(hr),t&&o(he),T(gt),t&&o(lr),t&&o(le),T(vt),t&&o(pr),t&&o(pe),T(kt),t&&o(ur),t&&o(E),T(Tt),T(Nt),T(Ie),T(zt),t&&o(mr),t&&o(fe),T(qt),t&&o(fr),t&&o(F),T($t),T(Ct),T(Ge),T(St),t&&o(_r),t&&o(Te),T(Dt),t&&o(gr),t&&o(x),T(Ot),T(Bt),T(He),T(Gt),t&&o(vr),t&&o(Ne),T(Wt),t&&o(kr),t&&o(M),T(Ht),T(Yt),T(Ue),T(Kt),t&&o(Tr),t&&o($e),T(Qt),t&&o(br),t&&o(C),T(Xt),T(no),T(Je),T(ro)}}}const cl={local:"prophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.ProphetNetConfig",title:"ProphetNetConfig"},{local:"transformers.ProphetNetTokenizer",title:"ProphetNetTokenizer"},{local:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",title:"ProphetNet specific outputs"},{local:"transformers.ProphetNetModel",title:"ProphetNetModel"},{local:"transformers.ProphetNetEncoder",title:"ProphetNetEncoder"},{local:"transformers.ProphetNetDecoder",title:"ProphetNetDecoder"},{local:"transformers.ProphetNetForConditionalGeneration",title:"ProphetNetForConditionalGeneration"},{local:"transformers.ProphetNetForCausalLM",title:"ProphetNetForCausalLM"}],title:"ProphetNet"};function hl(I,u,P){let{fw:m}=u;return I.$$set=y=>{"fw"in y&&P(0,m=y.fw)},[m]}class gl extends Zh{constructor(u){super();el(this,u,hl,il,tl,{fw:0})}}export{gl as default,cl as metadata};
