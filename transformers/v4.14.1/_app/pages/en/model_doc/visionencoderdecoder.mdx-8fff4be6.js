import{S as Np,i as Rp,s as Bp,e as r,k as i,w as k,t as o,L as Gp,c as a,d as n,m as l,a as s,x as j,h as t,b as c,J as e,g as y,y as M,q as D,o as V,B as $}from"../../../chunks/vendor-b1433968.js";import{T as Sp}from"../../../chunks/Tip-c3840994.js";import{D as je}from"../../../chunks/Docstring-ff504c58.js";import{C as tr}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as Cr}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function Up(Ge){let p,L,v,P,N;return{c(){p=r("p"),L=o(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),v=r("code"),P=o("Module"),N=o(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(C){p=a(C,"P",{});var q=s(p);L=t(q,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),v=a(q,"CODE",{});var G=s(v);P=t(G,"Module"),G.forEach(n),N=t(q,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),q.forEach(n)},m(C,q){y(C,p,q),e(p,L),e(p,v),e(v,P),e(p,N)},d(C){C&&n(p)}}}function Wp(Ge){let p,L,v,P,N;return{c(){p=r("p"),L=o(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),v=r("code"),P=o("Module"),N=o(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(C){p=a(C,"P",{});var q=s(p);L=t(q,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),v=a(q,"CODE",{});var G=s(v);P=t(G,"Module"),G.forEach(n),N=t(q,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),q.forEach(n)},m(C,q){y(C,p,q),e(p,L),e(p,v),e(v,P),e(p,N)},d(C){C&&n(p)}}}function Hp(Ge){let p,L,v,P,N,C,q,G,zr,nr,E,qr,Co,Ar,Fr,wt,Lr,Ir,zo,Or,Sr,qo,Nr,Rr,Ao,Br,Gr,Tt,Ur,Wr,Fo,Hr,Zr,Lo,Jr,Yr,Io,Kr,Qr,rr,Me,Xr,Ue,ea,oa,ar,X,ta,Oo,na,ra,So,aa,sa,sr,re,De,xt,We,da,kt,ia,dr,z,He,la,Ve,No,ca,ha,Ro,pa,ma,fa,ae,ga,Bo,ua,_a,Go,va,Ea,ba,jt,ya,wa,Ze,Ta,$e,Je,xa,Ye,ka,Uo,ja,Ma,Da,Pe,Ke,Va,se,$a,Mt,Pa,Ca,Dt,za,qa,ir,de,Ce,Vt,Qe,Aa,$t,Fa,lr,u,Xe,La,ie,Ia,Pt,Oa,Sa,Ct,Na,Ra,Ba,eo,Ga,oo,Ua,Wa,Ha,to,Za,no,Ja,Ya,Ka,zt,Qa,Xa,ro,es,Wo,os,ts,ns,ao,rs,so,as,ss,ds,ee,Ho,is,ls,qt,cs,hs,At,ps,ms,fs,I,io,gs,le,us,Zo,_s,vs,Ft,Es,bs,ys,ze,ws,Lt,Ts,xs,lo,ks,h,co,js,It,Ms,Ds,ce,Vs,Ot,$s,Ps,St,Cs,zs,qs,he,As,Nt,Fs,Ls,Rt,Is,Os,Ss,pe,me,Ns,Bt,Rs,Bs,Gt,Gs,Us,Ws,U,Hs,Ut,Zs,Js,Jo,Ys,Ks,Wt,Qs,Xs,ed,A,od,Ht,td,nd,Zt,rd,ad,Jt,sd,dd,Yt,id,ld,Kt,cd,hd,pd,W,md,Qt,fd,gd,Xt,ud,_d,en,vd,Ed,bd,fe,H,yd,on,wd,Td,tn,xd,kd,nn,jd,Md,Dd,Z,Vd,rn,$d,Pd,Yo,Cd,zd,an,qd,Ad,Fd,F,Ld,sn,Id,Od,dn,Sd,Nd,ln,Rd,Bd,cn,Gd,Ud,hn,Wd,Hd,Zd,ge,Jd,pn,Yd,Kd,mn,Qd,Xd,ei,ue,oi,fn,ti,ni,gn,ri,ai,si,_e,un,di,ii,_n,li,ci,vn,hi,pi,ho,mi,En,fi,gi,ui,bn,_i,vi,po,cr,ve,qe,yn,mo,Ei,wn,bi,hr,_,fo,yi,Ee,wi,Tn,Ti,xi,xn,ki,ji,Mi,go,Di,uo,Vi,$i,Pi,_o,Ci,vo,zi,qi,Ai,kn,Fi,Li,Eo,Ii,Ko,Oi,Si,Ni,bo,Ri,yo,Bi,Gi,Ui,oe,Qo,Wi,Hi,jn,Zi,Ji,Mn,Yi,Ki,Qi,O,wo,Xi,be,el,Xo,ol,tl,Dn,nl,rl,al,Ae,sl,Vn,dl,il,To,ll,m,xo,cl,$n,hl,pl,ye,ml,Pn,fl,gl,Cn,ul,_l,vl,ko,we,El,zn,bl,yl,qn,wl,Tl,xl,J,kl,An,jl,Ml,et,Dl,Vl,Fn,$l,Pl,Cl,Y,zl,Ln,ql,Al,In,Fl,Ll,On,Il,Ol,Sl,jo,K,Nl,Sn,Rl,Bl,Nn,Gl,Ul,Rn,Wl,Hl,Zl,Q,Jl,Bn,Yl,Kl,ot,Ql,Xl,Gn,ec,oc,tc,Te,nc,Un,rc,ac,Wn,sc,dc,ic,xe,lc,Hn,cc,hc,Zn,pc,mc,fc,ke,Jn,gc,uc,Yn,_c,vc,Kn,Ec,bc,Mo,yc,Qn,wc,Tc,xc,Xn,kc,jc,Do,pr;return C=new Cr({}),We=new Cr({}),He=new je({props:{name:"class transformers.VisionEncoderDecoderConfig",anchor:"transformers.VisionEncoderDecoderConfig",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L27",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments. Notably:</p>
<ul>
<li><strong>encoder</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration
object that defines the encoder config.</li>
<li><strong>decoder</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration
object that defines the decoder config.</li>
</ul>`,name:"kwargs"}]}}),Ze=new tr({props:{code:`from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

# Initializing a ViT & BERT style configuration
config_encoder = ViTConfig()
config_decoder = BertConfig()

config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a ViTBert model from a ViT & bert-base-uncased style configurations
model = VisionEncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder  = model.config.decoder
# set decoder config to causal lm
config_decoder.is_decoder = True
config_decoder.add_cross_attention = True

# Saving the model, including its configuration
model.save_pretrained('my-model')

# loading model and config from pretrained folder
encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained('my-model')
model = VisionEncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViT &amp; BERT style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = ViTConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViTBert model from a ViT &amp; bert-base-uncased style configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = model.config.encoder
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder  = model.config.decoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set decoder config to causal lm</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.add_cross_attention = <span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_decoder_config = VisionEncoderDecoderConfig.from_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>, config=encoder_decoder_config)`}}),Je=new je({props:{name:"from\\_encoder\\_decoder\\_configs",anchor:"transformers.VisionEncoderDecoderConfig.from_encoder_decoder_configs",parameters:[{name:"encoder_config",val:": PretrainedConfig"},{name:"decoder_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L92",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a></p>
`}}),Ke=new je({props:{name:"to\\_dict",anchor:"transformers.VisionEncoderDecoderConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py#L109",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Qe=new Cr({}),Xe=new je({props:{name:"class transformers.VisionEncoderDecoderModel",anchor:"transformers.VisionEncoderDecoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"encoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"decoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L154",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),io=new je({props:{name:"forward",anchor:"transformers.VisionEncoderDecoderModel.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L396",parametersDescription:[{anchor:"transformers.VisionEncoderDecoderModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using a feature extractor (e.g. if you use ViT as the encoder,
you should use <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>). See
<a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">transformers.ViTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>For training, <code>decoder_input_ids</code> are automatically created by the model by shifting the <code>labels</code>
to the right, replacing -100 by the <code>pad_token_id</code> and prepending them with the
<code>decoder_start_token_id</code>.`,name:"decoder_input_ids"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.VisionEncoderDecoderModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor of hidden-states at the output of the last layer of the
encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.VisionEncoderDecoderModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.VisionEncoderDecoderModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code>
indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.VisionEncoderDecoderModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.VisionEncoderDecoderModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.VisionEncoderDecoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VisionEncoderDecoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VisionEncoderDecoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a
plain tuple.
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a _decoder__ prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ze=new Sp({props:{$$slots:{default:[Up]},$$scope:{ctx:Ge}}}),lo=new tr({props:{code:`from transformers import TrOCRProcessor, VisionEncoderDecoderModel
import requests
from PIL import Image
import torch

processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')
model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')

# load image from the IAM dataset
url = "https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# training
model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
model.config.pad_token_id = processor.tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

pixel_values = processor(image, return_tensors="pt").pixel_values
text = "hello world"
labels = processor.tokenizer(text, return_tensors="pt").input_ids
outputs = model(pixel_values=pixel_values, labels=labels)
loss = outputs.loss

# inference (generation)
generated_ids = model.generate(pixel_values)
generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0],`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrOCRProcessor, VisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = TrOCRProcessor.from_pretrained(<span class="hljs-string">&#x27;microsoft/trocr-base-handwritten&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&#x27;microsoft/trocr-base-handwritten&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load image from the IAM dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = processor.tokenizer.cls_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = processor.tokenizer.pad_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.vocab_size = model.config.decoder.vocab_size

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;hello world&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = processor.tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># inference (generation)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]`}}),co=new je({props:{name:"from\\_encoder\\_decoder\\_pretrained",anchor:"transformers.VisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py#L250"}}),po=new tr({props:{code:`from transformers import VisionEncoderDecoderModel
# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized
model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained('google/vit-base-patch16-224-in21k', 'bert-base-uncased')
# saving model after fine-tuning
model.save_pretrained("./vit-bert")
# load fine-tuned model
model = VisionEncoderDecoderModel.from_pretrained("./vit-bert"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;google/vit-base-patch16-224-in21k&#x27;</span>, <span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-bert&quot;</span>)`}}),mo=new Cr({}),fo=new je({props:{name:"class transformers.FlaxVisionEncoderDecoderModel",anchor:"transformers.FlaxVisionEncoderDecoderModel",parameters:[{name:"config",val:": VisionEncoderDecoderConfig"},{name:"input_shape",val:": typing.Optional[typing.Tuple] = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L276",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on
GPUs) and <code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),wo=new je({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__",parameters:[{name:"pixel_values",val:": ndarray"},{name:"decoder_input_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_attention_mask",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_position_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"train",val:": bool = False"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L587",parametersDescription:[{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using the vision model&#x2019;s feature extractor. For example, using
<a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a>. See <a href="/docs/transformers/v4.14.1/en/model_doc/vit#transformers.ViTFeatureExtractor.__call__">transformers.ViTFeatureExtractor.<strong>call</strong>()</a> for
details.`,name:"pixel_values"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.decoder_position_ids",description:`<strong>decoder_position_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
range <code>[0, config.decoder.max_position_embeddings - 1]</code>.`,name:"decoder_position_ids"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxVisionEncoderDecoderModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>FlaxSeq2SeqLMOutput</code> instead
of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>FlaxSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"
>VisionEncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>FlaxSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ae=new Sp({props:{$$slots:{default:[Wp]},$$scope:{ctx:Ge}}}),To=new tr({props:{code:`from transformers import FlaxVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')

# load output tokenizer
tokenizer_output = GPT2Tokenizer.from_pretrained('gpt2')

# initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('vit', 'gpt2')

pixel_values = feature_extractor(images=image, return_tensors="np").pixel_values

# use GPT2's eos_token as the pad as well as eos token
model.config.eos_token_id = model.config.decoder.eos_token_id
model.config.pad_token_id = model.config.eos_token_id

# generation
sequences = model.generate(pixel_values, num_beams=4, max_length=12).sequences

captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=True),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxVisionEncoderDecoderModel, ViTFeatureExtractor, GPT2Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = ViTFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;google/vit-base-patch16-224-in21k&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load output tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_output = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;gpt2&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-gpt2 from pretrained ViT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;vit&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># use GPT2&#x27;s eos_token as the pad as well as eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.eos_token_id = model.config.decoder.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequences = model.generate(pixel_values, num_beams=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">12</span>).sequences

<span class="hljs-meta">&gt;&gt;&gt; </span>captions = tokenizer_output.batch_decode(sequences, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),xo=new je({props:{name:"from\\_encoder\\_decoder\\_pretrained",anchor:"transformers.FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"decoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py#L709"}}),Do=new tr({props:{code:`from transformers import FlaxVisionEncoderDecoderModel
# initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized
model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained('google/vit-base-patch16-224-in21k', 'gpt2')
# saving model after fine-tuning
model.save_pretrained("./vit-gpt2")
# load fine-tuned model
model = FlaxVisionEncoderDecoderModel.from_pretrained("./vit-gpt2"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxVisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a vit-gpt2 from a pretrained ViT and a pretrained GPT2 model. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;google/vit-base-patch16-224-in21k&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./vit-gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxVisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./vit-gpt2&quot;</span>)`}}),{c(){p=r("meta"),L=i(),v=r("h1"),P=r("a"),N=r("span"),k(C.$$.fragment),q=i(),G=r("span"),zr=o("Vision Encoder Decoder Models"),nr=i(),E=r("p"),qr=o("The "),Co=r("a"),Ar=o("VisionEncoderDecoderModel"),Fr=o(` can be used to initialize an image-to-text-sequence model with any
pretrained vision autoencoding model as the encoder (`),wt=r("em"),Lr=o("e.g."),Ir=i(),zo=r("a"),Or=o("ViT"),Sr=o(", "),qo=r("a"),Nr=o("BEiT"),Rr=o(", "),Ao=r("a"),Br=o("DeiT"),Gr=o(`)
and any pretrained language model as the decoder (`),Tt=r("em"),Ur=o("e.g."),Wr=i(),Fo=r("a"),Hr=o("RoBERTa"),Zr=o(", "),Lo=r("a"),Jr=o("GPT2"),Yr=o(", "),Io=r("a"),Kr=o("BERT"),Qr=o(")."),rr=i(),Me=r("p"),Xr=o(`The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for
example) `),Ue=r("a"),ea=o("TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),oa=o(` by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
Zhoujun Li, Furu Wei.`),ar=i(),X=r("p"),ta=o("An example of how to use a "),Oo=r("a"),na=o("VisionEncoderDecoderModel"),ra=o(" for inference can be seen in "),So=r("a"),aa=o("TrOCR"),sa=o("."),sr=i(),re=r("h2"),De=r("a"),xt=r("span"),k(We.$$.fragment),da=i(),kt=r("span"),ia=o("VisionEncoderDecoderConfig"),dr=i(),z=r("div"),k(He.$$.fragment),la=i(),Ve=r("p"),No=r("a"),ca=o("VisionEncoderDecoderConfig"),ha=o(` is the configuration class to store the configuration of a
`),Ro=r("a"),pa=o("VisionEncoderDecoderModel"),ma=o(`. It is used to instantiate a Vision-Encoder-Text-Decoder model
according to the specified arguments, defining the encoder and decoder configs.`),fa=i(),ae=r("p"),ga=o("Configuration objects inherit from "),Bo=r("a"),ua=o("PretrainedConfig"),_a=o(` and can be used to control the model
outputs. Read the documentation from `),Go=r("a"),va=o("PretrainedConfig"),Ea=o(" for more information."),ba=i(),jt=r("p"),ya=o("Examples:"),wa=i(),k(Ze.$$.fragment),Ta=i(),$e=r("div"),k(Je.$$.fragment),xa=i(),Ye=r("p"),ka=o("Instantiate a "),Uo=r("a"),ja=o("VisionEncoderDecoderConfig"),Ma=o(` (or a derived class) from a pre-trained encoder
model configuration and decoder model configuration.`),Da=i(),Pe=r("div"),k(Ke.$$.fragment),Va=i(),se=r("p"),$a=o("Serializes this instance to a Python dictionary. Override the default "),Mt=r("em"),Pa=o("to_dict()"),Ca=o(" from "),Dt=r("em"),za=o("PretrainedConfig"),qa=o("."),ir=i(),de=r("h2"),Ce=r("a"),Vt=r("span"),k(Qe.$$.fragment),Aa=i(),$t=r("span"),Fa=o("VisionEncoderDecoderModel"),lr=i(),u=r("div"),k(Xe.$$.fragment),La=i(),ie=r("p"),Ia=o(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Pt=r("code"),Oa=o("from_pretrained()"),Sa=o(` function and the decoder is loaded via
`),Ct=r("code"),Na=o("from_pretrained()"),Ra=o(` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like image captioning.`),Ba=i(),eo=r("p"),Ga=o(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),oo=r("a"),Ua=o("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),Wa=o(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ha=i(),to=r("p"),Za=o("Additionally, in "),no=r("a"),Ja=o("TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),Ya=o(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Ka=i(),zt=r("p"),Qa=o(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Xa=i(),ro=r("p"),es=o("This model inherits from "),Wo=r("a"),os=o("PreTrainedModel"),ts=o(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ns=i(),ao=r("p"),rs=o("This model is also a PyTorch "),so=r("a"),as=o("torch.nn.Module"),ss=o(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ds=i(),ee=r("p"),Ho=r("a"),is=o("VisionEncoderDecoderModel"),ls=o(` is a generic model class that will be instantiated as a
transformer architecture with one of the base vision model classes of the library as encoder and another one as
decoder when created with the :meth`),qt=r("em"),cs=o("~transformers.AutoModel.from_pretrained"),hs=o(` class method for the encoder and
:meth`),At=r("em"),ps=o("~transformers.AutoModelForCausalLM.from_pretrained"),ms=o(" class method for the decoder."),fs=i(),I=r("div"),k(io.$$.fragment),gs=i(),le=r("p"),us=o("The "),Zo=r("a"),_s=o("VisionEncoderDecoderModel"),vs=o(" forward method, overrides the "),Ft=r("code"),Es=o("__call__"),bs=o(" special method."),ys=i(),k(ze.$$.fragment),ws=i(),Lt=r("p"),Ts=o("Examples:"),xs=i(),k(lo.$$.fragment),ks=i(),h=r("div"),k(co.$$.fragment),js=i(),It=r("p"),Ms=o(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Ds=i(),ce=r("p"),Vs=o("The model is set in evaluation mode by default using "),Ot=r("code"),$s=o("model.eval()"),Ps=o(` (Dropout modules are deactivated). To
train the model, you need to first set it back in training mode with `),St=r("code"),Cs=o("model.train()"),zs=o("."),qs=i(),he=r("p"),As=o(`Params:
encoder`),Nt=r("em"),Fs=o("pretrained_model_name_or_path (:obj: _str"),Ls=o(", "),Rt=r("em"),Is=o("optional"),Os=o(`):
Information necessary to initiate the image encoder. Can be either:`),Ss=i(),pe=r("ul"),me=r("li"),Ns=o("A string, the "),Bt=r("em"),Rs=o("model id"),Bs=o(` of a pretrained model hosted inside a model repo on huggingface.co. An
example is `),Gt=r("code"),Gs=o("google/vit-base-patch16-224-in21k"),Us=o("."),Ws=i(),U=r("li"),Hs=o("A path to a "),Ut=r("em"),Zs=o("directory"),Js=o(` containing model weights saved using
`),Jo=r("a"),Ys=o("save_pretrained()"),Ks=o(", e.g., "),Wt=r("code"),Qs=o("./my_model_directory/"),Xs=o("."),ed=i(),A=r("li"),od=o("A path or url to a "),Ht=r("em"),td=o("tensorflow index checkpoint file"),nd=o(" (e.g, "),Zt=r("code"),rd=o("./tf_model/model.ckpt.index"),ad=o(`). In
this case, `),Jt=r("code"),sd=o("from_tf"),dd=o(" should be set to "),Yt=r("code"),id=o("True"),ld=o(` and a configuration object should be provided
as `),Kt=r("code"),cd=o("config"),hd=o(` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),pd=i(),W=r("p"),md=o("decoder"),Qt=r("em"),fd=o("pretrained_model_name_or_path (:obj: _str"),gd=o(", "),Xt=r("em"),ud=o("optional"),_d=o(", defaults to "),en=r("em"),vd=o("None"),Ed=o(`):
Information necessary to initiate the text decoder. Can be either:`),bd=i(),fe=r("ul"),H=r("li"),yd=o("A string, the "),on=r("em"),wd=o("model id"),Td=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),tn=r("code"),xd=o("bert-base-uncased"),kd=o(`, or namespaced under
a user or organization name, like `),nn=r("code"),jd=o("dbmdz/bert-base-german-cased"),Md=o("."),Dd=i(),Z=r("li"),Vd=o("A path to a "),rn=r("em"),$d=o("directory"),Pd=o(` containing model weights saved using
`),Yo=r("a"),Cd=o("save_pretrained()"),zd=o(", e.g., "),an=r("code"),qd=o("./my_model_directory/"),Ad=o("."),Fd=i(),F=r("li"),Ld=o("A path or url to a "),sn=r("em"),Id=o("tensorflow index checkpoint file"),Od=o(" (e.g, "),dn=r("code"),Sd=o("./tf_model/model.ckpt.index"),Nd=o(`). In
this case, `),ln=r("code"),Rd=o("from_tf"),Bd=o(" should be set to "),cn=r("code"),Gd=o("True"),Ud=o(` and a configuration object should be provided
as `),hn=r("code"),Wd=o("config"),Hd=o(` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),Zd=i(),ge=r("p"),Jd=o("model"),pn=r("em"),Yd=o("args (remaining positional arguments, _optional"),Kd=o(`):
All remaning positional arguments will be passed to the underlying model\u2019s `),mn=r("code"),Qd=o("__init__"),Xd=o(" method."),ei=i(),ue=r("p"),oi=o("kwargs (remaining dictionary of keyword arguments, "),fn=r("em"),ti=o("optional"),ni=o(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),gn=r("code"),ri=o("output_attentions=True"),ai=o(")."),si=i(),_e=r("ul"),un=r("li"),di=o("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),ii=i(),_n=r("li"),li=o("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),ci=i(),vn=r("li"),hi=o("To update the parent model configuration, do not use a prefix for each configuration parameter."),pi=i(),ho=r("p"),mi=o("Behaves differently depending on whether a "),En=r("code"),fi=o("config"),gi=o(" is provided or automatically loaded."),ui=i(),bn=r("p"),_i=o("Example:"),vi=i(),k(po.$$.fragment),cr=i(),ve=r("h2"),qe=r("a"),yn=r("span"),k(mo.$$.fragment),Ei=i(),wn=r("span"),bi=o("FlaxVisionEncoderDecoderModel"),hr=i(),_=r("div"),k(fo.$$.fragment),yi=i(),Ee=r("p"),wi=o(`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Tn=r("code"),Ti=o("from_pretrained()"),xi=o(` function and the decoder is loaded via
`),xn=r("code"),ki=o("from_pretrained()"),ji=o(` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like image captioning.`),Mi=i(),go=r("p"),Di=o(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),uo=r("a"),Vi=o("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),$i=o(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Pi=i(),_o=r("p"),Ci=o("Additionally, in "),vo=r("a"),zi=o("TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),qi=o(` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),Ai=i(),kn=r("p"),Fi=o(`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),Li=i(),Eo=r("p"),Ii=o("This model inherits from "),Ko=r("a"),Oi=o("FlaxPreTrainedModel"),Si=o(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ni=i(),bo=r("p"),Ri=o("This model is also a Flax Linen "),yo=r("a"),Bi=o("flax.nn.Module"),Gi=o(` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Ui=i(),oe=r("p"),Qo=r("a"),Wi=o("FlaxVisionEncoderDecoderModel"),Hi=o(` is a generic model class that will be instantiated as a
transformer architecture with the module (flax.nn.Module) of one of the base vision model classes of the library as
encoder module and another one as decoder module when created with the
:meth`),jn=r("em"),Zi=o("~transformers.FlaxAutoModel.from_pretrained"),Ji=o(` class method for the encoder and
:meth`),Mn=r("em"),Yi=o("~transformers.FlaxAutoModelForCausalLM.from_pretrained"),Ki=o(" class method for the decoder."),Qi=i(),O=r("div"),k(wo.$$.fragment),Xi=i(),be=r("p"),el=o("The "),Xo=r("a"),ol=o("FlaxVisionEncoderDecoderModel"),tl=o(" forward method, overrides the "),Dn=r("code"),nl=o("__call__"),rl=o(" special method."),al=i(),k(Ae.$$.fragment),sl=i(),Vn=r("p"),dl=o("Examples:"),il=i(),k(To.$$.fragment),ll=i(),m=r("div"),k(xo.$$.fragment),cl=i(),$n=r("p"),hl=o(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),pl=i(),ye=r("p"),ml=o(`Params:
encoder`),Pn=r("em"),fl=o("pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),gl=o(", "),Cn=r("em"),ul=o("optional"),_l=o(`):
Information necessary to initiate the encoder. Can be either:`),vl=i(),ko=r("ul"),we=r("li"),El=o("A string, the "),zn=r("em"),bl=o("model id"),yl=o(` of a pretrained model hosted inside a model repo on huggingface.co. An
example is `),qn=r("code"),wl=o("google/vit-base-patch16-224-in21k"),Tl=o("."),xl=i(),J=r("li"),kl=o("A path to a "),An=r("em"),jl=o("directory"),Ml=o(` containing model weights saved using
`),et=r("a"),Dl=o("save_pretrained()"),Vl=o(", e.g., "),Fn=r("code"),$l=o("./my_model_directory/"),Pl=o("."),Cl=i(),Y=r("p"),zl=o("decoder"),Ln=r("em"),ql=o("pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),Al=o(", "),In=r("em"),Fl=o("optional"),Ll=o(", defaults to "),On=r("em"),Il=o("None"),Ol=o(`):
Information necessary to initiate the decoder. Can be either:`),Sl=i(),jo=r("ul"),K=r("li"),Nl=o("A string, the "),Sn=r("em"),Rl=o("model id"),Bl=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Nn=r("code"),Gl=o("bert-base-uncased"),Ul=o(`, or namespaced under
a user or organization name, like `),Rn=r("code"),Wl=o("dbmdz/bert-base-german-cased"),Hl=o("."),Zl=i(),Q=r("li"),Jl=o("A path to a "),Bn=r("em"),Yl=o("directory"),Kl=o(` containing model weights saved using
`),ot=r("a"),Ql=o("save_pretrained()"),Xl=o(", e.g., "),Gn=r("code"),ec=o("./my_model_directory/"),oc=o("."),tc=i(),Te=r("p"),nc=o("model"),Un=r("em"),rc=o("args (remaining positional arguments, _optional"),ac=o(`):
All remaning positional arguments will be passed to the underlying model\u2019s `),Wn=r("code"),sc=o("__init__"),dc=o(" method."),ic=i(),xe=r("p"),lc=o("kwargs (remaining dictionary of keyword arguments, "),Hn=r("em"),cc=o("optional"),hc=o(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),Zn=r("code"),pc=o("output_attentions=True"),mc=o(")."),fc=i(),ke=r("ul"),Jn=r("li"),gc=o("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),uc=i(),Yn=r("li"),_c=o("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),vc=i(),Kn=r("li"),Ec=o("To update the parent model configuration, do not use a prefix for each configuration parameter."),bc=i(),Mo=r("p"),yc=o("Behaves differently depending on whether a "),Qn=r("code"),wc=o("config"),Tc=o(" is provided or automatically loaded."),xc=i(),Xn=r("p"),kc=o("Example:"),jc=i(),k(Do.$$.fragment),this.h()},l(d){const f=Gp('[data-svelte="svelte-1phssyn"]',document.head);p=a(f,"META",{name:!0,content:!0}),f.forEach(n),L=l(d),v=a(d,"H1",{class:!0});var Vo=s(v);P=a(Vo,"A",{id:!0,class:!0,href:!0});var er=s(P);N=a(er,"SPAN",{});var Mc=s(N);j(C.$$.fragment,Mc),Mc.forEach(n),er.forEach(n),q=l(Vo),G=a(Vo,"SPAN",{});var Dc=s(G);zr=t(Dc,"Vision Encoder Decoder Models"),Dc.forEach(n),Vo.forEach(n),nr=l(d),E=a(d,"P",{});var w=s(E);qr=t(w,"The "),Co=a(w,"A",{href:!0});var Vc=s(Co);Ar=t(Vc,"VisionEncoderDecoderModel"),Vc.forEach(n),Fr=t(w,` can be used to initialize an image-to-text-sequence model with any
pretrained vision autoencoding model as the encoder (`),wt=a(w,"EM",{});var $c=s(wt);Lr=t($c,"e.g."),$c.forEach(n),Ir=l(w),zo=a(w,"A",{href:!0});var Pc=s(zo);Or=t(Pc,"ViT"),Pc.forEach(n),Sr=t(w,", "),qo=a(w,"A",{href:!0});var Cc=s(qo);Nr=t(Cc,"BEiT"),Cc.forEach(n),Rr=t(w,", "),Ao=a(w,"A",{href:!0});var zc=s(Ao);Br=t(zc,"DeiT"),zc.forEach(n),Gr=t(w,`)
and any pretrained language model as the decoder (`),Tt=a(w,"EM",{});var qc=s(Tt);Ur=t(qc,"e.g."),qc.forEach(n),Wr=l(w),Fo=a(w,"A",{href:!0});var Ac=s(Fo);Hr=t(Ac,"RoBERTa"),Ac.forEach(n),Zr=t(w,", "),Lo=a(w,"A",{href:!0});var Fc=s(Lo);Jr=t(Fc,"GPT2"),Fc.forEach(n),Yr=t(w,", "),Io=a(w,"A",{href:!0});var Lc=s(Io);Kr=t(Lc,"BERT"),Lc.forEach(n),Qr=t(w,")."),w.forEach(n),rr=l(d),Me=a(d,"P",{});var mr=s(Me);Xr=t(mr,`The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for
example) `),Ue=a(mr,"A",{href:!0,rel:!0});var Ic=s(Ue);ea=t(Ic,"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),Ic.forEach(n),oa=t(mr,` by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang,
Zhoujun Li, Furu Wei.`),mr.forEach(n),ar=l(d),X=a(d,"P",{});var tt=s(X);ta=t(tt,"An example of how to use a "),Oo=a(tt,"A",{href:!0});var Oc=s(Oo);na=t(Oc,"VisionEncoderDecoderModel"),Oc.forEach(n),ra=t(tt," for inference can be seen in "),So=a(tt,"A",{href:!0});var Sc=s(So);aa=t(Sc,"TrOCR"),Sc.forEach(n),sa=t(tt,"."),tt.forEach(n),sr=l(d),re=a(d,"H2",{class:!0});var fr=s(re);De=a(fr,"A",{id:!0,class:!0,href:!0});var Nc=s(De);xt=a(Nc,"SPAN",{});var Rc=s(xt);j(We.$$.fragment,Rc),Rc.forEach(n),Nc.forEach(n),da=l(fr),kt=a(fr,"SPAN",{});var Bc=s(kt);ia=t(Bc,"VisionEncoderDecoderConfig"),Bc.forEach(n),fr.forEach(n),dr=l(d),z=a(d,"DIV",{class:!0});var S=s(z);j(He.$$.fragment,S),la=l(S),Ve=a(S,"P",{});var or=s(Ve);No=a(or,"A",{href:!0});var Gc=s(No);ca=t(Gc,"VisionEncoderDecoderConfig"),Gc.forEach(n),ha=t(or,` is the configuration class to store the configuration of a
`),Ro=a(or,"A",{href:!0});var Uc=s(Ro);pa=t(Uc,"VisionEncoderDecoderModel"),Uc.forEach(n),ma=t(or,`. It is used to instantiate a Vision-Encoder-Text-Decoder model
according to the specified arguments, defining the encoder and decoder configs.`),or.forEach(n),fa=l(S),ae=a(S,"P",{});var nt=s(ae);ga=t(nt,"Configuration objects inherit from "),Bo=a(nt,"A",{href:!0});var Wc=s(Bo);ua=t(Wc,"PretrainedConfig"),Wc.forEach(n),_a=t(nt,` and can be used to control the model
outputs. Read the documentation from `),Go=a(nt,"A",{href:!0});var Hc=s(Go);va=t(Hc,"PretrainedConfig"),Hc.forEach(n),Ea=t(nt," for more information."),nt.forEach(n),ba=l(S),jt=a(S,"P",{});var Zc=s(jt);ya=t(Zc,"Examples:"),Zc.forEach(n),wa=l(S),j(Ze.$$.fragment,S),Ta=l(S),$e=a(S,"DIV",{class:!0});var gr=s($e);j(Je.$$.fragment,gr),xa=l(gr),Ye=a(gr,"P",{});var ur=s(Ye);ka=t(ur,"Instantiate a "),Uo=a(ur,"A",{href:!0});var Jc=s(Uo);ja=t(Jc,"VisionEncoderDecoderConfig"),Jc.forEach(n),Ma=t(ur,` (or a derived class) from a pre-trained encoder
model configuration and decoder model configuration.`),ur.forEach(n),gr.forEach(n),Da=l(S),Pe=a(S,"DIV",{class:!0});var _r=s(Pe);j(Ke.$$.fragment,_r),Va=l(_r),se=a(_r,"P",{});var rt=s(se);$a=t(rt,"Serializes this instance to a Python dictionary. Override the default "),Mt=a(rt,"EM",{});var Yc=s(Mt);Pa=t(Yc,"to_dict()"),Yc.forEach(n),Ca=t(rt," from "),Dt=a(rt,"EM",{});var Kc=s(Dt);za=t(Kc,"PretrainedConfig"),Kc.forEach(n),qa=t(rt,"."),rt.forEach(n),_r.forEach(n),S.forEach(n),ir=l(d),de=a(d,"H2",{class:!0});var vr=s(de);Ce=a(vr,"A",{id:!0,class:!0,href:!0});var Qc=s(Ce);Vt=a(Qc,"SPAN",{});var Xc=s(Vt);j(Qe.$$.fragment,Xc),Xc.forEach(n),Qc.forEach(n),Aa=l(vr),$t=a(vr,"SPAN",{});var eh=s($t);Fa=t(eh,"VisionEncoderDecoderModel"),eh.forEach(n),vr.forEach(n),lr=l(d),u=a(d,"DIV",{class:!0});var T=s(u);j(Xe.$$.fragment,T),La=l(T),ie=a(T,"P",{});var at=s(ie);Ia=t(at,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Pt=a(at,"CODE",{});var oh=s(Pt);Oa=t(oh,"from_pretrained()"),oh.forEach(n),Sa=t(at,` function and the decoder is loaded via
`),Ct=a(at,"CODE",{});var th=s(Ct);Na=t(th,"from_pretrained()"),th.forEach(n),Ra=t(at,` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like image captioning.`),at.forEach(n),Ba=l(T),eo=a(T,"P",{});var Er=s(eo);Ga=t(Er,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),oo=a(Er,"A",{href:!0,rel:!0});var nh=s(oo);Ua=t(nh,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),nh.forEach(n),Wa=t(Er,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Er.forEach(n),Ha=l(T),to=a(T,"P",{});var br=s(to);Za=t(br,"Additionally, in "),no=a(br,"A",{href:!0,rel:!0});var rh=s(no);Ja=t(rh,"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),rh.forEach(n),Ya=t(br,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),br.forEach(n),Ka=l(T),zt=a(T,"P",{});var ah=s(zt);Qa=t(ah,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),ah.forEach(n),Xa=l(T),ro=a(T,"P",{});var yr=s(ro);es=t(yr,"This model inherits from "),Wo=a(yr,"A",{href:!0});var sh=s(Wo);os=t(sh,"PreTrainedModel"),sh.forEach(n),ts=t(yr,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),yr.forEach(n),ns=l(T),ao=a(T,"P",{});var wr=s(ao);rs=t(wr,"This model is also a PyTorch "),so=a(wr,"A",{href:!0,rel:!0});var dh=s(so);as=t(dh,"torch.nn.Module"),dh.forEach(n),ss=t(wr,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),wr.forEach(n),ds=l(T),ee=a(T,"P",{});var $o=s(ee);Ho=a($o,"A",{href:!0});var ih=s(Ho);is=t(ih,"VisionEncoderDecoderModel"),ih.forEach(n),ls=t($o,` is a generic model class that will be instantiated as a
transformer architecture with one of the base vision model classes of the library as encoder and another one as
decoder when created with the :meth`),qt=a($o,"EM",{});var lh=s(qt);cs=t(lh,"~transformers.AutoModel.from_pretrained"),lh.forEach(n),hs=t($o,` class method for the encoder and
:meth`),At=a($o,"EM",{});var ch=s(At);ps=t(ch,"~transformers.AutoModelForCausalLM.from_pretrained"),ch.forEach(n),ms=t($o," class method for the decoder."),$o.forEach(n),fs=l(T),I=a(T,"DIV",{class:!0});var te=s(I);j(io.$$.fragment,te),gs=l(te),le=a(te,"P",{});var st=s(le);us=t(st,"The "),Zo=a(st,"A",{href:!0});var hh=s(Zo);_s=t(hh,"VisionEncoderDecoderModel"),hh.forEach(n),vs=t(st," forward method, overrides the "),Ft=a(st,"CODE",{});var ph=s(Ft);Es=t(ph,"__call__"),ph.forEach(n),bs=t(st," special method."),st.forEach(n),ys=l(te),j(ze.$$.fragment,te),ws=l(te),Lt=a(te,"P",{});var mh=s(Lt);Ts=t(mh,"Examples:"),mh.forEach(n),xs=l(te),j(lo.$$.fragment,te),te.forEach(n),ks=l(T),h=a(T,"DIV",{class:!0});var g=s(h);j(co.$$.fragment,g),js=l(g),It=a(g,"P",{});var fh=s(It);Ms=t(fh,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),fh.forEach(n),Ds=l(g),ce=a(g,"P",{});var dt=s(ce);Vs=t(dt,"The model is set in evaluation mode by default using "),Ot=a(dt,"CODE",{});var gh=s(Ot);$s=t(gh,"model.eval()"),gh.forEach(n),Ps=t(dt,` (Dropout modules are deactivated). To
train the model, you need to first set it back in training mode with `),St=a(dt,"CODE",{});var uh=s(St);Cs=t(uh,"model.train()"),uh.forEach(n),zs=t(dt,"."),dt.forEach(n),qs=l(g),he=a(g,"P",{});var it=s(he);As=t(it,`Params:
encoder`),Nt=a(it,"EM",{});var _h=s(Nt);Fs=t(_h,"pretrained_model_name_or_path (:obj: _str"),_h.forEach(n),Ls=t(it,", "),Rt=a(it,"EM",{});var vh=s(Rt);Is=t(vh,"optional"),vh.forEach(n),Os=t(it,`):
Information necessary to initiate the image encoder. Can be either:`),it.forEach(n),Ss=l(g),pe=a(g,"UL",{});var lt=s(pe);me=a(lt,"LI",{});var ct=s(me);Ns=t(ct,"A string, the "),Bt=a(ct,"EM",{});var Eh=s(Bt);Rs=t(Eh,"model id"),Eh.forEach(n),Bs=t(ct,` of a pretrained model hosted inside a model repo on huggingface.co. An
example is `),Gt=a(ct,"CODE",{});var bh=s(Gt);Gs=t(bh,"google/vit-base-patch16-224-in21k"),bh.forEach(n),Us=t(ct,"."),ct.forEach(n),Ws=l(lt),U=a(lt,"LI",{});var Fe=s(U);Hs=t(Fe,"A path to a "),Ut=a(Fe,"EM",{});var yh=s(Ut);Zs=t(yh,"directory"),yh.forEach(n),Js=t(Fe,` containing model weights saved using
`),Jo=a(Fe,"A",{href:!0});var wh=s(Jo);Ys=t(wh,"save_pretrained()"),wh.forEach(n),Ks=t(Fe,", e.g., "),Wt=a(Fe,"CODE",{});var Th=s(Wt);Qs=t(Th,"./my_model_directory/"),Th.forEach(n),Xs=t(Fe,"."),Fe.forEach(n),ed=l(lt),A=a(lt,"LI",{});var R=s(A);od=t(R,"A path or url to a "),Ht=a(R,"EM",{});var xh=s(Ht);td=t(xh,"tensorflow index checkpoint file"),xh.forEach(n),nd=t(R," (e.g, "),Zt=a(R,"CODE",{});var kh=s(Zt);rd=t(kh,"./tf_model/model.ckpt.index"),kh.forEach(n),ad=t(R,`). In
this case, `),Jt=a(R,"CODE",{});var jh=s(Jt);sd=t(jh,"from_tf"),jh.forEach(n),dd=t(R," should be set to "),Yt=a(R,"CODE",{});var Mh=s(Yt);id=t(Mh,"True"),Mh.forEach(n),ld=t(R,` and a configuration object should be provided
as `),Kt=a(R,"CODE",{});var Dh=s(Kt);cd=t(Dh,"config"),Dh.forEach(n),hd=t(R,` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),R.forEach(n),lt.forEach(n),pd=l(g),W=a(g,"P",{});var Le=s(W);md=t(Le,"decoder"),Qt=a(Le,"EM",{});var Vh=s(Qt);fd=t(Vh,"pretrained_model_name_or_path (:obj: _str"),Vh.forEach(n),gd=t(Le,", "),Xt=a(Le,"EM",{});var $h=s(Xt);ud=t($h,"optional"),$h.forEach(n),_d=t(Le,", defaults to "),en=a(Le,"EM",{});var Ph=s(en);vd=t(Ph,"None"),Ph.forEach(n),Ed=t(Le,`):
Information necessary to initiate the text decoder. Can be either:`),Le.forEach(n),bd=l(g),fe=a(g,"UL",{});var ht=s(fe);H=a(ht,"LI",{});var Ie=s(H);yd=t(Ie,"A string, the "),on=a(Ie,"EM",{});var Ch=s(on);wd=t(Ch,"model id"),Ch.forEach(n),Td=t(Ie,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),tn=a(Ie,"CODE",{});var zh=s(tn);xd=t(zh,"bert-base-uncased"),zh.forEach(n),kd=t(Ie,`, or namespaced under
a user or organization name, like `),nn=a(Ie,"CODE",{});var qh=s(nn);jd=t(qh,"dbmdz/bert-base-german-cased"),qh.forEach(n),Md=t(Ie,"."),Ie.forEach(n),Dd=l(ht),Z=a(ht,"LI",{});var Oe=s(Z);Vd=t(Oe,"A path to a "),rn=a(Oe,"EM",{});var Ah=s(rn);$d=t(Ah,"directory"),Ah.forEach(n),Pd=t(Oe,` containing model weights saved using
`),Yo=a(Oe,"A",{href:!0});var Fh=s(Yo);Cd=t(Fh,"save_pretrained()"),Fh.forEach(n),zd=t(Oe,", e.g., "),an=a(Oe,"CODE",{});var Lh=s(an);qd=t(Lh,"./my_model_directory/"),Lh.forEach(n),Ad=t(Oe,"."),Oe.forEach(n),Fd=l(ht),F=a(ht,"LI",{});var B=s(F);Ld=t(B,"A path or url to a "),sn=a(B,"EM",{});var Ih=s(sn);Id=t(Ih,"tensorflow index checkpoint file"),Ih.forEach(n),Od=t(B," (e.g, "),dn=a(B,"CODE",{});var Oh=s(dn);Sd=t(Oh,"./tf_model/model.ckpt.index"),Oh.forEach(n),Nd=t(B,`). In
this case, `),ln=a(B,"CODE",{});var Sh=s(ln);Rd=t(Sh,"from_tf"),Sh.forEach(n),Bd=t(B," should be set to "),cn=a(B,"CODE",{});var Nh=s(cn);Gd=t(Nh,"True"),Nh.forEach(n),Ud=t(B,` and a configuration object should be provided
as `),hn=a(B,"CODE",{});var Rh=s(hn);Wd=t(Rh,"config"),Rh.forEach(n),Hd=t(B,` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),B.forEach(n),ht.forEach(n),Zd=l(g),ge=a(g,"P",{});var pt=s(ge);Jd=t(pt,"model"),pn=a(pt,"EM",{});var Bh=s(pn);Yd=t(Bh,"args (remaining positional arguments, _optional"),Bh.forEach(n),Kd=t(pt,`):
All remaning positional arguments will be passed to the underlying model\u2019s `),mn=a(pt,"CODE",{});var Gh=s(mn);Qd=t(Gh,"__init__"),Gh.forEach(n),Xd=t(pt," method."),pt.forEach(n),ei=l(g),ue=a(g,"P",{});var mt=s(ue);oi=t(mt,"kwargs (remaining dictionary of keyword arguments, "),fn=a(mt,"EM",{});var Uh=s(fn);ti=t(Uh,"optional"),Uh.forEach(n),ni=t(mt,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),gn=a(mt,"CODE",{});var Wh=s(gn);ri=t(Wh,"output_attentions=True"),Wh.forEach(n),ai=t(mt,")."),mt.forEach(n),si=l(g),_e=a(g,"UL",{});var ft=s(_e);un=a(ft,"LI",{});var Hh=s(un);di=t(Hh,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Hh.forEach(n),ii=l(ft),_n=a(ft,"LI",{});var Zh=s(_n);li=t(Zh,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),Zh.forEach(n),ci=l(ft),vn=a(ft,"LI",{});var Jh=s(vn);hi=t(Jh,"To update the parent model configuration, do not use a prefix for each configuration parameter."),Jh.forEach(n),ft.forEach(n),pi=l(g),ho=a(g,"P",{});var Tr=s(ho);mi=t(Tr,"Behaves differently depending on whether a "),En=a(Tr,"CODE",{});var Yh=s(En);fi=t(Yh,"config"),Yh.forEach(n),gi=t(Tr," is provided or automatically loaded."),Tr.forEach(n),ui=l(g),bn=a(g,"P",{});var Kh=s(bn);_i=t(Kh,"Example:"),Kh.forEach(n),vi=l(g),j(po.$$.fragment,g),g.forEach(n),T.forEach(n),cr=l(d),ve=a(d,"H2",{class:!0});var xr=s(ve);qe=a(xr,"A",{id:!0,class:!0,href:!0});var Qh=s(qe);yn=a(Qh,"SPAN",{});var Xh=s(yn);j(mo.$$.fragment,Xh),Xh.forEach(n),Qh.forEach(n),Ei=l(xr),wn=a(xr,"SPAN",{});var ep=s(wn);bi=t(ep,"FlaxVisionEncoderDecoderModel"),ep.forEach(n),xr.forEach(n),hr=l(d),_=a(d,"DIV",{class:!0});var x=s(_);j(fo.$$.fragment,x),yi=l(x),Ee=a(x,"P",{});var gt=s(Ee);wi=t(gt,`This class can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model
as the encoder and any pretrained text autoregressive model as the decoder. The encoder is loaded via
`),Tn=a(gt,"CODE",{});var op=s(Tn);Ti=t(op,"from_pretrained()"),op.forEach(n),xi=t(gt,` function and the decoder is loaded via
`),xn=a(gt,"CODE",{});var tp=s(xn);ki=t(tp,"from_pretrained()"),tp.forEach(n),ji=t(gt,` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like image captioning.`),gt.forEach(n),Mi=l(x),go=a(x,"P",{});var kr=s(go);Di=t(kr,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),uo=a(kr,"A",{href:!0,rel:!0});var np=s(uo);Vi=t(np,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),np.forEach(n),$i=t(kr,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),kr.forEach(n),Pi=l(x),_o=a(x,"P",{});var jr=s(_o);Ci=t(jr,"Additionally, in "),vo=a(jr,"A",{href:!0,rel:!0});var rp=s(vo);zi=t(rp,"TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models"),rp.forEach(n),qi=t(jr,` it is shown how leveraging large pretrained vision models for optical
character recognition (OCR) yields a significant performance improvement.`),jr.forEach(n),Ai=l(x),kn=a(x,"P",{});var ap=s(kn);Fi=t(ap,`After such a Vision-Encoder-Text-Decoder model has been trained/fine-tuned, it can be saved/loaded just like any
other models (see the examples for more information).`),ap.forEach(n),Li=l(x),Eo=a(x,"P",{});var Mr=s(Eo);Ii=t(Mr,"This model inherits from "),Ko=a(Mr,"A",{href:!0});var sp=s(Ko);Oi=t(sp,"FlaxPreTrainedModel"),sp.forEach(n),Si=t(Mr,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Mr.forEach(n),Ni=l(x),bo=a(x,"P",{});var Dr=s(bo);Ri=t(Dr,"This model is also a Flax Linen "),yo=a(Dr,"A",{href:!0,rel:!0});var dp=s(yo);Bi=t(dp,"flax.nn.Module"),dp.forEach(n),Gi=t(Dr,` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Dr.forEach(n),Ui=l(x),oe=a(x,"P",{});var Po=s(oe);Qo=a(Po,"A",{href:!0});var ip=s(Qo);Wi=t(ip,"FlaxVisionEncoderDecoderModel"),ip.forEach(n),Hi=t(Po,` is a generic model class that will be instantiated as a
transformer architecture with the module (flax.nn.Module) of one of the base vision model classes of the library as
encoder module and another one as decoder module when created with the
:meth`),jn=a(Po,"EM",{});var lp=s(jn);Zi=t(lp,"~transformers.FlaxAutoModel.from_pretrained"),lp.forEach(n),Ji=t(Po,` class method for the encoder and
:meth`),Mn=a(Po,"EM",{});var cp=s(Mn);Yi=t(cp,"~transformers.FlaxAutoModelForCausalLM.from_pretrained"),cp.forEach(n),Ki=t(Po," class method for the decoder."),Po.forEach(n),Qi=l(x),O=a(x,"DIV",{class:!0});var ne=s(O);j(wo.$$.fragment,ne),Xi=l(ne),be=a(ne,"P",{});var ut=s(be);el=t(ut,"The "),Xo=a(ut,"A",{href:!0});var hp=s(Xo);ol=t(hp,"FlaxVisionEncoderDecoderModel"),hp.forEach(n),tl=t(ut," forward method, overrides the "),Dn=a(ut,"CODE",{});var pp=s(Dn);nl=t(pp,"__call__"),pp.forEach(n),rl=t(ut," special method."),ut.forEach(n),al=l(ne),j(Ae.$$.fragment,ne),sl=l(ne),Vn=a(ne,"P",{});var mp=s(Vn);dl=t(mp,"Examples:"),mp.forEach(n),il=l(ne),j(To.$$.fragment,ne),ne.forEach(n),ll=l(x),m=a(x,"DIV",{class:!0});var b=s(m);j(xo.$$.fragment,b),cl=l(b),$n=a(b,"P",{});var fp=s($n);hl=t(fp,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),fp.forEach(n),pl=l(b),ye=a(b,"P",{});var _t=s(ye);ml=t(_t,`Params:
encoder`),Pn=a(_t,"EM",{});var gp=s(Pn);fl=t(gp,"pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),gp.forEach(n),gl=t(_t,", "),Cn=a(_t,"EM",{});var up=s(Cn);ul=t(up,"optional"),up.forEach(n),_l=t(_t,`):
Information necessary to initiate the encoder. Can be either:`),_t.forEach(n),vl=l(b),ko=a(b,"UL",{});var Vr=s(ko);we=a(Vr,"LI",{});var vt=s(we);El=t(vt,"A string, the "),zn=a(vt,"EM",{});var _p=s(zn);bl=t(_p,"model id"),_p.forEach(n),yl=t(vt,` of a pretrained model hosted inside a model repo on huggingface.co. An
example is `),qn=a(vt,"CODE",{});var vp=s(qn);wl=t(vp,"google/vit-base-patch16-224-in21k"),vp.forEach(n),Tl=t(vt,"."),vt.forEach(n),xl=l(Vr),J=a(Vr,"LI",{});var Se=s(J);kl=t(Se,"A path to a "),An=a(Se,"EM",{});var Ep=s(An);jl=t(Ep,"directory"),Ep.forEach(n),Ml=t(Se,` containing model weights saved using
`),et=a(Se,"A",{href:!0});var bp=s(et);Dl=t(bp,"save_pretrained()"),bp.forEach(n),Vl=t(Se,", e.g., "),Fn=a(Se,"CODE",{});var yp=s(Fn);$l=t(yp,"./my_model_directory/"),yp.forEach(n),Pl=t(Se,"."),Se.forEach(n),Vr.forEach(n),Cl=l(b),Y=a(b,"P",{});var Ne=s(Y);zl=t(Ne,"decoder"),Ln=a(Ne,"EM",{});var wp=s(Ln);ql=t(wp,"pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),wp.forEach(n),Al=t(Ne,", "),In=a(Ne,"EM",{});var Tp=s(In);Fl=t(Tp,"optional"),Tp.forEach(n),Ll=t(Ne,", defaults to "),On=a(Ne,"EM",{});var xp=s(On);Il=t(xp,"None"),xp.forEach(n),Ol=t(Ne,`):
Information necessary to initiate the decoder. Can be either:`),Ne.forEach(n),Sl=l(b),jo=a(b,"UL",{});var $r=s(jo);K=a($r,"LI",{});var Re=s(K);Nl=t(Re,"A string, the "),Sn=a(Re,"EM",{});var kp=s(Sn);Rl=t(kp,"model id"),kp.forEach(n),Bl=t(Re,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Nn=a(Re,"CODE",{});var jp=s(Nn);Gl=t(jp,"bert-base-uncased"),jp.forEach(n),Ul=t(Re,`, or namespaced under
a user or organization name, like `),Rn=a(Re,"CODE",{});var Mp=s(Rn);Wl=t(Mp,"dbmdz/bert-base-german-cased"),Mp.forEach(n),Hl=t(Re,"."),Re.forEach(n),Zl=l($r),Q=a($r,"LI",{});var Be=s(Q);Jl=t(Be,"A path to a "),Bn=a(Be,"EM",{});var Dp=s(Bn);Yl=t(Dp,"directory"),Dp.forEach(n),Kl=t(Be,` containing model weights saved using
`),ot=a(Be,"A",{href:!0});var Vp=s(ot);Ql=t(Vp,"save_pretrained()"),Vp.forEach(n),Xl=t(Be,", e.g., "),Gn=a(Be,"CODE",{});var $p=s(Gn);ec=t($p,"./my_model_directory/"),$p.forEach(n),oc=t(Be,"."),Be.forEach(n),$r.forEach(n),tc=l(b),Te=a(b,"P",{});var Et=s(Te);nc=t(Et,"model"),Un=a(Et,"EM",{});var Pp=s(Un);rc=t(Pp,"args (remaining positional arguments, _optional"),Pp.forEach(n),ac=t(Et,`):
All remaning positional arguments will be passed to the underlying model\u2019s `),Wn=a(Et,"CODE",{});var Cp=s(Wn);sc=t(Cp,"__init__"),Cp.forEach(n),dc=t(Et," method."),Et.forEach(n),ic=l(b),xe=a(b,"P",{});var bt=s(xe);lc=t(bt,"kwargs (remaining dictionary of keyword arguments, "),Hn=a(bt,"EM",{});var zp=s(Hn);cc=t(zp,"optional"),zp.forEach(n),hc=t(bt,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),Zn=a(bt,"CODE",{});var qp=s(Zn);pc=t(qp,"output_attentions=True"),qp.forEach(n),mc=t(bt,")."),bt.forEach(n),fc=l(b),ke=a(b,"UL",{});var yt=s(ke);Jn=a(yt,"LI",{});var Ap=s(Jn);gc=t(Ap,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Ap.forEach(n),uc=l(yt),Yn=a(yt,"LI",{});var Fp=s(Yn);_c=t(Fp,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),Fp.forEach(n),vc=l(yt),Kn=a(yt,"LI",{});var Lp=s(Kn);Ec=t(Lp,"To update the parent model configuration, do not use a prefix for each configuration parameter."),Lp.forEach(n),yt.forEach(n),bc=l(b),Mo=a(b,"P",{});var Pr=s(Mo);yc=t(Pr,"Behaves differently depending on whether a "),Qn=a(Pr,"CODE",{});var Ip=s(Qn);wc=t(Ip,"config"),Ip.forEach(n),Tc=t(Pr," is provided or automatically loaded."),Pr.forEach(n),xc=l(b),Xn=a(b,"P",{});var Op=s(Xn);kc=t(Op,"Example:"),Op.forEach(n),jc=l(b),j(Do.$$.fragment,b),b.forEach(n),x.forEach(n),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Zp)),c(P,"id","vision-encoder-decoder-models"),c(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(P,"href","#vision-encoder-decoder-models"),c(v,"class","relative group"),c(Co,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),c(zo,"href","/docs/transformers/v4.14.1/en/vit"),c(qo,"href","/docs/transformers/v4.14.1/en/beit"),c(Ao,"href","/docs/transformers/v4.14.1/en/deit"),c(Fo,"href","/docs/transformers/v4.14.1/en/roberta"),c(Lo,"href","/docs/transformers/v4.14.1/en/gpt2"),c(Io,"href","/docs/transformers/v4.14.1/en/bert"),c(Ue,"href","https://arxiv.org/abs/2109.10282"),c(Ue,"rel","nofollow"),c(Oo,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),c(So,"href","/docs/transformers/v4.14.1/en/trocr"),c(De,"id","transformers.VisionEncoderDecoderConfig"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.VisionEncoderDecoderConfig"),c(re,"class","relative group"),c(No,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"),c(Ro,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),c(Bo,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),c(Go,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),c(Uo,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"),c($e,"class","docstring"),c(Pe,"class","docstring"),c(z,"class","docstring"),c(Ce,"id","transformers.VisionEncoderDecoderModel"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.VisionEncoderDecoderModel"),c(de,"class","relative group"),c(oo,"href","https://arxiv.org/abs/1907.12461"),c(oo,"rel","nofollow"),c(no,"href","https://arxiv.org/abs/2109.10282"),c(no,"rel","nofollow"),c(Wo,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),c(so,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(so,"rel","nofollow"),c(Ho,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),c(Zo,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),c(I,"class","docstring"),c(Jo,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(Yo,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(h,"class","docstring"),c(u,"class","docstring"),c(qe,"id","transformers.FlaxVisionEncoderDecoderModel"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.FlaxVisionEncoderDecoderModel"),c(ve,"class","relative group"),c(uo,"href","https://arxiv.org/abs/1907.12461"),c(uo,"rel","nofollow"),c(vo,"href","https://arxiv.org/abs/2109.10282"),c(vo,"rel","nofollow"),c(Ko,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(yo,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),c(yo,"rel","nofollow"),c(Qo,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel"),c(Xo,"href","/docs/transformers/v4.14.1/en/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel"),c(O,"class","docstring"),c(et,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained"),c(ot,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained"),c(m,"class","docstring"),c(_,"class","docstring")},m(d,f){e(document.head,p),y(d,L,f),y(d,v,f),e(v,P),e(P,N),M(C,N,null),e(v,q),e(v,G),e(G,zr),y(d,nr,f),y(d,E,f),e(E,qr),e(E,Co),e(Co,Ar),e(E,Fr),e(E,wt),e(wt,Lr),e(E,Ir),e(E,zo),e(zo,Or),e(E,Sr),e(E,qo),e(qo,Nr),e(E,Rr),e(E,Ao),e(Ao,Br),e(E,Gr),e(E,Tt),e(Tt,Ur),e(E,Wr),e(E,Fo),e(Fo,Hr),e(E,Zr),e(E,Lo),e(Lo,Jr),e(E,Yr),e(E,Io),e(Io,Kr),e(E,Qr),y(d,rr,f),y(d,Me,f),e(Me,Xr),e(Me,Ue),e(Ue,ea),e(Me,oa),y(d,ar,f),y(d,X,f),e(X,ta),e(X,Oo),e(Oo,na),e(X,ra),e(X,So),e(So,aa),e(X,sa),y(d,sr,f),y(d,re,f),e(re,De),e(De,xt),M(We,xt,null),e(re,da),e(re,kt),e(kt,ia),y(d,dr,f),y(d,z,f),M(He,z,null),e(z,la),e(z,Ve),e(Ve,No),e(No,ca),e(Ve,ha),e(Ve,Ro),e(Ro,pa),e(Ve,ma),e(z,fa),e(z,ae),e(ae,ga),e(ae,Bo),e(Bo,ua),e(ae,_a),e(ae,Go),e(Go,va),e(ae,Ea),e(z,ba),e(z,jt),e(jt,ya),e(z,wa),M(Ze,z,null),e(z,Ta),e(z,$e),M(Je,$e,null),e($e,xa),e($e,Ye),e(Ye,ka),e(Ye,Uo),e(Uo,ja),e(Ye,Ma),e(z,Da),e(z,Pe),M(Ke,Pe,null),e(Pe,Va),e(Pe,se),e(se,$a),e(se,Mt),e(Mt,Pa),e(se,Ca),e(se,Dt),e(Dt,za),e(se,qa),y(d,ir,f),y(d,de,f),e(de,Ce),e(Ce,Vt),M(Qe,Vt,null),e(de,Aa),e(de,$t),e($t,Fa),y(d,lr,f),y(d,u,f),M(Xe,u,null),e(u,La),e(u,ie),e(ie,Ia),e(ie,Pt),e(Pt,Oa),e(ie,Sa),e(ie,Ct),e(Ct,Na),e(ie,Ra),e(u,Ba),e(u,eo),e(eo,Ga),e(eo,oo),e(oo,Ua),e(eo,Wa),e(u,Ha),e(u,to),e(to,Za),e(to,no),e(no,Ja),e(to,Ya),e(u,Ka),e(u,zt),e(zt,Qa),e(u,Xa),e(u,ro),e(ro,es),e(ro,Wo),e(Wo,os),e(ro,ts),e(u,ns),e(u,ao),e(ao,rs),e(ao,so),e(so,as),e(ao,ss),e(u,ds),e(u,ee),e(ee,Ho),e(Ho,is),e(ee,ls),e(ee,qt),e(qt,cs),e(ee,hs),e(ee,At),e(At,ps),e(ee,ms),e(u,fs),e(u,I),M(io,I,null),e(I,gs),e(I,le),e(le,us),e(le,Zo),e(Zo,_s),e(le,vs),e(le,Ft),e(Ft,Es),e(le,bs),e(I,ys),M(ze,I,null),e(I,ws),e(I,Lt),e(Lt,Ts),e(I,xs),M(lo,I,null),e(u,ks),e(u,h),M(co,h,null),e(h,js),e(h,It),e(It,Ms),e(h,Ds),e(h,ce),e(ce,Vs),e(ce,Ot),e(Ot,$s),e(ce,Ps),e(ce,St),e(St,Cs),e(ce,zs),e(h,qs),e(h,he),e(he,As),e(he,Nt),e(Nt,Fs),e(he,Ls),e(he,Rt),e(Rt,Is),e(he,Os),e(h,Ss),e(h,pe),e(pe,me),e(me,Ns),e(me,Bt),e(Bt,Rs),e(me,Bs),e(me,Gt),e(Gt,Gs),e(me,Us),e(pe,Ws),e(pe,U),e(U,Hs),e(U,Ut),e(Ut,Zs),e(U,Js),e(U,Jo),e(Jo,Ys),e(U,Ks),e(U,Wt),e(Wt,Qs),e(U,Xs),e(pe,ed),e(pe,A),e(A,od),e(A,Ht),e(Ht,td),e(A,nd),e(A,Zt),e(Zt,rd),e(A,ad),e(A,Jt),e(Jt,sd),e(A,dd),e(A,Yt),e(Yt,id),e(A,ld),e(A,Kt),e(Kt,cd),e(A,hd),e(h,pd),e(h,W),e(W,md),e(W,Qt),e(Qt,fd),e(W,gd),e(W,Xt),e(Xt,ud),e(W,_d),e(W,en),e(en,vd),e(W,Ed),e(h,bd),e(h,fe),e(fe,H),e(H,yd),e(H,on),e(on,wd),e(H,Td),e(H,tn),e(tn,xd),e(H,kd),e(H,nn),e(nn,jd),e(H,Md),e(fe,Dd),e(fe,Z),e(Z,Vd),e(Z,rn),e(rn,$d),e(Z,Pd),e(Z,Yo),e(Yo,Cd),e(Z,zd),e(Z,an),e(an,qd),e(Z,Ad),e(fe,Fd),e(fe,F),e(F,Ld),e(F,sn),e(sn,Id),e(F,Od),e(F,dn),e(dn,Sd),e(F,Nd),e(F,ln),e(ln,Rd),e(F,Bd),e(F,cn),e(cn,Gd),e(F,Ud),e(F,hn),e(hn,Wd),e(F,Hd),e(h,Zd),e(h,ge),e(ge,Jd),e(ge,pn),e(pn,Yd),e(ge,Kd),e(ge,mn),e(mn,Qd),e(ge,Xd),e(h,ei),e(h,ue),e(ue,oi),e(ue,fn),e(fn,ti),e(ue,ni),e(ue,gn),e(gn,ri),e(ue,ai),e(h,si),e(h,_e),e(_e,un),e(un,di),e(_e,ii),e(_e,_n),e(_n,li),e(_e,ci),e(_e,vn),e(vn,hi),e(h,pi),e(h,ho),e(ho,mi),e(ho,En),e(En,fi),e(ho,gi),e(h,ui),e(h,bn),e(bn,_i),e(h,vi),M(po,h,null),y(d,cr,f),y(d,ve,f),e(ve,qe),e(qe,yn),M(mo,yn,null),e(ve,Ei),e(ve,wn),e(wn,bi),y(d,hr,f),y(d,_,f),M(fo,_,null),e(_,yi),e(_,Ee),e(Ee,wi),e(Ee,Tn),e(Tn,Ti),e(Ee,xi),e(Ee,xn),e(xn,ki),e(Ee,ji),e(_,Mi),e(_,go),e(go,Di),e(go,uo),e(uo,Vi),e(go,$i),e(_,Pi),e(_,_o),e(_o,Ci),e(_o,vo),e(vo,zi),e(_o,qi),e(_,Ai),e(_,kn),e(kn,Fi),e(_,Li),e(_,Eo),e(Eo,Ii),e(Eo,Ko),e(Ko,Oi),e(Eo,Si),e(_,Ni),e(_,bo),e(bo,Ri),e(bo,yo),e(yo,Bi),e(bo,Gi),e(_,Ui),e(_,oe),e(oe,Qo),e(Qo,Wi),e(oe,Hi),e(oe,jn),e(jn,Zi),e(oe,Ji),e(oe,Mn),e(Mn,Yi),e(oe,Ki),e(_,Qi),e(_,O),M(wo,O,null),e(O,Xi),e(O,be),e(be,el),e(be,Xo),e(Xo,ol),e(be,tl),e(be,Dn),e(Dn,nl),e(be,rl),e(O,al),M(Ae,O,null),e(O,sl),e(O,Vn),e(Vn,dl),e(O,il),M(To,O,null),e(_,ll),e(_,m),M(xo,m,null),e(m,cl),e(m,$n),e($n,hl),e(m,pl),e(m,ye),e(ye,ml),e(ye,Pn),e(Pn,fl),e(ye,gl),e(ye,Cn),e(Cn,ul),e(ye,_l),e(m,vl),e(m,ko),e(ko,we),e(we,El),e(we,zn),e(zn,bl),e(we,yl),e(we,qn),e(qn,wl),e(we,Tl),e(ko,xl),e(ko,J),e(J,kl),e(J,An),e(An,jl),e(J,Ml),e(J,et),e(et,Dl),e(J,Vl),e(J,Fn),e(Fn,$l),e(J,Pl),e(m,Cl),e(m,Y),e(Y,zl),e(Y,Ln),e(Ln,ql),e(Y,Al),e(Y,In),e(In,Fl),e(Y,Ll),e(Y,On),e(On,Il),e(Y,Ol),e(m,Sl),e(m,jo),e(jo,K),e(K,Nl),e(K,Sn),e(Sn,Rl),e(K,Bl),e(K,Nn),e(Nn,Gl),e(K,Ul),e(K,Rn),e(Rn,Wl),e(K,Hl),e(jo,Zl),e(jo,Q),e(Q,Jl),e(Q,Bn),e(Bn,Yl),e(Q,Kl),e(Q,ot),e(ot,Ql),e(Q,Xl),e(Q,Gn),e(Gn,ec),e(Q,oc),e(m,tc),e(m,Te),e(Te,nc),e(Te,Un),e(Un,rc),e(Te,ac),e(Te,Wn),e(Wn,sc),e(Te,dc),e(m,ic),e(m,xe),e(xe,lc),e(xe,Hn),e(Hn,cc),e(xe,hc),e(xe,Zn),e(Zn,pc),e(xe,mc),e(m,fc),e(m,ke),e(ke,Jn),e(Jn,gc),e(ke,uc),e(ke,Yn),e(Yn,_c),e(ke,vc),e(ke,Kn),e(Kn,Ec),e(m,bc),e(m,Mo),e(Mo,yc),e(Mo,Qn),e(Qn,wc),e(Mo,Tc),e(m,xc),e(m,Xn),e(Xn,kc),e(m,jc),M(Do,m,null),pr=!0},p(d,[f]){const Vo={};f&2&&(Vo.$$scope={dirty:f,ctx:d}),ze.$set(Vo);const er={};f&2&&(er.$$scope={dirty:f,ctx:d}),Ae.$set(er)},i(d){pr||(D(C.$$.fragment,d),D(We.$$.fragment,d),D(He.$$.fragment,d),D(Ze.$$.fragment,d),D(Je.$$.fragment,d),D(Ke.$$.fragment,d),D(Qe.$$.fragment,d),D(Xe.$$.fragment,d),D(io.$$.fragment,d),D(ze.$$.fragment,d),D(lo.$$.fragment,d),D(co.$$.fragment,d),D(po.$$.fragment,d),D(mo.$$.fragment,d),D(fo.$$.fragment,d),D(wo.$$.fragment,d),D(Ae.$$.fragment,d),D(To.$$.fragment,d),D(xo.$$.fragment,d),D(Do.$$.fragment,d),pr=!0)},o(d){V(C.$$.fragment,d),V(We.$$.fragment,d),V(He.$$.fragment,d),V(Ze.$$.fragment,d),V(Je.$$.fragment,d),V(Ke.$$.fragment,d),V(Qe.$$.fragment,d),V(Xe.$$.fragment,d),V(io.$$.fragment,d),V(ze.$$.fragment,d),V(lo.$$.fragment,d),V(co.$$.fragment,d),V(po.$$.fragment,d),V(mo.$$.fragment,d),V(fo.$$.fragment,d),V(wo.$$.fragment,d),V(Ae.$$.fragment,d),V(To.$$.fragment,d),V(xo.$$.fragment,d),V(Do.$$.fragment,d),pr=!1},d(d){n(p),d&&n(L),d&&n(v),$(C),d&&n(nr),d&&n(E),d&&n(rr),d&&n(Me),d&&n(ar),d&&n(X),d&&n(sr),d&&n(re),$(We),d&&n(dr),d&&n(z),$(He),$(Ze),$(Je),$(Ke),d&&n(ir),d&&n(de),$(Qe),d&&n(lr),d&&n(u),$(Xe),$(io),$(ze),$(lo),$(co),$(po),d&&n(cr),d&&n(ve),$(mo),d&&n(hr),d&&n(_),$(fo),$(wo),$(Ae),$(To),$(xo),$(Do)}}}const Zp={local:"vision-encoder-decoder-models",sections:[{local:"transformers.VisionEncoderDecoderConfig",title:"VisionEncoderDecoderConfig"},{local:"transformers.VisionEncoderDecoderModel",title:"VisionEncoderDecoderModel"},{local:"transformers.FlaxVisionEncoderDecoderModel",title:"FlaxVisionEncoderDecoderModel"}],title:"Vision Encoder Decoder Models"};function Jp(Ge,p,L){let{fw:v}=p;return Ge.$$set=P=>{"fw"in P&&L(0,v=P.fw)},[v]}class tm extends Np{constructor(p){super();Rp(this,p,Jp,Hp,Bp,{fw:0})}}export{tm as default,Zp as metadata};
