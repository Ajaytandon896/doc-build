import{S as Ou,i as Uu,s as Hu,e as o,k as m,w as y,t as n,L as Qu,c as a,d as t,m as d,a as s,x as M,h as i,b as h,J as e,g as c,y as F,q as P,o as L,B as R}from"../../../chunks/vendor-b1433968.js";import{T as za}from"../../../chunks/Tip-c3840994.js";import{D as Me}from"../../../chunks/Docstring-ff504c58.js";import{I as Fe}from"../../../chunks/IconCopyLink-7029626d.js";function Vu(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue;return{c(){T=o("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),D=m(),v=o("ul"),z=o("li"),X=n("having all inputs as keyword arguments (like PyTorch models), or"),le=m(),I=o("li"),me=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=m(),u=o("p"),G=n("This second option is useful when using "),q=o("code"),W=n("tf.keras.Model.fit"),de=n(` method which currently requires having all
the tensors in the first argument of the model call function: `),x=o("code"),he=n("model(inputs)"),ae=n("."),H=m(),w=o("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),V=m(),g=o("ul"),b=o("li"),ee=n("a single Tensor with "),N=o("code"),se=n("input_ids"),te=n(" only and nothing else: "),S=o("code"),ce=n("model(inputs_ids)"),ne=m(),_=o("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=o("code"),re=n("model([input_ids, attention_mask])"),J=n(" or "),O=o("code"),oe=n("model([input_ids, attention_mask, token_type_ids])"),pe=m(),C=o("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=o("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){T=a(l,"P",{});var p=s(T);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),D=d(l),v=a(l,"UL",{});var K=s(v);z=a(K,"LI",{});var be=s(z);X=i(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=d(K),I=a(K,"LI",{});var we=s(I);me=i(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(t),K.forEach(t),Q=d(l),u=a(l,"P",{});var $=s(u);G=i($,"This second option is useful when using "),q=a($,"CODE",{});var ke=s(q);W=i(ke,"tf.keras.Model.fit"),ke.forEach(t),de=i($,` method which currently requires having all
the tensors in the first argument of the model call function: `),x=a($,"CODE",{});var ge=s(x);he=i(ge,"model(inputs)"),ge.forEach(t),ae=i($,"."),$.forEach(t),H=d(l),w=a(l,"P",{});var _e=s(w);j=i(_e,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),_e.forEach(t),V=d(l),g=a(l,"UL",{});var k=s(g);b=a(k,"LI",{});var A=s(b);ee=i(A,"a single Tensor with "),N=a(A,"CODE",{});var Ce=s(N);se=i(Ce,"input_ids"),Ce.forEach(t),te=i(A," only and nothing else: "),S=a(A,"CODE",{});var Te=s(S);ce=i(Te,"model(inputs_ids)"),Te.forEach(t),A.forEach(t),ne=d(k),_=a(k,"LI",{});var U=s(_);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=s(B);re=i(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),J=i(U," or "),O=a(U,"CODE",{});var ve=s(O);oe=i(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(t),U.forEach(t),pe=d(k),C=a(k,"LI",{});var Y=s(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=a(Y,"CODE",{});var $e=s(E);ue=i($e,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),$e.forEach(t),Y.forEach(t),k.forEach(t)},m(l,p){c(l,T,p),e(T,Z),c(l,D,p),c(l,v,p),e(v,z),e(z,X),e(v,le),e(v,I),e(I,me),c(l,Q,p),c(l,u,p),e(u,G),e(u,q),e(q,W),e(u,de),e(u,x),e(x,he),e(u,ae),c(l,H,p),c(l,w,p),e(w,j),c(l,V,p),c(l,g,p),e(g,b),e(b,ee),e(b,N),e(N,se),e(b,te),e(b,S),e(S,ce),e(g,ne),e(g,_),e(_,fe),e(_,B),e(B,re),e(_,J),e(_,O),e(O,oe),e(g,pe),e(g,C),e(C,ie),e(C,E),e(E,ue)},d(l){l&&t(T),l&&t(D),l&&t(v),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Ku(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue;return{c(){T=o("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),D=m(),v=o("ul"),z=o("li"),X=n("having all inputs as keyword arguments (like PyTorch models), or"),le=m(),I=o("li"),me=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=m(),u=o("p"),G=n("This second option is useful when using "),q=o("code"),W=n("tf.keras.Model.fit"),de=n(` method which currently requires having all
the tensors in the first argument of the model call function: `),x=o("code"),he=n("model(inputs)"),ae=n("."),H=m(),w=o("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),V=m(),g=o("ul"),b=o("li"),ee=n("a single Tensor with "),N=o("code"),se=n("input_ids"),te=n(" only and nothing else: "),S=o("code"),ce=n("model(inputs_ids)"),ne=m(),_=o("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=o("code"),re=n("model([input_ids, attention_mask])"),J=n(" or "),O=o("code"),oe=n("model([input_ids, attention_mask, token_type_ids])"),pe=m(),C=o("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=o("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){T=a(l,"P",{});var p=s(T);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),D=d(l),v=a(l,"UL",{});var K=s(v);z=a(K,"LI",{});var be=s(z);X=i(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=d(K),I=a(K,"LI",{});var we=s(I);me=i(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(t),K.forEach(t),Q=d(l),u=a(l,"P",{});var $=s(u);G=i($,"This second option is useful when using "),q=a($,"CODE",{});var ke=s(q);W=i(ke,"tf.keras.Model.fit"),ke.forEach(t),de=i($,` method which currently requires having all
the tensors in the first argument of the model call function: `),x=a($,"CODE",{});var ge=s(x);he=i(ge,"model(inputs)"),ge.forEach(t),ae=i($,"."),$.forEach(t),H=d(l),w=a(l,"P",{});var _e=s(w);j=i(_e,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),_e.forEach(t),V=d(l),g=a(l,"UL",{});var k=s(g);b=a(k,"LI",{});var A=s(b);ee=i(A,"a single Tensor with "),N=a(A,"CODE",{});var Ce=s(N);se=i(Ce,"input_ids"),Ce.forEach(t),te=i(A," only and nothing else: "),S=a(A,"CODE",{});var Te=s(S);ce=i(Te,"model(inputs_ids)"),Te.forEach(t),A.forEach(t),ne=d(k),_=a(k,"LI",{});var U=s(_);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=s(B);re=i(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),J=i(U," or "),O=a(U,"CODE",{});var ve=s(O);oe=i(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(t),U.forEach(t),pe=d(k),C=a(k,"LI",{});var Y=s(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=a(Y,"CODE",{});var $e=s(E);ue=i($e,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),$e.forEach(t),Y.forEach(t),k.forEach(t)},m(l,p){c(l,T,p),e(T,Z),c(l,D,p),c(l,v,p),e(v,z),e(z,X),e(v,le),e(v,I),e(I,me),c(l,Q,p),c(l,u,p),e(u,G),e(u,q),e(q,W),e(u,de),e(u,x),e(x,he),e(u,ae),c(l,H,p),c(l,w,p),e(w,j),c(l,V,p),c(l,g,p),e(g,b),e(b,ee),e(b,N),e(N,se),e(b,te),e(b,S),e(S,ce),e(g,ne),e(g,_),e(_,fe),e(_,B),e(B,re),e(_,J),e(_,O),e(O,oe),e(g,pe),e(g,C),e(C,ie),e(C,E),e(E,ue)},d(l){l&&t(T),l&&t(D),l&&t(v),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Gu(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue;return{c(){T=o("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),D=m(),v=o("ul"),z=o("li"),X=n("having all inputs as keyword arguments (like PyTorch models), or"),le=m(),I=o("li"),me=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=m(),u=o("p"),G=n("This second option is useful when using "),q=o("code"),W=n("tf.keras.Model.fit"),de=n(` method which currently requires having all
the tensors in the first argument of the model call function: `),x=o("code"),he=n("model(inputs)"),ae=n("."),H=m(),w=o("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),V=m(),g=o("ul"),b=o("li"),ee=n("a single Tensor with "),N=o("code"),se=n("input_ids"),te=n(" only and nothing else: "),S=o("code"),ce=n("model(inputs_ids)"),ne=m(),_=o("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=o("code"),re=n("model([input_ids, attention_mask])"),J=n(" or "),O=o("code"),oe=n("model([input_ids, attention_mask, token_type_ids])"),pe=m(),C=o("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=o("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){T=a(l,"P",{});var p=s(T);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),D=d(l),v=a(l,"UL",{});var K=s(v);z=a(K,"LI",{});var be=s(z);X=i(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=d(K),I=a(K,"LI",{});var we=s(I);me=i(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(t),K.forEach(t),Q=d(l),u=a(l,"P",{});var $=s(u);G=i($,"This second option is useful when using "),q=a($,"CODE",{});var ke=s(q);W=i(ke,"tf.keras.Model.fit"),ke.forEach(t),de=i($,` method which currently requires having all
the tensors in the first argument of the model call function: `),x=a($,"CODE",{});var ge=s(x);he=i(ge,"model(inputs)"),ge.forEach(t),ae=i($,"."),$.forEach(t),H=d(l),w=a(l,"P",{});var _e=s(w);j=i(_e,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),_e.forEach(t),V=d(l),g=a(l,"UL",{});var k=s(g);b=a(k,"LI",{});var A=s(b);ee=i(A,"a single Tensor with "),N=a(A,"CODE",{});var Ce=s(N);se=i(Ce,"input_ids"),Ce.forEach(t),te=i(A," only and nothing else: "),S=a(A,"CODE",{});var Te=s(S);ce=i(Te,"model(inputs_ids)"),Te.forEach(t),A.forEach(t),ne=d(k),_=a(k,"LI",{});var U=s(_);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=s(B);re=i(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),J=i(U," or "),O=a(U,"CODE",{});var ve=s(O);oe=i(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(t),U.forEach(t),pe=d(k),C=a(k,"LI",{});var Y=s(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=a(Y,"CODE",{});var $e=s(E);ue=i($e,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),$e.forEach(t),Y.forEach(t),k.forEach(t)},m(l,p){c(l,T,p),e(T,Z),c(l,D,p),c(l,v,p),e(v,z),e(z,X),e(v,le),e(v,I),e(I,me),c(l,Q,p),c(l,u,p),e(u,G),e(u,q),e(q,W),e(u,de),e(u,x),e(x,he),e(u,ae),c(l,H,p),c(l,w,p),e(w,j),c(l,V,p),c(l,g,p),e(g,b),e(b,ee),e(b,N),e(N,se),e(b,te),e(b,S),e(S,ce),e(g,ne),e(g,_),e(_,fe),e(_,B),e(B,re),e(_,J),e(_,O),e(O,oe),e(g,pe),e(g,C),e(C,ie),e(C,E),e(E,ue)},d(l){l&&t(T),l&&t(D),l&&t(v),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Xu(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue;return{c(){T=o("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),D=m(),v=o("ul"),z=o("li"),X=n("having all inputs as keyword arguments (like PyTorch models), or"),le=m(),I=o("li"),me=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=m(),u=o("p"),G=n("This second option is useful when using "),q=o("code"),W=n("tf.keras.Model.fit"),de=n(` method which currently requires having all
the tensors in the first argument of the model call function: `),x=o("code"),he=n("model(inputs)"),ae=n("."),H=m(),w=o("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),V=m(),g=o("ul"),b=o("li"),ee=n("a single Tensor with "),N=o("code"),se=n("input_ids"),te=n(" only and nothing else: "),S=o("code"),ce=n("model(inputs_ids)"),ne=m(),_=o("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=o("code"),re=n("model([input_ids, attention_mask])"),J=n(" or "),O=o("code"),oe=n("model([input_ids, attention_mask, token_type_ids])"),pe=m(),C=o("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=o("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){T=a(l,"P",{});var p=s(T);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),D=d(l),v=a(l,"UL",{});var K=s(v);z=a(K,"LI",{});var be=s(z);X=i(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=d(K),I=a(K,"LI",{});var we=s(I);me=i(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(t),K.forEach(t),Q=d(l),u=a(l,"P",{});var $=s(u);G=i($,"This second option is useful when using "),q=a($,"CODE",{});var ke=s(q);W=i(ke,"tf.keras.Model.fit"),ke.forEach(t),de=i($,` method which currently requires having all
the tensors in the first argument of the model call function: `),x=a($,"CODE",{});var ge=s(x);he=i(ge,"model(inputs)"),ge.forEach(t),ae=i($,"."),$.forEach(t),H=d(l),w=a(l,"P",{});var _e=s(w);j=i(_e,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),_e.forEach(t),V=d(l),g=a(l,"UL",{});var k=s(g);b=a(k,"LI",{});var A=s(b);ee=i(A,"a single Tensor with "),N=a(A,"CODE",{});var Ce=s(N);se=i(Ce,"input_ids"),Ce.forEach(t),te=i(A," only and nothing else: "),S=a(A,"CODE",{});var Te=s(S);ce=i(Te,"model(inputs_ids)"),Te.forEach(t),A.forEach(t),ne=d(k),_=a(k,"LI",{});var U=s(_);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=s(B);re=i(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),J=i(U," or "),O=a(U,"CODE",{});var ve=s(O);oe=i(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(t),U.forEach(t),pe=d(k),C=a(k,"LI",{});var Y=s(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=a(Y,"CODE",{});var $e=s(E);ue=i($e,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),$e.forEach(t),Y.forEach(t),k.forEach(t)},m(l,p){c(l,T,p),e(T,Z),c(l,D,p),c(l,v,p),e(v,z),e(z,X),e(v,le),e(v,I),e(I,me),c(l,Q,p),c(l,u,p),e(u,G),e(u,q),e(q,W),e(u,de),e(u,x),e(x,he),e(u,ae),c(l,H,p),c(l,w,p),e(w,j),c(l,V,p),c(l,g,p),e(g,b),e(b,ee),e(b,N),e(N,se),e(b,te),e(b,S),e(S,ce),e(g,ne),e(g,_),e(_,fe),e(_,B),e(B,re),e(_,J),e(_,O),e(O,oe),e(g,pe),e(g,C),e(C,ie),e(C,E),e(E,ue)},d(l){l&&t(T),l&&t(D),l&&t(v),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Wu(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue;return{c(){T=o("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),D=m(),v=o("ul"),z=o("li"),X=n("having all inputs as keyword arguments (like PyTorch models), or"),le=m(),I=o("li"),me=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=m(),u=o("p"),G=n("This second option is useful when using "),q=o("code"),W=n("tf.keras.Model.fit"),de=n(` method which currently requires having all
the tensors in the first argument of the model call function: `),x=o("code"),he=n("model(inputs)"),ae=n("."),H=m(),w=o("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),V=m(),g=o("ul"),b=o("li"),ee=n("a single Tensor with "),N=o("code"),se=n("input_ids"),te=n(" only and nothing else: "),S=o("code"),ce=n("model(inputs_ids)"),ne=m(),_=o("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=o("code"),re=n("model([input_ids, attention_mask])"),J=n(" or "),O=o("code"),oe=n("model([input_ids, attention_mask, token_type_ids])"),pe=m(),C=o("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=o("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){T=a(l,"P",{});var p=s(T);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),D=d(l),v=a(l,"UL",{});var K=s(v);z=a(K,"LI",{});var be=s(z);X=i(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=d(K),I=a(K,"LI",{});var we=s(I);me=i(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(t),K.forEach(t),Q=d(l),u=a(l,"P",{});var $=s(u);G=i($,"This second option is useful when using "),q=a($,"CODE",{});var ke=s(q);W=i(ke,"tf.keras.Model.fit"),ke.forEach(t),de=i($,` method which currently requires having all
the tensors in the first argument of the model call function: `),x=a($,"CODE",{});var ge=s(x);he=i(ge,"model(inputs)"),ge.forEach(t),ae=i($,"."),$.forEach(t),H=d(l),w=a(l,"P",{});var _e=s(w);j=i(_e,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),_e.forEach(t),V=d(l),g=a(l,"UL",{});var k=s(g);b=a(k,"LI",{});var A=s(b);ee=i(A,"a single Tensor with "),N=a(A,"CODE",{});var Ce=s(N);se=i(Ce,"input_ids"),Ce.forEach(t),te=i(A," only and nothing else: "),S=a(A,"CODE",{});var Te=s(S);ce=i(Te,"model(inputs_ids)"),Te.forEach(t),A.forEach(t),ne=d(k),_=a(k,"LI",{});var U=s(_);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=s(B);re=i(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),J=i(U," or "),O=a(U,"CODE",{});var ve=s(O);oe=i(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(t),U.forEach(t),pe=d(k),C=a(k,"LI",{});var Y=s(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=a(Y,"CODE",{});var $e=s(E);ue=i($e,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),$e.forEach(t),Y.forEach(t),k.forEach(t)},m(l,p){c(l,T,p),e(T,Z),c(l,D,p),c(l,v,p),e(v,z),e(z,X),e(v,le),e(v,I),e(I,me),c(l,Q,p),c(l,u,p),e(u,G),e(u,q),e(q,W),e(u,de),e(u,x),e(x,he),e(u,ae),c(l,H,p),c(l,w,p),e(w,j),c(l,V,p),c(l,g,p),e(g,b),e(b,ee),e(b,N),e(N,se),e(b,te),e(b,S),e(S,ce),e(g,ne),e(g,_),e(_,fe),e(_,B),e(B,re),e(_,J),e(_,O),e(O,oe),e(g,pe),e(g,C),e(C,ie),e(C,E),e(E,ue)},d(l){l&&t(T),l&&t(D),l&&t(v),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function ju(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue;return{c(){T=o("p"),Z=n("TF 2.0 models accepts two formats as inputs:"),D=m(),v=o("ul"),z=o("li"),X=n("having all inputs as keyword arguments (like PyTorch models), or"),le=m(),I=o("li"),me=n("having all inputs as a list, tuple or dict in the first positional arguments."),Q=m(),u=o("p"),G=n("This second option is useful when using "),q=o("code"),W=n("tf.keras.Model.fit"),de=n(` method which currently requires having all
the tensors in the first argument of the model call function: `),x=o("code"),he=n("model(inputs)"),ae=n("."),H=m(),w=o("p"),j=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),V=m(),g=o("ul"),b=o("li"),ee=n("a single Tensor with "),N=o("code"),se=n("input_ids"),te=n(" only and nothing else: "),S=o("code"),ce=n("model(inputs_ids)"),ne=m(),_=o("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=o("code"),re=n("model([input_ids, attention_mask])"),J=n(" or "),O=o("code"),oe=n("model([input_ids, attention_mask, token_type_ids])"),pe=m(),C=o("li"),ie=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=o("code"),ue=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(l){T=a(l,"P",{});var p=s(T);Z=i(p,"TF 2.0 models accepts two formats as inputs:"),p.forEach(t),D=d(l),v=a(l,"UL",{});var K=s(v);z=a(K,"LI",{});var be=s(z);X=i(be,"having all inputs as keyword arguments (like PyTorch models), or"),be.forEach(t),le=d(K),I=a(K,"LI",{});var we=s(I);me=i(we,"having all inputs as a list, tuple or dict in the first positional arguments."),we.forEach(t),K.forEach(t),Q=d(l),u=a(l,"P",{});var $=s(u);G=i($,"This second option is useful when using "),q=a($,"CODE",{});var ke=s(q);W=i(ke,"tf.keras.Model.fit"),ke.forEach(t),de=i($,` method which currently requires having all
the tensors in the first argument of the model call function: `),x=a($,"CODE",{});var ge=s(x);he=i(ge,"model(inputs)"),ge.forEach(t),ae=i($,"."),$.forEach(t),H=d(l),w=a(l,"P",{});var _e=s(w);j=i(_e,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in
the first positional argument :`),_e.forEach(t),V=d(l),g=a(l,"UL",{});var k=s(g);b=a(k,"LI",{});var A=s(b);ee=i(A,"a single Tensor with "),N=a(A,"CODE",{});var Ce=s(N);se=i(Ce,"input_ids"),Ce.forEach(t),te=i(A," only and nothing else: "),S=a(A,"CODE",{});var Te=s(S);ce=i(Te,"model(inputs_ids)"),Te.forEach(t),A.forEach(t),ne=d(k),_=a(k,"LI",{});var U=s(_);fe=i(U,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=a(U,"CODE",{});var Ee=s(B);re=i(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),J=i(U," or "),O=a(U,"CODE",{});var ve=s(O);oe=i(ve,"model([input_ids, attention_mask, token_type_ids])"),ve.forEach(t),U.forEach(t),pe=d(k),C=a(k,"LI",{});var Y=s(C);ie=i(Y,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),E=a(Y,"CODE",{});var $e=s(E);ue=i($e,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),$e.forEach(t),Y.forEach(t),k.forEach(t)},m(l,p){c(l,T,p),e(T,Z),c(l,D,p),c(l,v,p),e(v,z),e(z,X),e(v,le),e(v,I),e(I,me),c(l,Q,p),c(l,u,p),e(u,G),e(u,q),e(q,W),e(u,de),e(u,x),e(x,he),e(u,ae),c(l,H,p),c(l,w,p),e(w,j),c(l,V,p),c(l,g,p),e(g,b),e(b,ee),e(b,N),e(N,se),e(b,te),e(b,S),e(S,ce),e(g,ne),e(g,_),e(_,fe),e(_,B),e(B,re),e(_,J),e(_,O),e(O,oe),e(g,pe),e(g,C),e(C,ie),e(C,E),e(E,ue)},d(l){l&&t(T),l&&t(D),l&&t(v),l&&t(Q),l&&t(u),l&&t(H),l&&t(w),l&&t(V),l&&t(g)}}}function Ju(qe){let T,Z,D,v,z,X,le,I,me,Q,u,G,q,W,de,x,he,ae,H,w,j,V,g,b,ee,N,se,te,S,ce,ne,_,fe,B,re,J,O,oe,pe,C,ie,E,ue,l,p,K,be,we,$,ke,ge,_e,k,A,Ce,Te,U,Ee,ve,Y,$e,Yt,Bi,Go,Ni,Oi,Hs,dt,yt,Ia,Zt,Ui,qa,Hi,Qs,ye,er,Qi,Ye,Vi,Xo,Ki,Gi,Wo,Xi,Wi,tr,ji,Ji,Yi,rr,Zi,jo,el,tl,rl,or,ol,ar,al,xa,sl,nl,il,ll,Ze,sr,ml,Sa,dl,hl,nr,Jo,cl,Ba,fl,pl,Yo,ul,Na,gl,vl,Mt,ir,_l,lr,bl,Oa,Tl,kl,El,Ft,mr,wl,Ua,Cl,$l,Ha,Vs,ht,Pt,Qa,dr,yl,Va,Ml,Ks,xe,hr,Fl,Ve,Pl,Ka,Ll,Rl,Zo,Al,Dl,ea,zl,Il,cr,ql,xl,Sl,fr,Bl,ta,Nl,Ol,Ul,et,pr,Hl,Ga,Ql,Vl,ur,ra,Kl,Xa,Gl,Xl,oa,Wl,Wa,jl,Jl,Lt,gr,Yl,ja,Zl,Gs,ct,Rt,Ja,vr,em,Ya,tm,Xs,Se,_r,rm,Za,om,am,br,sm,aa,nm,im,lm,Tr,mm,kr,dm,hm,cm,Er,fm,sa,pm,um,Ws,ft,At,es,wr,gm,ts,vm,js,Be,Cr,_m,$r,bm,rs,Tm,km,Em,yr,wm,na,Cm,$m,ym,Mr,Mm,Fr,Fm,Pm,Lm,Pr,Rm,ia,Am,Dm,Js,pt,Dt,os,Lr,zm,as,Im,Ys,Ne,Rr,qm,Ar,xm,ss,Sm,Bm,Nm,Dr,Om,la,Um,Hm,Qm,zr,Vm,Ir,Km,Gm,Xm,qr,Wm,ma,jm,Jm,Zs,ut,zt,ns,xr,Ym,is,Zm,en,Oe,Sr,ed,ls,td,rd,Br,od,da,ad,sd,nd,Nr,id,Or,ld,md,dd,Ur,hd,ha,cd,fd,tn,gt,It,ms,Hr,pd,ds,ud,rn,Ue,Qr,gd,hs,vd,_d,Vr,bd,ca,Td,kd,Ed,Kr,wd,Gr,Cd,$d,yd,Xr,Md,fa,Fd,Pd,on,vt,qt,cs,Wr,Ld,fs,Rd,an,He,jr,Ad,ps,Dd,zd,Jr,Id,pa,qd,xd,Sd,Yr,Bd,Zr,Nd,Od,Ud,eo,Hd,ua,Qd,Vd,sn,_t,xt,us,to,Kd,gs,Gd,nn,Qe,ro,Xd,St,Wd,vs,jd,Jd,_s,Yd,Zd,oo,eh,ga,th,rh,oh,ao,ah,so,sh,nh,ih,no,lh,va,mh,dh,ln,bt,Bt,bs,io,hh,Ts,ch,mn,Pe,lo,fh,ks,ph,uh,mo,gh,_a,vh,_h,bh,ho,Th,co,kh,Eh,wh,Nt,Ch,fo,$h,ba,yh,Mh,dn,Tt,Ot,Es,po,Fh,ws,Ph,hn,Le,uo,Lh,go,Rh,Cs,Ah,Dh,zh,vo,Ih,Ta,qh,xh,Sh,_o,Bh,bo,Nh,Oh,Uh,Ut,Hh,To,Qh,ka,Vh,Kh,cn,kt,Ht,$s,ko,Gh,ys,Xh,fn,Re,Eo,Wh,Ms,jh,Jh,wo,Yh,Ea,Zh,ec,tc,Co,rc,$o,oc,ac,sc,Qt,nc,yo,ic,wa,lc,mc,pn,Et,Vt,Fs,Mo,dc,Ps,hc,un,Ae,Fo,cc,Ls,fc,pc,Po,uc,Ca,gc,vc,_c,Lo,bc,Ro,Tc,kc,Ec,Kt,wc,Ao,Cc,$a,$c,yc,gn,wt,Gt,Rs,Do,Mc,As,Fc,vn,De,zo,Pc,Ds,Lc,Rc,Io,Ac,ya,Dc,zc,Ic,qo,qc,xo,xc,Sc,Bc,Xt,Nc,So,Oc,Ma,Uc,Hc,_n,Ct,Wt,zs,Bo,Qc,Is,Vc,bn,ze,No,Kc,$t,Gc,qs,Xc,Wc,xs,jc,Jc,Yc,Oo,Zc,Fa,ef,tf,rf,Uo,of,Ho,af,sf,nf,jt,lf,Qo,mf,Pa,df,hf,Tn;return X=new Fe({}),W=new Fe({}),A=new Fe({}),Y=new Me({props:{name:"class transformers.CamembertConfig",anchor:"transformers.CamembertConfig",parameters:[{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/configuration_camembert.py#L35"}}),Zt=new Fe({}),er=new Me({props:{name:"class transformers.CamembertTokenizer",anchor:"transformers.CamembertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<s>NOTUSED', '</s>NOTUSED']"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert.py#L45",parametersDescription:[{anchor:"transformers.CamembertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.CamembertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"}]}}),sr=new Me({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert.py#L154",parametersDescription:[{anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ir=new Me({props:{name:"get_special_tokens_mask",anchor:"transformers.CamembertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert.py#L180",parametersDescription:[{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.CamembertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mr=new Me({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert.py#L207",parametersDescription:[{anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),dr=new Fe({}),hr=new Me({props:{name:"class transformers.CamembertTokenizerFast",anchor:"transformers.CamembertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"additional_special_tokens",val:" = ['<s>NOTUSED', '</s>NOTUSED']"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert_fast.py#L54",parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.CamembertTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"}]}}),pr=new Me({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert_fast.py#L140",parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),gr=new Me({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/tokenization_camembert_fast.py#L166",parametersDescription:[{anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CamembertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),vr=new Fe({}),_r=new Me({props:{name:"class transformers.CamembertModel",anchor:"transformers.CamembertModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L65",parametersDescription:[{anchor:"transformers.CamembertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),wr=new Fe({}),Cr=new Me({props:{name:"class transformers.CamembertForCausalLM",anchor:"transformers.CamembertForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L154",parametersDescription:[{anchor:"transformers.CamembertForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Lr=new Fe({}),Rr=new Me({props:{name:"class transformers.CamembertForMaskedLM",anchor:"transformers.CamembertForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L78",parametersDescription:[{anchor:"transformers.CamembertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),xr=new Fe({}),Sr=new Me({props:{name:"class transformers.CamembertForSequenceClassification",anchor:"transformers.CamembertForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L94",parametersDescription:[{anchor:"transformers.CamembertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Hr=new Fe({}),Qr=new Me({props:{name:"class transformers.CamembertForMultipleChoice",anchor:"transformers.CamembertForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L110",parametersDescription:[{anchor:"transformers.CamembertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Wr=new Fe({}),jr=new Me({props:{name:"class transformers.CamembertForTokenClassification",anchor:"transformers.CamembertForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L126",parametersDescription:[{anchor:"transformers.CamembertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),to=new Fe({}),ro=new Me({props:{name:"class transformers.CamembertForQuestionAnswering",anchor:"transformers.CamembertForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_camembert.py#L142",parametersDescription:[{anchor:"transformers.CamembertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),io=new Fe({}),lo=new Me({props:{name:"class transformers.TFCamembertModel",anchor:"transformers.TFCamembertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_tf_camembert.py#L79",parametersDescription:[{anchor:"transformers.TFCamembertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Nt=new za({props:{$$slots:{default:[Vu]},$$scope:{ctx:qe}}}),po=new Fe({}),uo=new Me({props:{name:"class transformers.TFCamembertForMaskedLM",anchor:"transformers.TFCamembertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_tf_camembert.py#L92",parametersDescription:[{anchor:"transformers.TFCamembertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ut=new za({props:{$$slots:{default:[Ku]},$$scope:{ctx:qe}}}),ko=new Fe({}),Eo=new Me({props:{name:"class transformers.TFCamembertForSequenceClassification",anchor:"transformers.TFCamembertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_tf_camembert.py#L108",parametersDescription:[{anchor:"transformers.TFCamembertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Qt=new za({props:{$$slots:{default:[Gu]},$$scope:{ctx:qe}}}),Mo=new Fe({}),Fo=new Me({props:{name:"class transformers.TFCamembertForMultipleChoice",anchor:"transformers.TFCamembertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_tf_camembert.py#L140",parametersDescription:[{anchor:"transformers.TFCamembertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Kt=new za({props:{$$slots:{default:[Xu]},$$scope:{ctx:qe}}}),Do=new Fe({}),zo=new Me({props:{name:"class transformers.TFCamembertForTokenClassification",anchor:"transformers.TFCamembertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_tf_camembert.py#L124",parametersDescription:[{anchor:"transformers.TFCamembertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Xt=new za({props:{$$slots:{default:[Wu]},$$scope:{ctx:qe}}}),Bo=new Fe({}),No=new Me({props:{name:"class transformers.TFCamembertForQuestionAnswering",anchor:"transformers.TFCamembertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/camembert/modeling_tf_camembert.py#L156",parametersDescription:[{anchor:"transformers.TFCamembertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a>) &#x2014; Model configuration class with all the parameters of the
model. Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),jt=new za({props:{$$slots:{default:[ju]},$$scope:{ctx:qe}}}),{c(){T=o("meta"),Z=m(),D=o("h1"),v=o("a"),z=o("span"),y(X.$$.fragment),le=m(),I=o("span"),me=n("CamemBERT"),Q=m(),u=o("h2"),G=o("a"),q=o("span"),y(W.$$.fragment),de=m(),x=o("span"),he=n("Overview"),ae=m(),H=o("p"),w=n("The CamemBERT model was proposed in "),j=o("a"),V=n("CamemBERT: a Tasty French Language Model"),g=n(` by
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\xE1rez, Yoann Dupont, Laurent Romary, \xC9ric Villemonte de la
Clergerie, Djam\xE9 Seddah, and Beno\xEEt Sagot. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a model
trained on 138GB of French text.`),b=m(),ee=o("p"),N=n("The abstract from the paper is the following:"),se=m(),te=o("p"),S=o("em"),ce=n(`Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available
models have either been trained on English data or on the concatenation of data in multiple languages. This makes
practical use of such models \u2014in all languages except English\u2014 very limited. Aiming to address this issue for French,
we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the
performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,
dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art
for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and
downstream applications for French NLP.`),ne=m(),_=o("p"),fe=n("Tips:"),B=m(),re=o("ul"),J=o("li"),O=n("This implementation is the same as RoBERTa. Refer to the "),oe=o("a"),pe=n("documentation of RoBERTa"),C=n(` for usage examples
as well as the information relative to the inputs and outputs.`),ie=m(),E=o("p"),ue=n("This model was contributed by "),l=o("a"),p=n("camembert"),K=n(". The original code can be found "),be=o("a"),we=n("here"),$=n("."),ke=m(),ge=o("h2"),_e=o("a"),k=o("span"),y(A.$$.fragment),Ce=m(),Te=o("span"),U=n("CamembertConfig"),Ee=m(),ve=o("div"),y(Y.$$.fragment),$e=m(),Yt=o("p"),Bi=n("This class overrides "),Go=o("a"),Ni=n("RobertaConfig"),Oi=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Hs=m(),dt=o("h2"),yt=o("a"),Ia=o("span"),y(Zt.$$.fragment),Ui=m(),qa=o("span"),Hi=n("CamembertTokenizer"),Qs=m(),ye=o("div"),y(er.$$.fragment),Qi=m(),Ye=o("p"),Vi=n("Adapted from "),Xo=o("a"),Ki=n("RobertaTokenizer"),Gi=n(" and "),Wo=o("a"),Xi=n("XLNetTokenizer"),Wi=n(`. Construct a
CamemBERT tokenizer. Based on `),tr=o("a"),ji=n("SentencePiece"),Ji=n("."),Yi=m(),rr=o("p"),Zi=n("This tokenizer inherits from "),jo=o("a"),el=n("PreTrainedTokenizer"),tl=n(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),rl=m(),or=o("p"),ol=n(`Attributes:
sp`),ar=o("em"),al=n("model ("),xa=o("code"),sl=n("SentencePieceProcessor"),nl=n(`):
The _SentencePiece`),il=n(" processor that is used for every conversion (string, tokens and IDs)."),ll=m(),Ze=o("div"),y(sr.$$.fragment),ml=m(),Sa=o("p"),dl=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),hl=m(),nr=o("ul"),Jo=o("li"),cl=n("single sequence: "),Ba=o("code"),fl=n("<s> X </s>"),pl=m(),Yo=o("li"),ul=n("pair of sequences: "),Na=o("code"),gl=n("<s> A </s></s> B </s>"),vl=m(),Mt=o("div"),y(ir.$$.fragment),_l=m(),lr=o("p"),bl=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Oa=o("code"),Tl=n("prepare_for_model"),kl=n(" method."),El=m(),Ft=o("div"),y(mr.$$.fragment),wl=m(),Ua=o("p"),Cl=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),$l=m(),Ha=o("div"),Vs=m(),ht=o("h2"),Pt=o("a"),Qa=o("span"),y(dr.$$.fragment),yl=m(),Va=o("span"),Ml=n("CamembertTokenizerFast"),Ks=m(),xe=o("div"),y(hr.$$.fragment),Fl=m(),Ve=o("p"),Pl=n("Construct a \u201Cfast\u201D CamemBERT tokenizer (backed by HuggingFace\u2019s "),Ka=o("em"),Ll=n("tokenizers"),Rl=n(` library). Adapted from
`),Zo=o("a"),Al=n("RobertaTokenizer"),Dl=n(" and "),ea=o("a"),zl=n("XLNetTokenizer"),Il=n(". Based on "),cr=o("a"),ql=n("BPE"),xl=n("."),Sl=m(),fr=o("p"),Bl=n("This tokenizer inherits from "),ta=o("a"),Nl=n("PreTrainedTokenizerFast"),Ol=n(` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),Ul=m(),et=o("div"),y(pr.$$.fragment),Hl=m(),Ga=o("p"),Ql=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Vl=m(),ur=o("ul"),ra=o("li"),Kl=n("single sequence: "),Xa=o("code"),Gl=n("<s> X </s>"),Xl=m(),oa=o("li"),Wl=n("pair of sequences: "),Wa=o("code"),jl=n("<s> A </s></s> B </s>"),Jl=m(),Lt=o("div"),y(gr.$$.fragment),Yl=m(),ja=o("p"),Zl=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Gs=m(),ct=o("h2"),Rt=o("a"),Ja=o("span"),y(vr.$$.fragment),em=m(),Ya=o("span"),tm=n("CamembertModel"),Xs=m(),Se=o("div"),y(_r.$$.fragment),rm=m(),Za=o("p"),om=n("The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),am=m(),br=o("p"),sm=n("This model inherits from "),aa=o("a"),nm=n("PreTrainedModel"),im=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),lm=m(),Tr=o("p"),mm=n("This model is also a PyTorch "),kr=o("a"),dm=n("torch.nn.Module"),hm=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),cm=m(),Er=o("p"),fm=n("This class overrides "),sa=o("a"),pm=n("RobertaModel"),um=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ws=m(),ft=o("h2"),At=o("a"),es=o("span"),y(wr.$$.fragment),gm=m(),ts=o("span"),vm=n("CamembertForCausalLM"),js=m(),Be=o("div"),y(Cr.$$.fragment),_m=m(),$r=o("p"),bm=n("CamemBERT Model with a "),rs=o("em"),Tm=n("language modeling"),km=n(" head on top for CLM fine-tuning."),Em=m(),yr=o("p"),wm=n("This model inherits from "),na=o("a"),Cm=n("PreTrainedModel"),$m=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ym=m(),Mr=o("p"),Mm=n("This model is also a PyTorch "),Fr=o("a"),Fm=n("torch.nn.Module"),Pm=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Lm=m(),Pr=o("p"),Rm=n("This class overrides "),ia=o("a"),Am=n("RobertaForCausalLM"),Dm=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Js=m(),pt=o("h2"),Dt=o("a"),os=o("span"),y(Lr.$$.fragment),zm=m(),as=o("span"),Im=n("CamembertForMaskedLM"),Ys=m(),Ne=o("div"),y(Rr.$$.fragment),qm=m(),Ar=o("p"),xm=n("CamemBERT Model with a "),ss=o("em"),Sm=n("language modeling"),Bm=n(" head on top."),Nm=m(),Dr=o("p"),Om=n("This model inherits from "),la=o("a"),Um=n("PreTrainedModel"),Hm=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Qm=m(),zr=o("p"),Vm=n("This model is also a PyTorch "),Ir=o("a"),Km=n("torch.nn.Module"),Gm=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Xm=m(),qr=o("p"),Wm=n("This class overrides "),ma=o("a"),jm=n("RobertaForMaskedLM"),Jm=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Zs=m(),ut=o("h2"),zt=o("a"),ns=o("span"),y(xr.$$.fragment),Ym=m(),is=o("span"),Zm=n("CamembertForSequenceClassification"),en=m(),Oe=o("div"),y(Sr.$$.fragment),ed=m(),ls=o("p"),td=n(`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),rd=m(),Br=o("p"),od=n("This model inherits from "),da=o("a"),ad=n("PreTrainedModel"),sd=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),nd=m(),Nr=o("p"),id=n("This model is also a PyTorch "),Or=o("a"),ld=n("torch.nn.Module"),md=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),dd=m(),Ur=o("p"),hd=n("This class overrides "),ha=o("a"),cd=n("RobertaForSequenceClassification"),fd=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),tn=m(),gt=o("h2"),It=o("a"),ms=o("span"),y(Hr.$$.fragment),pd=m(),ds=o("span"),ud=n("CamembertForMultipleChoice"),rn=m(),Ue=o("div"),y(Qr.$$.fragment),gd=m(),hs=o("p"),vd=n(`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),_d=m(),Vr=o("p"),bd=n("This model inherits from "),ca=o("a"),Td=n("PreTrainedModel"),kd=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Ed=m(),Kr=o("p"),wd=n("This model is also a PyTorch "),Gr=o("a"),Cd=n("torch.nn.Module"),$d=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),yd=m(),Xr=o("p"),Md=n("This class overrides "),fa=o("a"),Fd=n("RobertaForMultipleChoice"),Pd=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),on=m(),vt=o("h2"),qt=o("a"),cs=o("span"),y(Wr.$$.fragment),Ld=m(),fs=o("span"),Rd=n("CamembertForTokenClassification"),an=m(),He=o("div"),y(jr.$$.fragment),Ad=m(),ps=o("p"),Dd=n(`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),zd=m(),Jr=o("p"),Id=n("This model inherits from "),pa=o("a"),qd=n("PreTrainedModel"),xd=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Sd=m(),Yr=o("p"),Bd=n("This model is also a PyTorch "),Zr=o("a"),Nd=n("torch.nn.Module"),Od=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ud=m(),eo=o("p"),Hd=n("This class overrides "),ua=o("a"),Qd=n("RobertaForTokenClassification"),Vd=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),sn=m(),_t=o("h2"),xt=o("a"),us=o("span"),y(to.$$.fragment),Kd=m(),gs=o("span"),Gd=n("CamembertForQuestionAnswering"),nn=m(),Qe=o("div"),y(ro.$$.fragment),Xd=m(),St=o("p"),Wd=n(`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),vs=o("em"),jd=n("span start logits"),Jd=n(" and "),_s=o("em"),Yd=n("span end logits"),Zd=m(),oo=o("p"),eh=n("This model inherits from "),ga=o("a"),th=n("PreTrainedModel"),rh=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),oh=m(),ao=o("p"),ah=n("This model is also a PyTorch "),so=o("a"),sh=n("torch.nn.Module"),nh=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ih=m(),no=o("p"),lh=n("This class overrides "),va=o("a"),mh=n("RobertaForQuestionAnswering"),dh=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),ln=m(),bt=o("h2"),Bt=o("a"),bs=o("span"),y(io.$$.fragment),hh=m(),Ts=o("span"),ch=n("TFCamembertModel"),mn=m(),Pe=o("div"),y(lo.$$.fragment),fh=m(),ks=o("p"),ph=n("The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),uh=m(),mo=o("p"),gh=n("This model inherits from "),_a=o("a"),vh=n("TFPreTrainedModel"),_h=n(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),bh=m(),ho=o("p"),Th=n("This model is also a "),co=o("a"),kh=n("tf.keras.Model"),Eh=n(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),wh=m(),y(Nt.$$.fragment),Ch=m(),fo=o("p"),$h=n("This class overrides "),ba=o("a"),yh=n("TFRobertaModel"),Mh=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),dn=m(),Tt=o("h2"),Ot=o("a"),Es=o("span"),y(po.$$.fragment),Fh=m(),ws=o("span"),Ph=n("TFCamembertForMaskedLM"),hn=m(),Le=o("div"),y(uo.$$.fragment),Lh=m(),go=o("p"),Rh=n("CamemBERT Model with a "),Cs=o("em"),Ah=n("language modeling"),Dh=n(" head on top."),zh=m(),vo=o("p"),Ih=n("This model inherits from "),Ta=o("a"),qh=n("TFPreTrainedModel"),xh=n(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Sh=m(),_o=o("p"),Bh=n("This model is also a "),bo=o("a"),Nh=n("tf.keras.Model"),Oh=n(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Uh=m(),y(Ut.$$.fragment),Hh=m(),To=o("p"),Qh=n("This class overrides "),ka=o("a"),Vh=n("TFRobertaForMaskedLM"),Kh=n(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),cn=m(),kt=o("h2"),Ht=o("a"),$s=o("span"),y(ko.$$.fragment),Gh=m(),ys=o("span"),Xh=n("TFCamembertForSequenceClassification"),fn=m(),Re=o("div"),y(Eo.$$.fragment),Wh=m(),Ms=o("p"),jh=n(`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Jh=m(),wo=o("p"),Yh=n("This model inherits from "),Ea=o("a"),Zh=n("TFPreTrainedModel"),ec=n(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),tc=m(),Co=o("p"),rc=n("This model is also a "),$o=o("a"),oc=n("tf.keras.Model"),ac=n(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),sc=m(),y(Qt.$$.fragment),nc=m(),yo=o("p"),ic=n("This class overrides "),wa=o("a"),lc=n("TFRobertaForSequenceClassification"),mc=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),pn=m(),Et=o("h2"),Vt=o("a"),Fs=o("span"),y(Mo.$$.fragment),dc=m(),Ps=o("span"),hc=n("TFCamembertForMultipleChoice"),un=m(),Ae=o("div"),y(Fo.$$.fragment),cc=m(),Ls=o("p"),fc=n(`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),pc=m(),Po=o("p"),uc=n("This model inherits from "),Ca=o("a"),gc=n("TFPreTrainedModel"),vc=n(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),_c=m(),Lo=o("p"),bc=n("This model is also a "),Ro=o("a"),Tc=n("tf.keras.Model"),kc=n(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Ec=m(),y(Kt.$$.fragment),wc=m(),Ao=o("p"),Cc=n("This class overrides "),$a=o("a"),$c=n("TFRobertaForMultipleChoice"),yc=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),gn=m(),wt=o("h2"),Gt=o("a"),Rs=o("span"),y(Do.$$.fragment),Mc=m(),As=o("span"),Fc=n("TFCamembertForTokenClassification"),vn=m(),De=o("div"),y(zo.$$.fragment),Pc=m(),Ds=o("p"),Lc=n(`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Rc=m(),Io=o("p"),Ac=n("This model inherits from "),ya=o("a"),Dc=n("TFPreTrainedModel"),zc=n(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ic=m(),qo=o("p"),qc=n("This model is also a "),xo=o("a"),xc=n("tf.keras.Model"),Sc=n(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Bc=m(),y(Xt.$$.fragment),Nc=m(),So=o("p"),Oc=n("This class overrides "),Ma=o("a"),Uc=n("TFRobertaForTokenClassification"),Hc=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),_n=m(),Ct=o("h2"),Wt=o("a"),zs=o("span"),y(Bo.$$.fragment),Qc=m(),Is=o("span"),Vc=n("TFCamembertForQuestionAnswering"),bn=m(),ze=o("div"),y(No.$$.fragment),Kc=m(),$t=o("p"),Gc=n(`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),qs=o("em"),Xc=n("span start logits"),Wc=n(" and "),xs=o("em"),jc=n("span end logits"),Jc=n(")."),Yc=m(),Oo=o("p"),Zc=n("This model inherits from "),Fa=o("a"),ef=n("TFPreTrainedModel"),tf=n(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),rf=m(),Uo=o("p"),of=n("This model is also a "),Ho=o("a"),af=n("tf.keras.Model"),sf=n(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),nf=m(),y(jt.$$.fragment),lf=m(),Qo=o("p"),mf=n("This class overrides "),Pa=o("a"),df=n("TFRobertaForQuestionAnswering"),hf=n(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),this.h()},l(r){const f=Qu('[data-svelte="svelte-1phssyn"]',document.head);T=a(f,"META",{name:!0,content:!0}),f.forEach(t),Z=d(r),D=a(r,"H1",{class:!0});var Vo=s(D);v=a(Vo,"A",{id:!0,class:!0,href:!0});var Ss=s(v);z=a(Ss,"SPAN",{});var Bs=s(z);M(X.$$.fragment,Bs),Bs.forEach(t),Ss.forEach(t),le=d(Vo),I=a(Vo,"SPAN",{});var Ns=s(I);me=i(Ns,"CamemBERT"),Ns.forEach(t),Vo.forEach(t),Q=d(r),u=a(r,"H2",{class:!0});var Ko=s(u);G=a(Ko,"A",{id:!0,class:!0,href:!0});var Os=s(G);q=a(Os,"SPAN",{});var gf=s(q);M(W.$$.fragment,gf),gf.forEach(t),Os.forEach(t),de=d(Ko),x=a(Ko,"SPAN",{});var vf=s(x);he=i(vf,"Overview"),vf.forEach(t),Ko.forEach(t),ae=d(r),H=a(r,"P",{});var kn=s(H);w=i(kn,"The CamemBERT model was proposed in "),j=a(kn,"A",{href:!0,rel:!0});var _f=s(j);V=i(_f,"CamemBERT: a Tasty French Language Model"),_f.forEach(t),g=i(kn,` by
Louis Martin, Benjamin Muller, Pedro Javier Ortiz Su\xE1rez, Yoann Dupont, Laurent Romary, \xC9ric Villemonte de la
Clergerie, Djam\xE9 Seddah, and Beno\xEEt Sagot. It is based on Facebook\u2019s RoBERTa model released in 2019. It is a model
trained on 138GB of French text.`),kn.forEach(t),b=d(r),ee=a(r,"P",{});var bf=s(ee);N=i(bf,"The abstract from the paper is the following:"),bf.forEach(t),se=d(r),te=a(r,"P",{});var Tf=s(te);S=a(Tf,"EM",{});var kf=s(S);ce=i(kf,`Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available
models have either been trained on English data or on the concatenation of data in multiple languages. This makes
practical use of such models \u2014in all languages except English\u2014 very limited. Aiming to address this issue for French,
we release CamemBERT, a French version of the Bi-directional Encoders for Transformers (BERT). We measure the
performance of CamemBERT compared to multilingual models in multiple downstream tasks, namely part-of-speech tagging,
dependency parsing, named-entity recognition, and natural language inference. CamemBERT improves the state of the art
for most of the tasks considered. We release the pretrained model for CamemBERT hoping to foster research and
downstream applications for French NLP.`),kf.forEach(t),Tf.forEach(t),ne=d(r),_=a(r,"P",{});var Ef=s(_);fe=i(Ef,"Tips:"),Ef.forEach(t),B=d(r),re=a(r,"UL",{});var wf=s(re);J=a(wf,"LI",{});var En=s(J);O=i(En,"This implementation is the same as RoBERTa. Refer to the "),oe=a(En,"A",{href:!0});var Cf=s(oe);pe=i(Cf,"documentation of RoBERTa"),Cf.forEach(t),C=i(En,` for usage examples
as well as the information relative to the inputs and outputs.`),En.forEach(t),wf.forEach(t),ie=d(r),E=a(r,"P",{});var La=s(E);ue=i(La,"This model was contributed by "),l=a(La,"A",{href:!0,rel:!0});var $f=s(l);p=i($f,"camembert"),$f.forEach(t),K=i(La,". The original code can be found "),be=a(La,"A",{href:!0,rel:!0});var yf=s(be);we=i(yf,"here"),yf.forEach(t),$=i(La,"."),La.forEach(t),ke=d(r),ge=a(r,"H2",{class:!0});var wn=s(ge);_e=a(wn,"A",{id:!0,class:!0,href:!0});var Mf=s(_e);k=a(Mf,"SPAN",{});var Ff=s(k);M(A.$$.fragment,Ff),Ff.forEach(t),Mf.forEach(t),Ce=d(wn),Te=a(wn,"SPAN",{});var Pf=s(Te);U=i(Pf,"CamembertConfig"),Pf.forEach(t),wn.forEach(t),Ee=d(r),ve=a(r,"DIV",{class:!0});var Cn=s(ve);M(Y.$$.fragment,Cn),$e=d(Cn),Yt=a(Cn,"P",{});var $n=s(Yt);Bi=i($n,"This class overrides "),Go=a($n,"A",{href:!0});var Lf=s(Go);Ni=i(Lf,"RobertaConfig"),Lf.forEach(t),Oi=i($n,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),$n.forEach(t),Cn.forEach(t),Hs=d(r),dt=a(r,"H2",{class:!0});var yn=s(dt);yt=a(yn,"A",{id:!0,class:!0,href:!0});var Rf=s(yt);Ia=a(Rf,"SPAN",{});var Af=s(Ia);M(Zt.$$.fragment,Af),Af.forEach(t),Rf.forEach(t),Ui=d(yn),qa=a(yn,"SPAN",{});var Df=s(qa);Hi=i(Df,"CamembertTokenizer"),Df.forEach(t),yn.forEach(t),Qs=d(r),ye=a(r,"DIV",{class:!0});var Ie=s(ye);M(er.$$.fragment,Ie),Qi=d(Ie),Ye=a(Ie,"P",{});var Jt=s(Ye);Vi=i(Jt,"Adapted from "),Xo=a(Jt,"A",{href:!0});var zf=s(Xo);Ki=i(zf,"RobertaTokenizer"),zf.forEach(t),Gi=i(Jt," and "),Wo=a(Jt,"A",{href:!0});var If=s(Wo);Xi=i(If,"XLNetTokenizer"),If.forEach(t),Wi=i(Jt,`. Construct a
CamemBERT tokenizer. Based on `),tr=a(Jt,"A",{href:!0,rel:!0});var qf=s(tr);ji=i(qf,"SentencePiece"),qf.forEach(t),Ji=i(Jt,"."),Jt.forEach(t),Yi=d(Ie),rr=a(Ie,"P",{});var Mn=s(rr);Zi=i(Mn,"This tokenizer inherits from "),jo=a(Mn,"A",{href:!0});var xf=s(jo);el=i(xf,"PreTrainedTokenizer"),xf.forEach(t),tl=i(Mn,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Mn.forEach(t),rl=d(Ie),or=a(Ie,"P",{});var Fn=s(or);ol=i(Fn,`Attributes:
sp`),ar=a(Fn,"EM",{});var Pn=s(ar);al=i(Pn,"model ("),xa=a(Pn,"CODE",{});var Sf=s(xa);sl=i(Sf,"SentencePieceProcessor"),Sf.forEach(t),nl=i(Pn,`):
The _SentencePiece`),Pn.forEach(t),il=i(Fn," processor that is used for every conversion (string, tokens and IDs)."),Fn.forEach(t),ll=d(Ie),Ze=a(Ie,"DIV",{class:!0});var Ra=s(Ze);M(sr.$$.fragment,Ra),ml=d(Ra),Sa=a(Ra,"P",{});var Bf=s(Sa);dl=i(Bf,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Bf.forEach(t),hl=d(Ra),nr=a(Ra,"UL",{});var Ln=s(nr);Jo=a(Ln,"LI",{});var cf=s(Jo);cl=i(cf,"single sequence: "),Ba=a(cf,"CODE",{});var Nf=s(Ba);fl=i(Nf,"<s> X </s>"),Nf.forEach(t),cf.forEach(t),pl=d(Ln),Yo=a(Ln,"LI",{});var ff=s(Yo);ul=i(ff,"pair of sequences: "),Na=a(ff,"CODE",{});var Of=s(Na);gl=i(Of,"<s> A </s></s> B </s>"),Of.forEach(t),ff.forEach(t),Ln.forEach(t),Ra.forEach(t),vl=d(Ie),Mt=a(Ie,"DIV",{class:!0});var Rn=s(Mt);M(ir.$$.fragment,Rn),_l=d(Rn),lr=a(Rn,"P",{});var An=s(lr);bl=i(An,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Oa=a(An,"CODE",{});var Uf=s(Oa);Tl=i(Uf,"prepare_for_model"),Uf.forEach(t),kl=i(An," method."),An.forEach(t),Rn.forEach(t),El=d(Ie),Ft=a(Ie,"DIV",{class:!0});var Dn=s(Ft);M(mr.$$.fragment,Dn),wl=d(Dn),Ua=a(Dn,"P",{});var Hf=s(Ua);Cl=i(Hf,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),Hf.forEach(t),Dn.forEach(t),$l=d(Ie),Ha=a(Ie,"DIV",{class:!0}),s(Ha).forEach(t),Ie.forEach(t),Vs=d(r),ht=a(r,"H2",{class:!0});var zn=s(ht);Pt=a(zn,"A",{id:!0,class:!0,href:!0});var Qf=s(Pt);Qa=a(Qf,"SPAN",{});var Vf=s(Qa);M(dr.$$.fragment,Vf),Vf.forEach(t),Qf.forEach(t),yl=d(zn),Va=a(zn,"SPAN",{});var Kf=s(Va);Ml=i(Kf,"CamembertTokenizerFast"),Kf.forEach(t),zn.forEach(t),Ks=d(r),xe=a(r,"DIV",{class:!0});var tt=s(xe);M(hr.$$.fragment,tt),Fl=d(tt),Ve=a(tt,"P",{});var rt=s(Ve);Pl=i(rt,"Construct a \u201Cfast\u201D CamemBERT tokenizer (backed by HuggingFace\u2019s "),Ka=a(rt,"EM",{});var Gf=s(Ka);Ll=i(Gf,"tokenizers"),Gf.forEach(t),Rl=i(rt,` library). Adapted from
`),Zo=a(rt,"A",{href:!0});var Xf=s(Zo);Al=i(Xf,"RobertaTokenizer"),Xf.forEach(t),Dl=i(rt," and "),ea=a(rt,"A",{href:!0});var Wf=s(ea);zl=i(Wf,"XLNetTokenizer"),Wf.forEach(t),Il=i(rt,". Based on "),cr=a(rt,"A",{href:!0,rel:!0});var jf=s(cr);ql=i(jf,"BPE"),jf.forEach(t),xl=i(rt,"."),rt.forEach(t),Sl=d(tt),fr=a(tt,"P",{});var In=s(fr);Bl=i(In,"This tokenizer inherits from "),ta=a(In,"A",{href:!0});var Jf=s(ta);Nl=i(Jf,"PreTrainedTokenizerFast"),Jf.forEach(t),Ol=i(In,` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),In.forEach(t),Ul=d(tt),et=a(tt,"DIV",{class:!0});var Aa=s(et);M(pr.$$.fragment,Aa),Hl=d(Aa),Ga=a(Aa,"P",{});var Yf=s(Ga);Ql=i(Yf,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An CamemBERT sequence has the following format:`),Yf.forEach(t),Vl=d(Aa),ur=a(Aa,"UL",{});var qn=s(ur);ra=a(qn,"LI",{});var pf=s(ra);Kl=i(pf,"single sequence: "),Xa=a(pf,"CODE",{});var Zf=s(Xa);Gl=i(Zf,"<s> X </s>"),Zf.forEach(t),pf.forEach(t),Xl=d(qn),oa=a(qn,"LI",{});var uf=s(oa);Wl=i(uf,"pair of sequences: "),Wa=a(uf,"CODE",{});var ep=s(Wa);jl=i(ep,"<s> A </s></s> B </s>"),ep.forEach(t),uf.forEach(t),qn.forEach(t),Aa.forEach(t),Jl=d(tt),Lt=a(tt,"DIV",{class:!0});var xn=s(Lt);M(gr.$$.fragment,xn),Yl=d(xn),ja=a(xn,"P",{});var tp=s(ja);Zl=i(tp,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. CamemBERT, like
RoBERTa, does not make use of token type ids, therefore a list of zeros is returned.`),tp.forEach(t),xn.forEach(t),tt.forEach(t),Gs=d(r),ct=a(r,"H2",{class:!0});var Sn=s(ct);Rt=a(Sn,"A",{id:!0,class:!0,href:!0});var rp=s(Rt);Ja=a(rp,"SPAN",{});var op=s(Ja);M(vr.$$.fragment,op),op.forEach(t),rp.forEach(t),em=d(Sn),Ya=a(Sn,"SPAN",{});var ap=s(Ya);tm=i(ap,"CamembertModel"),ap.forEach(t),Sn.forEach(t),Xs=d(r),Se=a(r,"DIV",{class:!0});var ot=s(Se);M(_r.$$.fragment,ot),rm=d(ot),Za=a(ot,"P",{});var sp=s(Za);om=i(sp,"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),sp.forEach(t),am=d(ot),br=a(ot,"P",{});var Bn=s(br);sm=i(Bn,"This model inherits from "),aa=a(Bn,"A",{href:!0});var np=s(aa);nm=i(np,"PreTrainedModel"),np.forEach(t),im=i(Bn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Bn.forEach(t),lm=d(ot),Tr=a(ot,"P",{});var Nn=s(Tr);mm=i(Nn,"This model is also a PyTorch "),kr=a(Nn,"A",{href:!0,rel:!0});var ip=s(kr);dm=i(ip,"torch.nn.Module"),ip.forEach(t),hm=i(Nn,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Nn.forEach(t),cm=d(ot),Er=a(ot,"P",{});var On=s(Er);fm=i(On,"This class overrides "),sa=a(On,"A",{href:!0});var lp=s(sa);pm=i(lp,"RobertaModel"),lp.forEach(t),um=i(On,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),On.forEach(t),ot.forEach(t),Ws=d(r),ft=a(r,"H2",{class:!0});var Un=s(ft);At=a(Un,"A",{id:!0,class:!0,href:!0});var mp=s(At);es=a(mp,"SPAN",{});var dp=s(es);M(wr.$$.fragment,dp),dp.forEach(t),mp.forEach(t),gm=d(Un),ts=a(Un,"SPAN",{});var hp=s(ts);vm=i(hp,"CamembertForCausalLM"),hp.forEach(t),Un.forEach(t),js=d(r),Be=a(r,"DIV",{class:!0});var at=s(Be);M(Cr.$$.fragment,at),_m=d(at),$r=a(at,"P",{});var Hn=s($r);bm=i(Hn,"CamemBERT Model with a "),rs=a(Hn,"EM",{});var cp=s(rs);Tm=i(cp,"language modeling"),cp.forEach(t),km=i(Hn," head on top for CLM fine-tuning."),Hn.forEach(t),Em=d(at),yr=a(at,"P",{});var Qn=s(yr);wm=i(Qn,"This model inherits from "),na=a(Qn,"A",{href:!0});var fp=s(na);Cm=i(fp,"PreTrainedModel"),fp.forEach(t),$m=i(Qn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Qn.forEach(t),ym=d(at),Mr=a(at,"P",{});var Vn=s(Mr);Mm=i(Vn,"This model is also a PyTorch "),Fr=a(Vn,"A",{href:!0,rel:!0});var pp=s(Fr);Fm=i(pp,"torch.nn.Module"),pp.forEach(t),Pm=i(Vn,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Vn.forEach(t),Lm=d(at),Pr=a(at,"P",{});var Kn=s(Pr);Rm=i(Kn,"This class overrides "),ia=a(Kn,"A",{href:!0});var up=s(ia);Am=i(up,"RobertaForCausalLM"),up.forEach(t),Dm=i(Kn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Kn.forEach(t),at.forEach(t),Js=d(r),pt=a(r,"H2",{class:!0});var Gn=s(pt);Dt=a(Gn,"A",{id:!0,class:!0,href:!0});var gp=s(Dt);os=a(gp,"SPAN",{});var vp=s(os);M(Lr.$$.fragment,vp),vp.forEach(t),gp.forEach(t),zm=d(Gn),as=a(Gn,"SPAN",{});var _p=s(as);Im=i(_p,"CamembertForMaskedLM"),_p.forEach(t),Gn.forEach(t),Ys=d(r),Ne=a(r,"DIV",{class:!0});var st=s(Ne);M(Rr.$$.fragment,st),qm=d(st),Ar=a(st,"P",{});var Xn=s(Ar);xm=i(Xn,"CamemBERT Model with a "),ss=a(Xn,"EM",{});var bp=s(ss);Sm=i(bp,"language modeling"),bp.forEach(t),Bm=i(Xn," head on top."),Xn.forEach(t),Nm=d(st),Dr=a(st,"P",{});var Wn=s(Dr);Om=i(Wn,"This model inherits from "),la=a(Wn,"A",{href:!0});var Tp=s(la);Um=i(Tp,"PreTrainedModel"),Tp.forEach(t),Hm=i(Wn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Wn.forEach(t),Qm=d(st),zr=a(st,"P",{});var jn=s(zr);Vm=i(jn,"This model is also a PyTorch "),Ir=a(jn,"A",{href:!0,rel:!0});var kp=s(Ir);Km=i(kp,"torch.nn.Module"),kp.forEach(t),Gm=i(jn,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),jn.forEach(t),Xm=d(st),qr=a(st,"P",{});var Jn=s(qr);Wm=i(Jn,"This class overrides "),ma=a(Jn,"A",{href:!0});var Ep=s(ma);jm=i(Ep,"RobertaForMaskedLM"),Ep.forEach(t),Jm=i(Jn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jn.forEach(t),st.forEach(t),Zs=d(r),ut=a(r,"H2",{class:!0});var Yn=s(ut);zt=a(Yn,"A",{id:!0,class:!0,href:!0});var wp=s(zt);ns=a(wp,"SPAN",{});var Cp=s(ns);M(xr.$$.fragment,Cp),Cp.forEach(t),wp.forEach(t),Ym=d(Yn),is=a(Yn,"SPAN",{});var $p=s(is);Zm=i($p,"CamembertForSequenceClassification"),$p.forEach(t),Yn.forEach(t),en=d(r),Oe=a(r,"DIV",{class:!0});var nt=s(Oe);M(Sr.$$.fragment,nt),ed=d(nt),ls=a(nt,"P",{});var yp=s(ls);td=i(yp,`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),yp.forEach(t),rd=d(nt),Br=a(nt,"P",{});var Zn=s(Br);od=i(Zn,"This model inherits from "),da=a(Zn,"A",{href:!0});var Mp=s(da);ad=i(Mp,"PreTrainedModel"),Mp.forEach(t),sd=i(Zn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Zn.forEach(t),nd=d(nt),Nr=a(nt,"P",{});var ei=s(Nr);id=i(ei,"This model is also a PyTorch "),Or=a(ei,"A",{href:!0,rel:!0});var Fp=s(Or);ld=i(Fp,"torch.nn.Module"),Fp.forEach(t),md=i(ei,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ei.forEach(t),dd=d(nt),Ur=a(nt,"P",{});var ti=s(Ur);hd=i(ti,"This class overrides "),ha=a(ti,"A",{href:!0});var Pp=s(ha);cd=i(Pp,"RobertaForSequenceClassification"),Pp.forEach(t),fd=i(ti,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),ti.forEach(t),nt.forEach(t),tn=d(r),gt=a(r,"H2",{class:!0});var ri=s(gt);It=a(ri,"A",{id:!0,class:!0,href:!0});var Lp=s(It);ms=a(Lp,"SPAN",{});var Rp=s(ms);M(Hr.$$.fragment,Rp),Rp.forEach(t),Lp.forEach(t),pd=d(ri),ds=a(ri,"SPAN",{});var Ap=s(ds);ud=i(Ap,"CamembertForMultipleChoice"),Ap.forEach(t),ri.forEach(t),rn=d(r),Ue=a(r,"DIV",{class:!0});var it=s(Ue);M(Qr.$$.fragment,it),gd=d(it),hs=a(it,"P",{});var Dp=s(hs);vd=i(Dp,`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Dp.forEach(t),_d=d(it),Vr=a(it,"P",{});var oi=s(Vr);bd=i(oi,"This model inherits from "),ca=a(oi,"A",{href:!0});var zp=s(ca);Td=i(zp,"PreTrainedModel"),zp.forEach(t),kd=i(oi,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),oi.forEach(t),Ed=d(it),Kr=a(it,"P",{});var ai=s(Kr);wd=i(ai,"This model is also a PyTorch "),Gr=a(ai,"A",{href:!0,rel:!0});var Ip=s(Gr);Cd=i(Ip,"torch.nn.Module"),Ip.forEach(t),$d=i(ai,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ai.forEach(t),yd=d(it),Xr=a(it,"P",{});var si=s(Xr);Md=i(si,"This class overrides "),fa=a(si,"A",{href:!0});var qp=s(fa);Fd=i(qp,"RobertaForMultipleChoice"),qp.forEach(t),Pd=i(si,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),si.forEach(t),it.forEach(t),on=d(r),vt=a(r,"H2",{class:!0});var ni=s(vt);qt=a(ni,"A",{id:!0,class:!0,href:!0});var xp=s(qt);cs=a(xp,"SPAN",{});var Sp=s(cs);M(Wr.$$.fragment,Sp),Sp.forEach(t),xp.forEach(t),Ld=d(ni),fs=a(ni,"SPAN",{});var Bp=s(fs);Rd=i(Bp,"CamembertForTokenClassification"),Bp.forEach(t),ni.forEach(t),an=d(r),He=a(r,"DIV",{class:!0});var lt=s(He);M(jr.$$.fragment,lt),Ad=d(lt),ps=a(lt,"P",{});var Np=s(ps);Dd=i(Np,`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Np.forEach(t),zd=d(lt),Jr=a(lt,"P",{});var ii=s(Jr);Id=i(ii,"This model inherits from "),pa=a(ii,"A",{href:!0});var Op=s(pa);qd=i(Op,"PreTrainedModel"),Op.forEach(t),xd=i(ii,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ii.forEach(t),Sd=d(lt),Yr=a(lt,"P",{});var li=s(Yr);Bd=i(li,"This model is also a PyTorch "),Zr=a(li,"A",{href:!0,rel:!0});var Up=s(Zr);Nd=i(Up,"torch.nn.Module"),Up.forEach(t),Od=i(li,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),li.forEach(t),Ud=d(lt),eo=a(lt,"P",{});var mi=s(eo);Hd=i(mi,"This class overrides "),ua=a(mi,"A",{href:!0});var Hp=s(ua);Qd=i(Hp,"RobertaForTokenClassification"),Hp.forEach(t),Vd=i(mi,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),mi.forEach(t),lt.forEach(t),sn=d(r),_t=a(r,"H2",{class:!0});var di=s(_t);xt=a(di,"A",{id:!0,class:!0,href:!0});var Qp=s(xt);us=a(Qp,"SPAN",{});var Vp=s(us);M(to.$$.fragment,Vp),Vp.forEach(t),Qp.forEach(t),Kd=d(di),gs=a(di,"SPAN",{});var Kp=s(gs);Gd=i(Kp,"CamembertForQuestionAnswering"),Kp.forEach(t),di.forEach(t),nn=d(r),Qe=a(r,"DIV",{class:!0});var mt=s(Qe);M(ro.$$.fragment,mt),Xd=d(mt),St=a(mt,"P",{});var Us=s(St);Wd=i(Us,`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),vs=a(Us,"EM",{});var Gp=s(vs);jd=i(Gp,"span start logits"),Gp.forEach(t),Jd=i(Us," and "),_s=a(Us,"EM",{});var Xp=s(_s);Yd=i(Xp,"span end logits"),Xp.forEach(t),Us.forEach(t),Zd=d(mt),oo=a(mt,"P",{});var hi=s(oo);eh=i(hi,"This model inherits from "),ga=a(hi,"A",{href:!0});var Wp=s(ga);th=i(Wp,"PreTrainedModel"),Wp.forEach(t),rh=i(hi,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),hi.forEach(t),oh=d(mt),ao=a(mt,"P",{});var ci=s(ao);ah=i(ci,"This model is also a PyTorch "),so=a(ci,"A",{href:!0,rel:!0});var jp=s(so);sh=i(jp,"torch.nn.Module"),jp.forEach(t),nh=i(ci,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ci.forEach(t),ih=d(mt),no=a(mt,"P",{});var fi=s(no);lh=i(fi,"This class overrides "),va=a(fi,"A",{href:!0});var Jp=s(va);mh=i(Jp,"RobertaForQuestionAnswering"),Jp.forEach(t),dh=i(fi,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),fi.forEach(t),mt.forEach(t),ln=d(r),bt=a(r,"H2",{class:!0});var pi=s(bt);Bt=a(pi,"A",{id:!0,class:!0,href:!0});var Yp=s(Bt);bs=a(Yp,"SPAN",{});var Zp=s(bs);M(io.$$.fragment,Zp),Zp.forEach(t),Yp.forEach(t),hh=d(pi),Ts=a(pi,"SPAN",{});var eu=s(Ts);ch=i(eu,"TFCamembertModel"),eu.forEach(t),pi.forEach(t),mn=d(r),Pe=a(r,"DIV",{class:!0});var Ke=s(Pe);M(lo.$$.fragment,Ke),fh=d(Ke),ks=a(Ke,"P",{});var tu=s(ks);ph=i(tu,"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top."),tu.forEach(t),uh=d(Ke),mo=a(Ke,"P",{});var ui=s(mo);gh=i(ui,"This model inherits from "),_a=a(ui,"A",{href:!0});var ru=s(_a);vh=i(ru,"TFPreTrainedModel"),ru.forEach(t),_h=i(ui,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),ui.forEach(t),bh=d(Ke),ho=a(Ke,"P",{});var gi=s(ho);Th=i(gi,"This model is also a "),co=a(gi,"A",{href:!0,rel:!0});var ou=s(co);kh=i(ou,"tf.keras.Model"),ou.forEach(t),Eh=i(gi,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),gi.forEach(t),wh=d(Ke),M(Nt.$$.fragment,Ke),Ch=d(Ke),fo=a(Ke,"P",{});var vi=s(fo);$h=i(vi,"This class overrides "),ba=a(vi,"A",{href:!0});var au=s(ba);yh=i(au,"TFRobertaModel"),au.forEach(t),Mh=i(vi,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),vi.forEach(t),Ke.forEach(t),dn=d(r),Tt=a(r,"H2",{class:!0});var _i=s(Tt);Ot=a(_i,"A",{id:!0,class:!0,href:!0});var su=s(Ot);Es=a(su,"SPAN",{});var nu=s(Es);M(po.$$.fragment,nu),nu.forEach(t),su.forEach(t),Fh=d(_i),ws=a(_i,"SPAN",{});var iu=s(ws);Ph=i(iu,"TFCamembertForMaskedLM"),iu.forEach(t),_i.forEach(t),hn=d(r),Le=a(r,"DIV",{class:!0});var Ge=s(Le);M(uo.$$.fragment,Ge),Lh=d(Ge),go=a(Ge,"P",{});var bi=s(go);Rh=i(bi,"CamemBERT Model with a "),Cs=a(bi,"EM",{});var lu=s(Cs);Ah=i(lu,"language modeling"),lu.forEach(t),Dh=i(bi," head on top."),bi.forEach(t),zh=d(Ge),vo=a(Ge,"P",{});var Ti=s(vo);Ih=i(Ti,"This model inherits from "),Ta=a(Ti,"A",{href:!0});var mu=s(Ta);qh=i(mu,"TFPreTrainedModel"),mu.forEach(t),xh=i(Ti,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ti.forEach(t),Sh=d(Ge),_o=a(Ge,"P",{});var ki=s(_o);Bh=i(ki,"This model is also a "),bo=a(ki,"A",{href:!0,rel:!0});var du=s(bo);Nh=i(du,"tf.keras.Model"),du.forEach(t),Oh=i(ki,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),ki.forEach(t),Uh=d(Ge),M(Ut.$$.fragment,Ge),Hh=d(Ge),To=a(Ge,"P",{});var Ei=s(To);Qh=i(Ei,"This class overrides "),ka=a(Ei,"A",{href:!0});var hu=s(ka);Vh=i(hu,"TFRobertaForMaskedLM"),hu.forEach(t),Kh=i(Ei,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ei.forEach(t),Ge.forEach(t),cn=d(r),kt=a(r,"H2",{class:!0});var wi=s(kt);Ht=a(wi,"A",{id:!0,class:!0,href:!0});var cu=s(Ht);$s=a(cu,"SPAN",{});var fu=s($s);M(ko.$$.fragment,fu),fu.forEach(t),cu.forEach(t),Gh=d(wi),ys=a(wi,"SPAN",{});var pu=s(ys);Xh=i(pu,"TFCamembertForSequenceClassification"),pu.forEach(t),wi.forEach(t),fn=d(r),Re=a(r,"DIV",{class:!0});var Xe=s(Re);M(Eo.$$.fragment,Xe),Wh=d(Xe),Ms=a(Xe,"P",{});var uu=s(Ms);jh=i(uu,`CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),uu.forEach(t),Jh=d(Xe),wo=a(Xe,"P",{});var Ci=s(wo);Yh=i(Ci,"This model inherits from "),Ea=a(Ci,"A",{href:!0});var gu=s(Ea);Zh=i(gu,"TFPreTrainedModel"),gu.forEach(t),ec=i(Ci,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ci.forEach(t),tc=d(Xe),Co=a(Xe,"P",{});var $i=s(Co);rc=i($i,"This model is also a "),$o=a($i,"A",{href:!0,rel:!0});var vu=s($o);oc=i(vu,"tf.keras.Model"),vu.forEach(t),ac=i($i,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),$i.forEach(t),sc=d(Xe),M(Qt.$$.fragment,Xe),nc=d(Xe),yo=a(Xe,"P",{});var yi=s(yo);ic=i(yi,"This class overrides "),wa=a(yi,"A",{href:!0});var _u=s(wa);lc=i(_u,"TFRobertaForSequenceClassification"),_u.forEach(t),mc=i(yi,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),yi.forEach(t),Xe.forEach(t),pn=d(r),Et=a(r,"H2",{class:!0});var Mi=s(Et);Vt=a(Mi,"A",{id:!0,class:!0,href:!0});var bu=s(Vt);Fs=a(bu,"SPAN",{});var Tu=s(Fs);M(Mo.$$.fragment,Tu),Tu.forEach(t),bu.forEach(t),dc=d(Mi),Ps=a(Mi,"SPAN",{});var ku=s(Ps);hc=i(ku,"TFCamembertForMultipleChoice"),ku.forEach(t),Mi.forEach(t),un=d(r),Ae=a(r,"DIV",{class:!0});var We=s(Ae);M(Fo.$$.fragment,We),cc=d(We),Ls=a(We,"P",{});var Eu=s(Ls);fc=i(Eu,`CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Eu.forEach(t),pc=d(We),Po=a(We,"P",{});var Fi=s(Po);uc=i(Fi,"This model inherits from "),Ca=a(Fi,"A",{href:!0});var wu=s(Ca);gc=i(wu,"TFPreTrainedModel"),wu.forEach(t),vc=i(Fi,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Fi.forEach(t),_c=d(We),Lo=a(We,"P",{});var Pi=s(Lo);bc=i(Pi,"This model is also a "),Ro=a(Pi,"A",{href:!0,rel:!0});var Cu=s(Ro);Tc=i(Cu,"tf.keras.Model"),Cu.forEach(t),kc=i(Pi,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Pi.forEach(t),Ec=d(We),M(Kt.$$.fragment,We),wc=d(We),Ao=a(We,"P",{});var Li=s(Ao);Cc=i(Li,"This class overrides "),$a=a(Li,"A",{href:!0});var $u=s($a);$c=i($u,"TFRobertaForMultipleChoice"),$u.forEach(t),yc=i(Li,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Li.forEach(t),We.forEach(t),gn=d(r),wt=a(r,"H2",{class:!0});var Ri=s(wt);Gt=a(Ri,"A",{id:!0,class:!0,href:!0});var yu=s(Gt);Rs=a(yu,"SPAN",{});var Mu=s(Rs);M(Do.$$.fragment,Mu),Mu.forEach(t),yu.forEach(t),Mc=d(Ri),As=a(Ri,"SPAN",{});var Fu=s(As);Fc=i(Fu,"TFCamembertForTokenClassification"),Fu.forEach(t),Ri.forEach(t),vn=d(r),De=a(r,"DIV",{class:!0});var je=s(De);M(zo.$$.fragment,je),Pc=d(je),Ds=a(je,"P",{});var Pu=s(Ds);Lc=i(Pu,`CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Pu.forEach(t),Rc=d(je),Io=a(je,"P",{});var Ai=s(Io);Ac=i(Ai,"This model inherits from "),ya=a(Ai,"A",{href:!0});var Lu=s(ya);Dc=i(Lu,"TFPreTrainedModel"),Lu.forEach(t),zc=i(Ai,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ai.forEach(t),Ic=d(je),qo=a(je,"P",{});var Di=s(qo);qc=i(Di,"This model is also a "),xo=a(Di,"A",{href:!0,rel:!0});var Ru=s(xo);xc=i(Ru,"tf.keras.Model"),Ru.forEach(t),Sc=i(Di,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Di.forEach(t),Bc=d(je),M(Xt.$$.fragment,je),Nc=d(je),So=a(je,"P",{});var zi=s(So);Oc=i(zi,"This class overrides "),Ma=a(zi,"A",{href:!0});var Au=s(Ma);Uc=i(Au,"TFRobertaForTokenClassification"),Au.forEach(t),Hc=i(zi,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),zi.forEach(t),je.forEach(t),_n=d(r),Ct=a(r,"H2",{class:!0});var Ii=s(Ct);Wt=a(Ii,"A",{id:!0,class:!0,href:!0});var Du=s(Wt);zs=a(Du,"SPAN",{});var zu=s(zs);M(Bo.$$.fragment,zu),zu.forEach(t),Du.forEach(t),Qc=d(Ii),Is=a(Ii,"SPAN",{});var Iu=s(Is);Vc=i(Iu,"TFCamembertForQuestionAnswering"),Iu.forEach(t),Ii.forEach(t),bn=d(r),ze=a(r,"DIV",{class:!0});var Je=s(ze);M(No.$$.fragment,Je),Kc=d(Je),$t=a(Je,"P",{});var Da=s($t);Gc=i(Da,`CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),qs=a(Da,"EM",{});var qu=s(qs);Xc=i(qu,"span start logits"),qu.forEach(t),Wc=i(Da," and "),xs=a(Da,"EM",{});var xu=s(xs);jc=i(xu,"span end logits"),xu.forEach(t),Jc=i(Da,")."),Da.forEach(t),Yc=d(Je),Oo=a(Je,"P",{});var qi=s(Oo);Zc=i(qi,"This model inherits from "),Fa=a(qi,"A",{href:!0});var Su=s(Fa);ef=i(Su,"TFPreTrainedModel"),Su.forEach(t),tf=i(qi,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),qi.forEach(t),rf=d(Je),Uo=a(Je,"P",{});var xi=s(Uo);of=i(xi,"This model is also a "),Ho=a(xi,"A",{href:!0,rel:!0});var Bu=s(Ho);af=i(Bu,"tf.keras.Model"),Bu.forEach(t),sf=i(xi,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),xi.forEach(t),nf=d(Je),M(jt.$$.fragment,Je),lf=d(Je),Qo=a(Je,"P",{});var Si=s(Qo);mf=i(Si,"This class overrides "),Pa=a(Si,"A",{href:!0});var Nu=s(Pa);df=i(Nu,"TFRobertaForQuestionAnswering"),Nu.forEach(t),hf=i(Si,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Si.forEach(t),Je.forEach(t),this.h()},h(){h(T,"name","hf:doc:metadata"),h(T,"content",JSON.stringify(Yu)),h(v,"id","camembert"),h(v,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(v,"href","#camembert"),h(D,"class","relative group"),h(G,"id","overview"),h(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(G,"href","#overview"),h(u,"class","relative group"),h(j,"href","https://arxiv.org/abs/1911.03894"),h(j,"rel","nofollow"),h(oe,"href","/docs/transformers/v4.14.1/en/roberta"),h(l,"href","https://huggingface.co/camembert"),h(l,"rel","nofollow"),h(be,"href","https://camembert-model.fr/"),h(be,"rel","nofollow"),h(_e,"id","transformers.CamembertConfig"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#transformers.CamembertConfig"),h(ge,"class","relative group"),h(Go,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaConfig"),h(ve,"class","docstring"),h(yt,"id","transformers.CamembertTokenizer"),h(yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(yt,"href","#transformers.CamembertTokenizer"),h(dt,"class","relative group"),h(Xo,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizer"),h(Wo,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetTokenizer"),h(tr,"href","https://github.com/google/sentencepiece"),h(tr,"rel","nofollow"),h(jo,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),h(Ze,"class","docstring"),h(Mt,"class","docstring"),h(Ft,"class","docstring"),h(Ha,"class","docstring"),h(ye,"class","docstring"),h(Pt,"id","transformers.CamembertTokenizerFast"),h(Pt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Pt,"href","#transformers.CamembertTokenizerFast"),h(ht,"class","relative group"),h(Zo,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaTokenizer"),h(ea,"href","/docs/transformers/v4.14.1/en/model_doc/xlnet#transformers.XLNetTokenizer"),h(cr,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models"),h(cr,"rel","nofollow"),h(ta,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),h(et,"class","docstring"),h(Lt,"class","docstring"),h(xe,"class","docstring"),h(Rt,"id","transformers.CamembertModel"),h(Rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Rt,"href","#transformers.CamembertModel"),h(ct,"class","relative group"),h(aa,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(kr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(kr,"rel","nofollow"),h(sa,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaModel"),h(Se,"class","docstring"),h(At,"id","transformers.CamembertForCausalLM"),h(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(At,"href","#transformers.CamembertForCausalLM"),h(ft,"class","relative group"),h(na,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(Fr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Fr,"rel","nofollow"),h(ia,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForCausalLM"),h(Be,"class","docstring"),h(Dt,"id","transformers.CamembertForMaskedLM"),h(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Dt,"href","#transformers.CamembertForMaskedLM"),h(pt,"class","relative group"),h(la,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(Ir,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Ir,"rel","nofollow"),h(ma,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMaskedLM"),h(Ne,"class","docstring"),h(zt,"id","transformers.CamembertForSequenceClassification"),h(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(zt,"href","#transformers.CamembertForSequenceClassification"),h(ut,"class","relative group"),h(da,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(Or,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Or,"rel","nofollow"),h(ha,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),h(Oe,"class","docstring"),h(It,"id","transformers.CamembertForMultipleChoice"),h(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(It,"href","#transformers.CamembertForMultipleChoice"),h(gt,"class","relative group"),h(ca,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(Gr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Gr,"rel","nofollow"),h(fa,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),h(Ue,"class","docstring"),h(qt,"id","transformers.CamembertForTokenClassification"),h(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(qt,"href","#transformers.CamembertForTokenClassification"),h(vt,"class","relative group"),h(pa,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(Zr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Zr,"rel","nofollow"),h(ua,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForTokenClassification"),h(He,"class","docstring"),h(xt,"id","transformers.CamembertForQuestionAnswering"),h(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(xt,"href","#transformers.CamembertForQuestionAnswering"),h(_t,"class","relative group"),h(ga,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),h(so,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(so,"rel","nofollow"),h(va,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),h(Qe,"class","docstring"),h(Bt,"id","transformers.TFCamembertModel"),h(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Bt,"href","#transformers.TFCamembertModel"),h(bt,"class","relative group"),h(_a,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),h(co,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(co,"rel","nofollow"),h(ba,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaModel"),h(Pe,"class","docstring"),h(Ot,"id","transformers.TFCamembertForMaskedLM"),h(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ot,"href","#transformers.TFCamembertForMaskedLM"),h(Tt,"class","relative group"),h(Ta,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),h(bo,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(bo,"rel","nofollow"),h(ka,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),h(Le,"class","docstring"),h(Ht,"id","transformers.TFCamembertForSequenceClassification"),h(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ht,"href","#transformers.TFCamembertForSequenceClassification"),h(kt,"class","relative group"),h(Ea,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),h($o,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h($o,"rel","nofollow"),h(wa,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),h(Re,"class","docstring"),h(Vt,"id","transformers.TFCamembertForMultipleChoice"),h(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Vt,"href","#transformers.TFCamembertForMultipleChoice"),h(Et,"class","relative group"),h(Ca,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),h(Ro,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(Ro,"rel","nofollow"),h($a,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),h(Ae,"class","docstring"),h(Gt,"id","transformers.TFCamembertForTokenClassification"),h(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Gt,"href","#transformers.TFCamembertForTokenClassification"),h(wt,"class","relative group"),h(ya,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),h(xo,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(xo,"rel","nofollow"),h(Ma,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),h(De,"class","docstring"),h(Wt,"id","transformers.TFCamembertForQuestionAnswering"),h(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Wt,"href","#transformers.TFCamembertForQuestionAnswering"),h(Ct,"class","relative group"),h(Fa,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),h(Ho,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(Ho,"rel","nofollow"),h(Pa,"href","/docs/transformers/v4.14.1/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),h(ze,"class","docstring")},m(r,f){e(document.head,T),c(r,Z,f),c(r,D,f),e(D,v),e(v,z),F(X,z,null),e(D,le),e(D,I),e(I,me),c(r,Q,f),c(r,u,f),e(u,G),e(G,q),F(W,q,null),e(u,de),e(u,x),e(x,he),c(r,ae,f),c(r,H,f),e(H,w),e(H,j),e(j,V),e(H,g),c(r,b,f),c(r,ee,f),e(ee,N),c(r,se,f),c(r,te,f),e(te,S),e(S,ce),c(r,ne,f),c(r,_,f),e(_,fe),c(r,B,f),c(r,re,f),e(re,J),e(J,O),e(J,oe),e(oe,pe),e(J,C),c(r,ie,f),c(r,E,f),e(E,ue),e(E,l),e(l,p),e(E,K),e(E,be),e(be,we),e(E,$),c(r,ke,f),c(r,ge,f),e(ge,_e),e(_e,k),F(A,k,null),e(ge,Ce),e(ge,Te),e(Te,U),c(r,Ee,f),c(r,ve,f),F(Y,ve,null),e(ve,$e),e(ve,Yt),e(Yt,Bi),e(Yt,Go),e(Go,Ni),e(Yt,Oi),c(r,Hs,f),c(r,dt,f),e(dt,yt),e(yt,Ia),F(Zt,Ia,null),e(dt,Ui),e(dt,qa),e(qa,Hi),c(r,Qs,f),c(r,ye,f),F(er,ye,null),e(ye,Qi),e(ye,Ye),e(Ye,Vi),e(Ye,Xo),e(Xo,Ki),e(Ye,Gi),e(Ye,Wo),e(Wo,Xi),e(Ye,Wi),e(Ye,tr),e(tr,ji),e(Ye,Ji),e(ye,Yi),e(ye,rr),e(rr,Zi),e(rr,jo),e(jo,el),e(rr,tl),e(ye,rl),e(ye,or),e(or,ol),e(or,ar),e(ar,al),e(ar,xa),e(xa,sl),e(ar,nl),e(or,il),e(ye,ll),e(ye,Ze),F(sr,Ze,null),e(Ze,ml),e(Ze,Sa),e(Sa,dl),e(Ze,hl),e(Ze,nr),e(nr,Jo),e(Jo,cl),e(Jo,Ba),e(Ba,fl),e(nr,pl),e(nr,Yo),e(Yo,ul),e(Yo,Na),e(Na,gl),e(ye,vl),e(ye,Mt),F(ir,Mt,null),e(Mt,_l),e(Mt,lr),e(lr,bl),e(lr,Oa),e(Oa,Tl),e(lr,kl),e(ye,El),e(ye,Ft),F(mr,Ft,null),e(Ft,wl),e(Ft,Ua),e(Ua,Cl),e(ye,$l),e(ye,Ha),c(r,Vs,f),c(r,ht,f),e(ht,Pt),e(Pt,Qa),F(dr,Qa,null),e(ht,yl),e(ht,Va),e(Va,Ml),c(r,Ks,f),c(r,xe,f),F(hr,xe,null),e(xe,Fl),e(xe,Ve),e(Ve,Pl),e(Ve,Ka),e(Ka,Ll),e(Ve,Rl),e(Ve,Zo),e(Zo,Al),e(Ve,Dl),e(Ve,ea),e(ea,zl),e(Ve,Il),e(Ve,cr),e(cr,ql),e(Ve,xl),e(xe,Sl),e(xe,fr),e(fr,Bl),e(fr,ta),e(ta,Nl),e(fr,Ol),e(xe,Ul),e(xe,et),F(pr,et,null),e(et,Hl),e(et,Ga),e(Ga,Ql),e(et,Vl),e(et,ur),e(ur,ra),e(ra,Kl),e(ra,Xa),e(Xa,Gl),e(ur,Xl),e(ur,oa),e(oa,Wl),e(oa,Wa),e(Wa,jl),e(xe,Jl),e(xe,Lt),F(gr,Lt,null),e(Lt,Yl),e(Lt,ja),e(ja,Zl),c(r,Gs,f),c(r,ct,f),e(ct,Rt),e(Rt,Ja),F(vr,Ja,null),e(ct,em),e(ct,Ya),e(Ya,tm),c(r,Xs,f),c(r,Se,f),F(_r,Se,null),e(Se,rm),e(Se,Za),e(Za,om),e(Se,am),e(Se,br),e(br,sm),e(br,aa),e(aa,nm),e(br,im),e(Se,lm),e(Se,Tr),e(Tr,mm),e(Tr,kr),e(kr,dm),e(Tr,hm),e(Se,cm),e(Se,Er),e(Er,fm),e(Er,sa),e(sa,pm),e(Er,um),c(r,Ws,f),c(r,ft,f),e(ft,At),e(At,es),F(wr,es,null),e(ft,gm),e(ft,ts),e(ts,vm),c(r,js,f),c(r,Be,f),F(Cr,Be,null),e(Be,_m),e(Be,$r),e($r,bm),e($r,rs),e(rs,Tm),e($r,km),e(Be,Em),e(Be,yr),e(yr,wm),e(yr,na),e(na,Cm),e(yr,$m),e(Be,ym),e(Be,Mr),e(Mr,Mm),e(Mr,Fr),e(Fr,Fm),e(Mr,Pm),e(Be,Lm),e(Be,Pr),e(Pr,Rm),e(Pr,ia),e(ia,Am),e(Pr,Dm),c(r,Js,f),c(r,pt,f),e(pt,Dt),e(Dt,os),F(Lr,os,null),e(pt,zm),e(pt,as),e(as,Im),c(r,Ys,f),c(r,Ne,f),F(Rr,Ne,null),e(Ne,qm),e(Ne,Ar),e(Ar,xm),e(Ar,ss),e(ss,Sm),e(Ar,Bm),e(Ne,Nm),e(Ne,Dr),e(Dr,Om),e(Dr,la),e(la,Um),e(Dr,Hm),e(Ne,Qm),e(Ne,zr),e(zr,Vm),e(zr,Ir),e(Ir,Km),e(zr,Gm),e(Ne,Xm),e(Ne,qr),e(qr,Wm),e(qr,ma),e(ma,jm),e(qr,Jm),c(r,Zs,f),c(r,ut,f),e(ut,zt),e(zt,ns),F(xr,ns,null),e(ut,Ym),e(ut,is),e(is,Zm),c(r,en,f),c(r,Oe,f),F(Sr,Oe,null),e(Oe,ed),e(Oe,ls),e(ls,td),e(Oe,rd),e(Oe,Br),e(Br,od),e(Br,da),e(da,ad),e(Br,sd),e(Oe,nd),e(Oe,Nr),e(Nr,id),e(Nr,Or),e(Or,ld),e(Nr,md),e(Oe,dd),e(Oe,Ur),e(Ur,hd),e(Ur,ha),e(ha,cd),e(Ur,fd),c(r,tn,f),c(r,gt,f),e(gt,It),e(It,ms),F(Hr,ms,null),e(gt,pd),e(gt,ds),e(ds,ud),c(r,rn,f),c(r,Ue,f),F(Qr,Ue,null),e(Ue,gd),e(Ue,hs),e(hs,vd),e(Ue,_d),e(Ue,Vr),e(Vr,bd),e(Vr,ca),e(ca,Td),e(Vr,kd),e(Ue,Ed),e(Ue,Kr),e(Kr,wd),e(Kr,Gr),e(Gr,Cd),e(Kr,$d),e(Ue,yd),e(Ue,Xr),e(Xr,Md),e(Xr,fa),e(fa,Fd),e(Xr,Pd),c(r,on,f),c(r,vt,f),e(vt,qt),e(qt,cs),F(Wr,cs,null),e(vt,Ld),e(vt,fs),e(fs,Rd),c(r,an,f),c(r,He,f),F(jr,He,null),e(He,Ad),e(He,ps),e(ps,Dd),e(He,zd),e(He,Jr),e(Jr,Id),e(Jr,pa),e(pa,qd),e(Jr,xd),e(He,Sd),e(He,Yr),e(Yr,Bd),e(Yr,Zr),e(Zr,Nd),e(Yr,Od),e(He,Ud),e(He,eo),e(eo,Hd),e(eo,ua),e(ua,Qd),e(eo,Vd),c(r,sn,f),c(r,_t,f),e(_t,xt),e(xt,us),F(to,us,null),e(_t,Kd),e(_t,gs),e(gs,Gd),c(r,nn,f),c(r,Qe,f),F(ro,Qe,null),e(Qe,Xd),e(Qe,St),e(St,Wd),e(St,vs),e(vs,jd),e(St,Jd),e(St,_s),e(_s,Yd),e(Qe,Zd),e(Qe,oo),e(oo,eh),e(oo,ga),e(ga,th),e(oo,rh),e(Qe,oh),e(Qe,ao),e(ao,ah),e(ao,so),e(so,sh),e(ao,nh),e(Qe,ih),e(Qe,no),e(no,lh),e(no,va),e(va,mh),e(no,dh),c(r,ln,f),c(r,bt,f),e(bt,Bt),e(Bt,bs),F(io,bs,null),e(bt,hh),e(bt,Ts),e(Ts,ch),c(r,mn,f),c(r,Pe,f),F(lo,Pe,null),e(Pe,fh),e(Pe,ks),e(ks,ph),e(Pe,uh),e(Pe,mo),e(mo,gh),e(mo,_a),e(_a,vh),e(mo,_h),e(Pe,bh),e(Pe,ho),e(ho,Th),e(ho,co),e(co,kh),e(ho,Eh),e(Pe,wh),F(Nt,Pe,null),e(Pe,Ch),e(Pe,fo),e(fo,$h),e(fo,ba),e(ba,yh),e(fo,Mh),c(r,dn,f),c(r,Tt,f),e(Tt,Ot),e(Ot,Es),F(po,Es,null),e(Tt,Fh),e(Tt,ws),e(ws,Ph),c(r,hn,f),c(r,Le,f),F(uo,Le,null),e(Le,Lh),e(Le,go),e(go,Rh),e(go,Cs),e(Cs,Ah),e(go,Dh),e(Le,zh),e(Le,vo),e(vo,Ih),e(vo,Ta),e(Ta,qh),e(vo,xh),e(Le,Sh),e(Le,_o),e(_o,Bh),e(_o,bo),e(bo,Nh),e(_o,Oh),e(Le,Uh),F(Ut,Le,null),e(Le,Hh),e(Le,To),e(To,Qh),e(To,ka),e(ka,Vh),e(To,Kh),c(r,cn,f),c(r,kt,f),e(kt,Ht),e(Ht,$s),F(ko,$s,null),e(kt,Gh),e(kt,ys),e(ys,Xh),c(r,fn,f),c(r,Re,f),F(Eo,Re,null),e(Re,Wh),e(Re,Ms),e(Ms,jh),e(Re,Jh),e(Re,wo),e(wo,Yh),e(wo,Ea),e(Ea,Zh),e(wo,ec),e(Re,tc),e(Re,Co),e(Co,rc),e(Co,$o),e($o,oc),e(Co,ac),e(Re,sc),F(Qt,Re,null),e(Re,nc),e(Re,yo),e(yo,ic),e(yo,wa),e(wa,lc),e(yo,mc),c(r,pn,f),c(r,Et,f),e(Et,Vt),e(Vt,Fs),F(Mo,Fs,null),e(Et,dc),e(Et,Ps),e(Ps,hc),c(r,un,f),c(r,Ae,f),F(Fo,Ae,null),e(Ae,cc),e(Ae,Ls),e(Ls,fc),e(Ae,pc),e(Ae,Po),e(Po,uc),e(Po,Ca),e(Ca,gc),e(Po,vc),e(Ae,_c),e(Ae,Lo),e(Lo,bc),e(Lo,Ro),e(Ro,Tc),e(Lo,kc),e(Ae,Ec),F(Kt,Ae,null),e(Ae,wc),e(Ae,Ao),e(Ao,Cc),e(Ao,$a),e($a,$c),e(Ao,yc),c(r,gn,f),c(r,wt,f),e(wt,Gt),e(Gt,Rs),F(Do,Rs,null),e(wt,Mc),e(wt,As),e(As,Fc),c(r,vn,f),c(r,De,f),F(zo,De,null),e(De,Pc),e(De,Ds),e(Ds,Lc),e(De,Rc),e(De,Io),e(Io,Ac),e(Io,ya),e(ya,Dc),e(Io,zc),e(De,Ic),e(De,qo),e(qo,qc),e(qo,xo),e(xo,xc),e(qo,Sc),e(De,Bc),F(Xt,De,null),e(De,Nc),e(De,So),e(So,Oc),e(So,Ma),e(Ma,Uc),e(So,Hc),c(r,_n,f),c(r,Ct,f),e(Ct,Wt),e(Wt,zs),F(Bo,zs,null),e(Ct,Qc),e(Ct,Is),e(Is,Vc),c(r,bn,f),c(r,ze,f),F(No,ze,null),e(ze,Kc),e(ze,$t),e($t,Gc),e($t,qs),e(qs,Xc),e($t,Wc),e($t,xs),e(xs,jc),e($t,Jc),e(ze,Yc),e(ze,Oo),e(Oo,Zc),e(Oo,Fa),e(Fa,ef),e(Oo,tf),e(ze,rf),e(ze,Uo),e(Uo,of),e(Uo,Ho),e(Ho,af),e(Uo,sf),e(ze,nf),F(jt,ze,null),e(ze,lf),e(ze,Qo),e(Qo,mf),e(Qo,Pa),e(Pa,df),e(Qo,hf),Tn=!0},p(r,[f]){const Vo={};f&2&&(Vo.$$scope={dirty:f,ctx:r}),Nt.$set(Vo);const Ss={};f&2&&(Ss.$$scope={dirty:f,ctx:r}),Ut.$set(Ss);const Bs={};f&2&&(Bs.$$scope={dirty:f,ctx:r}),Qt.$set(Bs);const Ns={};f&2&&(Ns.$$scope={dirty:f,ctx:r}),Kt.$set(Ns);const Ko={};f&2&&(Ko.$$scope={dirty:f,ctx:r}),Xt.$set(Ko);const Os={};f&2&&(Os.$$scope={dirty:f,ctx:r}),jt.$set(Os)},i(r){Tn||(P(X.$$.fragment,r),P(W.$$.fragment,r),P(A.$$.fragment,r),P(Y.$$.fragment,r),P(Zt.$$.fragment,r),P(er.$$.fragment,r),P(sr.$$.fragment,r),P(ir.$$.fragment,r),P(mr.$$.fragment,r),P(dr.$$.fragment,r),P(hr.$$.fragment,r),P(pr.$$.fragment,r),P(gr.$$.fragment,r),P(vr.$$.fragment,r),P(_r.$$.fragment,r),P(wr.$$.fragment,r),P(Cr.$$.fragment,r),P(Lr.$$.fragment,r),P(Rr.$$.fragment,r),P(xr.$$.fragment,r),P(Sr.$$.fragment,r),P(Hr.$$.fragment,r),P(Qr.$$.fragment,r),P(Wr.$$.fragment,r),P(jr.$$.fragment,r),P(to.$$.fragment,r),P(ro.$$.fragment,r),P(io.$$.fragment,r),P(lo.$$.fragment,r),P(Nt.$$.fragment,r),P(po.$$.fragment,r),P(uo.$$.fragment,r),P(Ut.$$.fragment,r),P(ko.$$.fragment,r),P(Eo.$$.fragment,r),P(Qt.$$.fragment,r),P(Mo.$$.fragment,r),P(Fo.$$.fragment,r),P(Kt.$$.fragment,r),P(Do.$$.fragment,r),P(zo.$$.fragment,r),P(Xt.$$.fragment,r),P(Bo.$$.fragment,r),P(No.$$.fragment,r),P(jt.$$.fragment,r),Tn=!0)},o(r){L(X.$$.fragment,r),L(W.$$.fragment,r),L(A.$$.fragment,r),L(Y.$$.fragment,r),L(Zt.$$.fragment,r),L(er.$$.fragment,r),L(sr.$$.fragment,r),L(ir.$$.fragment,r),L(mr.$$.fragment,r),L(dr.$$.fragment,r),L(hr.$$.fragment,r),L(pr.$$.fragment,r),L(gr.$$.fragment,r),L(vr.$$.fragment,r),L(_r.$$.fragment,r),L(wr.$$.fragment,r),L(Cr.$$.fragment,r),L(Lr.$$.fragment,r),L(Rr.$$.fragment,r),L(xr.$$.fragment,r),L(Sr.$$.fragment,r),L(Hr.$$.fragment,r),L(Qr.$$.fragment,r),L(Wr.$$.fragment,r),L(jr.$$.fragment,r),L(to.$$.fragment,r),L(ro.$$.fragment,r),L(io.$$.fragment,r),L(lo.$$.fragment,r),L(Nt.$$.fragment,r),L(po.$$.fragment,r),L(uo.$$.fragment,r),L(Ut.$$.fragment,r),L(ko.$$.fragment,r),L(Eo.$$.fragment,r),L(Qt.$$.fragment,r),L(Mo.$$.fragment,r),L(Fo.$$.fragment,r),L(Kt.$$.fragment,r),L(Do.$$.fragment,r),L(zo.$$.fragment,r),L(Xt.$$.fragment,r),L(Bo.$$.fragment,r),L(No.$$.fragment,r),L(jt.$$.fragment,r),Tn=!1},d(r){t(T),r&&t(Z),r&&t(D),R(X),r&&t(Q),r&&t(u),R(W),r&&t(ae),r&&t(H),r&&t(b),r&&t(ee),r&&t(se),r&&t(te),r&&t(ne),r&&t(_),r&&t(B),r&&t(re),r&&t(ie),r&&t(E),r&&t(ke),r&&t(ge),R(A),r&&t(Ee),r&&t(ve),R(Y),r&&t(Hs),r&&t(dt),R(Zt),r&&t(Qs),r&&t(ye),R(er),R(sr),R(ir),R(mr),r&&t(Vs),r&&t(ht),R(dr),r&&t(Ks),r&&t(xe),R(hr),R(pr),R(gr),r&&t(Gs),r&&t(ct),R(vr),r&&t(Xs),r&&t(Se),R(_r),r&&t(Ws),r&&t(ft),R(wr),r&&t(js),r&&t(Be),R(Cr),r&&t(Js),r&&t(pt),R(Lr),r&&t(Ys),r&&t(Ne),R(Rr),r&&t(Zs),r&&t(ut),R(xr),r&&t(en),r&&t(Oe),R(Sr),r&&t(tn),r&&t(gt),R(Hr),r&&t(rn),r&&t(Ue),R(Qr),r&&t(on),r&&t(vt),R(Wr),r&&t(an),r&&t(He),R(jr),r&&t(sn),r&&t(_t),R(to),r&&t(nn),r&&t(Qe),R(ro),r&&t(ln),r&&t(bt),R(io),r&&t(mn),r&&t(Pe),R(lo),R(Nt),r&&t(dn),r&&t(Tt),R(po),r&&t(hn),r&&t(Le),R(uo),R(Ut),r&&t(cn),r&&t(kt),R(ko),r&&t(fn),r&&t(Re),R(Eo),R(Qt),r&&t(pn),r&&t(Et),R(Mo),r&&t(un),r&&t(Ae),R(Fo),R(Kt),r&&t(gn),r&&t(wt),R(Do),r&&t(vn),r&&t(De),R(zo),R(Xt),r&&t(_n),r&&t(Ct),R(Bo),r&&t(bn),r&&t(ze),R(No),R(jt)}}}const Yu={local:"camembert",sections:[{local:"overview",title:"Overview"},{local:"transformers.CamembertConfig",title:"CamembertConfig"},{local:"transformers.CamembertTokenizer",title:"CamembertTokenizer"},{local:"transformers.CamembertTokenizerFast",title:"CamembertTokenizerFast"},{local:"transformers.CamembertModel",title:"CamembertModel"},{local:"transformers.CamembertForCausalLM",title:"CamembertForCausalLM"},{local:"transformers.CamembertForMaskedLM",title:"CamembertForMaskedLM"},{local:"transformers.CamembertForSequenceClassification",title:"CamembertForSequenceClassification"},{local:"transformers.CamembertForMultipleChoice",title:"CamembertForMultipleChoice"},{local:"transformers.CamembertForTokenClassification",title:"CamembertForTokenClassification"},{local:"transformers.CamembertForQuestionAnswering",title:"CamembertForQuestionAnswering"},{local:"transformers.TFCamembertModel",title:"TFCamembertModel"},{local:"transformers.TFCamembertForMaskedLM",title:"TFCamembertForMaskedLM"},{local:"transformers.TFCamembertForSequenceClassification",title:"TFCamembertForSequenceClassification"},{local:"transformers.TFCamembertForMultipleChoice",title:"TFCamembertForMultipleChoice"},{local:"transformers.TFCamembertForTokenClassification",title:"TFCamembertForTokenClassification"},{local:"transformers.TFCamembertForQuestionAnswering",title:"TFCamembertForQuestionAnswering"}],title:"CamemBERT"};function Zu(qe,T,Z){let{fw:D}=T;return qe.$$set=v=>{"fw"in v&&Z(0,D=v.fw)},[D]}class ag extends Ou{constructor(T){super();Uu(this,T,Zu,Ju,Hu,{fw:0})}}export{ag as default,Yu as metadata};
