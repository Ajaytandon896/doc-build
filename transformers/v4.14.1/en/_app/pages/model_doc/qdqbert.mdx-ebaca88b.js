import{S as ir,i as lr,s as pr,e as n,k as h,w as d,t as p,M as cr,c as s,d as t,m as f,a as o,x as u,h as c,b as r,F as a,g as l,y as m,L as hr,q as g,o as v,B as _}from"../../chunks/vendor-ab4e3193.js";import{D as fr}from"../../chunks/Docstring-b69c0bd4.js";import{C as en}from"../../chunks/CodeBlock-516df0c5.js";import{I as w}from"../../chunks/IconCopyLink-d992940d.js";import"../../chunks/CopyButton-204b56db.js";function dr(tn){let z,Ve,y,$,nt,le,an,st,nn,Yt,B,O,ot,pe,sn,rt,on,ea,V,rn,ce,ln,pn,ta,Xe,cn,aa,Ue,it,hn,na,We,fn,sa,E,lt,pt,dn,un,ct,X,mn,he,gn,vn,ht,_n,Qn,ft,fe,bn,dt,wn,yn,En,ut,de,Dn,Je,$n,zn,oa,U,qn,ue,kn,Bn,ra,T,W,mt,me,Tn,gt,xn,ia,Q,jn,vt,An,Pn,ge,Nn,Cn,_t,Mn,Fn,Qt,Sn,In,ve,Ln,Rn,la,J,Hn,bt,On,Vn,pa,_e,ca,x,G,wt,Qe,Xn,yt,Un,ha,Ge,Wn,fa,be,da,j,K,Et,we,Jn,Dt,Gn,ua,q,Kn,ye,Zn,Yn,Ee,es,ts,ma,De,ga,A,Z,$t,$e,as,zt,ns,va,b,ze,ss,P,os,Ke,rs,is,qe,ls,ps,cs,N,hs,Ze,fs,ds,Ye,us,ms,gs,qt,vs,_s,ke,_a,C,Y,kt,Be,Qs,Bt,bs,Qa,Te,Tt,ba,M,ee,xt,xe,ws,jt,ys,wa,je,At,ya,F,te,Pt,Ae,Es,Nt,Ds,Ea,Pe,Ct,Da,S,ae,Mt,Ne,$s,Ft,zs,$a,Ce,St,za,I,ne,It,Me,qs,Lt,ks,qa,Fe,Rt,ka,L,se,Ht,Se,Bs,Ot,Ts,Ba,Ie,Vt,Ta,R,oe,Xt,Le,xs,Ut,js,xa,Re,Wt,ja,H,re,Jt,He,As,Gt,Ps,Aa,Oe,Kt,Pa;return le=new w({}),pe=new w({}),me=new w({}),_e=new en({props:{code:`import pytorch_quantization.nn as quant_nn
from pytorch_quantization.tensor_quant import QuantDescriptor

# The default tensor quantizer is set to use Max calibration method
input_desc = QuantDescriptor(num_bits=8, calib_method="max")
# The default tensor quantizer is set to be per-channel quantization for weights
weight_desc = QuantDescriptor(num_bits=8, axis=((0,)))
quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)
quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> pytorch_quantization.nn <span class="hljs-keyword">as</span> quant_nn</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> pytorch_quantization.tensor_quant <span class="hljs-keyword">import</span> QuantDescriptor</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># The default tensor quantizer is set to use Max calibration method</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">input_desc = QuantDescriptor(num_bits=<span class="hljs-number">8</span>, calib_method=<span class="hljs-string">&quot;max&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># The default tensor quantizer is set to be per-channel quantization for weights</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">weight_desc = QuantDescriptor(num_bits=<span class="hljs-number">8</span>, axis=((<span class="hljs-number">0</span>,)))</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc)</span>`}}),Qe=new w({}),be=new en({props:{code:`# Find the TensorQuantizer and enable calibration
for name, module in model.named_modules():
    if name.endswith('_input_quantizer'):
        module.enable_calib()
        module.disable_quant()  # Use full precision data to calibrate

# Feeding data samples
model(x)
# ...

# Finalize calibration
for name, module in model.named_modules():
    if name.endswith('_input_quantizer'):
        module.load_calib_amax()
        module.enable_quant()

# If running on GPU, it needs to call .cuda() again because new tensors will be created by calibration process
model.cuda()

# Keep running the quantized model
# ...,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># Find the TensorQuantizer and enable calibration</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> model.named_modules():</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">    <span class="hljs-keyword">if</span> name.endswith(<span class="hljs-string">&#x27;_input_quantizer&#x27;</span>):</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">        module.enable_calib()</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">        module.disable_quant()  <span class="hljs-comment"># Use full precision data to calibrate</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># Feeding data samples</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model(x)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># ...</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># Finalize calibration</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> model.named_modules():</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">    <span class="hljs-keyword">if</span> name.endswith(<span class="hljs-string">&#x27;_input_quantizer&#x27;</span>):</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">        module.load_calib_amax()</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">        module.enable_quant()</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># If running on GPU, it needs to call .cuda() again because new tensors will be created by calibration process</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model.cuda()</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># Keep running the quantized model</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># ...</span></span>`}}),we=new w({}),De=new en({props:{code:`from pytorch_quantization.nn import TensorQuantizer
TensorQuantizer.use_fb_fake_quant = True

# Load the calibrated model
...
# ONNX export
torch.onnx.export(...),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> pytorch_quantization.nn <span class="hljs-keyword">import</span> TensorQuantizer</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">TensorQuantizer.use_fb_fake_quant = <span class="hljs-literal">True</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># Load the calibrated model</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">...</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># ONNX export</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">torch.onnx.export(...)</span>`}}),$e=new w({}),ze=new fr({props:{name:"class transformers.QDQBertConfig",anchor:"transformers.QDQBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"use_cache",val:" = True"},{name:"is_encoder_decoder",val:" = False"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/qdqbert/configuration_qdqbert.py#L29",parametersDescription:[{anchor:"transformers.QDQBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the QDQBERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.QDQBertConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.QDQBertConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.QDQBertConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.QDQBertConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.QDQBertConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.QDQBertConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.QDQBertConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.QDQBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.QDQBertConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.QDQBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.QDQBertConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.QDQBertConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}]}}),ke=new en({props:{code:`from transformers import QDQBertModel, QDQBertConfig

# Initializing a QDQBERT bert-base-uncased style configuration
configuration = QDQBertConfig()

# Initializing a model from the bert-base-uncased style configuration
model = QDQBertModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> QDQBertModel, QDQBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a QDQBERT bert-base-uncased style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = QDQBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the bert-base-uncased style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = QDQBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Be=new w({}),xe=new w({}),Ae=new w({}),Ne=new w({}),Me=new w({}),Se=new w({}),Le=new w({}),He=new w({}),{c(){z=n("meta"),Ve=h(),y=n("h1"),$=n("a"),nt=n("span"),d(le.$$.fragment),an=h(),st=n("span"),nn=p("QDQBERT"),Yt=h(),B=n("h2"),O=n("a"),ot=n("span"),d(pe.$$.fragment),sn=h(),rt=n("span"),on=p("Overview"),ea=h(),V=n("p"),rn=p("The QDQBERT model can be referenced in "),ce=n("a"),ln=p(`Integer Quantization for Deep Learning Inference: Principles and Empirical
Evaluation`),pn=p(` by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius
Micikevicius.`),ta=h(),Xe=n("p"),cn=p("The abstract from the paper is the following:"),aa=h(),Ue=n("p"),it=n("em"),hn=p(`Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by
taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of
quantization parameters and evaluate their choices on a wide range of neural network models for different application
domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration
by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is
able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are
more difficult to quantize, such as MobileNets and BERT-large.`),na=h(),We=n("p"),fn=p("Tips:"),sa=h(),E=n("ul"),lt=n("li"),pt=n("p"),dn=p(`QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to (i) linear layer
inputs and weights, (ii) matmul inputs, (iii) residual add inputs, in BERT model.`),un=h(),ct=n("li"),X=n("p"),mn=p("QDQBERT requires the dependency of "),he=n("a"),gn=p("Pytorch Quantization Toolkit"),vn=p(". To install "),ht=n("code"),_n=p("pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com"),Qn=h(),ft=n("li"),fe=n("p"),bn=p("QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example "),dt=n("em"),wn=p("bert-base-uncased"),yn=p(`), and
perform Quantization Aware Training/Post Training Quantization.`),En=h(),ut=n("li"),de=n("p"),Dn=p(`A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for
SQUAD task can be found at `),Je=n("a"),$n=p("transformers/examples/research_projects/quantization-qdqbert/"),zn=p("."),oa=h(),U=n("p"),qn=p("This model was contributed by "),ue=n("a"),kn=p("shangz"),Bn=p("."),ra=h(),T=n("h3"),W=n("a"),mt=n("span"),d(me.$$.fragment),Tn=h(),gt=n("span"),xn=p("Set default quantizers"),ia=h(),Q=n("p"),jn=p(`QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to BERT by
`),vt=n("code"),An=p("TensorQuantizer"),Pn=p(" in "),ge=n("a"),Nn=p("Pytorch Quantization Toolkit"),Cn=p(". "),_t=n("code"),Mn=p("TensorQuantizer"),Fn=p(` is the module
for quantizing tensors, with `),Qt=n("code"),Sn=p("QuantDescriptor"),In=p(" defining how the tensor should be quantized. Refer to "),ve=n("a"),Ln=p(`Pytorch
Quantization Toolkit userguide`),Rn=p(" for more details."),la=h(),J=n("p"),Hn=p("Before creating QDQBERT model, one has to set the default "),bt=n("code"),On=p("QuantDescriptor"),Vn=p(` defining default tensor quantizers.
Example:`),pa=h(),d(_e.$$.fragment),ca=h(),x=n("h3"),G=n("a"),wt=n("span"),d(Qe.$$.fragment),Xn=h(),yt=n("span"),Un=p("Calibration"),ha=h(),Ge=n("p"),Wn=p(`Calibration is the terminology of passing data samples to the quantizer and deciding the best scaling factors for
tensors. After setting up the tensor quantizers, one can use the following example to calibrate the model:`),fa=h(),d(be.$$.fragment),da=h(),j=n("h3"),K=n("a"),Et=n("span"),d(we.$$.fragment),Jn=h(),Dt=n("span"),Gn=p("Export to ONNX"),ua=h(),q=n("p"),Kn=p("The goal of exporting to ONNX is to deploy inference by "),ye=n("a"),Zn=p("TensorRT"),Yn=p(`. Fake
quantization will be broken into a pair of QuantizeLinear/DequantizeLinear ONNX ops. After setting static member of
TensorQuantizer to use Pytorch\u2019s own fake quantization functions, fake quantized model can be exported to ONNX, follow
the instructions in `),Ee=n("a"),es=p("torch.onnx"),ts=p(". Example:"),ma=h(),d(De.$$.fragment),ga=h(),A=n("h2"),Z=n("a"),$t=n("span"),d($e.$$.fragment),as=h(),zt=n("span"),ns=p("QDQBertConfig"),va=h(),b=n("div"),d(ze.$$.fragment),ss=h(),P=n("p"),os=p("This is the configuration class to store the configuration of a "),Ke=n("a"),rs=p("QDQBertModel"),is=p(`. It is used to
instantiate an QDQBERT model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the BERT `),qe=n("a"),ls=p("bert-base-uncased"),ps=p(" architecture."),cs=h(),N=n("p"),hs=p("Configuration objects inherit from "),Ze=n("a"),fs=p("PretrainedConfig"),ds=p(` and can be used to control the model
outputs. Read the documentation from `),Ye=n("a"),us=p("PretrainedConfig"),ms=p(" for more information."),gs=h(),qt=n("p"),vs=p("Examples:"),_s=h(),d(ke.$$.fragment),_a=h(),C=n("h2"),Y=n("a"),kt=n("span"),d(Be.$$.fragment),Qs=h(),Bt=n("span"),bs=p("QDQBertModel"),Qa=h(),Te=n("div"),Tt=n("div"),ba=h(),M=n("h2"),ee=n("a"),xt=n("span"),d(xe.$$.fragment),ws=h(),jt=n("span"),ys=p("QDQBertLMHeadModel"),wa=h(),je=n("div"),At=n("div"),ya=h(),F=n("h2"),te=n("a"),Pt=n("span"),d(Ae.$$.fragment),Es=h(),Nt=n("span"),Ds=p("QDQBertForMaskedLM"),Ea=h(),Pe=n("div"),Ct=n("div"),Da=h(),S=n("h2"),ae=n("a"),Mt=n("span"),d(Ne.$$.fragment),$s=h(),Ft=n("span"),zs=p("QDQBertForSequenceClassification"),$a=h(),Ce=n("div"),St=n("div"),za=h(),I=n("h2"),ne=n("a"),It=n("span"),d(Me.$$.fragment),qs=h(),Lt=n("span"),ks=p("QDQBertForNextSentencePrediction"),qa=h(),Fe=n("div"),Rt=n("div"),ka=h(),L=n("h2"),se=n("a"),Ht=n("span"),d(Se.$$.fragment),Bs=h(),Ot=n("span"),Ts=p("QDQBertForMultipleChoice"),Ba=h(),Ie=n("div"),Vt=n("div"),Ta=h(),R=n("h2"),oe=n("a"),Xt=n("span"),d(Le.$$.fragment),xs=h(),Ut=n("span"),js=p("QDQBertForTokenClassification"),xa=h(),Re=n("div"),Wt=n("div"),ja=h(),H=n("h2"),re=n("a"),Jt=n("span"),d(He.$$.fragment),As=h(),Gt=n("span"),Ps=p("QDQBertForQuestionAnswering"),Aa=h(),Oe=n("div"),Kt=n("div"),this.h()},l(e){const i=cr('[data-svelte="svelte-1phssyn"]',document.head);z=s(i,"META",{name:!0,content:!0}),i.forEach(t),Ve=f(e),y=s(e,"H1",{class:!0});var Na=o(y);$=s(Na,"A",{id:!0,class:!0,href:!0});var Ns=o($);nt=s(Ns,"SPAN",{});var Cs=o(nt);u(le.$$.fragment,Cs),Cs.forEach(t),Ns.forEach(t),an=f(Na),st=s(Na,"SPAN",{});var Ms=o(st);nn=c(Ms,"QDQBERT"),Ms.forEach(t),Na.forEach(t),Yt=f(e),B=s(e,"H2",{class:!0});var Ca=o(B);O=s(Ca,"A",{id:!0,class:!0,href:!0});var Fs=o(O);ot=s(Fs,"SPAN",{});var Ss=o(ot);u(pe.$$.fragment,Ss),Ss.forEach(t),Fs.forEach(t),sn=f(Ca),rt=s(Ca,"SPAN",{});var Is=o(rt);on=c(Is,"Overview"),Is.forEach(t),Ca.forEach(t),ea=f(e),V=s(e,"P",{});var Ma=o(V);rn=c(Ma,"The QDQBERT model can be referenced in "),ce=s(Ma,"A",{href:!0,rel:!0});var Ls=o(ce);ln=c(Ls,`Integer Quantization for Deep Learning Inference: Principles and Empirical
Evaluation`),Ls.forEach(t),pn=c(Ma,` by Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev and Paulius
Micikevicius.`),Ma.forEach(t),ta=f(e),Xe=s(e,"P",{});var Rs=o(Xe);cn=c(Rs,"The abstract from the paper is the following:"),Rs.forEach(t),aa=f(e),Ue=s(e,"P",{});var Hs=o(Ue);it=s(Hs,"EM",{});var Os=o(it);hn=c(Os,`Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by
taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of
quantization parameters and evaluate their choices on a wide range of neural network models for different application
domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration
by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is
able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are
more difficult to quantize, such as MobileNets and BERT-large.`),Os.forEach(t),Hs.forEach(t),na=f(e),We=s(e,"P",{});var Vs=o(We);fn=c(Vs,"Tips:"),Vs.forEach(t),sa=f(e),E=s(e,"UL",{});var ie=o(E);lt=s(ie,"LI",{});var Xs=o(lt);pt=s(Xs,"P",{});var Us=o(pt);dn=c(Us,`QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to (i) linear layer
inputs and weights, (ii) matmul inputs, (iii) residual add inputs, in BERT model.`),Us.forEach(t),Xs.forEach(t),un=f(ie),ct=s(ie,"LI",{});var Ws=o(ct);X=s(Ws,"P",{});var Zt=o(X);mn=c(Zt,"QDQBERT requires the dependency of "),he=s(Zt,"A",{href:!0,rel:!0});var Js=o(he);gn=c(Js,"Pytorch Quantization Toolkit"),Js.forEach(t),vn=c(Zt,". To install "),ht=s(Zt,"CODE",{});var Gs=o(ht);_n=c(Gs,"pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com"),Gs.forEach(t),Zt.forEach(t),Ws.forEach(t),Qn=f(ie),ft=s(ie,"LI",{});var Ks=o(ft);fe=s(Ks,"P",{});var Fa=o(fe);bn=c(Fa,"QDQBERT model can be loaded from any checkpoint of HuggingFace BERT model (for example "),dt=s(Fa,"EM",{});var Zs=o(dt);wn=c(Zs,"bert-base-uncased"),Zs.forEach(t),yn=c(Fa,`), and
perform Quantization Aware Training/Post Training Quantization.`),Fa.forEach(t),Ks.forEach(t),En=f(ie),ut=s(ie,"LI",{});var Ys=o(ut);de=s(Ys,"P",{});var Sa=o(de);Dn=c(Sa,`A complete example of using QDQBERT model to perform Quatization Aware Training and Post Training Quantization for
SQUAD task can be found at `),Je=s(Sa,"A",{href:!0});var eo=o(Je);$n=c(eo,"transformers/examples/research_projects/quantization-qdqbert/"),eo.forEach(t),zn=c(Sa,"."),Sa.forEach(t),Ys.forEach(t),ie.forEach(t),oa=f(e),U=s(e,"P",{});var Ia=o(U);qn=c(Ia,"This model was contributed by "),ue=s(Ia,"A",{href:!0,rel:!0});var to=o(ue);kn=c(to,"shangz"),to.forEach(t),Bn=c(Ia,"."),Ia.forEach(t),ra=f(e),T=s(e,"H3",{class:!0});var La=o(T);W=s(La,"A",{id:!0,class:!0,href:!0});var ao=o(W);mt=s(ao,"SPAN",{});var no=o(mt);u(me.$$.fragment,no),no.forEach(t),ao.forEach(t),Tn=f(La),gt=s(La,"SPAN",{});var so=o(gt);xn=c(so,"Set default quantizers"),so.forEach(t),La.forEach(t),ia=f(e),Q=s(e,"P",{});var D=o(Q);jn=c(D,`QDQBERT model adds fake quantization operations (pair of QuantizeLinear/DequantizeLinear ops) to BERT by
`),vt=s(D,"CODE",{});var oo=o(vt);An=c(oo,"TensorQuantizer"),oo.forEach(t),Pn=c(D," in "),ge=s(D,"A",{href:!0,rel:!0});var ro=o(ge);Nn=c(ro,"Pytorch Quantization Toolkit"),ro.forEach(t),Cn=c(D,". "),_t=s(D,"CODE",{});var io=o(_t);Mn=c(io,"TensorQuantizer"),io.forEach(t),Fn=c(D,` is the module
for quantizing tensors, with `),Qt=s(D,"CODE",{});var lo=o(Qt);Sn=c(lo,"QuantDescriptor"),lo.forEach(t),In=c(D," defining how the tensor should be quantized. Refer to "),ve=s(D,"A",{href:!0,rel:!0});var po=o(ve);Ln=c(po,`Pytorch
Quantization Toolkit userguide`),po.forEach(t),Rn=c(D," for more details."),D.forEach(t),la=f(e),J=s(e,"P",{});var Ra=o(J);Hn=c(Ra,"Before creating QDQBERT model, one has to set the default "),bt=s(Ra,"CODE",{});var co=o(bt);On=c(co,"QuantDescriptor"),co.forEach(t),Vn=c(Ra,` defining default tensor quantizers.
Example:`),Ra.forEach(t),pa=f(e),u(_e.$$.fragment,e),ca=f(e),x=s(e,"H3",{class:!0});var Ha=o(x);G=s(Ha,"A",{id:!0,class:!0,href:!0});var ho=o(G);wt=s(ho,"SPAN",{});var fo=o(wt);u(Qe.$$.fragment,fo),fo.forEach(t),ho.forEach(t),Xn=f(Ha),yt=s(Ha,"SPAN",{});var uo=o(yt);Un=c(uo,"Calibration"),uo.forEach(t),Ha.forEach(t),ha=f(e),Ge=s(e,"P",{});var mo=o(Ge);Wn=c(mo,`Calibration is the terminology of passing data samples to the quantizer and deciding the best scaling factors for
tensors. After setting up the tensor quantizers, one can use the following example to calibrate the model:`),mo.forEach(t),fa=f(e),u(be.$$.fragment,e),da=f(e),j=s(e,"H3",{class:!0});var Oa=o(j);K=s(Oa,"A",{id:!0,class:!0,href:!0});var go=o(K);Et=s(go,"SPAN",{});var vo=o(Et);u(we.$$.fragment,vo),vo.forEach(t),go.forEach(t),Jn=f(Oa),Dt=s(Oa,"SPAN",{});var _o=o(Dt);Gn=c(_o,"Export to ONNX"),_o.forEach(t),Oa.forEach(t),ua=f(e),q=s(e,"P",{});var et=o(q);Kn=c(et,"The goal of exporting to ONNX is to deploy inference by "),ye=s(et,"A",{href:!0,rel:!0});var Qo=o(ye);Zn=c(Qo,"TensorRT"),Qo.forEach(t),Yn=c(et,`. Fake
quantization will be broken into a pair of QuantizeLinear/DequantizeLinear ONNX ops. After setting static member of
TensorQuantizer to use Pytorch\u2019s own fake quantization functions, fake quantized model can be exported to ONNX, follow
the instructions in `),Ee=s(et,"A",{href:!0,rel:!0});var bo=o(Ee);es=c(bo,"torch.onnx"),bo.forEach(t),ts=c(et,". Example:"),et.forEach(t),ma=f(e),u(De.$$.fragment,e),ga=f(e),A=s(e,"H2",{class:!0});var Va=o(A);Z=s(Va,"A",{id:!0,class:!0,href:!0});var wo=o(Z);$t=s(wo,"SPAN",{});var yo=o($t);u($e.$$.fragment,yo),yo.forEach(t),wo.forEach(t),as=f(Va),zt=s(Va,"SPAN",{});var Eo=o(zt);ns=c(Eo,"QDQBertConfig"),Eo.forEach(t),Va.forEach(t),va=f(e),b=s(e,"DIV",{class:!0});var k=o(b);u(ze.$$.fragment,k),ss=f(k),P=s(k,"P",{});var tt=o(P);os=c(tt,"This is the configuration class to store the configuration of a "),Ke=s(tt,"A",{href:!0});var Do=o(Ke);rs=c(Do,"QDQBertModel"),Do.forEach(t),is=c(tt,`. It is used to
instantiate an QDQBERT model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the BERT `),qe=s(tt,"A",{href:!0,rel:!0});var $o=o(qe);ls=c($o,"bert-base-uncased"),$o.forEach(t),ps=c(tt," architecture."),tt.forEach(t),cs=f(k),N=s(k,"P",{});var at=o(N);hs=c(at,"Configuration objects inherit from "),Ze=s(at,"A",{href:!0});var zo=o(Ze);fs=c(zo,"PretrainedConfig"),zo.forEach(t),ds=c(at,` and can be used to control the model
outputs. Read the documentation from `),Ye=s(at,"A",{href:!0});var qo=o(Ye);us=c(qo,"PretrainedConfig"),qo.forEach(t),ms=c(at," for more information."),at.forEach(t),gs=f(k),qt=s(k,"P",{});var ko=o(qt);vs=c(ko,"Examples:"),ko.forEach(t),_s=f(k),u(ke.$$.fragment,k),k.forEach(t),_a=f(e),C=s(e,"H2",{class:!0});var Xa=o(C);Y=s(Xa,"A",{id:!0,class:!0,href:!0});var Bo=o(Y);kt=s(Bo,"SPAN",{});var To=o(kt);u(Be.$$.fragment,To),To.forEach(t),Bo.forEach(t),Qs=f(Xa),Bt=s(Xa,"SPAN",{});var xo=o(Bt);bs=c(xo,"QDQBertModel"),xo.forEach(t),Xa.forEach(t),Qa=f(e),Te=s(e,"DIV",{class:!0});var jo=o(Te);Tt=s(jo,"DIV",{class:!0}),o(Tt).forEach(t),jo.forEach(t),ba=f(e),M=s(e,"H2",{class:!0});var Ua=o(M);ee=s(Ua,"A",{id:!0,class:!0,href:!0});var Ao=o(ee);xt=s(Ao,"SPAN",{});var Po=o(xt);u(xe.$$.fragment,Po),Po.forEach(t),Ao.forEach(t),ws=f(Ua),jt=s(Ua,"SPAN",{});var No=o(jt);ys=c(No,"QDQBertLMHeadModel"),No.forEach(t),Ua.forEach(t),wa=f(e),je=s(e,"DIV",{class:!0});var Co=o(je);At=s(Co,"DIV",{class:!0}),o(At).forEach(t),Co.forEach(t),ya=f(e),F=s(e,"H2",{class:!0});var Wa=o(F);te=s(Wa,"A",{id:!0,class:!0,href:!0});var Mo=o(te);Pt=s(Mo,"SPAN",{});var Fo=o(Pt);u(Ae.$$.fragment,Fo),Fo.forEach(t),Mo.forEach(t),Es=f(Wa),Nt=s(Wa,"SPAN",{});var So=o(Nt);Ds=c(So,"QDQBertForMaskedLM"),So.forEach(t),Wa.forEach(t),Ea=f(e),Pe=s(e,"DIV",{class:!0});var Io=o(Pe);Ct=s(Io,"DIV",{class:!0}),o(Ct).forEach(t),Io.forEach(t),Da=f(e),S=s(e,"H2",{class:!0});var Ja=o(S);ae=s(Ja,"A",{id:!0,class:!0,href:!0});var Lo=o(ae);Mt=s(Lo,"SPAN",{});var Ro=o(Mt);u(Ne.$$.fragment,Ro),Ro.forEach(t),Lo.forEach(t),$s=f(Ja),Ft=s(Ja,"SPAN",{});var Ho=o(Ft);zs=c(Ho,"QDQBertForSequenceClassification"),Ho.forEach(t),Ja.forEach(t),$a=f(e),Ce=s(e,"DIV",{class:!0});var Oo=o(Ce);St=s(Oo,"DIV",{class:!0}),o(St).forEach(t),Oo.forEach(t),za=f(e),I=s(e,"H2",{class:!0});var Ga=o(I);ne=s(Ga,"A",{id:!0,class:!0,href:!0});var Vo=o(ne);It=s(Vo,"SPAN",{});var Xo=o(It);u(Me.$$.fragment,Xo),Xo.forEach(t),Vo.forEach(t),qs=f(Ga),Lt=s(Ga,"SPAN",{});var Uo=o(Lt);ks=c(Uo,"QDQBertForNextSentencePrediction"),Uo.forEach(t),Ga.forEach(t),qa=f(e),Fe=s(e,"DIV",{class:!0});var Wo=o(Fe);Rt=s(Wo,"DIV",{class:!0}),o(Rt).forEach(t),Wo.forEach(t),ka=f(e),L=s(e,"H2",{class:!0});var Ka=o(L);se=s(Ka,"A",{id:!0,class:!0,href:!0});var Jo=o(se);Ht=s(Jo,"SPAN",{});var Go=o(Ht);u(Se.$$.fragment,Go),Go.forEach(t),Jo.forEach(t),Bs=f(Ka),Ot=s(Ka,"SPAN",{});var Ko=o(Ot);Ts=c(Ko,"QDQBertForMultipleChoice"),Ko.forEach(t),Ka.forEach(t),Ba=f(e),Ie=s(e,"DIV",{class:!0});var Zo=o(Ie);Vt=s(Zo,"DIV",{class:!0}),o(Vt).forEach(t),Zo.forEach(t),Ta=f(e),R=s(e,"H2",{class:!0});var Za=o(R);oe=s(Za,"A",{id:!0,class:!0,href:!0});var Yo=o(oe);Xt=s(Yo,"SPAN",{});var er=o(Xt);u(Le.$$.fragment,er),er.forEach(t),Yo.forEach(t),xs=f(Za),Ut=s(Za,"SPAN",{});var tr=o(Ut);js=c(tr,"QDQBertForTokenClassification"),tr.forEach(t),Za.forEach(t),xa=f(e),Re=s(e,"DIV",{class:!0});var ar=o(Re);Wt=s(ar,"DIV",{class:!0}),o(Wt).forEach(t),ar.forEach(t),ja=f(e),H=s(e,"H2",{class:!0});var Ya=o(H);re=s(Ya,"A",{id:!0,class:!0,href:!0});var nr=o(re);Jt=s(nr,"SPAN",{});var sr=o(Jt);u(He.$$.fragment,sr),sr.forEach(t),nr.forEach(t),As=f(Ya),Gt=s(Ya,"SPAN",{});var or=o(Gt);Ps=c(or,"QDQBertForQuestionAnswering"),or.forEach(t),Ya.forEach(t),Aa=f(e),Oe=s(e,"DIV",{class:!0});var rr=o(Oe);Kt=s(rr,"DIV",{class:!0}),o(Kt).forEach(t),rr.forEach(t),this.h()},h(){r(z,"name","hf:doc:metadata"),r(z,"content",JSON.stringify(ur)),r($,"id","qdqbert"),r($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r($,"href","#qdqbert"),r(y,"class","relative group"),r(O,"id","overview"),r(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(O,"href","#overview"),r(B,"class","relative group"),r(ce,"href","https://arxiv.org/abs/2004.09602"),r(ce,"rel","nofollow"),r(he,"href","https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization"),r(he,"rel","nofollow"),r(Je,"href","examples/research_projects/quantization-qdqbert/"),r(ue,"href","https://huggingface.co/shangz"),r(ue,"rel","nofollow"),r(W,"id","set-default-quantizers"),r(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(W,"href","#set-default-quantizers"),r(T,"class","relative group"),r(ge,"href","https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization"),r(ge,"rel","nofollow"),r(ve,"href","https://docs.nvidia.com/deeplearning/tensorrt/pytorch-quantization-toolkit/docs/userguide.html"),r(ve,"rel","nofollow"),r(G,"id","calibration"),r(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(G,"href","#calibration"),r(x,"class","relative group"),r(K,"id","export-to-onnx"),r(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(K,"href","#export-to-onnx"),r(j,"class","relative group"),r(ye,"href","https://developer.nvidia.com/tensorrt"),r(ye,"rel","nofollow"),r(Ee,"href","https://pytorch.org/docs/stable/onnx.html"),r(Ee,"rel","nofollow"),r(Z,"id","transformers.QDQBertConfig"),r(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Z,"href","#transformers.QDQBertConfig"),r(A,"class","relative group"),r(Ke,"href","/docs/transformers/v4.14.1/en/model_doc/qdqbert#transformers.QDQBertModel"),r(qe,"href","https://huggingface.co/bert-base-uncased"),r(qe,"rel","nofollow"),r(Ze,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),r(Ye,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),r(b,"class","docstring"),r(Y,"id","transformers.QDQBertModel"),r(Y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(Y,"href","#transformers.QDQBertModel"),r(C,"class","relative group"),r(Tt,"class","docstring"),r(Te,"class","docstring"),r(ee,"id","transformers.QDQBertLMHeadModel"),r(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ee,"href","#transformers.QDQBertLMHeadModel"),r(M,"class","relative group"),r(At,"class","docstring"),r(je,"class","docstring"),r(te,"id","transformers.QDQBertForMaskedLM"),r(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(te,"href","#transformers.QDQBertForMaskedLM"),r(F,"class","relative group"),r(Ct,"class","docstring"),r(Pe,"class","docstring"),r(ae,"id","transformers.QDQBertForSequenceClassification"),r(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ae,"href","#transformers.QDQBertForSequenceClassification"),r(S,"class","relative group"),r(St,"class","docstring"),r(Ce,"class","docstring"),r(ne,"id","transformers.QDQBertForNextSentencePrediction"),r(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(ne,"href","#transformers.QDQBertForNextSentencePrediction"),r(I,"class","relative group"),r(Rt,"class","docstring"),r(Fe,"class","docstring"),r(se,"id","transformers.QDQBertForMultipleChoice"),r(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(se,"href","#transformers.QDQBertForMultipleChoice"),r(L,"class","relative group"),r(Vt,"class","docstring"),r(Ie,"class","docstring"),r(oe,"id","transformers.QDQBertForTokenClassification"),r(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(oe,"href","#transformers.QDQBertForTokenClassification"),r(R,"class","relative group"),r(Wt,"class","docstring"),r(Re,"class","docstring"),r(re,"id","transformers.QDQBertForQuestionAnswering"),r(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),r(re,"href","#transformers.QDQBertForQuestionAnswering"),r(H,"class","relative group"),r(Kt,"class","docstring"),r(Oe,"class","docstring")},m(e,i){a(document.head,z),l(e,Ve,i),l(e,y,i),a(y,$),a($,nt),m(le,nt,null),a(y,an),a(y,st),a(st,nn),l(e,Yt,i),l(e,B,i),a(B,O),a(O,ot),m(pe,ot,null),a(B,sn),a(B,rt),a(rt,on),l(e,ea,i),l(e,V,i),a(V,rn),a(V,ce),a(ce,ln),a(V,pn),l(e,ta,i),l(e,Xe,i),a(Xe,cn),l(e,aa,i),l(e,Ue,i),a(Ue,it),a(it,hn),l(e,na,i),l(e,We,i),a(We,fn),l(e,sa,i),l(e,E,i),a(E,lt),a(lt,pt),a(pt,dn),a(E,un),a(E,ct),a(ct,X),a(X,mn),a(X,he),a(he,gn),a(X,vn),a(X,ht),a(ht,_n),a(E,Qn),a(E,ft),a(ft,fe),a(fe,bn),a(fe,dt),a(dt,wn),a(fe,yn),a(E,En),a(E,ut),a(ut,de),a(de,Dn),a(de,Je),a(Je,$n),a(de,zn),l(e,oa,i),l(e,U,i),a(U,qn),a(U,ue),a(ue,kn),a(U,Bn),l(e,ra,i),l(e,T,i),a(T,W),a(W,mt),m(me,mt,null),a(T,Tn),a(T,gt),a(gt,xn),l(e,ia,i),l(e,Q,i),a(Q,jn),a(Q,vt),a(vt,An),a(Q,Pn),a(Q,ge),a(ge,Nn),a(Q,Cn),a(Q,_t),a(_t,Mn),a(Q,Fn),a(Q,Qt),a(Qt,Sn),a(Q,In),a(Q,ve),a(ve,Ln),a(Q,Rn),l(e,la,i),l(e,J,i),a(J,Hn),a(J,bt),a(bt,On),a(J,Vn),l(e,pa,i),m(_e,e,i),l(e,ca,i),l(e,x,i),a(x,G),a(G,wt),m(Qe,wt,null),a(x,Xn),a(x,yt),a(yt,Un),l(e,ha,i),l(e,Ge,i),a(Ge,Wn),l(e,fa,i),m(be,e,i),l(e,da,i),l(e,j,i),a(j,K),a(K,Et),m(we,Et,null),a(j,Jn),a(j,Dt),a(Dt,Gn),l(e,ua,i),l(e,q,i),a(q,Kn),a(q,ye),a(ye,Zn),a(q,Yn),a(q,Ee),a(Ee,es),a(q,ts),l(e,ma,i),m(De,e,i),l(e,ga,i),l(e,A,i),a(A,Z),a(Z,$t),m($e,$t,null),a(A,as),a(A,zt),a(zt,ns),l(e,va,i),l(e,b,i),m(ze,b,null),a(b,ss),a(b,P),a(P,os),a(P,Ke),a(Ke,rs),a(P,is),a(P,qe),a(qe,ls),a(P,ps),a(b,cs),a(b,N),a(N,hs),a(N,Ze),a(Ze,fs),a(N,ds),a(N,Ye),a(Ye,us),a(N,ms),a(b,gs),a(b,qt),a(qt,vs),a(b,_s),m(ke,b,null),l(e,_a,i),l(e,C,i),a(C,Y),a(Y,kt),m(Be,kt,null),a(C,Qs),a(C,Bt),a(Bt,bs),l(e,Qa,i),l(e,Te,i),a(Te,Tt),l(e,ba,i),l(e,M,i),a(M,ee),a(ee,xt),m(xe,xt,null),a(M,ws),a(M,jt),a(jt,ys),l(e,wa,i),l(e,je,i),a(je,At),l(e,ya,i),l(e,F,i),a(F,te),a(te,Pt),m(Ae,Pt,null),a(F,Es),a(F,Nt),a(Nt,Ds),l(e,Ea,i),l(e,Pe,i),a(Pe,Ct),l(e,Da,i),l(e,S,i),a(S,ae),a(ae,Mt),m(Ne,Mt,null),a(S,$s),a(S,Ft),a(Ft,zs),l(e,$a,i),l(e,Ce,i),a(Ce,St),l(e,za,i),l(e,I,i),a(I,ne),a(ne,It),m(Me,It,null),a(I,qs),a(I,Lt),a(Lt,ks),l(e,qa,i),l(e,Fe,i),a(Fe,Rt),l(e,ka,i),l(e,L,i),a(L,se),a(se,Ht),m(Se,Ht,null),a(L,Bs),a(L,Ot),a(Ot,Ts),l(e,Ba,i),l(e,Ie,i),a(Ie,Vt),l(e,Ta,i),l(e,R,i),a(R,oe),a(oe,Xt),m(Le,Xt,null),a(R,xs),a(R,Ut),a(Ut,js),l(e,xa,i),l(e,Re,i),a(Re,Wt),l(e,ja,i),l(e,H,i),a(H,re),a(re,Jt),m(He,Jt,null),a(H,As),a(H,Gt),a(Gt,Ps),l(e,Aa,i),l(e,Oe,i),a(Oe,Kt),Pa=!0},p:hr,i(e){Pa||(g(le.$$.fragment,e),g(pe.$$.fragment,e),g(me.$$.fragment,e),g(_e.$$.fragment,e),g(Qe.$$.fragment,e),g(be.$$.fragment,e),g(we.$$.fragment,e),g(De.$$.fragment,e),g($e.$$.fragment,e),g(ze.$$.fragment,e),g(ke.$$.fragment,e),g(Be.$$.fragment,e),g(xe.$$.fragment,e),g(Ae.$$.fragment,e),g(Ne.$$.fragment,e),g(Me.$$.fragment,e),g(Se.$$.fragment,e),g(Le.$$.fragment,e),g(He.$$.fragment,e),Pa=!0)},o(e){v(le.$$.fragment,e),v(pe.$$.fragment,e),v(me.$$.fragment,e),v(_e.$$.fragment,e),v(Qe.$$.fragment,e),v(be.$$.fragment,e),v(we.$$.fragment,e),v(De.$$.fragment,e),v($e.$$.fragment,e),v(ze.$$.fragment,e),v(ke.$$.fragment,e),v(Be.$$.fragment,e),v(xe.$$.fragment,e),v(Ae.$$.fragment,e),v(Ne.$$.fragment,e),v(Me.$$.fragment,e),v(Se.$$.fragment,e),v(Le.$$.fragment,e),v(He.$$.fragment,e),Pa=!1},d(e){t(z),e&&t(Ve),e&&t(y),_(le),e&&t(Yt),e&&t(B),_(pe),e&&t(ea),e&&t(V),e&&t(ta),e&&t(Xe),e&&t(aa),e&&t(Ue),e&&t(na),e&&t(We),e&&t(sa),e&&t(E),e&&t(oa),e&&t(U),e&&t(ra),e&&t(T),_(me),e&&t(ia),e&&t(Q),e&&t(la),e&&t(J),e&&t(pa),_(_e,e),e&&t(ca),e&&t(x),_(Qe),e&&t(ha),e&&t(Ge),e&&t(fa),_(be,e),e&&t(da),e&&t(j),_(we),e&&t(ua),e&&t(q),e&&t(ma),_(De,e),e&&t(ga),e&&t(A),_($e),e&&t(va),e&&t(b),_(ze),_(ke),e&&t(_a),e&&t(C),_(Be),e&&t(Qa),e&&t(Te),e&&t(ba),e&&t(M),_(xe),e&&t(wa),e&&t(je),e&&t(ya),e&&t(F),_(Ae),e&&t(Ea),e&&t(Pe),e&&t(Da),e&&t(S),_(Ne),e&&t($a),e&&t(Ce),e&&t(za),e&&t(I),_(Me),e&&t(qa),e&&t(Fe),e&&t(ka),e&&t(L),_(Se),e&&t(Ba),e&&t(Ie),e&&t(Ta),e&&t(R),_(Le),e&&t(xa),e&&t(Re),e&&t(ja),e&&t(H),_(He),e&&t(Aa),e&&t(Oe)}}}const ur={local:"qdqbert",sections:[{local:"overview",sections:[{local:"set-default-quantizers",title:"Set default quantizers"},{local:"calibration",title:"Calibration"},{local:"export-to-onnx",title:"Export to ONNX"}],title:"Overview"},{local:"transformers.QDQBertConfig",title:"QDQBertConfig"},{local:"transformers.QDQBertModel",title:"QDQBertModel"},{local:"transformers.QDQBertLMHeadModel",title:"QDQBertLMHeadModel"},{local:"transformers.QDQBertForMaskedLM",title:"QDQBertForMaskedLM"},{local:"transformers.QDQBertForSequenceClassification",title:"QDQBertForSequenceClassification"},{local:"transformers.QDQBertForNextSentencePrediction",title:"QDQBertForNextSentencePrediction"},{local:"transformers.QDQBertForMultipleChoice",title:"QDQBertForMultipleChoice"},{local:"transformers.QDQBertForTokenClassification",title:"QDQBertForTokenClassification"},{local:"transformers.QDQBertForQuestionAnswering",title:"QDQBertForQuestionAnswering"}],title:"QDQBERT"};function mr(tn,z,Ve){let{fw:y}=z;return tn.$$set=$=>{"fw"in $&&Ve(0,y=$.fw)},[y]}class wr extends ir{constructor(z){super();lr(this,z,mr,dr,pr,{fw:0})}}export{wr as default,ur as metadata};
